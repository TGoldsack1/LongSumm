{
    "60641106": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "Instance segmentation is a fundamental computer vision problem, which aims to assign pixel-level instance labelling to a given image. While the standard semantic segmentation problem entails assigning class labels to each pixel in an image, it says nothing about the number of instances of each class in the image. Unlike semantic segmentation, instance segmentation is particularly difficult in terms of distinguising nearby and occluded objects. Segmenting at the instance level is useful for many tasks, such as highlighting the outline of objects for improved recognition and allowing robots to delineate and grasp individual objects. Obtaining instance level pixel labels is also significant with respect to general machine understanding of images.\nCounting the objects in an image is also of practical value, and is another problem of interest of this work. Traditionally, counting is performed in a task-specific setting, either by detection followed by regression, or by learning discriminatively with a counting distance metric [8]. Studies in applications such as image question answering [1, 16] also reveal that counting, especially on everyday objects, is a very challenging task on its own [3].\nOne of the main challenges of instance segmentation is object occlusion. Classical object detection pipelines [17] is composed of four stages: proposals, scoring, refinement, and non-maximal suppression (NMS). NMS typically utilizes a hard threshold that is fixed for the entire dataset. In cluttered scenes, NMS may suppress the detection results for a heavily occluded object because it has too much overlap with foreground objects. This challenge remains in the problem of instance segmentation, which is a more difficult version of object detection. One motivation of this work is to introduce a way of performing dynamic NMS to reason about occlusion.\nIn addition to object occlusion, another challenge is the dimensionality of the structured output, which is bounded by the number of pixels times the maximum number of objects. Standard fully\nar X\niv :1\n60 5.\n09 41\n0v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\nconvolutional networks (FCN) [11] will have trouble directly outputting all instance labels in a single shot. Recent work on instance segmentation [21, 27, 26] formulates complex graphical models, which results in a complex and time-consuming pipeline. Furthermore, these models cannot be trained in an end-to-end fashion.\nTo tackle both these challenges, we propose a new model based on a recurrent neural network (RNN) that utilizes visual attention, to perform instance segmentation. Our system addresses the dimensionality issue by using a temporal chain that outputs a single instance at a time. It also performs dynamic NMS, using an object that is already segmented to aid in the discovery of an occluded object later in the sequence. Using an RNN to segment one instance at a time is also inspired by human-like iterative and attentive counting processes. For real-world cluttered scenes, iterative counting with attention will likely perform better than a regression model that operates on the global image level.\nIn this work, we focus on instance segmentation of a single object-type per image. We evaluate our model on a number of challenging datasets: 1) CVPPP leaf segmentation dataset [12]; 2) KITTI car segmentation dataset [5]; and 3) on MS-COCO [10] images, where we train two different models, for \u201cperson\u201d and \u201czebra\u201d categories, and test the model with images that contain at least one instance of the chosen category. We show state-of-the-art performance on both CVPPP and KITTI dataset, and impressive counting ability on MS-COCO."
                },
                {
                    "heading": "2 Recurrent attention model",
                    "text": "Our proposed model has four major components: A) a 2D external memory that tracks the state of the segmented objects; B) a box proposal network responsible for localizing objects of interest; C) a segmentation network for segmenting image pixels within the box; and D) a scoring network that determines if an object instance has been found, and also decides when to stop. See Figure 2 for an illustration of these components.\nNotation. We use the following notation to describe the model architecture: x \u2208 RH\u00d7W is the input image; t indexes the iterations of the model, and \u03c4 indexes the glimpses of the inner RNN; yt, y\u2217t \u2208 (0, 1)H\u00d7W is the segmentation output/ground-truth sequence; st, s\u2217t \u2208 (0, 1) is the confidence score output/ground-truth sequence;W(z) = wT z + b is a learned affine transformation."
                },
                {
                    "heading": "2.1 Part A: Model input and external 2D memory",
                    "text": "We explore three variants of our model which differ in the first component. In one formulation the input is the raw image, and in the others the image is fed into a pretrained fully convolutional network (FCN). This Pretrained FCN has two channels of output. The first is a pixel-level foreground segmentation, produced by a variant of the DeconvNet [13] with skip connections. In addition to predicting this foreground mask, as a second channel we followed the work of Uhrig et al. [23] by producing a 2-d object angle map. For each foreground pixel, we calculate its relative angle towards\nthe centroid of the object, and quantize the angle into 8 different classes, as shown in Figure 3. The angle map forces the model to learn object boundary information, which is missing in the foreground segmentation. The architecture and training of these components are detailed in the Appendix. We denote D0(x) as the pretrained FCN applied to the original image.\nTo facilitate learning to sequentially enumerate the objects, we incorporate an external 2D memory in the RNN structure. We treat this 2D memory as the third channel of the model input. We explore two alternative formulations of this memory: 1) a cumulative canvas that stores the full history of segmentation outputs, and 2) a convolutional-RNN with extra parameters to dynamically adapt memory storage. Both operate at the full input resolution to precisely deal with occlusion.\n1. Cumulative canvas. We hypothesize that providing information of the completed segmentation helps the network reason about occluded objects and determine the next region of interest. The first channel of the canvas keeps adding new pixels from the output of the previous time step.\ndCanvast = [ct, D0(x)] , c Canvas 0 = 0, c Canvas t = max(ct\u22121, yt\u22121) \u2200t > 0 (1)\n2. Convolutional LSTM. One issue of the cumulative canvas is that the recurrent connection from the output of the previous time step into the canvas sometimes leads to training instability. In practice, we observe that reducing the gradient flowing back from the input of the canvas aids training. An alternative is to learn the \u201caddition\u201d operation with another RNN. Convolutional LSTM [20] is a form of RNN that uses convolution as its recurrent operator and thus is able to efficiently process a 2D image input and store a 2D hidden state. We initialize the hidden state of the ConvLSTM with the\nFCN output, and feed the output segmentation back into the ConvLSTM (See Figure 2, right). This allows the gradient to flow through the ConvLSTM without introducing instability.\ndConvLSTM0 = D0(x), d ConvLSTM t = ConvLSTM(dt\u22121, yt\u22121) \u2200t > 0 (2)"
                },
                {
                    "heading": "2.2 Part B: Box network",
                    "text": "The box network localizes objects of interest. The CNN in the box network outputs a H \u2032 \u00d7W \u2032 \u00d7 L feature map ubox,t. We employ a \u201csoft-attention\u201d mechanism here to extract useful information along spatial dimensions and feed a dimension L vector into the glimpse LSTM. Since one single glimpse may not give the upper network enough information to decide where exactly to draw the box, we allow the glimpse LSTM to look at different locations. \u03b1 is initialized to be uniform over all locations, and \u03c4 indexes the glimpses.\nubox,t = CNN(dt), zt,\u03c4 = LSTM( \u2211 h,w \u03b1h,wt,\u03c4 u h,w,l box,t , zt,\u03c4\u22121), \u03b1t,\u03c4+1 = MLP(zt,\u03c4 ) (3)\nWe pass the LSTM\u2019s hidden state through a linear layer to obtain predicted box coordinates. We parameterize the box by its normalized center gX,Y , and log size log \u03b4X,Y . A scaling factor \u03b3 is also predicted by the linear layer, and used when re-projecting the patch to the original image size.\n(g\u0303X,Y , log \u03b4\u0303X,Y , log \u03c3X,Y , \u03b3) =W(zt,end) (4)\ngX , gY = (g\u0303X + 1) W\n2 , (g\u0303Y + 1)\nH\n2 , \u03b4X , \u03b4Y = \u03b4\u0303XW, \u03b4\u0303YH (5)\nExtracting a sub-region. We follow DRAW [7] and use a Gaussian interpolation kernel to extract an N \u00d7 N patch from the x\u0303, a concatenation of the original image with dt. We further allow the model to output rectangular patches to account for different shapes of the object.\n\u00b5iX , \u00b5 j Y = gX + (\u03b4X + 1) \u00b7 (i\u2212N/2 + 0.5)/N, gY + (\u03b4Y + 1) \u00b7 (j \u2212N/2 + 0.5)/N (6)\nFX [a, i], FY [b, j] = 1\u221a\n2\u03c0\u03c3X exp\n( \u2212 (a\u2212 \u00b5 i X) 2\n2\u03c32X\n) ,\n1\u221a 2\u03c0\u03c3Y exp\n( \u2212 (b\u2212 \u00b5jY )2\n2\u03c32Y\n) (7)\npt = Extract(x\u0303t, FY , FX) \u2261 FTY x\u0303tFX (8)"
                },
                {
                    "heading": "2.3 Part C: Segmentation network",
                    "text": "The remaining task is to segment out the pixels that belong to the dominant object within the window. In the segmentation network, we adopt a variant of the DeconvNet [13] with skip connections, which appends deconvolution (or convolution transpose) layers after convolution layers to upsample the low-resolution feature map to a full-size segmentation. After the fully convolutional layers, we get a patch-level segmentation prediction heat map y\u0303t. We then re-project this patch prediction to the original image using the transpose of the previous computed Gaussian filters. The learned \u03b3 to magnifies the signal within the bounding box, and a constant \u03b2 to suppresses the pixels outside the box. Lastly, the sigmoid function produces final segmentation values between 0 and 1.\nyt = sigmoid ( \u03b3 \u00b7 Extract(y\u0303t, FTY , FTX)\u2212 \u03b2 ) (9)"
                },
                {
                    "heading": "2.4 Part D: Scoring network",
                    "text": "To estimate the number of objects in the image, and to terminate our sequential process, we incorporate a scoring network, similar to the one presented in [18]. Our scoring network takes information from the box and segmentation network to produce a score between 0 and 1.\nst = sigmoid(W(zt,end) +W(usegm)) (10)\nTermination condition. We train the entire model with a sequence length determined by the maximum number of objects plus one. During inference, we cut off iterations once the output score goes below 0.5. The loss function (described below) encourages scores to decrease monotonically."
                },
                {
                    "heading": "2.5 Loss functions",
                    "text": "Joint loss. The total loss function is a sum of three losses: the segmentation matching IoU loss Ly; the box IoU loss Lb; and the score cross-entropy loss Ls:\nL(y, b, s) = Ly(y, y \u2217) + Lb(b, b \u2217) + Ls(s, s \u2217) (11)\n(a) Matching IoU loss (mIOU). A primary challenge of instance segmentation involves matching model and ground-truth instances. we compute a maximum-weighted bipartite graph matching between the output instances and ground-truth instances [22] and [18]. Matching makes the loss insensitive to the ordering of the ground-truth instances. Unlike coverage scores proposed in [21] it directly penalizes both false positive and false negative segmentation. The matching weight Mi,j is the IoU score between a pair of segmentation. We use the Hungarian algorithm to compute the matching.\nMi,j = softIOU(yi, y\u2217j ) \u2261 \u2211 yi \u00b7 y\u2217j\u2211\nyi + y\u2217j \u2212 yi \u00b7 y\u2217j (12)\nLy(y, y \u2217) = \u2212mIOU(y, y\u2217) \u2261 \u2212 1\nN \u2211 i,j Mi,j1[match(yi) = y\u2217j ] (13)\n(b) Soft box IoU loss. Although the exact IoU can be derived from the 4-d box coordinates, its gradient vanishes when two boxes do not overlap, which can be problematic for gradient-based learning. Instead, we propose a soft version of the box IoU. We use the same Gaussian filter to re-project a constant patch on the original image, pad the ground-truth boxes and compute the mIOU between the predicted box and the matched padded ground-truth bounding box.\nbt = sigmoid(\u03b3 \u00b7 Extract(1, FTY , FTX)\u2212 \u03b2) (14) Lb(b, b \u2217) = \u2212mIOU(b,Pad(b\u2217)) (15)\n(c) Monotonic score loss. To facilitate automatic termination, the network should output more confident objects first. We proposed a loss function that encourages monotonically decreasing values in the score output. Iterations with target score 1 are compared to the lower bound of preceding scores, and 0 targets to the upper bound of subsequent scores.\nLs(s, s \u2217) = \u2211 t \u2212s\u2217t log ( max t\u2032=t...T\u22121 {st\u2032} ) \u2212 (1\u2212 s\u2217t ) log ( 1\u2212 min t\u2032=0...t {st\u2032} ) (16)"
                },
                {
                    "heading": "2.6 Training procedure and post-processing",
                    "text": "Bootstrap training. The box and segmentation networks rely on the output of each other to make decisions for the next time-step. Because of the coupled nature of the two networks, we propose a bootstrap training procedure: these networks are pre-trained with ground-truth segmentation and boxes, respectively, and in later stages we replace the ground-truth with the model predicted values.\nScheduled sampling. To smooth out the transition between stages, we explore the idea of \u201cscheduled sampling\u201d [2] where we gradually remove the reliance on ground-truth segmentation at the input of the network. As shown in Figure 2, during training there is a dynamic switch in the input of the external memory, to utilize either the maximally overlapping ground-truth instance segmentation, or the output of the network from the previous time step.\nPost-processing. We truncate segmentation outside the predicted foreground mask, fill holes with the labels from the nearest neighboring predicted instance, and remove object segmentation of size smaller than 425 square pixels. We study the effect of post-processing in ablation studies in Table 3."
                },
                {
                    "heading": "3 Related Work",
                    "text": "Instance segmentation has recently received a burst of research attention, as it provides higher level of precision of image understanding compared to object detection and semantic segmentation.\nInstance segmentation using graphical models. An early exploration of instance segmentation proposes a multi-stage pipeline composed of patch-wise features based on deep learning, combined\ninto a segmentation tree [21]. They formulated a new loss function, the coverage score, that calculated the amount of ground truth regions not covered by the model\u2019s instance segmentation. More recently, Zhang et al. [26] formulated a dense CRF for instance segmentation; . They apply a CNN on dense image patches to make local predictions, and constructed a dense CRF to produce globally consistent labellings. Their key contribution is a shifting-label potential that encourages consistency across different patches. They achieved strong results on the challenging KITTI object dataset; however, the graphical model formulation entails long running times, and their energy functions are dependent on instances being connected and having a clear depth ordering.\nInstance segmentation using CNN. Liang et al. [9] used a CNN to generate pixel-level object size information, and used clustering as a post-processing step. They added a regressor at the top of the CNN to estimate the count, which is the total number of clusters. An erroneous count can usually leads to poor segmentation. Dai et al. [4] proposed a pipeline-based approach and won the MS-COCO instance segmentation challenge. Their method first predicts bounding box proposals and extracts regions of interest (ROI), then uses shared features to perform segmentation within each ROI. Their architecture can also be fine-tuned end-to-end. However, since their method is based on detector proposals, it does not explicitly handle object occlusions, which may lead it to fail during non- maximal suppression (NMS). Uhrig et al. [23] presented another approach with FCN, and achieved very impressive results. Their FCN outputs three channels: semantic segmentation, object orientation and depth. Post-processing based on template matching and instance fusion produce the instance identities. Their approach is based on bottom-up clustering since their FCN can only provide pixel-level information, whereas our model is processing the image in a top-down fashion. Importantly, they also used ground-truth depth labels in training their model.\nInstance segmentation using RNN. Another recent line of research, e.g. [22, 15, 18] employs endto-end recurrent neural networks (RNN) to perform object detection and segmentation. A permutation agnostic loss function based on maximum weighted bipartite matching was proposed by [22]. To process an entire image, they treat each element of a 15 \u00d7 20 feature map individually. Similarly, our box proposal network also uses an RNN to generate box proposals: instead of running the image 300 times through the RNN, we only run it once by using a soft attention mechanism [24]. RomeraParedes and Torr [18] use convolutional LSTM (ConvLSTM) [20] to produce instance segmentation directly. However, since their ConvLSTM is required to handle object detection, inhibition, and segmentation all at the same time on a global scale, the final output loses precision. They add a dense CRF to restore the resolution. Compared to their approach, our segmentation network operates on a local level. Instead of resorting to graphical models, we added skip connections to restore the resolution.\nInstance counting. Previous work on object counting in images has mainly focused on crowds of pedestrians and biological cells [8]. Chattopadhyay et al. [3] focused on counting questions in VQA and proposed detector approaches as well as a regression based method (\u201cassociative subitizing\u201d) that works on a 3\u00d7 3 field of CNN features level. Note that unlike our approach, this method does not provide instance segmentations."
                },
                {
                    "heading": "4 Experiments",
                    "text": "CVPPP leaf segmentation. One instance segmentation benchmark is the CVPPP plant leaf dataset [12], which was developed due to the importance of instance segmentation in plant phenotyping. We ran the A1 subset of CVPPP plant leaf segmentation dataset. We trained our model on 128 labelled images, and report results on the 33 test images. We compare our performance to [18], and other top approaches that were published with the CVPPP conference; see the collation study [19] for details of these other approaches.\nKITTI car segmentation. Instance segmentation also provides rich information in the context of autonomous driving. Following [27, 26, 23], we also evaluated the model performance on KITTI car segmentation dataset. We trained the model with 3712 training images, and report performance on 144 test images. We also examine the relative importance of model components via ablation studies.\nMS-COCO counting. Additionally, we train class specific models on MS-COCO and test counting performance on the results. We chose \u201cperson\u201d and \u201czebra\u201d because these are the two of the most common classes in VQA questions. We report counting performance on images with at least one instance of the class: 677 zebra images, and 21,634 \u201cperson\u201d images.\nTable 1: Leaf segmentation and counting performance\nTable 2: Counting performance on MS-COCO\nEvaluation metrics. We report the metrics used by the other studies in the respective benchmarks: symmetric best dice (SBD) for leaf segmentation (see Equations 17, 18) and mean (weighted) coverage (MWCov, MUCov) for car segmentation (see Equations 19, 20). The coverage scores measure the instance-wise IoU for each ground-truth instance averaged over the image; MWCov further weights the score by the size of the ground-truth instance segmentation (larger objects get larger weights).\nDICE(A,B) = 2|A \u222aB| |A|+ |B|\nBD({Ai}, B) = max i DICE(Ai, B) (17)\nSBD({y\u0302i}, {yj}) = min  1 N \u2211 j BD({y\u0302i}, yj), 1 N \u2211 i BD(y\u0302i, {yj})  (18) MWCov({yi}, {y\u2217j }) = 1\nN \u2211 i |yi|\u2211 i |yi| max j IoU(yi, y\u2217j ) (19)\nMUCov({yi}, {y\u2217j }) = 1\nN \u2211 i max j IoU(yi, y\u2217j ) (20)\nCounting is measured in absolute difference in count (|DiC|) (see Equation 21), average false positive (AvgFP), and average false negative (AvgFN). False positive is the number of predicted instances that do not overlap with the ground-truth, and false negative is the number of ground-truth instances that do not overlap with the prediction.\n|DiC| = 1 N \u2211 i |counti \u2212 count\u2217i | (21)"
                },
                {
                    "heading": "4.1 Results & discussion",
                    "text": "In the leaf segmentation task, our best model outperforms the previous state-of-the-art by a large margin in both segmentation and counting. Table 1 shows that the models with FCN overfit and scores lower than the simpler version. This is sensible as the dataset size is small, and including the FCN significantly increases the input dimension and number of parameters.\nIn the car segmentation task, our model achieves the state-of-the-art MWCov shown in Table 3, but our MUCov is lower than results reported by Uhrig et al. [23]. One possible explanation is their inclusion of depth information during training, which may help the model disambiguate distant object boundaries. Moreover, their bottom-up \u201cinstance fusion\u201d method plays a crucial role (omitting this leads to a steep performance drop); this likely helps segment smaller objects, whereas our box network does not reliably detect distant cars.\nIn the zebra counting task, we found that our model outperforms the detector and NMS method, and associative-subitizing methods [3], but we are not doing as well in the person category. However, relative to these regression-based methods, our model permits insight into the recognition of each instance by inspecting the output segmentation. Figure 6 shows the relation between counting performance and number of instances. Mean absolute difference in count is around 1 for up to 18 leaves, 7 cars, 4 zebras and 3 people.\nFrom the figures above we see our model is handling a significant amount of object occlusion and truncation. We verified that the external memory helps with the counting process as the network first segments the more salient objects and then accounts for the occluded instances. In addition, our segmentation network can handle a range of object sizes because of the design of the box network.\nWe found that using scheduled sampling results in much better performance. It helps by making training resemble testing, gradually forcing the model to carry out a full sequence during training instead of relying on ground-truth input. Finally, the convolutional and attentional architecture significantly reduces the number of parameters and the performance is quite strong despite being trained with only 100 leaf images and 1000 zebra images."
                },
                {
                    "heading": "5 Conclusion",
                    "text": "In this work, we borrow intuition from human counting and formulate instance segmentation and counting as a recurrent attentive process. Our end-to-end recurrent architecture demonstrates significant improvement compared to earlier exploration of using RNN on the same tasks, and shows state-of-the-art results on challenging leaf and car segmentation datasets. We address the classic object occlusion problem with a recurrent external memory, and the attention structure brings segmentation at a fine resolution. Our model also shows promising counting performance on a portion of the MS-COCO dataset."
                },
                {
                    "heading": "A Training procedure specification",
                    "text": "We used the Adam optimizer with learning rate 0.001 and batch size of 8. The learning rate is multiplied by 0.85 for every 5000 steps of training.\nA.1 Scheduled sampling\nWe denote \u03b8t as the probability of feeding in ground-truth segmentation that has the greatest overlap with the previous prediction, as opposed to model output. \u03b8t follows exponential decay as training goes on, and for larger t, the decay occurs later:\n\u03b8t = min ( \u0393t exp ( \u2212epoch\u2212 S\nS2\n) , 1 ) (22)\n\u0393t = 1 + log(1 +Kt) (23)\nwhere epoch is the training index, S, S2, and K are constants. In the experiments reported here, their values are 10000, 2885, and 3."
                },
                {
                    "heading": "B Model architecture",
                    "text": "B.1 Foreground + Orientation FCN\nWe resize the image to uniform size. For CVPPP and MS-COCO dataset, we adopt a uniform size of 224\u00d7 224, and for KITTI, we adopt 128\u00d7 448. Table 4 lists the specification of all layers.\nTable 5: External memory specification\nName Filter spec Size CVPPP/MS-COCO Size KITTI\nConvLSTM 3\u00d7 3 224\u00d7 224\u00d7 9 128\u00d7 448\u00d7 9\nB.2 External memory\nB.3 Box network\nThe box network takes in 9 channel input. Either directly from the output of the FCN, or from the hidden state of the ConvLSTM. It goes through a CNN structure again and uses the attention vector predicted by the LSTM to perform dynamic pooling in the last layer. The CNN hyperparameters are listed in Table 6 and the LSTM and glimpse MLP hyperparameters are listed in Table 7. The glimpse MLP takes input from the hidden state of the LSTM and ouputs a vector of normalized weighting over all the box CNN feature map spatial grids.\nB.4 Segmentation network\nThe segmentation networks takes in a patch size 48 \u00d7 48 with multiple channels. The first three channels are the original image R, G, B channels. Then there are 8 channels of orientation angles, and then 1 channel of foreground heat map, all predicted by FCN. Full detail is listed in Table 8. Constant \u03b2 is chosen to be 5."
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "VQA: Visual question answering",
                    "authors": [
                        "S. Antol",
                        "A. Agrawal",
                        "J. Lu",
                        "M. Mitchell",
                        "D. Batra",
                        "C.L. Zitnick",
                        "D. Parikh"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                },
                {
                    "title": "Scheduled sampling for sequence prediction with recurrent neural networks",
                    "authors": [
                        "S. Bengio",
                        "O. Vinyals",
                        "N. Jaitly",
                        "N. Shazeer"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Counting everyday objects in everyday scenes",
                    "authors": [
                        "P. Chattopadhyay",
                        "R. Vedantam",
                        "R.S. Ramprasaath",
                        "D. Batra",
                        "D. Parikh"
                    ],
                    "venue": "CoRR, abs/1604.03505,",
                    "year": 2016
                },
                {
                    "title": "Instance-aware semantic segmentation via multi-task network cascades",
                    "authors": [
                        "J. Dai",
                        "K. He",
                        "J. Sun"
                    ],
                    "venue": "CoRR, abs/1512.04412,",
                    "year": 2015
                },
                {
                    "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
                    "authors": [
                        "A. Geiger",
                        "P. Lenz",
                        "R. Urtasun"
                    ],
                    "venue": "CVPR,",
                    "year": 2012
                },
                {
                    "title": "Learning to count leaves in rosette plants",
                    "authors": [
                        "M.V. Giuffrida",
                        "M. Minervini",
                        "S. Tsaftaris"
                    ],
                    "venue": "Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP),",
                    "year": 2015
                },
                {
                    "title": "DRAW: A recurrent neural network for image generation",
                    "authors": [
                        "K. Gregor",
                        "I. Danihelka",
                        "A. Graves",
                        "D.J. Rezende",
                        "D. Wierstra"
                    ],
                    "venue": "ICML,",
                    "year": 2015
                },
                {
                    "title": "Learning to count objects in images",
                    "authors": [
                        "V.S. Lempitsky",
                        "A. Zisserman"
                    ],
                    "venue": "NIPS,",
                    "year": 2010
                },
                {
                    "title": "Proposal-free network for instance-level object segmentation",
                    "authors": [
                        "X. Liang",
                        "Y. Wei",
                        "X. Shen",
                        "J. Yang",
                        "L. Lin",
                        "S. Yan"
                    ],
                    "venue": "CoRR, abs/1509.02636,",
                    "year": 2015
                },
                {
                    "title": "Microsoft COCO: Common Objects in Context",
                    "authors": [
                        "T. Lin",
                        "M. Maire",
                        "S. Belongie",
                        "J. Hays",
                        "P. Perona",
                        "D. Ramanan",
                        "P. Doll\u00e1r",
                        "C.L. Zitnick"
                    ],
                    "venue": "ECCV,",
                    "year": 2014
                },
                {
                    "title": "Fully convolutional networks for semantic segmentation",
                    "authors": [
                        "J. Long",
                        "E. Shelhamer",
                        "T. Darrell"
                    ],
                    "venue": "CVPR,",
                    "year": 2015
                },
                {
                    "title": "Finely-grained annotated datasets for imagebased plant phenotyping",
                    "authors": [
                        "M. Minervini",
                        "A. Fischbach",
                        "H. Scharr",
                        "S.A. Tsaftaris"
                    ],
                    "venue": "Pattern Recognition Letters,",
                    "year": 2015
                },
                {
                    "title": "Learning deconvolution network for semantic segmentation",
                    "authors": [
                        "H. Noh",
                        "S. Hong",
                        "B. Han"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                },
                {
                    "title": "3-d histogram-based segmentation and leaf detection for rosette plants",
                    "authors": [
                        "J. Pape",
                        "C. Klukas"
                    ],
                    "venue": "ECCV Workshops,",
                    "year": 2014
                },
                {
                    "title": "Learning to decompose for object detection and instance segmentation",
                    "authors": [
                        "E. Park",
                        "A.C. Berg"
                    ],
                    "venue": "CoRR, abs/1511.06449,",
                    "year": 2015
                },
                {
                    "title": "Exploring models and data for image question answering",
                    "authors": [
                        "M. Ren",
                        "R. Kiros",
                        "R.S. Zemel"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
                    "authors": [
                        "S. Ren",
                        "K. He",
                        "R.B. Girshick",
                        "J. Sun"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Recurrent instance segmentation",
                    "authors": [
                        "B. Romera-Paredes",
                        "P.H.S. Torr"
                    ],
                    "venue": "CoRR, abs/1511.08250,",
                    "year": 2015
                },
                {
                    "title": "Leaf segmentation in plant phenotyping: A collation study",
                    "authors": [
                        "H. Scharr",
                        "M. Minervini",
                        "A.P. French",
                        "C. Klukas",
                        "D.M. Kramer",
                        "X. Liu",
                        "I. Luengo",
                        "J. Pape",
                        "G. Polder",
                        "D. Vukadinovic",
                        "X. Yin",
                        "S.A. Tsaftaris"
                    ],
                    "venue": "Mach. Vis. Appl., 27(4):585\u2013606,",
                    "year": 2016
                },
                {
                    "title": "Convolutional LSTM network: A machine learning approach for precipitation nowcasting",
                    "authors": [
                        "X. Shi",
                        "Z. Chen",
                        "H. Wang",
                        "D. Yeung",
                        "W. Wong",
                        "W. Woo"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Instance segmentation of indoor scenes using a coverage loss",
                    "authors": [
                        "N. Silberman",
                        "D. Sontag",
                        "R. Fergus"
                    ],
                    "venue": "ECCV,",
                    "year": 2014
                },
                {
                    "title": "End-to-end people detection in crowded scenes",
                    "authors": [
                        "R. Stewart",
                        "M. Andriluka"
                    ],
                    "venue": "CoRR, abs/1506.04878,",
                    "year": 2016
                },
                {
                    "title": "Pixel-level encoding and depth layering for instance-level semantic labeling",
                    "authors": [
                        "J. Uhrig",
                        "M. Cordts",
                        "U. Franke",
                        "T. Brox"
                    ],
                    "venue": "CoRR, abs/1604.05096,",
                    "year": 2016
                },
                {
                    "title": "Show, attend and tell: Neural image caption generation with visual attention",
                    "authors": [
                        "K. Xu",
                        "J. Ba",
                        "R. Kiros",
                        "K. Cho",
                        "A.C. Courville",
                        "R. Salakhutdinov",
                        "R.S. Zemel",
                        "Y. Bengio"
                    ],
                    "venue": "ICML,",
                    "year": 2015
                },
                {
                    "title": "Multi-leaf tracking from fluorescence plant videos",
                    "authors": [
                        "X. Yin",
                        "X. Liu",
                        "J. Chen",
                        "D.M. Kramer"
                    ],
                    "venue": "ICIP,",
                    "year": 2014
                },
                {
                    "title": "Instance-level segmentation with deep densely connected MRFs",
                    "authors": [
                        "Z. Zhang",
                        "S. Fidler",
                        "R. Urtasun"
                    ],
                    "venue": "CoRR, abs/1512.06735,",
                    "year": 2015
                },
                {
                    "title": "Monocular object instance segmentation and depth ordering with CNNs",
                    "authors": [
                        "Z. Zhang",
                        "A.G. Schwing",
                        "S. Fidler",
                        "R. Urtasun"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                }
            ],
            "id": "SP:e3b3ed427ce635d059dffde6abfceaf21cefb8a5",
            "authors": [
                {
                    "name": "Mengye Ren",
                    "affiliations": []
                },
                {
                    "name": "Richard S. Zemel",
                    "affiliations": []
                }
            ],
            "abstractText": "While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset [12] and KITTI vehicle segmentation dataset [5].",
            "title": "End-to-End Instance Segmentation and Counting with Recurrent Attention"
        }
    },
    "70176908": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "The depth of deep neural networks confers representational power, but also makes model optimization more challenging. Training deep networks with gradient descent based methods is known to be difficult as a consequence of the vanishing and exploding gradient problem (Hochreiter & Schmidhuber, 1997). Typically, exploding gradients are avoided by clipping large gradients (Pascanu et al., 2013) or introducing an L2 or L1 weight norm penalty. The latter has the effect of bounding the spectral radius of the linear transformations, thus limiting the maximal gain across the transformation. Krueger & Memisevic (2015) attempt to stabilize the norm of propagating signals directly by penalizing differences in successive norm pairs in the forward pass and Pascanu et al. (2013) propose to penalize successive gradient norm pairs in the backward pass. These regularizers affect the network parameterization with respect to the data instead of penalizing weights directly.\nBoth expansivity and contractivity of linear transformations can also be limited by more tightly bounding their spectra. By limiting the transformations to be orthogonal, their singular spectra are limited to unitary gain causing the transformations to be norm-preserving. Le et al. (2015) and Henaff et al. (2016) have respectively shown that identity initialization and orthogonal initialization can be beneficial. Arjovsky et al. (2015) have gone beyond initialization, building unitary recurrent neural network (RNN) models with transformations that are unitary by construction which they achieved by composing multiple basic unitary transformations. The resulting transformations, for some n-dimensional input, cover only some subset of possible n \u00d7 n unitary matrices but appear to perform well on simple tasks and have the benefit of having low complexity in memory and computation.\nThe entire set of possible unitary or orthogonal parameterizations forms the Stiefel manifold. At a much higher computational cost, gradient descent optimization directly along this manifold can be done via geodesic steps (Nishimori, 2005; Tagare, 2011). Recent work (Wisdom et al., 2016) has proposed the optimization of unitary matrices along the Stiefel manifold using geodesic gradient descent. To produce a full-capacity parameterization for unitary matrices they use some insights\nar X\niv :1\n70 2.\n00 07\n1v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20\nfrom Tagare (2011), combining the use of a canonical inner products and Cayley transformations. Their experimental work indicates that full capacity unitary RNN models can solve the copy memory problem whereas both LSTM networks and restricted capacity unitary RNN models having similar complexity appear unable to solve the task for a longer sequence length (T = 2000).\nIn contrast, here we explore the optimization of real valued matrices within a configurable margin about the Stiefel manifold. We suspect that a strong constraint of orthogonality limits the model\u2019s representational power, hindering its performance, and may make optimization more difficult. We explore this hypothesis empirically by employing a factorization technique that allows us to limit the degree of deviation from the Stiefel manifold. While we use geodesic gradient descent, we simultaneously update the singular spectra of our matrices along Euclidean steps, allowing optimization to step away from the manifold while still curving about it."
                },
                {
                    "heading": "1.1 VANISHING AND EXPLODING GRADIENTS",
                    "text": "The issue of vanishing and exploding gradients as it pertains to the parameterization of neural networks can be illuminated by looking at the gradient back-propagation chain through a network.\nA neural network with n hidden layers has pre-activations\nai(hi\u22121) = Wi hi\u22121 + bi, i \u2208 {2, \u00b7 \u00b7 \u00b7 , n} (1)\nFor notational convenience, we combine parameters Wi and bi to form an affine matrix \u03b8. We can see that for some loss function L at layer n , the derivative with respect to parameters \u03b8i is:\n\u2202L \u2202\u03b8i = \u2202an+1 \u2202\u03b8i \u2202L \u2202an+1 (2)\nThe partial derivatives for the pre-activations can be decomposed as follows:\n\u2202ai+1 \u2202\u03b8i = \u2202ai \u2202\u03b8i \u2202hi \u2202ai \u2202ai+1 \u2202hi\n= \u2202ai \u2202\u03b8i DiWi+1 \u2192 \u2202ai+1 \u2202ai = DiWi+1,\n(3)\nwhere Di is the Jacobian corresponding to the activation function, containing partial derivatives of the hidden units at layer i + 1 with respect to the pre-activation inputs. Typically, D is diagonal. Following the above, the gradient in equation 2 can be fully decomposed into a recursive chain of matrix products:\n\u2202L \u2202\u03b8i = \u2202ai \u2202\u03b8i n\u220f j=i (DjWj+1) \u2202L \u2202an+1 (4)\nIn (Pascanu et al., 2013), it is shown that the 2-norm of \u2202ai+1 \u2202ai is bounded by the product of the norms of the non-linearity\u2019s Jacobian and transition matrix at time t (layer i ), as follows:\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2202at+1\u2202at \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 ||Dt|| ||Wt|| \u2264 \u03bbDt \u03bbWt = \u03b7t,\n\u03bbDt , \u03bbWt \u2208 R. (5)\nwhere \u03bbDt and \u03bbWt are the largest singular values of the non-linearity\u2019s Jacobian Dt and the transition matrix Wt . In RNNs, Wt is shared across time and can be simply denoted as W.\nEquation 5 shows that the gradient can grow or shrink at each layer depending on the gain of each layer\u2019s linear transformation W and the gain of the Jacobian D. The gain caused by each layer is magnified across all time steps or layers. It is easy to have extreme amplification in a recurrent neural network where W is shared across time steps and a non-unitary gain in W is amplified exponentially. The phenomena of extreme growth or contraction of the gradient across time steps or layers are known as the exploding and the vanishing gradient problems, respectively. It is sufficient for RNNs to have \u03b7t \u2264 1 at each time t to enable the possibility of vanishing gradients, typically for some large number of time steps T . The rate at which a gradient (or forward signal) vanishes\ndepends on both the parameterization of the model and on the input data. The parameterization may be conditioned by placing appropriate constraints on W. It is worth keeping in mind that the Jacobian D is typically contractive, thus tending to be norm-reducing) and is also data-dependent, whereas W can vary from being contractive to norm-preserving, to expansive and applies the same gain on the forward signal as on the back-propagated gradient signal."
                },
                {
                    "heading": "2 OUR APPROACH",
                    "text": "Vanishing and exploding gradients can be controlled to a large extent by controlling the maximum and minimum gain of W. The maximum gain of a matrix W is given by the spectral norm which is given by\n||W||2 = max [ ||Wx|| ||x|| ] . (6)\nBy keeping our weight matrix W close to orthogonal, one can ensure that it is close to a normpreserving transformation (where the spectral norm is equal to one, but the minimum gain is also one). One way to achieve this is via a simple soft constraint or regularization term of the form:\n\u03bb \u2211 i ||WTi Wi \u2212 I||2. (7)\nHowever, it is possible to formulate a more direct parameterization or factorization for W which permits hard bounds on the amount of expansion and contraction induced by W. This can be achieved by simply parameterizing W according to its singular value decomposition, which consists of the composition of orthogonal basis matrices U and V with a diagonal spectral matrix S containing the singular values which are real and positive by definition. We have\nW = USVT . (8)\nSince the spectral norm or maximum gain of a matrix is equal to its largest singular value, this decomposition allows us to control the maximum gain or expansivity of the weight matrix by controlling the magnitude of the largest singular value. Similarly, the minimum gain or contractivity of a matrix can be obtained from the minimum singular value.\nWe can keep the bases U and V orthogonal via geodesic gradient descent along the set of weights that satisfy UTU = I and VTV = I respectively. The submanifolds that satisfy these constraints are called Stiefel manifolds. We discuss how this is achieved in more detail below, then discuss our construction for bounding the singular values.\nDuring optimization, in order to maintain the orthogonality of an orthogonally-initialized matrix M, i.e. where M = U, M = V or M = W if so desired, we employ a Cayley transformation of the update step onto the Stiefel manifold of (semi-)orthogonal matrices, as in Nishimori (2005) and Tagare (2011). Given an orthogonally-initialized parameter matrix M and its Jacobian, G with respect to the objective function, an update is performed as follows:\nA = GMT \u2212MGT\nMnew = M+ (I+ \u03b7 2 A)\u22121(I\u2212 \u03b7 2 A),\n(9)\nwhere A is a skew-symmetric matrix (that depends on the Jacobian and on the parameter matrix) which is mapped to an orthogonal matrix via a Cayley transform and \u03b7 is the learning rate.\nWhile the update rule in (9) allows us to maintain an orthogonal hidden to hidden transition matrix W if desired, we are interested in exploring the effect of stepping away from the Stiefel manifold. As such, we parameterize the transition matrix W in factorized form, as a singular value decomposition with orthogonal bases U and V updated by geodesic gradient descent using the Cayley transform approach above.\nIf W is an orthogonal matrix, the singular values in the diagonal matrix S are all equal to one. However, in our formulation we allow these singular values to deviate from one and employ a sigmoidal parameterization to apply a hard constraint on the maximum and minimum amount of\ndeviation. Specifically, we define a margin m around 1 within which the singular values must lie. This is achieved with the parameterization\nsi = 2m(\u03c3(pi)\u2212 0.5) + 1, si \u2208 {diag(S)}, m \u2208 [0, 1]. (10)\nThe singular values are thus restricted to the range [1\u2212m, 1 +m] and the underlying parameters pi are updated freely via stochastic gradient descent. Note that this parameterization strategy also has implications on the step sizes that gradient descent based optimization will take when updating the singular values \u2013 they tend to be smaller compared to models with no margin constraining their values. Specifically, a singular value\u2019s progression toward a margin is slowed the closer it is to the margin. The sigmoidal parameterization can also impart another effect on the step size along the spectrum which needs to be accounted for. Considering 10, the gradient backpropagation of some loss L toward parameters pi is found as\ndL dpi = dsi dpi dL dsi = 2m d\u03c3(pi) dpi dL dsi . (11)\nFrom (11), it can be seen that the magnitude of the update step for pi is scaled by the margin hyperparameter m . This means for example that for margins less than one, the effective learning rate for the spectrum is reduced in proportion to the margin. Consequently, we adjust the learning rate along the spectrum to be independent of the margin by renormalizing it by 2m .\nThis margin formulation both guarantees singular values lie within a well defined range and slows deviation from orthogonality. Alternatively, one could enforce the orthogonality of U and V and impose a regularization term corresponding to a mean one Gaussian prior on these singular values. This encourages the weight matrix W to be norm preserving with a controllable strength equivalent to the variance of the Gaussian. We also explore this approach further below."
                },
                {
                    "heading": "3 EXPERIMENTS",
                    "text": "In this section, we explore hard and soft orthogonality constraints on factorized weight matrices for recurrent neural network hidden to hidden transitions. With hard orthogonality constraints on U and V, we investigate the effect of widening the spectral margin or bounds on convergence and performance. Loosening these bounds allows increasingly larger margins within which the transition matrix W can deviate from orthogonality. We confirm that orthogonal initialization is useful as noted in Henaff et al. (2016), and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence. We begin our analyses on tasks that are designed to stress memory: a sequence copying task and a basic addition task (Hochreiter & Schmidhuber, 1997). We then move on to tasks on real data that require models to capture long-range dependencies: digit classification based on sequential and permuted MNIST vectors (Le et al., 2015; LeCun et al., 1998). Finally, we look at a basic language modeling task using the Penn Treebank dataset (Marcus et al., 1993).\nThe copy and adding tasks, introduced by Hochreiter & Schmidhuber (1997), are synthetic benchmarks with pathologically hard long distance dependencies that require long-term memory in models. The copy task consists of an input sequence that must be remembered by the network, followed by a series of blank inputs terminated by a delimiter that denotes the point at which the network must begin to output a copy of the initial sequence. We use an input sequence of T + 20 elements that begins with a sub-sequence of 10 elements to copy, each containing a symbol ai \u2208 {a1 , ..., ap} out of p = 8 possible symbols. This sub-sequence is followed by T \u2212 1 elements of the blank category a0 which is terminated at step T by a delimiter symbol ap+1 and 10 more elements of the blank category. The network must learn to remember the initial 10 element sequence for T time steps and output it after receiving the delimiter symbol.\nThe goal of the adding task is to add two numbers together after a long delay. Each number is randomly picked at a unique position in a sequence of length T . The sequence is composed of T values sampled from a uniform distribution in the range [0, 1), with each value paired with an indicator value that identifies the value as one of the two numbers to remember (marked 1) or as a value to ignore (marked 0). The two numbers are positioned randomly in the sequence, the first in the range [0, T2 \u2212 1] and the second in the range [ T 2 , T \u2212 1], where 0 marks the first element. The network must learn to identify and remember the two numbers and output their sum.\nThe sequential MNIST task from Le et al. (2015), MNIST digits are flattened into vectors that can be traversed sequentially by a recurrent neural network. The goal is to classify the digit based on the sequential input of pixels. The simple variant of this task is with a simple flattening of the image matrices; the harder variant of this task includes a random permutation of the pixels in the input vector that is determined once for an experiment. The latter formulation introduces longer distance dependencies between pixels that must be interpreted by the classification model.\nThe English Penn Treebank (PTB) dataset from Marcus et al. (1993) is an annotated corpus of English sentences, commonly used for benchmarking language models. We employ a sequential character prediction task: given a sentence, a recurrent neural network must predict the next character at each step, from left to right. We use input sequences of variable length, with each sequence containing one sentence. We model 49 characters including lowercase letters (all strings are in lowercase), numbers, common punctuation, and an unknown character placeholder. In our experiments on two subsets of the data: in the first, we first use 23% of the data with strings with up to 75 characters and in the second we include over 99% of the dataset, picking strings with up to 300 characters."
                },
                {
                    "heading": "3.1 LOOSENING HARD ORTHOGONALITY CONSTRAINTS",
                    "text": "In this section, we experimentally explore the effect of loosening hard orthogonality constraints through loosening the spectral margin defined above for the hidden to hidden transition matrix.\nIn all experiments, we employed RMSprop (Tieleman & Hinton, 2012) when not using geodesic gradient descent. We used minibatches of size 50 and for generated data (the copy and adding tasks), we assumed an epoch length of 100 minibatches. We cautiously introduced gradient clipping at magnitude 100 (unless stated otherwise) in all of our RNN experiments although it may not be required and we consistently applied a small weight decay of 0.0001. Unless otherwise specified, we trained all simple recurrent neural networks with the hidden to hidden matrix factorization as in (8) using geodesic gradient descent on the bases (learning rate 10\u22126) and RMSprop on the other parameters (learning rate 0.0001), using a tanh transition nonlinearity, and clipping gradients of 100 magnitude. The neural network code was built on the Theano framework (Theano Development Team, 2016). When parameterizing a matrix in factorized form, we apply the weight decay on the composite matrix rather than on the factors in order to be consistent across experiments. For MNIST and PTB, test set metrics were computed based on the parameterization that gave the best validation set accuracy."
                },
                {
                    "heading": "3.1.1 CONVERGENCE ON SYNTHETIC MEMORY TASKS",
                    "text": "For different sequence lengths T of the copy and adding tasks, we trained a factorized RNN with 128 hidden units and various spectral margins m . For the copy task, we used Elman networks without a transition non-linearity as in Henaff et al. (2016). We discuss our investigations into the use of a non-linearity on the copy task in the Appendix.\nAs shown in Figure 1 we see an increase in the rate of convergence as we increase the spectral margin. This observation generally holds across the tested sequence lengths (T = 200, T = 500, T = 1000, T = 10000); however, large spectral margins hinder convergence on extremely long sequence lengths. At sequence length T = 10000, parameterizations with spectral margins larger than 0.001 converge slower than when using a margin of 0.001. In addition, the experiment without a margin failed to converge on the longest sequence length. This follows the expected pattern where stepping away from the Stiefel manifold may help with gradient descent optimization but loosening orthogonality constraints can reduce the stability of signal propagation through the network.\nFor the adding task, we trained a factorized RNN on T = 1000 length sequences, using a ReLU activation function on the hidden to hidden transition matrix. The mean squared error (MSE) is shown for different spectral margins in Figure 5 in the Appendix. Testing spectral margins m = 0, m = 1, m = 10, m = 100, and no margin, we find that the models with the purely orthogonal (m = 0) and the unconstrained (no margin) transition matrices failed to begin converging beyond baseline MSE within 2000 epochs."
                },
                {
                    "heading": "3.1.2 PERFORMANCE ON REAL DATA",
                    "text": "Having confirmed that an orthogonality constraint can negatively impact convergence rate, we seek to investigate the effect on model performance for tasks on real data. We show the results of experiments on permuted sequential MNIST in Table 2 and ordered sequential MNIST in Table 1. The loss curves are shown in Figure 6 in the Appendix and reveal an increased convergence rate for larger spectral margins. We trained the factorized RNN models with 128 hidden units for 120 epochs. We also trained an LSTM with 128 hidden units on both tasks for 150 epochs, configured with peephole connections, orthogonally initialized (and forget gate bias initialized to one), and trained with RMSprop (learning rate 0.0001, clipping gradients of magnitude 1).\nWe show the results of experiments on PTB character prediction, in terms of bits per character (bpc) and prediction accuracy, for a subset of short sequences (up to 75 characters; 23% of data) in Table 3 and for a subset of long sequences (up to 300 characters; 99% of data) in Table 4. We trained factorized RNN models with 512 hidden units for 200 epochs with geodesic gradient descent on the bases (learning rate 10\u22126) and RMSprop on the other parameters (learning rate 0.001), using a tanh transition nonlinearity, and clipping gradients of 30 magnitude.\nInterestingly, for both the ordered and permuted sequential MNIST tasks, models with a non-zero margin significantly outperform those that are constrained to have purely orthogonal transition matri-\nces (margin of zero). The best results on both the ordered and sequential MNIST tasks were yielded by models with a spectral margin of 0.1, at 94.10% accuracy and 91.44% accuracy, respectively. An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates. Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task. Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal. On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization (Glorot & Bengio, 2010) or initializing the matrix as identity. It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental. This suggests that while orthogonality helps early training by stabilizing gradient flow across many time steps, orthogonality constraints may need to be loosened on some tasks so as not to over-constrain the model\u2019s representational ability.\nCuriously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal, suggesting that evolution away from orthogonality is not a serious problem on MNIST. It is not surprising that orthogonality is useful for the MNIST tasks since they depend on long distance signal propagation with a single output at the end of the input sequence. On the other hand, character prediction with PTB produces an output at every time step. Constraining deviation from orthogonality proved detrimental for short sentences (Table 3) and beneficial when long sentences were included (Table 4). Furthermore, Glorot normal initialization did not perform worse than orthogonal initialization for PTB. Since an output is generated for every character in a sentence, short distance signal propagation is possible. Thus it is possible that the RNN is first learning very local dependencies between neighbouring characters and that given enough context, constraining deviation from orthogonality can help force the network to learn longer distance dependencies."
                },
                {
                    "heading": "3.1.3 SPECTRAL AND GRADIENT EVOLUTION",
                    "text": "It is interesting to note that even long sequence lengths (T=1000) in the copy task can be solved efficiently with rather large margins on the spectrum. In Figure 2 we look at the gradient propagation of the loss from the last time step in the network with respect to the hidden activations. We can see that for a purely orthogonal parameterization of the transition matrix (when the margin is zero), the gradient norm is preserved across time steps, as expected. We further observe that with increasing margin size, the number of update steps over which this norm preservation survives decreases, though surprisingly not as quickly as expected.\nAlthough the deviation of singular values from one should be slowed by the sigmoidal parameterizations, even parameterizations without a sigmoid (no margin) can be effectively trained for all but the longest sequence lengths. This suggests that the spectrum is not deviating far from orthogonality and that inputs to the hidden to hidden transitions are mostly not aligned along the dimensions of great-\nest expansion or contraction. We evaluated the spread of the spectrum in all of our experiments and found that indeed, singular values tend to stay well within their prescribed bounds and only reach the margin when using a very large learning rate that does not permit convergence. Furthermore, when transition matrices are initialized as orthogonal, singular values remain near one throughout training even without a sigmoidal margin for tasks that require long term memory (copy, adding, sequential MNIST). On the other hand, singular value distributions tend to drift away from one for PTB character prediction which may help explain why enforcing an orthogonality constraint can be helpful for this task, when modeling long sequences. Interestingly, singular values spread out less for longer sequence lengths (nevertheless, the T=10000 copy task could not be solved with no sigmoid on the spectrum).\nWe visualize the spread of singular values for different model parameterizations on the permuted sequential MNIST task in Figure 3. Curiously, we find that the distribution of singular values tends to shift upward to a mean of approximately 1.05 on both the ordered and permuted sequential MNIST tasks. We note that in those experiments, a tanh transition nonlinearity was used which is contractive in both the forward signal pass and the gradient backward pass. An upward shift in the distribution of singular values of the transition matrix would help compensate for that contraction. Indeed, (Saxe et al., 2013) describe this as a possibly good regime for learning in deep neural networks. That the model appears to evolve toward this regime suggests that deviating from it may incur a cost. This is interesting because the cost function cannot take into account numerical issues such as vanishing or exploding gradients (or forward signals); we do not know what could make this deviation costly. That the transition matrix may be compensating for the contraction of the tanh is supported by further experiments: applying a 1.05 pre-activation gain appears to allow a model with a margin of 0 to nearly match the top performance reached on both of the MNIST tasks. Furthermore, when using the OPLU norm-preserving activation function (Chernodub & Nowicki, 2016), we found that orthogonally initialized models performed equally well with all margins, achieving over 90% accuracy on the permuted sequential MNIST task. Unlike orthgonally initialized models, the RNN on the bottom right of Figure 3 with Glorot normal initialized transition matrices, begins and ends with a wide singular spectrum. While there is no clear positive shift in the distribution of singular values, the mean value appears to very gradually increase for both the ordered and permuted sequential MNIST tasks. If the model is to be expected to positively shift singular values to compensate for the contractivity of the tanh nonlinearity, it is not doing so well for the Glorot-initialized case; however, this may be due to the inefficiency of training as a result of vanishing gradients, given that initialization."
                },
                {
                    "heading": "3.2 EXPLORING SOFT ORTHOGONALITY CONSTRAINTS",
                    "text": "Having established that it may indeed be useful to step away from orthogonality, here we explore two forms of soft constraints (rather than hard bounds as above) on hidden to hidden transition matrix orthogonality. The first is a simple penalty that directly encourages a transition matrix W to be orthogonal, of the form \u03bb||WTW \u2212 I||22. This is similar to the orthogonality penalty introduced by Henaff et al. (2016). In the first two subfigures on the left of Figure 4, we explore the effect of weakening this form of regularization. We trained both a regular non-factorized RNN on the T = 200 copy task and a factorized RNN with orthogonal bases on the T = 500 copy task. For the regular RNN, we had to reduce the learning rate to 10\u22125. Here again we see that weakening the strength of the orthogonality-encouraging penalty can increase convergence speed.\nThe second approach we explore replaces the sigmoidal margin parameterization with a mean one Gaussian prior on the singular values. In the two right subfigures of Figure 4, we visualize the accuracy on the length 200 copy task, using geoSGD (learning rate 10\u22126) to keep U and V orthogonal and different strengths of a Gaussian prior with mean one on the singular values. We trained these experiments with regular SGD on the spectrum and other non-orthogonal parameter matrices, using a 10\u22125 learning rate. We see that priors which are too strong lead to slow convergence. Loosening the strength of the prior makes the optimization more efficient. Furthermore, we compare a direct parameterization of the spectrum (no sigmoid) in Figure 4 with a sigmoidal parameterization, using a large margin of 1. Without the sigmoidal parameterization, optimization quickly becomes unstable; on the other hand, the optimization also becomes unstable if the prior is removed completely in the sigmoidal formulation (margin 1). These results further motivate the idea that parameterizations that deviate from orthogonality may perform better than purely orthogonal ones, as long as they are sufficiently constrained to avoid instability during training."
                },
                {
                    "heading": "4 CONCLUSIONS",
                    "text": "We have explored a number of methods for controlling the expansivity of gradients during backpropagation based learning in RNNs through manipulating orthogonality constraints and regularization on matrices. Our experiments indicate that while orthogonal initialization may be beneficial, maintaining constraints on orthogonality can be detrimental. Indeed, moving away from hard constraints on matrix orthogonality can help improve optimization convergence rate and model performance. However, we also observe with synthetic tasks that relaxing regularization which encourages the spectral norms of weight matrices to be close to one, or allowing bounds on the spectral norms of weight matrices to be too wide, can reverse these gains and may lead to unstable optimization."
                },
                {
                    "heading": "ACKNOWLEDGMENTS",
                    "text": "We thank the Natural Sciences and Engineeering Research Council (NSERC) of Canada and Samsung for supporting this research."
                },
                {
                    "heading": "5 APPENDIX",
                    "text": ""
                },
                {
                    "heading": "5.1 ADDITIONAL FIGURES",
                    "text": ""
                },
                {
                    "heading": "5.2 COPY TASK NONLINEARITY",
                    "text": "We found that nonlinearities such as a rectified linear unit (ReLU) (Nair & Hinton, 2010) or hyperbolic tangent (tanh) made the copy task far more difficult to solve. Using tanh, a short sequence length (T = 100) copy task required both a soft constraint that encourages orthogonality and thousands of epochs for training. It is worth noting that in the unitary evolution recurrent neural network of Arjovsky et al. (2015), the non-linearity (referred to as the \u201dmodReLU\u201d) is actually initialized as an identity operation that is free to deviate from identity during training. Furthermore, Henaff et al. (2016) derive a solution mechanism for the copy task that drops the non-linearity from an RNN. To explore this further, we experimented with a parametric leaky ReLU activation function (PReLU) which introduces a trainable slope \u03b1 for negative valued inputs x , producing f (x ) = max(x , 0) + \u03b1min(x , 0) (He et al., 2015). Setting the slope \u03b1 to one would make the PReLU equivalent to an identity function. We experimented with clamping \u03b1 to 0.5, 0.7 or 1 in a factorized RNN with a spectral margin of 0.3 and found that only the model with \u03b1 = 1 solved the T = 1000 length copy task. We also experimented with a trainable slope \u03b1, initialized to 0.7 and found that it converges to 0.96, further suggesting the optimal solution for the copy task is without a transition nonlinearity. Since the copy task is purely a memory task, one may imagine that a transition nonlinearity such as a tanh or ReLU may be detrimental to the task as it can lose information. Thus, we also tried a recent activation function that preserves information, called an orthogonal permutation linear unit (OPLU) (Chernodub & Nowicki, 2016). The OPLU preserves norm, making a fully norm-preserving RNN possible. Interestingly, this activation function allowed us to recover identical results on the copy task to those without a nonlinearity for different spectral margins."
                },
                {
                    "heading": "5.3 METHOD RUNNING TIME",
                    "text": "Although the method proposed in section 2 relies on a matrix inversion, an operation with O(n3) complexity for an n \u00d7 n matrix, the running time of an RNN factorized in such a way actually remains reasonable. This running time is summarized in Table 5 and includes all computations in the graph, together with the matrix inversion. As this method is meant to be used only for the analysis in this work, we find the running times acceptable for that purpose. Models were run on an Nvidia GTX-770 GPU and were run against the T=100 length copy task."
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "Unitary evolution recurrent neural networks",
                    "authors": [
                        "Martin Arjovsky",
                        "Amar Shah",
                        "Yoshua Bengio"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06464,",
                    "year": 2015
                },
                {
                    "title": "Norm-preserving orthogonal permutation linear unit activation functions (oplu)",
                    "authors": [
                        "Artem Chernodub",
                        "Dimitri Nowicki"
                    ],
                    "venue": "arXiv preprint arXiv:1604.02313,",
                    "year": 2016
                },
                {
                    "title": "Understanding the difficulty of training deep feedforward neural networks",
                    "authors": [
                        "Xavier Glorot",
                        "Yoshua Bengio"
                    ],
                    "venue": "In Aistats,",
                    "year": 2010
                },
                {
                    "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
                    "authors": [
                        "Kaiming He",
                        "Xiangyu Zhang",
                        "Shaoqing Ren",
                        "Jian Sun"
                    ],
                    "venue": "In Proceedings of the IEEE international conference on computer vision,",
                    "year": 2015
                },
                {
                    "title": "Orthogonal rnns and long-memory tasks",
                    "authors": [
                        "Mikael Henaff",
                        "Arthur Szlam",
                        "Yann LeCun"
                    ],
                    "venue": "arXiv preprint arXiv:1602.06662,",
                    "year": 2016
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "Sepp Hochreiter",
                        "J\u00fcrgen Schmidhuber"
                    ],
                    "venue": "Neural computation,",
                    "year": 1997
                },
                {
                    "title": "Regularizing rnns by stabilizing activations",
                    "authors": [
                        "David Krueger",
                        "Roland Memisevic"
                    ],
                    "venue": "arXiv preprint arXiv:1511.08400,",
                    "year": 2015
                },
                {
                    "title": "A simple way to initialize recurrent networks of rectified linear units",
                    "authors": [
                        "Quoc V Le",
                        "Navdeep Jaitly",
                        "Geoffrey E Hinton"
                    ],
                    "venue": "arXiv preprint arXiv:1504.00941,",
                    "year": 2015
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Yann LeCun",
                        "L\u00e9on Bottou",
                        "Yoshua Bengio",
                        "Patrick Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "Building a large annotated corpus of english: The penn treebank",
                    "authors": [
                        "Mitchell P Marcus",
                        "Mary Ann Marcinkiewicz",
                        "Beatrice Santorini"
                    ],
                    "venue": "Computational linguistics,",
                    "year": 1993
                },
                {
                    "title": "Rectified linear units improve restricted boltzmann machines",
                    "authors": [
                        "Vinod Nair",
                        "Geoffrey E Hinton"
                    ],
                    "venue": "In Proceedings of the 27th International Conference on Machine Learning",
                    "year": 2010
                },
                {
                    "title": "A note on riemannian optimization methods on the stiefel and the grassmann manifolds",
                    "authors": [
                        "Yasunori Nishimori"
                    ],
                    "year": 2005
                },
                {
                    "title": "On the difficulty of training recurrent neural networks",
                    "authors": [
                        "Razvan Pascanu",
                        "Tomas Mikolov",
                        "Yoshua Bengio"
                    ],
                    "venue": "ICML (3),",
                    "year": 2013
                },
                {
                    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
                    "authors": [
                        "Andrew M Saxe",
                        "James L McClelland",
                        "Surya Ganguli"
                    ],
                    "venue": "arXiv preprint arXiv:1312.6120,",
                    "year": 2013
                },
                {
                    "title": "Notes on optimization on stiefel manifolds",
                    "authors": [
                        "Hemant D Tagare"
                    ],
                    "venue": "Technical report, Tech. Rep., Yale University,",
                    "year": 2011
                },
                {
                    "title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude",
                    "authors": [
                        "T. Tieleman",
                        "G. Hinton"
                    ],
                    "venue": "COURSERA: Neural Networks for Machine Learning,",
                    "year": 2012
                },
                {
                    "title": "Full-capacity unitary recurrent neural networks",
                    "authors": [
                        "Scott Wisdom",
                        "Thomas Powers",
                        "John R. Hershey",
                        "Jonathan Le Roux",
                        "Les Atlas"
                    ],
                    "year": 2016
                }
            ],
            "id": "SP:e6a1bf2d10a38887d3b79414a913061643e5c656",
            "authors": [
                {
                    "name": "Eugene Vorontsov",
                    "affiliations": []
                },
                {
                    "name": "Chiheb Trabelsi",
                    "affiliations": []
                },
                {
                    "name": "Samuel Kadoury",
                    "affiliations": []
                },
                {
                    "name": "Chris Pal",
                    "affiliations": []
                }
            ],
            "abstractText": "It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.",
            "title": "ON ORTHOGONALITY AND LEARNING RECURRENT NETWORKS WITH LONG TERM DEPENDENCIES"
        }
    },
    "93218559": {
        "X": {
            "sections": [
                {
                    "text": "As the training progresses, the approach promotes the emergence of \u201cdeep\u201d features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation.\nOverall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-ofthe-art on Office datasets."
                },
                {
                    "heading": "1. Introduction",
                    "text": "Deep feed-forward architectures have brought impressive advances to the state-of-the-art across a wide variety of machine-learning tasks and applications. At the moment, however, these leaps in performance come only when a large amount of labeled training data is available. At the same time, for problems lacking labeled data, it may be still possible to obtain training sets that are big enough for training large-scale deep models, but that suffer from the shift in data distribution from the actual data encountered\nat \u201ctest time\u201d. One particularly important example is synthetic or semi-synthetic training data, which may come in abundance and be fully labeled, but which inevitably have a distribution that is different from real data (Liebelt & Schmid, 2010; Stark et al., 2010; Va\u0301zquez et al., 2014; Sun & Saenko, 2014). Learning a discriminative classifier or other predictor in the presence of a shift between training and test distributions is known as domain adaptation (DA). A number of approaches to domain adaptation has been suggested in the context of shallow learning, e.g. in the situation when data representation/features are given and fixed. The proposed approaches then build the mappings between the source (training-time) and the target (test-time) domains, so that the classifier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains. The appeal of the domain adaptation approaches is the ability to learn a mapping between domains in the situation when the target domain data are either fully unlabeled (unsupervised domain annotation) or have few labeled samples (semi-supervised domain adaptation). Below, we focus on the harder unsupervised case, although the proposed approach can be generalized to the semi-supervised case rather straightforwardly. Unlike most previous papers on domain adaptation that worked with fixed feature representations, we focus on combining domain adaptation and deep feature learning within one training process (deep domain adaptation). Our goal is to embed domain adaptation into the process of learning representation, so that the final classification decisions are made based on features that are both discriminative and invariant to the change of domains, i.e. have the same or very similar distributions in the source and the target domains. In this way, the obtained feed-forward network can be applicable to the target domain without being hindered by the shift between the two domains. We thus focus on learning features that combine (i) discriminativeness and (ii) domain-invariance. This is achieved by jointly optimizing the underlying features as well as two discriminative classifiers operating on these features: (i) the label predictor that predicts class labels and is used both during training and at test time and (ii) the ar X iv :1 40 9. 74 95 v2 [ st at .M L ] 2 7 Fe b 20 15\ndomain classifier that discriminates between the source and the target domains during training. While the parameters of the classifiers are optimized in order to minimize their error on the training set, the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classifier and to maximize the loss of the domain classifier. The latter encourages domain-invariant features to emerge in the course of the optimization. Crucially, we show that all three training processes can be embedded into an appropriately composed deep feedforward network (Figure 1) that uses standard layers and loss functions, and can be trained using standard backpropagation algorithms based on stochastic gradient descent or its modifications (e.g. SGD with momentum). Our approach is generic as it can be used to add domain adaptation to any existing feed-forward architecture that is trainable by backpropagation. In practice, the only non-standard component of the proposed architecture is a rather trivial gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation. Below, we detail the proposed approach to domain adaptation in deep architectures, and present results on traditional deep learning image datasets (such as MNIST (LeCun et al., 1998) and SVHN (Netzer et al., 2011)) as well as on OFFICE benchmarks (Saenko et al., 2010), where the proposed method considerably improves over previous state-of-the-art accuracy."
                },
                {
                    "heading": "2. Related work",
                    "text": "A large number of domain adaptation methods have been proposed over the recent years, and here we focus on the most related ones. Multiple methods perform unsupervised domain adaptation by matching the feature distributions in the source and the target domains. Some approaches perform this by reweighing or selecting samples from the source domain (Borgwardt et al., 2006; Huang et al., 2006; Gong et al., 2013), while others seek an explicit feature space transformation that would map source distribution into the target ones (Pan et al., 2011; Gopalan et al., 2011; Baktashmotlagh et al., 2013). An important aspect of the distribution matching approach is the way the (dis)similarity between distributions is measured. Here, one popular choice is matching the distribution means in the kernel-reproducing Hilbert space (Borgwardt et al., 2006; Huang et al., 2006), whereas (Gong et al., 2012; Fernando et al., 2013) map the principal axes associated with each of the distributions. Our approach also attempts to match feature space distributions, however this is accomplished by modifying the feature representation itself rather than by reweighing or geometric transformation. Also, our method uses (implicitly) a rather different way to measure the disparity between distributions based on their separability by a deep discriminatively-trained classifier.\nSeveral approaches perform gradual transition from the source to the target domain (Gopalan et al., 2011; Gong et al., 2012) by a gradual change of the training distribution. Among these methods, (S. Chopra & Gopalan, 2013) does this in a \u201cdeep\u201d way by the layerwise training of a sequence of deep autoencoders, while gradually replacing source-domain samples with target-domain samples. This improves over a similar approach of (Glorot et al., 2011) that simply trains a single deep autoencoder for both domains. In both approaches, the actual classifier/predictor is learned in a separate step using the feature representation learned by autoencoder(s). In contrast to (Glorot et al., 2011; S. Chopra & Gopalan, 2013), our approach performs feature learning, domain adaptation and classifier learning jointly, in a unified architecture, and using a single learning algorithm (backpropagation). We therefore argue that our approach is simpler (both conceptually and in terms of its implementation). Our method also achieves considerably better results on the popular OFFICE benchmark. While the above approaches perform unsupervised domain adaptation, there are approaches that perform supervised domain adaptation by exploiting labeled data from the target domain. In the context of deep feed-forward architectures, such data can be used to \u201cfine-tune\u201d the network trained on the source domain (Zeiler & Fergus, 2013; Oquab et al., 2014; Babenko et al., 2014). Our approach does not require labeled target-domain data. At the same time, it can easily incorporate such data when it is available. An idea related to ours is described in (Goodfellow et al., 2014). While their goal is quite different (building generative deep networks that can synthesize samples), the way they measure and minimize the discrepancy between the distribution of the training data and the distribution of the synthesized data is very similar to the way our architecture measures and minimizes the discrepancy between feature distributions for the two domains. Finally, a recent and concurrent report by (Tzeng et al., 2014) also focuses on domain adaptation in feed-forward networks. Their set of techniques measures and minimizes the distance of the data means across domains. This approach may be regarded as a \u201cfirst-order\u201d approximation to our approach, which seeks a tighter alignment between distributions."
                },
                {
                    "heading": "3. Deep Domain Adaptation",
                    "text": ""
                },
                {
                    "heading": "3.1. The model",
                    "text": "We now detail the proposed model for the domain adaptation. We assume that the model works with input samples x \u2208 X , where X is some input space and certain labels (output) y from the label space Y . Below, we assume classification problems where Y is a finite set (Y = {1, 2, . . . L}), however our approach is generic and can handle any output label space that other deep feed-\nforward models can handle. We further assume that there exist two distributions S(x, y) and T (x, y) on X \u2297 Y , which will be referred to as the source distribution and the target distribution (or the source domain and the target domain). Both distributions are assumed complex and unknown, and furthermore similar but different (in other words, S is \u201cshifted\u201d from T by some domain shift). Our ultimate goal is to be able to predict labels y given the input x for the target distribution. At training time, we have an access to a large set of training samples {x1,x2, . . . ,xN} from both the source and the target domains distributed according to the marginal distributions S(x) and T (x). We denote with di the binary variable (domain label) for the i-th example, which indicates whether xi come from the source distribution (xi\u223cS(x) if di=0) or from the target distribution (xi\u223cT (x) if di=1). For the examples from the source distribution (di=0) the corresponding labels yi \u2208 Y are known at training time. For the examples from the target domains, we do not know the labels at training time, and we want to predict such labels at test time. We now define a deep feed-forward architecture that for each input x predicts its label y \u2208 Y and its domain label d \u2208 {0, 1}. We decompose such mapping into three parts. We assume that the input x is first mapped by a mapping Gf (a feature extractor) to a D-dimensional feature vector f \u2208 RD. The feature mapping may also include several feed-forward layers and we denote the vector of parameters of all layers in this mapping as \u03b8f , i.e. f = Gf (x; \u03b8f ). Then, the feature vector f is mapped by a mapping Gy (label predictor) to the label y, and we denote the parameters of this mapping with \u03b8y . Finally, the same feature vector f is mapped to the domain label d by a mapping Gd (domain\nclassifier) with the parameters \u03b8d (Figure 1). During the learning stage, we aim to minimize the label prediction loss on the annotated part (i.e. the source part) of the training set, and the parameters of both the feature extractor and the label predictor are thus optimized in order to minimize the empirical loss for the source domain samples. This ensures the discriminativeness of the features f and the overall good prediction performance of the combination of the feature extractor and the label predictor on the source domain. At the same time, we want to make the features f domain-invariant. That is, we want to make the distributions S(f) = {Gf (x; \u03b8f ) |x\u223cS(x)} and T (f) = {Gf (x; \u03b8f ) |x\u223cT (x)} to be similar. Under the covariate shift assumption, this would make the label prediction accuracy on the target domain to be the same as on the source domain (Shimodaira, 2000). Measuring the dissimilarity of the distributions S(f) and T (f) is however non-trivial, given that f is high-dimensional, and that the distributions themselves are constantly changing as learning progresses. One way to estimate the dissimilarity is to look at the loss of the domain classifier Gd, provided that the parameters \u03b8d of the domain classifier have been trained to discriminate between the two feature distributions in an optimal way. This observation leads to our idea. At training time, in order to obtain domain-invariant features, we seek the parameters \u03b8f of the feature mapping that maximize the loss of the domain classifier (by making the two feature distributions as similar as possible), while simultaneously seeking the parameters \u03b8d of the domain classifier that minimize the loss of the domain classifier. In addition, we seek to minimize the loss of the label predictor.\nMore formally, we consider the functional: E(\u03b8f , \u03b8y, \u03b8d) = \u2211 i=1..N di=0 Ly ( Gy(Gf (xi; \u03b8f ); \u03b8y), yi ) \u2212\n\u03bb \u2211 i=1..N Ld ( Gd(Gf (xi; \u03b8f ); \u03b8d), yi ) =\n= \u2211 i=1..N di=0 Liy(\u03b8f , \u03b8y)\u2212 \u03bb \u2211 i=1..N Lid(\u03b8f , \u03b8d) (1)\nHere, Ly(\u00b7, \u00b7) is the loss for label prediction (e.g. multinomial), Ld(\u00b7, \u00b7) is the loss for the domain classification (e.g. logistic), while Liy and L i d denote the corresponding loss functions evaluated at the i-th training example. Based on our idea, we are seeking the parameters \u03b8\u0302f , \u03b8\u0302y, \u03b8\u0302d that deliver a saddle point of the functional (1):\n(\u03b8\u0302f , \u03b8\u0302y) = arg min \u03b8f ,\u03b8y E(\u03b8f , \u03b8y, \u03b8\u0302d) (2)\n\u03b8\u0302d = arg max \u03b8d E(\u03b8\u0302f , \u03b8\u0302y, \u03b8d) . (3)\nAt the saddle point, the parameters \u03b8d of the domain classifier \u03b8d minimize the domain classification loss (since it enters into (1) with the minus sign) while the parameters \u03b8y of the label predictor minimize the label prediction loss. The feature mapping parameters \u03b8f minimize the label prediction loss (i.e. the features are discriminative), while maximizing the domain classification loss (i.e. the features are domain-invariant). The parameter \u03bb controls the trade-off between the two objectives that shape the features during learning. Below, we demonstrate that standard stochastic gradient solvers (SGD) can be adapted for the search of the saddle point (2)-(3)."
                },
                {
                    "heading": "3.2. Optimization with backpropagation",
                    "text": "A saddle point (2)-(3) can be found as a stationary point of the following stochastic updates:\n\u03b8f \u2190\u2212 \u03b8f \u2212 \u00b5 ( \u2202Liy \u2202\u03b8f \u2212 \u03bb\u2202L i d \u2202\u03b8f ) (4)\n\u03b8y \u2190\u2212 \u03b8y \u2212 \u00b5 \u2202Liy \u2202\u03b8y\n(5)\n\u03b8d \u2190\u2212 \u03b8d \u2212 \u00b5 \u2202Lid \u2202\u03b8d\n(6)\nwhere \u00b5 is the learning rate (which can vary over time). The updates (4)-(6) are very similar to stochastic gradient descent (SGD) updates for a feed-forward deep model that comprises feature extractor fed into the label predictor and into the domain classifier. The difference is the \u2212\u03bb factor in (4) (the difference is important, as without such factor,\nstochastic gradient descent would try to make features dissimilar across domains in order to minimize the domain classification loss). Although direct implementation of (4)- (6) as SGD is not possible, it is highly desirable to reduce the updates (4)-(6) to some form of SGD, since SGD (and its variants) is the main learning algorithm implemented in most packages for deep learning. Fortunately, such reduction can be accomplished by introducing a special gradient reversal layer (GRL) defined as follows. The gradient reversal layer has no parameters associated with it (apart from the meta-parameter \u03bb, which is not updated by backpropagation). During the forward propagation, GRL acts as an identity transform. During the backpropagation though, GRL takes the gradient from the subsequent level, multiplies it by \u2212\u03bb and passes it to the preceding layer. Implementing such layer using existing object-oriented packages for deep learning is simple, as defining procedures for forwardprop (identity transform), backprop (multiplying by a constant), and parameter update (nothing) is trivial. The GRL as defined above is inserted between the feature extractor and the domain classifier, resulting in the architecture depicted in Figure 1. As the backpropagation process passes through the GRL, the partial derivatives of the loss that is downstream the GRL (i.e. Ld) w.r.t. the layer parameters that are upstream the GRL (i.e. \u03b8f ) get multiplied by \u2212\u03bb, i.e. \u2202Ld\u2202\u03b8f is effectively replaced with \u2212\u03bb \u2202Ld \u2202\u03b8f\n. Therefore, running SGD in the resulting model implements the updates (4)-(6) and converges to a saddle point of (1). Mathematically, we can formally treat the gradient reversal layer as a \u201cpseudo-function\u201dR\u03bb(x) defined by two (incompatible) equations describing its forward- and backpropagation behaviour:\nR\u03bb(x) = x (7) dR\u03bb dx = \u2212\u03bbI (8)\nwhere I is an identity matrix. We can then define the objective \u201cpseudo-function\u201d of (\u03b8f , \u03b8y, \u03b8d) that is being optimized by the stochastic gradient descent within our method:\nE\u0303(\u03b8f , \u03b8y, \u03b8d) = \u2211 i=1..N di=0 Ly ( Gy(Gf (xi; \u03b8f ); \u03b8y), yi ) +\n\u2211 i=1..N Ld ( Gd(R\u03bb(Gf (xi; \u03b8f )); \u03b8d), yi ) (9)\nRunning updates (4)-(6) can then be implemented as doing SGD for (9) and leads to the emergence of features that are domain-invariant and discriminative at the same time. After the learning, the label predictor y(x) = Gy(Gf (x; \u03b8f ); \u03b8y) can be used to predict labels for samples from the target domain (as well as from the source domain).\nThe simple learning procedure outlined above can be rederived/generalized along the lines suggested in (Goodfellow et al., 2014) (see Appendix A)."
                },
                {
                    "heading": "3.3. Relation toH\u2206H-distance",
                    "text": "In this section we give a brief analysis of our method in terms ofH\u2206H-distance (Ben-David et al., 2010; Cortes & Mohri, 2011) which is widely used in the theory of nonconservative domain adaptation. Formally,\ndH\u2206H(S, T ) = 2 sup h1,h2\u2208H |Pf\u223cS [h1(f) 6= h2(f)]\u2212\n\u2212Pf\u223cT [h1(f) 6= h2(f)]| (10)\ndefines a discrepancy distance between two distributions S and T w.r.t. a hypothesis set H. Using this notion one can obtain a probabilistic bound (Ben-David et al., 2010) on the performance \u03b5T (h) of some classifier h from T evaluated on the target domain given its performance \u03b5S(h) on the source domain:\n\u03b5T (h) \u2264 \u03b5S(h) + 1\n2 dH\u2206H(S, T ) + C , (11)\nwhere S and T are source and target distributions respectively, and C does not depend on particular h. Consider fixed S and T over the representation space produced by the feature extractor Gf and a family of label predictorsHp. We assume that the family of domain classifiersHd is rich enough to contain the symmetric difference hypothesis set ofHp:\nHp\u2206Hp = {h |h = h1 \u2295 h2 , h1, h2 \u2208 Hp} . (12)\nIt is not an unrealistic assumption as we have a freedom to pick Hd whichever we want. For example, we can set the architecture of the domain discriminator to be the layerby-layer concatenation of two replicas of the label predictor followed by a two layer non-linear perceptron aimed to learn the XOR-function. Given the assumption holds, one can easily show that training the Gd is closely related to the estimation of dHp\u2206Hp(S, T ). Indeed,\ndHp\u2206Hp(S, T ) = = 2 sup\nh\u2208Hp\u2206Hp |Pf\u223cS [h(f) = 1]\u2212 Pf\u223cT [h(f) = 1]| \u2264\n\u2264 2 sup h\u2208Hd |Pf\u223cS [h(f) = 1]\u2212 Pf\u223cT [h(f) = 1]| =\n= 2 sup h\u2208Hd |1\u2212 \u03b1(h)| = 2 sup h\u2208Hd [\u03b1(h)\u2212 1]\n(13)\nwhere \u03b1(h) = Pf\u223cS [h(f) = 0] + Pf\u223cT [h(f) = 1] is maximized by the optimal Gd. Thus, optimal discriminator gives the upper bound for dHp\u2206Hp(S, T ). At the same time, backpropagation of the reversed gradient changes the representation space\nso that \u03b1(Gd) becomes smaller effectively reducing dHp\u2206Hp(S, T ) and leading to the better approximation of \u03b5T (Gy) by \u03b5S(Gy)."
                },
                {
                    "heading": "4. Experiments",
                    "text": "We perform extensive evaluation of the proposed approach on a number of popular image datasets and their modifications. These include large-scale datasets of small images popular with deep learning methods, and the OFFICE datasets (Saenko et al., 2010), which are a de facto standard for domain adaptation in computer vision, but have much fewer images.\nBaselines. For the bulk of experiments the following baselines are evaluated. The source-only model is trained without consideration for target-domain data (no domain classifier branch included into the network). The train-ontarget model is trained on the target domain with class labels revealed. This model serves as an upper bound on DA methods, assuming that target data are abundant and the shift between the domains is considerable. In addition, we compare our approach against the recently proposed unsupervised DA method based on subspace alignment (SA) (Fernando et al., 2013), which is simple to setup and test on new datasets, but has also been shown to perform very well in experimental comparisons with other \u201cshallow\u201d DA methods. To boost the performance of this baseline, we pick its most important free parameter (the number of principal components) from the range {2, . . . , 60}, so that the test performance on the target domain is maximized. To apply SA in our setting, we train a source-only model and then consider the activations of the last hidden layer in the label predictor (before the final linear classifier) as descriptors/features, and learn the mapping between the source and the target domains (Fernando et al., 2013). Since the SA baseline requires to train a new classifier after adapting the features, and in order to put all the compared settings on an equal footing, we retrain the last layer of the label predictor using a standard linear SVM (Fan et al., 2008) for all four considered methods (including ours; the performance on the target domain remains approximately the same after the retraining). For the OFFICE dataset (Saenko et al., 2010), we directly compare the performance of our full network (feature extractor and label predictor) against recent DA approaches using previously published results.\nCNN architectures. In general, we compose feature extractor from two or three convolutional layers, picking their exact configurations from previous works. We give the exact architectures in Appendix B. For the domain adaptator we stick to the three fully connected layers (x \u2192 1024 \u2192 1024 \u2192 2), except for MNIST where we used a simpler (x \u2192 100 \u2192 2) ar-\nchitecture to speed up the experiments. For loss functions, we set Ly and Ld to be the logistic regression loss and the binomial cross-entropy respectively.\nCNN training procedure. The model is trained on 128- sized batches. Images are preprocessed by the mean subtraction. A half of each batch is populated by the samples from the source domain (with known labels), the rest is comprised of the target domain (with unknown labels). In order to suppress noisy signal from the domain classifier at the early stages of the training procedure instead of fixing the adaptation factor \u03bb, we gradually change it from 0 to 1 using the following schedule:\n\u03bbp = 2\n1 + exp(\u2212\u03b3 \u00b7 p) \u2212 1, (14)\nwhere \u03b3 was set to 10 in all experiments (the schedule was not optimized/tweaked). Further details on the CNN training can be found in Appendix C.\nVisualizations. We use t-SNE (van der Maaten, 2013) projection to visualize feature distributions at different points of the network, while color-coding the domains (Figure 3). We observe strong correspondence between the success of the adaptation in terms of the classification accuracy for the target domain, and the overlap between the domain distributions in such visualizations.\nChoosing meta-parameters. In general, good unsupervised DA methods should provide ways to set metaparameters (such as \u03bb, the learning rate, the momentum\nrate, the network architecture for our method) in an unsupervised way, i.e. without referring to labeled data in the target domain. In our method, one can assess the performance of the whole system (and the effect of changing hyper-parameters) by observing the test error on the source domain and the domain classifier error. In general, we observed a good correspondence between the success of adaptation and these errors (adaptation is more successful when the source domain test error is low, while the domain classifier error is high). In addition, the layer, where the the domain adaptator is attached can be picked by computing difference between means as suggested in (Tzeng et al., 2014)."
                },
                {
                    "heading": "4.1. Results",
                    "text": "We now discuss the experimental settings and the results. In each case, we train on the source dataset and test on a different target domain dataset, with considerable shifts between domains (see Figure 2). The results are summarized in Table 1 and Table 2.\nMNIST \u2192 MNIST-M. Our first experiment deals with the MNIST dataset (LeCun et al., 1998) (source). In order to obtain the target domain (MNIST-M) we blend digits from the original set over patches randomly extracted from color photos from BSDS500 (Arbelaez et al., 2011). This operation is formally defined for two images I1, I2 as Ioutijk = |I1ijk \u2212 I2ijk|, where i, j are the coordinates of a pixel and k is a channel index. In other words, an output sample is produced by taking a patch from a photo and in-\nverting its pixels at positions corresponding to the pixels of a digit. For a human the classification task becomes only slightly harder compared to the original dataset (the digits are still clearly distinguishable) whereas for a CNN trained on MNIST this domain is quite distinct, as the background and the strokes are no longer constant. Consequently, the source-only model performs poorly. Our approach succeeded at aligning feature distributions (Figure 3), which led to successful adaptation results (considering that the adaptation is unsupervised). At the same time, the improvement over source-only model achieved by subspace alignment (SA) (Fernando et al., 2013) is quite modest, thus highlighting the difficulty of the adaptation task.\nSynthetic numbers\u2192 SVHN. To address a common scenario of training on synthetic data and testing on real data, we use Street-View House Number dataset SVHN (Netzer et al., 2011) as the target domain and synthetic digits as the source. The latter (SYN NUMBERS) consists of 500,000 images generated by ourselves from Windows fonts by varying the text (that includes different one-, two-, and three-digit numbers), positioning, orientation, background and stroke colors, and the amount of blur. The degrees of\nvariation were chosen manually to simulate SVHN, however the two datasets are still rather distinct, the biggest difference being the structured clutter in the background of SVHN images. The proposed backpropagation-based technique works well covering two thirds of the gap between training with source data only and training on target domain data with known target labels. In contrast, SA (Fernando et al., 2013) does not result in any significant improvement in the classification accuracy, thus highlighting that the adaptation task is even more challenging than in the case of the MNIST experiment.\nMNIST\u2194 SVHN. In this experiment, we further increase the gap between distributions, and test on MNIST and SVHN, which are significantly different in appearance. Training on SVHN even without adaptation is challenging \u2014 classification error stays high during the first 150 epochs. In order to avoid ending up in a poor local minimum we, therefore, do not use learning rate annealing here. Obviously, the two directions (MNIST \u2192 SVHN and SVHN \u2192 MNIST) are not equally difficult. As SVHN is more diverse, a model trained on SVHN is ex-\npected to be more generic and to perform reasonably on the MNIST dataset. This, indeed, turns out to be the case and is supported by the appearance of the feature distributions. We observe a quite strong separation between the domains when we feed them into the CNN trained solely on MNIST, whereas for the SVHN-trained network the features are much more intermixed. This difference probably explains why our method succeeded in improving the performance by adaptation in the SVHN \u2192 MNIST scenario (see Table 1) but not in the opposite direction (SA is not able to perform adaptation in this case either). Unsupervised adaptation from MNIST to SVHN gives a failure example for our approach (we are unaware of any unsupervised DA methods capable of performing such adaptation).\nSynthetic Signs \u2192 GTSRB. Overall, this setting is similar to the SYN NUMBERS \u2192 SVHN experiment, except the distribution of the features is more complex due to the significantly larger number of classes (43 instead of 10). For the source domain we obtained 100,000 synthetic images (which we call SYN SIGNS) simulating various photoshooting conditions. Once again, our method achieves a sensible increase in performance once again proving its suitability for the synthetic-to-real data adaptation. As an additional experiment, we also evaluate the proposed algorithm for semi-supervised domain adaptation, i.e. when one is additionally provided with a small amount of labeled target data. For that purpose we split GTSRB into the train set (1280 random samples with labels) and the validation set (the rest of the dataset). The validation part is used solely for the evaluation and does not participate in the adaptation. The training procedure changes slightly as the label predictor is now exposed to the target data. Figure 4 shows the change of the validation error throughout the training. While the graph clearly suggests that our method can be used in the semi-supervised setting, thorough verification of semi-supervised setting is left for future work.\nOffice dataset. We finally evaluate our method on OFFICE dataset, which is a collection of three distinct do-\nmains: AMAZON, DSLR, and WEBCAM. Unlike previously discussed datasets, OFFICE is rather small-scale with only 2817 labeled images spread across 31 different categories in the largest domain. The amount of available data is crucial for a successful training of a deep model, hence we opted for the fine-tuning of the CNN pre-trained on the ImageNet (Jia et al., 2014) as it is done in some recent DA works (Donahue et al., 2014; Tzeng et al., 2014; Hoffman et al., 2013). We make our approach more comparable with (Tzeng et al., 2014) by using exactly the same network architecture replacing domain mean-based regularization with the domain classifier. Following most previous works, we evaluate our method using 5 random splits for each of the 3 transfer tasks commonly used for evaluation. Our training protocol is close to (Tzeng et al., 2014; Saenko et al., 2010; Gong et al., 2012) as we use the same number of labeled source-domain images per category. Unlike those works and similarly to e.g. DLID (S. Chopra & Gopalan, 2013) we use the whole unlabeled target domain (as the premise of our method is the abundance of unlabeled data in the target domain). Under this transductive setting, our method is able to improve previously-reported state-of-the-art accuracy for unsupervised adaptation very considerably (Table 2), especially in the most challenging AMAZON\u2192WEBCAM scenario (the two domains with the largest domain shift)."
                },
                {
                    "heading": "5. Discussion",
                    "text": "We have proposed a new approach to unsupervised domain adaptation of deep feed-forward architectures, which allows large-scale training based on large amount of annotated data in the source domain and large amount of unannotated data in the target domain. Similarly to many previous shallow and deep DA techniques, the adaptation is achieved through aligning the distributions of features across the two domains. However, unlike previous approaches, the alignment is accomplished through standard backpropagation training. The approach is therefore rather scalable, and can be implemented using any deep learning package. To this end we plan to release the source code for the Gradient Reversal layer along with the usage examples as an extension to Caffe (Jia et al., 2014). Further evaluation on larger-scale tasks and in semisupervised settings constitutes future work. It is also interesting whether the approach can benefit from a good initialization of the feature extractor. For this, a natural choice would be to use deep autoencoder/deconvolution network trained on both domains (or on the target domain) in the same vein as (Glorot et al., 2011; S. Chopra & Gopalan, 2013), effectively using (Glorot et al., 2011; S. Chopra & Gopalan, 2013) as an initialization to our method."
                },
                {
                    "heading": "Appendix A. An alternative optimization",
                    "text": "approach\nThere exists an alternative construction (inspired by (Goodfellow et al., 2014)) that leads to the same updates (4)-(6). Rather than using the gradient reversal layer, the construction introduces two different loss functions for the domain classifier. Minimization of the first domain loss (Ld+) should lead to a better domain discrimination, while the second domain loss (Ld\u2212) is minimized when the domains are distinct. Stochastic updates for \u03b8f and \u03b8d are then defined as:\n\u03b8f \u2190\u2212 \u03b8f \u2212 \u00b5 ( \u2202Liy \u2202\u03b8f + \u2202Lid\u2212 \u2202\u03b8f )\n\u03b8d \u2190\u2212 \u03b8d \u2212 \u00b5 \u2202Lid+ \u2202\u03b8d ,\nThus, different parameters participate in the optimization of different losses In this framework, the gradient reversal layer constitutes a special case, corresponding to the pair of domain losses (Ld,\u2212\u03bbLd). However, other pairs of loss functions can be used. One example would be the binomial cross-entropy (Goodfellow et al., 2014):\nLd+(q, d) = \u2211 i=1..N di log(qi) + (1\u2212 di) log(1\u2212 qi) ,\nwhere d indicates domain indices and q is an output of the predictor. In that case \u201cadversarial\u201d loss is easily obtained by swapping domain labels, i.e.Ld\u2212(q, d) = Ld+(q, 1\u2212d). This particular pair has a potential advantage of producing stronger gradients at early learning stages if the domains are quite dissimilar. In our experiments, however, we did not observe any significant improvement resulting from this choice of losses."
                },
                {
                    "heading": "Appendix B. CNN architectures",
                    "text": "Four different architectures were used in our experiments (first three are shown in Figure 5):\n\u2022 A smaller one (a) if the source domain is MNIST. This architecture was inspired by the classical LeNet-5 (LeCun et al., 1998).\n\u2022 (b) for the experiments involving SVHN dataset. This one is adopted from (Srivastava et al., 2014).\n\u2022 (c) in the SYN SINGS \u2192 GTSRB setting. We used the single-CNN baseline from (Cires\u0327an et al., 2012) as our starting point.\n\u2022 Finally, we use pre-trained AlexNet from the Caffe-package (Jia et al., 2014) for the OFFICE domains. Adaptation architecture is identical to (Tzeng et al., 2014): 2-layer domain classifier (x \u2192 1024 \u2192\n1024 \u2192 2) is attached to the 256-dimensional bottleneck of fc7.\nThe domain classifier branch in all cases is somewhat arbitrary (better adaptation performance might be attained if this part of the architecture is tuned)."
                },
                {
                    "heading": "Appendix C. Training procedure",
                    "text": "We use stochastic gradient descent with 0.9 momentum and the learning rate annealing described by the following formula:\n\u00b5p = \u00b50\n(1 + \u03b1 \u00b7 p)\u03b2 ,\nwhere p is the training progress linearly changing from 0 to 1, \u00b50 = 0.01, \u03b1 = 10 and \u03b2 = 0.75 (the schedule was optimized to promote convergence and low error on the source domain). Following (Srivastava et al., 2014) we also use dropout and `2-norm restriction when we train the SVHN architecture."
                }
            ],
            "year": 2015,
            "references": [
                {
                    "title": "Contour detection and hierarchical image segmentation",
                    "authors": [
                        "Arbelaez",
                        "Pablo",
                        "Maire",
                        "Michael",
                        "Fowlkes",
                        "Charless",
                        "Malik",
                        "Jitendra"
                    ],
                    "year": 2011
                },
                {
                    "title": "Neural codes for image retrieval",
                    "authors": [
                        "Babenko",
                        "Artem",
                        "Slesarev",
                        "Anton",
                        "Chigorin",
                        "Alexander",
                        "Lempitsky",
                        "Victor S"
                    ],
                    "venue": "In ECCV, pp",
                    "year": 2014
                },
                {
                    "title": "Unsupervised domain adaptation by domain invariant projection",
                    "authors": [
                        "Baktashmotlagh",
                        "Mahsa",
                        "Harandi",
                        "Mehrtash Tafazzoli",
                        "Lovell",
                        "Brian C",
                        "Salzmann",
                        "Mathieu"
                    ],
                    "venue": "In ICCV, pp",
                    "year": 2013
                },
                {
                    "title": "A theory of learning from different",
                    "authors": [
                        "Ben-David",
                        "Shai",
                        "Blitzer",
                        "John",
                        "Crammer",
                        "Koby",
                        "Kulesza",
                        "Alex",
                        "Pereira",
                        "Fernando",
                        "Vaughan",
                        "Jennifer Wortman"
                    ],
                    "venue": "domains. JMLR,",
                    "year": 2010
                },
                {
                    "title": "Integrating structured biological data by kernel maximum mean discrepancy",
                    "authors": [
                        "Borgwardt",
                        "Karsten M",
                        "Gretton",
                        "Arthur",
                        "Rasch",
                        "Malte J",
                        "Kriegel",
                        "Hans-Peter",
                        "Sch\u00f6lkopf",
                        "Bernhard",
                        "Smola",
                        "Alexander J"
                    ],
                    "venue": "In ISMB,",
                    "year": 2006
                },
                {
                    "title": "Multi-column deep neural network for traffic sign classification",
                    "authors": [
                        "Cire\u015fan",
                        "Dan",
                        "Meier",
                        "Ueli",
                        "Masci",
                        "Jonathan",
                        "Schmidhuber",
                        "J\u00fcrgen"
                    ],
                    "venue": "Neural Networks,",
                    "year": 2012
                },
                {
                    "title": "Domain adaptation in regression",
                    "authors": [
                        "Cortes",
                        "Corinna",
                        "Mohri",
                        "Mehryar"
                    ],
                    "venue": "In Algorithmic Learning Theory,",
                    "year": 2011
                },
                {
                    "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
                    "authors": [
                        "Donahue",
                        "Jeff",
                        "Jia",
                        "Yangqing",
                        "Vinyals",
                        "Oriol",
                        "Hoffman",
                        "Judy",
                        "Zhang",
                        "Ning",
                        "Tzeng",
                        "Eric",
                        "Darrell",
                        "Trevor"
                    ],
                    "year": 2014
                },
                {
                    "title": "LIBLINEAR: A library for large linear classification",
                    "authors": [
                        "Fan",
                        "Rong-En",
                        "Chang",
                        "Kai-Wei",
                        "Hsieh",
                        "Cho-Jui",
                        "Wang",
                        "Xiang-Rui",
                        "Lin",
                        "Chih-Jen"
                    ],
                    "venue": "Journal of Machine Learning Research,",
                    "year": 2008
                },
                {
                    "title": "Unsupervised visual domain adaptation using subspace alignment",
                    "authors": [
                        "Fernando",
                        "Basura",
                        "Habrard",
                        "Amaury",
                        "Sebban",
                        "Marc",
                        "Tuytelaars",
                        "Tinne"
                    ],
                    "venue": "In ICCV,",
                    "year": 2013
                },
                {
                    "title": "Domain adaptive neural networks for object recognition",
                    "authors": [
                        "Ghifary",
                        "Muhammad",
                        "Kleijn",
                        "W Bastiaan",
                        "Zhang",
                        "Mengjie"
                    ],
                    "venue": "PRICAI",
                    "year": 2014
                },
                {
                    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
                    "authors": [
                        "Glorot",
                        "Xavier",
                        "Bordes",
                        "Antoine",
                        "Bengio",
                        "Yoshua"
                    ],
                    "venue": "In ICML, pp",
                    "year": 2011
                },
                {
                    "title": "Geodesic flow kernel for unsupervised domain adaptation",
                    "authors": [
                        "Gong",
                        "Boqing",
                        "Shi",
                        "Yuan",
                        "Sha",
                        "Fei",
                        "Grauman",
                        "Kristen"
                    ],
                    "venue": "In CVPR, pp. 2066\u20132073,",
                    "year": 2012
                },
                {
                    "title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation",
                    "authors": [
                        "Gong",
                        "Boqing",
                        "Grauman",
                        "Kristen",
                        "Sha",
                        "Fei"
                    ],
                    "venue": "In ICML, pp",
                    "year": 2013
                },
                {
                    "title": "Generative adversarial nets",
                    "authors": [
                        "Goodfellow",
                        "Ian",
                        "Pouget-Abadie",
                        "Jean",
                        "Mirza",
                        "Mehdi",
                        "Xu",
                        "Bing",
                        "Warde-Farley",
                        "David",
                        "Ozair",
                        "Sherjil",
                        "Courville",
                        "Aaron",
                        "Bengio",
                        "Yoshua"
                    ],
                    "venue": "In NIPS,",
                    "year": 2014
                },
                {
                    "title": "Domain adaptation for object recognition: An unsupervised approach",
                    "authors": [
                        "Gopalan",
                        "Raghuraman",
                        "Li",
                        "Ruonan",
                        "Chellappa",
                        "Rama"
                    ],
                    "venue": "In ICCV, pp",
                    "year": 2011
                },
                {
                    "title": "One-shot adaptation of supervised deep convolutional models",
                    "authors": [
                        "Hoffman",
                        "Judy",
                        "Tzeng",
                        "Eric",
                        "Donahue",
                        "Jeff",
                        "Jia",
                        "Yangqing",
                        "Saenko",
                        "Kate",
                        "Darrell",
                        "Trevor"
                    ],
                    "venue": "CoRR, abs/1312.6204,",
                    "year": 2013
                },
                {
                    "title": "Correcting sample selection bias by unlabeled data",
                    "authors": [
                        "Huang",
                        "Jiayuan",
                        "Smola",
                        "Alexander J",
                        "Gretton",
                        "Arthur",
                        "Borgwardt",
                        "Karsten M",
                        "Sch\u00f6lkopf",
                        "Bernhard"
                    ],
                    "venue": "In NIPS, pp",
                    "year": 2006
                },
                {
                    "title": "Caffe: Convolutional architecture for fast feature embedding",
                    "authors": [
                        "Jia",
                        "Yangqing",
                        "Shelhamer",
                        "Evan",
                        "Donahue",
                        "Jeff",
                        "Karayev",
                        "Sergey",
                        "Long",
                        "Jonathan",
                        "Girshick",
                        "Ross",
                        "Guadarrama",
                        "Sergio",
                        "Darrell",
                        "Trevor"
                    ],
                    "venue": "CoRR, abs/1408.5093,",
                    "year": 2014
                },
                {
                    "title": "Gradientbased learning applied to document recognition",
                    "authors": [
                        "Y. LeCun",
                        "L. Bottou",
                        "Y. Bengio",
                        "P. Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "Multi-view object class detection with a 3d geometric model",
                    "authors": [
                        "Liebelt",
                        "Joerg",
                        "Schmid",
                        "Cordelia"
                    ],
                    "venue": "In CVPR,",
                    "year": 2010
                },
                {
                    "title": "Reading digits in natural images with unsupervised feature learning",
                    "authors": [
                        "Netzer",
                        "Yuval",
                        "Wang",
                        "Tao",
                        "Coates",
                        "Adam",
                        "Bissacco",
                        "Alessandro",
                        "Wu",
                        "Bo",
                        "Ng",
                        "Andrew Y"
                    ],
                    "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning",
                    "year": 2011
                },
                {
                    "title": "Learning and transferring mid-level image representations using convolutional neural networks",
                    "authors": [
                        "M. Oquab",
                        "L. Bottou",
                        "I. Laptev",
                        "J. Sivic"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Domain adaptation via transfer component analysis",
                    "authors": [
                        "Pan",
                        "Sinno Jialin",
                        "Tsang",
                        "Ivor W",
                        "Kwok",
                        "James T",
                        "Yang",
                        "Qiang"
                    ],
                    "venue": "IEEE Transactions on Neural Networks,",
                    "year": 2011
                },
                {
                    "title": "Dlid: Deep learning for domain adaptation by interpolating between domains",
                    "authors": [
                        "S. Chopra",
                        "S. Balakrishnan",
                        "R. Gopalan"
                    ],
                    "venue": "In ICML Workshop on Challenges in Representation Learning,",
                    "year": 2013
                },
                {
                    "title": "Adapting visual category models to new domains",
                    "authors": [
                        "Saenko",
                        "Kate",
                        "Kulis",
                        "Brian",
                        "Fritz",
                        "Mario",
                        "Darrell",
                        "Trevor"
                    ],
                    "venue": "In ECCV, pp",
                    "year": 2010
                },
                {
                    "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function",
                    "authors": [
                        "Shimodaira",
                        "Hidetoshi"
                    ],
                    "venue": "Journal of Statistical Planning and Inference,",
                    "year": 2000
                },
                {
                    "title": "Dropout: A simple way to prevent neural networks from overfitting",
                    "authors": [
                        "Srivastava",
                        "Nitish",
                        "Hinton",
                        "Geoffrey",
                        "Krizhevsky",
                        "Alex",
                        "Sutskever",
                        "Ilya",
                        "Salakhutdinov",
                        "Ruslan"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 1929
                },
                {
                    "title": "Back to the future: Learning shape models from 3d CAD data",
                    "authors": [
                        "Stark",
                        "Michael",
                        "Goesele",
                        "Schiele",
                        "Bernt"
                    ],
                    "venue": "In BMVC, pp",
                    "year": 2010
                },
                {
                    "title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains",
                    "authors": [
                        "Sun",
                        "Baochen",
                        "Saenko",
                        "Kate"
                    ],
                    "venue": "In BMVC,",
                    "year": 2014
                },
                {
                    "title": "Frustratingly easy nbnn domain adaptation",
                    "authors": [
                        "Tommasi",
                        "Tatiana",
                        "Caputo",
                        "Barbara"
                    ],
                    "venue": "In ICCV,",
                    "year": 2013
                },
                {
                    "title": "Deep domain confusion: Maximizing for domain invariance",
                    "authors": [
                        "Tzeng",
                        "Eric",
                        "Hoffman",
                        "Judy",
                        "Zhang",
                        "Ning",
                        "Saenko",
                        "Kate",
                        "Darrell",
                        "Trevor"
                    ],
                    "venue": "CoRR, abs/1412.3474,",
                    "year": 2014
                },
                {
                    "title": "Virtual and real world adaptationfor pedestrian detection",
                    "authors": [
                        "V\u00e1zquez",
                        "David",
                        "L\u00f3pez",
                        "Antonio Manuel",
                        "Mar\u0131\u0301n",
                        "Javier",
                        "Ponsa",
                        "Daniel",
                        "Gomez",
                        "David Ger\u00f3nimo"
                    ],
                    "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
                    "year": 2014
                },
                {
                    "title": "Visualizing and understanding convolutional networks",
                    "authors": [
                        "Zeiler",
                        "Matthew D",
                        "Fergus",
                        "Rob"
                    ],
                    "venue": "CoRR, abs/1311.2901,",
                    "year": 2013
                }
            ],
            "id": "SP:7e9ec8e22237f483dd9147b015185fe959f30bc1",
            "authors": [
                {
                    "name": "Yaroslav Ganin",
                    "affiliations": []
                },
                {
                    "name": "Victor Lempitsky",
                    "affiliations": []
                }
            ],
            "abstractText": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary). As the training progresses, the approach promotes the emergence of \u201cdeep\u201d features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-ofthe-art on Office datasets.",
            "title": "Unsupervised Domain Adaptation by Backpropagation"
        }
    },
    "18304114": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "This paper describes swapout, a stochastic training method for general deep networks. Swapout is a generalization of dropout [17] and stochastic depth [6] methods. Dropout zeros the output of individual units at random during training, while stochastic depth skips entire layers at random during training. In comparison, the most general swapout network produces the value of each output unit independently by reporting the sum of a randomly selected subset of current and all previous layer outputs for that unit. As a result, while some units in a layer may act like normal feedforward units, others may produce skip connections and yet others may produce a sum of several earlier outputs. In effect, our method averages over a very large set of architectures that includes all architectures used by dropout and all used by stochastic depth.\nOur experimental work focuses on a version of swapout which is a natural generalization of the residual network [4, 5]. We show that this results in improvements in accuracy over residual networks with the same number of layers.\nImprovements in accuracy are often sought by increasing the depth, leading to serious practical difficulties. The number of parameters rises sharply, although recent works such as [16, 19] have addressed this by reducing the filter size [16, 19]. Another issue resulting from increased depth is the difficulty of training longer chains of dependent variables. Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly [19, 18, 4] or with additional losses applied to intermediate layers [19, 10]. At the time of writing, the deepest networks that have been successfully trained are residual networks (1001 layers [5]). We show that increasing the depth of our swapout networks increases their accuracy.\nThere is compelling experimental evidence that these very large depths are helpful, though this may be because architectural innovations introduced to make networks trainable reduce the capacity of the layers. The theoretical evidence that a depth of 1000 is required for practical problems is thin.\nar X\niv :1\n60 5.\n06 46\n5v 1\n[ cs\n.C V\n] 2\n0 M\nay 2\nBengio and Dellaleau argue that circuit efficiency constraints suggest increasing depth is important, because there are functions that require exponentially large shallow networks to compute [1]. Less experimental interest has been displayed in the width of the networks (the number of filters in a convolutional layer). We show that increasing the width of our swapout networks leads to significant improvements in their accuracy; an appropriately wide swapout network is competitive with a deep residual network that is 1.5 orders of magnitude deeper and has more parameters.\nContributions: Swapout is a novel stochastic training scheme that can sample from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. Swapout improves the performance of the residual networks for a model of the same depth. Wider but much shallower swapout networks are competitive with very deep residual networks."
                },
                {
                    "heading": "2 Related Work",
                    "text": "Convolutional neural networks have a long history (see the introduction of [9]). They are now intensively studied as a result of recent successes (e.g. [8]). Increasing the number of layers in a network improves performance [16, 19] if the network can be trained. A variety of significant architectural innovations improve trainability, including: the ReLU [12, 2]; batch normalization [7]; and allowing signals to skip layers.\nOur method exploits this skipping process. Highway networks use gated skip connections to allow information and gradients to pass unimpeded across several layers [18]. Residual networks use identity skip connections to further improve training [4]; extremely deep residual networks can be trained, and perform well [5]. In contrast to these architectures, our method skips at the unit level (below), and does so randomly.\nOur method employs randomness at training time. For a review of the history of random methods, see the introduction of [14], which shows that entirely randomly chosen features can produce an SVM that generalizes well. Randomly dropping out unit values (dropout [17]) discourages coadaptation between units. Randomly skipping layers (stochastic depth) [6] during training reliably leads to improvements at test time, likely because doing so regularizes the network. The precise details of the regularization remain uncertain, but it appears that stochastic depth represents a form of tying between layers; when a layer is dropped, other layers are encouraged to be able to replace it. Each method can be seen as training a network that averages over a family of architectures during inference. Dropout averages over architectures with \u201cmissing\u201d units and stochastic depth averages over architectures with \u201cmissing\u201d layers. Other successful recent randomized methods include dropconnect [20] which generalizes dropout by dropping individual connections instead of units (so dropping several connections together), and stochastic pooling [21] (which regularizes by replacing the deterministic pooling by randomized pooling). In contrast, our method skips layers randomly at a unit level enjoying the benefits of each method.\nRecent results show that (a) stochastic gradient descent with sufficiently few steps is stable (in the sense that changes to training data do not unreasonably disrupt predictions) and (b) dropout enhances that property, by reducing the value of a Lipschitz constant ([3], Lemma 4.4). We show our method enjoys the same behavior as dropout in this framework.\nLike dropout, the network trained with swapout depends on random variables. A reasonable strategy at test time with such a network is to evaluate multiple instances (with different samples used for the random variables) and average. Reliable improvements in accuracy are achievable by training distinct models (which have distinct sets of parameters), then averaging predictions [19], thereby forming an explicit ensemble. In contrast, each of the instances of our network in an average would draw from the same set of parameters (we call this an implicit ensemble). Srivastava et al. argue that, at test time, random values in a dropout network should be replaced with expectations, rather than taking an average over multiple instances [17] (though they use explicit ensembles, increasing the computational cost). Considerations include runtime at test; the number of samples required; variance; and experimental accuracy results. For our model, accurate values of these expectations are not available. In Section 4, we show that (a) swapout networks that use estimates of these expectations outperform strong comparable baselines and (b) in turn, these are outperformed by swapout networks that use an implicit ensemble."
                },
                {
                    "heading": "3 Swapout",
                    "text": "Notation and terminology: We use capital letters to represent tensors and to represent elementwise product (broadcasted for scalars). We use boldface 0 and 1 to represent tensors of 0 and 1 respectively. A network block is a set of simple layers in some specific configuration e.g. a convolution followed by a ReLU or a residual network block [4]. Several such potentially different blocks can be connected in the form of a directed acyclic graph to form the full network model.\nDropout kills individual units randomly; stochastic depth skips entire blocks of units randomly. Swapout allows individual units to be dropped, or to skip blocks randomly. Implementing swapout is a straightforward generalization of dropout. Let X be the input to some network block that computes F (X). The u\u2019th unit produces F (u)(X) as output. Let \u0398 be a tensor of i.i.d. Bernoulli random variables. Dropout computes the output Y of that block as\nY = \u0398 F (X). (1)\nIt is natural to think of dropout as randomly selecting an output from the set F (u) = {0, F (u)(X)} for the u\u2019th unit.\nSwapout generalizes dropout by expanding the choice ofF (u). Now write {\u0398i} forN distinct tensors of iid Bernoulli random variables indexed by i and with corresponding parameters {\u03b8i}. Let {Fi} be corresponding tensors consisting of values already computed somewhere in the network. Note that one of these Fi can be X itself (identity). However, Fi are not restricted to being a function of X and we drop the X to indicate this. Most natural choices for Fi are the outputs of earlier layers. Swapout computes the output of the layer in question by computing\nY = N\u2211 i=1 \u0398i Fi (2)\nand so, for unit u, we have F (u) = {F (u)1 , F (u) 2 , . . . , F (u) 1 + F (u) 2 , . . . , \u2211 i F (u) i }. We study the simplest case where\nY = \u03981 X + \u03982 F (X) (3)\nso that, for unit u, we have F (u) = {0, X(u), F (u)(X), X(u) + F (u)(X)}. Thus, each unit in the layer could be:\n1) dropped (choose 0); 2) a feedforward unit (choose F (u)(X));\n3) skipped (choose X(u));\n4) or a residual network unit (choose X(u) + F (u)(X)).\nSince a swapout network can clearly imitate a residual network, and since residual networks are currently the best-performing networks on various standard benchmarks, we perform exhaustive experimental comparisons with them.\nIf one accepts the view of dropout and stochastic depth as averaging over a set of architectures, then swapout extends the set of architectures used. Appropriate random choices of \u03981 and \u03982 yield: all architectures covered by dropout; all architectures covered by stochastic depth; and block level skip connections. But other choices yield unit level skip and residual connections.\nSwapout retains important properties of dropout. Swapout discourages co-adaptation by dropping units, but also by on occasion presenting units with inputs that have come from earlier layers. Dropout has been shown to enhance the stability of stochastic gradient descent ([3], lemma 4.4). This applies to swapout in its most general form, too. We extend the notation of that paper, and write L for a Lipschitz constant that applies to the network, \u2207f(v) for the gradient of the network f with parameters v, and D\u2207f(v) for the gradient of the dropped out version of the network. The crucial point in the relevant enabling lemma is that E[||Df(v) ||] < E[||\u2207f(v) ||] \u2264 L (the inequality implies improvements). Now write\u2207S [f ] (v) for the gradient of a swapout network, and \u2207G [f ] (v) for the gradient of the swapout network which achieves the largest Lipschitz constant by choice of \u0398i (this exists, because \u0398i is discrete). First, a Lipschitz constant applies to this network; second, E[||\u2207S [f ] (v) ||] \u2264 E[||\u2207G [f ] (v) ||] \u2264 L, so swapout makes stability no worse; third, we speculate light conditions on f should provide E[||\u2207S [f ] (v) ||] < E[||\u2207G [f ] (v) ||] \u2264 L, improving stability ([3] Section 4)."
                },
                {
                    "heading": "3.1 Inference in Stochastic Networks",
                    "text": "A model trained with swapout represents an entire family of networks with tied parameters, where members of the family were sampled randomly during training. There are two options for inference. We could either replace random variables with their expected values, as recommended by Srivastava et al. [17] (deterministic inference). Alternatively, we could sample several members of the family at random, and average their predictions (stochastic inference).\nThere is an important difference between swapout and dropout. In a dropout network, one can estimate expectations exactly (as long as the network isn\u2019t trained with batch normalization, below). This is because E[ReLU[\u0398 F (X)]] = ReLU[E[\u0398 F (X)]] (recall \u0398 is a tensor of Bernoulli random variables, and thus non-negative).\nIn a swapout network, one usually can not estimate expectations exactly. The problem is that E[ReLU[(\u03981X + \u03982Y )]] is not the same as ReLU[E[(\u03981X + \u03982Y )]] in general. Estimates of expectations that ignore this are successful, as the experiments show, but stochastic inference gives significantly better results.\nSrivastava et al. argue that deterministic inference is significantly less expensive in computation. We believe that Srivastava et al. may have overestimated how many samples are required for an accurate average, because they use distinct dropout networks in the average (Figure 11 in [17]). Our experience of stochastic inference with swapout has been positive, with the number of samples needed for good behavior small (Figure 2). Furthermore, computational costs of inference are smaller when each instance of the network uses the same parameters\nA technically more delicate point is that both dropout and swapout networks interact poorly with batch normalization if one uses deterministic inference. The problem is that the estimates collected by batch normalization during training may not reflect test time statistics. To see this consider two random variables X and Y and let \u03981,\u03982 \u223c Bernoulli(\u03b8). While E[\u03981X + \u03982Y ] = E[\u03b8X + \u03b8Y ] = \u03b8X + \u03b8Y , it can be shown that Var[\u03981X + \u03982Y ] \u2265 Var[\u03b8X + \u03b8Y ] with equality holding only for \u03b8 = 0 and \u03b8 = 1. Thus, the variance estimates collected by Batch Normalization during training do not represent the statistics observed during testing if the expected values of \u03981 and \u03982 are used in a deterministic inference scheme. These errors in scale estimation accumulate as more and more layers are stacked. This may explain why [6] reports that dropout doesn\u2019t lead to any improvement when used in residual networks with batch normalization."
                },
                {
                    "heading": "3.2 Baseline comparison methods",
                    "text": "ResNets: We compare with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2).\nDropout: We use standard dropout (replace equation 3 with equation 1).\nLayer Dropout: We replace equation 3 by Y = X + \u0398(1\u00d71)F (X). Here \u0398(1\u00d71) is a single Bernoulli random variable shared across all units.\nSkipForward: Equation 3 introduces two stochastic parameters \u03981 and \u03982. We also explore and compare with a simpler architecture, SkipForward, that introduces only one parameter but samples from a smaller set F (u) = {X(u), F (u)(X)} as below.\nY = \u0398 X + (1\u2212\u0398) F (X) (4)"
                },
                {
                    "heading": "4 Experiments",
                    "text": "We experiment extensively on the CIFAR-10 dataset and demonstrate that a model trained with swapout outperforms a comparable ResNet model. Further, a 32 layer wider model matches the performance of a 1001 layer ResNet on both CIFAR-10 and CIFAR-100 datasets.\nModel: We experiment with ResNet architectures as described in [4](referred to as v1) and in [5](referred to as v2). However, our implementation (referred to as ResNet Ours) has the following modifications which improve the performance of the original model (Table 1). Between blocks of different feature sizes we subsample using average pooling instead of strided convolutions and use projection shortcuts with learned parameters. For final prediction we follow a scheme similar to Network in Network [11]. We replace average pooling and fully connected layer by a 1x1 convolution layer followed by global average pooling to predict the logits that are fed into the softmax.\nLayers in ResNets are arranged in three groups with all convolutional layers in a group containing equal number of filters. We represent the number of filters in each group as a tuple with the smallest size as (16, 32, 64) (as used in [4]for CIFAR-10). We refer to this as width and experiment with various multiples of this base size represented as W \u00d7 1, W \u00d7 2 etc.\nTraining: We train using SGD with a batch size of 128, momentum of 0.9 and weight decay of 0.0001. Unless otherwise specified, we train all the models for a total 256 epochs. Starting from an initial learning rate of 0.1, we drop it by a factor of 10 after 196 epochs and then again after 224 epochs. We do the standard augmentation of left-right flips and random translations of up to four pixels. For translation, we pad the images by 4 pixels on all the sides and sample a random 32x32 crop. All the images in a mini-batch use the same crop. Note that dropout slows convergence ([17], A.4), and swapout should do so too for similar reasons. Thus using the same training schedule for all the methods should disadvantage swapout.\nModels trained with Swapout consistently outperform baselines: Table 1 compares Swapout with various 20 layer baselines. Models trained with Swapout consistently outperform all other models of similar architecture.\nThe stochastic training schedule matters: Different layers in a swapout network could be trained with different parameters of their Bernoulli distributions (the stochastic training schedule). Table 2 shows that different stochastic training schedules have a significant affect on the performance. We report the performance with deterministic as well as stochastic inference. These schedules differ in how the values of parameters \u03b81 and \u03b82 of the Bernoulli random variables in equation 3 are set for the different layers. Note that \u03b81 = \u03b82 = 0.5 corresponds to the maximum stochasticity. A schedule with less randomness in the early layers (bottom row) performs the best. This is expected because Swapout adds per unit noise and early layers have the largest number of units. Thus, low stochasticity in early layers significantly reduces the randomness in the system. We use this schedule for all the experiments unless otherwise stated.\nSwapout improves over ResNet architecture: From Table 3 it is evident that networks trained with Swapout consistently show better performance than corresponding ResNets, for most choices of width investigated, using just the deterministic inference. This difference indicates that the performance improvement is not just an ensemble effect.\nStochastic inference outperforms deterministic inference: Table 3 shows that the stochastic inference scheme outperforms the deterministic scheme in all the experiments. Prediction for each image is done by averaging the results of 30 stochastic forward passes. This difference is not just due to the widely reported effect that an ensemble of networks is better as networks in our ensemble share parameters. Instead, stochastic inference produces more accurate expectations and interacts better with batch normalization.\nStochastic inference needs few samples for a good estimate: Figure 2 shows the estimated accuracies as a function of the number of forward passes per image. It is evident that relatively few samples are enough for a good estimate of the mean. Compare Figure-11 of [17], which implies \u223c 50 samples are required.\nIncrease in width leads to considerable performance improvements: The number of filters in a convolutional layer is its width. Table 3 shows that the performance of a 20 layer model improves considerably as the width is increased both for the baseline ResNet v2 architecture as well as\nthe models trained with Swapout. Swapout is better able to use the available capacity than the corresponding ResNet with similar architecture and number of parameters. Table 4 compares models trained with Swapout with other approaches on CIFAR-10 while Table 5 compares on CIFAR-100. On both datasets our shallower but wider model compares well with 1001 layer ResNet model.\nSwapout uses parameters efficiently: Persistently over tables 1, 3, and 4, Swapout models with fewer parameters outperform other comparable models. For example, Swapout v2(32) W \u00d7 4 gets 4.76% error with 7.43M parameters in comparison to the ResNet version at 4.91% with 10.2M parameters.\nExperiments on CIFAR-100 confirm our results: Table 5 shows that Swapout is very effective as it improves the performance of a 20 layer model (ResNet Ours) by more than 2%. Widening the network and reducing the stochasticity leads to further improvements. Further, a wider but relatively shallow model trained with Swapout (22.72%; 7.46M params) is competitive with the best performing, very deep (1001 layer) latest ResNet model (22.71%;10.2M params)."
                },
                {
                    "heading": "5 Discussion and future work",
                    "text": "Swapout is a stochastic training method that shows reliable improvements in performance and leads to networks that use parameters efficiently. Relatively shallow swapout networks give comparable performance to extremely deep residual networks.\nWe have shown that different stochastic training schedules produce different behaviors, but have not searched for the best schedule in any systematic way. It may be possible to obtain improvements by doing so. We have described an extremely general swapout mechanism. It is straightforward using equation 2 to apply swapout to inception networks [19] (by using several different functions of the input and a sufficiently general form of convolution); to recurrent convolutional networks [13] (by\nchoosing Fi to have the form F \u25e6 F \u25e6 F . . .); and to gated networks. All our experiments focus on comparisons to residual networks because these are the current top performers on CIFAR-10 and CIFAR-100. It would be interesting to experiment with other versions of the method.\nAs with dropout and batch normalization, it is difficult to give a crisp explanation of why swapout works. We believe that our results support the idea that swapout causes some form of improvement in the optimization process. This is because relatively shallow networks with swapout reliably work as well as or better than quite deep alternatives; and because swapout is notably and reliably more efficient in its use of parameters than comparable deeper networks. Unlike dropout, swapout will often propagate gradients while still forcing units not to co-adapt. Furthermore, our swapout networks involve some form of tying between layers. When a unit sometimes sees layer i and sometimes layer i\u2212 j, the gradient signal will be exploited to encourage the two layers to behave similarly. The reason swapout is successful likely involves both of these points.\nAcknowledgments: This work is supported in part by ONR MURI Awards N00014-10-1-0934 and N00014-16-1-2007. We would like to thank NVIDIA for donating some of the GPUs used in this work."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "On the expressive power of deep architectures",
                    "authors": [
                        "Y. Bengio",
                        "O. Delalleau"
                    ],
                    "venue": "Proceedings of the 22nd International Conference on Algorithmic Learning Theory,",
                    "year": 2011
                },
                {
                    "title": "Deep sparse rectifier neural networks",
                    "authors": [
                        "X. Glorot",
                        "A. Bordes",
                        "Y. Bengio"
                    ],
                    "venue": "AISTATS,",
                    "year": 2011
                },
                {
                    "title": "Train faster, generalize better: Stability of stochastic gradient descent",
                    "authors": [
                        "M. Hardt",
                        "B. Recht",
                        "Y. Singer"
                    ],
                    "venue": "CoRR, abs/1509.01240,",
                    "year": 2015
                },
                {
                    "title": "Deep residual learning for image recognition",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "CoRR, abs/1512.03385,",
                    "year": 2015
                },
                {
                    "title": "Identity mappings in deep residual networks",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "CoRR, abs/1603.05027,",
                    "year": 2016
                },
                {
                    "title": "Deep networks with stochastic depth",
                    "authors": [
                        "G. Huang",
                        "Y. Sun",
                        "Z. Liu",
                        "D. Sedra",
                        "K.Q. Weinberger"
                    ],
                    "venue": "CoRR, abs/1603.09382,",
                    "year": 2016
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "S. Ioffe",
                        "C. Szegedy"
                    ],
                    "venue": "CoRR, abs/1502.03167,",
                    "year": 2015
                },
                {
                    "title": "Imagenet classification with deep convolutional neural networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS,",
                    "year": 2012
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Y. LeCun",
                        "L. Bottou",
                        "Y. Bengio",
                        "P. Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "Deeply-supervised nets",
                    "authors": [
                        "C.-Y. Lee",
                        "S. Xie",
                        "P. Gallagher",
                        "Z. Zhang",
                        "Z. Tu"
                    ],
                    "venue": "AISTATS,",
                    "year": 2015
                },
                {
                    "title": "Network in network",
                    "authors": [
                        "M. Lin",
                        "Q. Chen",
                        "S. Yan"
                    ],
                    "venue": "CoRR, abs/1312.4400,",
                    "year": 2013
                },
                {
                    "title": "Rectified linear units improve restricted boltzmann machines",
                    "authors": [
                        "V. Nair",
                        "G.E. Hinton"
                    ],
                    "venue": "ICML,",
                    "year": 2010
                },
                {
                    "title": "Recurrent convolutional neural networks for scene parsing",
                    "authors": [
                        "P.H. Pinheiro",
                        "R. Collobert"
                    ],
                    "venue": "arXiv preprint arXiv:1306.2795,",
                    "year": 2013
                },
                {
                    "title": "Random features for large-scale kernel machines",
                    "authors": [
                        "A. Rahimi",
                        "B. Recht"
                    ],
                    "venue": "NIPS,",
                    "year": 2007
                },
                {
                    "title": "Fitnets: Hints for thin deep nets",
                    "authors": [
                        "A. Romero",
                        "N. Ballas",
                        "S.E. Kahou",
                        "A. Chassang",
                        "C. Gatta",
                        "Y. Bengio"
                    ],
                    "venue": "ICLR,",
                    "year": 2015
                },
                {
                    "title": "Very deep convolutional networks for large-scale image recognition",
                    "authors": [
                        "K. Simonyan",
                        "A. Zisserman"
                    ],
                    "venue": "CoRR, abs/1409.1556,",
                    "year": 2014
                },
                {
                    "title": "Dropout: A simple way to prevent neural networks from overfitting",
                    "authors": [
                        "N. Srivastava",
                        "G. Hinton",
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "R. Salakhutdinov"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2014
                },
                {
                    "title": "Training very deep networks",
                    "authors": [
                        "R.K. Srivastava",
                        "K. Greff",
                        "J. Schmidhuber"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Going deeper with convolutions",
                    "authors": [
                        "C. Szegedy",
                        "W. Liu",
                        "Y. Jia",
                        "P. Sermanet",
                        "S.E. Reed",
                        "D. Anguelov",
                        "D. Erhan",
                        "V. Vanhoucke",
                        "A. Rabinovich"
                    ],
                    "venue": "CoRR, abs/1409.4842,",
                    "year": 2014
                },
                {
                    "title": "Regularization of neural networks using dropconnect",
                    "authors": [
                        "L. Wan",
                        "M. Zeiler",
                        "S. Zhang",
                        "Y.L. Cun",
                        "R. Fergus"
                    ],
                    "venue": "ICML, pages 1058\u20131066,",
                    "year": 2013
                },
                {
                    "title": "Stochastic pooling for regularization of deep convolutional neural networks",
                    "authors": [
                        "M.D. Zeiler",
                        "R. Fergus"
                    ],
                    "venue": "arXiv preprint arXiv:1301.3557,",
                    "year": 2013
                }
            ],
            "id": "SP:5de063c31725b578dcbaa0d2cee514f7e13874aa",
            "authors": [
                {
                    "name": "Saurabh Singh",
                    "affiliations": []
                },
                {
                    "name": "Derek Hoiem",
                    "affiliations": []
                },
                {
                    "name": "David Forsyth",
                    "affiliations": []
                }
            ],
            "abstractText": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR100. Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.",
            "title": "Swapout: Learning an ensemble of deep architectures"
        }
    },
    "53851426": {
        "X": {
            "sections": [
                {
                    "text": "Keywords: Survival analysis; risk model; patient specific survival prediction; calibration; discrimination\nar X\niv :1\n81 1.\n11 34\n7v 1\n[ cs\n.L G"
                },
                {
                    "heading": "1 Introduction",
                    "text": "When diagnosed with a terminal disease, many patients ask about their prognosis [21]: \u201cHow long will I live?\u201d, or \u201cWhat is the chance that I will live for 1 year... and the chance for 5 years?\u201d. Here it would be useful to have a meaningful \u201csurvival distribution\u201d S( t | ~x ) that provides, for each time t \u2265 0, the probability that this specific patient ~x will survive at least an additional t months. Unfortunately, many of the standard survival analysis tools cannot accurately answer such questions: (1) risk scores (e.g., Cox proportional hazard [10]) provide only relative survival measures, but not the calibrated probabilities desired; (2) single-time probability models (e.g., the Gail model [9]) provide a probability value but only for a single time point; and (3) class-based survival curves (like Kaplan-Meier, km [31]) are not specific to the patient, but rather an entire population.\nTo explain the last point, Figure 1[left] shows the km curve for patients with stage4 stomach cancer. Here, we can read off the claim that 50% of the patients will survive 11 months, and 95% will survive at least 2 months.1 While these estimates do apply to the population, on average, they are not designed to be \u201caccurate\u201d for an individual patient since these estimates do not include patient-specific information such as age, treatments administered, or general health conditions. It would be better to directly, and correctly, incorporate these important factors ~x explicitly in the prognostic models.\nThis heterogeneity of patients, coupled with the need to provide probabilistic estimates at several time points, has motivated the creation of several individual survival time distribution (isd) tools, each of which can use this wealth of healthcare information from earlier patients, to learn a more accurate prognostic model, which can then predict the isd of a novel patient based on all available patient-specific attributes. This paper considers several isd models: the Kalbfleisch-Prentice extension of the Cox (cox-kp) [29] and the elastic net Cox (coxen-kp) [55] model, the Accelerated Failure Time (aft) model [29], the Random Survival Forest model with Kaplan-Meier extensions (rsf-km), and the Multi-task Logistic Regression (mtlr) model [57]. Figure 1(middle, right) show survival curves (generated by mtlr) for two of these stage-4 stomach cancer patients, which incorporate other information about these individual patients, such as the patient\u2019s age, gender, blood work, etc. We see that these prognoses are very different; in particular, mtlr predicts that [middle] Patient #1\u2019s median survival time is 20.2 months, while [right] Patient #2\u2019s is only 2.6 months. The blue vertical lines show the actual times of death; we see that each of these patients passed away very close to mtlr\u2019s predictions of their respective median survival times.\nOne could then use such curves to make decisions about the individual patient. Of course, these decisions will only be helpful if the model is giving accurate information \u2013 i.e., only if it is appropriate to tell a patient that s/he has a 50% chance of dying before the median survival time of this predicted curve, and a 25% chance of dying before the time associated\n1 In general, a survival curve is a plot where each [x, y ] point represents (the curve\u2019s claim that) there is a y% chance of surviving at least x time. Hence, in Figure 1[left], the [ 11 months, 50% ] point means this curve predicts a 50% chance of living at least 11 months (and hence a 100\u221250 = 50% chance of dying within the first 11 months). The [ 2 months, 95% ] point means a 95% chance of surviving at least 2 months, and the [ 51 months, 5% ] point means a 5% chance of surviving at least 51 months.\nwith the 25% on the curve, etc. We focus on ways to learn such models from a \u201csurvival dataset\u201d (see below), describing earlier individuals. Survival prediction is similar to regression as both involve learning a model that regresses the covariates of an individual to estimate the value of a dependent real-valued response variable \u2013 here, that variable is \u201ctime to event\u201d (where the standard event is \u201cdeath\u201d). But survival prediction differs from the standard regression task as its response variable is not fully observed in all training instances \u2013 this task allows many of the instances to be \u201cright censored\u201d, in that we only see a lower bound of the response value. This might happen if a subject was alive when the study ended, meaning we only know that she lived at least (say) 5 years after the starting time, but do not know whether she actually lived 5 years and a day, or 30 years. This also happens if a subject drops out of a study, after say 2.3 years, and is then lost to follow-up; etc. Moreover, one cannot simply ignore such instances as it is common for many (or often, most) of the training instances to be right-censored; see Table 4. Such \u201cpartial label information\u201d is problematic for standard regression techniques, which assume the label is completely specified for each training instance. Fortunately, there are survival prediction algorithms that can learn an effective model, from a cohort that includes such censored data. Each such \u201csurvival dataset\u201d contains descriptions of a set of instances (e.g., patients), as well as two \u201clabels\u201d for each: one is the time, corresponding to the time from diagnosis to a final date (either death, or time of last follow-up) and the other is the status bit, which indicates whether the patient was alive at that final date. Section 2 summarizes several popular models for dealing with such survival data.\nThis paper provides three contributions: (1) Section 2 motivates the need for such isd models by showing how they differ from more standard survival analysis systems. (2) Section 3 then discusses several ways to evaluate such models, including standard measures (Concordance, 1-Calibration, Brier score), variants/extensions to familiar measures (L1-loss, Log-L1-loss), and also a novel approach, \u201cD-Calibration\u201d which can be used to assess the quality of the individual survival curves generated by isd models. (3) Section 4 evaluates several isd (and related) models (standard: km, cox-kp, aft and more recent: rsf-km,\ncoxen-kp, mtlr) on 8 diverse survival datasets, in terms of all 5 evaluation measures. We will see that mtlr does well \u2013 typically outperforming the other models in the various measures, and often showing vast improvement in terms of calibration metrics.\nThe appendices provide relevant auxiliary information: Appendix A describes some important nuances about survival curves. Appendix B provides further details concerning all the evaluation metrics and in particular, how each addresses censored observations. It also contains some relevant proofs about our novel D-Calibration metric. Appendix C then explains some additional aspects of the isd models considered in this paper. Lastly, Appendix D gives the detailed results from empirical evaluation \u2013 e.g., providing detailed tables corresponding to the results shown as figures in Section 4.2.\nFor readers who want an introduction to survival analysis and prediction, we recommend Applied Survival Analysis by Hosmer and Lemeshow [26]. Wang et al. [51] surveyed machine learning techniques and evaluation metrics for survival analysis. However, that work primarily overviewed the standard survival analysis models, then briefly discussed some of the evaluation techniques and application areas. Our work, instead, focuses on the isd-based models \u2013 first motivating why they are relevant for survival prediction (with a focus on medical situations) then providing empirical results showing the strengths and weaknesses of each of the models considered."
                },
                {
                    "heading": "2 Summary of Various Survival Analysis/Prediction",
                    "text": "Systems\nThere are many different survival analysis/prediction tools, designed to deal with various different tasks. We focus on tools that learn the model from a survival dataset,\nD = { [~xi, ti, \u03b4i] }i (1)\nwhich provides the values for features ~xi = [x (1) i , \u00b7 \u00b7 \u00b7 , x (k) i ] for each member of a cohort of historical patients, as well as the actual time of the \u201cevent\u201d ti \u2208 <\u22650 which is either death (uncensored) or the last visit (censored), and a bit \u03b4 \u2208 {0, 1} that serves as the indicator for death.2 See Figure 2, in the context of our isd framework.\nHere, we assume ~x is a vector of feature values describing a patients, using information that are available when that patient entered the study \u2013 e.g., when the patient was first diagnosed with the disease, or started the treatment. Additionally, we assume each patient has a death time, di, and a censoring time, ci, and assign ti := min{di, ci} and \u03b4i = I [ di \u2264 ci ] where I [ \u00b7 ] is the Indicator function \u2013 i.e., \u03b4i := 1 if di \u2264 ci or \u03b4i := 0 if di > ci. We follow the standard convention that di and ci are assumed independent.\nTo help categorize the space of survival prediction systems, we consider 3 independent characteristics:\n2 Throughout this work we focus on only Right-Censored survival data. Additionally, we constrain our work to the standard machine-learning framework, where our predictions are based only on information available at fixed time t0 (e.g., start of treatment). While these descriptions all apply when dealing with the time to an arbitrary event, our descriptions will refer to \u201ctime to death\u201d.\nThis section summarizes 5 (of the 2 \u00d7 3 \u00d7 2 = 12) classes of survival analysis tools (see Figure 3), giving typical uses of each, then discusses how they are interrelated.\n2.1 [R,1\u2200,i]: 1-value Individual Risk Models (cox)\nAn important class of survival analysis tools compute \u201crisk\u201d scores, r(~x) \u2208 < for each patient ~x, with the understanding that r(~xa) > r(~xb) corresponds to predicting that ~xa\nwill die before ~xb. Hence, this is a discriminative tool for comparing pairs of patients, or perhaps for \u201cwhat if\u201d analysis of a single patient (e.g., if he continues smoking, versus if he quits). These systems are typically evaluated using a discriminative measure, such as \u201cConcordance\u201d (discussed in Section 3.1). Notice these tools each return a single real value for each patient.\nOne standard generic tool here is the Cox Proportional Hazard (cox) model [10], which is used in a wide variety of applications. This models the hazard function3 as\nhcox( t, ~x ) = \u03bb0(t) exp(~\u03b2 T~x) (2)\nwhere ~\u03b2 are the learned weights for the features, and \u03bb0(t) is the baseline hazard function. We view this as a Risk Model by ignoring \u03bb0(t) (as \u03bb0(t) is the same for all patients), and focusing on just exp(~\u03b2T~x) \u2208 <+. (But see the cox-kp model below, in [P,\u221e,i].) There are many other tools for predicting an individual\u2019s risk score, typically with respect to some disease; see for example the Colditz-Rosner model [8], and the myriad of others appearing on the Disease Risk Index website4. For all of these models, the value returned is atemporal \u2013 i.e., it does not depend on a specific time. There are also tools that produce [R,\u221e,i] models, that return a risk score associated across all time point; see Section 3.1.\n3 The hazard function (also known as the failure rate, hazard rate, or force of mortality) h(t; ~x) = p(t | ~x)/S( t | ~x ) is essentially the chance that ~x will die at time t, given that s/he has lived until this time, using the survival PDF p(t | ~x). When continuous, h(t; ~x) = \u2212 ddt logS( t | ~x ).\n4http://www.diseaseriskindex.harvard.edu/update/"
                },
                {
                    "heading": "2.2 [R,1t\u2217,g]: Single-time Group Risk Predictors: Prognostic Scales (PPI, PaP)",
                    "text": "Another class of risk predictions explicitly focus on a single time, leading to prognostic scales, some of which are computed using Likert scales [40]. For example, the Palliative Prognostic Index (PPI) [35] computes a risk score for each terminally ill patient, which is then used to assign that patient into one of three groups. It then uses statistics about each group to predict that patients in one group will do better at this specific time (here, 3 weeks), than those in another group. Similarly, the Palliative Prognostic Score (PaP) [38] uses a patient\u2019s characteristics to assign him/her into one of 3 risk groups, which can be used to estimate the 30-day survival risk. (There are many other such prognostic scales, including [7, 2, 23].) Again, these tools are typically evaluated using Concordance.5"
                },
                {
                    "heading": "2.3 [P,1t\u2217,i]: Single-time Individual Probabilistic Predictors (Gail, PredictDepression)",
                    "text": "Another class of single-time predictors each produce a survival probability S\u0302( t\u2217 | ~x ) \u2208 [0, 1] for each individual patient ~x, for a single fixed time t\u2217 \u2013 which is the probability \u2208 [0, 1] that ~x will survive to at least time t\u2217. For example, the Gail model [Gail] [9] 6 estimates the probability that a woman will develop breast cancer within 5 years based on her responses to a number of survey questions. Similarly, the PredictDepression system [PredDep] [50] 7 predicts the probability that a patient will develop a major depressive episode in the next 4 years based on a small number of responses. The Apervite8 and R-calc9 websites each include dozens of such tools, each predicting the survival probability for 1 (or perhaps 2) fixed time points, for certain classes of diseases.\nNotice these probability values have semantic content, and are labels for individual patients (rather than risk-scores, which are only meaningful within the context of other patients\u2019 risk scores). These systems should be evaluated using a calibration measure, such as 1-Calibration or Brier score (discussed in Sections 3.3 and 3.4).\n2.4 [P,\u221e,g]: Group Survival Distribution (km) There are many systems that can produce a survival distribution: a graph of [t, S\u0302( t )], showing the survival probability S\u0302( t ) \u2208 [0, 1] for each time t \u2265 0; see Figure 1. The KaplanMeier analytic tool (km) is at the \u201cclass\u201d level, producing a distribution designed to apply to everyone in a sub-population: S\u0302( t | ~x ) = S\u0302( t ), for every ~x in some class \u2013 e.g., the km curve in Figure 1[left] applies to every patient ~x with stage-4 stomach cancer. The SEER\n5 Here, they do not compare pairs of individuals from the same group, but only patients from different groups, whose events are comparable (given censoring); see Section 3.1.\n6http://www.cancer.gov/bcrisktool/ 7http://predictingdepression.com/ 8https://apervita.com/community/clevelandclinic 9http://www.r-calc.com/ExistingFormulas.aspx?filter=CCQHS\nwebsite10 provides a set of Kaplan-Meier curves for various cancers. While patients can use such information to estimate their survival probabilities, the original goal of that analysis is to better understand the disease itself, perhaps by seeing whether some specific feature made a difference, or if a treatment was beneficial. For example, we could produce one curve for all stage-4 stomach cancer patients who had treatment tA, and another for the disjoint subset of patients who had no treatment; then run a log-rank test [22] to determine whether (on average) patients receiving treatment tA survived statistically longer than those who did not. Section 3 below describes various ways to evaluate [P,\u221e,i] models; we will use these measures to evaluate km models as well.\n2.5 [P,\u221e,i]: Individual Survival Distribution, isd (cox-kp, coxenkp, aft, rsf-km, mtlr)\nThe previous two subsections described two frameworks:\n\u2022 [P,1t\u2217 ,i] tools, which produce an individualized probability value S\u0302( t\u2217 | ~xi ) \u2208 [0, 1], but only for a single time t\u2217; and\n\u2022 [P,\u221e,g] tools, which produce the entire survival probability curve [t, S\u0302( t )] for all points t \u2265 0, but are not individuated \u2013 i.e., the same curve for all patients { ~xi }.\nHere, we consider an important extension: a tool that produces the entire survival probability curve { [t, S\u0302( t | ~xi )] }t for all points t \u2265 0, specific to each individual patient, ~xi. As noted in the previous section, this is required by any application that requires knowing meaningful survival probabilities for many time points. This model also allows us to compute other useful statistics, such as a specific patient\u2019s expected survival time.\nWe call each such system an \u201cIndividual Survival Distribution\u201d model, isd. While the Cox model is often used just to produce the risk score, it can be used as an isd, given an appropriate (learned) baseline hazard function \u03bb0(t); see Equation 2. We estimate this using the Kalbfleisch-Prentice estimator [29], and call this combination \u201ccox-kp\u201d; we also consider a regularized Cox model, namely the elastic net Cox with the Kalbfleisch-Prentice extension (coxen-kp). We also explore three other models: Accelerated Failure Time model [29] with the Weibull distribution (aft), Random Survival Forests with the Kaplan-Meier extension (rsf-km, described in Appendix C.3) [28] and Multi-task Logistic Regression system (mtlr) [57]. Figure 4 shows the curves from these various models, each over the same set of individuals.\nAbove, we briefly mentioned three evaluation methods: Concordance, 1-Calibration, and Brier score. We show below that we can use any of these methods to evaluate a isd model. In addition, we can also use variants of \u201cL1-loss\u201d, to see how far a predicted single-time differs from the true time of death; see Section 3.2. Each of these 4 methods considers only a single time point of the distribution, or an average of scores, each based on only a single time, or a single statistic (such as its median value). We also consider a novel evaluation measure,\n10http://seer.cancer.gov/\n\u201cD-Calibration\u201d, which uses the entire distribution of estimated survival probabilities; see Section 3.5."
                },
                {
                    "heading": "2.6 Other Issues",
                    "text": "The goal of many Survival Analysis tools is to identify relevant variables, which is different from our challenge here, of making a prediction about an individual. Some researchers use km to test whether a variable is relevant \u2013 e.g., they partition the data into two subsets, based on the value of that variable, then run km on each subset, and declare that variable to be relevant if a log-rank test claims these two curves are significantly different [22]. It is also a common use of the basic Cox model \u2013 in essence, by testing if the \u03b2\u0302i coefficient associated with feature xi (in Equation 2) is significantly different from 0 [48]. (We will later use this approach to select features, as a pre-processing step, before running the actual survival prediction model; see Section 4.1.)\nNote this \u201cg vs i \u201d distinction is not always crisp, as it depends on how many variables are involved \u2013 e.g., models that \u201cdescribe\u201d each instance using no variables (like km) are clearly \u201cg \u201d, while models that use dozens or more variables, enough to distinguish each patient from one another, are clearly \u201ci \u201d. But models that involve 2 or 3 variables typically will place each patient into one of a small number of \u201cclusters\u201d, and then assign the same values to each member of a cluster. By convention, we will catalog those models as \u201cg \u201d as the decision is not intended to be at an individual level.\nThe \u201c1t\u2217\u201d vs \u201c\u221e\u201d distinction can be blurry, if considering a system that produces a small number k > 1 of predictions for each individual \u2013 e.g., the Gail model provides a prediction of both 5 year and 25 year survival. We consider this system as a pair of \u201c1t\u2217\u201d-predictors, as those two models are different. (Technically, we could view them as \u201cGail[5year]\u201d versus \u201cGail[25year]\u201d models.)\nFinally, recall there are two types of frameworks that each return a single value for each instance: the single value returned by the [R,1\u2200,i]-model cox is atemporal \u2013 i.e., applies to the overall model \u2013 while each single value returned by the [P,1t\u2217 ,i]-model Gail and the [R,1t\u2217 ,g]-model PaP, is for a specific time, t\n\u2217. (Note there can also be [P,1\u2200,i]- and [R,1\u2200,g]models that are atemporal.)"
                },
                {
                    "heading": "2.7 Relationship of Distributional Models to Other Survival Anal-",
                    "text": "ysis Systems\nWe will use the term \u201cDistributional Model\u201d to refer to algorithms within the [P,\u221e,g] and [P,\u221e,i] frameworks \u2013 i.e., both km and isd models. Note that such models can match the functionality of the first 3 \u201cpersonalized\u201d approaches. First, to emulate [P,1t\u2217 ,i], we just need to evaluate the distribution at the specified single time t\u2217 \u2013 i.e., S\u0302( t\u2217 | ~x ). So for Patient #1 (from Figure 1), for t\u2217 = \u201c48 months\u201d, this would be 20%. Second, to emulate [R,1t\u2217 ,i], we can just use the negative of this value as the time-dependent risk score \u2013 so the 4-year risk for Patient #1 would be -0.20. Third, to deal with [R,1\u2200,i], we need to reduce the distribution to\na single real number, where larger values indicate shorter survival times. A simple candidate is the individual distribution\u2019s median value, which is where the survival curve crosses 50%.11 So for Patient #1 in Figure 1, the median is t\u0302 (0.5) 1 = 16 months. We can then view (the negative of) this scalar as the risk score for that patient. So for Patient #1, the \u201crisk\u201d would be r(~x1) = \u221216 . Fourth, to view the isd model in the [R,1\u2200,g] framework, we need to place the patients into a small number of \u201crelatively homogeneous\u201d bins. Here, we could quantize the (predicted) mean value \u2013 e.g., mapping a patient to Bin#1 if that mean is in [0, 15), Bin#2 if in [15, 27), and Bin#3 if in [27, 70]. (Here, this patient would be assigned to Bin#2.) Fifth, to view the isd model in the [R,1t\u2217 ,g] framework, associated with a time t\n\u2217, we could quantize the t\u2217-probability \u2013 e.g., quantize the S\u0302( t\u2217 = 48 months | ~x ) into 4 bins corresponding to the intervals [0, 0.20), [0.20, 0.57), [0.57, 0.83], and [0.83, 1.0].\nThese simple arguments show that a distributional model can produce the scalars used by five other frameworks [P,1t\u2217 ,i], [R,1t\u2217 ,i], [R,1\u2200,i], [R,1\u2200,g], and [R,1t\u2217 ,g]. Of course, a distributional model can also provide other information about the patient \u2013 not just the probability associated with one or two time points, but at essentially any time in the future, as well as the mean/median value. Another advantage of having such survival curves is visualization (see Figure 1): it allows the user (patient or clinician) to see the shape of the curve, which provides more information than simply knowing the median, or the chance of surviving 5 years, etc.\nThere are some subtle issued related to producing meaningful survival curves \u2013 e.g., many curves end at a non-zero value: note the km curve in Figure 4(top left) stops at (83, 0.12), rather than continue to intersect the x-axis at, perhaps (103, 0.0). This is true for many of the curves produced by the isds. Indeed, some of the curves do not even cross y = 0.5, which means the median time is not well-defined; cf. the top orange line on the aft curve (top right), which stops at (83, 0.65), as well as many of the other curves throughout that figure. This causes many problems, in both interpreting and evaluating isd models. Appendix A shows how we address this."
                },
                {
                    "heading": "3 Measures for Evaluating Survival Analysis/Prediction",
                    "text": "Models\nThe previous section mentioned 5 ways to evaluate a survival analysis/prediction model: Concordance, 1-Calibration, Brier score, L1-loss, and D-Calibration. This section will describe these \u2013 quickly summarizing the first four (standard) evaluation measures (and leaving the details, including discussion of censoring, for Appendix B) then providing a more thorough motivation and description of the fifth, D-Calibration. The next section shows how the 6 distribution-learning models perform with respect to these evaluations.\nFor notation, we will assume models were trained on a training dataset, formed from the same triples as shown in Equation 1, that is D = DU \u222aDC where DU = { [~xj, dj, \u03b4j = 1] }j\n11 Another candidate is the mean value of the distribution, which corresponds to the area under the survival curve; see Theorem B.1.\nis the set of uncensored instances (notice the event time, tj, here is written as dj), and DC = { [~xk, ck, \u03b4k = 0] }k is the set of censored instances (tk, here is written as ck). Note also that this training dataset D is disjoint from the validation dataset, V . Since models are evaluated on V and we save discussion of censoring for Appendix B, we assume here that all of V is uncensored \u2013 i.e., V = VU = { [~xj, dj, \u03b4j = 1] }j \u2248 { [~xj, dj] }j (to simplify notation)."
                },
                {
                    "heading": "3.1 Concordance",
                    "text": "As noted above, each individual risk model [R,1\u00b7,-] (i.e., [R,1\u00b7,i] or [R,1\u00b7,g], where 1\u00b7 can be either 1t\u2217 or 1\u2200) assigns to each individual ~x, a \u201crisk score\u201d r(~x) \u2208 <, where r(~xa) > r(~xb) means the model is predicting that ~xa will die before ~xb. Concordance (a.k.a. C-statistic, C-index) is commonly used to validate such risk models. Specifically, Concordance considers each pair of patients, and asks whether the predictor\u2019s values for those patients matches what actually happened to them. In particular, if the model gives ~xa a higher score than ~xb, then the model gets 1 point if ~xa dies before ~xb. If instead ~xb died before ~xa, the model gets 0 points for this pair. Concordance computes this for all pairs of comparable patients, and returns the average.\nWhen considering only uncensored patients, every pair is comparable, which means there are ( n 2 ) = n\u00b7(n\u22121) 2 pairs from n = |VU | elements. Given these comparable pairs, Concordance is calculated as,\nC \u2227 (VU , r(\u00b7) ) = 1\n|VU | \u00b7 (|VU |\u22121) 2\n\u00b7 \u2211\n[~xi,di]\u2208VU \u2211 [~xj ,dj ]\u2208VU : di<dj I [ r(~xi) > r(~xj) ] . (3)\nAs an example, consider the table of death times di and risk scores, for 5 patients, shown in Table 1[left]. Table 1[right] shows that these risk scores are correct in 7 of the ( 5 2 ) = 10 pairs, so the Concordance here is 7/10 = 0.7. This Concordance measure is very similar to the area under the receiver operating curve (AUC) and equivalent when di is constrained to values {0, 1} [33]. This Concordance measure is relevant when the goal is to rank or discriminate between patients \u2013 e.g., when one wants to know who will live longer between a pair of patients. (For\nexample, if we want to transplant an available liver to the patient who will die first \u2013 this corresponds to \u201curgency\u201d.) Concordance is the desired metric here due to it\u2019s interpretation, i.e. given two randomly selected patients, ~xa and ~xb, if a model with Concordance of 0.9 assigns a higher risk score to ~xa than ~xb, then there is a 90% chance that ~xa will die before ~xb.\nWhile [R,1\u2200,i] models (such as cox) provide a risk score that is independent of time, there are also [R,\u221e,i] models that produces a risk score r(~x, t) for an instance ~x that depends on time t; such as Aalen\u2019s additive regression model [1] or time-dependent Cox (td-Cox) [14], which uses time-dependent features. These models can be evaluated using time-dependent Concordance (aka, \u201ctime-dependent ROC curve analysis\u201d) [24].\nFinally, the [R,\u2212,g] systems compute a risk score, but then bin these scores into a small set of intervals. When computing Concordance, they then only consider patients in different bins. For example, if Bin1 = [0, 10] and Bin2 = [11, 20], then this evaluation would only consider pairs of patients (~xa, ~xb) where one is in Bin1 and the other is in Bin2 \u2013 e.g., r(~xa) \u2208 [0, 10] and r(~xb) \u2208 [11, 20]. (Hence, it will not consider the pair (~xc, ~xd) if both r(~xc), r(~xd) \u2208 [11, 20].)\nSee Appendix B.1 for more details, including a discussion of how this measure deals with censored instances and tied risk scores/death times."
                },
                {
                    "heading": "3.2 L1-loss",
                    "text": "Survival prediction is very similar to regression: given a description of a patient, predict a real number (his/her time of death). With this similarity in mind, one can evaluate a survival model using the techniques used to evaluate regression tasks, such as L1-loss \u2013 the average absolute value of the difference between the true time of death, di, and the predicted time d \u2227\ni: 1 n \u2211 i |di\u2212d \u2227\ni|. (We consider the L1-loss, rather than L2-loss which squares the differences, as the distribution of survival times is often right skewed, and L1-loss is less swayed by outliers than L2-loss.)\nOne challenge in applying this measure to our [P,\u221e,-] models is identifying the predicted time, d \u2227 i. Here, we will use the predicted median survival time, that is d \u2227 i = t\u0302 (0.5) i , leading to the following measure:\nL1( VU , { S\u0302( \u00b7 | ~xi ) }i ) = 1 |VU | \u2211\n[~xi,di]\u2208VU \u2223\u2223\u2223di \u2212 t\u0302(0.5)i \u2223\u2223\u2223 . (4) While we would like this value to be small, we should not expect it to be 0: if the distribution is meaningful, there should be a non-zero chance of dying at other times as well. For example, while the L1-loss is 0 for the Heaviside distribution at the time of death (shown in green in Figure 5), this is unrealistic.\nAppendix B.2 discusses many issues with the L1-loss measure, related to censored data, and reasons to consider using the log of survival time."
                },
                {
                    "heading": "3.3 1-Calibration",
                    "text": "The [P,1t\u2217 ,i] tools estimate the survival probability S\u0302( t \u2217 | ~x ) \u2208 [0, 1] for each instance ~x, at a single time point t\u2217. For example, the PredictDepression system [50] predicts the chance that a patient will have a major depression episode within the next 4 years, based on their current characteristics \u2013 i.e., this tool produces a single probability value S\u0302( 4yr | ~xi ) \u2208 [0, 1] for each patient described as ~xi. We can use 1-Calibration to measure the effectiveness of such predictors. To help explain this measure, consider the \u201cweatherman task\u201d of predicting, on day t, whether it will rain on day t+ 1. Given the uncertainty, forecasters provide probabilities. Imagine, for example, there were 10 times that the weatherman, Mr.W, predicted that there was a 30% chance that it would rain tomorrow. Here, if Mr.W was calibrated, we expect that it would rain 3 of these 10 times \u2013 i.e., 30%. Similarly, of the 20 times Mr.W claims that there is an 80% chance of rain tomorrow, we expect rain to occur 16 = 20 \u00d7 0.8 of the 20 times.\nHere, we have described a binary probabilistic prediction problem \u2013 i.e., predicting the chance that it will rain the next day. One of the most common calibration measures for such binary prediction problems is the Hosmer-Lemeshow goodness-of-fit test [25]. First, we sort the predicted probabilities for this time t\u2217 for all patients { S\u0302( t\u2217 | ~xi ) }i and group them into a number (B) of \u201cbins\u201d; commonly into deciles, i.e., B = 10 bins. Suppose there are 200 patients; the first bin would include the 20 patients with the largest S\u0302( t\u2217 | ~xi ) values, the second bin would contain the patients with the next highest set of values, and so on, for all 10 bins. Next, within each bin, we calculate the expected number of events, p\u0304j = 1 |Bj | \u2211 ~xi\u2208Bj(1 \u2212 S\u0302( t \u2217 | ~xi )). We also let nj = |Bj| be the size of the jth bin (here, n1 = n2 = \u00b7 \u00b7 \u00b7 = n10 = 200/10 = 20), and Oj be the number of patients (in the jth bin) who died before t\u2217. Recalling that di denotes Patient #i\u2019s time of death and letting oi = I [ di \u2264 t\u2217 ] denote the event status of the ith patient at t\u2217: for the jth bin, Bj, we have Oj = \u2211 ~xi\u2208Bj oi. Figure 6 graphs the 10 values of observed Oj and expected nj p\u0304j for the deciles, for two different tests (corresponding to two different isd-models, on the same dataset and t\u2217 time). Additionally, see Appendix B.3 for an example walking through 1-Calibration.\nFor each test, we can then compute the Hosmer-Lemeshow test statistic:\nHL \u2227 (VU , S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 (Oj \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) , (5)\nIf the model is 1-Calibrated, then this statistic follows a \u03c72B\u22122 distribution, which then can be used to find a p-value. For a given time t\u2217, finding p < 0.05 suggests the survival model is not well calibrated at t\u2217 \u2013 i.e., the predicted probabilities of survival at t\u2217 may not be representative of patient\u2019s true survival probability at t\u2217.\nReturning to Figure 6, the HL statistics are 5.99 and 321.44, for the left and right, leading to the p-values p =0.741 and p < 0.001 \u2013 meaning the left one passes but the right one does not. (This is not surprising, given that each pair of bars on the left are roughly the same height, while the pairs of the right are not.)\nNote that a [P,\u221e,i] model, which gives probabilities for multiple time points, may be calibrated at one time t1, but not be calibrated at another time t2, since Oj, and p\u0304j are dependent on the chosen time point. This issue motivated us to define a notion of calibration across a distribution of time points, D-Calibration, in Section 3.5. Appendix B.3 provides further details about 1-Calibration including ways to handle censored patients."
                },
                {
                    "heading": "3.4 Brier Score",
                    "text": "We often want a model to be both discriminative (high Concordance) and calibrated (passes the 1-Calibration test). While one can rank Concordance scores to compare two models\u2019\ndiscriminative abilities, 1-Calibration cannot rank models besides suggesting one model is calibrated (p \u2265 0.05) and one is not (p < 0.05) (as p-values are not intended to be ranked). The Brier score [6] is a commonly used metric that measures both calibration and discrimination; see Appendix B.4.1. Mathematically, the Brier score is the mean squared error between the {0, 1} event status at time t\u2217 and the predicted survival probability at t\u2217. Given a fully uncensored validation set VU , the Brier score, at time t \u2217, is\nBSt\u2217 ( VU , S\u0302( t \u2217 | \u00b7 ) )\n= 1 |VU | \u2211\n[~xi,di]\u2208VU\n( I [ di \u2264 t\u2217 ] \u2212 S\u0302( t\u2217 | ~xi ) )2 . (6)\nHere, a perfect model (that only predicts 1s and 0s as survival probabilities and is correct in every case) will get the perfect score of 0, whereas a reference model that gives S\u0302( t\u2217 | \u00b7 ) = 0.5 for all patients will get a score of 0.25.\nAn extension of the Brier score to an interval of time points is the Integrated Brier score, which will give an average Brier score across a time interval,\nIBS( \u03c4, VU , S\u0302( \u00b7 | \u00b7 ) ) = 1\n\u03c4 \u222b \u03c4 0 BSt ( VU , S\u0302( t | \u00b7 ) ) dt . (7)\nWe will use this measure for our analysis, where \u03c4 is the maximum event time of the combined training and validation datasets \u2013 this way, the interval evaluated is equivalent across crossvalidation folds.\nAs noted above, the Brier score measures both calibration and discrimination, implying it should be used when seeking a model that must perform well on both calibration and discrimination, or when one is investigating the overall performance of survival models. Appendix B.4 shows how to incorporate censoring into the Brier score, and discusses the decomposition of the Brier score into calibration and discriminative components."
                },
                {
                    "heading": "3.5 D-Calibration",
                    "text": "The previous sections summarized several common ways to evaluate standard survival prediction models, that produce only a single value for each patient \u2013 e.g., the patient\u2019s risk score, perhaps with respect to a single time, or the mean survival time. (Each is a [-,1\u00b7,-] model.) However, the [P,\u221e,-] tools produce a distribution \u2013 i.e., each is a function that maps [0,\u221e] to [0, 1] (with some constraints of course), such as the ones shown in Figure 4; see Footnote 1. It would be useful to have a measure that examines the entire distribution as a distribution.12\nA distributional calibration (D-Calibration) [3] measure addresses the critical question:\nShould the patient believe the predictions implied by the survival curve? (8)\nFirst, consider population-based models [P,\u221e,g], like Kaplan-Meier curves \u2013 e.g., Figure 1[left], for patients with stage-4 stomach cancer. If a patient has stage-4 stomach cancer, should\n12While the Integrated Brier score does consider all the points across the distribution, it simply views that distribution as a set of (x, y) points; see Appendix B.4.2 for further explanation.\ns/he believe that his/her median survival time is 11 months, and that s/he has a 75% chance of surviving more than 4 months? To test this, we could take 1000 new patients (with stage-4 stomach cancer) and ask whether \u2248500 of these patients lived at least 11 months, and if \u2248750 lived more than 4 months.\nFor notation, given a dataset, D, and [P,\u221e,g]-model \u0398, and any interval [a, b] \u2282 [0, 1], let\nD\u0398( [a, b] ) = { [~xi, di, \u03b4 = 1] \u2208 D | S\u0302\u0398( di ) \u2208 [a, b] } (9)\nbe the subset of (uncensored) patients in D whose time of death is assigned a probability (by \u0398) in the interval [a, b]. For example, D\u0398( [0.5, 1.0] ) is the subset of patients who lived at least the median survival time (using S\u0302\u0398( \u00b7 )\u2019s median), and D\u0398( [0.25, 1.0] ) is the subset who died after the 25th percentile of S\u0302\u0398( \u00b7 ). By the argument above, we expect D\u0398( [0, 0.5] ) to contain about 1/2 of D, and D\u0398( [0.25, 1.0] ) to contain about 3/4 of D. Indeed, for any interval [a, 1.0], we expect\n|D\u0398( [a, 1.0] )| |D|\n= 1\u2212 a (10)\nor in general |D\u0398( [a, b] )| |D|\n= b\u2212 a (11)\nThis leads to the idea of a survival distribution [P,\u221e,g] model, \u0398, being D-Calibrated: For each uncensored patient ~xi, we can observe when s/he died di, and also determine the percentile for that time, based on \u0398: S\u0302\u0398( di ). If \u0398 is D-Calibrated, we expect roughly 10% of the patients to die in the [90%, 100%] interval \u2013 i.e., |D\u0398( [0.9, 1.0] )||D| \u2248 1 \u2212 0.9 = 0.1 \u2013 and another 10% to die in the [80%, 90%) interval, and so forth for each of the 10 different 10%-intervals. More precisely, the set { S\u0302i( di ) } over all of the patients should be distributed uniformly on [0, 1], which means that each of the 10 bins would contain 10% of D.\nThis suggests a measure to evaluate a distributional model: see how close each of these 10 bins is to the expected 10%. We therefore use Pearson\u2019s \u03c72 test: compute the \u03c72-statistic with respect to the ten 10% intervals, and ask whether the bins appear uniform, at (say) the p > 0.05 level. Theorem B.2 (in Appendix B.5) states and proves the appropriateness of the Pearson\u2019s \u03c72 goodness-of-fit test.\nThis addresses the question posed at the start of this subsection (Equation 8):\nYes, a patient should believe the prediction from the survival curve\nwhenever this goodness-of-fit test reports p > 0.05.\n3.5.1 Dealing with Individual Survival Distributions, isd\nEverything above was for a population-based distributional model [P,\u221e,g]. These specific results do not apply to individual survival distributions [P,\u221e,i]: For example, consider a single patient, Patient #1, whose curve is shown in Figure 1[middle]. Should he believe this plot, which implies that his median survival time is 18 months, and that he has a 75% chance of surviving more than 13 months?\nIf we could observe 1000 patients exactly identical to this Patient #1, we could verify this claim by seeing their actual survival times: this survival curve is meaningful if its predictions matched the outcomes of those copies \u2013 e.g., if around 250 died in the first 13 months, another \u2248250 in months 13 to 18, etc.\nUnfortunately, however, we do not have 1000 \u201ccopies\u201d of Patient #1. But here we do have many other patients, each with his/her own characteristic survival curve, including the 4 curves shown in Figure 7. Notice each patient has his/her own distribution, and hence his/her own quartiles \u2013 e.g., the predicted median survival times for Patient A (resp., B, C and D), are 28.6 (resp., 65.7, 11.4, and 13.9) months; see Table 2. For these historical patients, we know the actual event time for each.13 Here, if our predictor is working correctly, we would expect that 2 of these 4 would pass away before respective median times, and the other 2 after their median times. Indeed, we would actually expect 1 to die in each of the 4 quartiles; the blue vertical lines (the actual times of death) show that, in fact, this does happen. See also Table 2.\nWith a slight extension to the earlier notation (Equation 9), for a dataset D and [P,\u221e,i]model \u0398, and any interval [a, b] \u2282 [0, 1], let\nD\u0398( [a, b] ) = { [~xi, di, \u03b4 = 1] \u2208 D | S\u0302\u0398( di | ~xi ) \u2208 [a, b] } (12)\nbe the subset of (uncensored) patients in the dataset D whose time of death is assigned a probability (based on its individual distribution, computed by \u0398) in the interval [a, b].\nAs above, we could put these S\u0302\u0398( di | ~xi ) into \u201c10%-bins\u201d, and then ask if each bin holds about 10% of the patients. The right-side of Figure 8 plots that information, for the isd \u0398\n13 Here we just consider uncensored patients; Appendix B.5 extends this to deal with censoring.\nlearned by mtlr from the NACD dataset (described in Section 4.1), as a sideways histogram. We see that each of these intervals is very close to 10%. In fact, the \u03c72 goodness-of-fit test yields p = 0.433, which suggests that this isd is sufficiently uniform that we can believe that these survival curves are D-calibrated.\nNote that Figure 8 is actually showing 5-fold cross-validation results: the survival curve for each patient was computed based on the model learned from the other 4/5 of the data, which is then applied to this patient [53]. Also, the rust-colored intervals correspond to the censored patients; see Appendix B.5 for an explanation."
                },
                {
                    "heading": "3.5.2 Relating D-Calibration to 1-Calibration",
                    "text": "This standard notion of 1-Calibration is very similar to D-Calibration, as both involve binning probability values and applying a goodness-of-fit test. However, 1-Calibration involves a single prediction time \u2013 here S\u0302( t\u2217 | ~xi ), which is the probability that the patient ~xi will survive at least to the specified time, t\u2217. Patients are then sorted by these probabilities, partitioned into equal-size bins, and assessed as to whether the observed survival rates for each bin match the predicted rates using a Hosmer-Lemeshow test. By contrast, D-Calibration considers the entire curve, S\u0302( t | ~xi ) over all times t \u2013 producing curves like the ones shown in Figures 1, 4, and 7. Each curve corresponds to a patient, who has an associated time of death, di. Here, we are considering the model\u2019s (estimated) probability of the patient\u2019s survival at his/her time of death, given by S\u0302i( di | ~xi ). These patients are then placed into B = 10 bins,14 based on the values of their associated probabilities, S\u0302i( di | ~xi ). Here the\n14Note the number of bins does not have to be 10 \u2013 we chose 10 to match the typical value chosen for the 1-Calibration test.\ngoodness-of-fit test measures whether the resulting bins are approximately equal-sized, as would be expected if \u0398 accurately estimated the true survival curves (argued further in Appendix B.5).\nNote D-Calibration tests the proportion of instances in bins across the entire [0, 1] interval, but this is not required for the \u201csingle probability\u201d 1-Calibration. For example, the single probability estimates for the rsf-km curve in Figure 3, at time 20, range only from 0.05 to 0.62. That is, the distribution calibration { S\u0302i( di | ~xi ) } should match the uniform distribution over [0,1], while the single probability calibration { S\u0302i( t\u2217 | ~xi ) } is instead expected to match the empirical percentage of deaths.\nTable 3 summarizes the differences between D-Calibration and 1-Calibration.15 To see that they are different, Proposition B.3, in Appendix B.5, gives a simple example of a model that is perfectly D-Calibrated but clearly not 1-Calibrated, and another example that is perfectly 1-Calibrated but clearly not D-Calibrated. In addition, we will see below several examples of this \u2013 e.g., coxen-kp is D-Calibrated for the GLI dataset, but it is not 1- Calibrated at any of the 5 time points considered, and aft is 1-Calibrated for the 50th and 75th percentiles of GBM but is not D-Calibrated.\n4 Evaluating isd Models\nSections 2.4 and 2.5 listed several distributional models (km, and the isds: cox-kp, coxenkp, aft, mtlr, and rsf-km), and Section 3 provided 5 different evaluation measures: Concordance, L1-loss, 1-Calibration, Integrated Brier score, and D-Calibration. This section provides an empirical comparison of these 6 models, with respect to all 5 of these measures, over 8 datasets.\nOf course, these 6 models do not include all possible survival models; they instead serve as a sample of the types of models available. The km, cox-kp, and aft model are all very common \u2013 these are standard approaches used throughout survival analysis and represent non-parametric, semi-parametric, and parametric models, respectively. As our preliminary studies with cox-kp suggested it was overfitting, we also included a regularized extension, using elastic net, coxen-kp. Since Random Survival Forests (rsf) were introduced in 2008, they have had a large impact on the survival analysis community. However, as the\n15Further differences occur when considering how censored patients are handled; see Appendices B.3 and B.5.\nKaplan-Meier extension to transform rsf into an isd is not well known, it is summarized in Appendix C.3. More recent still is the mtlr technique [57] that directly learns a survival distribution, by essentially learning the associated probability mass function (whose sequential right-to-left sum, when smoothed, is the survival distribution). We found some subsequent similar models, including \u201cMulti-Task Learning for Survival Analysis\u201d (MTLSA) [33], some deep learning variants [39, 32, 34], and a computationally demanding Bayesian regression trees model [44], but for brevity, we focused on just the first such model, mtlr.\nNote the distribution class D chosen for aft certainly influences its performance \u2013 e.g., it is possible that aft[Weibull] on a dataset may fail D-Calibration whereas aft[Log-Logistic] may pass; similarly for 1-Calibration at some time t\u2217, and the scores for Concordance, L1loss and Integrated Brier score will depend on that distribution class. This paper will focus on aft[Weibull] because, while still being parametric, the Weibull distribution is versatile enough to fit many datasets."
                },
                {
                    "heading": "4.1 Datasets and Evaluation Methodology",
                    "text": "There are many different survival datasets; here, we selected 8 publicly available medical datasets in order to cover a wide range of sample sizes, number of features, and proportions of censored patients. We excluded small datasets (with fewer than 150 instances) to reduce the variance in the evaluation metrics. Our datasets ranged from 170 to 2402 patients, from 12 to 7401 features, and percentage of censoring from 17.23% to 86.21%; see Table 4. Note that we have not included extremely high-dimensional data (with tens of thousands of features, often found in genomic datasets), as such data raises additional challenges beyond the scope of standard survival analysis; see [52] for methods to handle extremely high-dimensional data.\nThe Northern Alberta Cancer Dataset (NACD), with 2402 patients and 53 features, is a conglomerate of many different cancer patients, including lung, colorectal, head and neck, esophagus, stomach, and other cancers. In addition to using the complete NACD dataset, we considered the subset of 950 patients with colorectal cancer (Nacd-Col), with the same 53 features.\nAnother four datasets were retrieved from data generated by The Cancer Genome Atlas (TCGA) Research Network [15]: Glioblastoma multiforme (GBM; 592 patients, 12 features), Glioma (GLI; 1105 patients, 13 features), Rectum adenocarcinoma (READ; 170 patients, 18 features), and Breast invasive carcinoma (BRCA; 1095 patients, 61 features). To ensure a variety of feature/sample-size ratios, we consider only the clinical features in our experiments.\nLastly, we included two high-dimensional datasets: the Dutch Breast Cancer Dataset (DBCD) [49] contains 4919 microarray gene expression levels for 295 women with breast cancer, and the Diffuse Large B-Cell Lymphoma (DLBCL) [33] dataset contains 7401 features focusing on Lymphochip DNA microarrays for 240 biopsy samples.\nWe applied the following pre-processing steps to each dataset: We first removed any feature that was missing over 25% of its values, as well as any features containing only 1 unique value. For the remaining features, we \u201cone-hot encoded\u201d each nominal feature and then passed each feature to a univariate Cox filter, and removed any feature that was not\nsignificant at the p \u2264 0.10 level. Following feature selection, we replaced any missing value with the respective feature\u2019s mean value. (Note this feature selection was found to benefit all isd models across all performance metrics; data not shown.) Table 4 provides the dataset statistics and a full breakdown of feature numbers in each step.\nFollowing feature selection, features were normalized (transformed to zero mean with unit variance) and passed to models for five-fold cross validation (5CV). We compute the folds by sorting the instances by time and censorship, then placing each censored (resp., uncensored) instance sequentially into the folds \u2013 meaning all folds had roughly the same distribution of times, and censorships.\nFor coxen-kp, rsf-km, and mtlr we used an internal 5CV for hyper-parameter selection. There were no hyper-parameters to tune for the remaining models: cox, km, and aft.\nAs 1-Calibration required specific time points, and as models might perform well on some survival times but poorly on others, we chose five times to assess the calibration results of each model: the 10th, 25th, 50th, 75th, and 90th percentiles of survival times for each dataset. Here, we used the D\u2019Agostino-Nam translation to include censored patients for these evaluation results \u2013 see Appendix B.3. Appendix D.4 presents all 240 values (6 models \u00d7 8 datasets \u00d7 5 time-points); here we instead summarize the number of datasets that each model passed as 1-Calibrated (at p \u22650.05) for each percentile.\nFor all evaluations, we report the averaged 5CV results for Concordance, Integrated Brier score, and L1-loss. As Concordance requires a risk score, we use the negative of the median survival time and similarly use the median survival time for predictions for the L1-loss. To adjust for presence of censored data, we used the L1-Margin loss, given in Appendix B.2, which extends the \u201cUncensored L1-loss\u201d given in Section 3.2 (which considers only uncensored patients). Additionally, as 1-Calibration (resp., D-Calibration) results are reported as p-values, and it is not appropriate to average over the folds, we combined the predicted survival curves from all cross-validation folds for a single evaluation, and report the resulting p-value.\nEmpirical evaluations were completed in R version 3.4.4. The implementations of km, aft, and cox-kp can all be found in the survival package [47] whereas coxen-kp uses the\ncocktail function found in the fastcox package [56]. Both rsf and rsf-km come from the randomForestSRC package [27]. An implementation of mtlr (and of all the code used in this analysis) is publicly available on the GitHub account16 of the lead author."
                },
                {
                    "heading": "4.2 Empirical Results",
                    "text": "Below, we consider a dataset to be \u201cNice\u201d if its feature-to-sample-size ratio was less than 0.05 (for the final feature set) and its censoring was less than 55%; this includes four of the 8 datasets: GBM, Nacd-Col, GLI, NACD \u2013 which are shown first in all of our empirical studies. We let \u201cHigh-Censor\u201d datasets refer to READ and BRCA and \u201cHigh-Dimensional\u201d datasets refer to the other two (DLBCL and DBCD)."
                },
                {
                    "heading": "4.2.1 Concordance, Integrated Brier score, and L1-loss Results",
                    "text": "Figures 9, 10 and 11 give the empirical results for Concordance, Integrated Brier score, and L1-Margin loss respectively, where each circle is the score of the associated model on the dataset, and lines correspond to one standard deviation (over the 5 cross-validation folds). Appendix D provides the exact empirical results for these measures.\nBest Performance: The blue circles represent the best performing models, for each dataset; here we find that mtlr performs best on a majority of datasets: six of eight for Concordance and L1-loss, and seven of eight for the Integrated Brier score.\nNice Datasets: Recall that the first 4 datasets are Nice. Here, we find that most models performed comparably \u2013 and in particular, aft and cox-kp perform nearly as well as the other, more complex, models. aft even performs best in terms of L1-loss on GBM. The only exception was rsf-km, which did much worse on GBM and GLI, in all three measures.\nkm was worse than the various isd-models for all 3 measures. (The only exception was rsf-km, which was worse on for the datasets GLI and GBM for Integrated Brier score, and for those datasets and also Nacd-Col for L1-loss.)\nHigh-Censor Datasets \u2013 READ and BRCA: Note first that the variance in the evaluation metrics is generally higher on READ for all models (except km) due to the small number of uncensored patients within each test fold \u2013 this is not present in BRCA due to the larger sample size (1095). Again we find that coxen-kpand mtlr are similar for all three measures, but rsf-km performs consistently worse across all three metrics for both READ and BRCA. aft and cox-kp are either comparable (or inferior) to the other three isd-models: Concordance: worse performance but within error-bars for READ and BRCA; Integrated Brier score: similar for both READ and BRCA; L1-loss: slightly worse for READ and BRCA. Additionally, aft and cox-kp tend to show higher variance in evaluation estimates on READ than other models for all three measures.\nkm is worse than all 5 isd-models for Concordance, but comparable to the best for Integrated Brier score and L1-loss (actually scoring better than cox-kp and aft for L1-loss on READ and beating cox-kp on BRCA).\n16https://github.com/haiderstats/ISDEvaluation\nHigh-Dimensional Datasets \u2013 DBCD and DLBCL: There are no entries for cox-kp for these two datasets as it failed to run on them, likely due to the large number of features. As aft is unregularized, it is not surprising that it does poorly across all measures for these high-dimensional datasets \u2013 indeed, even worse than km, which did not use any features! We see that the other three isd-models \u2013 coxen-kp, mtlr and rsf-km \u2013 perform similarly to one another here, and km also achieves similar results (ignoring Concordance where km always achieves 0.5, as it gives identical predictions for all patients)."
                },
                {
                    "heading": "4.2.2 1-Calibration Results",
                    "text": "Table 5 gives the number of datasets each model passed for 1-Calibration for each time of interest. We see that mtlr is typically 1-Calibrated across the percentiles of survival times. Specifically, mtlr is 1-Calibrated for at a minimum of four of eight datasets for the 10th, 25th, 50th, and 90th percentiles, outperforming all other models considered. The 90th percentile appear to be the most challenging in general, as some models (aft, cox-kp, rsfkm) are not 1-Calibrated for any datasets, coxen-kp is 1-Calibrated for two, and mtlr is 1-Calibrated for four. The 75th percentile also showed to be challenging, however aft, coxkp, and rsf-km were 1-Calibrated for one, coxen-kp is 1-Calibrated for two, and mtlr is 1-Calibrated for three. The most challenging datasets for rsf-km once again were GBM, GLI, BRCA, and READ, for which rsf-km was 1-Calibrated only at the 10th percentile for READ \u2013 see Appendix D.4. Additional challenging datasets include the complete NACD and DBCD which were challenging for all models. As km assigns an identical prediction for all patients, it cannot partition patients into different bins, meaning it cannot be evaluated by 1-Calibration."
                },
                {
                    "heading": "4.2.3 D-Calibration Results",
                    "text": "Table 6, which gives the D-Calibration p-values for each model and dataset, shows that both km and mtlr pass D-Calibration for every dataset, with km receiving the highest possible p-value, p =1.000, for each. (In fact, Lemma 2 in Appendix B.5 proves that km is asymptotically D-Calibrated). While km will tend to be D-Calibrated, it is also the least informative model, since it assigns all patients the same survival curve. mtlr is also D-\nCalibrated for all datasets, but in addition, it also provides each patients with his/her own survival curve.\nFollowing km and mtlr, coxen-kp performed next best, only failing to be D-Calibrated for one dataset: NACD. rsf-km followed closely behind, being D-Calibrated for five of eight datasets, failing on GBM, GLI, and NACD. aft performed similarly to cox-kp, each of which being D-Calibrated on three of eight datasets.\nFigure 12 provides (sideways) histograms, to help visualize D-calibration. For each subfigure, each of the 10 horizontal bars should be 10%; we see a great deal of variance for the not-D-Calibrated cox-kp [left], a small (but acceptable) variability for the D-Calibrated mtlr [middle], and essentially perfect alignment for the D-Calibrated km [right]. See also Figure 8."
                },
                {
                    "heading": "5 Discussion",
                    "text": "Comparing different isd-models: Steyerberg et al. [46] noted two different types of performance measures of a survival analysis model \u2013 calibration and discrimination \u2013 each of which can be assessed separately:\nCalibration: \u201cOf 100 patients with a risk prediction of x%, do close to x experience the event?\u201d\nDiscrimination: \u201cDo patients with higher risk predictions experience the event sooner than those who have lower risk predictions?\u201d\nDiscrimination is a very important measure for some situations \u2013 e.g., if we have 2 patients who each need a kidney transplant, but there is only a single kidney donor, then we want to know which patient will die faster without the transplant [30]. As discussed in Section 3.1, Concordance measures how well a predictor does, in terms of this discrimination task.\nThis paper, however, motivates and studies models that produce an individual survival curve for a specific patient. Such isd tools may not be optimal for maximizing discrimination (and therefore Concordance); and even tools like cox and rsf, that were originally developed for discrimination, were then extended to produce these individual survival curves. Given this qualifier, we see (over the set of isd tools tested), mtlr scored best on Concordance for six of the eight datasets tested and rsf-km scored the best on the other two. (The relatively low performance of cox-kp is unexpected given the claim that \u201ca method designed to maximize the Cox\u2019s partial likelihood also ends up (approximately) maximizing the [concordance]\u201d [45].) However, when we look at the Nice datasets, 4 of the 5 isdmodels give nearly identical results (rsf-km differs by giving noticeably lower performance on GBM and GLI). These findings suggest that, for Nice datasets, more complex models (mtlr, rsf-km, and coxen-kp) do not offer large benefits in terms of Concordance. For the High-Dimensional datasets, mtlr and coxen-kp performed only marginally better than rsf-km for DBCD but noticeably better than rsf-km on DLBCL. Although these are only two datasets, this suggests that rsf-km may not be optimal for these high-dimensional datasets, in terms of Concordance. For the High-Censor datasets rsf-km saw much worse performance for Concordance (among other metrics) suggesting rsf-km may not be suitable for datasets with a high proportion of censored data.\nAs noted above, Concordance is only one measure for an isd tool. Given that an isd tool can produce a survival curve for each patient (and not just a single real-valued score), it can be used for various tasks, with various associated evaluations. For example, consider patients who are deciding whether to undergo an intensive medical procedure. Using the plots from Figure 7, note that Patient C has a very steep survival curve with a low median survival time, while Patient A has a shallow survival curve with a large median survival time. If we were to use this to predict the outcome of a procedure, we might expect Patient C to opt-out of the procedure, but Patient A to go through with it. Note the decision for Patient C is completely independent of Patient A, in that we could give the procedure to one, both, or neither of them. As these patients are not being ranked for a limited procedure, Concordance is not\nan appropriate metric and instead we need to evaluate such predictors using a calibration score \u2013 perhaps 1-Calibration or D-Calibration, as discussed in Sections 3.3 and 3.5.\nAs discussed in Section 3.3, 1-Calibration is particularly relevant for [P,1t\u2217 ,i] models \u2013 i.e., models that produce a probability score for only 1 time point (for each patient). We also noted that isd models, that produce individual survival curves, can also be evaluated using 1-Calibration, once the evaluator has identified the relevant specific time t\u2217. Here, we evaluated a variety of time points: the 10th, 25th, 50th, 75th and 90th percentiles of survival times for each dataset. We found mtlr to be superior to all the models considered here for all percentiles. The observation that mtlr was 1-Calibrated for a range of time points, across a large number of diverse datasets, suggests that the probabilities assigned by mtlr\u2019s survival curves are representative of the patients\u2019 true survival probabilities; the observation that the other models were not 1-Calibrated as often, calls into question their effectiveness here.\nOf course, our analysis is performing the 1-Calibration test for 5 models (km is excluded) across 8 datasets and 5 percentiles, meaning we are performing 200 statistical tests. We considered applying some p-value corrections \u2013 e.g., the Bonferroni correction \u2013 to reduce the chance of \u201cfalse-positives\u201d, which here would mean declaring a model that was truly calibrated, as not. However, the actual p-values (see Appendix D.4) show that including these corrections would actually benefit mtlr the most, further strengthening the claim that mtlr has excellent 1-Calibration performance.\nOur D-Calibration results further support the use of mtlr\u2019s individual survival curves over other isd-models, by showing that mtlr was the only isd-model to be D-Calibrated for all datasets. (Recall that km is technically not an isd since it gives one curve for all patients.) We see that different isd-models are quite different for this measure \u2013 e.g., aft and cox-kp produce significantly worse performance for D-Calibration, being D-Calibrated for only three datasets. As discussed in Section 4.2, aft is a completely parametric model, which means it cannot produce different shapes (see Figure 4[top-right]), likely impacting its ability to be D-Calibrated. (Our analysis showed only that aft[Weibull] is here not D-Calibrated; aft[\u03c7] for some other distribution class \u03c7, might be D-Calibrated for more datasets.)\nIn addition to discussing discrimination (Concordance) and calibration (1-Calibration, DCalibration) separately, we can also consider a hybrid evaluation metric \u2013 the Integrated Brier score \u2013 which measures a combination of both calibration and discrimination \u2013 see Section 3.4 and Appendix B.4. We see mtlr performing the best for seven of the eight datasets, however, mtlr is no longer superior for DBCD, one of the high-dimensional datasets, even though it was superior for Concordance. Instead, coxen-kp, rsf-km, and mtlr all perform nearly identical for these High-Dimensional datasets.\nThe Integrated Brier scores, along with 1-Calibration and D-Calibration results, collectively show mtlr outperforms other models (for calibration), and is followed by coxenkp and rsf-km. Specifically, coxen-kp and rsf-km are competitive to mtlr for HighDimensional datasets \u2013 the 1-Calibration metric shows that both coxen-kp and rsf-km match the performance of mtlr for DLBCL (coxen-kp and mtlr are 1-Calibrated across\nall percentiles and rsf-km is 1-Calibrated across three of five, though p-values are very close to the 0.05 threshold for the other two). DBCD appeared to be the more challenging HighDimensional dataset \u2013 mtlr and coxen-kp were 1-Calibrated for two of five percentiles and rsf-km was 1-Calibrated for one. This, coupled with the findings for Integrated Brier Score and D-Calibration, suggest that coxen-kp, rsf-km and mtlr are equally competitive for modeling individual patients\u2019 survival probabilities when dealing with a large number of features. However, this does not apply to smaller-dimensional datasets.\nrsf-km was not 1-Calibrated across any percentiles for GBM, GLI, BRCA, and only 1-Calibrated at the 10th percentile for READ, and was not D-Calibrated for GBM and GLI. This, along with the poor performance of rsf-km for all measures of GBM, GLI, READ, and BRCA suggests that rsf-km does not produce effective individual survival curves for low-dimensional datasets. Other experiments (not shown) suggest that rsf-km tends to overfit to the training set when given too few features. Additional meta-parameter tuning in these experiments was unable to correct for overfitting.\nGiven that survival prediction looks very similar to regression, it is tempting to evaluate such models using measures like L1-loss (which can lead to models like censored support vector regression [43]). A small L1-loss shows that a model can help with many important tasks, such as decisions about hospice, and for deciding about various treatments, based on their predicted survival times. However, simply because a model has the best performance for L1-loss does not mean the estimates are useful \u2013 consider the complete NACD dataset where mtlr has the best performance with an average L1-loss of 43.97 months. While this is the lowest average error, predicting the time of death up to an error of 43.97 months (\u22483.7 years) is likely not helpful to a patient, especially as the maximum follow-up time was 84.3 months.\nWhile the best model may not represent a \u201cgood\u201d model, our empirical results still showed mtlr had the lowest L1-loss on six of eight datasets, although all isd models performed comparably for the four Nice datasets (with the exception of rsf-km). We see that km is also competitive for the High-Censor datasets, but given the construction of the L1Margin loss, this is not surprising; see Appendix B.2. Moreover, the three complex models (coxen-kp, rsf-km, mtlr) appear comparable for the High-Dimensional datasets.\nWe also compared the models in terms of \u201cUncensored L1-loss\u201d, which just considers the loss on the uncensored instances; see Table 11 in Appendix D.3. We see km is no longer competitive for the High-Censor datasets, showing how influential this effect is. Instead, at least one of the complex models {coxen-kp, rsf-km, mtlr} outperforms aft and cox-kp for every dataset.\nThat appendix also motivates and defines the Log L1-loss, and its Table 12 shows that mtlr performs best in 4 of the datasets, and is either second or third best in the others.\nWhich isd-Model to Use?: As shown above, which isd-model works best depends on properties of the dataset, and on what we mean by \u201cbest\u201d. Table 7 summarizes our results here.\nIn general, for Nice datasets, mtlr was superior for calibration but for discrimination, all isd-models were equivalent, leading us to recommend using the simplest models: (cox-\nkp, aft). As we found that rsf-km would overfit the training data when the number of features was small (here, less than 34), we recommend avoiding rsf-km when there are so few features.\nFor High-Censor datasets, we recommend mtlr or coxen-kp when there are not many features (e.g., READ, BRCA) for both calibration and discrimination. Typically cox-kp and aft had poor performance and high variability for High-Censor datasets. For HighDimensional datasets with low censoring (less than 70% i.e., DLBCL), mtlr, coxen-kp, and rsf-km had the best performance for calibration. For discrimination, rsf-km seemed slightly worse for Concordance and Brier score, suggesting it may be a weaker model.\nTo explore whether examine if these findings hold in general, we examined 33 other public datasets \u2013 16 (Low Dimension, Low Censoring), 12 (Low Dimension, High Censoring), 4 (High Dimension, Low Censoring) and 1 (High Dimension, High Censoring) where High Censoring is \u2265 70%. Note that all Low Dimensional datasets were taken from the TCGA website whereas the other (High Dimensional) datasets arise from a variety of sources. The results from these 33 datasets are consistent with the findings reported here; specific results can be found on the lead author\u2019s RPubs site17. Given the low overall number of High-Dimensional datasets, these findings should be examined on further datasets.\nWhy use isd-Models?: As noted above, this paper considers only models that generate isds (i.e., [P,\u221e,i]). This is significantly different from models that only generate risk scores ([R,1\u2200,i]), as those models can only be evaluated using a discriminatory metric. While this discrimination task (and hence evaluation) is helpful for some situations (e.g., when deciding which patients should receive a limited resource), it is not helpful for others (e.g., deciding whether a patient should go to a hospice, or terminate a treatment). A patient\u2019s primary focus will be on his/her own survival, not how they rank among others \u2013 hence the risk score such models produce do not meaningfully inform individual patients.\nThe single point probability models, [P,1t\u2217 ,i], are a step in the direction for benefiting patients, but they are still often inadequate, as they apply only to a single time-point. While hospital administrators may want to know about specific time intervals (e.g., t\u2217 =\u201c30- day readmission\u201d probabilities), medical conditions seldom, if ever, are so precise. This is problematic as these probabilities can change dramatically over a short time interval \u2013 i.e., whenever a survival curve has a very steep drop. For example, consider Patient #5 (P5) in Figure 4 for the mtlr model. Here, we would optimistic about this patient if we considered\n17See http://rpubs.com/haiderstats/ISDEvaluationSupplement\nthe single point probability model at t\u2217= 6months, as S\u0302MTLR(P5 | 6months ) = 0.8, but very concerned if we instead used t\u2217 = 12months, as S\u0302MTLR(P5 | 12months ) = 0.3. Note this trend holds for the other isd-models shown; and also for many of the patients, including P6, P7, P10.\nThis suggests a model based on only a single time point may lead to inappropriate decisions for a patient. Note also that such a model might not even provide consistent relative rankings over a pair of patients \u2013 i.e., it might provide different discriminative conclusions. Consider patients P2 and P9 in Figure 4[mtlr]. Here, at t\u2217 = 20months, we would conclude that the purple P9 is doing worse (and so should get the available liver), but at t\u2217= 30months, that the orange P2 is more needy. (We see similar inversions for a few other pairs of patients in mtlr, and also for several pairs in the rsf model.)\nOf course, one could argue that we just need to use multiple single-time models. Even here, we would need to a priori specific the set of time points \u2013 should we use 6 months and 12 months, and perhaps also 30 months? And maybe 20 months?\nThis becomes a non-issue if we use individual survival distribution (isd; [P,\u221e,i]) models, which produce an entire survival curve, specifying a probability values for every future time point. Moreover, while risk score models can only be evaluated using a discrimination metric, these isd models can be evaluated using all metrics, making them an overall more versatile method for survival analysis.\nBottom line: In general, a survival task is based on both a dataset, and an objective, corresponding to the associated evaluation measure. Our isd framework is an all-around more flexible approach, as it can be evaluated using any of the 5 measures discussed here (Section 3) \u2013 both commonly-used and alternative. Importantly, when evaluating isd models discriminatively (using Concordance), the risk scores we advocate (mean/median survival time) have meaning to clinicians and patients, whereas a general risk score, in isolation, has no clinical relevance. Moreover, the resulting survival curves are easy to visualize, which adds further appeal."
                },
                {
                    "heading": "6 Conclusion",
                    "text": ""
                },
                {
                    "heading": "Future Work:",
                    "text": "This paper has focused on the most common situation for survival analysis: where all instances in the training data are described using a fixed number of features (see the matrix in Figure 2), there is no missing values, and each instance either has a specified time of death, or is right-censored \u2013 i.e., we have a lower bound on that patient\u2019s time of death. There are many techniques for addressing the first two issues \u2013 such as ways to \u201cencode\u201d a time series of EMRs as a fixed number of features, or using mean imputations. There are also relatively easy extensions to some of the models (e.g., mtlr) to handle left-censored instances (where the dataset specifies an upper-bound on the patient\u2019s time of death), or interval censored. These extensions, however, are beyond the scope of the current paper.\nContributions:\nThis paper has surveyed several different approaches to survival analysis, including assigning individualized risk scores [R,1\u2200,i], assigning individualized survival probabilities for a single time point [P,1t\u2217 ,i], modeling a population level survival distribution, [P,\u221e,g], and primarily isd (individual survival distribution; [P,\u221e,i]) models. We discussed the advantages of having an individual survival distribution for each patient, as this can help patients and clinicians to make informed decisions about treatments, lifestyle changes, and end-oflife care. We discussed how isd models can be used to compute Concordance measures for discrimination and L1-loss, but should primarily be evaluated using calibration metrics (Sections 3.3, and 3.5) as these measure the extent to which the individual survival curves represent the \u201ctrue\u201d survival of patients.\nNext, we identified various types of isd-models, and empirically evaluated them over a wide range of survival datasets \u2013 over a range of #features, #instance and %censoring. This analysis showed that mtlr was typically superior for the L1-loss, Integrated Brier score, and Concordance, but most importantly, showed it outperformed or matched all other models for the calibration metrics.\nIn conclusion, this paper explains why we encourage researchers, and practioners, to use isd-models (and especially ones similar to mtlr) to produce meaningful survival analysis tools, by showing how this can help patients and clinicians make informed healthcare decisions."
                },
                {
                    "heading": "Acknowledgements",
                    "text": "We gratefully acknowledge funding from NSERC, Amii, and Borealis AI (of RBC). We also thank Adam Kashlak for his insightful discussions regarding D-Calibration."
                },
                {
                    "heading": "A Extending Survival Curves to 0",
                    "text": "In practice, survival curves often stop at a non-zero probability \u2013 see Figure 4 and Figure 13[left] below. This is problematic as it means they do not correspond to complete distribution (recall a survival curve should be \u201c1\u2212CDF(t)\u201d, where CDF is the Cumulative Distribution Function) which leads to problems for many of the metrics, as it is not clear how to compute the mean, or the median, value of the distribution. One approach is to extend each of the curves, horizontally, to some arbitrary time and then drop each to zero (the degenerate case being dropping the survival probability to zero at the last observed time point). This approach has downsides: Dropping the curve to zero at the last observed time point produces curves whose mean survival times are actually a lower bound on the patient\u2019s mean survival time, which is likely too small. In the event that the last survival probability is above 0.5 (as is often the case for highly censored datasets) this may bias our estimate of the L1-loss, which is based on the median value. Alternatively, if we instead extend each curve to some arbitrary time and then drop the curve to zero, we need to decide on that extension, which also could bias the L1-loss.\nSince both standard approaches have clear downsides (and there is no way of knowing how the survival curves act beyond the sampled survival times), we chose to simply extrapolate survival curves using a simple linear fit: for each patient ~xi, draw a line from (0, 1) \u2013 i.e., time is zero and survival probability is 1 \u2013 to the last calculated survival probability, (tmax, S\u0302( tmax | ~xi )), then extend this line to the time for which survival probability equals 0 \u2013 i.e., (t0(~xi), 0) \u2013 see Figure 13[right]. Note that curves cannot cross within the extended interval, which means this extension will not change the discriminatory criteria.\nThere are extreme cases where a survival model will predict a survival curve with survival probabilities of 1 (up to machine precision) for all survival times (think \u201ca horizontal line, at p = 1\u201d) \u2013 this occurred for unregularized models on high-dimensional datasets. In these cases, this linear extrapolation will never reach 0. To address this, we fit the Kaplan-\nMeier curve with the linear extension described above to compute t0KM ; we then replace any infinite prediction with this value. Additionally, as the Kaplan-Meier curve is to represent the survival curve on a population level, we also truncated any patient\u2019s median survival time by t0KM ."
                },
                {
                    "heading": "B Evaluation Measures Supplementary Information",
                    "text": "This appendix provides additional information about the various evaluation measures."
                },
                {
                    "heading": "B.1 Concordance",
                    "text": "As discussed in Section 3.1, Concordance is designed to measure the discriminative ability of a model. This is challenging for censored data. For example, suppose we have two patients who were censored at t1 and t2. Since both patients were censored, there is no way of knowing which patient died first and hence the risk scores for these patients are incomparable. However, if one patient\u2019s censored time is later than the death time of another patient, we do know the true survival order of this pair: the second patient died before the first.\nTo be precise, we first need to define the set of comparable pairs, which is the subset of pairs of indices (here using the validation dataset (V ) and recalling that \u03b4 = 1 indicates a patient who died (uncensored)) containing all pair of instances when we know which patient died first:\nCP(V ) = {[i, j] \u2208 V \u00d7 V | ti < tj and \u03b4i = 1 } (13)\nNotice when the earlier event is uncensored (a death), we know the ordering of the deaths (whether the second time is censored or not) \u2013 see Figure 14. The ti < tj condition is to prevent double-counting such that |CP(V )| \u2264 (|V |\n2\n) .\nWe then consider how many of the possible pairs our predictor put in the correct order: That is, of all [i, j] pairs in CP(V ), we want to know how often r(~xi) > r(~xj) given that ti < tj. Hence, the Concordance index of V , with respect to the risk scores, r(\u00b7), is\nC\u0302(V, r(\u00b7) ) = 1 |CP(V )| \u2211 i:\u03b4i=1 \u2211 j: ti<tj I [ r(~xi) > r(~xj) ] . (14)\nOne issue is how to handle ties, in either risk scores or death times \u2013 i.e., for two patients, Patient A and Patient B, consider either r(~xA) = r(~xB) or dA = dB. The two standard approaches are (1) to give the model a score of 0.5 for ties (of either risk scores or death times), or (2) to remove tied pairs entirely [54]. The first option is equivalent to Kendall\u2019s tau, while the second leads to the Goodman-Kruskal gamma. The empirical evaluations (given in Section 4.2) use the first, as this gives Kaplan-Meier a Concordance index of 0.5 for all models. If we use the second option (excluding ties), then the Concordance for the Kaplan-Meier model is not well-defined."
                },
                {
                    "heading": "B.2 L1-loss, and variants",
                    "text": "As discussed in Section 3.2, survival analysis can be viewed as a regression problem that is attempting to minimize the difference between an estimated time of death and the true time of death. However, typical regression problems require having precise target values for each instance; here, many instances are censored \u2013 i.e., providing only lower bounds for the target values. One option is to simply remove all the censored patients and use the L1-loss given by Equation 4 (which we call \u201cUncensored L1-Loss\u201d); however, this will likely bias the true loss. Table 11 in Appendix D.3 provides the results for this Uncensored L1-loss over the 8 datasets. (We see that mtlr is best for 6 of these datasets.)\nOne way to incorporate censoring is to use the Hinge loss for censored patients, which assigns 0 loss to any patient whose censoring time ck is prior to the estimated median survival time, t\u0302\n(0.5) k \u2013 i.e., a loss of 0 if ck < t\u0302 (0.5) k \u2013 and a loss of ck \u2212 t\u0302 (0.5) k if the censoring time is\ngreater than t\u0302 (0.5) k . That is:\nL1hinge(V, {t\u0302(0.5)j }j ) = 1\n|V | [ \u2211 j\u2208VU |dj \u2212 t\u0302(0.5)j | + \u2211 k\u2208VC [ck \u2212 t\u0302(0.5)k ]+ ] . (15)\nwhere VU is the subset of the validation dataset that is uncensored, and VC is the censored subset, and [a]+ is the positive part of a, i.e.,\n[a]+ = max{a, 0} = { a if a \u2265 0 0 otherwise .\nThis formulation is an optimistic lower bound on the L1-loss for two reasons: (1) it gives a loss of 0 if the censoring occurs prior to the estimated survival time, implying that dk = t\u0302 (0.5) k ,\nand (2) it gives a loss of ck \u2212 t\u0302(0.5)k if the censoring time occurs after the estimated survival time, which assumes that dk = ck. Both are the best possible values for the unknown dk, given the constraints..\nOne weakness of the L1-Hinge loss is that if a model predicts very large survival times for all patients (both censored and observed), the hinge loss will give 0 loss for the censored patients; in datasets with a large proportion of censored patients, this leads to an optimistic score overall. Thus the hinge loss will favor models that tend to largely overestimate survival times as opposed to those models underestimating survival time.\nA third variant of L1-loss, the L1-Margin loss, assigns a \u201cBest-Guess\u201d value to the death time corresponding to ck, which is the patient\u2019s conditional expected survival time given they have survived up to ck \u2013 given by\nBG(ck) = ck +\n\u222b\u221e ck S(t) dt\nS(ck) (16)\nwhere S(\u00b7) is the survival function; Theorem B.1 proves this value corresponds to the conditional expectation. In practice we use Kaplan-Meier estimate, S\u0302KM( \u00b7 ), generated from the training dataset (disjoint from the validation dataset) as our estimate of S(\u00b7) in Equation 16.\nWe also realized that these BG(ck) estimates are more accurate for some patients, than for others. If ck \u2248 0 \u2013 that is, if the patient was censored near the beginning time \u2013 then we know very little about the true timing of when the death occurred, so the estimate BG(ck) is quite vague, which suggests we should give very little weight to the associated loss, |BG(ck)\u2212 t\u0302(0.5)k |. Letting \u03b1k be the weight associated with these terms, we would like \u03b1k \u2248 0. On the other hand, if cr is large \u2013 towards the longest survival time observed (call it dmax) \u2013 then there is a relatively narrow gap of time where this ~xr could have died (probably within the small interval (cr, dmax)); here, we should give a large weight to loss associated with this estimate.\nThis motivates us to define\nL1margin(V, {t\u0302(0.5)j } ) = 1 |VU |+ \u2211\nk\u2208VC \u03b1k [ \u2211 j\u2208VU |dj \u2212 t\u0302(0.5)j | + \u2211 k\u2208VC \u03b1k|BG(ck)\u2212 t\u0302(0.5)k | ] (17)\nwhere \u03b1k reflects the confidence in each Best-Guess estimate. To implement this, we set \u03b1k = 1\u2212S\u0302KM(ck), which gives little weight to instances with early censor times but considers late censor times to be almost equivalent to an observed death time. Note this is the version of L1-loss we presented in Figure 11, with details in Table 10.\nFor completeness, we prove Equation 16. (This claim is also proven by Gupta and Bradley [20], which uses mean residual life rather than expected total life.)\nTheorem B.1. The conditional expectation of time of death, D, given that a patient was censored at time c, is given by: E[D |D > c] = c+ \u222b\u221e c S(x) dx\nS(c) .\nProof. Let D be the r.v. for the time when a patient dies, and define\nS(c) = P (D > c) =\n\u222b \u221e c P (D = t) dt\nas the survival function \u2013 i.e., the probability that the patient dies after time c. Given this, the conditional probability is\nP (D = t |D > c ) = P (D = t, D > c ) P (D > c )\n= P (D = t, D > c )\nS( c ) =\n{ 0 if t < c\nP (D=t ) S( c )\notherwise .\nE[D |D > c ] = \u222b \u221e c t P (D = t ) S( c ) dt\n= 1\nS( c ) [\u222b \u221e c c P (D = t ) dt + \u222b \u221e c (t\u2212 c)P (D = t ) dt ]\n= 1\nS( c )\n[ c S( c ) + \u222b \u221e c (\u222b t c dx ) P (D = t) dt ] = c + 1\nS( c ) [\u222b \u221e c (\u222b \u221e x P (D = t) dt ) dx ] (18)\n= c +\n\u222b\u221e c S(x ) dx\nS( c ) .\nStep 18 is an application of Tonelli\u2019s theorem [41], which lets us swap the order of integration for a non-negative function. As desired, this quantity, E[D |D > c ], is always at least c. Moreover, when c = 0, this is\n0 +\n\u222b\u221e 0 S( t ) dt\n1 =\n\u222b \u221e 0 S( t ) dt = E[D ]\nwhich is the expected value of the distribution for this survival curve (and exactly the claim of the Theorem)."
                },
                {
                    "heading": "B.2.1 Log L1-loss",
                    "text": "The L1-loss measure implicitly assumes that the quality of a prediction, t\u0302 (0.5) j , depends only on how close it is to the truth dj\u2013 i.e., on |dj\u2212 t\u0302(0.5)j |. But this does not always match how we think of the error: if we predict Patient A will live for 120 months then found that he actually lived 117 months, we would consider our prediction very accurate. By contrast, if we predict Patient B will live 1 month, but then find she lived 4 months, we would consider this to be a poor prediction. Notice, however, the L1-loss for Patient A is |dA\u2212 t\u0302(0.5)A | = |120\u2212 117| = 3 months, which is the same as the L1-loss for Patient B: |dB \u2212 t\u0302(0.5)B | = |1\u2212 4| = 3 months!\nThis motivates us to consider the relative error, rather than an absolute error: here, as our prediction for Patient A is off by only 3 / 120 = 2.5%, we consider it good, whereas our\nprediction for Patient B is off by 3 / 1 = 300%. The Log-L1-loss reflects this:18\n`LogL1( di, t\u0302 (0.5) i ) = | log(di)\u2212 log(t\u0302 (0.5) i )| (19)\nTo compute the average Log-L1-loss over the dataset VU , we can use Equation 4 but using log(dj) rather than dj, etc. To avoid taking log 0, we replace 0 with half the minimum, positive death time (see Section B.6). Table 12 in Appendix D.3 provides the results here, over the 8 datasets. (We see that mtlr is best for 4 of these datasets.)\nB.3 1-Calibration\nTo demonstrate the description from Section 3.3, consider the following example: If there are n = 50 patients, then 50/10 = 5 will be in each bin, and the first bin B#1 will contain the 5 with lowest predicted probability values, and the second bin B#2 will contain the next smallest 5 values, and so forth \u2013 e.g.,\nB#1 = {0.32, 0.34, 0.43, 0.43, 0.48} B#2 = {0.55, 0.56, 0.61, 0.61, 0.72}\n...\nB#10 = {0.85, 0.85, 0.86, 0.87, 0.87}\nNow consider the 5 patients who belong to B#1. As the average of their probabilities is 0.32+0.34+0.43+0.43+0.48\n5 = 0.4, we should expect 40% of these 5 individuals to die in the next\n5 years \u2013 that is, 2 should die. We can then compare this prediction (0.40\u00d7 5 = 2) with the actual number of these B#1 patients who died. We can similarly compare the number of B#2 patients who actually died to the number predicted (based on the average of these 5 probability values, which here is 0.61\u00d7 5 = 3.05), and so forth.\nIn general, we say that the predictor is 1-Calibrated if these B predictions, for the B = 10 bins, are sufficiently close to the actual number of deaths with respect to these bins. Here, we use the Hosmer\u2013Lemeshow statistical test (given in Section 3.3) to see if the observed results were significant; repeating Equation 5:\nHL \u2227 (VU , S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 (Oj \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) ,\nwhere Oj is the number of observed events, nj is the number of patients, p\u0304j is the average predicted probability, and subscript j refers to within the jth of B bins.\n18 Note that the times mentioned in \u201cDoc, do I have a day, a week, a month or a year?\u201d are basically in a log-scale.\nB.3.1 Incorporating Censoring into 1-Calibration\nSurvival data typically contains some amount of censoring, making the exact number of deaths for the jth bin, Oj, unobservable when the bin contains patients censored before t\n\u2217. That is, given a censored patient whose censoring time occurred before the time of interest (ci < t\n\u2217) the patient may or may not have died by t\u2217. There are many standard techniques for incorporating censoring [19]; we use the D\u2019Agostino-Nam translation [12], which uses the within bin Kaplan-Meier curve in place of Oj. Specifically, the test statistic is given by,\nHL \u2227 DN (V, S\u0302( t \u2217 | \u00b7 ) ) =\nB\u2211 j=1 ( nj (1\u2212KMj(t\u2217)) \u2212 nj p\u0304j)2 nj p\u0304j (1\u2212 p\u0304j) , (20)\nwhere KMj(t \u2217) is the height of the Kaplan-Meier curve generated by the patients in the jth bin, evaluated at t\u2217. We use 1\u2212KMj(t\u2217) as we are predicting the number of deaths and not KMj(t \u2217) which instead gives the probability of survival at t\u2217. Note also that HL \u2227\nDN follows a \u03c72B\u22121 distribution, as opposed to the \u03c7 2 B\u22122 distribution for Equation 5."
                },
                {
                    "heading": "B.4 Brier Score Details",
                    "text": "This section supplements the description of the Brier score given in Section 3.4, discussing (1) the decomposition of the Brier score into calibration and discrimination components, (2) the failure of the Integrated Brier score to incorporate the full distribution of probabilities in survival curves, and (3) how to incorporate censoring into the Brier score."
                },
                {
                    "heading": "B.4.1 Brier Score Decomposition",
                    "text": "As mentioned in Section 3.4, the Brier score can be separated into calibration and discriminatory components. The original separations were the the work of Sanders [42] and Murphy [36, 37] and later put into the context of calibration and discrimination (also known as refinement) by DeGroot and Fineberg [13].\nRecall the notation and mathematical expression of the Brier score for a set of uncensored instances, VU ,\nBS ( S\u0302( t\u2217 | \u00b7 ), {~xi} ) =\n1 |VU | \u2211 i\u2208VU ( I [ di \u2264 t\u2217 ]\u2212 S\u0302(t\u2217|~xi) )2 .\nTo simplify notation, let pi = S\u0302( t \u2217 | ~xi ). The separation of the Brier score requires that a discrete, distinct number of predictions exist; here, assume there are K distinct values for pk for k = 1, . . . K.\nFurther, let nk be the total number of patients with pk as their prediction and hence |VU | = \u2211K k=1 nk. Finally, let \u03bbk be the observed proportion of patients who have died by t \u2217 and thus (1 \u2212 \u03bbk) is the proportion still alive. The separation theorem of the Brier score\nstates that BS = C + D, where C and D are nonnegative calibration and discriminatory scores where\nC = 1\n|VU | K\u2211 k=1 nk(\u03bbk \u2212 pk)2 (21)\nD = 1\n|VU | K\u2211 k=1 nk\u03bbk(1\u2212 \u03bbk). (22)\nNote the calibration score, C, is nearly equivalent (up to a factor of nk) to the numerator of the Hosmer-Lemeshow test (Equation 5). However, the Hosmer-Lemeshow test subscript refers to bins whereas here the subscript refers to a distinct value of pk. One can see that C represents a calibration score as the estimated probabilities, pk, must be close to the true proportion of deaths, \u03bbk in order to have a small score (lower is better). In fact, to satisfy C = 0, all predictions, pk must be equal to \u03bbk (Equation 21).\nThere are also similarities between D and the denominator of the Hosmer-Lemeshow test. However, note Equation 22 uses the the true proportion of deaths \u03bbk, whereas the Hosmer-Lemeshow test uses an estimated value, p\u0304. Note that D has a \u201cgood\u201d (low) score if all patients associated with a prediction probability pk have the same status \u2013 i.e., they either all die or are all still alive. To understand why this means D is a discriminatory measure, consider the extreme case where BS(\u00b7, \u00b7) = 0, which means both D = 0 and C = 0. For D = 0, all patients associated with each probability value must either be dead by t\u2217 or all be alive at t\u2217 \u2013 i.e., \u03bbk \u2208 {0, 1} for k = 1, 2; note only K = 2 is possible here. In turn, for C = 0, we require pk = \u03bbk for k = 1, 2, that is pk \u2208 {0, 1} \u2013 all predictions will be 1 or 0. Here we are discriminating perfectly between the patients who have died and the patients who are still alive, with a model that predicts only 1\u2019s or 0\u2019s. Of course, we should not require a model to estimate survival probabilities to be precisely 1 or 0, for the same reason that we do not expect the learned distribution to correspond to the Heaviside distribution shown in Figure 5.\nB.4.2 Integrated Brier score does not involve the Entire Distribution\nAt the beginning of Section 3.5, we claimed the Integrated Brier score (IBS) does not utilize the survival curves\u2019 full distribution of probabilities over all times. For example, on a km curve, we expect that 10% of patients will die in every 10% interval \u2013 e.g., 10% of all patients will die in the [0.5, 0.6) interval. While D-Calibration will debit a model that fails to do this, this Integrated Brier score does not require this. The most obvious example is the perfect model, where each patient is given the appropriate Heaviside distribution (Figure 5) at his/her time-of-death: here the only probabilities are {0,1} \u2013 here IBS(\u00b7, \u00b7) = 0, even though no patient\u2019s S\u0302Heaviside( di | ~xi ) is ever in [0.5, 0.6). However, as we have previously noted, the inherent stochasticity of the world means that meaningful distributions should include non-zero probabilities in other places as well, rather than placing all weight on a single time point.\nSince the Integrated Brier score fails to account for this, there is no guarantee that probabilities are meaningful across individual survival curves. This motivated us to introduce DCalibration, to determine whether a proposed isd-model produces meaningful distributions, with probabilities that reflect the number of deaths that have occurred in the population. To see that these two metrics are measuring different aspects, note that the Integrated Brier scores for the (aft, cox-kp, coxen-kp, and mtlr) models are all well within 1 standard error of one another for the GBM dataset, but only coxen-kp and mtlr are D-Calibrated. (This is also true for the GLI dataset.)\nB.4.3 Incorporating Censoring into the Brier score\nIn 1999, Graf et al. [18] proposed a way to compute the Brier Score for censored data, by using inverse probability of censoring weights (IPCW), which requires estimating the censoring survival function, denoted as G\u0302(t) over time points t. We can estimate G\u0302(t) by the Kaplan-Meier curve of the censoring distribution \u2013 i.e., swapping those who are censored with those who are not, (\u03b4Censi = 1 \u2212 \u03b4i) and building the standard Kaplan-Meier model. Intuitively, this IPCW weighting counteracts the sparsity of later observations \u2013 if a patient dies early, there is a good chance that di < ci meaning the event is observed, but if the patient survives for a long time, it becomes more likely that ci < di meaning this patient will be censored. Gerds et al. [16, 17] formalizes and proves this intuition.\nThe censored version of the Brier score for a given time, t\u2217, is calculated as\nBSt\u2217 ( V, S\u0302(t\u2217|\u00b7) ) = 1\n|V | |V |\u2211 i=1\nI [ ti \u2264 t\u2217, \u03b4i = 1 ] ( 0\u2212 S\u0302(t\u2217|~xi) )2\nG\u0302(ti) + I [ ti > t\u2217 ]\n( 1\u2212 S\u0302(t\u2217|~xi) )2 G\u0302(t\u2217)  , (23) where ti = min{di, ci}. The first part of Equation 23 considers only uncensored patients whereas the second part counts all patients whose event time is greater than t\u2217. The patients who were censored prior to t\u2217 are not explicitly included, but contribute based on their influence in G\u0302(\u00b7).\nAs G\u0302(t) is a decreasing step function of t, 1 G\u0302(t) is increasing, which means that patients who survive longer than t\u2217 have larger weights than patients that died earlier, since the longer surviving patients were more likely to become censored."
                },
                {
                    "heading": "B.5 D-Calibration",
                    "text": "We begin this section by justifying why, in the case of all uncensored patients, (1) the distribution of the survival function, {S(t)}t, should follow a uniform distribution, then (2) Following this discussion, we show how to incorporate censored patients into the DCalibration estimate, and finally, (3) that this combination of censored and uncensored patients will produce a uniform distribution for the goodness-of-fit test to test against.\nFor this analysis, we assume each patient ~xi has a true survival function, S( t | ~xi ), which is the probability that this patient will die after time t. Assume each patient has a time of death, di and a censoring time, ci, and ti = min {di, ci} is the observed event time. We also\nassume that censoring time is independent of death time, ci \u22a5 di. Given a validation set |V |, we first examine the case of all uncensored patients \u2013 i.e., ti = di for i = 1, . . . , |V |.\nLemma 1. The distribution of a patient\u2019s survival probability at the time of death S( di | ~xi ) is uniformly distributed on [0,1].\nProof. The probability integral transform [4] states that, for any random continuous variable, X, with cumulative distribution function given by Fx(\u00b7), the random variable Y = Fx(X) will follow a uniform distribution on [0,1], denoted as U(0, 1). Thus, given randomly sampled event times, t, we have F (t) \u223c U(0, 1). As the survival function is simply S(t) = 1 \u2212 F (t), its distribution is 1\u2212 U(0, 1), which also follows U(0, 1) and hence S(t) \u223c U(0, 1).\nThis Lemma shows that, given the true survival model, producing S( \u00b7 | ~xi ) curves, the distribution of S( di | ~xi ) should be uniform over event times. Thus if a learned model accurately learns the true survival function, S\u0302\u0398( \u00b7 | \u00b7 ) \u2248 S(\u00b7|\u00b7), we will expect the distribution across event times to be uniform. This is then tested using the goodness-of-fit test assuming each bin contains an equal proportions of patients.\nOf course, conditions become more complicated when considering censored patients. Suppose we have a censored patient \u2013 i.e., ti = ci \u2013 such that S( ci | ~xi ) = 0.25. Since the censoring time is a lower bound on the true death time, we know that S( di | ~xi ) \u2264 0.25, since ci < di and survival functions are monotonically decreasing as event time increases. If we are using deciles, we would like to know the probability that the time of death occurred in the [0.2,0.3) bin \u2013 i.e., P ( S(di|~xi) \u2208 [0.2, 0.3) | S(di|~xi) \u2264 0.25). Using the rules of conditional probability, this is computationally straightforward19:\nP (S(di) \u2208 [0.2, 0.3) |S(di) < 0.25 ) = P (S(di) \u2208 [0.2, 0.3), S(di) < 0.25 )\nP (S(di) < 0.25 )\n= P (S(di) \u2208 [0.2, 0.25)) P (S(di) < 0.25)\n= 0.05\n0.25 (as S(\u00b7) \u223c U(0, 1))\n= 0.2\nSimilarly, we can use the same logic as above to compute these probabilities for the other two bins, [0.1, 0.2) and [0.0, 0.1):\nP (S(di) \u2208 [0.1, 0.2) |S(di) < 0.25 ) = P (S(di) \u2208 [0.1, 0.2), S(di) < 0.25)\nP (S(di) < 0.25)\n= P (S(di) \u2208 [0.1, 0.2)) P (S(di) < 0.25)\n19To simplify notation, we drop the conditioning on ~xi of S(\u00b7|\u00b7).\n= 0.1\n0.25 (as S(\u00b7) \u223c U(0, 1))\n= 0.4\nand similarly for the [0.0, 0.1) bin. Note that these probabilities sum to one, (0.2+0.4+0.4) = 1, as desired.\nThis example motivates the following procedure to incorporate censored patients into the D-Calibration process: Given B bins that equally divide [0,1] into intervals of width BW = 1/B, suppose a patient is censored at time c with associated survival probability S(c). Let b1 be the infimum probability of the bin that contains S(c) \u2013 e.g., 0.2 for the example above where S(ci) = 0.25 \u2208 [0.2, 0.3). Then we assign the following weights to bins:\n(A) Bin [b1, b2) (which contains S(c)): S(c)\u2212b1 S(c) = 1\u2212 b1 S(c)\n(B) All following bins (i.e., the bins whose survival probabilities are all less than b1): BW S(c) = 1 B\u00b7S(c) ,\nNote this formulation follow directly from the example above. This weight assignment effectively \u201cblurs\u201d censored patients across the bins following the bin where the patient\u2019s learned survival curve, S\u0302\u0398( ci | i ) placed the censored patient.\nTo further illustrate this concept of blurring a patient across bins, consider a patient who is censored at t = 0 with S(ci) = 1. This patient is then blurred across all (B = 10) bins, adding a weight of 0.1 to all 10 bins. Alternatively, if a patient is censored very late, with S(ci) \u2264 0.1 then the patient is not blurred at all \u2013 only a weight of 1 is added to the last bin.\nThis identifies a weakness of D-Calibration: if a validation set contains N0 patients censored at time 0, then all bins are given an equal weight of N0/B; if N0 is large relative to the total number of patients, then the bins may appear uniform, no matter how the other patients are distributed, which means any model based on such heavily \u201ctime 0 censored\u201d data would be considered to be D-Calibrated.\nTo perform the goodness-of-fit test, we must first calculate the observed proportion of patients within each bin. Let Nk represent the observed proportion of patients within the interval [pk, pk+1) \u2013e.g., [pk, pk+1) = [0.2, 0.3) in the example above. We can formally calculate:\nNk = 1\n|V | |V |\u2211 i=1 [ I [S(di) \u2208 [pk, pk+1) \u2227 di \u2264 ci ] (24)\n+ S(ci)\u2212 pk S(ci) \u00b7 I [S(ci) \u2208 [pk, pk+1) \u2227 ci < di ] (25) + (pk+1 \u2212 pk)\nS(ci) \u00b7 I [S(ci) \u2265 pk+1 \u2227 ci < di ]\n] . (26)\nAbove, (24) refers to the weight that the patients with observed events contribute to the kth bin \u2013 i.e., each uncensored patient whose survival probability at time of death lands in [pk, pk+1) contribute a value of 1. Here, we consider di = ci to be an uncensored event. Next, (25) gives the weight from the censored patients whose survival probability at time of censoring is within the kth bin (item (A) above). Lastly, (26) gives the weights from censored patients whose survival probability was contained in a previous bin (item (B) above).\nTheorem B.2 below proves that the expected value ofNk is equal for all bins \u2013 i.e., E[Nk] = pk+1 \u2212 pk \u2013 which allows us to apply the goodness-of-fit test with uniform proportions.\nWe assume that all survival curves are strictly monotonically decreasing meaning we have the equality, di \u2264 ci \u21d0\u21d2 S(di) \u2265 S(ci)). This equivalence lets us replace di \u2264 ci with S(di) \u2265 S(ci), within the indicator functions in Nk. To simplify notation, we define Ik := [pk, pk+1), Sc := S( c | ~x ), and Sd := S( d | ~x ). The proof below shows that the expected value of the summand within Equations (24) \u2013 (26) above is equal to pk+1 \u2212 pk \u2013 i.e., we ignore 1|V | \u2211|V | i=1[\u00b7] and take the expected value of the term inside the summation.\nTheorem B.2. Given the formula for Nk (Equations (24) - (26)), if the true survival function S(\u00b7|\u00b7) is strictly monotonically decreasing then proportions are equal across all bins \u2013 i.e., E[Nk] = pk+1 \u2212 pk."
                },
                {
                    "heading": "Proof.",
                    "text": "E[Nk] =E [ I [Sd \u2208 Ik \u2227 Sd \u2265 Sc ]\n+ Sc \u2212 pk Sc \u00b7 I [Sc \u2208 Ik \u2227 Sc > Sd ] + (pk+1 \u2212 pk)\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2208 [pk+1, 1] ]\n]\n= E [ I [Sd \u2208 Ik \u2227 Sd \u2265 Sc ] ] + E\n[ Sc \u2212 pk Sc \u00b7 I [Sc \u2208 Ik \u2227 Sc > Sd ] ]\n+ E [\n(pk+1 \u2212 pk) Sc\n\u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ] ]\n= Pr[Sd \u2208 Ik \u2227 Sd \u2265 Sc ] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] \u2212 pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2208 Ik ] ] + (pk+1 \u2212 pk)E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ]\n]\n= Pr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] (I)\n\u2212 pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk ]\n] (II)\n+ pk+1E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc \u2265 pk+1 ]\n] (III)\nFocusing on the second probability in line (I), note Sc \u2208 Ik = [pk, pk+1) and Sc > Sd imply that Sd \u2208 [0, pk+1) which can be expanded to the cases for Sd < pk and Sd \u2208 Ik. Using this, we reformulate the probability by noting the equivalence of the event space,\nPr[Sc \u2208 Ik \u2227 Sc > Sd] = Pr[Sc \u2208 Ik \u2227 Sd < pk] + Pr[(Sc \u2227 Sd) \u2208 Ik \u2227 Sc > Sd].\nCombining the second piece above with the first probability in line (I), we again simplify by noting these probabilities bound Sc < pk+1,\nPr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[(Sc \u2227 Sd) \u2208 Ik \u2227 Sc > Sd] = Pr[Sd \u2208 Ik \u2227 Sc < pk+1].\nUsing this simplification we can rewrite the entirety of line (1),\nPr[ Sd \u2208 Ik \u2227 Sd \u2265 Sc ] + Pr[ Sc \u2208 Ik \u2227 Sc > Sd ] = Pr[ Sd \u2208 Ik \u2227 Sc < pk+1 ] + Pr[ Sc \u2208 Ik \u2227 Sd < pk ]\nRecalling the independence assumption, c \u22a5 d, we have the following equalities:\nPr[Sd \u2208 Ik \u2227 Sc < pk+1] = Pr[Sd \u2208 Ik] \u00b7 Pr[Sc < pk+1] = (pk+1 \u2212 pk) Pr[Sc < pk+1], Pr[Sc \u2208 Ik \u2227 Sd < pk] = Pr[Sc \u2208 Ik] \u00b7 Pr[Sd < pk] = pk Pr[Sc \u2208 Ik],\nwhere the final equalities are due to the uniformity of the survival function on d, S(d) \u223c U(0, 1). This then leaves the final simplification of line (I) as,\nPr[Sd \u2208 Ik \u2227 Sd \u2265 Sc] + Pr[Sc \u2208 Ik \u2227 Sc > Sd] = (pk+1 \u2212 pk) Pr[Sc < pk+1] + pk Pr[Sc \u2208 Ik].\nNow we address line (II) and analagously line (III): \u2212pk E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc > pk ]\n] = \u2212pk (\u222b 1 pk \u222b Sc 0 1 Sc f(Sc) dSd dSc ) (Def. of E[\u00b7])\n= \u2212pk (\u222b 1\npk\nSc Sc f(Sc) dSc\n)\n= \u2212pk Pr[Sc > pk]\nHere f is the probability distribution function (PDF) for the distribution generated by the survival function applied to a censored observation. As the censoring distribution is unknown f(Sc) is also unknown whereas f(Sd) would be the PDF of the uniform distribution.\nFollowing the steps above for line (III) analogously gives us pk+1 E [ 1\nSc \u00b7 I [Sc > Sd \u2227 Sc > pk+1 ]\n] = pk+1 Pr[Sc > pk+1]\nCombining the simplifications of lines (I), (II) and (III), we have the following,\nE[Nk] = (pk+1 \u2212 pk) Pr[Sc < pk+1] + pk Pr[Sc \u2208 Ik] (I) \u2212 pk Pr[Sc > pk] (II) + pk+1 Pr[Sc > pk+1] (III)\n= pk+1 (Pr[Sc < pk+1] + Pr[Sc > pk+1])\n\u2212 pk (Pr[Sc < pk+1]\u2212 Pr[Sc \u2208 [pk, pk+1) + Pr[Sc > pk])\n= pk+1 \u2212 pk\nThis proof requires the assumption that survival curves are strictly monotonically decreasing on [0,1]. This means survival curves will not contain any large flat areas, which means there will not be non-zero probability mass for S(ci) = S(di) when ci 6= di, which means certain terms in the proof below would fail to cancel with one another, leaving us with non-equivalent proportions within each bin (specifically higher proportions within bins that contain these flat lines).\nA natural corollary of Theorem B.2 is that all consistent estimators of the true survival distribution will be D-Calibrated (if the true survival distribution is strictly monotonic). Further, if survival time is independent and identically distributed (i.i.d.) across patients then there will only be one true survival curve for all patients, and thus, as Kaplan-Meier is uniformly consistent [5, 11]:\nLemma 2. The Kaplan-Meier distribution is asymptotically D-Calibrated.\nThis is consistent with the results given in Section 4.2, which showed that km always passed the D-Calibration test with a p-value 1.000, in all 8 datasets. Under all uncensored data, we would expect the typical 5% Type I error rate for claiming p <0.05 as significant, however in the presence of censored data a correct estimate of the survival distribution the proportion within bins become smoothed, boosting the p-value.\nProposition B.3. It is possible for a isd model to be perfectly D-calibrated but not 1- calibrated at a time t\u2217; and for (another) isd model to be perfectly 1-calibrated at time t\u2217 but not D-calibrated.\nProof. \u201c1-Calibration 6\u21d2 D-Calibration\u201d: Consider the model shown in Figure 15[left]. Here, the green curve corresponds to 4 apparently-identical patients {~xg,1, . . . , ~xg,4}, and the red curve, to apparently-identical {~xr,1, . . . , ~xr,4}. The \u201c\u2217\u201ds mark the time when each patient died, denoted as d~x for ~x. We intentionally use simple examples, with no censored patients, with curves that go to 0. Note this model assigns S\u0302(T1 | ~xg,i ) = 0.75 for each of the 4 green patients, and S\u0302(T1 | ~xr,j ) = 0.25 for each of the 4 red patients\nTo show that this model is 1-Calibrated, with respect to T1: Recall we first sort the S\u0302(T1 | ~x ) values, then partition them into k sets. Here, we consider k = 2, rather than the deciles earlier. The first set contains the 4 patients with S\u0302(T1 | ~x ) = 0.75 (i.e., the green patients); and the second, the 4 patients with S\u0302(T1 | ~x ) = 0.25. Now note that 3 of the 4 \u201cS\u0302(T1 | ~x ) = 0.75 patients\u201d are alive at T1; and 1 of the 4 \u201cS\u0302(T1 | ~x ) = 0.25 patients\u201d are alive at T1 \u2013 which means this model is perfectly 1-Calibrated at T1.\nHowever, this model is not D-Calibrated: To be consistent with the earlier 1-Calibration analysis, we partition the time intervals into 2 sets (not 10), as shown in Figure 15. Here, S\u0302( d~x | ~x ) \u2208 [0.5, 1] holds for only 1 patient, and S\u0302( d~x | ~x ) \u2208 [0, 0.5] holds for 7; if the model was D-Calibrated, each of these sets should contain 4 patients. \u201cD-Calibration 6\u21d2 1-Calibration\u201d: See Figure 15[right], where again, each line represent 4 different patients; notice the outcomes are different from those on the left. To see that this model is D-Calibrated, note there are 4 patients with S\u0302( d~x | ~x ) \u2208 [0.5, 1] (the green patients), and 4 with S\u0302( d~x | ~x ) \u2208 [0, 0.5] (for the red patients). However, the model is not 1-Calibrated, at T1: Of the 4 patients with S\u0302(T1 | ~x ) = 0.75, 2 are alive at T1; and of the 4 patients with S\u0302(T1 | ~x ) = 0.25, 2 are alive at T1. To be 1-Calibrated, there should be 3 living patients in the first set, and 1 in the second; hence this model is not 1-Calibrated at T1."
                },
                {
                    "heading": "B.6 Other Subtle Points",
                    "text": "All of these tools for producing survival curves are able to deal with \u201cright censored\u201d events: where the censored event time is a lower bound of the time of death. (This corresponds to, perhaps, the termination of a study, or when a participant left the study early.) There are other types of censoring, including \u201cleft censoring\u201d, which provides an upper-bound on the time of death (e.g., when a survey finds that the patient is currently dead, but does not know when previously this happened), and \u201cinterval censoring\u201d, when we can constrain the time of death to some interval. While there are extensions of each of these tools that can accommodate these alternate types of censoring, here we considered the most common case of having right-censored instances, and included only datasets that had only such instances.\nAs a second subtle issue: some of the methods involve taking the log of a predicted value, or of a true value; see Appendix B.2.1. This is clearly problematic if that value is 0 \u2013 e.g., if a patient died during a transplantation surgery. To avoid these errors, we replace any such 0 with the \u03b7 for a database, which is defined as 1/2 of the minimum observed positive time of any event, in that dataset. That is, we ignore all time=0 events, and then consider the smallest remaining value. If that value is, say, 1.0 day, then we set \u03b7 =0.5 days. Note that all other times are left unchanged."
                },
                {
                    "heading": "C Comments about Various isd\u2019s",
                    "text": "C.1 Comments about cox-kp\nNotice Equation 2 embodies two strong assumptions: (1) that the individual features are independent of one another (e.g., the outcome does not depend on a non-linear combination of the features), and (2) that these covariates are independent of time \u2013 which means that a blood test is as important just after an operation, as it is a year later, or a decade later. These assumptions mean the survival curves for different patients will have the same basic shape, and will not cross; see Figure 4 (middle-left). These simplifications allow the Cox model to suggest important information about individual features by examining the single coefficient \u03b2j associated with the j\nth feature, e.g., does \u201cbeing male\u201d increase the risk of dying from this specific cancer, or does it protect against this outcome (or neither). This \u201cneither\u201d case suggests that a given feature is not relevant to the prediction; for this reason, we used univariate Cox is a feature selection technique for our results.\nBy contrast, mtlr and rsf-km do not make these extreme assumptions, which means that a given feature can have different levels of importance at different times. Moreover, the curves for different patients can cross; see Figure 4. More relevant, however, we found that mtlr is more often D-Calibrated, and hence more meaningful for individual patients, than this \u201cpredictive Cox\u201d system; see Table 6. While this Cox analysis of survival may not be directly relevant for individual patients, there are still extreme benefits in being able to identify important features. By observing how different features impact survival, clinicians can be made aware of treatments or lifestyle changes that best help patients survival.\nC.2 Overview of mtlr\nConsider20 modeling the probability of survival of patients at each of a vector of time points \u03c4 = [t1, t2, . . . , tm] \u2013 e.g., \u03c4 could be the 60 monthly intervals from 1 month up to 60 months. We can set up a series of logistic regression models: For each patient, represented as ~x,\nS~\u03b8i(T \u2265 ti | ~x ) = ( 1 + exp(~\u03b8i \u00b7 ~x) )\u22121 , 1 \u2264 i \u2264 m, (27)\nwhere ~\u03b8i are the time-specific parameter vectors. While the input features ~x stay the same for all these classification tasks, the binary labels yi = [T \u2265 ti] can change depending on the threshold ti. We encode the survival time d of a patient as a sequence of binary values: y = y(d) = [y1, y2, . . . , ym], where yi = yi(d) \u2208 {0, 1} denotes the survival status of the patient at time ti, so that yi = 0 (no death event yet) for all i with ti < d, and yi = 1 (death) for all i with ti \u2265 d. Here there are m + 1 possible legal sequences of the form21 [0, 0, . . . , 1, 1, . . . , 1], including the sequence of all \u20180\u2019s and the sequence of all \u20181\u2019s. Our mtlr model computes the probability of observing the survival status sequence y = [y1, y2, . . . , ym] as:\nS\u0398(Y=[y1, y2, . . . , ym] | ~x ) = exp( \u2211m i=1 yi \u00d7 ~\u03b8i \u00b7 ~x)\u2211m\nk=0 exp(f\u0398(~x, k)) ,\nwhere \u0398 = [~\u03b81, . . . , ~\u03b8m], and f\u0398(~x, k) = \u2211m i=k+1( ~\u03b8i \u00b7 ~x) for 0 \u2264 k \u2264 m is the score of the sequence with the event occurring in the interval [tk, tk+1) before taking the logistic transform, with the boundary case f\u0398(~x,m) = 0 being the score for the sequence of all \u20180\u2019s. Given a dataset of n patients {~xr} with associated time of deaths {dr}, we find the optimal parameters (for the mtlr model) \u0398\u2217 as\n\u0398\u2217 = arg max \u0398 n\u2211 r=1 [ m\u2211 i=1 yj(dr)(~\u03b8i \u00b7~xr)\u2212 log m\u2211 k=0 exp f\u0398(~xr, k) ] \u2212 C 2 m\u2211 j=1 \u2016~\u03b8j\u20162 (28)\nwhere the C (for the regularizer) is found by an internal cross-validation process. There are many details here \u2013 e.g., to insure that the survival function starts at 1.0, and decreases monotonically and smoothly until reaching 0.0 for the final time point; to deal appropriately with censored patients; to decide how many time points to consider (m); and to minimize the risk of overfitting (by regularizing), and by selecting the relevant features. The paper by Yu et al. [57] provides the details.\nAfterwards, the isd-Predictor can use the learned mtlr-model \u0398\u2217 = [~\u03b81, . . . , ~\u03b8m] to produce a curve for a novel patient, who is represented as the vector of his/her covariates ~xj. This involves computing the m values, [f1(~xj, ~\u03b81), . . . , fr(~xj, ~\u03b8m)]; the running sum of these values is essentially the survival curve. We then use splines to produce a smooth monotonically decreasing curve \u2013 such as the 10 such curves shown in Figure 4 (bottomright).\n20 This paragraph is paraphrased from [57]; reprinted with permission of publisher/author. 21 Notice there are no \u20180\u2019s after a \u20181\u2019. This is the \u2018no zombie\u2019 rule: once someone dies, that person stays\ndead.\nC.3 Extension to Random Survival Forests (rsf-km)\nGiven a labeled dataset, a random survival forest learner will produce a set of T decision trees from a bootstrapped sample of the training data. It grows each tree recursively, starting from the root \u2013 identifying each position with the set of patients who arrive there. For each position, the growth stops if there are fewer than d0 deaths (where d0 is chosen via crossvalidation). Otherwise, it identifies the feature for this node: it first randomly draws a small random subset of the features to consider, then selects the feature (from that subset) that maximizes the difference in survival between two daughter nodes, based on the logrank test statistic (or some other chosen splitting rule). This becomes the rule of that node; and the learner then considers its two daughters, by splitting on the node\u2019s feature.\nEach leaf node in each tree corresponds to the set of training instances that reached that node. Given these learned trees, to classify a novel instance ~x, the random forest performance system will drop ~x into each of the trees, which will lead to T different leaf nodes, then use the T subsets of training instances to make a decision. Since each terminal node in the random survival forest contains a set of instances, we can use these instances to produce a Kaplan-Meier curve.22\nOnce the survival forest has been learned (with T trees), a patient is dropped into each of the T survival trees, leading to T leaf nodes, which produces T Kaplan-Meier curves. The rsf-km implementation then \u201caverages\u201d these curves, by taking a point-wise average across the curve for all time points \u2013 see Figure 16.23\nNote that the risk score generated by the median of the individual survival curves (produced here) does not necessarily result in the same ordering of patients as the risk scores of the original rsf implementation, which uses averaged cumulative hazards as a risk score. For this reason, we also applied the original rsf process to the datasets presented in the paper. We found that the Concordance scores were similar to that of rsf-km; mtlr still outperformed rsf on the datasets where mtlr outperformed rsf-km (data not shown).\n22 While the original paper does not consider survival curves, documentation https://kogalur.github. io/randomForestSRC/theory.html#section8.1 describing the inner workings of the R package states that survival curves in terminal nodes are created via the Kaplan-Meier estimator.\n23 The method for generating individual survival curves could not be found in any of the literature by the authors of random survival forests. Survival curves were reverse-engineered by the authors of this paper \u2013 all survival curves tested matched the methodology explained here."
                },
                {
                    "heading": "D Detailed Empirical Results",
                    "text": "This sub-appendix includes the tables that correspond to the figures given in Section 4.2. Further, Appendix D.4 provides the all p-values for the 1-Calibration tests."
                },
                {
                    "heading": "D.1 Concordance",
                    "text": "See Table 8 for the results corresponding to Figure 9."
                },
                {
                    "heading": "D.2 Brier Score",
                    "text": "See Table 9 for the results corresponding to Figure 10."
                },
                {
                    "heading": "D.3 Empirical Values of L1-Loss, and Variants",
                    "text": "Here we give the the results for the Margin-L1-loss (Table 10) as given in Figure 11. Additionally, we give results for the Uncensored L1-loss (Table 11) and the Log-Margin-L1-loss (Table 12).\nD.4 1-Calibration\nEach table corresponds to a different percentile of event times for each dataset. Moving down the 10th, 25th, 50th, 75th, and 90th percentiles are given. Bolded values indicate models which passed 1-Calibration (p > 0.05). The \u201cTotal\u201d column of each table gives the total number of datasets passed by each model \u2013 that is, the values in that columns correspond to Table 5."
                }
            ],
            "year": 2018,
            "references": [
                {
                    "title": "Survival and event history analysis: a process point of view",
                    "authors": [
                        "O. Aalen",
                        "O. Borgan",
                        "H. Gjessing"
                    ],
                    "venue": "Springer Science & Business Media,",
                    "year": 2008
                },
                {
                    "title": "Palliative performance scale (pps): a new tool",
                    "authors": [
                        "F. Anderson",
                        "G.M. Downing",
                        "J. Hill",
                        "L. Casorso",
                        "N. Lerch"
                    ],
                    "venue": "Journal of palliative care,",
                    "year": 1995
                },
                {
                    "title": "A novel learning algorithm to predict individual survival after liver transplantation for primary sclerosing cholangitis",
                    "authors": [
                        "A. Andres",
                        "A. Montano-Loza",
                        "R. Greiner",
                        "M. Uhlich",
                        "P. Jin",
                        "B. Hoehn",
                        "D. Bigam",
                        "J.A.M. Shapiro",
                        "N.M. Kneteman"
                    ],
                    "venue": "PloS one,",
                    "year": 2018
                },
                {
                    "title": "The probability integral transform and related results",
                    "authors": [
                        "J.E. Angus"
                    ],
                    "venue": "SIAM review,",
                    "year": 1994
                },
                {
                    "title": "A large sample study of the life table and product limit estimates under random censorship",
                    "authors": [
                        "N. Breslow",
                        "J. Crowley"
                    ],
                    "venue": "The Annals of Statistics,",
                    "year": 1974
                },
                {
                    "title": "Verification of weather forecasts",
                    "authors": [
                        "G.W. Brier",
                        "R.A. Allen"
                    ],
                    "venue": "In Compendium of meteorology,",
                    "year": 1951
                },
                {
                    "title": "Prediction of survival in terminal cancer patients in taiwan: constructing a prognostic scale",
                    "authors": [
                        "R.-B. Chuang",
                        "W.-Y. Hu",
                        "T.-Y. Chiu",
                        "C.-Y. Chen"
                    ],
                    "venue": "Journal of pain and symptom management,",
                    "year": 2004
                },
                {
                    "title": "Cumulative risk of breast cancer to age 70 years according to risk factor status: data from the nurses\u2019 health study",
                    "authors": [
                        "G.A. Colditz",
                        "B. Rosner"
                    ],
                    "venue": "American journal of epidemiology,",
                    "year": 2000
                },
                {
                    "title": "Validation studies for models projecting the risk of invasive and total breast cancer incidence",
                    "authors": [
                        "J.P. Costantino",
                        "M.H. Gail",
                        "D. Pee",
                        "S. Anderson",
                        "C.K. Redmond",
                        "J. Benichou",
                        "H.S. Wieand"
                    ],
                    "venue": "Journal of the National Cancer Institute,",
                    "year": 1999
                },
                {
                    "title": "Regression models and life-tables",
                    "authors": [
                        "D. Cox"
                    ],
                    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
                    "year": 1972
                },
                {
                    "title": "The rate of strong uniform consistency for the product-limit estimator",
                    "authors": [
                        "S. Cs\u00f6rg\u0150",
                        "L. Horv\u00e1th"
                    ],
                    "venue": "Zeitschrift fu\u0308r Wahrscheinlichkeitstheorie und verwandte Gebiete,",
                    "year": 1983
                },
                {
                    "title": "Evaluation of the performance of survival analysis models: discrimination and calibration measures",
                    "authors": [
                        "R. d\u2019Agostino",
                        "B.-H. Nam"
                    ],
                    "venue": "Handbook of statistics,",
                    "year": 2003
                },
                {
                    "title": "The comparison and evaluation of forecasters",
                    "authors": [
                        "M. DeGroot",
                        "S. Fienberg"
                    ],
                    "venue": "Journal of the Royal Statistical Society. Series D (The Statistician),",
                    "year": 1983
                },
                {
                    "title": "Time-dependent covariates in the cox proportional-hazards regression model",
                    "authors": [
                        "L.D. Fisher",
                        "D.Y. Lin"
                    ],
                    "venue": "Annual review of public health,",
                    "year": 1999
                },
                {
                    "title": "The performance of risk prediction models",
                    "authors": [
                        "T.A. Gerds",
                        "T. Cai",
                        "M. Schumacher"
                    ],
                    "venue": "Biometrical Journal: Journal of Mathematical Methods in Biosciences,",
                    "year": 2008
                },
                {
                    "title": "Consistent estimation of the expected brier score in general survival models with right-censored event times",
                    "authors": [
                        "T.A. Gerds",
                        "M. Schumacher"
                    ],
                    "venue": "Biometrical Journal,",
                    "year": 2006
                },
                {
                    "title": "Assessment and comparison of prognostic classification schemes for survival data",
                    "authors": [
                        "E. Graf",
                        "C. Schmoor",
                        "W. Sauerbrei",
                        "M. Schumacher"
                    ],
                    "venue": "Statistics in medicine,",
                    "year": 1999
                },
                {
                    "title": "Hosmer-Lemeshow goodness-of-fit test: Translations to the Cox Proportional Hazards Model",
                    "authors": [
                        "D. Guffey"
                    ],
                    "venue": "PhD thesis,",
                    "year": 2013
                },
                {
                    "title": "On representing the mean residual life in terms of the failure rate",
                    "authors": [
                        "R.C. Gupta",
                        "D.M. Bradley"
                    ],
                    "venue": "arXiv preprint math/0411297,",
                    "year": 2004
                },
                {
                    "title": "Prognosticating in patients with advanced cancer \u2013 observational study comparing the accuracy of clinicians\u2019 and patients\u2019 estimates of survival",
                    "authors": [
                        "B. Gwilliam",
                        "V. Keeley",
                        "C. Todd",
                        "C. Roberts",
                        "M. Gittins",
                        "L. Kelly",
                        "S. Barclay",
                        "P. Stone"
                    ],
                    "venue": "Annals of oncology,",
                    "year": 2012
                },
                {
                    "title": "Linear rank tests in survival analysis",
                    "authors": [
                        "D. Harrington"
                    ],
                    "venue": "Encyclopedia of Biostatistics,",
                    "year": 2005
                },
                {
                    "title": "A prognostic index in primary breast cancer",
                    "authors": [
                        "J. Haybittle",
                        "R. Blamey",
                        "C. Elston",
                        "J. Johnson",
                        "P. Doyle",
                        "F. Campbell",
                        "R. Nicholson",
                        "K. Griffiths"
                    ],
                    "venue": "British journal of cancer,",
                    "year": 1982
                },
                {
                    "title": "Survival model predictive accuracy and roc curves",
                    "authors": [
                        "P.J. Heagerty",
                        "Y. Zheng"
                    ],
                    "year": 2005
                },
                {
                    "title": "Goodness of fit tests for the multiple logistic regression model",
                    "authors": [
                        "D.W. Hosmer",
                        "S. Lemesbow"
                    ],
                    "venue": "Communications in statistics-Theory and Methods,",
                    "year": 1980
                },
                {
                    "title": "Applied survival analysis",
                    "authors": [
                        "D.W. Hosmer",
                        "S. Lemeshow",
                        "S. May"
                    ],
                    "year": 2011
                },
                {
                    "title": "Random Forests for Survival, Regression, and Classification (RF-SRC), 2018",
                    "authors": [
                        "H. Ishwaran",
                        "U. Kogalur"
                    ],
                    "venue": "R package version",
                    "year": 2018
                },
                {
                    "title": "Random survival forests",
                    "authors": [
                        "H. Ishwaran",
                        "U.B. Kogalur",
                        "E.H. Blackstone",
                        "M.S. Lauer"
                    ],
                    "venue": "The Annals of Applied Statistics,",
                    "year": 2008
                },
                {
                    "title": "The statistical analysis of failure time data",
                    "authors": [
                        "J. Kalbfleisch",
                        "R. Prentice"
                    ],
                    "venue": "Wiley New York:,",
                    "year": 2002
                },
                {
                    "title": "The model for end-stage liver disease",
                    "authors": [
                        "P.S. Kamath",
                        "W.R. Kim"
                    ],
                    "venue": "(meld). Hepatology,",
                    "year": 2007
                },
                {
                    "title": "Nonparametric estimation from incomplete observations",
                    "authors": [
                        "E. Kaplan",
                        "P. Meier"
                    ],
                    "venue": "Journal of the American Statistical Association,",
                    "year": 1958
                },
                {
                    "title": "Deepsurv: Personalized treatment recommender system using a cox proportional hazards deep neural network",
                    "authors": [
                        "J. Katzman",
                        "U. Shaham",
                        "J. Bates",
                        "A. Cloninger",
                        "T. Jiang",
                        "Y. Kluger"
                    ],
                    "venue": "arXiv preprint arXiv:1606.00931,",
                    "year": 2016
                },
                {
                    "title": "A multi-task learning formulation for survival analysis",
                    "authors": [
                        "Y. Li",
                        "J. Wang",
                        "J. Ye",
                        "C.K. Reddy"
                    ],
                    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
                    "year": 2016
                },
                {
                    "title": "Deep learning for patient-specific kidney graft survival analysis",
                    "authors": [
                        "M. Luck",
                        "T. Sylvain",
                        "H. Cardinal",
                        "A. Lodi",
                        "Y. Bengio"
                    ],
                    "venue": "arXiv preprint arXiv:1705.10245,",
                    "year": 2017
                },
                {
                    "title": "The palliative prognostic index: a scoring system for survival prediction of terminally ill cancer patients",
                    "authors": [
                        "T. Morita",
                        "J. Tsunoda",
                        "S. Inoue",
                        "S. Chihara"
                    ],
                    "venue": "Supportive Care in Cancer,",
                    "year": 1999
                },
                {
                    "title": "Scalar and vector partitions of the probability score: Part i. two-state situation",
                    "authors": [
                        "A.H. Murphy"
                    ],
                    "venue": "Journal of Applied Meteorology,",
                    "year": 1972
                },
                {
                    "title": "A new vector partition of the probability score",
                    "authors": [
                        "A.H. Murphy"
                    ],
                    "venue": "Journal of applied Meteorology,",
                    "year": 1973
                },
                {
                    "title": "A new palliative prognostic score: a first step for the staging of terminally ill cancer patients",
                    "authors": [
                        "M. Pirovano",
                        "M. Maltoni",
                        "O. Nanni",
                        "M. Marinari",
                        "M. Indelli",
                        "G. Zaninetta",
                        "V. Petrella",
                        "S. Barni",
                        "E. Zecca",
                        "E. Scarpi"
                    ],
                    "venue": "Journal of pain and symptom management,",
                    "year": 1999
                },
                {
                    "title": "Deep survival analysis",
                    "authors": [
                        "R. Ranganath",
                        "A. Perotte",
                        "N. Elhadad",
                        "D. Blei"
                    ],
                    "venue": "arXiv preprint arXiv:1608.02158,",
                    "year": 2016
                },
                {
                    "title": "The use of a simple likert scale to measure quality of life in brain tumor patients",
                    "authors": [
                        "M.P. Rogers",
                        "J. Orav",
                        "P.M. Black"
                    ],
                    "venue": "Journal of neuro-oncology,",
                    "year": 2001
                },
                {
                    "title": "On subjective probability forecasting",
                    "authors": [
                        "F. Sanders"
                    ],
                    "venue": "Journal of Applied Meteorology,",
                    "year": 1963
                },
                {
                    "title": "A support vector approach to censored targets",
                    "authors": [
                        "P. Shivaswamy",
                        "W. Chu",
                        "M. Jansche"
                    ],
                    "venue": "ICDM",
                    "year": 2007
                },
                {
                    "title": "Nonparametric survival analysis using bayesian additive regression trees (BART)",
                    "authors": [
                        "R.A. Sparapani",
                        "B.R. Logan",
                        "R.E. McCulloch",
                        "P.W. Laud"
                    ],
                    "venue": "Statistics in medicine,",
                    "year": 2016
                },
                {
                    "title": "On ranking in survival analysis: Bounds on the concordance index",
                    "authors": [
                        "H. Steck",
                        "B. Krishnapuram",
                        "C. Dehing-oberije",
                        "P. Lambin",
                        "V.C. Raykar"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2008
                },
                {
                    "title": "Assessing the performance of prediction models: a framework for some traditional and novel measures",
                    "authors": [
                        "E.W. Steyerberg",
                        "A.J. Vickers",
                        "N.R. Cook",
                        "T. Gerds",
                        "M. Gonen",
                        "N. Obuchowski",
                        "M.J. Pencina",
                        "M.W. Kattan"
                    ],
                    "venue": "Epidemiology (Cambridge, Mass.),",
                    "year": 2010
                },
                {
                    "title": "A Package for Survival",
                    "authors": [
                        "T.M. Therneau"
                    ],
                    "venue": "Analysis in S,",
                    "year": 2015
                },
                {
                    "title": "Modeling survival data: extending the Cox model",
                    "authors": [
                        "T.M. Therneau",
                        "P.M. Grambsch"
                    ],
                    "venue": "Springer Science & Business Media,",
                    "year": 2013
                },
                {
                    "title": "Cross-validated cox regression on microarray gene expression data",
                    "authors": [
                        "H.C. van Houwelingen",
                        "T. Bruinsma",
                        "A.A. Hart",
                        "L.J. van\u2019t Veer",
                        "L.F. Wessels"
                    ],
                    "venue": "Statistics in medicine,",
                    "year": 2006
                },
                {
                    "title": "A prediction algorithm for first onset of major depression in the general population: development and validation",
                    "authors": [
                        "J. Wang",
                        "J. Sareen",
                        "S. Patten",
                        "J. Bolton",
                        "N. Schmitz",
                        "A. Birney"
                    ],
                    "venue": "Journal of epidemiology and community health, pages jech\u20132013,",
                    "year": 2014
                },
                {
                    "title": "Machine learning for survival analysis: A survey",
                    "authors": [
                        "P. Wang",
                        "Y. Li",
                        "C.K. Reddy"
                    ],
                    "venue": "arXiv preprint arXiv:1708.04649,",
                    "year": 2017
                },
                {
                    "title": "Survival analysis with high-dimensional covariates",
                    "authors": [
                        "D.M. Witten",
                        "R. Tibshirani"
                    ],
                    "venue": "Statistical Methods in Medical Research,",
                    "year": 2010
                },
                {
                    "title": "Data Mining: Practical Machine Learning Tools and Techniques",
                    "authors": [
                        "I. Witten",
                        "E. Frank",
                        "M. Hall"
                    ],
                    "year": 2011
                },
                {
                    "title": "Investigating the effects of ties on measures of concordance",
                    "authors": [
                        "G. Yan",
                        "T. Greene"
                    ],
                    "venue": "Statistics in medicine,",
                    "year": 2008
                },
                {
                    "title": "A cocktail algorithm for solving the elastic net penalized cox\u2019s regression in high dimensions",
                    "authors": [
                        "Y. Yang",
                        "H. Zou"
                    ],
                    "venue": "Statistics and its Interface,",
                    "year": 2013
                },
                {
                    "title": "fastcox: Lasso and Elastic-Net Penalized Cox\u2019s Regression in High Dimensions Models using the Cocktail Algorithm, 2017",
                    "authors": [
                        "Y. Yang",
                        "H. Zou"
                    ],
                    "venue": "R package version 1.1.3",
                    "year": 2017
                }
            ],
            "id": "SP:b54a8d46da3ddd2d5902986345ef0805fa27d69e",
            "authors": [
                {
                    "name": "Humza Haider",
                    "affiliations": []
                },
                {
                    "name": "Bret Hoehn",
                    "affiliations": []
                },
                {
                    "name": "Sarah Davis",
                    "affiliations": []
                },
                {
                    "name": "Russell Greiner",
                    "affiliations": []
                }
            ],
            "abstractText": "An accurate model of a patient\u2019s individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (e.g., from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (e.g., the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model which provides an individual survival distribution which gives survival probabilities across all times \u2013 such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such \u201cindividual survival distribution\u201d (isd) models, and explains how they differ from standard models. It then discusses ways to evaluate such models \u2013 namely Concordance, 1-Calibration, Brier score, and various versions of L1-loss\u2013 and then motivates and defines a novel approach \u201cD-Calibration\u201d, which determines whether a model\u2019s probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several isd prediction tools, over a range of survival datasets.",
            "title": "Effective Ways to Build and Evaluate Individual Survival Distributions"
        }
    },
    "80004741": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25]. Unsupervised learning, where no task-specific labeling/feedback is provided on top of the input data, still remains one of the most difficult problems in machine learning but holds a bright future since a large number of tasks have little to no supervision.\nPopular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2]. In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) [34] to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling. Unsupervised learning models are often generative and supervised classifiers are often discriminative; generative model learning has been traditionally considered to be\n1\u2217 equal contribution.\na much harder task than discriminative learning [13], due to its intrinsic learning complexity, as well many assumptions and simplifications made about the underlying models.\nGenerative and discriminative models have traditionally been considered distinct and complementary to each other. In the past, connections have been built to combine the two families [13, 26, 40, 20]. In the presence of supervised information with a large amount of data, a discriminative classifier [24] exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity. Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39]. There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples. We will discuss in detail the relations and connections of our IGM\n1\nar X\niv :1\n70 4.\n07 82\n0v 1\n[ cs\n.C V\n] 2\n5 A\npr 2\n01 7\nwith these existing literature in the next section. In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.\nInspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating. There are a number of interesting properties about IGM that are worth highlighting:\n\u2022 CNN classifier as generator: No special condition on CNN architecture is needed in IGM and many existing CNN classifiers can be directly made into generators, if trained properly.\n\u2022 End-to-end self-evaluation and learning: Perform end-toend \u201cintrospective learning\u201d to self-classify between synthesized samples (pseudo-negatives) and the training data, followed by direct discriminative learning, to approach the target distribution.\n\u2022 Integrated unsupervised/supervised learning: Unsupervised and supervised learning can be carried out under similar pipelines, differing in the absence or presence of initial negative samples.\n\u2022 All backpropagation: Our synthesis-by-classification algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the classification stage for the CNN parameters.\n\u2022 Model-based anysize-image-generation: Since our generative modeling models the input image, we are able to train on images of a size and generate an image of a larger size while maintaining the coherence for the entire image.\n\u2022 Agnosticity to various vision applications: Due to its intrinsic modeling power being at the same time generative and discriminative, IGM can be adopted to many applications; under the same pipeline, we show a number of vision tasks in the experiments, including texture modeling, artistic style transference, face modeling, and semi-supervised image classification in this paper. Supervised classification cases of an algorithm in the same introspective learning family can been seen in [21]."
                },
                {
                    "heading": "2. Significance and related work",
                    "text": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44]. It builds on top of convolutional neural networks\n[25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14]. The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM. More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d). Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].\nThe history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19]. Each of these pioneering works points to some promising direction to unsupervised generative modeling. However the modeling power of these existing frameworks is still somewhat limited in computational and/or representational aspects. In addition, not too many of them sufficiently explore the power of discriminative modeling. Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41]. The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework. The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.\nRelationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution. Our IGM algorithm takes inspiration from GDL, but we also observe a number of limitations in GDL that will be overcome by IGM: GDL uses manually specified feature types (histograms and Haar filters), which are fairly limited by today\u2019s standard; the sampling process in GDL, based on Markov chain Monte Carlo (MCMC), is a big computational bottleneck; the experimental results for image modeling and classification were not satisfactory. To summarize, the main differences between GDL and IGM include: \u2022 The adoption of convolutional networks in IGM results in a\nsignificant boost to feature learning.\n\u2022 Introducing backpropagation to the synthesis/sampling process in IGM makes a fundamental improvement to the sam-\npling process in GDL that is otherwise slow and impractical.\n\u2022 An alternative algorithm, namely IGM-single (see Fig. 4), is additionally proposed to maintain a single classifier for IGM.\n\u2022 Higher quality results for image modeling are demonstrated in IGM.\nComparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM. We summarize the key differences between IGM and GAN. Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.\n\u2022 Unified generator/discriminator vs. separate generator and discriminator. IGM maintains a single model that is simultaneously a generator and a discriminator. The IGM generator therefore has self-awareness \u2014 being able to self-evaluate the difference between its generated samples (called pseudonegatives) w.r.t. the training data, followed by a direct CNN classifier training. GAN instead creates two convolutional networks, a generator and a discriminator.\n\u2022 Training. Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train [1]. IGM carries out a straightforward use of backpropagation in both the sampling and the classifier training stage, making the learning process direct. For example, for the textures shown in the experiments Fig. 2 and Fig. 6, all results by IGM are obtained under the identical setting without hyper-parameter tuning.\n\u2022 Modeling. The generator in GAN is a mapping from the features to the images. IGM directly models the underlying statistics of an image with an efficient sampling/inference process, which makes IGM flexible. For example, we are able to conduct model-based-anysize-generation in the texture modeling task by directly maintaining the underlying statistics of the entire image.\n\u2022 Speed. GAN performs a forward pass to reconstruct an image, which is generally faster than IGM where synthesis is carried out using backpropagation. IGM is still practically feasible since it takes only about 1\u2212 2 seconds to synthesize an image of size 64 \u00d7 64 and around 30 seconds to synthesize a texture image of size 320\u00d7 200 excluding the time to load the models.\n\u2022 Model size. Since a cascade of CNN classifiers (60 \u2212 200) are included in a single IGM model, IGM has a much larger model complexity than GAN. This is an advantage of GAN over IGM. Our alternative IGM-single model maintains a single CNN classifier but its generative power is worse than those of IGM and GAN.\nRelationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power. a). IGM focuses on the generator side studying its image construction capability. b). IGM consists of a sequence of cascading classifiers, whereas ICL is only composed of a single\nclassifier. Essentially, ICL is similar to IGM-single, with a small difference in the absence/presence of given negative samples. The generative modeling aspect of ICL/IGMsingle is not as competitive as IGM though. c). In ICL, a formulation for training a softmax multi-class classification was proposed, which is not in IGM. d). In addition, ICL focuses on single image patch, whereas IGM is able to model/synthesize an arbitrary size of image. A number of important image modeling tasks, including texture modeling, style transferring, face modeling, and semi-supervised learning are demonstrated here, which are not covered in ICL [21]."
                },
                {
                    "heading": "3. Method",
                    "text": "We describe below the introspective generative modeling (IGM) algorithm. We discuss the main formulation first, which bears some level of similarity to GDL [39]. However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power. The enhanced modeling power comes mainly from CNNs due to its end-to-end learning with automatic feature learning and tuning when backpropagating on the network parameters; enhanced computational power also largely from CNNs due to a natural implementation of sampling by backpropagating on the input image. GDL is similar to IGM (see Fig. 3), but IGM-single (see Fig. 4) maintains a single CNN as opposed to having a sequence of classifiers in both GDL and IGM. We motivate the formulation of IGM from the Bayes theory, similar to GDL."
                },
                {
                    "heading": "3.1. Formulation",
                    "text": "We start the discussion by borrowing notation from [39]. Suppose we are given a set of training images (patches): S = {xi, i = 1..n}. We focus on patch-based input first and let x \u2208 Rm be a data sample (an image patch of size say m = 64 \u00d7 64). We adopt the pseudo-negative concept defined in [39] and define class labels y \u2208 {\u22121,+1}, indicating x being a negative or a positive sample. Here we assume the positive samples with label y = +1 are the patterns/targets we want to study. A generative model computes for p(y,x) = p(x|y)p(y), which captures the underlying generation process of x for class y. A discriminative classifier instead computes p(y|x). Under the Bayes rule, similar to the motivation in [39]:\np(x|y = +1) = p(y = +1|x)p(y = \u22121) p(y = \u22121|x)p(y = +1) p(x|y = \u22121),\n(1) which can be further simplified when assuming equal priors p(y = +1) = p(y = \u22121):\np(x|y = +1) = p(y = +1|x) 1\u2212 p(y = +1|x) p(x|y = \u22121). (2)\nBased on Eq. (2), a generative model for the positive samples (patterns of interest) p(x|y = +1) can be fully represented by a generative model for the negatives p(x|y = \u22121) and a discriminative classifier p(y = +1|x), if both p(x|y = \u22121) and p(y = +1|x) can be accurately obtained/learned. However, this seemingly intriguing property is, in a way, a chicken-and-egg problem. To faithfully learn the positive patterns p(x|y = +1), we need to have a representative p(x|y = \u22121), which is equally difficult, if not more. For clarity, we now use p\u2212(x) to represent p(x|y = \u22121). In the GDL algorithm [39], a solution was given to learning p(x|y = +1) by using an iterative process starting from an initial reference distribution of the negatives p\u22120 (x), e.g. a Gaussian distribution U(x) on the entire space of x \u2208 Rm:\np\u22120 (x) = U(x)\np\u2212t (x) = 1\nZt qt(y = +1|x) qt(y = \u22121|x) \u00b7 p\u2212t\u22121(x), t = 1..T (3)\nwhere Zt = \u222b qt(y=+1|x)\nqt(y=\u22121|x)p \u2212 t\u22121(x)dx. Our hope is to grad-\nually learn p\u2212t (x) by following this iterative process of Eq. 3:\np\u2212t (x) t=\u221e\u2192 p(x|y = +1), (4)\nsuch that the samples drawn x \u223c p\u2212t (x) become indistinguishable from the given training samples. The samples drawn from x \u223c p\u2212t (x) are called pseudo-negatives, following a definition in [39]. Next, we present the realization of Eq. 3, namely IGM (consisting of a sequence of CNN classifiers and see Fig. 3) and additionally IGM-single (maintaining a single CNN classifier and see Fig. 4). 3.2. IGM Training\nNext, we present our introspective generative modeling algorithm using a sequence of classifiers, called IGM. The given (unlabeled) training set is defined as S = {xi, i = 1..n}, which is turned into S+ = {(xi, yi = +1), i = 1..n} within the discriminative setting. We start from an initial pseudo-negative set\nS0\u2212 = {(xi,\u22121), i = 1, ..., l}\nwhere xi \u223c p\u22120 (x) = U(x) which is a Gaussian distribution. A working set for t = 1..T\nSt\u22121\u2212 = {(xi,\u22121), i = 1, ..., l}.\nthen includes the pseudo-negative samples self-generated from each round. l indicates the number of pseudonegatives generated at each round. We carry out learning with t = 1...T to iteratively obtain\nqt(y = +1|x), qt(y = \u22121|x) (5)\nby updating classifier Ct on S+ \u222a St\u22121\u2212 . The reason for using q is because it is an approximation to the true p due\nAlgorithm 1 Outline of the IGM algorithm. Input: Given a set of training data S+ = {(xi, yi = +1), i = 1..n} with x \u2208 <m. Initialization: obtain an initial distribution e.g. Gaussian for the pseudo-negative samples: p\u22120 (x) = U(x). Create S 0 \u2212 =\n{(xi,\u22121), i = 1, ..., l} with xi \u223c p\u22120 (x) For t=1..T 1. Classification-step: Train CNN classifier Ct on S+ \u222a St\u22121\u2212 , resulting in qt(y = +1|x). 2. Update the model: p\u2212t (x) = 1Zt qt(y=+1|x) qt(y=\u22121|x)p \u2212 t\u22121(x). 3. Synthesis-step: sample l pseudo-negative samples xi \u223c p\u2212t (x), i = 1, ..., l from the current model p \u2212 t (x) using a variational sampling procedure (backpropagation on the input) to obtain St\u2212 = {(xi,\u22121), i = 1, ..., l}. 4. t \u2190 t + 1 and go back to step 1 until convergence (e.g. indistinguishable to the given training samples). End\nto limited samples drawn in <m. At each time t, we then compute\np\u2212t (x) = 1\nZt qt(y = +1|x) qt(y = \u22121|x) p\u2212t\u22121(x), (6)\nwhere Zt = \u222b qt(y=+1|x)\nqt(y=\u22121|x)p \u2212 t\u22121(x)dx. We draw new sam-ples\nxi \u223c p\u2212t (x) to have the pseudo-negative set:\nSt\u2212 = {(xi,\u22121), i = 1, ..., l}. (7)\nAlgorithm 1 describes the learning process. The pipeline of IGM is shown in Fig. 3, which consists of (1) a synthesis step and (2) a classification step. A sequence of CNN classifiers is progressively learned. With the pseudo-negatives being gradually generated, the classification boundary gets tightened and approaches the target distribution."
                },
                {
                    "heading": "3.2.1 Classification-step",
                    "text": "The classification-step can be viewed as training a normal classifier on the training set S+ \u222a St\u2212 where S+ = {(xi, yi = +1), i = 1..n}. St\u2212 = {(xi,\u22121), i = 1, ..., l} for t \u2265 1. We use a CNN as our base classifier. When training a classifier Ct on S+ \u222a St\u2212, we denote the parameters to be learned in Ct by a high-dimensional vector Wt = (w (0) t ,w (1) t ) which might consist of millions of parameters. w(1)t denotes the weights on the top layer combining the features \u03c6(x;w(0)t ) and w (0) t carries all the internal representations. Without loss of generality, we assume a sigmoid function for the discriminative probability\nqt(y|x;Wt) = 1/(1 + exp{\u2212y < w(1)t , \u03c6(x;w (0) t ) >}).\n(8) Both w(1)t and w (0) t can be learned by the standard stochastic gradient descent algorithm via backpropagation to minimize a cross-entropy loss with an additional term on the\npseudo-negatives: L(Wt) = \u2212 i=1..n\u2211\n(xi,+1)\u2208S+\nln qt(+1|xi;Wt)\u2212 i=1..l\u2211\n(xi,\u22121)\u2208St\u2212\nln qt(\u22121|xi;Wt)"
                },
                {
                    "heading": "3.2.2 Synthesis-step",
                    "text": "In the classification step, we obtain qt(y|x;Wt) which is then used to update p\u2212t (x) according to Eq. (6):\np\u2212t (x) = t\u220f a=1 1 Za qa(y = +1|x;Wa) qa(y = \u22121|x;Wa) p\u22120 (x). (9)\nIn the synthesis-step, our goal is to draw fair samples from p\u2212t (x). The sampling process is carried out by backpropagation, but now we need to go through a sequence classifiers by using 1Za qa(y=+1|x;Wa) qa(y=\u22121|x;Wa) , a = 1..t. This can be timeconsuming. In practice, we can simply perform backpropagation on the previous set St\u22121\u2212 by taking 1 Zt qt(y=+1|x;Wt) qt(y=\u22121|x;Wt) . Therefore, generating pseudo-negative samples when training IGM does not need a large overhead. Additional Gaussian noise can be added to the stochastic gradient as in [43] but we did not observe a big difference in the quality of samples in practice. This is probably due to the equivalent class [45] where the probability mass is widely distributed over an extremely large image space. Sampling strategies\nIn [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow. Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis. Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29]. When conducting experiments, some alternative sampling schemes using SGD can be applied: i) earlystopping once x becomes positive (or after a small fixed number of steps); or ii) sampling with equilibrium after long steps. We found early-stopping effective and efficient, which can be viewed as contrastive divergence [7] where a short Markov chain is simulated.\nNote that the partition function (normalization) Za is a constant that is not dependent on the sample x. Let\ngt(x) = qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) = exp{< w(1)t , \u03c6(x;w (0) t ) >}, (10) and take its ln, which is nicely turned into the logit of qt(y = +1|x;Wt)\nln gt(x) =< w (1) t , \u03c6(x;w (0) t ) > . (11)\nStarting from a x drawn from p\u2212t\u22121(x), we directly increase < w\n(1) t , \u03c6(x;w (0) t ) > using stochastic gradient ascent on x via backpropagation which allows us to obtain fair samples\nsubject to Eq. (9). A noise can be injected as in [43] when performing SGD sampling. Overall model The overall IGM model after T stages of training becomes:\np\u2212T (x) = 1\nZ T\u220f t=1 qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) p\u22120 (x)\n= 1\nZ T\u220f t=1 exp{< w(1)t , \u03c6(x;w (0) t ) >}p\u22120 (x),\n(12) where Z = \u222b \u220fT\nt=1 exp{< w (1) t , \u03c6(x;w (0) t ) >}p\u22120 (x)dx.\nIGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39]. Theorem 1 KL[p(x|y = +1)||p\u2212t+1(x)] \u2264 KL[p(x|y = +1)||p\u2212t (x)] whereKL denotes the Kullback-Leibler divergences, and p(x|y = +1) \u2261 p+(x)."
                },
                {
                    "heading": "3.3. An alternative: IGM-single",
                    "text": "We briefly present the IGM-single algorithm, which is similar to the introspective classifier learning algorithm [21] with the difference without the presence of input negative samples. The pipeline of IGM-single is shown in Fig. 4. A key aspect here is that we maintain a single CNN classifier throughout the entire learning process.\nIn the classification step, we obtain qt(y|x;Wt) (similar as Eq. 8) which is then used to update p\u2212t (x) according to Eq. (13):\np\u2212t (x) = 1\nZt qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) p\u22120 (x). (13)\nIn the synthesis-step, we draw samples from p\u2212t (x). Overall model The overall IGM-single model after T stages of training becomes:\np\u2212T (x) = 1\nZT exp{< w(1)T , \u03c6(x;w (0) T ) >}p \u2212 0 (x), (14) where ZT = \u222b exp{< w(1)T , \u03c6(x;w (0) T ) >}p \u2212 0 (x)dx."
                },
                {
                    "heading": "3.4. Model-based anysize-image-generation",
                    "text": "pT (I) \u221d T\u220f\nt=1 m1\u220f i=1 m2\u220f j=1 gt(x(i, j))p \u2212 0 (x(i, j)) (15)\nwhere gt(x(i, j)) (see Eq. 10) denotes the score of the patch of size e.g. 64 \u00d7 64 for x(i, j) under the discriminator at round t. Fig. 5 gives an illustration for one round of sampling. This allows us to synthesize much larger images by being able to enforce the coherence and interactions surrounding a particular pixel. In practice, we add stochasticity and efficiency to the synthesis process by randomly sampling these set of patches."
                },
                {
                    "heading": "4. Experiments",
                    "text": "We evaluate both IGM and IGM-single. In each method, we adopt the discriminator architecture of [33] which involves an input size of 64x64x3 in the RGB colorspace, four convolutional layers using 5 \u00d7 5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively. We include batch normalization after each convolutional layer (excluding the first) and use leaky ReLU activations with leak slope 0.2. The classification layer flattens the input and finally feeds it into a sigmoid activation.\nThis serves as the discriminator for the 64 \u00d7 64 patches we extract from the training image(s). Note that is is a gen-\neral purpose architecture with no modifications made for a specific task in mind.\nIn texture synthesis and artistic style, we make use of the \u201canysize-image-generation\u201d architecture by adding a \u201chead\u201d to the network that, at each forward pass of the network, randomly selects some number (equal to the desired batch size) of 64 \u00d7 64 random patches (possibly overlapping) from the full sized images and passes them to the discriminator. This allows us to retain the whole space of patches within a training image rather than select some subset of them in advance to use during training."
                },
                {
                    "heading": "4.1. Texture synthesis",
                    "text": "Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32]. Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11]. We train similar textures to [41]. Each source texture is resized to 256 \u00d7 256, used as the single \u201cpositive\u201d example in the training set and a set of 200 negative examples are initially sampled from a normal distribution with \u03c3 = 0.3 of size 320 \u00d7 320 after adding padding of 32 pixels to each spatial dimension of the image to ensure each pixel of the 256\u00d7256 center has equal probability of being extracted in some patch. 1000 patches are extracted randomly across the training images and fed to the discriminator at each forward pass of the network (during training and synthesis stages) from a batch size of 100 images \u2014 50 random positives and negatives when training and 100 pseudo-negatives during synthesis. At each round, our classifier is finetuned using stochastic gradient descent with learning rate 0.01 from the previous round\u2019s classifier after the augmentation of the negative set with the 200\n320\u00d7 320 synthesized pseudo-negatives. Pseudo-negatives from more recent rounds are chosen in mini-batches with higher probability than those of earlier rounds in order to ensure the discriminator learns from its most recent mistakes as well as provide for more efficient training when the set of accumulated negatives has grown large in later rounds. During the synthesis stage, pseudo-negatives are synthesized using the previous round\u2019s pseudo-negatives as their initialization. Adam is used with a learning rate of 0.1 and \u03b2 = 0.5 and stops early when the average probability of the patches under the discriminator is more likely than not to be a positive across some window of previous steps, usually 20, in order to reduce variance. This allows us to, on average, cross the decision boundary of the current iteration of the discriminator. We find this sampling strategy to attain a good balance in effectiveness and efficiency. Empirically, we find training the networks for 70 rounds to provide good results in terms of synthesis and distillation of the model\u2019s knowledge.\nNew textures are synthesized under IGM by: sampling from the same distribution used initially during training (in our case, normally distributed with \u03c3 = 0.3), performing backpropagation of the synthesis using the saved parameters of the networks for each round, and feeding the resulting partial synthesis to the next round. The same early stopping criterion is used as outlined during training, however, the number of patches is dialed down to match the number being synthesized. We use about 10 patches per image when synthesizing a 256 \u00d7 256 image since this matches the average number of patches extracted per image during training. Making the number of patches much larger than corresponding ratio used in the training process has shown to generate images of lower quality and diversity.\nUnder IGM-single there is a single network, and thus only a single round of synthesis takes place to transform the initial noise to a high probability texture.\nConsidering the results in Fig. 2 and 6, we see that IGM generates images of similar quality to [41], however, it is usually more faithful to the structure of the input images. In Fig. 2, the \u201cbricks\u201d texture synthesized by IGM is very strict about the grout lines being straight to ensure the bricks are rectilinear. Similarly, in Fig. 6, the \u201cforest\u201d texture preserves continuity but allows for some of the variation in angle and path that the tree trunks take. The \u201cdiamond\u201d texture is reflective of the grid-like pattern seen from the input image and does not allow for overlap or differently sized diamonds. In the bottom row of \u201cpebbles\u201d, the resulting synthesis captures the size of the pebbles seen in the input image as well as the variation in color and shading."
                },
                {
                    "heading": "4.2. Artistic style transfer",
                    "text": "We also attempt to transfer artistic style as shown in [14]. However, our architecture makes no use of additional networks for content and texture transferring task uses a loss\nfunctions during synthesis to minimize\n\u2212 ln p(Istyle | I) \u221d \u03b1\u00b7||Istyle\u2212I||2\u2212(1\u2212\u03b1)\u00b7ln p\u2212style(Istyle),\nwhere I is an input image and Istyle is its stylized version, and p\u2212style(I) denotes the model learned from the training style image. We include a L2 fidelity term during synthesis, weighted by a parameter \u03b1, making Istyle not too far away from the input image I. We choose \u03b1 = 0.3 and average the L2 difference between the original content image and the current stylized image at each step of synthesis. Two examples of the artistic style transfer are shown in Fig. 7."
                },
                {
                    "heading": "4.3. Face modeling",
                    "text": "The CelebA dataset [28] is used in our face modeling experiment, which consists of 202, 599 face images. We crop the center 64 \u00d7 64 patches in these images as our positive examples. For the classification step, we use stochastic gradient descent with learning rate 0.01 and a batch size of 100 images, which contains 50 random positives and 50 random negatives. For the synthesis step, we use the Adam optimizer with learning rate 0.01 and \u03b2 = 0.5 and stop early when the pseudo-negatives cross the decision boundary. In Fig. 8, we show some face examples generated by our model and the DCGAN model."
                },
                {
                    "heading": "4.4. SVHN unsupervised learning",
                    "text": "The SVHN [31] dataset consists of color images of house numbers collected by Google Street View. The training set consists of 73, 257 images, the extra set consists of 531, 131 images, and the test set has 26, 032 images. The images are of the size 32 \u00d7 32. We combine the training and extra set as our positive examples for unsupervised learning. Following the same settings in the face modeling experiments, our IGM model can generate examples as shown in Fig. 9."
                },
                {
                    "heading": "4.5. SVHN semi-supervised classification",
                    "text": "We perform the semi-supervised classification experiment by following the procedure outlined in [33]. We first train a model on the SVHN training and extra set in an unsupervised way, as in Section 4.4. Then, we train an L2-SVM on the learned representations of this model. The features from the last three convolutional layers are concatenated to form a 14336-dimensional feature vector. A 10, 000 example held-out validation set is taken from the training set and is used for model selection. The SVM classifier is trained on 1000 examples taken at random from the remainder of the training set. The test error rate is averaged over 100 different SVMs trained on random 1000-example training sets. Within the same setting, our IGM model achieves the test error rate of 36.44 \u00b1 0.72% and the DCGAN model achieves 33.13 \u00b1 0.83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset)."
                },
                {
                    "heading": "5. Conclusion",
                    "text": "Introspective generative modeling points to an encouraging direction for unsupervised image modeling that capitalizes on the power of discriminative deep convolutional neural networks. It can be adopted for a wide range of problems in computer vision and machine learning."
                },
                {
                    "heading": "6. Acknowledgement",
                    "text": "This work is supported by NSF IIS-1618477 and a Northrop Grumman Contextual Robotics grant. We thank Saining Xie, Jun-Yan Zhu, Jiajun Wu, Stella Yu, and Alexei Efros for helpful discussions."
                }
            ],
            "year": 2017,
            "references": [
                {
                    "title": "Wasserstein gan",
                    "authors": [
                        "M. Arjovsky",
                        "S. Chintala",
                        "L. Bottou"
                    ],
                    "venue": "arXiv preprint arXiv:1701.07875,",
                    "year": 2017
                },
                {
                    "title": "Autoencoders, unsupervised learning, and deep architectures",
                    "authors": [
                        "P. Baldi"
                    ],
                    "venue": "ICML unsupervised and transfer learning, 27,",
                    "year": 2012
                },
                {
                    "title": "Scaling learning algorithms towards ai",
                    "authors": [
                        "Y. Bengio",
                        "Y. LeCun"
                    ],
                    "venue": "Large-scale kernel machines, 34(5),",
                    "year": 2007
                },
                {
                    "title": "Latent dirichlet allocation",
                    "authors": [
                        "D.M. Blei",
                        "A.Y. Ng",
                        "M.I. Jordan"
                    ],
                    "venue": "Journal of machine Learning research, 3(Jan):993\u2013 1022,",
                    "year": 2003
                },
                {
                    "title": "Random Forests",
                    "authors": [
                        "L. Breiman"
                    ],
                    "venue": "Machine Learning,",
                    "year": 2001
                },
                {
                    "title": "Neural photo editing with introspective adversarial networks",
                    "authors": [
                        "A. Brock",
                        "T. Lim",
                        "J. Ritchie",
                        "N. Weston"
                    ],
                    "venue": "arXiv preprint arXiv:1609.07093,",
                    "year": 2016
                },
                {
                    "title": "On contrastive divergence learning",
                    "authors": [
                        "M.A. Carreira-Perpinan",
                        "G. Hinton"
                    ],
                    "venue": "AISTATS, volume 10, pages 33\u201340. Citeseer,",
                    "year": 2005
                },
                {
                    "title": "Stochastic gradient hamiltonian monte carlo",
                    "authors": [
                        "T. Chen",
                        "E.B. Fox",
                        "C. Guestrin"
                    ],
                    "venue": "ICML,",
                    "year": 2014
                },
                {
                    "title": "Inducing features of random fields",
                    "authors": [
                        "S. Della Pietra",
                        "V. Della Pietra",
                        "J. Lafferty"
                    ],
                    "venue": "IEEE transactions on pattern analysis and machine intelligence, 19(4):380\u2013393,",
                    "year": 1997
                },
                {
                    "title": "Pattern Classification",
                    "authors": [
                        "R.O. Duda",
                        "P.E. Hart",
                        "D.G. Stork"
                    ],
                    "venue": "2nd edition,",
                    "year": 2000
                },
                {
                    "title": "Texture synthesis by nonparametric sampling",
                    "authors": [
                        "A.A. Efros",
                        "T.K. Leung"
                    ],
                    "venue": "ICCV,",
                    "year": 1999
                },
                {
                    "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
                    "authors": [
                        "Y. Freund",
                        "R.E. Schapire"
                    ],
                    "venue": "J. of Comp. and Sys. Sci., 55(1),",
                    "year": 1997
                },
                {
                    "title": "The elements of statistical learning, volume 1",
                    "authors": [
                        "J. Friedman",
                        "T. Hastie",
                        "R. Tibshirani"
                    ],
                    "venue": "Springer series in statistics,",
                    "year": 2001
                },
                {
                    "title": "A neural algorithm of artistic style",
                    "authors": [
                        "L.A. Gatys",
                        "A.S. Ecker",
                        "M. Bethge"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Generative adversarial nets",
                    "authors": [
                        "I. Goodfellow",
                        "J. Pouget-Abadie",
                        "M. Mirza",
                        "B. Xu",
                        "D. Warde-Farley",
                        "S. Ozair",
                        "A. Courville",
                        "Y. Bengio"
                    ],
                    "venue": "NIPS,",
                    "year": 2014
                },
                {
                    "title": "General pattern theory-A mathematical study of regular structures",
                    "authors": [
                        "U. Grenander"
                    ],
                    "venue": "Clarendon Press,",
                    "year": 1993
                },
                {
                    "title": "Pyramid-based texture analysis/synthesis",
                    "authors": [
                        "D.J. Heeger",
                        "J.R. Bergen"
                    ],
                    "venue": "SIGGRAPH, pages 229\u2013238,",
                    "year": 1995
                },
                {
                    "title": "The\u201d wake-sleep\u201d algorithm for unsupervised neural networks",
                    "authors": [
                        "G.E. Hinton",
                        "P. Dayan",
                        "B.J. Frey",
                        "R.M. Neal"
                    ],
                    "venue": "Science, 268(5214):1158,",
                    "year": 1995
                },
                {
                    "title": "A fast learning algorithm for deep belief nets",
                    "authors": [
                        "G.E. Hinton",
                        "S. Osindero",
                        "Y.W. Teh"
                    ],
                    "venue": "Neural computation, 18:1527\u2013 1554,",
                    "year": 2006
                },
                {
                    "title": "Machine learning: discriminative and generative",
                    "authors": [
                        "T. Jebara"
                    ],
                    "year": 2012
                },
                {
                    "title": "Introspective classifier learning: Empower generatively",
                    "authors": [
                        "L. Jin",
                        "J. Lazarow",
                        "Z. Tu"
                    ],
                    "venue": "arXiv preprint arXiv:,",
                    "year": 2017
                },
                {
                    "title": "Principal component analysis",
                    "authors": [
                        "I. Jolliffe"
                    ],
                    "venue": "Wiley Online Library,",
                    "year": 2002
                },
                {
                    "title": "DCGAN-tensorflow",
                    "authors": [
                        "T. Kim"
                    ],
                    "venue": "https://github.com/ carpedm20/DCGAN-tensorflow,",
                    "year": 2016
                },
                {
                    "title": "ImageNet Classification with Deep Convolutional Neural Networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS,",
                    "year": 2012
                },
                {
                    "title": "Backpropagation applied to handwritten zip code recognition",
                    "authors": [
                        "Y. LeCun",
                        "B. Boser",
                        "J.S. Denker",
                        "D. Henderson",
                        "R. Howard",
                        "W. Hubbard",
                        "L. Jackel"
                    ],
                    "venue": "Neural Computation,",
                    "year": 1989
                },
                {
                    "title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators",
                    "authors": [
                        "P. Liang",
                        "M.I. Jordan"
                    ],
                    "venue": "ICML,",
                    "year": 2008
                },
                {
                    "title": "Monte Carlo strategies in scientific computing",
                    "authors": [
                        "J.S. Liu"
                    ],
                    "venue": "Springer Science & Business Media,",
                    "year": 2008
                },
                {
                    "title": "Deep learning face attributes in the wild",
                    "authors": [
                        "Z. Liu",
                        "P. Luo",
                        "X. Wang",
                        "X. Tang"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                },
                {
                    "title": "Stochastic gradient descent as approximate bayesian inference",
                    "authors": [
                        "S. Mandt",
                        "M.D. Hoffman",
                        "D.M. Blei"
                    ],
                    "venue": "arXiv preprint arXiv:1704.04289,",
                    "year": 2017
                },
                {
                    "title": "Deepdream - a code example for visualizing neural networks",
                    "authors": [
                        "A. Mordvintsev",
                        "C. Olah",
                        "M. Tyka"
                    ],
                    "venue": "Google Research,",
                    "year": 2015
                },
                {
                    "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
                    "authors": [
                        "Y. Netzer",
                        "T. Wang",
                        "A. Coates",
                        "A. Bissacco",
                        "B. Wu",
                        "A.Y. Ng"
                    ],
                    "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
                    "year": 2011
                },
                {
                    "title": "A parametric texture model based on joint statistics of complex wavelet coefficients",
                    "authors": [
                        "J. Portilla",
                        "E.P. Simoncelli"
                    ],
                    "venue": "Int\u2019l j. of computer vision, 40(1):49\u201370,",
                    "year": 2000
                },
                {
                    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                    "authors": [
                        "A. Radford",
                        "L. Metz",
                        "S. Chintala"
                    ],
                    "venue": "arXiv:1511.06434,",
                    "year": 2015
                },
                {
                    "title": "Modeling by shortest data description",
                    "authors": [
                        "J. Rissanen"
                    ],
                    "venue": "Automatica, 14(5):465\u2013471,",
                    "year": 1978
                },
                {
                    "title": "Fields of experts: A framework for learning image priors",
                    "authors": [
                        "S. Roth",
                        "M.J. Black"
                    ],
                    "venue": "CVPR,",
                    "year": 2005
                },
                {
                    "title": "Improved techniques for training gans",
                    "authors": [
                        "T. Salimans",
                        "I. Goodfellow",
                        "W. Zaremba",
                        "V. Cheung",
                        "A. Radford",
                        "X. Chen"
                    ],
                    "venue": "arXiv preprint arXiv:1606.03498,",
                    "year": 2016
                },
                {
                    "title": "Normalized cuts and image segmentation",
                    "authors": [
                        "J. Shi",
                        "J. Malik"
                    ],
                    "venue": "PAMI, 22(8):888\u2013905,",
                    "year": 2000
                },
                {
                    "title": "Adagan: Boosting generative models",
                    "authors": [
                        "I. Tolstikhin",
                        "S. Gelly",
                        "O. Bousquet",
                        "C.-J. Simon-Gabriel",
                        "B. Sch\u00f6lkopf"
                    ],
                    "venue": "arXiv:1701.02386,",
                    "year": 2017
                },
                {
                    "title": "Learning generative models via discriminative approaches",
                    "authors": [
                        "Z. Tu"
                    ],
                    "venue": "CVPR,",
                    "year": 2007
                },
                {
                    "title": "Brain anatomical structure segmentation by hybrid discriminative/generative models",
                    "authors": [
                        "Z. Tu",
                        "K.L. Narr",
                        "P. Doll\u00e1r",
                        "I. Dinov",
                        "P.M. Thompson",
                        "A.W. Toga"
                    ],
                    "venue": "IEEE Tran. on Medical Imag.,",
                    "year": 2008
                },
                {
                    "title": "Texture networks: Feed-forward synthesis of textures and stylized images",
                    "authors": [
                        "D. Ulyanov",
                        "V. Lebedev",
                        "A. Vedaldi",
                        "V. Lempitsky"
                    ],
                    "venue": "ICML,",
                    "year": 2016
                },
                {
                    "title": "The nature of statistical learning theory",
                    "authors": [
                        "V.N. Vapnik"
                    ],
                    "venue": "Springer-Verlag New York, Inc.,",
                    "year": 1995
                },
                {
                    "title": "Bayesian learning via stochastic gradient langevin dynamics",
                    "authors": [
                        "M. Welling",
                        "Y.W. Teh"
                    ],
                    "venue": "ICML,",
                    "year": 2011
                },
                {
                    "title": "Self supervised boosting",
                    "authors": [
                        "M. Welling",
                        "R.S. Zemel",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS,",
                    "year": 2002
                },
                {
                    "title": "Equivalence of julesz ensembles and frame models",
                    "authors": [
                        "Y.N. Wu",
                        "S.C. Zhu",
                        "X. Liu"
                    ],
                    "venue": "International Journal of Computer Vision, 38(3),",
                    "year": 2000
                },
                {
                    "title": "Cooperative training of descriptor and generator networks",
                    "authors": [
                        "J. Xie",
                        "Y. Lu",
                        "S.-C. Zhu",
                        "Y.N. Wu"
                    ],
                    "venue": "arXiv:1609.09408,",
                    "year": 2016
                },
                {
                    "title": "A theory of generative convnet",
                    "authors": [
                        "J. Xie",
                        "Y. Lu",
                        "S.-C. Zhu",
                        "Y.N. Wu"
                    ],
                    "venue": "ICML,",
                    "year": 2016
                },
                {
                    "title": "Feature extraction from faces using deformable templates",
                    "authors": [
                        "A.L. Yuille",
                        "P.W. Hallinan",
                        "D.S. Cohen"
                    ],
                    "venue": "International journal of computer vision, 8(2):99\u2013111,",
                    "year": 1992
                },
                {
                    "title": "Vision as bayesian inference: analysis by synthesis",
                    "authors": [
                        "A.L. Yuille",
                        "D. Kersten"
                    ],
                    "venue": "Trends in cognitive sciences,",
                    "year": 2006
                },
                {
                    "title": "Energy-based generative adversarial network",
                    "authors": [
                        "J. Zhao",
                        "M. Mathieu",
                        "Y. LeCun"
                    ],
                    "venue": "arXiv:1609.03126,",
                    "year": 2016
                },
                {
                    "title": "Minimax entropy principle and its application to texture modeling",
                    "authors": [
                        "S.C. Zhu",
                        "Y.N. Wu",
                        "D. Mumford"
                    ],
                    "venue": "Neural Computation, 9(8):1627\u20131660,",
                    "year": 1997
                }
            ],
            "id": "SP:b04d4b1e8b510180726f49a66dbaaf23c9ef64a0",
            "authors": [
                {
                    "name": "Justin Lazarow",
                    "affiliations": []
                },
                {
                    "name": "Long Jin",
                    "affiliations": []
                },
                {
                    "name": "Zhuowen Tu",
                    "affiliations": []
                }
            ],
            "abstractText": "We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semisupervised learning. 1",
            "title": "Introspective Generative Modeling: Decide Discriminatively"
        }
    },
    "87827594": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "The machine learning community is well-practised at learning representations of data-points and sequences. A middle-ground between these two is representing, or summarizing, datasets - unordered collections of vectors, such as photos of a particular person, recordings of a given speaker or a document as a bag-of-words. Where these sets take the form of i.i.d samples from some distribution, such summaries are called statistics. We explore the idea of using neural networks to learn statistics and we refer to our approach as the neural statistician. The key result of our approach is a statistic network that takes as input a set of vectors and outputs a vector of summary statistics specifying a generative model of that set. The advantages of our approach are that it is:\n\u2022 Unsupervised: It provides principled and unsupervised way to learn summary statistics as a variational encoder of a generative model. \u2022 Data efficient: If one has a large number of small but related datasets, modelling the datasets jointly\nenables us to gain statistical strength. \u2022 Parameter Efficient: By using summary statistics instead of say categorical labellings of each\ndataset, we decouple the number of parameters of the model from the number of datasets. \u2022 Capable of small-shot learning: If the datasets correspond to examples from different classes, class\nembeddings, (summary statistics associated with examples from a class) allow us to handle new classes at test time."
                },
                {
                    "heading": "2 Problem Statement",
                    "text": "We are given datasets Di for i \u2208 I. Each dataset Di = {x1, . . . , xki} consists of a number of i.i.d samples from an associated distribution pi over Rn. The task can be split into learning and inference components. The learning component is to produce a generative model p\u0302i for each dataset Di. We assume there is a common underlying generative process p such that pi = p(\u00b7|ci) for ci \u2208 Rl drawn from p(c). We refer to c as the context. The inference component is to give an approximate posterior over the context q(c|D) for a given dataset produced by a statistic network.\nar X\niv :1\n60 6.\n02 18\n5v 1\n[ st\nat .M\nL ]\n7 J\nun 2\n01 6"
                },
                {
                    "heading": "3 Neural Statistician",
                    "text": "In order to exploit the assumption of a hierarchical generative process over datasets we will use a \u2018parameter-transfer approach\u2019 (see [25]) to extend the variational autoencoder model of Kingma and Welling [16]."
                },
                {
                    "heading": "3.1 Variational Autoencoder",
                    "text": "The variational autoencoder is a latent variable model p(x|z; \u03b8) with parameters \u03b8. For each observed x, a corresponding latent variable z is drawn from p(z) so that\np(x) = \u222b p(x|z; \u03b8)p(z) dz (1)\nThe generative parameters \u03b8 are learned by introducing a recognition network q(z|x;\u03c6) with parameters \u03c6. The recognition network gives an approximate posterior over the latent variables that can then be used to give a variational lower bound on the log-likelihood\nLx = Eq(z|x,\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|x;\u03c6)\u2016p(z)) (2)\nWe can then optimize Lx with respect to \u03c6 and \u03b8 using the reparameterization trick introduced by Kingma and Welling [16] and Rezende et al. [27] to take gradients."
                },
                {
                    "heading": "3.2 Basic Model",
                    "text": "We extend this model as shown on the left in Figure 3 to include a latent variable c, the context, that varies per dataset D. Now the likelihood of a particular dataset D is given by\np(D) = \u222b p(c) [\u220f x\u2208D \u222b p(x|z; \u03b8)p(z|c; \u03b8) dz ] dc (3)\nThe prior p(c) is chosen to be a spherical Gaussian with zero-mean and unit variances. The conditional p(z|c; \u03b8) is Gaussian with diagonal covariance, where the mean and variance depend on c through a neural network. Similarly the observation model p(x|z; \u03b8) will be a simple likelihood function appropriate to the data modality with dependence on z parameterized by a neural network. We use approximate inference networks q(z|x, c, \u03c6), q(c|D;\u03c6) with parameters \u03c6 to calculate and optimize a variational lower bound on the log-likelihood, where again the likelihood forms are diagonal Gaussians parameterized by neural networks:\nLD = Eq(c|D;\u03c6) [\u2211 x\u2208d Eq(z,|c,x;\u03c6) [log p(x|z; \u03b8)]\u2212DKL (q(z|c, x;\u03c6)\u2016p(z|c; \u03b8)) ] \u2212DKL (q(c|D;\u03c6)\u2016p(c)) (4)\nNote that q(c|D,\u03c6) accepts as input a dataset D and we refer to this as the statistic network. We describe this in Subsection 3.4.\n3.3 Full Model Algorithm 1 Sampling a Dataset of size k\nsample c \u223c p(c) for i = 1 to k do\nsample zi,L \u223c p(zL|c, \u03b8) for j = L\u2212 1 to 1 do\nsample zi,j \u223c p(zj |zi,j+1, c, \u03b8) end for sample xi \u223c p(x|zi,1, . . . , zi,L, c, \u03b8)\nend for The basic model works well for modelling simple datasets, but struggles when the datasets have complex internal structure. To increase the sophistication of the model we use multiple stochastic layers z1, . . . , zk and introduce skipconnections for both the inference and generative networks. The model is shown graphically in Figure 3. The probability of a dataset D is then given by\np(D) = \u222b p(c) \u220f x\u2208D \u222b p(x|c, z1:L; \u03b8)p(zL|c; \u03b8) L\u22121\u220f i=1 p(zi|zi+1, c; \u03b8) dz1:L dc (5)\nand the generative process for the full model is described in Algorithm 1. The full approximate posterior factorizes analogously as\nq(c, z1:L|D;\u03c6) = q(c|D;\u03c6) \u220f x\u2208D q(zL|x, c;\u03c6) L\u22121\u220f i=1 q(zi|zi+1, x, c;\u03c6) (6)\nFor convenience we give the variational lower bound as sum of a three parts\nLD = RD + CD + LD (7)\nthe reconstruction term RD = Eq(c|D;\u03c6) \u2211 x\u2208D Eq(z1:L|c,x;\u03c6) log p(x|z1:L, c; \u03b8) (8)\nthe context divergence CD = DKL (q(c|D;\u03c6)\u2016p(c)) (9)\nand the latent divergences\nLD = Eq(c,z1:L|D;\u03c6) [\u2211 x\u2208D DKL (q(zL|c, x;\u03c6)\u2016p(zL|c; \u03b8))\n+ L\u22121\u2211 i=1 DKL (q(zi|zi+1, c, x;\u03c6)\u2016p(zi|zi+1, c; \u03b8))\n] (10)\nThe skip-connections p(zi|zi+1, c, \u03b8) and q(zi|zi+1, x) allow the context to specify a more precise distribution for each latent variable by explaining-away more generic aspects of the dataset at each stochastic layer. This architecture was inspired by recent work on probabilistic ladder networks in Kaae S\u00f8nderby et al. [14]. Complementing these are the skip-connections from each latent variable to the observation p(x|z1:L, c; \u03b8), the intuition here is that each stochastic layer can focus on representing a certain level of abstraction, since its information does not need to be copied into the next layer, a similar approach was used in Maal\u00f8e et al. [22]. Note that we are optimizing over many datasets D: we want to maximize the expectation of LD over all datasets. We do this optimization using stochastic gradient descent. In contrast to a variational autoencoder where a minibatch would consist of a subsample of datapoints from the dataset, we use minibatches consisting of a subsample of datasets - tensors of shape (batch size, sample size, number of features)."
                },
                {
                    "heading": "3.4 Statistic Network",
                    "text": "In addition to the standard recognition networks we use a statistic network q(c|D;\u03c6) to give an approximate posterior over the context c given a dataset D = {x1, . . . , xk} . This is a feedforward neural network consisting of three main elements: \u2022 An instance encoder E that takes each individual datapoint xi to a vector ei = E(xi).\n\u2022 An exchangeable instance pooling layer that collapses the matrix (e1, . . . , ek) to a single prestatistic vector v. Examples include elementwise means, sums, products, geometric means and maximum. We use the sample mean for all experiments. \u2022 A final post-pooling network that takes v to a parameterization of a diagonal Gaussian. We note that the humble sample mean already gives the statistic network a great deal of representational power due to the fact that the instance encoder can learn a representation where averaging makes sense. For example since the instance encoder can approximate a polynomial on a compact domain, and so can the post-pooling network, a statistic network can approximate any moment of a distribution."
                },
                {
                    "heading": "4 Related Work",
                    "text": "Due to the general nature of the problem considered, our work touches on many different topics which we now attempt to summarize.\nTopics models and graphical models We note the resemblance of the graphical model in Figure 3 to the form of a topic model. In contrast to traditional topic models we do not use discrete latent variables, or restrict to discrete data. In addition we use more flexible conditional distributions and dependency structures parameterized by deep neural networks, although recent work has explored this for document models in Miao et al. [23]. Along related lines are \u2018structured variational autoencoders\u2019 [13] where they treat the general problem of integrating graphical models with variational autoencoders.\nTransfer learning There is a considerable literature on transfer learning, for a survey see Pan and Yang [25]. There they discuss \u2018parameter-transfer\u2019 approaches whereby parameters or priors are shared across datasets, and our work fits into that paradigm. For examples see Lawrence and Platt [20] where share they priors between Gaussian processes, and Evgeniou and Pontil [5] where they take an SVM-like approach to share kernels.\nOne-shot Learning Learning quickly from small amounts of data is a topic of great interest. In particular we have seen Lake et al. [18] where they use Bayesian program induction, and [17] where they train a Siamese ([3]) convolutional network for one-shot image classification We note the relation to the recent work [28] in which they use a conditional recurrent variational autoencoder capable of one-shot generalization by taking as extra input a conditioning data point. The important differences here are that we jointly model datasets and datapoints and consider datasets of any size.\nMultiple-Instance Learning There is previous work on classifying sets in multiple-instance learning, for a useful survey see Cheplygina et al. [2]. Typical approaches involve adapting kernel based methods such as support measure machines [24], support distribution machines [26] and multiple-instance-kernels [7].\nSet2Seq A highly related work is Vinyals et al. [31] where they explore architectures for mapping sets to sequences. There they use an LSTM to repeatedly compute weighted-averages of the datapoints. They use this to tackle problems such as sorting a list of numbers. The main difference between their work and ours is that they primarily consider supervised problems, whereas we present a general unsupervised method for learning representations of sets of i.i.d instances. In future work we may also explore recurrently computing statistics.\nABC There has also been work on learning summary statistics for Approximate Bayesian Computation by either learning to predict the parameters generating a sample as a supervised problem, or by using kernel embeddings as infinite dimensional summary statistics. See [6] for instance for kernel-based approaches. More recently Jiang et al. [12] used deep neural networks to predict the parameters generating the data. The crucial differences are that their problem is supervised, they do not leverage any exchangeability properties the data may have, nor can it deal with varying sample sizes."
                },
                {
                    "heading": "5 Experimental Results",
                    "text": "Given an input set x1, . . . xk we can use the statistic network to calculate an approximate posterior over contexts q(c|x1, . . . , xk;\u03c6). Each context c specifies a generative model p(x|c; \u03b8). To get samples from the model we set c to the mean of the approximate posterior and then sample directly\nfrom the conditional distributions. We use the Adam optimization algorithm [15] for all experiments, batch normalization [10] for models with mulitple stochastic layers and we always use a batch size of 16. We primarily use the Theano [29] framework with the Lasagne [4] library, but the final experiments with face data were done using Tensorflow [1]."
                },
                {
                    "heading": "5.1 Simple 1-D Distributions",
                    "text": "In our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-D datasets by distribution family. We generated a collection of synthetic 1-D datasets each containing 200 samples. Datasets consist of samples from either an Exponential, Gaussian, Uniform or Laplacian distribution with equal probability. Means and variances are sampled from U [\u22121, 1] and U [0.5, 2] respectively. Training data contains 10K sets. We used a model with a single stochastic layer with 32 units for z and 3 units for c. We used three dense layers before and after pooling in the statistic network each with 128 units with Rectified Linear Unit (ReLU) activations. For q(z|x, c) and p(x|z, c) we used three dense layers with ReLU activations and 128 units. Figure 2 on the left shows a 3-D scatter plot of the summary statistics learned. Notice that the different families of distribution cluster. It is interesting to observe that the Exponential cluster is differently orientated to the others, perhaps reflecting the fact that it is the only non-symmetric distribution. We also see that between the Gaussian and Laplacian cluster there is an area of ambiguity which is as one might expect."
                },
                {
                    "heading": "5.2 Bernoulli Data with Varying Sample Size",
                    "text": "If we observe one coin flip come up heads we gain little information about the coin; if we see fifty heads in a row we are all but certain that the coin is biased. But in both cases the observed proportion of heads is 100%. The lesson is that the sample size can be important for accurate inference of the generative parameters, and so a neural statistician should be able to take sample size into account. The problem is that the sample mean records only the relative proportions of values, for instance: the sample mean s\u0304 = 1k \u2211k i=1 xi is equal to the sample mean of the repeated data 1 kr \u2211k i=1 \u2211r j=1 xi is equal to the sample mean of the single datapoint x1 = s\u0304. The approximate posterior over the context given by the statistic network is a function of a sample mean. Hence the approximate posterior is also partially invariant to the sample size in the above sense. This means that the statistic network q(c|D) has no way to represent uncertainty about the context resulting from differing sample sizes. In order to handle uncertainty over the context resulting from differing sample sizes, we augment our recognition model by introducing a prior-interpolation layer. The prior-interpolation layer modifies the output of the statistic network q(c|D) described in Subsection 3.4 as follows: given that the output of q(c|D) is \u00b5c and \u03c32c parameterizing the approximate posterior N (c;\u00b5c, \u03c32c ), we define \u00b5\u2217 and (\u03c32c ) \u2217, the interpolated parameters by\n\u00b5\u2217c = f\u03b1(n) \u00b7 0 + (1\u2212 f\u03b1(n)) \u00b7 \u00b5c and (\u03c32c )\u2217 = f\u03b2(n) \u00b7 1 + (1\u2212 f\u03b2(n)) \u00b7 \u03c32c (11)\ngiving an interpolated approximate posterior N (c;\u00b5\u2217c , (\u03c32c )\u2217), where n is the sample size, f\u03b1, f\u03b2 are monotonically decreasing functions with outputs in [0, 1] with parameters \u03b1 and \u03b2 (learned through optimizing the variational lower bound in Equation 7), and such that f\u03b1(0) = f\u03b2(0) = 1 and f\u03b1(n), f\u03b2(n) \u2192 0 as n \u2192 \u221e. This allows the statistic network to interpolate its proposed approximate posterior coordinate-wise with the prior p(c) = N (c; 0, 1). To demonstrate that this simple intervention can work we perform an experiment on synthetic data where we can calculate the true posterior analytically. The datasets consist of samples from Bernoulli distributions with probability p \u223c U [0, 1]. Sample sizes are between 1 and 20 with equal probability. We chose f\u03b1(n) = exp(\u2212 exp(\u03b1) \u00b7 n) and f\u03b2(n) = exp(\u2212 exp(\u03b2) \u00b7 n) as our prior-interpolation layer. We use a single stochastic layer with 2 units for c and 2 units for z. There is no instance encoder in the statistic network, the pre-statistic vector is simply the average of each dataset. There are three dense post-pooling layers with 16 ReLU units. The other subnetworks each have 2 dense layers with 32 ReLU units. We use a Bernoulli likelihood for the decoder p(x|z, c). Since the Beta distribution is a conjugate prior for the Bernoulli, we know the exact posterior for p given observations d. In Figure 2 on the right we compare samples from the approximate posterior given by the model and the true posterior across a range of sample sizes, and we see that the model is indeed able to account for uncertainty. In order to sample the probability from our model we sample a context from the approximate posterior and then sample x \u223c p(x|c; \u03b8) 100 times and average the result to give p. This simple modification improves the average log-likelihood per data point from \u22120.627 when training without the prior-interpolation layer to \u22120.597, for comparison the true likelihood of the test data is \u22120.595. 5.3 Spatial MNIST\nBuilding on the previous experiments we investigate 2-D datasets that have complex structure, but the datapoints contain little information by themselves. We created a dataset called spatial MNIST. In spatial MNIST each image from MNIST [21] is turned into a dataset by interpreting the normalised pixel intensities as a probability density and sampling coordinate values. An example is shown in Figure 3. This creates two-dimensional spatial datasets. We used a sample size of 50. Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u \u223c U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see [30] for a discussion of this point).\nWe used 3 stochastic layers with 2 units for each z and 64 units for c. Each subnetwork contained 3 dense layers with 256 ReLU units each. We used a Gaussian likelihood for p(x|z1:3, c \u03b8) . In addition to being able to sample from the model conditioned on a set of inputs, we can also to summarize a dataset by choosing a subset S \u2286 d to minimise the KL divergence of q(C|d;\u03c6) from q(C|S;\u03c6). We do this greedily by iteratively discarding points from the full sample. The results are shown in Figure 5.3. We see that the model is capable of handling complex arrangements of datapoints. We also see that it can select sensible subsets of a dataset as a summary, notice that the model tends to select \u2018critical points\u2019 of the shape just as a human might do."
                },
                {
                    "heading": "5.4 MNIST",
                    "text": "We now move on to modelling high dimensional data. A natural source of such datasets are images of a particular class. We begin with sets of MNIST digits of the same class with sample sizes of 10. We\ntreat the data as binary by interpreting the pixel intensities as Bernoulli probabilities and sampling. We resample the training data at each epoch to combat overfitting following [14]. Since the MNIST digits exhibit relatively small amounts of variation, we use a single stochastic layer with 32 units for both z and c. The statistic network has three pre and post pooling dense layers with 64 ReLU units each. The other subnetworks each have three dense layers with 256 ReLU units each. We used a Bernoulli likelihood for the decoder. Conditioned samples are shown in Figure 5 on the right. The samples appear well-formed and so we have demonstrated that it is possible to flexibly condition generative models of high-dimensional data on-the-fly."
                },
                {
                    "heading": "5.5 Omniglot",
                    "text": "Next we work with the OMNIGLOT data [18]. This contains 1628 classes of handwritten characters but with just 20 examples per class. This makes it an excellent test-bed for transfer / small-shot learning. Here the datasets correspond to samples from a given class, and we use a sample size of 5. We save 5 examples per class for the test set, and in addition we hold out 20 entire classes to test generalization to new classes. We created new classes with data augmentation by rotating and reflecting characters. We resized the images to 28\u00d7 28. As with MNIST we treated the data as binary and sampled a binarization of the image at each epoch. We used three stochastic layers with 16 units for each z and 64 units for c. The decoder p(x|z1, z2, z3, c) consisted of: three dense layers with 256 ReLU units, followed a dense layer H with 10\u00d7 7\u00d7 7 linear units, followed by ReLU convolutional layers with 64\u2192 64\u2192 32 filters of size 3\u00d7 3 interspersed with 2\u00d7 2 upsampling layers, followed by a 1\u00d7 1 convolution with a single filter and a sigmoid activation. The output was then passed through a spatial transformer network [11] effecting an affine change of coordinates with parameters a linear projection of H . The decoder used a Bernoulli likelihood. All other subnetworks had 3 dense layers with 256 ReLU units. In Figure 5 on the left we show samples from the model conditioned on classes seen in the training data. The good quality of the samples indicate that the model is indeed able to represent a large variety of different datasets. In Figure 6 we show two examples of small-shot learning by conditioning on samples of unseen characters from OMNIGLOT, and conditioning on samples of digits from MNIST. The OMNIGLOT samples are mostly of a high-quality, and this shows that the neural statistician can generalize even to new datasets. The transfer to MNIST represents a harder task and produces less attractive samples, but they are still recognisably the correct digit. We believe that the results could be improved further through use of a convolutional encoder and/or more aggressive data augmentation. As a further test we considered small-shot classification of both unseen OMNIGLOT characters and MNIST digits. Given a sets of labelled examples of each class D0, . . . , D9 for MNIST say we computed the approximate posteriors q(C|Di;\u03c6) using the statistic network. Then for each test image x we also computed the posterior q(C|x;\u03c6) and classified it according to training dataset Di minimizing the KL divergence from the test context to the training context. We tried this with either 1 or 5 labelled examples per class. We implemented two simple baselines, 1-nearest neighbour on the raw features, and 1-nearest neighbour on features learned by an autoencoder trained on the OMNIGLOT training data. The autoencoder had 5 hidden layers with 256,256,64,256,256 respective\nReLU units, the middle layer was used as the feature embedding. The results are shown in Table 1. We also include similar results from Koch [17] a discriminatively trained model, but note that they are not directly comparable since they used different splits of characters when training their model. We include their results to give a rough idea of how our model compares to one trained for this task. We find that the performance is remarkably good, especially considering that the model itself was not discriminatively trained."
                },
                {
                    "heading": "5.6 Youtube Faces",
                    "text": "Finally, we provide a proof of concept for generating faces of a particular person. We use the Youtube Faces Database from Wolf et al. [32]. It contains 3, 245 videos of 1, 595 different people. We use the aligned and cropped to face version, resize to 64\u00d7 64. The validation and test sets contain 100 unique people each, and there is no overlap of persons between data splits. The sets were created by\nsampling frames randomly without replacement from each video, we use a set size of 5 frames. We resample the sets for the training data each epoch. Our architecture for this problem is based on one presented in [19]. We used a single stochastic layer with 500 dimensional latent c and 16 dimensional z variable. The statistic network and the inference network q(z|x, c) share a common convolutional encoder consisting of layers with (filter sizes, numbers of filters, strides) given by (3,32,1), (3,32,1), (2,32,2), (3,64,1), (3,64,1), (2,64,2),(3,128,1), (3,128,1), (2,128,2), (3,256,1), (3,256,1), (2,256,2) respectively, all with ReLU activations. The statistic network continues with a dense ReLU layer with 1000 units, followed by average pooling and a dense layer to c. The inference network continues by concatenating with the sample from the statistic network, then a 1000 unit ReLU layer then a linear mapping to z. The decoder begins with a 1000 unit ReLU layer, followed by a linear layer with 256 \u00d7 8 \u00d7 8 units followed by convolutional layers with (filter sizes, numbers of filters, strides) given by (3,256,1), (3,256,1), (3,256,2), (3,128,1), (3,128,1), (2,128,2), (3,64,1), (3,64,1),(2,64,2), (3,32,1), (3,32,1), (2,32,2), (1,3,1) respectively, with stride 1 layers being convolutions and stride 2 layers being transpose-convolutional layers. All activations are ReLUs except for the last layer which is a sigmoid. The likelihood function is a Gaussian, but where the variance parameters are shared across all datapoints, this was found to make training faster and more stable. The results are shown in Figure 7. Whilst there is room for improvement, we see that it is possible to specify a highly complex distribution on-the-fly with a set of photos of a previously unseen person. The samples conditioned on an input set have a reasonable likeness of the input faces. We also show the ability of the model to generate new datasets and see that the samples have a consistent identity and varied poses. Our approach can easily benefit from advances in deep generative models by upgrading our base generative model, and so in the future we could for instance use a recurrent generative model as in [8], [28], [9]."
                },
                {
                    "heading": "6 Conclusions and Future Work",
                    "text": "Our goal was to demonstrate that it is both possible and profitable to work at a level of abstraction of datasets rather than just datapoints. We have shown how it is possible to learn to represent datasets using a statistic network, and that these statistics enable highly flexible and efficient models that can do transfer learning, small shot classification, cluster distributions, summarize datasets and more. Avenues for future research are engineering, methodological and application based. In terms of engineering we believe that there are gains to be had by more thorough exploration of different (larger) architectures. In terms of methodology we want to look at: improved methods of representing uncertainty resulting from sample size; models explicitly designed trained for small-shot classification; supervised and semi-supervised approaches to classifiying either datasets or datapoints within the dataset. One\nadvantage we have yet to explore is that by specifying classes implicitly in terms of sets, we can combine multiple data sources with potentially different labels, or multiple labels. We can also easily train on any unlabelled data because this corresponds to sets of size one. We also want to consider questions such as: What are desirable properties for statistics to have as representations? How can we enforce these? Can we use ideas from classical work on estimators? In terms of applications we are interested in applying this framework to learning embeddings of speakers for speech problems or customer embeddings in commercial problems."
                },
                {
                    "heading": "Acknowledgements",
                    "text": "This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh."
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http: //tensorflow.org/. Software available from tensorflow.org",
                    "authors": [
                        "Mart\u00edn Abadi",
                        "Ashish Agarwal",
                        "Paul Barham",
                        "Eugene Brevdo",
                        "Zhifeng Chen"
                    ],
                    "year": 2015
                },
                {
                    "title": "On classification with bags, groups and sets",
                    "authors": [
                        "Veronika Cheplygina",
                        "David M.J. Tax",
                        "Marco Loog"
                    ],
                    "venue": "Pattern Recognition Letters,",
                    "year": 2015
                },
                {
                    "title": "Learning a similarity metric discriminatively, with application to face verification",
                    "authors": [
                        "S. Chopra",
                        "R. Hadsell",
                        "Y. LeCun"
                    ],
                    "venue": "In Computer Vision and Pattern Recognition,",
                    "year": 2005
                },
                {
                    "title": "Regularized multi\u2013task learning",
                    "authors": [
                        "Theodoros Evgeniou",
                        "Massimiliano Pontil"
                    ],
                    "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
                    "year": 2004
                },
                {
                    "title": "Kernel Bayes\u2019 rule: Bayesian inference with positive definite kernels",
                    "authors": [
                        "Kenji Fukumizu",
                        "Le Song",
                        "Arthur Gretton"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2013
                },
                {
                    "title": "Multi-instance kernels",
                    "authors": [
                        "Thomas Gartner",
                        "Peter A. Flach",
                        "Adam Kowalczyk",
                        "Alex J. Smola"
                    ],
                    "venue": "Proc. 19th International Conf. on Machine Learning,",
                    "year": 2002
                },
                {
                    "title": "Draw: A recurrent neural network for image generation",
                    "authors": [
                        "Karol Gregor",
                        "Ivo Danihelka",
                        "Alex Graves",
                        "Danilo Rezende",
                        "Daan Wierstra"
                    ],
                    "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
                    "year": 2015
                },
                {
                    "title": "Towards conceptual compression",
                    "authors": [
                        "Karol Gregor",
                        "Frederic Besse",
                        "Danilo Jimenez Rezende",
                        "Ivo Danihelka",
                        "Daan Wierstra"
                    ],
                    "venue": "arXiv preprint arXiv:1604.08772,",
                    "year": 2016
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "Sergey Ioffe",
                        "Christian Szegedy"
                    ],
                    "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
                    "year": 2015
                },
                {
                    "title": "Spatial transformer networks",
                    "authors": [
                        "Max Jaderberg",
                        "Karen Simonyan",
                        "Andrew Zisserman"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2015
                },
                {
                    "title": "Learning summary statistic for approximate Bayesian computation via deep neural network",
                    "authors": [
                        "Bai Jiang",
                        "Tung-yu Wu",
                        "Charles Zheng",
                        "Wing H Wong"
                    ],
                    "venue": "arXiv preprint arXiv:1510.02175,",
                    "year": 2015
                },
                {
                    "title": "Structured vaes: Composing probabilistic graphical models and variational autoencoders",
                    "authors": [
                        "Matthew J Johnson",
                        "David Duvenaud",
                        "Alexander B Wiltschko",
                        "Sandeep R Datta",
                        "Ryan P Adams"
                    ],
                    "venue": "arXiv preprint arXiv:1603.06277,",
                    "year": 2016
                },
                {
                    "title": "How to train deep variational autoencoders and probabilistic ladder networks",
                    "authors": [
                        "Casper Kaae S\u00f8nderby",
                        "Tapani Raiko",
                        "Lars Maal\u00f8e",
                        "S\u00f8ren Kaae S\u00f8nderby",
                        "Ole Winther"
                    ],
                    "venue": "arXiv preprint arXiv:1602.02282,",
                    "year": 2016
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "Diederik Kingma",
                        "Jimmy Ba"
                    ],
                    "venue": "arXiv preprint arXiv:1412.6980,",
                    "year": 2014
                },
                {
                    "title": "Auto-encoding variational Bayes",
                    "authors": [
                        "Diederik P Kingma",
                        "Max Welling"
                    ],
                    "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),",
                    "year": 2014
                },
                {
                    "title": "Siamese neural networks for one-shot image recognition",
                    "authors": [
                        "Gregory Koch"
                    ],
                    "venue": "Doctoral dissertation, University of Toronto,",
                    "year": 2015
                },
                {
                    "title": "Human-level concept learning through probabilistic program induction",
                    "authors": [
                        "Brenden M Lake",
                        "Ruslan Salakhutdinov",
                        "Joshua B Tenenbaum"
                    ],
                    "year": 2015
                },
                {
                    "title": "Discriminative regularization for generative models",
                    "authors": [
                        "Alex Lamb",
                        "Vincent Dumoulin",
                        "Aaron Courville"
                    ],
                    "venue": "arXiv preprint arXiv:1602.03220,",
                    "year": 2016
                },
                {
                    "title": "Learning to learn with the informative vector machine",
                    "authors": [
                        "Neil D Lawrence",
                        "John C Platt"
                    ],
                    "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
                    "year": 2004
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Yann LeCun",
                        "L\u00e9on Bottou",
                        "Yoshua Bengio",
                        "Patrick Haffner"
                    ],
                    "venue": "Proceedings of the IEEE,",
                    "year": 1998
                },
                {
                    "title": "Auxiliary deep generative models",
                    "authors": [
                        "Lars Maal\u00f8e",
                        "Casper Kaae S\u00f8nderby",
                        "S\u00f8ren Kaae S\u00f8nderby",
                        "Ole Winther"
                    ],
                    "venue": "arXiv preprint arXiv:1602.05473,",
                    "year": 2016
                },
                {
                    "title": "Neural variational inference for text processing",
                    "authors": [
                        "Yishu Miao",
                        "Lei Yu",
                        "Phil Blunsom"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06038,",
                    "year": 2015
                },
                {
                    "title": "Learning from distributions via support measure machines",
                    "authors": [
                        "Krikamol Muandet",
                        "Kenji Fukumizu",
                        "Francesco Dinuzzo",
                        "Bernhard Sch\u00f6lkopf"
                    ],
                    "venue": "Advances in Neural Information Processing Systems",
                    "year": 2012
                },
                {
                    "title": "A survey on transfer learning",
                    "authors": [
                        "Sinno Jialin Pan",
                        "Qiang Yang"
                    ],
                    "venue": "Knowledge and Data Engineering, IEEE Transactions on,",
                    "year": 2010
                },
                {
                    "title": "Support distribution machines",
                    "authors": [
                        "Barnab\u00e1s P\u00f3czos",
                        "Liang Xiong",
                        "Dougal J Sutherland",
                        "Jeff Schneider"
                    ],
                    "venue": "Technical Report,",
                    "year": 2012
                },
                {
                    "title": "Stochastic backpropagation and approximate inference in deep generative models",
                    "authors": [
                        "Danilo Jimenez Rezende",
                        "Shakir Mohamed",
                        "Daan Wierstra"
                    ],
                    "venue": "In Proceedings of The 31st International Conference on Machine Learning,",
                    "year": 2014
                },
                {
                    "title": "One-shot generalization in deep generative models",
                    "authors": [
                        "Danilo Jimenez Rezende",
                        "Shakir Mohamed",
                        "Ivo Danihelka",
                        "Karol Gregor",
                        "Daan Wierstra"
                    ],
                    "venue": "arXiv preprint arXiv:1603.05106,",
                    "year": 2016
                },
                {
                    "title": "A note on the evaluation of generative models",
                    "authors": [
                        "L. Theis",
                        "A. van den Oord",
                        "M. Bethge"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2016
                },
                {
                    "title": "Order matters: sequence to sequence for sets",
                    "authors": [
                        "Oriol Vinyals",
                        "Samy Bengio",
                        "Manjunath Kudlur"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2016
                },
                {
                    "title": "Face recognition in unconstrained videos with matched background similarity",
                    "authors": [
                        "Lior Wolf",
                        "Tal Hassner",
                        "Itay Maoz"
                    ],
                    "venue": "In Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2011
                }
            ],
            "id": "SP:b109d322e488fb4c3410090789dd5dda676bb832",
            "authors": [
                {
                    "name": "Harrison Edwards",
                    "affiliations": []
                }
            ],
            "abstractText": "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.",
            "title": "Towards a Neural Statistician"
        }
    },
    "65847395": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "The domain of representation learning has undergone tremendous advances due to improved supervised learning techniques. However, unsupervised learning has the potential to leverage large pools of unlabeled data available to us, and extend these advances to modalities that are otherwise impractical or impossible.\nOne principled approach to unsupervised learning is generative probabilistic modeling. Not only do generative probabilistic models have the ability to create novel content, they also have a wide range of reconstruction related applications including inpainting [54, 39, 52], denoising [3], colorization [63], and super-resolution [7].\nAs data of interest are generally highly-dimensional and highly structured, the challenge in this domain is building models that are powerful enough to capture its complexity yet still trainable. We address this challenge by introducing real-valued non-volume preserving (real NVP) transformations, a tractable yet expressive set of models for modeling high-dimensional data."
                },
                {
                    "heading": "2 Related work",
                    "text": "Substantial work on probabilistic generative models has been focused on training models using maximum likelihood. When designing generative models, care needs to be taken to make both inference and learning tractable. These design choices are often expressed in terms of probabilistic graphical models. As these models rely on simple conditional distributions, the introduction of anonymous latent variables has been used to make these models more expressive.\nOccurrences of such models include probabilistic undirected graphs such as Restricted Boltzmann Machines [51] and Deep Boltzmann Machines [46]. These models were successfully trained by taking advantage of the conditional independence property of their bipartite structure to allow efficient exact\n\u2217Work was done when author was at Google Brain.\nar X\niv :1\n60 5.\n08 80\n3v 1\n[ cs\n.L G\n] 2\n7 M\nor approximate posterior inference on latent variables. However, because of the intractability of their associated marginal distribution, their training, evaluation and sampling procedures necessitate the use of approximations like Mean Field inference and Markov Chain Monte Carlo, whose convergence time for such complex models remains undetermined. Furthermore, these approximations can often hinder their performance [5].\nDirected graphical models rely on an ancestral sampling procedure, which is appealing both for its conceptual and computational simplicity. They lack, however, the conditional independence structure of undirected models, making exact and approximate posterior inference on latent variables cumbersome [49]. Recent advances in stochastic variational inference [22] and amortized inference [11, 36, 28, 42], allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log-likelihood [38]. In particular, the variational autoencoder algorithm [28, 42] simultaneously learns a generative network, that maps gaussian latent variables z to samples x, and semantically meaningful features by exploiting the reparametrization trick [60]. Its success in leveraging recent advances in backpropagation [44, 32] in deep neural networks resulted in its adoption for several applications ranging from speech synthesis [10] to language modeling [6]. Still, the approximation in the inference process limits its ability to learn high dimensional deep representations, motivating recent work in improving approximate inference [35, 41, 48, 56, 8, 52].\nSuch approximations can be avoided altogether by abstaining from using latent variables. Autoregressive models [15, 30] can implement this strategy while typically retaining a great deal of flexibility. This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to an fixed ordering over dimensions, simplifying log-likelihood evaluation and sampling. Recent work in this line of research have successfully taken advantage of recent advances in recurrent networks [44], in particular longshort term memory [21], and residual networks [20, 19] in order to learn state-of-the-art generative image models [54, 39] and language models [26]. But the ordering of the dimensions, although often arbitrary, can be critical to the training of the model [59]. The sequential nature of this model limits its computational efficiency. For example, its sampling procedure is sequential and non-parallelizable. Additionally, there is no natural latent representation associated with autoregressive models, and they have not been shown to be useful for semi-supervised learning.\nGenerative adversarial networks [17] on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether. Instead, the generative network is associated with a discriminator network whose task is to distinguish between samples and real data. Rather than using an intractable log-likelihood, this discriminator network provides the training signal in an adversarial fashion. Successfully trained GAN models [17, 12, 40] can consistently generate sharp and realistically looking samples [31]. However, metrics that measure the diversity in the generated samples are currently intractable [55, 18]. Additionally, instability in their training process [40] requires careful hyperparameter tuning to avoid diverging behaviors.\nGiven the constraints of bijectivity, training a generative network g would be theoretically possible using the change of variable formula:\npX(x) = pZ(z) \u2223\u2223\u2223\u2223det(\u2202g(z)\u2202z )\u2223\u2223\u2223\u2223\u22121 . (1)\nThis formula has been mentioned in several papers including the maximum likelihood formulation of independent components analysis (ICA) [4, 23], gaussianization [9] and deep density models [43, 14, 3]. However, as a naive application of this formula is in general impractical for highdimensional data, ICA practitioners preferred to use more tractable principle like ensemble learning [58]. As the existence proof of nonlinear ICA solutions [24] suggests, auto-regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA, where the residual corresponds to the independent components."
                },
                {
                    "heading": "3 Model definition",
                    "text": "In this paper, we will introduce a more flexible class of architectures that can tractably implement maximum likelihood on continuous data using this change of variable formula. Building on our previous work in [14], we will define a powerful class of bijective functions which will enable exact and tractable density evaluation and exact and tractable inference. These bijections will tie the sampling and inference processes, which will make exact sampling as efficient as exact inference. Moreover, the increased flexibility will allow us not to rely on a fixed form reconstruction cost such as square error [31, 40], and output sharper samples from trained models as a consequence. Also, this flexibility will help us leverage recent advances in batch normalization [25] and residual networks [19, 20]."
                },
                {
                    "heading": "3.1 Change of variable formula",
                    "text": "Given a simple prior probability distribution pZ and a bijection f (with g = f\u22121), the change of variable formula is defined as\npX(x) = pZ ( f(x) ) \u2223\u2223\u2223\u2223det(\u2202f(x)\u2202xT )\u2223\u2223\u2223\u2223 (2)\nlog (pX(x)) = log ( pZ ( f(x) )) + log (\u2223\u2223\u2223\u2223det(\u2202f(x)\u2202xT )\u2223\u2223\u2223\u2223) , (3)\nwhere \u2202f(x) \u2202xT is the Jacobian of f at x.\nExact samples from the resulting distribution can be generated by using the inverse transform sampling rule [13]. A sample z \u223c pZ is drawn in the latent space, and its inverse image x = f\u22121(z) = g(z) generates a sample in the original space. Computing the density on a point x would be done by computing the density on its image f(x) and computing the associated Jacobian determinant det ( \u2202f(x) \u2202xT ) . See also Figure 1."
                },
                {
                    "heading": "3.2 Coupling layers",
                    "text": "Computing the Jacobian of functions with high-dimensional domain and codomain and computing the determinants of large matrices are in general computationally very expensive. This combined with the restriction to bijective functions makes Equation 2 appear impractical for modeling arbitrary distributions.\nAs we show however, by careful design of the function f , a bijective model can be learned which is both tractable and extremely flexible. As computing the Jacobian determinant of the transformation is crucial to effectively train using this principle, our work exploits the simple observation that the determinant of a triangular matrix can be efficiently computed as the product of its diagonal terms.\nWe will build a flexible and tractable bijective function by stacking a sequence of simple bijections. In each simple bijection, part of the input vector is updated using a function which is simple to invert, but which depends on the remainder of the input vector in a complex way. We refer to each of these simple bijections as an affine coupling layer. Given a D dimensional input x and d < D, the output y of an affine coupling layer follows the equations\ny1:d = x1:d (4) yd+1:D = xd+1:D exp ( l(x1:d) ) +m(x1:d), (5)\nwhere l and m are functions Rd 7\u2192 RD\u2212d and is the Hadamard product or element-wise product (see Figure 2(a))."
                },
                {
                    "heading": "3.3 Properties",
                    "text": "The Jacobian of this transformation is\n\u2202y\n\u2202xT =\n[ Id 0\n\u2202yd+1:D \u2202xT1:d\ndiag ( exp(l) ) ] , (6) where diag ( exp(l) ) is the diagonal matrix whose diagonal elements correspond to the vector\nexp ( l(x1:d) ) . Given the observation that this Jacobian is triangular, we can efficiently compute\nits determinant as exp( \u2211 j l(x1:d)j). Since computing the Jacobian determinant of the coupling layer operation does not involve computing the Jacobian of l or m, these functions can be arbitrarily complex. We will make them deep convolutional neural networks. Note that the hidden layers of l and m will have more features than their input or output layers.\nAnother interesting property of these coupling layers in the context of defining probabilistic models is their invertibility. Indeed, computing the inverse is no more complex than the forward propagation (see Figure 2(b)), {\ny1:d = x1:d yd+1:D = xd+1:D exp ( l(x1:d) ) +m(x1:d)\n(7)\n\u21d4 { x1:d = y1:d xd+1:D = ( yd+1:D \u2212m(y1:d) ) exp ( \u2212 l(y1:d)\n) . (8)"
                },
                {
                    "heading": "3.4 Masked convolution",
                    "text": "Partitioning can be implemented using a binary mask b, and using the functional form for y,\ny = b x+ (1\u2212 b) ( x exp ( l(b x) ) +m(b x) ) . (9)\nWe use two partitionings that exploit the local correlation structure of images: spatial checkerboard patterns, and channel-wise masking (see Figure 3). The spatial checkerboard pattern mask has value 1 where the sum of spatial coordinates is odd, and 0 otherwise. The channel-wise mask b is 1 for the first half of the channel dimensions and 0 for the second half. For the models presented here, both l(\u00b7) and m(\u00b7) are rectified convolutional networks."
                },
                {
                    "heading": "3.5 Combining coupling layers",
                    "text": "Although coupling layers can be powerful, their forward transformation leaves some components unchanged. This difficulty can be overcome by composing coupling layers in an alternating pattern, such that the components that are left unchanged in one coupling layer are updated in the next (see Figure 4(a))."
                },
                {
                    "heading": "3.6 Multi-scale architecture",
                    "text": "We implement a multi-scale architecture using a squeezing operation: for each channel, it divides the image into subquares of shape 2\u00d7 2\u00d7 c, then reshapes them into subsquares of shape 1\u00d7 1\u00d7 4c. The squeezing operation transforms an s \u00d7 s \u00d7 c tensor into an s2 \u00d7 s 2 \u00d7 4c tensor (see Figure 3), effectively trading spatial size for number of channels.\nAt each scale, we combine several operations into a sequence: we first apply three coupling layers with alternating checkerboard masks, then perform a squeezing operation, and finally apply three more coupling layers with channel-wise masking. The channel-wise masking is chosen so that the resulting partitioning is not redundant with the previous checkerboard masking (see Figure 3). For the final scale, we only apply four coupling layers in with alternating checkerboard masks.\nPropagating a D dimensional vector through all the coupling layers would be cumbersome, in terms of computational and memory cost, and in terms of the number of parameters that would need to be trained. For this reason we follow the design choice of [50] and factor out half of the dimensions at\nregular intervals (see Equation 11). We can define this operation recursively (see Figure 4(b)),\nh(0) = x (10)\n(z(i+1), h(i+1)) = f (i+1)(h(i)) (11)\nz(L) = f (L)(h(L\u22121)) (12)\nz = (z(1), . . . , z(L)). (13)\nIn our experiments, for i < L. The sequence of coupling-squeezing-coupling operations described above is performed per layer when computing f (i) (Equation 11). At each layer, as the spatial resolution is reduced, the number of hidden layer features in l and m is doubled. All variables which have been factored out are concatenated to obtain the final transformed output (Equation 13).\nAs a consequence, the model must first Gaussianize layers which are factored out at an earlier layer. This follows a philosophy similar to guiding intermediate layers using intermediate classifiers [33], and having multiple layers of latent variables which represent different levels of abstraction [46, 42]."
                },
                {
                    "heading": "3.7 Batch normalization",
                    "text": "To further improve the propagation of training signal, we use deep residual networks [19, 20] with batch normalization [25] and weight normalization [2, 47] in m and l. As described in Appendix E we introduce and use a novel variant of batch normalization which is based on a running average over recent minibatches, and is thus more robust when training with very small minibatches.\nWe also use apply batch normalization to the whole coupling layer output. The effects of batch normalization are easily included in the Jacobian computation, since it acts as a linear rescaling on each dimension. This form of batch normalization can be seen as similar to reward normalization in deep reinforcement learning [37]."
                },
                {
                    "heading": "4 Experiments",
                    "text": ""
                },
                {
                    "heading": "4.1 Procedure",
                    "text": "The algorithm described in Equation 2 shows how to learn distributions on unbounded space. In general, the data of interest have bounded magnitude. For examples, the pixel values of an image typically lie in [0, 256]D after application of the recommended jittering procedure [57, 55]. In order to reduce the impact of boundary effects, we instead model the density of logit(\u03b1+(1\u2212\u03b1) x), where \u03b1 is picked here as .05. We take into account this transformation when computing log-likelihood and bits per dimension. We also use horizontal flips for CIFAR-10, CelebA and LSUN.\nWe train our model on four natural image datasets: CIFAR-10 [29], Imagenet [45], Large-scale Scene Understanding (LSUN) [62], CelebFaces Attributes (CelebA) [34]. More specifically, we train on the downsampled to 32\u00d7 32 and 64\u00d7 64 versions of Imagenet [39]. For the LSUN dataset, we train on the bedroom, tower and church outdoor categories. The procedure for LSUN is the same as in [40]: we downsample the image so that the smallest side is 96 pixels and take random crops of 64\u00d7 64. For CelebA, we use the same procedure as in [31].\nWe use the multi-scale architecture described in Section 3.6 and use deep convolutional residual networks in the coupling layers with skip-connections as suggested by [39]. Our multi-scale architecture is repeated recursively until the input of the last recursion is a 4\u00d7 4\u00d7 c tensor. For datasets of images of size 32\u00d7 32, we use 4 residual blocks with 32 hidden feature maps for the first coupling layers with checkerboard masking. Only 2 residual blocks are used for images of size 64\u00d7 64. We use a batch size of 64. For CIFAR-10, we use 8 residual blocks, 64 feature maps, and downscale only once. We optimize with ADAM [27] with default hyperparameters.\nWe set the prior pZ to be an isotropic unit norm Gaussian. However, any distribution could be used for pZ , including distributions that are also learned during training, such as from an auto-regressive model, or (with slight modifications to the training objective) a variational autoencoder."
                },
                {
                    "heading": "4.2 Results",
                    "text": "We show in Table 1 that the number of bits per dimension, while not improving over the Pixel RNN [39] baseline, is competitive with other generative methods. As we notice that our performance increases with the number of parameters, larger models are likely to further improve performance. For CelebA and LSUN, the bits per dimension for the validation set was decreasing throughout training, so little overfitting is expected.\nWe show in Figure 5 samples generated from the model with training examples from the dataset for comparison. As mentioned in [55, 18], maximum likelihood is a principle that values diversity over sample quality in a limited capacity setting. As a result, our model outputs sometimes highly improbable samples as we can notice especially on CelebA. As opposed to variational autoencoders, the samples generated from our model look not only globally coherent but also sharp. Our hypothesis is that as opposed to these models, real NVP does not rely on fixed form reconstruction cost like an L2 norm which tends to reward capturing low frequency components more heavily than high frequency components. On Imagenet and LSUN, our model seems to have captured well the notion\nof background/foreground and lighting interactions such as luminosity and consistent light source direction for reflectance and shadows.\nWe also illustrate the smooth semantically consistent meaning of our latent variables. In the latent space, we define a manifold based on four validation examples z(1), z(2), z(3), z(4), and parametrized by two parameters \u03c6 and \u03c6\u2032 by,\nz = cos(\u03c6) ( cos(\u03c6\u2032)z(1) + sin(\u03c6 \u2032)z(2) ) + sin(\u03c6) ( cos(\u03c6\u2032)z(3) + sin(\u03c6 \u2032)z(4) ) . (14)\nWe project the resulting manifold back into the data space by computing g(z). Results are shown Figure 6. We observe that the model seems to have organized the latent space with a notion of meaning that goes well beyond pixel space interpolation. More visualization are shown in the Appendix."
                },
                {
                    "heading": "5 Discussion and conclusion",
                    "text": "In this paper, we have defined a class of invertible functions with tractable Jacobian determinant, enabling exact and tractable log-likelihood evaluation, inference, and sampling. We have shown that this class of generative model achieves competitive performances, both in terms of sample quality and log-likelihood. Many avenues exist to further improve the functional form of the transformations, for instance by exploiting the latest advances in dilated convolutions [61] and residual networks architectures [53]\nThis paper presented a technique bridging the gap between auto-regressive models, variational autoencoders, and generative adversarial networks. Like auto-regressive models, it allows tractable and exact log-likelihood evaluation for training. It allows however a much more flexible functional form and, similar to variational autoencoders, it can define a meaningful latent space. Finally, like generative adversarial networks, our technique does not require the use of a fixed form reconstruction cost, and instead defines a cost in terms of higher level features, generating sharper images.\nNot only can this generative model be conditioned to create a structured output algorithm but, as the resulting class of invertible transformations can be treated as a probability distribution in a modular way, it can also be used to improve upon other probabilistic models like auto-regressive models and variational autoencoders. For variational autoencoders, these transformations could be used both to design more interesting reconstruction cost [31] and to augment stochastic inference models [41]. Probabilistic models in general can also benefit from batch normalization techniques as applied in this paper."
                },
                {
                    "heading": "6 Acknowledgments",
                    "text": "The authors thank the developers of Tensorflow [1]. We thank Sherry Moore, David Andersen and Jon Shlens for their help in implementing the model. We thank A\u00e4ron van den Oord, Yann Dauphin, Kyle Kastner, Chelsea Finn, Ben Poole and David Warde-Farley for fruitful discussions. Finally, we thank Rafal Jozefowicz and George Dahl for their input on a draft of the paper."
                },
                {
                    "heading": "A Samples",
                    "text": ""
                },
                {
                    "heading": "B Manifold",
                    "text": ""
                },
                {
                    "heading": "C Extrapolation",
                    "text": "Our convolutional architecture is only aware of the position of considered pixel through edge effects in colvolutions, therefore our model is similar to a stationary process. Inspired by the texture generation work by [16, 54] and extrapolation test with DCGAN [40], we also evaluate the statistics captured by our model by generating images twice or ten times as large as present in the dataset. As we can observe in the following figures, our model seems to successfully create a \u201ctexture\u201d representation of the dataset while maintaining a spatial smoothness through the image."
                },
                {
                    "heading": "D Latent variables semantic",
                    "text": "As in [18], we further try to grasp the semantic of our learned layers latent variables by doing ablation tests. We infer the latent variables and resample the lowest levels of latent variables from a standard gaussian, increasing the highest level affected by this resampling. As we can see in the following figures, the semantic of our latent space seems to be more on a graphic level rather than higher level concept. Although the heavy use of convolution improves learning by exploiting image prior knowledge, it is also likely to be responsible for this limitation."
                },
                {
                    "heading": "E Batch normalization",
                    "text": "We further experimented with batch normalization by using a weighted average of a moving average of the layer statistics \u00b5\u0303t, \u03c3\u03032t and the current batch batch statistics \u00b5\u0302t, \u03c3\u03022t ,\n\u00b5\u0303t+1 = \u03c1\u00b5\u0303t + (1\u2212 \u03c1)\u00b5\u0302t (15) \u03c3\u03032t+1 = \u03c1\u03c3\u0303 2 t + (1\u2212 \u03c1)\u03c3\u03022t , (16)\nwhere \u03c1 is the momentum. When using \u00b5\u0303t+1, \u03c3\u03032t+1, we only propagate gradient through the current batch statistics \u00b5\u0302t, \u03c3\u03022t . We observe that using this lag helps the model train with very small minibatches.\nWe used batch normalization with a moving average for our results on CIFAR-10."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
                    "authors": [
                        "Mart\u0131n Abadi",
                        "Ashish Agarwal",
                        "Paul Barham",
                        "Eugene Brevdo",
                        "Zhifeng Chen",
                        "Craig Citro",
                        "Greg S Corrado",
                        "Andy Davis",
                        "Jeffrey Dean",
                        "Matthieu Devin"
                    ],
                    "venue": "arXiv preprint arXiv:1603.04467,",
                    "year": 2016
                },
                {
                    "title": "Understanding symmetries in deep networks",
                    "authors": [
                        "Vijay Badrinarayanan",
                        "Bamdev Mishra",
                        "Roberto Cipolla"
                    ],
                    "venue": "arXiv preprint arXiv:1511.01029,",
                    "year": 2015
                },
                {
                    "title": "Density modeling of images using a generalized normalization transformation",
                    "authors": [
                        "Johannes Ball\u00e9",
                        "Valero Laparra",
                        "Eero P Simoncelli"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06281,",
                    "year": 2015
                },
                {
                    "title": "An information-maximization approach to blind separation and blind deconvolution",
                    "authors": [
                        "Anthony J Bell",
                        "Terrence J Sejnowski"
                    ],
                    "venue": "Neural computation,",
                    "year": 1995
                },
                {
                    "title": "Stochastic gradient estimate variance in contrastive divergence and persistent contrastive divergence",
                    "authors": [
                        "Mathias Berglund",
                        "Tapani Raiko"
                    ],
                    "venue": "arXiv preprint arXiv:1312.6002,",
                    "year": 2013
                },
                {
                    "title": "Generating sentences from a continuous space",
                    "authors": [
                        "Samuel R Bowman",
                        "Luke Vilnis",
                        "Oriol Vinyals",
                        "Andrew M Dai",
                        "Rafal Jozefowicz",
                        "Samy Bengio"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06349,",
                    "year": 2015
                },
                {
                    "title": "Super-resolution with deep convolutional sufficient statistics",
                    "authors": [
                        "Joan Bruna",
                        "Pablo Sprechmann",
                        "Yann LeCun"
                    ],
                    "venue": "arXiv preprint arXiv:1511.05666,",
                    "year": 2015
                },
                {
                    "title": "Importance weighted autoencoders",
                    "authors": [
                        "Yuri Burda",
                        "Roger Grosse",
                        "Ruslan Salakhutdinov"
                    ],
                    "venue": "arXiv preprint arXiv:1509.00519,",
                    "year": 2015
                },
                {
                    "title": "A Gopinath. Gaussianization",
                    "authors": [
                        "Scott Shaobing Chen",
                        "Ramesh"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2000
                },
                {
                    "title": "A recurrent latent variable model for sequential data",
                    "authors": [
                        "Junyoung Chung",
                        "Kyle Kastner",
                        "Laurent Dinh",
                        "Kratarth Goel",
                        "Aaron C Courville",
                        "Yoshua Bengio"
                    ],
                    "venue": "In Advances in neural information processing systems,",
                    "year": 2015
                },
                {
                    "title": "The helmholtz machine",
                    "authors": [
                        "Peter Dayan",
                        "Geoffrey E Hinton",
                        "Radford M Neal",
                        "Richard S Zemel"
                    ],
                    "venue": "Neural computation,",
                    "year": 1995
                },
                {
                    "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
                    "authors": [
                        "Emily L. Denton",
                        "Soumith Chintala",
                        "Arthur Szlam",
                        "Rob Fergus"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                    "year": 2015
                },
                {
                    "title": "Sample-based non-uniform random variate generation",
                    "authors": [
                        "Luc Devroye"
                    ],
                    "venue": "In Proceedings of the 18th conference on Winter simulation,",
                    "year": 1986
                },
                {
                    "title": "Nice: non-linear independent components estimation",
                    "authors": [
                        "Laurent Dinh",
                        "David Krueger",
                        "Yoshua Bengio"
                    ],
                    "venue": "arXiv preprint arXiv:1410.8516,",
                    "year": 2014
                },
                {
                    "title": "Graphical models for machine learning and digital communication",
                    "authors": [
                        "Brendan J Frey"
                    ],
                    "venue": "MIT press,",
                    "year": 1998
                },
                {
                    "title": "Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems",
                    "authors": [
                        "Leon A. Gatys",
                        "Alexander S. Ecker",
                        "Matthias Bethge"
                    ],
                    "venue": "Annual Conference on Neural Information Processing Systems",
                    "year": 2015
                },
                {
                    "title": "Generative adversarial nets",
                    "authors": [
                        "Ian J. Goodfellow",
                        "Jean Pouget-Abadie",
                        "Mehdi Mirza",
                        "Bing Xu",
                        "David Warde-Farley",
                        "Sherjil Ozair",
                        "Aaron C. Courville",
                        "Yoshua Bengio"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                    "year": 2014
                },
                {
                    "title": "Towards conceptual compression",
                    "authors": [
                        "Karol Gregor",
                        "Frederic Besse",
                        "Danilo Jimenez Rezende",
                        "Ivo Danihelka",
                        "Daan Wierstra"
                    ],
                    "venue": "arXiv preprint arXiv:1604.08772,",
                    "year": 2016
                },
                {
                    "title": "Deep residual learning for image recognition",
                    "authors": [
                        "Kaiming He",
                        "Xiangyu Zhang",
                        "Shaoqing Ren",
                        "Jian Sun"
                    ],
                    "venue": "CoRR, abs/1512.03385,",
                    "year": 2015
                },
                {
                    "title": "Identity mappings in deep residual networks",
                    "authors": [
                        "Kaiming He",
                        "Xiangyu Zhang",
                        "Shaoqing Ren",
                        "Jian Sun"
                    ],
                    "year": 2016
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "Sepp Hochreiter",
                        "J\u00fcrgen Schmidhuber"
                    ],
                    "venue": "Neural Computation,",
                    "year": 1997
                },
                {
                    "title": "Stochastic variational inference",
                    "authors": [
                        "Matthew D Hoffman",
                        "David M Blei",
                        "Chong Wang",
                        "John Paisley"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2013
                },
                {
                    "title": "Independent component analysis, volume 46",
                    "authors": [
                        "Aapo Hyv\u00e4rinen",
                        "Juha Karhunen",
                        "Erkki Oja"
                    ],
                    "year": 2004
                },
                {
                    "title": "Nonlinear independent component analysis: Existence and uniqueness results",
                    "authors": [
                        "Aapo Hyv\u00e4rinen",
                        "Petteri Pajunen"
                    ],
                    "venue": "Neural Networks,",
                    "year": 1999
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "Sergey Ioffe",
                        "Christian Szegedy"
                    ],
                    "venue": "arXiv preprint arXiv:1502.03167,",
                    "year": 2015
                },
                {
                    "title": "Exploring the limits of language modeling",
                    "authors": [
                        "Rafal J\u00f3zefowicz",
                        "Oriol Vinyals",
                        "Mike Schuster",
                        "Noam Shazeer",
                        "Yonghui Wu"
                    ],
                    "year": 2016
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "Diederik Kingma",
                        "Jimmy Ba"
                    ],
                    "venue": "arXiv preprint arXiv:1412.6980,",
                    "year": 2014
                },
                {
                    "title": "Auto-encoding variational bayes",
                    "authors": [
                        "Diederik P Kingma",
                        "Max Welling"
                    ],
                    "venue": "arXiv preprint arXiv:1312.6114,",
                    "year": 2013
                },
                {
                    "title": "Learning multiple layers of features from tiny",
                    "authors": [
                        "Alex Krizhevsky",
                        "Geoffrey Hinton"
                    ],
                    "year": 2009
                },
                {
                    "title": "The neural autoregressive distribution estimator",
                    "authors": [
                        "Hugo Larochelle",
                        "Iain Murray"
                    ],
                    "venue": "In AISTATS,",
                    "year": 2011
                },
                {
                    "title": "Autoencoding beyond pixels using a learned similarity",
                    "authors": [
                        "Anders Boesen Lindbo Larsen",
                        "S\u00f8ren Kaae S\u00f8nderby",
                        "Ole Winther"
                    ],
                    "venue": "metric. CoRR,",
                    "year": 2015
                },
                {
                    "title": "Efficient backprop",
                    "authors": [
                        "Yann A LeCun",
                        "L\u00e9on Bottou",
                        "Genevieve B Orr",
                        "Klaus-Robert M\u00fcller"
                    ],
                    "venue": "In Neural networks: Tricks of the trade,",
                    "year": 2012
                },
                {
                    "title": "Deep learning face attributes in the wild",
                    "authors": [
                        "Ziwei Liu",
                        "Ping Luo",
                        "Xiaogang Wang",
                        "Xiaoou Tang"
                    ],
                    "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
                    "year": 2015
                },
                {
                    "title": "Auxiliary deep generative models",
                    "authors": [
                        "Lars Maal\u00f8e",
                        "Casper Kaae S\u00f8nderby",
                        "S\u00f8ren Kaae S\u00f8nderby",
                        "Ole Winther"
                    ],
                    "venue": "arXiv preprint arXiv:1602.05473,",
                    "year": 2016
                },
                {
                    "title": "Neural variational inference and learning in belief networks",
                    "authors": [
                        "Andriy Mnih",
                        "Karol Gregor"
                    ],
                    "venue": "arXiv preprint arXiv:1402.0030,",
                    "year": 2014
                },
                {
                    "title": "Human-level control through deep reinforcement learning",
                    "authors": [
                        "Volodymyr Mnih",
                        "Koray Kavukcuoglu",
                        "David Silver",
                        "Andrei A Rusu",
                        "Joel Veness",
                        "Marc G Bellemare",
                        "Alex Graves",
                        "Martin Riedmiller",
                        "Andreas K Fidjeland",
                        "Georg Ostrovski"
                    ],
                    "venue": "Nature, 518(7540):529\u2013533,",
                    "year": 2015
                },
                {
                    "title": "A view of the em algorithm that justifies incremental, sparse, and other variants",
                    "authors": [
                        "Radford M Neal",
                        "Geoffrey E Hinton"
                    ],
                    "venue": "In Learning in graphical models,",
                    "year": 1998
                },
                {
                    "title": "Pixel recurrent neural networks",
                    "authors": [
                        "Aaron van den Oord",
                        "Nal Kalchbrenner",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "arXiv preprint arXiv:1601.06759,",
                    "year": 2016
                },
                {
                    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                    "authors": [
                        "Alec Radford",
                        "Luke Metz",
                        "Soumith Chintala"
                    ],
                    "venue": "CoRR, abs/1511.06434,",
                    "year": 2015
                },
                {
                    "title": "Variational inference with normalizing flows",
                    "authors": [
                        "Danilo Jimenez Rezende",
                        "Shakir Mohamed"
                    ],
                    "venue": "arXiv preprint arXiv:1505.05770,",
                    "year": 2015
                },
                {
                    "title": "Stochastic backpropagation and approximate inference in deep generative models",
                    "authors": [
                        "Danilo Jimenez Rezende",
                        "Shakir Mohamed",
                        "Daan Wierstra"
                    ],
                    "venue": "arXiv preprint arXiv:1401.4082,",
                    "year": 2014
                },
                {
                    "title": "High-dimensional probability estimation with deep density models",
                    "authors": [
                        "Oren Rippel",
                        "Ryan Prescott Adams"
                    ],
                    "venue": "arXiv preprint arXiv:1302.5125,",
                    "year": 2013
                },
                {
                    "title": "Learning representations by backpropagating errors",
                    "authors": [
                        "David E Rumelhart",
                        "Geoffrey E Hinton",
                        "Ronald J Williams"
                    ],
                    "venue": "Cognitive modeling,",
                    "year": 1988
                },
                {
                    "title": "Imagenet large scale visual recognition challenge",
                    "authors": [
                        "Olga Russakovsky",
                        "Jia Deng",
                        "Hao Su",
                        "Jonathan Krause",
                        "Sanjeev Satheesh",
                        "Sean Ma",
                        "Zhiheng Huang",
                        "Andrej Karpathy",
                        "Aditya Khosla",
                        "Michael Bernstein"
                    ],
                    "venue": "International Journal of Computer Vision,",
                    "year": 2015
                },
                {
                    "title": "Deep boltzmann machines",
                    "authors": [
                        "Ruslan Salakhutdinov",
                        "Geoffrey E Hinton"
                    ],
                    "venue": "In International conference on artificial intelligence and statistics,",
                    "year": 2009
                },
                {
                    "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
                    "authors": [
                        "Tim Salimans",
                        "Diederik P Kingma"
                    ],
                    "venue": "arXiv preprint arXiv:1602.07868,",
                    "year": 2016
                },
                {
                    "title": "Markov chain monte carlo and variational inference: Bridging the gap",
                    "authors": [
                        "Tim Salimans",
                        "Diederik P Kingma",
                        "Max Welling"
                    ],
                    "venue": "arXiv preprint arXiv:1410.6460,",
                    "year": 2014
                },
                {
                    "title": "Mean field theory for sigmoid belief networks",
                    "authors": [
                        "Lawrence K Saul",
                        "Tommi Jaakkola",
                        "Michael I Jordan"
                    ],
                    "venue": "Journal of artificial intelligence research,",
                    "year": 1996
                },
                {
                    "title": "Very deep convolutional networks for large-scale image recognition",
                    "authors": [
                        "Karen Simonyan",
                        "Andrew Zisserman"
                    ],
                    "venue": "arXiv preprint arXiv:1409.1556,",
                    "year": 2014
                },
                {
                    "title": "Information processing in dynamical systems: Foundations of harmony theory",
                    "authors": [
                        "Paul Smolensky"
                    ],
                    "venue": "Technical report, DTIC Document,",
                    "year": 1986
                },
                {
                    "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
                    "authors": [
                        "Jascha Sohl-Dickstein",
                        "Eric A. Weiss",
                        "Niru Maheswaranathan",
                        "Surya Ganguli"
                    ],
                    "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
                    "year": 2015
                },
                {
                    "title": "Resnet in resnet",
                    "authors": [
                        "Sasha Targ",
                        "Diogo Almeida",
                        "Kevin Lyman"
                    ],
                    "venue": "Generalizing residual architectures. CoRR,",
                    "year": 2016
                },
                {
                    "title": "Generative image modeling using spatial lstms",
                    "authors": [
                        "Lucas Theis",
                        "Matthias Bethge"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2015
                },
                {
                    "title": "A note on the evaluation of generative models",
                    "authors": [
                        "Lucas Theis",
                        "A\u00e4ron Van Den Oord",
                        "Matthias Bethge"
                    ],
                    "venue": "CoRR, abs/1511.01844,",
                    "year": 2015
                },
                {
                    "title": "Variational gaussian process",
                    "authors": [
                        "Dustin Tran",
                        "Rajesh Ranganath",
                        "David M Blei"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06499,",
                    "year": 2015
                },
                {
                    "title": "Rnade: The real-valued neural autoregressive densityestimator",
                    "authors": [
                        "Benigno Uria",
                        "Iain Murray",
                        "Hugo Larochelle"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2013
                },
                {
                    "title": "An unsupervised ensemble learning method for nonlinear dynamic state-space models",
                    "authors": [
                        "Harri Valpola",
                        "Juha Karhunen"
                    ],
                    "venue": "Neural computation,",
                    "year": 2002
                },
                {
                    "title": "Order matters: Sequence to sequence for sets",
                    "authors": [
                        "Oriol Vinyals",
                        "Samy Bengio",
                        "Manjunath Kudlur"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06391,",
                    "year": 2015
                },
                {
                    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
                    "authors": [
                        "Ronald J Williams"
                    ],
                    "venue": "Machine learning,",
                    "year": 1992
                },
                {
                    "title": "Multi-scale context aggregation by dilated convolutions",
                    "authors": [
                        "Fisher Yu",
                        "Vladlen Koltun"
                    ],
                    "venue": "arXiv preprint arXiv:1511.07122,",
                    "year": 2015
                },
                {
                    "title": "Construction of a large-scale image dataset using deep learning with humans in the loop",
                    "authors": [
                        "Fisher Yu",
                        "Yinda Zhang",
                        "Shuran Song",
                        "Ari Seff",
                        "Jianxiong Xiao"
                    ],
                    "venue": "arXiv preprint arXiv:1506.03365,",
                    "year": 2015
                }
            ],
            "id": "SP:0d64188134c96729a9a6f325b45f15cacbd71d29",
            "authors": [
                {
                    "name": "Laurent Dinh",
                    "affiliations": []
                },
                {
                    "name": "Jascha Sohl-Dickstein",
                    "affiliations": []
                },
                {
                    "name": "Samy Bengio",
                    "affiliations": []
                }
            ],
            "abstractText": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
            "title": "Density estimation using Real NVP"
        }
    },
    "72992946": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "Ordinary recurrent neural networks typically have two types of memory that have very different time scales, very different capacities and very different computational roles. The history of the sequence currently being processed is stored in the hidden activity vector, which acts as a short-term memory that is updated at every time step. The capacity of this memory is O(H) where H is the number of hidden units. Long-term memory about how to convert the current input and hidden vectors into the next hidden vector and a predicted output vector is stored in the weight matrices connecting the hidden units to themselves and to the inputs and outputs. These matrices are typically updated at the end of a sequence and their capacity is O(H2) +O(IH) +O(HO) where I and O are the numbers of input and output units.\nLong short-term memory networks Hochreiter and Schmidhuber [1997] are a more complicated type of RNN that work better for discovering long-range structure in sequences for two main reasons: First, they compute increments to the hidden activity vector at each time step rather than recomputing the full vector1. This encourages information in the hidden states to persist for much longer. Second, they allow the hidden activities to determine the states of gates that scale the effects of the weights. These multiplicative interactions allow the effective weights to be dynamically adjusted by the input or hidden activities via the gates. However, LSTMs are still limited to a short-term memory capacity of O(H) for the history of the current sequence.\nSeveral researchers Hinton and Plaut [1987], Schmidhuber [1992] have suggested that neural networks could benefit from a third form of memory that has much higher storage capacity than the neural activities but much faster dynamics than the standard \u201cslow\u201d weights. This memory could store information specific to the history of the current sequence so that this information is available to influence the ongoing processing without using up the memory capacity of the hidden activities.\n1This assumes the \u201cremember gates \u201d of the LSTM memory cells are set to one.\nar X\niv :1\n61 0.\n06 25\n8v 1\n[ st\nat .M\nL ]\n2 0\nO ct\nUntil recently, however, there was surprisingly little investigation of other forms of memory in recurrent nets despite strong psychological evidence that it exists and obvious computational reasons why it was needed."
                },
                {
                    "heading": "2 Evidence from physiology that temporary memory may not be stored as neural activities",
                    "text": "Processes like working memory, attention, and priming operate on timescale of 100ms to minutes. This is simultaneously too slow to be mediated by neural activations without dynamical attractor states (10ms timescale) and too fast for long-term synaptic plasticity mechanisms to kick in (minutes to hours). While artificial neural network research has typically focused on methods to maintain temporary state in activation dynamics, that focus may be inconsistent with evidence that the brain also\u2014or perhaps primarily\u2014maintains temporary state information by short-term synaptic plasticity mechanisms Tsodyks et al. [1998], Abbott and Regehr [2004], Barak and Tsodyks [2007].\nThe brain implements a variety of short-term plasticity mechanisms that operate on intermediate timescale. For example, short term facilitation is implemented by leftover [Ca2+] in the axon terminal after depolarization while short term depression is implemented by presynaptic neurotransmitter depletion Zucker and Regehr [2002]. Spike-time dependent plasticity can also be invoked on this timescale Markram et al. [1997], Bi and Poo [1998]. These plasticity mechanisms are all synapsespecific. Thus they are more accurately modeled by a memory with O(H2) capacity than the O(H) of standard recurrent artificial recurrent neural nets and LSTMs."
                },
                {
                    "heading": "3 Fast Associative Memory",
                    "text": "One of the main preoccupations of neural network research in the 1970s and early 1980s Willshaw et al. [1969], Kohonen [1972], Anderson and Hinton [1981], Hopfield [1982] was the idea that memories were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns were reconstructed when needed from information stored in the weights of an associative network and the very same weights could store many different memories An auto-associative memory that has N2 weights cannot be expected to store more that N real-valued vectors with N components each. How close we can come to this upper bound depends on which storage rule we use. Hopfield nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately 0.15N binary vectors using weights that require log(N) bits each. Much more efficient use can be made of the weights by using an iterative, error correction storage rule to learn weights that can retrieve each bit of a pattern from all the other bits Gardner [1988], but for our purposes maximizing the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer product rule to store hidden activity vectors in fast weights that decay rapidly. The usual weights in an RNN will be called slow weights and they will learn by stochastic gradient descent in an objective function taking into account the fact that changes in the slow weights will lead to changes in what gets stored automatically in the fast associative memory.\nA fast associative memory has several advantages when compared with the type of memory assumed by a Neural Turing Machine (NTM) Graves et al. [2014], Neural Stack Grefenstette et al. [2015], or Memory Network Weston et al. [2014]. First, it is not at all clear how a real brain would implement the more exotic structures in these models e.g., the tape of the NTM, whereas it is clear that the brain could implement a fast associative memory in synapses with the appropriate dynamics. Second, in a fast associative memory there is no need to decide where or when to write to memory and where or when to read from memory. The fast memory is updated all the time and the writes are all superimposed on the same fast changing component of the strength of each synapse. Every time the input changes there is a transition to a new hidden state which is determined by a combination of three sources of information: The new input via the slow input-to-hidden weights, C, the previous hidden state via the slow transition weights, W , and the recent history of hidden state vectors via the fast weights, A. The effect of the first two sources of information on the new hidden state can be computed once and then maintained as a sustained boundary condition for a brief iterative settling process which allows the fast weights to influence the new hidden state. Assuming that the fast weights decay exponentially, we now show that the effect of the fast weights on the hidden vector during an iterative settling phase is to provide an additional input that is proportional to the sum over\nall recent hidden activity vectors of the scalar product of that recent hidden vector with the current hidden activity vector, with each term in this sum being weighted by the decay rate raised to the power of how long ago that hidden vector occurred. So fast weights act like a kind of attention to the recent past but with the strength of the attention being determined by the scalar product between the current hidden vector and the earlier hidden vector rather than being determined by a separate parameterized computation of the type used in neural machine translation models Bahdanau et al. [2015].\nThe update rule for the fast memory weight matrix, A, is simply to multiply the current fast weights by a decay rate, \u03bb, and add the outer product of the hidden state vector, h(t), multiplied by a learning rate, \u03b7:\nA(t+ 1) = \u03bbA(t) + \u03b7h(t)h(t)T (1)\nThe next vector of hidden activities, h(t + 1), is computed in two steps. The \u201cpreliminary\u201d vector h0(t + 1) is determined by the combined effects of the input vector x(t) and the previous hidden vector: h0(t + 1) = f(Wh(t) + Cx(t)), where W and C are slow weight matrices and f(.) is the nonlinearity used by the hidden units. The preliminary vector is then used to initiate an \u201cinner loop\u201d iterative process which runs for S steps and progressively changes the hidden state into h(t+ 1) = hS(t+ 1)\nhs+1(t+ 1) = f([Wh(t) + Cx(t)] +A(t)hs(t+ 1)), (2)\nwhere the terms inside [\u00b7] are the sustained boundary condition. In a real neural net, A could be implemented by rapidly changing synapses but in a computer simulation that uses sequences which have fewer time steps than the dimensionality of h, A will be of less than full rank and it is more efficient to compute the term A(t)hs(t + 1) without ever computing the full fast weight matrix, A. Assuming A is 0 at the beginning of the sequence,\nA(t) = \u03b7 \u03c4=t\u2211 \u03c4=1 \u03bbt\u2212\u03c4h(\u03c4)h(\u03c4)T (3)\nA(t)hs+1(t+ 1) = \u03b7 \u03c4=t\u2211 \u03c4=1 \u03bbt\u2212\u03c4h(\u03c4)[h(\u03c4)Ths(t+ 1)] (4)\nThe term in square brackets is just the scalar product of an earlier hidden state vector, h(\u03c4), with the current hidden state vector, hs(t+1), during the iterative inner loop. So at each iteration of the inner loop, the fast weight matrix is exactly equivalent to attending to past hidden vectors in proportion to their scalar product with the current hidden vector, weighted by a decay factor. During the inner loop iterations, attention will become more focussed on past hidden states that manage to attract the current hidden state.\nThe equivalence between using a fast weight matrix and comparing with a set of stored hidden state vectors is very helpful for computer simulations. It allows us to explore what can be done with fast weights without incurring the huge penalty of having to abandon the use of mini-batches during training. At first sight, mini-batches cannot be used because the fast weight matrix is different for every sequence, but comparing with a set of stored hidden vectors does allow mini-batches."
                },
                {
                    "heading": "3.1 Layer normalized fast weights",
                    "text": "A potential problem with fast associative memory is that the scalar product of two hidden vectors could vanish or explode depending on the norm of the hidden vectors. Recently, layer normalization Ba et al. [2016] has been shown to be very effective at stablizing the hidden state dynamics in RNNs and reducing training time. Layer normalization is applied to the vector of summed inputs to all the recurrent units at a particular time step. It uses the mean and variance of the components of this vector to re-center and re-scale those summed inputs. Then, before applying the nonlinearity, it includes a learned, neuron-specific bias and gain. We apply layer normalization to the fast associative memory as follows:\nhs+1(t+ 1) = f(LN [Wh(t) + Cx(t) +A(t)hs(t+ 1)]) (5) where LN [.] denotes layer normalization. We found that applying layer normalization on each iteration of the inner loop makes the fast associative memory more robust to the choice of learning rate and decay hyper-parameters. For the rest of the paper, fast weight models are trained using layer normalization and the outer product learning rule with fast learning rate of 0.5 and decay rate of 0.95, unless otherwise noted."
                },
                {
                    "heading": "4 Experimental results",
                    "text": "To demonstrate the effectiveness of the fast associative memory, we first investigated the problems of associative retrieval (section 4.1) and MNIST classification (section 4.2). We compared fast weight models to regular RNNs and LSTM variants. We then applied the proposed fast weights to a facial expression recognition task using a fast associative memory model to store the results of processing at one level while examining a sequence of details at a finer level (section 4.3). The hyper-parameters of the experiments were selected through grid search on the validation set. All the models were trained using mini-batches of size 128 and the Adam optimizer Kingma and Ba [2014]. A description of the training protocols and the hyper-parameter settings we used can be found in the Appendix. Lastly, we show that fast weights can also be used effectively to implement reinforcement learning agents with memory (section 4.4)."
                },
                {
                    "heading": "4.1 Associative retrieval",
                    "text": "We start by demonstrating that the method we propose for storing and retrieving temporary memories works effectively for a toy task to which it is very well suited. Consider a task where multiple key-value pairs are presented in a sequence. At the end of the sequence, one of the keys is presented and the model must predict the value that was temporarily associated with the key. We used strings that contained characters from English alphabet, together with the digits 0 to 9. To construct a training sequence, we first randomly sample a character from the alphabet without replacement. This is the first key. Then a single digit is sampled as the associated value for that key. After generating a sequence of K character-digit pairs, one of the K different characters is selected at random as the query and the network must predict the associated digit. Some examples of such string sequences and their targets are shown below:\nInput string Target c9k8j3f1??c 9 j0a5s5z2??a 5\nwhere \u2018?\u2019 is the token to separate the query from the key-value pairs. We generated 100,000 training examples, 10,000 validation examples and 20,000 test examples. To solve this task, a standard RNN has to end up with hidden activities that somehow store all of the key-value pairs after the keys and values are presented sequentially. This makes it a significant challenge for models only using slow weights.\nWe used a neural network with a single recurrent layer for this experiment. The recurrent network processes the input sequence one character at a time. The input character is first converted into a learned 100-dimensional embedding vector which then provides input to the recurrent layer2. The output of the recurrent layer at the end of the sequence is then processed by another hidden layer of 100 ReLUs before the final softmax layer. We augment the ReLU RNN with a fast associative memory and compare it to an LSTM model with the same architecture. Although the original LSTMs do not have explicit long-term storage capacity, recent work from Danihelka et. al. Danihelka et al. [2016] extended LSTMs by adding complex associative memory. In our experiments, we compared fast associative memory to both LSTM variants.\nFigure 1 and Table 1 show that when the number of recurrent units is small, the fast associative memory significantly outperforms the LSTMs with the same number of recurrent units. The result fits with our hypothesis that the fast associative memory allows the RNN to use its recurrent units more effectively. In addition to having higher retrieval accuracy, the model with fast weights also converges faster than the LSTM models."
                },
                {
                    "heading": "4.2 Integrating glimpses in visual attention models",
                    "text": "Despite their many successes, convolutional neural networks are computationally expensive and the representations they learn can be hard to interpret. Recently, visual attention models Mnih et al. [2014], Ba et al. [2015], Xu et al. [2015] have been shown to overcome some of the limitations in ConvNets. One can understand what signals the algorithm is using by seeing where the model is looking. Also, the visual attention model is able to selectively focus on important parts of visual space and thus avoid any detailed processing of much of the background clutter. In this section, we show that visual attention models can use fast weights to store information about object parts, though we use a very restricted set of glimpses that do not correspond to natural parts of the objects.\nGiven an input image, a visual attention model computes a sequence of glimpses over regions of the image. The model not only has to determine where to look next, but also has to remember what it has seen so far in its working memory so that it can make the correct classification later. Visual attention models can learn to find multiple objects in a large static input image and classify them correctly, but the learnt glimpse policies are typically over-simplistic: They only use a single scale of glimpses and they tend to scan over the image in a rigid way. Human eye movements and fixations are far more complex. The ability to focus on different parts of a whole object at different scales allows humans to apply the very same knowledge in the weights of the network at many different scales, but it requires some form of temporary memory to allow the network to integrate what it discovered in a set of glimpses. Improving the model\u2019s ability to remember recent glimpses should help the visual attention model to discover non-trivial glimpse policies. Because the fast weights can store all the glimpse information in the sequence, the hidden activity vector is freed up to learn how to intelligently integrate visual information and retrieve the appropriate memory content for the final classifier.\nTo explicitly verify that larger memory capacity is beneficial to visual attention-based models, we simplify the learning process in the following way: First, we provide a pre-defined glimpse control signal so the model knows where to attend rather than having to learn the control policy through reinforcement learning. Second, we introduce an additional control signal to the memory cells so\n2To make the architecture for this task more similar to the architecture for the next task we first compute a 50 dimensional embedding vector and then expand this to a 100-dimensional embedding.\nthe attention model knows when to store the glimpse information. A typical visual attention model is complex and has high variance in its performance due to the need to learn the policy network and the classifier at the same time. Our simplified learning procedure enables us to discern the performance improvement contributed by using fast weights to remember the recent past.\nWe consider a simple recurrent visual attention model that has a similar architecture to the RNN from the previous experiment. It does not predict where to attend but rather is given a fixed sequence of locations: the static input image is broken down into four non-overlapping quadrants recursively with two scale levels. The four coarse regions, down-sampled to 7 \u00d7 7, along with their the four 7\u00d77 quadrants are presented in a single sequence as shown in Figure 1. Notice that the two glimpse scales form a two-level hierarchy in the visual space. In order to solve this task successfully, the attention model needs to integrate the glimpse information from different levels of the hierarchy. One solution is to use the model\u2019s hidden states to both store and integrate the glimpses of different scales. A much more efficient solution is to use a temporary \u201ccache\u201d to store any of the unfinished glimpse computation when processing the glimpses from a finer scale in the hierarchy. Once the computation is finished at that scale, the results can be integrated with the partial results at the higher level by \u201cpopping\u201d the previous result from the \u201ccache\u201d. Fast weights, therefore, can act as a neurally plausible \u201ccache\u201d for storing partial results. The slow weights of the same model can then specialize in integrating glimpses at the same scale. Because the slow weights are shared for all glimpse scales, the model should be able to store the partial results at several levels in the same set of fast weights, though we have only demonstrated the use of fast weights for storage at a single level.\nWe evaluated the multi-level visual attention model on the MNIST handwritten digit dataset. MNIST is a well-studied problem on which many other techniques have been benchmarked. It contains the ten classes of handwritten digits, ranging from 0 to 9. The task is to predict the class label of an isolated and roughly normalized 28x28 image of a digit. The glimpse sequence, in this case, consists of 24 patches of 7\u00d7 7 pixels.\nTable 2 compares classification results for a ReLU RNN with a multi-level fast associative memory against an LSTM that gets the same sequence of glimpses. Again the result shows that when the number of hidden units is limited, fast weights give a significant improvement over the other models. As we increase the memory capacities, the multi-level fast associative memory consistently outperforms the LSTM in classification accuracy.\nUnlike models that must integrate a sequence of glimpses, convolutional neural networks process all the glimpses in parallel and use layers of hidden units to hold all their intermediate computational results. We further demonstrate the effectiveness of the fast weights by comparing to a three-layer convolutional neural network that uses the same patches as the glimpses presented to the visual attention model. From Table 2, we see that the multi-level model with fast weights reaches a very similar performance to the ConvNet model without requiring any biologically implausible weight sharing."
                },
                {
                    "heading": "4.3 Facial expression recognition",
                    "text": "To further investigate the benefits of using fast weights in the multi-level visual attention model, we performed facial expression recognition tasks on the CMU Multi-PIE face database Gross et al. [2010]. The dataset was preprocessed to align each face by eyes and nose fiducial points. It was downsampled to 48 \u00d7 48 greyscale. The full dataset contains 15 photos taken from cameras with different viewpoints for each illumination \u00d7 expression \u00d7 identity \u00d7 session condition. We used only the images taken from the three central cameras corresponding to \u221215\u25e6, 0\u25e6, 15\u25e6 views since facial expressions were not discernible from the more extreme viewpoints. The resulting dataset contained > 100, 000 images. 317 identities appeared in the training set with the remaining 20 identities in the test set.\nGiven the input face image, the goal is to classify the subject\u2019s facial expression into one of the six different categories: neutral, smile, surprise, squint, disgust and scream. The task is more realistic and challenging than the previous MNIST experiments. Not only does the dataset have unbalanced numbers of labels, some of the expressions, for example squint and disgust, are are very hard to distinguish. In order to perform well on this task, the models need to generalize over different lighting conditions and viewpoints. We used the same multi-level attention model as in the MNIST experiments with 200 recurrent hidden units. The model sequentially attends to non-overlapping 12x12 pixel patches at two different scales and there are, in total, 24 glimpses. Similarly, we designed a two layer ConvNet that has a 12x12 receptive fields.\nFrom Table 3, we see that the multi-level fast weights model that knows when to store information outperforms the LSTM and the IRNN. The results are consistent with previous MNIST experiments. However, ConvNet is able to perform better than the multi-level attention model on this near frontal\nface dataset. We think the efficient weight-sharing and architectural engineering in the ConvNet combined with the simultaneous availability of all the information at each level of processing allows the ConvNet to generalize better in this task. Our use of a rigid and predetermined policy for where to glimpse eliminates one of the main potential advantages of the multi-level attention model: It can process informative details at high resolution whilst ignoring most of the irrelevant details. To realize this advantage we will need to combine the use of fast weights with the learning of complicated policies."
                },
                {
                    "heading": "4.4 Agents with memory",
                    "text": "While different kinds of memory and attention have been studied extensively in the supervised learning setting Graves [2014], Mnih et al. [2014], Bahdanau et al. [2015], the use of such models for learning long range dependencies in reinforcement learning has received less attention.\nWe compare different memory architectures on a partially observable variant of the game \u201dCatch\u201d described in Mnih et al. [2014]. The game is played on an N \u00d7N screen of binary pixels and each episode consists of N frames. Each trial begins with a single pixel, representing a ball, appearing somewhere in the first row of the column and a two pixel \u201dpaddle\u201d controlled by the agent in the bottom row. After observing a frame, the agent gets to either keep the paddle stationary or move it right or left by one pixel. The ball descends by a single pixel after each frame. The episode ends when the ball pixel reaches the bottom row and the agent receives a reward of +1 if the paddle touches the ball and a reward of\u22121 if it doesn\u2019t. Solving the fully observable task is straightforward and requires the agent to move the paddle to the column with the ball. We make the task partiallyobservable by providing the agent blank observations after the M th frame. Solving the partiallyobservable version of the game requires remembering the position of the paddle and ball after M frames and moving the paddle to the correct position using the stored information.\nWe used the recently proposed asynchronous advantage actor-critic method Mnih et al. [2016] to train agents with three types of memory on different sizes of the partially observable Catch task. The three agents included a ReLU RNN, an LSTM, and a fast weights RNN. Figure 5 shows learning progress of the different agents on two variants of the game N = 16,M = 3 and N = 24,M = 5. The agent using the fast weights architecture as its policy representation (shown in green) is able to learn faster than the agents using ReLU RNN or LSTM to represent the policy. The improvement obtained by fast weights is also more significant on the larger version of the game which requires more memory."
                },
                {
                    "heading": "5 Conclusion",
                    "text": "This paper contributes to machine learning by showing that the performance of RNNs on a variety of different tasks can be improved by introducing a mechanism that allows each new state of the hidden units to be attracted towards recent hidden states in proportion to their scalar products with the current state. Layer normalization makes this kind of attention work much better. This is a form of attention to the recent past that is somewhat similar to the attention mechanism that has recently been used to dramatically improve the sequence-to-sequence RNNs used in machine translation.\nThe paper has interesting implications for computational neuroscience and cognitive science. The ability of people to recursively apply the very same knowledge and processing apparatus to a whole sentence and to an embedded clause within that sentence or to a complex object and to a major part of that object has long been used to argue that neural networks are not a good model of higher-level cognitive abilities. By using fast weights to implement an associative memory for the recent past, we have shown how the states of neurons could be freed up so that the knowledge in the connections of a neural network can be applied recursively. This overcomes the objection that these models can only do recursion by storing copies of neural activity vectors, which is biologically implausible."
                },
                {
                    "heading": "A Experimental details",
                    "text": "A.1 Associative retrieval\nWe used a single hidden layer recurrent neural network which takes a 100 dimensional embedding vector as its input. We compared the fast weights memory against three other different RNN architecture: IRNN, standard LSTM and associative LSTM. The non-recurrent slow recurrent weights are initialized from uniform distribution between (\u2212 \u221a H, \u221a H), where H is the number outgoing weights. The slow weights learning rate is tuned using the 10,000 validation examples.\nBelow, we provide the specific hyper-parameter settings for the models used in the experiments:\nFast weights: The fast weights learning rate, \u03b7, is set to 0.5 and the fast weights decay rate, \u03bb, is set to 0.9. The fast weights are updated once at every time step. We experimented with more iterations for the \u201cinner loop\u201d and the performance are similar. The recurrent slow weights are initialized to an identity matrix scaled by 0.05. We use the ReLU activation for f(\u00b7) in the recurrent layer.\nIRNN: The recurrent slow weights are initialized to an identity matrix scaled by 0.5. ReLU is used as the non-linearity in the recurrent layer.\nAssociative LSTM: We used 4 copies of memory cells for the associative LSTM. There are 3 readwrite heads used for storage and retrieval memory access.\nA.2 Integrating glimpses in visual attention models: MNIST and Facial expression recognition\nBoth tasks used the similar parameter initialization and the hyper-parameter setup that are comparable to the associative retrieval task mentioned above.\nA.3 Agents with memory\nAll agents used recurrent networks to represent the policy. At each time step the input was passed through a hidden layer with 128 ReLU units and then passed to the recurrent core. All agents used 128 recurrent cells. The output at every step was a softmax over the valid actions and a single linear output for the estimate of the value function. We used random search to find hyperparameters values for the learning rate, the number of Hebbian steps, and fast weight learning rate and decay where applicable. We averaged results over the top 5 models.\nB Implementing the fast weights \u201cinner loop\u201d in biological neural networks\nWe considered two different ways of performing this inner loop settling. In method 1 (which is what we use) the inputs to the hidden units after an outer loop transition using W are stored and provide sustained boundary conditions during the inner loop settling. In method 2 (which is more biologically plausible) we simply add the identity matrix to the fast weight matrix so that the inner loop settling tends to sustain the hidden activity vector. For ReLUs, these two methods are equivalent when the fast weight matrix is zero . They are similar but not exactly equivalent when the fast weights are non-zero. Using layer normalization, we found that method 1 worked slightly better than Method 2, but Method 2 would be much easier to implement in a biological network."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Models of information processing in the brain",
                    "authors": [
                        "James A Anderson",
                        "Geoffrey E Hinton"
                    ],
                    "venue": "Parallel models of associative memory,",
                    "year": 1981
                },
                {
                    "title": "Multiple object recognition with visual attention",
                    "authors": [
                        "J. Ba",
                        "V. Mnih",
                        "K. Kavukcuoglu"
                    ],
                    "venue": "In International Conference on Learning Representations,",
                    "year": 2015
                },
                {
                    "title": "Neural machine translation by jointly learning to align and translate",
                    "authors": [
                        "D. Bahdanau",
                        "K. Cho",
                        "Y. Bengio"
                    ],
                    "venue": "In International Conference on Learning Representations,",
                    "year": 2015
                },
                {
                    "title": "Persistent activity in neural networks with dynamic synapses",
                    "authors": [
                        "Omri Barak",
                        "Misha Tsodyks"
                    ],
                    "venue": "PLoS Comput Biol,",
                    "year": 2007
                },
                {
                    "title": "Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type",
                    "authors": [
                        "Guo-qiang Bi",
                        "Mu-ming Poo"
                    ],
                    "venue": "The Journal of neuroscience,",
                    "year": 1998
                },
                {
                    "title": "Associative long short-term memory",
                    "authors": [
                        "Ivo Danihelka",
                        "Greg Wayne",
                        "Benigno Uria",
                        "Nal Kalchbrenner",
                        "Alex Graves"
                    ],
                    "venue": "arXiv preprint arXiv:1602.03032,",
                    "year": 2016
                },
                {
                    "title": "The space of interactions in neural network models",
                    "authors": [
                        "Elizabeth Gardner"
                    ],
                    "venue": "Journal of physics A: Mathematical and general,",
                    "year": 1988
                },
                {
                    "title": "Generating sequences with recurrent neural networks",
                    "authors": [
                        "A. Graves"
                    ],
                    "year": 2014
                },
                {
                    "title": "Neural turing machines",
                    "authors": [
                        "Alex Graves",
                        "Greg Wayne",
                        "Ivo Danihelka"
                    ],
                    "venue": "arXiv preprint arXiv:1410.5401,",
                    "year": 2014
                },
                {
                    "title": "Learning to transduce with unbounded memory",
                    "authors": [
                        "Edward Grefenstette",
                        "Karl Moritz Hermann",
                        "Mustafa Suleyman",
                        "Phil Blunsom"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2015
                },
                {
                    "title": "Using fast weights to deblur old memories",
                    "authors": [
                        "Geoffrey E Hinton",
                        "David C Plaut"
                    ],
                    "venue": "In Proceedings of the ninth annual conference of the Cognitive Science Society,",
                    "year": 1987
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "Sepp Hochreiter",
                        "J\u00fcrgen Schmidhuber"
                    ],
                    "venue": "Neural computation,",
                    "year": 1997
                },
                {
                    "title": "Neural networks and physical systems with emergent collective computational abilities",
                    "authors": [
                        "John J Hopfield"
                    ],
                    "venue": "Proceedings of the national academy of sciences,",
                    "year": 1982
                },
                {
                    "title": "Adam: a method for stochastic optimization",
                    "authors": [
                        "D. Kingma",
                        "J.L. Ba"
                    ],
                    "year": 2014
                },
                {
                    "title": "Correlation matrix memories",
                    "authors": [
                        "Teuvo Kohonen"
                    ],
                    "venue": "Computers, IEEE Transactions on,",
                    "year": 1972
                },
                {
                    "title": "Regulation of synaptic efficacy by coincidence of postsynaptic aps and epsps",
                    "authors": [
                        "Henry Markram",
                        "Joachim L\u00fcbke",
                        "Michael Frotscher",
                        "Bert Sakmann"
                    ],
                    "year": 1997
                },
                {
                    "title": "Recurrent models of visual attention",
                    "authors": [
                        "V. Mnih",
                        "N. Heess",
                        "A. Graves",
                        "K. Kavukcuoglu"
                    ],
                    "venue": "In Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "Asynchronous methods for deep reinforcement learning",
                    "authors": [
                        "Volodymyr Mnih",
                        "Adria Puigdomenech Badia",
                        "Mehdi Mirza",
                        "Alex Graves",
                        "Timothy P Lillicrap",
                        "Tim Harley",
                        "David Silver",
                        "Koray Kavukcuoglu"
                    ],
                    "venue": "In International Conference on Machine Learning,",
                    "year": 2016
                }
            ],
            "id": "SP:2d097d720c2332aff57179b0947034c2d3f62354",
            "authors": [
                {
                    "name": "Jimmy Ba",
                    "affiliations": []
                },
                {
                    "name": "Geoffrey Hinton",
                    "affiliations": []
                },
                {
                    "name": "Joel Z. Leibo",
                    "affiliations": []
                },
                {
                    "name": "Catalin Ionescu",
                    "affiliations": []
                }
            ],
            "abstractText": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \u201cfast weights\u201d can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.",
            "title": "Using Fast Weights to Attend to the Recent Past"
        }
    },
    "16280529": {
        "X": {
            "sections": [
                {
                    "text": "ar X\niv :1\n60 7.\n05 69\n0v 1\n[ cs\n.N E\n] 1"
                },
                {
                    "heading": "General Result",
                    "text": "Let f(x) be a probability density function (PDF) over x \u2208 RD and cumulative density function (CDF) F (x). f can be rewritten as\nf(x) =\nD \u220f\nd=1\nfd(xd|x<d) (1)\nwhere x<d = x1, . . . , xd\u22121, f1(x1|x<1) = f1(x1) and fd is the marginal PDF of xd conditioned on x<d. A sample x\u0302 can be drawn from f using the multivariate quantile transform: first draw a vector of D independent samples\nu = (u1, . . . , uD) from U(0, 1), then recursively define x\u0302 as\nx\u03021 = F \u22121 1 (u1) (2) x\u0302d = F \u22121 d (ud|x\u0302<d) (3)\nwhere F\u22121d is the quantile function (inverse CDF) corresponding to the PDF fd. Inverting Eq. 3 and applying the definition of a univariate CDF yields\nFd(x\u0302d|x\u0302<d) =\n\u222b x\u0302d\nt=\u2212\u221e\nfd(t|x\u0302<d)dt = ud (4)\nAssume that f depends on some parameter \u03b8. The general form of Leibniz integral rule tells us that\n\u2202Fd(x\u0302d|x\u0302<d)\n\u2202\u03b8 = fd(x\u0302d|x\u0302<d)\n\u2202x\u0302d\n\u2202\u03b8 +\n\u222b x\u0302d\nt=\u2212\u221e\n\u2202fd(t|x\u0302<d)\n\u2202\u03b8 dt =\n\u2202ud\n\u2202\u03b8 = 0 (5)\nand therefore \u2202x\u0302d\n\u2202\u03b8 = \u2212\n1\nfd(x\u0302d|x\u0302<d)\n\u222b x\u0302d\nt=\u2212\u221e\n\u2202fd(t|x\u0302<d)\n\u2202\u03b8 dt (6)\nIf the above integral is intractable it can be estimated with Monte-Carlo sampling, as long as fd(t|x\u0302<d) can be sampled and Fd(x\u0302d|x\u0302<d) is tractable:\n\u222b x\u0302d\nt=\u2212\u221e\n\u2202fd(t|x\u0302<d)\n\u2202\u03b8 dt =\n\u222b x\u0302d\nt=\u2212\u221e\nfd(t|x\u0302<d) \u2202 log fd(t|x\u0302<d)\n\u2202\u03b8 dt (7)\n= Fd(x\u0302d|x\u0302<d)\n\u222b \u221e\nt=\u2212\u221e\nfd(t \u2264 x\u0302d|x\u0302<d) \u2202 log fd(t|x\u0302<d)\n\u2202\u03b8 dt (8)\n\u2248 Fd(x\u0302d|x\u0302<d)\nN\nN \u2211\nn=1\n\u2202 log fd(t n|x\u0302<d)\n\u2202\u03b8 ; tn \u223c fd(t \u2264 x\u0302d|x\u0302<d) (9)\nwhere\nfd(t \u2264 x\u0302d|x\u0302<d) =\n{\nfd(t|x\u0302<d) Fd(x\u0302d|x\u0302<d) if t \u2264 x\u0302d 0 otherwise (10)\nwhich can be sampled by drawing from fd(t|x\u0302<d) and rejecting the result if it is greater than x\u0302d.\nLet h be the expectation over f of an arbitrary differentiable function g of x (e.g. a loss function) and denote by Q(u) the sample from f returned by the quantile transform applied to u. Then\nh =\n\u222b\nu\u2208[0,1]D g(Q(u))du (11)\nand hence\n\u2202h \u2202\u03b8 =\n\u222b\nu\u2208[0,1]D\n\u2202g(Q(u))\n\u2202\u03b8 du (12)\n=\n\u222b\nu\u2208[0,1]D\nD \u2211\nd=1\n\u2202g(Q(u))\n\u2202Qd(u)\n\u2202Qd(u)\n\u2202\u03b8 du (13)\nwhich can be estimated with Monte-Carlo sampling:\n\u2202h \u2202\u03b8 \u2248 1 N\nN \u2211\nn=1\nD \u2211\nd=1\n\u2202g(xn)\n\u2202xnd\n\u2202xnd \u2202\u03b8\n(14)\nwhere xn \u223c f(x). Note that the above estimator does not require Q to be known, as long as f can be sampled."
                },
                {
                    "heading": "Application to Mixture Density Weights",
                    "text": "If f is a mixture density distribution with K components then\nf(x) = K \u2211\nk=1\n\u03c0kf k(x) (15)\nand\nfd(xd|x<d) =\nK \u2211\nk=1\nPr(k|x<d)f k d (xd|x<d) (16)\nwhere Pr(k|x<d) is the posterior responsibility of the component k, given the prior mixture density weight \u03c0k and the observation sequence x<d.\nIn what follows we will assume that the mixture components have diagonal covariance, so that fkd (xd|x<d) = f k d (xd). It should be possible to extend the analysis to non-diagonal components, but that is left for future work. Abbreviating Pr(k|x<d) to p k d and applying the diagonal covariance of the components, Eq. 16 becomes\nfd(xd|x<d) = \u2211\nk\npkdf k d (xd) (17)\nwhere pkd is defined by the following recurrence relation:\npk1 = \u03c0k (18) pkd = pkd\u22121f k d\u22121(xd\u22121)\nfd\u22121(xd\u22121|x<d\u22121) (19)\nWe seek the derivatives of h with respect to the mixture weights \u03c0j , after the weights have been normalised (e.g. by a softmax function). Setting xd = t and differentiating Eq. 17 gives\n\u2202fd(t|x<d)\n\u2202\u03c0j =\n\u2211\nk\n[\n\u2202pkd \u2202\u03c0j fkd (t) + \u2202fkd (t) \u2202t \u2202t \u2202\u03c0j pkd\n]\n(20)\nSetting x = x\u0302 where x\u0302 is a sample drawn from f , and observing that \u2202t \u2202\u03c0j = 0 if\nt does not depend on f , we can substitute the above into Eq. 6 to get\n\u2202x\u0302d \u2202\u03c0j = \u2212\n1\nfd(x\u0302d|x\u0302<d)\n\u2211\nk\n\u2202pkd \u2202\u03c0j \u222b x\u0302d\nt=\u2212\u221e\nfkd (t)dt (21)\n= \u2212 1\nfd(x\u0302d|x\u0302<d)\n\u2211\nk\n\u2202 log pkd \u2202\u03c0j pkdF k d (x\u0302d) (22)\nDifferentiating Eq. 19 yields (after some rearrangement)\n\u2202 log pkd \u2202\u03c0j = \u2202 log pkd\u22121 \u2202\u03c0j \u2212 \u2211\nl\npld \u2202 log pld\u22121\n\u2202\u03c0j (23)\n+\n[\n\u2202 log fkd\u22121(x\u0302d\u22121)\n\u2202x\u0302d\u22121 \u2212 \u2211\nl\npld \u2202 log f ld\u22121(x\u0302d\u22121)\n\u2202x\u0302d\u22121\n]\n\u2202x\u0302d\u22121\n\u2202\u03c0j (24)\n\u2202 log pkd \u2202\u03c0j and \u2202x\u0302d \u2202\u03c0j can then be obtained with a joint recursion, starting from the initial conditions\n\u2202 log pk1 \u2202\u03c0j = \u03b4jk \u03c0j (25)\n\u2202x\u03021 \u2202\u03c0j = \u2212\nF j 1 (x\u03021) f1(x\u03021) (26)\nWe are now ready to approximate \u2202h \u2202\u03c0j by substituting into Eq. 14:\n\u2202h\n\u2202\u03c0j \u2248\n1\nN\nN \u2211\nn=1\nD \u2211\nd=1\n\u2202g(xn)\n\u2202xnd \u2202xnd \u2202\u03c0j ; xn \u223c f(x) (27)\nPseudocode for the complete computation is provided in Algorithm 1.\ninitialise \u2202h \u2202\u03c0j \u2190 0 for n = 1 to N do draw x \u223c f(x) pk1 \u2190 \u03c0k \u2202 log pk\n1 \u2202\u03c0j \u2190 \u03b4jk \u03c0j\n\u2202x1 \u2202\u03c0j \u2190 \u2212\nF j 1 (x1) f1(x1)\nf1(x1)\u2190 \u2211 k \u03c0kf k 1 (x1) for d = 2 to D do fd(xd|x<d)\u2190 \u2211 k p k df k d (xd)\npkd \u2190 pkd\u22121f k d\u22121(xd\u22121)\nfd\u22121(xd\u22121|x<d\u22121)\n\u2202 log pkd \u2202\u03c0j \u2190 \u2202 log pkd\u22121 \u2202\u03c0j \u2212 \u2211 l p l d \u2202 log pld\u22121 \u2202\u03c0j +\n\u2202xd\u22121 \u2202\u03c0j\n[\n\u2202 log fkd\u22121(xd\u22121)\n\u2202xd\u22121 \u2212 \u2211 l p l d\n\u2202 log f ld\u22121(xd\u22121)\n\u2202xd\u22121\n]\n\u2202xd \u2202\u03c0j \u2190 \u2212 1 fd(xd|x<d)\n\u2211\nk \u2202 log pkd \u2202\u03c0j pkdF k d (xd)\nend for \u2202h \u2202\u03c0j \u2190 \u2202h \u2202\u03c0j + \u2211 d \u2202g(x) \u2202xd \u2202xd \u2202\u03c0j\nend for \u2202h \u2202\u03c0j \u2190 1 N \u2202h \u2202\u03c0j\nAlgorithm 1: Stochastic Backpropagation through Mixture Density Weights"
                },
                {
                    "heading": "Acknowledgements",
                    "text": "Useful discussions and comments were provided by Ivo Danihelka, Danilo Rezende, Remi Munos, Diederik Kingma, Charles Blundell, Mevlana Gemici, Nando de Freitas, and Andriy Mnih."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Weight Uncertainty in Neural Networks",
                    "authors": [
                        "C. Blundell",
                        "J. Cornebise",
                        "K. Kavukcuoglu",
                        "D. Wierstra"
                    ],
                    "venue": "ArXiv e-prints, May",
                    "year": 2015
                },
                {
                    "title": "Practical variational inference for neural networks",
                    "authors": [
                        "A. Graves"
                    ],
                    "venue": "Advances in Neural Information Processing Systems, volume 24, pages 2348\u20132356.",
                    "year": 2011
                },
                {
                    "title": "DRAW: A recurrent neural network for image generation",
                    "authors": [
                        "K. Gregor",
                        "I. Danihelka",
                        "A. Graves",
                        "D. Wierstra"
                    ],
                    "venue": "ArXiv e-prints, March",
                    "year": 2015
                },
                {
                    "title": "Deep autoregressive networks",
                    "authors": [
                        "K. Gregor",
                        "I. Danihelka",
                        "A. Mnih",
                        "C. Blundell",
                        "D. Wierstra"
                    ],
                    "venue": "Proceedings of the 31st International Conference on Machine Learning,",
                    "year": 2014
                },
                {
                    "title": "Variational dropout and the local reparameterization trick",
                    "authors": [
                        "D.P. Kingma",
                        "T. Salimans",
                        "M. Welling"
                    ],
                    "venue": "ArXiv e-prints, June",
                    "year": 2015
                },
                {
                    "title": "Auto-encoding variational bayes",
                    "authors": [
                        "D.P. Kingma",
                        "M. Welling"
                    ],
                    "venue": "Proceedings of the International Conference on Learning Representations,",
                    "year": 2014
                },
                {
                    "title": "Stochastic backpropagation and approximate inference in deep generative models",
                    "authors": [
                        "D.J. Rezende",
                        "S. Mohamed",
                        "D. Wierstra"
                    ],
                    "venue": "Proceedings of the 31st International Conference on Machine Learning, pages 1278\u20131286,",
                    "year": 2014
                }
            ],
            "id": "SP:9432cf3f7b661473bd64153e8c6f362332197c1f",
            "authors": [
                {
                    "name": "Alex Graves",
                    "affiliations": []
                }
            ],
            "abstractText": "The ability to backpropagate stochastic gradients through continuous latent distributions has been crucial to the emergence of variational autoencoders [4, 6, 7, 3] and stochastic gradient variational Bayes [2, 5, 1]. The key ingredient is an unbiased and low-variance way of estimating gradients with respect to distribution parameters from gradients evaluated at distribution samples. The \u201creparameterization trick\u201d [6] provides a class of transforms yielding such estimators for many continuous distributions, including the Gaussian and other members of the location-scale family. However the trick does not readily extend to mixture density models, due to the difficulty of reparameterizing the discrete distribution over mixture weights. This report describes an alternative transform, applicable to any continuous multivariate distribution with a differentiable density function from which samples can be drawn, and uses it to derive an unbiased estimator for mixture density weight derivatives. Combined with the reparameterization trick applied to the individual mixture components, this estimator makes it straightforward to train variational autoencoders with mixture-distributed latent variables, or to perform stochastic variational inference with a mixture density variational posterior. General Result Let f(x) be a probability density function (PDF) over x \u2208 R and cumulative density function (CDF) F (x). f can be rewritten as",
            "title": "Stochastic Backpropagation through Mixture Density Distributions"
        }
    },
    "18000122": {
        "X": {
            "sections": [
                {
                    "text": "Background Neural word embeddings have been widely used in biomedical Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words. Many biomedical applications use different textual resources (e.g., Wikipedia and biomedical articles) to train word embeddings and apply these word embeddings to downstream biomedical applications. However, there has been little work on evaluating the word embeddings trained from these resources.\nMethods In this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings utilizing clinical notes available at Mayo Clinic and articles from the PubMed Central (PMC), respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen, Hliaoutakis, MayoSRS, and UMNSRS. In extrinsic evaluation, we applied word embeddings to downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), using data from shared tasks.\nResults Qualitative evaluation shows that the word embeddings trained from EHR and PubMed can find more relevant similar medical terms than these from GloVe and Google News. In the intrinsic quantitative evaluation, the semantic similarity captured by the word embeddings trained from EHR are closer to human experts\u2019 judgments on all four tested datasets. In the extrinsic quantitative evaluation, the word embeddings trained from EHR has the best F1 score of 0.900 for the clinical IE task; no word embeddings improve the performance in terms of inference average precision and mean average precision for the biomedical IR task; and the word embeddings trained on Google News has the best overall F1 score of 0.790 for the RE task.\n*Corresponding authors.\nar X\niv :1\n80 2.\n00 40\n0v 2\n[ cs\n.I R\n] 9\n2 Conclusion Based on the evaluation results, we can draw the following conclusions. First, the word embeddings\ntrained on EHR and PubMed can capture the semantics of medical terms better than those trained on GloVe and Google News and find more relevant similar medical terms, and are closer to human experts\u2019 judgments, compared to these trained on GloVe and Google News. Second, there does not exist a consistent global ranking of word embedding quality for downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained on biomedical domain corpora do not necessarily have better performance than those trained on other general domain corpora for any downstream biomedical NLP tasks.\nIndex Terms\nword embeddings; natural language processing; biomedical application; deep learning;\nI. INTRODUCTION\nNeural word embeddings have been widely used in Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words [1], [2], [3]. There has been an increasing number of studies applying word embeddings in common NLP tasks, such as information extraction (IE) [4], [5], [6], information retrieval (IR) [7], sentiment analysis [8], [9], question answering[10], [11], and text summarization [12], [13]. Recently in the biomedical domain word embeddings have been remarkably utilized in applications like biomedical named entity recognition (NER) [14], [15], medical synonym extraction[16], relation extraction (RE) including chemical-disease relation [17], drugdrug interaction [18], [19] and protein-protein interaction [20], biomedical IR [21], [22] and medical abbreviation disambiguation [23].\nMany biomedical applications use task corpora to train word embeddings or use external data resources such as Wikipedia [21], [24] to train word embeddings based on an implicit assumption that the external resources contain the knowledge that could be used to enhance domain tasks [25], [26], [27]. A number of pre-trained word embeddings are publicly available, such as Google News embeddings 1 and GloVe embeddings 2. These embeddings could capture semantics of general English words from a large corpus. However, one question remains unanswered: Do we need to train word embeddings for a specific NLP task since there are a number of public pre-trained word embeddings? This question becomes more significant for biomedical applications, and particularly more important for the clinical domain. The reason is that few electrical health records (EHRs) data are publicly available due to Health Insurance Portability and Accountability Act (HIPAA) rule while a big volume of biomedical literature data is available online. However, there has been little work on evaluating the word embeddings trained from these textual resources for biomedical applications, to the best of our knowledge.\nIn this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we utilized\n1https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit 2https://nlp.stanford.edu/projects/glove/\n3 clinical notes from the EHR system at Mayo Clinic and biomedical publications from the PubMed Central (PMC)3 to train word embeddings separately. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen [28], Hliaoutakis [29], MayoSRS [30], and UMNSRS [31], [32]. In extrinsic evaluation, we applied word embeddings to downstream NLP applications in the biomedical domain including clinical IE, biomedical IR, and RE, and measured the performance of word embeddings."
                },
                {
                    "heading": "II. RELATED WORK",
                    "text": "Due to the successful usage of word embeddings in a variety of NLP applications, there exists recent work on evaluation of word embeddings in representing word semantics. Most of the previous work focuses on evaluating word embeddings generated by different approaches. Baroni et al [33] presented the first systematic evaluation of word embeddings generated by count models (using DISSECT4 on a corpus of 2.8 billion tokens constructed by concatenating ukWaC5 the English GloVe6 and the British National Corpus7), CBOW [1] (using word2vec8 on the same corpus as DISSECT), Distributional Memory model9 (on the same corpus as DISSECT), and Collobert and Weston model10 (on the GloVe), and tested them on fourteen benchmarks in five categories: semantic relatedness (a dataset of semantic benchmarks constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numerical scale, such as Rubenstein and Goodenough\u2019s dataset [34] and WordSim353 [35]), synonym detection (a dataset containing 80 multiple-choice questions that pair a target term with 4 synonym candidates, such as TOEFL dataset [36]), concept categorization (given a set of nominal concepts, the task is to group them into natural categories, such as Almuhareb-Poesio dataset [37]), selectional preferences (a dataset containing verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb, such as Ulrike Pado\u2019s dataset [38]), and analogy (a dataset containing semantic and syntactic analogy questions, such as Mikolov\u2019s dataset [1]). They found that the word2vec model, CBOW, performed the best for almost all the tasks. Schnabel et al [39] trained the CBOW model of word2vec [1], C&W embeddings [40], Hellinger PCA [41], GloVe [42], TSCCA [43] and Sparse Random Projections [44] on a 2008 GloVe dump, and\n3http://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ 4http://clic.cimec.unitn.it/composes/ 5http://wacky.sslmit.unibo.it/ 6http://en.GloVe.org/ 7http://www.natcorp.ox.ac.uk/ 8https://code.google.com/p/word2vec/ 9http://clic.cimec.unitn.it/dm/ 10http://ronan.collobert.com/senna/\n4 tested on the same fourteen datasets. They also found the CBOW outperformed other embeddings on 10 datasets. In addition to this intrinsic evaluation, they conducted extrinsic evaluation by using the embeddings as input features to two downstream tasks, namely noun phrase chunking and sentiment classification, and found the results of CBOW were also among the best. Ghannay et al [45] conducted a similar intrinsic evaluation, they additionally evaluated the skip-gram models of word2vec [1], CSLM word embeddings [46], dependency-based word embeddings [3], and combined word embeddings on NLP tasks (i.e., Part-Of-Speech Tagging, chunking, named entity recognition, mention detection) and linguistic tasks using Mikolov\u2019s dataset [1] and the WordSim353 dataset[35]. They trained these word embeddings on the Gigaword corpus composed of 4 billion words and found that the dependency-based word embeddings gave the best performance on the NLP tasks and combination of the embeddings yielded significant improvement. In Nayak et al\u2019s study [47], they recommended that the evaluation should test both syntactic and semantic properties of the word embeddings and that the tasks should be closer to real-word applications. However, none of these studies evaluate word embeddings in the biomedical domain and none of these datasets focus on biomedical data.\nAs most of the aforementioned studies evaluate word embeddings in the general (i.e., non-medical) NLP domain, only one recent paper by Pakhomov et al [32] is about evaluating word embeddings in the biomedical domain, to the best of our knowledge. They trained the CBOW model on two biomedical corpora, namely clinical notes and biomedical publications, and one general English corpora, namely GloVe. The word embeddings were evaluated on subsets of UMNSRS dataset, which consisted of pairs of medical terms with the similarity of each pair assessed by medical experts, and on a document retrieval task and a word sense disambiguation task. They found that the semantics captured by the embeddings computed from biomedical publications were on par with that from clinical notes. Inspired by this work, we would like to conduct a complementary study to extend their evaluation of word embeddings by 1) utilizing four datasets to evaluate word embeddings on capturing medical term semantics; 2) conducting a qualitative evaluation; and 3) examining word embeddings on more biomedical application.\nIn this work, we provide a comparison of the quality of word embeddings trained separately from different\nresources. Specifically, our contributions are:\n1) We performed qualitative evaluation where we manually inspected five most similar medical words to a given\nset of target medical words and plotted a visualization of selected medical words from those word embeddings. 2) We performed quantitative evaluation, including extrinsic and intrinsic evaluation. In the intrinsic evaluation,\nwe used four published datasets for measuring semantic similarity between medical terms.\n3) In extrinsic evaluation, we evaluated word embeddings by applying them to three publicly shared biomedical\ntasks, including biomedical IR, NER, and RE, and one institutional clinical NLP task."
                },
                {
                    "heading": "III. WORD EMBEDDINGS",
                    "text": "Since it has been shown that the word2vec outperforms other approaches in generating good embeddings in general NLP tasks [33], [39], the skip-gram model of word2vec is utilized as the approach for generating word embeddings in this paper. Since no evidence shows the CBOW outperforms the skip-gram model or vice versa, we arbitrarily chose the skip-gram model of word2vec.\n5"
                },
                {
                    "heading": "A. Brief Introduction",
                    "text": "Word embeddings can be represented as a mapping V \u2192 RD : w 7\u2192 \u03b8 which maps a word w from a vocabulary V to a real-valued vector \u03b8 in an embedding space with a dimension of D. The skip-gram model is an architecture proposed by Mikolov et al [1], which trains embeddings using the negative-sampling procedure. It constructed with the focus word as the single input vector, and the target contextual words are at the output layer. Negative-sampling updates a sample of output words per iteration, and the target output words should be kept in the sample and gets updated while a few non-target words are added as negative samples. Mathematically, given a target word w and its contextual word h, the goal is to maximize the log-likelihood on the training data, i.e.,\nmax J = logP (h|w),\nwhere P (h|w) is the conditional probability in the neural probabilistic language model that is usually defined as:\nP (h|w) = \u03c3(score(w, h)) = e score(w,h)\u2211\nw\u2208V e score(w,h)\n,\nwhere \u03c3()\u0307 is a softmax function that normalize real vector into a probability vector. Accordingly, the log-likelihood function can be written as:\nJ = score(w, h)\u2212 log( \u2211 w\u2208V escore(w,h)).\nNegative-sample is adopted here to avoid expensive computation over |V | words, i.e.,\nJ \u2032 = \u2211\nw,h\u2208D\nlogQ\u03b8(D = 1|w, h) + \u2211\nw,h\u2208D\u2032 logQ\u03b8(D = 0|w, h),\nwhere D is the observed data, D\u2032 is the unobserved data, \u03b8 is the embedding vector, and Q\u03b8(D = 1|w, h) is the probability of w and h being observed. The word embeddings can be computed by maximizing the log-likelihood function."
                },
                {
                    "heading": "B. Parameter Settings",
                    "text": "We tested dimensions of 20, 60 and 100 for word embeddings trained on EHR and PubMed and chose 100 for EHR and 60 for PubMed according to their performance in our intrinsic evaluation. Similarly, we chose the dimension of 100 for GloVe, and that of 300 for Google News since only 300 dimension was publicly available for Google News. The whole results of using different dimensions for word embeddings are provided in Appendix A. For training word embeddings on the EHR and PubMed, we set the window size to 5 words, the minimum word frequency to 7 (i.e., the words occurred less than 7 times in the corpus were ignored), and the negative sampling to 5. These parameters were selected based on previous studies [1], [3], [19].\nIV. DATA"
                },
                {
                    "heading": "A. Text Corpora",
                    "text": "We compared word embeddings computed from four different kinds of corpora. The first corpus, denoted as EHR, was from the Electronic Health Record system at Mayo Clinic. It contains textual clinical notes for a cohort\n6 of 113k patients receiving their primary care at Mayo Clinic, spanning a period of 15 years from 1998 to 2013. The vocabulary size |V | of this corpus is 103k. The second corpus, denoted as PubMed, is from a snapshot of the Open Access Subset of PMC in 2016. PMC is an online digital database of freely available full-text biomedical literature. The PubMed contains 1.25 million biomedical articles and 2 million distinct words (i.e., |V |). As comparisons, two additional public pre-trained word embeddings from two general English resources, i.e., Google News embeddings 11 and GloVe embeddings12, were also considered in the evaluation. The Google News embeddings have embeddings for 3 million words from Google News, trained using the wor2vec [1]. The GloVe embeddings have embeddings for 400k words from a snapshot of Wikipedia in 2014 and Gigaword Fifth Edition13, trained using the GloVe model [42]."
                },
                {
                    "heading": "B. Pre-processing",
                    "text": "The PubMed was pre-processed minimally by removing punctuations (one exception is that we replaced \u2018-\u2019 by \u2018 \u2019 if two words were connected by \u2018-\u2019 and we treated them as one word), lowercasing, and replacing all digits with \u201d7\u201d. We conducted additional pre-processing on the EHR since the narratives written by physicians are more sparse than research articles. Specifically, the section of \u201cFamily history\u201d in the corpus was removed if it was semi-structured [48]. See an example of the \u201dFamily history\u201d section in Table I\nTABLE I: An example of the \u201dFamily history\u201d section.\nMOTHER Stroke/TIA BROTHERS 4 brothers alive 1 brother deceased SISTERS 2 sisters alive DAUGHTERS 1 daughter alive Heart disease\nThe section of \u201dVital Signs\u201d was totally removed since it did not contain contextual information for training\nword embeddings. See an example of the \u201dVital Signs\u201d section in Table II.\nTABLE II: An example of the \u201dFamily history\u201d section.\nHeight: 149.1 cm. Weight: 44.5 kg. BSA(G): 1.3573 M2. BMI: 20.02 KG/M2.\nNote that these pre-processing strategies are specific for the clinical notes at Mayo Clinic. Moreover, we replace all text contractions with their respective complete text. For example, \u201dcan\u2019t\u201d is replaced with \u201dcan not\u201d. For clinical\n11https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing 12http://nlp.stanford.edu/data/glove.6B.zip 13https://catalog.ldc.upenn.edu/LDC2011T07\n7 corpus, we removed all the clinical notes metadata and note sections headers, dates, phone numbers, weight and height information and punctuations from clinical corpus. For PubMed articles, we removed all the websites url, email addresses, twitter handler and punctuations."
                },
                {
                    "heading": "V. QUALITATIVE EVALUATION",
                    "text": "The first experiment is qualitative evaluation by manually inspecting the five most similar words to a given target word. We used the commonly employed cosine similarity to calculate the most similar words. Suppose w1 and w2 are two words, the similarity between w1 and w2 is defined as\nsimilarity(w1, w2) = \u03b81 \u00b7 \u03b82 \u2016\u03b81\u2016\u2016\u03b82\u2016 , (1)\nwhere \u03b81 and \u03b82 are vector representations for w1 and w2 in the embedding space, respectively. If the target word is a medical phrase s1 consisting of multiple words, i.e., s1 = w1, w2, ..., wn, the similarity function becomes\nsimilarity(s1, w2) = \u03981\u03b8\u03072 \u2016\u03981\u2016\u2016\u03b82\u2016\n(2)\nwhere \u03981 = 1n \u2211n i \u03b8i is the representation for s1 in the embedding space. This is different from Pakhomov et al\u2019s study [32] where only single word terms were considered.\nTable III lists eight target words in three medical categories, i.e., disorder, symptom and drug, and their five most\nsimilar words induced by different word embeddings.\nFor the first target word describing an disorder, diabetes, EHR and PubMed find its synonym, mellitus, in the most similar words while GloVe and Google News fail to find it. EHR finds two terms related to co-morbidities of diabetes, cholesterolemia and dyslipidemia, and a most common modifier term, uncontrolled. PubMed finds terms relevant to co-existing conditions for diabetes, such as cardiovascular (very possibly from cardiovascular disease), nonalcoholic (very possibly from nonalcoholic fatty liver disease), obesity, and polycystic (very possibly from polycystic ovary syndrome which is a hyperandrogenic disorder that is associated with a high-risk of development of Type 2 diabetes). Most of these terms are from medical research topics and thus occur in the PubMed articles. GloVe finds two related terms, hypertension and obesity, while other three terms, i.e., arthritis, cancer and alzheimer, are less relevant disease names. Google News finds two morphological terms, diabetics and diabetic, relevant to the target words, one synonym, diabetes mellitus, and one related disease name, heart disease. We can draw similar conclusions for the second and third disorder words.\nThe dyspnea example in the symptom category demonstrates the advantage of EHR and PubMed. EHR finds palpitations, a common cause of dyspnea, and orthopnea, exertional, and doe (dyspnea on exertion) are synonyms or specific conditions for dyspnea. PubMed finds related symptoms, sweats and orthopnea, a synonym breathlessness, a relevant disorder hypotension, and a term relevant to the symptom rhonchi. Wikipedia finds synonyms shortness and breathlessness, and less relevant symptoms cyanosis and photophobia. Google News finds less relevant symptoms pruritus and rhinorrhea and less relevant disease nasopharyngitis. Similar observations can be found for sore throat and low blood pressure as well.\nThe drug category further differentials the word embeddings. In opioid, EHR finds opiate, benzodiazepine, sedative, polypharmacy, which are very relevant medications. PubMed finds nmda receptor, affective motivational,\n8\n20 10 0 10 20 20 10 0\n10 20 acetylcysteine\nacne activase\nactonel\nadenitis adenosine\nagitation\nagranulocytosis albumin\nalbuterol\nalcohol allergy\nallopurinol\namantadine\namiodarone\namoxil\nampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nantabuse\naphonia\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxia\natenolol\natherosclerosis\nativan\navandia\navelox\nbabesiosis\nbacitracin\nbacteremia\nbactroban\nbandemia\nbedwetting\nblanching\nblastomycoses\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncalan\ncandidiasis\ncarbatrol\ncarboplatin\ncardiomyopathies\ncardiomyopathy\ncardizem\ncardura\ncarsickness\ncatapres\ncataract cataracts\ncatatonia\ncatch\ncefaclor\ncefazolin cefepime\ncefoxitin\nceftazidime\nchills\nchloramphenicol\ncholestasis\ncholestyramine\ncipro\ncirrhosis isplatin\nclonus\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg\ncortisone\ncoumadin\ncozaar\ncromolyn\ncrowning\ncyanosis\ndeafness\ndehydration\ndementia\ndementias\ndermatitis\ndermatomyositis\ndexamethasone\ndiabetes\nvomitingdiarrhea\ndigoxin\ndilantin diovandizziness\ndoxycyline\ndrooling\ndulcolax\nduragesic\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis\nepilepsy\nerythema\nerythromycin\nesmolol\nethanol\nexophthalmos\nfamvirfatigue\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis\nglucagen\nglucophageglucotrol\nglyburide\ngoiter\ngonorrhea\ngranulocytosis\ngrimaces\nhaemorrhoids\nhalitosis\nheadache\nheartburn\nhematemesis\nhemicrania\nhemiplegia\nhemochromatosis\nhemoglobinopathy\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomegalyhepatosplenomegalyherniahernias\nherpes\nhistoplasmosis\nhunger\nhyperacusis\nhyperesthesiahyperextension\nperglycemia\nhyperlipidemia\nhypersomnolence\nhyperthyroidism\nhypoproteinemia\nhypothyroidism\nhytrin\nence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nisosorbide\nkeflex\nketamine\nketonuria\nlasix\nlethargy\nleukopenia\nlevaquin\nlevophed\nlidocaine\nlipitor lisinopril\nlisteriosis\nloperamide\nlopid\nlovastatin\nmacule\nmaculopapule\nmalaria\nmalnutrition\nmannerism\nmannitol\nmastodynia\nmedrol meningism\nmeningitis\nmetatarsalgia\nmethadone\nmethotrexate\nmittelschmerz\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathymyositis\nnarcan\nnausea\nnephritis\nneuralgia\nneuropathy\nnitroglycerine\nincontinnoctu ria\nnystagmus\nobesity\nosteoporosis\notitis\npain\npallor\npancreatitis\nparasitemia\nparesis\npenicillin\npepcid\nperitonitis\nphenobarbital\nphotopsia photosensitization\nplague\nplavix\npneumonia\npneu oniae\npolydipsiapolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nprostatism\nproteinuria\nprotonix\nprozac\npsoriasis\nrabies\nrales\nregurgitation\nreticulocytosis\nrheumatism\nhonchi\nr bitussin\nrogaine\nsandimmune\nschistosomiasis\nsciatica\nscleroderma\nseasickness\nseizures\nsepticemia\nserevent\nsilvade e\nsinemet singulair\nsleeplessness sluggishness\nsmallpox\nsnoring\nspasm\nspiriva\nstarvation\nstridor syncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopen a\nthromboembolism\nthrombophilias\nthrombus\ntiredness\ntoothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvancocin\nvasculitis\nvertigo\nvfend\nvicodin\nweakness\nwellbutrin\nwelts\nwheezing\nxanax\nxenical\nzantac\nzetia\nzithromax\nzocor\nzofran\nzoloft\nzovirax (a) EHR\n20 10 0 10 20\n20\n10\n0\n10\n20\nacetylcysteine\nacne\nactivaseactonel\nadenitis\nadenosine\nagitation\nagranulocytosis\nalbumin\nalbuterol\nalcohol\nallergy\nallopurinol\namantadine\namenorrhoea\namiodarone amoxil\nampicillin\namyloidoses\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nanoxemia\nantabuse\naphonia\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxia\natenolol\natherosclerosis\nativan\navandia\navelox\nbabesiosis\nbacitracin\nbacteremia\nbactroban\nbandemia\nbedwetting\nblanching\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncalan\ncamelpox\ncandidiases\ncandidia is\ncarboplatin\ncardialgia\ncardiomyopathiescardiomyopathy\ncardizem\ncardura\ncarsickness\ncataractcataracts\ncatatonia\ncatch\ncefaclor\nazidime cefazolin cefepime\ncefoxitin ceft\nceftiaxone\nchills\nchloramphenicol\ncholestasis\ncholestyramine\nchyluria\ncipro\ncirrhosis\ncisplatin\nclonus\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg\ncorkscrewing\ncortisone\ncoumadin\ncozaar\ncromolyn\ncrowning\ncyan sis deafness\ndehydration\ndementia\ndementias\ndermatitisdermatomyositis\ndexamethasone\ndiabetes\ndiarrhea\ndigoxin\ndilantin diovan\ndizziness\ndoxycyline\ndrooling\ndulcolax\nduragesic\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis epilepsy\nerythema\nerythromycin\nesmolol\nethanol\nexophthalmos\nfatigue\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis\nglucagen\nglucophage\nglucotrol\nglyburide\ngoiter\ngonorrhea\ngranulocytosis\ngrimaces\nhyperemesis\nhaemorrhoids\nhalitosis\nheadacheheartburn\nhematemesis hemiballismus\nhemicrania\nhemiplegia\nhemochromatosis\nhemoglobinopathy\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomeg ly\nhepatosplenomegaly\nhernia\nhernias\nherpes\nhistoplasmosis\nhun er\nhyperacusis\nhyperesthesia\nhyperglycemiahyperlipidemia\nhyperoxia\nhypersomnolence\nhyperthyroidism\nhypoproteinemia\nhypothyroidism\nhytrin\nincontinence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nischemias\nisosorbide\nkeflex\nketamine\nketonuria\nlasix\nlethargy\nleukopenia\nlevaquin\nlevophed\nlidocaine\nlipitor\nlisinopril\nlisteriosis\nloperamide\nlopid\nlovastatin\nmacule\nmaculopapule malaria\nmalnutrition\nmannerism\nmannitol\nmastodynia\nmedrol\nmeningism\nmeningitis\nmenouria\nmethadone\nmethotrexate\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathy\nyositis\nnarcan\nnausea\nnephritis\nneuralgia neuropathy\nnitroglycerine\nnocturia\nob sity\nopisthotonus\nosteoporosis\notitis\novernutrition\npain\npallor\npancreatitis\nparasitemia\npenicillinpepcid\nperitonitis\nphenobarbital\nphotopsia\nphotosensitization\nplague\nplavix\npneumonia\npneumoniae\npolydipsiapolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nprostatism\nproteinuria\nprotonixprozac\npsoriasis\npyorrhea\nrabies\nrales\nregurgitation\nreticulocytosis\nrheumatism\nrhonchi\nrobitussin rogaine\nsandimmune\nschistosomiasis\nsciatica\nscleroderma seizures\nsepticemia\nserevent\nsilvadene\nsinemet\nsingulair\nsleeplessness\nsluggishness\nsmallpox\nsnoring\nspasm\nspiriva\nstarvation\nstridor\nsyncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopenia\nthromboembolismthrombophilias\nthrombus\ntiredness\ntoothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvancoc\nvasculitis\nvermox\nvertigo\nvfend\nvicodin\nvirilism\nvomiting\nweakness\nwellbutrin\nwelts\nwheezing\nxanax xenical\nzantac\nzetia\nzithromax\nzocor\nzofran\nzoloft\nzovirax\nzoonosis\n(b) PubMed\n15 10 5 0 5 10 15\n10\n5\n0 5 10 acetylcysteine\nacne\nactivase actonel\nadenosine\nagit tion\nagranulocytosis airsickness albumin albuterol\nalcohol\nallergy\nallopurinol amantadine amenorrhoea amiodarone ampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia anovulation antabuse\nappendicitis arteriosclerosis arthralgia\narthritis\nasthma\nataxia atenolol a herosclerosis ativan avandia babesiosis bacitracin bacteremia bedwetting\nblanching\nbleomycin\nbronchitis\nbrucellosis cachexia calamine calan camelpox\ncandidiasis\ncarboplatin cardiomyopathycardizem cardura\ncataract cataracts\ncatatonia\ncatch\nchills\nchloramphenicol\ncholestasis cipro\ncirrhosis cisplatin clonus\nclubbing\ncluttering\ncoccidioidomycosis codeine colchicine\ncolitis\ncomatose\nconstipation\nconvulsion\ncoreg corkscrewing cortisone coumadin cozaar\ncrowning\ncyanosis\ndeafness\ndehydration\ndementia\ndementias\ndermatitis dermatomyositis dexamethasone\ndiabetes\ndiarrhea\ndigoxin dilantin diovan\ndizziness\ndrooling\nduragesic\ndysentery\ndyslipidemia dyspnea\nearache\nechinacea\nemaciation enalapril\nepilepsy\nerythema erythromycin ethanol exophthalmos famvir\nfatigue\nfibrillation\nflatulence\nflushing\nfosamax garlic\nga troenteritis\ngiardiasis\nglaucoma\nglomerulosclerosis glucophage goiter\ngrimaces\nhyperemesis\nhalitosis\nheadache\nheartburn\nhemiplegia\nhemochromatosis\nhemophilia hemoptysis heparin hepatomegaly hepatosplenomegaly herniahernias histoplasmosis hyperacusis hyperextension hyperglycemia hyperlipidemia hyperthyroidismothyroidism hytrin\nincontinenceinfertility\ninfluenza\ninfluenzae iron ischemiaisosorbide ketamine lasix\nlethargy\nleukopenia levaquin lidocaine lipitor lisinopril\nlisteriosis\nlovastatin mannerism mannitol\nmeningitis\nmethadone methotrexate morphine motrin\nmycosis myopathy myositis narcan\nnausea\nnephritis\nneuralgia\nneuropathy\nnitroglycerine\nnystagmus\nobesity\nosteoporosis\notitis\npain\npallor\npancreatitis paresis penicillin pepcid peritonitis phenobarbital\nplague\nplavix\npneumonia\npneumoniae polydipsia polyuria prednisolone prilosec probenecid propofol propranolol proteinuria protonix\npsoriasis\nrabies\nrales\nregurgitation\nrheumatism\nrobitussin rogaine\nschistosomiasis\nsciatica\nscleroderma\nseasickness\nseizures\ns pt cemia serevent singulair\nsleeplessness\nsluggishness\nsmallpox\nsnoring\nspasm\nsyncope synthroid tamiflu\nthalassemia\nthirsty\nthrombocytopenia thrombo mbolism thrombus\ntir dness\ntoothache\ntorticollis\ntrembling\ntremor tums tylenol\nvaccinia\nvasculitis\nvertigo\nvicodin\nvomiting weakness\nwelts\nwheezing\nxanax xenical zantac zetia zithromax zocor zofran zovirax\ncomatose zoonosis\n(c) GloVe\n30 20 10 0 10 20 30\n20\n15\n10\n5\n0\n5\n10\n15\n20\nacetylcysteine\nacne\nadenosine\nagitation\nagranulocytosis\nairsickness\nlb min\nalbuterol\nalcohol\nallergy\nallopurinol amantadine\namenorrhoea\namiodarone\namoxil\nampicillin\nanemia\naneurysm\nangina\nanorexia\nanosmia\nanovulation\nantabuse\nappendicitis\narteriosclerosis\narthralgia\narthritis\naspirin\nasthma\nataxi\natenolol\natherosclerosis\nativanavandia\nbabesiosis\nbacitracin\nbacteremia\nbedwetting\nblanching\nbleomycin\nblepharospasm\nbronchitis\nbrucellosis\ncachexia\ncalamine\ncandidiasis\ncarboplatin\ncardiomyopathie\ncardiomyopathy\ncardizem\ncarsickness\ncataractcataracts\ncatatonia\ncatch\ncefazolin\ncefepi e ceftazidime\nchills\nchloramphenicol\ncholestasis cholestyramine cipro\ncirrhosis\ncisplatin\nclubbing\ncluttering\ncoccidioidomycosis\ncodeine\ncolchicine\ncolitis\ncomatose\nconstipation\nconvulsio corkscrewing\ncortisone\ncoumadin\ncromolyn\nc owning\ncyanosisdeafness dehydration\ndementia dementias\ndermatitis\ndermatomyositis\ndexamethasone\ndiabetes\ndiarrhea\ndigoxin\ndilantin\ndiovan\ndizziness\ndrooling\ndysentery\ndyslipidemia\ndyspnea\ndysuria\nearache\nechinacea\nemaciation\nenalapril\nencephalitis\nepilepsy\nerythema\nerythromycin\nethanol\nfibrillation\nflatulence\nflushing\nfosamax\ngarlic\ngastroenteritis\ngiardiasis\nglaucoma glomerulosclerosis\nglucophage\nglyburide\ngoiter\ngonorrhea\ngrimaces\nhyperemesis\nhae orrhoidshalitosis\nheadacheheartburn\nhemiplegia\nhemochromatosis\nhemophilia\nhemoptysis\nheparin\nhepatitis\nhepatomegaly\nherniahernias\nherpes\nhistoplasmosis\nhunger\nhyperacusis\nhyperextension\nhyperglycemi\nhyperlipidemia\nhyperoxia\nhyperthyroidismhypothyroidism\nincontinence\ninfertility\ninfluenza\ninfluenzae\ninsulin\niron\nischemia\nisosorbide keflex\nketamine\nlasix\nlethargy\nleukopenia levaquin\nlidocaine\nlipitor\nlisinopril\nlisteriosis\nloperamide\nlovastatin\nmalaria\nmalnutrition\nmannerism\nmannitol\nmeningitis\nmetatarsalgia\nmethadone\nmethotrexate\nmorphine\nmotrin\nmycoses\nmycosis\nmyelosuppression\nmyopathymyositis\nnausea\nnephritis\nneuralgia\nn uropath\nnitroglycerine\nnocturia\nnystagmus\nobesity\nosteoporosis\notitis\novernutrition\npain\npallor\npancreatitis\nparasitemia\nparesis\npenicillin\nperitonitis\nphenobarbital\nplague\nplavix\npneumonia pneumoniae\npolydipsiaolyuria\nprednisolone\nprilosec\nprobenecid\npropofol\npropranolol\nproteinuria\nprozac\npsoriasis\nrabies\nrales\nregurgitation\nrheumatism\nrogaine\nschistosomiasis\nsciatica\nsclerod rm\nseasickness\nseizures\nepticemia\nsleeplessness\nsluggi hne s\nsmallpox\nsnoring\nspasm\nstarvation\nstridor\nsyncope\nsynthroid\nsyphilis\ntamiflu\nthalassemia\nthirsty\nthrombocytopenia thromboembolism\nthrombus\nfatiguetiredness toothache\ntorticollis\ntrembling\ntremor\ntuberculosis\ntums\ntylenol\nultram\nurolithiasis\nvaccinia\nvasculitis\nvertigo\nvicodin\nvomiting\nweakness\nwellbutrin\nwelts\nwheezing\nxanax\nxenical\nzan ac\nzithromax zoloftzovirax\nzoonosis\n(d) Google News\nFig. 1: Example clusters in different word embeddings.\nnaloxone precipitated, hyperlocomotion, whi h are related to th mechanism of action of opioid. GloVe finds analgesic and less relevant anti-inflammatory, and Google News finds opioid-related phrases and relevant term antipsychotics. In aspirin, EHR also finds very clinically relevant used terms and PubMed finds relevant terms in research articles while GloVe and G ogle New only find medication names.\nIt is obvious shown from these target words that EHR and PubMed can capture the semantics of medical terms better than GloVe and Google News and find more rel vant similar edical terms. However, EHR and PubMed find simil r medical terms fr m different perspectives due to their different focuses. EHR contains clinical narratives and thus it is closer to clinical languag . Y t, it contains terms with different morphologies and even typos, such as melitis, ca er nd thraot as listed in Table III. Differently, PubMed contains mo e medical terminology used in medical articles, and finds similar words mostly from a medical research perspective.\nIn order to visualize the semantics of medical terms captured by different word embeddings, we extracted 377 medical terms from the UMNSRS dataset [31], [32] and plotted the word embeddings for these medical terms using t-SNE [49]. Example clusters of medical terms n the word embeddings ar shown n Figure 1. Figure 1a depicts a cluster of symptoms in word embeddings trained on EHR, such as \u201cheartburn\u201d, \u201cvomiting\u201d and \u201cnausea\u201d. As shown in Figure 1b, a cluster of an antibiotic medications can be obviously observed from PubMed embeddings, such as \u201cbacitracin\u201d, \u201ccefoxitin\u201d, and \u201cchloramphenicol\u201d. Figures 1c and 1d illustrate clusters of symptoms in w rd embeddings trained from GloVe and Google News, respectively. Since we did not employ any clustering method, these clusters were intuitively ob erved from the visualization. The visualization of the wh le 377 medical terms can be found in Appendix B.\n9\n10"
                },
                {
                    "heading": "VI. QUANTITATIVE EVALUATION",
                    "text": "We tested word embeddings with quantitative evaluation to show the qualitative differences between them. We utilized extrinsic and intrinsic evaluation, where the former used four published datasets for measuring semantic similarity between medical terms and the latter used downstream biomedical tasks to evaluate word embeddings.\nA. Intrinsic Evaluation\nWe tested word embeddings on four published biomedical datasets commonly used to measure semantic similarity between medical terms. The first is Pedersen\u2019s dataset [28] that consists of 30 medical term pairs that were scored by physician experts according to their relatedness. The second is Hliaoutakis\u2019s dataset [29] consisting of 34 medical term pairs with similarity scores obtained by human judgments. The third one is MayoSRS dataset, developed by Pakhomov et al [30], which consists of 101 clinical term pairs whose relatedness was determined by nine medical coders and three physicians from the Mayo Clinic. The relatedness of each term pair was assessed based on a four point scale: (4.0) practically synonymous, (3.0) related, (2.0) marginally related and (1.0) unrelated. We evaluated word embeddings on the mean score of the physicians and medical coders. The fourth is UMNSRS similarity dataset, also developed by Pakhomov et al [31], which consists of 566 medical term pairs whose semantic similarity was determined independently by eight medical residents from the University of Minnesota Medical School. The similarity and relatedness of each term pair was annotated based on a continuous scale by having the resident touch a bar on a touch sensitive computer screen to indicate the degree of similarity or relatedness.\nFor each pair of medical terms in the testing datasets, we used Equations 1 and 2 to calculate the semantic similarity for each pair in the embeddings space. Since some medical terms might not exist in the vocabulary of word embeddings, we used fastText [50] to compute word vectors for these out-of-vocabulary medical terms. Pearson correlation coefficient was employed to calculate the correlation between similarity scores from human judgments and those from word embeddings. Table IV lists the results for the four datasets. Overall, the semantic similarity captured by the word embeddings trained on EHR are closer to human experts\u2019 judgments, compared with other word embeddings. PubMed performs worse than EHR but has a comparative result for the UMNSRS dataset. GloVe and Google News are inferior to EHR and PubMed and perform similarly in representing medical semantics. Note that the four datasets and corresponding semantic similarity scores from both human experts and word embeddings are provided in the supplementary Excel file.\n11"
                },
                {
                    "heading": "B. Extrinsic Evaluation",
                    "text": "Extrinsic evaluations are used to measure the contribution of word embeddings to specific biomedical tasks. In\nthis evaluation, we applied word embeddings to three prevalent shared tasks: clinical IE, biomedical IR, and RE.\n1) Clinical Information Extraction: We evaluated word embeddings on two clinical IE tasks. The first experiment is a shared task while the second is an institutional task. We would like to examine whether our local institutional word embeddings are better than external pre-trained word embeddings.\nIn the first experiment, we evaluated the word embeddings on an institutional information extraction task at Mayo Clinic. In this task, a set of 1000 radiology reports was examined to detect whether a hand and figure/wrist fracture could be identified. Reports were drawn from a cohort of residents of Olmsted County, aged 18 or older, who experienced fractures in 2009-2011. Each report was annotated by a medical expert with multiple years of experience abstracting fractures by assigning \u201c1\u201d if a hand and figure/wrist fracture was found, or \u201c0\u201d otherwise.\nIn our experiment, word embeddings were employed as features for machine learning models and evaluated by precision, recall and F1 scores [52]. For a clinical document d = {w1, w2, .., wM} where wi, i = 1, 2, ...,M is the ith word and M is the total number of words in this document, the feature vector x of document d is defined by\nx = 1\nM M\u2211 i xi,\nwhere xi is the embedding vector for word wi from the word embedding matrix. Then x was utilized as input to the machine learning models. A prevalent machine learning model, Support Vector Machine (SVM), was tested in this experiment. We performed 10-fold cross validation on the dataset and reported means of precision, recall and F1 scores from the cross validation. As comparison, the baseline method used term frequency features as input. Table V reports the means of precision, recall, and F1 score from the cross validation.\nThe word embeddings trained from EHR are superior to other word embeddings on all metrics (precision: 0.974, recall: 0.972, F1 score: 0.972). The fracture dataset in this experiment is curated from the same EHR system as the EHR corpus used to train word embeddings, and thus they have identical sublanguage characteristics embedded in word embeddings. The word embeddings trained on PubMed also have comparable results (precision: 0.946, recall: 0.943, F1 score: 0.942). Since this task is a medical task with specific medical terminologies, word embeddings trained on Google News have the worst performance. However, the word embeddings trained on GloVe are close to those trained on EHR with 0.02 difference on F1 score without statistically significance (p<0.01). This experiment shows that word embeddings trained on local corpus have the best performance for a local task but those trained on external Wikipedia corpus also have comparable performance.\n12\nWe secondly tested word embeddings on the i2b2 (Informatics for Integrating Biology to the Bedside) 2006 smoking status extraction shared task [51]. Participants of this task were asked to develop automatic NLP systems to determine the smoking status of patients from their discharge records in Partners HealthCare. For each discharge record, an automatic system should be able to categorize it into five pre-determined smoking status categories: past smoker, current smoker, smoker, non-smoker, and unknown, where a past and current smoker are distinguished based on temporal expressions in the patient\u2019s medical records. The dataset contains a total of 389 documents, including 35 documents of current smoker, 66 of non-smoker, 36 of past smoker, and 252 of unknown.\nThe experimental results are shown in Table VI. First, it is obviously shown that word embedding features perform better than term frequency features due to the semantics embedded in word embeddings. From Table VI, we can observe that using word embeddings trained on EHR has the best performance with a F1 score of 0.900. This result is interesting since the EHR corpus used to train the word embeddings is from a different institution. The reason might be that the smoking dataset has similar sublanguage characteristics as our EHR corpus. This result indicates that effective word embeddings can be shared across institutions for clinical NLP tasks. Another interesting observation is that the performance of word embeddings trained on Google News is close to that on EHR corpus with a comparable F1 score and a better recall. The performance difference is not statistically significant (p<0.01). This implies that word embeddings trained on a public dataset may not be definitely inferior to these trained on a medical specific dataset for a medical IE task. The likely cause is that the terminology used in smoking status extraction task also appears frequently in news, such as medications and advice for smokers.\n2) Biomedical Information Retrieval: We utilized Text REtreival Conference 2016 Clinical Decision Support (TREC 2016 CDS) track as the experimental data sets for evaluating word embeddings for biomedical IR. TREC 2016 CDS track focuses on biomedical literature retrieval that helps physicians find the precise literature information and make the best clinical decision at the point of care [53]. EHRs from MIMIC-III data set [54] were utilized to generate the query topics. Those topics are categorized into three most common types, namely Diagnosis, Test and Treatment, according to physicians\u2019 information needs, and 10 topics are provided for each type. Each topic is comprised of a note field (admission note), a description field (jargons and clinical abbreviations are removed) and a summary field (simplified version of the description). The participants are required to use only one of these three fields in their submissions and at least one submission must utilize the note field. Submitted systems should retrieve relevant biomedical articles from a given PMC article collection for each given query topic to answer three corresponding clinical questions: What is the patient\u2019s diagnosis? What tests should the patient receive? How should the patient be treated?. Each IR system can retrieve up to 1000 documents per query.\n13\nIn order to make comparison as fair as possible, we first implemented an IR system as a baseline system following [22] and then employed the simplest query expansion using word embeddings. That is, we expanded each query term with five most similar terms from word embeddings. Indri [55] was utilized as our indexing and retrieval tool. The preprocessing included stopword removal and Porter stemming. The stopword list was based on the PubMed stopwords 14. The article-id, title, abstract and body fields of each document were indexed. Language models with two-stage smoothing [56] was used to obtain all the retrieval results. Official metrics, Inferred Normalized Discounted Cumulated Gain (infNDCG) [57], Inferred Average Precision (infAP) [57], Precision at 10 (P@10) and Mean Average Precision (MAP), were utilized to measure the performance.\nTable VII shows the results on the TREC 2016 CDS track. It is interesting that query expansion using word embeddings almost does not improve retrieval performance and even worsen the performance when infAP and MAP were used as metrics. By comparing the retrieval performance of different word embeddings, we observe that EHR and PubMed perform slight better than GloVe and Google News without statistical significance (p<0.01). This result implies that applying different word embeddings trained from different resources has almost no significant difference for biomedical IR tasks. We also note that this may also be due to the query expansion method used in our evaluation.\n3) Relation Extraction: Drug-drug interaction (DDI) extraction is a specific RE task in the biomedical domain. DDI is an unexpected change in a drug\u2019s effect on the human body when the drug and a second drug are coprescribed and taken together. Automatic extracting DDI information from literature is a challenging and important research topic since the volume of the published literature grows rapidly and greatly. In this experiment, we evaluate word embeddings on DDIExtraction 2013 challenge corpus [58]. The dataset for DDIExtraction 2013 was composed of sentences describing DDIs from the DrugBank database and MedLine abstracts. In this dataset, drug entities and DDIs were annotated at the sentence level and each sentence could contain two or more drugs. An RE system should be able to automatically extract DDI drug pairs from a sentence. We leveraged the baseline system introduced in [19] and evaluated word embeddings by concatenating to the baseline features. We utilized Random Forest [59] as the classifier since it had the best performance in [19]. F1 score was used as the evaluation metric.\nTable VIII shows the results on the DDIExtraction 2013 challenge. We can see that the overall performance of word embeddings trained on Google News is the best. The reason is that the semantics of general English terms in the context of drug mentions are more important for determining the drug interactions. For example, in the\n14http://www.ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T.stopwords/\n14\nsentence \u201cAcarbose may interact with metformin\u201d, the term \u201cinteract\u201d is crucial to classify the relation. Since these crucial terms are generally not medical terminology, word embeddings trained on Google News where the corpus represents general English are able to capture the semantics of these terms. However, the superiority of Google News to other corpora is minor for this task. Another interesting observation is that word embeddings trained from PubMed have the best performance for the DrugBank corpus while these from Google News perform the best for the MedLine corpus. Though MedLine abstracts are from PubMed articles, this result shows that word embeddings trained from the same corpus are not necessarily superior to other embeddings."
                },
                {
                    "heading": "VII. CONCLUSION AND DISCUSSION",
                    "text": "In this study, we provided an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we selected a set of medical words and evaluated the five most similar medical words. We then analyzed word embeddings through the visualization of those word embeddings. We conducted extrinsic and intrinsic evaluation for quantitative evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms while extrinsic evaluation evaluated word embeddings in three downstream biomedical NLP applications, i.e., clinical IE, biomedical IR, and RE.\nBased on the evaluation results, we can draw the following conclusions. First, the word embeddings trained on EHR and PubMed can capture the semantics of medical terms better than those trained on GloVe and Google News and find more relevant similar medical terms. However, EHR finds similar terms vis a vis clinical language while PubMed contains more medical terminology used in medical articles, and finds similar words mostly from a medical research perspective. Second, the medical semantic similarity captured by the word embeddings trained on EHR and PubMed are closer to human experts\u2019 judgments, compared to these trained on GloVe and Google News. Third, there does not exist a consistent global ranking of word embedding quality for downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, word embeddings trained from biomedical domain corpora do not necessarily have better performance than those trained on other general domain corpora. That is, there might be no significant difference when word embeddings trained from an out-domain corpus were employed for a biomedical NLP application. However, the performance of word embeddings trained from a local corpus is better for local NLP tasks.\nIn a biomedical NLP application, our experiment implicitly show that applying word embeddings trained from corpora in a general domain, such as Wikipedia and news, is not significantly inferior to applying those obtained\n15\nfrom biomedical or clinical domain, which is usually difficult to access due to privacy. This result is consistent with but more general than the conclusion drawn in [32]. Thus, lack of access to a domain-specific corpus is not necessarily a barrier to the use of word embeddings in practical implementations.\nAs a future direction, we would like to evaluate word embeddings on more downstream biomedical NLP applications, such as medial named entity recognition and clinical note summarization. We will investigate whether different word embeddings represent language characteristics differently for a corpus, such as term frequency and medical concepts. We also want to assess word embeddings across health care institutions using different EHR systems and investigate how sublanguage characteristics affect the portability of word embeddings. Moreover, we want to apply clustering methods on word embeddings and compare the word-level and concept-level difference between clusters of medical terms."
                },
                {
                    "heading": "VIII. ACKNOWLEDGEMENT",
                    "text": "This work has been supported by the National Institute of Health (NIH) grants R01LM011934 and R01GM102282."
                },
                {
                    "heading": "APPENDIX A",
                    "text": "RESULTS OF USING DIFFERENT DIMENSIONS FOR WORD EMBEDDINGS.\n18"
                },
                {
                    "heading": "APPENDIX B",
                    "text": "VISUALIZATION OF WORD EMBEDDINGS.\n19\n20\n21"
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "Linguistic regularities in continuous space word representations.",
                    "authors": [
                        "T. Mikolov",
                        "W.-t. Yih",
                        "G. Zweig"
                    ],
                    "venue": "in hlt-Naacl,",
                    "year": 2013
                },
                {
                    "title": "Learning for biomedical information extraction: Methodological review of recent advances",
                    "authors": [
                        "F. Liu",
                        "J. Chen",
                        "A. Jagannatha",
                        "H. Yu"
                    ],
                    "venue": "arXiv preprint arXiv:1606.07993, 2016.",
                    "year": 2016
                },
                {
                    "title": "Dependency-based word embeddings.",
                    "authors": [
                        "O. Levy",
                        "Y. Goldberg"
                    ],
                    "year": 2014
                },
                {
                    "title": "Clinical information extraction applications: A literature review",
                    "authors": [
                        "Y. Wang",
                        "L. Wang",
                        "M. Rastegar-Mojarad",
                        "S. Moon",
                        "F. Shen",
                        "N. Afzal",
                        "S. Liu",
                        "Y. Zeng",
                        "S. Mehrabi",
                        "S. Sohn"
                    ],
                    "venue": "Journal of biomedical informatics, 2017.",
                    "year": 2017
                },
                {
                    "title": "Relation classification via convolutional deep neural network.",
                    "authors": [
                        "D. Zeng",
                        "K. Liu",
                        "S. Lai",
                        "G. Zhou",
                        "J. Zhao"
                    ],
                    "venue": "in COLING,",
                    "year": 2014
                },
                {
                    "title": "Employing word representations and regularization for domain adaptation of relation extraction.",
                    "authors": [
                        "T.H. Nguyen",
                        "R. Grishman"
                    ],
                    "year": 2014
                },
                {
                    "title": "Word embedding based generalized language model for information retrieval",
                    "authors": [
                        "D. Ganguly",
                        "D. Roy",
                        "M. Mitra",
                        "G.J. Jones"
                    ],
                    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 795\u2013798.",
                    "year": 2015
                },
                {
                    "title": "Learning sentiment-specific word embedding for twitter sentiment classification.",
                    "authors": [
                        "D. Tang",
                        "F. Wei",
                        "N. Yang",
                        "M. Zhou",
                        "T. Liu",
                        "B. Qin"
                    ],
                    "year": 2014
                },
                {
                    "title": "Learning word vectors for sentiment analysis",
                    "authors": [
                        "A.L. Maas",
                        "R.E. Daly",
                        "P.T. Pham",
                        "D. Huang",
                        "A.Y. Ng",
                        "C. Potts"
                    ],
                    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011, pp. 142\u2013150.",
                    "year": 2011
                },
                {
                    "title": "Exploring models and data for image question answering",
                    "authors": [
                        "M. Ren",
                        "R. Kiros",
                        "R. Zemel"
                    ],
                    "venue": "Advances in neural information processing systems, 2015, pp. 2953\u20132961.",
                    "year": 2015
                },
                {
                    "title": "Question answering over freebase with multi-column convolutional neural networks.",
                    "authors": [
                        "L. Dong",
                        "F. Wei",
                        "M. Zhou",
                        "K. Xu"
                    ],
                    "year": 2015
                },
                {
                    "title": "Extractive summarization by maximizing semantic volume.",
                    "authors": [
                        "D. Yogatama",
                        "F. Liu",
                        "N.A. Smith"
                    ],
                    "venue": "in EMNLP,",
                    "year": 2015
                },
                {
                    "title": "A neural attention model for abstractive sentence summarization",
                    "authors": [
                        "A.M. Rush",
                        "S. Chopra",
                        "J. Weston"
                    ],
                    "venue": "arXiv preprint arXiv:1509.00685, 2015.",
                    "year": 2015
                },
                {
                    "title": "Evaluating word representation features in biomedical named entity recognition tasks",
                    "authors": [
                        "B. Tang",
                        "H. Cao",
                        "X. Wang",
                        "Q. Chen",
                        "H. Xu"
                    ],
                    "venue": "BioMed research international, vol. 2014, 2014.  16",
                    "year": 2014
                },
                {
                    "title": "Effects of semantic features on machine learning-based drug name recognition systems: word embeddings vs. manually constructed dictionaries",
                    "authors": [
                        "S. Liu",
                        "B. Tang",
                        "Q. Chen",
                        "X. Wang"
                    ],
                    "venue": "Information, vol. 6, no. 4, pp. 848\u2013865, 2015.",
                    "year": 2015
                },
                {
                    "title": "Mining and ranking biomedical synonym candidates from wikipedia",
                    "authors": [
                        "A.N. Jagannatha",
                        "J. Chen",
                        "H. Yu"
                    ],
                    "venue": "Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis (Louhi), 2015, pp. 142\u2013151.",
                    "year": 2015
                },
                {
                    "title": "A crd-wel system for chemical-disease relations extraction",
                    "authors": [
                        "Z. Jiang",
                        "L. Jin",
                        "L. Li",
                        "M. Qin",
                        "C. Qu",
                        "J. Zheng",
                        "D. Huang"
                    ],
                    "venue": "The fifth BioCreative challenge evaluation workshop, 2015, pp. 317\u2013326.",
                    "year": 2015
                },
                {
                    "title": "Drug-drug interaction extraction via convolutional neural networks",
                    "authors": [
                        "S. Liu",
                        "B. Tang",
                        "Q. Chen",
                        "X. Wang"
                    ],
                    "venue": "Computational and mathematical methods in medicine, vol. 2016, 2016.",
                    "year": 2016
                },
                {
                    "title": "Dependency embeddings and amr embeddings for drug-drug interaction extraction from biomedical texts",
                    "authors": [
                        "Y. Wang",
                        "S. Liu",
                        "M. Rastegar-Mojarad",
                        "L. Wang",
                        "F. Shen",
                        "F. Liu",
                        "H. Liu"
                    ],
                    "venue": "Proceedings of the 8th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM, 2017.",
                    "year": 2017
                },
                {
                    "title": "A general protein-protein interaction extraction architecture based on word representation and feature selection",
                    "authors": [
                        "Z. Jiang",
                        "L. Li",
                        "D. Huang"
                    ],
                    "venue": "International Journal of Data Mining and Bioinformatics, vol. 14, no. 3, pp. 276\u2013291, 2016.",
                    "year": 2016
                },
                {
                    "title": "Cbnu at trec 2016 clinical decision support track",
                    "authors": [
                        "S.-H. Jo",
                        "K.-S. Lee"
                    ],
                    "venue": "Text REtrieval Conference (TREC 2016), 2016.",
                    "year": 2016
                },
                {
                    "title": "An ensemble model of clinical information extraction and information retrieval for clinical decision support.",
                    "authors": [
                        "Y. Wang",
                        "M. Rastegar-Mojarad",
                        "R.K. Elayavilli",
                        "S. Liu",
                        "H. Liu"
                    ],
                    "venue": "TREC,",
                    "year": 2016
                },
                {
                    "title": "Clinical abbreviation disambiguation using neural word embeddings",
                    "authors": [
                        "Y. Wu",
                        "J. Xu",
                        "Y. Zhang",
                        "H. Xu"
                    ],
                    "venue": "Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP), 2015, pp. 171\u2013176.",
                    "year": 2015
                },
                {
                    "title": "Megaro, \u201cSemi-supervised information retrieval system for clinical decision support.",
                    "authors": [
                        "H. Gurulingappa",
                        "L. Toldo",
                        "C. Schepers",
                        "A. Bauer"
                    ],
                    "venue": "TREC,",
                    "year": 2016
                },
                {
                    "title": "Knowledge discovery from biomedical ontologies in cross domains",
                    "authors": [
                        "F. Shen",
                        "Y. Lee"
                    ],
                    "venue": "PloS one, vol. 11, no. 8, p. e0160005, 2016.",
                    "year": 2016
                },
                {
                    "title": "Predicate oriented pattern analysis for biomedical knowledge discovery",
                    "authors": [
                        "F. Shen",
                        "H. Liu",
                        "S. Sohn",
                        "D.W. Larson",
                        "Y. Lee"
                    ],
                    "venue": "Intelligent Information Management, vol. 8, no. 03, pp. 66\u201385, 2016.",
                    "year": 2016
                },
                {
                    "title": "Bmqgen: Biomedical query generator for knowledge discovery",
                    "authors": [
                        "F. Shen",
                        "H. Liu",
                        "S. Sohn",
                        "D. Larson",
                        "Y. Lee"
                    ],
                    "venue": "Bioinformatics and Biomedicine (BIBM), 2015 IEEE International Conference on, pp. 1092\u20131097, 2015.",
                    "year": 2015
                },
                {
                    "title": "Measures of semantic similarity and relatedness in the biomedical domain",
                    "authors": [
                        "T. Pedersen",
                        "S.V. Pakhomov",
                        "S. Patwardhan",
                        "C.G. Chute"
                    ],
                    "venue": "Journal of biomedical informatics, vol. 40, no. 3, pp. 288\u2013299, 2007.",
                    "year": 2007
                },
                {
                    "title": "Semantic similarity measures in mesh ontology and their application to information retrieval on medline",
                    "authors": [
                        "A. Hliaoutakis"
                    ],
                    "venue": "Master\u2019s thesis, 2005.",
                    "year": 2005
                },
                {
                    "title": "Towards a framework for developing semantic relatedness reference standards",
                    "authors": [
                        "S.V. Pakhomov",
                        "T. Pedersen",
                        "B. McInnes",
                        "G.B. Melton",
                        "A. Ruggieri",
                        "C.G. Chute"
                    ],
                    "venue": "Journal of biomedical informatics, vol. 44, no. 2, pp. 251\u2013265, 2011.",
                    "year": 2011
                },
                {
                    "title": "Semantic similarity and relatedness between clinical terms: an experimental study",
                    "authors": [
                        "S. Pakhomov",
                        "B. McInnes",
                        "T. Adam",
                        "Y. Liu",
                        "T. Pedersen",
                        "G.B. Melton"
                    ],
                    "venue": "AMIA annual symposium proceedings, vol. 2010. American Medical Informatics Association, 2010, p. 572.",
                    "year": 2010
                },
                {
                    "title": "Corpus domain effects on distributional semantic modeling of medical terms",
                    "authors": [
                        "S.V. Pakhomov",
                        "G. Finley",
                        "R. McEwan",
                        "Y. Wang",
                        "G.B. Melton"
                    ],
                    "venue": "Bioinformatics, vol. 32, no. 23, pp. 3635\u20133644, 2016.",
                    "year": 2016
                },
                {
                    "title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.",
                    "authors": [
                        "M. Baroni",
                        "G. Dinu",
                        "G. Kruszewski"
                    ],
                    "year": 2014
                },
                {
                    "title": "Contextual correlates of synonymy",
                    "authors": [
                        "H. Rubenstein",
                        "J.B. Goodenough"
                    ],
                    "venue": "Communications of the ACM, vol. 8, no. 10, pp. 627\u2013633, 1965.",
                    "year": 1965
                },
                {
                    "title": "Placing search in context: The concept revisited",
                    "authors": [
                        "L. Finkelstein",
                        "E. Gabrilovich",
                        "Y. Matias",
                        "E. Rivlin",
                        "Z. Solan",
                        "G. Wolfman",
                        "E. Ruppin"
                    ],
                    "venue": "Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 406\u2013414.",
                    "year": 2001
                },
                {
                    "title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.",
                    "authors": [
                        "T.K. Landauer",
                        "S.T. Dumais"
                    ],
                    "venue": "Psychological review,",
                    "year": 1997
                },
                {
                    "title": "Attributes in lexical acquisition",
                    "authors": [
                        "A. Almuhareb"
                    ],
                    "venue": "Ph.D. dissertation, University of Essex, 2006.",
                    "year": 2006
                },
                {
                    "title": "Dependency-based construction of semantic space models",
                    "authors": [
                        "S. Pad\u00f3",
                        "M. Lapata"
                    ],
                    "venue": "Computational Linguistics, vol. 33, no. 2, pp. 161\u2013199, 2007.",
                    "year": 2007
                },
                {
                    "title": "Evaluation methods for unsupervised word embeddings.",
                    "authors": [
                        "T. Schnabel",
                        "I. Labutov",
                        "D.M. Mimno",
                        "T. Joachims"
                    ],
                    "venue": "in EMNLP,",
                    "year": 2015
                },
                {
                    "title": "Natural language processing (almost) from scratch",
                    "authors": [
                        "R. Collobert",
                        "J. Weston",
                        "L. Bottou",
                        "M. Karlen",
                        "K. Kavukcuoglu",
                        "P. Kuksa"
                    ],
                    "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u20132537, 2011.",
                    "year": 2011
                },
                {
                    "title": "Word emdeddings through hellinger pca",
                    "authors": [
                        "R. Lebret",
                        "R. Collobert"
                    ],
                    "venue": "arXiv preprint arXiv:1312.5542, 2013.",
                    "year": 2013
                },
                {
                    "title": "Glove: Global vectors for word representation",
                    "authors": [
                        "J. Pennington",
                        "R. Socher",
                        "C. Manning"
                    ],
                    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532\u20131543.",
                    "year": 2014
                },
                {
                    "title": "Two step cca: A new spectral method for estimating vector models of words",
                    "authors": [
                        "P. Dhillon",
                        "J. Rodu",
                        "D. Foster",
                        "L. Ungar"
                    ],
                    "venue": "arXiv preprint arXiv:1206.6403, 2012.",
                    "year": 2012
                },
                {
                    "title": "Very sparse random projections",
                    "authors": [
                        "P. Li",
                        "T.J. Hastie",
                        "K.W. Church"
                    ],
                    "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 287\u2013296.",
                    "year": 2006
                },
                {
                    "title": "Word embedding evaluation and combination.",
                    "authors": [
                        "S. Ghannay",
                        "B. Favre",
                        "Y. Esteve",
                        "N. Camelin"
                    ],
                    "venue": "in LREC,",
                    "year": 2016
                },
                {
                    "title": "Cslm-a modular open-source continuous space language modeling toolkit.",
                    "authors": [
                        "H. Schwenk"
                    ],
                    "venue": "INTERSPEECH,",
                    "year": 2013
                },
                {
                    "title": "Evaluating word embeddings using a representative suite of practical tasks",
                    "authors": [
                        "N. Nayak",
                        "G. Angeli",
                        "C.D. Manning"
                    ],
                    "venue": "ACL 2016, p. 19, 2016.",
                    "year": 2016
                },
                {
                    "title": "Systematic analysis of free-text family history in electronic health record",
                    "authors": [
                        "Y. Wang",
                        "L. Wang",
                        "M. Rastegar-Mojarad",
                        "S. Liu",
                        "F. Shen",
                        "H. Liu"
                    ],
                    "venue": "AMIA Summits on Translational Science Proceedings, vol. 2017, p. 104, 2017.",
                    "year": 2017
                },
                {
                    "title": "Visualizing data using t-sne",
                    "authors": [
                        "L. v. d. Maaten",
                        "G. Hinton"
                    ],
                    "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.",
                    "year": 2008
                },
                {
                    "title": "Enriching word vectors with subword information",
                    "authors": [
                        "P. Bojanowski",
                        "E. Grave",
                        "A. Joulin",
                        "T. Mikolov"
                    ],
                    "venue": "arXiv preprint arXiv:1607.04606, 2016.",
                    "year": 2016
                },
                {
                    "title": "Identifying patient smoking status from medical discharge records",
                    "authors": [
                        "\u00d6. Uzuner",
                        "I. Goldstein",
                        "Y. Luo",
                        "I. Kohane"
                    ],
                    "venue": "Journal of the American Medical Informatics Association, vol. 15, no. 1, pp. 14\u201324, 2008.",
                    "year": 2008
                },
                {
                    "title": "A distant supervision paradigm for clinical information extraction",
                    "authors": [
                        "Y. Wang",
                        "E. Atkinson",
                        "S. Amin",
                        "H. Liu"
                    ],
                    "venue": "2018.",
                    "year": 2018
                },
                {
                    "title": "Overview of the trec 2016 clinical decision support track.",
                    "authors": [
                        "K. Roberts",
                        "D. Demner-Fushman",
                        "E.M. Voorhees",
                        "W.R. Hersh"
                    ],
                    "venue": "TREC,",
                    "year": 2016
                },
                {
                    "title": "Mimic-iii, a freely accessible critical care database",
                    "authors": [
                        "A.E. Johnson",
                        "T.J. Pollard",
                        "L. Shen",
                        "L.-w. H. Lehman",
                        "M. Feng",
                        "M. Ghassemi",
                        "B. Moody",
                        "P. Szolovits",
                        "L.A. Celi",
                        "R.G. Mark"
                    ],
                    "venue": "Scientific data, vol. 3, 2016.",
                    "year": 2016
                },
                {
                    "title": "Indri: A language model-based search engine for complex queries",
                    "authors": [
                        "T. Strohman",
                        "D. Metzler",
                        "H. Turtle",
                        "W.B. Croft"
                    ],
                    "venue": "Proceedings of the International Conference on Intelligent Analysis, vol. 2, no. 6. Citeseer, 2005, pp. 2\u20136.",
                    "year": 2005
                },
                {
                    "title": "Two-stage language models for information retrieval",
                    "authors": [
                        "C. Zhai",
                        "J. Lafferty"
                    ],
                    "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2002, pp. 49\u201356.",
                    "year": 2002
                },
                {
                    "title": "A simple and efficient sampling method for estimating ap and ndcg",
                    "authors": [
                        "E. Yilmaz",
                        "E. Kanoulas",
                        "J.A. Aslam"
                    ],
                    "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2008, pp. 603\u2013610.",
                    "year": 2008
                },
                {
                    "title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013).",
                    "authors": [
                        "I. Segura Bedmar",
                        "P. Mart\u0131\u0301nez",
                        "M. Herrero Zazo"
                    ],
                    "venue": "Association for Computational Linguistics,",
                    "year": 2013
                }
            ],
            "id": "SP:4b9bcb35acb444bc5c62eac7aa5930b0bc56819a",
            "authors": [
                {
                    "name": "Yanshan Wang",
                    "affiliations": []
                },
                {
                    "name": "Sijia Liu",
                    "affiliations": []
                },
                {
                    "name": "Naveed Afzal",
                    "affiliations": []
                },
                {
                    "name": "Majid Rastegar-Mojarad",
                    "affiliations": []
                },
                {
                    "name": "Liwei Wang",
                    "affiliations": []
                },
                {
                    "name": "Feichen Shen",
                    "affiliations": []
                },
                {
                    "name": "Paul Kingsbury",
                    "affiliations": []
                },
                {
                    "name": "Hongfang Liu",
                    "affiliations": []
                }
            ],
            "abstractText": "Background Neural word embeddings have been widely used in biomedical Natural Language Processing (NLP) applications as they provide vector representations of words capturing the semantic properties of words and the linguistic relationship between words. Many biomedical applications use different textual resources (e.g., Wikipedia and biomedical articles) to train word embeddings and apply these word embeddings to downstream biomedical applications. However, there has been little work on evaluating the word embeddings trained from these resources. Methods In this study, we provide an empirical evaluation of word embeddings trained from four different resources, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings utilizing clinical notes available at Mayo Clinic and articles from the PubMed Central (PMC), respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. We performed the evaluation qualitatively and quantitatively. In qualitative evaluation, we manually inspected five most similar medical words to a given set of target medical words, and then analyzed word embeddings through the visualization of those word embeddings. In quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. Intrinsic evaluation directly tested semantic relationships between medical words using four published datasets for measuring semantic similarity between medical terms, i.e., Pedersen, Hliaoutakis, MayoSRS, and UMNSRS. In extrinsic evaluation, we applied word embeddings to downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), using data from shared tasks. Results Qualitative evaluation shows that the word embeddings trained from EHR and PubMed can find more relevant similar medical terms than these from GloVe and Google News. In the intrinsic quantitative evaluation, the semantic similarity captured by the word embeddings trained from EHR are closer to human experts\u2019 judgments on all four tested datasets. In the extrinsic quantitative evaluation, the word embeddings trained from EHR has the best F1 score of 0.900 for the clinical IE task; no word embeddings improve the performance in terms of inference average precision and mean average precision for the biomedical IR task; and the word embeddings trained on Google News has the best overall F1 score of 0.790 for the RE task.",
            "title": "A Comparison of Word Embeddings for the Biomedical Natural Language Processing"
        }
    },
    "6799048": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "In recent years, neural network based models have become the workhorse of natural language understanding and generation. They empower industrial machine translation (Wu et al., 2016) and text generation (Kannan et al., 2016) systems and show state-of-the-art performance on numerous benchmarks including Recognizing Textual Entailment (Gong et al., 2017), Visual Question Answering (Jiang et al., 2018), and Reading Comprehension (Wang et al., 2018). Despite these successes, a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world. Investigations on the three aforementioned tasks have shown that neural models easily latch onto statistical regularities which are omnipresent in existing datasets (Agrawal et al., 2016; Gururangan et al., 2018; Jia & Liang, 2017) and extremely hard to avoid in large scale data collection. Having learned such dataset-specific solutions, neural networks fail to make correct predictions for examples that are even slightly out of domain, yet are trivial for humans. These findings have been corroborated by a recent investigation on a synthetic instruction-following task (Lake & Baroni, 2018), in which seq2seq models (Sutskever et al., 2014; Bahdanau et al., 2015) have shown little systematicity (Fodor & Pylyshyn, 1988) in how they generalize, that is they do not learn general rules on how to compose words and fail spectacularly when for example asked to interpret \u201cjump twice\u201d after training on \u201cjump\u201d, \u201crun twice\u201d and \u201cwalk twice\u201d.\nAn appealing direction to improve the generalization capabilities of neural models is to add modularity and structure to their design to make them structurally resemble the kind of rules they are\n\u2217Equal contribution\nar X\niv :1\n81 1.\n12 88\n9v 3\n[ cs\n.C L\n] 2\n1 A\npr 2\nsupposed to learn (Andreas et al., 2016; Gaunt et al., 2016). For example, in the Neural Module Network paradigm (NMN, Andreas et al. (2016)), a neural network is assembled from several neural modules, where each module is meant to perform a particular subtask of the input processing, much like a computer program composed of functions. The NMN approach is intuitively appealing but its widespread adoption has been hindered by the large amount of domain knowledge that is required to decide (Andreas et al., 2016) or predict (Johnson et al., 2017; Hu et al., 2017) how the modules should be created (parametrization) and how they should be connected (layout) based on a natural language utterance. Besides, their performance has often been matched by more traditional neural models, such as FiLM (Perez et al., 2017), Relations Networks (Santoro et al., 2017), and MAC networks (Hudson & Manning, 2018). Lastly, generalization properties of NMNs, to the best of our knowledge, have not been rigorously studied prior to this work.\nHere, we investigate the impact of explicit modularity and structure on systematic generalization of NMNs and contrast their generalization abilities to those of generic models. For this case study, we focus on the task of visual question answering (VQA), in particular its simplest binary form, when the answer is either \u201cyes\u201d or \u201cno\u201d. Such a binary VQA task can be seen as a fundamental task of language understanding, as it requires one to evaluate the truth value of the utterance with respect to the state of the world. Among many systematic generalization requirements that are desirable for a VQA model, we choose the following basic one: a good model should be able to reason about all possible object combinations despite being trained on a very small subset of them. We believe that this is a key prerequisite to using VQA models in the real world, because they should be robust at handling unlikely combinations of objects. We implement our generalization demands in the form of a new synthetic dataset, called Spatial Queries On Object Pairs (SQOOP), in which a model has to perform spatial relational reasoning about pairs of randomly scattered letters and digits in the image (e.g. answering the question \u201cIs there a letter A left of a letter B?\u201d). The main challenge in SQOOP is that models are evaluated on all possible object pairs, but trained on only a subset of them.\nOur first finding is that NMNs do generalize better than other neural models when layout and parametrization are chosen appropriately. We then investigate which factors contribute to improved generalization performance and find that using a layout that matches the task (i.e. a tree layout, as opposed to a chain layout), is crucial for solving the hardest version of our dataset. Lastly, and perhaps most importantly, we experiment with existing methods for making NMNs more end-to-end by inducing the module layout (Johnson et al., 2017) or learning module parametrization through soft-attention over the question (Hu et al., 2017). Our experiments show that such end-to-end approaches often fail by not converging to tree layouts or by learning a blurred parameterization for modules, which results in poor generalization on the hardest version of our dataset. We believe that our findings challenge the intuition of researchers in the field and provide a foundation for improving systematic generalization of neural approaches to language understanding."
                },
                {
                    "heading": "2 THE SQOOP DATASET FOR TESTING SYSTEMATIC GENERALIZATION",
                    "text": "We perform all experiments of this study on the SQOOP dataset. SQOOP is a minimalistic VQA task that is designed to test the model\u2019s ability to interpret unseen combinations of known relation and object words. Clearly, given known objects X, Y and a known relation R, a human can easily verify whether or not the objects X and Y are in relation R. Some instances of such queries are common in daily life (is there a cup on the table), some are extremely rare (is there a violin under the car), and some are unlikely but have similar, more likely counter-parts (is there grass on the frisbee vs is there a frisbee on the grass). Still, a person can easily answer these questions by understanding them as just the composition of the three separate concepts. Such compositional reasoning skills are clearly required for language understanding models, and SQOOP is explicitly designed to test for them.\nConcretely speaking, SQOOP requires observing a 64 \u00d7 64 RGB image x and answering a yes-no question q = XRY about whether objects X and Y are in a spatial relation R. The questions are represented in a redundancy-free XRY form; we did not aim to make the questions look like natural language. Each image contains 5 randomly chosen and randomly positioned objects. There are 36 objects: the latin letters A-Z and digits 0-9, and there are 4 relations: LEFT OF, RIGHT OF, ABOVE, and BELOW. This results in 36 \u00b7 35 \u00b7 4 = 5040 possible unique questions (we do not allow questions about identical objects). To make negative examples challenging, we ensure that both X and Y of a question are always present in the associated image and that there are distractor objects Y \u2032 6= Y\na: S above T? Yes\nb: W left of A? No\nFigure 2: A positive (top) and negative (bottom) example from the SQOOP dataset.\nand X \u2032 6= X such that XRY \u2032 and X \u2032RY are both true for the image. These extra precautions guarantee that answering a question requires the model to locate all possible X and Y then check if any pair of them are in the relation R. Two SQOOP examples are shown in Figure 2.\nOur goal is to discover which models can correctly answer questions about all 36 \u00b735 possible object pairs in SQOOP after having been trained on only a subset. For this purpose we build training sets containing 36 \u00b7 4 \u00b7 k unique questions by sampling k different right-hand-side (RHS) objects Y1, Y2, ..., Yk for each left-hand-side (LHS) object X. We use this procedure instead of just uniformly sampling object pairs in order to ensure that each object appears in at least one training question, thereby keeping the all versions of the dataset solvable. We will refer to k as the #rhs/lhs parameter of the dataset. Our test set is composed from the remaining 36 \u00b7 4 \u00b7 (35\u2212 k) questions. We generate training and test sets for rhs/lhs values of 1,2,4,8 and 18, as well as a control version of the dataset, #rhs/lhs=35, in which both the training and the test set contain all the questions (with different images). Note that lower #rhs/lhs versions are harder for generalization due to the presence of spurious dependencies between the words X and Y to which the models may adapt. In order to exclude a possible compounding factor of overfitting on the training images, all our training sets contain 1 million examples, so for a dataset with #rhs/lhs = k we generate approximately 106/(36 \u00b7 4\u00b7k) different images per unique question. Appendix D contains pseudocode for SQOOP generation."
                },
                {
                    "heading": "3 MODELS",
                    "text": "A great variety of VQA models have been recently proposed in the literature, among which we can distinguish two trends. Some of the recently proposed models, such as FiLM (Perez et al., 2017) and Relation Networks (RelNet, Santoro et al. (2017)) are highly generic and do not require any taskspecific knowledge to be applied on a new dataset. On the opposite end of the spectrum are modular and structured models, typically flavours of Neural Module Networks (Andreas et al., 2016), that do require some knowledge about the task at hand to be instantiated. Here, we evaluate systematic generalization of several state-of-the-art models in both families. In all models, the image x is first fed through a CNN based network, that we refer to as the stem, to produce a feature-level 3D tensor hx. This is passed through a model-specific computation conditioned on the question q, to produce a joint representation hq x. Lastly, this representation is fed into a fully-connected classifier network to produce logits for prediction. Therefore, the main difference between the models we consider is how the computation hq x = model(hx, q) is performed."
                },
                {
                    "heading": "3.1 GENERIC MODELS",
                    "text": "We consider four generic models in this paper: CNN+LSTM, FiLM, Relation Network (RelNet), and Memory-Attention-Control (MAC) network. For CNN+LSTM, FiLM, and RelNet models, the question q is first encoded into a fixed-size representation hq using a unidirectional LSTM network. CNN+LSTM flattens the 3D tensor hx to a vector and concatenates it with hq to produce hq x:\nhq x = [flatten(hx);hq]. (1)\nRelNet (Santoro et al., 2017) uses a network g which is applied to all pairs of feature columns of hx concatenated with the question representation hq , all of which is then pooled to obtain hq x:\nhq x = \u2211 i,j g(hx(i), hx(j), hq) (2)\nwhere hx(i) is the i-th feature column of hx. FiLM networks (Perez et al., 2017) use N convolutional FiLM blocks applied to hx. A FiLM block is a residual block (He et al., 2016) in which a feature-wise affine transformation (FiLM layer) is inserted after the 2nd convolutional layer. The FiLM layer is conditioned on the question at hand via prediction of the scaling and shifting parameters \u03b3n and \u03b2n:\n[\u03b3n;\u03b2n] =W n q hq + b n q (3)\nh\u0303nq x = BN(W n 2 \u2217ReLU(Wn1 \u2217 hn\u22121q x + bn)) (4)\nhnq x = h n\u22121 q x +ReLU(\u03b3n h\u0303nq x \u2295 \u03b2n) (5)\nwhere BN stands for batch normalization (Ioffe & Szegedy, 2015), \u2217 stands for convolution and stands for element-wise multiplications. hnq x is the output of the n-th FiLM block and h 0 q x = hx. The output of the last FiLM block hNq x undergoes an extra 1 \u00d7 1 convolution and max-pooling to produce hq x. MAC network of Hudson & Manning (2018) produces hq x by repeatedly applying a Memory-Attention-Composition (MAC) cell that is conditioned on the question through an attention mechanism. The MAC model is too complex to be fully described here and we refer the reader to the original paper for details."
                },
                {
                    "heading": "3.2 NEURAL MODULE NETWORKS",
                    "text": "Neural Module Networks (NMN) (Andreas et al., 2016) are an elegant approach to question answering that constructs a question-specific network by composing together trainable neural modules, drawing inspiration from symbolic approaches to question answering (Malinowski & Fritz, 2014). To answer a question with an NMN, one first constructs the computation graph by making the following decisions: (a) how many modules and of which types will be used, (b) how will the modules be connected to each other, and (c) how are these modules parametrized based on the question. We refer to the aspects (a) and (b) of the computation graph as the layout and the aspect (c) as the parametrization. In the original NMN and in many follow-up works, different module types are used to perform very different computations, e.g. the Find module from Hu et al. (2017) performs trainable convolutions on the input attention map, whereas the And module from the same paper computes an element-wise maximum for two input attention maps. In this work, we follow the trend of using more homogeneous modules started by Johnson et al. (2017), who use only two types of modules: unary and binary, both performing similar computations. We restrict our study to NMNs with homogeneous modules because they require less prior knowledge to be instantiated and because they performed well in our preliminary experiments despite their relative simplicity. We go one step further than Johnson et al. (2017) and retain a single binary module type, using a zero tensor for the second input when only one input is available. Additionally, we choose to use exactly three modules, which simplifies the layout decision to just determining how the modules are connected. Our preliminary experiments have shown that, even after these simplifications, NMNs are far ahead of other models in terms of generalization.\nIn the original NMN, the layout and parametrization were set in an ad-hoc manner for each question by analyzing a dependency parse. In the follow-up works (Johnson et al., 2017; Hu et al., 2017), these aspects of the computation are predicted by learnable mechanisms with the goal of reducing the amount of background knowledge required to apply the NMN approach to a new task. We experiment with the End-to-End NMN (N2NMN) (Hu et al., 2017) paradigm from this family, which predicts the layout with a seq2seq model (Sutskever et al., 2014) and computes the parametrization of the modules using a soft attention mechanism. Since all the questions in SQOOP have the same structure, we do not employ a seq2seq model but instead have a trainable layout variable and trainable attention variables for each module.\nFormally, our NMN is constructed by repeatedly applying a generic neural module f(\u03b8, \u03b3, s0, s1), which takes as inputs the shared parameters \u03b8, the question-specific parametrization \u03b3 and the lefthand side and right-hand side inputs s0 and s1. Three such modules are connected and conditioned\non a question q = (q1, q2, q3) as follows:\n\u03b3k = 3\u2211 i=1 \u03b1k,ie(qi) (6)\nsmk = k\u22121\u2211 j=\u22121 \u03c4k,jm sj (7)\nsk = f(\u03b8, \u03b3k, s 0 k, s 1 k) (8)\nhqx = s3 (9)\nIn the equations above, s\u22121 = 0 is the zero tensor input, s0 = hx are the image features outputted by the stem, e is the embedding table for question words. k \u2208 {1, 2, 3} is the module number, sk is the output of the k-th module and smk are its left (m = 0) and right (m = 1) inputs. We refer to A = (\u03b1k,i) and T = (\u03c4k,jm ) as the parametrization attention matrix and the layout tensor respectively.\nWe experiment with two choices for the NMN\u2019s generic neural module: the Find module from Hu et al. (2017) and the Residual module from Johnson et al. (2017). The equations for the Residual module are as follows:\n[W k1 ; b k 1 ;W k 2 ; b k 2 ;W k 3 ; b k 3 ] = \u03b3k (10)\ns\u0303k = ReLU(W k 3 \u2217 [s0k; s1k] + bk3), (11)\nfResidual(\u03b3k, s 0 k, s 1 k) = ReLU(s\u0303k +W k 1 \u2217ReLU(W k2 \u2217 s\u0303k + bk2)) + bk1), (12)\nand for Find module as follows:\n[W1; b1;W2; b2] = \u03b8, (13)\nfFind(\u03b8, \u03b3k, s 0 k, s 1 k) = ReLU(W1 \u2217 \u03b3k ReLU(W2 \u2217 [ s0k; s 1 k ] + b2) + b1). (14)\nIn the formulas above allW \u2019s stand for convolution weights, and all b\u2019s are biases. Equations 10 and 13 should be understood as taking vectors \u03b3k and \u03b8 respectively and chunking them into weights and biases. The main difference between Residual and Find is that in Residual all parameters depend on the questions words (hence \u03b8 is omitted from the signature of fResidual), where as in Find convolutional weights are the same for all questions, and only the element-wise multipliers \u03b3k vary based on the question. We note that the specific Find module we use in this work is slightly different from the one used in (Hu et al., 2017) in that it outputs a feature tensor, not just an attention map. This change was required in order to connect multiple Find modules in the same way as we connect multiple residual ones.\nBased on the generic NMN model described above, we experiment with several specific architectures that differ in the way the modules are connected and parametrized (see Figure 1). In NMN-Chain the modules form a sequential chain. Modules 1, 2 and 3 are parametrized based on the first object word, second object word and the relation word respectively, which is achieved by setting the attention maps \u03b11, \u03b12, \u03b13 to the corresponding one-hot vectors. We also experiment with giving the image features hx as the right-hand side input to all 3 modules and call the resulting model NMN-ChainShortcut. NMN-Tree is similar to NMN-Chain in that the attention vectors are similarly hardcoded, but we change the connectivity between the modules to be tree-like. Stochastic N2NMN follows the N2NMN approach by Hu et al. (2017) for inducing layout. We treat the layout T as a stochastic latent variable. T is allowed to take two values: Ttree as in NMN-Tree, and Tchain as in NMN-Chain. We calculate the output probabilities by marginalizing out the layout i.e. probability of answer being \u201cyes\u201d is computed as p(yes|x, q) = \u2211T\u2208{Ttree,Tchain} p(yes|T, x, q)p(T ). Lastly, Attention N2NMN uses the N2NMN method for learning parametrization (Hu et al., 2017). It is structured just like NMN-Tree but has \u03b1k computed as softmax(\u03b1\u0303k), where \u03b1\u0303k is a trainable vector. We use Attention N2NMN only with the Find module because using it with the Residual module would involve a highly non-standard interpolation between convolutional weights."
                },
                {
                    "heading": "4 EXPERIMENTS",
                    "text": "In our experiments we aimed to: (a) understand which models are capable of exhibiting systematic generalization as required by SQOOP, and (b) understand whether it is possible to induce, in an end-to-end way, the successful architectural decisions that lead to systematic generalization.\nAll models share the same stem architecture which consists of 6 layers of convolution (8 for Relation Networks), batch normalization and max pooling. The input to the stem is a 64 \u00d7 64 \u00d7 3 image, and the feature dimension used throughout the stem is 64. Further details can be found in Appendix A. The code for all experiments is available online1."
                },
                {
                    "heading": "4.1 WHICH MODELS GENERALIZE BETTER?",
                    "text": "We report the performance for all models on datasets of varying difficulty in Figure 3. Our first observation is that the modular and tree-structured NMN-Tree model exhibits strong systematic generalization. Both versions of this model, with Residual and Find modules, robustly solve all versions of our dataset, including the most challenging #rhs/lhs=1 split.\nThe results of NMN-Tree should be contrasted with those of generic models. 2 out of 4 models (Conv+LSTM and RelNet) are not able to learn to answer all SQOOP questions, no matter how easy the split was (for high #rhs/lhs Conv+LSTM overfitted and RelNet did not train). The results of other two models, MAC and FiLM, are similar. Both models are clearly able to solve the SQOOP task, as suggested by their almost perfect < 1% error rate on the control #rhs/lhs=35 split, yet they struggle to generalize on splits with lower #rhs/lhs. In particular, we observe 13.67\u00b1 9.97% errors for MAC and a 34.73 \u00b1 4.61% errors for FiLM on the hardest #rhs/lhs=1 split. For the splits of intermediate difficulty we saw the error rates of both models decreasing as we increased the #rhs/lhs ratio from 2 to 18. Interestingly, even with 18 #rhs/lhs some MAC and FiLM runs result in a test error rate of\u223c 2%. Given the simplicity and minimalism of SQOOP questions, we believe that these results should be considered a failure to pass the SQOOP test for both MAC and FiLM. That said, we note a difference in how exactly FiLM and MAC fail on #rhs/lhs=1: in several runs (3 out of 15) MAC exhibits a strong generalization performance (\u223c 0.5% error rate), whereas in all runs of FiLM the error rate is about 30%. We examine the successful MAC models and find that they converge to a successful setting of the control attention weights, where specific MAC units consistently attend to the right questions words. In particular, MAC models that generalize strongly for each question seem to have a unit focusing strongly on X and a unit focusing strongly on Y (see Appendix B for more details). As MAC was the strongest competitor of NMN-Tree across generic models, we perform an ablation study for this model, in which we vary the number of modules and hidden units, as well as experiment with weight decay. These modifications do not result in any significant reduction of the gap between MAC and NMN-Tree. Interestingly, we find that using the default high number of MAC units, namely 12, is helpful, possibly because it increases the likelihood that at least one unit converges to focus on X and Y words (see Appendix B for details)."
                },
                {
                    "heading": "4.2 WHAT IS ESSENTIAL TO STRONG GENERALIZATION OF NMN?",
                    "text": "The superior generalization of NMN-Tree raises the following question: what is the key architectural difference between NMN-Tree and generic models that explains the performance gap between them? We consider two candidate explanations. First, the NMN-Tree model differs from the generic models in that it does not use a language encoder and is instead built from modules that are parametrized by question words directly. Second, NMN-Tree is structured in a particular way, with the idea that modules 1 and 2 may learn to locate objects and module 3 can learn to reason about object locations independently of their identities. To understand which of the two differences is responsible for the superior generalization, we compare the performance of the NMN-Tree, NMN-Chain and NMNChain-Shortcut models (see Figure 1). These 3 versions of NMN are similar in that none of them are using a language encoder, but they differ in how the modules are connected. The results in Figure 3 show that for both Find and Residual module architectures, using a tree layout is absolutely crucial (and sufficient) for generalization, meaning that the generalization gap between NMN-Tree and generic models can not be explained merely by the language encoding step in the latter. In particular, NMN-Chain models perform barely above random chance, doing even worse than generic models on\n1https://github.com/rizar/systematic-generalization-sqoop\nthe #rhs/lhs=1 version of the dataset and dramatically failing even on the easiest #rhs/lhs=18 split. This is in stark contrast with NMN-Tree models that exhibits nearly perfect performance on the hardest #rhs/lhs=1 split. As a sanity check we train NMN-Chain models on the vanilla #rhs/lhs=35 split. We find that NMN-Chain has little difficulty learning to answer SQOOP questions when it sees all of them at training time, even though it previously shows poor generalization when testing on unseen examples. Interestingly, NMN-Chain-Shortcut performs much better than NMN-Chain and quite similarly to generic models. We find it remarkable that such a slight change in the model layout as adding shortcut connections from image features hx to the modules results in a drastic change in generalization performance. In an attempt to understand why NMN-Chain generalizes so poorly we compare the test set responses of the 5 NMN-Chain models trained on #rhs/lhs=1 split. Notably, there was very little agreement between predictions of these 5 runs (Fleiss \u03ba = 0.05), suggesting that NMN-Chain performs rather randomly outside of the training set."
                },
                {
                    "heading": "4.3 CAN THE RIGHT KIND OF NMN BE INDUCED?",
                    "text": "The strong generalization of the NMN-Tree is impressive, but a significant amount of prior knowledge about the task was required to come up with the successful layout and parametrization used in this model. We therefore investigate whether the amount of such prior knowledge can be reduced by fixing one of these structural aspects and inducing the other."
                },
                {
                    "heading": "4.3.1 LAYOUT INDUCTION",
                    "text": "In our layout induction experiments, we use the Stochastic N2NMN model which treats the layout as a stochastic latent variable with two values (Ttree and Tchain, see Section 3.2 for details). We experiment with N2NMNs using both Find and Residual modules and report results with different\ninitial conditions, p0(tree) \u2208 0.1, 0.5, 0.9. We believe that the initial probability p0(tree) = 0.1 should not be considered small, since in more challenging datasets the space of layouts would be exponentially large, and sampling the right layout in 10% of all cases should be considered a very lucky initialization. We repeat all experiments on #rhs/lhs=1 and on #rhs/lhs=18 splits, the former to study generalization, and the latter to control whether the failures on #rhs/lhs=1 are caused specifically by the difficulty of this split. The results (see Table 1) show that the success of layout induction (i.e. converging to a p(tree) close to 0.9) depends in a complex way on all the factors that we considered in our experiments. The initialization has the most influence: models initialized with p0(tree) = 0.1 typically do not converge to a tree (exception being experiments with Residual module on #rhs/lhs=18, in which 3 out of 5 runs converged to a solution with a high p(tree)). Likewise, models initialized with p0(tree) = 0.9 always stay in a regime with a high p(tree). In the intermediate setting of p0(tree) = 0.5 we observe differences in behaviors for Residual and Find modules. In particular, N2NMN based on Residual modules stays spurious with p(tree) = 0.5\u00b1 0.08 when #rhs/lhs=1, whereas N2NMN based on Find modules always converges to a tree.\nOne counterintuitive result in Table 1 is that for the Stochastic N2NMNs with Residual modules, trained with p0(tree) = 0.5 and #rhs/lhs=1, make just 1.64\u00b11.79% test error despite never resolving the layout uncertainty through training (p200K(tree) = 0.56 \u00b1 0.06). We offer an investigation of this result in Appendix C."
                },
                {
                    "heading": "4.3.2 PARAMETRIZATION INDUCTION",
                    "text": "Next, we experiment with the Attention N2NMN model (see Section 3.2) in which the parametrization is learned for each module as an attention-weighted average of word embeddings. In these experiments, we fix the layout to be tree-like and sample the pre-softmax attention weights \u03b1\u0303 from a uniform distribution U [0; 1]. As in the layout induction investigations, we experiment with several SQOOP splits, namely we try #rhs/lhs \u2208 {1, 2, 18}. The results (reported in Table 2) show that Attention N2NMN fails dramatically on #rhs/lhs=1 but quickly catches up as soon as #rhs/lhs is increased to 2. Notably, 9 out of 10 runs on #rhs/lhs=2 result in almost perfect performance, and 1 run completely fails to generalize (26% error rate), resulting in a high 8.18% variance of the mean\nerror rate. All 10 runs on the split with 18 rhs/lhs generalize flawlessly. Furthermore, we inspect the learned attention weights and find that for typical successful runs, module 3 focuses on the relation word, whereas modules 1 and 2 focus on different object words (see Figure 6) while still focusing on the relation word. To better understand the relationship between successful layout induction and generalization, we define an attention quality metric \u03ba = minw\u2208{X,Y }maxk\u22081,2 \u03b1k,w/(1\u2212 \u03b1k,R). Intuitively, \u03ba is large when for each word w \u2208 X,Y there is a module i that focuses mostly on this word. The renormalization by 1/(1 \u2212 \u03b1k,R) is necessary to factor out the amount of attention that modules 1 and 2 assign to the relation word. For the ground-truth parametrization that we use for NMN-Tree \u03ba takes a value of 1, and if both modules 1 and 2 focus on X, completely ignoring Y, \u03ba equals 0. The scatterplot of the test error rate versus \u03ba (Figure 5) shows that for #rhs/lhs=1 high generalization is strongly associated with higher \u03ba, meaning that it is indeed necessary to have different modules strongly focusing on different object words in order to generalize in this most challenging setting. Interestingly, for #rhs/lhs=2 we see a lot of cases where N2NMN generalizes well despite attention being rather spurious (\u03ba \u2248 0.6). In order to put Attention N2NMN results in context we compare them to those of MAC (see Table 2). Such a comparison can be of interest because both models perform attention over the question. For 1 rhs/lhs MAC seems to be better on average, but as we increase #rhs/lhs to 2 we note that Attention N2NMN succeeds in 9 out of 10 cases on the #rhs/lhs=2 split, much more often than 1 success out of 10 observed for MAC2. This result suggests that Attention N2NMNs retains some of the strong generalization potential of NMNs with hard-coded parametrization."
                },
                {
                    "heading": "5 RELATED WORK",
                    "text": "The notion of systematicity was originally introduced by (Fodor & Pylyshyn, 1988) as the property of human cognition whereby \u201cthe ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents\u201d. They illustrate this with an example that no English\n2If we judge a run successful when the error rate is lower than \u03c4 = 1%, these success rates are different with a p-value of 0.001 according to the Fisher exact test. Same holds for any other threshold \u03c4 \u2208 [1%; 5%].\nspeaker can understand the phrase \u201cJohn loves the girl\u201d without being also able to understand the phrase \u201cthe girl loves John\u201d. The question of whether or not connectionist models of cognition can account for the systematicity phenomenon has been a subject of a long debate in cognitive science (Fodor & Pylyshyn, 1988; Smolensky, 1987; Marcus, 1998; 2003; Calvo & Colunga, 2003). Recent research has shown that lack of systematicity in the generalization is still a concern for the modern seq2seq models (Lake & Baroni, 2018; Bastings et al., 2018; Loula et al., 2018). Our findings about the weak systematic generalization of generic VQA models corroborate the aforementioned seq2seq results. We also go beyond merely stating negative generalization results and showcase the high systematicity potential of adding explicit modularity and structure to modern deep learning models.\nBesides the theoretical appeal of systematicity, our study is inspired by highly related prior evidence that when trained on downstream language understanding tasks, neural networks often generalize poorly and latch on to dataset-specific regularities. Agrawal et al. (2016) report how neural models exploit biases in a VQA dataset, e.g. responding \u201csnow\u201d to the question \u201cwhat covers the ground\u201d regardless of the image because \u201csnow\u201d is the most common answer to this question. Gururangan et al. (2018) report that many successes in natural language entailment are actually due to exploiting statistical biases as opposed to solving entailment, and that state-of-the-art systems are much less performant when tested on unbiased data. Jia & Liang (2017) demonstrate that seemingly state-ofthe-art reading comprehension system can be misled by simply appending an unrelated sentence that resembles the question to the document.\nUsing synthetic VQA datasets to study grounded language understanding is a recent trend started by the CLEVR dataset (Johnson et al., 2016). CLEVR images are 3D-rendered and CLEVR questions are longer and more complex than ours, but in the associated generalization split CLEVR-CoGenT the training and test distributions of images are different. In our design of SQOOP we aimed instead to minimize the difference between training and test images to make sure that we test a model\u2019s ability to interpret unknown combinations of known words. The ShapeWorld family of datasets by Kuhnle & Copestake (2017) is another synthetic VQA platform with a number of generalization tests, but none of them tests SQOOP-style generalization of relational reasoning to unseen object pairs. Most closely related to our work is the recent study of generalization to long-tail questions about rare objects done by Bingham et al. (2017). They do not, however, consider as many models as we do and do not study the question of whether the best-performing models can be made end-to-end.\nThe key paradigm that we test in our experiments is Neural Module Networks (NMN). Andreas et al. (2016) introduced NMNs as a modular, structured VQA model where a fixed number of handcrafted neural modules (such as Find, or Compare) are chosen and composed together in a layout determined by the dependency parse of the question. Andreas et al. (2016) show that the modular structure allows answering questions that are longer than the training ones, a kind of generalization that is complementary to the one we study here. Hu et al. (2017) and Johnson et al. (2017) followed up by making NMNs end-to-end, removing the non-differentiable parser. Both Hu et al. (2017) and Johnson et al. (2017) reported that several thousands of ground-truth layouts are required to pretrain the layout predictor in order for their approaches to work. In a recent work, Hu et al. (2018) attempt to soften the layout decisions, but training their models end-to-end from scratch performed substantially lower than best models on the CLEVR task. Gupta & Lewis (2018) report successful layout induction on CLEVR for a carefully engineered heterogeneous NMN that takes a scene graph as opposed to a raw image as the input."
                },
                {
                    "heading": "6 CONCLUSION AND DISCUSSION",
                    "text": "We have conducted a rigorous investigation of an important form of systematic generalization required for grounded language understanding: the ability to reason about all possible pairs of objects despite being trained on a small subset of such pairs. Our results allow one to draw two important conclusions. For one, the intuitive appeal of modularity and structure in designing neural architectures for language understanding is now supported by our results, which show how a modular model consisting of general purpose residual blocks generalizes much better than a number of baselines, including architectures such as MAC, FiLM and RelNet that were designed specifically for visual reasoning. While this may seem unsurprising, to the best of our knowledge, the literature has lacked such a clear empirical evidence in favor of modular and structured networks before this work. Importantly, we have also shown how sensitive the high performance of the modular models is to the\nlayout of modules, and how a tree-like structure generalizes much stronger than a typical chain of layers.\nOur second key conclusion is that coming up with an end-to-end and/or soft version of modular models may be not sufficient for strong generalization. In the very setting where strong generalization is required, end-to-end methods often converge to a different, less compositional solution (e.g. a chain layout or blurred attention). This can be observed especially clearly in our NMN layout and parametrization induction experiments on the #rhs/lhs=1 version of SQOOP, but notably, strong initialization sensitivity of layout induction remains an issue even on the #rhs/lhs=18 split. This conclusion is relevant in the view of recent work in the direction of making NMNs more end-toend (Suarez et al., 2018; Hu et al., 2018; Hudson & Manning, 2018; Gupta & Lewis, 2018). Our findings suggest that merely replacing hard-coded components with learnable counterparts can be insufficient, and that research on regularizers or priors that steer the learning towards more systematic solutions can be required. That said, our parametrization induction results on the #rhs/lhs=2 split are encouraging, as they show that compared to generic models, a weaker nudge (in the form of a richer training signal or a prior) towards systematicity may suffice for end-to-end NMNs.\nWhile our investigation has been performed on a synthetic dataset, we believe that it is the realworld language understanding where our findings may be most relevant. It is possible to construct a synthetic dataset that is bias-free and that can only be solved if the model has understood the entirety of the dataset\u2019s language. It is, on the contrary, much harder to collect real-world datasets that do not permit highly dataset-specific solutions, as numerous dataset analysis papers of recent years have shown (see Section 5 for a review). We believe that approaches that can generalize strongly from imperfect and biased data will likely be required, and our experiments can be seen as a simulation of such a scenario. We hope, therefore, that our findings will inform researchers working on language understanding and provide them with a useful intuition about what facilitates strong generalization and what is likely to inhibit it."
                },
                {
                    "heading": "ACKNOWLEDGEMENTS",
                    "text": "We thank Maxime Chevalier-Boisvert, Yoshua Bengio and Jacob Andreas for useful discussions. This research was enabled in part by support provided by Compute Canada (www.computecanada.ca), NSERC, Canada Research Chairs and Microsoft Research. We also thank Nvidia for donating NVIDIA DGX-1 used for this research."
                },
                {
                    "heading": "A EXPERIMENT DETAILS",
                    "text": "We trained all models by minimizing the cross entropy loss log p(y|x, q) on the training set, where y \u2208 {yes, no} is the correct answer, x is the image, q is the question. In all our experiments we used the Adam optimizer (Kingma & Ba, 2015) with hyperparameters \u03b1 = 0.0001, \u03b21 = 0.9, \u03b22 = 0.999, = 10\u221210. We continuously monitored validation set performance of all models during training, selected the best one and reported its performance on the test set. The number of training iterations for each model was selected in preliminary investigations based on our observations of how long it takes for different models to converge. This information, as well as other training details, can be found in Table 3."
                },
                {
                    "heading": "B ADDITIONAL RESULTS FOR MAC MODEL",
                    "text": "We performed an ablation study in which we varied the number of MAC units, the model dimensionality and the level of weight decay for the MAC model. The results can be found in Table 4.\nWe also perform qualitative investigations to understand the high variance in MAC\u2019s performance. In particular, we focus on control attention weights (c) for each run and aim to understand if runs that generalize have clear differences when compared to runs that failed. Interestingly, we observe that in successful runs each wordw \u2208 X,Y has a unit that is strongly focused on it. To present our observations in quantitative terms, we plot attention quality \u03ba = minw\u2208{X,Y }maxk\u2208[1;12] \u03b1k,w/(1\u2212\u03b1k,R), where \u03b1 are control scores vs accuracy in Figure 7 for each run (see Section 4.3.2 for an explanation of \u03ba). We can clearly see a positive correlation between \u03ba and error rate, especially for low #rhs/lhs.\nNext, we experiment with a hard-coded variation of MAC. In this model, we use hard-coded control scores such that given a SQOOP question XRY, the first half of all modules focuses on X while the second half focuses on Y. The relationship between MAC and hardcoded MAC is similar to that between NMN-Tree and end-to-end NMN with parameterization induction. However, this model has not performed as well as the successful runs of MAC. We hypothesize that this could be due to the interactions between the control scores and the visual attention part of the model.\nC INVESTIGATION OF CORRECT PREDICTIONS WITH SPURIOUS LAYOUTS\nIn Section 4.3.1 we observed that an NMN with the Residual module can answer test questions with a relative low error rate of 1.64 \u00b1 1.79%, despite being a mixture of a tree and a chain (see\nresults in Table 1, p0(tree) = 0.5). Our explanation for this phenomenon is as follows: when connected in a tree, modules of such spurious models generalize well, and when connected as a chain they generalize poorly. The output distribution of the whole model is thus a mixture of the mostly correct p(y|T = Ttree, x, q) and mostly random p(y|T = Tchain, x, q). We verify our reasoning by explicitly evaluating test accuracies for p(y|T = Ttree, x, q) and p(y|T = Tchain, x, q), and find them to be around 99% and 60% respectively, confirming our hypothesis. As a result the predictions of the spurious models with p(tree) \u2248 0.5 have lower confidence than those of sharp tree models, as indicated by the high log loss of 0.27\u00b1 0.04. We visualize the progress of structure induction for the Residual module with p0(tree) = 0.5 in Figure 4 which shows how p(tree) saturates to 1.0 for #rhs/lhs=18 and remains around 0.5 when #rhs/lhs=1."
                },
                {
                    "heading": "D SQOOP PSEUDOCODE",
                    "text": "Algorithm 1 Pseudocode for creating SQOOP 1: S \u2190 {A,B,C, . . . , Z, 0,1,2,3, . . . , 9} 2: Rel\u2190 {LEFT-OF, RIGHT-OF, ABOVE, BELOW} . relations 3: function CREATESQOOP(k) 4: TrainQuestions\u2190 [] 5: AllQuestions\u2190 [] 6: for all X in S do 7: AllRhs\u2190 RandomSample(S \\ {X}, k) . sample without replacement from S \\ {X} 8: AllQuestions\u2190 {X} \u00d7Rel \u00d7 (S \\ {X}) \u222aAllQuestions 9: for all R, Y in AllRhs\u00d7Rel do 10: TrainQuestions\u2190 (X,R, Y ) \u222a TrainQuestions 11: end for 12: end for 13: TestQuestions\u2190 AllQuestions \\ TrainQuestions 14: function GENERATEEXAMPLE(X,R, Y ) 15: a \u223c {Yes, No} 16: if a = Yes then 17: I \u2190 place X and Y objects so that R holds . create the image 18: I \u2190 sample 3 objects from S and add to I 19: else 20: repeat 21: X \u2032\u2190 Sample X \u2032 from S \\ {X} 22: Y \u2032 \u2190 Sample Y \u2032 from S \\ {Y } 23: I \u2190 place X \u2032 and Y objects so that R holds . create the image 24: I \u2190 add X and Y \u2032 objects to I so that R holds 25: I \u2190 sample 1 more object from S and add to I 26: until X and Y are not in relation R in I 27: end if 28: return I , X,R, Y , a 29: end function 30: Train \u2190 sample 106|TrainQuestions| examples for each (X,R,Y) \u2208 TrainQuestions from\nGENERATEEXAMPLE(X,R, Y ) 31: Test\u2190 sample 10 examples for each (X,R,Y) \u2208 TestQuestions from GENERATEEXAM-\nPLE(X,R, Y ) 32: end function"
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "Analyzing the Behavior of Visual Question Answering Models",
                    "authors": [
                        "Aishwarya Agrawal",
                        "Dhruv Batra",
                        "Devi Parikh"
                    ],
                    "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
                    "year": 2016
                },
                {
                    "title": "Neural Module Networks",
                    "authors": [
                        "Jacob Andreas",
                        "Marcus Rohrbach",
                        "Trevor Darrell",
                        "Dan Klein"
                    ],
                    "venue": "In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2016
                },
                {
                    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
                    "authors": [
                        "Dzmitry Bahdanau",
                        "Kyunghyun Cho",
                        "Yoshua Bengio"
                    ],
                    "venue": "In Proceedings of the 2015 International Conference on Learning Representations,",
                    "year": 2015
                },
                {
                    "title": "Jump to better conclusions: SCAN both left and right",
                    "authors": [
                        "Joost Bastings",
                        "Marco Baroni",
                        "Jason Weston",
                        "Kyunghyun Cho",
                        "Douwe Kiela"
                    ],
                    "venue": "In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
                    "year": 2018
                },
                {
                    "title": "Characterizing how Visual Question Answering scales with the world",
                    "authors": [
                        "Eli Bingham",
                        "Piero Molino",
                        "Paul Szerlip",
                        "Obermeyer Fritz",
                        "Goodman Noah"
                    ],
                    "venue": "In NIPS 2017 Visually-Grounded Interaction and Language Workshop,",
                    "year": 2017
                },
                {
                    "title": "The statistical brain: Reply to Marcus The algebraic mind",
                    "authors": [
                        "Francisco Calvo",
                        "Eliana Colunga"
                    ],
                    "venue": "In Proceedings of the Annual Meeting of the Cognitive Science Society,",
                    "year": 2003
                },
                {
                    "title": "Pylyshyn. Connectionism and cognitive architecture: A critical analysis",
                    "authors": [
                        "Jerry A. Fodor",
                        "Zenon W"
                    ],
                    "year": 1988
                },
                {
                    "title": "Differentiable Programs with Neural Libraries",
                    "authors": [
                        "Alexander L. Gaunt",
                        "Marc Brockschmidt",
                        "Nate Kushman",
                        "Daniel Tarlow"
                    ],
                    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
                    "year": 2016
                },
                {
                    "title": "Natural Language Inference over Interaction Space",
                    "authors": [
                        "Yichen Gong",
                        "Heng Luo",
                        "Jian Zhang"
                    ],
                    "venue": "In Proceedings of the 2018 International Conference on Learning Representations,",
                    "year": 2017
                },
                {
                    "title": "Neural Compositional Denotational Semantics for Question Answering",
                    "authors": [
                        "Nitish Gupta",
                        "Mike Lewis"
                    ],
                    "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
                    "year": 2018
                },
                {
                    "title": "Annotation Artifacts in Natural Language Inference Data",
                    "authors": [
                        "Suchin Gururangan",
                        "Swabha Swayamdipta",
                        "Omer Levy",
                        "Roy Schwartz",
                        "Samuel R. Bowman",
                        "Noah A. Smith"
                    ],
                    "venue": "In Proceedings of NAACL-HLT",
                    "year": 2018
                },
                {
                    "title": "Deep residual learning for image recognition",
                    "authors": [
                        "Kaiming He",
                        "Xiangyu Zhang",
                        "Shaoqing Ren",
                        "Jian Sun"
                    ],
                    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
                    "year": 2016
                },
                {
                    "title": "Learning to Reason: End-to-End Module Networks for Visual Question Answering",
                    "authors": [
                        "Ronghang Hu",
                        "Jacob Andreas",
                        "Marcus Rohrbach",
                        "Trevor Darrell",
                        "Kate Saenko"
                    ],
                    "venue": "In Proceedings of 2017 IEEE International Conference on Computer Vision,",
                    "year": 2017
                },
                {
                    "title": "Explainable Neural Computation via Stack Neural Module Networks",
                    "authors": [
                        "Ronghang Hu",
                        "Jacob Andreas",
                        "Trevor Darrell",
                        "Kate Saenko"
                    ],
                    "venue": "In Proceedings of 2018 European Conference on Computer Vision,",
                    "year": 2018
                },
                {
                    "title": "Compositional Attention Networks for Machine Reasoning",
                    "authors": [
                        "Drew A. Hudson",
                        "Christopher D. Manning"
                    ],
                    "venue": "In Proceedings of the 2018 International Conference on Learning Representations,",
                    "year": 2018
                },
                {
                    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
                    "authors": [
                        "Sergey Ioffe",
                        "Christian Szegedy"
                    ],
                    "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
                    "year": 2015
                },
                {
                    "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
                    "authors": [
                        "Robin Jia",
                        "Percy Liang"
                    ],
                    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
                    "year": 2017
                },
                {
                    "title": "Pythia v0.1: The winning entry to the vqa challenge 2018",
                    "authors": [
                        "Yu Jiang",
                        "Vivek Natarajan",
                        "Xinlei Chen",
                        "Marcus Rohrbach",
                        "Dhruv Batra",
                        "Devi Parikh"
                    ],
                    "venue": "https://github.com/ facebookresearch/pythia,",
                    "year": 2018
                },
                {
                    "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                    "authors": [
                        "Justin Johnson",
                        "Bharath Hariharan",
                        "Laurens van der Maaten",
                        "Li Fei-Fei",
                        "C. Lawrence Zitnick",
                        "Ross Girshick"
                    ],
                    "venue": "In Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2016
                },
                {
                    "title": "Inferring and Executing Programs for Visual Reasoning",
                    "authors": [
                        "Justin Johnson",
                        "Bharath Hariharan",
                        "Laurens van der Maaten",
                        "Judy Hoffman",
                        "Li Fei-Fei",
                        "C. Lawrence Zitnick",
                        "Ross Girshick"
                    ],
                    "venue": "In Proceedings of 2017 IEEE International Conference on Computer Vision,",
                    "year": 2017
                },
                {
                    "title": "Smart Reply: Automated Response Suggestion for Email",
                    "authors": [
                        "Anjuli Kannan",
                        "Karol Kurach",
                        "Sujith Ravi",
                        "Tobias Kaufmann",
                        "Andrew Tomkins",
                        "Balint Miklos",
                        "Greg Corrado",
                        "Laszlo Lukacs",
                        "Marina Ganea",
                        "Peter Young",
                        "Vivek Ramavajjala"
                    ],
                    "venue": "In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
                    "year": 2016
                },
                {
                    "title": "Adam: A Method for Stochastic Optimization",
                    "authors": [
                        "Diederik P. Kingma",
                        "Jimmy Ba"
                    ],
                    "venue": "In Proceedings of the 2015 International Conference on Learning Representations,",
                    "year": 2015
                },
                {
                    "title": "ShapeWorld - A new test methodology for multimodal language understanding",
                    "authors": [
                        "Alexander Kuhnle",
                        "Ann Copestake"
                    ],
                    "venue": "[cs],",
                    "year": 2017
                },
                {
                    "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
                    "authors": [
                        "Brenden M. Lake",
                        "Marco Baroni"
                    ],
                    "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
                    "year": 2018
                },
                {
                    "title": "Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks",
                    "authors": [
                        "Joao Loula",
                        "Marco Baroni",
                        "Brenden M. Lake"
                    ],
                    "venue": "In Proceedings of the 2018 BlackboxNLP EMNLP Workshop,",
                    "year": 2018
                },
                {
                    "title": "A Multi-world Approach to Question Answering About Realworld Scenes Based on Uncertain Input",
                    "authors": [
                        "Mateusz Malinowski",
                        "Mario Fritz"
                    ],
                    "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,",
                    "year": 2014
                },
                {
                    "title": "Rethinking Eliminative Connectionism",
                    "authors": [
                        "Gary F. Marcus"
                    ],
                    "venue": "Cognitive Psychology,",
                    "year": 1998
                },
                {
                    "title": "The algebraic mind: Integrating connectionism and cognitive science",
                    "authors": [
                        "Gary F. Marcus"
                    ],
                    "venue": "MIT press,",
                    "year": 2003
                },
                {
                    "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
                    "authors": [
                        "Ethan Perez",
                        "Florian Strub",
                        "Harm de Vries",
                        "Vincent Dumoulin",
                        "Aaron Courville"
                    ],
                    "venue": "Proceedings of the 2017 AAAI Conference on Artificial Intelligence,",
                    "year": 2017
                },
                {
                    "title": "A simple neural network module for relational reasoning",
                    "authors": [
                        "Adam Santoro",
                        "David Raposo",
                        "David G.T. Barrett",
                        "Mateusz Malinowski",
                        "Razvan Pascanu",
                        "Peter Battaglia",
                        "Timothy Lillicrap"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems",
                    "year": 2017
                },
                {
                    "title": "The constituent structure of connectionist mental states: A reply to Fodor and Pylyshyn",
                    "authors": [
                        "Paul Smolensky"
                    ],
                    "venue": "Southern Journal of Philosophy,",
                    "year": 1987
                },
                {
                    "title": "DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer",
                    "authors": [
                        "Joseph Suarez",
                        "Justin Johnson",
                        "Fei-Fei Li"
                    ],
                    "venue": "[cs],",
                    "year": 2018
                },
                {
                    "title": "Sequence to Sequence Learning with Neural Networks",
                    "authors": [
                        "Ilya Sutskever",
                        "Oriol Vinyals",
                        "Quoc V Le"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems",
                    "year": 2014
                },
                {
                    "title": "Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering",
                    "authors": [
                        "Wei Wang",
                        "Ming Yan",
                        "Chen Wu"
                    ],
                    "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
                    "year": 2018
                }
            ],
            "id": "SP:a91f2e1cf9298403f1f4e685ab88b9a8be7975c7",
            "authors": [
                {
                    "name": "Dzmitry Bahdanau",
                    "affiliations": []
                },
                {
                    "name": "Shikhar Murty",
                    "affiliations": []
                }
            ],
            "abstractText": "Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.",
            "title": "SYSTEMATIC GENERALIZATION: WHAT IS REQUIRED AND CAN IT BE LEARNED?"
        }
    },
    "73705154": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "Discovering efficient representations of discrete high dimensional concepts has been a key challenge in a variety of applications recently [3]. Using various types of neural networks, high-dimensional raw data can be transformed to continuous real-valued concept vectors that efficiently capture their latent relationship from data. Such succinct representations have been shown to improve the performance of various complex tasks across domains spanning from image processing [22, 17, 34], language modeling [4, 24], word embedding [25, 29], music information retrieval [30], sentiment analysis [31], and more recently multi-modal learning of images and text [18].\nEfficient representations for concepts is an important, if not essential, element in healthcare as well. Healthcare concepts contain rich latent relationships that cannot be represented by simple one-hot coding [28, Chapter 2.3.2]. For example, pneumonia and bronchitis are clearly more related than pneumonia and obesity. In one-hot coding, such relationship between different codes are not represented. Despite its limitation, many healthcare applications [7, 1] still use the simple sum over one-hot vectors to derive patient feature vectors. To overcome this limitation, it is common in healthcare applications, to rely on carefully designed feature representations [32, 16, 36]. However, this process often involves supervision information and ad-hoc feature engineering that requires considerable expert medical knowledge and is not scalable in general.\nRecently, studies have shown that it is possible to learn efficient representations of healthcare concepts without medical expertise and still significantly improve the performance of various\nar X\niv :1\n60 2.\n05 56\n8v 1\n[ cs\n.L G\n] 1\n7 Fe\nb 20\nhealthcare applications. Choi et al. [9] learned distributed representations of medical codes (e.g. diagnosis, medication, procedure codes) using Skip-gram [25] and applied them to heart failure prediction. Choi et al. [10] also learned the representations for medical concepts from a medical claims dataset and compared the learned representations to existing medical ontologies and code groupers. Despite these progress, learning efficient representations of healthcare concepts, however, is still an open challenge. The difficulty stems from several aspects:\n1. Healthcare data have a unique structure where the visits are temporally ordered but the medical codes within a visit form an unordered set. A sequence of visits possesses sequential relationship among them which cannot be captured by simply aggregating code-level representations. Moreover, given the demographic information for patients, the structure becomes more complex.\n2. Learned representations should be interpretable. While the interpretability of the model in the clinical domain is considered to be an essential requirement, some of the state-of-the art representation learning methods such as recurrent neural networks (RNN) are difficult to interpret.\n3. The algorithm should be scalable enough to handle real-world healthcare datasets with millions of patients and hundred millions of visits.\nTo address such challenges in healthcare concept representation learning, we propose Med2Vec and make the following contributions.\n\u2022 We propose Med2Vec, a simple and robust algorithm to efficiently learn succinct code-, and visit-level representations by using real-world electronic health record (EHR) datasets, without depending on expert medical knowledge.\n\u2022 Med2Vec learns interpretable representations and enables clinical applications to offer more than just improved performances. We conducted detailed user study with clinical experts to validate the interpretability of the resulting representation.\n\u2022 We conduct experiments to demonstrate the scalability of Med2Vec, and show that our model can be readily applied to near 30K medical codes over two large datasets with 3 million and 5.5 million visits, respectively.\n\u2022 We apply the learned representations to various real-world health problems and demonstrate the improved performance enabled by Med2Vec compared to popular baselines.\nIn the following section, we discuss related works, then describe our method in section 3. In section 4, we explain experiment design and interpretation method in detail. We present the results and discussion in section 5. Then we conclude this paper with future work in section 6."
                },
                {
                    "heading": "2 Preliminaries and Related Work",
                    "text": "In this section, we first describe the preliminary ideas used in learning representation for words. Then, we review the algorithms developed for representing healthcare data."
                },
                {
                    "heading": "2.1 Learning representation for words",
                    "text": "Representation learning of words using neural network based methods have been studied since the early 2000\u2019s [4, 12, 27, 33]. Among these techniques, Skip-gram [25] is the basis of many concept representation learning methods, including our own. Skip-gram is able to capture the subtle relationships between words, thus outperforming the previous works in a word analogy task[23].\nGiven a sequence of words w1, w2, . . . , wT , Skip-gram learns the word representations based on the co-occurrence information of words inside a context window of a predefined size. The key principle of Skip-gram is that a word\u2019s representation should be able to predict the neighboring words. The objective of Skip-gram is to maximize the following average log probability.\n1\nT T\u2211 t=1 \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt)\nwhere c is the size of the context window. The conditional probability is defined by the softmax function:\np(wO|wI) = exp\n( v\u2032>wOvwI ) \u2211W\nw=1 exp ( v\u2032>w vwI ) where vw and v \u2032 w are the input and output vector representations of word w. W is the number of words in the vocabulary. Basically, Skip-gram tries to maximize the softmax probability of the inner product of the center word\u2019s vector and its context word\u2019s vectors.1\nPennington et al. proposed GloVe, [29] which learns another word representations by using a similar principle as Skip-gram. GloVe uses the global word co-occurrence matrix to learn the word representations. Since the global co-occurrence matrix is often sparse, GloVe can be computationally less demanding than Skip-gram, which is a neural network model using the sliding context window. On the other hand, GloVe employs a weighting function that could require a considerable amount tuning effort.\n1Mikolov et al. [25] also use hierarchical softmax and negative sampling to speed up the learning process. We focus on the original simple formulation.\nBeyond one level representation like Skip-gram and GloVe, researchers also proposed hierarchical learning representations for the text corpus, which has some analogy to our healthcare setting with two level concepts namely: codes and visits. Le and Mikolov [20] proposes to learn representations for paragraphs and words simultaneously by treating paragraphs as one of the words. However, their algorithm assigns a fixed set of vectors for both words and paragraphs in the training data. Moreover, their approach does not capture the sequential order among paragraphs. Skip-thought [19] proposes an encoder-decoder structure: an encoder (Gated Recurrent Units (GRU) in their case) learns a representation for a sentence that is able to regenerate its surrounding sentences (via GRU again). Skip-thought cannot be applied directly to EHR data because unlike words in sentences, the codes in a visit are unordered. Also, the interpretation of Skip-thought model is difficult, as they rely on complex RNNs."
                },
                {
                    "heading": "2.2 Representation learning in healthcare",
                    "text": "Recently researchers start to explore the possibility of efficient representation learning in the medical domain.\nMedical text analysis Minarro et al. [26] learns the representations of medical terms by applying Skip-gram to various medical text collected from PubMed, Merck Manuals, Medscape and Wikipedia. De Vine et al. [13] learns the representations of UMLS concepts from free-text patient records and medical journal abstracts. They first replaced the words in documents to UMLS concepts, then applied Skip-gram to learn the distributed representations of the concepts. However, none of them studied longitudinal EHR data with a large number of medical codes.\nStructured visit records analysis Choi et al. [9], and Choi et al. [10] both learned the distributed representation of medical codes (e.g. diagnosis, medication, procedure codes) from structured longitudinal visit records of patients using Skip-gram. In addition, the authors demonstrated that simply aggregating the learned representation of medical codes to create a visit representation leads to improved predictive performance. However, simply aggregating the code representations is not the optimal method to generate a visit representation as it completely ignores the temporal relations across adjacent visits. We believe that taking advantage of the two-level information (the co-occurrence of codes within a visit and the sequential nature of visits) and the demographic information of patients will give us better representation for both medical codes and patient visits.\nChoi et al. [8] trained a recurrent neural networks (RNN) model to analyze the longitudinal patient records in a temporal order, and predict the diagnosis and medication codes the patient will receive in the future. In [8], the hidden layer of the RNN can be seen as the representation of the patient status over time. However, despite its outstanding performance, RNNs are difficult to interpret."
                },
                {
                    "heading": "3 Method",
                    "text": "In this section, we describe the proposed algorithm Med2Vec. We start by mathematically formulating the EHR data structure and our goal. Then we describe our approach in a top-down fashion. We also explain how to interpret the learned representations. We conclude this section with complexity analysis.\nEHR structure and our notation We denote the set of all medical codes c1, c2, . . . , c|C| in our EHR dataset by C with size |C|. EHR data for each patient is in the form of a sequence of visits V1, . . . , VT where each visit contains a subset of medical codes Vt \u2286 C. Without loss of generality, all algorithms will be presented for a single patient to avoid cluttered notations. The goal of Med2Vec is to learn two types of representations:\nCode representations We aim to learn an embedding function fC : C 7\u2192 Rm+ that maps every code in the set of all medical codes C to non-negative real-valued vectors of dimension m. The non-negativity constraint is introduced to improve interpretability, as discussed in details in Section 3.5.\nVisit representations Our second task is to learn another embedding function fV : V 7\u2192 Rn that maps every visit (a set of medical codes) to a real-valued vector of dimension n. The set V is the power set of the set of codes C."
                },
                {
                    "heading": "3.1 Med2Vec architecture",
                    "text": "Figure 2 depicts the architecture of Med2Vec. Given a visit Vt, we use a multi-layer perceptron (MLP) to to generate the corresponding visit representation vt. First, visit Vt is represented by a binary vector xt \u2208 {0, 1}|C| where the i-th entry is 1 only if ci \u2208 Vt. Then xt is converted to an intermediate visit representation ut \u2208 Rm as follows,\nut = ReLU(Wcxt + bc) (1)\nusing the code weight matrix Wc \u2208 Rm\u00d7|C| and the bias vector bc \u2208 Rm. The rectified linear unit is defines as ReLU(v) = max(v,0). Note that max() applies element-wise to vectors. We use\nthe rectified linear unit (ReLU) as the activation function to enable interpretability, which will be discussed in section 3.3.\nWe concatenate the demographic information dt \u2208 Rd, where d is the size of the demographic information vector, to the intermediate visit representation ut and create the final visit representation vt \u2208 Rn as follows,\nvt = ReLU(Wv[ut,dt] + bv)\nusing the visit weight matrix Wv \u2208 Rn\u00d7(m+d) and the bias vector bv \u2208 Rn, where n is the predefined size of the visit representation. We use ReLU once again as the activation function. We discuss our efficient training procedure of the parameters Wc, bc,Wv and bv in the next subsection."
                },
                {
                    "heading": "3.2 Learning from the visit-level information",
                    "text": "As mentioned in the introduction, the sequential information of visits can be exploited for learning efficient representations of visits and potentially codes. We train the MLP using a very straightforward intuition as follows: a visit describes a state in a continuous process that is a patient\u2019s clinical experience. Therefore, given a visit representation, we should be able to predict what has happened in the past, and what will happen in the future. Specifically, given a visit representation vt, we train a softmax classifier that predicts the medical codes of the visits within a context window. We minimize the cross entropy error as follows,\nmin Ws,bs\n1 T T\u2211 t=1 \u2211 \u2212w\u2264i\u2264w,i6=0 \u2212xt+i> log y\u0302t \u2212 (1\u2212 xt+i)> log(1\u2212 y\u0302t), (2)\nwhere y\u0302t = exp(Wsvt + bs)\u2211|C|\nj=1 exp(Ws[j, :]vt + bs[j])\nwhere Ws \u2208 R|C|\u00d7n and bs \u2208 R|C| are the weight matrix and bias vector for the softmax classifier, w the predefined context window size, exp the element-wise exponential function, and 1 denotes an all one vector. We have used MATLAB\u2019s notation for selecting a row in Ws and a coordinate of bs."
                },
                {
                    "heading": "3.3 Learning from the code-level information",
                    "text": "As we described in the introduction, healthcare datasets contain two-level information: visit-level sequence information and code-level co-occurrence information. Since the loss function in Eq. (2) can efficiently capture the sequence level information, now we need to find a way to use the second source of information, i.e., the intra-visit co-occurrence of the codes.\nA natural choice to capture the code co-occurrence information is to use Skip-gram. The main idea would be that the representations for the codes that occur in the same visit should predict each other. To embed Skip-gram in Med2Vec, we can train Wc \u2208 Rm\u00d7|C| (which also produces intermediate visit level representations) so that the i-th column of Wc will be the representation for the i-th medical code among total |C| codes. Note that given the unordered nature of the codes inside a visit, unlike the original Skip-gram, we do not distinguish between the \u201cinput\u201d medical code and the \u201coutput\u201d medical code. In text, it is sensible to assume that a word can serve a different role as a center word and a context word, whereas in EHR datasets, we cannot classify codes as center or context codes. It is also desirable to learn the representations of different types of\ncodes (e.g. diagnosis, medication, procedure code) in the same latent space so that we can capture the hidden relationships between them.\nHowever, precise interpretation of Skip-gram codes will be difficult as Wc will have positive and negative values. For intuitive interpretation, we should learn code representations with non-negative values. Note that in Eq.(1), if the binary vector xt is a one-hot vector, then the intermediate visit representation ut becomes a code representation. Therefore, using the Skip-gram algorithm, we train the non-negative weight ReLU(Wc) instead of Wc. This will not only use the intra-visit co-occurrence information, but also guarantee non-negative code representations. Moreover, ReLU produces sparse code representations, which further facilitates easier interpretation of the codes.\nThe code representations to be learned is denoted as a matrix W \u2032c = ReLU(Wc) \u2208 Rm\u00d7|C|. From a sequence of visits V1, V2, . . . , VT , the code-level representations can be learned by maximizing the following log-likelihood,\nmin W \u2032c\n1\nT T\u2211 t=1 \u2211 i:ci\u2208Vt \u2211 j:cj\u2208Vt,j 6=i log p(cj |ci), (3)\nwhere p(cj |ci) = exp\n( W \u2032c[:, j] >W \u2032c[:, i] )\n\u2211|C| k=1 exp ( W \u2032c[:, k] >W \u2032c[:, i] ) . (4)"
                },
                {
                    "heading": "3.4 Unified training",
                    "text": "The single unified framework can be obtained by adding the two objective functions (3) and (2) as follows,\nargmin W ,b\n1\nT T\u2211 t=1 { \u2212 \u2211\ni:ci\u2208Vt \u2211 j:cj\u2208Vt,j 6=i log p(cj |ci)\n+ \u2211\n\u2212w\u2264k\u2264w,k 6=0 \u2212x>t+k log y\u0302t \u2212 (1\u2212 xt+k)> log(1\u2212 y\u0302t) } By combining the two objective functions we learn both code representations and visit representations from the same source of patient visit records, exploiting both intra-visit co-occurrence information as well as inter-visit sequential information at the same time."
                },
                {
                    "heading": "3.5 Interpretation of learned representations",
                    "text": "While the original Skip-gram learns code representations that have interesting properties such as additivity, in healthcare we need stronger interpretability. We need to be able to associate clinical meaning to each dimension of both code and visit representations. Interpreting the learned representations is based on analyzing each coordinate in both code and visit embedding spaces.\nInterpreting code representations If information is properly embedded into a lower dimensional non-negative space, each coordinate of the lower dimension can be readily interpreted. Nonnegative matrix factorization (NMF) is a good example. Since we trained ReLU(Wc) \u2208 Rm\u00d7|C|, a non-negative matrix, to represent the medical codes, we can employ a simple method to interpret\nthe meaning of each coordinate of the m-dimensional code embedding space. We can find the top k codes that have the largest values for the i-th coordinate of the code embedding space as follows,\nargsort(Wc[i, :])[1 : k]\nwhere argsort returns the indices of a vector that index its values in a descending order. By studying the returned medical codes, we can view each coordinate as a disease group. Detailed examples are given in section 5.1\nInterpreting visit representations To interpret the learned visit vectors, we can use the same principle we used for interpreting the code representation. For the i-th coordinate of the n-dimensional visit embedding space, we can find the top k coordinates of the code embedding space that have the strongest values as follows,\nargsort(Wv[i, :])[1 : k]\nwhere we use the same argsort as before. Once we obtain a set of code coordinates, we can use the knowledge learned from interpreting the code representations to understand how each visit coordinate is associated with a group of diseases. This simple interpretation is possible because the intermediate visit representation ut is a non-negative vector, due to the ReLU activation function.\nIn the experiments, we also tried to find the input vector xt that most activates the target visit coordinate [14, 21]. However, the results were very sensitive to the initial value of xt, and even averaging over multiple samples were producing unreliable results."
                },
                {
                    "heading": "3.6 Complexity analysis",
                    "text": "We first analyze the computational complexity of the code-level objective function Eq. (3). Without loss of generality, we assume the visit records of all patients are concatenated into a single sequence of visits. Then the complexity for Eq. (3) is as follows,\nO(TM2|C|m)\nwhere T is the number of visits, M2 is the average of squared number of medical codes within a visit, |C| the number of unique medical codes, m the size of the code representation. The M2 factor comes from iterating over all possible pairs of codes within a visit. The complexity of the visit-level objective function Eq.(2) is as follows,\nO(Tw(|C|(m+ n) +mn))\nwhere w is the size of the context window, n the size of the visit representation. The added terms come from generating a visit representation via MLP. Since size of code representation m and size of visit representation n generally have the same order of magnitude, we can replace n with m. Furthermore, m is generally smaller than |C| by at least two orders of magnitude. Therefore the overall complexity of Med2Vec can be simplified as follows.\nO(T |C|m(M2 + w))\nHere we notice that M2 is generally larger than w. In our work, the average number of codes M per visit for two datasets are 7.88 and 3.19 according to Tables 1, respectively, whereas we select\nthe window size w to be at most 5 in our experiments. Therefore the complexity of Med2Vec is dominated by the code representation learning process, for which we use the Skip-gram algorithm. This means that exploiting visit-level information to learn efficient representations for both visits and codes does not incur much additional cost."
                },
                {
                    "heading": "4 Experiments",
                    "text": "In this section, we evaluate the performance of Med2Vec in both public and proprietary datasets. First we describe the datasets. Then we describe evaluation strategies for code and visit representations, along with implementation details. Then we present the experiment results of code and visit representations with discussion. We conclude with convergence and scalability study. We make the source code of Med2Vec publicly available at https://github.com/mp2893/med2vec."
                },
                {
                    "heading": "4.1 Dataset description",
                    "text": "We evaluate performance of Med2Vec on a dataset provided by Children\u2019s Healthcare of Atlanta (CHOA)2. We extract visit records from the dataset, where each visit contains several medical codes (e.g. diagnosis, medication, procedure codes). The diagnosis codes follow ICD-9 codes, the medication codes are denoted by National Drug Codes (NDC), and the procedure codes follow Category I of Current Procedural Terminology (CPT). We exclude patients who had less that two visits to showcase Med2Vec\u2019s ability to use sequential information of visits. The basic statistics of the dataset are summarized in Table 1. The data are fully de-identified and do not include any personal health information (PHI).\nWe divide the dataset into two groups in a 4:1 ratio. The former is used to train Med2Vec. The latter is held off for evaluating the visit-level representations, where we train models to predict visit-related labels. The details of the evaluation will be provided in the following subsections.\nWe also use CMS dataset, a publicly available3 synthetic EHR dataset. The basic information of CMS is also given in Table 1. Compared to CHOA dataset, the CMS dataset has more patients\n2http://www.choa.org/ 3https://www.cms.gov/Medicare/Quality-Initiatives-Patient-Assessment-Instruments/OASIS/DataSet.\nhtml\nbut fewer unique medical codes. The average number of codes per visit is also smaller than that of CHOA dataset. Since CMS dataset is synthetic, we use it only for testing the scalability of Med2Vec and baseline models in section 4.7."
                },
                {
                    "heading": "4.2 Evaluation Strategy of code representations",
                    "text": "Qualitative evaluation by medical experts For a comprehensive qualitative evaluation, we perform a relatedness test by selecting 100 most frequent diagnosis codes and their 5 closest diagnoses, medications and procedures in terms of cosine similarity. This allow us to know if the learned representations effectively capture the latent relationships among them. Two medical experts from CHOA check each item and assign related, possible and unrelated labels.\nQuantitative evaluation with baselines We use medical code groupers to quantitatively evaluate the code representations. Code groupers are used to collapse individual medical codes into clinically meaningful categories. For example, Clinical Classifications Software (CCS) groups ICD9 diagnosis codes into 283 categories such as tuberculosis, bacterial infection, and viral infection.\nWe apply K-means clustering to the learned code representations and calculate the normalized mutual information (NMI) based on the group label of each code. We use the CCS as the ground truth for evaluating the code representation for diagnosis. For medication code evaluation, we use American Hospital Formulary Service (AHFS) pharmacologic-therapeutic classification, which groups NDC codes into 165 categories. For procedure code evaluation, we use the second-level grouping of CPT category I, which groups CPT codes into 115 categories.Thus, we set the number of clusters k to 283, 165, 115 respectively for the diagnosis, medication, procedure code evaluation, which matches the numbers of groups from individual groupers.\nFor baselines, we use popular methods that efficiently exploit co-occurrence information. Skipgram (which is used in learning representations of medical concepts by [10, 9]) is trained using Eq. (3). GloVe will be trained on the co-occurrence matrix of medical codes, for which we counted the codes co-occurring within a visit. Additionally, we also report well-known baselines such as singular value decomposition on the co-occurrence matrix."
                },
                {
                    "heading": "4.3 Evaluation strategy of visit representation",
                    "text": "We evaluate the quality of the visit representations by performing two visit-level prediction tasks: predicting the future visit and predicting the present status. The former will evaluate a visit representation\u2019s potential effectiveness in predictive healthcare while the latter will evaluate the how well it captures the information in the given visit. The details of the two tasks are given below. Predicting future medical codes: We predict the medical codes that will occur in the next visit using the visit representations. Specifically, given two consecutive visits Vi and Vj , the medical codes c \u2208 Vj will be the target y, the medical codes c \u2208 Vi will be the input x, and we use softmax to predict y given x. The predictive performance will be measured by Top-k Recall, which mimics the differential diagnosis conducted by doctors. We set k = 30 to cover even the complex cases of CHOA dataset, as over 167,000 visits are assigned with more than 20 medical codes according to Table 1. We predict the grouped medical codes, obtained by the medical groupers used in Section 4.2.\nPredicting Clinical Risk Groups (CRG) level: A patient\u2019s CRG level indicates his severity level. It ranges from 1 to 9, including 5a and 5b. The CRG levels can be divided into two groups: non-severe (CRG 1-5a) and severe (CRG 5b-9). Given a visit, we use logistic regression to predict the binary CRG class associated with the visit. We use Area Under The Curve (AUC) to measure the classification accuracy, as it is more robust to class imbalance in data.\nBaselines For baselines, we use the following methods. Binary vector model (One-hot+): In order to compare with the raw input data, we use the binary vector xt as the visit representation. Stacked autoencoder (SA): Stacked autoencoder is one of the most popular unsupervised representation learning algorithms [35]. Using the binary vector xt concatenated with patient demographic information as the input, we train a 3-layer stacked autoencoder (SA) [5] to minimize the reconstruction error. The trained SA will then be used to generate visit representations. Sum of Skip-gram vectors (Skip-gram+): We first learn the code-level representations with Skip-gram only (Eq. (3)). Then for the visit-level representation, we simply add the representations of the codes within the visit. This approach was proven very effective for heart failure prediction in [9]. We append patient demographic information at the end. Sum of GloVe vectors (GloVe+): We perform the same process as Skip-gram+, but use GloVe vectors instead of Skip-gram vectors. We use the recommended hyperparameter setting from [29].\nEvaluation details We use the held-off dataset, which was not used to learn the code and visit representations, to perform the two prediction tasks. The held-off dataset contains 672,110 visits assigned with CRG levels. In order to train the predictors, we divide the held-off data to training and testing folds with ration 4:1. Both softmax and logistic regression are trained for 10 epochs on the training fold. We perform 5-fold cross validation for each task to tune the regularization parameter. For all baseline models and Med2Vec, we use age, sex and ethnicity as the demographic information in the input data."
                },
                {
                    "heading": "4.4 Implementation and training details",
                    "text": "For learning code and visit representations using Med2Vec and all baselines, we use Adadelta [37] in a mini-batch fashion. For Skip-gram, SA and Med2Vec, we use 1,000 visits4 per batch. For GloVe, we use 1,000 non-zero entries of the co-occurrence matrix per batch. The optimization terminates after a fixed number of epochs. In section 4.6, we show the relationship between training epochs and the performance. We also show the convergence behavior of Med2Vec and the baselines in section 4.7.\nMed2Vec, Skip-gram, GloVe and SA are implemented with Theano 0.7.0 [6]. K-means clustering for the code-level evaluation and SVD are performed using Scikit-learn 0.14.1. Softmax and logistic regression models for the visit-level evaluation are implemented with Keras 0.3.1, and trained for 10 epochs. All tasks are executed on a machine equipped with Intel Xeon E5-2697v3, 256GB memory and two Nvidia K80 Tesla cards.\nWe train multiple models using various hyperparameter settings. For all models we vary the size of the code representations m (or the size of the hidden layer for SA), and the number of\n4for efficient computation, we preprocessed the EHR dataset so that the visit records of all patients are concatenated into a single sequence of visits.\nTable 2: Average score of the medical codes from the relatedness test. 2 was assigned for related,\n1 for possible and 0 for unrelated\n10 epochs.\nModel Diagnosis Medication Procedure SVD (\u03c3V >) 0.1824 0.0843 0.1781 Skip-gram 0.2251 0.1216 0.2432 GloVe 0.4205 0.2163 0.3499 Med2Vec 0.2328 0.1089 0.21\ntraining epochs. Additionally for Med2Vec, we vary the size of the visit representations n, and the size of the visit context window w.\nTo alleviate the curse of dimensionality when training the softmax classifier (Eq.(2)) of Med2Vec, we always use the medical code groupers of section 4.2 so that the softmax classifier is trained to predict the grouped medical codes instead of the exact medical codes. To confirm the impact of this strategy, we train an additional Med2Vec without using the medical code groupers."
                },
                {
                    "heading": "4.5 Results of the code-level evaluation",
                    "text": "Table 2 shows the average score of the medical codes from the qualitative code evaluation. On average, Med2Vec successfully captures the relationship between medical codes. However, Med2Vec seems to have a hard time capturing proper representation of medications. This is due to the precise nature the medication prescription. For example, Med2Vec calculated that Ofloxacin, an antibiotic sometimes used to treat middle-ear infection, was related to sensorineural hearling loss (SNHL), an inner-ear problem. On the surface level, this is a wrong relationship. But Med2Vec can be seen as capturing the deeper relationship between medical concepts that is not always clear on the surface level.\nTable 3 shows the clustering NMI of diagnosis, medication and procedure codes, measured for various models. Med2Vec shows more or less similar conformity to the existing groupers as Skipgram. SVD shows the weakest conformity among all models. GloVe exhibits significantly stronger conformity than any other models. Exploiting the global co-occurrence matrix seems to help learn code representations where similar codes are closer to each other in terms of Euclidean distance.\nHowever, the degree of conformity of the code representations to the groupers does not necessarily indicate how well the code representations capture the hidden relationships. For example, CCS categorizes ICD9 224.4 Benign neoplasm of cornea as CCS 47 Other and unspecified benign neoplasm, and ICD9 370.00 Unspecified corneal ulcer as CCS 91 Other eye disorders. But the two diagnosis codes are both eye related problems, and they could be considered related in that sense. Therefore we recommend the readers use the evaluation results for comparing the performance between Med2Vec and other baselines, rather than for measuring the absolute performance.\nIn the following visit-level evaluation, we show that the code representations\u2019 strong conformity\nto the existing groupers alone does not directly transfer to good visit representations."
                },
                {
                    "heading": "4.6 Results of the visit-level evaluation",
                    "text": "The first row of Figure 3 shows the Recall@30 for predicting the future medical codes. First, in all of the experiments, Med2Vec achieves the highest performance, despite the fact that it is constrained to be positive and interpretable. The second observation is that Med2Vec\u2019s performance is robust to choice of the hyperparameters in a wide range of values. Comparing to a more volatile performance of Skip-gram, we can see that including the visit information in training not only improves the performance, but also stabilizes it too.\nAnother fascinating aspect of the results is the overfitting pattern in different algorithms. Increasing the code representation size degrades the performance of all of the algorithms, as it leads to overfitting. Similar behavior can be seen as we train GloVe+ for more epochs which suggests early stopping technique should be used in representation learning [2]. For Med2Vec, increasing the visit representation size n seems to have the strongest influence to its predictive performance.\nThe bottom row of figures in Figure 3 shows the AUC for predicting the CRG class of the given visit. The overfitting patterns are not as prominent as the previous task. This is due to the different nature of the two prediction tasks. While the goal of CRG prediction is to predict a value related to the current visit, predicting the future codes is taking a step away from the current visit. This different nature of the two tasks also contributes to the better performance of One-hot+ on the CRG prediction. One-hot+ contains the entire information of the given visit, although in a very high-dimensional space. Therefore predicting the CRG level, which has a tight relationship with the medical codes within a visit, is an easier task for One-hot+ than predicting the future codes.\nTable 4 shows the performance comparison between two different Med2Vec models. The top model is trained with the grouped codes as explained in section 4.4, while the bottom models is trained with the exact codes. Considering the marginal difference of the CRG prediction AUC, it is evident that our strategy to alleviate the curse of dimensionality was beneficial. Moreover, using the grouped codes will improve the training speed as the softmax function will require less computation."
                },
                {
                    "heading": "4.7 Convergence behavior and scalability",
                    "text": "We first compare the convergence behavior of Med2Vec with Skip-gram (Eq. (3)), GloVe and SA. For SA, we measure the convergence behavior of a single-layer. We train the models for 50 epochs and plot the normalized difference of the loss value Lt\u2212Lt\u22121Lt , where Lt denotes the loss value at time t. To study the scalability of the models, we use both CHOA dataset and CMS dataset. We vary the size of the training data and plot the time taken for each model to run one epoch.\nThe left figure of Fig 4 shows the convergence behavior of all models when trained on the CHOA dataset. SA shows the most stable convergence behavior, which is natural given that we used a single-layer SA, a much less complex model compared to GloVe, Skip-gram and Med2Vec. All models except SA seem to reach convergence after 10 epochs of training. Note that Med2Vec shows similar, if not better convergence behavior compared to Skip-gram even with added complexity.\nThe center figure of Fig 4 shows the minutes taken to train all models for one epoch using the CHOA dataset. As we have analzyed in section ssec:complexity, Med2Vec takes essentially the same time to train for one epoch. Both Skip-gram and Med2Vec, however, takes longer than SA and GloVe. This is mainly due to having the softmax function for training the code representations. GloVe, which is trained on the very sparse co-occurrence matrix naturally takes the least time to train.\nThe right figure of Fig 4 shows the training time when using the CMS dataset. Note that Med2Vec and Skip-gram takes similar time to train as SA. This is due to the smaller number of codes per visit, which is the computationally dominating factor of both Med2Vec and Skip-gram. GloVe takes less time as the number of unique codes are smaller in the CMS dataset. SA, on the other hand, takes more time because the number of visits have doubled while the the number of unique codes is about 73% of that of the CHOA dataset."
                },
                {
                    "heading": "5 Interpretation",
                    "text": "Given the importance of interpretability in healthcare, we demonstrate three stages of interpretability for our model in collaboration with the medical experts from CHOA. First, to analyze the learned code representations we show top five medical codes for each of six coordinates of the code embedding space and explain the characteristic of each coordinate. This way, we show how we can annotate each dimension of the code embedding space with clinical concepts. The six coordinates are specifically chosen so that they can be used in the later stages. Second, we demonstrate the interpretability of Med2Vec\u2019s visit representations by analyzing the meaning of two coordinates in the visit embedding space.\nFinally, we extend the interpretability of Med2Vec to a real-world task, the CRG prediction, and analyze the medical codes that have strong influence on the CRG level. Once we learn the logistic regression weight wLR for the CRG prediction, we can extract knowledge from the learned weights by analyzing the visit coordinates to which the weights are strongly connected.\nInstead of analyzing the visit coordinates, however, we propose an approximate way of directly finding out which code coordinate plays an important role in predicting the CRG class. Our goal is to find ut such that maximizes the output activation as follows 5\nu?t = argmax ut,\u2016ut\u20162=1,ut 0 [ReLU(Wvut + bv)] >wLR (5)\nGiven the fact that ReLU(\u00b7) is an increasing function (not-strictly though), we make an approximation and find the solution without the ReLU(\u00b7) term. The approximate solution can be found in closed form u?t \u221d (W>v wLR)+. Finally, we calculate the element-wise product of u?t and max(Wc + bc). This is to take into account the fact that each code coordinate has different\n5As we are interested in influential codes, we assume the demographic information vector is zero vector and omit it for ease of notation.\nmaximum value. Therefore, instead of simply selecting the code coordinate with the strongest connection to the CRG level, we consider each coordinate\u2019s maximum ability to activate the positive CRG prediction.\nThe resulting vector will show the maximum influence each code coordinate can have on the CRG prediction."
                },
                {
                    "heading": "5.1 Results",
                    "text": "Table 5 shows top ten codes with the largest value in each of the six coordinates of the code embedding space. The coordinate 112 is clearly related to sickle-cell disease and organ transplant. The two are closely related in that sickle cell disease can be treated with bone-marrow transplant. Prograf is a medication used for preventing organ rejection. Coordinate 152 groups medical codes related to sports-related injuries, specifically broken bones. Coordinate 141 is related to brain injuries and hearing loss due to the brain injuries. Neurofibromatosis(NF) is also related to this coordinate because it can cause tumors along the nerves in the brain. Cystic fibrosis(CF) seems to be a weak link in this group as it is only related to NF in the sense that both NF and CF are genetically inherited. Coordinate 184 clearly represents medical codes related to epilepsy. Epilepsy is often accompanied by convulsions, which can cause joint pain. Cerebral artery occlusion is related epilepsy in the sense that epileptic seizures can be a manifestation of cerebral arterial occlusive diseases[11]. Also, both blood in feces and the joint pain can be attributed to Henoch\u2013Scho\u0308nlein\npurpura, a disease primarily found in children. Coordinate 190 groups diseases that are caused by congenital chromosome anomalies, especially the autosome. Acquired hypothyroidism seems to be an outlier of this coordinate. Coordinate 199 is strongly related to congenital paralysis. Baclofen is a medication used as a muscle relaxer. Quadraplegia patients can have weakened respiratory function due to impaired abdominal muscles[15], in which case tracheostomy could be required.\nWe now analyze two visit coordinates: coordinate 50 and 41. Both visit coordinates have the strongest connection to the logistic regression learned for the CRG prediction. For visit coordinate 50, the two strongest code coordinates connected to it are code coordinates 112 and 152. Then naturally, from our analysis above, we can easily see that visit coordinate 50 is strongly activated by sickle-cell disease and sports-related injuries. For visit coordinate 41, code coordinates 141 and 184 have the strongest connection. Again from the analysis above, we can directly infer that visit coordinate 41 can be seen as a patient group consisting of brain damage & hearing loss patients and epilepsy patients. By repeating this process, we can find the code coordinates that are likely to strongly influence the CRG level.\nHowever, finding the influential code coordinates for CRG level can be achieved without analyzing the visit representation if we use Eq.(5). Applying Eq.(5) to the logistic regression weight of the CRG prediction, we learned that code coordinates 190 and 199 are the two strongest influencer of the CRG level. Using the analysis from above, we can naturally conclude that patients suffering from congenital chromosome anomalies or congenital paralysis are most likely to be considered to be in severe states, which is obviously true in any clinical setting."
                },
                {
                    "heading": "6 Conclusion",
                    "text": "In this paper, we proposed Med2Vec, a scalable two layer neural network for learning lower dimensional representations for medical concepts. Med2Vec incorporates both code co-occurence information and visit sequence information of the EHR data which improves the accuracy of both code and visit representations. Throughout several experiments, we successfully demonstrated the superior performance of Med2Vec in two predictive tasks and provided clinical interpretation of the learned representations."
                },
                {
                    "heading": "7 Acknowledgments",
                    "text": "This work was supported by the National Science Foundation, award IIS- #1418511 and CCF#1533768, Children\u2019s Healthcare of Atlanta, CDC I-SMILE project, Google Faculty Award, AWS Research Award, Microsoft Azure Research Award and UCB."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Learning deep architectures for ai",
                    "authors": [
                        "Y. Bengio"
                    ],
                    "venue": "Foundations and Trends R \u00a9 in Machine Learning",
                    "year": 2009
                },
                {
                    "title": "Representation learning: A review and new perspectives",
                    "authors": [
                        "Y. Bengio",
                        "A. Courville",
                        "P. Vincent"
                    ],
                    "venue": "PAMI",
                    "year": 2013
                },
                {
                    "title": "A neural probabilistic language model",
                    "authors": [
                        "Y. Bengio",
                        "R. Ducharme",
                        "P. Vincent",
                        "C. Janvin"
                    ],
                    "venue": "JMLR",
                    "year": 2003
                },
                {
                    "title": "et al",
                    "authors": [
                        "Y. Bengio",
                        "P. Lamblin",
                        "D. Popovici",
                        "H. Larochelle"
                    ],
                    "venue": "Greedy layer-wise training of deep networks. NIPS",
                    "year": 2007
                },
                {
                    "title": "Theano: a cpu and gpu math expression compiler",
                    "authors": [
                        "J. Bergstra",
                        "O. Breuleux",
                        "F. Bastien",
                        "P. Lamblin",
                        "R. Pascanu",
                        "G. Desjardins",
                        "J. Turian",
                        "D. Warde-Farley",
                        "Y. Bengio"
                    ],
                    "venue": "Proceedings of SciPy",
                    "year": 2010
                },
                {
                    "title": "Cloud-based predictive modeling system and its application to asthma readmission prediction",
                    "authors": [
                        "R. Chen",
                        "H. Su",
                        "Y. Zhen",
                        "M. Khalilia",
                        "D. Hirsch",
                        "M. Thompson",
                        "T. Davis",
                        "Y. Peng",
                        "S. Lin",
                        "J. Tejedor-Sojo",
                        "E. Searles",
                        "J. Sun"
                    ],
                    "venue": "AMIA. AMIA",
                    "year": 2015
                },
                {
                    "title": "Doctor ai: Predicting clinical events via recurrent neural networks",
                    "authors": [
                        "E. Choi",
                        "M.T. Bahadori",
                        "J. Sun"
                    ],
                    "venue": "arXiv preprint arXiv:1511.05942",
                    "year": 2015
                },
                {
                    "title": "Medical concept representation learning from electronic health records and its application on heart failure prediction",
                    "authors": [
                        "E. Choi",
                        "A. Schuetz",
                        "W.F. Stewart",
                        "J. Sun"
                    ],
                    "venue": "arXiv preprint arXiv:1602.03686",
                    "year": 2016
                },
                {
                    "title": "Learning low-dimensional representations of medical concepts. 2016",
                    "authors": [
                        "Y. Choi",
                        "C.Y.-I. Chiu",
                        "D. Sontag"
                    ],
                    "year": 2016
                },
                {
                    "title": "Epileptic seizures in cerebral arterial occlusive disease",
                    "authors": [
                        "L. Cocito",
                        "E. Favale",
                        "L. Reni"
                    ],
                    "venue": "Stroke",
                    "year": 1982
                },
                {
                    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
                    "authors": [
                        "R. Collobert",
                        "J. Weston"
                    ],
                    "venue": "ICML",
                    "year": 2008
                },
                {
                    "title": "Medical semantic similarity with a neural language model",
                    "authors": [
                        "L. De Vine",
                        "G. Zuccon",
                        "B. Koopman",
                        "L. Sitbon",
                        "P. Bruza"
                    ],
                    "venue": "KDD",
                    "year": 2014
                },
                {
                    "title": "Visualizing higher-layer features of a deep network",
                    "authors": [
                        "D. Erhan",
                        "Y. Bengio",
                        "A. Courville",
                        "P. Vincent"
                    ],
                    "venue": "University of Montreal",
                    "year": 2009
                },
                {
                    "title": "Lung volumes and mechanics of breathing in tetraplegics",
                    "authors": [
                        "J. Forner"
                    ],
                    "venue": "Spinal Cord, 18(4):258\u2013 266",
                    "year": 1980
                },
                {
                    "title": "Unfolding physiological state: Mortality modelling in intensive care units",
                    "authors": [
                        "M. Ghassemi",
                        "T. Naumann",
                        "F. Doshi-Velez",
                        "N. Brimmer",
                        "R. Joshi",
                        "A. Rumshisky",
                        "P. Szolovits"
                    ],
                    "venue": "KDD",
                    "year": 2014
                },
                {
                    "title": "A fast learning algorithm for deep belief nets",
                    "authors": [
                        "G.E. Hinton",
                        "S. Osindero",
                        "Y.-W. Teh"
                    ],
                    "venue": "Neural computation",
                    "year": 2006
                },
                {
                    "title": "A multiplicative model for learning distributed text-based attribute representations",
                    "authors": [
                        "R. Kiros",
                        "R. Zemel",
                        "R.R. Salakhutdinov"
                    ],
                    "venue": "NIPS",
                    "year": 2014
                },
                {
                    "title": "Skip-thought vectors",
                    "authors": [
                        "R. Kiros",
                        "Y. Zhu",
                        "R. Salakhutdinov",
                        "R.S. Zemel",
                        "A. Torralba",
                        "R. Urtasun",
                        "S. Fidler"
                    ],
                    "venue": "NIPS",
                    "year": 2015
                },
                {
                    "title": "Distributed representations of sentences and documents",
                    "authors": [
                        "Q. Le",
                        "T. Mikolov"
                    ],
                    "venue": "ICML",
                    "year": 2014
                },
                {
                    "title": "Building high-level features using large scale unsupervised learning",
                    "authors": [
                        "Q.V. Le"
                    ],
                    "venue": "ICASSP",
                    "year": 2013
                },
                {
                    "title": "Gradient-based learning applied to document recognition",
                    "authors": [
                        "Y. LeCun",
                        "L. Bottou",
                        "Y. Bengio",
                        "P. Haffner"
                    ],
                    "venue": "Proceedings of the IEEE",
                    "year": 1998
                },
                {
                    "title": "Efficient estimation of word representations in vector space",
                    "authors": [
                        "T. Mikolov",
                        "K. Chen",
                        "G. Corrado",
                        "J. Dean"
                    ],
                    "venue": "arXiv preprint arXiv:1301.3781",
                    "year": 2013
                },
                {
                    "title": "Recurrent neural network based language model",
                    "authors": [
                        "T. Mikolov",
                        "M. Karafi\u00e1t",
                        "L. Burget",
                        "J. Cernock\u1ef3",
                        "S. Khudanpur"
                    ],
                    "venue": "INTERSPEECH",
                    "year": 2010
                },
                {
                    "title": "Distributed representations of words and phrases and their compositionality",
                    "authors": [
                        "T. Mikolov",
                        "I. Sutskever",
                        "K. Chen",
                        "G.S. Corrado",
                        "J. Dean"
                    ],
                    "venue": "NIPS",
                    "year": 2013
                },
                {
                    "title": "Exploring the application of deep learning techniques on medical text corpora",
                    "authors": [
                        "J.A. Minarro-Gim\u00e9nez",
                        "O. Ma\u0155\u0131n-Alonso",
                        "M. Samwald"
                    ],
                    "venue": "Studies in health technology and informatics",
                    "year": 2013
                },
                {
                    "title": "A scalable hierarchical distributed language model",
                    "authors": [
                        "A. Mnih",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS",
                    "year": 2009
                },
                {
                    "title": "Machine learning: a probabilistic perspective",
                    "authors": [
                        "K.P. Murphy"
                    ],
                    "venue": "MIT press",
                    "year": 2012
                },
                {
                    "title": "Glove: Global vectors for word representation",
                    "authors": [
                        "J. Pennington",
                        "R. Socher",
                        "C.D. Manning"
                    ],
                    "venue": "EMNLP",
                    "year": 2014
                },
                {
                    "title": "Improved musical onset detection with convolutional neural networks",
                    "authors": [
                        "J. Schluter",
                        "S. Bock"
                    ],
                    "venue": "ICASSP",
                    "year": 2014
                },
                {
                    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                    "authors": [
                        "R. Socher",
                        "A. Perelygin",
                        "J.Y. Wu",
                        "J. Chuang",
                        "C.D. Manning",
                        "A.Y. Ng",
                        "C. Potts"
                    ],
                    "venue": "EMNLP",
                    "year": 2013
                },
                {
                    "title": "Supervised patient similarity measure of heterogeneous patient records",
                    "authors": [
                        "J. Sun",
                        "F. Wang",
                        "J. Hu",
                        "S. Edabollahi"
                    ],
                    "venue": "KDD Explorations",
                    "year": 2012
                },
                {
                    "title": "Word representations: a simple and general method for semi-supervised learning",
                    "authors": [
                        "J. Turian",
                        "L. Ratinov",
                        "Y. Bengio"
                    ],
                    "venue": "ACL",
                    "year": 2010
                },
                {
                    "title": "Extracting and composing robust features with denoising autoencoders",
                    "authors": [
                        "P. Vincent",
                        "H. Larochelle",
                        "Y. Bengio",
                        "P.-A. Manzagol"
                    ],
                    "venue": "ICML",
                    "year": 2008
                },
                {
                    "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
                    "authors": [
                        "P. Vincent",
                        "H. Larochelle",
                        "I. Lajoie",
                        "Y. Bengio",
                        "P.-A. Manzagol"
                    ],
                    "venue": "JMLR",
                    "year": 2010
                },
                {
                    "title": "C",
                    "authors": [
                        "Y. Wang",
                        "K. Ng",
                        "R.J. Byrd",
                        "J. Hu",
                        "S. Ebadollahi",
                        "Z. Daar"
                    ],
                    "venue": "deFilippi, S. R. Steinhubl, and W. F. Stewart. Early detection of heart failure with varying prediction windows by structured and unstructured data in electronic health records. In EMBC",
                    "year": 2015
                },
                {
                    "title": "Adadelta: An adaptive learning rate method",
                    "authors": [
                        "M.D. Zeiler"
                    ],
                    "venue": "arXiv preprint arXiv:1212.5701",
                    "year": 2012
                }
            ],
            "id": "SP:efa023cd875b030926b55530cb941e1806eb968b",
            "authors": [
                {
                    "name": "Edward Choi",
                    "affiliations": []
                },
                {
                    "name": "Mohammad Taha Bahadori",
                    "affiliations": []
                },
                {
                    "name": "Elizabeth Searles",
                    "affiliations": []
                },
                {
                    "name": "Catherine Coffey",
                    "affiliations": []
                },
                {
                    "name": "Jimeng Sun",
                    "affiliations": []
                }
            ],
            "abstractText": "Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skipgram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.",
            "title": "Multi-layer Representation Learning for Medical Concepts"
        }
    },
    "97592875": {
        "X": {
            "sections": [
                {
                    "heading": "1 INTRODUCTION",
                    "text": "Unsupervised learning is a fundamental, unsolved problem (Hastie et al., 2009) and has seen promising results in domains such as image recognition (Le et al., 2013) and natural language understanding (Ramachandran et al., 2017). A central use case of unsupervised learning methods is enabling better or more efficient learning of downstream tasks by, by training on top of unsupervised representations (Reed et al., 2014; Cheung et al., 2015; Chen et al., 2016) or fine-tuning a learned model (Erhan et al., 2010). However, since the downstream objective requires access to supervision, the objectives used for unsupervised learning are only a rough proxy for downstream performance. If a central goal of unsupervised learning is to learn useful representations, can we derive an unsupervised learning objective that explicitly takes into account how the representation will be used?\nThe use of unsupervised representations for downstream tasks is closely related to the objective of meta-learning techniques: finding a learning procedure that is more efficient and effective than learning from scratch. However, unlike unsupervised learning, meta-learning methods require large, labeled datasets and hand-specified task distributions. We propose a method that aims to learn a learning procedure, without supervision, that is useful for solving a wide range of new, userspecified tasks. With only raw, unlabeled observations, our model\u2019s goal is to learn a useful prior such that, after meta-training, when presented with a modestly-sized dataset for a human-specified task, the model can transfer its prior experience to efficiently learn to perform the new task. If we can build such an unsupervised meta-learning algorithm, we can enable few-shot learning of new tasks without needing any labeled data nor any pre-defined tasks.\nTo perform unsupervised meta-learning, we need to construct a set of tasks in an unsupervised and automated fashion. We study several options for how the tasks can be generated automatically from the unlabeled data. We find that a good task distribution should be diverse, but also not too difficult: na\u0131\u0308ve random approaches for task generation can produce tasks that contain insufficient regularity to enable useful meta-learning. To that end, our method proposes tasks by first learning a representation of the input via unsupervised learning (a variety of unsupervised methods can be used here),\n\u2020Work done as a visiting student researcher at the University of California, Berkeley.\nar X\niv :1\n81 0.\n02 33\n4v 2\n[ cs\n.L G\n] 6\nO ct\n2 01\n8\nand then performing an overcomplete partitioning of the dataset to construct numerous potential categorizations of the data. We show how we can derive classification tasks from these categorizations, for use with meta-learning algorithms. Surprisingly, even with relatively simple mechanisms for task construction, such as k-means clustering, this kind of unsupervised meta-learning can acquire priors that, when used to learn new tasks, can learn those tasks more effectively than methods that use unsupervised representations directly. That is, the learning algorithm acquired through unsupervised meta-learning achieves better downstream performance than the original representation used to derive meta-training tasks, without introducing any additional assumptions or supervision.\nThe core idea of this paper is that meta-learning combined with unsupervised task construction can lead to representations that are more useful for learning downstream tasks, compared to existing approaches. In the following sections, we formalize our unsupervised meta-learning problem assumptions and goal, which match those of unsupervised learning, and we develop an unsupervised learning algorithm that performs meta-learning on tasks constructed by clustering unsupervised representations. We instantiate our method with two meta-learning algorithms and compare to state-of-the-art unsupervised learning methods. Across four image datasets (MNIST, Omniglot, miniImageNet, and CelebA), we find that our method consistently leads to effective downstream learning of a variety of tasks, including character recognition tasks, object classification tasks, and attribute recognition tasks, without requiring any labels or hand-designed tasks during meta-learning and where hyperparameters of our method are held constant across all domains. We show that, even though our unsupervised meta-learning algorithm trains for one-shot generalization, one instantiation of our approach performs well not only on few-shot learning, but also when learning downstream tasks with up to 50 training examples per class. In fact, some of our results begin to approach the performance of fully-supervised meta-learning techniques trained with fully-specified task distributions."
                },
                {
                    "heading": "2 UNSUPERVISED META-LEARNING",
                    "text": "In this section, we describe our problem setting in relation to that of unsupervised and semisupervised learning, and present our approach."
                },
                {
                    "heading": "2.1 PROBLEM STATEMENT",
                    "text": "Our goal is to leverage unlabeled data for efficient learning of a range of downstream tasks. Hence, during unsupervised learning, we assume access to an unlabeled dataset D = {xi}. Then, after unsupervised learning, we want to apply what was learned with the unlabeled data towards learning a variety of downstream, human-specified tasks from a modest amount of labeled data, potentially as few as a single example per class. These downstream tasks may, in general, involve data with different underlying classes or attributes (in contrast to typical semi-supervised learning problem assumptions), but are assumed to have inputs from the same distribution as the one from which datapoints in D are drawn. Concretely, we assume that downstream tasks are M -way classification tasks, and that the goal is to learn an accurate classifier using K labeled datapoints (xk,yk) from each of the M classes, where K is relatively small (i.e. between 1 and 50).\nAn algorithm that solves this problem needs to leverage the unsupervised data in a way that produces an effective procedure to learn the downstream tasks. Akin to unsupervised learning, and in contrast to the typical semi-supervised learning problem statement, the unsupervised learning phase involves no access to information about the downstream tasks, other than the fact that they are M -way classification tasks, for variable M upper-bounded by N . The upper bound N is assumed to be known during unsupervised learning, but otherwise, the values of M and K are not known a priori. As a result, the unsupervised learning phase needs to acquire a sufficiently general prior for applicability to a range of classification tasks with variable quantities of data and classes. This problem definition is our prototype for a practical use-case in which an application-specific image recognition model needs to be fitted without an abundance of labeled data."
                },
                {
                    "heading": "2.2 ALGORITHM OVERVIEW",
                    "text": "We approach this problem from a meta-learning perspective, framing the goal as the acquisition, from unlabeled data, of an efficient learning procedure that is transferable to human-designed tasks. In particular, we aim to construct classification tasks from the unlabeled data and then learn how\nto efficiently learn these tasks. If such unsupervised tasks are adequately structured and diverse, then meta-learning these tasks should enable fast learning of new, human-provided tasks. A key question, then, is how to automatically construct such classification tasks from unlabeled data. The first na\u0131\u0308ve approach we could consider is randomly sampling from D and applying random labels. However, with such a scheme there is no consistency between a task\u2019s training data and query data, and hence nothing to be learned during each task, let alone across tasks. We find in our experiments that this results in failed meta-learning. In the unsupervised learning literature, common distance functions operating in learned embedding spaces have been shown to qualitatively correspond to semantic meaning (e.g., see Cheung et al. (2015); Bojanowski & Joulin (2017); Caron et al. (2018)). We consider using such an embedding space to construct tasks with internal structure. We note that, while a given representation may not be directly suitable for highly-efficient learning of new tasks (which would require the representation to be precisely aligned or adaptable to the classes of those tasks), we can still leverage it for the construction of structured and diverse tasks, a process for which requirements are less strict.\nOverall, our algorithm consists of the following steps: (1) use unsupervised learning to map unlabeled datapoints into embeddings, (2) construct tasks using the unsupervised embeddings, and (3) run meta-learning across the tasks to learn a learning procedure. Then, at meta-test time, we deploy the learning procedure to various downstream tasks. In the rest of this section, we discuss these steps in more detail, including a discussion of multiple ways to derive tasks from unsupervised embeddings."
                },
                {
                    "heading": "2.3 DEFINITIONS",
                    "text": "Before presenting our approach in detail, we formally define the notion of an unsupervised representation learning algorithm, a downstream task, a meta-learning algorithm, and a meta-learning task generation procedure.\nUnsupervised representation learning. We define an unsupervised learning algorithm U as a procedure that takes as input an unlabeled dataset (i.e. D) and produces a mapping from datapoints x to embeddings z.\nTask. We define an M -way K-shot classification task T to consist of K training datapoints and labels {(xk, lk)} per class, which are used for learning a classifier, and Q query datapoints and labels per class, on which the learned classifier is evaluated. That is, a task consists of K +Q = R datapoints and labels for each of the M classes.\nMeta-learning. A supervised meta-learning algorithmM(\u00b7) takes as input a set of supervised metatraining tasks {Tt} and produces a learning procedureF(\u00b7). At meta-test time, F ingests the training data of a new, held-out task Tt\u2032 to produce a classifier ft\u2032(\u00b7). This classifier can be used to predict the label of new datapoints: l\u0302 = ft\u2032(x). At a high level, the quintessential meta-learning strategy is to haveM iterate over {Tt}, cycling between applying the current form of the learning procedure F on training data from Tt, assessing its performance by calculating some loss L on the task\u2019s query data, and optimizing L to improve the learning procedure. We build upon two meta-learning algorithms: model agnostic meta-learning (MAML) (Finn et al., 2017) and prototypical networks (ProtoNets) (Snell et al., 2017). MAML aims to learn the initial parameters of a deep network such that one or a few gradient steps leads to effective generalization; concretely, it specifiesF as gradient descent starting from a meta-learned weight initialization. ProtoNets aim to learn an embedding such that a class can be effectively represented by the mean of its examples\u2019 embeddings; F is the computation of these class prototypes via the meta-learned embedding, and f is a linear classifier that predicts the class whose prototype is closest in Euclidean distance to the query\u2019s embedding.\nTask generation for meta-learning. We briefly summarize how tasks are typically generated from labeled datasets {(xi,yi)} for supervised meta-learning, as introduced by Santoro et al. (2016). For simplicity, consider the case where the labels are discrete scalar values yi. To construct an N -way classification task T (assuming N is not greater than the number of unique yi), we can sample N classes, sample R datapoints {xr}n for each of the N classes, and sample a permutation of N onehot task-specific labels (ln) to be assigned to each of theN sampled classes. The task is then defined as T = {(xn,r, ln) | xn,r \u2208 {xr}n}. The learner F only uses K of the R examples of each class to learn ft; the remaining Q datapoints are held out for assessment.\nAlgorithm 1 CACTUs for classification 1: procedure CACTUS(U ,D, P, k, T,N,Km-tr, Q) 2: Run unsupervised learning algorithm U on D, resulting in a model capable of producing embeddings\n{zi} from observations {xi}. 3: Run k-means on {zi} P times (with random scaling) to generate a set of partitions {Pp = {Cc}p}. 4: for t from 1 to the number of desired tasks T do 5: Sample a partition P from the set of partitions {Pp}. 6: Sample a cluster Cn without replacement from P for each of the N classes desired for each task. 7: Sample an embedding zr without replacement from Cn for each of the R = Km-tr +Q training and query examples desired for each class, and note the corresponding datapoint xn,r . 8: Sample a permutation (ln) of N one-hot labels. 9: Construct Tt = {(xn,r, ln)}.\n10: return {Tt}"
                },
                {
                    "heading": "2.4 UNSUPERVISED META-LEARNING WITH AUTOMATICALLY CONSTRUCTED TASKS",
                    "text": "We return to the key question of how to perform unsupervised meta-learning by constructing tasks from unlabeled data D = {xi}. Notice that in the supervised meta-learning task generation procedure detailed in Section 2.3, the labels yi induce a partition P = {Cc} over {xi} by assigning all datapoints with label yc to subset Cc. Once a partition is at hand, task generation is simple; we can reduce the problem of constructing tasks to that of constructing a partition over {xi}. All we need now is a principled alternative to human-specified labels for defining the partition.\nAs motivated previously, we first run an out-of-the-box unsupervised learning algorithm U on D, then map the data {xi} into the embedding space Z , producing {zi}. The simplest way to define a partition on {zi} (and correspondingly, {xi}) is to use random hyperplanes to sliceZ into subspaces and assign the embeddings that lie in the c-th non-empty subspace to subset Cc. However, a given hyperplane can group together two arbitrarily far embeddings, or separate two arbitrarily close ones. This problem can be partially alleviated by extending the hyperplane boundaries with a non-zero margin, as empirically shown in Section 4.2. But, we can obtain subsets that are both distinctive and internally coherent by instead performing clustering, letting the c-th cluster define subset Cc. To produce a diverse task set, we generate P partitions {Pp} by running clustering P times, applying random scaling to the embedding dimensions to induce a different metric for each run of clustering. We use the k-means clustering algorithm for its simplicity, computational tractability, and effectiveness. We derive tasks for meta-learning from the clusterings using the procedure detailed in Section 2.3, except we begin the construction of each task by sampling a partition from {Pp}. With the partitions being constructed over {zi}, we have one more design decision to make: should we perform meta-learning on embeddings or images? We consider that, to successfully solve new evaluation tasks, a learning procedure F that takes embeddings as input would depend on the embedding function\u2019s ability to generalize to out-of-distribution observations. On the other hand, by meta-learning on images, F can separately adapt f to each evaluation task from the rawest level of representation. Thus, we choose to meta-learn on images.\nWe call our method Clustering to Automatically Construct Tasks for Unsupervised meta-learning (CACTUs). We detail the task construction algorithm in Algorithm 1, and provide an illustration of the complete approach in Figure 1."
                },
                {
                    "heading": "3 RELATED WORK",
                    "text": "The method we propose aims to address the unsupervised learning problem (Hastie et al., 2009; Le et al., 2013), namely acquiring a transferable learning procedure without labels. We show that our method is complementary to a number of unsupervised representation learning methods, including ACAI (Berthelot et al., 2018), BiGAN (Donahue et al., 2017; Dumoulin et al., 2017), DeepCluster (Caron et al., 2018), and InfoGAN (Chen et al., 2016): our method achieves better performance on few-shot learning tasks compared to directly learning on top of these representations. The ability to use what was learned during unsupervised learning to better or more efficiently learn a variety of downstream tasks, i.e. unsupervised pre-training, is arguably one of the most practical applications of unsupervised learning methods and has a long history in neural network training (Hinton et al.,\n2006; Bengio et al., 2007; Ranzato et al., 2006; Vincent et al., 2008; Erhan et al., 2010). Unsupervised pre-training has demonstrated success in a number of domains, including speech recognition Yu et al. (2010), image classification (Zhang et al., 2017), machine translation (Ramachandran et al., 2017), and text classification (Dai & Le, 2015; Howard & Ruder, 2018; Radford et al., 2018). Our approach can be viewed as an unsupervised learning algorithm that explicitly optimizes for few-shot transferability. As a result, we can expect it to better learn human-specified downstream tasks, compared to unsupervised learning methods that optimize for other metrics, such as reconstruction Vincent et al. (2010); Higgins et al. (2017), fidelity of constructed images (Radford et al., 2016; Salimans et al., 2016; Donahue et al., 2017; Dumoulin et al., 2017), representation interpolation (Berthelot et al., 2018), disentanglement (Bengio et al., 2013; Reed et al., 2014; Cheung et al., 2015; Chen et al., 2016; Mathieu et al., 2016; Denton & Birodkar, 2017), and clustering (Coates & Ng, 2012; Kra\u0308henbu\u0308hl et al., 2016; Bojanowski & Joulin, 2017; Caron et al., 2018). We empirically evaluate this hypothesis in the next section. In contrast to many previous evaluations of unsupervised pre-training, we focus on settings in which only a small amount of data for the downstream tasks is available, where the unsupervised data can be maximally useful.\nUnsupervised pre-training followed by supervised learning can be viewed as a special case of the semi-supervised learning problem (Zhu, 2011; Kingma et al., 2014; Rasmus et al., 2015; Oliver et al., 2018). However, in contrast to our problem statement, semi-supervised learning methods assume that a significant proportion of the unlabeled data, if not all of it, shares underlying labels with the labeled data. Additionally, our approach and other unsupervised learning methods are wellsuited for transferring their learned representation to many possible downstream tasks or labelings, whereas semi-supervised learning methods typically optimize for performance on a single task, with respect to a single labeling of the data.\nOur method builds upon the ideas of meta-learning (Schmidhuber, 1987; Bengio et al., 1991; Naik & Mammone, 1992) and few-shot learning (Santoro et al., 2016; Vinyals et al., 2016; Ravi & Larochelle, 2017; Munkhdalai & Yu, 2017; Snell et al., 2017). We apply two meta-learning algorithms, model-agnostic meta-learning (Finn et al., 2017) and prototypical networks (Snell et al., 2017), to tasks constructed in an unsupervised manner. Similar to our problem setting, some prior works have aimed to learn an unsupervised learning procedure with supervised data (Garg & Kalai, 2017; Metz et al., 2018). Instead, we consider a problem setting that is entirely unsupervised, aiming to learn efficient learning algorithms using unlabeled datasets. Our problem setting is similar to that considered by Gupta et al. (2018), but we develop an approach that is suitable for supervised downstream tasks, rather than reinforcement learning problems, and demonstrate our algorithm on problems with high-dimensional visual observations."
                },
                {
                    "heading": "4 EXPERIMENTS",
                    "text": "We begin the experimental section by presenting our research questions and how our experiments are designed to address them.\nBenefit of Meta-Learning. Is there any significant benefit to doing meta-learning on tasks derived from unsupervised representations, or are the representations themselves already sufficient for downstream supervised learning of new tasks? To investigate this, we run MAML and ProtoNets on\ntasks generated via CACTUs (CACTUs-MAML, CACTUs-ProtoNets). We measure performance by comparing to five alternate algorithms. Embedding knn-nearest neighbors first infers the embeddings of the downstream task images. For a query test image, it predicts the plurality vote of the labels of the knn training images that are closest in the embedding space (by L2 distance) to the query\u2019s embedding. Embedding linear classifier also begins by inferring the embeddings of the downstream task images. It then fits a linear classifier using the NK training embeddings and labels, and predicts labels for the query embeddings using the classifier. Embedding multilayer perceptron is essentially embedding linear classifier but with a hidden layer of 128 units and tuned dropout (Srivastava et al., 2014). To isolate the effect of meta-learning on images, we also compare to embedding cluster matching, i.e. directly using the meta-training clusters for classification by labeling clusters with a task\u2019s training data via plurality vote. If a query datapoint maps to an unlabeled cluster, the closest labeled cluster is used. Finally, as a baseline, we also train a model with the MAML architecture from scratch for each evaluation task.\nWe also investigate this question in the scenario of an idealized version of our problem statement in which (1) embeddings are already very well-suited to solving the downstream tasks and (2) the data distributions of meta-training and meta-test time perfectly overlap. We use the MNIST dataset for this purpose. For details, please see Appendix A.\nDifferent Embedding Spaces. Does the proposed unsupervised task-generation procedure result in successful meta-learning for many distinct methods for learning the task-generating embeddings? To investigate this, we run unsupervised meta-learning using four unsupervised learning algorithms to learn the embeddings: ACAI (Berthelot et al., 2018), BiGAN (Donahue et al., 2017), DeepCluster (Caron et al., 2018), and InfoGAN (Chen et al., 2016). These four approaches collectively cover the following range of objectives and frameworks in the unsupervised learning literature: generative modeling, two-player games, reconstruction, representation interpolation, discriminative clustering, and information maximization. We describe these methods in more detail in Appendix B.\nApplicability to Different Tasks. Can unsupervised meta-learning yield a good prior for a variety of task types? In other words, can unsupervised meta-learning yield a good representation for tasks that assess the ability to distinguish between features on different scales, or tasks with various amounts of supervision signal? To investigate this, we evaluate our procedure on tasks assessing recognition of character identity, object identity, and facial attributes. For this purpose we choose to use the existing Omniglot and miniImageNet datasets, but also construct a new few-shot classification benchmark based on the CelebA dataset and its binary attribute annotations. For miniImageNet, we consider both few-shot downstream tasks and tasks involving larger datasets (up to 50-shot). For details on the datasets and tasks, please see Appendix C.\nOracle. How does the performance of our unsupervised meta-learning method compare to supervised meta-learning with a human-specified, near-optimal task distribution derived from a labeled dataset? To investigate this, we use labeled versions of the meta-training datasets to run MAML and ProtoNets as supervised meta-learning algorithms (Oracle-MAML, Oracle-ProtoNets). To facilitate fair comparison with the unsupervised variants, we control for the relevant hyperparameters.\nTask Construction Ablation. How do the alternatives for constructing tasks from the embeddings, as introduced in Section 2.4, compare? To investigate this, we run MAML on tasks constructed via clustering (CACTUs-MAML) and MAML on tasks constructed via random hyperplane slices of the embedding space with varying margin (Hyperplanes-MAML). The latter partitioning procedure is detailed in Appendix D. For the experiments where tasks are constructed via clustering, we also investigate the effect of sampling based on a single partition versus multiple partitions. We additionally experiment with tasks based on random assignments of images to \u201cclusters\u201d (Random-MAML) with the Omniglot dataset."
                },
                {
                    "heading": "4.1 EXPERIMENTAL PROTOCOL SUMMARY",
                    "text": "As discussed by Oliver et al. (2018), keeping proper experimental protocol is particularly important when evaluating unsupervised and semi-supervised learning algorithms. Our foremost concern is to avoid falsely embellishing the capabilities of our approach by overfitting to the specific datasets and task types that we consider. To this end, we adhere to two key principles. We do not do any architecture engineering: we use architectures from prior work as-is, or lightly adapt them to our needs if necessary. We also keep hyperparameters related to the unsupervised meta-learning\nstage as constant as possible across all experiments, including the MAML and ProtoNets model architectures. We assume knowledge of an upper bound on the number of classes N present in each downstream meta-testing task for each dataset. However, regardless of the number of shots K, we do not assume knowledge of K during unsupervised meta-learning; hence, we fix the meta-training algorithm to use N -way 1-shot tasks.\nWe partition each dataset into train, validation, and test splits. For Omniglot and miniImageNet, these splits contain disjoint sets of classes. For all algorithms, we run unsupervised learning on the unlabeled training split and report performance on downstream tasks generated from the labeled data of the testing split, generated using the procedure from Section 2.3. For the supervised meta-learning oracles, meta-training tasks are constructed in the same manner but from the dataset\u2019s training split. See Figure 2 for illustrative examples of embedding-derived clusters and evaluation tasks.\nTo facilitate troubleshooting, we used the labels of the validation split (instead of clustering embeddings) to construct tasks for meta-validation. However, because our aim is to perform meta-learning without supervision, we did not tune hyperparameters beyond performing a simple sanity test. After preliminary exploration, we fixed MAML and ProtoNets hyperparameters across all respective experiments. We use a fixed number of meta-training iterations, since there is no suitable criterion for early stopping. For detailed hyperparameter and architecture information, please see Appendix E.\nWhen we experiment with the methods used as comparisons to unsupervised meta-learning, we err on the side of providing more supervision and data than technically allowed. Specifically, we separately tune hyperparameters for each dataset and each amount of supervision signal in an evaluation task on the labeled version of the validation split. When necessary, we also use the entire testing split\u2019s statistics to perform dimensionality reduction."
                },
                {
                    "heading": "4.2 RESULTS",
                    "text": "Primary results (excluding MNIST) are summarized in Tables 1, 2, and 3. Task construction ablations are summarized in Tables 4 and 5.\nBenefit of Meta-Learning. CACTUs-MAML consistently yields a learning procedure that results in more successful downstream task performance than all other unsupervised methods, including those that learn on top of the embedding that generated meta-training tasks for MAML. We find the same result for CACTUs-ProtoNets for 1-shot downstream tasks. However, as noted by Snell et al. (2017), ProtoNets perform best when meta-training shot and meta-testing shot are matched; this characteristic prevents ProtoNets from improving upon ACAI for 20-way 5-shot Omniglot and upon DeepCluster for 50-shot miniImageNet. We attribute the success of CACTUs-based metalearning over the embedding-based methods to two factors: its practice in distinguishing between many distinct sets of clusters from modest amounts of signal, and the underlying classes of the test-\ning split data being out-of-distribution. In principle, the latter factor is solely responsible for the success over embedding cluster matching, since this algorithm can be viewed as a meta-learner on embeddings that trivially obtains perfect accuracy (via memorization) on the meta-training tasks. The same factor also helps explain why training from standard network initialization is, in general, competitive with directly using the task-generating embedding as a representation. On the other hand, the MNIST results (Table 8 in Appendix F) suggest that when the meta-training and meta-testing data distributions have perfect overlap and the embedding is well-suited enough that embedding cluster matching alone can achieve high performance, CACTUs-MAML yields only a small relative benefit, as we would expect.\nDifferent Embedding Spaces. CACTUs is effective for a variety of unsupervised learning methods used for task generation. The performance of unsupervised meta-learning can largely be predicted by the performance of the embedding-based non-meta-learning methods. For example, the ACAI embedding does well with Omniglot, leading to the best unsupervised results with ACAI CACTUsMAML. Likewise, on miniImageNet, the best performing prior embedding (DeepCluster) also corresponds to the best performing unsupervised meta-learner (DeepCluster CACTUs-MAML).\nApplicability to Different Tasks. CACTUs-MAML learns an effective prior for a variety of task types. This can be attributed to the application-agnostic task-generation process and the expressive power of MAML (Finn & Levine, 2018). We also observe that, despite all meta-learning models being trained for N -way 1-shot classification of unsupervised tasks, the models work well for a variety of M -way K-shot tasks, where M \u2264 N and K \u2264 50. As mentioned previously, the representation that CACTUs-ProtoNets learns is best suited for downstream tasks which match the single shot used for meta-training.\nOracle. The penalty for not having ground truth labels to construct near-optimal tasks ranges from substantial to severe, depending on the difficulty of the downstream task. Easier downstream tasks (which have fewer classes and/or more supervision) incur less of a penalty. We conjecture that with such tasks, the difference in the usefulness of the priors matters less since the downstream task-specific evidence has more power to shape the posterior.\nTask Construction Ablation. As seen in Tables 4 and 5, CACTUs-MAML consistently outperforms Hyperplanes-MAML with any margin. We hypothesize that this is due to the issues with zero-margin Hyperplanes-MAML pointed out in Section 2.4, and the fact that nonzero-margin Hyperplanes-MAML is able to use less of the training split to generate tasks than CACTUs-MAML is. Using non-zero margin with Hyperplanes-MAML is crucial for miniImageNet, but not for Omniglot. We conjecture that the enforced degree of separation between classes is needed for miniImageNet because of the dataset\u2019s high diversity. Meta-learning on random tasks (Table 4) results in a prior that is much less useful than any other considered algorithm, including standard network initialization; evidently, practicing badly is worse than not practicing at all.\nNote on Overfitting. Because of the combinatorially many unsupervised tasks we can create from multiple partitions of the dataset, we do not observe substantial overfitting to the unsupervised metatraining tasks. However, we observe that meta-training performance is sometimes worse than metatest time performance, which is likely due to a portion of the automatically generated tasks being based on nonsensical clusters (for examples, see Figure 2). Additionally, we find that, with a few exceptions, using multiple partitions has a regularizing effect on the meta-learner: a diverse task set reduces overfitting to the meta-training tasks and increases the applicability of the learned prior."
                },
                {
                    "heading": "5 DISCUSSION",
                    "text": "We demonstrate that meta-learning on tasks produced using simple mechanisms based on unsupervised representations improves upon the utility of these representations in learning downstream tasks. We empirically show that this holds across instances of datasets, task difficulties, and unsupervised representations, while fixing key hyperparameters across all experiments.\nIn a sense, CACTUs can be seen as a facilitating interface between an unsupervised representation learning method and a meta-learning algorithm. As shown in the results, the meta-learner\u2019s performance significantly depends on the nature and quality of the task-generating embedding. We can expect our method to yield better performance as the methods that produce these embedding functions improve, becoming better suited for generating diverse yet distinctive clusterings of the data. However, the gap between CACTUs and supervised meta-learning will likely persist because, with the latter, the meta-training task distribution is human-designed to mimic the expected evaluation task distribution as much as possible. Indeed, to some extent, supervised meta-learning algorithms offload the effort of designing and tuning algorithms onto the effort of designing and tuning task distributions. With its evaluation-agnostic task generation, CACTUs-based meta-learning trades off performance in specific use-cases for broad applicability and the ability to train on unlabeled data. In principle, CACTUs-based meta-learning may outperform supervised meta-learning when the latter is trained on a misaligned task distribution. We leave this investigation to future work.\nA potential concern of our experimental evaluation is that MNIST, Omniglot, and miniImageNet exhibit particular structure in the underlying class distribution (i.e., perfectly balanced classes), since they were designed to be supervised learning benchmarks. In more practical applications of machine learning, such structure would likely not exist. Our CelebA results indicate that CACTUs is effective even in the case of a dataset without neatly balanced classes or attributes. An interesting direction for future work is to better characterize the performance of CACTUs and other unsupervised learning methods with highly-unstructured, unlabeled datasets.\nSince MAML and ProtoNets produce nothing more than a learned representation, our method can be viewed as deriving, from a previous unsupervised representation, a new representation particularly suited for learning downstream tasks. aBeyond visual classification tasks, the notion of using unsupervised pre-training is generally applicable to a wide range of domains, including regression, speech (Oord et al., 2018), language (Howard & Ruder, 2018), and reinforcement learning (Shelhamer et al., 2017). Hence, our unsupervised meta-learning approach has the potential to improve unsupervised representations for a variety of such domains, an exciting avenue for future work."
                },
                {
                    "heading": "ACKNOWLEDGMENTS",
                    "text": "We thank Kelvin Xu, Richard Zhang, Brian Cheung, Ben Poole, Aa\u0308ron van den Oord, Luke Metz, and Siddharth Reddy for feedback on an early draft of this paper."
                },
                {
                    "heading": "APPENDIX A MNIST EXPERIMENTS",
                    "text": "The MNIST dataset consists of 70,000 hand-drawn examples of the 10 numerical digits. Our split respects the original MNIST 60,000/10,000 training/testing split. We assess on 10-way classification tasks. This setup results in examples from all 10 digits being present for both meta-training and meta-testing, making the probem setting essentially equivalent to that of semi-supervised learning sans a fixed permutation of the labels. The MNIST scenario is thus a special case of the problem setting considered in the rest of the paper. For MNIST, we only experiment with MAML as the meta-learning algorithm.\nFor ACAI and InfoGAN we constructed the validation split from the last 5,000 examples of the training split; for BiGAN this figure was 10,000. After training the ACAI model and inferring embeddings, manually assigning labels to 10 clusters by inspection results in a classification accuracy of 96.00% on the testing split. As the ACAI authors observe, we found it important to whiten the ACAI embeddings before clustering. The same metric for the InfoGAN embedding (taking an argmax over the categorical dimensions instead of actually running clustering) is 96.83%. Note that these results are an upper-bound for embedding cluster matching. To see this, consider the 10-way 1-shot scenario. 1 example sampled from each cluster is insufficient to guarantee the optimal label for that cluster; 1 example sampled from each label is not guaranteed to each end up in the optimal category.\nAside from CACTUs-MAML, embedding knn-nearest neighbors, embedding linear classifier, and embedding direct clustering, we also ran CACTUs-MAML on embeddings instead of raw images, using a simple model with 2 hidden layers with 64 units each, and all other MAML hyperparameters being the same as in Table 6.\nDeparting from the fixed k = 500 used for all other datasets, we deliberately use k = 10 to better understand the limitations of CACTUs-MAML. The results can be seen in Table 8 in Appendix C. In brief, with the better embeddings (ACAI and InfoGAN), there is only little benefit of CACTUsMAML over embedding cluster matching. Additionally, even in the best cases, CACTUs-MAML falls short of state-of-the-art semi-supervised learning methods."
                },
                {
                    "heading": "APPENDIX B THE UNSUPERVISED LEARNING ZOO",
                    "text": "We evaluate four distinct unsupervised learning methods for learning the task-generating embeddings.\nIn adversarially constrained autoencoder interpolation (ACAI), a convolutional autoencoder\u2019s pixelwise L2 loss is regularized with a term encouraging meaningful interpolations in the latent space (Berthelot et al., 2018). Specifically, a critic network takes as input a synthetic image generated from a convex combination of the latents of two dataset samples, and regresses to the mixing factor. The decoder of the autoencoder and the generator for the critic are one and the same. The regularization term is minimized when the autoencoder fools the critic into predicting that the synthetic image is a real sample.\nThe bidirectional GAN (BiGAN) is an instance of a generative-adversarial framework in which the generator produces both synthetic image and embedding from real embedding and image, respectively (Donahue et al., 2017; Dumoulin et al., 2017). Discrimination is done in joint imageembedding space.\nThe DeepCluster method does discriminative clustering by alternating between clustering the features of a convolutional neural network and using the clusters as labels to optimize the network weights via backpropagating a standard classification loss (Caron et al., 2018).\nThe InfoGAN framework conceptually decomposes the generator\u2019s input into a latent code and incompressible noise (Chen et al., 2016). The structure of the latent code is hand-specified based on knowledge of the dataset. The canonical GAN minimax objective is regularized with a mutual information term between the code and the generated image. In practice, this term is optimized using variational inference, involving the approximation of the posterior with an auxiliary distribution Q(code|image) parameterized by a recognition network.\nWhereas ACAI explicitly optimizes pixel-wise reconstruction error, BiGAN only encourages the fidelity of generated image and latent samples with respect to their respective prior distributions. While InfoGAN also encourages the fidelity of generated images, it leverages domain-specific knowledge to impose a favorable structure on the embedding space and information-theoretic methods for optimization. DeepCluster departs from the aforementioned methods in that it is not concerned with generation or decoding, and only seeks to learn general-purpose visual features by way of end-to-end discriminative clustering."
                },
                {
                    "heading": "APPENDIX C DATASET INFORMATION",
                    "text": "The Omniglot dataset consists of 1623 characters each with 20 hand-drawn examples. Ignoring the alphabets from which the characters originate, we use 1100, 100, and 423 characters for our training, validation, and testing splits. The miniImageNet dataset consists of 100 classes each with 600 examples. The images are predominantly natural and realistic. We use the same training/validation/testing splits of 64/16/20 classes as proposed by Ravi & Larochelle (2017). The CelebA dataset includes 202,599 facial images of celebrities and 40 binary attributes that annotate every image. We follow the prescribed 162,770/19,867/19,962 data split.\nFor Omniglot and miniImageNet, supervised meta-learning tasks and evaluation tasks are constructed exactly as detailed in Section 2.3: for an N -way K-shot task with Q queries per class, we sample N classes from the data split and K + Q datapoints per class, labeling the task\u2019s data with a random permutation of N one-hot vectors.\nFor CelebA, we consider binary classification tasks (i.e., 2-way), each defined by 3 attributes and an ordering of 3 Booleans, one for each attribute. Every image in a task-specific class shares all task-specific attributes with each other and none with images in the other class. For example, the task illustrated in Figure 2 involves distinguishing between images whose subjects satisfy not Sideburns, Straight Hair, and not Young, and those whose subjects satisfy Sideburns, not Straight Hair, and Young. To keep with the idea of having distinct classes for meta-training and meta-testing, we split the task-defining attributes. For the supervised meta-learning oracle, we construct meta-training tasks from the first 20 attributes (when alphabetically ordered), meta-validation tasks from the next 10, and meta-testing tasks from the last 10. Discarding tasks with too few examples in either class, this results in 4287, 391, and 402 task prototypes (but many more possible tasks). We use the same meta-test time tasks to evaluate the unsupervised methods. We only consider assessment with 5-shot tasks because, given that there are multiple attributes other than the task-defining ones, any 1-shot task is likely to be ill-defined."
                },
                {
                    "heading": "APPENDIX D TASK CONSTRUCTION VIA RANDOM HYPERPLANES",
                    "text": "In this section, we describe how to generate tasks via random hyperplanes in the embedding space. We first describe a procedure to generate a partition P of the set of embeddings {zi} for constructing meta-training tasks. A given hyperplane slices the embedding space into two, so for an N -way task, we need H = dlog2Ne hyperplanes to define sufficiently many subsets/classes for a task. To randomly define a hyperplane in d-dimensional embedding space, we sample a normal vector n and a point on the plane z0, each with d elements. For an embedding point z, the signed point-plane distance is given by n|n|2 \u00b7 (z\u2212 z0). Defining H hyperplanes in this manner, we discard embeddings for which the signed point-plane distance to any of the H hyperplanes lies within (\u2212m,m), where m is a desired margin. The H hyperplanes collectively define 2H subspaces. We assign embedding points in the c-th subspace to subset Cc. We define the partition as P = {Cc}. We prune subsets that do not have at least R = Km-tr +Q members, and check that the partition has at least N remaining subsets; if not, we reject the partition and restart the procedure. After obtaining partitions {Pp}, meta-training tasks can be generated by following Algorithm 1 from Line 4.\nIn terms of practical implementation, we pre-compute 1000 hyperplanes and pruned pairs of subsets of {zi}. We generate partitions by sampling combinations of the hyperplanes and taking intersections of their associated subsets to define the elements of the partition. We determine the number of partitions needed for a given Hyperplanes-MAML run by the number of meta-training tasks desired for the meta-learner: we fix 100 tasks per partition."
                },
                {
                    "heading": "APPENDIX E HYPERPARAMETERS AND ARCHITECTURES",
                    "text": "E.1 MAML\nFor MNIST and Omniglot we use the same 4-block convolutional architecture as used by Finn et al. (2017) for their Omniglot experiments, but with 32 filters (instead of 64) for each convolutional layer. For miniImageNet and CelebA we used the same architecture as Finn et al. (2017) used for their miniImageNet experiments. When evaluating the trained 20-way Omniglot model with 5-way tasks, we prune the unused output dimensions. The outer optimizer is Adam (Kingma & Ba, 2014), and the inner optimizer is SGD. We build on the authors\u2019 publicly available codebase found at https://github.com/cbfinn/maml.\nWhen using batch normalization (Ioffe & Szegedy, 2015) to process a task\u2019s training or query inputs, we observe that using only 1 query datapoint per class can allow the model to exploit batch statistics, learning a strategy analogous to a process of elimination that causes significant, but spurious, improvement in accuracy. To mitigate this, we fix 5 queries per class for every task\u2019s evaluation phase, meta-training or meta-testing.\nE.2 PROTONETS\nFor the three considered datasets we use the same architecture as used by Snell et al. (2017) for their Omniglot and miniImageNet experiments. This is a 4-block convolutional architecture with each block consisting of a convolutional layer with 64 3 \u00d7 3 filters, stride 1, and padding 1, followed by BatchNorm, ReLU activation, and 2 \u00d7 2 MaxPooling. The ProtoNets embedding is simply the flattened output of the last block. We follow the authors and use the Adam optimizer, but do not use a learning rate scheduler. We build upon the authors\u2019 publicly available codebase found at https://github.com/jakesnell/prototypical-networks.\nE.3 USE OF UNSUPERVISED LEARNING METHODS\nACAI (Berthelot et al., 2018): We run ACAI for MNIST and Omniglot. We pad the images by 2 and use the authors\u2019 architecture. We use a 256-dimensional embedding for all datasets. We build upon the authors\u2019 publicly available codebase found at https://github.com/brain-research/acai.\nWe unsuccessfully try running ACAI on 64\u00d7 64 miniImageNet and CelebA. To facilitate this input size, we add one block consisting of two convolutional layers (512 filters each) and one downsampling/upsampling layer to the encoder and decoder. However, because of ACAI\u2019s pixel-wise reconstruction loss, for these datasets the ACAI embedding prioritizes information about the few \u201cfeatures\u201d that dominate the reconstruction pixel count, resulting in clusters that only corresponded to a limited range of factors, such as background color and pose. For curiosity\u2019s sake, we tried running meta-learning on tasks derived from these uninteresting clusters anyways, and found that the meta-learner quickly produced a learning procedure that obtained high accuracy on the meta-training tasks. However, this learned prior was not useful for solving downstream tasks.\nBiGAN (Donahue et al., 2017): For MNIST, we follow the BiGAN authors and specify a uniform 50-dimensional prior on the unit hypercube for the latent. The BiGAN authors use a 200- dimensional version of the same prior for their ImageNet experiments, so we follow suit for Omniglot, miniImageNet, and CelebA. For MNIST and Omniglot, we use the permutation-invariant architecture (i.e. fully connected layers only) used by the authors for their MNIST results; for miniImageNet and CelebA, we randomly crop to 64\u00d764 and use the AlexNet-inspired architecture used by Donahue et al. (2017) for their ImageNet results. We build upon the authors\u2019 publicly available codebase found at https://github.com/jeffdonahue/bigan.\nDeepCluster (Caron et al., 2018): We run DeepCluster for miniImageNet and CelebA, which we respectively randomly crop and resize to 64\u00d764. We modify the first layer of the AlexNet architecture used by the authors to accommodate this input size. We follow the authors and use the input to the (linear) output layer as the embedding. These are 4096-dimensional, so we follow the authors and apply PCA to reduce the dimensionality to 256, followed by whitening. We build upon the authors\u2019 publicly available codebase found at https://github.com/facebookresearch/deepcluster.\nInfoGAN (Chen et al., 2016): We only run InfoGAN for MNIST. We follow the InfoGAN authors and specify the product of a 10-way categorical distribution and a 2-dimensional uniform distribution as the latent code. Given an image, we use the recognition network to obtain its embedding. We build upon the authors\u2019 publicly available codebase found at https://github.com/openai/InfoGAN."
                },
                {
                    "heading": "APPENDIX F EXPERIMENTAL RESULTS",
                    "text": "The following pages contain full experimental results for the MNIST, Omniglot, miniImageNet, and CelebA datasets, including consolidated versions of the tables found in the main text and some comparisons to prior work. The metric is classification accuracy averaged over 1000 tasks based on human-specified labels of the testing split, with 95% confidence intervals. d: dimensionality of embedding, h: number of hidden units in a fully connected layer, k: number of clusters in a partition, P : number of partitions used during meta-learning, m: margin on boundary-defining hyperplanes.\nTa bl\ne 8:\nM N\nIS T\ndi gi\ntc la\nss ifi\nca tio\nn re\nsu lts\nav er\nag ed\nov er\n10 00\nta sk\ns. \u00b1\nde no\nte s\na 95\n% co\nnfi de\nnc e\nin te\nrv al\n.k :n\num be\nro fc\nlu st\ner s\nin a\npa rt\niti on\n,P :n\num be\nro fp\nar tit\nio ns\nus ed\ndu ri ng m et ale ar ni ng\nA lg\nor ith\nm \\\n(w ay\n,s ho\nt) (1\n0, 1)\n(1 0,\n5) (1\n0, 10\n)\nAC A\nI, d =\n2 5 6\nE m\nbe dd\nin g k\nnn -n\nea re\nst ne\nig hb\nor s\n74 .4\n9 \u00b1\n0. 82\n% 88\n.8 0 \u00b1\n0. 27\n% 91\n.9 0 \u00b1\n0. 17 % E m be dd in g lin ea rc la ss ifi er 76 .5 3 \u00b1 0. 81 % 92 .1 7 \u00b1 0. 25 % 94 .5 8 \u00b1 0. 15 % E m be dd in g cl us te rm at ch in g, k = 1 0 91 .2 8 \u00b1 0. 58 % 95 .9 2 \u00b1 0. 16 % 96 .0 1 \u00b1 0. 12 % C A C T U sM A M L on im ag es (o ur s) ,P = 1 ,k = 1 0 92 .6 6 \u00b1 0. 34 % 96 .0 8 \u00b1 0. 12 % 96 .2 9 \u00b1 0. 12 % C A C T U sM A M L on em be dd in gs (o ur s) ,P = 1 ,k = 1 0 94 .7 7 \u00b1 0. 28 % 96 .5 6 \u00b1 0. 11 % 96 .8 0 \u00b1 0. 11 % B iG A N ,d = 5 0 E m be dd in g k nn -n ea re st ne ig hb or s 29 .2 5 \u00b1 0. 83 % 44 .5 9 \u00b1 0. 44 % 51 .9 8 \u00b1 0. 30 % E m be dd in g lin ea rc la ss ifi er 30 .8 6 \u00b1 0. 89 % 51 .6 9 \u00b1 0. 44 % 60 .7 0 \u00b1 0. 31 % E m be dd in g cl us te rm at ch in g, k = 1 0 33 .7 2 \u00b1 0. 54 % 50 .2 1 \u00b1 0. 36 % 52 .9 5 \u00b1 0. 34 % C A C T U sM A M L on im ag es (o ur s) ,P = 1 ,k = 1 0 43 .7 5 \u00b1 0. 46 % 62 .2 0 \u00b1 0. 33 % 68 .3 8 \u00b1 0. 29 % C A C T U sM A M L on im ag es (o ur s) ,P = 1 0 0 ,k = 1 0 49 .7 3 \u00b1 0. 45 % 77 .0 5 \u00b1 0. 30 % 83 .9 0 \u00b1 0. 24 % C A C T U sM A M L on em be dd in gs (o ur s) ,P = 1 ,k = 1 0 36 .3 3 \u00b1 0. 48 % 51 .7 8 \u00b1 0. 34 % 57 .4 1 \u00b1 0. 30 % C A C T U sM A M L on em be dd in gs (o ur s) ,P = 1 0 0 ,k = 1 0 37 .3 2 \u00b1 0. 41 % 60 .7 4 \u00b1 0. 34 % 67 .3 4 \u00b1 0. 30 % In fo G A N ,d = 1 2 E m be dd in g k nn -n ea re st ne ig hb or s 94 .5 3 \u00b1 0. 51 % 96 .0 5 \u00b1 0. 17 % 96 .2 4 \u00b1 0. 12 % E m be dd in g lin ea rc la ss ifi er 95 .7 8 \u00b1 0. 42 % 96 .6 1 \u00b1 0. 21 % 96 .8 5 \u00b1 0. 11 % E m be dd in g cl us te rm at ch in g, k = 1 0 93 .4 2 \u00b1 0. 57 % 96 .9 7 \u00b1 0. 15 % 96 .9 9 \u00b1 0. 10 % C A C T U sM A M L on im ag es (o ur s) ,P = 1 ,k = 1 0 95 .3 0 \u00b1 0. 23 % 97 .1 8 \u00b1 0. 10 % 97 .2 8 \u00b1 0. 10 % C A C T U sM A M L on im ag es (o ur s) ,P = 1 0 0 ,k = 1 0 96 .0 8 \u00b1 0. 19 % 97 .2 2 \u00b1 0. 10 % 97 .3 1 \u00b1 0. 09 % C A C T U sM A M L on em be dd in gs (o ur s) ,P = 1 ,k = 1 0 96 .6 9 \u00b1 0. 17 % 97 .1 3 \u00b1 0. 10 % 97 .2 3 \u00b1 0. 10 % C A C T U sM A M L on em be dd in gs (o ur s) ,P = 1 0 0 ,k = 1 0 96 .4 8 \u00b1 0. 17 % 97 .0 8 \u00b1 0. 10 % 97 .2 2 \u00b1 0. 10 % Su pe rv is ed pr etr ai ni ng O ra cl eM A M L (c on tr ol ) 97 .3 1 \u00b1 0. 17 % 98 .5 1 \u00b1 0. 07 % 98 .5 1 \u00b1 0. 07 %\nTa bl\ne 9:\nO m\nni gl\not ch\nar ac\nte r\ncl as\nsi fic\nat io\nn re\nsu lts\nav er\nag ed\nov er\n10 00\nta sk\ns. \u00b1\nde no\nte s\na 95\n% co\nnfi de\nnc e\nin te\nrv al\n. d\n: di\nm en\nsi on\nal ity\nof em\nbe dd\nin g,\nh :\nnu m\nbe r\nof hi\ndd en\nun its\nin a\nfu lly\nco nn\nec te\nd la\nye r, k\n:n um\nbe ro\nfc lu\nst er\ns in\na pa\nrt iti\non ,P\n:n um\nbe ro\nfp ar\ntit io\nns us\ned du\nri ng\nm et\nale\nar ni\nng ,m\n:m ar\ngi n\non bo\nun da\nry -d\nefi ni\nng hy\npe rp\nla ne\ns.\nA lg\nor ith\nm \\\n(w ay\n,s ho\nt) (5\n,1 )\n(5 ,5\n) (2\n0, 1)\n(2 0,\n5)\nB as\nel in es Tr ai ni\nng fr\nom sc\nra tc\nh 52\n.5 0 \u00b1\n0. 84\n% 74\n.7 8 \u00b1\n0. 69\n% 24\n.9 1 \u00b1\n0. 33\n% 47\n.6 2 \u00b1\n0. 44 % R an do m -M A M L ,P = 2 4 0 0 ,k = 5 0 0 25 .9 9 \u00b1 0. 73 % 25 .7 4 \u00b1 0. 69 % 6. 51 \u00b1 0. 18 % 6. 74 \u00b1 0. 18 % AC A I, d = 2 5 6 E m be dd in g k nn -n ea re st ne ig hb or s 57 .4 6 \u00b1 1. 35 % 81 .1 6 \u00b1 0. 57 % 39 .7 3 \u00b1 0. 38 % 66 .3 8 \u00b1 0. 36 % E m be dd in g lin ea rc la ss ifi er 61 .0 8 \u00b1 1. 32 % 81 .8 2 \u00b1 0. 58 % 43 .2 0 \u00b1 0. 69 % 66 .3 3 \u00b1 0. 36 % E m be dd in g M L P w ith dr op ou t, h = 1 2 8 51 .9 5 \u00b1 0. 82 % 77 .2 0 \u00b1 0. 65 % 30 .6 5 \u00b1 0. 39 % 58 .6 2 \u00b1 0. 41 % E m be dd in g cl us te rm at ch in g, k = 5 0 0 54 .9 4 \u00b1 0. 85 % 71 .0 9 \u00b1 0. 77 % 32 .1 9 \u00b1 0. 40 % 45 .9 3 \u00b1 0. 40 % H yp er pl an es -M A M L (o ur s) ,P = 2 4 0 0 ,m = 0 62 .3 4 \u00b1 0. 82 % 81 .8 1 \u00b1 0. 60 % 39 .3 0 \u00b1 0. 37 % 63 .1 8 \u00b1 0. 38 % H yp er pl an es -M A M L (o ur s) ,P = 2 4 0 0 ,m = 1 .2 62 .4 4 \u00b1 0. 82 % 83 .2 0 \u00b1 0. 58 % 41 .8 6 \u00b1 0. 38 % 65 .2 3 \u00b1 0. 37 % C A C T U sM A M L (o ur s) ,P = 1 ,k = 5 0 0 66 .4 9 \u00b1 0. 80 % 85 .6 0 \u00b1 0. 53 % 45 .0 4 \u00b1 0. 41 % 69 .1 4 \u00b1 0. 36 % C A C T U sM A M L (o ur s) ,P = 1 0 0 ,k = 5 0 0 68 .8 4 \u00b1 0. 80 % 87 .7 8 \u00b1 0. 50 % 48 .0 9 \u00b1 0. 41 % 73 .3 6 \u00b1 0. 34 % C A C T U sPr ot oN et s (o ur s) ,P = 1 0 0 ,k = 5 0 0 68 .1 2 \u00b1 0. 84 % 83 .5 8 \u00b1 0. 61 % 47 .7 5 \u00b1 0. 43 % 66 .2 7 \u00b1 0. 37 % B iG A N ,d = 2 0 0 E m be dd in g k nn -n ea re st ne ig hb or s 49 .5 5 \u00b1 1. 27 % 68 .0 6 \u00b1 0. 71 % 27 .3 7 \u00b1 0. 33 % 46 .7 0 \u00b1 0. 36 % E m be dd in g lin ea rc la ss ifi er 48 .2 8 \u00b1 1. 25 % 68 .7 2 \u00b1 0. 66 % 27 .8 0 \u00b1 0. 61 % 45 .8 2 \u00b1 0. 37 % E m be dd in g M L P w ith dr op ou t, h = 1 2 8 40 .5 4 \u00b1 0. 79 % 62 .5 6 \u00b1 0. 79 % 19 .9 2 \u00b1 0. 32 % 40 .7 1 \u00b1 0. 40 % E m be dd in g cl us te rm at ch in g, k = 5 0 0 43 .9 6 \u00b1 0. 80 % 58 .6 2 \u00b1 0. 78 % 21 .5 4 \u00b1 0. 32 % 31 .0 6 \u00b1 0. 37 % H yp er pl an es -M A M L (o ur s) ,P = 2 4 0 0 ,m = 0 53 .6 0 \u00b1 0. 82 % 74 .6 0 \u00b1 0. 69 % 29 .0 2 \u00b1 0. 33 % 50 .7 7 \u00b1 0. 39 % H yp er pl an es -M A M L (o ur s) ,P = 2 4 0 0 ,m = 0 .5 53 .1 8 \u00b1 0. 81 % 73 .5 5 \u00b1 0. 69 % 29 .9 8 \u00b1 0. 35 % 50 .1 4 \u00b1 0. 38 % C A C T U sM A M L (o ur s) ,P = 1 ,k = 5 0 0 55 .9 2 \u00b1 0. 80 % 76 .2 8 \u00b1 0. 65 % 32 .4 4 \u00b1 0. 35 % 54 .2 2 \u00b1 0. 39 % C A C T U sM A M L (o ur s) ,P = 1 0 0 ,k = 5 0 0 58 .1 8 \u00b1 0. 81 % 78 .6 6 \u00b1 0. 65 % 35 .5 6 \u00b1 0. 36 % 58 .6 2 \u00b1 0. 38 % C A C T U sPr ot oN et s (o ur s) ,P = 1 0 0 ,k = 5 0 0 54 .7 4 \u00b1 0. 82 % 71 .6 9 \u00b1 0. 73 % 33 .4 0 \u00b1 0. 37 % 50 .6 2 \u00b1 0. 39 % Su pe rv is ed m et ale ar ni ng O ra cl eM A M L (c on tr ol ) 94 .4 6 \u00b1 0. 35 % 98 .8 3 \u00b1 0. 12 % 84 .6 0 \u00b1 0. 32 % 96 .2 9 \u00b1 0. 13 % O ra cl ePr ot oN et s (c on tr ol ) 98 .3 5 \u00b1 0. 22 % 99 .5 8 \u00b1 0. 09 % 95 .3 1 \u00b1 0. 18 % 98 .8 1 \u00b1 0. 07 % O ra cl eM A M L \u2020 (F in n et al ., 20 17 ) 98 .7 \u00b1 0. 4 % 99 .9 \u00b1 0. 1 % 95 .8 \u00b1 0. 3 % 98 .9 \u00b1 0. 2 % \u2020 R es ul tu se d 64 fil te rs pe rc on vo lu tio na ll ay er ,3 \u00d7 da ta au gm en ta tio n, an d fo ld ed th e va lid at io n se ti nt o th e tr ai ni ng se t af te rh yp er pa ra m et er tu ni ng .\nTa bl\ne 10\n:m in\niI m\nag eN\net ob\nje ct\ncl as\nsi fic\nat io\nn re\nsu lts\nav er\nag ed\nov er\n10 00\nta sk\ns. Fo\nrc om\npa ri\nso n\npu rp\nos es\n,w e\non ly\nsh ow\nre su\nlts fr\nom pr\nio rs\nup er\nvi se\nd m\net a-\nle ar\nni ng\nm et\nho ds\nth at\nus e\nth e\nsa m\ne 4-\nbl oc\nk m\net a-\nle ar\nne ra\nrc hi\nte ct\nur e\nw e\nco ns\nid er\n.\u00b1 de\nno te\ns a\n95 %\nco nfi\nde nc\ne in\nte rv\nal .d\n:d im\nen si\non al\nity of\nem be\ndd in\ng, h\n:n um\nbe ro\nfh id\nde n\nun its\nin a\nfu lly\nco nn\nec te\nd la\nye r,\nk :n\num be\nro fc\nlu st\ner s\nin a\npa rt\niti on\n,P :n\num be\nro fp\nar tit\nio ns\nus ed\ndu ri\nng m\net a-\nle ar\nni ng\n,m :m\nar gi\nn on\nbo un\nda ry\n-d efi\nni ng\nhy pe\nrp la\nne s.\nA lg\nor ith\nm \\\n(w ay\n,s ho\nt) (5\n,1 )\n(5 ,5\n) (5\n,2 0)\n(5 ,5\n0)\nB as\nel in es Tr ai ni\nng fr\nom sc\nra tc\nh 27\n.5 9 \u00b1\n0. 59\n% 38\n.4 8 \u00b1\n0. 66\n% 51\n.5 3 \u00b1\n0. 72\n% 59\n.6 3 \u00b1\n0. 74\n%\nB iG\nA N\n,d =\n2 0 0\nE m\nbe dd\nin g k\nnn -n\nea re\nst ne\nig hb\nor s\n25 .5\n6 \u00b1\n1. 08\n% 31\n.1 0 \u00b1\n0. 63\n% 37\n.3 1 \u00b1\n0. 40\n% 43\n.6 0 \u00b1\n0. 37 % E m be dd in g lin ea rc la ss ifi er 27 .0 8 \u00b1 1. 24 % 33 .9 1 \u00b1 0. 64 % 44 .0 0 \u00b1 0. 45 % 50 .4 1 \u00b1 0. 37 % E m be dd in g M L P w ith dr op ou t, h = 1 2 8 22 .9 1 \u00b1 0. 54 % 29 .0 6 \u00b1 0. 63 % 40 .0 6 \u00b1 0. 72 % 48 .3 6 \u00b1 0. 71 % E m be dd in g cl us te rm at ch in g, k = 5 0 0 24 .6 3 \u00b1 0. 56 % 29 .4 9 \u00b1 0. 58 % 33 .8 9 \u00b1 0. 63 % 36 .1 3 \u00b1 0. 64 % H yp er pl an es -M A M L (o ur s) ,P = 4 8 0 0 ,m = 0 20 .0 0 \u00b1 0. 00 % 20 .0 0 \u00b1 0. 00 % 20 .0 0 \u00b1 0. 00 % 20 .0 0 \u00b1 0. 00 % H yp er pl an es -M A M L (o ur s) ,P = 4 8 0 0 ,m = 0 .9 29 .6 7 \u00b1 0. 64 % 41 .9 2 \u00b1 0. 69 % 51 .3 2 \u00b1 0. 71 % 54 .7 2 \u00b1 0. 71 % C A C T U sM A M L (o ur s) ,P = 1 ,k = 5 0 0 37 .7 5 \u00b1 0. 74 % 52 .5 9 \u00b1 0. 75 % 62 .7 0 \u00b1 0. 68 % 67 .9 8 \u00b1 0. 68 % C A C T U sM A M L (o ur s) ,P = 5 0 ,k = 5 0 0 36 .2 4 \u00b1 0. 74 % 51 .2 8 \u00b1 0. 68 % 61 .3 3 \u00b1 0. 67 % 66 .9 1 \u00b1 0. 68 % C A C T U sPr ot oN et s (o ur s) ,P = 5 0 ,k = 5 0 0 36 .6 2 \u00b1 0. 70 % 50 .1 6 \u00b1 0. 73 % 59 .5 6 \u00b1 0. 68 % 63 .2 7 \u00b1 0. 67 % D ee pC lu st er ,d = 2 5 6 E m be dd in g k nn -n ea re st ne ig hb or s 28 .9 0 \u00b1 1. 25 % 42 .2 5 \u00b1 0. 67 % 56 .4 4 \u00b1 0. 43 % 63 .9 0 \u00b1 0. 38 % E m be dd in g lin ea rc la ss ifi er 29 .4 4 \u00b1 1. 22 % 39 .7 9 \u00b1 0. 64 % 56 .1 9 \u00b1 0. 43 % 65 .2 8 \u00b1 0. 34 % E m be dd in g M L P w ith dr op ou t, h = 1 2 8 29 .0 3 \u00b1 0. 61 % 39 .6 7 \u00b1 0. 69 % 52 .7 1 \u00b1 0. 62 % 60 .9 5 \u00b1 0. 63 % E m be dd in g cl us te rm at ch in g, k = 5 0 0 22 .2 0 \u00b1 0. 50 % 23 .5 0 \u00b1 0. 52 % 24 .9 7 \u00b1 0. 54 % 26 .8 7 \u00b1 0. 55 % H yp er pl an es -M A M L (o ur s) ,P = 4 8 0 0 ,m = 0 20 .0 2 \u00b1 0. 06 % 20 .0 1 \u00b1 0. 01 % 20 .0 0 \u00b1 0. 01 % 20 .0 1 \u00b1 0. 02 % H yp er pl an es -M A M L (o ur s) ,P = 4 8 0 0 ,m = 0 .1 35 .8 5 \u00b1 0. 66 % 49 .5 4 \u00b1 0. 72 % 60 .6 8 \u00b1 0. 69 % 65 .5 5 \u00b1 0. 66 % C A C T U sM A M L (o ur s) ,P = 1 ,k = 5 0 0 38 .7 5 \u00b1 0. 70 % 52 .7 3 \u00b1 0. 72 % 62 .7 2 \u00b1 0. 69 % 67 .7 7 \u00b1 0. 62 % C A C T U sM A M L (o ur s) ,P = 5 0 ,k = 5 0 0 39 .9 0 \u00b1 0. 74 % 53 .9 7 \u00b1 0. 70 % 63 .8 4 \u00b1 0. 70 % 69 .6 4 \u00b1 0. 63 % C A C T U sPr ot oN et s (o ur s) ,P = 5 0 ,k = 5 0 0 39 .1 8 \u00b1 0. 71 % 53 .3 6 \u00b1 0. 70 % 61 .5 4 \u00b1 0. 68 % 63 .5 5 \u00b1 0. 64 % Su pe rv is ed m et ale ar ni ng O ra cl eM A M L (c on tr ol ) 46 .8 1 \u00b1 0. 77 % 62 .1 3 \u00b1 0. 72 % 71 .0 3 \u00b1 0. 69 % 75 .5 4 \u00b1 0. 62 % O ra cl ePr ot oN et s (c on tr ol ) 46 .5 6 \u00b1 0. 76 % 62 .2 9 \u00b1 0. 71 % 70 .0 5 \u00b1 0. 65 % 72 .0 4 \u00b1 0. 60 % O ra cl eM A M L (F in n et al ., 20 17 ) 48 .7 0 \u00b1 1. 84 % 63 .1 1 \u00b1 0. 92 % M et ale ar ne rL ST M (R av i& L ar oc he lle ,2 01 7) 43 .4 4 \u00b1 0. 77 % 60 .6 0 \u00b1 0. 71 % M at ch in g ne tw or k FC E (V in ya ls et al ., 20 16 ) 43 .5 6 \u00b1 0. 84 % 55 .3 1 \u00b1 0. 73 % Su pe rv is ed pr etr ai ni ng Fi ne tu ni ng (R av i& L ar oc he lle ,2 01 7) 28 .8 6 \u00b1 0. 54 % 49 .7 9 \u00b1 0. 79 % N ea re st ne ig hb or s (R av i& L ar oc he lle ,2 01 7) 41 .0 8 \u00b1 0. 70 % 51 .0 4 \u00b1 0. 65 %"
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "Learning a synaptic learning rule",
                    "authors": [
                        "Yoshua Bengio",
                        "Samy Bengio",
                        "Jocelyn Cloutier"
                    ],
                    "venue": "In International Joint Conference on Neural Networks (IJCNN),",
                    "year": 1991
                },
                {
                    "title": "Greedy layer-wise training of deep networks",
                    "authors": [
                        "Yoshua Bengio",
                        "Pascal Lamblin",
                        "Dan Popovici",
                        "Hugo Larochelle"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
                    "year": 2007
                },
                {
                    "title": "Representation learning: A review and new perspectives",
                    "authors": [
                        "Yoshua Bengio",
                        "Aaron Courville",
                        "Pascal Vincent"
                    ],
                    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
                    "year": 2013
                },
                {
                    "title": "Understanding and improving interpolation in autoencoders via an adversarial regularizer",
                    "authors": [
                        "David Berthelot",
                        "Colin Raffel",
                        "Aurko Roy",
                        "Ian Goodfellow"
                    ],
                    "venue": "arXiv preprint arXiv:1807.07543,",
                    "year": 2018
                },
                {
                    "title": "Unsupervised learning by predicting noise",
                    "authors": [
                        "Piotr Bojanowski",
                        "Armand Joulin"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2017
                },
                {
                    "title": "Deep clustering for unsupervised learning of visual features",
                    "authors": [
                        "Mathilde Caron",
                        "Piotr Bojanowski",
                        "Armand Joulin",
                        "Matthijs Douze"
                    ],
                    "venue": "In European Conference on Computer Vision (ECCV),",
                    "year": 2018
                },
                {
                    "title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
                    "authors": [
                        "Xi Chen",
                        "Yan Duan",
                        "Rein Houthooft",
                        "John Schulman",
                        "Ilya Sutskever",
                        "Pieter Abbeel"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2016
                },
                {
                    "title": "Discovering hidden factors of variation in deep networks",
                    "authors": [
                        "Brian Cheung",
                        "Jesse A Livezey",
                        "Arjun K Bansal",
                        "Bruno A Olshausen"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2015
                },
                {
                    "title": "Learning feature representations with k-means",
                    "authors": [
                        "Adam Coates",
                        "Andrew Y Ng"
                    ],
                    "venue": "In Neural Networks: Tricks of the Trade. Springer,",
                    "year": 2012
                },
                {
                    "title": "Semi-supervised sequence learning",
                    "authors": [
                        "Andrew M Dai",
                        "Quoc V Le"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2015
                },
                {
                    "title": "Unsupervised learning of disentangled representations from video",
                    "authors": [
                        "Emily L Denton",
                        "Vighnesh Birodkar"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2017
                },
                {
                    "title": "Adversarial feature learning",
                    "authors": [
                        "Jeff Donahue",
                        "Philipp Kr\u00e4henb\u00fchl",
                        "Trevor Darrell"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2017
                },
                {
                    "title": "Adversarially learned inference",
                    "authors": [
                        "Vincent Dumoulin",
                        "Ishmael Belghazi",
                        "Ben Poole",
                        "Olivier Mastropietro",
                        "Alex Lamb",
                        "Martin Arjovsky",
                        "Aaron Courville"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2017
                },
                {
                    "title": "Why does unsupervised pre-training help deep learning",
                    "authors": [
                        "Dumitru Erhan",
                        "Yoshua Bengio",
                        "Aaron Courville",
                        "Pierre-Antoine Manzagol",
                        "Pascal Vincent",
                        "Samy Bengio"
                    ],
                    "venue": "Journal of Machine Learning Research (JMLR),",
                    "year": 2010
                },
                {
                    "title": "Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm",
                    "authors": [
                        "Chelsea Finn",
                        "Sergey Levine"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2018
                },
                {
                    "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
                    "authors": [
                        "Chelsea Finn",
                        "Pieter Abbeel",
                        "Sergey Levine"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2017
                },
                {
                    "title": "Supervising unsupervised learning",
                    "authors": [
                        "Vikas K Garg",
                        "Adam Kalai"
                    ],
                    "venue": "arXiv preprint arXiv:1709.05262,",
                    "year": 2017
                },
                {
                    "title": "Unsupervised metalearning for reinforcement learning",
                    "authors": [
                        "Abhishek Gupta",
                        "Benjamin Eysenbach",
                        "Chelsea Finn",
                        "Sergey Levine"
                    ],
                    "venue": "arXiv preprint arXiv:1806.04640,",
                    "year": 2018
                },
                {
                    "title": "Unsupervised learning",
                    "authors": [
                        "Trevor Hastie",
                        "Robert Tibshirani",
                        "Jerome Friedman"
                    ],
                    "venue": "In The Elements of Statistical Learning. Springer,",
                    "year": 2009
                },
                {
                    "title": "\u03b2-VAE: Learning basic visual concepts with a constrained variational framework",
                    "authors": [
                        "Irina Higgins",
                        "Loic Matthey",
                        "Arka Pal",
                        "Christopher Burgess",
                        "Xavier Glorot",
                        "Matthew Botvinick",
                        "Shakir Mohamed",
                        "Alexander Lerchner"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2017
                },
                {
                    "title": "A fast learning algorithm for deep belief nets",
                    "authors": [
                        "Geoffrey E Hinton",
                        "Simon Osindero",
                        "Yee-Whye Teh"
                    ],
                    "venue": "Neural Computation,",
                    "year": 2006
                },
                {
                    "title": "Universal language model fine-tuning for text classification",
                    "authors": [
                        "Jeremy Howard",
                        "Sebastian Ruder"
                    ],
                    "venue": "In Association for Computational Linguistics (ACL),",
                    "year": 2018
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "Sergey Ioffe",
                        "Christian Szegedy"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2015
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "Diederik Kingma",
                        "Jimmy Ba"
                    ],
                    "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),",
                    "year": 2014
                },
                {
                    "title": "Semi-supervised learning with deep generative models",
                    "authors": [
                        "Diederik P Kingma",
                        "Shakir Mohamed",
                        "Danilo Jimenez Rezende",
                        "Max Welling"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2014
                },
                {
                    "title": "Data-dependent initializations of convolutional neural networks",
                    "authors": [
                        "Philipp Kr\u00e4henb\u00fchl",
                        "Carl Doersch",
                        "Jeff Donahue",
                        "Trevor Darrell"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2016
                },
                {
                    "title": "Building high-level features using large scale unsupervised learning",
                    "authors": [
                        "Quoc V Le",
                        "Marc\u2019Aurelio Ranzato",
                        "Rajat Monga",
                        "Matthieu Devin",
                        "Gregory S. Corrado",
                        "Kai Chen",
                        "Jeffrey Dean",
                        "Andrew Y Ng"
                    ],
                    "venue": "In International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
                    "year": 2013
                },
                {
                    "title": "Disentangling factors of variation in deep representation using adversarial training",
                    "authors": [
                        "Michael F Mathieu",
                        "Junbo Jake Zhao",
                        "Junbo Zhao",
                        "Aditya Ramesh",
                        "Pablo Sprechmann",
                        "Yann LeCun"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2016
                },
                {
                    "title": "Learning to learn without labels",
                    "authors": [
                        "Luke Metz",
                        "Niru Maheswaranathan",
                        "Brian Cheung",
                        "Jascha Sohl-Dickstein"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2018
                },
                {
                    "title": "Meta-neural networks that learn by learning",
                    "authors": [
                        "Devang K Naik",
                        "RJ Mammone"
                    ],
                    "venue": "In International Joint Conference on Neural Networks (IJCNN),",
                    "year": 1992
                },
                {
                    "title": "Realistic evaluation of deep semi-supervised learning algorithms",
                    "authors": [
                        "Avital Oliver",
                        "Augustus Odena",
                        "Colin Raffel",
                        "Ekin D Cubuk",
                        "Ian J Goodfellow"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2018
                },
                {
                    "title": "Representation learning with contrastive predictive coding",
                    "authors": [
                        "Aaron van den Oord",
                        "Yazhe Li",
                        "Oriol Vinyals"
                    ],
                    "venue": "arXiv preprint arXiv:1807.03748,",
                    "year": 2018
                },
                {
                    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                    "authors": [
                        "Alec Radford",
                        "Luke Metz",
                        "Soumith Chintala"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2016
                },
                {
                    "title": "Improving language understanding by generative pre-training",
                    "authors": [
                        "Alec Radford",
                        "Karthik Narasimhan",
                        "Tim Salimans",
                        "Ilya Sutskever"
                    ],
                    "year": 2018
                },
                {
                    "title": "Unsupervised pretraining for sequence to sequence learning",
                    "authors": [
                        "Prajit Ramachandran",
                        "Peter J Liu",
                        "Quoc V Le"
                    ],
                    "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
                    "year": 2017
                },
                {
                    "title": "Efficient learning of sparse representations with an energy-based model",
                    "authors": [
                        "Marc\u2019Aurelio Ranzato",
                        "Christopher Poultney",
                        "Sumit Chopra",
                        "Yann LeCun"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2006
                },
                {
                    "title": "Semisupervised learning with ladder networks",
                    "authors": [
                        "Antti Rasmus",
                        "Mathias Berglund",
                        "Mikko Honkala",
                        "Harri Valpola",
                        "Tapani Raiko"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2015
                },
                {
                    "title": "Optimization as a model for few-shot learning",
                    "authors": [
                        "Sachin Ravi",
                        "Hugo Larochelle"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2017
                },
                {
                    "title": "Learning to disentangle factors of variation with manifold interaction",
                    "authors": [
                        "Scott Reed",
                        "Kihyuk Sohn",
                        "Yuting Zhang",
                        "Honglak Lee"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2014
                },
                {
                    "title": "Improved techniques for training GANs",
                    "authors": [
                        "Tim Salimans",
                        "Ian Goodfellow",
                        "Wojciech Zaremba",
                        "Vicki Cheung",
                        "Alec Radford",
                        "Xi Chen"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2016
                },
                {
                    "title": "Metalearning with memory-augmented neural networks",
                    "authors": [
                        "Adam Santoro",
                        "Sergey Bartunov",
                        "Matthew Botvinick",
                        "Daan Wierstra",
                        "Timothy Lillicrap"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2016
                },
                {
                    "title": "Evolutionary principles in self-referential learning",
                    "authors": [
                        "J\u00fcrgen Schmidhuber"
                    ],
                    "venue": "PhD thesis, Institut fu\u0308r Informatik, Technische Universita\u0308t Mu\u0308nchen,",
                    "year": 1987
                },
                {
                    "title": "Loss is its own reward: Selfsupervision for reinforcement learning",
                    "authors": [
                        "Evan Shelhamer",
                        "Parsa Mahmoudieh",
                        "Max Argus",
                        "Trevor Darrell"
                    ],
                    "venue": "In International Conference on Learning Representations (ICLR),",
                    "year": 2017
                },
                {
                    "title": "Prototypical networks for few-shot learning",
                    "authors": [
                        "Jake Snell",
                        "Kevin Swersky",
                        "Richard S Zemel"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2017
                },
                {
                    "title": "Dropout: A simple way to prevent neural networks from overfitting",
                    "authors": [
                        "Nitish Srivastava",
                        "Geoffrey Hinton",
                        "Alex Krizhevsky",
                        "Ilya Sutskever",
                        "Ruslan Salakhutdinov"
                    ],
                    "venue": "Journal of Machine Learning Research (JMLR),",
                    "year": 2014
                },
                {
                    "title": "Extracting and composing robust features with denoising autoencoders",
                    "authors": [
                        "Pascal Vincent",
                        "Hugo Larochelle",
                        "Yoshua Bengio",
                        "Pierre-Antoine Manzagol"
                    ],
                    "venue": "In International Conference on Machine Learning (ICML),",
                    "year": 2008
                },
                {
                    "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
                    "authors": [
                        "Pascal Vincent",
                        "Hugo Larochelle",
                        "Isabelle Lajoie",
                        "Yoshua Bengio",
                        "Pierre-Antoine Manzagol"
                    ],
                    "venue": "Journal of Machine Learning Research (JMLR),",
                    "year": 2010
                },
                {
                    "title": "Matching networks for one shot learning",
                    "authors": [
                        "Oriol Vinyals",
                        "Charles Blundell",
                        "Timothy Lillicrap",
                        "Koray Kavukcuoglu",
                        "Daan Wierstra"
                    ],
                    "venue": "In Neural Information Processing Systems (NIPS),",
                    "year": 2016
                },
                {
                    "title": "Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition",
                    "authors": [
                        "Dong Yu",
                        "Li Deng",
                        "George Dahl"
                    ],
                    "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
                    "year": 2010
                },
                {
                    "title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction",
                    "authors": [
                        "Richard Zhang",
                        "Phillip Isola",
                        "Alexei A Efros"
                    ],
                    "venue": "In Computer Vision and Pattern Recognition",
                    "year": 2017
                },
                {
                    "title": "Semi-supervised learning",
                    "authors": [
                        "Xiaojin Zhu"
                    ],
                    "venue": "In Encyclopedia of Machine Learning. Springer,",
                    "year": 2011
                },
                {
                    "title": "ImageNet results. We build upon the authors\u2019 publicly available codebase found at https://github.com/jeffdonahue/bigan. DeepCluster (Caron et al., 2018): We run DeepCluster for miniImageNet and CelebA, which we respectively randomly crop and resize to 64\u00d764. We modify the first layer of the AlexNet architecture used by the authors",
                    "authors": [
                        "Donahue"
                    ],
                    "year": 2018
                }
            ],
            "id": "SP:dba97d18e2ae2a2488e2884766a8f444d6126dc7",
            "authors": [
                {
                    "name": "Kyle Hsu",
                    "affiliations": []
                },
                {
                    "name": "Sergey Levine",
                    "affiliations": []
                },
                {
                    "name": "Chelsea Finn",
                    "affiliations": []
                }
            ],
            "abstractText": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering unsupervised representations, lead to good performance on a variety of downstream tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the representation learned by four prior unsupervised learning methods."
        }
    },
    "81623492": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "Convolutional neural networks (CNNs) have proven to be effective models for tackling a variety of visual tasks [19, 23, 29, 41]. For each convolutional layer, a set of filters are learned to express local spatial connectivity patterns along input channels. In other words, convolutional filters are expected to be informative combinations by fusing spatial and channel-wise information together, while restricted in local receptive fields. By stacking a series of convolutional layers interleaved with non-linearities and downsampling, CNNs are capable of capturing hierarchical patterns with global receptive fields as powerful image descriptions. Recent work has demonstrated the performance of networks can be improved by explicitly embedding learning mechanisms that help capture spatial correlations without\n\u2217Equal contribution.\nrequiring additional supervision. One such approach was popularised by the Inception architectures [14, 39], which showed that the network can achieve competitive accuracy by embedding multi-scale processes in its modules. More recent work has sought to better model spatial dependence [1, 27] and incorporate spatial attention [17].\nIn contrast to these methods, we investigate a different aspect of architectural design - the channel relationship, by introducing a new architectural unit, which we term the \u201cSqueeze-and-Excitation\u201d (SE) block. Our goal is to improve the representational power of a network by explicitly modelling the interdependencies between the channels of its convolutional features. To achieve this, we propose a mechanism that allows the network to perform feature recalibration, through which it can learn to use global information to selectively emphasise informative features and suppress less useful ones.\nThe basic structure of the SE building block is illustrated in Fig. 1. For any given transformation Ftr : X \u2192 U, X \u2208 RW \u2032\u00d7H\u2032\u00d7C\u2032 ,U \u2208 RW\u00d7H\u00d7C , (e.g. a convolution or a set of convolutions), we can construct a corresponding SE block to perform feature recalibration as follows. The features U are first passed through a squeeze operation, which aggregates the feature maps across spatial dimensions W \u00d7 H to produce a channel descriptor. This descriptor embeds the global distribution of channel-wise feature responses, enabling information from the global receptive field of the network to be leveraged by its lower layers. This is followed by an excitation operation, in which sample-specific activations, learned for each channel by a self-gating mechanism based on channel dependence, govern the excitation of each channel. The feature maps U are then reweighted to generate the output of the SE block which can then be fed directly into subsequent layers.\nAn SE network can be generated by simply stacking a collection of SE building blocks. SE blocks can also be used as a drop-in replacement for the original block at any depth in the architecture. However, while the template for the building block is generic, as we show in Sec. 6.3, the role it performs at different depths adapts to the needs of the network. In the early layers, it learns to excite informative\nar X\niv :1\n70 9.\n01 50\n7v 1\n[ cs\n.C V\n] 5\nS ep\n2 01\n7\nfeatures in a class agnostic manner, bolstering the quality of the shared lower level representations. In later layers, the SE block becomes increasingly specialised, and responds to different inputs in a highly class-specific manner. Consequently, the benefits of feature recalibration conducted by SE blocks can be accumulated through the entire network.\nThe development of new CNN architectures is a challenging engineering task, typically involving the selection of many new hyperparameters and layer configurations. By contrast, the design of the SE block outlined above is simple, and can be used directly with existing state-of-the-art architectures whose convolutional layers can be strengthened by direct replacement with their SE counterparts. Moreover, as shown in Sec. 4, SE blocks are computationally lightweight and impose only a slight increase in model complexity and computational burden. To support these claims, we develop several SENets, namely SE-ResNet, SEInception, SE-ResNeXt and SE-Inception-ResNet and provide an extensive evaluation of SENets on the ImageNet 2012 dataset [30]. Further, to demonstrate the general applicability of SE blocks, we also present results beyond ImageNet, indicating that the proposed approach is not restricted to a specific dataset or a task.\nUsing SENets, we won the first place in the ILSVRC 2017 classification competition. Our top performing model ensemble achieves a 2.251% top-5 error on the test set. This represents a \u223c25% relative improvement in comparison to the winner entry of the previous year (with a top-5 error of 2.991%). Our models and related materials have been made available to the research community1."
                },
                {
                    "heading": "2. Related Work",
                    "text": "Deep architectures. A wide range of work has shown that restructuring the architecture of a convolutional neural network in a manner that eases the learning of deep features can yield substantial improvements in performance. VGGNets [35] and Inception models [39] demonstrated the benefits that could be attained with an increased depth, significantly outperforming previous approaches on ILSVRC 2014. Batch normalization (BN) [14] improved gradient\n1https://github.com/hujie-frank/SENet\npropagation through deep networks by inserting units to regulate layer inputs stabilising the learning process, which enables further experimentation with a greater depth. He et al. [9, 10] showed that it was effective to train deeper networks by restructuring the architecture to learn residual functions through the use of identity-based skip connections which ease the flow of information across units. More recently, reformulations of the connections between network layers [5, 12] have been shown to further improve the learning and representational properties of deep networks.\nAn alternative line of research has explored ways to tune the functional form of the modular components of a network. Grouped convolutions can be used to increase cardinality (the size of the set of transformations) [13, 43] to learn richer representations. Multi-branch convolutions can be interpreted as a generalisation of this concept, enabling more flexible compositions of convolutional operators [14, 38, 39, 40]. Cross-channel correlations are typically mapped as new combinations of features, either independently of spatial structure [6, 18] or jointly by using standard convolutional filters [22] with 1\u00d7 1 convolutions, while much of this work has concentrated on the objective of reducing model and computational complexity. This approach reflects an assumption that channel relationships can be formulated as a composition of instance-agnostic functions with local receptive fields. In contrast, we claim that providing the network with a mechanism to explicitly model dynamic, non-linear dependencies between channels using global information can ease the learning process, and significantly enhance the representational power of the network.\nAttention and gating mechanisms. Attention can be viewed, broadly, as a tool to bias the allocation of available processing resources towards the most informative components of an input signal. The development and understanding of such mechanisms has been a longstanding area of research in the neuroscience community [15, 16, 28] and has seen significant interest in recent years as a powerful addition to deep neural networks [20, 25]. Attention has been shown to improve performance across a range of tasks, from localisation and understanding in images [3, 17] to sequence-based models [2, 24]. It is typically implemented in combination with a gating function (e.g. a softmax or\nsigmoid) and sequential techniques [11, 37]. Recent work has shown its applicability to tasks such as image captioning [4, 44] and lip reading [7], in which it is exploited to efficiently aggregate multi-modal data. In these applications, it is typically used on top of one or more layers representing higher-level abstractions for adaptation between modalities. Highway networks [36] employ a gating mechanism to regulate the shortcut connection, enabling the learning of very deep architectures. Wang et al. [42] introduce a powerful trunk-and-mask attention mechanism using an hourglass module [27], inspired by its success in semantic segmentation. This high capacity unit is inserted into deep residual networks between intermediate stages. In contrast, our proposed SE-block is a lightweight gating mechanism, specialised to model channel-wise relationships in a computationally efficient manner and designed to enhance the representational power of modules throughout the network."
                },
                {
                    "heading": "3. Squeeze-and-Excitation Blocks",
                    "text": "The Squeeze-and-Excitation block is a computational unit which can be constructed for any given transformation Ftr : X \u2192 U, X \u2208 RW\n\u2032\u00d7H\u2032\u00d7C\u2032 ,U \u2208 RW\u00d7H\u00d7C . For simplicity of exposition, in the notation that follows we take Ftr to be a standard convolutional operator. Let V = [v1,v2, . . . ,vC ] denote the learned set of filter kernels, where vc refers to the parameters of the c-th filter. We can then write the outputs of Ftr as U = [u1,u2, . . . ,uC ] where\nuc = vc \u2217X = C\u2032\u2211 s=1 vsc \u2217 xs. (1)\nHere \u2217 denotes convolution, vc = [v1c ,v2c , . . . ,vC \u2032\nc ] and X = [x1,x2, . . . ,xC \u2032 ] (to simplify the notation, bias terms are omitted). Here vsc is a 2D spatial kernel, and therefore represents a single channel of vc which acts on the corresponding channel of X. Since the output is produced by a summation through all channels, the channel dependencies are implicitly embedded in vc, but these dependencies are entangled with the spatial correlation captured by the filters. Our goal is to ensure that the network is able to increase its sensitivity to informative features so that they can be exploited by subsequent transformations, and to suppress less useful ones. We propose to achieve this by explicitly modelling channel interdependencies to recalibrate filter responses in two steps, squeeze and excitation, before they are fed into next transformation. A diagram of an SE building block is shown in Fig. 1."
                },
                {
                    "heading": "3.1. Squeeze: Global Information Embedding",
                    "text": "In order to tackle the issue of exploiting channel dependencies, we first consider the signal to each channel in the output features. Each of the learned filters operate with a\nlocal receptive field and consequently each unit of the transformation output U is unable to exploit contextual information outside of this region. This is an issue that becomes more severe in the lower layers of the network whose receptive field sizes are small.\nTo mitigate this problem, we propose to squeeze global spatial information into a channel descriptor. This is achieved by using global average pooling to generate channel-wise statistics. Formally, a statistic z \u2208 RC is generated by shrinking U through spatial dimensions W \u00d7H , where the c-th element of z is calculated by:\nzc = Fsq(uc) = 1\nW \u00d7H W\u2211 i=1 H\u2211 j=1 uc(i, j). (2)\nDiscussion. The transformation output U can be interpreted as a collection of the local descriptors whose statistics are expressive for the whole image. Exploiting such information is prevalent in feature engineering work [31, 34, 45]. We opt for the simplest, global average pooling, while more sophisticated aggregation strategies could be employed here as well."
                },
                {
                    "heading": "3.2. Excitation: Adaptive Recalibration",
                    "text": "To make use of the information aggregated in the squeeze operation, we follow it with a second operation which aims to fully capture channel-wise dependencies. To fulfil this objective, the function must meet two criteria: first, it must be flexible (in particular, it must be capable of learning a nonlinear interaction between channels) and second, it must learn a non-mutually-exclusive relationship as multiple channels are allowed to be emphasised opposed to onehot activation. To meet these criteria, we opt to employ a simple gating mechanism with a sigmoid activation:\ns = Fex(z,W) = \u03c3(g(z,W)) = \u03c3(W2\u03b4(W1z)), (3)\nwhere \u03b4 refers to the ReLU [26] function, W1 \u2208 R C r \u00d7C and W2 \u2208 RC\u00d7 C r . To limit model complexity and aid generalisation, we parameterise the gating mechanism by forming a bottleneck with two fully-connected (FC) layers around the non-linearity, i.e. a dimensionality-reduction layer with parameters W1 with reduction ratio r (we set it to be 16, and this parameter choice is discussed in Sec. 6.3), a ReLU and then a dimensionality-increasing layer with parameters W2. The final output of the block is obtained by rescaling the transformation output U with the activations:\nx\u0303c = Fscale(uc, sc) = sc \u00b7 uc, (4)\nwhere X\u0303 = [x\u03031, x\u03032, . . . , x\u0303C ] and Fscale(uc, sc) refers to channel-wise multiplication between the feature map uc \u2208 RW\u00d7H and the scalar sc.\nDiscussion. The activations act as channel weights adapted to the input-specific descriptor z. In this regard, SE blocks intrinsically introduce dynamics conditioned on the input, helping to boost feature discriminability."
                },
                {
                    "heading": "3.3. Exemplars: SE-Inception and SE-ResNet",
                    "text": "The flexibility of the SE block means that it can be directly applied to transformations beyond standard convolutions. To illustrate this point, we develop SENets by integrating SE blocks into two popular network families of architectures, Inception and ResNet. SE blocks are constructed for the Inception network by taking the transformation Ftr to be an entire Inception module (see Fig. 2). By making this change for each such module in the architecture, we construct an SE-Inception network.\nResidual networks and their variants have shown to be highly effective at learning deep representations. We de-\nvelop a series of SE blocks that integrate with ResNet [9], ResNeXt [43] and Inception-ResNet [38] respectively. Fig. 3 depicts the schema of an SE-ResNet module. Here, the SE block transformation Ftr is taken to be the nonidentity branch of a residual module. Squeeze and excitation both act before summation with the identity branch."
                },
                {
                    "heading": "4. Model and Computational Complexity",
                    "text": "An SENet is constructed by stacking a set of SE blocks. In practice, it is generated by replacing each original block (i.e. residual block) with its corresponding SE counterpart (i.e. SE-residual block). We describe the architecture of SE-ResNet-50 and SE-ResNeXt-50 in Table 1.\nFor the proposed SE block to be viable in practice, it must provide an acceptable model complexity and computational overhead which is important for scalability. To illustrate the cost of the module, we take the comparison between ResNet-50 and SE-ResNet-50 as an example, where the accuracy of SE-ResNet-50 is obviously superior to ResNet-50 and approaching a deeper ResNet-101 network (shown in Table 2). ResNet-50 requires \u223c3.86 GFLOPs in a single forward pass for a 224 \u00d7 224 pixel input image. Each SE block makes use of a global average pooling operation in the squeeze phase and two small fully connected layers in the excitation phase, followed by an inexpensive channel-wise scaling operation. In aggregate, SE-ResNet-50 requires \u223c3.87 GFLOPs, corresponding to only a 0.26% relative increase over the original ResNet-50.\nIn practice, with a training mini-batch of 256 images, a single pass forwards and backwards through ResNet-50 takes 190ms, compared to 209ms for SE-ResNet-50 (both timings are performed on a server with 8 NVIDIA Titan X GPUs). We argue that it is a reasonable overhead as global pooling and small inner-product operations are less optimised in existing GPU libraries. Moreover, due to its importance for embedded device applications, we also benchmark CPU inference time for each model: for a 224\u00d7224 pixel input image, ResNet-50 takes 164ms, compared to 167ms for SE-ResNet-50. The small additional computational overhead required by the SE block is justified by its contribution to model performance (discussed in detail in Sec. 6).\nNext, we consider the additional parameters introduced by the proposed block. All additional parameters are contained in the two fully connected layers of the gating mechanism, which constitute a small fraction of the total network capacity. More precisely, the number of additional parameters introduced is given by:\n2\nr S\u2211 s=1 Ns \u00b7 Cs2 (5)\nwhere r denotes the reduction ratio (we set r to 16 in all our experiments), S refers to the number of stages (where each\nstage refers to the collection of blocks operating on feature maps of a common spatial dimension), Cs denotes the dimension of the output channels for stage s and Ns refers to the repeated block number. In total, SE-ResNet-50 introduces \u223c2.5 million additional parameters beyond the \u223c25 million parameters required by ResNet-50, corresponding to a \u223c10% increase in the total number of parameters. The majority of these additional parameters come from the last stage of the network, where excitation is performed across the greatest channel dimensions. However, we found that the comparatively expensive final stage of SE blocks could be removed at a marginal cost in performance (<0.1% top-1 error on ImageNet dataset) to reduce the relative parameter increase to \u223c4%, which may prove useful in cases where parameter usage is a key consideration."
                },
                {
                    "heading": "5. Implementation",
                    "text": "During training, we follow standard practice and perform data augmentation with random-size cropping [39] to 224\u00d7 224 pixels (299\u00d7 299 for Inception-ResNet-v2 [38] and SE-Inception-ResNet-v2) and random horizontal flipping. Input images are normalised through mean channel subtraction. In addition, we adopt the data balancing strategy described in [32] for mini-batch sampling to compensate for the uneven distribution of classes. The networks are trained on our distributed learning system \u201cROCS\u201d which is capable of handing efficient parallel training of large networks. Optimisation is performed using synchronous SGD with momentum 0.9 and a mini-batch size of 1024 (split into sub-batches of 32 images per GPU across 4 servers, each containing 8 GPUs). The initial learning rate is set to\n0.6 and decreased by a factor of 10 every 30 epochs. All models are trained for 100 epochs from scratch, using the weight initialisation strategy described in [8]."
                },
                {
                    "heading": "6. Experiments",
                    "text": "In this section we conduct extensive experiments on the ImageNet 2012 dataset [30] for the purposes: first, to explore the impact of the proposed SE block for the basic networks with different depths and second, to investigate its capacity of integrating with current state-of-the-art network architectures, which aim to a fair comparison between SENets and non-SENets rather than pushing the performance. Next, we present the results and details of the models for ILSVRC 2017 classification task. Furthermore, we perform experiments on the Places365-Challenge scene classification dataset [48] to investigate how well SENets are able to generalise to other datasets. Finally, we investigate the role of excitation and give some analysis based on experimental phenomena."
                },
                {
                    "heading": "6.1. ImageNet Classification",
                    "text": "The ImageNet 2012 dataset is comprised of 1.28 million training images and 50K validation images from 1000 classes. We train networks on the training set and report the top-1 and the top-5 errors using centre crop evaluations on the validation set, where 224 \u00d7 224 pixels are cropped from each image whose shorter edge is first resized to 256 (299 \u00d7 299 from each image whose shorter edge is first resized to 352 for Inception-ResNet-v2 and SE-InceptionResNet-v2).\nNetwork depth. We first compare the SE-ResNet against a collection of standard ResNet architectures. Each ResNet and its corresponding SE-ResNet are trained with identical optimisation schemes. The performance of the different networks on the validation set is shown in Table 2, which shows that SE blocks consistently improve performance across different depths with an extremely small increase in computational complexity.\nRemarkably, SE-ResNet-50 achieves a single-crop top-5 validation error of 6.62%, exceeding ResNet-50 (7.48%) by 0.86% and approaching the performance achieved by the much deeper ResNet-101 network (6.52% top-5 error) with only half of the computational overhead (3.87 GFLOPs vs. 7.58 GFLOPs). This pattern is repeated at greater depth, where SE-ResNet-101 (6.07% top-5 error) not only matches, but outperforms the deeper ResNet-152 network (6.34% top-5 error) by 0.27%. Fig. 4 depicts the training and validation curves of SE-ResNets and ResNets, respectively. While it should be noted that the SE blocks themselves add depth, they do so in an extremely computationally efficient manner and yield good returns even at the point at which extending the depth of the base architecture achieves diminishing returns. Moreover, we see that the performance improvements are consistent through training across a range of different depths, suggesting that the improvements induced by SE blocks can be used in combination with adding more depth to the base architecture.\nIntegration with modern architectures. We next investigate the effect of combining SE blocks with another two state-of-the-art architectures, Inception-ResNet-v2 [38] and ResNeXt [43]. The Inception architecture constructs modules of convolutions as multibranch combinations of factorised filters, reflecting the Inception hypothesis [6] that spatial correlations and cross-channel correlations can be mapped independently. In contrast, the ResNeXt architecture asserts that richer representations can be obtained by aggregating combinations of sparsely connected (in the channel dimension) convolutional features. Both approaches introduce prior-structured correlations in modules. We construct SENet equivalents of these networks, SEInception-ResNet-v2 and SE-ResNeXt (the configuration of SE-ResNeXt-50 (32\u00d74d) is given in Table 1). Like previous experiments, the same optimisation scheme is used for\nboth the original networks and their SENet counterparts. The results given in Table 2 illustrate the significant performance improvement induced by SE blocks when introduced into both architectures. In particular, SE-ResNeXt-50 has a top-5 error of 5.49% which is superior to both its direct counterpart ResNeXt-50 (5.90% top-5 error) as well as the deeper ResNeXt-101 (5.57% top-5 error), a model which has almost double the number of parameters and computational overhead. As for the experiments of Inception-ResNet-v2, we conjecture the difference of cropping strategy might lead to the gap between their reported result and our re-implemented one, as their original image size has not been clarified in [38] while we crop the 299 \u00d7 299 region from a relative larger image (where the shorter edge is resized to 352). SE-InceptionResNet-v2 (4.79% top-5 error) outperforms our reimplemented Inception-ResNet-v2 (5.21% top-5 error) by 0.42% (a relative improvement of 8.1%) as well as the reported result in [38]. The optimisation curves for each network are depicted in Fig. 5, illustrating the consistency of the improvement yielded by SE blocks throughout the training process.\nFinally, we assess the effect of SE blocks when operating on a non-residual network by conducting experiments with the BN-Inception architecture [14] which provides good performance at a lower model complexity. The results of the comparison are shown in Table 2 and the training curves are shown in Fig. 6, exhibiting the same phenomena that emerged in the residual architectures. In particular, SE-BNInception achieves a lower top-5 error of 7.14% in comparison to BN-Inception whose error rate is 7.89%. These experiments demonstrate that improvements induced by SE blocks can be used in combination with a wide range of architectures. Moreover, this result holds for both residual and non-residual foundations.\nResults on ILSVRC 2017 Classification Competition. ILSVRC [30] is an annual computer vision competition which has proved to be a fertile ground for model developments in image classification. The training and validation data of the ILSVRC 2017 classification task are drawn from the ImageNet 2012 dataset, while the test set consists of an additional unlabelled 100K images. For the purposes of the competition, the top-5 error metric is used to rank entries.\nSENets formed the foundation of our submission to the challenge where we won first place. Our winning entry comprised a small ensemble of SENets that employed a standard multi-scale and multi-crop fusion strategy to obtain a 2.251% top-5 error on the test set. This result represents a\u223c25% relative improvement on the winning entry of 2016 (2.99% top-5 error). One of our high-performing networks is constructed by integrating SE blocks with a modified ResNeXt [43] (details of the modifications are provided in Appendix A). We compare the proposed architecture with\nthe state-of-the-art models on the ImageNet validation set in Table 3. Our model achieves a top-1 error of 18.68% and a top-5 error of 4.47% using a 224\u00d7 224 centre crop evaluation on each image (where the shorter edge is first resized to 256). To enable a fair comparison with previous models, we also provide a 320 \u00d7 320 centre crop evaluation, obtaining the lowest error rate under both the top-1 (17.28%) and the top-5 (3.79%) error metrics."
                },
                {
                    "heading": "6.2. Scene Classification",
                    "text": "Large portions of the ImageNet dataset consist of images dominated by single objects. To evaluate our proposed model in more diverse scenarios, we also evaluate it on the Places365-Challenge dataset [48] for scene classifica-\ntion. This dataset comprises 8 million training images and 36, 500 validation images across 365 categories. Relative to classification, the task of scene understanding can provide a better assessment of the ability of a model to generalise well and handle abstraction, since it requires the capture of more complex data associations and robustness to a greater level of appearance variation.\nWe use ResNet-152 as a strong baseline to assess the effectiveness of SE blocks and follow the evaluation protocol in [33]. Table 4 shows the results of training a ResNet-152 model and a SE-ResNet-152 for the given task. Specifically, SE-ResNet-152 (11.01% top-5 error) achieves a lower validation error than ResNet-152 (11.61% top-5 error), providing evidence that SE blocks can perform well on different datasets. This SENet surpasses the previous state-of-theart model Places-365-CNN [33] which has a top-5 error of 11.48% on this task."
                },
                {
                    "heading": "6.3. Analysis and Discussion",
                    "text": "Reduction ratio. The reduction ratio r introduced in Eqn. (5) is an important hyperparameter which allows us to vary the capacity and computational cost of the SE blocks in the model. To investigate this relationship, we conduct experiments based on the SE-ResNet-50 architecture for a range of different r values. The comparison in Table 5 reveals that performance does not improve monotonically with increased capacity. This is likely to be a result of enabling the SE block to overfit the channel interdependencies of the training set. In particular, we found that setting r = 16 achieved a good tradeoff between accuracy and complexity and consequently, we used this value for all experiments.\nThe role of Excitation. While SE blocks have been empirically shown to improve network performance, we would\nalso like to understand how the self-gating excitation mechanism operates in practice. To provide a clearer picture of the behaviour of SE blocks, in this section we study example activations from the SE-ResNet-50 model and examine their distribution with respect to different classes at different blocks. Specifically, we sample four classes from the ImageNet dataset that exhibit semantic and appearance diversity, namely goldfish, pug, plane and cliff (example images from these classes are shown in Fig. 7). We then draw fifty samples for each class from the validation set and compute the average activations for fifty uniformly sampled channels in the last SE block in each stage (immediately prior to downsampling) and plot their distribution in Fig. 8. For reference, we also plot the distribution of average activations across all 1000 classes.\nWe make the following three observations about the role of Excitation in SENets. First, the distribution across different classes is nearly identical in lower layers, e.g. SE 2 3. This suggests that the importance of feature channels is likely to be shared by different classes in the early stages of the network. Interestingly however, the second observation is that at greater depth, the value of each channel becomes much more class-specific as different classes exhibit different preferences to the discriminative value of features e.g. SE 4 6 and SE 5 1. The two observations are consistent with findings in previous work [21, 46], namely that lower layer features are typically more general (i.e. class agnostic in the context of classification) while higher layer features have greater specificity. As a result, representation learning benefits from the recalibration induced by SE blocks which adaptively facilitates feature extraction and specialisation to the extent that it is needed. Finally, we observe a somewhat different phenomena in the last stage of the network. SE 5 2 exhibits an interesting tendency towards a saturated state in which most of the activations are close to 1 and the remainder are close to 0. At the point at which all activations take the value 1, this block would become a standard residual block. At the end of the network in the SE 5 3 (which is immediately followed by global pooling prior before classifiers), a similar pattern emerges over different classes, up to a slight change in scale (which could be tuned by the classifiers). This suggests that SE 5 2 and SE 5 3 are less important than previous blocks in providing recalibration to the network. This finding is consistent\nwith the result of the empirical investigation in Sec. 4 which demonstrated that the overall parameter count could be significantly reduced by removing the SE blocks for the last stage with only a marginal loss of performance (< 0.1% top-1 error)."
                },
                {
                    "heading": "7. Conclusion",
                    "text": "In this paper we proposed the SE block, a novel architectural unit designed to improve the representational capacity of a network by enabling it to perform dynamic channelwise feature recalibration. Extensive experiments demonstrate the effectiveness of SENets which achieve state-ofthe-art performance on multiple datasets. In addition, they provide some insight into the limitations of previous architectures in modelling channel-wise feature dependencies, which we hope may prove useful for other tasks requiring strong discriminative features. Finally, the feature importance induced by SE blocks may be helpful to related fields such as network pruning for compression.\nAcknowledgements. We would like to thank Professor Andrew Zisserman for his helpful comments and Samuel Albanie for his discussions and writing edit for the paper. We would like to thank Chao Li for his contributions in the memory optimisation of the training system. Li Shen is supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract number 2014-14071600010. The views and conclusions contained herein are those of the author and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon."
                }
            ],
            "year": 2017,
            "references": [
                {
                    "title": "Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks",
                    "authors": [
                        "S. Bell",
                        "C.L. Zitnick",
                        "K. Bala",
                        "R. Girshick"
                    ],
                    "venue": "CVPR,",
                    "year": 2016
                },
                {
                    "title": "Joint line segmentation and transcription for endto-end handwritten paragraph recognition",
                    "authors": [
                        "T. Bluche"
                    ],
                    "venue": "NIPS,",
                    "year": 2016
                },
                {
                    "title": "Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks",
                    "authors": [
                        "C. Cao",
                        "X. Liu",
                        "Y. Yang",
                        "Y. Yu",
                        "J. Wang",
                        "Z. Wang",
                        "Y. Huang",
                        "L. Wang",
                        "C. Huang",
                        "W. Xu",
                        "D. Ramanan",
                        "T.S. Huang"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                },
                {
                    "title": "SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning",
                    "authors": [
                        "L. Chen",
                        "H. Zhang",
                        "J. Xiao",
                        "L. Nie",
                        "J. Shao",
                        "W. Liu",
                        "T. Chua"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Dual path networks",
                    "authors": [
                        "Y. Chen",
                        "J. Li",
                        "H. Xiao",
                        "X. Jin",
                        "S. Yan",
                        "J. Feng"
                    ],
                    "venue": "arXiv:1707.01629,",
                    "year": 2017
                },
                {
                    "title": "Xception: Deep learning with depthwise separable convolutions",
                    "authors": [
                        "F. Chollet"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Lip reading sentences in the wild",
                    "authors": [
                        "J.S. Chung",
                        "A. Senior",
                        "O. Vinyals",
                        "A. Zisserman"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "ICCV,",
                    "year": 2015
                },
                {
                    "title": "Deep residual learning for image recognition",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "CVPR,",
                    "year": 2016
                },
                {
                    "title": "Identity mappings in deep residual networks",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "ECCV,",
                    "year": 2016
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "S. Hochreiter",
                        "J. Schmidhuber"
                    ],
                    "venue": "Neural computation,",
                    "year": 1997
                },
                {
                    "title": "Densely connected convolutional networks",
                    "authors": [
                        "G. Huang",
                        "Z. Liu",
                        "K.Q. Weinberger",
                        "L. Maaten"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Deep roots: Improving CNN efficiency with hierarchical filter groups",
                    "authors": [
                        "Y. Ioannou",
                        "D. Robertson",
                        "R. Cipolla",
                        "A. Criminisi"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "S. Ioffe",
                        "C. Szegedy"
                    ],
                    "venue": "ICML,",
                    "year": 2015
                },
                {
                    "title": "Computational modelling of visual attention",
                    "authors": [
                        "L. Itti",
                        "C. Koch"
                    ],
                    "venue": "Nature reviews neuroscience,",
                    "year": 2001
                },
                {
                    "title": "A model of saliency-based visual attention for rapid scene analysis",
                    "authors": [
                        "L. Itti",
                        "C. Koch",
                        "E. Niebur"
                    ],
                    "venue": "IEEE TPAMI,",
                    "year": 1998
                },
                {
                    "title": "Spatial transformer networks",
                    "authors": [
                        "M. Jaderberg",
                        "K. Simonyan",
                        "A. Zisserman",
                        "K. Kavukcuoglu"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Speeding up convolutional neural networks with low rank expansions",
                    "authors": [
                        "M. Jaderberg",
                        "A. Vedaldi",
                        "A. Zisserman"
                    ],
                    "venue": "BMVC,",
                    "year": 2014
                },
                {
                    "title": "ImageNet classification with deep convolutional neural networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS,",
                    "year": 2012
                },
                {
                    "title": "Learning to combine foveal glimpses with a third-order boltzmann machine",
                    "authors": [
                        "H. Larochelle",
                        "G.E. Hinton"
                    ],
                    "venue": "NIPS,",
                    "year": 2010
                },
                {
                    "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
                    "authors": [
                        "H. Lee",
                        "R. Grosse",
                        "R. Ranganath",
                        "A.Y. Ng"
                    ],
                    "venue": "ICML,",
                    "year": 2009
                },
                {
                    "title": "Network in network",
                    "authors": [
                        "M. Lin",
                        "Q. Chen",
                        "S. Yan"
                    ],
                    "venue": "arXiv:1312.4400,",
                    "year": 2013
                },
                {
                    "title": "Fully convolutional networks for semantic segmentation",
                    "authors": [
                        "J. Long",
                        "E. Shelhamer",
                        "T. Darrell"
                    ],
                    "venue": "CVPR,",
                    "year": 2015
                },
                {
                    "title": "Learnable pooling with context gating for video classification",
                    "authors": [
                        "A. Miech",
                        "I. Laptev",
                        "J. Sivic"
                    ],
                    "venue": "arXiv:1706.06905,",
                    "year": 2017
                },
                {
                    "title": "Recurrent models of visual attention",
                    "authors": [
                        "V. Mnih",
                        "N. Heess",
                        "A. Graves",
                        "K. Kavukcuoglu"
                    ],
                    "venue": "NIPS,",
                    "year": 2014
                },
                {
                    "title": "Rectified linear units improve restricted boltzmann machines",
                    "authors": [
                        "V. Nair",
                        "G.E. Hinton"
                    ],
                    "venue": "ICML,",
                    "year": 2010
                },
                {
                    "title": "Stacked hourglass networks for human pose estimation",
                    "authors": [
                        "A. Newell",
                        "K. Yang",
                        "J. Deng"
                    ],
                    "venue": "ECCV,",
                    "year": 2016
                },
                {
                    "title": "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information",
                    "authors": [
                        "B.A. Olshausen",
                        "C.H. Anderson",
                        "D.C.V. Essen"
                    ],
                    "venue": "Journal of Neuroscience,",
                    "year": 1993
                },
                {
                    "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
                    "authors": [
                        "S. Ren",
                        "K. He",
                        "R. Girshick",
                        "J. Sun"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "ImageNet large scale visual recognition challenge",
                    "authors": [
                        "O. Russakovsky",
                        "J. Deng",
                        "H. Su",
                        "J. Krause",
                        "S. Satheesh",
                        "S. Ma",
                        "Z. Huang",
                        "A. Karpathy",
                        "A. Khosla",
                        "M. Bernstein",
                        "A.C. Berg",
                        "L. Fei-Fei"
                    ],
                    "venue": "IJCV,",
                    "year": 2015
                },
                {
                    "title": "Image classification with the fisher vector: Theory and practice",
                    "authors": [
                        "J. Sanchez",
                        "F. Perronnin",
                        "T. Mensink",
                        "J. Verbeek"
                    ],
                    "venue": "RR-8209, INRIA,",
                    "year": 2013
                },
                {
                    "title": "Relay backpropagation for effective learning of deep convolutional neural networks",
                    "authors": [
                        "L. Shen",
                        "Z. Lin",
                        "Q. Huang"
                    ],
                    "venue": "ECCV,",
                    "year": 2016
                },
                {
                    "title": "Places401 and places365 models",
                    "authors": [
                        "L. Shen",
                        "Z. Lin",
                        "G. Sun",
                        "J. Hu"
                    ],
                    "venue": "https://github.com/lishen-shirley/ Places2-CNNs,",
                    "year": 2016
                },
                {
                    "title": "Multi-level discriminative dictionary learning with application to large scale image classification",
                    "authors": [
                        "L. Shen",
                        "G. Sun",
                        "Q. Huang",
                        "S. Wang",
                        "Z. Lin",
                        "E. Wu"
                    ],
                    "venue": "IEEE TIP,",
                    "year": 2015
                },
                {
                    "title": "Very deep convolutional networks for large-scale image recognition",
                    "authors": [
                        "K. Simonyan",
                        "A. Zisserman"
                    ],
                    "venue": "ICLR,",
                    "year": 2015
                },
                {
                    "title": "Training very deep networks",
                    "authors": [
                        "R.K. Srivastava",
                        "K. Greff",
                        "J. Schmidhuber"
                    ],
                    "venue": "NIPS,",
                    "year": 2015
                },
                {
                    "title": "Deep networks with internal selective attention through feedback connections",
                    "authors": [
                        "M.F. Stollenga",
                        "J. Masci",
                        "F. Gomez",
                        "J. Schmidhuber"
                    ],
                    "venue": "NIPS,",
                    "year": 2014
                },
                {
                    "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
                    "authors": [
                        "C. Szegedy",
                        "S. Ioffe",
                        "V. Vanhoucke",
                        "A. Alemi"
                    ],
                    "venue": "arXiv:1602.07261,",
                    "year": 2016
                },
                {
                    "title": "Going deeper with convolutions",
                    "authors": [
                        "C. Szegedy",
                        "W. Liu",
                        "Y. Jia",
                        "P. Sermanet",
                        "S. Reed",
                        "D. Anguelov",
                        "D. Erhan",
                        "V. Vanhoucke",
                        "A. Rabinovich"
                    ],
                    "venue": "CVPR,",
                    "year": 2015
                },
                {
                    "title": "Rethinking the inception architecture for computer vision",
                    "authors": [
                        "C. Szegedy",
                        "V. Vanhoucke",
                        "S. Ioffe",
                        "J. Shlens",
                        "Z. Wojna"
                    ],
                    "venue": "CVPR,",
                    "year": 2016
                },
                {
                    "title": "DeepPose: Human pose estimation via deep neural networks",
                    "authors": [
                        "A. Toshev",
                        "C. Szegedy"
                    ],
                    "venue": "CVPR,",
                    "year": 2014
                },
                {
                    "title": "Residual attention network for image classification",
                    "authors": [
                        "F. Wang",
                        "M. Jiang",
                        "C. Qian",
                        "S. Yang",
                        "C. Li",
                        "H. Zhang",
                        "X. Wang",
                        "X. Tang"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Aggregated residual transformations for deep neural networks",
                    "authors": [
                        "S. Xie",
                        "R. Girshick",
                        "P. Dollar",
                        "Z. Tu",
                        "K. He"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Show, attend and tell: Neural image caption generation with visual attention",
                    "authors": [
                        "K. Xu",
                        "J. Ba",
                        "R. Kiros",
                        "K. Cho",
                        "A. Courville",
                        "R. Salakhudinov",
                        "R. Zemel",
                        "Y. Bengio"
                    ],
                    "venue": "ICML,",
                    "year": 2015
                },
                {
                    "title": "Linear spatial pyramid matching using sparse coding for image classification",
                    "authors": [
                        "J. Yang",
                        "K. Yu",
                        "Y. Gong",
                        "T. Huang"
                    ],
                    "venue": "CVPR,",
                    "year": 2009
                },
                {
                    "title": "How transferable are features in deep neural networks",
                    "authors": [
                        "J. Yosinski",
                        "J. Clune",
                        "Y. Bengio",
                        "H. Lipson"
                    ],
                    "venue": "In NIPS,",
                    "year": 2014
                },
                {
                    "title": "Polynet: A pursuit of structural diversity in very deep networks",
                    "authors": [
                        "X. Zhang",
                        "Z. Li",
                        "C.C. Loy",
                        "D. Lin"
                    ],
                    "venue": "CVPR,",
                    "year": 2017
                },
                {
                    "title": "Places: A 10 million image database for scene recognition",
                    "authors": [
                        "B. Zhou",
                        "A. Lapedriza",
                        "A. Khosla",
                        "A. Oliva",
                        "A. Torralba"
                    ],
                    "venue": "IEEE TPAMI,",
                    "year": 2017
                }
            ],
            "id": "SP:2230ca26478c04475fadd9a9ba46effefe9c1090",
            "authors": [
                {
                    "name": "Jie Hu",
                    "affiliations": []
                },
                {
                    "name": "Li Shen",
                    "affiliations": []
                },
                {
                    "name": "Gang Sun",
                    "affiliations": []
                }
            ],
            "abstractText": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the \u201cSqueeze-and-Excitation\u201d(SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a \u223c25% relative improvement over the winning entry of 2016.",
            "title": "Squeeze-and-Excitation Networks"
        }
    },
    "63993374": {
        "X": {
            "sections": [
                {
                    "text": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.\nMany problems in image processing, computer graphics, and computer vision can be posed as \u201ctranslating\u201d an input image into a corresponding output image. Just as a concept\nmay be expressed in either English or French, a scene may be rendered as an RGB image, a gradient field, an edge map, a semantic label map, etc. In analogy to automatic language translation, we define automatic image-to-image translation as the problem of translating one possible representation of a scene into another, given sufficient training data (see Figure 1). One reason language translation is difficult is because the mapping between languages is rarely one-to-one \u2013 any given concept is easier to express in one language than another. Similarly, most image-to-image translation problems are either many-to-one (computer vision) \u2013 mapping photographs to edges, segments, or semantic labels, or one-to-many (computer graphics) \u2013 mapping labels or sparse user inputs to realistic images. Traditionally, each of these tasks has been tackled with separate, special-purpose machinery (e.g., [7, 15, 11, 1, 3, 37, 21, 26, 9, 42, 46]), despite the fact that the setting is always the same: predict pixels from pixels. Our goal in this paper is to develop a common framework for all these problems.\n1\nar X\niv :1\n61 1.\nThe community has already taken significant steps in this direction, with convolutional neural nets (CNNs) becoming the common workhorse behind a wide variety of image prediction problems. CNNs learn to minimize a loss function \u2013 an objective that scores the quality of results \u2013 and although the learning process is automatic, a lot of manual effort still goes into designing effective losses. In other words, we still have to tell the CNN what we wish it to minimize. But, just like Midas, we must be careful what we wish for! If we take a naive approach, and ask the CNN to minimize Euclidean distance between predicted and ground truth pixels, it will tend to produce blurry results [29, 46]. This is because Euclidean distance is minimized by averaging all plausible outputs, which causes blurring. Coming up with loss functions that force the CNN to do what we really want \u2013 e.g., output sharp, realistic images \u2013 is an open problem and generally requires expert knowledge.\nIt would be highly desirable if we could instead specify only a high-level goal, like \u201cmake the output indistinguishable from reality\u201d, and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recently proposed Generative Adversarial Networks (GANs) [14, 5, 30, 36, 47]. GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. Blurry images will not be tolerated since they look obviously fake. Because GANs learn a loss that adapts to the data, they can be applied to a multitude of tasks that traditionally would require very different kinds of loss functions.\nIn this paper, we explore GANs in the conditional setting. Just as GANs learn a generative model of data, conditional GANs (cGANs) learn a conditional generative model [14]. This makes cGANs suitable for image-to-image translation tasks, where we condition on an input image and generate a corresponding output image.\nGANs have been vigorously studied in the last two years and many of the techniques we explore in this paper have been previously proposed. Nonetheless, earlier papers have focused on specific applications, and it has remained unclear how effective image-conditional GANs can be as a general-purpose solution for image-toimage translation. Our primary contribution is to demonstrate that on a wide variety of problems, conditional GANs produce reasonable results. Our second contribution is to present a simple framework sufficient to achieve good results, and to analyze the effects of several important architectural choices. Code is available at https://github.com/phillipi/pix2pix."
                },
                {
                    "heading": "1. Related work",
                    "text": "Structured losses for image modeling Image-to-image translation problems are often formulated as per-pixel clas-\nsification or regression [26, 42, 17, 23, 46]. These formulations treat the output space as \u201cunstructured\u201d in the sense that each output pixel is considered conditionally independent from all others given the input image. Conditional GANs instead learn a structured loss. Structured losses penalize the joint configuration of the output. A large body of literature has considered losses of this kind, with popular methods including conditional random fields [2], the SSIM metric [40], feature matching [6], nonparametric losses [24], the convolutional pseudo-prior [41], and losses based on matching covariance statistics [19]. Our conditional GAN is different in that the loss is learned, and can, in theory, penalize any possible structure that differs between output and target.\nConditional GANs We are not the first to apply GANs in the conditional setting. Previous works have conditioned GANs on discrete labels [28], text [32], and, indeed, images. The image-conditional models have tackled inpainting [29], image prediction from a normal map [39], image manipulation guided by user constraints [49], future frame prediction [27], future state prediction [48], product photo generation [43], and style transfer [25]. Each of these methods was tailored for a specific application. Our framework differs in that nothing is application-specific. This makes our setup considerably simpler than most others.\nOur method also differs from these prior works in several architectural choices for the generator and discriminator. Unlike past work, for our generator we use a \u201cU-Net\u201dbased architecture [34], and for our discriminator we use a convolutional \u201cPatchGAN\u201d classifier, which only penalizes structure at the scale of image patches. A similar PatchGAN architecture was previously proposed in [25], for the purpose of capturing local style statistics. Here we show that this approach is effective on a wider range of problems, and we investigate the effect of changing the patch size."
                },
                {
                    "heading": "2. Method",
                    "text": "GANs are generative models that learn a mapping from random noise vector z to output image y: G : z \u2192 y [14]. In contrast, conditional GANs learn a mapping from observed image x and random noise vector z, to y: G : {x, z} \u2192 y. The generator G is trained to produce outputs that cannot be distinguished from \u201creal\u201d images by an adversarially trained discrimintor, D, which is trained to do as well as possible at detecting the generator\u2019s \u201cfakes\u201d. This training procedure is diagrammed in Figure 2."
                },
                {
                    "heading": "2.1. Objective",
                    "text": "The objective of a conditional GAN can be expressed as\nLcGAN (G,D) =Ex,y\u223cpdata(x,y)[logD(x, y)]+ Ex\u223cpdata(x),z\u223cpz(z)[log(1\u2212D(x,G(x, z))],\n(1)\nwhere G tries to minimize this objective against an adversarial D that tries to maximize it, i.e. G\u2217 = argminG maxD LcGAN (G,D).\nTo test the importance of conditioning the discrimintor, we also compare to an unconditional variant in which the discriminator does not observe x:\nLGAN (G,D) =Ey\u223cpdata(y)[logD(y)]+ Ex\u223cpdata(x),z\u223cpz(z)[log(1\u2212D(G(x, z))].\n(2)\nPrevious approaches to conditional GANs have found it beneficial to mix the GAN objective with a more traditional loss, such as L2 distance [29]. The discriminator\u2019s job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground truth output in an L2 sense. We also explore this option, using L1 distance rather than L2 as L1 encourages less blurring:\nLL1(G) = Ex,y\u223cpdata(x,y),z\u223cpz(z)[\u2016y \u2212G(x, z)\u20161]. (3)\nOur final objective is\nG\u2217 = argmin G max D LcGAN (G,D) + \u03bbLL1(G). (4)\nWithout z, the net could still learn a mapping from x to y, but would produce deterministic outputs, and therefore fail to match any distribution other than a delta function. Past conditional GANs have acknowledged this and provided Gaussian noise z as an input to the generator, in addition to x (e.g., [39]). In initial experiments, we did not find\nthis strategy effective \u2013 the generator simply learned to ignore the noise \u2013 which is consistent with Mathieu et al. [27]. Instead, for our final models, we provide noise only in the form of dropout, applied on several layers of our generator at both training and test time. Despite the dropout noise, we observe very minor stochasticity in the output of our nets. Designing conditional GANs that produce stochastic output, and thereby capture the full entropy of the conditional distributions they model, is an important question left open by the present work."
                },
                {
                    "heading": "2.2. Network architectures",
                    "text": "We adapt our generator and discriminator architectures from those in [30]. Both generator and discriminator use modules of the form convolution-BatchNorm-ReLu [18]. Details of the architecture are provided in the appendix, with key features discussed below."
                },
                {
                    "heading": "2.2.1 Generator with skips",
                    "text": "A defining feature of image-to-image translation problems is that they map a high resolution input grid to a high resolution output grid. In addition, for the problems we consider, the input and output differ in surface appearance, but both are renderings of the same underlying structure. Therefore, structure in the input is roughly aligned with structure in the output. We design the generator architecture around these considerations.\nMany previous solutions [29, 39, 19, 48, 43] to problems in this area have used an encoder-decoder network [16]. In such a network, the input is passed through a series of layers that progressively downsample, until a bottleneck layer, at which point the process is reversed (Figure 3). Such a network requires that all information flow pass through all the layers, including the bottleneck. For many image translation problems, there is a great deal of low-level information shared between the input and output, and it would be\ndesirable to shuttle this information directly across the net. For example, in the case of image colorizaton, the input and output share the location of prominent edges.\nTo give the generator a means to circumvent the bottleneck for information like this, we add skip connections, following the general shape of a \u201cU-Net\u201d [34] (Figure 3). Specifically, we add skip connections between each layer i and layer n\u2212 i, where n is the total number of layers. Each skip connection simply concatenates all channels at layer i with those at layer n\u2212 i."
                },
                {
                    "heading": "2.2.2 Markovian discriminator (PatchGAN)",
                    "text": "It is well known that the L2 loss \u2013 and L1, see Figure 4 \u2013 produces blurry results on image generation problems [22]. Although these losses fail to encourage highfrequency crispness, in many cases they nonetheless accurately capture the low frequencies. For problems where this is the case, we do not need an entirely new framework to enforce correctness at the low frequencies. L1 will already do.\nThis motivates restricting the GAN discriminator to only model high-frequency structure, relying on an L1 term to force low-frequency correctness (Eqn. 4). In order to model high-frequencies, it is sufficient to restrict our attention to the structure in local image patches. Therefore, we design a discriminator architecture \u2013 which we term a PatchGAN \u2013 that only penalizes structure at the scale of patches. This discriminator tries to classify if each N \u00d7 N patch in an image is real or fake. We run this discriminator convolutationally across the image, averaging all responses to provide the ultimate output of D.\nIn Section 3.4, we demonstrate that N can be much smaller than the full size of the image and still produce high quality results. This is advantageous because a smaller PatchGAN has fewer parameters, runs faster, and can be applied on arbitrarily large images.\nSuch a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. This connection was previously explored in [25], and is also the common assumption in models of texture [8, 12] and style [7, 15, 13, 24]. Our PatchGAN can therefore be understood as a form of texture/style loss."
                },
                {
                    "heading": "2.3. Optimization and inference",
                    "text": "To optimize our networks, we follow the standard approach from [14]: we alternate between one gradient descent step on D, then one step on G. We use minibatch SGD and apply the Adam solver [20].\nAt inference time, we run the generator net in exactly the same manner as during the training phase. This differs from the usual protocol in that we apply dropout at test time,\nand we apply batch normalization [18] using the statistics of the test batch, rather than aggregated statistics of the training batch. This approach to batch normalization, when the batch size is set to 1, has been termed \u201cinstance normalization\u201d and has been demonstrated to be effective at image generation tasks [38]. In our experiments, we use batch size 1 for certain experiments and 4 for others, noting little difference between these two conditions."
                },
                {
                    "heading": "3. Experiments",
                    "text": "To explore the generality of conditional GANs, we test the method on a variety of tasks and datasets, including both graphics tasks, like photo generation, and vision tasks, like semantic segmentation:\n\u2022 Semantic labels\u2194photo, trained on the Cityscapes dataset [4]. \u2022 Architectural labels\u2192photo, trained on the CMP Fa-\ncades dataset [31]. \u2022 Map\u2194aerial photo, trained on data scraped from\nGoogle Maps. \u2022 BW\u2192color photos, trained on [35]. \u2022 Edges\u2192photo, trained on data from [49] and [44]; bi-\nnary edges generated using the HED edge detector [42] plus postprocessing. \u2022 Sketch\u2192photo: tests edges\u2192photo models on human-\ndrawn sketches from [10]. \u2022 Day\u2192night, trained on [21].\nDetails of training on each of these datasets are provided in the Appendix. In all cases, the input and output are simply 1-3 channel images. Qualitative results are shown in Figures 8, 9, 10, 11, 12, 14, 15, 16, and 13. Several failure cases are highlighted in Figure 17. More comprehensive results are available at https://phillipi.github.io/pix2pix/.\nData requirements and speed We note that decent results can often be obtained even on small datasets. Our facade training set consists of just 400 images (see results in Figure 12), and the day to night training set consists of only 91 unique webcams (see results in Figure 13). On datasets of this size, training can be very fast: for example, the results shown in Figure 12 took less than two hours of training on a single Pascal Titan X GPU. At test time, all models run in well under a second on this GPU."
                },
                {
                    "heading": "3.1. Evaluation metrics",
                    "text": "Evaluating the quality of synthesized images is an open and difficult problem [36]. Traditional metrics such as perpixel mean-squared error do not assess joint statistics of the result, and therefore do not measure the very structure that structured losses aim to capture.\nIn order to more holistically evaluate the visual quality of our results, we employ two tactics. First, we run\n\u201creal vs fake\u201d perceptual studies on Amazon Mechanical Turk (AMT). For graphics problems like colorization and photo generation, plausibility to a human observer is often the ultimate goal. Therefore, we test our map generation, aerial photo generation, and image colorization using this approach.\nSecond, we measure whether or not our synthesized cityscapes are realistic enough that off-the-shelf recognition system can recognize the objects in them. This metric is similar to the \u201cinception score\u201d from [36], the object detection evaluation in [39], and the \u201csemantic interpretability\u201d measure in [46].\nAMT perceptual studies For our AMT experiments, we followed the protocol from [46]: Turkers were presented with a series of trials that pitted a \u201creal\u201d image against a \u201cfake\u201d image generated by our algorithm. On each trial, each image appeared for 1 second, after which the images disappeared and Turkers were given unlimited time to respond as to which was fake. The first 10 images of each session were practice and Turkers were given feedback. No feedback was provided on the 40 trials of the main experiment. Each session tested just one algorithm at a time, and Turkers were not allowed to complete more than one session. \u223c 50 Turkers evaluated each algorithm. All images were presented at 256 \u00d7 256 resolution. Unlike [46], we did not include vigilance trials. For our colorization experiments, the real and fake images were generated from the same grayscale input. For map\u2194aerial photo, the real and fake images were not generated from the same input, in order to make the task more difficult and avoid floor-level results.\nFCN-score While quantitative evaluation of generative models is known to be challenging, recent works [36, 39, 46] have tried using pre-trained semantic classifiers to measure the discriminability of the generated images as a pseudo-metric. The intuition is that if the generated images are realistic, classifiers trained on real images will be able to classify the synthesized image correctly as well. To this end, we adopt the popular FCN-8s [26] architecture for semantic segmentation, and train it on the cityscapes dataset. We then score synthesized photos by the classification accuracy against the labels these photos were synthesized from."
                },
                {
                    "heading": "3.2. Analysis of the objective function",
                    "text": "Which components of the objective in Eqn. 4 are important? We run ablation studies to isolate the effect of the L1 term, the GAN term, and to compare using a discriminator conditioned on the input (cGAN, Eqn. 1) against using an unconditional discriminator (GAN, Eqn. 2).\nFigure 4 shows the qualitative effects of these variations on two labels\u2192photo problems. L1 alone leads to reasonable but blurry results. The cGAN alone (setting \u03bb = 0 in Eqn. 4) gives much sharper results, but results in some artifacts in facade synthesis. Adding both terms together (with \u03bb = 100) reduces these artifacts.\nWe quantify these observations using the FCN-score on the cityscapes labels\u2192photo task (Table 1): the GAN-based objectives achieve higher scores, indicating that the synthesized images include more recognizable structure. We also test the effect of removing conditioning from the discriminator (labeled as GAN). In this case, the loss does not penalize mismatch between the input and output; it only cares that the output look realistic. This variant results in very poor performance; examining the results reveals that the generator collapsed into producing nearly the exact same output regardless of input photograph. Clearly it is important, in this case, that the loss measure the quality of the match between input and output, and indeed cGAN performs much better than GAN. Note, however, that adding an L1 term also encourages that the output respect the input, since the L1 loss penalizes the distance between ground truth outputs, which match the input, and synthesized outputs, which may not. Correspondingly, L1+GAN is also effective at creating realistic renderings that respect the in-\nL1 1x1 16x16 70x70 256x256\nFigure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric (Table 2). Please see https://phillipi.github.io/pix2pix/ for additional examples.\nput label maps. Combining all terms, L1+cGAN, performs similarly well.\nColorfulness A striking effect of conditional GANs is that they produce sharp images, hallucinating spatial structure even where it does not exist in the input label map. One might imagine cGANs have a similar effect on \u201csharpening\u201d in the spectral dimension \u2013 i.e. making images more colorful. Just as L1 will incentivize a blur when it is uncertain where exactly to locate an edge, it will also incentivize an average, grayish color when it is uncertain which of several plausible color values a pixel should take on. Specially, L1 will be minimized by choosing the median of of the conditional probability density function over possible colors. An adversarial loss, on the other hand, can in principle become aware that grayish outputs are unrealistic, and encourage matching the true color distribution [14]. In Figure 7, we investigate if our cGANs actually achieve this effect on the Cityscapes dataset. The plots show the marginal distri-\nbutions over output color values in Lab color space. The ground truth distributions are shown with a dotted line. It is apparent that L1 leads to a narrower distribution than the ground truth, confirming the hypothesis that L1 encourages average, grayish colors. Using a cGAN, on the other hand, pushes the output distribution closer to the ground truth."
                },
                {
                    "heading": "3.3. Analysis of the generator architecture",
                    "text": "A U-Net architecture allows low-level information to shortcut across the network. Does this lead to better results? Figure 5 compares the U-Net against an encoder-decoder on cityscape generation U-Net. The encoder-decoder is created simply by severing the skip connections in the U-Net. The encoder-decoder is unable to learn to generate realistic images in our experiments, and indeed collapses to producing nearly identical results for each input label map. The advantages of the U-Net appear not to be specific to conditional GANs: when both U-Net and encoder-decoder are trained\n648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\nCVPR #385\nCVPR #385\nCVPR 2016 Submission #385. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(a)\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(b)\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(c)\nHistogram intersection against ground truth\nLoss L a b L1 0.81 0.69 0.70 cGAN 0.87 0.74 0.84 L1+cGAN 0.86 0.84 0.82 PixelGAN 0.83 0.68 0.78\n(d) Figure 5: Color distribution matching property of the cGAN, tested on Cityscapes. (c.f. Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions.\nL1 1x1 16x16 70x70 256x256\nFigure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric (Table 2).\nClassification Ours L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth\nFigure 7: Colorization results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebalancing) from [46]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo!labels. Figure 8 shows qualitative results, and quantitative classification accuracies are reported in Table 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \u201clabels\u201d, which are\nInput Ground truth L1 cGAN\nFigure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects.\nnearly discrete, rather than \u201cimages\u201d, with their continuousvalued variation. Although cGANs achieve some success, they are far from the best available method for solving this\nproblem: simply using L1 regression gets better scores than\nusing a cGAN, as shown in Table 4. We argue that for vision\nproblems, the goal (i.e. predicting output close to ground\ntruth) may be less ambiguous than graphics tasks, and re-\nconstruction losses like L1 are mostly sufficient.\n4. Conclusion\nThe results in this paper suggest that conditional adver-\nsarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.\n7\nlo g\nP (L\n)\nlo g\nP (a\n)\nlo g\nP (b\n)\nL a b\n(a) 48 49 50 51 52 53 54 55 56 57 58 59 60\n661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n#385 CVPR #385\nCVPR 2016 Submission #385. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u2212 1\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(a)\n70 90 110 130 150 \u2212 1\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(b)\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(c)\nHistogram int rsection against ground truth\nLoss L a b L1 0.81 0.69 .70 cGAN 0.87 0.74 0.84 L1+cGAN 0.86 0.84 0.82 PixelGAN 0.83 0.68 0.78\n(d) Figure 5: Color distribution matching property of the cGAN, tested on Cityscapes. (c.f. Figure 1 of the original GAN aper [14]). Note that the histogram intersection scores are dominated by diff rences in the high pro ability region, which are imperceptible in the plots,\nhich show log probability and therefore emphasize differences in the low probability regions.\nL1 1x1 16x16 70x70 256x256\nFigure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric (Table 2).\nClassification Ours L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth\nFigure 7: Colorization results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebalancing) from [46]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo!labels. Figure 8 shows qualitative results, and quantitative classification accuracies are reported in Table 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \u201clabels\u201d, which are\nInput Ground truth L1 cGAN\nFigure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects.\nnearly discrete, rather than \u201cimages\u201d, with their continuousvalued variation. Although cGANs achieve some success, they are far from the best available method for solving this\nproblem: simply using L1 regression gets better scores than\nusing a cGAN, as shown in Table 4. We argue that for vision\nproblems, the goal (i.e. predicting output close to ground\ntruth) may be less ambiguous than graphics tasks, and re-\nconstruction losses like L1 are mostly sufficient.\n4. Conclusion\nThe results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them ap-\nplicable in a wide variety of settings.\n7\nlo g\nP (L\n)\nlo g\nP (a\n)\nlo g\nP (b\n)\nL a b\n(b) 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 6 8 699 700 701\n702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\nCVPR #385\nCVPR #385\nCVPR 2016 Sub ission #385. CONFIDENTIAL REVIE COPY. DO NOT DISTRIBUTE.\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(a)\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(b)\n70 90 110 130 150 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nb\n70 90 110 130 \u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\na 0 20 40 60 80 100\n\u221211\n\u22129\n\u22127\n\u22125\n\u22123\n\u22121\nL\nL1 cGAN L1+cGAN L1+pixelcGAN Ground truth\n(c)\nHistogram intersection against ground truth\nLoss L a b L1 0.81 0.69 0.70 cGAN 0.87 0.74 0.84 L1+cGAN 0.86 0.84 0.82 PixelGAN 0.83 0.68 0.78\n(d) Figure 5: Color distribution matching property of the cGAN, tested on Cityscapes. (c.f. Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions.\nL1 1x1 16x16 70x70 256x256\nFigure 6: Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The 1x1 PixelGAN encourages greater color diversity but has no effect on spatial statistics. The 16x16 PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The 70x70 PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (coforfulness) dimensions. The full 256x256 ImageGAN produces results that are visually similar to the 70x70 PatchGAN, but somewhat lower quality according to our FCN-score metric (Table 2).\nClassification Ours L2 [44] (rebal.) [44] (L1 + cGAN) Ground truth\nFigure 7: C lorizatio results of conditional GANs versus the L2 regression from [44] and the full method (classification with rebal ncing) from [46]. The cGANs can produce compelling colorizat ons (first two rows), but hav a common failure mode of producing a grayscale or desaturated result (last row).\nT begin to t st this, we train a cGAN (with/without L1 loss) on cityscape pho o!labels. F gure 8 show qualitative results, and quantitative classification accuracies are reported in T l 4. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \u201clabels\u201d, which are\nInput Ground truth L1 cGAN\nFigure 8: Applying a conditional GAN to semantic segmentation. The cGAN produces sharp images that look at glance like the ground truth, but in fact include many small, hallucinated objects.\nnearly discrete, rather than \u201cimages\u201d, with their continuousvalued variation. Although cGANs achieve some success,\nthey are far from the best available method for solving this\nproblem: simply using L1 regression gets better scores than\nusing a cGAN, as shown in Table 4. We argue that for vision\nproblems, th goal (i.e. predicting output clo e to ground\ntruth) may be less ambiguous than graphics tasks, and re-\nconstruction losses like L1 are mostly sufficient.\n4. Conclusion\nThe results in this paper suggest t at conditional adversarial networks are a promising approach f r many imageto-image tr nslation task , especially th se involving highly structured gr phical outputs. These network learn a loss\nadapted to the task and data at hand, which makes them ap-\nplicable in a wide variety of settings.\n7\nlo g\nP (L\n)\nlo g\nP (a\n)\nlo g\nP (b\n)\nL a b\n(c)\nHistogram intersection against ground truth\nLoss L a b L1 0.81 0.69 0.70 cGAN 0.87 0.74 0.84 L1+cGAN 0.86 0.84 0.82 PixelGAN 0.83 0.68 0.78\n(d)\nFigure 7: Color distribution matching property of the cGAN, tested on Cityscapes. (c.f. Figure 1 of the original GAN paper [14]). Note that the histogram intersection scores are dominated by differences in the high probability region, which are imperceptible in the plots, which show log probability and therefore emphasize differences in the low probability regions.\nwith an L1 loss, the U-Net again achieves the superior results (Figure 5)."
                },
                {
                    "heading": "3.4. From PixelGANs to PatchGans to Imag GANs",
                    "text": "We test e effect of varying the patch size N of our discriminator recepti e fields, from a 1 \u00d7 1 \u201cPixel \u201d to a full 256 \u00d7 256 \u201cImageGAN\u201d1. Figure 6 shows qualitative results of this analysis and Table 2 quantifies the effects using the FCN-score. Note that elsewhere in this paper, unless specified, all experiments use 70\u00d7 70 PatchGANs, and for this section all experiments use an L1+cGAN loss.\nThe PixelGAN has no effect on spatial sharpness, but does increase the colorfulness of the results (quantified in Figure 7). For example, the bus in Figure 6 is painted gray when the net is trained with an L1 loss, but becomes red with the PixelGAN loss. Color histogram matching is a common problem in image processing [33], and PixelGANs may be a promising lightweight solution.\nUsing a 16\u00d716 PatchGAN is sufficient to promote sharp outputs, but also leads to tiling artifacts. The 70\u00d770 PatchGAN alleviates these artifacts. Scaling beyond this, to the full 256\u00d7 256 ImageGAN, does not appear to improve the visual quality of the results, and in fact gets a considerably lower FCN-score (Table 2). This may be because the ImageGAN has many more parameters and greater depth than the 70\u00d7 70 PatchGAN, and may be harder to train.\nFully- volutional translation An advantage of the PatchGAN is that a fixed-size patch discriminator can be applied to arbitrarily large images. We may also apply the generator convolutionally, on larger images than those on which it was trained. We test this on the map\u2194aerial photo task. After training a generator on 256\u00d7256 images, we test it on 512\u00d7512 images. The results in Figure 8 demonstrate the effectiveness of this approach.\n1We achieve this variation in patch size by adjusting th depth of the GAN discriminator. Details of this process, and the discriminator architectures are provided in the appendix\nClassification Ours L2 [46] (rebal.) [46] (L1 + cGAN) Ground truth\nFigure 9: Colorization results of conditional GANs versus the L2 regression from [46] and the full method (classification with rebalancing) from [48]. The cGANs can produce compelling colorizations (first two rows), but have a common failure mode of producing a grayscale or desaturated result (last row).\nPhoto\u2192Map Map\u2192 Photo Loss % Turkers labeled real % Turkers labeled real L1 2.8% \u00b1 1.0% 0.8% \u00b1 0.3% L1+cGAN 6.1% \u00b1 1.3% 18.9% \u00b1 2.5%\nTable 3: AMT \u201creal vs fake\u201d test on m ps\u2194aerial photos.\nMeth d % Turkers labeled real L2 regression from [46] 16.3% \u00b1 2.4% Zhang et al. 2016 [46] 27.8% \u00b1 2.7% Ours 22.5% \u00b1 1.6%\nTable 4: AMT \u201creal vs fake\u201d test on colorization."
                },
                {
                    "heading": "3.5. Perceptual validation",
                    "text": "We validate the perceptual realism of our results on the tasks of map\u2194aerial photograph and grayscale\u2192color. Results of our AMT experiment for map\u2194photo are given in Table 3. The aerial photos generated by our method fooled participants on 18.9% of trials, significantly above the L1 baseline, which produces blurry results and nearly never fooled participants. In contrast, in the photo\u2192map directionm our method only fooled participants on 6.1% of trials, and this was not significantly different than the performance of the L1 baseline (based on bootstrap test). This may be because minor structural errors are more visible in maps, which have rigid geometry, than in aerial photographs, which are more chaotic.\nWe trained colorization on ImageNet [35], and tested on the test split introduced by [46, 23]. Our method, with L1+cGAN loss, fooled participants on 22.5% of trials (Table 4). We also tested the results of [46] and a variant of their method that used an L2 loss (see [46] for details). The conditional GAN scored similarly to the L2 variant of [46] (difference insignificant by bootstrap test), but fell short of [46]\u2019s full method, which fooled participants on 27.8% of trials in our experiment. We note that their method was specifically engineered to do well on colorization."
                },
                {
                    "heading": "3.6. Semantic segmentation",
                    "text": "Conditional GANs appear to be effective on problems where the output is highly detailed or photographic, as is common in image processing and graphics tasks. What about vision problems, like semantic segmentation, where the output is instead less complex than the input?\nTo begin to test this, we train a cGAN (with/without L1 loss) on cityscape photo\u2192labels. Figure 10 shows qualitative results, and quantitative classification accuracies are reported in Table 5. Interestingly, cGANs, trained without the L1 loss, are able to solve this problem at a reasonable degree of accuracy. To our knowledge, this is the first demonstration of GANs successfully generating \u201clabels\u201d, which are nearly discrete, rather than \u201cimages\u201d, with their continuousvalued variation2. Although cGANs achieve some success, they are far from the best available method for solving this problem: simply using L1 regression gets better scores than using a cGAN, as shown in Table 5. We argue that for vision problems, the goal (i.e. predicting output close to ground truth) may be less ambiguous than graphics tasks, and reconstruction losses like L1 are mostly sufficient."
                },
                {
                    "heading": "4. Conclusion",
                    "text": "The results in this paper suggest that conditional adversarial networks are a promising approach for many imageto-image translation tasks, especially those involving highly structured graphical outputs. These networks learn a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.\nAcknowledgments: We thank Richard Zhang and Deepak Pathak for helpful discussions. This work was supported in part by NSF SMA1514512, NGA NURI, IARPA via Air Force Research Laboratory, Intel Corp, and hardware donations by nVIDIA. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL or the U.S. Government.\n2Note that the label maps we train on are not exactly discrete valued, as they are resized from the original maps using bilinear interpolation and saved as jpeg images, with some compression artifacts."
                },
                {
                    "heading": "5. Appendix",
                    "text": ""
                },
                {
                    "heading": "5.1. Network architectures",
                    "text": "We adapt our network architectures from those in [30]. Code for the models is available at https://github.com/phillipi/pix2pix.\nLet Ck denote a Convolution-BatchNorm-ReLU layer with k filters. CDk denotes a a Convolution-BatchNormDropout-ReLU layer with a dropout rate of 50%. All convolutions are 4\u00d7 4 spatial filters applied with stride 2. Convolutions in the encoder, and in the discriminator, downsample by a factor of 2, whereas in the decoder they upsample by a factor of 2."
                },
                {
                    "heading": "5.1.1 Generator architectures",
                    "text": "The encoder-decoder architecture consists of: encoder: C64-C128-C256-C512-C512-C512-C512-C512 decoder: CD512-CD512-CD512-C512-C512-C256-C128 -C64\nAfter the last layer in the decoder, a convolution is applied to map to the number of output channels (3 in general, except in colorization, where it is 2), followed by a Tanh function. As an exception to the above notation, BatchNorm is not applied to the first C64 layer in the encoder. All ReLUs in the encoder are leaky, with slope 0.2, while ReLUs in the decoder are not leaky.\nThe U-Net architecture is identical except with skip connections between each layer i in the encoder and layer n\u2212 i in the decoder, where n is the total number of layers. The skip connections concatenate activations from layer i to layer n \u2212 i. This changes the number of channels in the decoder:\nU-Net decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128"
                },
                {
                    "heading": "5.1.2 Discriminator architectures",
                    "text": "The 70\u00d7 70 discriminator architecture is: C64-C128-C256-C512\nAfter the last layer, a convolution is applied to map to a 1 dimensional output, followed by a Sigmoid function. As an exception to the above notation, BatchNorm is not applied to the first C64 layer. All ReLUs are leaky, with slope 0.2.\nAll other discriminators follow the same basic architecture, with depth varied to modify the receptive field size:\n1\u00d7 1 discriminator: C64-C128 (note, in this special case, all convolutions are 1\u00d7 1 spatial filters) 16\u00d7 16 discriminator: C64-C128\n256\u00d7 256 discriminator: C64-C128-C256-C512-C512-C512\nNote the the 256\u00d7256 discriminator has receptive fields that could cover up to 574 \u00d7 574 pixels, if they were available, but since the input images are only 256 \u00d7 256 pixels, only 256\u00d7256 pixels are seen, and so we refer to this setting as the 256\u00d7 256 discriminator."
                },
                {
                    "heading": "5.2. Training details",
                    "text": "Random jitter was applied by resizing the 256\u00d7256 input images to 286 \u00d7 286 and then randomly cropping back to size 256\u00d7 256.\nAll networks were trained from scratch. Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02.\nSemantic labels\u2192photo 2975 training images from the Cityscapes training set [4], trained for 200 epochs, batch size 1, with random jitter and mirroring. We used the Cityscapes val set for testing.\nArchitectural labels\u2192photo 400 training images from [31], trained for 200 epochs, batch size 1, with random jitter and mirroring. Data from was split into train and test randomly.\nMaps\u2194aerial photograph 1096 training images scraped from Google Maps, trained for 200 epochs, batch size 1, with random jitter and mirroring. Images were sampled from in and around New York City. Data was then split into train and test about the median latitude of the sampling region (with a buffer region added to ensure that no training pixel appeared in the test set).\nBW\u2192color 1.2 million training images (Imagenet training set [35]), trained for\u223c 6 epochs, batch size 4, with only mirroring, no random jitter. Tested on subset of Imagenet val set, following protocol of [46] and [23].\nEdges\u2192shoes 50k training images from UT Zappos50K dataset [45] trained for 15 epochs, batch size 4. Data from was split into train and test randomly.\nEdges\u2192Handbag 137K Amazon Handbag images from [49], trained for 15 epochs, batch size 4. Data from was split into train and test randomly.\nDay\u2192night 17823 training images extracted from 91 webcams, from [21] trained for 17 epochs, batch size 4, with random jitter and mirroring. We use 91 webcams as training, and 10 webcams for test."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "A non-local algorithm for image denoising",
                    "authors": [
                        "A. Buades",
                        "B. Coll",
                        "J.-M. Morel"
                    ],
                    "venue": "In CVPR,",
                    "year": 2005
                },
                {
                    "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
                    "authors": [
                        "L.-C. Chen",
                        "G. Papandreou",
                        "I. Kokkinos",
                        "K. Murphy",
                        "A.L. Yuille"
                    ],
                    "venue": "In ICLR,",
                    "year": 2015
                },
                {
                    "title": "Sketch2photo: internet image montage",
                    "authors": [
                        "T. Chen",
                        "M.-M. Cheng",
                        "P. Tan",
                        "A. Shamir",
                        "S.-M. Hu"
                    ],
                    "venue": "ACM Transactions on Graphics (TOG),",
                    "year": 2009
                },
                {
                    "title": "The cityscapes dataset for semantic urban scene understanding",
                    "authors": [
                        "M. Cordts",
                        "M. Omran",
                        "S. Ramos",
                        "T. Rehfeld",
                        "M. Enzweiler",
                        "R. Benenson",
                        "U. Franke",
                        "S. Roth",
                        "B. Schiele"
                    ],
                    "venue": "In CVPR),",
                    "year": 2016
                },
                {
                    "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
                    "authors": [
                        "E.L. Denton",
                        "S. Chintala",
                        "R. Fergus"
                    ],
                    "venue": "In NIPS,",
                    "year": 2015
                },
                {
                    "title": "Generating images with perceptual similarity metrics based on deep networks",
                    "authors": [
                        "A. Dosovitskiy",
                        "T. Brox"
                    ],
                    "venue": "arXiv preprint arXiv:1602.02644,",
                    "year": 2016
                },
                {
                    "title": "Image quilting for texture synthesis and transfer",
                    "authors": [
                        "A.A. Efros",
                        "W.T. Freeman"
                    ],
                    "venue": "In SIGGRAPH,",
                    "year": 2001
                },
                {
                    "title": "Texture synthesis by nonparametric sampling",
                    "authors": [
                        "A.A. Efros",
                        "T.K. Leung"
                    ],
                    "venue": "In ICCV,",
                    "year": 1999
                },
                {
                    "title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
                    "authors": [
                        "D. Eigen",
                        "R. Fergus"
                    ],
                    "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
                    "year": 2015
                },
                {
                    "title": "How do humans sketch objects",
                    "authors": [
                        "M. Eitz",
                        "J. Hays",
                        "M. Alexa"
                    ],
                    "year": 2012
                },
                {
                    "title": "Removing camera shake from a single photograph",
                    "authors": [
                        "R. Fergus",
                        "B. Singh",
                        "A. Hertzmann",
                        "S.T. Roweis",
                        "W.T. Freeman"
                    ],
                    "venue": "In ACM Transactions on Graphics (TOG),",
                    "year": 2006
                },
                {
                    "title": "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks",
                    "authors": [
                        "L.A. Gatys",
                        "A.S. Ecker",
                        "M. Bethge"
                    ],
                    "venue": "arXiv preprint arXiv:1505.07376,",
                    "year": 2015
                },
                {
                    "title": "Image style transfer using convolutional neural networks. CVPR, 2016",
                    "authors": [
                        "L.A. Gatys",
                        "A.S. Ecker",
                        "M. Bethge"
                    ],
                    "year": 2016
                },
                {
                    "title": "Generative adversarial nets",
                    "authors": [
                        "I. Goodfellow",
                        "J. Pouget-Abadie",
                        "M. Mirza",
                        "B. Xu",
                        "D. Warde-Farley",
                        "S. Ozair",
                        "A. Courville",
                        "Y. Bengio"
                    ],
                    "venue": "In NIPS,",
                    "year": 2014
                },
                {
                    "title": "Image analogies",
                    "authors": [
                        "A. Hertzmann",
                        "C.E. Jacobs",
                        "N. Oliver",
                        "B. Curless",
                        "D.H. Salesin"
                    ],
                    "venue": "In SIGGRAPH,",
                    "year": 2001
                },
                {
                    "title": "Reducing the dimensionality of data with neural networks",
                    "authors": [
                        "G.E. Hinton",
                        "R.R. Salakhutdinov"
                    ],
                    "venue": "Science, 313(5786):504\u2013507,",
                    "year": 2006
                },
                {
                    "title": "Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification",
                    "authors": [
                        "S. Iizuka",
                        "E. Simo-Serra",
                        "H. Ishikawa"
                    ],
                    "venue": "ACM Transactions on Graphics (TOG),",
                    "year": 2016
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "S. Ioffe",
                        "C. Szegedy"
                    ],
                    "year": 2015
                },
                {
                    "title": "Perceptual losses for real-time style transfer and super-resolution",
                    "authors": [
                        "J. Johnson",
                        "A. Alahi",
                        "L. Fei-Fei"
                    ],
                    "year": 2016
                },
                {
                    "title": "Adam: A method for stochastic optimization",
                    "authors": [
                        "D. Kingma",
                        "J. Ba"
                    ],
                    "venue": "ICLR,",
                    "year": 2015
                },
                {
                    "title": "Transient attributes for high-level understanding and editing of outdoor scenes",
                    "authors": [
                        "P.-Y. Laffont",
                        "Z. Ren",
                        "X. Tao",
                        "C. Qian",
                        "J. Hays"
                    ],
                    "venue": "ACM Transactions on Graphics (TOG),",
                    "year": 2014
                },
                {
                    "title": "Autoencoding beyond pixels using a learned similarity metric",
                    "authors": [
                        "A.B.L. Larsen",
                        "S.K. S\u00f8nderby",
                        "O. Winther"
                    ],
                    "venue": "arXiv preprint arXiv:1512.09300,",
                    "year": 2015
                },
                {
                    "title": "Learning representations for automatic colorization",
                    "authors": [
                        "G. Larsson",
                        "M. Maire",
                        "G. Shakhnarovich"
                    ],
                    "venue": "ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Combining markov random fields and convolutional neural networks for image synthesis",
                    "authors": [
                        "C. Li",
                        "M. Wand"
                    ],
                    "venue": "CVPR, 2016",
                    "year": 2016
                },
                {
                    "title": "Precomputed real-time texture synthesis with markovian generative adversarial networks",
                    "authors": [
                        "C. Li",
                        "M. Wand"
                    ],
                    "venue": "ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Fully convolutional networks for semantic segmentation",
                    "authors": [
                        "J. Long",
                        "E. Shelhamer",
                        "T. Darrell"
                    ],
                    "venue": "In CVPR,",
                    "year": 2015
                },
                {
                    "title": "Deep multi-scale video prediction beyond mean square error",
                    "authors": [
                        "M. Mathieu",
                        "C. Couprie",
                        "Y. LeCun"
                    ],
                    "venue": "ICLR, 2016",
                    "year": 2016
                },
                {
                    "title": "Conditional generative adversarial nets",
                    "authors": [
                        "M. Mirza",
                        "S. Osindero"
                    ],
                    "venue": "arXiv preprint arXiv:1411.1784,",
                    "year": 2014
                },
                {
                    "title": "Context encoders: Feature learning by inpainting",
                    "authors": [
                        "D. Pathak",
                        "P. Krahenbuhl",
                        "J. Donahue",
                        "T. Darrell",
                        "A.A. Efros"
                    ],
                    "venue": "CVPR, 2016",
                    "year": 2016
                },
                {
                    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
                    "authors": [
                        "A. Radford",
                        "L. Metz",
                        "S. Chintala"
                    ],
                    "venue": "arXiv preprint arXiv:1511.06434,",
                    "year": 2015
                },
                {
                    "title": "Radim Tyle\u010dek. Spatial pattern templates for recognition of objects with regular structure",
                    "authors": [
                        "\u0160. R"
                    ],
                    "venue": "In Proc. GCPR, Saarbrucken, Germany,",
                    "year": 2013
                },
                {
                    "title": "Generative adversarial text to image synthesis",
                    "authors": [
                        "S. Reed",
                        "Z. Akata",
                        "X. Yan",
                        "L. Logeswaran",
                        "B. Schiele",
                        "H. Lee"
                    ],
                    "venue": "arXiv preprint arXiv:1605.05396,",
                    "year": 2016
                },
                {
                    "title": "Color transfer between images",
                    "authors": [
                        "E. Reinhard",
                        "M. Ashikhmin",
                        "B. Gooch",
                        "P. Shirley"
                    ],
                    "venue": "IEEE Computer Graphics and Applications,",
                    "year": 2001
                },
                {
                    "title": "U-net: Convolutional networks for biomedical image segmentation",
                    "authors": [
                        "O. Ronneberger",
                        "P. Fischer",
                        "T. Brox"
                    ],
                    "venue": "In MIC- CAI,",
                    "year": 2015
                },
                {
                    "title": "Imagenet large scale visual recognition",
                    "authors": [
                        "O. Russakovsky",
                        "J. Deng",
                        "H. Su",
                        "J. Krause",
                        "S. Satheesh",
                        "S. Ma",
                        "Z. Huang",
                        "A. Karpathy",
                        "A. Khosla",
                        "M. Bernstein"
                    ],
                    "venue": "challenge. IJCV,",
                    "year": 2015
                },
                {
                    "title": "Improved techniques for training gans",
                    "authors": [
                        "T. Salimans",
                        "I. Goodfellow",
                        "W. Zaremba",
                        "V. Cheung",
                        "A. Radford",
                        "X. Chen"
                    ],
                    "venue": "arXiv preprint arXiv:1606.03498,",
                    "year": 2016
                },
                {
                    "title": "Data-driven hallucination of different times of day from a single outdoor photo",
                    "authors": [
                        "Y. Shih",
                        "S. Paris",
                        "F. Durand",
                        "W.T. Freeman"
                    ],
                    "venue": "ACM Transactions on Graphics (TOG),",
                    "year": 2013
                },
                {
                    "title": "Instance normalization: The missing ingredient for fast stylization",
                    "authors": [
                        "D. Ulyanov",
                        "A. Vedaldi",
                        "V. Lempitsky"
                    ],
                    "venue": "arXiv preprint arXiv:1607.08022,",
                    "year": 2016
                },
                {
                    "title": "Generative image modeling using style and structure adversarial networks",
                    "authors": [
                        "X. Wang",
                        "A. Gupta"
                    ],
                    "venue": "ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Image quality assessment: from error visibility to structural similarity",
                    "authors": [
                        "Z. Wang",
                        "A.C. Bovik",
                        "H.R. Sheikh",
                        "E.P. Simoncelli"
                    ],
                    "venue": "IEEE Transactions on Image Processing,",
                    "year": 2004
                },
                {
                    "title": "Top-down learning for structured labeling with convolutional pseudoprior",
                    "authors": [
                        "S. Xie",
                        "X. Huang",
                        "Z. Tu"
                    ],
                    "year": 2015
                },
                {
                    "title": "Holistically-nested edge detection",
                    "authors": [
                        "S. Xie",
                        "Z. Tu"
                    ],
                    "venue": "In ICCV,",
                    "year": 2015
                },
                {
                    "title": "Pixellevel domain transfer",
                    "authors": [
                        "D. Yoo",
                        "N. Kim",
                        "S. Park",
                        "A.S. Paek",
                        "I.S. Kweon"
                    ],
                    "venue": "ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Fine-Grained Visual Comparisons with Local Learning",
                    "authors": [
                        "A. Yu",
                        "K. Grauman"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Fine-grained visual comparisons with local learning",
                    "authors": [
                        "A. Yu",
                        "K. Grauman"
                    ],
                    "venue": "In CVPR, pages 192\u2013199,",
                    "year": 2014
                },
                {
                    "title": "Colorful image colorization",
                    "authors": [
                        "R. Zhang",
                        "P. Isola",
                        "A.A. Efros"
                    ],
                    "venue": "ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Energy-based generative adversarial network",
                    "authors": [
                        "J. Zhao",
                        "M. Mathieu",
                        "Y. LeCun"
                    ],
                    "venue": "arXiv preprint arXiv:1609.03126,",
                    "year": 2016
                },
                {
                    "title": "Learning temporal transformations from time-lapse videos",
                    "authors": [
                        "Y. Zhou",
                        "T.L. Berg"
                    ],
                    "venue": "In ECCV, 2016",
                    "year": 2016
                },
                {
                    "title": "Generative visual manipulation on the natural image manifold",
                    "authors": [
                        "J.-Y. Zhu",
                        "P. Kr\u00e4henb\u00fchl",
                        "E. Shechtman",
                        "A.A. Efros"
                    ],
                    "venue": "In ECCV, 2016",
                    "year": 2016
                }
            ],
            "id": "SP:072fd0b8d471f183da0ca9880379b3bb29031b6a",
            "authors": [
                {
                    "name": "Phillip Isola",
                    "affiliations": []
                },
                {
                    "name": "Jun-Yan Zhu",
                    "affiliations": []
                },
                {
                    "name": "Tinghui Zhou",
                    "affiliations": []
                },
                {
                    "name": "Alexei A. Efros",
                    "affiliations": []
                }
            ],
            "abstractText": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either. Many problems in image processing, computer graphics, and computer vision can be posed as \u201ctranslating\u201d an input image into a corresponding output image. Just as a concept may be expressed in either English or French, a scene may be rendered as an RGB image, a gradient field, an edge map, a semantic label map, etc. In analogy to automatic language translation, we define automatic image-to-image translation as the problem of translating one possible representation of a scene into another, given sufficient training data (see Figure 1). One reason language translation is difficult is because the mapping between languages is rarely one-to-one \u2013 any given concept is easier to express in one language than another. Similarly, most image-to-image translation problems are either many-to-one (computer vision) \u2013 mapping photographs to edges, segments, or semantic labels, or one-to-many (computer graphics) \u2013 mapping labels or sparse user inputs to realistic images. Traditionally, each of these tasks has been tackled with separate, special-purpose machinery (e.g., [7, 15, 11, 1, 3, 37, 21, 26, 9, 42, 46]), despite the fact that the setting is always the same: predict pixels from pixels. Our goal in this paper is to develop a common framework for all these problems. 1 ar X iv :1 61 1. 07 00 4v 1 [ cs .C V ] 2 1 N ov 2 01 6 The community has already taken significant steps in this direction, with convolutional neural nets (CNNs) becoming the common workhorse behind a wide variety of image prediction problems. CNNs learn to minimize a loss function \u2013 an objective that scores the quality of results \u2013 and although the learning process is automatic, a lot of manual effort still goes into designing effective losses. In other words, we still have to tell the CNN what we wish it to minimize. But, just like Midas, we must be careful what we wish for! If we take a naive approach, and ask the CNN to minimize Euclidean distance between predicted and ground truth pixels, it will tend to produce blurry results [29, 46]. This is because Euclidean distance is minimized by averaging all plausible outputs, which causes blurring. Coming up with loss functions that force the CNN to do what we really want \u2013 e.g., output sharp, realistic images \u2013 is an open problem and generally requires expert knowledge. It would be highly desirable if we could instead specify only a high-level goal, like \u201cmake the output indistinguishable from reality\u201d, and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recently proposed Generative Adversarial Networks (GANs) [14, 5, 30, 36, 47]. GANs learn a loss that tries to classify if the output image is real or fake, while simultaneously training a generative model to minimize this loss. Blurry images will not be tolerated since they look obviously fake. Because GANs learn a loss that adapts to the data, they can be applied to a multitude of tasks that traditionally would require very different kinds of loss functions. In this paper, we explore GANs in the conditional setting. Just as GANs learn a generative model of data, conditional GANs (cGANs) learn a conditional generative model [14]. This makes cGANs suitable for image-to-image translation tasks, where we condition on an input image and generate a corresponding output image. GANs have been vigorously studied in the last two years and many of the techniques we explore in this paper have been previously proposed. Nonetheless, earlier papers have focused on specific applications, and it has remained unclear how effective image-conditional GANs can be as a general-purpose solution for image-toimage translation. Our primary contribution is to demonstrate that on a wide variety of problems, conditional GANs produce reasonable results. Our second contribution is to present a simple framework sufficient to achieve good results, and to analyze the effects of several important architectural choices. Code is available at https://github.com/phillipi/pix2pix.",
            "title": "Image-to-Image Translation with Conditional Adversarial Networks"
        }
    },
    "8240954": {
        "X": {
            "sections": [
                {
                    "text": "Introduction\nDeep Learning has set new records at different benchmarks and led to various commercial applications [25, 33]. Recurrent neural networks (RNNs) [18] achieved new levels at speech and natural language processing, for example at the TIMIT benchmark [12] or at language translation [36], and are already employed in mobile devices [31]. RNNs have won handwriting recognition challenges (Chinese and Arabic handwriting) [33, 13, 6] and Kaggle challenges, such as the \u201cGrasp-and Lift EEG\u201d competition. Their counterparts, convolutional neural networks (CNNs) [24] excel at vision and video tasks. CNNs are on par with human dermatologists at the visual detection of skin cancer [9]. The visual processing for self-driving cars is based on CNNs [19], as is the visual input to AlphaGo which has beaten one of the best human GO players [34]. At vision challenges, CNNs are constantly winning, for example\nar X\niv :1\n70 6.\n02 51\n5v 1\n[ cs\n.L G\n] 8\nJ un\nat the large ImageNet competition [23, 16], but also almost all Kaggle vision challenges, such as the \u201cDiabetic Retinopathy\u201d and the \u201cRight Whale\u201d challenges [8, 14].\nHowever, looking at Kaggle challenges that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines (SVMs) are winning most of the competitions. Deep Learning is notably absent, and for the few cases where FNNs won, they are shallow. For example, the HIGGS challenge, the Merck Molecular Activity challenge, and the Tox21 Data challenge were all won by FNNs with at most four hidden layers. Surprisingly, it is hard to find success stories with FNNs that have many hidden layers, though they would allow for different levels of abstract representations of the input [3].\nTo robustly train very deep CNNs, batch normalization evolved into a standard to normalize neuron activations to zero mean and unit variance [20]. Layer normalization [2] also ensures zero mean and unit variance, while weight normalization [32] ensures zero mean and unit variance if in the previous layer the activations have zero mean and unit variance. However, training with normalization techniques is perturbed by stochastic gradient descent (SGD), stochastic regularization (like dropout), and the estimation of the normalization parameters. Both RNNs and CNNs can stabilize learning via weight sharing, therefore they are less prone to these perturbations. In contrast, FNNs trained with normalization techniques suffer from these perturbations and have high variance in the training error (see Figure 1). This high variance hinders learning and slows it down. Furthermore, strong regularization, such as dropout, is not possible as it would further increase the variance which in turn would lead to divergence of the learning process. We believe that this sensitivity to perturbations is the reason that FNNs are less successful than RNNs and CNNs.\nSelf-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance in their training errors (see Figure 1). SNNs push neuron activations to zero mean and unit variance thereby leading to the same effect as batch normalization, which enables to robustly learn many layers. SNNs are based on scaled exponential linear units \u201cSELUs\u201d which induce self-normalizing properties like variance stabilization which in turn avoids exploding and vanishing gradients.\nSelf-normalizing Neural Networks (SNNs)\nNormalization and SNNs. For a neural network with activation function f , we consider two consecutive layers that are connected by a weight matrix W . Since the input to a neural network is a random variable, the activations x in the lower layer, the network inputs z = Wx, and the activations y = f(z) in the higher layer are random variables as well. We assume that all activations xi of the lower layer have mean \u00b5 := E(xi) and variance \u03bd := Var(xi). An activation y in the higher layer has mean \u00b5\u0303 := E(y) and variance \u03bd\u0303 := Var(y). Here E(.) denotes the expectation and Var(.) the variance of a random variable. A single activation y = f(z) has net input z = wTx. For n units with activation xi, 1 6 i 6 n in the lower layer, we define n times the mean of the weight vector w \u2208 Rn as \u03c9 := \u2211n i=1 wi and n times the second moment as \u03c4 := \u2211n i=1 w 2 i .\nWe consider the mapping g that maps mean and variance of the activations from one layer to mean and variance of the activations in the next layer\n( \u00b5 \u03bd ) 7\u2192 ( \u00b5\u0303 \u03bd\u0303 ) : ( \u00b5\u0303 \u03bd\u0303 ) = g ( \u00b5 \u03bd ) . (1)\nNormalization techniques like batch, layer, or weight normalization ensure a mapping g that keeps (\u00b5, \u03bd) and (\u00b5\u0303, \u03bd\u0303) close to predefined values, typically (0, 1). Definition 1 (Self-normalizing neural net). A neural network is self-normalizing if it possesses a mapping g : \u2126 7\u2192 \u2126 for each activation y that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on (\u03c9, \u03c4) in \u2126. Furthermore, the mean and the variance remain in the domain \u2126, that is g(\u2126) \u2286 \u2126, where \u2126 = {(\u00b5, \u03bd) | \u00b5 \u2208 [\u00b5min, \u00b5max], \u03bd \u2208 [\u03bdmin, \u03bdmax]}. When iteratively applying the mapping g, each point within \u2126 converges to this fixed point.\nTherefore, we consider activations of a neural network to be normalized, if both their mean and their variance across samples are within predefined intervals. If mean and variance of x are already within\nthese intervals, then also mean and variance of y remain in these intervals, i.e., the normalization is transitive across layers. Within these intervals, the mean and variance both converge to a fixed point if the mapping g is applied iteratively.\nTherefore, SNNs keep normalization of activations when propagating them through layers of the network. The normalization effect is observed across layers of a network: in each layer the activations are getting closer to the fixed point. The normalization effect can also observed be for two fixed layers across learning steps: perturbations of lower layer activations or weights are damped in the higher layer by drawing the activations towards the fixed point. If for all y in the higher layer, \u03c9 and \u03c4 of the corresponding weight vector are the same, then the fixed points are also the same. In this case we have a unique fixed point for all activations y. Otherwise, in the more general case, \u03c9 and \u03c4 differ for different y but the mean activations are drawn into [\u00b5min, \u00b5max] and the variances are drawn into [\u03bdmin, \u03bdmax].\nConstructing Self-Normalizing Neural Networks. We aim at constructing self-normalizing neural networks by adjusting the properties of the function g. Only two design choices are available for the function g: (1) the activation function and (2) the initialization of the weights.\nFor the activation function, we propose \u201cscaled exponential linear units\u201d (SELUs) to render a FNN as self-normalizing. The SELU activation function is given by\nselu(x) = \u03bb { x if x > 0 \u03b1ex \u2212 \u03b1 if x 6 0 . (2)\nSELUs allow to construct a mapping g with properties that lead to SNNs. SNNs cannot be derived with (scaled) rectified linear units (ReLUs), sigmoid units, tanh units, and leaky ReLUs. The activation function is required to have (1) negative and positive values for controlling the mean, (2) saturation regions (derivatives approaching zero) to dampen the variance if it is too large in the lower layer, (3) a slope larger than one to increase the variance if it is too small in the lower layer, (4) a continuous curve. The latter ensures a fixed point, where variance damping is equalized by variance increasing. We met these properties of the activation function by multiplying the exponential linear unit (ELU) [7] with \u03bb > 1 to ensure a slope larger than one for positive net inputs.\nFor the weight initialization, we propose \u03c9 = 0 and \u03c4 = 1 for all units in the higher layer. The next paragraphs will show the advantages of this initialization. Of course, during learning these assumptions on the weight vector will be violated. However, we can prove the self-normalizing property even for weight vectors that are not normalized, therefore, the self-normalizing property can be kept during learning and weight changes.\nDeriving the Mean and Variance Mapping Function g. We assume that the xi are independent from each other but share the same mean \u00b5 and variance \u03bd. Of course, the independence assumptions is not fulfilled in general. We will elaborate on the independence assumption below. The network input z in the higher layer is z = wTx for which we can infer the following moments E(z) =\u2211n i=1 wi E(xi) = \u00b5 \u03c9 and Var(z) = Var( \u2211n i=1 wi xi) = \u03bd \u03c4 , where we used the independence of the xi. The net input z is a weighted sum of independent, but not necessarily identically distributed variables xi, for which the central limit theorem (CLT) states that z approaches a normal distribution: z \u223c N (\u00b5\u03c9, \u221a \u03bd\u03c4) with density pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4). According to the CLT, the larger n, the closer is z to a normal distribution. For Deep Learning, broad layers with hundreds of neurons xi are common. Therefore the assumption that z is normally distributed is met well for most currently used neural networks (see Figure A8). The function g maps the mean and variance of activations in the lower layer to the mean \u00b5\u0303 = E(y) and variance \u03bd\u0303 = Var(y) of the activations y in the next layer:\ng : ( \u00b5 \u03bd ) 7\u2192 ( \u00b5\u0303 \u03bd\u0303 ) : \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = \u222b \u221e \u2212\u221e selu(z) pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4) dz (3)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = \u222b \u221e \u2212\u221e selu(z)2 pN(z;\u00b5\u03c9, \u221a \u03bd\u03c4) dz \u2212 (\u00b5\u0303)2 .\nThese integrals can be analytically computed and lead to following mappings of the moments:\n\u00b5\u0303 = 1\n2 \u03bb\n( (\u00b5\u03c9) erf ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (4)\n\u03b1 e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u03b1 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) + \u00b5\u03c9 )\n\u03bd\u0303 = 1 2 \u03bb2 (( (\u00b5\u03c9)2 + \u03bd\u03c4 )( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u03b12 ( \u22122e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) (5)\n+e2(\u00b5\u03c9+\u03bd\u03c4) erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 (\u00b5\u03c9) \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) ) \u2212 (\u00b5\u0303)2\nStable and Attracting Fixed Point (0,1) for Normalized Weights. We assume a normalized weight vector w with \u03c9 = 0 and \u03c4 = 1. Given a fixed point (\u00b5, \u03bd), we can solve equations Eq. (4) and Eq. (5) for \u03b1 and \u03bb. We chose the fixed point (\u00b5, \u03bd) = (0, 1), which is typical for activation normalization. We obtain the fixed point equations \u00b5\u0303 = \u00b5 = 0 and \u03bd\u0303 = \u03bd = 1 that we solve for \u03b1 and \u03bb and obtain the solutions \u03b101 \u2248 1.6733 and \u03bb01 \u2248 1.0507, where the subscript 01 indicates that these are the parameters for fixed point (0, 1). The analytical expressions for \u03b101 and \u03bb01 are given in Eq. (14). We are interested whether the fixed point (\u00b5, \u03bd) = (0, 1) is stable and attracting. If the Jacobian of g has a norm smaller than 1 at the fixed point, then g is a contraction mapping and the fixed point is stable. The (2x2)-Jacobian J (\u00b5, \u03bd) of g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) evaluated at the fixed point (0, 1) with \u03b101 and \u03bb01 is\nJ (\u00b5, \u03bd) = \u2202 \u00b5 new(\u00b5,\u03bd) \u2202\u00b5 \u2202 \u00b5new(\u00b5,\u03bd) \u2202\u03bd\n\u2202 \u03bd new(\u00b5,\u03bd) \u2202\u00b5 \u2202 \u03bdnew(\u00b5,\u03bd) \u2202\u03bd\n , J (0, 1) = (0.0 0.0888340.0 0.782648 ) . (6)\nThe spectral norm of J (0, 1) (its largest singular value) is 0.7877 < 1. That means g is a contraction mapping around the fixed point (0, 1) (the mapping is depicted in Figure 2). Therefore, (0, 1) is a stable fixed point of the mapping g.\nStable and Attracting Fixed Points for Unnormalized Weights. A normalized weight vector w cannot be ensured during learning. For SELU parameters \u03b1 = \u03b101 and \u03bb = \u03bb01, we show in the next theorem that if (\u03c9, \u03c4) is close to (0, 1), then g still has an attracting and stable fixed point that is close to (0, 1). Thus, in the general case there still exists a stable fixed point which, however, depends on (\u03c9, \u03c4). If we restrict (\u00b5, \u03bd, \u03c9, \u03c4) to certain intervals, then we can show that (\u00b5, \u03bd) is mapped to\nthe respective intervals. Next we present the central theorem of this paper, from which follows that SELU networks are self-normalizing under mild conditions on the weights.\nTheorem 1 (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the following intervals \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1], that define the functions\u2019 domain \u2126. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (3) has the stable fixed point (\u00b5, \u03bd) = (0, 1), whereas for other \u03c9 and \u03c4 the mapping Eq. (3) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (3) to this fixed point.\nProof. We provide a proof sketch (see detailed proof in Appendix Section A3). With the Banach fixed point theorem we show that there exists a unique attracting and stable fixed point. To this end, we have to prove that a) g is a contraction mapping and b) that the mapping stays in the domain, that is, g(\u2126) \u2286 \u2126. The spectral norm of the Jacobian of g can be obtained via an explicit formula for the largest singular value for a 2\u00d7 2 matrix. g is a contraction mapping if its spectral norm is smaller than 1. We perform a computer-assisted proof to evaluate the largest singular value on a fine grid and ensure the precision of the computer evaluation by an error propagation analysis of the implemented algorithms on the according hardware. Singular values between grid points are upper bounded by the mean value theorem. To this end, we bound the derivatives of the formula for the largest singular value with respect to \u03c9, \u03c4, \u00b5, \u03bd. Then we apply the mean value theorem to pairs of points, where one is on the grid and the other is off the grid. This shows that for all values of \u03c9, \u03c4, \u00b5, \u03bd in the domain \u2126, the spectral norm of g is smaller than one. Therefore, g is a contraction mapping on the domain \u2126. Finally, we show that the mapping g stays in the domain \u2126 by deriving bounds on \u00b5\u0303 and \u03bd\u0303. Hence, the Banach fixed-point theorem holds and there exists a unique fixed point in \u2126 that is attained.\nConsequently, feed-forward neural networks with many units in each layer and with the SELU activation function are self-normalizing (see definition 1), which readily follows from Theorem 1. To give an intuition, the main property of SELUs is that they damp the variance for negative net inputs and increase the variance for positive net inputs. The variance damping is stronger if net inputs are further away from zero while the variance increase is stronger if net inputs are close to zero. Thus, for large variance of the activations in the lower layer the damping effect is dominant and the variance decreases in the higher layer. Vice versa, for small variance the variance increase is dominant and the variance increases in the higher layer.\nHowever, we cannot guarantee that mean and variance remain in the domain \u2126. Therefore, we next treat the case where (\u00b5, \u03bd) are outside \u2126. It is especially crucial to consider \u03bd because this variable has much stronger influence than \u00b5. Mapping \u03bd across layers to a high value corresponds to an exploding gradient, since the Jacobian of the activation of high layers with respect to activations in lower layers has large singular values. Analogously, mapping \u03bd across layers to a low value corresponds to an vanishing gradient. Bounding the mapping of \u03bd from above and below would avoid both exploding and vanishing gradients. Theorem 2 states that the variance of neuron activations of\nSNNs is bounded from above, and therefore ensures that SNNs learn robustly and do not suffer from exploding gradients.\nTheorem 2 (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126+: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5): \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd.\nThe proof can be found in the Appendix Section A3. Thus, when mapped across many layers, the variance in the interval [3, 16] is mapped to a value below 3. Consequently, all fixed points (\u00b5, \u03bd) of the mapping g (Eq. (3)) have \u03bd < 3. Analogously, Theorem 3 states that the variance of neuron activations of SNNs is bounded from below, and therefore ensures that SNNs do not suffer from vanishing gradients.\nTheorem 3 (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126\u2212: \u22120.1 6 \u00b5 6 0.1, and \u22120.1 6 \u03c9 6 0.1. For the domain 0.02 6 \u03bd 6 0.16 and 0.8 6 \u03c4 6 1.25 as well as for the domain 0.02 6 \u03bd 6 0.24 and 0.9 6 \u03c4 6 1.25, the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases: \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd.\nThe proof can be found in the Appendix Section A3. All fixed points (\u00b5, \u03bd) of the mapping g (Eq. (3)) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) ensures a lower bound on the variance \u03bd. Therefore SELU networks control the variance of the activations and push it into an interval, whereafter the mean and variance move toward the fixed point. Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the mean, too. In all experiments, we observed that self-normalizing neural networks push the mean and variance of activations into the domain \u2126 .\nInitialization. Since SNNs have a fixed point at zero mean and unit variance for normalized weights \u03c9 = \u2211n i=1 wi = 0 and \u03c4 = \u2211n i=1 w 2 i = 1 (see above), we initialize SNNs such that these constraints are fulfilled in expectation. We draw the weights from a Gaussian distribution with E(wi) = 0 and variance Var(wi) = 1/n. Uniform and truncated Gaussian distributions with these moments led to networks with similar behavior. The \u201cMSRA initialization\u201d is similar since it uses zero mean and variance 2/n to initialize the weights [17]. The additional factor 2 counters the effect of rectified linear units.\nNew Dropout Technique. Standard dropout randomly sets an activation x to zero with probability 1 \u2212 q for 0 < q 6 1. In order to preserve the mean, the activations are scaled by 1/q during training. If x has mean E(x) = \u00b5 and variance Var(x) = \u03bd, and the dropout variable d follows a binomial distribution B(1, q), then the mean E(1/qdx) = \u00b5 is kept. Dropout fits well to rectified linear units, since zero is in the low variance region and corresponds to the default value. For scaled exponential linear units, the default and low variance value is limx\u2192\u2212\u221e selu(x) = \u2212\u03bb\u03b1 = \u03b1\u2032. Therefore, we propose \u201calpha dropout\u201d, that randomly sets inputs to \u03b1\u2032. The new mean and new variance is E(xd + \u03b1\u2032(1 \u2212 d)) = q\u00b5 + (1 \u2212 q)\u03b1\u2032, and Var(xd + \u03b1\u2032(1 \u2212 d)) = q((1 \u2212 q)(\u03b1\u2032 \u2212 \u00b5)2 + \u03bd). We aim at keeping mean and variance to their original values after \u201calpha dropout\u201d, in order to ensure the self-normalizing property even for \u201calpha dropout\u201d. The affine transformation a(xd + \u03b1\u2032(1 \u2212 d)) + b allows to determine parameters a and b such that mean and variance are kept to their values: E(a(xd + \u03b1\u2032(1 \u2212 d)) + b) = \u00b5 and Var(a(xd + \u03b1\u2032(1 \u2212 d)) + b) = \u03bd . In contrast to dropout, a and b will depend on \u00b5 and \u03bd, however our SNNs converge to activations with zero mean and unit variance. With \u00b5 = 0 and \u03bd = 1, we obtain a = ( q + \u03b1\u20322q(1\u2212 q) )\u22121/2 and\nb = \u2212 ( q + \u03b1\u20322q(1\u2212 q) )\u22121/2 ((1\u2212 q)\u03b1\u2032). The parameters a and b only depend on the dropout rate 1\u2212 q and the most negative activation \u03b1\u2032. Empirically, we found that dropout rates 1\u2212 q = 0.05 or 0.10 lead to models with good performance. \u201cAlpha-dropout\u201d fits well to scaled exponential linear units by randomly setting activations to the negative saturation value.\nApplicability of the central limit theorem and independence assumption. In the derivative of the mapping (Eq. (3)), we used the central limit theorem (CLT) to approximate the network inputs z = \u2211n i=1 wixi with a normal distribution. We justified normality because network inputs represent a weighted sum of the inputs xi, where for Deep Learning n is typically large. The Berry-Esseen\ntheorem states that the convergence rate to normality is n\u22121/2 [22]. In the classical version of the CLT, the random variables have to be independent and identically distributed, which typically does not hold for neural networks. However, the Lyapunov CLT does not require the variable to be identically distributed anymore. Furthermore, even under weak dependence, sums of random variables converge in distribution to a Gaussian distribution [5].\nExperiments\nWe compare SNNs to other deep networks at different benchmarks. Hyperparameters such as number of layers (blocks), neurons per layer, learning rate, and dropout rate, are adjusted by grid-search for each dataset on a separate validation set (see Section A4). We compare the following FNN methods:\n\u2022 \u201cMSRAinit\u201d: FNNs without normalization and with ReLU activations and \u201cMicrosoft weight initialization\u201d [17].\n\u2022 \u201cMSRAinit\u201d: FNNs without normalization and with ReLU activations and \u201cMicrosoft weight initialization\u201d [17].\n\u2022 \u201cBatchNorm\u201d: FNNs with batch normalization [20]. \u2022 \u201cLayerNorm\u201d: FNNs with layer normalization [2]. \u2022 \u201cWeightNorm\u201d: FNNs with weight normalization [32]. \u2022 \u201cHighway\u201d: Highway networks [35]. \u2022 \u201cResNet\u201d: Residual networks [16] adapted to FNNs using residual blocks with 2 or 3 layers\nwith rectangular or diavolo shape.\n\u2022 \u201cSNNs\u201d: Self normalizing networks with SELUs with \u03b1 = \u03b101 and \u03bb = \u03bb01 and the proposed dropout technique and initialization strategy.\n121 UCI Machine Learning Repository datasets. The benchmark comprises 121 classification datasets from the UCI Machine Learning repository [10] from diverse application areas, such as physics, geology, or biology. The size of the datasets ranges between 10 and 130, 000 data points and the number of features from 4 to 250. In abovementioned work [10], there were methodological mistakes [37] which we avoided here. Each compared FNN method was optimized with respect to its architecture and hyperparameters on a validation set that was then removed from the subsequent analysis. The selected hyperparameters served to evaluate the methods in terms of accuracy on the pre-defined test sets (details on the hyperparameter selection are given in Section A4). The accuracies are reported in the Table A11. We ranked the methods by their accuracy for each prediction task and compared their average ranks. SNNs significantly outperform all competing networks in pairwise comparisons (paired Wilcoxon test across datasets) as reported in Table 1 (left panel).\nWe further included 17 machine learning methods representing diverse method groups [10] in the comparison and the grouped the data sets into \u201csmall\u201d and \u201clarge\u201d data sets (for details see Section A4). On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs and other FNNs. On 46 larger datasets with at least 1000 data points, SNNs show the highest performance followed by SVMs and random forests (see right panel of Table 1, for complete results see Tables A12 and A12). Overall, SNNs have outperformed state of the art machine learning methods on UCI datasets with more than 1,000 data points.\nTypically, hyperparameter selection chose SNN architectures that were much deeper than the selected architectures of other FNNs, with an average depth of 10.8 layers, compared to average depths of 6.0 for BatchNorm, 3.8 WeightNorm, 7.0 LayerNorm, 5.9 Highway, and 7.1 for MSRAinit networks. For ResNet, the average number of blocks was 6.35. SNNs with many more than 4 layers often provide the best predictive accuracies across all neural networks.\nDrug discovery: The Tox21 challenge dataset. The Tox21 challenge dataset comprises about 12,000 chemical compounds whose twelve toxic effects have to be predicted based on their chemical structure. We used the validation sets of the challenge winners for hyperparameter selection (see\nSection A4) and the challenge test set for performance comparison. We repeated the whole evaluation procedure 5 times to obtain error bars. The results in terms of average AUC are given in Table 2. In 2015, the challenge organized by the US NIH was won by an ensemble of shallow ReLU FNNs which achieved an AUC of 0.846 [28]. Besides FNNs, this ensemble also contained random forests and SVMs. Single SNNs came close with an AUC of 0.845\u00b10.003. The best performing SNNs have 8 layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers. Also batchnorm and weightnorm networks, typically perform best with shallow networks of 2 to 4 layers (Table 2). The deeper the networks, the larger the difference in performance between SNNs and other methods (see columns 5\u20138 of Table 2). The best performing method is an SNN with 8 layers.\nSNN 83.7 \u00b1 0.3 84.4 \u00b1 0.5 84.2 \u00b1 0.4 83.9 \u00b1 0.5 84.5 \u00b1 0.2 83.5 \u00b1 0.5 82.5 \u00b1 0.7 Batchnorm 80.0 \u00b1 0.5 79.8 \u00b1 1.6 77.2 \u00b1 1.1 77.0 \u00b1 1.7 75.0 \u00b1 0.9 73.7 \u00b1 2.0 76.0 \u00b1 1.1 WeightNorm 83.7 \u00b1 0.8 82.9 \u00b1 0.8 82.2 \u00b1 0.9 82.5 \u00b1 0.6 81.9 \u00b1 1.2 78.1 \u00b1 1.3 56.6 \u00b1 2.6 LayerNorm 84.3 \u00b1 0.3 84.3 \u00b1 0.5 84.0 \u00b1 0.2 82.5 \u00b1 0.8 80.9 \u00b1 1.8 78.7 \u00b1 2.3 78.8 \u00b1 0.8 Highway 83.3 \u00b1 0.9 83.0 \u00b1 0.5 82.6 \u00b1 0.9 82.4 \u00b1 0.8 80.3 \u00b1 1.4 80.3 \u00b1 2.4 79.6 \u00b1 0.8 MSRAinit 82.7 \u00b1 0.4 81.6 \u00b1 0.9 81.1 \u00b1 1.7 80.6 \u00b1 0.6 80.9 \u00b1 1.1 80.2 \u00b1 1.1 80.4 \u00b1 1.9 ResNet 82.2 \u00b1 1.1 80.0 \u00b1 2.0 80.5 \u00b1 1.2 81.2 \u00b1 0.7 81.8 \u00b1 0.6 81.2 \u00b1 0.6 na\nAstronomy: Prediction of pulsars in the HTRU2 dataset. Since a decade, machine learning methods have been used to identify pulsars in radio wave signals [27]. Recently, the High Time Resolution Universe Survey (HTRU2) dataset has been released with 1,639 real pulsars and 16,259 spurious signals. Currently, the highest AUC value of a 10-fold cross-validation is 0.976 which has been achieved by Naive Bayes classifiers followed by decision tree C4.5 with 0.949 and SVMs with 0.929. We used eight features constructed by the PulsarFeatureLab as used previously [27]. We assessed the performance of FNNs using 10-fold nested cross-validation, where the hyperparameters were selected in the inner loop on a validation set (for details on the hyperparameter selection see\nSection A4). Table 3 reports the results in terms of AUC. SNNs outperform all other methods and have pushed the state-of-the-art to an AUC of 0.98.\nWe have introduced self-normalizing neural networks for which we have proved that neuron activations are pushed towards zero mean and unit variance when propagated through the network. Additionally, for activations not close to unit variance, we have proved an upper and lower bound on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient problems. Therefore, SNNs work well for architectures with many layers, allowed us to introduce a novel regularization scheme, and learn very robustly. On 121 UCI benchmark datasets, SNNs have outperformed other FNNs with and without normalization techniques, such as batch, layer, and weight normalization, or specialized architectures, such as Highway or Residual networks. SNNs also yielded the best results on drug discovery and astronomy tasks. The best performing SNN architectures are typically very deep in contrast to other FNNs.\nReferences\nThe references are provided in Section A7.\nAppendix\nContents\nA1 Background 10\nA2 Theorems 12\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1) . . . . . . . . . . . . 12\nA2.2 Theorem 2: Decreasing Variance from Above . . . . . . . . . . . . . . . . . . . . 12\nA2.3 Theorem 3: Increasing Variance from Below . . . . . . . . . . . . . . . . . . . . . 12\nA3 Proofs of the Theorems 13\nA3.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA3.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nA3.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA3.4 Lemmata and Other Tools Required for the Proofs . . . . . . . . . . . . . . . . . . 19\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one 19\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain . . . . 28\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting . . . . . . . 29\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding . . . . . . . 32\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1. . . 33\nA3.4.6 Intermediate Lemmata and Proofs . . . . . . . . . . . . . . . . . . . . . . 37\nA4 Additional information on experiments 84\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters . . . . . . . . . 85\nA4.2 121 UCI Machine Learning Repository data sets: detailed results . . . . . . . . . . 87\nA4.3 Tox21 challenge data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . 92\nA4.4 HTRU2 data set: Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nA5 Other fixed points 97\nA6 Bounds determined by numerical methods 97\nA7 References 98\nList of figures 100\nList of tables 100\nBrief index 102\nThis appendix is organized as follows: the first section sets the background, definitions, and formulations. The main theorems are presented in the next section. The following section is devoted to the proofs of these theorems. The next section reports additional results and details on the performed computational experiments, such as hyperparameter selection. The last section shows that our theoretical bounds can be confirmed by numerical methods as a sanity check.\nThe proof of theorem 1 is based on the Banach\u2019s fixed point theorem for which we require (1) a contraction mapping, which is proved in Subsection A3.4.1 and (2) that the mapping stays within its domain, which is proved in Subsection A3.4.2 For part (1), the proof relies on the main Lemma 12, which is a computer-assisted proof, and can be found in Subsection A3.4.1. The validity of the computer-assisted proof is shown in Subsection A3.4.5 by error analysis and the precision of the functions\u2019 implementation. The last Subsection A3.4.6 compiles various lemmata with intermediate results that support the proofs of the main lemmata and theorems.\nA1 Background\nWe consider a neural network with activation function f and two consecutive layers that are connected by weight matrix W . Since samples that serve as input to the neural network are chosen according to a distribution, the activations x in the lower layer, the network inputs z = Wx, and activations y = f(z) in the higher layer are all random variables. We assume that all units xi in the lower layer have mean activation \u00b5 := E(xi) and variance of the activation \u03bd := Var(xi) and a unit y in the higher layer has mean activation \u00b5\u0303 := E(y) and variance \u03bd\u0303 := Var(y). Here E(.)\ndenotes the expectation and Var(.) the variance of a random variable. For activation of unit y, we have net input z = wTx and the scaled exponential linear unit (SELU) activation y = selu(z), with\nselu(x) = \u03bb { x if x > 0 \u03b1ex \u2212 \u03b1 if x 6 0 . (7)\nFor n units xi, 1 6 i 6 n in the lower layer and the weight vector w \u2208 Rn, we define n times the mean by \u03c9 := \u2211n i=1 wi and n times the second moment by \u03c4 := \u2211n i=1 w 2 i .\nWe define a mapping g from mean \u00b5 and variance \u03bd of one layer to the mean \u00b5\u0303 and variance \u03bd\u0303 in the next layer:\ng : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) . (8)\nFor neural networks with scaled exponential linear units, the mean is of the activations in the next layer computed according to\n\u00b5\u0303 = \u222b 0 \u2212\u221e \u03bb\u03b1(exp(z)\u2212 1)pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz + \u222b \u221e 0 \u03bbzpGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz , (9)\nand the second moment of the activations in the next layer is computed according to\n\u03be\u0303 = \u222b 0 \u2212\u221e \u03bb2\u03b12(exp(z)\u2212 1)2pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz + \u222b \u221e 0 \u03bb2z2pGauss(z;\u00b5\u03c9, \u221a \u03bd\u03c4)dz . (10)\nTherefore, the expressions \u00b5\u0303 and \u03bd\u0303 have the following form:\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2 \u03bb\n( \u2212(\u03b1+ \u00b5\u03c9) erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (11)\n\u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 + 2\u00b5\u03c9 ) \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1))2 (12)\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1 2 \u03bb2 (( (\u00b5\u03c9)2 + \u03bd\u03c4 )( erf ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 1 ) + (13)\n\u03b12 ( \u22122e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + e2(\u00b5\u03c9+\u03bd\u03c4) erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 (\u00b5\u03c9) \u221a \u03bd\u03c4e\u2212 (\u00b5\u03c9)2 2(\u03bd\u03c4) )\nWe solve equations Eq. 4 and Eq. 5 for fixed points \u00b5\u0303 = \u00b5 and \u03bd\u0303 = \u03bd. For a normalized weight vector with \u03c9 = 0 and \u03c4 = 1 and the fixed point (\u00b5, \u03bd) = (0, 1), we can solve equations Eq. 4 and Eq. 5 for \u03b1 and \u03bb. We denote the solutions to fixed point (\u00b5, \u03bd) = (0, 1) by \u03b101 and \u03bb01.\n\u03b101 = \u2212\n\u221a 2 \u03c0\nerfc (\n1\u221a 2\n) exp ( 1 2 ) \u2212 1 \u2248 1.67326 (14)\n\u03bb01 =\n( 1\u2212 erfc ( 1\u221a 2 )\u221a e ) 2\u03c0(\n2 erfc (\u221a 2 ) e2 + \u03c0 erfc ( 1\u221a 2 )2 e\u2212 2(2 + \u03c0) erfc ( 1\u221a 2 )\u221a e+ \u03c0 + 2 )\u22121/2 \u03bb01 \u2248 1.0507 .\nThe parameters \u03b101 and \u03bb01 ensure\n\u00b5\u0303(0, 0, 1, 1, \u03bb01, \u03b101) = 0\n\u03bd\u0303(0, 0, 1, 1, \u03bb01, \u03b101) = 1\nSince we focus on the fixed point (\u00b5, \u03bd) = (0, 1), we assume throughout the analysis that \u03b1 = \u03b101 and \u03bb = \u03bb01. We consider the functions \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101), \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101), and \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) on the domain \u2126 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.95, 1.1]}. Figure 2 visualizes the mapping g for \u03c9 = 0 and \u03c4 = 1 and \u03b101 and \u03bb01 at few pre-selected points. It can be seen that (0, 1) is an attracting fixed point of the mapping g.\nA2 Theorems\nA2.1 Theorem 1: Stable and Attracting Fixed Points Close to (0,1)\nTheorem 1 shows that the mapping g defined by Eq. (4) and Eq. (5) exhibits a stable and attracting fixed point close to zero mean and unit variance. Theorem 1 establishes the self-normalizing property of self-normalizing neural networks (SNNs). The stable and attracting fixed point leads to robust learning through many layers. Theorem 1 (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1]. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (\u00b5, \u03bd) = (0, 1). For other \u03c9 and \u03c4 the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nA2.2 Theorem 2: Decreasing Variance from Above\nThe next Theorem 2 states that the variance of unit activations does not explode through consecutive layers of self-normalizing networks. Even more, a large variance of unit activations decreases when propagated through the network. In particular this ensures that exploding gradients will never be observed. In contrast to the domain in previous subsection, in which \u03bd \u2208 [0.8, 1.5], we now consider a domain in which the variance of the inputs is higher \u03bd \u2208 [3, 16] and even the range of the mean is increased \u00b5 \u2208 [\u22121, 1]. We denote this new domain with the symbol \u2126++ to indicate that the variance lies above the variance of the original domain \u2126. In \u2126++, we can show that the variance \u03bd\u0303 in the next layer is always smaller then the original variance \u03bd. Concretely, this theorem states that: Theorem 2 (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126++: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25 we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd . (15) The variance decreases in [3, 16] and all fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) have \u03bd < 3.\nA2.3 Theorem 3: Increasing Variance from Below\nThe next Theorem 3 states that the variance of unit activations does not vanish through consecutive layers of self-normalizing networks. Even more, a small variance of unit activations increases when propagated through the network. In particular this ensures that vanishing gradients will never be observed. In contrast to the first domain, in which \u03bd \u2208 [0.8, 1.5], we now consider two domains \u2126\u22121 and \u2126\u22122 in which the variance of the inputs is lower 0.05 6 \u03bd 6 0.16 and 0.05 6 \u03bd 6 0.24, and even the parameter \u03c4 is different 0.9 6 \u03c4 6 1.25 to the original \u2126. We denote this new domain with the symbol \u2126\u2212i to indicate that the variance lies below the variance of the original domain \u2126. In \u2126 \u2212 1 and \u2126\u22122 , we can show that the variance \u03bd\u0303 in the next layer is always larger then the original variance \u03bd, which means that the variance does not vanish through consecutive layers of self-normalizing networks. Concretely, this theorem states that:\nTheorem 3 (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the two domains \u2126\u22121 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.16, 0.8 6 \u03c4 6 1.25} and \u2126\u22122 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.9 6 \u03c4 6 1.25}. The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd (16)\nin both \u2126\u22121 and \u2126 \u2212 2 . All fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance \u03bd.\nA3 Proofs of the Theorems\nA3.1 Proof of Theorem 1\nWe have to show that the mapping g defined by Eq. (4) and Eq. (5) has a stable and attracting fixed point close to (0, 1). To proof this statement and Theorem 1, we apply the Banach fixed point theorem which requires (1) that g is a contraction mapping and (2) that g does not map outside the function\u2019s domain, concretely:\nTheorem 4 (Banach Fixed Point Theorem). Let (X, d) be a non-empty complete metric space with a contraction mapping f : X \u2192 X . Then f has a unique fixed-point xf \u2208 X with f(xf ) = xf . Every sequence xn = f(xn\u22121) with starting element x0 \u2208 X converges to the fixed point: xn \u2212\u2212\u2212\u2212\u2192\nn\u2192\u221e xf .\nContraction mappings are functions that map two points such that their distance is decreasing:\nDefinition 2 (Contraction mapping). A function f : X \u2192 X on a metric space X with distance d is a contraction mapping, if there is a 0 6 \u03b4 < 1, such that for all points u and v in X: d(f(u), f(v)) 6 \u03b4d(u,v).\nTo show that g is a contraction mapping in \u2126 with distance \u2016.\u20162, we use the Mean Value Theorem for u, v \u2208 \u2126\n\u2016g(u)\u2212 g(v)\u20162 6M \u2016u\u2212 v\u20162, (17)\nin which M is an upper bound on the spectral norm the JacobianH of g. The spectral norm is given by the largest singular value of the Jacobian of g. If the largest singular value of the Jacobian is smaller than 1, the mapping g of the mean and variance to the mean and variance in the next layer is contracting. We show that the largest singular value is smaller than 1 by evaluating the function for the singular value S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. To this end, we have to bound the gradient of S with respect to (\u00b5, \u03c9, \u03bd, \u03c4). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1 (Lemma 12). To show that the mapping does not map outside the function\u2019s domain, we derive bounds on the expressions for the mean and the variance (Lemma 13). Section A3.4.1 and Section A3.4.2 are concerned with the contraction mapping and the image of the function domain of g, respectively.\nWith the results that the largest singular value of the Jacobian is smaller than one (Lemma 12) and that the mapping stays in the domain \u2126 (Lemma 13), we can prove Theorem 1. We first recall Theorem 1:\nTheorem (Stable and Attracting Fixed Points). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.95, 1.1]. For \u03c9 = 0 and \u03c4 = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (\u00b5, \u03bd) = (0, 1). For other \u03c9 and \u03c4 the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (\u03c9, \u03c4) in the (\u00b5, \u03bd)-domain: \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. All points within the (\u00b5, \u03bd)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.\nProof. According to Lemma 12 the mapping g (Eq. (4) and Eq. (5)) is a contraction mapping in the given domain, that is, it has a Lipschitz constant smaller than one. We showed that (\u00b5, \u03bd) = (0, 1) is a fixed point of the mapping for (\u03c9, \u03c4) = (0, 1).\nThe domain is compact (bounded and closed), therefore it is a complete metric space. We further have to make sure the mapping g does not map outside its domain \u2126. According to Lemma 13, the mapping maps into the domain \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617]. Now we can apply the Banach fixed point theorem given in Theorem 4 from which the statement of the theorem follows.\nA3.2 Proof of Theorem 2\nFirst we recall Theorem 2: Theorem (Decreasing \u03bd). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126++: \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 3 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25 we have for the mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5)\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < \u03bd . (18)\nThe variance decreases in [3, 16] and all fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) have \u03bd < 3.\nProof. We start to consider an even larger domain \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25. We prove facts for this domain and later restrict to 3 6 \u03bd 6 16, i.e. \u2126++. We consider the function g of the difference between the second moment \u03be\u0303 in the next layer and the variance \u03bd in the lower layer:\ng(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 \u03bd . (19)\nIf we can show that g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < 0 for all (\u00b5, \u03c9, \u03bd, \u03c4) \u2208 \u2126++, then we would obtain our desired result \u03bd\u0303 6 \u03be\u0303 < \u03bd. The derivative with respect to \u03bd is according to Theorem 16:\n\u2202\n\u2202\u03bd g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 1 < 0 . (20)\nTherefore g is strictly monotonically decreasing in \u03bd. Since \u03be\u0303 is a function in \u03bd\u03c4 (these variables only appear as this product), we have for x = \u03bd\u03c4\n\u2202\n\u2202\u03bd \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03bd = \u2202 \u2202x \u03be\u0303 \u03c4 (21)\nand \u2202\n\u2202\u03c4 \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03c4 = \u2202 \u2202x \u03be\u0303 \u03bd . (22)\nTherefore we have according to Theorem 16:\n\u2202\n\u2202\u03c4 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u03bd\n\u03c4\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > 0 . (23)\nTherefore \u2202\n\u2202\u03c4 g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03c4 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > 0 . (24)\nConsequently, g is strictly monotonically increasing in \u03c4 . Now we consider the derivative with respect to \u00b5 and \u03c9. We start with \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), which is\n\u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (25)\n\u03bb2\u03c9 ( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) .\nWe consider the sub-function\u221a 2\n\u03c0\n\u221a \u03bd\u03c4 \u2212 \u03b12 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (26)\nWe set x = \u03bd\u03c4 and y = \u00b5\u03c9 and obtain\u221a 2\n\u03c0\n\u221a x\u2212 \u03b12 ( e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 e ( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n)) . (27)\nThe derivative to this sub-function with respect to y is \u03b12 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y\u221a 2 \u221a x\n) \u2212 e (x+y)2 2x (x+ y) erfc ( x+y\u221a 2 \u221a x )) x = (28)\n\u221a 2\u03b12 \u221a x  e (2x+y)22x (x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 e (x+y)2 2x (x+y) erfc ( x+y\u221a 2 \u221a x ) \u221a 2 \u221a x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y. The derivative to this sub-function with respect to x is\n1 2 \u221a \u03c0x2 \u221a \u03c0\u03b12\n( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+ y\u221a\n2 \u221a x\n) (29)\n\u2212e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+ y\u221a\n2 \u221a x\n)) \u2212 \u221a 2 ( \u03b12 \u2212 1 ) x3/2.\nThe sub-function is increasing in x, since the derivative is larger than zero: \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2x3/2 ( \u03b12 \u2212 1 ) 2 \u221a \u03c0x2 >\n(30)\n\u221a \u03c0\u03b12  (2x\u2212y)(2x+y)2\u221a \u03c0 ( 2x+y\u221a 2 \u221a x + \u221a( 2x+y\u221a 2 \u221a x )2 +2 ) \u2212 (x\u2212y)(x+y)2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) \u2212\u221a2x3/2 (\u03b12 \u2212 1)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2( \u221a 2 \u221a x)\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2(\u221a2\u221ax)\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212\u221a2x3/2 (\u03b12 \u2212 1) 2 \u221a \u03c0x2 =\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n>\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+2(2x+y)+1 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+0.878\u00b72(x+y)+0.8782 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y+1)2 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y+0.878)2 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2x\u2212y)(2x+y)2\u221a \u03c0(2(2x+y)+1) \u2212 (x\u2212y)(x+y)2\u221a \u03c0(2(x+y)+0.878) ) \u2212 x ( \u03b12 \u2212 1 ) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2(x+y)+0.878)(2x\u2212y)(2x+y)2\u221a\n\u03c0 \u2212 (x\u2212y)(x+y)(2(2x+y)+1)2\u221a \u03c0 ) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2 +\n\u221a \u03c0\u03b12 ( \u2212x ( \u03b12 \u2212 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.878) ) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2 =\n8x3 + 12x2y + 4.14569x2 + 4xy2 \u2212 6.76009xy \u2212 1.58023x+ 0.683154y2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2\n>\n8x3 \u2212 0.1 \u00b7 12x2 + 4.14569x2 + 4 \u00b7 (0.0)2x\u2212 6.76009 \u00b7 0.1x\u2212 1.58023x+ 0.683154 \u00b7 (0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0x3/2\n=\n8x2 + 2.94569x\u2212 2.25624 (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0 \u221a x =\n8(x\u2212 0.377966)(x+ 0.746178) (2(2x+ y) + 1)(2(x+ y) + 0.878) \u221a 2 \u221a \u03c0 \u221a x > 0 .\nWe explain this chain of inequalities:\n\u2022 First inequality: We applied Lemma 22 two times. \u2022 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2022 Second inequality part 1: we applied\n0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (31)\n\u2022 Second inequality part 2: we show that for a = 110 (\u221a 960+169\u03c0 \u03c0 \u2212 13 ) following holds:\n8x \u03c0 \u2212\n( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0 \u2212 2a > 0 and\n\u2202 \u2202y 8x \u03c0 \u2212\n( a2 + 2a(x+ y) ) = \u22122a < 0. Therefore the minimum is at border for minimal x\nand maximal y:\n8 \u00b7 1.2 \u03c0 \u2212  2 10 (\u221a 960 + 169\u03c0 \u03c0 \u2212 13 ) (1.2 + 0.1) + ( 1 10 (\u221a 960 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (32)\nThus\n8x\n\u03c0 > a2 + 2a(x+ y) . (33)\nfor a = 110 (\u221a 960+169\u03c0 \u03c0 \u2212 13 ) > 0.878.\n\u2022 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.878).\n\u2022 We set \u03b1 = \u03b101 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x = \u03bd\u03c4 = 1.5 \u00b7 0.8 = 1.2 and minimal y = \u00b5\u03c9 = \u22121 \u00b7 0.1 = \u22120.1. We further minimize the function\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) > \u22120.1e 0.1 2 2\u00b71.2 ( 2\u2212 erfc ( 0.1\u221a 2 \u221a 1.2 )) . (34)\nWe compute the minimum of the term in brackets of \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) in Eq. (25):\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + (35)\n\u03b1201\n( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4 >\n\u03b1201\n( \u2212 ( e ( 1.2\u22120.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2\u2212 0.1\u221a\n2 \u221a 1.2\n) \u2212 e ( 2\u00b71.2\u22120.1\u221a 2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2\u2212 0.1\u221a\n2 \u221a 1.2\n))) \u2212\n0.1e 0.12 2\u00b71.2 ( 2\u2212 erfc ( 0.1\u221a 2 \u221a 1.2 )) + \u221a 1.2 \u221a 2 \u03c0 = 0.212234 .\nTherefore the term in brackets of Eq. (25) is larger than zero. Thus, \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign\nof \u03c9. Since \u03be\u0303 is a function in \u00b5\u03c9 (these variables only appear as this product), we have for x = \u00b5\u03c9 \u2202\n\u2202\u03bd \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u00b5 = \u2202 \u2202x \u03be\u0303 \u03c9 (36)\nand \u2202\n\u2202\u03c9 \u03be\u0303 =\n\u2202 \u2202x \u03be\u0303 \u2202x \u2202\u03c9 = \u2202 \u2202x \u03be\u0303 \u00b5 . (37)\n\u2202\n\u2202\u03c9 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u00b5\n\u03c9\n\u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) . (38)\nSince \u2202\u2202\u00b5 \u03be\u0303 has the sign of \u03c9, \u2202 \u2202\u00b5 \u03be\u0303 has the sign of \u00b5. Therefore\n\u2202\n\u2202\u03c9 g(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) =\n\u2202\n\u2202\u03c9 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) (39)\nhas the sign of \u00b5.\nWe now divide the \u00b5-domain into \u22121 6 \u00b5 6 0 and 0 6 \u00b5 6 1. Analogously we divide the \u03c9-domain into \u22120.1 6 \u03c9 6 0 and 0 6 \u03c9 6 0.1. In this domains g is strictly monotonically. For all domains g is strictly monotonically decreasing in \u03bd and strictly monotonically increasing in \u03c4 . Note that we now consider the range 3 6 \u03bd 6 16. For the maximal value of g we set \u03bd = 3 (we set it to 3!) and \u03c4 = 1.25.\nWe consider now all combination of these domains:\n\u2022 \u22121 6 \u00b5 6 0 and \u22120.1 6 \u03c9 6 0: g is decreasing in \u00b5 and decreasing in \u03c9. We set \u00b5 = \u22121 and \u03c9 = \u22120.1.\ng(\u22121,\u22120.1, 3, 1.25, \u03bb01, \u03b101) = \u22120.0180173 . (40)\n\u2022 \u22121 6 \u00b5 6 0 and 0 6 \u03c9 6 0.1: g is increasing in \u00b5 and decreasing in \u03c9. We set \u00b5 = 0 and \u03c9 = 0.\ng(0, 0, 3, 1.25, \u03bb01, \u03b101) = \u22120.148532 . (41)\n\u2022 0 6 \u00b5 6 1 and \u22120.1 6 \u03c9 6 0: g is decreasing in \u00b5 and increasing in \u03c9. We set \u00b5 = 0 and \u03c9 = 0.\ng(0, 0, 3, 1.25, \u03bb01, \u03b101) = \u22120.148532 . (42)\n\u2022 0 6 \u00b5 6 1 and 0 6 \u03c9 6 0.1: g is increasing in \u00b5 and increasing in \u03c9. We set \u00b5 = 1 and \u03c9 = 0.1.\ng(1, 0.1, 3, 1.25, \u03bb01, \u03b101) = \u22120.0180173 . (43)\nTherefore the maximal value of g is \u22120.0180173.\nA3.3 Proof of Theorem 3\nFirst we recall Theorem 3:\nTheorem (Increasing \u03bd). We consider \u03bb = \u03bb01, \u03b1 = \u03b101 and the two domains \u2126\u22121 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.16, 0.8 6 \u03c4 6 1.25} and \u2126\u22122 = {(\u00b5, \u03c9, \u03bd, \u03c4) | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.9 6 \u03c4 6 1.25} . The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) increases\n\u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) > \u03bd (44)\nin both \u2126\u22121 and \u2126 \u2212 2 . All fixed points (\u00b5, \u03bd) of mapping Eq. (5) and Eq. (4) ensure for 0.8 6 \u03c4 that \u03bd\u0303 > 0.16 and for 0.9 6 \u03c4 that \u03bd\u0303 > 0.24. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance \u03bd.\nProof. The mean value theorem states that there exists a t \u2208 [0, 1] for which\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) \u2212 \u03be\u0303(\u00b5, \u03c9, \u03bdmin, \u03c4, \u03bb01, \u03b101) = (45) \u2202 \u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd + t(\u03bdmin \u2212 \u03bd), \u03c4, \u03bb01, \u03b101) (\u03bd \u2212 \u03bdmin) .\nTherefore\n\u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = \u03be\u0303(\u00b5, \u03c9, \u03bdmin, \u03c4, \u03bb01, \u03b101) + (46) \u2202 \u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd + t(\u03bdmin \u2212 \u03bd), \u03c4, \u03bb01, \u03b101) (\u03bd \u2212 \u03bdmin) .\nTherefore we are interested to bound the derivative of the \u03be-mapping Eq. (13) with respect to \u03bd:\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (47)\n1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) \u2212\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) .\nThe sub-term Eq. (308) enters the derivative Eq. (47) with a negative sign! According to Lemma 18, the minimal value of sub-term Eq. (308) is obtained by the largest largest \u03bd, by the smallest \u03c4 , and the largest y = \u00b5\u03c9 = 0.01. Also the positive term erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 is multiplied by \u03c4 , which is minimized by using the smallest \u03c4 . Therefore we can use the smallest \u03c4 in whole formula Eq. (47) to lower bound it.\nFirst we consider the domain 0.05 6 \u03bd 6 0.16 and 0.8 6 \u03c4 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.8 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( \u2212 0.01\u221a\n2 \u221a 0.05\u00b70.8\n) in order to obtain the maximal\nnegative contribution. Thus, applying Lemma 18, we obtain the lower bound on the derivative:\n1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) \u2212\n(48)\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) >\n1 2 0.8e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.8\u03bb201\n( \u03b1201 ( \u2212 ( e ( 0.16\u00b70.8+0.01\u221a 2 \u221a 0.16\u00b70.8 )2 erfc ( 0.16 \u00b7 0.8 + 0.01\u221a\n2 \u221a 0.16 \u00b7 0.8\n) \u2212\n2e\n( 2\u00b70.16\u00b70.8+0.01\u221a\n2 \u221a 0.16\u00b70.8 )2 erfc ( 2 \u00b7 0.16 \u00b7 0.8 + 0.01\u221a\n2 \u221a 0.16 \u00b7 0.8\n))) \u2212 erfc ( \u2212 0.01\u221a\n2 \u221a 0.05 \u00b7 0.8\n) + 2 ) ) > 0.969231 .\nFor applying the mean value theorem, we require the smallest \u03bd\u0303(\u03bd). We follow the proof of Lemma 8, which shows that at the minimum y = \u00b5\u03c9 must be maximal and x = \u03bd\u03c4 must be minimal. Thus, the smallest \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) is \u03be\u0303(0.01, 0.01, 0.05, 0.8, \u03bb01, \u03b101) = 0.0662727 for 0.05 6 \u03bd and 0.8 6 \u03c4 .\nTherefore the mean value theorem and the bound on (\u00b5\u0303)2 (Lemma 43) provide\n\u03bd\u0303 = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101))2 > (49) 0.0662727 + 0.969231(\u03bd \u2212 0.05)\u2212 0.005 = 0.01281115 + 0.969231\u03bd > 0.08006969 \u00b7 0.16 + 0.969231\u03bd > 1.049301\u03bd > \u03bd .\nNext we consider the domain 0.05 6 \u03bd 6 0.24 and 0.9 6 \u03c4 6 1.25. The factor consisting of the exponential in front of the brackets has its smallest value for e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.9 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc ( \u2212 0.01\u221a\n2 \u221a 0.05\u00b70.9\n) in order to obtain the maximal\nnegative contribution.\nThus, applying Lemma 18, we obtain the lower bound on the derivative: 1 2 \u03bb2\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ))) \u2212\n(50)\nerfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) >\n1 2 0.9e\u2212 0.01\u00b70.01 2\u00b70.05\u00b70.9\u03bb201\n( \u03b1201 ( \u2212 ( e ( 0.24\u00b70.9+0.01\u221a 2 \u221a 0.24\u00b70.9 )2 erfc ( 0.24 \u00b7 0.9 + 0.01\u221a\n2 \u221a 0.24 \u00b7 0.9\n) \u2212\n2e\n( 2\u00b70.24\u00b70.9+0.01\u221a\n2 \u221a 0.24\u00b70.9 )2 erfc ( 2 \u00b7 0.24 \u00b7 0.9 + 0.01\u221a\n2 \u221a 0.24 \u00b7 0.9\n))) \u2212 erfc ( \u2212 0.01\u221a\n2 \u221a 0.05 \u00b7 0.9\n) + 2 ) ) > 0.976952 .\nFor applying the mean value theorem, we require the smallest \u03bd\u0303(\u03bd). We follow the proof of Lemma 8, which shows that at the minimum y = \u00b5\u03c9 must be maximal and x = \u03bd\u03c4 must be minimal. Thus, the smallest \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) is \u03be\u0303(0.01, 0.01, 0.05, 0.9, \u03bb01, \u03b101) = 0.0738404 for 0.05 6 \u03bd and 0.9 6 \u03c4 . Therefore the mean value theorem and the bound on (\u00b5\u0303)2 (Lemma 43) gives\n\u03bd\u0303 = \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 (\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101))2 > (51) 0.0738404 + 0.976952(\u03bd \u2212 0.05)\u2212 0.005 = 0.0199928 + 0.976952\u03bd > 0.08330333 \u00b7 0.24 + 0.976952\u03bd > 1.060255\u03bd > \u03bd .\nA3.4 Lemmata and Other Tools Required for the Proofs\nA3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one\nIn this section, we show that the largest singular value of the Jacobian of the mapping g is smaller than one. Therefore, g is a contraction mapping. This is even true in a larger domain than the original \u2126. We do not need to restrict \u03c4 \u2208 [0.95, 1.1], but we can extend to \u03c4 \u2208 [0.8, 1.25]. The range of the other variables is unchanged such that we consider the following domain throughout this section: \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nJacobian of the mapping. In the following, we denote two Jacobians: (1) the Jacobian J of the mapping h : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03be\u0303), and (2) the JacobianH of the mapping g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) because the influence of \u00b5\u0303 on \u03bd\u0303 is small, and many properties of the system can already be seen on J .\nJ = ( J11 J12 J21 J22 ) = ( \u2202 \u2202\u00b5 \u00b5\u0303 \u2202 \u2202\u03bd \u00b5\u0303\n\u2202 \u2202\u00b5 \u03be\u0303 \u2202 \u2202\u03bd \u03be\u0303\n) (52)\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 \u2212 2\u00b5\u0303J11 J22 \u2212 2\u00b5\u0303J12 ) (53)\nThe definition of the entries of the Jacobian J is:\nJ11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (54)\n1 2 \u03bb\u03c9\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (55)\n1 4 \u03bb\u03c4\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 (\u03b1\u2212 1) \u221a 2\n\u03c0\u03bd\u03c4 e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4\n)\nJ21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (56)\n\u03bb2\u03c9 ( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )\nJ22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (57)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )\nProof sketch: Bounding the largest singular value of the Jacobian. If the largest singular value of the Jacobian is smaller than 1, then the spectral norm of the Jacobian is smaller than 1. Then the mapping Eq. (4) and Eq. (5) of the mean and variance to the mean and variance in the next layer is contracting.\nWe show that the largest singular value is smaller than 1 by evaluating the function S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. Toward this end we have to bound the gradient of S with respect to (\u00b5, \u03c9, \u03bd, \u03c4). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1.\nThe singular values of the 2\u00d7 2 matrix\nA = ( a11 a12 a21 a22 ) (58)\nare\ns1 = 1\n2\n(\u221a (a11 + a22)2 + (a21 \u2212 a12)2 + \u221a (a11 \u2212 a22)2 + (a12 + a21)2 ) (59)\ns2 = 1\n2\n(\u221a (a11 + a22)2 + (a21 \u2212 a12)2 \u2212 \u221a (a11 \u2212 a22)2 + (a12 + a21)2 ) . (60)\nWe used an explicit formula for the singular values [4]. We now setH11 = a11,H12 = a12,H21 = a21,H22 = a22 to obtain a formula for the largest singular value of the Jacobian depending on (\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1). The formula for the largest singular value for the Jacobian is: S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = (\u221a (H11 +H22)2 + (H21 \u2212H12)2 + \u221a (H11 \u2212H22)2 + (H12 +H21)2 ) =\n(61)\n= 1\n2 (\u221a (J11 + J22 \u2212 2\u00b5\u0303J12)2 + (J21 \u2212 2\u00b5\u0303J11 \u2212 J12)2 +\u221a\n(J11 \u2212 J22 + 2\u00b5\u0303J12)2 + (J12 + J21 \u2212 2\u00b5\u0303J11)2 ) ,\nwhere J are defined in Eq. (54) and we left out the dependencies on (\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) in order to keep the notation uncluttered, e.g. we wrote J11 instead of J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1).\nBounds on the derivatives of the Jacobian entries. In order to bound the gradient of the singular value, we have to bound the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 . The values \u03bb and \u03b1 are fixed to \u03bb01 and \u03b101. The 16 derivatives of the 4 Jacobian entries with respect to the 4 variables are:\n\u2202J11 \u2202\u00b5 = 1 2 \u03bb\u03c92e\u2212 \u00b52\u03c92 2\u03bd\u03c4 \u03b1e (\u00b5\u03c9+\u03bd\u03c4)22\u03bd\u03c4 erfc(\u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 \u221a 2 \u03c0 (\u03b1\u2212 1)\u221a \u03bd\u03c4  (62) \u2202J11 \u2202\u03c9 = 1 2 \u03bb \u2212e\u2212\u00b52\u03c922\u03bd\u03c4  \u221a 2 \u03c0 (\u03b1\u2212 1)\u00b5\u03c9\u221a \u03bd\u03c4 \u2212 \u03b1(\u00b5\u03c9 + 1)e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2\n) \u2202J11 \u2202\u03bd = 1 4 \u03bb\u03c4\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 ( (\u03b1\u2212 1)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n)) \u2202J11 \u2202\u03c4 = 1 4 \u03bb\u03bd\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 ( (\u03b1\u2212 1)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n)) \u2202J12 \u2202\u00b5 = \u2202J11 \u2202\u03bd\n\u2202J12 \u2202\u03c9 = 1 4 \u03bb\u00b5\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n( (\u03b1\u2212 1)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 )) \u2202J12 \u2202\u03bd = 1 8 \u03bbe\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b1\u03c42e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\u221a\n2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n\u03bd5/2 \u221a \u03c4\n+\n\u221a \u03c4(\u03b1+ \u03b1\u00b5\u03c9 \u2212 1)\n\u03bd3/2 \u2212 \u03b1\u03c4\n3/2\n\u221a \u03bd )) \u2202J12 \u2202\u03c4 = 1 8 \u03bbe\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( 2\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1\u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\u221a\n2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n(\u03bd\u03c4)3/2 + \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 )) \u2202J21 \u2202\u00b5 = \u03bb2\u03c92 ( \u03b12 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n2\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) \u2202J21 \u2202\u03c9 = \u03bb2 ( \u03b12(\u00b5\u03c9 + 1) ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n\u03b12(2\u00b5\u03c9 + 1)e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u00b5\u03c9 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) \u2202J21 \u2202\u03bd = 1 2 \u03bb2\u03c4\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J21 \u2202\u03c4 = 1 2 \u03bb2\u03bd\u03c9e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J22 \u2202\u00b5 = \u2202J21 \u2202\u03bd\n\u2202J22 \u2202\u03c9 = 1 2 \u03bb2\u00b5\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2 \u03c0 (\u22121) ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4  \u2202J22 \u2202\u03bd = 1 4 \u03bb2\u03c42e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ( \u03b12 ( \u2212e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) +\n8\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1\n2 \u221a \u03bd\u03c4 )) \u2202J22 \u2202\u03c4 = 1 4 \u03bb2 ( \u22122\u03b12e\u2212 \u00b52\u03c92 2\u03bd\u03c4 e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212\n\u03b12\u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 4\u03b12e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n8\u03b12\u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) +\u221a\n2 \u03c0 e\u2212 \u00b52\u03c92 2\u03bd\u03c4\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n\u221a \u03bd\u03c4\n\u2212 3\u03b12 \u221a \u03bd\u03c4 )) Lemma 5 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 hold:\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5\n\u2223\u2223\u2223\u2223 < 0.0031049101995398316 (63)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.055872374194189\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.03749149348255419\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.031242911235461816\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.21232788238624354\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.2124377655377270\n\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.02220441024325437\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.146955401845684\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.17980135762932363\n\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 1.805740052651535\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.396685907216327\nProof. See proof 39.\nBounds on the entries of the Jacobian. Lemma 6 (Bound on J11). The absolute value of the function J11 = 12\u03bb\u03c9 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )\nis bounded by |J11| 6 0.104497 in the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.8 6 \u03bd 6 1.5, and 0.8 6 \u03c4 6 1.25 for \u03b1 = \u03b101 and \u03bb = \u03bb01.\nProof.\n|J11| = \u2223\u2223\u2223\u222312\u03bb\u03c9 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ))\u2223\u2223\u2223\u2223 6 |1\n2 ||\u03bb||\u03c9| (|\u03b1|0.587622 + 1.00584) 6 0.104497,\n(64) where we used that (a) J11 is strictly monotonically increasing in \u00b5\u03c9 and |2 \u2212 erfc (\n0.01\u221a 2 \u221a \u03bd\u03c4\n) | 6\n1.00584 and (b) Lemma 47 that |e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) | 6 e0.01+ 0.642 erfc ( 0.01+0.64\u221a\n2 \u221a 0.64\n) = 0.587622\nLemma 7 (Bound on J12). The absolute value of the function J12 = 14\u03bb\u03c4 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 (\u03b1\u2212 1) \u221a 2 \u03c0\u03bd\u03c4 e \u2212\u00b5 2\u03c92 2\u03bd\u03c4 ) is bounded by |J12| 6 0.194145 in the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.8 6 \u03bd 6 1.5, and 0.8 6 \u03c4 6 1.25 for \u03b1 = \u03b101 and \u03bb = \u03bb01.\nProof.\n|J12| 6 1\n4 |\u03bb||\u03c4 | \u2223\u2223\u2223\u2223\u2223 ( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) \u2212 (\u03b1\u2212 1) \u221a 2 \u03c0\u03bd\u03c4 e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )\u2223\u2223\u2223\u2223\u2223 6 1 4 |\u03bb||\u03c4 | |0.983247\u2212 0.392294| 6\n0.194035 (65)\nFor the first term we have 0.434947 6 e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 0.587622 after Lemma 47 and for\nthe second term 0.582677 6 \u221a\n2 \u03c0\u03bd\u03c4 e\n\u2212\u00b5 2\u03c92\n2\u03bd\u03c4 6 0.997356, which can easily be seen by maximizing or minimizing the arguments of the exponential or the square root function. The first term scaled by \u03b1 is 0.727780 6 \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 0.983247 and the second term scaled by \u03b1 \u2212 1 is\n0.392294 6 (\u03b1 \u2212 1) \u221a\n2 \u03c0\u03bd\u03c4 e\n\u2212\u00b5 2\u03c92\n2\u03bd\u03c4 6 0.671484. Therefore, the absolute difference between these terms is at most 0.983247\u2212 0.392294 leading to the derived bound.\nBounds on mean, variance and second moment. For deriving bounds on \u00b5\u0303, \u03be\u0303, and \u03bd\u0303, we need the following lemma. Lemma 8 (Derivatives of the Mapping). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nThe derivative \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nThe derivative \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nProof. See 40.\nLemma 9 (Bounds on mean, variance and second moment). The expressions \u00b5\u0303, \u03be\u0303, and \u03bd\u0303 for \u03b1 = \u03b101 and \u03bb = \u03bb01 are bounded by \u22120.041160 < \u00b5\u0303 < 0.087653, 0.703257 < \u03be\u0303 < 1.643705 and 0.695574 < \u03bd\u0303 < 1.636023 in the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 15], \u03c9 \u2208 [\u22120.1, 0.1], \u03c4 \u2208 [0.8, 1.25].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to \u03bd and \u00b5 are either positive or have the sign of \u03c9. Therefore with given sign of \u03c9 the mappings are strict monotonic and the their maxima and minima are found at the borders. The minimum of \u00b5\u0303 is obtained at \u00b5\u03c9 = \u22120.01 and its maximum at \u00b5\u03c9 = 0.01 and \u03c3 and \u03c4 at minimal or maximal values, respectively. It follows that \u22120.041160 < \u00b5\u0303(\u22120.1, 0.1, 0.8, 0.8, \u03bb01, \u03b101) 6\u00b5\u0303 6 \u00b5\u0303(0.1, 0.1, 1.5, 1.25, \u03bb01, \u03b101) < 0.087653.\n(66)\nSimilarly, the maximum and minimum of \u03be\u0303 is obtained at the values mentioned above:\n0.703257 < \u03be\u0303(\u22120.1, 0.1, 0.8, 0.8, \u03bb01, \u03b101) 6\u03be\u0303 6 \u03be\u0303(0.1, 0.1, 1.5, 1.25, \u03bb01, \u03b101) < 1.643705. (67)\nHence we obtain the following bounds on \u03bd\u0303:\n0.703257\u2212 \u00b5\u03032 < \u03be\u0303 \u2212 \u00b5\u03032 < 1.643705\u2212 \u00b5\u03032 (68) 0.703257\u2212 0.007683 < \u03bd\u0303 < 1.643705\u2212 0.007682\n0.695574 < \u03bd\u0303 < 1.636023.\nUpper Bounds on the Largest Singular Value of the Jacobian. Lemma 10 (Upper Bounds on Absolute Derivatives of Largest Singular Value). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. The absolute values of derivatives of the largest singular value S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (61) with respect to (\u00b5, \u03c9, \u03bd, \u03c4) are bounded as follows:\n\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.32112 , (69)\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 \u2223\u2223\u2223\u2223 < 2.63690 , (70)\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd \u2223\u2223\u2223\u2223 < 2.28242 , (71)\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.98610 . (72)\nProof. The Jacobian of our mapping Eq. (4) and Eq. (5) is defined as\nH = ( H11 H12 H21 H22 ) = ( J11 J12 J21 \u2212 2\u00b5\u0303J11 J22 \u2212 2\u00b5\u0303J12 ) (73)\nand has the largest singular value\nS(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2\n(\u221a (H11 \u2212H22)2 + (H12 +H21)2 + \u221a (H11 +H22)2 + (H12 \u2212H21)2 ) ,\n(74)\naccording to the formula of Blinn [4].\nWe obtain\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H11 \u2212H22\u221a (H11 \u2212H22)2 + (H12 +H21)2 + H11 +H22\u221a (H11 +H22)2 + (H21 \u2212H12)2 )\u2223\u2223\u2223\u2223\u2223 < (75)\n1\n2 \u2223\u2223\u2223\u2223\u2223\u2223 1\u221a (H12+H21)2 (H11\u2212H22)2 + 1 \u2223\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223\u2223 1\u221a (H21\u2212H12)2 (H11+H22)2 + 1 \u2223\u2223\u2223\u2223\u2223\u2223  < 1 + 1 2 = 1\nand analogously\u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H12 +H21\u221a (H11 \u2212H22)2 + (H12 +H21)2 \u2212 H21 \u2212H12\u221a (H11 +H22)2 + (H21 \u2212H12)2 )\u2223\u2223\u2223\u2223\u2223 < 1 (76)\nand\u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H21 \u2212H12\u221a (H11 +H22)2 + (H21 \u2212H12)2 + H12 +H21\u221a (H11 \u2212H22)2 + (H12 +H21)2 )\u2223\u2223\u2223\u2223\u2223 < 1 (77)\nand\u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u222312 ( H11 +H22\u221a (H11 +H22)2 + (H21 \u2212H12)2 \u2212 H11 \u2212H22\u221a (H11 \u2212H22)2 + (H12 +H21)2 )\u2223\u2223\u2223\u2223\u2223 < 1 . (78)\nWe have \u2202S\n\u2202\u00b5 =\n\u2202S \u2202H11 \u2202H11 \u2202\u00b5 + \u2202S \u2202H12 \u2202H12 \u2202\u00b5 + \u2202S \u2202H21 \u2202H21 \u2202\u00b5 + \u2202S \u2202H22 \u2202H22 \u2202\u00b5\n(79)\n\u2202S \u2202\u03c9 = \u2202S \u2202H11 \u2202H11 \u2202\u03c9 + \u2202S \u2202H12 \u2202H12 \u2202\u03c9 + \u2202S \u2202H21 \u2202H21 \u2202\u03c9 + \u2202S \u2202H22 \u2202H22 \u2202\u03c9\n(80)\n\u2202S \u2202\u03bd = \u2202S \u2202H11 \u2202H11 \u2202\u03bd + \u2202S \u2202H12 \u2202H12 \u2202\u03bd + \u2202S \u2202H21 \u2202H21 \u2202\u03bd + \u2202S \u2202H22 \u2202H22 \u2202\u03bd\n(81)\n\u2202S \u2202\u03c4 = \u2202S \u2202H11 \u2202H11 \u2202\u03c4 + \u2202S \u2202H12 \u2202H12 \u2202\u03c4 + \u2202S \u2202H21 \u2202H21 \u2202\u03c4 + \u2202S \u2202H22 \u2202H22 \u2202\u03c4\n(82)\n(83) from which follows using the bounds from Lemma 5: Derivative of the singular value w.r.t. \u00b5:\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 \u2223\u2223\u2223\u2223 6 (84)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u00b5\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11|2 + 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5\n\u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| |J11| 6 0.0031049101995398316 + 0.031242911235461816 + 0.02220441024325437 + 0.14983446469110305+\n2 \u00b7 0.104497 \u00b7 0.087653 + 2 \u00b7 0.1044972+ 2 \u00b7 0.194035 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035 < 0.32112,\nwhere we used the results from the lemmata 5, 6, 7, and 9. Derivative of the singular value w.r.t. \u03c9:\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 \u2223\u2223\u2223\u2223 6 (85)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03c9\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c9\n\u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c9\n\u2223\u2223\u2223\u2223 6 (86) 2.38392 + 2 \u00b7 1.055872374194189 \u00b7 0.087653 + 2 \u00b7 0.1044972 + 2 \u00b7 0.031242911235461816 \u00b7 0.087653 + 2 \u00b7 0.194035 \u00b7 0.104497 < 2.63690 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that \u00b5\u0303 is symmetric for \u00b5, \u03c9. Derivative of the singular value w.r.t. \u03bd:\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd \u2223\u2223\u2223\u2223 6 (87)\n\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03bd \u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03bd\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03bd\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| |J12|+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd\n\u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12|2 6 2.19916 + 2 \u00b7 0.031242911235461816 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035+ 2 \u00b7 0.21232788238624354 \u00b7 0.087653 + 2 \u00b7 0.1940352 < 2.28242 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9.\nDerivative of the singular value w.r.t. \u03c4 :\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 \u2223\u2223\u2223\u2223 6 (88)\u2223\u2223\u2223\u2223 \u2202S\u2202H11 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H12 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H21 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202S\u2202H22 \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202H11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202H22\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21 \u2212 2\u00b5\u0303J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22 \u2212 2\u00b5\u0303J12\u2202\u03c4\n\u2223\u2223\u2223\u2223 6\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J11| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c4\n\u2223\u2223\u2223\u2223+ 2 \u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 |\u00b5\u0303|+ 2 |J12| \u2223\u2223\u2223\u2223\u2202\u00b5\u0303\u2202\u03c4\n\u2223\u2223\u2223\u2223 6 (89) 2.82643 + 2 \u00b7 0.03749149348255419 \u00b7 0.087653 + 2 \u00b7 0.104497 \u00b7 0.194035+ 2 \u00b7 0.2124377655377270 \u00b7 0.087653 + 2 \u00b7 0.1940352 < 2.98610 ,\nwhere we used the results from the lemmata 5, 6, 7, and 9 and that \u00b5\u0303 is symmetric for \u03bd, \u03c4 .\nLemma 11 (Mean Value Theorem Bound on Deviation from Largest Singular Value). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. The distance of the singular value at S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) and that at S(\u00b5 + \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) is bounded as follows:\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < (90) 0.32112 |\u2206\u00b5|+ 2.63690 |\u2206\u03c9|+ 2.28242 |\u2206\u03bd|+ 2.98610 |\u2206\u03c4 | .\nProof. The mean value theorem states that a t \u2208 [0, 1] exists for which S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (91) \u2202S\n\u2202\u00b5 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u00b5 +\n\u2202S \u2202\u03c9 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03c9 +\n\u2202S \u2202\u03bd (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03bd +\n\u2202S \u2202\u03c4 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2206\u03c4\nfrom which immediately follows that\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| 6 (92)\u2223\u2223\u2223\u2223\u2202S\u2202\u00b5 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u00b5| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03c9 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03c9| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03bd (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03bd| +\u2223\u2223\u2223\u2223\u2202S\u2202\u03c4 (\u00b5+ t\u2206\u00b5, \u03c9 + t\u2206\u03c9, \u03bd + t\u2206\u03bd, \u03c4 + t\u2206\u03c4, \u03bb01, \u03b101) \u2223\u2223\u2223\u2223 |\u2206\u03c4 | .\nWe now apply Lemma 10 which gives bounds on the derivatives, which immediately gives the statement of the lemma.\nLemma 12 (Largest Singular Value Smaller Than One). We set \u03b1 = \u03b101 and \u03bb = \u03bb01 and restrict the range of the variables to \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25]. The the largest singular value of the Jacobian is smaller than 1:\nS(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) < 1 . (93)\nTherefore the mapping Eq. (4) and Eq. (5) is a contraction mapping.\nProof. We set \u2206\u00b5 = 0.0068097371, \u2206\u03c9 = 0.0008292885, \u2206\u03bd = 0.0009580840, and \u2206\u03c4 = 0.0007323095.\nAccording to Lemma 11 we have\n|S(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) \u2212 S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < (94) 0.32112 \u00b7 0.0068097371 + 2.63690 \u00b7 0.0008292885+ 2.28242 \u00b7 0.0009580840 + 2.98610 \u00b7 0.0007323095 < 0.008747 .\nFor a grid with grid length \u2206\u00b5 = 0.0068097371, \u2206\u03c9 = 0.0008292885, \u2206\u03bd = 0.0009580840, and \u2206\u03c4 = 0.0007323095, we evaluated the function Eq. (61) for the largest singular value in the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25]. We did this using a computer. According to Subsection A3.4.5 the precision if regarding error propagation and precision of the implemented functions is larger than 10\u221213. We performed the evaluation on different operating systems and different hardware architectures including CPUs and GPUs. In all cases the function Eq. (61) for the largest singular value of the Jacobian is bounded by 0.9912524171058772.\nWe obtain from Eq. (94):\nS(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101) 6 0.9912524171058772 + 0.008747 < 1 . (95)\nA3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain\nWe further have to investigate whether the the mapping Eq. (4) and Eq. (5) maps into a predefined domains. Lemma 13 (Mapping into the domain). The mapping Eq. (4) and Eq. (5) map for \u03b1 = \u03b101 and \u03bb = \u03bb01 into the domain \u00b5 \u2208 [\u22120.03106, 0.06773] and \u03bd \u2208 [0.80009, 1.48617] with \u03c9 \u2208 [\u22120.1, 0.1] and \u03c4 \u2208 [0.95, 1.1].\nProof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to \u03b1 = \u03b101 and \u03bb = \u03bb01 are either positive or have the sign of \u03c9. Therefore with given sign of \u03c9 the mappings are strict monotonic and the their maxima and minima are found at the\nborders. The minimum of \u00b5\u0303 is obtained at \u00b5\u03c9 = \u22120.01 and its maximum at \u00b5\u03c9 = 0.01 and \u03c3 and \u03c4 at their minimal and maximal values, respectively. It follows that:\n\u22120.03106 < \u00b5\u0303(\u22120.1, 0.1, 0.8, 0.95, \u03bb01, \u03b101) 6\u00b5\u0303 6 \u00b5\u0303(0.1, 0.1, 1.5, 1.1, \u03bb01, \u03b101) < 0.06773, (96)\nand that \u00b5\u0303 \u2208 [\u22120.1, 0.1].\nSimilarly, the maximum and minimum of \u03be\u0303( is obtained at the values mentioned above:\n0.80467 < \u03be\u0303(\u22120.1, 0.1, 0.8, 0.95, \u03bb01, \u03b101) 6\u03be\u0303 6 \u03be\u0303(0.1, 0.1, 1.5, 1.1, \u03bb01, \u03b101) < 1.48617. (97)\nSince |\u03be\u0303 \u2212 \u03bd\u0303| = |\u00b5\u03032| < 0.004597, we can conclude that 0.80009 < \u03bd\u0303 < 1.48617 and the variance remains in [0.8, 1.5].\nCorollary 14. The image g(\u2126\u2032) of the mapping g : (\u00b5, \u03bd) 7\u2192 (\u00b5\u0303, \u03bd\u0303) (Eq. (8)) and the domain \u2126\u2032 = {(\u00b5, \u03bd)| \u2212 0.1 6 \u00b5 6 0.1, 0.8 6 \u00b5 6 1.5} is a subset of \u2126\u2032:\ng(\u2126\u2032) \u2286 \u2126\u2032, (98)\nfor all \u03c9 \u2208 [\u22120.1, 0.1] and \u03c4 \u2208 [0.95, 1.1].\nProof. Directly follows from Lemma 13.\nA3.4.3 Lemmata for proofing Theorem 2: The variance is contracting\nMain Sub-Function. We consider the main sub-function of the derivate of second moment, J22 (Eq. (54)):\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u2212\u03b12e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) (99)\nthat depends on \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u03bd\u03c4 and y = \u00b5\u03c9. Algebraic reformulations provide the formula in the following form:\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u2212 y2 2x )( e (x+y)2 2x erfc ( y + x\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( y + 2x\u221a\n2 \u221a x\n)) \u2212 erfc ( y\u221a 2 \u221a x ) + 2 ) (100)\nFor \u03bb = \u03bb01 and \u03b1 = \u03b101, we consider the domain \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and, 0.8 6 \u03c4 6 1.25.\nFor x and y we obtain: 0.8 \u00b7 1.5 = 1.2 6 x 6 20 = 1.25 \u00b7 16 and 0.1 \u00b7 (\u22121) = \u22120.1 6 y 6 0.1 = 0.1 \u00b7 1. In the following we assume to remain within this domain. Lemma 15 (Main subfunction). For 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (101)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. See proof 44.\nThe graph of the subfunction in the specified domain is displayed in Figure A3.\nX y\nf(x,y)\n\u22120.132\n\u22120.130\n\u22120.128\n\u22120.126\n\u22120.10 \u22120.05 0.00 0.05 0.10\nf( 1\n.2 ,y\n)\ny\nFigure A3: Left panel: Graphs of the main subfunction f(x, y) = e (x+y)2 2x erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x erfc (\n2x+y\u221a 2 \u221a x\n) treated in Lemma 15. The function is negative and monotonically increasing\nwith x independent of y. Right panel: Graphs of the main subfunction at minimal x = 1.2. The graph shows that the function f(1.2, y) is strictly monotonically decreasing in y.\nTheorem 16 (Contraction \u03bd-mapping). The mapping of the variance \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) given in Eq. (5) is contracting for \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u2126+: \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, that is,\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03bd\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\n\u2223\u2223\u2223\u2223 < 1 . (102) Proof. In this domain \u2126+ we have the following three properties (see further below): \u2202\u2202\u03bd \u03be\u0303 < 1, \u00b5\u0303 > 0, and \u2202\u2202\u03bd \u00b5\u0303 > 0. Therefore, we have\n\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03bd\u0303 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03be\u0303 \u2212 2\u00b5\u0303 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u03be\u0303 \u2223\u2223\u2223\u2223 < 1 (103)\n\u2022 We first proof that \u2202\u2202\u03bd \u03be\u0303 < 1 in an even larger domain that fully contains \u2126 +. According to\nEq. (54), the derivative of the mapping Eq. (5) with respect to the variance \u03bd is\n\u2202\n\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) = (104)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) +\n2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) .\nFor \u03bb = \u03bb01, \u03b1 = \u03b101, \u22121 6 \u00b5 6 1, \u22120.1 6 \u03c9 6 0.1 1.5 6 \u03bd 6 16, and 0.8 6 \u03c4 6 1.25, we first show that the derivative is positive and then upper bound it.\nAccording to Lemma 15, the expression\ne (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) (105)\nis negative. This expression multiplied by positive factors is subtracted in the derivative Eq. (104), therefore, the whole term is positive. The remaining term\n2\u2212 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) (106)\nof the derivative Eq. (104) is also positive according to Lemma 21. All factors outside the brackets in Eq. (104) are positive. Hence, the derivative Eq. (104) is positive.\nThe upper bound of the derivative is:\n1 2 \u03bb201\u03c4\n( \u03b1201 ( \u2212e\u00b5\u03c9+ \u03bd\u03c42 ) erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + (107)\n2\u03b1201e 2\u00b5\u03c9+2\u03bd\u03c4 erfc\n( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) =\n1 2 \u03bb201\u03c4\n( \u03b1201 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )( e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212\n2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u03b1201 ( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 )( e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212\n2e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n))( \u2212e\u2212 \u00b52\u03c92 2\u03bd\u03c4 ) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u2212e0.0\u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n)) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) 6\n1 2 1.25\u03bb201\n( \u2212e0.0\u03b1201 ( e ( 1.2+0.1\u221a 2 \u221a 1.2 )2 erfc ( 1.2 + 0.1\u221a\n2 \u221a 1.2\n) \u2212\n2e\n( 2\u00b71.2+0.1\u221a\n2 \u221a 1.2 )2 erfc ( 2 \u00b7 1.2 + 0.1\u221a\n2 \u221a 1.2\n)) \u2212 erfc ( 0.1\u221a 2 \u221a 1.2 ) + 2 ) 6\n0.995063 < 1 .\nWe explain the chain of inequalities:\n\u2013 First equality brings the expression into a shape where we can apply Lemma 15 for the the function Eq. (101).\n\u2013 First inequality: The overall factor \u03c4 is bounded by 1.25. \u2013 Second inequality: We apply Lemma 15. According to Lemma 15 the function\nEq. (101) is negative. The largest contribution is to subtract the most negative value of the function Eq. (101), that is, the minimum of function Eq. (101). According to Lemma 15 the function Eq. (101) is strictly monotonically increasing in x and strictly monotonically decreasing in y for x = 1.2. Therefore the function Eq. (101) has its minimum at minimal x = \u03bd\u03c4 = 1.5 \u00b70.8 = 1.2 and maximal y = \u00b5\u03c9 = 1.0 \u00b70.1 = 0.1. We insert these values into the expression.\n\u2013 Third inequality: We use for the whole expression the maximal factor e\u2212 \u00b52\u03c92\n2\u03bd\u03c4 < 1 by setting this factor to 1.\n\u2013 Fourth inequality: erfc is strictly monotonically decreasing. Therefore we maximize its argument to obtain the least value which is subtracted. We use the minimal x = \u03bd\u03c4 = 1.5 \u00b7 0.8 = 1.2 and the maximal y = \u00b5\u03c9 = 1.0 \u00b7 0.1 = 0.1.\n\u2013 Sixth inequality: evaluation of the terms.\n\u2022 We now show that \u00b5\u0303 > 0. The expression \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) (Eq. (4)) is strictly monotonically increasing im \u00b5\u03c9 and \u03bd\u03c4 . Therefore, the minimal value in \u2126+ is obtained at \u00b5\u0303(0.01, 0.01, 1.5, 0.8) = 0.008293 > 0.\n\u2022 Last we show that \u2202\u2202\u03bd \u00b5\u0303 > 0. The expression \u2202 \u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4) = J12(\u00b5, \u03c9, \u03bd, \u03c4) (Eq. (54))\ncan we reformulated as follows:\nJ12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = \u03bb\u03c4e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4 (\u221a \u03c0\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2(\u03b1\u22121)\u221a \u03bd\u03c4 ) 4 \u221a \u03c0 (108)\nis larger than zero when the term \u221a \u03c0\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2(\u03b1\u22121)\u221a \u03bd\u03c4\nis larger than zero. This term obtains its minimal value at \u00b5\u03c9 = 0.01 and \u03bd\u03c4 = 16 \u00b7 1.25, which can easily be shown using the Abramowitz bounds (Lemma 22) and evaluates to 0.16, therefore J12 > 0 in \u2126+.\nA3.4.4 Lemmata for proofing Theorem 3: The variance is expanding\nMain Sub-Function From Below. We consider functions in \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u00b5\u03c9 and y = \u03bd\u03c4 .\nFor \u03bb = \u03bb01 and \u03b1 = \u03b101, we consider the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25.\nFor x and y we obtain: 0.8 \u00b70.00875 = 0.007 6 x 6 0.875 = 1.25 \u00b70.7 and 0.1 \u00b7 (\u22120.1) = \u22120.01 6 y 6 0.01 = 0.1 \u00b7 0.1. In the following we assume to be within this domain. In this domain, we consider the main sub-function of the derivate of second moment in the next layer, J22 (Eq. (54)):\n\u2202\n\u2202\u03bd \u03be\u0303 =\n1 2 \u03bb2\u03c4\n( \u2212\u03b12e\u00b5\u03c9+ \u03bd\u03c42 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + 2\u03b12e2\u00b5\u03c9+2\u03bd\u03c4 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 ) (109)\nthat depends on \u00b5\u03c9 and \u03bd\u03c4 , therefore we set x = \u03bd\u03c4 and y = \u00b5\u03c9. Algebraic reformulations provide the formula in the following form:\n\u2202\n\u2202\u03bd \u03be\u0303 = (110)\n1 2 \u03bb2\u03c4\n( \u03b12 ( \u2212e\u2212 y2 2x )( e (x+y)2 2x erfc ( y + x\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( y + 2x\u221a\n2 \u221a x\n)) \u2212 erfc ( y\u221a 2 \u221a x ) + 2 ) Lemma 17 (Main subfunction Below). For 0.007 6 x 6 0.875 and\u22120.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (111)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 \u00b7 0.8, x = 0.56 = 0.7 \u00b7 0.8, x = 0.128 = 0.16 \u00b7 0.8, and x = 0.216 = 0.24 \u00b7 0.9 (lower bound of 0.9 on \u03c4 ).\nProof. See proof 45.\nLemma 18 (Monotone Derivative). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25. We are interested of the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u00b7\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (112)\nThe derivative of the equation above with respect to\n\u2022 \u03bd is larger than zero;\n\u2022 \u03c4 is smaller than zero for maximal \u03bd = 0.7, \u03bd = 0.16, and \u03bd = 0.24 (with 0.9 6 \u03c4 );\n\u2022 y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.008750.8 = 0.007, \u03bd\u03c4 = 0.70.8 = 0.56, \u03bd\u03c4 = 0.160.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216.\nProof. See proof 46.\nA3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.\nError Analysis. We investigate the error propagation for the singular value (Eq. (61)) if the function arguments \u00b5, \u03c9, \u03bd, \u03c4 suffer from numerical imprecisions up to . To this end, we first derive error propagation rules based on the mean value theorem and then we apply these rules to the formula for the singular value. Lemma 19 (Mean value theorem). For a real-valued function f which is differentiable in the closed interval [a, b], there exists t \u2208 [0, 1] with\nf(a) \u2212 f(b) = \u2207f(a + t(b\u2212 a)) \u00b7 (a \u2212 b) . (113)\nIt follows that for computation with error \u2206x, there exists a t \u2208 [0, 1] with\n|f(x + \u2206x) \u2212 f(x)| 6 \u2016\u2207f(x + t\u2206x)\u2016 \u2016\u2206x\u2016 . (114)\nTherefore the increase of the norm of the error after applying function f is bounded by the norm of the gradient \u2016\u2207f(x + t\u2206x)\u2016. We now compute for the functions, that we consider their gradient and its 2-norm:\n\u2022 addition: f(x) = x1 + x2 and \u2207f(x) = (1, 1), which gives \u2016\u2207f(x)\u2016 = \u221a 2.\nWe further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 + x2 + \u2206x1 + \u2206x2 \u2212 x1 \u2212 x2| 6 |\u2206x1|+ |\u2206x2| . (115)\nAdding n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 xi + \u2206xi \u2212 n\u2211 i=1 xi \u2223\u2223\u2223\u2223\u2223 6 n\u2211 i=1 |\u2206xi| 6 n |\u2206xi|max . (116)\n\u2022 subtraction: f(x) = x1 \u2212 x2 and \u2207f(x) = (1,\u22121), which gives \u2016\u2207f(x)\u2016 = \u221a 2.\nWe further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 \u2212 x2 + \u2206x1 \u2212\u2206x2 \u2212 x1 + x2| 6 |\u2206x1|+ |\u2206x2| . (117)\nSubtracting n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2212(xi + \u2206xi) + n\u2211 i=1 xi \u2223\u2223\u2223\u2223\u2223 6 n\u2211 i=1 |\u2206xi| 6 n |\u2206xi|max . (118)\n\u2022 multiplication: f(x) = x1x2 and\u2207f(x) = (x2, x1), which gives \u2016\u2207f(x)\u2016 = \u2016x\u2016. We further know that\n|f(x + \u2206x)\u2212 f(x)| = |x1 \u00b7 x2 + \u2206x1 \u00b7 x2 + \u2206x2 \u00b7 x1 + \u2206x1 \u00b7\u2206xs \u2212 x1 \u00b7 x2| 6 (119)\n|\u2206x1| |x2|+ |\u2206x2| |x1|+O(\u22062) .\nMultiplying n terms gives:\u2223\u2223\u2223\u2223\u2223 n\u220f i=1 (xi + \u2206xi) \u2212 n\u220f i=1 xi \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 n\u220f i=1 xi n\u2211 i=1 \u2206xi xi + O(\u22062) \u2223\u2223\u2223\u2223\u2223 6 (120) n\u220f i=1 |xi| n\u2211 i=1 \u2223\u2223\u2223\u2223\u2206xixi \u2223\u2223\u2223\u2223 + O(\u22062) 6 n n\u220f i=1 |xi| \u2223\u2223\u2223\u2223\u2206xixi \u2223\u2223\u2223\u2223 max + O(\u22062) .\n\u2022 division: f(x) = x1x2 and \u2207f(x) = ( 1 x2 ,\u2212x1\nx22\n) , which gives \u2016\u2207f(x)\u2016 = \u2016x\u2016\nx22 .\nWe further know that |f(x + \u2206x)\u2212 f(x)| = \u2223\u2223\u2223\u2223x1 + \u2206x1x2 + \u2206x2 \u2212 x1x2 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 (x1 + \u2206x1)x2 \u2212 x1(x2 + \u2206x2)(x2 + \u2206x2)x2 \u2223\u2223\u2223\u2223 = (121)\u2223\u2223\u2223\u2223\u2206x1 \u00b7 x2 \u2212\u2206x2 \u00b7 x1x22 + \u2206x2 \u00b7 x2 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2206x1x2 \u2212 \u2206x2 \u00b7 x1x22 \u2223\u2223\u2223\u2223+O(\u22062) .\n\u2022 square root: f(x) = \u221a x and f \u2032(x) = 1\n2 \u221a x , which gives |f \u2032(x)| = 1 2 \u221a x .\n\u2022 exponential function: f(x) = exp(x) and f \u2032(x) = exp(x), which gives |f \u2032(x)| = exp(x).\n\u2022 error function: f(x) = erf(x) and f \u2032(x) = 2\u221a\n\u03c0 exp(\u2212x2), which gives |f \u2032(x)| = 2\u221a \u03c0 exp(\u2212x2).\n\u2022 complementary error function: f(x) = erfc(x) and f \u2032(x) = \u2212 2\u221a\n\u03c0 exp(\u2212x2), which gives |f \u2032(x)| = 2\u221a \u03c0 exp(\u2212x2).\nLemma 20. If the values \u00b5, \u03c9, \u03bd, \u03c4 have a precision of , the singular value (Eq. (61)) evaluated with the formulas given in Eq. (54) and Eq. (61) has a precision better than 292 .\nThis means for a machine with a typical precision of 2\u221252 = 2.220446 \u00b7 10\u221216, we have the rounding error \u2248 10\u221216, the evaluation of the singular value (Eq. (61)) with the formulas given in Eq. (54) and Eq. (61) has a precision better than 10\u221213 > 292 .\nProof. We have the numerical precision of the parameters \u00b5, \u03c9, \u03bd, \u03c4 , that we denote by \u2206\u00b5,\u2206\u03c9,\u2206\u03bd,\u2206\u03c4 together with our domain \u2126.\nWith the error propagation rules that we derived in Subsection A3.4.5, we can obtain bounds for the numerical errors on the following simple expressions:\n\u2206 (\u00b5\u03c9) 6 \u2206\u00b5 |\u03c9|+ \u2206\u03c9 |\u00b5| 6 0.2 (122) \u2206 (\u03bd\u03c4) 6 \u2206\u03bd |\u03c4 |+ \u2206\u03c4 |\u03bd| 6 1.5 + 1.5 = 3\n\u2206 (\u03bd\u03c4\n2\n) 6 (\u2206(\u03bd\u03c4)2 + \u22062 |\u03bd\u03c4 |) 1\n22 6 (6 + 1.25 \u00b7 1.5 )/4 < 2\n\u2206 (\u00b5\u03c9 + \u03bd\u03c4) 6 \u2206 (\u00b5\u03c9) + \u2206 (\u03bd\u03c4) = 3.2 \u2206 ( \u00b5\u03c9 + \u03bd\u03c4\n2\n) 6 \u2206 (\u00b5\u03c9) + \u2206 (\u03bd\u03c4 2 ) < 2.2\n\u2206 (\u221a \u03bd\u03c4 ) 6 \u2206 (\u03bd\u03c4)\n2 \u221a \u03bd\u03c4\n6 3\n2 \u221a 0.64 = 1.875\n\u2206 (\u221a 2 ) 6 \u22062\n2 \u221a 2 6\n1 2 \u221a 2\n\u2206 (\u221a 2 \u221a \u03bd\u03c4 ) 6 \u221a 2\u2206 (\u221a \u03bd\u03c4 ) + \u03bd\u03c4\u2206 (\u221a 2 ) 6 \u221a\n2 \u00b7 1.875 + 1.5 \u00b7 1.25 \u00b7 1 2 \u221a 2 < 3.5\n\u2206 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) 6 ( \u2206 (\u00b5\u03c9) \u221a 2 \u221a \u03bd\u03c4 + |\u00b5\u03c9|\u2206 (\u221a 2 \u221a \u03bd\u03c4 )) 1(\u221a 2 \u221a \u03bd\u03c4 )2 6\n( 0.2 \u221a 2 \u221a 0.64 + 0.01 \u00b7 3.5 ) 1\n2 \u00b7 0.64 < 0.25\n\u2206 ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 ( \u2206 (\u00b5\u03c9 + \u03bd\u03c4) \u221a 2 \u221a \u03bd\u03c4 + |\u00b5\u03c9 + \u03bd\u03c4 |\u2206 (\u221a 2 \u221a \u03bd\u03c4 )) 1(\u221a\n2 \u221a \u03bd\u03c4 )2 6(\n3.2 \u221a 2 \u221a 0.64 + 1.885 \u00b7 3.5 ) 1\n2 \u00b7 0.64 < 8 .\nUsing these bounds on the simple expressions, we can now calculate bounds on the numerical errors of compound expressions:\n\u2206 ( erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) 6 2\u221a \u03c0 e \u2212 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )2 \u2206 ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) < (123)\n2\u221a \u03c0 0.25 < 0.3\n\u2206 ( erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) 6 2\u221a \u03c0 e \u2212 ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 \u2206 ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) < (124)\n2\u221a \u03c0 8 < 10\n\u2206 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) 6 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) \u2206 ( e\u00b5\u03c9+ \u03bd\u03c4 2 ) < (125)\ne0.94752.2 < 5.7 (126)\nSubsequently, we can use the above results to get bounds for the numerical errors on the Jacobian entries (Eq. (54)), applying the rules from Subsection A3.4.5 again:\n\u2206 (J11) = \u2206 ( 1\n2 \u03bb\u03c9\n( \u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + 2 )) < 6 , (127)\nand we obtain \u2206 (J12) < 78 , \u2206 (J21) < 189 , \u2206 (J22) < 405 and \u2206 (\u00b5\u0303) < 52 . We also have bounds on the absolute values on Jij and \u00b5\u0303 (see Lemma 6, Lemma 7, and Lemma 9), therefore we can propagate the error also through the function that calculates the singular value (Eq. (61)).\n\u2206 (S(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)) = (128)\n\u2206\n( 1\n2 (\u221a (J11 + J22 \u2212 2\u00b5\u0303J12)2 + (J21 \u2212 2\u00b5\u0303J11 \u2212 J12)2 +\u221a\n(J11 \u2212 J22 + 2\u00b5\u0303J12)2 + (J12 + J21 \u2212 2\u00b5\u0303J11)2 )) < 292 .\nPrecision of Implementations. We will show that our computations are correct up to 3 ulps. For our implementation in GNU C library and the hardware architectures that we used, the precision of all mathematical functions that we used is at least one ulp. The term \u201culp\u201d (acronym for \u201cunit in the last place\u201d) was coined by W. Kahan in 1960. It is the highest precision (up to some factor smaller 1), which can be achieved for the given hardware and floating point representation.\nKahan defined ulp as [21]:\n\u201cUlp(x) is the gap between the two finite floating-point numbers nearest x, even if x is one of them. (But ulp(NaN) is NaN.)\u201d\nHarrison defined ulp as [15]:\n\u201can ulp in x is the distance between the two closest straddling floating point numbers a and b, i.e. those with a 6 x 6 b and a 6= b assuming an unbounded exponent range.\u201d\nIn the literature we find also slightly different definitions [29].\nAccording to [29] who refers to [11]:\n\u201cIEEE-754 mandates four standard rounding modes:\u201d \u201cRound-to-nearest: r(x) is the floating-point value closest to x with the usual distance; if two floating-point value are equally close to x, then r(x) is the one whose least significant bit is equal to zero.\u201d \u201cIEEE-754 standardises 5 operations: addition (which we shall note \u2295 in order to distinguish it from the operation over the reals), subtraction ( ), multiplication (\u2297), division ( ), and also square root.\u201d \u201cIEEE-754 specifies em exact rounding [Goldberg, 1991, \u00a71.5]: the result of a floating-point operation is the same as if the operation were performed on the real numbers with the given inputs, then rounded according to the rules in the preceding section. Thus, x \u2295 y is defined as r(x + y), with x and y taken as elements of R \u222a {\u2212\u221e,+\u221e}; the same applies for the other operators.\u201d\nConsequently, the IEEE-754 standard guarantees that addition, subtraction, multiplication, division, and squared root is precise up to one ulp.\nWe have to consider transcendental functions. First the is the exponential function, and then the complementary error function erfc(x), which can be computed via the error function erf(x).\nIntel states [29]:\n\u201cWith the Intel486 processor and Intel 387 math coprocessor, the worst- case, transcendental function error is typically 3 or 3.5 ulps, but is some- times as large as 4.5 ulps.\u201d\nAccording to https://www.mirbsd.org/htman/i386/man3/exp.htm and http: //man.openbsd.org/OpenBSD-current/man3/exp.3:\n\u201cexp(x), log(x), expm1(x) and log1p(x) are accurate to within an ulp\u201d\nwhich is the same for freebsd https://www.freebsd.org/cgi/man.cgi?query=exp&sektion= 3&apropos=0&manpath=freebsd:\n\u201cThe values of exp(0), expm1(0), exp2(integer), and pow(integer, integer) are exact provided that they are representable. Otherwise the error in these functions is generally below one ulp.\u201d\nThe same holds for \u201cFDLIBM\u201d http://www.netlib.org/fdlibm/readme:\n\u201cFDLIBM is intended to provide a reasonably portable (see assumptions below), reference quality (below one ulp for major functions like sin,cos,exp,log) math library (libm.a).\u201d\nIn http://www.gnu.org/software/libc/manual/html_node/ Errors-in-Math-Functions.html we find that both exp and erf have an error of 1 ulp while erfc has an error up to 3 ulps depending on the architecture. For the most common architectures as used by us, however, the error of erfc is 1 ulp.\nWe implemented the function in the programming language C. We rely on the GNU C Library [26]. According to the GNU C Library manual which can be obtained from http://www.gnu.org/ software/libc/manual/pdf/libc.pdf, the errors of the math functions exp, erf , and erfc are not larger than 3 ulps for all architectures [26, pp. 528]. For the architectures ix86, i386/i686/fpu, and m68k/fpmu68k/m680x0/fpu that we used the error are at least one ulp [26, pp. 528].\n2*exp(\u2212x^2)/(sqrt(pi)*(sqrt(x^2+4/pi)+x))\n2*exp(\u2212x^2)/(sqrt(pi)*(sqrt(x^2+2)+x))\nFunction\nerfc(x)\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0 0.5 1.0 1.5 2.0\nx\ny\nFigure A4: Graphs of the upper and lower bounds on erfc. The lower bound 2e \u2212x2\n\u221a \u03c0( \u221a x2+2+x) (red), the\nupper bound 2e \u2212x2 \u221a \u03c0 (\u221a x2+ 4\u03c0+x ) (green) and the function erfc(x) (blue) as treated in Lemma 22.\nA3.4.6 Intermediate Lemmata and Proofs\nSince we focus on the fixed point (\u00b5, \u03bd) = (0, 1), we assume for our whole analysis that \u03b1 = \u03b101 and \u03bb = \u03bb01. Furthermore, we restrict the range of the variables \u00b5 \u2208 [\u00b5min, \u00b5max] = [\u22120.1, 0.1], \u03c9 \u2208 [\u03c9min, \u03c9max] = [\u22120.1, 0.1], \u03bd \u2208 [\u03bdmin, \u03bdmax] = [0.8, 1.5], and \u03c4 \u2208 [\u03c4min, \u03c4max] = [0.8, 1.25]. For bounding different partial derivatives we need properties of different functions. We will bound a the absolute value of a function by computing an upper bound on its maximum and a lower bound on its minimum. These bounds are computed by upper or lower bounding terms. The bounds get tighter if we can combine terms to a more complex function and bound this function. The following lemmata give some properties of functions that we will use in bounding complex functions.\nThroughout this work, we use the error function erf(x) := 1\u221a \u03c0 \u222b x \u2212x e\n\u2212t2 and the complementary error function erfc(x) = 1\u2212 erf(x). Lemma 21 (Basic functions). exp(x) is strictly monotonically increasing from 0 at \u2212\u221e to\u221e at\u221e and has positive curvature.\nAccording to its definition erfc(x) is strictly monotonically decreasing from 2 at \u2212\u221e to 0 at\u221e.\nNext we introduce a bound on erfc:\nLemma 22 (Erfc bound from Abramowitz).\n2e\u2212x 2 \u221a \u03c0 (\u221a x2 + 2 + x ) < erfc(x) 6 2e\u2212x2\u221a \u03c0 (\u221a x2 + 4\u03c0 + x ) , (129)\nfor x > 0.\nProof. The statement follows immediately from [1] (page 298, formula 7.1.13).\nThese bounds are displayed in figure A4.\nLemma 23 (Function ex 2 erfc(x)). ex 2\nerfc(x) is strictly monotonically decreasing for x > 0 and has positive curvature (positive 2nd order derivative), that is, the decreasing slowes down.\nA graph of the function is displayed in Figure A5.\n0 1\n2\n3\n4\n5\n\u22121 0 1 2 3\ne x p (x\n^2 )*\ne rf\nc (x\n)\nx\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n\u22121 0 1 2 3\nx *e\nx p\n(x ^2\n)* e rf\nc (x\n)\nx\nFigure A5: Graphs of the functions ex 2 erfc(x) (left) and xex 2\nerfc(x) (right) treated in Lemma 23 and Lemma 24, respectively.\nProof. The derivative of ex 2 erfc(x) is\n\u2202ex 2 erfc(x)\n\u2202x = 2ex\n2 x erfc(x)\u2212 2\u221a \u03c0 . (130)\nUsing Lemma 22, we get\n\u2202ex 2 erfc(x)\n\u2202x = 2ex\n2 x erfc(x)\u2212 2\u221a \u03c0 < 4x \u221a \u03c0 (\u221a x2 + 4\u03c0 + x ) \u2212 2\u221a \u03c0 =\n2\n( 2\u221a\n4 \u03c0x2\n+1+1 \u2212 1 ) \u221a \u03c0 < 0\n(131)\nThus ex 2 erfc(x) is strictly monotonically decreasing for x > 0.\nThe second order derivative of ex 2 erfc(x) is\n\u22022ex 2 erfc(x)\n\u2202x2 = 4ex\n2 x2 erfc(x) + 2ex 2 erfc(x)\u2212 4x\u221a \u03c0 . (132)\nAgain using Lemma 22 (first inequality), we get\n2 (( 2x2 + 1 ) ex 2\nerfc(x)\u2212 2x\u221a \u03c0\n) > (133)\n4 ( 2x2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x ) \u2212 4x\u221a \u03c0 =\n4 ( x2 \u2212 \u221a x2 + 2x+ 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 4 ( x2 \u2212 \u221a x4 + 2x2 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x\n) > 4 ( x2 \u2212 \u221a x4 + 2x2 + 1 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 0 For the last inequality we added 1 in the numerator in the square root which is subtracted, that is, making a larger negative term in the numerator.\nLemma 24 (Properties of xex 2 erfc(x)). The function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1\u221a\n\u03c0 .\nProof. The derivative of xex 2 erfc(x) is\n2ex 2 x2 erfc(x) + ex 2 erfc(x)\u2212 2x\u221a \u03c0 . (134)\nThis derivative is positive since\n2ex 2 x2 erfc(x) + ex 2 erfc(x)\u2212 2x\u221a \u03c0 = (135) ex 2 ( 2x2 + 1 )\nerfc(x)\u2212 2x\u221a \u03c0 >\n2 ( 2x2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x ) \u2212 2x\u221a \u03c0 = 2 (( 2x2 + 1 ) \u2212 x (\u221a x2 + 2 + x )) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 2 ( x2 \u2212 x \u221a x2 + 2 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x ) = 2 (x2 \u2212 x\u221ax2 + 2 + 1)\u221a \u03c0 (\u221a x2 + 2 + x ) > 2 ( x2 \u2212 x \u221a x2 + 1x2 + 2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 2 ( x2 \u2212 \u221a x4 + 2x2 + 1 + 1\n) \u221a \u03c0 (\u221a x2 + 2 + x ) = 2 ( x2 \u2212 \u221a (x2 + 1) 2 + 1 ) \u221a \u03c0 (\u221a x2 + 2 + x\n) = 0 . We apply Lemma 22 to x erfc(x)ex 2 and divide the terms of the lemma by x, which gives\n2 \u221a \u03c0 (\u221a\n2 x2 + 1 + 1\n) < x erfc(x)ex2 6 2\u221a \u03c0 (\u221a\n4 \u03c0x2 + 1 + 1 ) . (136) For limx\u2192\u221e both the upper and the lower bound go to 1\u221a\u03c0 .\nLemma 25 (Function \u00b5\u03c9). h11(\u00b5, \u03c9) = \u00b5\u03c9 is monotonically increasing in \u00b5\u03c9. It has minimal value t11 = \u22120.01 and maximal value T11 = 0.01.\nProof. Obvious.\nLemma 26 (Function \u03bd\u03c4 ). h22(\u03bd, \u03c4) = \u03bd\u03c4 is monotonically increasing in \u03bd\u03c4 and is positive. It has minimal value t22 = 0.64 and maximal value T22 = 1.875.\nProof. Obvious.\nLemma 27 (Function \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ). h1(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9+\u03bd\u03c4\u221a2\u221a\u03bd\u03c4 is larger than zero and increasing in both \u03bd\u03c4 and \u00b5\u03c9. It has minimal value t1 = 0.5568 and maximal value T1 = 0.9734.\nProof. The derivative of the function \u00b5\u03c9+x\u221a 2 \u221a x with respect to x is\n1\u221a 2 \u221a x \u2212 \u00b5\u03c9 + x 2 \u221a 2x3/2 = 2x\u2212 (\u00b5\u03c9 + x) 2 \u221a 2x3/2 = x\u2212 \u00b5\u03c9 2 \u221a 2x3/2 > 0 , (137)\nsince x > 0.8 \u00b7 0.8 and \u00b5\u03c9 < 0.1 \u00b7 0.1.\nLemma 28 (Function \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ). h2(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9+2\u03bd\u03c4\u221a2\u221a\u03bd\u03c4 is larger than zero and increasing in both \u03bd\u03c4 and \u00b5\u03c9. It has minimal value t2 = 1.1225 and maximal value T2 = 1.9417.\nProof. The derivative of the function \u00b5\u03c9+2x\u221a 2 \u221a x with respect to x is \u221a\n2\u221a x \u2212 \u00b5\u03c9 + 2x 2 \u221a 2x3/2 = 4x\u2212 (\u00b5\u03c9 + 2x) 2 \u221a 2x3/2 = 2x\u2212 \u00b5\u03c9 2 \u221a 2x3/2 > 0 . (138)\nLemma 29 (Function \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ). h3(\u00b5, \u03c9, \u03bd, \u03c4) = \u00b5\u03c9\u221a2\u221a\u03bd\u03c4 monotonically decreasing in \u03bd\u03c4 and monotonically increasing in \u00b5\u03c9. It has minimal value t3 = \u22120.0088388 and maximal value T3 = 0.0088388.\nProof. Obvious. Lemma 30 (Function (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n)2 ). h4(\u00b5, \u03c9, \u03bd, \u03c4) = ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )2 has a minimum at 0 for \u00b5 = 0 or\n\u03c9 = 0 and has a maximum for the smallest \u03bd\u03c4 and largest |\u00b5\u03c9| and is larger or equal to zero. It has minimal value t4 = 0 and maximal value T4 = 0.000078126.\nProof. Obvious. Lemma 31 (Function \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4\n). \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4 > 0 and decreasing in \u03bd\u03c4 .\nProof. Statements follow directly from elementary functions square root and division. Lemma 32 (Function 2 \u2212 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) ). 2 \u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) > 0 and decreasing in \u03bd\u03c4 and\nincreasing in \u00b5\u03c9.\nProof. Statements follow directly from Lemma 21 and erfc. Lemma 33 (Function \u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) ). For \u03bb = \u03bb01 and \u03b1 = \u03b101,\u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) < 0 and increasing in both \u03bd\u03c4 and \u00b5\u03c9.\nProof. We consider the function \u221a\n2 \u03c0 ( (\u03b1\u22121)\u00b5\u03c9 x3/2 \u2212 \u03b1\u221a x ) , which has the derivative with respect to x:\u221a\n2\n\u03c0\n( \u03b1\n2x3/2 \u2212 3(\u03b1\u2212 1)\u00b5\u03c9 2x5/2\n) . (139)\nThis derivative is larger than zero, since\u221a 2\n\u03c0\n( \u03b1\n2(\u03bd\u03c4)3/2 \u2212 3(\u03b1\u2212 1)\u00b5\u03c9 2(\u03bd\u03c4)5/2\n) >\n\u221a 2 \u03c0 ( \u03b1\u2212 3(\u03b1\u22121)\u00b5\u03c9\u03bd\u03c4 ) 2(\u03bd\u03c4)3/2 > 0 . (140)\nThe last inequality follows from \u03b1\u2212 3\u00b70.1\u00b70.1(\u03b1\u22121)0.8\u00b70.8 > 0 for \u03b1 = \u03b101. We next consider the function \u221a\n2 \u03c0 ( (\u03b1\u22121)x (\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4 ) , which has the derivative with respect to x:\u221a\n2 \u03c0 (\u03b1\u2212 1) (\u03bd\u03c4)3/2 > 0 . (141)\nLemma 34 (Function \u221a\n2 \u03c0\n( (\u22121)(\u03b1\u22121)\u00b52\u03c92\n(\u03bd\u03c4)3/2 + \u2212\u03b1+\u03b1\u00b5\u03c9+1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 )\n). The function\u221a 2 \u03c0 ( (\u22121)(\u03b1\u22121)\u00b52\u03c92 (\u03bd\u03c4)3/2 + \u2212\u03b1+\u03b1\u00b5\u03c9+1\u221a \u03bd\u03c4 \u2212 \u03b1 \u221a \u03bd\u03c4 ) < 0 is decreasing in \u03bd\u03c4 and increasing in \u00b5\u03c9.\nProof. We define the function\u221a 2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\nx3/2 + \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1\u221a x \u2212 \u03b1 \u221a x\n) (142)\nwhich has as derivative with respect to x:\u221a 2\n\u03c0\n( 3(\u03b1\u2212 1)\u00b52\u03c92\n2x5/2 \u2212 \u2212\u03b1+ \u03b1\u00b5\u03c9 + 1 2x3/2 \u2212 \u03b1 2 \u221a x\n) = (143)\n1\u221a 2\u03c0x5/2\n( 3(\u03b1\u2212 1)\u00b52\u03c92 \u2212 x(\u2212\u03b1+ \u03b1\u00b5\u03c9 + 1)\u2212 \u03b1x2 ) .\nThe derivative of the term 3(\u03b1 \u2212 1)\u00b52\u03c92 \u2212 x(\u2212\u03b1 + \u03b1\u00b5\u03c9 + 1) \u2212 \u03b1x2 with respect to x is \u22121 + \u03b1\u2212 \u00b5\u03c9\u03b1\u2212 2\u03b1x < 0, since 2\u03b1x > 1.6\u03b1. Therefore the term is maximized with the smallest value for x, which is x = \u03bd\u03c4 = 0.8 \u00b7 0.8. For \u00b5\u03c9 we use for each term the value which gives maximal contribution. We obtain an upper bound for the term:\n3(\u22120.1 \u00b7 0.1)2(\u03b101 \u2212 1)\u2212 (0.8 \u00b7 0.8)2\u03b101 \u2212 0.8 \u00b7 0.8((\u22120.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1) = \u22120.243569 . (144)\nTherefore the derivative with respect to x = \u03bd\u03c4 is smaller than zero and the original function is decreasing in \u03bd\u03c4\nWe now consider the derivative with respect to x = \u00b5\u03c9. The derivative with respect to x of the function \u221a\n2\n\u03c0\n( \u2212\u03b1 \u221a \u03bd\u03c4 \u2212 (\u03b1\u2212 1)x 2\n(\u03bd\u03c4)3/2 + \u2212\u03b1+ \u03b1x+ 1\u221a \u03bd\u03c4\n) (145)\nis \u221a 2 \u03c0 (\u03b1\u03bd\u03c4 \u2212 2(\u03b1\u2212 1)x)\n(\u03bd\u03c4)3/2 . (146)\nSince \u22122x(\u22121 + \u03b1) + \u03bd\u03c4\u03b1 > \u22122 \u00b7 0.01 \u00b7 (\u22121 + \u03b101) + 0.8 \u00b7 0.8\u03b101 > 1.0574 > 0, the derivative is larger than zero. Consequently, the original function is increasing in \u00b5\u03c9.\nThe maximal value is obtained with the minimal \u03bd\u03c4 = 0.8 \u00b7 0.8 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The maximal value is\u221a\n2\n\u03c0\n( 0.1 \u00b7 0.1\u03b101 \u2212 \u03b101 + 1\u221a\n0.8 \u00b7 0.8 + 0.120.12(\u22121)(\u03b101 \u2212 1) (0.8 \u00b7 0.8)3/2 \u2212 \u221a\n0.8 \u00b7 0.8\u03b101 ) = \u22121.72296 .\n(147)\nTherefore the original function is smaller than zero. Lemma 35 (Function \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4 ) ). For \u03bb = \u03bb01 and \u03b1 = \u03b101,\u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4\n) < 0 and increasing in both \u03bd\u03c4 and \u00b5\u03c9.\nProof. The derivative of the function\u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\nx3/2 \u2212 3\u03b1\n2 \u221a x\n) (148)\nwith respect to x is\u221a 2\n\u03c0\n( 3\u03b12\n2x3/2 \u2212\n3 ( \u03b12 \u2212 1 ) \u00b5\u03c9\n2x5/2\n) = 3 ( \u03b12x\u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9 )\n\u221a 2\u03c0x5/2\n> 0 , (149)\nsince \u03b12x\u2212 \u00b5\u03c9(\u22121 + \u03b12) > \u03b12010.8 \u00b7 0.8\u2212 0.1 \u00b7 0.1 \u00b7 (\u22121 + \u03b1201) > 1.77387 The derivative of the function \u221a\n2\n\u03c0\n(( \u03b12 \u2212 1 ) x\n(\u03bd\u03c4)3/2 \u2212 3\u03b1\n2 \u221a \u03bd\u03c4\n) (150)\nwith respect to x is \u221a 2 \u03c0 ( \u03b12 \u2212 1 ) (\u03bd\u03c4)3/2 > 0 . (151)\nThe maximal function value is obtained by maximal \u03bd\u03c4 = 1.5 \u00b7 1.25 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The maximal value is \u221a\n2 \u03c0\n( 0.1\u00b70.1(\u03b1201\u22121)\n(1.5\u00b71.25)3/2 \u2212 3\u03b1201\u221a 1.5\u00b71.25\n) = \u22124.88869. Therefore the function is\nnegative.\nLemma 36 (Function \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) ). The function\u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) < 0 is decreasing in \u03bd\u03c4 and increasing in \u00b5\u03c9.\nProof. The derivative of the function\u221a 2\n\u03c0\n(( \u03b12 \u2212 1 ) \u00b5\u03c9\n\u221a x\n\u2212 3\u03b12 \u221a x ) (152)\nwith respect to x is\u221a 2\n\u03c0\n( \u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9\n2x3/2 \u2212 3\u03b1\n2\n2 \u221a x\n) = \u2212 ( \u03b12 \u2212 1 ) \u00b5\u03c9 \u2212 3\u03b12x\n\u221a 2\u03c0x3/2\n< 0 , (153)\nsince \u22123\u03b12x\u2212 \u00b5\u03c9(\u22121 + \u03b12) < \u22123\u03b12010.8 \u00b7 0.8 + 0.1 \u00b7 0.1(\u22121 + \u03b1201) < \u22125.35764. The derivative of the function \u221a\n2\n\u03c0\n(( \u03b12 \u2212 1 ) x\n\u221a \u03bd\u03c4\n\u2212 3\u03b12 \u221a \u03bd\u03c4 ) (154)\nwith respect to x is \u221a 2 \u03c0 ( \u03b12 \u2212 1 ) \u221a \u03bd\u03c4 > 0 . (155)\nThe maximal function value is obtained for minimal \u03bd\u03c4 = 0.8 \u00b7 0.8 and the maximal \u00b5\u03c9 = 0.1 \u00b7 0.1. The value is \u221a\n2 \u03c0\n( 0.1\u00b70.1(\u03b1201\u22121)\u221a\n0.8\u00b70.8 \u2212 3 \u221a 0.8 \u00b7 0.8\u03b1201 ) = \u22125.34347. Thus, the function is\nnegative.\nLemma 37 (Function \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) ). The function \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) > 0 is\nincreasing in \u03bd\u03c4 and decreasing in \u00b5\u03c9.\nProof. The derivative of the function\nxe (\u00b5\u03c9+x)2 2x erfc ( \u00b5\u03c9 + x\u221a\n2 \u221a x\n) (156)\nwith respect to x is\ne (\u00b5\u03c9+x)2 2x ( x(x+ 2)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+x\u221a\n2 \u221a x ) 2x + \u00b5\u03c9 \u2212 x\u221a 2\u03c0 \u221a x . (157)\nThis derivative is larger than zero, since\ne (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4 ) 2\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 > (158)\n0.4349 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) 2\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 >\n0.5 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) \u221a\n2\u03c0\u03bd\u03c4 + \u00b5\u03c9 \u2212 \u03bd\u03c4\u221a 2\u03c0 \u221a \u03bd\u03c4 =\n0.5 ( \u03bd\u03c4(\u03bd\u03c4 + 2)\u2212 \u00b52\u03c92 ) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 \u03bd\u03c4)\n\u221a 2\u03c0\u03bd\u03c4\n=\n\u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 0.5(\u03bd\u03c4)2 \u2212 \u03bd\u03c4 \u221a \u03bd\u03c4 + \u03bd\u03c4\u221a\n2\u03c0\u03bd\u03c4 =\n\u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + (0.5\u03bd\u03c4 \u2212 \u221a \u03bd\u03c4)\n2 + 0.25(\u03bd\u03c4)2\u221a\n2\u03c0\u03bd\u03c4 > 0 .\nWe explain this chain of inequalities:\n\u2022 The first inequality follows by applying Lemma 23 which says that e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4 ) is strictly monotonically decreasing. The minimal value that is larger than 0.4349 is taken on at the maximal values \u03bd\u03c4 = 1.5 \u00b7 1.25 and \u00b5\u03c9 = 0.1 \u00b7 0.1.\n\u2022 The second inequality uses 120.4349 \u221a 2\u03c0 = 0.545066 > 0.5.\n\u2022 The equalities are just algebraic reformulations. \u2022 The last inequality follows from \u22120.5\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 0.25(\u03bd\u03c4)2 > 0.25(0.8 \u00b7 0.8)2 \u2212\n0.5 \u00b7 (0.1)2(0.1)2 \u2212 0.1 \u00b7 0.1 \u00b7 \u221a 0.8 \u00b7 0.8 = 0.09435 > 0.\nTherefore the function is increasing in \u03bd\u03c4 .\nDecreasing in \u00b5\u03c9 follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows form the fact that erfc and the exponential function are positive and that \u03bd\u03c4 > 0.\nLemma 38 (Function \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) ). The function \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) > 0\nis increasing in \u03bd\u03c4 and decreasing in \u00b5\u03c9.\nProof. The derivative of the function\nxe (\u00b5\u03c9+2x)2 2x erfc ( \u00b5\u03c9 + 2x\u221a\n2 \u221a 2x\n) (159)\nis\ne (\u00b5\u03c9+2x)2 4x (\u221a \u03c0e (\u00b5\u03c9+2x)2 4x ( 2x(2x+ 1)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+2x\n2 \u221a x\n) + \u221a x(\u00b5\u03c9 \u2212 2x) ) 2 \u221a \u03c0x . (160)\nWe only have to determine the sign of \u221a \u03c0e (\u00b5\u03c9+2x)2 4x ( 2x(2x+ 1)\u2212 \u00b52\u03c92 ) erfc ( \u00b5\u03c9+2x\n2 \u221a x\n) + \u221a x(\u00b5\u03c9\u2212\n2x) since all other factors are obviously larger than zero.\nThis derivative is larger than zero, since\n\u221a \u03c0e (\u00b5\u03c9+2\u03bd\u03c4)2 4\u03bd\u03c4 ( 2\u03bd\u03c4(2\u03bd\u03c4 + 1)\u2212 \u00b52\u03c92 ) erfc\n( \u00b5\u03c9 + 2\u03bd\u03c4\n2 \u221a \u03bd\u03c4\n) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 2\u03bd\u03c4) > (161)\n0.463979 ( 2\u03bd\u03c4(2\u03bd\u03c4 + 1)\u2212 \u00b52\u03c92 ) + \u221a \u03bd\u03c4(\u00b5\u03c9 \u2212 2\u03bd\u03c4) =\n\u2212 0.463979\u00b52\u03c92 + \u00b5\u03c9 \u221a \u03bd\u03c4 + 1.85592(\u03bd\u03c4)2 + 0.927958\u03bd\u03c4 \u2212 2\u03bd\u03c4 \u221a \u03bd\u03c4 =\n\u00b5\u03c9 (\u221a \u03bd\u03c4 \u2212 0.463979\u00b5\u03c9 ) + 0.85592(\u03bd\u03c4)2 + ( \u03bd\u03c4 \u2212 \u221a \u03bd\u03c4 )2 \u2212 0.0720421\u03bd\u03c4 > 0 .\nWe explain this chain of inequalities:\n\u2022 The first inequality follows by applying Lemma 23 which says that e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) is strictly monotonically decreasing. The minimal value\nthat is larger than 0.261772 is taken on at the maximal values \u03bd\u03c4 = 1.5 \u00b7 1.25 and \u00b5\u03c9 = 0.1 \u00b7 0.1. 0.261772 \u221a \u03c0 > 0.463979.\n\u2022 The equalities are just algebraic reformulations.\n\u2022 The last inequality follows from \u00b5\u03c9 ( \u221a \u03bd\u03c4 \u2212 0.463979\u00b5\u03c9) + 0.85592(\u03bd\u03c4)2 \u2212 0.0720421\u03bd\u03c4 > 0.85592 \u00b7 (0.8 \u00b7 0.8)2 \u2212 0.1 \u00b7 0.1 (\u221a 1.5 \u00b7 1.25 + 0.1 \u00b7 0.1 \u00b7 0.463979 ) \u2212\n0.0720421 \u00b7 1.5 \u00b7 1.25 > 0.201766.\nTherefore the function is increasing in \u03bd\u03c4 .\nDecreasing in \u00b5\u03c9 follows from decreasing of ex 2\nerfc(x) according to Lemma 23. Positivity follows from the fact that erfc and the exponential function are positive and that \u03bd\u03c4 > 0.\nLemma 39 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J11(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J12(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), J21(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1), and J22(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) with respect to \u00b5, \u03c9, \u03bd, and \u03c4 hold:\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5\n\u2223\u2223\u2223\u2223 < 0.0031049101995398316 (162)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.055872374194189\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.03749149348255419\n\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.031242911235461816\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.21232788238624354\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.2124377655377270\n\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.02220441024325437\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.146955401845684\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.17980135762932363\n\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.14983446469110305\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 1.805740052651535\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 2.396685907216327\nProof. For each derivative we compute a lower and an upper bound and take the maximum of the absolute value. A lower bound is determined by minimizing the single terms of the functions that represents the derivative. An upper bound is determined by maximizing the single terms of the functions that represent the derivative. Terms can be combined to larger terms for which the maximum and the minimum must be known. We apply many previous lemmata which state properties of functions representing single or combined terms. The more terms are combined, the tighter the bounds can be made.\nNext we go through all the derivatives, where we use Lemma 25, Lemma 26, Lemma 27, Lemma 28, Lemma 29, Lemma 30, Lemma 21, and Lemma 23 without citing. Furthermore, we use the bounds on the simple expressions t11,t22, ..., and T4 as defined the aforementioned lemmata:\n\u2022 \u2202J11\u2202\u00b5\nWe use Lemma 31 and consider the expression \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 \u221a\n2 \u03c0 (\u03b1\u22121)\u221a \u03bd\u03c4\nin brackets. An upper bound on the maximum of is\n\u03b101e t21 erfc(t1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nT22 = 0.591017 . (163)\nA lower bound on the minimum is\n\u03b101e T 21 erfc(T1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nt22 = 0.056318 . (164)\nThus, an upper bound on the maximal absolute value is\n1 2 \u03bb01\u03c9 2 maxe t4\n\u03b101et21 erfc(t1)\u2212 \u221a 2 \u03c0 (\u03b101 \u2212 1)\u221a\nT22\n = 0.0031049101995398316 . (165)\n\u2022 \u2202J11\u2202\u03c9\nWe use Lemma 31 and consider the expression \u221a 2 \u03c0 (\u03b1\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 \u03b1(\u00b5\u03c9 +\n1)e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) in brackets.\nAn upper bound on the maximum is\u221a 2 \u03c0 (\u03b101 \u2212 1)T11\u221a\nt22 \u2212 \u03b101(t11 + 1)eT 2 1 erfc(T1) = \u22120.713808 . (166)\nA lower bound on the minimum is\u221a 2 \u03c0 (\u03b101 \u2212 1)t11\u221a\nt22 \u2212 \u03b101(T11 + 1)et 2 1 erfc(t1) = \u22120.99987 . (167)\nThis term is subtracted, and 2\u2212 erfc(x) > 0, therefore we have to use the minimum and the maximum for the argument of erfc.\nThus, an upper bound on the maximal absolute value is\n1 2 \u03bb01\n\u2212et4  \u221a 2 \u03c0 (\u03b101 \u2212 1)t11\u221a\nt22 \u2212 \u03b101(T11 + 1)et 2 1 erfc(t1)\n \u2212 erfc(T3) + 2  =\n(168) 1.055872374194189 .\n\u2022 \u2202J11\u2202\u03bd We consider the term in brackets\n\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n( (\u03b1\u2212 1)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 \u03b1\u221a \u03bd\u03c4\n) . (169)\nWe apply Lemma 33 for the first sub-term. An upper bound on the maximum is\n\u03b101e t21 erfc(t1) +\n\u221a 2\n\u03c0\n( (\u03b101 \u2212 1)T11\nT 3/2 22\n\u2212 \u03b101\u221a T22\n) = 0.0104167 . (170)\nA lower bound on the minimum is\n\u03b101e T 21 erfc(T1) +\n\u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n) = \u22120.95153 . (171)\nThus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03c4max\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (172)\n0.031242911235461816 .\n\u2022 \u2202J11\u2202\u03c4 We use the results of item \u2202J11\u2202\u03bd were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03bdmax\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (173)\n0.03749149348255419 .\n\u2022 \u2202J12\u2202\u00b5 Since \u2202J12\u2202\u00b5 = \u2202J11 \u2202\u03bd , an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u03c4max\u03c9maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (174)\n0.031242911235461816 .\n\u2022 \u2202J12\u2202\u03c9 We use the results of item \u2202J11\u2202\u03bd were the brackets are only differently scaled. Thus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb01\u00b5max\u03c4maxe t4\n( \u03b101e T 21 erfc(T1) + \u221a 2\n\u03c0\n( (\u03b101 \u2212 1)t11\nt 3/2 22\n\u2212 \u03b101\u221a t22\n)) = (175)\n0.031242911235461816 .\n\u2022 \u2202J12\u2202\u03bd For the second term in brackets, we see that \u03b101\u03c42mine\nT 21 erfc(T1) = 0.465793 and \u03b101\u03c4 2 maxe t21 erfc(t1) = 1.53644. We now check different values for\u221a 2\n\u03c0\n( (\u22121)(\u03b1\u2212 1)\u00b52\u03c92\n\u03bd5/2 \u221a \u03c4\n+\n\u221a \u03c4(\u03b1+ \u03b1\u00b5\u03c9 \u2212 1)\n\u03bd3/2 \u2212 \u03b1\u03c4\n3/2\n\u221a \u03bd\n) , (176)\nwhere we maximize or minimize all single terms.\nA lower bound on the minimum of this expression is\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52max\u03c92max\n\u03bd 5/2 min \u221a \u03c4min\n+\n\u221a \u03c4min(\u03b101 + \u03b101t11 \u2212 1)\n\u03bd 3/2 max\n\u2212 \u03b101\u03c4 3/2 max\u221a\n\u03bdmin\n) = (177)\n\u2212 1.83112 .\nAn upper bound on the maximum of this expression is\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n+\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax\n) = (178)\n0.0802158 .\nAn upper bound on the maximum is\n1 8 \u03bb01e t4\n(\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax + (179)\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n) + \u03b101\u03c4 2 maxe t21 erfc(t1) ) = 0.212328 .\nA lower bound on the minimum is 1\n8 \u03bb01e\nt4 ( \u03b101\u03c4 2 mine\nT 21 erfc(T1) + (180)\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52max\u03c92max\n\u03bd 5/2 min \u221a \u03c4min\n+\n\u221a \u03c4min(\u03b101 + \u03b101t11 \u2212 1)\n\u03bd 3/2 max\n\u2212 \u03b101\u03c4 3/2 max\u221a\n\u03bdmin\n)) =\n\u2212 0.179318 .\nThus, an upper bound on the maximal absolute value is\n1 8 \u03bb01e t4\n(\u221a 2\n\u03c0\n( (\u22121)(\u03b101 \u2212 1)\u00b52min\u03c92min\n\u03bd 5/2 max \u221a \u03c4max\n\u2212 \u03b101\u03c4 3/2 min\u221a\n\u03bdmax + (181)\n\u221a \u03c4max(\u03b101 + \u03b101T11 \u2212 1)\n\u03bd 3/2 min\n) + \u03b101\u03c4 2 maxe t21 erfc(t1) ) = 0.21232788238624354 .\n\u2022 \u2202J12\u2202\u03c4 We use Lemma 34 to obtain an upper bound on the maximum of the expression of the lemma:\u221a\n2\n\u03c0\n( 0.12 \u00b7 0.12(\u22121)(\u03b101 \u2212 1)\n(0.8 \u00b7 0.8)3/2 \u2212 \u221a 0.8 \u00b7 0.8\u03b101 + (0.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1\u221a 0.8 \u00b7 0.8\n) = \u22121.72296 .\n(182)\nWe use Lemma 34 to obtain an lower bound on the minimum of the expression of the lemma:\u221a 2\n\u03c0\n( 0.12 \u00b7 0.12(\u22121)(\u03b101 \u2212 1)\n(1.5 \u00b7 1.25)3/2 \u2212 \u221a 1.5 \u00b7 1.25\u03b101 + (\u22120.1 \u00b7 0.1)\u03b101 \u2212 \u03b101 + 1\u221a 1.5 \u00b7 1.25\n) = \u22122.2302 .\n(183)\nNext we apply Lemma 37 for the expression \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . We use Lemma 37\nto obtain an upper bound on the maximum of this expression:\n1.5 \u00b7 1.25e (1.5\u00b71.25\u22120.1\u00b70.1)2\n2\u00b71.5\u00b71.25 \u03b101 erfc\n( 1.5 \u00b7 1.25\u2212 0.1 \u00b7 0.1\u221a\n2 \u221a 1.5 \u00b7 1.25\n) = 1.37381 . (184)\nWe use Lemma 37 to obtain an lower bound on the minimum of this expression:\n0.8 \u00b7 0.8e (0.8\u00b70.8+0.1\u00b70.1)2\n2\u00b70.8\u00b70.8 \u03b101 erfc\n( 0.8 \u00b7 0.8 + 0.1 \u00b7 0.1\u221a\n2 \u221a 0.8 \u00b7 0.8\n) = 0.620462 . (185)\nNext we apply Lemma 23 for 2\u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . An upper bound on this expres-\nsion is\n2e (0.8\u00b70.8\u22120.1\u00b70.1)2\n20.8\u00b70.8 \u03b101 erfc\n( 0.8 \u00b7 0.8\u2212 0.1 \u00b7 0.1\u221a\n2 \u221a 0.8 \u00b7 0.8\n) = 1.96664 . (186)\nA lower bound on this expression is\n2e (1.5\u00b71.25+0.1\u00b70.1)2\n2\u00b71.5\u00b71.25 \u03b101 erfc\n( 1.5 \u00b7 1.25 + 0.1 \u00b7 0.1\u221a\n2 \u221a 1.5 \u00b7 1.25\n) = 1.4556 . (187)\nThe sum of the minimal values of the terms is\u22122.23019+0.62046+1.45560 = \u22120.154133. The sum of the maximal values of the terms is \u22121.72295 + 1.37380 + 1.96664 = 1.61749. Thus, an upper bound on the maximal absolute value is\n1 8 \u03bb01e t4\n( \u03b101T22e (t11+T22) 2 2T22 erfc ( t11 + T22\u221a\n2 \u221a T22\n) + (188)\n2\u03b101e t21 erfc(t1) +\n\u221a 2\n\u03c0\n( \u2212 (\u03b101 \u2212 1)T 2 11\nt 3/2 22\n+ \u2212\u03b101 + \u03b101T11 + 1\u221a\nt22 \u2212\n\u03b101 \u221a t22 )) = 0.2124377655377270 .\n\u2022 \u2202J21\u2202\u00b5 An upper bound on the maximum is\n\u03bb201\u03c9 2 max ( \u03b1201e T 21 ( \u2212e\u2212T4 ) erfc(T1) + 2\u03b1 2 01e t22et4 erfc(t2) \u2212 erfc(T3) + 2 ) = (189)\n0.0222044 .\nA upper bound on the absolute minimum is\n\u03bb201\u03c9 2 max ( \u03b1201e t21 ( \u2212e\u2212t4 ) erfc(t1) + 2\u03b1 2 01e T 22 eT4 erfc(T2) \u2212 erfc(t3) + 2 ) = (190)\n0.00894889 .\nThus, an upper bound on the maximal absolute value is\n\u03bb201\u03c9 2 max ( \u03b1201e T 21 ( \u2212e\u2212T4 ) erfc(T1) + 2\u03b1 2 01e t22et4 erfc(t2) \u2212 erfc(T3) + 2 ) = (191)\n0.02220441024325437 .\n\u2022 \u2202J21\u2202\u03c9 An upper bound on the maximum is\n\u03bb201 ( \u03b1201(2T11 + 1)e t22e\u2212t4 erfc(t2) + 2T11(2\u2212 erfc(T3)) + (192)\n\u03b1201(t11 + 1)e T 21 ( \u2212e\u2212T4 ) erfc(T1) +\n\u221a 2\n\u03c0\n\u221a T22e \u2212t4 ) = 1.14696 .\nA lower bound on the minimum is\n\u03bb201 ( \u03b1201(T11 + 1)e t21 ( \u2212e\u2212t4 ) erfc(t1) + (193)\n\u03b1201(2t11 + 1)e T 22 e\u2212T4 erfc(T2) + 2t11(2\u2212 erfc(T3))+\n\u221a 2\n\u03c0\n\u221a t22e \u2212T4 ) = \u22120.359403 .\nThus, an upper bound on the maximal absolute value is\n\u03bb201 ( \u03b1201(2T11 + 1)e t22e\u2212t4 erfc(t2) + 2T11(2\u2212 erfc(T3)) + (194)\n\u03b1201(t11 + 1)e T 21 ( \u2212e\u2212T4 ) erfc(T1) +\n\u221a 2\n\u03c0\n\u221a T22e \u2212t4 ) = 1.146955401845684 .\n\u2022 \u2202J21\u2202\u03bd An upper bound on the maximum is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (195)\n0.149834 .\nA lower bound on the minimum is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (196)\n\u2212 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (197)\n0.14983446469110305 .\n\u2022 \u2202J21\u2202\u03c4 An upper bound on the maximum is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (198)\n0.179801 .\nA lower bound on the minimum is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (199)\n\u2212 0.0421242 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03bdmax\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (200)\n0.17980135762932363 .\n\u2022 \u2202J22\u2202\u00b5 We use the fact that \u2202J22\u2202\u00b5 = \u2202J21 \u2202\u03bd . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u03c4max\u03c9maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (201)\n0.14983446469110305 .\n\u2022 \u2202J22\u2202\u03c9 An upper bound on the maximum is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (202)\n0.149834 .\nA lower bound on the minimum is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212et21) erfc(t1) + 4\u03b1201eT 22 erfc(T2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a t22  = (203)\n\u2212 0.0351035 . Thus, an upper bound on the maximal absolute value is\n1 2 \u03bb201\u00b5max\u03c4maxe \u2212t4\n\u03b1201 (\u2212eT 21 ) erfc(T1) + 4\u03b1201et22 erfc(t2) + \u221a 2 \u03c0 (\u22121) ( \u03b1201 \u2212 1 ) \u221a T22  = (204)\n0.14983446469110305 .\n\u2022 \u2202J22\u2202\u03bd\nWe apply Lemma 35 to the expression \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\n(\u03bd\u03c4)3/2 \u2212 3\u03b1 2 \u221a \u03bd\u03c4\n) . Using Lemma 35, an\nupper bound on the maximum is 1\n4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212eT 2 1 ) erfc(T1) + 8\u03b1 2 01e\nt22 erfc(t2) + (205)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) T11\nT 3/2 22\n\u2212 3\u03b1 2 01\u221a T22\n)) = 1.19441 .\nUsing Lemma 35, a lower bound on the minimum is 1\n4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212et 2 1 ) erfc(t1) + 8\u03b1 2 01e\nT 22 erfc(T2) + (206)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) t11\nt 3/2 22\n\u2212 3\u03b1 2 01\u221a t22\n)) = \u22121.80574 .\nThus, an upper bound on the maximal absolute value is\n\u2212 1 4 \u03bb201\u03c4 2 maxe\n\u2212t4 ( \u03b1201 ( \u2212et 2 1 ) erfc(t1) + 8\u03b1 2 01e\nT 22 erfc(T2) + (207)\u221a 2\n\u03c0\n(( \u03b1201 \u2212 1 ) t11\nt 3/2 22\n\u2212 3\u03b1 2 01\u221a t22\n)) = 1.805740052651535 .\n\u2022 \u2202J22\u2202\u03c4\nWe apply Lemma 36 to the expression \u221a\n2 \u03c0\n( (\u03b12\u22121)\u00b5\u03c9\u221a\n\u03bd\u03c4 \u2212 3\u03b12\n\u221a \u03bd\u03c4 ) .\nWe apply Lemma 37 to the expression \u03bd\u03c4e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) . We apply Lemma 38 to\nthe expression \u03bd\u03c4e (\u00b5\u03c9+2\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) .\nWe combine the results of these lemmata to obtain an upper bound on the maximum:\n1 4 \u03bb201\n( \u2212\u03b1201t22e\u2212T4e (T11+t22) 2 2t22 erfc ( T11 + t22\u221a\n2 \u221a t22\n) + (208)\n8\u03b1201T22e \u2212t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e T 21 e\u2212T4 erfc(T1) + 4\u03b1 2 01e t22e\u2212t4 erfc(t2) + 2(2\u2212 erfc(T3)) +\u221a 2\n\u03c0 e\u2212T4\n(( \u03b1201 \u2212 1 ) T11\u221a\nt22 \u2212 3\u03b1201\n\u221a t22 )) = 2.39669 .\nWe combine the results of these lemmata to obtain an lower bound on the minimum: 1\n4 \u03bb201\n( 8\u03b1201t22e \u2212T4e (T11+2t22) 2 2t22 erfc ( T11 + 2t22\u221a\n2 \u221a t22\n) + (209)\n\u03b1201T22e \u2212t4e\n(t11+T22) 2\n2T22 erfc ( t11 + T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e t21e\u2212t4 erfc(t1) + 4\u03b1 2 01e T 22 e\u2212T4 erfc(T2) + 2(2\u2212 erfc(t3)) + \u221a 2\n\u03c0 e\u2212t4\n(( \u03b1201 \u2212 1 ) t11\u221a\nT22 \u2212 3\u03b1201\n\u221a T22 )) = \u22121.17154 .\nThus, an upper bound on the maximal absolute value is\n1 4 \u03bb201\n( \u2212\u03b1201t22e\u2212T4e (T11+t22) 2 2t22 erfc ( T11 + t22\u221a\n2 \u221a t22\n) + (210)\n8\u03b1201T22e \u2212t4e\n(t11+2T22) 2\n2T22 erfc ( t11 + 2T22\u221a\n2 \u221a T22\n) \u2212\n2\u03b1201e T 21 e\u2212T4 erfc(T1) + 4\u03b1 2 01e t22e\u2212t4 erfc(t2) + 2(2\u2212 erfc(T3)) +\u221a 2\n\u03c0 e\u2212T4\n(( \u03b1201 \u2212 1 ) T11\u221a\nt22 \u2212 3\u03b1201\n\u221a t22 )) = 2.396685907216327 .\nLemma 40 (Derivatives of the Mapping). We assume \u03b1 = \u03b101 and \u03bb = \u03bb01. We restrict the range of the variables to the domain \u00b5 \u2208 [\u22120.1, 0.1], \u03c9 \u2208 [\u22120.1, 0.1], \u03bd \u2208 [0.8, 1.5], and \u03c4 \u2208 [0.8, 1.25].\nThe derivative \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nThe derivative \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\nThe derivative \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is positive.\nProof. \u2022 \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\n(2\u2212 erfc(x) > 0 according to Lemma 21 and ex2 erfc(x) is also larger than zero according to Lemma 23. Consequently, has \u2202\u2202\u00b5 \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) the sign of \u03c9.\n\u2022 \u2202\u2202\u03bd \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\nLemma 23 says ex 2 erfc(x) is decreasing in \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4\n. The first term (negative) is increasing in \u03bd\u03c4 since it is proportional to minus one over the squared root of \u03bd\u03c4 .\nWe obtain a lower bound by setting \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 = 1.5\u00b71.25+0.1\u00b70.1\u221a 2 \u221a 1.5\u00b71.25 for the e x2 erfc(x)\nterm. The term in brackets is larger than e ( 1.5\u00b71.25+0.1\u00b70.1\u221a 2 \u221a 1.5\u00b71.25 )2 \u03b101 erfc ( 1.5\u00b71.25+0.1\u00b70.1\u221a\n2 \u221a 1.5\u00b71.25 ) \u2212\u221a\n2 \u03c00.8\u00b70.8 (\u03b101 \u2212 1) = 0.056 Consequently, the function is larger than zero.\n\u2022 \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1)\nWe consider the sub-function\u221a 2\n\u03c0\n\u221a \u03bd\u03c4 \u2212 \u03b12 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) .\n(211)\nWe set x = \u03bd\u03c4 and y = \u00b5\u03c9 and obtain\u221a 2\n\u03c0\n\u221a x\u2212 \u03b12 ( e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 e ( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n)) . (212)\nThe derivative of this sub-function with respect to y is \u03b12 ( e (2x+y)2 2x (2x+ y) erfc (\n2x+y\u221a 2 \u221a x\n) \u2212 e (x+y)2 2x (x+ y) erfc ( x+y\u221a 2 \u221a x )) x = (213)\n\u221a 2\u03b12 \u221a x  e (2x+y)22x (x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 e (x+y)2 2x (x+y) erfc ( x+y\u221a 2 \u221a x ) \u221a 2 \u221a x  x > 0 .\nThe inequality follows from Lemma 24, which states that zez 2\nerfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y.\nThe derivative of this sub-function with respect to x is \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2 ( \u03b12 \u2212 1 ) x3/2\n2 \u221a \u03c0x2\n.\n(214)\nThe sub-function is increasing in x, since the derivative is larger than zero: \u221a \u03c0\u03b12 ( e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a\n2 \u221a x\n) \u2212 e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x )) \u2212 \u221a 2x3/2 ( \u03b12 \u2212 1 ) 2 \u221a \u03c0x2 >\n(215)\n\u221a \u03c0\u03b12  (2x\u2212y)(2x+y)2\u221a \u03c0 ( 2x+y\u221a 2 \u221a x + \u221a( 2x+y\u221a 2 \u221a x )2 +2 ) \u2212 (x\u2212y)(x+y)2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) \u2212\u221a2x3/2 (\u03b12 \u2212 1)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2( \u221a 2 \u221a x)\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2(\u221a2\u221ax)\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212\u221a2x3/2 (\u03b12 \u2212 1) 2 \u221a \u03c0x2 =\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+4x ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+ 8x\u03c0 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n>\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y)2+2(2x+y)+1 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y)2+0.782\u00b72(x+y)+0.7822 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12\n( (2x\u2212y)(2x+y)2\n\u221a \u03c0 ( 2x+y+ \u221a (2x+y+1)2 ) \u2212 (x\u2212y)(x+y)2\u221a \u03c0 ( x+y+ \u221a (x+y+0.782)2 ))\u2212 x (\u03b12 \u2212 1) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2x\u2212y)(2x+y)2\u221a \u03c0(2(2x+y)+1) \u2212 (x\u2212y)(x+y)2\u221a \u03c0(2(x+y)+0.782) ) \u2212 x ( \u03b12 \u2212 1 ) \u221a\n2 \u221a \u03c0x3/2\n=\n\u221a \u03c0\u03b12 ( (2(x+y)+0.782)(2x\u2212y)(2x+y)2\u221a\n\u03c0 \u2212 (x\u2212y)(x+y)(2(2x+y)+1)2\u221a \u03c0 ) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2 +\n\u221a \u03c0\u03b12 ( \u2212x ( \u03b12 \u2212 1 ) (2(2x+ y) + 1)(2(x+ y) + 0.782) ) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2 =\n8x3 + (12y + 2.68657)x2 + (y(4y \u2212 6.41452)\u2212 1.40745)x+ 1.22072y2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2\n>\n8x3 + (2.68657\u2212 120.01)x2 + (0.01(\u22126.41452\u2212 40.01)\u2212 1.40745)x+ 1.22072(0.0)2\n(2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0x3/2\n=\n8x2 + 2.56657x\u2212 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x =\n8x2 + 2.56657x\u2212 1.472 (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x =\n8(x+ 0.618374)(x\u2212 0.297553) (2(2x+ y) + 1)(2(x+ y) + 0.782) \u221a 2 \u221a \u03c0 \u221a x > 0 .\nWe explain this chain of inequalities:\n\u2013 First inequality: We applied Lemma 22 two times. \u2013 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2013 Second inequality part 1: we applied\n0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 . (216)\n\u2013 Second inequality part 2: we show that for a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) following\nholds: 8x\u03c0 \u2212 ( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0\u22122a > 0 and \u2202\u2202y 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = \u22122a > 0. Therefore the minimum is at border for minimal x and maximal y:\n8 \u00b7 0.64 \u03c0 \u2212  2 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ) (0.64 + 0.01) + ( 1 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (217)\nThus 8x\n\u03c0 > a2 + 2a(x+ y) . (218)\nfor a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) > 0.782.\n\u2013 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\n\u2013 We set \u03b1 = \u03b101 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved.\nThe sub-function has its minimal value for minimal x and minimal y x = \u03bd\u03c4 = 0.8 \u00b7 0.8 = 0.64 and y = \u00b5\u03c9 = \u22120.1 \u00b7 0.1 = \u22120.01. We further minimize the function\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) > \u22120.01e 0.01 2 20.64 ( 2\u2212 erfc ( 0.01\u221a 2 \u221a 0.64 )) . (219)\nWe compute the minimum of the term in brackets of \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1):\n\u00b5\u03c9e \u00b52\u03c92 2\u03bd\u03c4 ( 2\u2212 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 )) + (220)\n\u03b1201\n( \u2212 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n))) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4 >\n\u03b1201\n( \u2212 ( e ( 0.64\u22120.01\u221a 2 \u221a 0.64 )2 erfc ( 0.64\u2212 0.01\u221a\n2 \u221a 0.64\n) \u2212 e ( 20.64\u22120.01\u221a 2 \u221a 0.64 )2 erfc ( 2 \u00b7 0.64\u2212 0.01\u221a\n2 \u221a 0.64\n))) \u2212\n0.01e 0.012 20.64 ( 2\u2212 erfc ( 0.01\u221a 2 \u221a 0.64 )) + \u221a 0.64 \u221a 2 \u03c0 = 0.0923765 .\nTherefore the term in brackets is larger than zero.\nThus, \u2202\u2202\u00b5 \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) has the sign of \u03c9.\n\u2022 \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) We look at the sub-term\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) . (221)\nWe obtain a chain of inequalities:\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) > (222)\n2 \u00b7 2 \u221a \u03c0 ( 2x+y\u221a\n2 \u221a x +\n\u221a( 2x+y\u221a\n2 \u221a x\n)2 + 2 ) \u2212 2 \u221a \u03c0 ( x+y\u221a 2 \u221a x + \u221a( x+y\u221a 2 \u221a x )2 + 4\u03c0 ) =\n2 \u221a 2 \u221a x ( 2\u221a\n(2x+y)2+4x+2x+y \u2212 1\u221a\n(x+y)2+ 8x\u03c0 +x+y ) \u221a \u03c0 >\n2 \u221a 2 \u221a x ( 2\u221a\n(2x+y)2+2(2x+y)+1+2x+y \u2212 1\u221a (x+y)2+0.782\u00b72(x+y)+0.7822+x+y ) \u221a \u03c0 =\n2 \u221a 2 \u221a x (\n2 2(2x+y)+1 \u2212 1 2(x+y)+0.782 ) \u221a \u03c0\n=( 2 \u221a 2 \u221a x )\n(2(2(x+ y) + 0.782)\u2212 (2(2x+ y) + 1)) \u221a \u03c0((2(x+ y) + 0.782)(2(2x+ y) + 1))\n=( 2 \u221a 2 \u221a x )\n(2y + 0.782 \u00b7 2\u2212 1) \u221a \u03c0((2(x+ y) + 0.782)(2(2x+ y) + 1)) > 0 .\nWe explain this chain of inequalities:\n\u2013 First inequality: We applied Lemma 22 two times. \u2013 Equalities factor out \u221a 2 \u221a x and reformulate.\n\u2013 Second inequality part 1: we applied 0 < 2y =\u21d2 (2x+ y)2 + 4x+ 1 < (2x+ y)2 + 2(2x+ y) + 1 = (2x+ y + 1)2 .\n(223)\n\u2013 Second inequality part 2: we show that for a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) following\nholds: 8x\u03c0 \u2212 ( a2 + 2a(x+ y) ) > 0. We have \u2202\u2202x 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = 8\u03c0\u22122a > 0 and \u2202\u2202y 8x \u03c0 \u2212 ( a2 + 2a(x+ y) ) = \u22122a < 0. Therefore the minimum is at border for minimal x and maximal y:\n8 \u00b7 0.64 \u03c0 \u2212  2 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ) (0.64 + 0.01) + ( 1 20 (\u221a 2048 + 169\u03c0 \u03c0 \u2212 13 ))2 = 0 . (224)\nThus 8x\n\u03c0 > a2 + 2a(x+ y) . (225)\nfor a = 120 (\u221a 2048+169\u03c0 \u03c0 \u2212 13 ) > 0.782.\n\u2013 Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x+ y) + 0.782).\nWe know that (2\u2212 erfc(x) > 0 according to Lemma 21. For the sub-term we derived\n2e\n( 2x+y\u221a 2 \u221a x )2 erfc ( 2x+ y\u221a\n2 \u221a x\n) \u2212 e ( x+y\u221a 2 \u221a x )2 erfc ( x+ y\u221a\n2 \u221a x\n) > 0 . (226)\nConsequently, both terms in the brackets of \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) are larger than zero. Therefore \u2202\u2202\u03bd \u03be\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) is larger than zero.\nLemma 41 (Mean at low variance). The mapping of the mean \u00b5\u0303 (Eq. (4))\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) = 1\n2 \u03bb\n( \u2212(\u03b1+ \u00b5\u03c9) erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + (227)\n\u03b1e\u00b5\u03c9+ \u03bd\u03c4 2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) + \u221a 2\n\u03c0\n\u221a \u03bd\u03c4e\u2212 \u00b52\u03c92 2\u03bd\u03c4 + 2\u00b5\u03c9 ) in the domain \u22120.1 6 \u00b5 6 \u22120.1, \u22120.1 6 \u03c9 6 \u22120.1, and 0.02 6 \u03bd\u03c4 6 0.5 is bounded by\n|\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| < 0.289324 (228)\nand\nlim \u03bd\u21920 |\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)| = \u03bb\u00b5\u03c9. (229)\nWe can consider \u00b5\u0303 with given \u00b5\u03c9 as a function in x = \u03bd\u03c4 . We show the graph of this function at the maximal \u00b5\u03c9 = 0.01 in the interval x \u2208 [0, 1] in Figure A6.\nProof. Since \u00b5\u0303 is strictly monotonically increasing with \u00b5\u03c9\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) 6 (230) \u00b5\u0303(0.1, 0.1, \u03bd, \u03c4, \u03bb, \u03b1) 6\n1 2 \u03bb\n( \u2212(\u03b1+ 0.01) erfc ( 0.01\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1e0.01+ \u03bd\u03c4 2 erfc ( 0.01 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 ) + \u221a 2 \u03c0 \u221a \u03bd\u03c4e\u2212 0.012 2\u03bd\u03c4 + 2 \u00b7 0.01 ) 6\n1 2 \u03bb01\n( e 0.05 2 +0.01\u03b101 erfc ( 0.02 + 0.01\u221a\n2 \u221a 0.02\n) \u2212 (\u03b101 + 0.01) erfc ( 0.01\u221a 2 \u221a 0.02 ) + e\u2212 0.012 2\u00b70.5 \u221a 0.5 \u221a 2 \u03c0 + 0.01 \u00b7 2 ) < 0.21857,\nwhere we have used the monotonicity of the terms in \u03bd\u03c4 .\n0.5 1.0 1.5 2.0 2.5 3.0\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nx\nFigure A6: The graph of function \u00b5\u0303 for low variances x = \u03bd\u03c4 for \u00b5\u03c9 = 0.01, where x \u2208 [0, 3], is displayed in yellow. Lower and upper bounds based on the Abramowitz bounds (Lemma 22) are displayed in green and blue, respectively.\nSimilarly, we can use the monotonicity of the terms in \u03bd\u03c4 to show that\n\u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb, \u03b1) > \u00b5\u0303(0.1,\u22120.1, \u03bd, \u03c4, \u03bb, \u03b1) > \u22120.289324, (231)\nsuch that |\u00b5\u0303| < 0.289324 at low variances. Furthermore, when (\u03bd\u03c4)\u2192 0, the terms with the arguments of the complementary error functions erfc and the exponential function go to infinity, therefore these three terms converge to zero. Hence, the remaining terms are only 2\u00b5\u03c9 12\u03bb.\nLemma 42 (Bounds on derivatives of \u00b5\u0303 in \u2126\u2212). The derivatives of the function \u00b5\u0303(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101 (Eq. (4)) with respect to \u00b5, \u03c9, \u03bd, \u03c4 in the domain \u2126\u2212 = {\u00b5, \u03c9, \u03bd, \u03c4 | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.8 6 \u03c4 6 1.25} can be bounded as follows:\n\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.14 (232)\u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.14\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.52\u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 0.11.\nProof. The expression\n\u2202\n\u2202\u00b5 \u00b5\u0303 = J11 =\n1 2 \u03bb\u03c9e \u2212(\u00b5\u03c9)2 2\u03bd\u03c4\n( 2e (\u00b5\u03c9)2 2\u03bd\u03c4 \u2212 e (\u00b5\u03c9)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4 ) + \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )) (233)\ncontains the terms e (\u00b5\u03c9)2 2\u03bd\u03c4 erfc (\n\u00b5\u03c9\u221a 2 \u221a \u03bd\u03c4\n) and e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) which are monotonically de-\ncreasing in their arguments (Lemma 23). We can therefore obtain their minima and maximal at the minimal and maximal arguments. Since the first term has a negative sign in the expression, both terms reach their maximal value at \u00b5\u03c9 = \u22120.01, \u03bd = 0.05, and \u03c4 = 0.8.\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303\n\u2223\u2223\u2223\u2223 6 12 |\u03bb\u03c9| \u2223\u2223\u2223(2\u2212 e0.03535532 erfc (0.0353553) + \u03b1e0.1060662 erfc (0.106066))\u2223\u2223\u2223 < 0.133 (234)\nSince, \u00b5\u0303 is symmetric in \u00b5 and \u03c9, these bounds also hold for the derivate to \u03c9.\n0.0 0.2 0.4 0.6 0.8 1.0 x0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nh(x)\nFigure A7: The graph of the function h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) is displayed. It has a local maximum at x = \u03bd\u03c4 \u2248 0.187342 and h(x) \u2248 0.00451457 in the domain x \u2208 [0, 1].\nWe use the argumentation that the term with the error function is monotonically decreasing (Lemma 23) again for the expression\n\u2202\n\u2202\u03bd \u00b5\u0303 = J12 = (235)\n= 1\n4 \u03bb\u03c4e\u2212\n\u00b52\u03c92\n2\u03bd\u03c4 ( \u03b1e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 (\u03b1\u2212 1) \u221a 2\n\u03c0\u03bd\u03c4 ) 6\u2223\u2223\u2223\u222314\u03bb\u03c4\n\u2223\u2223\u2223\u2223 (|1.1072\u2212 2.68593|) < 0.52. We have used that the term 1.1072 6 \u03b101e (\u00b5\u03c9+\u03bd\u03c4)2 2\u03bd\u03c4 erfc ( \u00b5\u03c9+\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) 6 1.49042 and the term\n0.942286 6 (\u03b1 \u2212 1) \u221a\n2 \u03c0\u03bd\u03c4 6 2.68593. Since \u00b5\u0303 is symmetric in \u03bd and \u03c4 , we only have to chance outermost term \u2223\u2223 1 4\u03bb\u03c4 \u2223\u2223 to \u2223\u2223 14\u03bb\u03bd\u2223\u2223 to obtain the estimate \u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303\u2223\u2223 < 0.11.\nLemma 43 (Tight bound on \u00b5\u03032 in \u2126\u2212). The function \u00b5\u03032(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101) (Eq. (4)) is bounded by\u2223\u2223\u00b5\u03032\u2223\u2223 < 0.005 (236) (237)\nin the domain \u2126\u2212 = {\u00b5, \u03c9, \u03bd, \u03c4 | \u2212 0.1 6 \u00b5 6 0.1,\u22120.1 6 \u03c9 6 0.1, 0.05 6 \u03bd 6 0.24, 0.8 6 \u03c4 6 1.25}.\nWe visualize the function \u00b5\u03032 at its maximal \u00b5\u03bd = \u22120.01 and for x = \u03bd\u03c4 in the form h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) in Figure A7.\nProof. We use a similar strategy to the one we have used to show the bound on the singular value (Lemmata 10, 11, and 12), where we evaluted the function on a grid and used bounds on the derivatives together with the mean value theorem. Here we have\u2223\u2223\u00b5\u03032(\u00b5, \u03c9, \u03bd, \u03c4, \u03bb01, \u03b101)\u2212 \u00b5\u03032(\u00b5+ \u2206\u00b5, \u03c9 + \u2206\u03c9, \u03bd + \u2206\u03bd, \u03c4 + \u2206\u03c4, \u03bb01, \u03b101)\u2223\u2223 6 (238)\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u00b5|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03c9|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03bd|+ \u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u03032 \u2223\u2223\u2223\u2223 |\u2206\u03c4 |.\nWe use Lemma 42 and Lemma 41, to obtain\u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u00b5\u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.14 = 0.08101072 (239)\u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03c9 \u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.14 = 0.08101072\n\u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03bd \u00b5\u0303 \u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.52 = 0.30089696\u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u03032 \u2223\u2223\u2223\u2223 = 2 |\u00b5\u0303| \u2223\u2223\u2223\u2223 \u2202\u2202\u03c4 \u00b5\u0303\n\u2223\u2223\u2223\u2223 < 2 \u00b7 0.289324 \u00b7 0.11 = 0.06365128 We evaluated the function \u00b5\u03032 in a grid G of \u2126\u2212 with \u2206\u00b5 = 0.001498041, \u2206\u03c9 = 0.001498041, \u2206\u03bd = 0.0004033190, and \u2206\u03c4 = 0.0019065994 using a computer and obtained the maximal value maxG(\u00b5\u0303) 2 = 0.00451457, therefore the maximal value of \u00b5\u03032 is bounded by\nmax (\u00b5,\u03c9,\u03bd,\u03c4)\u2208\u2126\u2212\n(\u00b5\u0303)2 6 (240)\n0.00451457 + 0.001498041 \u00b7 0.08101072 + 0.001498041 \u00b7 0.08101072+ 0.0004033190 \u00b7 0.30089696 + 0.0019065994 \u00b7 0.06365128 < 0.005. (241)\nFurthermore we used error propagation to estimate the numerical error on the function evaluation. Using the error propagation rules derived in Subsection A3.4.5, we found that the numerical error is smaller than 10\u221213 in the worst case.\nLemma 44 (Main subfunction). For 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (242)\nis smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.\nProof. We first consider the derivative of sub-function Eq. (101) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (243)\nwith respect to x is \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n(244) \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x (2x+ y)(2x\u2212 y) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0  e (x+y)22x (x\u2212y)(x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 2e (2x+y)2 2x (2x+y)(2x\u2212y) erfc ( 2x+y\u221a 2 \u221a x ) \u221a 2 \u221a x + (3x\u2212 y) 2 \u221a 2 \u221a \u03c0x2 \u221a x .\nWe consider the numerator\n\u221a \u03c0 e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x+ y)(2x\u2212 y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n+ (3x\u2212 y) . (245)\nFor bounding this value, we use the approximation\nez 2 erfc(z) \u2248 2.911\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (246)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is positive in the range [0.7, 3.2]. This range contains all possible arguments of erfc that we consider. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x  \u2212 (247) 2.911(x\u2212 y)(x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 ) \u2212\n2 \u00b7 2.911(2x\u2212 y)(2x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(2x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112\n)  .\nWe numerically determined 0.0113556 6 E(x, y) 6 0.0169551 for 1.2 6 x 6 20 and \u22120.1 6 y 6 0.1. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate. We subtract an additional safety gap of 0.0131259 from our approximation to ensure that the inequality via the approximation holds true. With this safety gap the inequality would hold true even for negative x, where the approximation error becomes negative and the safety gap would compensate. Of course, the safety gap of 0.0131259 is not necessary for our analysis but may help or future investigations.\nWe have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x\u2212 y) + e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u221a\u03c0 > (248)\n(3x\u2212 y) +  2.911(x\u2212 y)(x+ y)(\u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 + (2.911\u22121) \u221a \u03c0(x+y)\u221a 2 \u221a x )(\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y)2.911(\u221a 2 \u221a x )(\u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112 + (2.911\u22121) \u221a \u03c0(2x+y)\u221a\n2 \u221a x\n) \u221a\u03c0 \u2212 0.0131259 =\n(3x\u2212 y) +  (\u221a2\u221ax2.911) (x\u2212 y)(x+ y)(\u221a \u03c0(x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(x+ y) \u221a \u03c0 ) (\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y) (\u221a 2 \u221a x2.911 )(\u221a 2 \u221a x ) (\u221a \u03c0(2x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(2x+ y) \u221a \u03c0 ) \u221a\u03c0 \u2212 0.0131259 =\n(3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a (x+ y)2 + 2\u00b72.911\n2x \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 > (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( 2.9112\n\u03c0\n)2 + (x+ y)2 + 2\u00b72.911\n2x \u03c0 + 2\u00b72.9112y \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( x+ y + 2.911 2\n\u03c0 )2 \u2212 2(2x\u2212 y)(2x+ y)\n(2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) 2.911(x+ y) + 2.911 2\n\u03c0\n\u2212 2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0131259 = (3x\u2212 y) + (x\u2212 y)(x+ y)\n(x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0131259 = (3x\u2212 y) + (x\u2212 y)(x+ y) (x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0131259 =\n( \u22122(2x\u2212 y)2.911 ( (x+ y) + 2.911\n\u03c0 ) (2x+ y) +(\n(x+ y) + 2.911\n\u03c0\n) (3x\u2212 y \u2212 0.0131259) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n) +\n(x\u2212 y)(x+ y) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 )) ((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 ))\u22121 =(\n((x\u2212 y)(x+ y) + (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) (\u221a (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) \u2212\n(249) 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y))((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 22.9112x\n\u03c0\n))\u22121 > 0 .\nWe explain this sequence of inequalities:\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then subtracting a safety gap (which would not be necessary for the current analysis). \u2022 Equalities: The factor \u221a 2 \u221a x is factored out and canceled.\n\u2022 Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n\u2022 Equalities: solve for the term and factor out. \u2022 Bringing all terms to the denominator ( (x+ y) + 2.911\u03c0 )( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x \u03c0 ) .\n\u2022 Equalities: Multiplying out and expanding terms.\n\u2022 Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (248), which we show to be positive in order to show > 0 in Eq. (248). The numerator is\n((x\u2212 y)(x+ y) + (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) (\u221a (2x+ y)2 + 5.39467x+ 3.822x+ 1.911y ) \u2212\n(250) 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y) = \u2212 5.822(2x\u2212 y)(x+ y + 0.9266)(2x+ y) + (3.822x+ 1.911y)((x\u2212 y)(x+ y)+ (3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) + ((x\u2212 y)(x+ y)+\n(3x\u2212 y \u2212 0.0131259)(x+ y + 0.9266)) \u221a (2x+ y)2 + 5.39467x =\n\u2212 8.0x3 + ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x\u2212\n8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y = \u2212 8.0x3 + ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x\u2212\n8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y .\nThe factor in front of the root is positive. If the term, that does not contain the root, was positive, then the whole expression would be positive and we would have proofed that the numerator is positive. Therefore we consider the case that the term, that does not contain the root, is negative. The term that contains the root must be larger than the other term in absolute values.\n\u2212 ( \u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.xy2 + 1.69548xy \u2212 0.0464849x+ 2.y3 + 3.59885y2 \u2212 0.0232425y ) < (251)( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )\u221a (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (248). Thus, we have the inequality:( \u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.xy2 + 1.69548xy \u2212 0.0464849x+ 2.y3 + 3.59885y2 \u2212 0.0232425y )2 < (252)( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.76667x\u2212 2y2 \u2212 0.939726y \u2212 0.0121625 )2 ( (2x+ y)2 + 5.39467x ) \u2212 (253)(\n\u22128.0x3 \u2212 8.0x2y \u2212 11.0044x2 + 2.0xy2 + 1.69548xy \u2212 0.0464849x+ 2.0y3 + 3.59885y2 \u2212 0.0232425y )2 =\n\u2212 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3\u2212 13.543x2y2 \u2212 28.8455x2y \u2212 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x\u2212 10.6365y5 \u2212 11.927y4 + 0.190151y3 \u2212 0.000392287y2 .\nWe obtain the inequalities:\n\u2212 1.2227x5 + 40.1006x4y + 27.7897x4 + 41.0176x3y2 + 64.5799x3y + 39.4762x3 + 10.9422x2y3\u2212 (254)\n13.543x2y2 \u2212 28.8455x2y \u2212 0.364625x2 + 0.611352xy4 + 6.83183xy3 + 5.46393xy2+ 0.121746xy + 0.000798008x\u2212 10.6365y5 \u2212 11.927y4 + 0.190151y3 \u2212 0.000392287y2 =\n\u2212 1.2227x5 + 27.7897x4 + 41.0176x3y2 + 39.4762x3 \u2212 13.543x2y2 \u2212 0.364625x2+ y ( 40.1006x4 + 64.5799x3 + 10.9422x2y2 \u2212 28.8455x2 + 6.83183xy2 + 0.121746x \u2212\n10.6365y4 + 0.190151y2 )\n+ 0.611352xy4 + 5.46393xy2 + 0.000798008x\u2212 11.927y4 \u2212 0.000392287y2 > \u2212 1.2227x5 + 27.7897x4 + 41.0176 \u00b7 (0.0)2x3 + 39.4762x3 \u2212 13.543 \u00b7 (0.1)2x2 \u2212 0.364625x2\u2212 0.1 \u00b7 ( 40.1006x4 + 64.5799x3 + 10.9422 \u00b7 (0.1)2x2 \u2212 28.8455x2 + 6.83183 \u00b7 (0.1)2x+ 0.121746x +\n10.6365 \u00b7 (0.1)4 + 0.190151 \u00b7 (0.1)2 ) +\n0.611352 \u00b7 (0.0)4x+ 5.46393 \u00b7 (0.0)2x+ 0.000798008x\u2212 11.927 \u00b7 (0.1)4 \u2212 0.000392287 \u00b7 (0.1)2 = \u2212 1.2227x5 + 23.7796x4 + (20 + 13.0182)x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > \u2212 1.2227x5 + 24.7796x4 + 13.0182x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > 13.0182x3 + 2.37355x2 \u2212 0.0182084x\u2212 0.000194074 > 0 .\nWe used 24.7796 \u00b7 (20)4 \u2212 1.2227 \u00b7 (20)5 = 52090.9 > 0 and x 6 20. We have proofed the last inequality > 0 of Eq. (248).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (255)\nis strictly monotonically increasing in x.\nThe main subfunction is smaller than zero. Next we show that the sub-function Eq. (101) is smaller than zero. We consider the limit:\nlim x\u2192\u221e\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) = 0 (256)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (257)\nis smaller than zero.\nBehavior of the main subfunction with respect to y at minimal x. We now consider the derivative of sub-function Eq. (101) with respect to y. We proofed that sub-function Eq. (101) is strictly monotonically increasing independent of y. In the proof of Theorem 16, we need the minimum of sub-function Eq. (101). Therefore we are only interested in the derivative of sub-function Eq. (101) with respect to y for the minimum x = 12/10 = 1.2\nConsequently, we insert the minimum x = 12/10 = 1.2 into the sub-function Eq. (101). The main terms become\nx+ y\u221a 2 \u221a x = y + 1.2\u221a 2 \u221a 1.2 = y\u221a 2 \u221a 1.2 + \u221a 1.2\u221a 2 = 5y + 6 2 \u221a 15 (258)\nand 2x+ y\u221a\n2 \u221a x\n= y + 1.2 \u00b7 2\u221a\n2 \u221a 1.2 = y\u221a 2 \u221a 1.2 + \u221a 1.2 \u221a 2 = 5y + 12 2 \u221a 15 . (259)\nSub-function Eq. (101) becomes:\ne\n( y\n\u221a 2 \u221a\n12 10\n+ \u221a 12 10\u221a 2 )2 erfc  y\u221a 2 \u221a\n12 10\n+ \u221a 12 10\u221a 2 \u2212 2e ( y \u221a 2 \u221a 12 10 + \u221a 2 \u221a 12 10 )2 erfc  y\u221a 2 \u221a\n12 10\n+ \u221a 2\n\u221a 12\n10  . (260)\nThe derivative of this function with respect to y is \u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y+6\n2 \u221a 15\n) \u2212 2e 160 (5y+12)2(5y + 12) erfc ( 5y+12\n2 \u221a 15\n)) + 30\n6 \u221a 15\u03c0 . (261)\nWe again will use the approximation of Ren and MacKenzie [30]\nez 2 erfc(z) = 2.911\n\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (262)\nTherefore we first perform an error analysis. We estimated the maximum and minimum of\n\u221a 15\u03c0  2 \u00b7 2.911(5y + 12)\u221a \u03c0(2.911\u22121)(5y+12)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+12\n2 \u221a 15\n)2 + 2.9112 \u2212 2.911(5y + 6) \u221a \u03c0(2.911\u22121)(5y+6)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+6\n2 \u221a 15\n)2 + 2.9112 + 30 + (263)\n\u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 \u221a 15\n) \u2212 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 \u221a 15\n)) + 30 .\nWe obtained for the maximal absolute error the value 0.163052. We added an approximation error of 0.2 to the approximation of the derivative. Since we want to show that the approximation upper bounds the true expression, the addition of the approximation error is required here. We get a sequence of inequalities:\n\u221a 15\u03c0 ( e 1 60 (5y+6) 2 (5y + 6) erfc ( 5y + 6\n2 \u221a 15\n) \u2212 2e 160 (5y+12) 2 (5y + 12) erfc ( 5y + 12\n2 \u221a 15\n)) + 30 6\n(264)\n\u221a 15\u03c0  2.911(5y + 6)\u221a \u03c0(2.911\u22121)(5y+6)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+6\n2 \u221a 15\n)2 + 2.9112 \u2212 2 \u00b7 2.911(5y + 12) \u221a \u03c0(2.911\u22121)(5y+12)\n2 \u221a 15 +\n\u221a \u03c0 ( 5y+12\n2 \u221a 15\n)2 + 2.9112 + 30 + 0.2 =\n(30 \u00b7 2.911)(5y + 6) (2.911\u2212 1)(5y + 6) + \u221a (5y + 6)2 + ( 2 \u221a\n15\u00b72.911\u221a \u03c0\n)2 \u2212 2(30 \u00b7 2.911)(5y + 12) (2.911\u2212 1)(5y + 12) + \u221a (5y + 12)2 + ( 2 \u221a\n15\u00b72.911\u221a \u03c0 )2 + 30 + 0.2 =(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2\u2212 2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0\n)2 \n (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2 (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2  \u22121 < 0 .\nWe explain this sequence of inequalities.\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then adding the error bound to ensure that the approximation is larger than the true value. \u2022 First equality: The factor 2 \u221a 15 and 2 \u221a \u03c0 are factored out and canceled.\n\u2022 Second equality: Bringing all terms to the denominator(2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a152.911\u221a\n\u03c0 )2 (265) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2 . \u2022 Last inequality < 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last term in Eq. (264). We have to proof that this numerator is smaller than zero in order to proof the last inequality of Eq. (264). The numerator is\n(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (266) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0\n)2 \u2212 2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 .2.911\u221a\n\u03c0 )2 . We now compute upper bounds for this numerator:\n(0.2 + 30) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 (267) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a \u03c0 )2\u2212\n2 \u00b7 30 \u00b7 2.911(5y + 12) (2.911\u2212 1)(5y + 6) + \u221a\u221a\u221a\u221a(5y + 6)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2+ 2.911 \u00b7 30(5y + 6) (2.911\u2212 1)(5y + 12) + \u221a\u221a\u221a\u221a(5y + 12)2 +(2\u221a15 \u00b7 2.911\u221a\n\u03c0 )2 = \u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y\u2212\n5093.97y \u2212 1403.37 \u221a (5y + 6)2 + 161.84 + 30.2 \u221a (5y + 6)2 + 161.84 \u221a (5y + 12)2 + 161.84+\n870.253 \u221a (5y + 12)2 + 161.84\u2212 4075.17 <\n\u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y\u2212\n5093.97y \u2212 1403.37 \u221a (6 + 5 \u00b7 (\u22120.1))2 + 161.84 + 30.2 \u221a (6 + 5 \u00b7 0.1)2 + 161.84 \u221a (12 + 5 \u00b7 0.1)2 + 161.84+\n870.253 \u221a (12 + 5 \u00b7 0.1)2 + 161.84\u2212 4075.17 =\n\u2212 1414.99y2 \u2212 584.739 \u221a (5y + 6)2 + 161.84y + 725.211 \u221a (5y + 12)2 + 161.84y \u2212 5093.97y \u2212 309.691 <\ny ( \u2212584.739 \u221a (5y + 6)2 + 161.84 + 725.211 \u221a (5y + 12)2 + 161.84\u2212 5093.97 ) \u2212 309.691 <\n\u2212 0.1 ( 725.211 \u221a (12 + 5 \u00b7 (\u22120.1))2 + 161.84\u2212 584.739 \u221a (6 + 5 \u00b7 0.1)2 + 161.84\u2212 5093.97 ) \u2212 309.691 =\n\u2212 208.604 .\nFor the first inequality we choose y in the roots, so that positive terms maximally increase and negative terms maximally decrease. The second inequality just removed the y2 term which is always negative, therefore increased the expression. For the last inequality, the term in brackets is negative for all settings of y. Therefore we make the brackets as negative as possible and make the whole term positive by multiplying with y = \u22120.1. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (268)\nis strictly monotonically decreasing in y for the minimal x = 1.2.\nLemma 45 (Main subfunction below). For 0.007 6 x 6 0.875 and \u22120.01 6 y 6 0.01, the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (269)\nsmaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 \u00b7 0.8, x = 0.56 = 0.7 \u00b7 0.8, x = 0.128 = 0.16 \u00b7 0.8, and x = 0.216 = 0.24 \u00b7 0.9 (lower bound of 0.9 on \u03c4 ).\nProof. We first consider the derivative of sub-function Eq. (111) with respect to x. The derivative of the function\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (270)\nwith respect to x is \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x ( 4x2 \u2212 y2 ) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n(271) \u221a \u03c0 ( e (x+y)2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u2212 2e (2x+y)2 2x (2x+ y)(2x\u2212 y) erfc ( 2x+y\u221a 2 \u221a x )) + \u221a 2 \u221a x(3x\u2212 y)\n2 \u221a \u03c0x2\n=\n\u221a \u03c0  e (x+y)22x (x\u2212y)(x+y) erfc( x+y\u221a2\u221ax)\u221a 2 \u221a x \u2212 2e (2x+y)2 2x (2x+y)(2x\u2212y) erfc ( 2x+y\u221a 2 \u221a x ) \u221a 2 \u221a x + (3x\u2212 y) \u221a\n22 \u221a \u03c0 \u221a xx2\n.\nWe consider the numerator\n\u221a \u03c0 e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x+ y)(2x\u2212 y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n+ (3x\u2212 y) . (272)\nFor bounding this value, we use the approximation\nez 2 erfc(z) \u2248 2.911\u221a \u03c0(2.911\u2212 1)z + \u221a \u03c0z2 + 2.9112 . (273)\nfrom Ren and MacKenzie [30]. We start with an error analysis of this approximation. According to Ren and MacKenzie [30] (Figure 1), the approximation error is both positive and negative in the range [0.175, 1.33]. This range contains all possible arguments of erfc that we consider in this subsection. Numerically we maximized and minimized the approximation error of the whole expression\nE(x, y) = e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x  \u2212 (274) 2.911(x\u2212 y)(x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 ) \u2212\n2 \u00b7 2.911(2x\u2212 y)(2x+ y)(\u221a 2 \u221a x )(\u221a\u03c0(2.911\u22121)(2x+y)\u221a\n2 \u221a x\n+ \u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112\n)  .\nWe numerically determined \u22120.000228141 6 E(x, y) 6 0.00495688 for 0.08 6 x 6 0.875 and \u22120.01 6 y 6 0.01. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate.\nWe use an error gap of \u22120.0003 to countermand the error due to the approximation. We have the sequences of inequalities using the approximation of Ren and MacKenzie [30]:\n(3x\u2212 y) + e (x+y) 2 2x (x\u2212 y)(x+ y) erfc ( x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u2212 2e\n(2x+y)2 2x (2x\u2212 y)(2x+ y) erfc (\n2x+y\u221a 2 \u221a x ) \u221a\n2 \u221a x\n\u221a\u03c0 > (275)\n(3x\u2212 y) +  2.911(x\u2212 y)(x+ y)(\u221a \u03c0 ( x+y\u221a 2 \u221a x )2 + 2.9112 + (2.911\u22121) \u221a \u03c0(x+y)\u221a 2 \u221a x )(\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y)2.911(\u221a 2 \u221a x )(\u221a \u03c0 (\n2x+y\u221a 2 \u221a x\n)2 + 2.9112 + (2.911\u22121) \u221a \u03c0(2x+y)\u221a\n2 \u221a x\n) \u221a\u03c0 \u2212 0.0003 =\n(3x\u2212 y) +  (\u221a2\u221ax2.911) (x\u2212 y)(x+ y)(\u221a \u03c0(x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(x+ y) \u221a \u03c0 ) (\u221a 2 \u221a x ) \u2212\n2(2x\u2212 y)(2x+ y) (\u221a 2 \u221a x2.911 )(\u221a 2 \u221a x ) (\u221a \u03c0(2x+ y)2 + 2 \u00b7 2.9112x+ (2.911\u2212 1)(2x+ y) \u221a \u03c0 ) \u221a\u03c0 \u2212 0.0003 =\n(3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a (x+ y)2 + 2\u00b72.911\n2x \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 > (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( 2.9112\n\u03c0\n)2 + (x+ y)2 + 2\u00b72.911\n2x \u03c0 + 2\u00b72.9112y \u03c0\n\u2212\n2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + 2.911\n (x\u2212 y)(x+ y) (2.911\u2212 1)(x+ y) + \u221a( x+ y + 2.911 2\n\u03c0 )2 \u2212 2(2x\u2212 y)(2x+ y)\n(2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + 2.911  (x\u2212 y)(x+ y) 2.911(x+ y) + 2.911 2\n\u03c0\n\u2212 2(2x\u2212 y)(2x+ y) (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0 \u2212 0.0003 = (3x\u2212 y) + (x\u2212 y)(x+ y)\n(x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0003 =\n(3x\u2212 y) + (x\u2212 y)(x+ y) (x+ y) + 2.911\u03c0 \u2212 2(2x\u2212 y)(2x+ y)2.911 (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x\n\u03c0\n\u2212 0.0003 =\n( \u22122(2x\u2212 y)2.911 ( (x+ y) + 2.911\n\u03c0 ) (2x+ y) +(\n(x+ y) + 2.911\n\u03c0\n) (3x\u2212 y \u2212 0.0003) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n) +\n(x\u2212 y)(x+ y) ( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 )) ((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n))\u22121 =\n( \u22128x3 \u2212 8x2y + 4x2 \u221a (2x+ y)2 + 5.39467x\u2212 10.9554x2 + 2xy2 \u2212 2y2 \u221a (2x+ y)2 + 5.39467x +\n1.76901xy + 2xy \u221a (2x+ y)2 + 5.39467x+ 2.7795x \u221a (2x+ y)2 + 5.39467x \u2212\n0.9269y \u221a (2x+ y)2 + 5.39467x\u2212 0.00027798 \u221a\n(2x+ y)2 + 5.39467x\u2212 0.00106244x + 2y3 + 3.62336y2 \u2212 0.00053122y )(( (x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0 ))\u22121 =(\n\u22128x3 + ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x \u2212\n8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )((\n(x+ y) + 2.911\n\u03c0\n)( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 +\n2 \u00b7 2.9112x \u03c0\n))\u22121 > 0 .\nWe explain this sequence of inequalities:\n\u2022 First inequality: The approximation of Ren and MacKenzie [30] and then subtracting an error gap of 0.0003. \u2022 Equalities: The factor \u221a 2 \u221a x is factored out and canceled.\n\u2022 Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.\n\u2022 Equalities: solve for the term and factor out. \u2022 Bringing all terms to the denominator ( (x+ y) + 2.911\u03c0 )( (2.911\u2212 1)(2x+ y) + \u221a (2x+ y)2 + 2\u00b72.911 2x \u03c0 ) .\n\u2022 Equalities: Multiplying out and expanding terms.\n\u2022 Last inequality > 0 is proofed in the following sequence of inequalities.\nWe look at the numerator of the last expression of Eq. (275), which we show to be positive in order to show > 0 in Eq. (275). The numerator is\n\u2212 8x3 + ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x \u2212\n(276)\n8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y . The factor 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 in front of the root is positive:\n4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 > (277) \u22122y2 + 0.007 \u00b7 2y \u2212 0.9269y + 4 \u00b7 0.0072 + 2.7795 \u00b7 0.007\u2212 0.00027798 =\n\u22122y2 \u2212 0.9129y + 2.77942 = \u22122(y + 1.42897)(y \u2212 0.972523) > 0 . If the term that does not contain the root would be positive, then everything is positive and we have proofed the the numerator is positive. Therefore we consider the case that the term that does not contain the root is negative. The term that contains the root must be larger than the other term in absolute values. \u2212 ( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y ) < (278)( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )\u221a (2x+ y)2 + 5.39467x .\nTherefore the squares of the root term have to be larger than the square of the other term to show > 0 in Eq. (275). Thus, we have the inequality:( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )2 <\n(279)\n( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) .\nThis is equivalent to 0 < ( 4x2 + 2xy + 2.7795x\u2212 2y2 \u2212 0.9269y \u2212 0.00027798 )2 ( (2x+ y)2 + 5.39467x ) \u2212 (280)( \u22128x3 \u2212 8x2y \u2212 10.9554x2 + 2xy2 + 1.76901xy \u2212 0.00106244x+ 2y3 + 3.62336y2 \u2212 0.00053122y )2 =\nx \u00b7 4.168614250 \u00b7 10\u22127 \u2212 y22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3 \u2212 13.1726x2y2\u2212 27.8148x2y \u2212 0.00833715x2 + 0.0139728xy4 + 5.47537xy3+ 4.65089xy2 + 0.00277916xy \u2212 10.7858y5 \u2212 12.2664y4 + 0.00436492y3 .\nWe obtain the inequalities:\nx \u00b7 4.168614250 \u00b7 10\u22127 \u2212 y22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ (281) 43.0875x4y + 30.8113x4 + 43.1084x3y2 + 68.989x3y + 41.6357x3 + 10.7928x2y3\u2212 13.1726x2y2 \u2212 27.8148x2y \u2212 0.00833715x2+ 0.0139728xy4 + 5.47537xy3 + 4.65089xy2 + 0.00277916xy \u2212 10.7858y5 \u2212 12.2664y4 + 0.00436492y3 > x \u00b7 4.168614250 \u00b7 10\u22127 \u2212 (0.01)22.049216091 \u00b7 10\u22127 \u2212 0.0279456x5+ 0.0 \u00b7 43.0875x4 + 30.8113x4 + 43.1084(0.0)2x3 + 0.0 \u00b7 68.989x3 + 41.6357x3+ 10.7928(0.0)3x2 \u2212 13.1726(0.01)2x2 \u2212 27.8148(0.01)x2 \u2212 0.00833715x2+ 0.0139728(0.0)4x+ 5.47537(0.0)3x+ 4.65089(0.0)2x+\n0.0 \u00b7 0.00277916x\u2212 10.7858(0.01)5 \u2212 12.2664(0.01)4 + 0.00436492(0.0)3 = x \u00b7 4.168614250 \u00b7 10\u22127 \u2212 1.237626189 \u00b7 10\u22127 \u2212 0.0279456x5 + 30.8113x4 + 41.6357x3 \u2212 0.287802x2 >\n\u2212 ( x\n0.007\n)3 1.237626189 \u00b7 10\u22127 + 30.8113x4 \u2212 (0.875) \u00b7 0.0279456x4 + 41.6357x3 \u2212 (0.287802x)x 2\n0.007 =\n30.7869x4 + 0.160295x3 > 0 .\nWe used x > 0.007 and x 6 0.875 (reducing the negative x4-term to a x3-term). We have proofed the last inequality > 0 of Eq. (275).\nConsequently the derivative is always positive independent of y, thus\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (282)\nis strictly monotonically increasing in x.\nNext we show that the sub-function Eq. (111) is smaller than zero. We consider the limit:\nlim x\u2192\u221e\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) = 0 (283)\nThe limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (284)\nis smaller than zero.\nWe now consider the derivative of sub-function Eq. (111) with respect to y. We proofed that subfunction Eq. (111) is strictly monotonically increasing independent of y. In the proof of Theorem 3, we need the minimum of sub-function Eq. (111). First, we are interested in the derivative of subfunction Eq. (111) with respect to y for the minimum x = 0.007 = 7/1000.\nConsequently, we insert the minimum x = 0.007 = 7/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n7 1000\n+\n\u221a 7\n1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n7 1000\n+\n\u221a 7\n1000\u221a 2 \u2212 (285) 2e ( y \u221a 2 \u221a 7 1000 + \u221a 2 \u221a 7 1000 )2 erfc\n y\u221a 2 \u221a\n7 1000\n+ \u221a 2\n\u221a 7\n1000  = e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 \u221a 35\n) \u2212 2e (500y+7)2 3500 erfc ( 500y + 7\n10 \u221a 35\n) .\nThe derivative of this function with respect to y is( 1000y\n7 + 1\n) e 500y2 7 +y+ 7 2000 erfc ( 1000y + 7\n20 \u221a 35\n) \u2212 (286)\n1 7 4e (500y+7)2 3500 (500y + 7) erfc\n( 500y + 7\n10 \u221a 35\n) + 20 \u221a 5\n7\u03c0 >(\n1 + 1000 \u00b7 (\u22120.01)\n7\n) e\u22120.01+ 7 2000 + 500\u00b7(\u22120.01)2 7 erfc ( 7 + 1000 + (\u22120.01)\n20 \u221a 35\n) \u2212\n1 7 4e (7+500\u00b70.01)2 3500 (7 + 500 \u00b7 0.01) erfc\n( 7 + 500 \u00b7 0.01\n10 \u221a 35\n) + 20 \u221a 5\n7\u03c0 > 3.56 .\nFor the first inequality, we use Lemma 24. Lemma 24 says that the function xex 2\nerfc(x) has the sign of x and is monotonically increasing to 1\u221a\n\u03c0 . Consequently, we inserted the maximal y = 0.01 to\nmake the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive.\nConsequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (287)\nis strictly monotonically increasing in y for the minimal x = 0.007.\nNext, we consider x = 0.7 \u00b7 0.8 = 0.56, which is the maximal \u03bd = 0.7 and minimal \u03c4 = 0.8. We insert the minimum x = 0.56 = 56/100 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n56 100\n+ \u221a 56 100\u221a 2 )2 erfc  y\u221a 2 \u221a\n56 100\n+ \u221a 56 100\u221a 2 \u2212 (288) 2e ( y \u221a 2 \u221a 56 100 + \u221a 2 \u221a 56 100 )2 erfc\n y\u221a 2 \u221a\n56 100\n+ \u221a 2\n\u221a 56\n100  . The derivative with respect to y is:\n5e\n( 5y\n2 \u221a 7 + \u221a 7 5 )2 ( 5y\n2 \u221a 7 + \u221a 7 5\n) erfc ( 5y\n2 \u221a 7 + \u221a 7 5 ) \u221a\n7 \u2212 (289)\n10e\n( 5y\n2 \u221a 7 + 2 \u221a 7 5 )2 ( 5y\n2 \u221a 7 + 2 \u221a 7 5\n) erfc ( 5y\n2 \u221a 7 + 2 \u221a 7 5 ) \u221a\n7 + 5\u221a 7\u03c0 >\n5e\n(\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7 )2 (\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7\n) erfc (\u221a 7\n5 \u2212 0.01\u00b75 2 \u221a 7 ) \u221a\n7 \u2212\n10e\n( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7 )2 ( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7\n) erfc ( 2 \u221a\n7 5 + 0.01\u00b75 2 \u221a 7 ) \u221a\n7 + 5\u221a 7\u03c0 > 0.00746 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (290)\nis strictly monotonically increasing in y for x = 0.56.\nNext, we consider x = 0.16 \u00b7 0.8 = 0.128, which is the minimal \u03c4 = 0.8. We insert the minimum x = 0.128 = 128/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n128 1000\n+ \u221a 128 1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n128 1000\n+ \u221a 128 1000\u221a 2 \u2212 (291) 2e ( y \u221a 2 \u221a 128 1000 + \u221a 2 \u221a 128 1000 )2 erfc\n y\u221a 2 \u221a\n128 1000\n+ \u221a 2\n\u221a 128\n1000  = e 125y2 32 +y+ 8 125 erfc ( 125y + 16\n20 \u221a 10\n) \u2212 2e (125y+32)2 4000 erfc ( 125y + 32\n20 \u221a 10\n) .\nThe derivative with respect to y is:\n1\n16\n( e 125y2 32 +y+ 8 125 (125y + 16) erfc ( 125y + 16\n20 \u221a 10\n) \u2212 (292)\n2e (125y+32)2 4000 (125y + 32) erfc\n( 125y + 32\n20 \u221a 10\n) + 20 \u221a 10\n\u03c0\n) >\n1\n16\n( (16 + 125(\u22120.01))e\u22120.01+ 8125 + 125(\u22120.01)2 32 erfc ( 16 + 125(\u22120.01)\n20 \u221a 10\n) \u2212\n2e (32+1250.01)2 4000 (32 + 1250.01) erfc\n( 32 + 1250.01\n20 \u221a 10\n) + 20 \u221a 10\n\u03c0\n) > 0.4468 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (293)\nis strictly monotonically increasing in y for x = 0.128.\nNext, we consider x = 0.24 \u00b7 0.9 = 0.216, which is the minimal \u03c4 = 0.9 (here we consider 0.9 as lower bound for \u03c4 ). We insert the minimum x = 0.216 = 216/1000 into the sub-function Eq. (111):\ne\n( y\n\u221a 2 \u221a\n216 1000\n+ \u221a 216 1000\u221a 2 )2 erfc  y\u221a 2 \u221a\n216 1000\n+ \u221a 216 1000\u221a 2 \u2212 (294) 2e ( y \u221a 2 \u221a 216 1000 + \u221a 2 \u221a 216 1000 )2 erfc\n y\u221a 2 \u221a\n216 1000\n+ \u221a 2\n\u221a 216\n1000\n =\ne (125y+27)2 6750 erfc\n( 125y + 27\n15 \u221a 30\n) \u2212 2e (125y+54)2 6750 erfc ( 125y + 54\n15 \u221a 30 ) The derivative with respect to y is:\n1\n27\n( e (125y+27)2 6750 (125y + 27) erfc ( 125y + 27\n15 \u221a 30\n) \u2212 (295)\n2e (125y+54)2 6750 (125y + 54) erfc\n( 125y + 54\n15 \u221a 30\n) + 15 \u221a 30\n\u03c0\n) >\n1\n27\n( (27 + 125(\u22120.01))e (27+125(\u22120.01))2 6750 erfc ( 27 + 125(\u22120.01)\n15 \u221a 30\n) \u2212\n2e (54+1250.01)2 6750 (54 + 1250.01) erfc\n( 54 + 1250.01\n15 \u221a 30\n) + 15 \u221a 30\n\u03c0\n) ) > 0.211288 .\nFor the first inequality we applied Lemma 24 which states that the function xex 2\nerfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = \u22120.01 to make the positive term less positive. Consequently\ne (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) \u2212 2e (2x+y)2 2x erfc ( 2x+ y\u221a\n2 \u221a x\n) (296)\nis strictly monotonically increasing in y for x = 0.216.\nLemma 46 (Monotone Derivative). For \u03bb = \u03bb01, \u03b1 = \u03b101 and the domain \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25. We are interested of the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u00b7\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2 \u00b7 \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) . (297)\nThe derivative of the equation above with respect to\n\u2022 \u03bd is larger than zero;\n\u2022 \u03c4 is smaller than zero for maximal \u03bd = 0.7, \u03bd = 0.16, and \u03bd = 0.24 (with 0.9 6 \u03c4 );\n\u2022 y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.00875 \u00b7 0.8 = 0.007, \u03bd\u03c4 = 0.7 \u00b7 0.8 = 0.56, \u03bd\u03c4 = 0.16 \u00b7 0.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216.\nProof. We consider the domain: \u22120.1 6 \u00b5 6 0.1, \u22120.1 6 \u03c9 6 0.1, 0.00875 6 \u03bd 6 0.7, and 0.8 6 \u03c4 6 1.25.\nWe use Lemma 17 to determine the derivatives. Consequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (298)\nwith respect to \u03bd is larger than zero, which follows directly from Lemma 17 using the chain rule.\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (299)\nwith respect to y = \u00b5\u03c9 is larger than zero for \u03bd\u03c4 = 0.00875 \u00b7 0.8 = 0.007, \u03bd\u03c4 = 0.7 \u00b7 0.8 = 0.56, \u03bd\u03c4 = 0.16 \u00b7 0.8 = 0.128, and \u03bd\u03c4 = 0.24 \u00b7 0.9 = 0.216, which also follows directly from Lemma 17. We now consider the derivative with respect to \u03c4 , which is not trivial since \u03c4 is a factor of the whole expression. The sub-expression should be maximized as it appears with negative sign in the mapping for \u03bd.\nFirst, we consider the function for the largest \u03bd = 0.7 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 .\nThe expression becomes\n\u03c4 e ( 7\u03c4 10 + 1 100 \u221a 2 \u221a 7\u03c4 10 )2 erfc  7\u03c410 + 1100\u221a 2 \u221a\n7\u03c4 10\n\u2212 2e ( 2\u00b77\u03c4 10 + 1 100 \u221a 2 \u221a 7\u03c4 10 )2 erfc  2\u00b77\u03c410 + 1100\u221a 2 \u221a\n7\u03c4 10\n  . (300)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (70\u03c4+1)2 14000\u03c4 (700\u03c4(7\u03c4 + 20)\u2212 1) erfc ( 70\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n) \u2212 (301)\n2e (140\u03c4+1)2 14000\u03c4 (2800\u03c4(7\u03c4 + 5)\u2212 1) erfc ( 140\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n)) + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 ) ( 14000 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 97 < E < 186. Therefore we add 200 to the numerator when we use the approximation Ren and MacKenzie [30]. We obtain the inequalities:\n\u221a \u03c0 ( e (70\u03c4+1)2 14000\u03c4 (700\u03c4(7\u03c4 + 20)\u2212 1) erfc ( 70\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n) \u2212 (302)\n2e (140\u03c4+1)2 14000\u03c4 (2800\u03c4(7\u03c4 + 5)\u2212 1) erfc ( 140\u03c4 + 1\n20 \u221a 35 \u221a \u03c4\n)) + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(700\u03c4(7\u03c4 + 20)\u2212 1)\u221a \u03c0(2.911\u22121)(70\u03c4+1)\n20 \u221a 35 \u221a \u03c4\n+ \u221a \u03c0 (\n70\u03c4+1 20 \u221a 35 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1) \u221a \u03c0(2.911\u22121)(140\u03c4+1)\n20 \u221a 35 \u221a \u03c4\n+ \u221a \u03c0 (\n140\u03c4+1 20 \u221a 35 \u221a \u03c4\n)2 + 2.9112  + 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 =\n\u221a \u03c0  (700\u03c4(7\u03c4 + 20)\u2212 1) (20 \u00b7 \u221a35 \u00b7 2.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 2.911 \u221a 35 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 \u2212\n2(2800\u03c4(7\u03c4 + 5)\u2212 1) ( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 + (\n20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 ) =((\n20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 )(\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) +\n2.911 \u00b7 20 \u221a 35 \u221a \u03c0(700\u03c4(7\u03c4 + 20)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) \u2212\n\u221a \u03c02 \u00b7 20 \u00b7 \u221a 35 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u221a 35 \u00b7 2.911 \u00b7 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u221a 35 \u00b7 2.911 \u00b7 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 20 \u221a 35 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 20 \u221a 35(210\u03c4 \u2212 1) \u221a \u03c4 + 200 )(\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) (303)(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) +\n2.911 \u00b7 20 \u221a 35 \u221a \u03c0(700\u03c4(7\u03c4 + 20)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(140\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(140\u03c4 + 1)2 ) \u2212\n\u221a \u03c02 \u00b7 20 \u00b7 \u221a 35 \u00b7 2.911(2800\u03c4(7\u03c4 + 5)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(70\u03c4 + 1) + \u221a( 20 \u00b7 \u221a 35 \u00b7 2.911 \u221a \u03c4 )2 + \u03c0(70\u03c4 + 1)2 ) =\n\u2212 1.70658\u00d7 107 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c43/2+\n4200 \u221a 35 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c43/2 +\n8.60302\u00d7 106 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c43/2 \u2212 2.89498\u00d7 107\u03c43/2 \u2212\n1.21486\u00d7 107 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c45/2 + 8.8828\u00d7 106 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c45/2 \u2212\n2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 + 2.24868\u00d7 107\u03c42 + 94840.5 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4\u03c4 +\n47420.2 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4\u03c4 + 481860\u03c4 + 710.354 \u221a \u03c4 +\n820.213 \u221a \u03c4 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 + 677.432 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u2212\n1011.27 \u221a \u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 \u2212 20 \u221a 35 \u221a \u03c4 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n200 \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n677.432 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 + 2294.57 =\n\u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 +( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 94840.5\u03c4 + 820.213 \u221a \u03c4 + 677.432 ) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 +( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 47420.2\u03c4 \u2212 1011.27 \u221a \u03c4 + 677.432\n) \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4 +\n2.24868\u00d7 107\u03c42 + 481860.\u03c4 + 710.354 \u221a \u03c4 + 2294.57 6\n\u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 820.213 \u221a 1.25 + 1.25 \u00b7 94840.5 + 677.432 ) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 \u2212 1011.27 \u221a 0.8 + 1.25 \u00b7 47420.2 + 677.432\n) \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200\n) \u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+ 2.24868\u00d7 107\u03c42 + 710.354 \u221a\n1.25 + 1.25 \u00b7 481860 + 2294.57 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145.\n)\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4+(\n8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2 )\u221a\n\u03c0(140\u03c4 + 1)2 + 118635\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a \u03c0(70\u03c4 + 1)2 + 118635\u03c4 \u221a \u03c0(140\u03c4 + 1)2 + 118635\u03c4+\n2.24868\u00d7 107\u03c42 + 605413 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2\n)\u221a 19600\u03c0(\u03c4 + 1.94093)(\u03c4 + 0.0000262866)+(\n\u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145. )\u221a\n4900\u03c0(\u03c4 + 7.73521)(\u03c4 + 0.0000263835)+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 ) \u221a\n19600\u03c0(\u03c4 + 1.94093)(\u03c4 + 0.0000262866) \u221a 4900\u03c0(\u03c4 + 7.73521)(\u03c4 + 0.0000263835)+\n2.24868\u00d7 107\u03c42 + 605413 6 \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( 8.60302\u00d7 106\u03c43/2 + 8.8828\u00d7 106\u03c45/2 + 59048.2\n)\u221a 19600\u03c0(\u03c4 + 1.94093)\u03c4+(\n\u22121.70658\u00d7 107\u03c43/2 \u2212 1.21486\u00d7 107\u03c45/2 + 120145. )\u221a\n4900\u03c01.00003(\u03c4 + 7.73521)\u03c4+( 4200 \u221a 35\u03c43/2 \u2212 20 \u221a 35 \u221a \u03c4 + 200 )\u221a 19600\u03c01.00003(\u03c4 + 1.94093)\u03c4\u221a\n4900\u03c01.00003(\u03c4 + 7.73521)\u03c4+\n2.24868\u00d7 107\u03c42 + 605413 = \u2212 2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2+( \u22123.64296\u00d7 106\u03c43/2 + 7.65021\u00d7 108\u03c45/2 + 6.15772\u00d7 106\u03c4\n) \u221a \u03c4 + 1.94093\n\u221a \u03c4 + 7.73521 + 2.24868\u00d7 107\u03c42+(\n2.20425\u00d7 109\u03c43 + 2.13482\u00d7 109\u03c42 + 1.46527\u00d7 107 \u221a \u03c4 )\u221a\n\u03c4 + 1.94093+( \u22121.5073\u00d7 109\u03c43 \u2212 2.11738\u00d7 109\u03c42 + 1.49066\u00d7 107 \u221a \u03c4 )\u221a\n\u03c4 + 7.73521 + 605413 6 \u221a\n1.25 + 1.94093 \u221a 1.25 + 7.73521 ( \u22123.64296\u00d7 106\u03c43/2 + 7.65021\u00d7 108\u03c45/2 + 6.15772\u00d7 106\u03c4 ) +\n\u221a 1.25 + 1.94093 ( 2.20425\u00d7 109\u03c43 + 2.13482\u00d7 109\u03c42 + 1.46527\u00d7 107 \u221a \u03c4 )\n+ \u221a 0.8 + 7.73521 ( \u22121.5073\u00d7 109\u03c43 \u2212 2.11738\u00d7 109\u03c42 + 1.49066\u00d7 107 \u221a \u03c4 ) \u2212\n2.89498\u00d7 107\u03c43/2 \u2212 2.43651\u00d7 107\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2 + 2.24868\u00d7 107\u03c42 + 605413 = \u2212 4.84561\u00d7 107\u03c43/2 + 4.07198\u00d7 109\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2\u2212 4.66103\u00d7 108\u03c43 \u2212 2.34999\u00d7 109\u03c42+ 3.29718\u00d7 107\u03c4 + 6.97241\u00d7 107 \u221a \u03c4 + 605413 6\n605413\u03c43/2\n0.83/2 \u2212 4.84561\u00d7 107\u03c43/2+\n4.07198\u00d7 109\u03c45/2 \u2212 1.46191\u00d7 109\u03c47/2\u2212 4.66103\u00d7 108\u03c43 \u2212 2.34999\u00d7 109\u03c42 + 3.29718\u00d7 10 7 \u221a \u03c4\u03c4\u221a\n0.8 +\n6.97241\u00d7 107\u03c4 \u221a \u03c4\n0.8 = \u03c43/2 ( \u22124.66103\u00d7 108\u03c43/2 \u2212 1.46191\u00d7 109\u03c42 \u2212 2.34999\u00d7 109 \u221a \u03c4+\n4.07198\u00d7 109\u03c4 + 7.64087\u00d7 107 ) 6\n\u03c43/2 ( \u22124.66103\u00d7 108\u03c43/2 \u2212 1.46191\u00d7 109\u03c42 + 7.64087\u00d7 10 7 \u221a \u03c4\u221a\n0.8 \u2212\n2.34999\u00d7 109 \u221a \u03c4 + 4.07198\u00d7 109\u03c4 ) =\n\u03c42 ( \u22121.46191\u00d7 109\u03c43/2 + 4.07198\u00d7 109 \u221a \u03c4 \u2212 4.66103\u00d7 108\u03c4 \u2212 2.26457\u00d7 109 ) 6(\n\u22122.26457\u00d7 109 + 4.07198\u00d7 109 \u221a 0.8\u2212 4.66103\u00d7 1080.8\u2212 1.46191\u00d7 1090.83/2 ) \u03c42 =\n\u2212 4.14199\u00d7 107\u03c42 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.0000263835 under the root. We increased positive terms by setting \u03c4 + 0.000026286 = 1.00003\u03c4 and \u03c4 + 0.000026383 = 1.00003\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.0000263830.8 = 1.00003, thus \u03c4 + 0.000026286 < \u03c4 + 0.000026383 6 1.00003\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.8 and increased positive terms by inserting \u03c4 = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of \u03c4 .\nFor the last 6-sign we used the function\n\u22121.46191\u00d7 109\u03c43/2 + 4.07198\u00d7 109 \u221a \u03c4 \u2212 4.66103\u00d7 108\u03c4 \u2212 2.26457\u00d7 109 (304)\nThe derivative of this function is\n\u22122.19286\u00d7 109 \u221a \u03c4 + 2.03599\u00d7 109\u221a \u03c4 \u2212 4.66103\u00d7 108 (305)\nand the second order derivative is\n\u22121.01799\u00d7 10 9 \u03c43/2 \u2212 1.09643\u00d7 10 9 \u221a \u03c4 < 0 . (306)\nThe derivative at 0.8 is smaller than zero:\n\u2212 2.19286\u00d7 109 \u221a 0.8\u2212 4.66103\u00d7 108 + 2.03599\u00d7 10 9\n\u221a 0.8\n= (307)\n\u2212 1.51154\u00d7 108 < 0 .\nSince the second order derivative is negative, the derivative decreases with increasing \u03c4 . Therefore the derivative is negative for all values of \u03c4 that we consider, that is, the function Eq. (304) is strictly monotonically decreasing. The maximum of the function Eq. (304) is therefore at 0.8. We inserted 0.8 to obtain the maximum.\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (308)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.7.\nNext, we consider the function for the largest \u03bd = 0.16 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 .\nThe expression becomes\n\u03c4 e ( 16\u03c4 100 + 1 100 \u221a 2 \u221a 16\u03c4 100 )2 erfc  16\u03c4100 + 1100\u221a 2 \u221a\n16\u03c4 100\n\u2212 e ( 2 16\u03c4 100 + 1 100 \u221a 2 \u221a 16\u03c4 100 )2 erfc  2 16\u03c4100 + 1100\u221a 2 \u221a\n16\u03c4 100\n  . (309)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (16\u03c4+1)2 3200\u03c4 (128\u03c4(2\u03c4 + 25)\u2212 1) erfc ( 16\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n) \u2212 (310)\n2e (32\u03c4+1)2 3200\u03c4 (128\u03c4(8\u03c4 + 25)\u2212 1) erfc ( 32\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n)) + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 ) ( 3200 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 1.1 < E < 12. Therefore we add 20 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities:\n\u221a \u03c0 ( e (16\u03c4+1)2 3200\u03c4 (128\u03c4(2\u03c4 + 25)\u2212 1) erfc ( 16\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n) \u2212 (311)\n2e (32\u03c4+1)2 3200\u03c4 (128\u03c4(8\u03c4 + 25)\u2212 1) erfc ( 32\u03c4 + 1\n40 \u221a 2 \u221a \u03c4\n)) + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(128\u03c4(2\u03c4 + 25)\u2212 1)\u221a \u03c0(2.911\u22121)(16\u03c4+1)\n40 \u221a 2 \u221a \u03c4\n+ \u221a \u03c0 (\n16\u03c4+1 40 \u221a 2 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(128\u03c4(8\u03c4 + 25)\u2212 1) \u221a \u03c0(2.911\u22121)(32\u03c4+1)\n40 \u221a 2 \u221a \u03c4\n+ \u221a \u03c0 (\n32\u03c4+1 40 \u221a 2 \u221a \u03c4\n)2 + 2.9112  + 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 =\n\u221a \u03c0  (128\u03c4(2\u03c4 + 25)\u2212 1) (40\u221a22.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 \u2212\n2(128\u03c4(8\u03c4 + 25)\u2212 1) ( 40 \u221a 22.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 + 40 \u221a\n2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 =((\n40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 )(\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) + +\n2.911 \u00b7 40 \u221a 2 \u221a \u03c0(128\u03c4(2\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 22.911(128\u03c4(8\u03c4 + 25)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 20, we first factored out 40 \u221a 2 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:( 40 \u221a 2(48\u03c4 \u2212 1) \u221a \u03c4 + 20 )(\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) (312)(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 2 \u221a \u03c0(128\u03c4(2\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(32\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(32\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 22.911(128\u03c4(8\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(16\u03c4 + 1) + \u221a( 40 \u221a 22.911 \u221a \u03c4 )2 + \u03c0(16\u03c4 + 1)2 ) =\n\u2212 1.86491\u00d7 106 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2+\n1920 \u221a 2 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2+\n940121 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c43/2 \u2212 3.16357\u00d7 106\u03c43/2\u2212\n303446 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c45/2 + 221873 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c45/2 \u2212 608588\u03c45/2\u2212\n8.34635\u00d7 106\u03c47/2 + 117482.\u03c42 + 2167.78 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4\u03c4+\n1083.89 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u03c4+ 11013.9\u03c4 + 339.614 \u221a \u03c4 + 392.137 \u221a \u03c4 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+\n67.7432 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u2212 483.478 \u221a \u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4\u2212 40 \u221a 2 \u221a \u03c4 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n20 \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n67.7432 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4 + 229.457 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 2167.78\u03c4 + 392.137 \u221a \u03c4 + 67.7432 ) \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+( 940121\u03c43/2 + 221873\u03c45/2 + 1083.89\u03c4 \u2212 483.478 \u221a \u03c4 + 67.7432 )\n\u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+ 117482.\u03c42 + 11013.9\u03c4 + 339.614 \u221a \u03c4 + 229.457 6\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 392.137 \u221a 1.25 + 1.252167.78 + 67.7432 ) \u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+( 940121\u03c43/2 + 221873\u03c45/2 \u2212 483.478 \u221a 0.8 + 1.251083.89 + 67.7432\n) \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+ 117482.\u03c42 + 339.614 \u221a 1.25 + 1.2511013.9 + 229.457 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4+(\n940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a\n\u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a \u03c0(16\u03c4 + 1)2 + 27116.5\u03c4 \u221a \u03c0(32\u03c4 + 1)2 + 27116.5\u03c4+\n117482\u03c42 + 14376.6 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( 940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a 1024\u03c0(\u03c4 + 8.49155)(\u03c4 + 0.000115004)+(\n\u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a\n256\u03c0(\u03c4 + 33.8415)(\u03c4 + 0.000115428)+( 1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a 1024\u03c0(\u03c4 + 8.49155)(\u03c4 + 0.000115004)\u221a\n256\u03c0(\u03c4 + 33.8415)(\u03c4 + 0.000115428)+\n117482.\u03c42 + 14376.6 6\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( 940121\u03c43/2 + 221873\u03c45/2 + 990.171 )\u221a 1024\u03c01.00014(\u03c4 + 8.49155)\u03c4+(\n1920 \u221a 2\u03c43/2 \u2212 40 \u221a 2 \u221a \u03c4 + 20 )\u221a 256\u03c01.00014(\u03c4 + 33.8415)\u03c4 \u221a 1024\u03c01.00014(\u03c4 + 8.49155)\u03c4+(\n\u22121.86491\u00d7 106\u03c43/2 \u2212 303446\u03c45/2 + 3215.89 )\u221a 256\u03c0(\u03c4 + 33.8415)\u03c4+\n117482.\u03c42 + 14376.6 =\n\u2212 3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2+( \u221291003\u03c43/2 + 4.36814\u00d7 106\u03c45/2 + 32174.4\u03c4 )\u221a \u03c4 + 8.49155\n\u221a \u03c4 + 33.8415 + 117482.\u03c42+(\n1.25852\u00d7 107\u03c43 + 5.33261\u00d7 107\u03c42 + 56165.1 \u221a \u03c4 )\u221a\n\u03c4 + 8.49155+( \u22128.60549\u00d7 106\u03c43 \u2212 5.28876\u00d7 107\u03c42 + 91200.4 \u221a \u03c4 )\u221a\n\u03c4 + 33.8415 + 14376.6 6 \u221a\n1.25 + 8.49155 \u221a 1.25 + 33.8415 ( \u221291003\u03c43/2 + 4.36814\u00d7 106\u03c45/2 + 32174.4\u03c4 ) +\n\u221a 1.25 + 8.49155 ( 1.25852\u00d7 107\u03c43 + 5.33261\u00d7 107\u03c42 + 56165.1 \u221a \u03c4 )\n+ \u221a 0.8 + 33.8415 ( \u22128.60549\u00d7 106\u03c43 \u2212 5.28876\u00d7 107\u03c42 + 91200.4 \u221a \u03c4 ) \u2212\n3.16357\u00d7 106\u03c43/2 \u2212 608588\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2 + 117482.\u03c42 + 14376.6 =\n\u2212 4.84613\u00d7 106\u03c43/2 + 8.01543\u00d7 107\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2\u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42+ 594875.\u03c4 + 712078. \u221a \u03c4 + 14376.6 6\n14376.6\u03c43/2\n0.83/2 \u2212 4.84613\u00d7 106\u03c43/2+\n8.01543\u00d7 107\u03c45/2 \u2212 8.34635\u00d7 106\u03c47/2\u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42 + 594875. \u221a \u03c4\u03c4\u221a\n0.8 +\n712078.\u03c4 \u221a \u03c4\n0.8 =\n\u2212 3.1311 \u00b7 106\u03c43/2 \u2212 1.44725 \u00b7 108\u03c42 + 8.01543 \u00b7 107\u03c45/2 \u2212 1.13691 \u00b7 107\u03c43\u2212 8.34635 \u00b7 106\u03c47/2 6\n\u2212 3.1311\u00d7 106\u03c43/2 + 8.01543\u00d7 10 7 \u221a\n1.25\u03c45/2\u221a \u03c4 \u2212\n8.34635\u00d7 106\u03c47/2 \u2212 1.13691\u00d7 107\u03c43 \u2212 1.44725\u00d7 108\u03c42 = \u2212 3.1311\u00d7 106\u03c43/2 \u2212 8.34635\u00d7 106\u03c47/2 \u2212 1.13691\u00d7 107\u03c43 \u2212 5.51094\u00d7 107\u03c422 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.00011542 under the root. We increased positive terms by setting \u03c4 + 0.00011542 = 1.00014\u03c4 and \u03c4 + 0.000115004 = 1.00014\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.000115420.8 < 1.000142, thus \u03c4 + 0.000115004 < \u03c4 + 0.00011542 6 1.00014\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.8 and increased positive terms by inserting \u03c4 = 1.25. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.8 to obtain terms with corresponding exponents of \u03c4 .\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (313)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.16.\nNext, we consider the function for the largest \u03bd = 0.24 and the largest y = \u00b5\u03c9 = 0.01 for determining the derivative with respect to \u03c4 . However we assume 0.9 6 \u03c4 , in order to restrict the domain of \u03c4 .\nThe expression becomes\n\u03c4 e ( 24\u03c4 100 + 1 100 \u221a 2 \u221a 24\u03c4 100 )2 erfc  24\u03c4100 + 1100\u221a 2 \u221a\n24\u03c4 100\n\u2212 e ( 2 24\u03c4 100 + 1 100 \u221a 2 \u221a 24\u03c4 100 )2 erfc  2 24\u03c4100 + 1100\u221a 2 \u221a\n24\u03c4 100\n  . (314)\nThe derivative with respect to \u03c4 is(\u221a \u03c0 ( e (24\u03c4+1)2 4800\u03c4 (192\u03c4(3\u03c4 + 25)\u2212 1) erfc ( 24\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n) \u2212 (315)\n2e (48\u03c4+1)2 4800\u03c4 (192\u03c4(12\u03c4 + 25)\u2212 1) erfc ( 48\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n)) + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 ) ( 4800 \u221a \u03c0\u03c4 )\u22121 .\nWe are considering only the numerator and use again the approximation of Ren and MacKenzie [30]. The error analysis on the whole numerator gives an approximation error 14 < E < 32. Therefore we add 32 to the numerator when we use the approximation of Ren and MacKenzie [30]. We obtain the inequalities: \u221a \u03c0 ( e (24\u03c4+1)2 4800\u03c4 (192\u03c4(3\u03c4 + 25)\u2212 1) erfc ( 24\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n) \u2212 (316)\n2e (48\u03c4+1)2 4800\u03c4 (192\u03c4(12\u03c4 + 25)\u2212 1) erfc ( 48\u03c4 + 1\n40 \u221a 3 \u221a \u03c4\n)) + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 6\n\u221a \u03c0  2.911(192\u03c4(3\u03c4 + 25)\u2212 1)\u221a \u03c0(2.911\u22121)(24\u03c4+1)\n40 \u221a 3 \u221a \u03c4\n+ \u221a \u03c0 (\n24\u03c4+1 40 \u221a 3 \u221a \u03c4\n)2 + 2.9112 \u2212\n2 \u00b7 2.911(192\u03c4(12\u03c4 + 25)\u2212 1) \u221a \u03c0(2.911\u22121)(48\u03c4+1)\n40 \u221a 3 \u221a \u03c4\n+ \u221a \u03c0 (\n48\u03c4+1 40 \u221a 3 \u221a \u03c4\n)2 + 2.9112 + 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 =\n\u221a \u03c0  (192\u03c4(3\u03c4 + 25)\u2212 1) (40\u221a32.911\u221a\u03c4) \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 \u2212\n2(192\u03c4(12\u03c4 + 25)\u2212 1) ( 40 \u221a 32.911 \u221a \u03c4 )\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 + 40 \u221a\n3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 =((\n40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 )(\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) ( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 3 \u221a \u03c0(192\u03c4(3\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 32.911(192\u03c4(12\u03c4 + 25)\u2212 1)\n\u221a \u03c4 ( \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 )) (( \u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2\n) ( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ))\u22121 .\nAfter applying the approximation of Ren and MacKenzie [30] and adding 200, we first factored out 40 \u221a 3 \u221a \u03c4 . Then we brought all terms to the same denominator.\nWe now consider the numerator:\n( 40 \u221a 3(72\u03c4 \u2212 1) \u221a \u03c4 + 32 )(\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) (317)(\n\u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) +\n2.911 \u00b7 40 \u221a 3 \u221a \u03c0(192\u03c4(3\u03c4 + 25)\u2212 1) \u221a \u03c4\n( \u221a \u03c0(2.911\u2212 1)(48\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(48\u03c4 + 1)2 ) \u2212\n2 \u221a \u03c040 \u221a 32.911(192\u03c4(12\u03c4 + 25)\u2212 1) \u221a \u03c4(\n\u221a \u03c0(2.911\u2212 1)(24\u03c4 + 1) + \u221a( 40 \u221a 32.911 \u221a \u03c4 )2 + \u03c0(24\u03c4 + 1)2 ) =\n\u2212 3.42607\u00d7 106 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2+\n2880 \u221a 3 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2+\n1.72711\u00d7 106 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c43/2 \u2212 5.81185\u00d7 106\u03c43/2 \u2212\n836198 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c45/2 + 611410 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c45/2\u2212\n1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2 + 422935.\u03c42 + 5202.68 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4\u03c4+\n2601.34 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u03c4 + 26433.4\u03c4 + 415.94 \u221a \u03c4 + 480.268 \u221a \u03c4 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 +\n108.389 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u2212 592.138 \u221a \u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4\u2212 40 \u221a 3 \u221a \u03c4 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 +\n32 \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 +\n108.389 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4 + 367.131 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 5202.68\u03c4 + 480.268 \u221a \u03c4 + 108.389 ) \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2601.34\u03c4 \u2212 592.138 \u221a \u03c4 + 108.389\n) \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+ 422935.\u03c42 + 26433.4\u03c4 + 415.94 \u221a \u03c4 + 367.131 6\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 480.268 \u221a 1.25 + 1.255202.68 + 108.389 ) \u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 \u2212 592.138 \u221a 0.9 + 1.252601.34 + 108.389\n) \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+ 422935\u03c42 + 415.94 \u221a 1.25 + 1.2526433.4 + 367.131 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4+(\n1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a\n\u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a \u03c0(24\u03c4 + 1)2 + 40674.8\u03c4 \u221a \u03c0(48\u03c4 + 1)2 + 40674.8\u03c4+\n422935\u03c42 + 33874 =\n\u2212 5.81185\u00d7 106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a 2304\u03c0(\u03c4 + 5.66103)(\u03c4 + 0.0000766694)+(\n\u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\u221a\n576\u03c0(\u03c4 + 22.561)(\u03c4 + 0.0000769518)+( 2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a 2304\u03c0(\u03c4 + 5.66103)(\u03c4 + 0.0000766694)\u221a\n576\u03c0(\u03c4 + 22.561)(\u03c4 + 0.0000769518)+\n422935\u03c42 + 33874 6\n\u2212 5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( 1.72711\u00d7 106\u03c43/2 + 611410\u03c45/2 + 2798.31 )\u221a 2304\u03c01.0001(\u03c4 + 5.66103)\u03c4+(\n2880 \u221a 3\u03c43/2 \u2212 40 \u221a 3 \u221a \u03c4 + 32 )\u221a 2304\u03c01.0001(\u03c4 + 5.66103)\u03c4 \u221a 576\u03c01.0001(\u03c4 + 22.561)\u03c4+(\n\u22123.42607\u00d7 106\u03c43/2 \u2212 836198\u03c45/2 + 7148.69 )\n\u221a 576\u03c0(\u03c4 + 22.561)\u03c4+\n422935\u03c42 + 33874. =\n\u2212 5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2+( \u2212250764.\u03c43/2 + 1.8055\u00d7 107\u03c45/2 + 115823.\u03c4 ) \u221a \u03c4 + 5.66103\n\u221a \u03c4 + 22.561 + 422935.\u03c42+(\n5.20199\u00d7 107\u03c43 + 1.46946\u00d7 108\u03c42 + 238086. \u221a \u03c4 )\u221a\n\u03c4 + 5.66103+( \u22123.55709\u00d7 107\u03c43 \u2212 1.45741\u00d7 108\u03c42 + 304097. \u221a \u03c4 )\u221a\n\u03c4 + 22.561 + 33874. 6 \u221a\n1.25 + 5.66103 \u221a 1.25 + 22.561 ( \u2212250764.\u03c43/2 + 1.8055\u00d7 107\u03c45/2 + 115823.\u03c4 ) +\n\u221a 1.25 + 5.66103 ( 5.20199\u00d7 107\u03c43 + 1.46946\u00d7 108\u03c42 + 238086. \u221a \u03c4 )\n+ \u221a 0.9 + 22.561 ( \u22123.55709\u00d7 107\u03c43 \u2212 1.45741\u00d7 108\u03c42 + 304097. \u221a \u03c4 ) \u2212\n5.81185106\u03c43/2 \u2212 1.67707\u00d7 106\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2 + 422935.\u03c42 + 33874. 6 33874.\u03c43/2\n0.93/2 \u2212 9.02866\u00d7 106\u03c43/2 + 2.29933\u00d7 108\u03c45/2 \u2212 3.44998\u00d7 107\u03c47/2\u2212\n3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 + 1.48578\u00d7 10 6 \u221a \u03c4\u03c4\u221a\n0.9 +\n2.09884\u00d7 106\u03c4 \u221a \u03c4\n0.9 =\n\u2212 5.09079\u00d7 106\u03c43/2 + 2.29933\u00d7 108\u03c45/2\u2212 3.44998\u00d7 107\u03c47/2 \u2212 3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 6\n\u2212 5.09079\u00d7 106\u03c43/2 + 2.29933\u00d7 10 8 \u221a\n1.25\u03c45/2\u221a \u03c4 \u2212 3.44998\u00d7 107\u03c47/2\u2212\n3.5539\u00d7 107\u03c43 \u2212 3.19193\u00d7 108\u03c42 = \u2212 5.09079\u00d7 106\u03c43/2 \u2212 3.44998\u00d7 107\u03c47/2 \u2212 3.5539\u00d7 107\u03c43 \u2212 6.21197\u00d7 107\u03c42 < 0 .\nFirst we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for \u03c4 for some positive terms and value of 0.9 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting \u03c4 = \u03c4 + 0.0000769518 under the root. We increased positive terms by setting \u03c4 + 0.0000769518 = 1.0000962\u03c4 and \u03c4 + 0.0000766694 = 1.0000962\u03c4 under the root for positive terms. The positive terms are increase, since 0.8+0.00007695180.8 < 1.0000962, thus \u03c4 + 0.0000766694 < \u03c4 + 0.0000769518 6 1.0000962\u03c4 . For the next inequality we decreased negative terms by inserting \u03c4 = 0.9 and increased positive terms by inserting \u03c4 = 1.25. The next\nequality expands the terms. We use upper bound of 1.25 and lower bound of 0.9 to obtain terms with corresponding exponents of \u03c4 .\nConsequently, the derivative of\n\u03c4 ( e ( \u00b5\u03c9+\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + \u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n) \u2212 2e ( \u00b5\u03c9+2\u03bd\u03c4\u221a 2 \u221a \u03bd\u03c4 )2 erfc ( \u00b5\u03c9 + 2\u03bd\u03c4\u221a\n2 \u221a \u03bd\u03c4\n)) (318)\nwith respect to \u03c4 is smaller than zero for maximal \u03bd = 0.24 and the domain 0.9 6 \u03c4 6 1.25.\nLemma 47. In the domain \u22120.01 6 y 6 0.01 and 0.64 6 x 6 1.875, the function f(x, y) = e 1 2 (2y+x) erfc ( x+y\u221a\n2x\n) has a global maximum at y = 0.64 and x = \u22120.01 and a global minimum at\ny = 1.875 and x = 0.01.\nProof. f(x, y) = e 1 2 (2y+x) erfc ( x+y\u221a\n2x\n) is strictly monotonically decreasing in x, since its derivative\nwith respect to x is negative:\ne\u2212 y2 2x (\u221a \u03c0x3/2e (x+y)2 2x erfc ( x+y\u221a 2 \u221a x ) + \u221a 2(y \u2212 x) )\n2 \u221a \u03c0x3/2\n< 0\n\u21d0\u21d2 \u221a \u03c0x3/2e (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) + \u221a 2(y \u2212 x) < 0\n\u221a \u03c0x3/2e (x+y)2 2x erfc ( x+ y\u221a\n2 \u221a x\n) + \u221a 2(y \u2212 x) 6\n2x3/2\nx+y\u221a 2 \u221a x\n+ \u221a (x+y)2\n2x + 4 \u03c0\n+ y \u221a 2\u2212 x \u221a 2 6\n2 \u00b7 0.643/2\n0.01+0.64\u221a 2 \u221a 0.64 + \u221a (0.01+0.64)2 2\u00b70.64 + 4 \u03c0 + 0.01 \u221a 2\u2212 0.64 \u221a 2 = \u22120.334658 < 0. (319)\nThe two last inqualities come from applying Abramowitz bounds 22 and from the fact that the expression 2x 3/2\nx+y\u221a 2 \u221a x +\n\u221a (x+y)2\n2x + 4 \u03c0\n+ y \u221a 2\u2212 x \u221a 2 does not change monotonicity in the domain and hence\nthe maximum must be found at the border. For x = 0.64 that maximizes the function f(x, y) is monotonically in y, because its derivative w.r.t. y at x = 0.64 is\ney ( 1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) < 0\n\u21d0\u21d2 ( 1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) < 0(\n1.37713 erfc(0.883883y + 0.565685)\u2212 1.37349e\u22120.78125(y+0.64) 2 ) 6(\n1.37713 erfc(0.883883 \u00b7 \u22120.01 + 0.565685)\u2212 1.37349e\u22120.78125(0.01+0.64) 2 ) =\n0.5935272325870631\u2212 0.987354705867739 < 0. (320)\nTherefore, the values y = 0.64 and x = \u22120.01 give a global maximum of the function f(x, y) in the domain \u22120.01 6 y 6 0.01 and 0.64 6 x 6 1.875 and the values y = 1.875 and x = 0.01 give the global minimum.\nA4 Additional information on experiments\nIn this section, we report the hyperparameters that were considered for each method and data set and give details on the processing of the data sets.\nA4.1 121 UCI Machine Learning Repository data sets: Hyperparameters\nFor the UCI data sets, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using 15% of the training data as validation set. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. If multiple hyperparameters provided identical performance on the validation set, we preferred settings with a higher number of layers, lower learning rates and higher dropout rates. All methods had the chance to adjust their hyperparameters to the data set at hand.\nTable A4: Hyperparameters considered for self-normalizing networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0.05, 0} Layer form {rectangular, conic}\nTable A5: Hyperparameters considered for ReLU networks with MS initialization in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2,3,4,8,16,32} Learning rate {0.01, 0.1, 1} Dropout rate {0.5, 0} Layer form {rectangular, conic}\nTable A6: Hyperparameters considered for batch normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Batchnorm} Layer form {rectangular, conic}\nTable A7: Hyperparameters considered for weight normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Weightnorm} Layer form {rectangular, conic}\nTable A8: Hyperparameters considered for layer normalized networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden units {1024, 512, 256} Number of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Normalization {Layernorm} Layer form {rectangular, conic}\nTable A9: Hyperparameters considered for Highway networks in the UCI data sets.\nHyperparameter Considered values\nNumber of hidden layers {2, 3, 4, 8, 16, 32} Learning rate {0.01, 0.1, 1} Dropout rate {0, 0.5}\nTable A10: Hyperparameters considered for Residual networks in the UCI data sets.\nHyperparameter Considered values\nNumber of blocks {2, 3, 4, 8, 16} Number of neurons per blocks {1024, 512, 256} Block form {rectangular, diavolo} Bottleneck {25%, 50%} Learning rate {0.01, 0.1, 1}\nA4.2 121 UCI Machine Learning Repository data sets: detailed results\nMethods compared. We used data sets and preprocessing scripts by Fern\u00e1ndez-Delgado et al. [10] for data preparation and defining training and test sets. With several flaws in the method comparison[37] that we avoided, the authors compared 179 machine learning methods of 17 groups in their experiments. The method groups were defined by Fern\u00e1ndez-Delgado et al. [10] as follows: Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), Boosting, Rule-based, logistic and multinomial regression, Discriminant Analysis (DA), Bagging, Nearest Neighbour, DecisionTree, other Ensembles, Neural Networks, Bayesian, Other Methods, generalized linear models (GLM), Partial least squares and principal component regression (PLSR), and Stacking. However, many of methods assigned to those groups were merely different implementations of the same method. Therefore, we selected one representative of each of the 17 groups for method comparison. The representative method was chosen as the group\u2019s method with the median performance across all tasks. Finally, we included 17 other machine learning methods of Fern\u00e1ndez-Delgado et al. [10], and 6 FNNs, BatchNorm, WeightNorm, LayerNorm, Highway, Residual and MSRAinit networks, and self-normalizing neural networks (SNNs) giving a total of 24 compared methods.\nResults of FNN methods for all 121 data sets. The results of the compared FNN methods can be found in Table A11.\nSmall and large data sets. We assigned each of the 121 UCI data sets into the group \u201clarge datasets\u201d or \u201csmall datasets\u201d if the had more than 1,000 data points or less, respectively. We expected that Deep Learning methods require large data sets to competitive to other machine learning methods. This resulted in 75 small and 46 large data sets.\nResults. The results of the method comparison are given in Tables A12 and A13 for small and large data sets, respectively. On small data sets, SVMs performed best followed by RandomForest and SNNs. On large data sets, SNNs are the best method followed by SVMs and Random Forest.\nimage-segmentation 2310 19 0.9114 0.9090 0.9024 0.8919 0.8481 0.8938 0.8838 ionosphere 351 34 0.8864 0.9091 0.9432 0.9545 0.9432 0.9318 0.9432 iris 150 5 0.9730 0.9189 0.8378 0.9730 0.9189 1.0000 0.9730 led-display 1000 8 0.7640 0.7200 0.7040 0.7160 0.6280 0.6920 0.6480 lenses 24 5 0.6667 1.0000 1.0000 0.6667 0.8333 0.8333 0.6667 letter 20000 17 0.9726 0.9712 0.8984 0.9762 0.9796 0.9580 0.9742 libras 360 91 0.7889 0.8667 0.8222 0.7111 0.7444 0.8000 0.8333 low-res-spect 531 101 0.8571 0.8496 0.9023 0.8647 0.8571 0.8872 0.8947 lung-cancer 32 57 0.6250 0.3750 0.1250 0.2500 0.5000 0.5000 0.2500 lymphography 148 19 0.9189 0.7297 0.7297 0.6757 0.7568 0.7568 0.7838 magic 19020 11 0.8692 0.8629 0.8673 0.8723 0.8713 0.8690 0.8620 mammographic 961 6 0.8250 0.8083 0.7917 0.7833 0.8167 0.8292 0.8208 miniboone 130064 51 0.9307 0.9250 0.9270 0.9254 0.9262 0.9272 0.9313 molec-biol-promoter 106 58 0.8462 0.7692 0.6923 0.7692 0.7692 0.6923 0.4615 molec-biol-splice 3190 61 0.9009 0.8482 0.8833 0.8557 0.8519 0.8494 0.8607 monks-1 556 7 0.7523 0.6551 0.5833 0.7546 0.9074 0.5000 0.7014 monks-2 601 7 0.5926 0.6343 0.6389 0.6273 0.3287 0.6644 0.5162 monks-3 554 7 0.6042 0.7454 0.5880 0.5833 0.5278 0.5231 0.6991 mushroom 8124 22 1.0000 1.0000 1.0000 1.0000 0.9990 0.9995 0.9995 musk-1 476 167 0.8739 0.8655 0.8992 0.8739 0.8235 0.8992 0.8992 musk-2 6598 167 0.9891 0.9945 0.9915 0.9964 0.9982 0.9927 0.9951 nursery 12960 9 0.9978 0.9988 1.0000 0.9994 0.9994 0.9966 0.9966 oocytes_merluccius_nucleus_4d 1022 42 0.8235 0.8196 0.7176 0.8000 0.8078 0.8078 0.7686 oocytes_merluccius_states_2f 1022 26 0.9529 0.9490 0.9490 0.9373 0.9333 0.9020 0.9412 oocytes_trisopterus_nucleus_2f 912 26 0.7982 0.8728 0.8289 0.7719 0.7456 0.7939 0.8202 oocytes_trisopterus_states_5b 912 33 0.9342 0.9430 0.9342 0.8947 0.8947 0.9254 0.8991 optical 5620 63 0.9711 0.9666 0.9644 0.9627 0.9716 0.9638 0.9755 ozone 2536 73 0.9700 0.9732 0.9716 0.9669 0.9669 0.9748 0.9716 page-blocks 5473 11 0.9583 0.9708 0.9656 0.9605 0.9613 0.9730 0.9708 parkinsons 195 23 0.8980 0.9184 0.8367 0.9184 0.8571 0.8163 0.8571 pendigits 10992 17 0.9706 0.9714 0.9671 0.9708 0.9734 0.9620 0.9657 pima 768 9 0.7552 0.7656 0.7188 0.7135 0.7188 0.6979 0.6927 pittsburg-bridges-MATERIAL 106 8 0.8846 0.8462 0.9231 0.9231 0.8846 0.8077 0.9231 pittsburg-bridges-REL-L 103 8 0.6923 0.7692 0.6923 0.8462 0.7692 0.6538 0.7308 pittsburg-bridges-SPAN 92 8 0.6957 0.5217 0.5652 0.5652 0.5652 0.6522 0.6087 pittsburg-bridges-T-OR-D 102 8 0.8400 0.8800 0.8800 0.8800 0.8800 0.8800 0.8800 pittsburg-bridges-TYPE 105 8 0.6538 0.6538 0.5385 0.6538 0.1154 0.4615 0.6538 planning 182 13 0.6889 0.6667 0.6000 0.7111 0.6222 0.6444 0.6889 plant-margin 1600 65 0.8125 0.8125 0.8375 0.7975 0.7600 0.8175 0.8425 plant-shape 1600 65 0.7275 0.6350 0.6325 0.5150 0.2850 0.6575 0.6775 plant-texture 1599 65 0.8125 0.7900 0.7900 0.8000 0.8200 0.8175 0.8350 post-operative 90 9 0.7273 0.7273 0.5909 0.7273 0.5909 0.5455 0.7727 primary-tumor 330 18 0.5244 0.5000 0.4512 0.3902 0.5122 0.5000 0.4512 ringnorm 7400 21 0.9751 0.9843 0.9692 0.9811 0.9843 0.9719 0.9827 seeds 210 8 0.8846 0.8654 0.9423 0.8654 0.8654 0.8846 0.8846 semeion 1593 257 0.9196 0.9296 0.9447 0.9146 0.9372 0.9322 0.9447 soybean 683 36 0.8511 0.8723 0.8617 0.8670 0.8883 0.8537 0.8484 spambase 4601 58 0.9409 0.9461 0.9435 0.9461 0.9426 0.9504 0.9513 spect 265 23 0.6398 0.6183 0.6022 0.6667 0.6344 0.6398 0.6720 spectf 267 45 0.4973 0.6043 0.8930 0.7005 0.2299 0.4545 0.5561 statlog-australian-credit 690 15 0.5988 0.6802 0.6802 0.6395 0.6802 0.6860 0.6279 statlog-german-credit 1000 25 0.7560 0.7280 0.7760 0.7720 0.7520 0.7400 0.7400\nmethodGroup method avg. rank p-value\nSVM LibSVM_weka 9.3 RandomForest RRFglobal_caret 9.6 2.5e-01 SNN SNN 9.6 3.8e-01 LMR SimpleLogistic_weka 9.9 1.5e-01 NeuralNetworks lvq_caret 10.1 1.0e-01 MARS gcvEarth_caret 10.7 3.6e-02 MSRAinit MSRAinit 11.0 4.0e-02 LayerNorm LayerNorm 11.3 7.2e-02 Highway Highway 11.5 8.9e-03 DiscriminantAnalysis mda_R 11.8 2.6e-03 Boosting LogitBoost_weka 11.9 2.4e-02 Bagging ctreeBag_R 12.1 1.8e-03 ResNet ResNet 12.3 3.5e-03 BatchNorm BatchNorm 12.6 4.9e-04 Rule-based JRip_caret 12.9 1.7e-04 WeightNorm WeightNorm 13.0 8.3e-05 DecisionTree rpart2_caret 13.6 7.0e-04 OtherEnsembles Dagging_weka 13.9 3.0e-05 Nearest Neighbour NNge_weka 14.0 7.7e-04 OtherMethods pam_caret 14.2 1.5e-04 PLSR simpls_R 14.3 4.6e-05 Bayesian NaiveBayes_weka 14.6 1.2e-04 GLM bayesglm_caret 15.0 1.6e-06 Stacking Stacking_weka 20.9 2.2e-12\nA4.3 Tox21 challenge data set: Hyperparameters\nFor the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using the validation set defined by the challenge winners [28]. The hyperparameter space was chosen to be similar to the hyperparameters that were tested by Mayr et al. [28]. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u22126 \u22123 0 3 6\nd e\nn s it y\nnetwork inputs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u22126 \u22123 0 3 6\nd e\nn s it y\nnetwork inputs\nFigure A8: Distribution of network inputs of an SNN for the Tox21 data set. The plots show the distribution of network inputs z of the second layer of a typical Tox21 network. The red curves display a kernel density estimator of the network inputs and the black curve is the density of a standard normal distribution. Left panel: At initialization time before learning. The distribution of network inputs is close to a standard normal distribution. Right panel: After 40 epochs of learning. The distributions of network inputs is close to a normal distribution.\nDistribution of network inputs. We empirically checked the assumption that the distribution of network inputs can well be approximated by a normal distribution. To this end, we investigated the density of the network inputs before and during learning and found that these density are close to normal distributions (see Figure A8).\nA4.4 HTRU2 data set: Hyperparameters\nFor the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested cross-validation procedure. Concretely, if M was the testing fold, we used M \u2212 1 as validation fold, and for M = 1 we used fold 10 for validation. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested \u201crectangular\u201d and \u201cconic\u201d layers \u2013 rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.\nA5 Other fixed points\nA similar analysis with corresponding function domains can be performed for other fixed points, for example for \u00b5 = \u00b5\u0303 = 0 and \u03bd = \u03bd\u0303 = 2, which leads to a SELU activation function with parameters \u03b102 = 1.97126 and \u03bb02 = 1.06071.\nA6 Bounds determined by numerical methods\nIn this section we report bounds on previously discussed expressions as determined by numerical methods (min and max have been computed).\n0(\u00b5=0.06,\u03c9=0,\u03bd=1.35,\u03c4=1.12) < \u2202J11 \u2202\u00b5 < .00182415(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.47845,\u03c4=0.883374)\n(321)\n0.905413(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J11 \u2202\u03c9 < 1.04143(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n\u22120.0151177(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J11 \u2202\u03bd < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.015194(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J11 \u2202\u03c4 < 0.015194(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0151177(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25) < \u2202J12 \u2202\u00b5 < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J12 \u2202\u03c9 < 0.0151177(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.00785613(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J12 \u2202\u03bd < 0.0315805(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0.0799824(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J12 \u2202\u03c4 < 0.110267(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0(\u00b5=0.06,\u03c9=0,\u03bd=1.35,\u03c4=1.12) < \u2202J21 \u2202\u00b5 < 0.0174802(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n0.0849308(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=0.8) < \u2202J21 \u2202\u03c9 < 0.695766(\u00b5=0.1,\u03c9=0.1,\u03bd=1.5,\u03c4=1.25)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J21 \u2202\u03bd < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0673083(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=0.8) < \u2202J21 \u2202\u03c4 < 0.0673083(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.5,\u03c4=0.8)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u00b5 < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.0600823(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u03c9 < 0.0600823(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=0.8,\u03c4=1.25)\n\u22120.276862(\u00b5=\u22120.01,\u03c9=\u22120.01,\u03bd=0.8,\u03c4=1.25) < \u2202J22 \u2202\u03bd < \u22120.084813(\u00b5=\u22120.1,\u03c9=0.1,\u03bd=1.5,\u03c4=0.8)\n0.562302(\u00b5=0.1,\u03c9=\u22120.1,\u03bd=1.5,\u03c4=1.25) < \u2202J22 \u2202\u03c4 < 0.664051(\u00b5=0.1,\u03c9=0.1,\u03bd=0.8,\u03c4=0.8)\n\u2223\u2223\u2223\u2223\u2202J11\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.00182415(0.0031049101995398316) (322)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c9 \u2223\u2223\u2223\u2223 < 1.04143(1.055872374194189)\u2223\u2223\u2223\u2223\u2202J11\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\n\u2223\u2223\u2223\u2223\u2202J11\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.015194(0.03749149348255419)\u2223\u2223\u2223\u2223\u2202J12\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.0151177(0.031242911235461816)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0315805(0.21232788238624354)\u2223\u2223\u2223\u2223\u2202J12\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.110267(0.2124377655377270)\u2223\u2223\u2223\u2223\u2202J21\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0174802(0.02220441024325437)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.695766(1.146955401845684)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J21\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.0673083(0.17980135762932363)\u2223\u2223\u2223\u2223\u2202J22\u2202\u00b5 \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c9 \u2223\u2223\u2223\u2223 < 0.0600823(0.14983446469110305)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03bd \u2223\u2223\u2223\u2223 < 0.562302(1.805740052651535)\u2223\u2223\u2223\u2223\u2202J22\u2202\u03c4 \u2223\u2223\u2223\u2223 < 0.664051(2.396685907216327)\nA7 References\n[1] Abramowitz, M. and Stegun, I. (1964). Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series. National Bureau of Standards, 10th edition.\n[2] Ba, J. L., Kiros, J. R., and Hinton, G. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n[3] Bengio, Y. (2013). Deep learning of representations: Looking forward. In Proceedings of the First International Conference on Statistical Language and Speech Processing, pages 1\u201337, Berlin, Heidelberg.\n[4] Blinn, J. (1996). Consider the lowly 2\u00d72 matrix. IEEE Computer Graphics and Applications, pages 82\u201388.\n[5] Bradley, R. C. (1981). Central limit theorems under weak dependence. Journal of Multivariate Analysis, 11(1):1\u201316.\n[6] Cires\u0327an, D. and Meier, U. (2015). Multi-column deep neural networks for offline handwritten chinese character classification. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1\u20136. IEEE.\n[7] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (ELUs). 5th International Conference on Learning Representations, arXiv:1511.07289.\n[8] Dugan, P., Clark, C., LeCun, Y., and Van Parijs, S. (2016). Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications. arXiv preprint arXiv:1605.00982.\n[9] Esteva, A., Kuprel, B., Novoa, R., Ko, J., Swetter, S., Blau, H., and Thrun, S. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542(7639):115\u2013118.\n[10] Fern\u00e1ndez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems. Journal of Machine Learning Research, 15(1):3133\u20133181.\n[11] Goldberg, D. (1991). What every computer scientist should know about floating-point arithmetic. ACM Comput. Surv., 223(1):5\u201348.\n[12] Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing (ICASSP), pages 6645\u20136649.\n[13] Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information processing systems, pages 545\u2013552.\n[14] Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., et al. (2016). Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA, 316(22):2402\u20132410.\n[15] Harrison, J. (1999). A machine-checked theory of floating point arithmetic. In Bertot, Y., Dowek, G., Hirschowitz, A., Paulin, C., and Th\u00e9ry, L., editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs\u201999, volume 1690 of Lecture Notes in Computer Science, pages 113\u2013130. Springer-Verlag.\n[16] He, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\n[17] He, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1026\u20131034.\n[18] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8):1735\u20131780.\n[19] Huval, B., Wang, T., Tandon, S., et al. (2015). An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716.\n[20] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456.\n[21] Kahan, W. (2004). A logarithm too clever by half. Technical report, University of California, Berkeley.\n[22] Korolev, V. and Shevtsova, I. (2012). An improvement of the Berry\u2013Esseen inequality with applications to Poisson and mixed Poisson random sums. Scandinavian Actuarial Journal, 2012(2):81\u2013105.\n[23] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105.\n[24] LeCun, Y. and Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995.\n[25] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436\u2013444.\n[26] Loosemore, S., Stallman, R. M., McGrath, R., Oram, A., and Drepper, U. (2016). The GNU C Library: Application Fundamentals. GNU Press, Free Software Foundation, 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA, 2.24 edition.\n[27] Lyon, R., Stappers, B., Cooper, S., Brooke, J., and Knowles, J. (2016). Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach. Monthly Notices of the Royal Astronomical Society, 459(1):1104\u20131123.\n[28] Mayr, A., Klambauer, G., Unterthiner, T., and Hochreiter, S. (2016). DeepTox: Toxicity prediction using deep learning. Frontiers in Environmental Science, 3:80.\n[29] Muller, J.-M. (2005). On the definition of ulp(x). Technical Report Research report RR2005-09, Laboratoire de l\u2019Informatique du Parall\u00e9lisme.\n[30] Ren, C. and MacKenzie, A. R. (2007). Closed-form approximations to the error and complementary error functions and their applications in atmospheric science. Atmos. Sci. Let., pages 70\u201373.\n[31] Sak, H., Senior, A., Rao, K., and Beaufays, F. (2015). Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.\n[32] Salimans, T. and Kingma, D. P. (2016). Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909.\n[33] Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61:85\u2013117.\n[34] Silver, D., Huang, A., Maddison, C., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489.\n[35] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Training very deep networks. In Advances in Neural Information Processing Systems, pages 2377\u20132385.\n[36] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112.\n[37] Wainberg, M., Alipanahi, B., and Frey, B. J. (2016). Are random forests truly the best classifiers? Journal of Machine Learning Research, 17(110):1\u20135.\nList of Figures\n1 FNN and SNN trainin error curves . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Visualization of the mapping g . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nA3 Graph of the main subfunction of the derivative of the second moment . . . . . . . 30\nA4 Graph of the Abramowitz bound for the complementary error function. . . . . . . . 37\nA5 Graphs of the functions ex 2 erfc(x) and xex 2 erfc(x). . . . . . . . . . . . . . . . . 38\nA6 The graph of function \u00b5\u0303 for low variances . . . . . . . . . . . . . . . . . . . . . . 56\nA7 Graph of the function h(x) = \u00b5\u03032(0.1,\u22120.1, x, 1, \u03bb01, \u03b101) . . . . . . . . . . . . . 57 A8 Distribution of network inputs in Tox21 SNNs. . . . . . . . . . . . . . . . . . . . 94\nList of Tables\n1 Comparison of seven FNNs on 121 UCI tasks . . . . . . . . . . . . . . . . . . . . 8\n2 Comparison of FNNs at the Tox21 challenge dataset . . . . . . . . . . . . . . . . . 8\n3 Comparison of FNNs and reference methods at HTRU2 . . . . . . . . . . . . . . . 9\nA4 Hyperparameters considered for self-normalizing networks in the UCI data sets. . . 85\nA5 Hyperparameters considered for ReLU networks in the UCI data sets. . . . . . . . 85\nA6 Hyperparameters considered for batch normalized networks in the UCI data sets. . 85\nA7 Hyperparameters considered for weight normalized networks in the UCI data sets. . 86\nA8 Hyperparameters considered for layer normalized networks in the UCI data sets. . . 86\nA9 Hyperparameters considered for Highway networks in the UCI data sets. . . . . . . 86\nA10 Hyperparameters considered for Residual networks in the UCI data sets. . . . . . . 86\nA11 Comparison of FNN methods on all 121 UCI data sets. . . . . . . . . . . . . . . . 88\nA12 Method comparison on small UCI data sets . . . . . . . . . . . . . . . . . . . . . 90\nA13 Method comparison on large UCI data sets . . . . . . . . . . . . . . . . . . . . . . . 91\nA14 Hyperparameters considered for self-normalizing networks in the Tox21 data set. . 92\nA15 Hyperparameters considered for ReLU networks in the Tox21 data set. . . . . . . . 92\nA16 Hyperparameters considered for batch normalized networks in the Tox21 data set. . 92\nA17 Hyperparameters considered for weight normalized networks in the Tox21 data set. 93\nA18 Hyperparameters considered for layer normalized networks in the Tox21 data set. . 93\nA19 Hyperparameters considered for Highway networks in the Tox21 data set. . . . . . 93\nA20 Hyperparameters considered for Residual networks in the Tox21 data set. . . . . . 93\nA21 Hyperparameters considered for self-normalizing networks on the HTRU2 data set. 95\nA22 Hyperparameters considered for ReLU networks on the HTRU2 data set. . . . . . . 95\nA23 Hyperparameters considered for BatchNorm networks on the HTRU2 data set. . . . 95\nA24 Hyperparameters considered for WeightNorm networks on the HTRU2 data set. . . 96\nA25 Hyperparameters considered for LayerNorm networks on the HTRU2 data set. . . . 96\nA26 Hyperparameters considered for Highway networks on the HTRU2 data set. . . . . 96\nA27 Hyperparameters considered for Residual networks on the HTRU2 data set. . . . . 96\nBrief index\nAbramowitz bounds, 37\nBanach Fixed Point Theorem, 13 bounds\nderivatives of Jacobian entries, 21 Jacobian entries, 23 mean and variance, 24 singular value, 25, 27\ncentral limit theorem, 6 complementary error function\nbounds, 37 definition, 37\ncomputer-assisted proof, 33 contracting variance, 29\ndefinitions, 2 domain\nsingular value, 19 Theorem 1, 12 Theorem 2, 12 Theorem 3, 13\ndropout, 6\nerf, 37 erfc, 37 error function\nbounds, 37 definition, 37 properties, 39\nexpanding variance, 32 experiments, 7, 85\nastronomy, 8 HTRU2, 8, 95\nhyperparameters, 95 methods compared, 7 Tox21, 7, 92\nhyperparameters, 8, 92 UCI, 7, 85\ndetails, 85 hyperparameters, 85 results, 86\ninitialization, 6\nJacobian, 20 bounds, 23 definition, 20 derivatives, 21 entries, 20, 23 singular value, 21 singular value bound, 25\nlemmata, 19 Jacobian bound, 19\nmapping g, 2, 4\ndefinition, 11 mapping in domain, 29\nself-normalizing neural networks, 2 SELU\ndefinition, 3 parameters, 4, 11\nTheorem 1, 5, 12 proof, 13 proof sketch, 5 Theorem 2, 6, 12 proof, 14 Theorem 3, 6, 12 proof, 18"
                }
            ],
            "year": 2017,
            "references": [
                {
                    "title": "Handbook of Mathematical Functions, volume 55 of Applied Mathematics Series",
                    "authors": [
                        "M. Abramowitz",
                        "I. Stegun"
                    ],
                    "venue": "National Bureau of Standards,",
                    "year": 1964
                },
                {
                    "title": "Deep learning of representations: Looking forward",
                    "authors": [
                        "Y. Bengio"
                    ],
                    "venue": "In Proceedings of the First International Conference on Statistical Language and Speech Processing,",
                    "year": 2013
                },
                {
                    "title": "Consider the lowly 2\u00d72 matrix",
                    "authors": [
                        "J. Blinn"
                    ],
                    "venue": "IEEE Computer Graphics and Applications,",
                    "year": 1996
                },
                {
                    "title": "Central limit theorems under weak dependence",
                    "authors": [
                        "R.C. Bradley"
                    ],
                    "venue": "Journal of Multivariate Analysis,",
                    "year": 1981
                },
                {
                    "title": "Multi-column deep neural networks for offline handwritten chinese character classification",
                    "authors": [
                        "D. Cire\u015fan",
                        "U. Meier"
                    ],
                    "venue": "In 2015 International Joint Conference on Neural Networks (IJCNN),",
                    "year": 2015
                },
                {
                    "title": "Fast and accurate deep network learning by exponential linear units (ELUs)",
                    "authors": [
                        "Clevert",
                        "D.-A",
                        "T. Unterthiner",
                        "S. Hochreiter"
                    ],
                    "venue": "5th International Conference on Learning Representations,",
                    "year": 2015
                },
                {
                    "title": "Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications",
                    "authors": [
                        "P. Dugan",
                        "C. Clark",
                        "Y. LeCun",
                        "S. Van Parijs"
                    ],
                    "venue": "arXiv preprint arXiv:1605.00982",
                    "year": 2016
                },
                {
                    "title": "Dermatologist-level classification of skin cancer",
                    "authors": [
                        "A. Esteva",
                        "B. Kuprel",
                        "R. Novoa",
                        "J. Ko",
                        "S. Swetter",
                        "H. Blau",
                        "S. Thrun"
                    ],
                    "year": 2017
                },
                {
                    "title": "Do we need hundreds of classifiers to solve real world classification problems",
                    "authors": [
                        "M. Fern\u00e1ndez-Delgado",
                        "E. Cernadas",
                        "S. Barro",
                        "D. Amorim"
                    ],
                    "venue": "Journal of Machine Learning Research,",
                    "year": 2014
                },
                {
                    "title": "What every computer scientist should know about floating-point arithmetic",
                    "authors": [
                        "D. Goldberg"
                    ],
                    "venue": "ACM Comput. Surv.,",
                    "year": 1991
                },
                {
                    "title": "Speech recognition with deep recurrent neural networks. In IEEE International conference on acoustics, speech and signal processing",
                    "authors": [
                        "A. Graves",
                        "A. Mohamed",
                        "G. Hinton"
                    ],
                    "year": 2013
                },
                {
                    "title": "Offline handwriting recognition with multidimensional recurrent neural networks",
                    "authors": [
                        "A. Graves",
                        "J. Schmidhuber"
                    ],
                    "venue": "In Advances in neural information processing systems,",
                    "year": 2009
                },
                {
                    "title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs",
                    "authors": [
                        "V. Gulshan",
                        "L. Peng",
                        "M. Coram",
                        "M.C. Stumpe",
                        "D. Wu",
                        "A. Narayanaswamy",
                        "S. Venugopalan",
                        "K. Widner",
                        "T. Madams",
                        "J Cuadros"
                    ],
                    "year": 2016
                },
                {
                    "title": "A machine-checked theory of floating point arithmetic",
                    "authors": [
                        "J. Harrison"
                    ],
                    "venue": "editors, Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs\u201999,",
                    "year": 1999
                },
                {
                    "title": "Deep residual learning for image recognition",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                    "year": 2015
                },
                {
                    "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
                    "year": 2015
                },
                {
                    "title": "Long short-term memory",
                    "authors": [
                        "S. Hochreiter",
                        "J. Schmidhuber"
                    ],
                    "venue": "Neural Computation,",
                    "year": 1997
                },
                {
                    "title": "An empirical evaluation of deep learning on highway driving",
                    "authors": [
                        "B. Huval",
                        "T. Wang",
                        "S Tandon"
                    ],
                    "venue": "arXiv preprint arXiv:1504.01716",
                    "year": 2015
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "S. Ioffe",
                        "C. Szegedy"
                    ],
                    "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
                    "year": 2015
                },
                {
                    "title": "A logarithm too clever by half",
                    "authors": [
                        "W. Kahan"
                    ],
                    "venue": "Technical report,",
                    "year": 2004
                },
                {
                    "title": "An improvement of the Berry\u2013Esseen inequality with applications to Poisson and mixed Poisson random sums",
                    "authors": [
                        "V. Korolev",
                        "I. Shevtsova"
                    ],
                    "venue": "Scandinavian Actuarial Journal,",
                    "year": 2012
                },
                {
                    "title": "Imagenet classification with deep convolutional neural networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G. Hinton"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2012
                },
                {
                    "title": "Convolutional networks for images, speech, and time series",
                    "authors": [
                        "Y. LeCun",
                        "Y. Bengio"
                    ],
                    "venue": "The handbook of brain theory and neural networks,",
                    "year": 1995
                },
                {
                    "title": "The GNU C Library: Application Fundamentals",
                    "authors": [
                        "S. Loosemore",
                        "R.M. Stallman",
                        "R. McGrath",
                        "A. Oram",
                        "U. Drepper"
                    ],
                    "venue": "GNU Press, Free Software Foundation,",
                    "year": 2016
                },
                {
                    "title": "Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach",
                    "authors": [
                        "R. Lyon",
                        "B. Stappers",
                        "S. Cooper",
                        "J. Brooke",
                        "J. Knowles"
                    ],
                    "venue": "Monthly Notices of the Royal Astronomical Society,",
                    "year": 2016
                },
                {
                    "title": "DeepTox: Toxicity prediction using deep learning",
                    "authors": [
                        "A. Mayr",
                        "G. Klambauer",
                        "T. Unterthiner",
                        "S. Hochreiter"
                    ],
                    "venue": "Frontiers in Environmental Science,",
                    "year": 2016
                },
                {
                    "title": "On the definition of ulp(x)",
                    "authors": [
                        "Muller",
                        "J.-M"
                    ],
                    "venue": "Technical Report Research report RR2005-09, Laboratoire de l\u2019Informatique du Paralle\u0301lisme",
                    "year": 2005
                },
                {
                    "title": "Closed-form approximations to the error and complementary error functions and their applications in atmospheric science",
                    "authors": [
                        "C. Ren",
                        "A.R. MacKenzie"
                    ],
                    "venue": "Atmos. Sci. Let.,",
                    "year": 2007
                },
                {
                    "title": "Fast and accurate recurrent neural network acoustic models for speech recognition",
                    "authors": [
                        "H. Sak",
                        "A. Senior",
                        "K. Rao",
                        "F. Beaufays"
                    ],
                    "venue": "arXiv preprint arXiv:1507.06947",
                    "year": 2015
                },
                {
                    "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
                    "authors": [
                        "T. Salimans",
                        "D.P. Kingma"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2016
                },
                {
                    "title": "Deep learning in neural networks: An overview",
                    "authors": [
                        "J. Schmidhuber"
                    ],
                    "venue": "Neural Networks,",
                    "year": 2015
                },
                {
                    "title": "Mastering the game of Go with deep neural networks and tree search",
                    "authors": [
                        "D. Silver",
                        "A. Huang",
                        "C Maddison"
                    ],
                    "year": 2016
                },
                {
                    "title": "Training very deep networks",
                    "authors": [
                        "R.K. Srivastava",
                        "K. Greff",
                        "J. Schmidhuber"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2015
                },
                {
                    "title": "Sequence to sequence learning with neural networks",
                    "authors": [
                        "I. Sutskever",
                        "O. Vinyals",
                        "Q.V. Le"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2014
                }
            ],
            "id": "SP:e4feb9cb5ce67b08751ba1596e78e4b85d776876",
            "authors": [
                {
                    "name": "G\u00fcnter Klambauer",
                    "affiliations": []
                },
                {
                    "name": "Thomas Unterthiner",
                    "affiliations": []
                },
                {
                    "name": "Andreas Mayr",
                    "affiliations": []
                },
                {
                    "name": "Sepp Hochreiter",
                    "affiliations": []
                }
            ],
            "abstractText": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \u201cscaled exponential linear units\u201d (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance \u2014 even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",
            "title": "Self-Normalizing Neural Networks"
        }
    },
    "27086937": {
        "X": {
            "sections": [
                {
                    "text": "ar X\niv :1\n60 9.\n06 48\n0v 1\n[ q-\nbi o.\nG N\n] 2\n1 Se\np 20\n16 1\nIndex Terms\u2014Sparse logistic regression, network-regularized penalty, survival risk prediction, feature selection\n\u2726"
                },
                {
                    "heading": "1 INTRODUCTION",
                    "text": "P REDICTING clinical risk and discovering molecularprognostic signatures are key topics in genomic studies [1], [2]. Large number of genomic datasets (e.g., TCGA [3], [4]) have been rapidly generated on cancer and other complex disease [3], [5], [6], [7]. These available cancer genomic data provides us an unprecedented opportunity to predict the development risk of cancer via integrating diverse molecular profiling data [8], [5]. Recently, some prognostic models have been proposed via integrating clinical and gene expression data [9], [10], [11]. Most of these methods are based on the Cox proportional hazards model [12] and a few are designed based on other machine learning methods for this task. We can also easily dichotomize the survival time into a binary outcome and obtain a typical classification problem which can been solved by many supervised machine learning methods.\nLogistic regression (LR) is one of such a classical method and has been widely used for classification [13]. However the traditional LR model employs all (or most) variables for predicting and lead to a non-sparse solution with limited interpretability. The sparsity principle is an important strategy for interpretable analysis in statistics and machine learning. Recently, a number of studies have focused on developing regularized or penalized LR models to encourage sparse solutions and use a limited number of variables for\n\u2022 Wenwen Min, and Juan Liu are with the State Key Laboratory of Software Engineering, School of Computer, Wuhan University, Wuhan 430072, China. E-mail: {mww, liujuan}@whu.edu.cn. \u2022 Shihua Zhang is with National Center for Mathematics and Interdisciplinary Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China. E-mail: zsh@amss.ac.cn.\nManuscript received XXX, 2016; revised XXX, 2016.\npredicting [14], [15], [16], [17], [18], [19], [20]. Many of the penalized LR models apply Lasso as a penalty function to induce sparsity. However the Lasso fails to select strongly correlated variables together and tends to select a variable from them [21]. Thus, many generalizations of the Lasso have been proposed to solve the limits of Lasso [22], including elastic net, group Lasso, and fused Lasso, etc. On the other hand, some researchers proposed refined regression models with network-based penalties [23], [24], [25]. These models are expect to get more accurate prediction and better interpretability via integrating prior knowledge.\nMotivated by the development of sparse coding and network-regularized norm [23], [26], we address the double tasks \u2013 feature selection and class prediction \u2013 by using a network-based penalty in the Logistic Regression (LR) framework. Specifically, we first focus on a traditional network-regularized penalty:\nR1(w) = \u03bb\u2016w\u20161 + \u03b7wTLw, (1) where L is the normalized Laplacian matrix encoding a prior network (e.g., a protein interaction network). The first term is a L1-norm penalty to induce sparsity. The second term wTLw = \u2211 i\u223cj Aij(wi/ \u221a di \u2212 wj/ \u221a dj)\n2 is a quadratic-Laplacian norm penalty to force the coefficients of w to be smooth. More importantly, it is a convex penalty. Thus, such a regularized LR model and its reduced forms can be solved effectively. However if those estimated wi and wj have opposite signs, then the traditional networkregularized penalty may not perform well. To address this limitation, we focus on a novel network-based penalty [27]:\nR2(w) = \u03bb\u2016w\u20161 + \u03b7|w|TL|w|, (2) where |w|TL|w| = \u2211i\u223cj Aij(|wi|/ \u221a di \u2212 |wj |/ \u221a dj) 2 which is adopted to eliminate the effects of symbols of the\n2 estimated coefficients. However, the novel penalty is not differentiable at the zero point. Intuitively, it is a challenge issue to solve such a regularized LR model using the conventional gradient descent method. Here we develop two methods to solve it. We first propose to use the following penalty:\n\u03bb\np\u2211\ni=1\n|wi|+ \u03b7 \u2211\ni\u223cj\nAij\n( sign(w\u0302i)wi\u221a\ndi \u2212 sign(w\u0302j)wj\u221a dj\n)2\nto replaceR2, i.e., |wi| \u2248 sign(w\u0302i)wi, where w\u0302 is computed using the maximum likelihood estimation for classical LR model. The new penalty is a convex function and has been used for a regression model [24]. Similarly, we can effectively solve this regularized LR model. We also try to solve the novel penalty (R2(w)) directly. Fortunately, Hastie et al. [28] (see Page 112) find that R2 has a good property (i.e., condition regularity [28], [29]), which inspires us to solve it via a cycle coordinate descent algorithm.\nTo sum up, our key contributions are two-fold. First, we introduce a unified Network-regularized Sparse LR (NSLR) framework enabling the Lasso, elastic net and networkregularized LR models are all special cases of it. More importantly, this framework can be efficiently solved using a coordinate-wise Newton algorithm. Second, we propose a novel network-regularized LR model to eliminate the sensitivity of the conventional network-regularized penalty to the signs of estimated coefficients. Here we adopt an approximate algorithm and a coordinate-wise Newton algorithm to solve it, respectively. Finally, we apply our methods to Glioblastoma multiforme (GBM) gene expression and cancer clinical data from TCGA database [3], and a protein interaction network data from Pathway Commons [30] for survival risk prediction and biomarker discovery. Our first key is to identify some gene biomarkers for survival risk prediction. In addition, based on the prediction probability scores of GBM patients, we can divide them into different subtypes relating to survival output. Furthermore, we apply our methods to a lung cancer dataset with two subtypes including Lung adenocarcinoma (LUAD) and Lung squamous-cell carcinoma (LUSC) to identify subtypespecific biomarkers."
                },
                {
                    "heading": "2 METHOD",
                    "text": "In this section, we first briefly review the typical logistic regression model and an iteratively re-weight least squares (IRLS) learning strategy based on a coordinate-wise Newton method. We further introduce the two network-regularized LR models together with the basic regularized LR models (Lasso and elastic net) in a unified framework in Section 2.3 and suggest a general coordinate-wise Newton algorithm for solving this framework. Lastly, we propose a novel network-regularized sparse LR model in Section 2.5. We can use an approximate strategy (AdaNet.LR in Section 2.3) to solve it. In addition, we also develop an efficient coordinatewise Newton algorithm to solve the AdaNet.LR directly (Section 2.5)."
                },
                {
                    "heading": "2.1 Logistic Regression",
                    "text": "Here we consider a binary classification problem. Given n training samples D = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xn, yn)} where xi \u2208\nR p is a p-dimensional column vector and label yi \u2208 {0, 1}. We first write the logistic function as follows:\np(x;w, b) = Pr(y = 1|x;w, b) = 1 1 + exp(\u2212wTx\u2212 b) .\nFor convenience, let w = (w, b) and x = (x, 1). We can rewrite it:\np(x;w) = Pr(y = 1|x;w) = 1 1 + exp(\u2212wTx) , (3)\nwhere w is the weight vector of coefficients, and p(\u00b7) is a sigmoid function. Here we assume that the n training examples are generated independently. We thus can obtain the following log-likelihood:\n\u2113(w) = n\u2211\ni=1\nlogPr(yi|xi;w)\n= n\u2211\ni=1\n{yi log p(xi;w) + (1\u2212 yi) log(1\u2212 p(xi;w))}\n= n\u2211\ni=1\n{yiwTxi \u2212 log(1 + exp(wTxi)}. (4)"
                },
                {
                    "heading": "2.2 A Learning Algorithm for Logistic Regression",
                    "text": "Clearly, the above likelihood is a convex function. Thus we can maximize the likelihood via its gradient equations with respect to w:\n\u2207w\u2113(w) = n\u2211\ni=1\n{xi(yi \u2212 p(xi;w))}\n= XT (y \u2212 p) = ~0, (5)\nwhere y = (y1, \u00b7 \u00b7 \u00b7 , yn) is a n-dimensional column vector, X = (x1, \u00b7 \u00b7 \u00b7 ,xn) is a n \u00d7 (p + 1) matrix and p = (p(x1;w), \u00b7 \u00b7 \u00b7 , p(xn;w)) is also a n-dimensional column vector. Here we adapt a Newton (or Newton-Raphson) algorithm to maximize problem (4). To this end, we first compute the Hessian matrix H of Eq. (4).\nH(w) = \u22072w\u2113(w) = \u2212 n\u2211\ni=1\n{xixTi p(xi;w)(1 \u2212 p(xi;w))}\n= \u2212XT\u039bX, (6)\nwhere \u039b is a n\u00d7n diagonal matrix with \u039bii = p(xi;w)(1\u2212 p(xi;w)). Starting with w\nold, we thus can obtain a Newton update rule:\nwnew = wold \u2212 [H(wold)]\u22121\u2207w\u2113(w) |w=wold . (7)\nCombining Eq. (5), Eq. (6) and Eq. (7), we can rewrite the Newton update step as follows:\nwnew = wold + (XT\u039bX)\u22121XT (y \u2212 p) = (XT\u039bX)\u22121XT\u039bz, (8)\nwhere z = Xwold + \u039b\u22121(y \u2212 p). We can see that the Newton update step is equivalent to solve the following linear equations with respect to wnew :\n(XT\u039bX)wnew \u2212XT\u039bz = 0. (9)\n3 g e n e sample\nX s a m p le y\nProtein-protein Interaction Network\nL\nModel\nlabel\ndeath (1) or alive (0)\n525 samples\n2/3 training 1/3 testing\nsurvival\nanalysis\ndeath within\none year?\nsurvival analysis death within one year?\n(136Y; 157N) (68Y; 79N)\ntraining\nclassifier\ntesting\nclassifier\nBA\nFig. 1: (A) A novel network-regularized sparse logistic regression framework by considering the difference between the absolute values of the coefficients. It combines the gene expression data, the normalized Laplacian matrix L encoding the protein interaction network and the clinical binary outcome to train a prediction model. (B) Illustration of sample divisions with an application using GBM data from TCGA. Here we collect a total of 525 GBM samples which are randomly divided into two subsets, including 350 training samples and 175 testing samples. We dichotomize the survival time of patients into clinical binary outcomes to formulate it into a classification problem. For each patient, we dichotomize its survival time into a binary outcome based on his/her survival time. For training samples, there are 57 censored patients kept for using in survival analysis. For testing samples, there are 32 censored patients. In the training process, we first remove those censored samples to train the regularized LR models. Then, we re-test all the training data and all the testing data (including censored samples) with the trained models. Based on the predicted LR scores computed via Pr(y = 1|x;w), we divide all the training and testing samples into different risk groups for further survival analysis.\nNote that the diagonal matrix \u039b and the vector z are updated using the following two formulas:\n\u039bii = p(xi;w old)(1\u2212 p(xi;wold)), (10)\nzi = x T i w\nold + yi \u2212 p(xi;wold)\n\u039bii . (11)\nSince XT\u039bX = (\u039b1/2X)T (\u039b1/2X) is a symmetric positive semi-definite matrix, we can obtain the optimal solution of linear system (9) via minimizing the following Quadratic Program (QP) problem:\nwnew = argmin w\n1 2 wT (XT\u039bX)w \u2212wT (XT\u039bz). (12)\nThat is to say that each Newton update step in Eq. (7) is equivalent to minimize a QP problem. To avoid inverse matrix operation, we apply a cyclic coordinate descent method [31], [32] to solve these QP problems. After a few steps of Newton update, we can get the optimal solution of Eq. (4). This process is also known as the iteratively re-weight least squares (IRLS) strategy [33], [34]."
                },
                {
                    "heading": "2.3 A Unified Regularized Logistic Regression Framework",
                    "text": "Here we first introduce the Network-regularized Sparse LR (NSLR) models in a unified regularized LR framework as follows:\nmin w,b {\u2212 1 n \u2113(w, b) +R(w)} (13)\nwith\nR(w) = \u03bb{\u03b1 p\u2211\ni=1\n|wi|+ (1\u2212 \u03b1)\n2 wTMw}, (14)\nwhere n denotes the number of samples and \u03b1 \u2208 [0, 1]. We can solve this framework via the above IRLS strategy. In other words, each Newton update step of Eq. (13) is equivalent to solving a new QP problem. For convenience, we define the coefficient vector w = (w1, \u00b7 \u00b7 \u00b7 ,wp) and \u03b8 = (w1, \u00b7 \u00b7 \u00b7 ,wp, b), where b is the intercept. Then, the QP problem can be defined as follows:\nL(w, b) = 1 2n \u03b8T (XT\u039bX)\u03b8 \u2212 1 n \u03b8T (XT\u039bz) +R(w), (15) where R(w) is a generalized network-regularized penalty. With different M in R(w), the penalty reduces to different cases including Lasso, Elastic net and two networkregularized ones.\nCase 1: M = 0. The penalty function Eq. (14) reduces\nto R(w) = \u03bb p\u2211\ni=1 |wi|, which is the Lasso penalty [35]. We\ndenote this case as Lasso.LR. It has been widely used in bioinformatics [14], [15], [17]. However, Lasso has some potential limits [21]. For example, it fails to select strongly correlated variable group and only selects one variable from such group and ignores the others in it.\nCase 2: M = I. The penalty function Eq. (14) reduces\nto R(w) = \u03bb p\u2211\ni=1 |wi| + (1/2)\u03b7wTw, which is the so-called\n4 Elastic net penalty [21]. We denote this case as Elastic.LR which has adopted in [21]. However, to incorporate the known biological networks, a network-regularized penalty is needed in LR model.\nCase 3: M = L, where L = I \u2212 D1/2AD1/2 is a symmetric normalized Laplacian matrix and A \u2208 Rp\u00d7p is the adjacency matrix for a given prior network (e.g., a protein interaction network). If vertex i and vertex j are connected, then Aij = 1 and Aij = 0 otherwise. The degree of vertex i is defined by di = \u2211p j=1 Aij . Thus, L = (Lij) can be re-written as follows:\nLij =    1, if i = j and di 6= 0, \u2212 Aij\u221a didj , if i and j are connected,\n0, otherwise.\n(16)\nLi and Li [23] applied such a network-regularized regression model for analyzing genomic data. Zhang et al. [36] applied this penalty into the LR model for molecular pathway identification. We denote the Network-regularized LR model as Network.LR. They solve this model using the CVX package which was implemented for solving convex optimization problems [37]. In this paper, compared with the method based on CVX, we develop a simple coordinatewise Newton algorithm to avoid inverse matrix operation. However, the typical network-regularized penalty ignores that the pairwise variables of coefficients (linked in the prior network) may have opposite signs.\nCase 4: M = L\u2217. To consider the signs of the estimated vector w, we can adopt an adaptive network-based penalty R(w):\n\u03bb1\np\u2211\ni=1\n|wi|+ \u03b71 \u2211\ni\u223cj\nAij\n( sign(w\u0302i)wi\u221a\ndi \u2212 sign(w\u0302j)wj\u221a dj\n)2 ,\nwhere \u03bb1 = \u03bb\u03b1, \u03b71 = \u03bb(1\u2212\u03b1)\n2 and w\u0302 is computed using the maximum likelihood estimation for the classical LR model. It can considered as an approximation of |wj | using sign(w\u0302j)wj . At same time, L \u2217 can be re-written as:\nL\u2217ij =    1, if i = j and di 6= 0, \u2212 sign(w\u0302i)sign(w\u0302j)Aij\u221a didj , if i and j are connected,\n0, otherwise.\n(17)\nHere we denote the Adaptive Network-regularized LR as AdaNet.LR.\nIn addition to the four cases, we also consider a novel network-regularized penalty: R(w) = \u03bb\u2211pi=1 |wi| + \u03b7|w|TM |w| [27], to consider the opposite signs of pairwise variables directly. The new penalty can eliminate the sensitivity of the typical one to the signs of the feature correlation. However, it is non-differentiable at zero point and thus the general gradient descent method cannot solve AbsNet.LR directly. In the Section 2.5, we will introduce a clever way to solve it. Note that the AdaNet.LR model can be regarded as an approximation of the AbsNet.LR model."
                },
                {
                    "heading": "2.4 A Learning Algorithm for NSLR",
                    "text": "Here we apply a coordinate-wise Newton algorithm to solve the NSLR framework. We first use a cyclic coordinate\ndescent algorithm [31], [32] to solve the QP problem in Eq. (15). Without loss of generality, we extend the p \u00d7 p matrix M to a (p+ 1)\u00d7 (p+ 1) matrix as follows:\nM \u2190 [ M 0\n0 0\n] .\nLet \u03bb = n \u00b7 \u03bb and w = (w1, \u00b7 \u00b7 \u00b7 ,wp, b). We can obtain a unified form of Eq. (15):\nL(w) = 1 2 wT (XT\u039bX)w \u2212wT (XT\u039bz) +R(w), (18) whereR(w) = \u03bb{\u03b1\u2211pi=1 |wi|+(1\u2212\u03b1)/2wTMw}. We can easily prove that the objective function (18) is a convex one. To minimize it, we first obtain the gradient of w as follows:\n\u2207wL(w) = (XT\u039bX)w\u2212XT\u039bz+\u03b1~s+(1\u2212\u03b1)\u03bbMw, (19) where ~s is a column vector. Let B = XT\u039bX +(1\u2212\u03b1)\u03bbM , and t = XT\u039bz. Furthermore, we can also obtain the gradient with respect to wj :\n\u2202L \u2202wj = Bjjwj+ \u2211\ni6=j\nBjiwi\u2212tj+\u03b1\u03bbsj , j = 1, \u00b7 \u00b7 \u00b7 , p, (20)\nwhere sj = sign(wj) if wj 6= 0, sj = 0 otherwise. Let \u2202L \u2202wj = 0,\nthus we have the following update rule for wj :\nwj = S(tj \u2212 \u2211\ni6=j\nBjiwi, \u03b1\u03bb)/Bjj , (21)\nwhere the soft-thresholding function is defined as\nS(a, \u03c1) = sign(a)(|a| \u2212 \u03c1)+. Similarly, we can also get the update rule for the bias term b (note that b = wp+1). Given k = p+1, then we write the gradient of b:\n\u2202L \u2202b = Bkkwk + \u2211\ni6=k\nBkiwi \u2212 tk. (22)\nLet \u2202L\u2202b = 0. It leads to the update rule for b:\nb = (tk \u2212 \u2211\ni6=k\nBkiwi)/Bkk. (23)\nBriefly, here we employ the iteratively re-weight least squares (IRLS) strategy to solve the unified regularized LR framework. In each iteration, we first update Eq. (10) and Eq. (11) to get a new constrained QP problem (18). Then we apply a cycle coordinate descent algorithm to solve it. This process is repeated until convergence. To summarize, we propose the following Algorithm 1."
                },
                {
                    "heading": "2.5 A Novel Network-regularized LR",
                    "text": "In the subsection, we focus on a novel Network-regularized Logistic Regression model with absolute operation (AbsNet.LR) as follows:\nmin w,b {\u2212 1 n \u2113(w, b) + \u03bb\np\u2211\ni=1\n|wi|+ \u03b7|w|TL|w|}, (24)\n5 Algorithm 1 NSLR learning algorithm\nRequire: Training data D = {X,y}, a penalty matrix M \u2208 R p\u00d7p, two parameters \u03bb and \u03b1. Ensure: w. 1: Initialize w = 0 2: Set \u03bb = n \u00b7 \u03bb where n is the number of samples. 3: repeat 4: Update \u039b using Eq. (10) 5: Update z using Eq. (11) 6: Update B = XT\u039bX + (1\u2212 \u03b1)\u03bbM and t = XT\u039bz 7: for j = 1 to p do 8: Update wj using Eq. (21) 9: end for 10: Update the intercept b using Eq. (23) 11: Compute the criteria J = \u2212(1/n)\u2113(w, b) + R(w) for testing convergence 12: until The objective function J converges a minimum 13: return w\nwhere \u2113(w, b) = n\u2211\ni=1 {yiwTxi \u2212 log(1 + exp(wTxi)} and\n|w| = (w1,w2, \u00b7 \u00b7 \u00b7 ,wp)T . As we have discussed that AdaNet.LR model can be considered as an approximation of it (see Section 2.3 for Case 4). AdaNet.LR applies a convex penalty to replace the one in Eq. (24) enabling it can be solved by NSLR algorithm (Algorithm 1). In the subsection we employ a coordinate-wise Newton algorithm to solve it directly. For each Newton update step, we focus on the following optimization problem:\nL(w, b) = 1 2n wT (XT\u039bX)w \u2212 1 n wT (XT\u039bz) +R(w),\n(25) where R(w) = \u03bb{\u03b1\u2211pi=1 |wi| + (1 \u2212 \u03b1)/2|w|TL|w|}. The coordinate-wise descent strategy does not work for some penalties (e.g., fused lasso) [28]. However, Hastie et al. [28] (see page 112) find the penalty R(w) shows a good property: condition regularity, implying that if the iteration moving along all coordinate directions fails to enable the objective function decrease, then it achieves the minimum [28], [29]. Thus we can minimize Eq. (25) via a cycle coordinate descent method. For convenience, let B = XT\u039bX , t = XT\u039bz and \u03b7 = \u03bb(1 \u2212 \u03b1)/2, we can rewrite the subproblem as follows:\nLk = 1\n2n (Bkkw\n2 k + 2\n\u2211\nj 6=k\nBkjwjwk + \u2211\nj 6=k\n\u2211\ni6=k\nBjiwjwi)\n\u2212 1 n (wktk +\n\u2211\nj 6=k\nwjtj) + \u03bb(|wk|+ \u2211\nj 6=k\n|wj |)\n+\u03b7Lkkw 2 k + 2\u03b7\n\u2211\nj 6=k\nLkj |wj ||wk|\n+\u03b7 \u2211\nj 6=k\n\u2211\ni6=k\nLji|wj ||wi|. (26)\nThen we have the subgradient equation of wk:\n\u2202L \u2202wk = (1/n)(Bkkwk + \u2211\nj 6=k\nBkjwj)\u2212 (1/n)tk + \u03bbsk\n+2\u03b7(Lkkwk + sk \u2211\nj 6=k\nLkj |wj |). (27)\nLet \u2202L\u2202wk = 0 and \u03c4 = Bkk + 2n\u03b7Lkk. We obtain the following update rule:\nwk = S(tk \u2212 \u2211\nj 6=k\nBkjwj , n\u03bb+ 2n\u03b7 \u2211\nj 6=k\nLkj |wj |)/\u03c4. (28)\nSimilarly, we can also get the update rule for the bias term b. The sub-gradient is given by\n\u2202L \u2202b = Bkkwk + \u2211\ni6=k\nBkiwi \u2212 tk, (29)\nwhere k = p + 1, and b = wk. Let \u2202L \u2202b = 0. It leads to the update rule:\nb = (tk \u2212 \u2211\ni6=k\nBkiwi)/Bkk. (30)\nTo summarize, we propose the Algorithm 2 to solve the AbsNet.LR model.\nAlgorithm 2 AbsNet.LR learning algorithm\nRequire: Training data D = {X,y}, a normalized Laplacian matrix L \u2208 Rp\u00d7p, two parameters \u03bb and \u03b1. Ensure: w. 1: Initialize w = 0 2: Set \u03bb = n \u00b7 \u03bb, where n is the number of samples. 3: repeat 4: Update \u039b using Eq. (10) 5: Update z using Eq. (11) 6: Update B = XT\u039bX and t = XT\u039bz 7: for k = 1 to p do 8: Update wk using Eq. (28) 9: end for 10: Update the intercept b using Eq. (30) 11: Compute the criteria J = \u2212(1/n)\u2113(w, b) + R(w) for testing convergence 12: until The objective function J converges to a mimimum 13: return w\nTuning parameter selection. Selecting the regularized parameters {\u03bb, \u03b1} for NSLR and AbsNet.LR is a very important task. Here these parameters are selected via maximizing the index \u2013 Area Under Curve (AUC). Suppose there are n1 positive class samples and n2 negative class samples in a given dataset. Given a binary classifier, {s1, \u00b7 \u00b7 \u00b7 , sn1} are the scores for the positive points and {t1, \u00b7 \u00b7 \u00b7 , tn2} are the scores for the negative points. The AUC of this classifier is calculated using the following formula:\nAUC = 1\nn1n2\nn1\u2211\ni=1\nn2\u2211\nj=1\n( I(si > tj) + 1\n2 I(si = tj)\n) , (31)\nwhere I(si > tj) is 1, if si > tj , and 0 otherwise. However, sometimes the parameter selection may be unnecessary in a real application. For example, we may choose suitable parameters to obtain a solution for the desired degree of sparsity."
                },
                {
                    "heading": "3 SYNTHETIC DATA EXPERIMENTS",
                    "text": "To compare the performance of different regularized LR models, we first generate a sparse coefficient vector w with p = 100 and a bias term b as follows:\nw = [sample(N (0, 1), 40), rep(0, 60)], b = 0,\n6 where sample(N (0, 1), 40) denotes a vector of length 40, whose elements are sampled from a standard normal distribution, and rep(0, 60) denotes a vector of length 60, whose entries are zero. To generate an expression matrix X \u2208 R500\u00d7100, we define a covariance matrix of the variables \u03a3 \u2208 R100\u00d7100, where \u03a3ij = 0.6 when 1 \u2264 i 6= j \u2264 40, \u03a3ii = 1 when i = 1, \u00b7 \u00b7 \u00b7 , 100 and the others entries are zero. It can ensure that the 40 feature vectors are strong correlated in X . We generate the X using the function \u201cmvrnorm\u201d in the MASS R package with parameters mu = 0 and sigma = \u03a3. Furthermore, we also compute the binary response y = (y1, \u00b7 \u00b7 \u00b7 ,y500) based on a Bernoulli distribution with the following formula:\nyi = I(p(xi) \u2265 0.5), p(xi) = 1\n1 + exp(\u2212wTxi \u2212 b) ,\nwhere xi denotes the i-th column of X T . Finally, we also generate a prior network A \u2208 R100\u00d7100, whose node pairs among all the first 40 nodes are connected with probability p11 = 0.3, and the remaining ones are connected with probability p12 = 0.1. Note we set the observed matrix X\u2217 = X + \u03b3\u01eb, where the elements of \u01eb are randomly sampled from a standard normal distribution, and \u03b3 = 3 is used to control the signal-to-noise ratio. The observed X\u2217 contains 500 samples with 100 features. We randomly select 300 samples as the training samples and the remaining samples as the testing samples. We test all regularized LR models on the synthetic training data using 5-fold crossvalidation strategy to select the optimal parameters. We repeat the simulations over 50 times. We then compute the average AUCs of 50 experiments about classification on the testing synthetic data. To compare the performance on variable selection, we also calculate the average sensitivity/specificity scores with respect to the selected variables (nonzero of w), and the average numbers of edges of the selected variables in the prior network.\nHere we define the sensitivity as the percentage of true non-zero entries discovered, and the specificity as the percentage of true zero entries discovered, respectively. The overall results are shown in Table 1. Generally, these network-based regularized LR models (especially AbsNet.LR) are superior to other algorithms with respect to AUCs. Moreover, AbsNet.LR obtains the relatively higher sensitivity in the selected variables, and relatively larger number of edges of them in the prior network compared to others."
                },
                {
                    "heading": "4 APPLICATION TO GBM DATA",
                    "text": ""
                },
                {
                    "heading": "4.1 The GBM Data",
                    "text": "We download the level 3 gene expression data (Broad Institute HT-HG-U133A platform) and clinical data of GBM\nfrom the TCGA database [3]. We employ two alternative methods to impute the missing data in gene expression profiling: (1) the k-nearest neighbor algorithm (knn) method implemented as the \u201cimpute\u201d R package; (2) the mean of considered genes. We find that the two methods only have little effect on the final results. Thus, we simply adopt the second method to impute the missing data. We obtain the gene expression data of 1,2042 genes across 525 patients. Furthermore, we standardize the expression of each gene across all samples using the \u201cscale\u201d function in R. We also download a protein interaction network (PPI) data from Pathway Commons [30]. Finally, we obtain a gene expression data with 1,0963 genes and a PPI network with 24,8403 edges. Here our goal of this application is to predict the survival risk of GBM patients (Fig. 1A). We first dichotomize the survival time of patients into a binary outcome through a designated cutoff. Here we consider one year as the cutoff time to balance the number of positive and negative samples (Fig. 1B and Table 2).\nWe apply the univariate Cox proportional hazard model [12] to assess and filter the 1,0963 genes of GBM gene expression data (Table 2). Finally, we obtain 2001 genes with P < 0.05 for further analysis. Only those genes are used in all the regularized LR models."
                },
                {
                    "heading": "4.2 Results of the GBM Data",
                    "text": "We first randomly select 2/3 samples (n = 350) as the training samples and 1/3 samples (n = 175) as the testing samples (Fig. 1B). We consider \u03bb \u2208 {0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4} and \u03b1 \u2208 {0.05, 0.1, 0.2, 0.5, 0.7, 0.9} to form a total of 42 pair parameter sets. We first learn the different regularized LR methods on the GBM training data using 5-fold cross-validation. Then we test all the methods on the GBM testing data. We find the regularized LR models are superior to the typical LR model (Table 3). Generally, Lasso.LR is inferior to other regularized methods (Elastic.LR, Network.LR, AdaNet.LR and AbsNet.LR) whose results are relatively consistent. However, AbsNet.LR, AdaNet.LR and Network.LR identify a gene set with more edges via integrating the prior network data. All these results imply the importance of integrating the prior protein interaction network data to improve the prediction accuracy and biological interpretation. Therefore, we only focus on the result analysis of AbsNet.LR which obtains AUC = 0.6627 with \u03bb = 0.3 and \u03b1 = 0.1, and a gene set with 157 genes.\nTo evaluate the biological relevance of the identified gene set, we apply the DAVID online web tool [38] to perform the Gene Ontology (GO) biological processes enrichment analysis. The GO biological processes with p-values < 0.05 are selected as significant ones. We find the genes is related to some important ones including positive regulation of kinase activity, synaptic transmission glutamatergic and inositol metabolic process. The identified biological processes\n7 1aturday Seminar KM curve for training data KM curve for testing dataA B E F NETO2 RPS28 CETP ZNF208 MTHFSD SMYD3 LDB1 KCNG1 NEU2 EDG5 ELSPBP1 CACNA1D TFR2 RPL39L HOXD11 HOXD10 PITX1 SRD5A2L CLEC5A DDIT3 HEMK1 CRELD1 BDH1 ETNK2 PRKAG2 AKR1B1 GABRA4 PPFIA4 EEF1A2\nNPTXR\nCLEC5A TFR2 RPL39L BDH1 PITX1 HOXD11 HOXD10 DDIT3 AKR1B1 SMYD3 EDG5 NEU2 GABRA4 KCNG1 LDB1 ZNF208 NETO2 CETP RPS28 ELSPBP1 CACNA1D SRD5A2L ETNK2 HEMK1 CRELD1 MTHFSD PRKAG2 PPFIA4 NPTXR EEF1A2\np<0.002 (testing data)p<4.0e-06 (training data)\ntraining data testing data\n0 .0\n0 .2\n0 .4\n0 .6\n0 .8\n1 .0\n0 .0\n0 .2\n0 .4\n0 .6\n0 .8\n1 .0\nhigh int low high int low 2\n0\n4\n0\n6 0\n8\n0\na g\ne (\ny e\na rs\n)\n2 0\n4 0\n6 0\n8 0\nhigh int low high int low\n0 1 2 3 4\ntime (years) time (years)\ns u\nrv iv\na l\n0 1 2 3 4\nC D\nFig. 2: Performance evaluation in the training and testing data. We divide the GBM patients into different groups according to their estimated LR scores. (A) and (B) Kaplan-Meier (KM) survival curves for for low-, intermediate (int)-, and high-risk groups. P -values are computed using the log-rank test. (C) and (D) Age distribution in the three GBM patient groups with p-value <4.0e-06 for training data and p-value < 0.002 for testing data (using an analysis of variance model ). (E) and (F) The expression heat maps of the training and testing data based on the top 30 genes.\nare consistent with recent literature reports. For example \u2018the positive regulation of protein kinase activity\u2019 process is well-known to be related to GBM and its activation is a key mechanism for GBM development [39].\nFurthermore, we extract the top 30 genes corresponding to the largest absolute values of the estimated coefficients (by the AbsNet.LR). Based on the top 30 genes, we re-train the typical LR model in the training data. In the independent testing data, LR obtain an AUC of 0.6727. However, on average, only AUC of 0.55 are obtained with randomly select 30 genes. Next, based on the built LR model, we re-test all the training data and all testing data (Fig. 1B) and based on the predicted LR scores computed via Pr(y = 1|x;w), we divide all the training and testing samples into three groups, called low-, intermediate- and high-risk ones, respectively (Fig. 2A and B). We can also see the expression heat maps of the top 30 genes with the three groups (Fig. 2E and 2F). We find that some genes in the specific group are high expressed, while some are low expressed in the other group. Interestingly, we also find that these top genes form a set of sub-networks (e.g., Fig. 3A and B), which are sugested to be related with GBM [40], [41], [42], [43].\n8\nFurthermore, we study whether the defined groups have significant relationships with some other clinical variables (e.g., age). We find that the patients of high-risk group are old than those in low and intermediate-risk group on both training and testing data (Fig. 2C and D). All these results show that the identified gene set is related to the survival outcome of GBM patients."
                },
                {
                    "heading": "5 APPLICATION TO LUNG CANCER DATA",
                    "text": ""
                },
                {
                    "heading": "5.1 Non-Small Cell Lung Cancer Expression Data",
                    "text": "We download two subtypes of non-small cell lung cancer gene expression datasets from TCGA [4], including Lung adenocarcinoma (LUAD) with 350 samples and Lung squamous-cell carcinoma (LUSC) with 315 samples. Here we consider the level 3 expression data of TCGA which is quantified at the gene and transcript levels using RNA-Seq data by an expectation maximization method [44]. Then the expression values are log2-transformed. We first process the LUAD and LUSC gene expression data to keep the genes that are appeared in the KEGG pathway database from the Molecular Signatures Database (MSigDB) [45]. Finally, we obtain 3230 genes related with 151 KEGG pathways. Here we consider each KEGG pathway as a fully connected subnetwork to build a prior network with 143438 edges."
                },
                {
                    "heading": "5.2 Results of the Lung Cancer Data",
                    "text": "Here we apply all the methods to the lung cancer data to identify subtype-specific biomarkers. Compared to the integration of PPI network in the GBM data, we focus on the KEGG pathway as the prior information. We randomly select all the samples of 2/3 as the training samples and remaining ones are as the independent test samples. We show the results from all the regularized LR models in the independent training set (Table 4). In total, 69 genes are selected by the AbsNet.LR. All the genes identified by other regularized LR models (Lasso.LR, Elastic.LR, Network.LR and AdaNet.LR) are included in the 69 genes. The network-regularized LR models give similar results, which are superior to that of typical LR and Lasso.LR models. In addition, compared to the other network-regularized LR models, AbsNet.LR identifies a gene set with more edges in the KEGG pathway network.\nFurthermore, we evaluate the biological relevance of the identified 69 genes using the DAVID online web tool [38] and find several significantly enriched KEGG pathways relating to lung cancer, including metabolism of xenobiotics by cytochrome P450 (GSTA2, CYP3A5, CYP2F1, CYP3A7, CYP2C9, CYP2C8, UGT2A1) [46], linoleic acid metabolism (CYP3A5, CYP3A7, CYP2C9, AKR1B10, CYP2C8) [47] and retinol metabolism (CYP3A5, CYP3A7, CYP2C9, CYP2C8, UGT2A1, RPE65) [48] and so on."
                },
                {
                    "heading": "6 COMPUTING PLATFORM AND RUNNING TIME",
                    "text": "All scripts were run on a desktop personal computer with an Inter(R) Core(TM) i7-4770 CPU@ 3.4 GHz and 16 GB memory running Windows OS and R 3.2.5. The code is available at http://page.amss.ac.cn/shihua.zhang/. For the synthetic data experiments, the memory used is about 140 MB and the running time is about 0.7 hours. For the application to GBM data, the memory used is about 150 MB and the running time is about 0.8 hours. For the application to lung cancer data, the memory used is about 160 MB and the running time is about 1 hour."
                },
                {
                    "heading": "7 CONCLUSION",
                    "text": "In this paper, we first introduce the typical networkregularized LR models with others in a unified framework. Although the typical network-regularized LR model can incorporate such prior information to get better biological interpretability, it fails to focus on the opposite effect of variables with different signs. To solve this limitation, we adopt a novel network-regularized penalty R = \u03bb\u2016w\u20161 + \u03b7|w|TL|w| into LR model (denoted as AbsNet.LR). However, the novel penalty is not differentiable\n9 at the zero point, enforcing it is hard to be solved using the typical gradient descent method. To this end, we first adapt the adaptive network-regularized penalty (note that it is convex) to approximate this novel network-regularized penalty and develop an adaptive network-regularized LR (AdaNet.LR) model which can be solved easily using the NSLR algorithm (Algorithm 1). We further find that the novel network-regularized penalty has a good property \u2013 condition regularity [28], [29], which inspires us to solve it via a cycle coordinate-wise Newton algorithm efficiently. We note that the binary LR-based classification models can be easily extended to the multiclass or multinomial classification problem [34], [49].\nApplications to the synthetic data show that the present methods are more effective compared to the typical ones. We also apply them to the GBM expression and a protein interaction network data to predict mortality probability of patients within one year. In addition, we apply them to a lung cancer expression data of two subtypes to identify subtype-specific biomarkers. We find a gene set with 69 genes which are tightly connected in the prior network. Functional enrichment analysis of these genes discovers a few KEGG pathways relating to the lung cancer clearly."
                },
                {
                    "heading": "ACKNOWLEDGMENT",
                    "text": "Shihua Zhang and Juan Liu are the corresponding authors of this paper. Wenwen Min would like to thank the support of National Center for Mathematics and Interdisciplinary Sciences, Academy of Mathematics and Systems Science, CAS during his visit. This work was supported by the National Science Foundation of China [61379092, 61422309, 61621003], the Strategic Priority Research Program of the Chinese Academy of Sciences (CAS) (XDB13040600), the Outstanding Young Scientist Program of CAS, CAS Frontier Science Research Key Project \u2013 Top Young Scientist (No. QYZDB-SSW-SYS008) and the Key Laboratory of Random Complex Structures and Data Science, CAS."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Breast tumor subgroups reveal diverse clinical prognostic power",
                    "authors": [
                        "Z. Liu",
                        "X.-S. Zhang",
                        "S. Zhang"
                    ],
                    "venue": "Scientific reports, vol. 4, 4002; DOI:10.1038/srep04002, 2014.",
                    "year": 2014
                },
                {
                    "title": "Comparative pan-cancer DNA methylation analysis reveals cancer common and specific patterns",
                    "authors": [
                        "X. Yang",
                        "L. Gao",
                        "S. Zhang"
                    ],
                    "venue": "Briefings in Bioinformatics, pp. 1\u201313; bbw063, 2016.",
                    "year": 2016
                },
                {
                    "title": "The somatic genomic landscape of glioblastoma",
                    "authors": [
                        "C. Benz",
                        "J. Barnholtz-Sloan",
                        "W. Barrett",
                        "Q. Ostrom",
                        "Y. Wolinsky",
                        "K. Black",
                        "B. Bose",
                        "P. Boulos",
                        "M. Boulos",
                        "J. Brown"
                    ],
                    "venue": "Cell, vol. 155, no. 2, pp. 462\u2013 477, 2013.",
                    "year": 2013
                },
                {
                    "title": "Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin",
                    "authors": [
                        "K.A. Hoadley",
                        "C. Yau",
                        "D.M. Wolf",
                        "A.D. Cherniack",
                        "D. Tamborero",
                        "S. Ng",
                        "M.D.M. Leiserson",
                        "B. Niu",
                        "M.D. Mclellan",
                        "V. Uzunangelov"
                    ],
                    "venue": "Cell, vol. 158, no. 4, pp. 929\u2013944, 2014.",
                    "year": 2014
                },
                {
                    "title": "Principles and methods of integrative genomic analyses in cancer",
                    "authors": [
                        "V.N. Kristensen",
                        "O.C. Lingj\u00e6rde",
                        "H.G. Russnes",
                        "H.K.M. Vollan",
                        "A. Frigessi",
                        "A.-L. B\u00f8rresen-Dale"
                    ],
                    "venue": "Nature Reviews Cancer, vol. 14, no. 5, pp. 299\u2013313, 2014.",
                    "year": 2014
                },
                {
                    "title": "Efficient methods for identifying mutated driver pathways in cancer",
                    "authors": [
                        "J. Zhao",
                        "S. Zhang",
                        "L.-Y. Wu",
                        "X.-S. Zhang"
                    ],
                    "venue": "Bioinformatics, vol. 28, no. 22, pp. 2940\u20132947, 2012.",
                    "year": 2012
                },
                {
                    "title": "Discovery of multi-dimensional modules by integrative analysis of cancer genomic data",
                    "authors": [
                        "S. Zhang",
                        "C.-C. Liu",
                        "W. Li",
                        "H. Shen",
                        "P.W. Laird",
                        "X.J. Zhou"
                    ],
                    "venue": "Nucleic acids research, vol. 40, no. 19, pp. 9397\u20139391, 2012.",
                    "year": 2012
                },
                {
                    "title": "A novel computational framework for simultaneous integration of multiple types of genomic data to identify microRNA-gene regulatory modules",
                    "authors": [
                        "S. Zhang",
                        "Q. Li",
                        "J. Liu",
                        "X.J. Zhou"
                    ],
                    "venue": "Bioinformatics, vol. 27, no. 13, pp. i401\u2013i409, 2011.",
                    "year": 2011
                },
                {
                    "title": "Integrated modeling of clinical and gene expression information for personalized prediction of disease outcomes",
                    "authors": [
                        "J. Pittman",
                        "E. Huang",
                        "H. Dressman",
                        "C.-F. Horng",
                        "S.H. Cheng",
                        "M.- H. Tsou",
                        "C.-M. Chen",
                        "A. Bild",
                        "E.S. Iversen",
                        "A.T. Huang"
                    ],
                    "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 22, pp. 8431\u20138436, 2004.",
                    "year": 2004
                },
                {
                    "title": "Network-based survival analysis reveals subnetwork signatures for predicting outcomes of ovarian cancer treatment",
                    "authors": [
                        "W. Zhang",
                        "T. Ota",
                        "V. Shridhar",
                        "J. Chien",
                        "B. Wu",
                        "R. Kuang"
                    ],
                    "venue": "Plos Computational Biology, vol. 9, no. 3, pp. 760\u2013766, 2013.",
                    "year": 2013
                },
                {
                    "title": "Regularization paths for cox\u2019s proportional hazards model via coordinate descent.",
                    "authors": [
                        "N. Simon",
                        "J. Friedman",
                        "T. Hastie",
                        "R. Tibshirani"
                    ],
                    "venue": "Journal of Statistical Software,",
                    "year": 2011
                },
                {
                    "title": "Regression models and life-tables",
                    "authors": [
                        "D.R. Cox"
                    ],
                    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), vol. 34, no. 2, pp. 187\u2013 220, 1972.",
                    "year": 1972
                },
                {
                    "title": "Logistic regression for disease classification using microarray data: model selection in a large p and small n case",
                    "authors": [
                        "J. Liao",
                        "K.-V. Chin"
                    ],
                    "venue": "Bioinformatics, vol. 23, no. 15, pp. 1945\u20131951, 2007.",
                    "year": 1945
                },
                {
                    "title": "A simple and efficient algorithm for gene selection using sparse logistic regression.",
                    "authors": [
                        "S.K. Shevade",
                        "S.S. Keerthi"
                    ],
                    "venue": "Bioinformatics, vol. 19,",
                    "year": 2003
                },
                {
                    "title": "Gene selection in cancer classification using sparse logistic regression with bayesian regularization",
                    "authors": [
                        "G.C. Cawley",
                        "N.L.C. Talbot"
                    ],
                    "venue": "Bioinformatics, vol. 22, no. 19, pp. 2348\u20132355, 2006.",
                    "year": 2006
                },
                {
                    "title": "Genomewide association analysis by lasso penalized logistic regression",
                    "authors": [
                        "T.T. Wu",
                        "Y.F. Chen",
                        "T. Hastie",
                        "E. Sobel",
                        "K. Lange"
                    ],
                    "venue": "Bioinformatics, vol. 25, no. 6, pp. 714\u2013721, 2009.",
                    "year": 2009
                },
                {
                    "title": "Kabn, \u201cClassification of mislabelled microarrays using robust sparse logistic regression.",
                    "authors": [
                        "A.J. Bootkrajang"
                    ],
                    "venue": "Bioinformatics, vol. 29,",
                    "year": 2013
                },
                {
                    "title": "Advanced colorectal neoplasia risk stratification by penalized logistic regression",
                    "authors": [
                        "Y. Lin",
                        "M. Yu",
                        "S. Wang",
                        "R. Chappell",
                        "T.F. Imperiale"
                    ],
                    "venue": "Statistical Methods in Medical Research, vol. 25, no. 4, pp. 1677\u20131691, 2016.",
                    "year": 2016
                },
                {
                    "title": "A novel adaptive penalized logistic regression for uncovering biomarker associated with anti-cancer drug sensitivity",
                    "authors": [
                        "H. Park",
                        "Y. Shiraishi",
                        "S. Imoto",
                        "S. Miyano"
                    ],
                    "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics, no. 1, pp. 1, PrePrints, 2016.",
                    "year": 2016
                },
                {
                    "title": "Regularized logistic regression with adjusted adaptive elastic net for gene selection in high dimensional cancer classification",
                    "authors": [
                        "Z.Y. Algamal",
                        "M.H. Lee"
                    ],
                    "venue": "Computers in Biology and Medicine, vol. 67, pp. 136\u2013145, 2015.",
                    "year": 2015
                },
                {
                    "title": "Regression shrinkage and selection via the lasso: a retrospective",
                    "authors": [
                        "R. Tibshirani"
                    ],
                    "venue": "Journal of the Royal Statistical Society, vol. 73, no. 3, pp. 273\u2013282, 2011.",
                    "year": 2011
                },
                {
                    "title": "Network-constrained regularization and variable selection for analysis of genomic data",
                    "authors": [
                        "C. Li",
                        "H. Li"
                    ],
                    "venue": "Bioinformatics, vol. 24, no. 9, pp. 1175\u20131182, 2008.",
                    "year": 2008
                },
                {
                    "title": "Variable selection and regression analysis for graphstructured covariates with an application to genomics",
                    "authors": [
                        "\u2014\u2014"
                    ],
                    "venue": "The annals of applied statistics, vol. 4, no. 3, pp. 1498\u20131516, 2010.",
                    "year": 2010
                },
                {
                    "title": "Joint network and node selection for pathway-based genomic data analysis.",
                    "authors": [
                        "S. Zhe",
                        "S.A. Naqvi",
                        "Y. Yang",
                        "Y. Qi"
                    ],
                    "venue": "Bioinformatics, vol. 29,",
                    "year": 2013
                },
                {
                    "title": "Graph regularized nonnegative matrix factorization for data representation",
                    "authors": [
                        "D. Cai",
                        "X. He",
                        "J. Han",
                        "T.S. Huang"
                    ],
                    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548\u20131560, 2011.",
                    "year": 2011
                },
                {
                    "title": "L0-norm sparse graph-regularized svd for biclustering",
                    "authors": [
                        "W. Min",
                        "J. Liu",
                        "S. Zhang"
                    ],
                    "venue": "arXiv preprint arXiv:1603.06035, 2016.",
                    "year": 2016
                },
                {
                    "title": "Statistical learning with sparsity: the lasso and generalizations",
                    "authors": [
                        "T. Hastie",
                        "R. Tibshirani",
                        "M. Wainwright"
                    ],
                    "year": 2015
                },
                {
                    "title": "Convergence of a block coordinate descent method for nondifferentiable minimization",
                    "authors": [
                        "P. Tseng"
                    ],
                    "venue": "Journal of Optimization Theory and Applications, vol. 109, no. 3, pp. 475\u2013494, 2001.",
                    "year": 2001
                },
                {
                    "title": "Pathway  10 commons, a web resource for biological pathway data.",
                    "authors": [
                        "E.G. Cerami",
                        "B.E. Gross",
                        "E. Demir",
                        "I. Rodchenkov",
                        "O. Babur",
                        "N. Anwar",
                        "N. Schultz",
                        "G.D. Bader",
                        "C. Sander"
                    ],
                    "venue": "Nucleic Acids Research,",
                    "year": 2010
                },
                {
                    "title": "Pathwise coordinate optimization",
                    "authors": [
                        "J. Friedman",
                        "T. Hastie",
                        "H. H\u00f6fling",
                        "R. Tibshirani"
                    ],
                    "venue": "The Annals of Applied Statistics, vol. 1, no. 2, pp. 302\u2013332, 2007.",
                    "year": 2007
                },
                {
                    "title": "Regularization paths for generalized linear models via coordinate descent",
                    "authors": [
                        "J. Friedman",
                        "T. Hastie",
                        "R. Tibshirani"
                    ],
                    "venue": "Journal of statistical software, vol. 33, no. 1, pp. 1\u201322, 2010.",
                    "year": 2010
                },
                {
                    "title": "Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives",
                    "authors": [
                        "P.J. Green"
                    ],
                    "venue": "J.royal Stat.soc.ser.b, vol. 46, no. 2, pp. 149\u2013192, 1984.",
                    "year": 1984
                },
                {
                    "title": "Sparse multinomial logistic regression: Fast algorithms and generalization bounds",
                    "authors": [
                        "B. Krishnapuram",
                        "L. Carin",
                        "M.A. Figueiredo",
                        "A.J. Hartemink"
                    ],
                    "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 27, no. 6, pp. 957\u2013968, 2005.",
                    "year": 2005
                },
                {
                    "title": "Regression shrinkage and selection via the lasso",
                    "authors": [
                        "R. Tibshirani"
                    ],
                    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), vol. 58, no. 1, pp. 267\u2013288, 1996.",
                    "year": 1996
                },
                {
                    "title": "Molecular pathway identification using biological network-regularized logistic models",
                    "authors": [
                        "W. Zhang",
                        "Y. Wan",
                        "G.I. Allen",
                        "K. Pang",
                        "M.L. Anderson",
                        "Z. Liu"
                    ],
                    "venue": "BMC genomics, vol. 14, no. Suppl 8, pp. 1\u20138, 2013.",
                    "year": 2013
                },
                {
                    "title": "CVX: Matlab software for disciplined convex programming, version 2.1",
                    "authors": [
                        "M. Grant",
                        "S. Boyd"
                    ],
                    "venue": "Mar. 2014.",
                    "year": 2014
                },
                {
                    "title": "Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources",
                    "authors": [
                        "D.W. Huang",
                        "B.T. Sherman",
                        "R.A. Lempicki"
                    ],
                    "venue": "Nature protocols, vol. 4, no. 1, pp. 44\u201357, 2009.",
                    "year": 2009
                },
                {
                    "title": "Protein kinase d2 is a novel regulator of glioblastoma growth and tumor formation",
                    "authors": [
                        "N. Azoitei",
                        "A. Kleger",
                        "N. Schoo",
                        "D.R. Thal",
                        "C. Brunner",
                        "G.V. Pusapati",
                        "A. Filatova",
                        "F. Genze",
                        "P. M\u00f6ller",
                        "T. Acker"
                    ],
                    "venue": "Neuro-oncology, vol. 13, no. 7, pp. 710\u2013724, 2011.",
                    "year": 2011
                },
                {
                    "title": "Glioma tumor grade correlates with parkin depletion in mutant p53-linked tumors and results from loss of function of p53 transcriptional  activity.",
                    "authors": [
                        "J. Viotti",
                        "E. Duplan",
                        "C. Caillava",
                        "J. Condat",
                        "T. Goiran",
                        "C. Giordano",
                        "Y. Marie",
                        "A. Idbaih",
                        "J.Y. Delattre",
                        "J. Honnorat"
                    ],
                    "venue": "Oncogene, vol. 33,",
                    "year": 2013
                },
                {
                    "title": "Glioblastoma cells inhibit astrocytic p53-expression favoring cancer malignancy",
                    "authors": [
                        "D. Biasoli"
                    ],
                    "venue": "Oncogenesis, vol. 3, no. e123, 2014.",
                    "year": 2014
                },
                {
                    "title": "Targeting microrna- 23a to inhibit glioma cell invasion via hoxd10.",
                    "authors": [
                        "X. Hu",
                        "D. Chen",
                        "Y. Cui",
                        "Z. Li",
                        "J. Huang"
                    ],
                    "venue": "Scientific Reports,",
                    "year": 2013
                },
                {
                    "title": "Transferrin receptor 2 is frequently and highly expressed in glioblastomas",
                    "authors": [
                        "A. Calzolari",
                        "L.M. Larocca",
                        "S. Deaglio",
                        "V. Finisguerra",
                        "A. Boe",
                        "C. Raggi",
                        "L. Ricci-Vitani",
                        "F. Pierconti",
                        "F. Malavasi",
                        "R.D. Maria"
                    ],
                    "venue": "Translational Oncology, vol. 3, no. 2, pp. 123\u2013134, 2010.",
                    "year": 2010
                },
                {
                    "title": "What is the expectation maximization algorithm?",
                    "authors": [
                        "C.B. Do",
                        "S. Batzoglou"
                    ],
                    "venue": "Nature biotechnology,",
                    "year": 2008
                },
                {
                    "title": "Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles",
                    "authors": [
                        "A. Subramanian",
                        "P. Tamayo",
                        "V.K. Mootha",
                        "S. Mukherjee",
                        "B.L. Ebert",
                        "M.A. Gillette",
                        "A. Paulovich",
                        "S.L. Pomeroy",
                        "T.R. Golub",
                        "E.S. Lander"
                    ],
                    "venue": "Proceedings of the National Academy of Sciences, vol. 102, no. 43, pp. 15 545\u201315 550, 2005.",
                    "year": 2005
                },
                {
                    "title": "Serum free fatty acid biomarkers of lung cancer",
                    "authors": [
                        "J. Liu",
                        "P.J. Mazzone",
                        "J.P. Cata",
                        "A. Kurz",
                        "M. Bauer",
                        "E.J. Mascha",
                        "D.I. Sessler"
                    ],
                    "venue": "Chest, vol. 146, no. 3, pp. 670\u2013679, 2014.",
                    "year": 2014
                },
                {
                    "title": "Abnormal expression of genes that regulate retinoid metabolism and signaling in nonsmall-cell lung cancer",
                    "authors": [
                        "E.S. Kuznetsova",
                        "O.L. Zinovieva",
                        "N.Y. Oparina",
                        "M.M. Prokofjeva",
                        "P.V. Spirin",
                        "I.A. Favorskaya",
                        "I.B. Zborovskaya",
                        "N.A. Lisitsyn",
                        "V.S. Prassolov",
                        "T.D. Mashkova"
                    ],
                    "venue": "Molecular Biology, vol. 50, no. 2, pp. 220\u2013 229, 2016.",
                    "year": 2016
                },
                {
                    "title": "Sparse multinomial logistic regression via bayesian L1 regularisation",
                    "authors": [
                        "G.C. Cawley",
                        "N.L. Talbot",
                        "M. Girolami"
                    ],
                    "venue": "Advances in Neural Information Processing Systems, vol. 19, pp. 209\u2013216, 2007.",
                    "year": 2007
                }
            ],
            "id": "SP:ccc9e6935ad5d3ad567b073db121f6e5a06c5546",
            "authors": [
                {
                    "name": "Wenwen Min",
                    "affiliations": []
                },
                {
                    "name": "Juan Liu",
                    "affiliations": []
                },
                {
                    "name": "Shihua Zhang",
                    "affiliations": []
                }
            ],
            "abstractText": "Molecular profiling data (e.g., gene expression) has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression (LR) framework with regularized term \u03bb\u2016w\u20161 + \u03b7wMw, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different M . This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated wi and wj have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty \u03bb\u2016w\u20161 + \u03b7|w|M |w| to consider the difference between the absolute values of the coefficients. And we develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency.",
            "title": "Network-regularized Sparse Logistic Regression Models for Clinical Risk Prediction and Biomarker Discovery"
        }
    },
    "96599676": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "Features based on convolutional networks (CNNs) [29] have now led to the best results on a range of vision tasks: image classification [28, 36], object segmentation and detection [18, 22], action classification [35], pose estimation [37] and fine-grained category recognition [44, 6]. We have thus moved from the era of HOG and SIFT to the era of convolutional network features. Therefore, understanding these features and how best to exploit them is of wide applicability.\nTypically, recognition algorithms use the output of the last layer of the CNN. This makes sense when the task is assigning category labels to images or bounding boxes: the last layer is the most sensitive to category-level semantic information and the most invariant to \u201cnuisance\u201d variables such as pose, illumination, articulation, precise location and so on. However, when the task we are interested in is finer-\ngrained, such as one of segmenting the detected object or estimating its pose, these nuisance variables are precisely what we are interested in. For such applications, the top layer is thus not the optimal representation.\nThe information that is generalized over in the top layer is present in intermediate layers, but intermediate layers are also much less sensitive to semantics. For instance, bar detectors in early layers might localize bars precisely, but cannot discriminate between bars that are horse legs and bars that are tree trunks. This observation suggests that reasoning at multiple levels of abstraction and scale is necessary, mirroring other problems in computer vision where reasoning across multiple levels has proven beneficial. For example, in optical flow, coarse levels of the image pyramid are good for correspondence, but finer levels are needed for accurate measurement, and a multiscale strategy is used to get the best of both worlds [7].\nIn this paper, we think of the layers of a convolutional network as a non-linear counterpart of the image pyramids used in optical flow and other vision tasks. Our hypothesis is that the information of interest is distributed over all levels of the CNN and should be exploited in this way. We define the \u201chypercolumn\u201d at a given input location as the outputs of all units above that location at all layers of the CNN, stacked into one vector. (Because adjacent layers are strongly correlated, in practice we need not consider all layers but can simply sample a few.) Figure 1 shows a visualization of the idea. We borrow the term \u201chypercolumn\u201d from neuroscience, where it is used to describe a set of V1 neurons sensitive to edges at multiple orientations and multiple frequencies arranged in a columnar structure [24]. However, our hypercolumn includes not just edge detectors but also more semantic units and is thus a more general notion.\nWe show the utility of the hypercolumn representation\nar X\niv :1\n41 1.\n57 52\nv2 [\ncs .C\nV ]\n2 5\non two kinds of problems that require precise localization. The first problem is simultaneous detection and segmentation (SDS) [22], where the aim is to both detect and segment every instance of an object category in the image. The second problem deals with detecting an object and localizing its parts. We consider two variants of this: one, locating the keypoints [43], and two, segmenting out each part [41, 40, 3, 31].\nWe present a general framework for tackling these and other fine-grained localization tasks by framing them as pixel classification and using hypercolumns as pixel descriptors. We formulate our entire system as a neural network, allowing end-to-end training for particular tasks simply by changing the target labels. Our empirical results are:\n1. On SDS, the previous state-of-the-art was 49.7 mean APr [22]. Substituting hypercolumns into the pipeline of [22] improves this to 52.8. We also propose a more efficient pipeline that allows us to use a larger network, pushing up the performance to 60.0.\n2. On keypoint prediction, we show that a simple keypoint prediction scheme using hypercolumns achieves a 3.3 point gain in the APK metric [43] over prior approaches working with only the top layer features [20]. While there isn\u2019t much prior work on labeling parts of objects, we show that the hypercolumn framework is significantly better (by 6.6 points on average) than a strong baseline based on the top layer features."
                },
                {
                    "heading": "2. Related work",
                    "text": "Combining features across multiple levels: Burt and Adelson introduced Laplacian pyramids [8], a representation that is widely used in computer vision. Koenderink and van Doorn [27] used \u201cjets\u201d, which are sets of partial derivatives of intensity up to a particular order, to estimate\nedge orientation, curvature, etc. Malik and Perona [32] used the output of a bank of filters as a representation for texture discrimination. This representation also proved useful for optical flow [39] and stereo [26]. While the filter banks in these works cover multiple scales, they are still restricted to simple linear filters, whereas many of the features in the hypercolumn representation are highly non-linear functions of the image.\nThere has also been work in convolutional networks that combines multiple levels of abstraction and scale. Farabet et al. [15] combine CNN outputs from multiple scales of an image to do semantic segmentation. Tompson et al. [37] use a similar idea for detecting parts and estimating pose. However, the features being combined still come from the same level of the CNN and hence have similar invariance. Sermanet et al. [34] combine subsampled intermediate layers with the top layer for pedestrian detection. In contrast, since we aim for precise localization, we maintain the high resolution of the lower layers and upsample the higher layers instead. In contemporary work, Long et al. [30] also use multiple layers for their fully convolutional semantic segmentation system.\nDetection and segmentation: The task of simultaneous detection and segmentation task, introduced in [22], requires one to detect and segment every instance of a category in the image. SDS differs from classical bounding box detection in its requirement of a segmentation and from classical semantic segmentation in its requirement of separate instances. There has been other prior work on segmenting out instances of a category, mostly starting from bounding box detections. Borenstein and Ullman [4] first suggested the idea of using class-specific knowledge for segmentation. Yang et al. [42] use figure ground masks associated with DPM detectors [16] to segment out detected objects and reason about depth orderings. Parkhi et al. [33] use color models extracted from the detected cat and dog heads to segment them out. Dai and Hoiem [12] generalize this reasoning to all categories. Fidler et al. [17] and Dong et al. [13] combine object detections from DPM [16] with semantic segmentation outputs from O2P [9] to improve both systems. Current leading methods use CNNs to score bottom-up object proposals, both for object detection [18] and for SDS [22, 11].\nPose estimation and part labeling: Current best performers for pose estimation are based on CNNs. Toshev and Szegedy [38] use a CNN to regress to keypoint locations. Tompson et al. [37] show large improvements over stateof-the-art by predicting a heatmap for each keypoint, where the value of the heatmap at a location is the probability of the keypoint at that location. These algorithms show results in the setting where the rough location of the person\nis known. Yang and Ramanan [43] propose a more realistic setting where the location of the person is not known and one has to both detect the person and identify his/her keypoints. Gkioxari et al. [21] show some results in this setting using HOG-based detectors, but in their later work [20] show large gains using CNNs.\nRelated to pose estimation is the task of segmenting out the different parts of a person, a task typically called \u201cobject parsing\u201d. Yamaguchi et al. [41, 40] parse fashion photographs into clothing items. There has also been work on parsing pedestrians [3, 31]. Ionescu et al. [25] jointly infer part segmentations and pose. However, the setting is typically tightly cropped bounding boxes of pedestrians, while we are interested in the completely unconstrained case."
                },
                {
                    "heading": "3. Pixel classification using hypercolumns",
                    "text": "Problem setting: We assume an object detection system that gives us a set of detections. Each detection comes with a bounding box, a category label and a score (and sometimes an initial segmentation hypothesis). The detections have already been subjected to non-maximum suppression. For every detection, we want to segment out the object, segment its parts or predict its keypoints.\nFor each task, we expand the bounding box of the detection slightly and predict a heatmap on this expanded box. The type of information encoded by this heatmap depends on the particular task. For segmentation, the heatmap encodes the probability that a particular location is inside the object. For part labeling, we predict a separate heatmap for each part, where each heatmap is the probability a location belongs to that part. For keypoint prediction, again we output a separate heatmap for each keypoint, with each heatmap encoding the probability that the keypoint is at a particular location.\nIn each case, we predict a 50\u00d750 heatmap that we resize to the size of the expanded bounding box and splat onto the image. Thus, in our framework, these diverse fine-grained localization problems are addressed as the unified task of assigning a probability to each of the 50 \u00d7 50 locations or, in other words, of classifying each location. We solve this classification problem using the hypercolumn representation as described in detail below.\nComputing the hypercolumn representation: We take the cropped bounding box, resize it to a fixed size and feed it into a CNN as in [18]. For each location, we extract features from a set of layers by taking the outputs of the units that are \u201cabove\u201d the location (as shown in Figure 1). All the intermediate outputs in a CNN are feature maps (the output of a fully connected layer can be seen as a 1 \u00d7 1 feature map). However, because of subsampling and pooling operations in the CNN, these feature maps need not be at the same resolution as the input or the target output size. So\nwhich unit lies above a particular location is ambiguous. We get around this by simply resizing each feature map to the size we want with bilinear interpolation. If we denote the feature map by F and the upsampled feature map by f , then the feature vector for the ith location has the form:\nfi = \u2211 k \u03b1ikFk (1)\n\u03b1ik depends on the position of i and k in the box and feature map respectively.\nWe concatenate features from some or all of the feature maps in the network into one long vector for every location which we call the hypercolumn at that location. As an example, using pool2 (256 channels), conv4 (384 channels) and fc7 (4096 channels) from the architecture of [28] would lead to a 4736 dimensional vector.\nInterpolating into a grid of classifiers: Because these feature maps are the result of convolutions and poolings, they do not encode any information about where in the bounding box a given pixel lies. However, location can be an important feature. For instance, in a person bounding box, the head is more likely to be at the top of the bounding box than at the bottom. Thus a pixel that looks like a nose should be considered as part of the person if it occurs at the top of the box and should be classified as background otherwise. The reasoning should be the opposite for a foot-like pixel. This is a highly non-linear effect of location, and such reasoning cannot be achieved simply by a location-specific bias. (Indeed, our classifiers include (x, y) as features but assign negligible weight to them). Such reasoning requires different classifiers for each location.\nLocation is also needed to make better use of the features from the fully connected layers at the top. Since these features are shared by all the locations in the bounding box, they can at best contribute a global instance-specific bias. However, with a different classifier at each location, we can have a separate instance-specific bias for each location. Thus location-specific classifiers in conjunction with the global, instance-level features from the fully connected layer produce an instance-specific prior.\nThe simplest way to get a location-specific classifier is to train separate classifiers for each of the 50 \u00d7 50 locations. However, doing so has three problems. One, it dramatically reduces the amount of data each classifier sees during training. In our training sets, some categories may have only a few hundred instances, while the dimensionality of the feature vector is of the order of several thousand. Thus, having fewer parameters and more sharing of data is necessary to prevent overfitting. Two, training this many classifiers is computationally expensive, since we will have to train 2500 classifiers for 20 categories. Three, while we do want the classifier to vary with location, the classifier should change slowly: two adjacent pixels that are similar to each other in\nappearance should also be classified similarly. Our solution is to train a coarse K \u00d7 K grid of classifiers and interpolate between them. In our experiments we use K = 5 or 10. For the interpolation, we use an extension of bilinear interpolation where we interpolate a grid of functions instead of a grid of values. Concretely, each classifier in the grid is a function gk(\u00b7) that takes in a feature vector and outputs a probability between 0 and 1. We use this coarse grid of functions to define the function hi at each pixel i as a linear combination of the nearby grid functions, analogous to Equation 1:\nhi(\u00b7) = \u2211 k \u03b1ikgk(\u00b7) (2)\nIf the feature vector at the ith pixel is fi, then the score of the ith pixel is:\npi = \u2211 k \u03b1ikgk(fi) = \u2211 k \u03b1ikpik (3)\nwhere pik is the probability output by the kth classifier for the ith pixel. Thus, at test time we run all our K2 classifiers on all the pixels. Then, at each pixel, we linearly combine the outputs of all classifiers at that pixel using the above equation to produce the final prediction. Note that the coefficients of the linear combination depend on the location.\nTraining this interpolated classifier is a hard optimization problem. We use a simple heuristic and ignore the interpolation at train time, using it only at test time.We divide each training bounding box into a K \u00d7 K grid. The training data for the kth classifier consists only of pixels from the kth grid cell across all training instances. Each classifier is trained using logistic regression. This training methodology does not directly optimize the loss we would encounter at test time, but allows us to use off-the-shelf code such as liblinear [14] to train the logistic regressor.\nEfficient classification using convolutions and upsampling: Our system requires us to resize every feature map to 50 \u00d7 50 and then classify each location. But resizing feature maps with hundreds of channels can be expensive. However, we know we are going to run several linear classifiers on top of the hypercolumn features and we can use this knowledge to save computation as follows: each feature map with c channels will give rise to a c-dimensional block of features in the hypercolumn representation of a location, and this block will have a corresponding block of weights in the classifiers. Thus if fi is the feature vector at location i, then fi will be composed of blocks f (j) i corresponding to the jth feature map. A linear classifier w will decompose similarly. The dot product between w and fi can then be written as:\nwT fi = \u2211 j w(j)T f (j) i (4)\nThe jth term in the decomposition corresponds to a linear classifier on top of the upsampled jth feature map. However, since the upsampling is a linear operation, we can first apply the classifier and then upsample using Equation 1:\nf (j) i = \u2211 k \u03b1 (j) ik F (j) k (5)\nw(j)T f (j) i = \u2211 k \u03b1 (j) ik w (j)TF (j) k (6)\nWe note that this insight was also used by Barron et al. [2] in their volumetric semantic segmentation system.\nObserve that applying a classifier to each location in a feature map is the same as a 1\u00d71 convolution. Thus, to run a linear classifier on top of hypercolumn features, we break it into blocks corresponding to each feature map, run 1\u00d7 1 convolutions on each feature map to produce score maps, upsample all score maps to the target resolution, and sum.\nWe consider a further modification to this pipeline where we replace the 1\u00d7 1 convolution with a general n\u00d7 n convolution. This corresponds to looking not only at the unit directly above a pixel but also the neighborhood of the unit. This captures the pattern of activations of a whole neighborhood, which can be more informative than a single unit, especially in the lower layers of the network.\nRepresentation as a neural network: We can write our final hypercolumn classifier using additional layers grafted onto the original CNN as shown in Figure 2. For each feature map, we stack on an additional convolutional layer. Each such convolutional layer has K2 channels, corresponding to the K2 classifiers we want to train. We can choose any kernel size for the convolutions as described above, although for fully connected layers that produce 1\u00d71 feature maps, we are restricted to 1 \u00d7 1 convolutions. We take the outputs of all these layers, upsample them using bilinear interpolation and sum them. Finally, we pass these outputs through a sigmoid, and combine the K2 heatmaps using equation 3 to give our final output. Each operation is differentiable and can be back-propagated over.\nRepresenting our pipeline as a neural network allows us to train the whole network (including the CNN from which we extract features) for this task. For such training, we feed in the target 50 \u00d7 50 heatmap as a label. The loss is the sum of logistic losses (or equivalently, the sum of the negative log likelihoods) over all the 50 \u00d7 50 locations. We found that treating the sigmoids, the linear combination and the log likelihood as a single composite function and computing the gradient with respect to that led to simpler, more numerically stable expressions. Instead of training the network from scratch, we use a pretrained network and finetune, i.e., do backpropagation with a small learning rate. Finally, this representation as a neural network also allows us to train the grid classifiers together and use classifier in-\nterpolation during training, instead of training separate grid classifiers independent of each other.\nTraining classifiers for segmentation and part localization: For each category we take bottom-up MCG candidates [1] that overlap a ground truth instance by 70% or more. For each such candidate, we find the ground truth instance it overlaps most with, and crop that ground truth instance to the expanded bounding box of the candidate. Depending on the task we are interested in (SDS, keypoint prediction or part labeling), we then use the labeling of the cropped ground truth instance to label locations in the expanded bounding box as positive or negative. For SDS, locations inside the instance are considered positive, while locations outside are considered negative. For part labeling, locations inside a part are positive and all other locations are negative. For keypoint prediction, the true keypoint location is positive and locations outside a certain radius (we use 10% of the bounding box diagonal) of the true location are labeled negative."
                },
                {
                    "heading": "4. Experiments on SDS",
                    "text": "Our first testbed is the SDS task. Our baseline for this task is the algorithm presented in [22]. This pipeline scores bottom-up region proposals from [1] using CNN features computed on both the cropped bounding box of the region and the cropped region foreground. The regions are subjected to non-max suppression. Finally, the surviving candidates are refined using figure-ground predictions based on the top layer features.\nAs our first system for SDS, we use the same pipeline as above, but replace the refinement step with one based on hypercolumns. (We also add a bounding box regression step [18] so as to start from the best available bounding box). We present results with this pipeline in section 4.1,\nwhere we show that hypercolumn-based refinement is significantly better than the refinement in [22], and is especially accurate when it comes to capturing fine details of the segmentation. We also evaluate several ablations of our system to unpack this performance gain. For ease of reference, we call this System 1.\nOne issue with this system is its computational cost. Extracting features from region foregrounds is expensive and doubles the time taken. Further, while CNN-based bounding box detection [18] can be speeded up dramatically using approaches such as [23], no such speedups exist for region classification. To address these drawbacks, we propose as our second system the pipeline shown in Figure 3. This pipeline starts with bounding box detections after nonmaximum suppression. We expand this set of detections by adding nearby high-scoring boxes that were removed by non-maximum suppression but may be better localized (explained in detail below). This expanded set is only twice as large as the original set, and about two orders of magnitude smaller than the full set of bottom-up proposals. For each candidate in this set, we predict a segmentation, and score this candidate using CNN features computed on the segmentation. Because region-based features are computed only on a small set, the pipeline is much more efficient. We call this system System 2.\nThis pipeline relies crucially on our ability to predict a good segmentation from just bounding boxes. We use hypercolumns to make this prediction. In section 4.2, we show that these predictions are accurate, and significantly better than predictions based on the top layer of the CNN.\nFinally, the efficiency of this pipeline also allows us to experiment with larger but more expressive architectures. While [22] used the architecture proposed by Krizhevsky et al. [28] (referred to as \u201cT-Net\u201d henceforth, following [19]) for both the box features and the region features, we show in section 4.2 that the architecture proposed by Simonyan and Zisserman [36] (referred to as \u201cO-Net\u201d henceforth [19]) is significantly better."
                },
                {
                    "heading": "4.1. System 1: Refinement using hypercolumns",
                    "text": "In our first set of experiments, we compare a hypercolumn-based refinement to that proposed in [22]. We use the ranked hypotheses produced by [22] and refine each hypothesis using hypercolumns. For the CNN, we use the same network that was used for the region classification (described as C in [22]). This network consists of two pathways, each based on T-Net. It takes in both the cropped bounding box as well as the cropped foreground. For the hypercolumn representation we use the top-level fc7 features, the conv4 features from both pathways using a 1\u00d7 1 neighborhood, and the pool2 features from the box pathway with a 3\u00d7 3 neighborhood. We choose these layers because they are spread out evenly in the network and capture a diverse\nset of features. In addition, for each location, we add as features a 0 or 1 encoding if the location was inside the original region candidate, and a coarse 10\u00d7 10 discretization of the original candidate flattened into a 100-dimensional vector. This is to be commensurate with [22] where these features were used in the refinement step. We use a 10 \u00d7 10 grid of classifiers. As a last step, we project our predictions to superpixels by averaging the prediction over each superpixel. We train on VOC2012 Train and evaluate on VOC2012 Val.\nTable 1 shows the results of our experiments. The first two columns show the performance reported in [22] with and without the refinement step. \u201cHyp\u201d is the result we get using hypercolumns, without bounding box regression or finetuning. Our mean APr at 0.5 is 1.5 points higher, and at 0.7 is 6.3 points higher, indicating that our refinement is much better than that of [22] and is a large improvement over the original candidate. Bounding box regression and finetuning the network both provide significant gains, and with both of these, our mean APr at 0.5 is 3.1 points higher and at 0.7 is 8.4 points higher than [22].\nTable 1 also shows the results of several ablations of our model (all without bounding box regression or finetuning):\n1. Only fc7 uses only fc7 features and is thus similar to the refinement step in [22]. We include this baseline to confirm that we can replicate those results.\n2. fc7+pool2, fc7+conv4 and pool2+conv4 are refinement systems that use hypercolumns but leave out features from conv4, pool2 and fc7 respectively. Each of these baselines performs worse than our full system. In each case the difference is statistically significant at a confidence threshold of 0.05, computed using paired sample permutation tests.\n3. The 1 \u00d7 1, 2 \u00d7 2 and 5 \u00d7 5 models use different grid resolutions, with the 1 \u00d7 1 grid amounting to a single\nclassifier. There is a significant loss in performance (2.4 points at 0.7 overlap) when using a 1 \u00d7 1 grid. However this baseline still outperforms [22] indicating that even without our grid classifiers (and without fc7, since the global fc7 features are ineffectual without the grid), the hypercolumn representation by itself is quite powerful. A 5 \u00d7 5 grid is enough to recover full performance.\nFinally, following [22], we take our Hyp+FT+bbox-reg system and use the pasting scheme of [9] to obtain a semantic segmentation. We get a mean IU of 54.6 on VOC2012 Segmentation Test, 3 points higher than [22] (51.6 mean IU)."
                },
                {
                    "heading": "4.2. System 2: SDS from bounding box detections",
                    "text": "For our experiments with System 2, we use the detections of R-CNN [18] as the starting point. R-CNN uses CNNs to classify bounding box proposals from selective search. We use the final output after non-max suppression and bounding box regression. However, to allow direct comparison with our previous experiments, we retrained RCNN to work with box proposals from MCG [1]. We do all training on VOC2012 Train.\nWe first evaluate our segmentation predictions. As before, we use the same network as the detector to compute the hypercolumn transform features. We first experiment with the T-Net architecture. We use the layers fc7, conv4 with a neighborhood of 1, and pool2 with a neighborhood of 3. For computational reasons we do not do any finetuning. We use superpixel projection as before.\nWe show results in Table 2. Since we use only one network operating on bounding boxes instead of two working on both the box and the region, we expect a drop in performance. We find that this is the case, but the loss is small: we get a mean APr of 49.1 at 0.5 and 29.1 at 0.7, compared to 51.9 and 32.4 when we have the region features. In fact, our performance is nearly as good as [22] at 0.5 and about 4 points better at 0.7, and we get this accuracy starting from just the bounding box.\nTo see how much of this performance is coming from the hypercolumn representation, we also run a baseline using just fc7 features. As expected, this baseline is only able to output a fuzzy segmentation, compared to the sharp delineation we get using hypercolumns. It performs considerably worse, losing 5 points at 0.5 overlap and almost 13 points at 0.7 overlap. Figure 4 shows example segmentations.\nWe now replace the T-Net architecture for the O-Net architecture. This architecture is significantly larger, but provides an 8 point gain in detection AP [19]. We again retrain the R-CNN system using this architecture on MCG bounding box proposals. Again, for the hypercolumn representation we use the same network as the detector. We use the layers fc7, conv4 with a neighborhood of 1 and pool3 with a\nneighborhood of 3. (We use pool3 instead of pool2 because the pool3 feature map has about half the resolution and is thus easier to work with.)\nWe observe that the O-Net architecture is significantly better than the T-Net: we get a boost of 7.5 points at the 0.5 overlap threshold and 8 points at the 0.7 threshold. We also find that this architecture gives us the best performance on the SDS task so far: with simple bounding box detection followed by our hypercolumn-based mask prediction, we achieve a mean APr of 56.5 at an overlap threshold of 0.5 and a mean APr of 37.0 at an overlap threshold of 0.7. These numbers are about 6.8 and 11.7 points better than the results of [22]. Last but not the least, we observe that the large gap between our hypercolumn system and the only-fc7 baseline persists, and is equally large for the O-Net architecture. This implies that the gain provided by hypercolumns is not specific to a particular network architecture. Figure 4 visualizes our O-Net results.\nWe now implement the full pipeline proposed in Figure 3. For this, we expand the initial pool of detections as follows. We pick boxes with score higher than a threshold that were suppressed by NMS but that overlap the detections by less than 0.7. We then do a non-max suppression with a lenient threshold of 0.7 to get a pool of candidates to rescore. Starting from 20K initial detections per category across the dataset, our expanded pool is typically less than 50K per category, and less than 600K in total.\nNext we segment each candidate using hypercolumns and score it using a CNN trained to classify regions. This network has the same architecture as O-Net. However, instead of a bounding box, this network takes as input the bounding box with the region background masked out. This network is trained as described in [22]. We use features from the topmost layer of this network and concatenate them with the features from the top layer of the detection network, and feed these into an SVM. For training data, we use our expanded pool of candidates on the training set, and take all candidates for which segmentation predictions overlap groundtruth by more than 70% as positive and those with overlap less than 50% as negative. After rescoring, we do a non-max suppression using region overlap to get the final set of detections (we use an overlap threshold of 0.3).\nWe get 60.0 mean APr at 0.5, and 40.4 mean APr at 0.7. These numbers are state-of-the-art on the SDS benchmark (in contemporary work, [11] get slightly higher performance at 0.5 but do not report the performance at 0.7; our gains are orthogonal to theirs). Finally, on the semantic segmentation benchmark, we get a mean IU of 62.6, which is comparable to state-of-the-art."
                },
                {
                    "heading": "5. Experiments on part localization",
                    "text": "We evaluate part localization in the unconstrained detection setting, where the task is to both detect the object and\nlabel its keypoints/segment its parts. This is different from most prior work on these problems [38, 37, 41, 40, 3, 31], which operates on the immediate vicinity of ground-truth instances. We start from the detections of [22]. We use the same features and network as in Section 4.1. As before, we do all training on VOC2012 Train.\nKeypoint prediction We evaluate keypoint prediction on the \u201cperson\u201d category using the protocol described in [21]. The test set for evaluating keypoints is the person images in the second half of VOC2009 val. We use the APK metric [43], which evaluates keypoint predictions in a detection setting. Each detection comes with a keypoint prediction and a score. A predicted keypoint within a threshold distance (0.2 of the torso height) of the ground-truth keypoint is a true positive, and is a false positive otherwise. The area under the PR curve gives the APK for that keypoint.\nWe start from the person detections of [22]. We use bounding box regression to start from a better bounding box. As described in Section 3 we train a separate system for each keypoint using the hypercolumn representation. We use keypoint annotations collected by [5]. We produce a heatmap for each keypoint and then take the highest scoring location of the heatmap as the keypoint prediction.\nThe APK metric requires us to attach a score with each keypoint prediction. This score must combine the confidence in the person detection and the confidence in the keypoint prediction, since predicting a keypoint when the keypoint is invisible counts as a false positive. For this score we multiply the value of the keypoint heatmap at the predicted location with the score output by the person detector (which we pass through a sigmoid).\nResults are shown in Table 3. We compare our perfor-\nmance to [20], the previous best on this dataset. Gkioxari et al. [20] finetuned a network for pose, person detection and action classification, and then trained an SVM to assign a score to the keypoint predictions. Without any finetuning for pose, our system achieves a 1.8 point boost. A baseline system trained using our pipeline but with just the fc7 features performs significantly worse than our system, and is even worse than a HOG-based method [21]. This confirms that the gains we get are from the hypercolumn representation. Figure 5 shows some example predictions.\nFinetuning the network as described in Section 3 gives an additional 1.5 point gain, raising mean APK to 18.5. Part labeling We evaluate part labeling on the articulated object categories in PASCAL VOC: person, horse, cow, sheep, cat, dog, bird. We use the part annotations provided by [10]. We group the parts into top-level parts: head, torso,\narms and legs for person, head, torso, legs and tail for the four-legged animals and head, torso, legs, wings, tail for the bird. We train separate classifiers for each part. At test time, we use the Hyp+bbox-reg+FT system from Section 4.1 to predict a figure-ground mask for each detection, and to every pixel in the figure-ground mask, we assign the part with the highest score at that pixel.\nFor evaluation, we modify the definition of intersectionover-union in the APr metric [22]: we count in the intersection only those pixels for which we also get the part label correct. We call this metric APrpart. As before, we evaluate both our system and a baseline that uses only fc7 features. Table 4 shows our results. We get a large gain in almost all categories by using hypercolumns. Note that this gain is entirely due to improvements in the part labeling, since both methods use the same figure-ground mask. Figure 6 shows some example part labelings."
                },
                {
                    "heading": "6. Conclusion",
                    "text": "We have shown that the hypercolumn representation provides large gains in three different tasks. We also believe that this representation might prove useful for other finegrained tasks such as attribute or action classification. We leave an investigation of this to future work.\nAcknowledgments. This work was supported by ONR MURI N000141010933, a Google Research Grant and a Microsoft Research fellowship. We thank NVIDIA for providing GPUs through their academic program."
                }
            ],
            "year": 2015,
            "references": [
                {
                    "title": "Multiscale combinatorial grouping",
                    "authors": [
                        "P. Arbel\u00e1ez",
                        "J. Pont-Tuset",
                        "J. Barron",
                        "F. Marques",
                        "J. Malik"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Volumetric semantic segmentation using pyramid context features",
                    "authors": [
                        "J.T. Barron",
                        "P. Arbel\u00e1ez",
                        "S.V.E. Ker\u00e4nen",
                        "M.D. Biggin",
                        "D.W. Knowles",
                        "J. Malik"
                    ],
                    "year": 2013
                },
                {
                    "title": "Shape-based pedestrian parsing",
                    "authors": [
                        "Y. Bo",
                        "C.C. Fowlkes"
                    ],
                    "venue": "In CVPR,",
                    "year": 2011
                },
                {
                    "title": "Class-specific, top-down segmentation",
                    "authors": [
                        "E. Borenstein",
                        "S. Ullman"
                    ],
                    "venue": "In ECCV",
                    "year": 2002
                },
                {
                    "title": "Detecting people using mutually consistent poselet activations",
                    "authors": [
                        "L. Bourdev",
                        "S. Maji",
                        "T. Brox",
                        "J. Malik"
                    ],
                    "venue": "In ECCV,",
                    "year": 2010
                },
                {
                    "title": "Improved bird species recognition using pose normalized deep convolutional nets",
                    "authors": [
                        "S. Branson",
                        "G. Van Horn",
                        "P. Perona",
                        "S. Belongie"
                    ],
                    "venue": "In BMVC,",
                    "year": 2014
                },
                {
                    "title": "High accuracy optical flow estimation based on a theory for warping",
                    "authors": [
                        "T. Brox",
                        "A. Bruhn",
                        "N. Papenberg",
                        "J. Weickert"
                    ],
                    "venue": "In ECCV",
                    "year": 2004
                },
                {
                    "title": "The laplacian pyramid as a compact image code",
                    "authors": [
                        "P.J. Burt",
                        "E.H. Adelson"
                    ],
                    "venue": "Communications, IEEE Transactions on,",
                    "year": 1983
                },
                {
                    "title": "Semantic segmentation with second-order pooling",
                    "authors": [
                        "J. Carreira",
                        "R. Caseiro",
                        "J. Batista",
                        "C. Sminchisescu"
                    ],
                    "venue": "In ECCV,",
                    "year": 2012
                },
                {
                    "title": "Detect what you can: Detecting and representing objects using holistic models and body parts",
                    "authors": [
                        "X. Chen",
                        "R. Mottaghi",
                        "X. Liu",
                        "S. Fidler",
                        "R. Urtasun",
                        "A. Yuille"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Convolutional feature masking for joint object and stuff segmentation",
                    "authors": [
                        "J. Dai",
                        "K. He",
                        "J. Sun"
                    ],
                    "venue": "In CVPR,",
                    "year": 2015
                },
                {
                    "title": "Learning to localize detected objects",
                    "authors": [
                        "Q. Dai",
                        "D. Hoiem"
                    ],
                    "venue": "In CVPR,",
                    "year": 2012
                },
                {
                    "title": "Towards unified object detection and semantic segmentation",
                    "authors": [
                        "J. Dong",
                        "Q. Chen",
                        "S. Yan",
                        "A. Yuille"
                    ],
                    "venue": "In European Conference on Computer Vision (ECCV),",
                    "year": 2014
                },
                {
                    "title": "Liblinear: A library for large linear classification",
                    "authors": [
                        "R.-E. Fan",
                        "K.-W. Chang",
                        "C.-J. Hsieh",
                        "X.-R. Wang",
                        "C.- J. Lin"
                    ],
                    "year": 2008
                },
                {
                    "title": "Learning hierarchical features for scene labeling",
                    "authors": [
                        "C. Farabet",
                        "C. Couprie",
                        "L. Najman",
                        "Y. LeCun"
                    ],
                    "year": 2013
                },
                {
                    "title": "Object detection with discriminatively trained partbased models",
                    "authors": [
                        "P.F. Felzenszwalb",
                        "R.B. Girshick",
                        "D. McAllester",
                        "D. Ramanan"
                    ],
                    "year": 2010
                },
                {
                    "title": "Bottom-up segmentation for top-down detection",
                    "authors": [
                        "S. Fidler",
                        "R. Mottaghi",
                        "A. Yuille",
                        "R. Urtasun"
                    ],
                    "venue": "In CVPR,",
                    "year": 2013
                },
                {
                    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
                    "authors": [
                        "R. Girshick",
                        "J. Donahue",
                        "T. Darrell",
                        "J. Malik"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
                    "authors": [
                        "R. Girshick",
                        "J. Donahue",
                        "T. Darrell",
                        "J. Malik"
                    ],
                    "venue": "arXiv preprint arXiv:1409.1556,",
                    "year": 2014
                },
                {
                    "title": "CNNs for pose estimation and action detection",
                    "authors": [
                        "G. Gkioxari",
                        "B. Hariharan",
                        "R. Girshick",
                        "J. Malik. R"
                    ],
                    "year": 2014
                },
                {
                    "title": "Using k-poselets for detecting people and localizing their keypoints",
                    "authors": [
                        "G. Gkioxari",
                        "B. Hariharan",
                        "R. Girshick",
                        "J. Malik"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Simultaneous detection and segmentation",
                    "authors": [
                        "B. Hariharan",
                        "P. Arbel\u00e1ez",
                        "R. Girshick",
                        "J. Malik"
                    ],
                    "venue": "In ECCV,",
                    "year": 2014
                },
                {
                    "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
                    "authors": [
                        "K. He",
                        "X. Zhang",
                        "S. Ren",
                        "J. Sun"
                    ],
                    "venue": "In ECCV,",
                    "year": 2014
                },
                {
                    "title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex",
                    "authors": [
                        "D.H. Hubel",
                        "T.N. Wiesel"
                    ],
                    "venue": "The Journal of physiology,",
                    "year": 1962
                },
                {
                    "title": "Iterated second-order label sensitive pooling for 3d human pose estimation",
                    "authors": [
                        "C. Ionescu",
                        "J. Carreira",
                        "C. Sminchisescu"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Determining three-dimensional shape from orientation and spatial frequency disparities",
                    "authors": [
                        "D.G. Jones",
                        "J. Malik"
                    ],
                    "venue": "In ECCV,",
                    "year": 1992
                },
                {
                    "title": "Representation of local geometry in the visual system",
                    "authors": [
                        "J.J. Koenderink",
                        "A.J. van Doorn"
                    ],
                    "venue": "Biological cybernetics,",
                    "year": 1987
                },
                {
                    "title": "Imagenet classification with deep convolutional neural networks",
                    "authors": [
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "G.E. Hinton"
                    ],
                    "venue": "In NIPS,",
                    "year": 2012
                },
                {
                    "title": "Backpropagation applied to handwritten zip code recognition",
                    "authors": [
                        "Y. LeCun",
                        "B. Boser",
                        "J.S. Denker",
                        "D. Henderson",
                        "R.E. Howard",
                        "W. Hubbard",
                        "L.D. Jackel"
                    ],
                    "venue": "Neural computation,",
                    "year": 1989
                },
                {
                    "title": "Fully convolutional networks for semantic segmentation",
                    "authors": [
                        "J. Long",
                        "E. Schelhamer",
                        "T. Darrell"
                    ],
                    "venue": "In CVPR,",
                    "year": 2015
                },
                {
                    "title": "Pedestrian parsing via deep decompositional network",
                    "authors": [
                        "P. Luo",
                        "X. Wang",
                        "X. Tang"
                    ],
                    "venue": "In ICCV,",
                    "year": 2013
                },
                {
                    "title": "Preattentive texture discrimination with early vision mechanisms",
                    "authors": [
                        "J. Malik",
                        "P. Perona"
                    ],
                    "venue": "Journal of the Optical Society of America A,",
                    "year": 1990
                },
                {
                    "title": "The truth about cats and dogs",
                    "authors": [
                        "O.M. Parkhi",
                        "A. Vedaldi",
                        "C. Jawahar",
                        "A. Zisserman"
                    ],
                    "venue": "In ICCV,",
                    "year": 2011
                },
                {
                    "title": "Pedestrian detection with unsupervised multi-stage feature learning",
                    "authors": [
                        "P. Sermanet",
                        "K. Kavukcuoglu",
                        "S. Chintala",
                        "Y. LeCun"
                    ],
                    "venue": "In CVPR,",
                    "year": 2013
                },
                {
                    "title": "Two-stream convolutional networks for action recognition in videos",
                    "authors": [
                        "K. Simonyan",
                        "A. Zisserman"
                    ],
                    "venue": "arXiv preprint arXiv:1406.2199,",
                    "year": 2014
                },
                {
                    "title": "Very deep convolutional networks for large-scale image recognition",
                    "authors": [
                        "K. Simonyan",
                        "A. Zisserman"
                    ],
                    "venue": "arXiv preprint arXiv:1409.1556,",
                    "year": 2014
                },
                {
                    "title": "Joint training of a convolutional network and a graphical model for human pose estimation",
                    "authors": [
                        "J. Tompson",
                        "A. Jain",
                        "Y. LeCun",
                        "C. Bregler"
                    ],
                    "venue": "In NIPS (To appear),",
                    "year": 2014
                },
                {
                    "title": "Deeppose: Human pose estimation via deep neural networks",
                    "authors": [
                        "A. Toshev",
                        "C. Szegedy"
                    ],
                    "venue": "In CVPR,",
                    "year": 2014
                },
                {
                    "title": "Robust computation of optical flow in a multi-scale differential framework",
                    "authors": [
                        "J. Weber",
                        "J. Malik"
                    ],
                    "year": 1995
                },
                {
                    "title": "Paper doll parsing: Retrieving similar styles to parse clothing items",
                    "authors": [
                        "K. Yamaguchi",
                        "M.H. Kiapour",
                        "T.L. Berg"
                    ],
                    "venue": "In ICCV,",
                    "year": 2013
                },
                {
                    "title": "Parsing clothing in fashion photographs",
                    "authors": [
                        "K. Yamaguchi",
                        "M.H. Kiapour",
                        "L.E. Ortiz",
                        "T.L. Berg"
                    ],
                    "venue": "In CVPR,",
                    "year": 2012
                },
                {
                    "title": "Layered object models for image segmentation",
                    "authors": [
                        "Y. Yang",
                        "S. Hallman",
                        "D. Ramanan",
                        "C.C. Fowlkes"
                    ],
                    "year": 2012
                },
                {
                    "title": "Articulated human detection with flexible mixtures of parts",
                    "authors": [
                        "Y. Yang",
                        "D. Ramanan"
                    ],
                    "year": 2013
                },
                {
                    "title": "Partbased R-CNNs for fine-grained category detection",
                    "authors": [
                        "N. Zhang",
                        "J. Donahue",
                        "R. Girshick",
                        "T. Darrell"
                    ],
                    "venue": "In ECCV,",
                    "year": 2014
                }
            ],
            "id": "SP:9447d4487beda47e959d320a1174b563d8ee284c",
            "authors": [
                {
                    "name": "Bharath Hariharan",
                    "affiliations": []
                },
                {
                    "name": "Pablo Arbel\u00e1ez",
                    "affiliations": []
                },
                {
                    "name": "Ross Girshick",
                    "affiliations": []
                }
            ],
            "abstractText": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean AP [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.",
            "title": "Hypercolumns for Object Segmentation and Fine-grained Localization"
        }
    },
    "11790475": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "Current machine learning methods seem weak when they are required to generalize beyond the training distribution, which is what is often needed in practice. It is not enough to obtain good generalization on a test set sampled from the same distribution as the training data, we would also like what has been learned in one setting to generalize well in other related distributions. These distributions may involve the same concepts that were seen previously by the learner, with the changes typically arising because of actions of agents. More generally, we would like what has been learned previously to form a rich base from which very fast adaptation to a new but related distribution can take place, i.e., obtain good transfer. Some new concept may have to be learned but because most of the other relevant concepts have already been captured by the learner (as well as how they can be composed), learning can be very fast on the transfer distribution.\nShort of any assumption, it is impossible to have a successful transfer to an unrelated distribution. In this paper we focus on the assumption that the changes are sparse when the knowledge is represented in an appropriately modularized way, with only one or a few of the modules having changed. This is especially relevant when the distributional change is due to actions by one or more agents, such as the interventions discussed in the causality literature (Pearl, 2009; Peters et al., 2017), where a single causal variable is clamped to a particular value. In general, it is difficult for agents to influence many underlying causal variables at a time, and although this paper is not about agent learning as such, this is a property of the world that we propose to exploit here, to help discovering these variables and how they are causally related to each other.\nar X\niv :1\n90 1.\n10 91\n2v 2\n[ cs\n.L G\n] 4\nF eb\nTo motivate the need for inferring causal structure, consider that interventions may be actually performed or may be imagined. In order to properly plan in a way that takes into account interventions, one needs to imagine a possible change to the joint distribution of the variables of interest due to an intervention, even one that has never been observed before. This goes beyond good transfer learning and requires causal learning and causal reasoning. For this purpose, it is not sufficient to learn the joint distribution of the observed variables. One also should learn enough about the underlying high-level variables and their causal relations to be able to properly infer the effect of an intervention. For example, A=Raining causes B=Open Umbrella (and not vice-versa). Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates A and B (captured by P (B|A)), but will have an impact on the marginal P (B). Conversely, an agent\u2019s intervention on B (Open umbrella) will have no effect on the marginal distribution of A (Raining). That asymmetry is generally not visible from the (A,B) training pairs alone, until a change of distribution occurs, e.g. due to an intervention. This motivates the setup of this paper, where one learns from a set of distributions arising from not necessarily known interventions, not simply to capture a joint distribution but to discover the some underlying causal structure.\nMachine learning methods are often exploiting some form of assumption about the data distribution (or else, the no free lunch theorem tells us that we cannot have any confidence in generalization). In this paper, we are considering not just assumptions on the data distribution but also on how it changes (e.g., when going from a training distribution to a transfer distribution, possibly resulting from some agent\u2019s actions). We propose to rely on the assumption that, when the knowledge about the distribution is appropriately represented, these changes would be small. This arises because of an underlying assumption (but more difficult to verify directly) that only one or few of the ground truth mechanisms have been changed, due to some generalized form of intervention leading to the modified distribution.\nHow can we exploit this assumption? As we explain theoretically and verify experimentally here, if we have the right knowledge representation, then we should get fast adaptation to the transfer distribution when starting from a model that is well trained on the training distribution. This arises because of our assumption that the ground truth data generative process is obtained as the composition of independent mechanisms and that, very few ground truth mechanisms and parameters need to change when going from the training distribution to the transfer distribution. A model capturing a corresponding factorization of knowledge would thus require just a few updates, a few examples, for this adaptation to the transfer distribution. As shown below, the expected gradient on the unchanged parameters would be near 0 (if the model was already well trained on the training distribution), so the effective search space during adaptation to the transfer distribution would be greatly reduced, which tends to produce fast adaptation, as found experimentally.\nThus, based on the assumption of small change in the right knowledge representation space, we can define a meta-learning objective that measures the speed of adaptation, i.e., a form of regret, in order to optimize the way in which knowledge should be represented, factorized and structured. This is the core idea presented in this paper. Note that a stronger signal can be obtained when there are more non-stationarities, i.e., many changes in distribution, just like in meta-learning we get better results with more meta-examples.\nIn this way, we can take what is normally considered a nuisance in machine learning (changes in distribution due to non-stationarity, uncontrolled interventions, etc.) and turn that into a training signal to find a good way to factorize knowledge into components and mechanisms that match the assumption of small change. Thus, we end up optimizing in an end-to-end way the very thing we care about at the end, i.e. fast transfer and robustness to distributional changes. If the data was really generated from the composition of independent causal mechanisms (Peters et al., 2017), then there exists a good factorization of knowledge that mimics that structure. If in addition, at each time step, agents in the real world tend to only be able to change one or very few high-level variables (or the associated mechanisms producing them), then our assumption of small change (in the right representation) should be generally valid. Also, in addition to obtaining fast transfer, we may be able to recover a good approximation of the true causal decomposition into independent mechanisms (to the extent that the observations and interventions can reveal those mechanisms).\nIn this paper, we begin exploring the above ideas with specific experiments on synthetically generated data in order to validate them and demonstrate the existence of simple algorithms to exploit them. However it is clear to us that much more work will be needed to evaluate the proposed approach in a diversity of settings and with different specific parametrizations, training objectives, environments, etc. We begin with what are maybe the simplest possible settings and evaluate whether the above approach can be used to\nlearn the direction of causality. We then study the crucial question of obtaining a training signal about how to transform raw observed data into a representation space where the latent variables can be modeled by a sparse causal graph with sparse distributional changes and show results that confirm that the correct encoder leads to a better value of our expected regret meta-learning objective."
                },
                {
                    "heading": "2. Which is Cause and Which is Effect?",
                    "text": "To anchor ideas and show an example of application of the above-proposed meta-objective for knowledge decomposition, we consider in this section the problem of determining if variable A causes variable B or vice-versa. The learner observes training samples (a, b) from a pair of related distributions, which by convention we call the training distribution and the transfer distribution. Note that based only on samples from a single (training) distribution, in general both the A \u2192 B model (A causes B) and the B \u2192 A model (vice-versa, see Equation (1) below) tend to perform as well in terms of ordinary generalization (to a test set sampled from the training distribution), see also a theoretical argument and simulation results in Appendix A. To highlight the power of the proposed meta-learning objective, we consider the situation where lots of examples are available for the training distribution but very few for the transfer distribution. In fact, as we will argue below, the training signal that will allow us to infer the correct causal direction will be stronger if we have access to many short transfer adaptation episodes. Short episodes are most informative because after having seen a lot of data from the transfer distribution, it will not matter much whether A causes B or vice-versa (when there is enough training data compared to the number of free parameters, both models converge towards an optimal estimation of the joint). However, in order to generalize quickly from very few examples of the transfer distribution, it does matter to have made the correct choice of the causal direction. Let us now justify this in more detail below and then demonstrate this by simulations."
                },
                {
                    "heading": "2.1 Learning a Causal Graph with two Discrete Variables",
                    "text": "Let both A and B be discrete variables each taking N possible values and consider the following two parametrizations (the A\u2192 B model and the B \u2192 A model) to estimate their joint distribution:\nPA\u2192B(A,B) = PA\u2192B(A)PA\u2192B(B | A) PB\u2192A(A,B) = PB\u2192A(B)PB\u2192A(A | B) (1)\nEach of these two graphical models (denoted A\u2192 B and B \u2192 A) decomposes the joint into two separately parametrized modules, each corresponding to a different causal mechanism associated with the probability of a variable given its parents in the graph. This amounts to four modules: PA\u2192B(A), PA\u2192B(B | A), PB\u2192A(B) and PB\u2192A(A | B). We will train both models independently. Since we assume in this section that the pairs (A,B) are completely observed, we can use a simple maximum likelihood estimator to independently train all four modules (the log-likelihood of the joint decomposes into separate objective functions, one for each conditional, in a directed graphical model with fully observed variables). In the discrete case with tabular parametrization, the maximum likelihood estimator can be computed analytically, and corresponds to the appropriately normalized relative frequencies. Let \u03b8 denote the parameters of all these models, split into sub-vectors for each module, e.g., \u03b8A|B for the N\n2 conditional probabilities for each possible value of B and each possible value of A. In our experiments, we parametrized these probabilities via softmax of unnormalized quantities."
                },
                {
                    "heading": "2.1.1 The Advantage of the Correct Causal Model",
                    "text": "First, let us consider simply the likelihood of the training data only (i.e., no change of distribution) for the different causal models considered. Both models have O(N2) parameters, and maximum likelihood estimation leads to indistinguishable test set performance (where the test set is sampled from the training distribution). See Appendix A for a demonstration that both models would have the same likelihood, and associated experimental results. These results are not surprising in light of the existing literature on non-identifiability of causality from observations (Pearl, 2009; Peters et al., 2017), but they highlight the importance of using changes in distribution to provide a signal about the causal structure.\nNow instead let us compare the performance of our two hypotheses (A\u2192 B vs B \u2192 A) in terms of how fast the two models adapt on a transfer distribution after having been trained on the training distribution. We will assume simple stochastic gradient descent on the parameters for this adaptation but other procedures could be used, of course. Without loss of generality, let A\u2192 B be the correct causal model. To make the case stronger, let us consider that the change between the two distributions amounts to a random change in the parameters of the true P (A) for the cause A (because this will have an impact on the effect B, which can be picked up and reveal the causal direction). We do not assume that the learner knows what intervention was performed, unlike in more common approaches to causal discovery and controlled experiments. We only assume that some change happened and we try to exploit that to reveal structural causal information."
                },
                {
                    "heading": "2.2 Experiments on Adaptation to the transfer distribution",
                    "text": "We present experiments comparing the learning curve of the correct causal model on the transfer distribution vs the learning curve of the incorrect model. The adaptation with only a few gradient steps on data coming from a different, but related, transfer distribution is critical in getting a signal that can be leveraged by our meta-learning algorithm. To show the effect of this adaptation, and motivate our use of only a small amount of data from the transfer distribution, we experimented with a model on discrete random variables taking N = 10 possible values.\nIn this experiment, we fixed the underlying causal model to be A \u2192 B, and trained the modules for each marginal and conditional distributions with maximum likelihood on a large amount of data from some training distribution, as explained in Appendix A. See also Appendix G.1 and Table G.1 for details on the definitions of these modules.\nWe then adapt all the modules on data coming from a transfer distribution, corresponding on an intervention on the random variable A (i.e., the marginal P (A) of the ground truth model is modified, while leaving P (B | A) fixed). We used RMSprop for the adaptation, with the same learning rate. For assessing reproducibility and statistical robustness, the experiment was repeated over 100 different training distributions, and over 100 transfer distributions for each training distributions, leading to 10 000 experiments overall. The procedure to acquire different training/transfer distributions is detailed in Appendix G.1.\nIn Figure 1, we report the log-likelihoods of both models, evaluated on a large test set of 10 000 from the transfer distribution. We can see that as the number of examples from the transfer distribution (equal to the number of adaptation steps) increases, the two models eventually reach the same log-likelihood, reflecting\nour observation from Appendix A. However the causal model A\u2192 B adapts faster than the other model B \u2192 A, with the most informative part of the trajectory (where the difference is the largest) is within the first 10 to 20 examples."
                },
                {
                    "heading": "2.2.1 Parameter Counting Argument",
                    "text": "A simple parameter counting arguments helps us understand what we are observing in Figure 1. First, consider the expected gradient on the parameters of the different modules, during the adaptation phase to the transfer distribution, which we designate as adaptation episode, and corresponds to learning from a meta-example.\nProposition 1 The expected gradient over the transfer distribution of the regret (accumulated negative log-likelihood during the adaptation episode) with respect to the module parameters is zero for the parameters of the modules that (a) were correctly learned in the training phase, and (b) have the correct set of causal parents, corresponding to the ground truth causal graph, if (c) the corresponding ground truth conditional distributions did not change from the training distribution to the transfer distribution.\nThe proof is given in Appendix B. The basic justification for this proposition is that for the modules that were correctly learned in the training distribution and whose ground truth conditional distribution did not change with the transfer distribution, the parameters already are at a maximum of the log-likelihood over the transfer distribution, so the expected gradient is zero.\nAs a consequence, the effective number of parameters that need to be adapted, when one has the correct causal graph structure, is reduced to those of the mechanisms that actually changed from the training to the transfer distribution. Since sample complexity - the number of training examples necessary to learn a model - grows approximately linearly (Ehrenfeucht et al., 1989) with VC-dimension (Vapnik and Chervonenkis, 1971), and since VC-dimension grows approximately linearly in the number of parameters in linear models and neural networks Shalev-Shwartz and Ben-David (2014), the learning curve on the transfer distribution will tend to improve faster for the model with the correct causal structure, for which fewer parameters need to be changed. Interestingly, we do not need to have the whole causal graph correctly specified before getting benefits from this phenomenon. If we only have part of the causal graph correctly specified and we change our causal hypothesis to include one more correctly specified mechanism, then we will obtain a gain in terms of the adaptation sample complexity (which shows up when the change in distribution does not touch that mechanism). This nice property also shows up in Proposition 4 (Appendix F), showing a decoupling of the meta-objective across the independent mechanisms.\nLet us consider the special case we have been studying up to now. We have four modules, two of which (PA\u2192B(A) and PB\u2192A(B)) are marginal discrete distributions over N values, which require each N \u2212 1 free parameters. The other two modules are conditional probability tables that have N rows each with N \u2212 1 free parameters, i.e., a total of N(N \u2212 1) free parameters. If A\u2192 B is the correct model and the transfer distribution only changed the true P (A) (the cause), and if P (B | A) had been correctly estimated on the training distribution, then for the correct model only N \u22121 parameters need to be re-estimated. On the other hand, because of Bayes\u2019 rule, under the incorrect model (B \u2192 A), a change in P (A) leads to new parameters for both P (B) and P (A | B), i.e., all N(N \u2212 1) + (N \u2212 1) = N2 \u2212 1 parameters must be re-estimated. In this case we see that sample complexity may be O(N2) for the incorrect model while it would be O(N) for the correct model (assuming linear relationship between sample complexity and number of free parameters). Of course, if the change in distribution had been over P (B | A) instead of P (A), the advantage would not have been as great. This would motivate information gathering actions generally resulting in a very sparse change in the mechanisms."
                },
                {
                    "heading": "2.3 Smooth parameterization of the causal structure",
                    "text": "In the more general case with many more than two hypotheses for the structure of the causal graph, there will be an exponentially large set of possible causal structures explaining the data and we won\u2019t be able to enumerate all of them (and pick the best one after observing episodes of adaptation). However, we can parameterize our belief about an exponentially large set of hypotheses by keeping track of the probability for each directed edge of the graph to be present, i.e., specify for each variable B whether some variable A is a\ndirect causal parent of B (for all pairs (A,B) in the graph). We will develop such a smooth parametrization further in Appendix F, but it hinges on gradually changing our belief in the individual binary decisions associated with each edge of the causal graph, so we can jointly do gradient descent on all these beliefs at the same time.\nIn this section, we study the simplest possible version of this idea, representing that edge belief via a structural parameter \u03b3 with sigmoid(\u03b3) = sigmoid(\u03b3), our believed probability that A\u2192 B is the correct choice. For that single pair of variables scenario, let us consider two explanations for the data (as in the above sections, for models A\u2192 B and B \u2192 A), one with probability p(A\u2192 B) = sigmoid(\u03b3) and the other with probability p(B \u2192 A) = 1\u2212 sigmoid(\u03b3). We can write down our transfer objective as a log-likelihood over the mixture of these two models. Note this is different from the usual mixture models, which assume separately for each example that it was sampled from one component or another with some probability. Here, we assume that all of the observed data was sampled from one component or the other. The transfer data regret (negative log-likelihood accumulated along the online adaptation trajectory) under that mixture is therefore as follows:\nR = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A] (2)\nwhere LA\u2192B and LB\u2192A are the online likelihoods of both models respectively on the transfer data. They are defined as\nLA\u2192B = T\u220f t=1 PA\u2192B(at, bt ; \u03b8t)\nLB\u2192A = T\u220f t=1 PB\u2192A(at, bt ; \u03b8t),\nwhere {(at, bt)}t is the set of transfer examples for a given episode and \u03b8t aggregates all the modules\u2019 parameters as of time step t (since the parameters could be updated after each observation of an example (at, bt) from the transfer distribution). Pmodel(a, b; \u03b8) is the likelihood of example (a, b) under some model that has parameters \u03b8.\nThe quantity of interest here is \u2202R\u2202\u03b3 , which is our training signal for updating \u03b3. In the experiments below, after each episode involving T transfer examples we update \u03b3 by doing one step of gradient descent, to reduce the transfer negative log-likelihood or regret R. What we are proposing is a meta-learning framework in which the inner training loop updates the module parameters (separately) as examples are seen (from either distribution being currently observed), while the outer loop updates the structural parameters (here it is only the scalar \u03b3) with respect to the transfer negative log-likelihood.\nThe gradient of the transfer log-likelihood with respect to the structural parameter \u03b3 is pushing sigmoid(\u03b3) towards the posterior probability that the correct model is A\u2192 B and (1\u2212 sigmoid(\u03b3)) towards the posterior probability that the correct model is B \u2192 A:\nProposition 2 The gradient of the negative log-likelihood of the transfer data in Equation (2) wrt. the structural parameter \u2202R\u2202\u03b3 is given by\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2), (3)\nwhere D2 is the transfer data, and P (A \u2192 B | D2) is the posterior probability of the hypothesis A \u2192 B (when the alternative is B \u2192 A). Furthermore, this can be equivalently written as\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 \u03c3(\u03b3 + \u2206), (4)\nwhere \u2206 = logLA\u2192B \u2212 logLB\u2192A is the difference between the log-likelihoods of the two hypotheses on the transfer data D2.\nThe proof is given in Appendix D. Note how this posterior probability is basically measuring which hypothesis is better explaining the episode transfer data D2 overall along the adaptation trajectory. D2 is a metaexample for updating the structural parameters like \u03b3. Larger \u2206 of one hypothesis over the other leads to moving meta-parameters faster towards the favoured hypothesis. This difference in online accumulated log-likelihoods \u2206 also relates to log-likelihood scores in score-based methods for structure learning of graphical models (Koller and Friedman, 2009)1.\nTo find where SGD converges, note that the actual posterior depends on the prior sigmoid(\u03b3) and thus keeps changing after each gradient step. We are really doing SGD on the expected value of R over transfer sets D2. Equating the gradient of this expected value to zero to look for the stationary convergence point, we thus see sigmoid(\u03b3) on both sides of the equation, and we obtain convergence when the new value of sigmoid(\u03b3) is consistent with the old value, as clarified in this proposition.\nProposition 3 Stochastic gradient descent (with appropriately decreasing learning rate) on ED2 [R] with steps from \u2202R\u2202\u03b3 converges towards sigmoid(\u03b3) = 1 if ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or \u03c3(\u03b3) = 0 otherwise.\nThe proof is given in Section E of the Appendix, and shows that optimizing \u03b3 will end up picking the correct hypothesis, i.e., the one that has the smallest regret (or fastest convergence), measured as the accumulated log-likelihood as adaptation proceeds on the transfer distributions sampled from the distribution D2, which we can think of like a distribution over tasks, in meta-learning. This analogy with meta-learning also appears in our gradient-based adaptation procedure, which is linked to existing methods like the first-order approximation of MAML (Finn et al., 2017), and its related algorithms (Nichol et al., 2018). Algorithm 1 (Appendix C) illustrates the general pseudo-code for the proposed meta-learning framework."
                },
                {
                    "heading": "2.3.1 Experimental Results",
                    "text": "To illustrate the convergence result from Proposition 3, we experiment with learning the structural parameter \u03b3 in a bivariate model, with discrete random variables, each taking N = 10 and N = 100 possible values. In this experiment, we assume that the underlying causal model (unknown to the algorithm) is fixed to A\u2192 B, so that we want the structural parameter to eventually converge to \u03c3(\u03b3) = 1. The details of the experimental setup can be found in Appendix G.1.\n1. One can see logLA\u2192B as a score attributed to graph A \u2192 B, analogously for logLB\u2192A. The gradient is then pushing toward the graph with the highest score.\nIn Figure 2, we show the evolution of \u03c3(\u03b3) (which is the model\u2019s belief of A\u2192 B being the correct causal model) as the number of episodes increases. Starting from an equal belief for both A\u2192 B and B \u2192 A to occur (\u03c3(\u03b3) = 0.5), the structural parameter converges to \u03c3(\u03b3) = 1 within 500 episodes.\nThis observation is consistent across a range of domains, including models with multimodal or multivariate continuous variables, and different parametrizations of the models. In Appendix G.2, we present results for two discrete variables but using MLPs to parametrize the conditional distributions, and where there are more causal hypotheses: we consider one binary choice for each directed edge in the graph, to decide whether one variable is a direct causal parent or not. Figure 3 shows that the correct causal graph is quickly recovered. To estimate the gradient, we use a generalization of the regret loss (introduced above, Equation (2)) and its gradient, described in Appendix F.\nIn Appendix G.3, we consider the case of continuous scalar multimodal variables. The ground truth joint distribution is obtained by making the effect B a non-linear function f(A) of cause A, where f is a randomly generated spline. Figure G.1 shows an example of a resulting joint distribution. We model the conditionals with mixture density networks (Bishop, 1994) and the marginals by a Gaussian mixture. We obtain results that are similar to the discrete case, with the correct causal interpretation being recovered quickly, as illustrated in Figure G.2.\nWe also show in Appendix G.4 results on models with two continuous random variables, each being distributed as a multivariate Gaussian, with N = 10 dimensions. Similar to the experiment with discrete random variables, the same argument about parameter counting mentioned in Section 2.2.1 holds here. Again, we obtain results consistent with the previous examples, where the structural parameter \u03b3 converges to 1, effectively recovering the correct causal model A\u2192 B."
                },
                {
                    "heading": "3. Representation Learning",
                    "text": "So far, we have assumed that the system has unrestricted access to the true underlying causal variables, A and B. However in many realistic scenarios for learning agents, the observations available to the learner might not be instances of the true causal variables but sensory-level data instead, like pixels and sounds. If this is the case, our working assumption \u2013 that the correct causal graph will be sparsely connected, made of independent components, and affected sparsely by distributional shifts \u2013 can not be expected to hold true in general in the space of observed variables. To tackle this, we propose to follow the deep learning objective of disentangling the underlying causal variables (Bengio et al., 2013), and learn a representation in which these properties hold. In the simplest form of this setting, the learner must map its raw observations to a hidden\nrepresentation space H via an encoder E . The encoder is trained such that the hidden space H helps to optimize the meta-transfer objective described above, i.e., we consider the encoder, along with \u03b3, as part of the set of structural or meta-parameters to be optimized with respect to the meta-transfer objective.\nTo study this simplified setting, we consider that our raw observations (X,Y ) originate from the true causal variables (A,B) via the action of a ground truth decoder D (or generator network) that the learner is not aware of but is implicitly trying to invert, as illustrated in Figure 4. The variables A, B, X and Y are assumed to be scalars, and we first consider D be a rotation matrix such that:[\nX Y\n] = R(\u03b8D) [ A B ] (5)\nThe encoder is set to another rotation matrix, one that maps the observations X,Y to the hidden representation U, V as follows: [\nU V\n] = R(\u03b8E) [ X Y ] (6)\nThe causal modules are now to be trained on the variables U and V in the same way as detailed in Section 2, as if they were observed directly. Indeed, if the encoder is valid one would obtain either (U, V ) = (A,B) or (U, V ) = (B,A) up to a negative sign, but we say in that case and without loss of generality that (U, V ) recovered (A,B), corresponding to the solution \u03b8E = \u2212\u03b8D. In this case, the model U \u2192 V is causal and should therefore have an advantage over the anticausal model V \u2192 U , as far as adaptation speed on the transfer distribution is concerned. However, if the encoder is not valid, one would obtain superpositions of the form:\nU = cos(\u03b8)A\u2212 sin(\u03b8)B (7) V = sin(\u03b8)A+ cos(\u03b8)B (8)\nwhere \u03b8 = \u03b8E + \u03b8D. In the extremum where \u03b8 = \u03c0 4 , it is clear that the model U \u2192 V will not have an advantage over the model V \u2192 U in terms of regret on the transfer distribution. However, the question we are interested in is whether it is possible to learn the encoder \u03b8E . We verify this experimentally using Algorithm 1, but where the meta-parameters are now both \u03b3 (choosing between cause and effect which is which) and the parameters of the encoder (here the angle of a rotation matrix). The details of that experiment are provided in Appendix H, which illustrates \u2013 see Figure 5 \u2013 how the proposed objective can disentangle (here in a very simple setting) the ground truth variables (up to permutation)."
                },
                {
                    "heading": "4. Related Work",
                    "text": "Although this paper focuses on the causal graph, the proposed objective is motivated by the more general question of discovering the underlying causal variables (and their dependencies) that explain the environment of a learner and make it possible for that learner to plan appropriately. The discovery of underlying explanatory variables has come under different names, in particular the notion of disentangling underlying variables (Bengio et al., 2013). As stated already by Bengio et al. (2013) and clearly demonstrated by Locatello et al. (2018), assumptions, priors or biases are necessary to identify the underlying explanatory\n\u2212\u03c04 . For the former, we obtain \u03b8 = 0 corresponding to the correct causal graph U \u2192 V ; and for the latter, \u03b8 = \u03c02 , corresponding to the correct causal graph V \u2192 U . The encoder parameter \u03b8E is trained jointly with the structural parameter, and we find that it converges to one of the two valid solutions. Further details and the corresponding evolution of the structural parameter can be found in Appendix H.\nvariables. The latter paper (Locatello et al., 2018) also reviews and evaluates recent work on disentangling, and discusses different metrics that have been proposed. An extreme view of disentangling is that the explanatory variables should be marginally independent, and many deep generative models (Goodfellow et al., 2016) and independent component analysis models (Hyva\u0308rinen et al., 2001; Hyva\u0308rinen et al., 2018) are built on this assumption. However, the kinds of high-level variables that we manipulate with natural language are not marginally independent: they are related to each other through statements that are usually expressed in sentences (e.g., a classical symbolic AI fact or rule), involving only a few concepts at a time. This kind of assumption has been proposed to help discover such linguistically relevant high-level representations from raw observations, such as the consciousness prior (Bengio, 2017), with the idea that humans focus at any particular time on just a few concepts that are present to our consciousness. The work presented here could provide an interesting meta-learning objective to help learn such encoders as well as figure out how the resulting variables are related to each other. In that case, one should distinguish two important assumptions: the first one is that the causal graph is sparse (has few edges, as in the consciousness prior (Bengio, 2017) and in some methods to learn Bayes net structure, e.g. (Schmidt et al., 2007)); and the second one is that it changes sparsely due to interventions (which is the focus of this work).\nApproaches for Bayesian network structure learning based on discrete search over model structures and simulated annealing are reviewed in Heckerman et al. (1995). There, it has been common to use Minimum Description Length (MDL) principles to score and search over models Lam and Bacchus (1993); Friedman and Goldszmidt (1998), or the Bayesian Information Criterion (BIC) to search for models with high relative posterior probability Heckerman et al. (1995). Prior work such as Heckerman et al. (1995) has also relied upon purely observational data, without the possibility of interventions and therefore focused on learning likelihood or hypothesis equivalence classes for network structures. Since then, numerous methods have also been devised to infer the causal direction from purely observational data (Peters et al., 2017), based on specific, generally parametric assumptions, on the underlying causal graph. Pearl\u2019s seminal work on do-calculus Pearl (1995, 2009); Bareinboim and Pearl (2016) lays a foundation for expressing the impact of interventions on probabilistic graphical models \u2013 we use it in our work. In contrast, here we are proposing a meta-learning objective function for learning causal structure, not requiring any specific constraints on causal graph structure, only on the sparsity of the changes in distribution in the correct causal graph parametrization.\nOur work is also related to other recent advances in causation, domain adaptation and transfer learning. Magliacane et al. (2018) have sought to identify a subset of features that lead to the best predictions for a variable of interest in a source domain such that the conditional distribution of the variable of interest given these features is the same in the target domain. Johansson et al. (2016) examine counterfactual inference and formulate it as a domain adaptation problem. Shalit et al. (2017) propose a technique called counterfactual regression for estimating individual treatment effects from observational data. Rojas-Carulla et al. (2018) propose a method to find an optimal subset that makes the target independent from the selection variables. To do so, they make the assumption that if the conditional distribution of the target given some subset is invariant across different source domains, then this conditional distribution must also be the same in the target domain. Parascandolo et al. (2017) propose an algorithm to recover a set of independent causal mechanisms by establishing competition between mechanisms, hence driving specialization. Alet et al. (2018) proposed a meta learing algorithm to recover a set of specialized modules, but did not establish any connections to causal mechanisms. More recently, Dasgupta et al. (2019) adopted a meta-learning approach to draw causal inferences from purely observational data."
                },
                {
                    "heading": "5. Conclusion and Future Work",
                    "text": "We have established in very simple bivariate settings that the rate at which a learner adapts to sparse changes in the distribution of observed data can be exploited to select or optimize causal structure and disentangle the causal variables. This relies on the assumption that with the correct causal structure, those distributional changes are localized and sparse. We have demonstrated these ideas through theoretical results as well as experimental validation. See https://github.com/authors-1901-10912/ A-Meta-Transfer-Objective-For-Learning-To-Disentangle-Causal-Mechanisms for source code of the experiments.\nThis work is only a first step in the direction of optimizing causal structure based on the speed of adaptation to modified distributions. On the experimental side, many settings other than those studied here should be considered, with different kinds of parametrizations, richer and larger causal graphs, different kinds of optimization procedures, etc. Also, much more needs to be done in exploring how the proposed ideas can be used to learn good representations in which the causal variables are disentangled, since we have only experimented at this point with the simplest possible encoder with a single degree of freedom. Scaling up these ideas would permit their application towards improving the way in which learning agents deal with non-stationarities, and thus improving sample complexity and robustness of learning agents."
                },
                {
                    "heading": "Acknowledgements",
                    "text": "We would like to acknowledge support for this project from NSERC, CIFAR and Canada Research Chairs, as well as the feedback from Re\u0301mi Le Priol, Isabelle Lacroix, Alexandre Piche\u0301, and Akram Erraqabi. AG would like to thank Sergey Levine, Chelsea Finn, Michael Chang, Abhishek Gupta for useful discussions."
                },
                {
                    "heading": "Appendix A. Results on Non-Identifiability of Causal Structure",
                    "text": "We show here that the maximum likelihood estimation of both models specified in Equation (1) yields the same estimated distribution over A and B, i.e., the joint likelihood on the training distribution is not sufficient to distinguish the A\u2192 B and B \u2192 A causal models, in the non-parametric case (no assumption at all on the family of distributions). Let\n\u03b8i = PA\u2192B(A = i) \u03b8j|i = PA\u2192B(B = j | A = i) \u03b7j = PB\u2192A(B = j) \u03b7i|j = PB\u2192A(A = i | B = j).\nWe now state the maximum likelihood estimators for each models:\n\u03b8\u0302i = ni/n \u03b8\u0302j|i = nij/ni\n\u03b7\u0302j = nj/n \u03b7\u0302i|j = nij/nj (9)\nwhere n is the total number of observations, ni the number of times we observed A = i, nj the number of times we observed B = j and nij the number of times we observed A = i and B = j jointly. We can now compute the likelihood for each model:\nP\u0302A\u2192B(A,B) = \u03b8\u0302i\u03b8\u0302j|i = nij/n\nP\u0302B\u2192A(A,B) = \u03b7\u0302j \u03b7\u0302i|j = nij/n (10)\nwhich is what we intended to show. To illustrate this result, we also experiment with learning the modules for both models A \u2192 B and B \u2192 A with SGD. In Figure A.1, we show the difference in log-likelihoods between these two models, evaluated on training and test data sampled from the same distribution, during training. We can see that while the model A\u2192 B fits the data faster than the other model (corresponding to a positive difference in Figure A.1), both models achieve the same log-likelihoods on both models at convergence. This shows that the two models are indistinguishable based on data sampled from the same distribution, even on test data."
                },
                {
                    "heading": "Appendix B. Proof of the Zero-Gradient Proposition",
                    "text": "Let us restate more formally and prove Proposition 1.\nProposition 1 Consider conditional probability modules P\u03b8i(Vi|pa(i, V,Bi)) where Bij = 1 indicates that Vj is among the parents pa(i, V,Bi) of Vi in a directed acyclic causal graph. Consider ground truth training distribution P1 and transfer distribution P2 over these variables, and ground truth causal structure B. The joint log-likelihood L(V ) for a sample V with respect to the module parameters \u03b8 decomposed into module parameters \u03b8i is L(V ) = \u2211 i logP\u03b8i(Vi|pa(i, V,Bi)). If (a) a model has the correct causal structure B, and (b) it been trained perfectly on P1, leading to estimated parameters \u03b8, and (c) the ground truth P1 and P2 only differ from each other only for some P (Vi|pa(i, V,Bi)) for i \u2208 C, then EV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] = 0 for i /\u2208 C.\nProof Let V\u2212i be the subset of V excluding Vi. We can simplify the expected gradient as follows.\nEV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] =\u2211\nV P2(V ) \u2211 k \u2202 \u2202\u03b8i logP\u03b8k(Vk|pa(k, V,Bk))\n= 1i\u2208C \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P2(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi)) + 1i/\u2208C \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P1(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi))\n(11)\nwhere the second equality is obtained because \u03b8i does not influence module k 6= i, and P2 is the same P1 for conditionals with i /\u2208 C (assumption (c)). Now for the special case of i /\u2208 C, we obtain\nEV\u223cP2 [ \u2202L(V ) \u2202\u03b8i ] = \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P1(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi)) = \u2211 V\u2212i P2(V\u2212i) \u2211 Vi P\u03b8i(Vi|pa(i, V,Bi))\n\u2202\n\u2202\u03b8i logP\u03b8i(Vi|pa(i, V,Bi))\n= 0 (12)\nwhere the second equality arises from assumption (b), and the last line from zeroing the inner sum via the general identity \u2211\nv\np\u03b8(v) \u2202\n\u2202\u03b8 log p\u03b8(v) =\n\u2202\n\u2202\u03b8 \u2211 v p\u03b8(v) = \u22021 \u2202\u03b8 = 0."
                },
                {
                    "heading": "Appendix C. Pseudo-Code",
                    "text": "Draw initial meta-parameters of learner Draw a training set from training distr. Set causal structure to include all edges Initialize learner parameters for this model Pre-train the learner\u2019s parameters on the training set Repeat J times\nDraw a transfer distr. Draw causal structure(s) according to meta-parameters Repeat T times\nSample minibatch from transfer distribution Accumulate online log-likelihood of minibatch Update the model parameters accordingly\nCompute the meta-parameters gradient estimator Update the meta-parameters by SGD Optionally reset parameters to pre-training value\nAlgorithm 1: Meta-Transfer Learning of Causal Structure"
                },
                {
                    "heading": "Appendix D. Proof of the Structural Parameter Gradient Proposition",
                    "text": "Let us restate more formally and prove Proposition 2.\nProposition 2 The gradient of the negative log-likelihood regret of the transfer data\nR = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A]\nwith respect to the structural parameter \u03b3 (where \u03c3(\u03b3) = P (A\u2192 B)) is given by\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2), (13)\nwhere D2 is the transfer data, and P (A \u2192 B | D2) is the posterior probability of the hypothesis A \u2192 B (when the alternative is B \u2192 A), defined by applying Bayes rule to P (D2 | A \u2192 B) = \u220fT t=1 P (at, bt|A \u2192 B, \u03b8t) = LA\u2192B. Furthermore, this can be equivalently written as\n\u2202R \u2202\u03b3 = \u03c3(\u03b3)\u2212 \u03c3(\u03b3 + \u2206), (14)\nwhere \u2206 = logLA\u2192B\u2212 logLB\u2192A is the difference between the log-likelihoods of the two hypotheses on transfer data D2.\nProof First note that, using Bayes rule,\nP (A\u2192 B | D2) = P (D2 | A\u2192 B)P (A\u2192 B) P (D2 | A\u2192 B)P (A\u2192 B) + P (D2 | B \u2192 A)P (B \u2192 A)\n= LA\u2192B\u03c3(\u03b3)\nLA\u2192B\u03c3(\u03b3) + LB\u2192A(1\u2212 \u03c3(\u03b3))\n= \u03c3(\u03b3)LA\u2192B\nM , (15)\nwhere M = \u03c3(\u03b3)LA\u2192B + (1\u2212 \u03c3(\u03b3))LB\u2192A is the online likelihood of the transfer data under the mixture, so that the regret is R = \u2212 logM . For the second line above, note that\nP (D2|A\u2192 B) = T\u220f t=1 P (at, bt|A\u2192 B, {(as, bs)}t\u22121s=1)\n= T\u220f t=1 P (at, bt|A\u2192 B, \u03b8t) = LA\u2192B (16)\nwhere \u03b8t encapsulates the information about {(as, bs)}t\u22121s=1) (through some adaptation procedure). Since we only consider the two hypotheses A\u2192 B and B \u2192 A, we also have P (B \u2192 A | D2) = (1\u2212\u03c3(\u03b3))LB\u2192AM = 1\u2212 P (A\u2192 B | D2). Then\n\u2202R \u2202\u03b3 = \u2212\u03c3(\u03b3)(1\u2212 \u03c3(\u03b3))LA\u2192B \u2212 \u03c3(\u03b3)(1\u2212 \u03c3(\u03b3))LB\u2192A M\n= \u03c3(\u03b3)P (B \u2192 A | D2) \u2212 (1\u2212 \u03c3(\u03b3))P (A\u2192 B | D2) = \u03c3(\u03b3) + \u03c3(\u03b3)P (A\u2192 B | D2) \u2212 P (A\u2192 B | D2)\u2212 \u03c3(\u03b3)P (A\u2192 B | D2) = \u03c3(\u03b3)\u2212 P (A\u2192 B | D2) (17)\nwhich concludes the first part of the proof. Moreover, in order to prove the equivalent formulation in Equation (14), it is sufficient to prove that P (A \u2192 B | D2) = \u03c3(\u03b3 + \u2206). Using the logit function \u03c3\u22121(z) = log z1\u2212z , and the expression in Equation (15), we have\n\u03c3\u22121(P (A\u2192 B | D2)) = log \u03c3(\u03b3)LA\u2192B\nM \u2212 \u03c3(\u03b3)LA\u2192B\n= log \u03c3(\u03b3)LA\u2192B\n(1\u2212 \u03c3(\u03b3))LB\u2192A\n= log \u03c3(\u03b3)\n1\u2212 \u03c3(\u03b3)\ufe38 \ufe37\ufe37 \ufe38 = \u03b3 +\nlogLA\u2192B \u2212 logLB\u2192A\ufe38 \ufe37\ufe37 \ufe38 = \u2206\n= \u03b3 + \u2206 (18)"
                },
                {
                    "heading": "Appendix E. Proof of the Proposition on the Convergence Point of Gradient",
                    "text": "Descent on the Structural Parameter\nWe use the same notation as in the above proof and statement.\nProposition 3 Stochastic gradient descent (with appropriately decreasing learning rate) on ED2 [R], with R = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A] and with steps following \u2202R\u2202\u03b3 converges towards sigmoid(\u03b3) = 1 if ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or \u03c3(\u03b3) = 0 otherwise.\nProof We are going to consider the fixed point of gradient descent when the gradient is zero, since we already know that SGD converges with an appropriately decreasing learning rate. Let us introduce some notation to simplify the algebra: p = sigmoid(\u03b3), M = pLA\u2192B + (1\u2212 p)LB\u2192A, so R = logM , and define\nP1 = pLA\u2192B M = P (A\u2192 B | D2), and P2 = (1\u2212p)LB\u2192A M = 1\u2212 P1. Framing the stationary point in terms of p rather than \u03b3 gives us the inequality constraints \u2212p \u2264 0 and p\u2212 1 \u2264 0 and no equality constraint. Applying the KKT conditions with constraint functions \u2212p and p\u2212 1 gives us\nED2 [ \u2202R \u2202p ] = \u2212\u00b51 + \u00b52\n\u00b5i \u2265 0 \u00b51p = 0\n\u00b52(p\u2212 1) = 0 (19)\nWe already see from the last two equations that if p \u2208 (0, 1) (i.e. excluding 0 and 1), we must have \u00b51 = \u00b52 = 0, i.e., E[ \u2202R \u2202p ] = 0 (with drop the D2 subscript on E when it is clear from context). Let us study that case first and show that it leads to an inconsistent set of equations (thus forcing the solution to be either p = 0 or p = 1). Let us rewrite the gradient to highlight p in it:\n\u2202R \u2202p =P (A\u2192 B | D2)\u2212 p\n= pLA\u2192B\npLA\u2192B + (1\u2212 p)LB\u2192A \u2212 p\n= pLA\u2192B \u2212 p(pLA\u2192B + (1\u2212 p)LB\u2192A)\nM\n= p(1\u2212 p)LA\u2192B \u2212 p(1\u2212 p)LB\u2192A\nM\n=p(1\u2212 p)LA\u2192B \u2212 LB\u2192A M\n(20)\nThe KKT conditions with the above two inequality constraints for 0 \u2264 p \u2264 1 give\nE [ \u2202R \u2202p ] = \u00b52 \u2212 \u00b51. (21)\nIf we consider the solutions p \u2208 (0, 1) (i.e., \u00b51 = \u00b52 = 0) we now show that we get a contradiction. First note that to satisfy the above equation with \u00b51 = \u00b52 = 0 means that either p = 0 or p = 1 (which is inconsistent with the assumption that p \u2208 (0, 1)) or that E [LA\u2192B\u2212LB\u2192A M ] = 0. Let us consider that equation, and since p 6= 0 and p 6= 1 we can either multiply by p or by 1\u2212 p on both sides. Assuming p 6= 0 and multiplying by p gives\n0 = E\n[ p(LA\u2192B \u2212 LB\u2192A)\nM\n] = E [ P1 \u2212\npLB\u2192A M ] = E [ P1 \u2212\nLB\u2192A \u2212 LB\u2192A \u2212 pLB\u2192A M ] = E [ P1 +\nLB\u2192A M\n\u2212 P2 ]\n= E [ P1 +\nLB\u2192A M\n\u2212 (1\u2212 P1) ] = E [ LB\u2192A M \u2212 1 ] . (22)\nFor this equation to be satisfied, we need LB\u2192A = M all the time, since LB\u2192A \u2264M by construction. This would however correspond to p = 0. Similarly, assuming p 6= 1 we can multiply the stationarity equation by 1\u2212 p and get\n0 = E\n[ (1\u2212 p)(LA\u2192B \u2212 LB\u2192A)\nM ] = E [ (1\u2212 p)LA\u2192B\nM \u2212 P2 ] = E [ LA\u2192B M \u2212 P1 \u2212 (1\u2212 P1) ] = E [ LA\u2192B M \u2212 1 ]\n(23)\nAgain, this can only be 0 if LA\u2192B = M all the time, i.e., p = 1. We conclude that the solutions p \u2208 (0, 1) are not possible because they would lead to inconsistent conclusions, which leaves only p = 0 or p = 1. When p = 0 we have E[R] = E[logLA\u2192B ], and when p = 1 we have E[R] = E[logLB\u2192A]. Thus the minimum will be achieved at p = 1 when ED2 [logLA\u2192B ] > ED2 [logLB\u2192A], or p = 0 otherwise."
                },
                {
                    "heading": "Appendix F. More Than Two Causal Hypotheses",
                    "text": "In this section, we consider one approach to generalize to more than two causal structures. We consider m variables, corresponding to O(2m 2\n) possible causal graphs, since each variable Vj could be (or not) a direct cause of any variable Vi, leading to m\n2 binary decisions. Note that a causal graph can in principle have cycles (if time is not measured with sufficient precision), although having a directed acyclic graph allows a much simpler sampling procedure (ancestral sampling). In our experiments the ground truth graph will always be directed, to make sampling easier and faster, but the learning procedure will not directly assume that. Motivated by the mechanism independence assumption, we propose a heuristic to learn the causal graph in which we independently parametrize the binary probability pij that Vj is a parent (direct cause) of Vi. As was the case for Section 2, we parametrize this Binomial distribution via binary edges Bij that specify the graph structure:\nBij \u223c Bernoulli(pij), P (B) = \u220f ij P (Bij). (24)\nwhere pij = sigmoid(\u03b3ij). Let us define the parents of Vi, given B, as the set of Vj \u2019s such that Bij = 1:\npa(i, V,Bi) = {Vj | Bij = 1, j 6= i} (25)\nwhere Bi is the bit vector with elements Bij (and Bii = 0 is ignored). Similarly, we could parametrize the causal graph with a structural causal model where some of the inputs (from variable j) of each function (for variable i) can be ignored with some probability pij :\nVi = fi(\u03b8i, Bi, V,Ni) (26)\nwhere Ni is an independent noise source to generate Vi and fi parametrizes the generator (as in a GAN), while not being allowed to use variable Vj unless Bij = 1 (and of course not being allowed to use Vi). We can consider that fi is a kind of neural network similar to the denoising auto-encoders or with dropout on the input, where Bi is a binary mask vector that prevents fi from using some of the Vj \u2019s (for which Bij = 0).\nThe conditional likelihood PBi(Vi = vti | pa(i, vt, Bi)) measures how well the model that uses the incoming edges Bi for node i performs for example vt. We build a multiplicative (or exponentiated) form of regret by multiplying these likelihoods as \u03b8t changes during an adaptation episode, for node i:\nLBi = \u220f t PBi(Vi = vti | pa(i, vt, Bi)). (27)\nThe overall exponentiated regret for the given graph structure B is LB = \u220f i LBi . Similarly to the bivariate case, we want to consider a mixture over all the possible graph structures, but where each component must explain the whole adaptation sequence, thus we define as a loss for the generalized multi-variable case\nR = \u2212 logEB [LB ] (28)\nNote the expectation over the 2m 2\npossible values of B, which is intractable. However, we can still get an efficient stochastic gradient estimator, which can be computed separately for each node of the graph (with samples arising only out of Bi, the incoming edges into Vi):\nProposition 4 The overall regret (Equation (28)) rewrites R = \u2212 \u2211 i log \u2211 Bi P (Bi)LBi (29)\nand if we are willing to consider multiple samples of B in parallel, a biased but asymptotically unbiased (as the number K of these samples B(k) increases to infinity) estimator of the gradient of the overall regret with respect to meta-parameters can be defined:\ngij =\n\u2211 k(\u03c3(\u03b3ij)\u2212B (k) ij )L\n(k) Bi\u2211\nk L (k) Bi\n(30)\nwhere the (k) index indicates the values obtained for the k-th draw of B. Proof Recall that LB = \u220f i LBi so we can rewrite the regress loss as follows:\nR = \u2212 logEB [LB ] = \u2212 log \u2211 B P (B)LB\n= \u2212 log \u2211 B1 \u2211 B2 . . . \u2211 BM \u220f i P (Bi)LBi\n= \u2212 log \u220f i (\u2211 Bi P (Bi)LBi ) = \u2212\n\u2211 i log \u2211 Bi P (Bi)LBi (31)\nSo the regret gradient on meta-parameters \u03b3i of node i is\n\u2202R \u2202\u03b3i\n= \u2212 \u2211 Bi P (Bi)LBi \u2202 logP (Bi) \u2202\u03b3i\u2211\nBi P (Bi)LBi\n= \u2212 EBi [LBi \u2202 logP (Bi) \u2202\u03b3i ]\nEBi [LBi ] (32)\nNote that with the sigmoidal parametrization of P (Bij),\nlogP (Bij) = Bij log sigmoid(\u03b3ij) + (1\u2212Bij) log(1\u2212 sigmoid(\u03b3ij))\nas in the cross-entropy loss. Its gradient can similarly be simplified to\n\u2202 logP (Bij)\n\u2202\u03b3ij = Bij sigmoid(\u03b3ij) sigmoid(\u03b3ij)(1\u2212 sigmoid(\u03b3ij))\n\u2212 (1\u2212Bij) (1\u2212 sigmoid(\u03b3ij)) sigmoid(\u03b3ij)(1\u2212 sigmoid(\u03b3ij)))\n= Bij \u2212 sigmoid(\u03b3ij) (33)\nA biased but asymptotically unbiased estimator of \u2202R\u2202\u03b3ij is thus obtained by sampling K graphs (over which the means below are run):\ngij = \u2211 k (\u03c3(\u03b3ij)\u2212B(k)ij ) L B (k) i\u2211 k\u2032 LB(k\u2032)i (34)\nwhere index (k) indicates the k-th draw of B, and we obtain a weighted sum of the individual binomial gradients weighted by the relative regret of each draw B (k) i of Bi, leading to Equation (30).\nThis decomposition is good news because the loss is a sum of independent terms, one per node i, depending only of Bi and and similarly gij only depends on Bi rather than the full graph structure. We use the estimator from Equation (30) in the general pseudo-code for meta-transfer learning of causal structure displayed in Algorithm 1."
                },
                {
                    "heading": "Appendix G. Results on Learning which is Cause and which is Effect",
                    "text": "In order to assess the performance of our meta-learning algorithm, we applied it on generated data from three different domains: discrete random variables, multimodal continuous random variables and multivariate gaussian-distributed variables. In this section, we describe the setups for all three experiments, along with additional results to complement the results described in the main text. Note that in all these experiments, we fix the structure of the ground-truth to be A\u2192 B, and only perform interventions on the cause A.\nG.1 Discrete variables and Two Causal Hypotheses\nWe consider a bivariate model, where both random variables are sampled from a categorical distribution. The underlying ground-truth model can be described as\nA \u223c Categorical(\u03c0A) B | A = a \u223c Categorical(\u03c0B|a), (35)\nwith \u03c0A is a probability vector of size N , and \u03c0B|a is a probability vector of size N , which depends on the value of the variable A. In our experiment, each random variable can take one of N = 10 values. Since we are working with only two variables, the only two possible models are:\n\u2022 Model A\u2192 B: P (A,B) = P (A)P (B | A)\n\u2022 Model B \u2192 A: P (A,B) = P (B)P (A | B) We build 4 different modules, corresponding to the model of each possible marginal and conditional distribution. These modules\u2019 definition and their corresponding parameters are shown in Table G.1.\nDistribution Module Parameters Dimension\nModel A\u2192 B P (A) P (xA = i ; \u03b8A) = [softmax(\u03b8A)]i \u03b8A N P (B | A) P (xB = j | xA = i ; \u03b8B|A) = [softmax(\u03b8B|A(i))]j \u03b8B|A N2 Model B \u2192 A P (B) P (xB = j ; \u03b8B) = [softmax(\u03b8B)]j \u03b8B N P (A | B) P (xA = i | xB = j ; \u03b8A|B) = [softmax(\u03b8A|B(j))]i \u03b8A|B N2\nTable G.1: Description of the 2 models, with the parametrization of each module, for a bivariate model with discrete random variables. Model A\u2192 B and Model B \u2192 A both have the same number of parameters N2 +N .\nIn order to get a set of initial parameters, we first train all 4 modules on a training distribution. This training distribution corresponds to a fixed choice of \u03c0 (1) A and \u03c0B|a (for all N possible values of a). Note that the superscript in \u03c0 (1) A emphasizes the fact that this defines the distribution prior to intervention, with the mechanism P (B | A) being unchanged by the intervention. These probability vectors are sampled randomly from a uniform Dirichlet distribution\n\u03c0 (1) A \u223c Dirichlet(1N )\n\u03c0B|a \u223c Dirichlet(1N ) \u2200a \u2208 [1, N ]. (36)\nGiven this initial training distribution, we can sample a large dataset of training examples {(ai, bi)}ni=1 from the ground-truth model, using ancestral sampling.\na \u223c Categorical(\u03c0(1)A ) b \u223c Categorical(\u03c0B|a). (37)\nUsing this large dataset from the training distribution, we can train all 4 modules using gradient descent, or any other advanced first-order optimizer, like RMSprop. The parameters \u03b8A, \u03b8B|A, \u03b8B & \u03b8A|B of the different modules found after this initial training will be used as the initial parameters for the adaptation on a new transfer distribution.\nSimilar to the way we defined the training distribution, we can define a transfer distribution as a soft intervention on the random variable A. In this experiment, this accounts for changing the distribution of A, that is with a new probability vector \u03c0 (2) A , also sampled randomly from a uniform Dirichlet distribution\n\u03c0 (2) A \u223c Dirichlet(1N ) (38)\nTo perform adaptation on the transfer distribution, we also sample a smaller dataset of transfer examples D2 = {(ai, bi}mi=1, with m n the size of the training set. In our experiment, we used m = 20 transfer examples. We also used ancestral sampling on this new transfer distribution to acquire samples, similar to Equation (37) (with \u03c0 (2) A instead of \u03c0 (1) A ).\nStarting from the parameters estimated after the initial training on the training distribution, we perform a few steps of adaptation on the modules parameters \u03b8A, \u03b8B|A, \u03b8B & \u03b8A|B using T steps of gradient descent based on the transfer dataset D2. The value of the likelihoods for both models is recorded as well, and computed as\nLA\u2192B = T\u220f t=1 P (at | \u03b8(t)A )P (bt | at ; \u03b8 (t) B|A)\nLB\u2192A = T\u220f t=1 P (bt | \u03b8(t)B )P (at | bt ; \u03b8 (t) A|B), (39)\nwhere (at,bt) represents a mini-batch of examples from D2, and the superscript t on the parameters highlights the fact that these likelihoods are computed after t steps of adaptation. This product over t ensures that we monitor the progress of adaptation along the whole trajectory. In this experiment, we used T = 2 steps of gradient descent on mini-batch of size 10 for the adaptation.\nFinally, in order to update the structural parameter \u03b3, we can use Proposition 2 to compute the gradient of the loss L with respect to \u03b3:\nR(\u03b3) = \u2212 log[\u03c3(\u03b3)LA\u2192B + (1\u2212 \u03c3(\u03b3))LB\u2192A] (40) \u2202R \u2202\u03b3 = \u03c3(\u03b3 + \u2206)\u2212 \u03c3(\u03b3), (41)\nwhere \u2206 = logLA\u2192B \u2212 logLB\u2192A. The update of \u03b3 can be one step of gradient descent, or using any first-order optimizer like RMSprop. We perform multiple interventions over the course of meta-training by sampling multiple transfer distributions, and following the same steps of adaptation and update of the structural parameter \u03b3.\nIn Figure 2, we report the evolution of the structural parameter \u03b3 (or rather, \u03c3(\u03b3)) as a function of the number of meta-training steps or, similarly, the number of different interventions made on the causal model. The model\u2019s belief P (A \u2192 B) = \u03c3(\u03b3) indeed converges to 1, proving that the algorithm was capable of recovering the correct causal direction A\u2192 B.\nG.2 Discrete Variables with MLP Parametrization\nWe consider a bivariate model similar to the ones defined above, where each random variable is sampled from a categorical distribution. Instead of expressing probabilities in a tabular form, we train M = 2 simple feed-foward neural networks (MLP), one per conditional variable. MLP i is the independent mechanism of causal variable i that determines the conditional probability of the N discrete choices for variable i, given its parents.\nEach MLP receives M concatenated N -dimensional one-hot vectors, masked appropriately according to the chosen causal structure B, i.e., with the j-th input of the i-th MLP being multiplied by Bij . Each\ndirected edge presence or absence is thus indicated by Bij , with Bij = 1 if variable j is a direct causal parent of variable i. The MLP maps the MN input units through one hidden layer that contains H = 4M hidden units and a ReLU non-linearity, and then maps the H hidden units to N output units and a softmax representing a predicted categorical distribution.\nThe causal structure belief is specified by an M \u00d7M matrix \u03b3, with \u03c3(\u03b3ij) the estimated probability that variable i is directly caused by variable j. The causal structure Bij is drawn from Ber(\u03b3ij), as per Algorithm 1. We generalize the estimator introduced for the 2-hypotheses case as per Appendix F, i.e., we use the gradient estimator in Equation 30.\nTo evaluate the correctness of the structure being learnt, we measure the cross entropy between the ground-truth SCM and the learned SCM. In Figure 3 we show this cross-entropy over different episodes of training for bivariate discrete distributions with either 10 categories or 100 categories. Both models are first pretrained for 100 examples with fully connected edges before starting training on the transfer distributions.\nG.3 Continuous Multimodal Variables\nConsider a family of joint distributions P\u00b5(A,B) over the causal variables A and B sampled from the structural causal model (SCM):\nA \u223c P\u00b5(A) = N (\u00b5, \u03c32 = 4) B := f(A) +NB NB \u223c N (\u00b5 = 0, \u03c32 = 1) (42)\nwhere f is a randomly generated spline and NB is sampled i.i.d from the unit-normal distribution. To obtain the spline, we sample the K points {xk}Kk=1 uniformly spaced from the interval [\u2212RA, RA], and another K points {yk}Kk=1 uniform randomly from the interval [\u2212RB , RB ]. This yields K pairs {(xk, yk)}Kk=1, which make the knots of a second-order spline. We set K = 8, RA = RB = 8 for our experiments. In Figure G.1, we plot samples from one such SCM for the training distribution (\u00b5 = 0) and two transfer distributions (\u00b5 = \u00b14).\n10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 A\n15\n10\n5\n0\n5\n10\nB\nTransfer ( = 4) Transfer ( = + 4) Training\nFigure G.1: Train (red) and transfer (green and blue) samples from an SCM generated with the procedure described in Equation (42). The green data-points are sampled from P\u00b5=(\u22124)(A,B), whereas the blue data-points are samples from P\u00b5=(+4)(A,B) and the red data points (training set) are from P\u00b5=0(A,B).\nThe conditionals P (B | A ; \u03b8B|A) and P (A | B ; \u03b8A|B) are parameterized as 2-layer Mixture Density Networks Bishop (1994) with 32 hidden units and 10 components. The marginals P (A | \u03b8A) and P (B | \u03b8B) are parameterized as Gaussian Mixture Models, also with 10 components. The training now follows as described below.\nSimilar to Appendix G.1, we first pre-train the modules corresponding to the conditionals and marginals on the training distribution. To that end, we select P\u00b5=0(A,B) as the training distribution, sample a\n(large) training dataset {(ai, bi)}ni=1 from it using ancestral sampling, and solve the following two problems independently until convergence:\nmax \u03b8A,\u03b8B|A n\u2211 i=1 logP (ai | \u03b8A)P (bi | ai; \u03b8B|A) (43)\nmax \u03b8B ,\u03b8A|B n\u2211 i=1 logP (bi | \u03b8B)P (ai | bi; \u03b8A|B) (44)\nThe adaptation performance of A\u2192 B and B \u2192 A models can now be evaluated on transfer distributions. For a \u00b5 sampled uniformly in [\u22124, 4], we select P\u00b5(A\u2032, B\u2032) as the transfer distribution, and denote with (A\u2032, B\u2032) samples from it. Both models are fine-tuned on P\u00b5(A\n\u2032, B\u2032) for T = 10 iterations (see Algorithm 1), and the area under the corresponding negative-log-likelihood curves becomes the regret:\nRA\u2192B = \u2212 T\u2211 t=1 logP (B\u2032|A\u2032; \u03b8(t)A\u2192B)P (A \u2032|\u03b8(T )A\u2192B) (45)\nand likewise for RB\u2192A. In these experiments, the modules corresponding to the marginals (ie. GMM) are learned offline via Expectation Maximization, and we denote with P (A\u2032|\u03b8(T )A\u2192B) the trained model. These can now be used to define the following meta-objective for the structural meta-parameter \u03b3:\nR(\u03b3) = log[\u03c3(\u03b3)eRA\u2192B + (1\u2212 \u03c3(\u03b3))eRB\u2192A ] (46)\nThe structural regretR(\u03b3) is now minimized with respect to \u03b3 for 200 iterations (updates of \u03b3). Figure G.2 shows the evolution of \u03c3(\u03b3) as training progresses. This is expected, given that we expect the causal model to perform better on the transfer distributions, i.e. we expect RA\u2192B < RB\u2192A in expection. Consequently, assigning a larger weight to RA\u2192B optimizes the objective.\n0 100 200 300 400 Number of episodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n( )\nN = 10\nFigure G.2: Evolution of the sigmoid of structural meta-parameter \u03c3(\u03b3) with training iterations. It is indeed expected to increase if A\u2192 B is the true causal graph (see Equation (46)).\nG.4 Linear Gaussian Model\nIn this experiment, the two variables we consider are vectors (i.e. A \u2208 Rd and B \u2208 Rd). The ground truth causal model is given by\nA \u223c N (\u00b5A,\u03a3A) B := \u03b21A+ \u03b20 +NB NB \u223c N (0,\u03a3B) (47)\nwhere \u00b5A \u2208 Rd, \u03b20 \u2208 Rd and \u03b21 \u2208 Rd\u00d7d. \u03a3A and \u03a3B are d\u00d7 d covariance matrices2. In our experiments, d = 100. Once again, we want to identify the correct causal direction between A and B. To do so, we consider two models: A\u2192 B and B \u2192 A. We parameterize both models symmetrically:\nPA\u2192B(A) = N (A; \u00b5\u0302A, \u03a3\u0302A) PA\u2192B(B | A = a) = N (B; W\u03021a+ W\u03020, \u03a3\u0302A\u2192B)\nPB\u2192A(B) = N (B; \u00b5\u0302B , \u03a3\u0302B) PB\u2192A(A | B = b) = N (B; V\u03021b+ V\u03020, \u03a3\u0302B\u2192A) (48)\nNote that each covariance matrix is parameterized using the Cholesky decomposition. Unlike previous experiments, we are not conducting any pre-training on actual data. Instead, we fix the parameters of both models to their exact values according to the ground truth parameters introduced in Equation 47. For model A\u2192 B, this can be done trivially. For the second model, we can compute its exact parameters analytically. Once the exact parameters are set, both models are equivalent in the sense that PA\u2192B(A,B) = PB\u2192A(A,B) \u2200A,B.\nEach meta-learning episode starts by initializing the parameters of both models to the values identified during the pre-training. Afterward, a transfer distribution is sampled (i.e. \u00b5A \u223c N (0, I)). Then, both models are trained on samples from this distribution, for 10 iterations only. During this adaptation, the log-likelihoods of both models are accumulated in order to compute LA\u2192B and LB\u2192A. At this stage, we compute the meta objective estimate R = \u2212 log [sigmoid(\u03b3)LA\u2192B + (1\u2212 sigmoid(\u03b3))LB\u2192A], compute its gradient w.r.t. \u03b3 and update \u03b3.\nFigure G.3 shows that, after 200 episodes, \u03c3(\u03b3) converges to 1, indicating the success of the method on this particular task.\nFigure G.3: Convergence of the causal belief (to the correct answer) as a function of the number of metalearning episodes, for the linear Gaussian experiments."
                },
                {
                    "heading": "Appendix H. Results on Learning the Correct Encoder",
                    "text": "The causal variables (A,B) are sampled from the distribution described in Eqn 42, and are mapped to observations (X,Y ) \u223c P\u00b5(X,Y ) via a hidden (and a priori unknown) decoder D = R(\u03b8D), where R is a rotation matrix. The observations are then mapped to the hidden state (U, V ) \u223c P\u00b5(U, V ) via the encoder E = R(\u03b8E). The computational graph is depicted in Figure 4.\n2. Ground truth parameters \u00b5A, \u03b21 and \u03b20 are sampled from a Gaussian distribution, while \u03a3A and \u03a3B are sampled from an inverse Wishart distribution.\nAnalogous to Equation 46 in Appendix G.3, we now define the regret over the variables (U, V ) instead of (A,B):\nR(\u03b3, \u03b8E) = log[\u03c3(\u03b3)eRU\u2192V + (1\u2212 \u03c3(\u03b3))eRV\u2192U ] (49)\nwhere the dependence on \u03b8E is implicit in (U, V ). In every meta-training iteration, the U \u2192 V and V \u2192 U models are trained on the training distribution P\u00b5=0(U, V ) for T\n\u2032 = 20 iterations. Subsequently, the regrets RU\u2192V and RV\u2192U are obtained by a process identical to that described in Equation 45 of Appendix G.3 (albeit with variables (U, V ) and T = 5). Finally, the gradients of R(\u03b3, \u03b8E) are evaluated and the metaparameters \u03b3 and \u03b8E are updated. This process is repeated for 1000 meta-iterations, and Figure 5 shows the evolution of \u03b8E as training progresses (where \u03b8D has been set to \u2212\u03c04 ). Further, Figure H.1 shows the corresponding evolution of the structural parameter \u03b3.\n0 200 400 600 800 1000 Iterations\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSt ru\nct ur\nal P\nar am\net er\nFigure H.1: Evolution of the structural parameter \u03b3 as training progresses with the encoder. The corresponding evolution of the encoder parameter \u03b8E is shown in Figure 5. Observe that the system converges to \u03b8 = 0, implying that the correct causal direction is U \u2192 V and the parameter \u03b3 should increase with meta-training iterations."
                }
            ],
            "year": 2019,
            "references": [
                {
                    "title": "Modular meta-learning",
                    "authors": [
                        "Ferran Alet",
                        "Tom\u00e1s Lozano-P\u00e9rez",
                        "Leslie P Kaelbling"
                    ],
                    "venue": "arXiv preprint arXiv:1806.10166,",
                    "year": 2018
                },
                {
                    "title": "Causal inference and the data-fusion problem",
                    "authors": [
                        "Elias Bareinboim",
                        "Judea Pearl"
                    ],
                    "venue": "Proceedings of the National Academy of Sciences,",
                    "year": 2016
                },
                {
                    "title": "The consciousness prior",
                    "authors": [
                        "Yoshua Bengio"
                    ],
                    "venue": "arXiv preprint arXiv:1709.08568,",
                    "year": 2017
                },
                {
                    "title": "Representation learning: A review and new perspectives",
                    "authors": [
                        "Yoshua Bengio",
                        "Aaron Courville",
                        "Pascal Vincent"
                    ],
                    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),",
                    "year": 2013
                },
                {
                    "title": "Mixture density networks",
                    "authors": [
                        "Christopher M Bishop"
                    ],
                    "venue": "Technical report, Citeseer,",
                    "year": 1994
                },
                {
                    "title": "Causal Reasoning from Meta-reinforcement Learning",
                    "authors": [
                        "Ishita Dasgupta",
                        "Jane Wang",
                        "Silvia Chiappa",
                        "Jovana Mitrovic",
                        "Pedro Ortega",
                        "David Raposo",
                        "Edward Hughes",
                        "Peter Battaglia",
                        "Matthew Botvinick",
                        "Zeb Kurth-Nelson"
                    ],
                    "year": 1901
                },
                {
                    "title": "A general lower bound on the number of examples needed for learning",
                    "authors": [
                        "Andrzej Ehrenfeucht",
                        "David Haussler",
                        "Michael Kearns",
                        "Leslie Valiant"
                    ],
                    "venue": "Information and Computation,",
                    "year": 1989
                },
                {
                    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                    "authors": [
                        "Chelsea Finn",
                        "Pieter Abbeel",
                        "Sergey Levine"
                    ],
                    "venue": "International Conference on Machine Learning (ICML),",
                    "year": 2017
                },
                {
                    "title": "Learning bayesian networks with local structure. In Learning in graphical models, pages 421\u2013459",
                    "authors": [
                        "Nir Friedman",
                        "Moises Goldszmidt"
                    ],
                    "year": 1998
                },
                {
                    "title": "Deep Learning. MIT Press, 2016",
                    "authors": [
                        "Ian J. Goodfellow",
                        "Yoshua Bengio",
                        "Aaron Courville"
                    ],
                    "venue": "URL http: //deeplearningbook.org",
                    "year": 2016
                },
                {
                    "title": "Learning bayesian networks: The combination of knowledge and statistical data",
                    "authors": [
                        "David Heckerman",
                        "Dan Geiger",
                        "David M Chickering"
                    ],
                    "venue": "Machine learning,",
                    "year": 1995
                },
                {
                    "title": "Independent Component Analysis",
                    "authors": [
                        "Aapo Hyv\u00e4rinen",
                        "Juha Karhunen",
                        "Erkki Oja"
                    ],
                    "venue": "ISBN 047140540X",
                    "year": 2001
                },
                {
                    "title": "Nonlinear ica using auxiliary variables and generalized contrastive learning",
                    "authors": [
                        "Aapo Hyv\u00e4rinen",
                        "Hiroaki Sasaki",
                        "Richard E. Turner"
                    ],
                    "year": 2018
                },
                {
                    "title": "Learning representations for counterfactual inference",
                    "authors": [
                        "Fredrik Johansson",
                        "Uri Shalit",
                        "David Sontag"
                    ],
                    "venue": "In International Conference on Machine Learning,",
                    "year": 2016
                },
                {
                    "title": "Probabilistic Graphical Models: Principles and Techniques",
                    "authors": [
                        "Daphne Koller",
                        "Nir Friedman"
                    ],
                    "venue": "MIT press,",
                    "year": 2009
                },
                {
                    "title": "Using causal information and local measures to learn bayesian networks",
                    "authors": [
                        "Wai Lam",
                        "Fahiem Bacchus"
                    ],
                    "venue": "In Proceedings of the Ninth international conference on Uncertainty in artificial intelligence,",
                    "year": 1993
                },
                {
                    "title": "Challenging common assumptions in the unsupervised learning of disentangled",
                    "authors": [
                        "Francesco Locatello",
                        "Stefan Bauer",
                        "Mario Lucic",
                        "Sylvain Gelly",
                        "Bernhard Sch\u00f6lkopf",
                        "Olivier Bachem"
                    ],
                    "venue": "representations. CoRR,",
                    "year": 2018
                },
                {
                    "title": "Domain adaptation by using causal inference to predict invariant conditional distributions",
                    "authors": [
                        "Sara Magliacane",
                        "Thijs van Ommen",
                        "Tom Claassen",
                        "Stephan Bongers",
                        "Philip Versteeg",
                        "Joris M Mooij"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2018
                },
                {
                    "title": "On First-Order Meta-Learning Algorithms",
                    "authors": [
                        "Alex Nichol",
                        "Joshua Achiam",
                        "John Schulman"
                    ],
                    "year": 2018
                },
                {
                    "title": "Learning independent causal mechanisms",
                    "authors": [
                        "Giambattista Parascandolo",
                        "Niki Kilbertus",
                        "Mateo Rojas-Carulla",
                        "Bernhard Sch\u00f6lkopf"
                    ],
                    "venue": "arXiv preprint arXiv:1712.00961,",
                    "year": 2017
                },
                {
                    "title": "Causal diagrams for empirical research",
                    "authors": [
                        "Judea Pearl"
                    ],
                    "venue": "Biometrika, 82(4):669\u2013688,",
                    "year": 1995
                },
                {
                    "title": "Elements of causal inference: foundations and learning algorithms",
                    "authors": [
                        "Jonas Peters",
                        "Dominik Janzing",
                        "Bernhard Sch\u00f6lkopf"
                    ],
                    "venue": "MIT press,",
                    "year": 2017
                },
                {
                    "title": "Invariant models for causal transfer learning",
                    "authors": [
                        "Mateo Rojas-Carulla",
                        "Bernhard Sch\u00f6lkopf",
                        "Richard Turner",
                        "Jonas Peters"
                    ],
                    "venue": "The Journal of Machine Learning Research,",
                    "year": 2018
                },
                {
                    "title": "Learning graphical model structure using l1-regularization paths",
                    "authors": [
                        "Mark W. Schmidt",
                        "Alexandru Niculescu-Mizil",
                        "Kevin P. Murphy"
                    ],
                    "venue": "In AAAI,",
                    "year": 2007
                },
                {
                    "title": "Understanding Machine Learning - from Theory to Algorithms",
                    "authors": [
                        "Shai Shalev-Shwartz",
                        "Shai Ben-David"
                    ],
                    "year": 2014
                },
                {
                    "title": "Estimating individual treatment effect: generalization bounds and algorithms",
                    "authors": [
                        "Uri Shalit",
                        "Fredrik D Johansson",
                        "David Sontag"
                    ],
                    "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
                    "year": 2017
                },
                {
                    "title": "On the uniform convergence of relative frequencies of events to their probabilities",
                    "authors": [
                        "V.N. Vapnik",
                        "A.Y. Chervonenkis"
                    ],
                    "venue": "Theory of Probability and its Applications,",
                    "year": 1971
                }
            ],
            "id": "SP:492ba3ad3f0cb85f0636bc275fecd7e7960709da",
            "authors": [
                {
                    "name": "Yoshua Bengio",
                    "affiliations": []
                },
                {
                    "name": "Tristan Deleu",
                    "affiliations": []
                },
                {
                    "name": "Nasim Rahaman",
                    "affiliations": []
                },
                {
                    "name": "Nan Rosemary Ke",
                    "affiliations": []
                },
                {
                    "name": "S\u00e9bastien Lachapelle",
                    "affiliations": []
                },
                {
                    "name": "Olexa Bilaniuk",
                    "affiliations": []
                },
                {
                    "name": "Anirudh Goyal",
                    "affiliations": []
                },
                {
                    "name": "Christopher Pal",
                    "affiliations": []
                }
            ],
            "abstractText": "We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.",
            "title": "A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms"
        }
    },
    "45612165": {
        "X": {
            "sections": [
                {
                    "heading": "1. Introduction",
                    "text": "The current success of deep learning hinges on the ability to apply gradient-based optimization to high-capacity models. This approach has achieved impressive results on many large-scale supervised tasks with raw sensory input, such as image classification (He et al., 2015), speech recognition (Yu & Deng, 2012), and games (Mnih et al., 2015; Silver et al., 2016). Notably, performance in such tasks is typically evaluated after extensive, incremental training on large data sets. In contrast, many problems of interest re-\nquire rapid inference from small quantities of data. In the limit of \u201cone-shot learning,\u201d single observations should result in abrupt shifts in behavior.\nThis kind of flexible adaptation is a celebrated aspect of human learning (Jankowski et al., 2011), manifesting in settings ranging from motor control (Braun et al., 2009) to the acquisition of abstract concepts (Lake et al., 2015). Generating novel behavior based on inference from a few scraps of information \u2013 e.g., inferring the full range of applicability for a new word, heard in only one or two contexts \u2013 is something that has remained stubbornly beyond the reach of contemporary machine intelligence. It appears to present a particularly daunting challenge for deep learning. In situations when only a few training examples are presented one-by-one, a straightforward gradient-based solution is to completely re-learn the parameters from the data available at the moment. Such a strategy is prone to poor learning, and/or catastrophic interference. In view of these hazards, non-parametric methods are often considered to be better suited.\nHowever, previous work does suggest one potential strategy for attaining rapid learning from sparse data, and hinges on the notion of meta-learning (Thrun, 1998; Vilalta & Drissi, 2002). Although the term has been used in numerous senses (Schmidhuber et al., 1997; Caruana, 1997; Schweighofer & Doya, 2003; Brazdil et al., 2003), meta-learning generally refers to a scenario in which an agent learns at two levels, each associated with different time scales. Rapid learning occurs within a task, for example, when learning to accurately classify within a particular dataset. This learning is guided by knowledge accrued more gradually across tasks, which captures the way in which task structure varies across target domains (Giraud-Carrier et al., 2004; Rendell et al., 1987; Thrun, 1998). Given its two-tiered organization, this form of meta-\nar X\niv :1\n60 5.\n06 06\n5v 1\n[ cs\n.L G\n] 1\n9 M\nlearning is often described as \u201clearning to learn.\u201d\nIt has been proposed that neural networks with memory capacities could prove quite capable of meta-learning (Hochreiter et al., 2001). These networks shift their bias through weight updates, but also modulate their output by learning to rapidly cache representations in memory stores (Hochreiter & Schmidhuber, 1997). For example, LSTMs trained to meta-learn can quickly learn never-before-seen quadratic functions with a low number of data samples (Hochreiter et al., 2001).\nNeural networks with a memory capacity provide a promising approach to meta-learning in deep networks. However, the specific strategy of using the memory inherent in unstructured recurrent architectures is unlikely to extend to settings where each new task requires significant amounts of new information to be rapidly encoded. A scalable solution has a few necessary requirements: First, information must be stored in memory in a representation that is both stable (so that it can be reliably accessed when needed) and element-wise addressable (so that relevant pieces of information can be accessed selectively). Second, the number of parameters should not be tied to the size of the memory. These two characteristics do not arise naturally within standard memory architectures, such as LSTMs. However, recent architectures, such as Neural Turing Machines (NTMs) (Graves et al., 2014) and memory networks (Weston et al., 2014), meet the requisite criteria. And so, in this paper we revisit the meta-learning problem and setup from the perspective of a highly capable memory-augmented neural network (MANN) (note: here on, the term MANN will refer to the class of external-memory equipped networks, and not other \u201cinternal\u201d memory-based architectures, such as LSTMs).\nWe demonstrate that MANNs are capable of meta-learning in tasks that carry significant short- and long-term memory demands. This manifests as successful classification of never-before-seen Omniglot classes at human-like accuracy after only a few presentations, and principled function estimation based on a small number of samples. Additionally, we outline a memory access module that emphasizes memory access by content, and not additionally on memory location, as in original implementations of the NTM (Graves et al., 2014). Our approach combines the best of two worlds: the ability to slowly learn an abstract method for obtaining useful representations of raw data, via gradient descent, and the ability to rapidly bind never-beforeseen information after a single presentation, via an external memory module. The combination supports robust metalearning, extending the range of problems to which deep learning can be effectively applied."
                },
                {
                    "heading": "2. Meta-Learning Task Methodology",
                    "text": "Usually, we try to choose parameters \u03b8 to minimize a learning cost L across some dataset D. However, for metalearning, we choose parameters to reduce the expected learning cost across a distribution of datasets p(D):\n\u03b8\u2217 = argmin\u03b8ED\u223cp(D)[L(D; \u03b8)]. (1)\nTo accomplish this, proper task setup is critical (Hochreiter et al., 2001). In our setup, a task, or episode, involves the presentation of some dataset D = {dt}Tt=1 = {(xt, yt)}Tt=1. For classification, yt is the class label for an image xt, and for regression, yt is the value of a hidden function for a vector with real-valued elements xt, or simply a real-valued number xt (here on, for consistency, xt will be used). In this setup, yt is both a target, and is presented as input along with xt, in a temporally offset manner; that is, the network sees the input sequence (x1, null), (x2, y1), . . . , (xT , yT\u22121). And so, at time t the correct label for the previous data sample (yt\u22121) is provided as input along with a new query xt (see Figure 1 (a)). The network is tasked to output the appropriate label for xt (i.e., yt) at the given timestep. Importantly, labels are shuffled from dataset-to-dataset. This prevents the network from slowly learning sample-class bindings in its weights. Instead, it must learn to hold data samples in memory until the appropriate labels are presented at the next timestep, after which sample-class information can be bound and stored for later use (see Figure 1 (b)). Thus, for a given episode, ideal performance involves a random guess for the first presentation of a class (since the appropriate label can not be inferred from previous episodes, due to label shuffling), and the use of memory to achieve perfect accuracy thereafter. Ultimately, the system aims at modelling the predictive distribution p(yt|xt, D1:t\u22121; \u03b8), inducing a corresponding loss at each time step.\nThis task structure incorporates exploitable metaknowledge: a model that meta-learns would learn to bind data representations to their appropriate labels regardless of the actual content of the data representation or label, and would employ a general scheme to map these bound representations to appropriate classes or function values for prediction."
                },
                {
                    "heading": "3. Memory-Augmented Model",
                    "text": ""
                },
                {
                    "heading": "3.1. Neural Turing Machines",
                    "text": "The Neural Turing Machine is a fully differentiable implementation of a MANN. It consists of a controller, such as a feed-forward network or LSTM, which interacts with an external memory module using a number of read and write heads (Graves et al., 2014). Memory encoding and retrieval in a NTM external memory module is rapid, with vector\nrepresentations being placed into or taken out of memory potentially every time-step. This ability makes the NTM a perfect candidate for meta-learning and low-shot prediction, as it is capable of both long-term storage via slow updates of its weights, and short-term storage via its external memory module. Thus, if a NTM can learn a general strategy for the types of representations it should place into memory and how it should later use these representations for predictions, then it may be able use its speed to make accurate predictions of data that it has only seen once.\nThe controllers employed in our model are are either LSTMs, or feed-forward networks. The controller interacts with an external memory module using read and write heads, which act to retrieve representations from memory or place them into memory, respectively. Given some input, xt, the controller produces a key, kt, which is then either stored in a row of a memory matrix Mt, or used to retrieve a particular memory, i, from a row; i.e., Mt(i). When retrieving a memory, Mt is addressed using the cosine similarity measure,\nK ( kt,Mt(i) ) =\nkt \u00b7Mt(i) \u2016 kt \u2016\u2016Mt(i) \u2016 , (2)\nwhich is used to produce a read-weight vector, wrt , with elements computed according to a softmax:\nwrt (i)\u2190 exp ( K ( kt,Mt(i) ))\u2211 j exp ( K ( kt,Mt(j)\n)) . (3) A memory, rt, is retrieved using this weight vector:\nrt \u2190 \u2211 i wrt (i)Mt(i). (4)\nThis memory is used by the controller as the input to a classifier, such as a softmax output layer, and as an additional input for the next controller state."
                },
                {
                    "heading": "3.2. Least Recently Used Access",
                    "text": "In previous instantiations of the NTM (Graves et al., 2014), memories were addressed by both content and location. Location-based addressing was used to promote iterative steps, akin to running along a tape, as well as long-distance jumps across memory. This method was advantageous for sequence-based prediction tasks. However, this type of access is not optimal for tasks that emphasize a conjunctive coding of information independent of sequence. As such, writing to memory in our model involves the use of a newly designed access module called the Least Recently Used Access (LRUA) module.\nThe LRUA module is a pure content-based memory writer that writes memories to either the least used memory location or the most recently used memory location. This module emphasizes accurate encoding of relevant (i.e., recent) information, and pure content-based retrieval. New information is written into rarely-used locations, preserving recently encoded information, or it is written to the last used location, which can function as an update of the memory with newer, possibly more relevant information. The distinction between these two options is accomplished with an interpolation between the previous read weights and weights scaled according to usage weights wut . These usage weights are updated at each time-step by decaying the previous usage weights and adding the current read and\nwrite weights:\nwut \u2190 \u03b3wut\u22121 +wrt +wwt . (5)\nHere, \u03b3 is a decay parameter and wrt is computed as in (3). The least-used weights, wlut , for a given time-step can then be computed using wut . First, we introduce the notation m(v, n) to denote the nth smallest element of the vector v. Elements of wlut are set accordingly:\nwlut (i) =\n{ 0 if wut (i) > m(w u t , n)\n1 if wut (i) \u2264 m(wut , n) , (6)\nwhere n is set to equal the number of reads to memory. To obtain the write weights wwt , a learnable sigmoid gate parameter is used to compute a convex combination of the previous read weights and previous least-used weights:\nwwt \u2190 \u03c3(\u03b1)wrt\u22121 + (1\u2212 \u03c3(\u03b1))wlut\u22121. (7)\nHere, \u03c3(\u00b7) is a sigmoid function, 11+e\u2212x , and \u03b1 is a scalar gate parameter to interpolate between the weights. Prior to writing to memory, the least used memory location is computed from wut\u22121 and is set to zero. Writing to memory then occurs in accordance with the computed vector of write weights:\nMt(i)\u2190Mt\u22121(i) + wwt (i)kt,\u2200i (8)\nThus, memories can be written into the zeroed memory slot or the previously used slot; if it is the latter, then the least used memories simply get erased."
                },
                {
                    "heading": "4. Experimental Results",
                    "text": ""
                },
                {
                    "heading": "4.1. Data",
                    "text": "Two sources of data were used: Omniglot, for classification, and sampled functions from a Gaussian process (GP) with fixed hyperparameters, for regression. The Omniglot dataset consists of over 1600 separate classes with only a few examples per class, aptly lending to it being called the transpose of MNIST (Lake et al., 2015). To reduce the risk of overfitting, we performed data augmentation by randomly translating and rotating character images. We also created new classes through 90\u25e6, 180\u25e6 and 270\u25e6 rotations of existing data. The training of all models was performed on the data of 1200 original classes (plus augmentations), with the rest of the 423 classes (plus augmentations) being used for test experiments. In order to reduce the computational time of our experiments we downscaled the images to 20\u00d7 20."
                },
                {
                    "heading": "4.2. Omniglot Classification",
                    "text": "We performed a number of iterations of the basic task described in Section 2. First, our MANN was trained using\none-hot vector representations as class labels (Figure 2). After training on 100,000 episodes with five randomly chosen classes with randomly chosen labels, the network was given a series of test episodes. In these episodes, no further learning occurred, and the network was to predict the class labels for never-before-seen classes pulled from a disjoint test set from within Omniglot. The network exhibited high classification accuracy on just the second presentation of a sample from a class within an episode (82.8%), reaching up to 94.9% accuracy by the fifth instance and 98.1% accuracy by the tenth.\nFor classification using one-hot vector representations, one relevant baseline is human performance. Participants were first given instructions detailing the task: an image would appear, and they must choose an appropriate digit label from the integers 1 through 5. Next, the image was presented and they were to make an un-timed prediction as to its class label. The image then disappeared, and they were given visual feedback as to their correctness, along with the correct label. The correct label was presented regardless of the accuracy of their prediction, allowing them to further reinforce correct decisions. After a short delay of two seconds, a new image appeared and they repeated the prediction process. The participants were not permitted to view previous images, or to use a scratch pad for externalization of memory. Performance of the MANN surpassed that of a human on each instance. Interestingly, the MANN displayed better than random guessing on the first instance within a class. Seemingly, it employed a strategy of educated guessing; if a particular sample produced a key that was a poor match to any of the bindings stored in external memory, then the network was less likely to choose the class labels associated with these stored bindings, and hence increased its probability of correctly guessing this new class on the first instance. A similar strategy was reported qualitatively by the human participants. We were unable to accumulate an appreciable amount of data from participants on the fifteen class case, as it proved much too difficult and highly demotivating. For all intents and purposes, as the number of classes scale to fifteen and beyond, this type of binding surpasses human working memory capacity, which is limited to storing only a handful of arbitrary bindings (Cowan, 2010).\nSince learning the weights of a classifier using large onehot vectors becomes increasingly difficult with scale, a different approach for labeling classes was employed so that the number of classes presented in a given episode could be arbitrarily increased. These new labels consisted of strings of five characters, with each character assuming one of five possible values. Characters for each label were uniformly sampled from the set {\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019, \u2018e\u2019}, producing random strings such as \u2018ecdba\u2019. Strings were represented as concatenated one-hot vectors, and hence were of length 25\nwith five elements assuming a value of 1, and the rest 0. This combinatorial approach allows for 3125 possible labels, which is nearly twice the number of classes in the dataset. Therefore, the probability that a given class assumed the same label in any two episodes throughout training was greatly reduced. This also meant, however, that the guessing strategy exhibited by the network for the first instance of a particular class within an episode would prob-\nably be abolished. Nonetheless, this method allowed for episodes containing a large number of unique classes.\nTo confirm that the network was able to learn using these class representations, the previously described experiment was repeated (See Table 2). Notably, a MANN with a standard NTM access module was unable to reach comparable performance to a MANN with LRU Access. Given this success, the experiment was scaled to up to fifteen unique classes presented in episodes of length 100, with the network exhibiting similar performance.\nWe considered a set of baselines, such as a feed-forward RNN, LSTM, and a nonparametric nearest neighbours classifier that used either raw-pixel input or features extracted by an autoencoder. The autoencoder consisted of an encoder and decoder each with two 200-unit layers with leaky ReLU activations, and an output bottleneck layer of 32 units. The resultant architecture contained significantly more parameters than the MANN and, additionally, was\nallowed to train on three times as much augmented data. The highest accuracies from our experiments are reported, which were achieved using a single nearest neighbour for prediction and features from the output bottleneck layer of the autoencoder. Importantly, the nearest neighbour classifier had an unlimited amount of memory, and could automatically store and retrieve all previously seen examples. This provided the kNN with an distinct advantage, even when raw pixels were used as input representations. Although using rich features extracted by the autoencoder further improved performance, the kNN baseline was clearly outperformed by the MANN."
                },
                {
                    "heading": "4.2.1. PERSISTENT MEMORY INTERFERENCE",
                    "text": "A good strategy to employ in this classification task, and the strategy that was artificially imposed thus-far, is to wipe the external memory from episode to episode. Since each episode contains unique classes, with unique labels, any information persisting in memory across episodes inevitably acts as interference for the episode at hand. To test the effects of memory interference, we performed the classification task without wiping the external memory between episodes.\nThis task proved predictably difficult, and the network was less robust in its ability to achieve accurate classification (Figure 3). For example, in the case of learning one-hot vector labels in an episode that contained five unique classes, learning progressed much slower than in the memory-wipe condition, and did not produce the characteristic fast spike in accuracy seen in the memory-wipe condition (Figure 2). Interestingly, there were conditions in which learning was not compromised appreciably. In the case of learning ten unique classes in episodes of length 75, for example, classification accuracy reached comparable levels. Exploring the requirements for robust performance is a topic of future work."
                },
                {
                    "heading": "4.2.2. CURRICULUM TRAINING",
                    "text": "Given the successful one-shot classification in episodes with fifteen classes, we employed a curriculum training regime to further scale the classification capabilities of the model. The network was first tasked to classify fifteen classes per episode, and every 10,000 episodes of training thereafter, the maximum number of classes presented per episode incremented by one (Figure 4). The network maintained a high level of accuracy even as the number of classes incremented higher throughout training. After training, at the 100,000 episode mark, the network was tested on episodes with 50 classes. Similar tests continued, increasing the maximum number of classes to 100. The network generally exhibited gradually decaying performance as the number of classes increased towards 100.\nThe training limit of the network seemed to have not been reached, as its performance continued to rise throughout up until the 100,000 episode mark. Assessing the maximum capacity of the network offers an interesting opportunity for future work."
                },
                {
                    "heading": "4.3. Regression",
                    "text": "Since our MANN architecture generated a broad strategy for meta-learning, we reasoned that it would be able to adequately perform regression tasks on never-before-seen functions. To test this, we generated functions using from a GP prior with a fixed set of hyper-parameters and trained our network using unique functions in each episode. Each episode involved the presentation of x-values (either 1, 2, or 3-dimensional) along with time-offset function values (i.e., f(xt\u22121)). A successful strategy involves the binding of x-values with the appropriate function values and storage of these bindings in the external memory. Since individual x-values were only presented once per episode, successful function prediction involved an accurate content-\nbased look-up of proximal information in memory. Thus, unlike in the image-classification scenario, this task demands a broader read from memory: the network must learn to interpolate from previously seen points, which most likely involves a strategy to have a more blended readout from memory. Such an interpolation strategy in the image classification scenario is less obvious, and probably not necessary.\nNetwork performance was compared to true GP predictions of samples presented in the same order as was seen by the network. Importantly, a GP is able to perform complex queries over all data points (such as covariance matrix inversion) in one step. In contrast, a MANN can only make local updates to its memory, and hence can only approximate such functionality. In our experiments, the GP was initiated with the correct hyper-parameters for the sampled function, giving it an advantage in function prediction. As seen in Figure 5, the MANN predictions track the underlying function, with its output variance increasing as it predicts function values that are distance from the values it has already received.\nThese results were extended to 2-dimensional and 3- dimensional cases (Fig 6), with the GP again having access to the correct hyper-parameters for the sampled functions. In both the 2-dimensional and 3-dimensional cases, the log-likelihood predictions of the MANN tracks appreciably well versus the GP, with predictions becoming more accurate as samples are stored in the memory."
                },
                {
                    "heading": "5. Discussion & Future Work",
                    "text": "Many important learning problems demand an ability to draw valid inferences from small amounts of data, rapidly and knowledgeably adjusting to new information. Such problems pose a particular challenge for deep learning, which typically relies on slow, incremental parameter changes. We investigated an approach to this problem based on the idea of meta-learning. Here, gradual, incremental learning encodes background knowledge that spans tasks, while a more flexible memory resource binds information particular to newly encountered tasks. Our central contribution is to demonstrate the special utility of a particular class of MANNs for meta-learning. These are deeplearning architectures containing a dedicated, addressable memory resource that is structurally independent from the mechanisms that implement process control. The MANN examined here was found to display performance superior to a LSTM in two meta-learning tasks, performing well in classification and regression tasks when only sparse training data was available.\nA critical aspect of the tasks studied is that they cannot be performed based solely on rote memory. New information must be flexibly stored and accessed, with correct performance demanding more than just accurate retrieval. Specifically, it requires that inferences be drawn from new data based on longer-term experience, a faculty sometimes referred as \u201cinductive transfer.\u201d MANNs are well-suited to meet these dual challenges, given their combination of flexible memory storage with the rich capacity of deep archi-\ntectures for representation learning.\nMeta-learning is recognized as a core ingredient of human intelligence, and an essential test domain for evaluating models of human cognition. Given recent successes in modeling human skills with deep networks, it seems worthwhile to ask whether MANNs embody a promising hypothesis concerning the mechanisms underlying human metalearning. In informal comparisons against human subjects, the MANN employed in this paper displayed superior performance, even at set-sizes that would not be expected to overtax human working memory capacity. However, when memory is not cleared between tasks, the MANN suffers from proactive interference, as seen in many studies of human memory and inference (Underwood, 1957). These preliminary observations suggest that MANNs may provide a useful heuristic model for further investigation into the computational basis of human meta-learning.\nThe work we presented leaves several clear openings for next-stage development. First, our experiments employed a new procedure for writing to memory that was prima facie well suited to the tasks studied. It would be interesting to consider whether meta-learning can itself discover optimal memory-addressing procedures. Second, although we tested MANNs in settings where task parameters changed across episodes, the tasks studied contained a high degree of shared high-level structure. Training on a wider range of tasks would seem likely to reintroduce standard challenges associated with continual learning, including the risk of catastrophic interference. Finally, it may be of interest to examine MANN performance in meta-learning tasks requiring active learning, where observations must be actively selected."
                },
                {
                    "heading": "6. Acknowledgements",
                    "text": "The authors would like to thank Ivo Danihelka and Greg Wayne for helpful discussions and prior work on the NTM and LRU Access architectures, as well as Yori Zwols, and many others at Google DeepMind for reviewing the manuscript."
                },
                {
                    "heading": "6.1. Additional model details",
                    "text": "Our model is a variant of a Neural Turing Machine (NTM) from Graves et al. It consists of a number of differentiable components: a controller, read and write heads, an external memory, and an output distribution. The controller receives input data (see section 7) directly, and also provides an input to the output distribution. Each of these components will be addressed in turn.\nThe controllers in our experiments are feed-forward networks or Long Short-Term Memories (LSTMs). For the best performing networks, the controller is a LSTM with 200 hidden units. The controller receives some concatenated input (xt,yt\u22121) (see section 7 for details) and updates its state according to:\ng\u0302f , g\u0302i, g\u0302o, u\u0302 = Wxh(xt,yt\u22121) +W hhht\u22121 + b h, (9)\ngf = \u03c3(g\u0302f ), (10)\ngi = \u03c3(g\u0302i), (11) go = \u03c3(g\u0302o), (12) u = tanh(u\u0302), (13)\nct = g f ct\u22121 + gi u, (14) ht = g o tanh(ct), (15)\not = (ht, rt) (16)\nwhere g\u0302f , g\u0302o, and g\u0302i are the forget gates, output gates, and input gates, respectively, bh are the hidden state biases, ct is the cell state, ht is the hidden state, rt is the vector read from memory, ot is the concatenated output of the controller, represents element-wise multiplication, and (\u00b7, \u00b7) represents vector concatenation. Wxh are the weights from the input (xt,yt\u22121) to the hidden state, and Whh are the weights between hidden states connected through time. The read vector rt is computed using content-based\naddressing using a cosine distance measure, as described in the main text, and is repeated below for self completion.\nThe network has an external memory module, Mt, that is both read from and written to. The rows of Mt serve as memory \u2018slots\u2019, with the row vectors themselves constituting individual memories. For reading, the controller cell state serves as a query for Mt. First, a cosine distance measure is computed for the query key vector (here notated as kt) and each individual row in memory:\nK ( kt,Mt(i) ) =\nkt \u00b7Mt(i) \u2016 kt \u2016\u2016Mt(i) \u2016 , (17)\nNext, these similarity measures are used to produce a readweight vector wrt , with elements computed according to a softmax:\nwrt (i)\u2190 exp ( K ( kt,Mt(i) ))\u2211 j exp ( K ( kt,Mt(j)\n)) . (18) A memory, rt, is then retrieved using these read-weights:\nrt \u2190 \u2211 i wrt (i)Mt(i). (19)\nFinally, rt is concatenated with the controller hidden state, ht, to produce the network\u2019s output ot (see equation (16)). The number of reads from memory is a free parameter, and both one and four reads were experimented with. Four reads was ultimately chosen for the reported experimental results. Multiple reads is implemented as additional concatenation to the output vector, rather than any sort of combination or interpolation.\nTo write to memory, we implemented a new content-based access module called Least Recently Used Access (LRUA). LRUA writes to either the most recently read location, or the least recently used location, so as to preserve recent, and hence potentially useful memories, or to update recently encoded information. Usage weights wut are computed each time-step to keep track of the locations most recently read or written to:\nwut \u2190 \u03b3wut\u22121 +wrt +wwt , (20)\nwhere \u03b3 is a decay parameter. The least-used weights, wlut , for a given time-step can then be computed using wut . First, we introduce the notation m(v, n) to denote the nth smallest element of the vector v. Elements of wlut are set accordingly:\nwlut (i) =\n{ 0 if wut (i) > m(w u t , n)\n1 if wut (i) \u2264 m(wut , n) , (21)\nwhere n is set to equal the number of reads to memory.\nTo obtain the write weights wwt , a learnable sigmoid gate parameter is used to compute a convex combination of the previous read weights and previous least-used weights:\nwwt \u2190 \u03c3(\u03b1)wrt\u22121 + (1\u2212 \u03c3(\u03b1))wlut\u22121, (22)\nwhere \u03b1 is a dynamic scalar gate parameter to interpolate between the weights. Prior to writing to memory, the least used memory location is computed from wut\u22121 and is set to zero. Writing to memory then occurs in accordance with the computed vector of write weights:\nMt(i)\u2190Mt\u22121(i) + wwt (i)kt,\u2200i (23)"
                },
                {
                    "heading": "6.2. Output distribution",
                    "text": "The controller\u2019s output, ot, is propagated to an output distribution. For classification tasks using one-hot labels, the controller output is first passed through a linear layer with an output size equal to the number of classes to be classified per episode. This linear layer output is then passed as input to the output distribution. For one-hot classification, the output distribution is a categorical distribution, implemented as a softmax function. The categorical distribution produces a vector of class probabilities, pt, with elements:\npt(i) = exp(Wop(i)ot)\u2211 j exp(Won(j)ot) , (24)\nwhere Wop are the weights from the controller output to the linear layer output.\nFor classification using string labels, the linear output size is kept at 25. This allows for the output to be split into five equal parts each of size five. Each of these parts is then sent to an independent categorical distribution that computes probabilities across its five inputs. Thus, each of these categorical distributions independently predicts a \u2018letter,\u2019 and these letters are then concatenated to produce the five-character-long string label that serves as the network\u2019s class prediction (see figure 8).\nA similar implementation is used for regression tasks. The linear output from the controller outputs two values: \u00b5 and \u03c3, which are passed to a Gaussian distribution sampler as predicted mean and variance values. The Gaussian sampling distribution then computes probabilities for the target value yt using these values."
                },
                {
                    "heading": "6.3. Learning",
                    "text": "For one-hot label classification, given the probabilities output by the network, pt, the network minimizes the episode loss of the input sequence:\nL(\u03b8) = \u2212 \u2211 t yTt logpt, (25)\nwhere yt is the target one-hot or string label at time t (note: for a given one-hot class-label vector yt, only one element assumes the value 1, and for a string-label vector, five elements assume the value 1, one per five-element \u2018chunk\u2019).\nFor string label classification, the loss is similar:\nL(\u03b8) = \u2212 \u2211 t \u2211 c yTt (c) logpt(c). (26)\nHere, the (c) indexes a five-element long \u2018chunk\u2019 of the vector label, of which there are a total of five.\nFor regression, the network\u2019s output distribution is a Gaussian, and as such receives two-values from the controller output\u2019s linear layer at each time-step: predictive \u00b5 and \u03c3 values, which parameterize the output distribution. Thus, the network minimizes the negative log-probabilities as determined by the Gaussian output distribution given these parameters and the true target yt."
                },
                {
                    "heading": "7. Classification input data",
                    "text": "Input sequences consist of flattened, pixel-level representations of images xt and time-offset labels yt\u22121 (see figure 8 for an example sequence of images and class identities for an episode of length 50, with five unique classes). First, N unique classes are sampled from the Omniglot dataset, where N is the maximum number of unique classes per episode. N assumes a value of either 5, 10, or 15, which is indicated in the experiment description or table of results in the main text. Samples from the Omniglot source set are pulled, and are kept if they are members of the set of n unique classes for that given episode, and discarded otherwise. 10N samples are kept, and constitute the image data for the episode. And so, in this setup, the number of samples per unique class are not necessarily equal, and some classes may not have any representative samples. Omniglot images are augmented by applying a random rotation uniformly sampled between \u2212 \u03c016 and \u03c0 16 , and by applying a random translation in the x- and y- dimensions uniformly sampled between -10 and 10 pixels. The images are then downscaled to 20x20. A larger class-dependent rotation is then applied, wherein each sample from a particular class is rotated by either 0, \u03c02 , \u03c0, or 3\u03c0 2 (note: this class-specific rotation is randomized each episode, so a given class may experience different rotations from episode-to-episode). The image is then flattened into a vector, concatenated with a randomly chosen, episode-specific label, and fed as input to the network controller.\nClass labels are randomly chosen for each class from episode-to-episode. For one-hot label experiments, labels are of size N , where N is the maximum number of unique classes that can appear in a given episode."
                },
                {
                    "heading": "8. Task",
                    "text": "Either 5, 10, or 15 unique classes are chosen per episode. Episode lengths are ten times the number of unique classes (i.e., 50, 100, or 150 respectively), unless explicitly mentioned otherwise. Training occurs for 100 000 episodes. At the 100 000 episode mark, the task continues; however, data are pulled from a disjoint test set (i.e., samples from classes 1201-1623 in the omniglot dataset), and weight updates are ceased. This is deemed the \u201ctest phase.\u201d\nFor curriculum training, the maximum number of unique classes per episode increments by 1 every 10 000 training episodes. Accordingly, the episode length increases to 10 times this new maximum."
                },
                {
                    "heading": "9. Parameters",
                    "text": "9.0.1. OPTIMIZATION\nRmsprop was used with a learning rate of 1e\u22124 and max learning rate of 5e\u22121, decay of 0.95 and momentum 0.9.\n9.0.2. FREE PARAMETER GRID SEARCH\nA grid search was performed over number of parameters, with the values used shown in parentheses: memory slots (128), memory size (40), controller size (200 hidden units\nfor a LSTM), learning rate (1e\u22124), and number of reads from memory (4). Other free parameters were left constant: usage decay of the write weights (0.99), minibatch size (16),"
                },
                {
                    "heading": "9.1. Comparisons and controls evaluation metrics",
                    "text": ""
                },
                {
                    "heading": "9.1.1. HUMAN COMPARISON",
                    "text": "For the human comparison task, participants perform the exact same experiment as the network: they observe sequences of images and time-offset labels (sequence length = 50, number of unique classes = 5), and are challenged to predict the class identity for the current input image by inputting a single digit on a keypad. However, participants view class labels the integers 1 through 5, rather than one-hot vectors or strings. There is no time limit for their choice. Participants are made aware of the goals of the task prior to starting, and they perform a single, non-scored trial run prior to their scored trials. Nine participants each performed two scored trials."
                },
                {
                    "heading": "9.1.2. KNN",
                    "text": "When no data is available (i.e., at the start of training), the kNN classifier randomly returns a single class as its prediction. So, for the first data point, the probability that the prediction is correct is 1N where N is number of unique classes in a given episode. Thereafter, it predicts a class from classes that it has observed. So, all instances of samples that are not members of the first observed class cannot be correctly classified until at least one instance is passed to the classifier. Since statistics are averaged across classes, first instance accuracy becomes 1N ( 1 N +0) = 1 N2 , which is 4% and 0.4% for 5 and 15 classes per episode, respectively."
                }
            ],
            "year": 2016,
            "references": [
                {
                    "title": "Motor task variation induces structural learning",
                    "authors": [
                        "Braun",
                        "Daniel A",
                        "Aertsen",
                        "Ad",
                        "Wolpert",
                        "Daniel M",
                        "Mehring",
                        "Carsten"
                    ],
                    "venue": "Current Biology,",
                    "year": 2009
                },
                {
                    "title": "Ranking learning algorithms: Using ibl and meta-learning on accuracy and time results",
                    "authors": [
                        "Brazdil",
                        "Pavel B",
                        "Soares",
                        "Carlos",
                        "Da Costa",
                        "Joaquim Pinto"
                    ],
                    "venue": "Machine Learning,",
                    "year": 2003
                },
                {
                    "title": "Multitask learning",
                    "authors": [
                        "Caruana",
                        "Rich"
                    ],
                    "venue": "Machine learning,",
                    "year": 1997
                },
                {
                    "title": "The magical mystery four how is working memory capacity limited, and why",
                    "authors": [
                        "Cowan",
                        "Nelson"
                    ],
                    "venue": "Current Directions in Psychological Science,",
                    "year": 2010
                },
                {
                    "title": "Introduction to the special issue on meta-learning",
                    "authors": [
                        "Giraud-Carrier",
                        "Christophe",
                        "Vilalta",
                        "Ricardo",
                        "Brazdil",
                        "Pavel"
                    ],
                    "venue": "Machine learning,",
                    "year": 2004
                },
                {
                    "title": "Neural turing machines",
                    "authors": [
                        "Graves",
                        "Alex",
                        "Wayne",
                        "Greg",
                        "Danihelka",
                        "Ivo"
                    ],
                    "venue": "arXiv preprint arXiv:1410.5401,",
                    "year": 2014
                },
                {
                    "title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
                    "authors": [
                        "He",
                        "Kaiming",
                        "Zhang",
                        "Xiangyu",
                        "Ren",
                        "Shaoqing",
                        "Sun",
                        "Jian"
                    ],
                    "venue": "arXiv preprint arXiv:1502.01852,",
                    "year": 2015
                },
                {
                    "title": "Long shortterm memory",
                    "authors": [
                        "Hochreiter",
                        "Sepp",
                        "Schmidhuber",
                        "J\u00fcrgen"
                    ],
                    "venue": "Neural computation,",
                    "year": 1997
                },
                {
                    "title": "Learning to learn using gradient descent",
                    "authors": [
                        "Hochreiter",
                        "Sepp",
                        "Younger",
                        "A Steven",
                        "Conwell",
                        "Peter R"
                    ],
                    "venue": "In Artificial Neural NetworksICANN",
                    "year": 2001
                },
                {
                    "title": "Meta-learning in computational intelligence, volume 358",
                    "authors": [
                        "Jankowski",
                        "Norbert",
                        "Duch",
                        "W\u0142odzis\u0142aw",
                        "Grabczewski",
                        "Krzysztof"
                    ],
                    "venue": "Springer Science & Business Media,",
                    "year": 2011
                },
                {
                    "title": "Human-level concept learning through probabilistic program induction",
                    "authors": [
                        "Lake",
                        "Brenden M",
                        "Salakhutdinov",
                        "Ruslan",
                        "Tenenbaum",
                        "Joshua B"
                    ],
                    "year": 2015
                },
                {
                    "title": "Layered concept-learning and dynamically variable bias management",
                    "authors": [
                        "Rendell",
                        "Larry A",
                        "Sheshu",
                        "Raj",
                        "Tcheng",
                        "David K"
                    ],
                    "venue": "In IJCAI,",
                    "year": 1987
                },
                {
                    "title": "Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental selfimprovement",
                    "authors": [
                        "Schmidhuber",
                        "J\u00fcrgen",
                        "Zhao",
                        "Jieyu",
                        "Wiering",
                        "Marco"
                    ],
                    "venue": "Machine Learning,",
                    "year": 1997
                },
                {
                    "title": "Meta-learning in reinforcement learning",
                    "authors": [
                        "Schweighofer",
                        "Nicolas",
                        "Doya",
                        "Kenji"
                    ],
                    "venue": "Neural Networks,",
                    "year": 2003
                },
                {
                    "title": "Lifelong learning algorithms",
                    "authors": [
                        "Thrun",
                        "Sebastian"
                    ],
                    "venue": "In Learning to learn,",
                    "year": 1998
                },
                {
                    "title": "Interference and forgetting",
                    "authors": [
                        "Underwood",
                        "Benton J"
                    ],
                    "venue": "Psychological review,",
                    "year": 1957
                },
                {
                    "title": "A perspective view and survey of meta-learning",
                    "authors": [
                        "Vilalta",
                        "Ricardo",
                        "Drissi",
                        "Youssef"
                    ],
                    "venue": "Artificial Intelligence Review,",
                    "year": 2002
                },
                {
                    "title": "Automatic Speech Recognition",
                    "authors": [
                        "Yu",
                        "Dong",
                        "Deng",
                        "Li"
                    ],
                    "year": 2012
                }
            ],
            "id": "SP:bbd0e204f48a45735e1065c8b90b298077b73192",
            "authors": [
                {
                    "name": "Adam Santoro",
                    "affiliations": []
                },
                {
                    "name": "Sergey Bartunov",
                    "affiliations": []
                },
                {
                    "name": "Matthew Botvinick",
                    "affiliations": []
                },
                {
                    "name": "Daan Wierstra",
                    "affiliations": []
                }
            ],
            "abstractText": "Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \u201cone-shot learning.\u201d Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory locationbased focusing mechanisms.",
            "title": "One-shot Learning with Memory-Augmented Neural Networks"
        }
    },
    "58600020": {
        "X": {
            "sections": [
                {
                    "heading": "1 Introduction",
                    "text": "In many Computer Vision problems, such as overhead (aerial or satellite) or biomedical image analysis, there are no dominant up-down or left-right relationships. For example, if the task is to detect cars in aerial images, the absolute orientation of a car is not a discriminant feature. Only the relative orientation to surrounding elements such as roads and buildings can establish a prior on the car\npresence likelihood. If the absolute orientation of the image image is changed, e.g. by following a different flightpath, we would expect the car detector to score the same values over all the cars, independently from their new orientations in the rotated image. In this case, we say that the problem is rotation equivariant: rotating the input is expected to result in the same output, but rotated. On the other hand, if we were confronted to a classification setting, in which we are only interested in the presence or absence of cars in the scene, the car flag (i.e whether any car is present or not) and the corresponding score should remain constant, no matter the absolute orientation of the input scene. In this case the problem is rotation invariant. The more general case would be rotation covariance, in which the output changes as a function of the rotation of the input with some predefined behavior. Taking again the same example, a rotation covariant problem would be to retrieve the absolute orientation of cars with respect to geographical coordinates: in this case, a rotation of the image should produce a corresponding change of the angle predicted for each car.\nar X\niv :1\n61 2.\n09 34\n6v 1\n[ cs\n.C V\n] 2\n9 D\nec 2\nThroughout this article we will make use of the terms equivariance, invariance and covariance of a function f(\u00b7) with respect to a transformation g(\u00b7) in the following sense:\n- equivariance: f(g(\u00b7)) = g(f(\u00b7)), - invariance: f(g(\u00b7)) = f(\u00b7), - covariance: f(g(\u00b7)) = g\u2032(f(\u00b7)),\nwhere g\u2032(\u00b7) is another transformation that is itself function of g(\u00b7). With the above definitions, equivariance and invariance are special cases of covariance. We illustrate these properties in Fig. 1.\nIn this paper, we propose a network that naturally encodes these three properties with respect to rotations. In the following we will recall how they are achieved for translations, before discussing our own proposition."
                },
                {
                    "heading": "1.1 Dealing with translations in CNNs",
                    "text": "The success of Convolutional Neural Networks (CNNs) is partly due to the translation equivariant nature of the convolution operation. To understand why it is more complex to achieve natural equivariance to rotations by means of convolutions, we will briefly summarize how translation equivariance is obtained by standard CNNs.\nTranslation equivariance in CNNs: A convolution of an image x \u2208 RM\u00d7N\u00d7d with a filter w \u2208 Rm\u00d7n\u00d7d, written y = w \u2217 x, is computed by applying the same scalar product operation over all overlapping m \u00d7 n windows (unit stride) on x. If x undergoes an integer translation in the horizontal and vertical directions by (p, q) pixels, the same pixel neighborhoods in x will exist in the translated x, but again translated by (p, q) pixels. Therefore, any neighborhood-based operation such as the convolution is translation equivariant when applied to images or other data structures characterized by local neighborhoods with identical arrangement. The fact that the operation is local and produces a single scalar per neighborhood has another advantageous effect: the output can be effortlessly re-arranged in useful ways. For instance, one can set the spatial structure of the activations to match the one of the input.\nFrom equivariance to invariance or covariance: Translation equivariance or invariance are highly desirable in problems such as image classification, where a\nlocal translation does not change the object class (invariance); and in dense prediction tasks as semantic segmentation and edge extraction, where the output should be subject to the same translation as the input (equivariance). In order to obtain full invariance to small translations, standard CNN architectures apply an additional invariant, non-linear local pooling operator, such as the average or the maximum. This usually comes at the cost of some information loss, typically related to the local distribution of the input values. Such loss is usually controlled by limiting the spatial influence of the pooling operator. If some form of translation covariance is required, some of this information should reach the last layer, for instance via skip connections [22]."
                },
                {
                    "heading": "1.2 Incorporating rotation equivariance in CNNs",
                    "text": "If we want to account for an additional transformation (as in our case jointly achieving equivariance to 2D translation and rotation), the structure of the layer activations is no longer straightforward. One possibility would be to return a series of values corresponding to the application of rotated versions of the canonical filter w at each location in x. In this case, the activations y would be a 3D tensor where a translation in the 3rd dimension corresponds to a rotation of w. This would correspond to a rotation covariant operator that keeps all the orientation information in the image. The covariance achieved in this way could easily be transformed into equivariance or invariance by means of pooling across orientations and spatial windows in deeper layers.\nWhen processing the 2nd layer in the network, convolutional filters at different orientations must become 3D and be applied as 3D convolutions across the roto-translation space. This must be done for each orientation of the 3D filters, thus yielding a 4D tensor as output. The 3rd layer will return a 5D feature map, the 4th layer a 6D one and so on. This explosion in the dimensionality would result in an enormous increase of the memory requirements, the number of parameters and the computational complexity. Therefore, this na\u0131\u0308ve approach to rotation covariant CNNs would not be practical for CNNs with more than one convolutional layer. Note that this problem does not arise when using standard convolutions achieving translation\nequivariance only, since the output feature maps are always of the same dimensionality as the inputs, except for the dimension accounting for the number of features.\nTo overcome the exponential increase of the number of parameters, but still propagate useful information about the most important orientations, we propose to perform a single-binned max-pooling operation across the newly added orientation dimension. At each location, it takes the largest activation across orientations and the angle at which it occurred. This way, we are able to keep the 2D arrangement of the image throughout the CNN layers, while achieving inherent rotation equivariance and allowing the network to make use of the information about the orientation of feature activations in previous layers. Similar to spatial max-pooling, this propagates only information about the maximal activation, discarding all information about non-maximal activations. Since the atomic element in the hidden feature maps is no longer a scalar as in conventional CNNs but a 2D vector, each map can be treated as a vector field. We therefore name the proposed method Rotation Equivariant Vector Field\nNetworks (RotEqNet)."
                },
                {
                    "heading": "2 Related work",
                    "text": "Two families of approaches explicitly account for rotation invariance or equivariance: 1) those that transform the inputs (image or feature maps) and 2) those that rotate the filters. The proposed RotEqNet belongs to the latter.\n1) Rotating the inputs: Jaderberg et al. [13] propose the Spatial Transformer layer, which learns how to crop and transform a region of the image (or a feature map) that is passed to the next layer. This operator tends to find a relevant region for the task at hand and then transforms it to some canonical form, improving the learning process by reducing pose variations in subsequent layers. Laptev et al. [16] use several rotated versions of the image as inputs to the same CNN and perform a pooling operation across the different feature vectors at the first fully connected layer. Such scheme allows another subsequent fully connected layer to choose among ro-\ntated inputs to compute the classification. In the work by Cheng et al. [5], several rotated versions of an image are used by the CNN in single minibatch. Their representations after the first fully connected layer are then encouraged to be similar, forcing the CNN to learn rotation invariance. Henriques et al. [10] warp the images such that the translation equivariance inherent to convolutions is transformed into rotation and scale equivariance.\nOn the one hand, these methods have the advantage of using conventional CNN implementations, since they transform only the input images or the feature maps. On the other hand, their main disadvantage is that they can only consider global transformations of the input image. If this is well suited for tasks such as image classification, it limits their applicability to other problems (e.g. semantic segmentation), where relative orientations of some objects with respect to others can help identifying them. It is worth mentioning that this family of approaches also includes standard data augmentation strategies consisting in applying random rotations and flips to training samples [25]: given enough training samples and model capacity, a CNN might learn that different orientations should score the same by simply learning equivalent filters at different orientations [18].\n2) Rotating the filters: Gens and Domingos [9] tackle the problem of the exploding dimensionality (discussed in Sect. 1.2) by applying learnable kernel-based pooling operations and sampling the symmetry space at each layer. This way, they avoid applying the filters exhaustively across the (high dimensional) feature maps by selectively sampling few rotations. By doing so, only the least important information is lost from layer to layer. Cohen et al. [6] use a smaller symmetry group, composed of a flipping and four 90o rotations, and perform pooling within the group (what they call coset pooling). They apply coset pooling only in the deeper layers of the network, since they found that pooling in the early layers discards important information and harms the classification performance. Instead of defining explicitly a symmetry group, Ngiam et al. [21] pool across several untied filters, thus letting the network learn what kind of invariance is useful. Sifre et al. [24] use wavelets that are separable in the rototranslational space instead of learned filters, allowing for more efficient computation. Another approach for avoiding the dimensionality explosion in the space of rotations\nis to limit the network to be shallow: Sohn et al. [26] and Kivinen et al. [14] propose such a scheme with unsupervised Restricted Boltzmann Machines (RBM), while Marcos et al. [20] implement it with supervised CNNs consisting of a single convolutional layer.\nThese works find a compromise between the computational resources required and the amount of orientation information kept throughout the layers, by either keeping the model shallow or accounting for a very small amount of orientations. We propose to avoid such compromise by applying a pooling over a large number of orientations and passing forward both the maximum magnitude and the orientation at which it occurred. This modification allows to build deeper rotation equivariant architectures, where the deeper layers become aware of the dominant orientations in the previous ones with minimal memory requirements, since we avoid the above mentioned explosion of the dimensionality of the feature maps and filters."
                },
                {
                    "heading": "3 Rotation equivariant vector field networks",
                    "text": "In this work we focus on achieving rotation equivariance by performing convolutions with several rotated instances of the same filter (see Fig. 2), which we call the canonical filter. The canonical filter w is rotated at R different orientations, evenly spaced in a given interval of angles. In the experiments of Sect. 4 we deal with problems requiring either full invariance, equivariance or covariance, so we use the interval \u03b1 = [0o, 360o]. However, this interval could be tightened if an adequate range of tilts is known beforehand.\nThe output of filter w at a specific image location will consist of the magnitude of the maximal activation across the orientations, plus the corresponding angle. If we convert this polar representation into Cartesian coordinates, each filter w produces a vector field feature map z \u2208 RH\u00d7W\u00d72, where the output of each location consists of two values [u, v] \u2208 R2 representing the maximal activation in both magnitude and direction. Since the feature maps have become vector fields, from this moment on the filters applied to them must also be vector fields, as seen in Fig. 3.\nThe advantage of representing z \u2208 RH\u00d7W\u00d72 in Carte-\nsian coordinates is that the horizontal and vertical components [u, v] are orthogonal, and thus a convolution of two vector fields can be computed using standard convolutions separately in each component:\n(z \u2217w) = (zu \u2217wu) + (zv \u2217wv), (1)\nwhere subscripts u and v denote horizontal and vertical components."
                },
                {
                    "heading": "3.1 RotEqNet building blocks",
                    "text": "RotEqNet requires different CNN building blocks to be able to handle vectors fields as inputs or outputs (Fig. 2 and Fig. 3). In the following, we present our reformulation of traditional CNN blocks to account for both vector field activations and filters. The implementation is based on the MatConvNet [28] toolbox."
                },
                {
                    "heading": "3.1.1 Rotating convolution and its transpose",
                    "text": "Rotating convolution (RotConv): Given an input image withm/2 zero-padding x \u2208 RH+m/2\u00d7W+m/2\u00d7d, we apply the filter w \u2208 Rm\u00d7m\u00d7d at R different orientations, corresponding to the rotation angles:\n\u03b1r = 360\nR r \u2200r = 1, 2 . . . R. (2)\nEach of these rotated versions of the filter (Fig. 2b and Fig. 3b) is computed by resampling w with bicubic interpolation after rotation of \u03b1r degrees around the filter center:\nwr = rotate(w, \u03b1r). (3)\nNote that interpolation when resampling the filters or the image is always required unless only rotations of multiples of 90o are considered. In practice, this means that the rotation equivariance will only be approximate (a fact that we will use to our advantage in the experiments, see Sect. 4) .\nThe position [i\u2032, j\u2032] after rotation of a specific filter weight, originally located at [i, j] in the canonical form, is\n[i\u2032, j\u2032] = [i, j] [ cos(\u03b1r) sin(\u03b1r) \u2212 sin(\u03b1r) cos(\u03b1r) ] . (4)\nCoordinates are relative to the center of the filter. Since the rotation can force weights near the corners of the filter to be relocated outside of its spatial support, only the\nweights within a circle of diameter m pixels are used to compute the convolutions. The output tensor y \u2208 RH\u00d7W\u00d7R (Fig. 2c and Fig. 3c) consists ofR feature maps computed as\ny(r) = (x \u2217wr) \u2200r = 1, 2 . . . R, (5)\nwhere (\u2217) is a standard convolution operator such that (x \u2217w)[i, j] = \u2211 m \u2211 n x[i\u2212m, j \u2212 n]w[m,n], (6)\nwhere [m,n] is the neighborhood considered by the filter. The tensor y encodes the roto-translation output space such that rotation in the input corresponds to a translation across the feature maps. Only the canonical, non rotated, version of w is actually stored in the model. During backpropagation, gradients corresponding to each rotated filter, \u2207wr, are aligned back to the canonical form and added together:\n\u2207w = \u2211 r rotate(\u2207wr,\u2212\u03b1r). (7)\nNote that this block can be applied on conventional color images and feature maps (Fig. 2a) or on vector field feature maps (Fig. 2b) by applying it independently in each component and summing the resulting 3D tensors (see Eq. (1)).\nRotating convolution transpose (RotConv>): When tackling problems such as semantic segmentation, boundary detection or normal estimation, it has been shown that dense prediction strategies returning outputs of the same size as the inputs offer the best results [15, 19, 22, 29]. The convolution transpose operator, the adjoint of the convolution operator, can be used to learn interpolation filters to upsample activations [19, 22].\nAs with the convolution, the rotating convolution transpose can also be implemented using a standard convolution transpose module:\nx = \u2211 r (y(r) \u2217wr)>. (8)\nwhere (\u2217)> is the convolution transpose operator. Note that the output can also be a vector field, just by defining w as a vector field filter."
                },
                {
                    "heading": "3.1.2 Orientation pooling and its un-pooling",
                    "text": "Orientation pooling (OP): Given the 3D tensor y, the role of the orientation pooling is to convert it to a 2D vector field z \u2208 RH\u00d7W\u00d72 (Fig. 2d and Fig. 3d). It avoids the exploding dimensionality problem, while keeping information about the maximally activating orientation of w. First, we extract a 2D map of the largest activations \u03c1 \u2208 RH\u00d7W and their corresponding orientations \u03b8 \u2208 RH\u00d7W . Specifically, for activations located at [i, j]:\n\u03c1[i, j] = max r y[i, j, r], (9) \u03b8[i, j] = 360\nR arg maxr y[i, j, r]. (10)\nThis can be treated as a polar representation of a 2D vector field as long as \u03c1[i, j] \u2265 0 \u2200i, j, condition that is met when using any function on y that returns non-negative values prior to the OP. We employ the common Rectified Linear Unit (ReLu) operation, defined as ReLu(x) = max(x, 0), to \u03c1, as it provides non-saturating, sparse nonlinear activations offering stable training. This representation can then be transformed into Cartesian coordinates as:\nu = ReLu(\u03c1) cos(\u03b8) (11) v = ReLu(\u03c1) sin(\u03b8) (12)\nwith u,v \u2208 RH\u00d7W . The 2D vector field z is then built as:\nz = [ 1 0 ] u + [ 0 1 ] v (13)\nOrientation un-pooling: The adjoint of the orientation pooling is the orientation un-pooling. It takes a vector field z and outputs a stack of scalar maps y, where each individual map y(r) is such that:\ny(r) = \u03c1 \u00b7 [ \u03b1r \u2212 360\n2R < \u03b8 \u2265 \u03b1r +\n360\n2R\n] , r \u2208 1 . . . R.\n(14) The resulting y will contain mostly zeros and its values will be such that the result of applying an orientation pooling operator to y will result in z. The orientation un-pooling block is required for the backpropagation of gradients through an OP block. It can also be used for rotation equivariant up-sampling after a RotConv> layer, to\nconvert vector field z into roto-translation y feature maps. An alternative up-sampling is to use a RotConv operator directly on z."
                },
                {
                    "heading": "3.1.3 Spatial pooling (SP) for vector fields",
                    "text": "The max-pooling operator is commonly used in CNNs for obtaining some invariance to small deformations and reducing the size of the feature maps. This is done by taking the input feature map x \u2208 RM\u00d7N\u00d7d and downsampling it to xp \u2208 R M p \u00d7 N p \u00d7d. This operation is performed by taking the maximum value contained in each one of the C non-overlapping p \u00d7 p regions of x, indexed by c. It is computed as xp[c] = maxi\u2208c x[i], which can be generalized as:\nyp[c] = y[j], where j = arg maxi\u2208c y[i]. (15)\nThis allows us to define vector field max-pooling as:\nzp[c] = z[j], where j = arg maxi\u2208c \u03c1[i], (16)\nwhere \u03c1 is a standard scalar map containing the magnitudes of the vectors in z."
                },
                {
                    "heading": "3.1.4 Batch normalization (BN) for vector fields",
                    "text": "BN [12] normalizes each feature map across a minibatch to zero mean and unit standard deviation. It has been shown to improve the convergence of commonly employed stochastic gradient descent training algorithms.\nIn our case, the feature maps are not scalar fields, but vector fields representing the magnitude of an activation and the orientation of the filter generating it. In this case, BN should limit its effect to normalizing the magnitudes of the vectors to unit standard deviation. It would not make sense to normalize the angles, since their values are already bounded and changing the distribution would alter important information about relative and global orientations. Given a vector field feature map z and its map of magnitudes \u03c1, we compute batch normalization as:\nz\u0302 = z\u221a\nvar(\u03c1) . (17)"
                },
                {
                    "heading": "3.1.5 Dropout for vector fields",
                    "text": "Another commonly used layer in CNNs is Dropout [27]. Dropout makes use of the ideas behind ensemble learning (that an ensemble of independently trained learners tends to produce a better average response than any learner by itself) by randomly switching off some of the neurons at each training iteration. This reduces the risk of overfitting by preventing groups of neurons to co-adapt too tightly. Applying Dropout to vector fields only requires to ensure that both components of the filter are being switched off or on simultaneously. We apply Dropout on both fully connected and convolutional layers, the latter by switching on or off a filter for all locations in each of the samples."
                },
                {
                    "heading": "3.2 The Rot-O-Block",
                    "text": "It is not always desirable to use the maximum activation angle of a filter. For instance, when the filter shows an approximate radial symmetry, this information would be misleading, since all directions are equivalent, but OP chooses only one. The consequence is that two image locations that are almost identical can produce orthogonal activations. Thus, we propose to use a learning block, that allows each filter to return a learned combination of the vector field and the magnitude field outputs. We name it Rot-O-block.\nThe structure of this block is shown in Fig. 4. It takes as input a scalar tensor yin \u2208 RM\u00d7N\u00d7R\u00b7K , where R is the number of orientations used by the previous RotConv (RC in Fig. 4) layer and K is the number of learnable filters. An orientation pooling operation (OP) is applied on yin: this returns simultaneously the corresponding vector field tensor z \u2208 RM\u00d7N\u00d7K\u00d72 and its magnitude map \u03c1 \u2208 RM\u00d7N\u00d7K . Then, we separate the flows and process them separately by applying a RotConv on each activation. Spatial pooling (SP) and dropout can be optionally applied separately on the independent activations. The outputs are finally concatenated to produce the output yout."
                },
                {
                    "heading": "4 Experiments",
                    "text": "We explore the performance of the proposed method on datasets where the orientation of the patterns of interest\nis arbitrary. This is very often the case in biomedical imaging and remote sensing, since the orientation of the camera is usually not correlated to the patterns of interest. We will apply RotEqNet to problems from these two fields, as well to a well-known computer vision benchmark dataset, MNIST-rot. Each one of these case studies will allow us to analyze the performances of RotEqNet in problems needing respectively invariance, equivariance and covariance to rotations. Based on a sensitivity analysis, all the results are reported using a number of orientations R = 17 for the RotConv and RotConv> layers."
                },
                {
                    "heading": "4.1 Invariance: MNIST-rot",
                    "text": "MNIST-rot [17] is a variant of the original MNIST digit recognition dataset with an additional random rotation between 0\u25e6 and 360\u25e6 applied to each 28 \u00d7 28 image. The training set is also considerably smaller than the standard MNIST, with 12k samples, from which 10k are used for training and 2k for validation and model selection. The test set consists of 50k samples. Since we aim at predicting the same label independently from the rotation that has been applied, the MNIST-rot problem requires rotation invariance.\nModel: We test three CNN models with the same architecture, shown in Table 1: 1) a cascade of three RotO-Blocks, 2) a network with three blocks using only the vector field path of the Rot-O-Block and 3) a network using only the scalar field path. The models are trained for 150 epochs, starting with a large learning rate, weight decay and dropout rate of 0.01, 0.05 and 0.4, respectively. All the three hyper-parameters were gradually reduced to reach 0.0001, 0.005 and 0.0. The hyper-parameters were kept the same across all layers and all networks. We used batch normalization before every convolutional layer ex-\ncept the first and last ones.\nTest time data augmentation: We also perform data augmentation at test time, a technique often used with approximately invariant or equivariant CNNs [8, 11]. In particular, we show to the network several rotated versions of the same image using a fixed set of angles between 0o and 90o. Even if doing rotation-based data augmentation at test time might seem counter-intuitive in a rotation invariant model, the different rotations coupled to resampling of the images and the filters (cf. Sect. 3.1.1) will produce slightly different activations. After this, we average the scores obtained by all views of the same image to compute the final prediction.\nResults: The results obtained on the test set of the MNIST-rot dataset are reported in Table 2. The proposed RotEqNet, with the Rot-O-Block architecture, allows to match the results obtained by state-of-the-art TIpooling [16] and even improve over them when using testtime data augmentation.\nThe latter further improves the accuracy, by allowing the network to score different resamplings for each test image. This is very beneficial, in particular since input images are relatively small and consequently interpolation plays an even more critical role."
                },
                {
                    "heading": "4.2 Equivariance: ISBI 2012 Challenge",
                    "text": "This dataset [1] considers the problem of segmenting neuronal structures in electron microscope (EM) stacks [3]. In this semantic segmentation problem we need to locate precisely the neuron membrane boundaries. A rotation in the inputs should lead to the same rotation in the output, making the ISBI 2012 problem a good candidate to study rotation equivariance.\nThe data consist of two EM stacks of drosophila neurons, each with 30 images of size 512 \u00d7 512 pixels (Fig. 5a). One stack is used for training and the other for testing. The ground truth for the training stack consists of binary images of the same resolution as the data (Fig. 5b). The ground truth for the test stack is kept private and the results are to be submitted to an evaluation server 1.\nModel: We transform the original binary membrane vs. non-membrane segmentation problem into a three class segmentation problem: 1) non-membrane, 2) central membrane pixels and 3) membrane border pixels. Pixels within the membrane in the original label image but not belonging to either 2) or 3) are consider to belong to no class (Fig. 5c). This way we can assign a higher penalization to the non-membrane pixels right next to the membrane that those in the middle of the cells. The central membrane scores are used as the final prediction.\nSince we are dealing with a dense prediction problem in which spatial autorcorrelation at different resolution levels has to be learned, we apply an encoder-decoder CNN\n1http://brainiac2.mit.edu/isbi challenge/\ninvolving either two or three stages in the encoder and decoder blocks. Table 3 shows the architecture for the two-stage CNN. The output from the two-stage and threestage RotEqNets are averaged together to obtain the final result.\nResults: A detailed explanation on the evaluation metrics used in the challenge can be found in the ISBI 2012\nchallenge website 1 and in [1]. The winners of the challenge were Chen et al. [4], although Beier et al. [2] had the highest scores at the time of writing. These two works focus on the use of a post-processing pipeline. Our rotation equivariant prediction provides results comparable to the state-of-the-art using raw CNN probabilities [7, 8, 23] with a linear and relatively simple architecture (see Table 4)."
                },
                {
                    "heading": "4.3 Covariance: car orientation estimation",
                    "text": "In the last experiment, we test our method in a rotation covariant setting involving the estimation of car orientations from above-head imagery. We use the dataset provided by the authors of [10], which is based on images issued from Google Maps. The dataset is composed by 15 tiles, where cars\u2019 bounding boxes and car orientations have been manually annotated. We implement our approach in a way similar to [10]. We crop a 48\u00d748 square patch around every car, based on the centerpoint of the provided bounding box. We then use these crops for both training and testing of the model. As in [10], we use the cars in the first 10 images (388 cars) to train the network and those in the last 5 images (193 cars) to test.\nModel: The function we want to learn is covariant with respect to rotations because a rotation by \u2206\u03b1o in the input image should result in a numerical increase by \u2206\u03b1o of the angle predicted. In particular, we try to predict the sine and cosine of \u03b1o, because they are continuous with respect to \u2206\u03b1o. The network\u2019s architecture is illustrated in Table 5. For the output we use a tanh non-linearity, followed by a layer normalizing the output vector to unitnorm.Since the first fully connected layer is not rotation equivariant nor invariant, it is important to train the network with data augmentation based on rotations and flippings. This ensures that there will be no preferred orientations inherited from a biased training set. The weight decay and dropout rate were fixed to 0.1 and 0.3 respectively and the learning rate was gradually decreased from 10\u22123 to 10\u22125. All the filters were initialized from a normal distribution with zero mean and 10\u22123 standard deviation.\nResults: We report in Table 6 the average output of the last 30 training epochs out of a total of 330 for each\nmodel. This allows to overcome the lack of a validation set to select the best model. We compare three similar architectures with different degrees of orientation information passed to the last layer: 1) The two RotConv layers return a vector field, 2) the two RotConv layers return scalar fields and 3) only the second RotConv returns a vector field.\nThe use of vector outputs of RotEqNet improves substantially the results, outperforming both state of the art\non the dataset [10] and a version of RotEqNet using only scalar outputs at each layers. We also observe that alternating a first RotConv layer returning a scalar output and a second returning a vector output improves performances, probably due to the fact that at the original resolution of the images the orientation vector field is noisy, while it becomes more informative after the first spatial pooling. In Fig. 6 we show the error distribution in the test set for the hybrid model. Note how most samples, 83%, are predicted with less than 15o of error, while most of the contribution to the total error comes from the 7% of samples with error higher than 150o, in which the front of the car has been mistaken with the rear.\nSensitivity to R: In order to understand the sensitivity of RotEqNet to the number of angles R, we trained the hybrid model using R = 21 and tested it for different values of R (see Table 7). We observed relatively small changes in the test error. Since R = 17 gave the best results, we chose it for all the experiments."
                },
                {
                    "heading": "5 Conclusion",
                    "text": "We have presented a new way of hard-coding rotation equivariance in CNNs by applying each filter at different orientations and extracting a vector field feature map that encodes the maximum activation magnitude and angle.\nExperiments on classification, segmentation and orientation estimation problems show the suitability of this approach for solving problems that are inherently rotation equivariant, invariant or covariant."
                }
            ],
            "year": 2021,
            "references": [
                {
                    "title": "Crowdsourcing the creation of image segmentation algorithms for connectomics",
                    "authors": [
                        "I. Arganda-Carreras",
                        "S.C. Turaga",
                        "D.R. Berger",
                        "D. Cire\u015fan",
                        "A. Giusti",
                        "L.M. Gambardella",
                        "J. Schmidhuber",
                        "D. Laptev",
                        "S. Dwivedi",
                        "J.M. Buhmann"
                    ],
                    "venue": "Frontiers in Neuroanatomy,",
                    "year": 2015
                },
                {
                    "title": "An efficient fusion move algorithm for the minimum cost lifted multicut problem",
                    "authors": [
                        "T. Beier",
                        "B. Andres",
                        "U. K\u00f6the",
                        "F.A. Hamprecht"
                    ],
                    "venue": "Proceeding of the European Conf. on Computer Vision (ECCV), pages 715\u2013730. Springer,",
                    "year": 2016
                },
                {
                    "title": "An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy",
                    "authors": [
                        "A. Cardona",
                        "S. Saalfeld",
                        "S. Preibisch",
                        "B. Schmid",
                        "A. Cheng",
                        "J. Pulokas",
                        "P. Tomancak",
                        "V. Hartenstein"
                    ],
                    "venue": "PLoS Biol, 8(10):e1000502,",
                    "year": 2010
                },
                {
                    "title": "Deep contextual networks for neuronal structure segmentation",
                    "authors": [
                        "H. Chen",
                        "X.J. Qi",
                        "J.Z. Cheng",
                        "P.A. Heng"
                    ],
                    "venue": "Proceeding of the Conf. on Artificial Intelligence (AAAI),",
                    "year": 2016
                },
                {
                    "title": "RIFD-CNN: Rotation- Invariant and Fisher Discriminative convolutional neural networks for object detection",
                    "authors": [
                        "G. Cheng",
                        "P. Zhou",
                        "J. Han"
                    ],
                    "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 2884\u20132893,",
                    "year": 2016
                },
                {
                    "title": "Group equivariant convolutional networks",
                    "authors": [
                        "T.S. Cohen",
                        "M. Welling"
                    ],
                    "venue": "arXiv preprint arXiv:1602.07576,",
                    "year": 2016
                },
                {
                    "title": "The importance of skip connections in biomedical image segmentation",
                    "authors": [
                        "M. Drozdzal",
                        "E. Vorontsov",
                        "G. Chartrand",
                        "S. Kadoury",
                        "C. Pal"
                    ],
                    "venue": "Proceeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA), pages 179\u2013187. Springer,",
                    "year": 2016
                },
                {
                    "title": "Deep models for brain em image segmentation: novel insights and improved performance",
                    "authors": [
                        "A. Fakhry",
                        "H. Peng",
                        "S. Ji"
                    ],
                    "venue": "Bioinformatics, page btw165,",
                    "year": 2016
                },
                {
                    "title": "Deep symmetry networks",
                    "authors": [
                        "R. Gens",
                        "P.M. Domingos"
                    ],
                    "venue": "Advances in neural information processing systems, pages 2537\u20132545,",
                    "year": 2014
                },
                {
                    "title": "Warped convolutions: Efficient invariance to spatial transformations",
                    "authors": [
                        "J.F. Henriques",
                        "A. Vedaldi"
                    ],
                    "venue": "arXiv preprint arXiv:1609.04382,",
                    "year": 2016
                },
                {
                    "title": "When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition",
                    "authors": [
                        "G. Hu",
                        "Y. Yang",
                        "D. Yi",
                        "J. Kittler",
                        "W. Christmas",
                        "S.Z. Li",
                        "T. Hospedales"
                    ],
                    "venue": "Proceedings of the IEEE Conf. on Computer Vision Workshops (CVPRW), pages 142\u2013150,",
                    "year": 2015
                },
                {
                    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
                    "authors": [
                        "S. Ioffe",
                        "C. Szegedy"
                    ],
                    "venue": "arXiv preprint arXiv:1502.03167,",
                    "year": 2015
                },
                {
                    "title": "Spatial transformer networks",
                    "authors": [
                        "M. Jaderberg",
                        "K. Simonyan",
                        "A. Zisserman"
                    ],
                    "venue": "In Advances in Neural Information Processing Systems,",
                    "year": 2015
                },
                {
                    "title": "Transformation equivariant boltzmann machines",
                    "authors": [
                        "J.J. Kivinen",
                        "C.K. Williams"
                    ],
                    "venue": "Proceedings of the Intl. Conf. on Artificial Neural Networks (ICANN), pages 1\u20139. Springer,",
                    "year": 2011
                },
                {
                    "title": "Surpassing humans in boundary detection using deep learning",
                    "authors": [
                        "I. Kokkinos"
                    ],
                    "venue": "arXiv preprint arXiv:1511.07386,",
                    "year": 2015
                },
                {
                    "title": "TI-pooling: transformation-invariant pooling for feature learning in convolutional neural networks",
                    "authors": [
                        "D. Laptev",
                        "N. Savinov",
                        "J.M. Buhmann",
                        "M. Pollefeys"
                    ],
                    "venue": "arXiv preprint arXiv:1604.06318,",
                    "year": 2016
                },
                {
                    "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
                    "authors": [
                        "H. Larochelle",
                        "D. Erhan",
                        "A. Courville",
                        "J. Bergstra",
                        "Y. Bengio"
                    ],
                    "venue": "Proceedings of the Intl. Conf. on Machine Learning (ICML), pages 473\u2013480. ACM,",
                    "year": 2007
                },
                {
                    "title": "Understanding image representations by measuring their equivariance and equivalence",
                    "authors": [
                        "K. Lenc",
                        "A. Vedaldi"
                    ],
                    "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 991\u2013999,",
                    "year": 2015
                },
                {
                    "title": "Fully convolutional networks for semantic segmentation",
                    "authors": [
                        "J. Long",
                        "E. Shelhamer",
                        "T. Darrell"
                    ],
                    "venue": "Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2015
                },
                {
                    "title": "Learning rotation invariant convolutional filters for texture classification",
                    "authors": [
                        "D. Marcos",
                        "M. Volpi",
                        "D. Tuia"
                    ],
                    "venue": "arXiv preprint arXiv:1604.06720,",
                    "year": 2016
                },
                {
                    "title": "Tiled convolutional neural networks",
                    "authors": [
                        "J. Ngiam",
                        "Z. Chen",
                        "D. Chia",
                        "P.W. Koh",
                        "Q.V. Le",
                        "A.Y. Ng"
                    ],
                    "venue": "Advances in Neural Information Processing Systems, pages 1279\u2013 1287,",
                    "year": 2010
                },
                {
                    "title": "Learning deconvolution network for semantic segmentation",
                    "authors": [
                        "H. Noh",
                        "S. Hong",
                        "B. Han"
                    ],
                    "venue": "Proceeding of the IEEE Conf. Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2015
                },
                {
                    "title": "U-net: Convolutional networks for biomedical image segmentation",
                    "authors": [
                        "O. Ronneberger",
                        "P. Fischer",
                        "T. Brox"
                    ],
                    "venue": "Proceedings of the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 234\u2013241. Springer,",
                    "year": 2015
                },
                {
                    "title": "Rotation, scaling and deformation invariant scattering for texture discrimination",
                    "authors": [
                        "L. Sifre",
                        "S. Mallat"
                    ],
                    "venue": "Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1233\u20131240,",
                    "year": 2013
                },
                {
                    "title": "Best practices for convolutional neural networks applied to visual document analysis",
                    "authors": [
                        "P.Y. Simard",
                        "D. Steinkraus",
                        "J.C. Platt"
                    ],
                    "venue": "Proceedings of the Intl. Conf. on Document Analysis and Recognition (ICDAR), volume 3, pages 958\u2013962,",
                    "year": 2003
                },
                {
                    "title": "Learning invariant representations with local transformations",
                    "authors": [
                        "K. Sohn",
                        "H. Lee"
                    ],
                    "venue": "arXiv preprint arXiv:1206.6418,",
                    "year": 2012
                },
                {
                    "title": "Dropout: a simple way to prevent neural networks from overfitting",
                    "authors": [
                        "N. Srivastava",
                        "G.E. Hinton",
                        "A. Krizhevsky",
                        "I. Sutskever",
                        "R. Salakhutdinov"
                    ],
                    "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,",
                    "year": 2014
                },
                {
                    "title": "Matconvnet \u2013 convolutional neural networks for matlab",
                    "authors": [
                        "A. Vedaldi",
                        "K. Lenc"
                    ],
                    "venue": "Proceeding of the ACM Intl. Conf. on Multimedia,",
                    "year": 2015
                },
                {
                    "title": "Designing deep networks for surface normal estimation",
                    "authors": [
                        "X. Wang",
                        "D.F. Fouhey",
                        "A. Gupta"
                    ],
                    "venue": "Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),",
                    "year": 2015
                }
            ],
            "id": "SP:b18014b0d3485647f6149b44b02d2ed3c002faa5",
            "authors": [
                {
                    "name": "Diego Marcos",
                    "affiliations": []
                },
                {
                    "name": "Michele Volpi",
                    "affiliations": []
                },
                {
                    "name": "Nikos Komodakis",
                    "affiliations": []
                },
                {
                    "name": "Devis Tuia",
                    "affiliations": []
                }
            ],
            "abstractText": "We propose a method to encode rotation equivariance or invariance into convolutional neural networks (CNNs). Each convolutional filter is applied with several orientations and returns a vector field that represents the magnitude and angle of the highest scoring rotation at the given spatial location. To propagate information about the main orientation of the different features to each layer in the network, we propose an enriched orientation pooling, i.e. max and argmax operators over the orientation space, allowing to keep the dimensionality of the feature maps low and to propagate only useful information. We name this approach RotEqNet. We apply RotEqNet to three datasets: first, a rotation invariant classification problem, the MNIST-rot benchmark, in which we improve over the state-of-the-art results. Then, a neuron membrane segmentation benchmark, where we show that RotEqNet can be applied successfully to obtain equivariance to rotation with a simple fully convolutional architecture. Finally, we improve significantly the state-of-the-art on the problem of estimating cars\u2019 absolute orientation in aerial images, a problem where the output is required to be covariant with respect to the object\u2019s orientation.",
            "title": "Rotation equivariant vector field networks"
        }
    }
}