{
  "sections": [{
    "text": "CCS CONCEPTS • Information systems Location based services; Geographic information systems.\nKEYWORDS Customer prediction; geographical preference; reputation reliance; pairwise ranking\nACM Reference Format: Ruirui Li, Jyun-Yu Jiang, Chelsea J.-T. Ju, andWeiWang. 2019. CORALS:Who are My Potential New Customers? Tapping into the Wisdom of Customers’ Decisions. In Proceedings of The Twelfth ACM International Conference on\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM ’19, February 11–15, 2019, Melbourne, VIC, Australia © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5940-5/19/02. . . $15.00 https://doi.org/10.1145/3289600.3290995\nWeb Search and Data Mining (WSDM ’19).ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3289600.3290995"
  }, {
    "heading": "1 INTRODUCTION",
    "text": "Recommender system has attracted substantial attention from researchers since the last decade and has revolutionized the e-commerce industry. Various recommender systems have been developed to facilitate the matching between customers with appropriate products or services, such as movies on Netflix, music on Last.fm, and merchandises on Amazon. For customers, recommendations improve user experience by providing helpful suggestions to explore and discover relevant products or services. For providers, these recommendations increase the propensity of purchases from customers.\nOver the past few years, the prevalence of GPS-enabled devices, such as smart phones, establish the prosperity of location-based social networks (LBSN), such as Foursquare, Yelp, and Facebook Local. LBSN attracts millions of users to share their social friendship and their locations via check-ins. For example, an average of 142 million users check in at local businesses via Yelp every month [42]. Foursquare has 55 million monthly active users and 8 million daily check-ins on the Swarm application [29]. Facebook Local, powered by 70 million businesses [6], facilitates the discovery of local events and places for over one billion active daily users [25]. The checkins, which contain abundant hints of user preferences on locations, allow us to identify potential new customers for local businesses.\nTo identify potential new customers, the most crucial thing is to understand a customer’s decision-making process. However, it is a complex process, and can be influenced by multiple factors. Most investigated factors are personal preferences and geographical convenience. Personal preferences are learned from customers’ historical check-ins by applying collaborative filtering or matrix factorization techniques. The learned preferences, in return, help us find out new businesses which customers are interested in. In addition, check-in locations provide an ancillary resource to interpret customers’ decisions from the perspective of geographical convenience. According to the Tobler’s first law of geography and the law of demand, the propensity of a customer for a local business is inversely proportional to the distance between the customer and the business, which is similar to the probability of purchasing an item being inversely proportional to the cost.\nThere are also studies which show that customers prefer learning from local experts who know the neighborhood well and have firsthand experience [1, 36]. This is because that online reviews are becoming more and more influential in establishing and promoting the reputation of local businesses than ever before. The emergence of numerous review sites has created an unprecedented and ongoing online conversation about local businesses. Therefore, a business’\nreputation is more public and more accessible. Customers are able to see over the “wall” of corporate messaging at what lies behind. They can get a sense of a business’ true essence through the shared experiences of other customers. These changes in marketing lead to a change in customers’ habits. Customers are becoming more and more review-dependent. This is consistent with the study conducted by BrightLocal [3]. Compared with the trend in 2010, the number of people who search for local businesses before consumption is doubled in 2015 and 2016. Moreover, among all the participants in the study, 92% of the customers regularly or occasionally read online reviews, which help them judge whether a local business provides good services or not. Therefore, the impact of online reviews is non-negligible and growing.\nAlthough identifying potential new customers is crucial for local businesses, it is still a very challenging task due to the following three reasons.\n• Data Sparsity. To know and comment on a local business, a customer has to physically visit that business. Thus, the cost is higher than that of rating a movie or a song online. Even if a customer makes the effort to visit the business, he/she often does not check in due to privacy or safety concerns [38], let alone writing a review. Therefore, customers’ check-in data is much sparser than other rating data for movies and music. Table 1 shows the statistics and the densities of four well-known movie and music rating datasets, together with two LBSN datasets, i.e., the Yelp challenge dataset and the Foursquare dataset. Here the density of a dataset is calculated by the number of ratings/checkins divided by the product of the number of users and the number of items/businesses. The densities of Yelp and Foursquare datasets are much lower than the ones of Netflix, MovieLens, Last.fm, and Yahoo! music datasets. The extremely sparse check-in data makes it challenging for us to accurately model customers’ preferences. • Geographical Influence. The first law of geography states that everything is related to everything else, but near things are more related than distant things [30]. Many studies show that people tend to visit nearby local businesses or explore businesses near the ones that they have visited before [41]. Therefore, a challenge is how to estimate customers’ activity trajectories or zones based on the sparse check-in data. Beyond this estimation, a more challenging fact is that the geographical influence is both customer-dependent and business-dependent. If a customer owns a car, he can visit a faraway business with less effort than ones who cannot drive and rely on public transits. On the other hand, the geographical factor has different impacts on different types of businesses. For example, customers tend to visit nearby fast-food businesses for convenience. However, they may be willing to travel farther to visit other types of businesses, such as museums, where they are more closely connected with cultures and get inspirations, or salons where they can have their hair cut and styled by professionals.\n• Reputation Influence. Nowadays, more and more customers rely on online reviews to get a sense of the reputation of local businesses. These reviews implicitly influence customers’ decisions towards visiting a business. The influence of reviews is also both customer-dependent and business-dependent. Different customers have different opinions on the same review. Moreover, similar reviews may have different impacts on different types of businesses. For example, a review such as “A bit of long wait” to a fast-food business may give other customers a very negative impression. However, the impact may be milder if the same comment is made on theme parks, such as Universal Studios.\nIn addition to the three challenges above, given the heterogeneous information on check-in, location, online reviews, current works also lack an integrated analysis of personal preferences, geographical influence, and business reputation when modeling customers’ decisions. To the best of our knowledge, this work is the first one considering all these factors under the scenario of recommending customers for local businesses. To be more specific, the main contributions of this work are as follows: • We propose a customer recommendation model, CORALS, which, based on historical check-in information, integrates customers’ personal preferences, geographical influence, and business reputation. In addition, the model is also capable of incorporating other factors such as expenses. Moreover, the model offers high interpretability by providing the quantitative importance of incorporated factors for different types of local businesses. • We present a comprehensive empirical evaluation of our approach against 12 recommendation methods on two real-world datasets. The results show that our approach, CORALS, outperforms all baseline methods in suggesting potential new customers for local businesses in different cities."
  }, {
    "heading": "2 METHODOLOGY",
    "text": "Table 2 lists the notations we use in this paper. We use bold letters for vectors and normal letters for scalars.\nAs we mentioned in the introduction, the key to addressing the recommendation problem is to accurately understand customers’ decision-making processes. In this work, we decompose it into\nthree main factors: a customer i’s personal preference tb,i over a business b, the geographical convenience дb,i of business b for customer i , and customer i’s reliance rb,i on business b’s reputation. In addition, the contributions of дb,i and rb,i are given byw д b and wrb , respectively. Formally, the tendency of a customer i’s visiting a business b is given by:\ntb,i +w д bдb,i +w r brb,i . (1)\nHigher tendency indicates higher check-in likelihood. Given an observed check-in from customer i on business b, denoted by (b, i), applying the idea of pair-wise comparisons, we sample another customer j who has not checked in at business b. Now, for a business b, we have an observed check-in (b, i) and a sampled unobserved check-in (b, j). It is logical to hypothesize that compared with customer j, customer i is more likely to visit business b. We construct the model by maximizing a posteriori over all observed and sampled check-ins:\nC = ∏\n(b,i), j p(i >b j |Θ)p(Θ), (2)\nwhere Θ is a set of parameters, which define the model. p(i >b j |Θ) gives the probability that a customer i prefers a business b more than another customer j does under the model. Formally,\np(i >b j |Θ) = δ [(tb,i −tb, j )+w д b (дb,i −дb, j )+w r b (rb,i −rb, j )], (3)\nwhere δ (x) is the sigmoid function:\nδ (x) = 1 1 + e−x . (4)\nIn addition, the personal preference of customer i on business b is given by: tb,i = pb · qi , (5) where pb and qi are business and customer vector representations in the preference hidden space, respectively. Similarly, the reputation reliance of a customer i on a business b is given by:\nrb,i = ub · di , (6) where ub and di are business and customer vector representations in the reputation hidden space, respectively. Using Gaussian priors Θ ∼ N (0, λθ I ) to model the parameters, we have\np(Θ) = 1√ 2πσ e(− ∥Θ∥ 2 2σ 2 ). (7)\nSubstituting Equations 3, 4, 5, 6, 7 into the objective Equation 2, we can derive maximizing a posteriori as follows: C ∝ ln ∏ (b,i ), j p(i >b j |Θ)p(Θ)\n= ∑\n(b,i ), j ln δ [(tb,i − tb, j ) +wдb (дb,i − дb, j ) +w r b (rb,i − rb, j )] + lnp(Θ)\n= ∑\n(b,i ), j ln δ [(tb,i − tb, j ) +wдb (дb,i − дb, j ) +w r b (rb,i − rb, j )] − λ ∥Θ∥ 2\n= ∑\n(b,i ), j {ln δ [(pb · qi − pb · qj ) +wдb (дb,i − дb, j ) +w r b (ub · di−\nub · dj )] − λ ∥Θ∥2 },\nwhere λ is a set of regularization parameters for Θ. Here, the businesses’ geographical convenience, дb,i , and businesses’ reputations, ub , are inputs. These two factors will be discussed in Sections 2.1 and 2.2, respectively.\nAlgorithm 1: Parameter optimization with AdaGrad\nInput: learning rate η, max iteration itermax , regularization weights λ, max number of samples Smax ;\nOutput: Θ 1 Initialization: initialize Θ with Normal distribution N (0,0.01),\niter = 0, Θopt = Θ, erropt = errvali ; 2 repeat 3 foreach observed check-in (b, i) do 4 Counter cnt = Smax ; 5 while cnt > 0 do 6 Randomly generate an unobserved customer j ; 7 if (tb,i − tb, j ) +wдb (дb,i − дb, j ) +w r b (rb,i − rb, j ) > 0 then 8 cnt - -; 9 else\n10 foreach involved θ do 11 ∇θ ts = ∂ J\n∂θ ts ; 12 nts+1θ = n ts θ + (∇θ\nts )2; 13 θ ts+1 = θ ts − η√\nntsθ +ϵ ∇θ ts ;\n14 break;\n15 if errvali < erropt then 16 erropt = errvali ; 17 Θopt = Θ;"
  }, {
    "heading": "18 else",
    "text": "19 Θ = Θopt ;"
  }, {
    "heading": "20 iter + +;",
    "text": "21 until iter > itermax ; 22 Return Θopt ;\nTo optimize top ranked customers in the recommendation list, we apply the weighted approximate ranking strategy proposed in [33] to optimize precision@k . Algorithm 1 summarizes the optimization process. First, the parameters Θ are initialized using Normal distributions. The optimization process is iterative. In each iteration, it goes through each observed check-in in the training set. For each observed check-in (b, i), we sample a random customer j who has not visited business b. If the preference order between i and j on business b is correctly predicted using the current Θ, we randomly sample another customer to find a violation. This process repeats at most Smax times until we find such a violation. Once we find a violation, we update the corresponding parameter θ , θ ∈ Θ. After iterating through each check-in in the training set, we evaluate the performance using the validation set. If the performance increases, we accept the updates on Θ. Otherwise, we reject the updates. This step helps us avoid adopting over-fitting parameters on the training data. The optimization terminates when iter reaches the maximum number of iterations.\nAs we mentioned in the introduction, since many businesses and customers have limited numbers of check-ins, the check-in data is extremely sparse. However, the parameters of these businesses and customers can be immensely useful and informative to the problem we want to optimize. To effectively leverage the sparse data, AdaGrad [8] is proposed to give a higher learning rate to the parameters that are more sparse in the data. We adopt this concept to adjust the learning rate adaptively for each individual parameter θ , which is shown in lines 10-13 of Algorithm 1. AdaGrad modifies\nthe general learning rate η at each time step ts for every parameter θ based on the past gradients that have been computed for θ . nts+1θ records the sum of the squares of the gradients with respect to θ up to the tsth time step1. ϵ is a smoothing term that avoids division by zero. In this way, AdaGrad makes it such that parameters that are more sparse in the data have a higher learning rate which translates into a larger update for that parameter."
  }, {
    "heading": "2.1 Geographical Convenience Inference",
    "text": "In this section, we discuss how to infer the geographical convenience of a business b for a user i , i.e., дb,i , based on customer i’s historical check-ins.\nWe apply a Gaussian mixture model (GMM) [27] to make the inference. A Gaussian mixture model is a weighted sum ofM component Gaussian densities:\np(l|Φ) = M∑\nm=1 αmд(l|µm , Σm ), (8)\nwhere l is a 2-dimensional location vector (i.e. latitude and longitude), αm ,m = 1, ...,M , are the mixture weights, and д(l|µm , Σm ) are the component Gaussian densities. Each component density is a 2-variate Gaussian function of the form,\nд(l|µm , Σm ) = 1\n2π |Σm |1/2 e− 1 2 (l−µm )′Σ−1m (l−µm ),\nwith mean location vector µm and covariance matrix Σm . The complete Gaussianmixturemodel is parameterized by themean location vectors, covariance matrices and mixture weights from all component densities. These parameters are further collectively notated by Φ. For a particular customer, given a sequence of his N check-in locations, represented by N location vectors L = {l1, .., lN }, the GMM likelihood, assuming conditional independence between the location vectors, can be written as:\np(L|Φ) = N∏ n=1 p(ln |Φ).\nWe use the Expectation-Maximization (EM) [7] algorithm to estimate the parameters. The EM algorithm begins with an initial model Φ, to estimate a newmodel Φ̄, such that p(L|Φ̄) ≥ p(L|Φ). The new model then becomes the initial model for the next iteration and the process is repeated until convergence. In each EM iteration, re-estimation Equations 9, 10, and 11 are used to guarantee a monotonic increase in the model’s likelihood value in the E-step.\nMixture weights: ᾱm = 1 N N∑ n=1 p(m |ln ,Φ), (9)\nLocation means: µ̄m = ∑N n=1 p(m |ln ,Φ) · ln∑N n=1 p(m |ln ,Φ) , (10)\nVariances: σ̄ 2m = ∑N n=1 p(m |ln ,Φ) · l2n∑N n=1 p(m |ln ,Φ) − µ̄2m , (11)\nIn the M-step, the posteriori probability for componentm is given by\np(m |ln ,Φ) = αmд(ln |µm , Σm )∑M\nm=1 αmд(ln |µm , Σm ) .\n1In one iteration, the same parameter may be optimized multiple times. Each optimization counts 1 time step.\nTo determine the number of Gaussian componentsM , we apply affinity propagation [9] to cluster each customer’s check-ins. The number of clusters yields the number of Gaussian components.\nAfter the GMM construction for a customer i , given the geographical location lb of a businessb, as shown in Equation 8,p(lb |Φ) gives the geographical convenience дb,i of the business b for each customer i ."
  }, {
    "heading": "2.2 Business Reputation Inference from Reviews",
    "text": "In this section, we discuss how to model the business reputation, ub , based on the reviews commented on the local businesses.\nThere are two main challenges. First, reviews differ in their lengths. Some reviews are informative and have more words while others are not. This challenge makes it difficult to model the business’s reputation ub in a fixed-length vector. Second, for a particular business, some reviews are older while others are more recent. They may have different influences on the reputation of the business.\nTo solve the first challenge, we apply a distributed memory model proposed in [16]. Figure 1 shows the framework for the vector learning task, which is to predict a word given other words in a context. Formally, given a sequence of training words o1, o2, o3, ..., oH , the objective of the model is to maximize the average log probability\n1 H H−k∑ h=k loдp(oh |oh−k , ...,oh+k ).\nThe prediction task is performed via a multiclass classifier, i.e., softmax. Then, we have:\np(oh |oh−k , ...,oh+k ) = eyoh∑ o e yo .\nEach yoh is the un-normalized log probability for each output word oh , calculated as:\nyoh = V0 +Vz(oh−k , ...,oh+k ,U ), where V0 and V are the softmax parameters. z is constructed by a concatenation of a review vector and word vectors from O . Both review and word vectors are trained using stochastic gradient descent (SGD) and the gradient is obtained via back propagation. At each step of SGD, we sample a fixed-length context from a random review, compute the error gradient and update the parameters in the model. Once the parameters get converged, we obtain the dense representation of each review. In order to address the impact of the\nchronological order of the reviews, we use the vector of the most recent review as the reputation vector of the business, ub ."
  }, {
    "heading": "3 EXPERIMENTS",
    "text": "In this section, we conduct extensive experiments on two real-world datasets to evaluate the performance of CORALS."
  }, {
    "heading": "3.1 Datasets and Experimental Settings",
    "text": "The experiments are conducted on two datasets. One is the most recently released dataset from the Yelp challenge. The other is the Foursquare dataset. The Yelp dataset contains interactions between customers and businesses, with 4.1M reviews and 947K tips by 1M users for 144K businesses. We investigate the recommendation tasks in 7 large cities. The Foursquare dataset contains interactions between customers and businesses in Los Angeles and New York. Table 3 shows the statistics for the 9 cities in the two datasets.\nUnfortunately, some businesses do not accumulate an adequate amount of check-ins. Moreover, some customers lack sufficient check-ins to infer their preferences. We follow the data cleaning strategy in [13] and filter out businesses and customers whose check-ins are less than 20 for the Yelp dataset. For the Foursquare dataset, we follow the cleaning steps in [2] and remove business and customers that have less than 8 check-ins. For each business, its check-ins are sorted in chronological order based on the timestamps. The first 50% of the check-ins are used as the training data. The following 20% are used for validation and the remaining 30% are used as the test data for evaluation. Table 4 shows the parameter settings of CORALS in the experiments. These parameters are tuned by grid search."
  }, {
    "heading": "3.2 Baselines",
    "text": "To compare our approach with others, the following 12 methods are adopted as baselines.\n• Weighted Regularized MF (WRMF). WRMF [15] minimizes the square error loss by assigning both observed and unobserved check-ins with different weights based on matrix factorization. • Maximum Margin MF (MMMF). MMMF [32] minimizes the hinge loss based on matrix factorization. • Bayesian Personalized Ranking MF (BPRMF). BPRMF [26] optimizes Area Under the Curve (AUC) based on pairs of observed check-ins and sampled unobserved check-ins. • CofiRank. CofiRank [31] optimizes the estimation of a ranking loss based on Normalized Discounted Cumulative Gain (NDCG). • CLiMF. CLiMF [28] optimizes a different ranking-oriented loss, i.e., Mean Reciprocal Rank (MRR) loss. • WARP. In [33], Weighted Approximate-Rank Pairwise loss is proposed to optimize precision@k . WARP loss differs from AUC loss in updating parameters. WARP keeps drawing negative samples until getting a disordered prediction or reaching a cutoff value. • kOS. k−Order Statistic loss is proposed in [34] and provides a variant that optimizes precision@k . • USG. USG [41] is a collaborative filteringmethod. It utilizes social and geographical information to improve recommendations. • GeoMF. GeoMF [20] is a geographically weighted matrix factorization model. • Rank-GeoFM. Rank-GeoFM [18] incorporates geographical and temporal information to provide recommendations.\n• ASMF. ASMF [17] utilizes geographical information, social information, and attributes of businesses to enhance the accuracy of recommendations. • ARMF. ARMF [17] extends ASMF by applying ranking losses. Among these 12 baseline methods, WRMF is a point-wise matrix factorization method while MMMF and BPRMF are pair-wise based. CofiRank, CLiMF, WARP, kOS focus on optimizing top ranked positions. USG, GeoMF, Rank-GeoFM, ASMF, and ARMF utilize additional information, such as check-in locations, social relationship, businesses’ attributes, and temporal information to improve the accuracy of recommendations. All parameters in baselines are tuned based on their guidelines.\nIn addition to the above baselines, we also implement CORALS 2 with two other gradient-based parameter optimization strategies, i.e. SGD and RMSprop [12]. • CORALS-SGD. CORALS-SGD applies SGD to conduct optimizations. All parameters share the same learning rate. • CORALS-RMSprop. RMSprop [12] is applied to optimize learning rates adaptively. It addresses the issue of radically diminishing learning rates in AdaGrad."
  }, {
    "heading": "3.3 Recommendation Performance",
    "text": "In this section, we evaluate the performances of CORALS and its variants against the 12 baseline methods. Mean Average Precision (MAP) is adopted as the evaluation metric. Given a ranked list rl of potential new customers, the average precision for a business b is:\napb = 1 ω |r l |∑ pos=1 precision(pos) ∗ rel(pos) (12)\nwhere ω is the number of new customers who visit a business b in the test set, pos denotes the position in the ranked list rl and |rl | gives the total number of potential new customers in rl . Customers are ranked decreasingly based on how likely they will come in rl . precision(pos) is the precision of a cut-off rank list from 1 to pos , and rel(pos) is an indicator function that equals to 1 if the customer visits b in the test set, 0 otherwise. For example, three new customers visit a business b (i.e., ω = 3) in the test set and they are ranked at position 2, 4, and 7 in rl , respectively. Therefore, apb = 1 3 ( 1 2 + 2 4 + 3 7 ). The mean average precision is the average of the average precision of all businesses.\nMAP = |B |∑ b=1 apb/|B | (13)\nMAP ranges from 0 to 1, and a higher value indicates a better performance in recommendation.\nTable 5 shows the recommendation performances of different methods on the nine cities from the two datasets. The top seven rows show the performances based on the cities in the Yelp dataset, while the bottom two rows show the performances based on the cities in the Foursquare dataset. In addition, we further show the average recommendation performances for the top (10%) and tail (10%) businesses3 in each city to demonstrate how each method performs when there is a relatively rich or poor amount of check-ins, respectively. For example, WRMF achieves 0.026 on average for all 2To distinguish the parameter learning algorithms used in CORALS and its variants, we also call CORALS CORALS-AdaGrad. 3Businesses are sorted based on their check-in numbers. Top businesses are the ones that have more check-ins, while tail businesses are the ones that have fewer check-ins.\nbusinesses in Charlotte. It achieves 0.058 and 0.014 on average for the top and tail businesses in Charlotte, respectively. We observe that the more check-ins we have for businesses, the more accurate recommendations we can achieve. This observation applies to businesses in almost all nine cities under the 15 methods. This is because the more check-ins we have for businesses, the more accurately we can infer the style, the geographical influence, and the reputation of the businesses.\nMMMF, BPRMF, CofiRank, CLiMF, WARP, and kOS achieve better recommendation performances than WRMF in general. This verifies that methods achieving low prediction errors do not necessarily have high recommendation accuracies. In other words, directly optimizing the predicted check-ins may not always provide the best recommendation lists to businesses. CofiRank, CLiMF, WARP, and kOS further outperformMMMF and BPRMF due to their optimizing strategies. They optimize NDCG, MRR, precision@k , and precision@k , respectively, which all focus on better optimizing the top-ranked customers on the list. BPRMF, which optimizes AUC, focuses on optimizing the entire list of customers. CofiRank, CLiMF, WARP, and kOS outperform USG, which shows the advantage of the learning-to-rank recommendation methods. Even without utilizing location and social information, they can accurately infer customer preferences and achieve good recommendation performances. In general, WARP achieves the best recommendation performance among the 7 methods in the upper table, where only check-in information is utilized to infer customer preference.\nGeoMF, Rank-GeoFM, ASMF, and ARMF outperform WRMF, MMF, BPRMF, CofiRank, CLiMF, and kOS in general. It shows that incorporating ancillary information compensate for the sparsity issue in location-based recommendation tasks. The performance of Rank-GeoFM is not as good as the one of GeoMF. This is because Rank-GeoFM, which incorporates temporal information, intends to predict the next point of interest (POI) to visit, while the task in this work is to predict new customers for POIs. GeoMF achieves better MAP than ASMF and ARMF. This might be because ASMF and ARMF focus on utilizing social information, while learning geographical influence might be a better way to improve recommendation performances in location-based tasks.\nCORALS-AdaGrad or its variants outperform all 12 baseline methods with few exceptions, which demonstrates the effectiveness of CORALS-AdaGrad. In particular, CORALS-AdaGrad increases the meanMAP by 51% and 33% against WARP and GeoMF, respectively. Bold numbers in Table 5 indicate the winners for the same city and the same group of the business. In summary, CORALS-AdaGrad or its variants win in all scenarios except in Las Vegas where WARP and ASMF score slightly better."
  }, {
    "heading": "3.4 Geographical Preference Inference",
    "text": "In this section, we will use examples to verify that the geographical influence is both business-dependent and customer-dependent.\nWe first use three case examples to show the geographical influence on different types of local businesses. We select three local businesses in Phoenix, i.e. the Phoenix Art Museum, a branch of McDonald’s, and Alo Cafe. Figures 2a, 2b, 2c show the locations of the three businesses, represented by a blue mark each, together with the heat maps of their visitors. The location of a visitor is estimated by the average of all locations he/she has visited. There are two interesting observations. First, Phoenix Art Museum has more check-ins than McDonald’s and Alo Cafe do. Second, the majority of the check-ins of McDonald’s and Alo Cafe come from their nearby regions while the visitors of Phoenix Art Museum are scattered in the entire Phoenix. In addition, the number of museums in Phoenix is much fewer than the numbers of fast-food businesses and cafes. The rationale behind the observations is that people tend to get services from nearby businesses if the services are available since it takes less effort. However, for some business that is only available in a remote location, the customers may be more tolerant of traveling a long distance. Therefore, businesses such as fast-food and cafes get influenced more by the geographical convenience than businesses like museums. In CORALS, parameterwдb is used to model the geographical influence on a business b. Higher values ofwдb indicate greater influences on the geographical convenience. In Section 3.6, we show a detailed analysis ofwдb on various types of businesses.\nThen, we study the geographical influence on individual customers. We randomly sample two customers from Las Vegas and plot their check-ins in Figures 4a and 4b, respectively. We observe that the two customers have their own exploration preferences. User 1 tends to explore the main street in Las Vegas, while user 2 not only explores the main street but also checks in at the northwestern region of Las Vegas. Given a local business b, represented by the black marker, GMM tells дb,u1 < дb,u2 , which indicates that business b is more geographically convenient for user 2. The geographical convenience information, embedded in the GMM, helps CORALS better understand customers’ decision-making processes from the perspective of the convenience of the local businesses.\nNote that for each customer, we group his/her check-ins by affinity propagation to derive the number of components in the GMM. Figure 3 shows the customer percentage distributions over the number of exploration centers in different cities. We observe that most customers have only one or two exploration centers. The rationale behind it is that most customers explore around their workplaces or/and residences, which is consistent with the findings in the previous study [5].\nTable 5: Recommendation performance (MAP). The upper table shows the performances ofmethods using only check-in information, and the lower table demonstrates the performances of methods using both check-in and heterogeneous information. Mean represents the average performance on all businesses in a city. Top represents the average performance on the top 10% businesses that have more check-ins, and Tail represents the average performance on the tail 10% businesses with fewer check-ins.\nMethod WRMF MMMF BPRMF CofiRank CLiMF WARP kOS City Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Charlotte 0.026 0.058 0.014 0.028 0.049 0.028 0.029 0.049 0.029 0.031 0.052 0.024 0.034 0.058 0.024 0.044 0.060 0.036 0.038 0.057 0.024 Cleveland 0.041 0.086 0.029 0.039 0.072 0.025 0.040 0.073 0.030 0.043 0.078 0.042 0.050 0.081 0.043 0.055 0.077 0.046 0.053 0.085 0.041 Las Vegas 0.004 0.012 0.001 0.009 0.016 0.004 0.009 0.016 0.005 0.013 0.024 0.009 0.013 0.022 0.008 0.017 0.025 0.015 0.014 0.023 0.010 Madison 0.067 0.136 0.043 0.066 0.134 0.042 0.058 0.122 0.034 0.063 0.129 0.037 0.054 0.107 0.031 0.061 0.115 0.039 0.058 0.115 0.038 Phoenix 0.004 0.010 0.002 0.008 0.015 0.006 0.008 0.015 0.006 0.011 0.020 0.008 0.011 0.020 0.009 0.020 0.026 0.015 0.015 0.022 0.013 Pittsburgh 0.028 0.067 0.015 0.027 0.053 0.014 0.027 0.054 0.013 0.031 0.065 0.017 0.037 0.073 0.020 0.044 0.071 0.033 0.040 0.069 0.027 Toronto 0.009 0.019 0.005 0.011 0.018 0.009 0.011 0.017 0.009 0.014 0.025 0.011 0.014 0.022 0.011 0.021 0.031 0.019 0.019 0.029 0.015 Los Angeles 0.005 0.007 0.003 0.005 0.006 0.004 0.009 0.009 0.011 0.010 0.008 0.006 0.008 0.013 0.004 0.011 0.019 0.006 0.009 0.012 0.004 New York 0.002 0.003 0.001 0.003 0.004 0.001 0.005 0.005 0.007 0.004 0.004 0.006 0.005 0.005 0.006 0.005 0.007 0.005 0.004 0.005 0.003 Method USG GeoMF Rank-GeoFM ASMF ARMF CORALS-AdaGrad CORALS-RMSprop CORALS-SGD City Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Mean Top Tail Charlotte 0.029 0.039 0.021 0.035 0.058 0.024 0.035 0.041 0.027 0.027 0.049 0.021 0.037 0.084 0.021 0.056 0.087 0.050 0.055 0.087 0.046 0.056 0.087 0.048 Cleveland 0.048 0.075 0.031 0.044 0.089 0.029 0.043 0.068 0.038 0.047 0.081 0.034 0.056 0.110 0.051 0.091 0.169 0.059 0.090 0.171 0.053 0.085 0.164 0.044 Las Vegas 0.008 0.019 0.004 0.017 0.024 0.012 0.011 0.018 0.010 0.018 0.029 0.011 0.010 0.016 0.005 0.014 0.026 0.010 0.014 0.026 0.010 0.014 0.026 0.010 Madison 0.063 0.104 0.047 0.077 0.148 0.038 0.063 0.112 0.044 0.072 0.151 0.048 0.089 0.184 0.043 0.116 0.192 0.091 0.121 0.210 0.095 0.118 0.212 0.105 Phoenix 0.010 0.017 0.006 0.020 0.023 0.017 0.014 0.016 0.011 0.017 0.023 0.012 0.016 0.019 0.011 0.021 0.029 0.018 0.020 0.030 0.016 0.020 0.029 0.018 Pittsburgh 0.030 0.055 0.023 0.038 0.069 0.032 0.042 0.047 0.030 0.047 0.071 0.030 0.041 0.090 0.033 0.057 0.115 0.035 0.057 0.116 0.034 0.055 0.115 0.033 Toronto 0.014 0.026 0.010 0.022 0.030 0.021 0.016 0.020 0.013 0.018 0.025 0.014 0.012 0.037 0.004 0.027 0.038 0.025 0.026 0.040 0.022 0.026 0.038 0.024 Los Angeles 0.020 0.025 0.017 0.021 0.021 0.017 0.008 0.011 0.006 0.009 0.010 0.007 0.008 0.011 0.004 0.021 0.028 0.023 0.022 0.025 0.023 0.019 0.024 0.019 New York 0.005 0.003 0.006 0.010 0.008 0.008 0.003 0.004 0.002 0.006 0.009 0.005 0.003 0.003 0.003 0.012 0.008 0.012 0.011 0.008 0.010 0.012 0.009 0.011\nFigure 3: Exploration Center Distribution"
  }, {
    "heading": "3.5 Reputation Influence Analysis",
    "text": "In this section, we investigate how theMAP performance of CORALS changes with the number of reviews considered when constructing businesses’ reputation vectors. First, we do not incorporate any reviews, notated as 0 reviews. Then, we use 1, 3, 5, 7, 9, and 11 most recent reviews to construct the reputation vectors of businesses,\nrespectively. Figure 5 shows the performance of CORALS (measured by MAP) on the nine cities. In particular, the performance on the Yelp dataset is plotted in solid lines, while the performance on the Foursquare dataset is plotted in dashed lines. When we ignore review information in the model, the performance is relatively poor. As long as we incorporate the information of the most recent review, the performance improves. For example, the performance increases from 0.081 to 0.097 for Madison. However, when we incorporate more reviews to construct reputation vectors, the performance gain is marginal. This is mainly due to the fact that customers only read a few latest reviews to perceive the reputation of the local business."
  }, {
    "heading": "3.6 Analysis on Contributions of Geographical Convenience and Reputation Reliance",
    "text": "In this section, we analyze to what extent the geographical convenience and online reviews affect customers’ decisions in visiting various types of local businesses.\nWe look into five types of local businesses, i.e. fast-food, bar, cafe, salon, and museum in the two largest cities, i.e., Phoenix and Las\nVegas, in terms of the number of customers and businesses. The type of the business is inferred from the name of the business. For each type of businesses, we look into their geographical influence weights wд and reputation influence weights wr , and calculate the type-wise median of the influence weights. Table 6 shows the analysis based on the businesses in Phoenix. There are 144 fast-food restaurants, 302 bars, 394 cafes, 144 salons, and 8 museums. The geographical influences of fast-food restaurants, bars, and cafes are all around 0.316. For salons, the low geographical influence weight, 0.306, indicates that customers arewilling to travel a little bit farther for better haircare services. For museums, which are fewer in quantity, customers have to travel farther compared with other types of businesses. The geographical influence decreases to 0.248. Moreover, the reputation also has distinct influences on different types of businesses. The reviews on fast-food restaurants, bars, and cafes have a relatively small influence on customers’ decisions since customers care more about the convenience of these types of local businesses. For museums and salons, where customers care more about the experiences, reviews have a stronger influence. Table 7 shows the same analysis based on the businesses in Las Vegas, which is consistent with most discoveries in Phoenix. There is one interesting discovery about the geographical influence on bars in Las Vegas, which indicates that customers in Las Vegas are willing to take more effort in visiting faraway bars compared with fast-food restaurants, cafes, and even salons. The rationale behind it is that there are many attractive shows and events in Las Vegas bars."
  }, {
    "heading": "4 RELATEDWORK",
    "text": "Many recent studies [4, 20, 35, 37, 41, 43, 47] show that there is a strong correlation between customers’ check-in activities and geographical distances. Thus leveraging geographical influences to improve the recommendation accuracy has been noticed by most of the current location-based recommendation work. For example, Cheng et al. [4] first detect multiple centers for each customer based on their check-in histories. Then it recommends a business with the probability that is inversely proportional to the distance between the location of the business and the nearest customer center. In [41], geographical influence is modeled by a power-law distribution between the check-in probability and the pair-wise distance of two check-ins. [21, 45] utilize the kernel density estimation to study customers’ check-ins and avoid employing a specific distribution. [24] exploits geographical neighborhood information by assuming that customers have similar preferences on neighboring POIs and POIs in the same region may share similar user preferences.\nCollaborative filtering algorithm is also used to fuse the check-in and geographical information, such as POI popularity, social influence, temporal influence, and content information [10, 11, 14, 17, 19, 22, 41, 43, 44, 46]. [11] conducts sentiment analysis on customers’ comments to infer how good a POI is. [14, 43] apply topic models to incorporate content information. [17] uses friendship to identify potential check-ins and optimizes POI recommendations based on observed, potential, and unobserved check-ins. [40] investigates the temporal matching between POI popularity and customer regularities to recommend POIs. To model the dynamic and sequential preferences of customers, Xie et al. [38] developed a graph-based embedding model to learn the representations of POIs and recommend POIs. [23] developed a bi-weighted low-rank graph model to learn customer interests and their sequential preferences in a coherent way. PACE [39] explores the use of deep neural networks for learning user preferences over POIs.\nThe proposed method, CORALS, not only provides an integrated analysis of the joint effect of multiple factors, i.e., personal preference, geographical influence, and business reputation, applying the state-of-the-art learning-to-rank strategy, but also aims at proposing an explainable and flexible framework to look into the importance of integrated factors."
  }, {
    "heading": "5 CONCLUSION AND FUTUREWORK",
    "text": "In this work, we study the problem of recommending new customers to local businesses in LBSN. We look into the customers’ decision-making processes and propose a model, CORALS, which integrates customers’ personal preferences, geographical influence, and businesses’ reputation. We conduct extensive experiments to demonstrate the effectiveness of CORALS comparing to 12 different baseline methods on two real-world datasets. CORALS is flexible to incorporate new features, such as the average expense on the businesses and the customers’ tolerances of the expenses. The social network information can also be easily integrated with weighted negative sampling. Moreover, CORALS can quantify the importance of incorporated factors for different types of local businesses."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "This project was supported by NIH U01HG008488 and Yelp Inc.. We would like to thank Shuo Song, Bin Bi, and Zijun Xue for their insightful comments and discussions. We also thank the reviewers for their constructive feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Local experts and online review sites",
    "authors": ["Judd Antin", "Marco de Sá", "Elizabeth F. Churchill"],
    "venue": "In CSCW ’12,",
    "year": 2012
  }, {
    "title": "Location-based and preferenceaware recommendation using sparse geo-social networking data",
    "authors": ["Jie Bao", "Yu Zheng", "Mohamed F. Mokbel"],
    "venue": "In SIGSPATIAL ’12,",
    "year": 2012
  }, {
    "title": "Fused Matrix Factorization with Geographical and Social Influence in Location-Based Social Networks",
    "authors": ["Chen Cheng", "Haiqin Yang", "Irwin King", "Michael R. Lyu"],
    "venue": "In AAAI ’12, July 22-26,",
    "year": 2012
  }, {
    "title": "Friendship and mobility: user movement in location-based social networks",
    "authors": ["Eunjoon Cho", "Seth A. Myers", "Jure Leskovec"],
    "venue": "In SIGKDD ’11,",
    "year": 2011
  }, {
    "title": "Maximum likelihood from incomplete data via the EM algorithm",
    "authors": ["Arthur P Dempster", "NanM Laird", "Donald B Rubin"],
    "venue": "Journal of the royal statistical society. Series B (methodological)",
    "year": 1977
  }, {
    "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
    "authors": ["John C. Duchi", "Elad Hazan", "Yoram Singer"],
    "venue": "Journal of Machine Learning Research",
    "year": 2011
  }, {
    "title": "Clustering by passing messages between data",
    "authors": ["Brendan J Frey", "Delbert Dueck"],
    "venue": "points. science 315,",
    "year": 2007
  }, {
    "title": "Exploring temporal effects for location recommendation on location-based social networks",
    "authors": ["Huiji Gao", "Jiliang Tang", "Xia Hu", "Huan Liu"],
    "venue": "In RecSys ’13,",
    "year": 2013
  }, {
    "title": "Content-Aware Point of Interest Recommendation on Location-Based Social Networks",
    "authors": ["Huiji Gao", "Jiliang Tang", "Xia Hu", "Huan Liu"],
    "venue": "In AAAI ’15, January",
    "year": 2015
  }, {
    "title": "Overview of minibatch gradient descent. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_ slides_lec6.pdf",
    "authors": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky"],
    "year": 2012
  }, {
    "title": "Spatial topic modeling in online social media for location recommendation",
    "authors": ["Bo Hu", "Martin Ester"],
    "venue": "In RecSys ’13,",
    "year": 2013
  }, {
    "title": "Social Topic Modeling for Point-of-Interest Recommendation in Location-Based Social Networks",
    "authors": ["Bo Hu", "Martin Ester"],
    "venue": "In ICDM ’14,",
    "year": 2014
  }, {
    "title": "Collaborative Filtering for Implicit Feedback Datasets",
    "authors": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky"],
    "venue": "In (ICDM ’08),",
    "year": 2008
  }, {
    "title": "Distributed Representations of Sentences and Documents",
    "authors": ["Quoc V. Le", "Tomas Mikolov"],
    "venue": "In ICML ’14, Beijing,",
    "year": 2014
  }, {
    "title": "Point-of-Interest Recommendations: Learning Potential Check-ins from Friends",
    "authors": ["Huayu Li", "Yong Ge", "Richang Hong", "Hengshu Zhu"],
    "venue": "In SIGKDD ’16,",
    "year": 2016
  }, {
    "title": "Rank-GeoFM: A Ranking based Geographical Factorization Method for Point of Interest Recommendation",
    "authors": ["Xutao Li", "Gao Cong", "Xiaoli Li", "Tuan-Anh Nguyen Pham", "Shonali Krishnaswamy"],
    "venue": "In SIGIR ’15,",
    "year": 2015
  }, {
    "title": "Content-Aware Collaborative Filtering for Location Recommendation Based on Human Mobility Data",
    "authors": ["Defu Lian", "Yong Ge", "Fuzheng Zhang", "Nicholas Jing Yuan", "Xing Xie", "Tao Zhou", "Yong Rui"],
    "venue": "In ICDM ’15,",
    "year": 2015
  }, {
    "title": "GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation",
    "authors": ["Defu Lian", "Cong Zhao", "Xing Xie", "Guangzhong Sun", "Enhong Chen", "Yong Rui"],
    "venue": "In KDD ’14,",
    "year": 2014
  }, {
    "title": "Modeling human location data with mixtures of kernel densities",
    "authors": ["Moshe Lichman", "Padhraic Smyth"],
    "venue": "In KDD ’14,",
    "year": 2014
  }, {
    "title": "Learning geographical preferences for point-of-interest recommendation",
    "authors": ["Bin Liu", "Yanjie Fu", "Zijun Yao", "Hui Xiong"],
    "venue": "In KDD ’13,",
    "year": 2013
  }, {
    "title": "Unified Pointof-Interest Recommendation with Temporal Interval Assessment",
    "authors": ["Yanchi Liu", "Chuanren Liu", "Bin Liu", "Meng Qu", "Hui Xiong"],
    "venue": "In SIGKDD ’16,",
    "year": 2016
  }, {
    "title": "Exploiting Geographical Neighborhood Characteristics for Location Recommendation",
    "authors": ["Yong Liu", "Wei Wei", "Aixin Sun", "Chunyan Miao"],
    "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,",
    "year": 2014
  }, {
    "title": "2018. 28 Powerful Facebook Stats Your Brand Can’t Ignore in 2018",
    "authors": ["Maddy Osman"],
    "year": 2018
  }, {
    "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback",
    "authors": ["Steffen Rendle", "Christoph Freudenthaler", "ZenoGantner", "Lars Schmidt-Thieme"],
    "venue": "In UAI ’09,",
    "year": 2009
  }, {
    "title": "Gaussian Mixture Models",
    "authors": ["Douglas A. Reynolds"],
    "venue": "In Encyclopedia of Biometrics",
    "year": 2009
  }, {
    "title": "CLiMF: learning to maximize reciprocal rank with collaborative less-is-more filtering",
    "authors": ["Yue Shi", "Alexandros Karatzoglou", "Linas Baltrunas", "Martha Larson", "Nuria Oliver", "Alan Hanjalic"],
    "venue": "In RecSys ’12,",
    "year": 2012
  }, {
    "title": "2016.  By the numbers: 20 important Foursquare Stats.  http://expandedramblings.com/index.php/ by-the-numbers-interesting-foursquare-user-stats",
    "authors": ["Craig Smith"],
    "year": 2016
  }, {
    "title": "A computer movie simulating urban growth in the Detroit region",
    "authors": ["Waldo R Tobler"],
    "venue": "Economic geography 46,",
    "year": 1970
  }, {
    "title": "COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking",
    "authors": ["Markus Weimer", "Alexandros Karatzoglou", "Quoc V. Le", "Alexander J. Smola"],
    "venue": "In Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems,",
    "year": 2007
  }, {
    "title": "Improving maximum margin matrix factorization",
    "authors": ["Markus Weimer", "Alexandros Karatzoglou", "Alexander J. Smola"],
    "venue": "Machine Learning 72,",
    "year": 2015
  }, {
    "title": "WSABIE: Scaling Up to Large Vocabulary Image Annotation",
    "authors": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"],
    "venue": "In IJCAI ’11,",
    "year": 2011
  }, {
    "title": "Learning to rank recommendations with the k-order statistic loss",
    "authors": ["Jason Weston", "Hector Yee", "Ron J. Weiss"],
    "venue": "In RecSys ’13,",
    "year": 2013
  }, {
    "title": "Who will Attend This Event Together? Event Attendance Prediction via Deep LSTM Networks",
    "authors": ["Xian Wu", "Yuxiao Dong", "Baoxu Shi", "Ananthram Swami", "Nitesh V. Chawla"],
    "venue": "In Proceedings of the 2018 SIAM International Conference on Data Mining,",
    "year": 2018
  }, {
    "title": "Reliable fake review detection via modeling temporal and behavioral patterns",
    "authors": ["Xian Wu", "Yuxiao Dong", "Jun Tao", "Chao Huang", "Nitesh V. Chawla"],
    "venue": "IEEE International Conference on Big Data, BigData 2017,",
    "year": 2017
  }, {
    "title": "RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data",
    "authors": ["Xian Wu", "Baoxu Shi", "Yuxiao Dong", "Chao Huang", "Louis Faust", "Nitesh V. Chawla"],
    "venue": "In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018,",
    "year": 2018
  }, {
    "title": "Learning Graph-based POI Embedding for Location-based Recommendation",
    "authors": ["Min Xie", "Hongzhi Yin", "Hao Wang", "Fanjiang Xu", "Weitong Chen", "Sen Wang"],
    "venue": "In CIKM ’16,",
    "year": 2016
  }, {
    "title": "Bridging Collaborative Filtering and Semi-Supervised Learning: A Neural Approach for POI Recommendation",
    "authors": ["Carl Yang", "Lanxiao Bai", "Chao Zhang", "Quan Yuan", "Jiawei Han"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2017
  }, {
    "title": "POI Recommendation: A Temporal Matching between POI Popularity and User Regularity",
    "authors": ["Zijun Yao", "Yanjie Fu", "Bin Liu", "Yanchi Liu", "Hui Xiong"],
    "venue": "In ICDM",
    "year": 2016
  }, {
    "title": "Exploiting geographical influence for collaborative point-of-interest recommendation",
    "authors": ["Mao Ye", "Peifeng Yin", "Wang-Chien Lee", "Dik Lun Lee"],
    "venue": "In SIGIR ’11,",
    "year": 2011
  }, {
    "title": "LCARS: A Spatial Item Recommender System",
    "authors": ["Hongzhi Yin", "Bin Cui", "Yizhou Sun", "Zhiting Hu", "Ling Chen"],
    "venue": "ACM Trans. Inf. Syst. 32,",
    "year": 2014
  }, {
    "title": "Time-aware point-of-interest recommendation",
    "authors": ["Quan Yuan", "Gao Cong", "ZongyangMa", "Aixin Sun", "Nadia Magnenat-Thalmann"],
    "venue": "In SIGIR ’13,",
    "year": 2013
  }, {
    "title": "iGSLR: personalized geo-social location recommendation: a kernel density estimation approach",
    "authors": ["Jia-Dong Zhang", "Chi-Yin Chow"],
    "venue": "In SIGSPATIAL ’13,",
    "year": 2013
  }, {
    "title": "GeoSoCa: Exploiting Geographical, Social and Categorical Correlations for Point-of-Interest Recommendations",
    "authors": ["Jia-Dong Zhang", "Chi-Yin Chow"],
    "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
    "year": 2015
  }, {
    "title": "Spatiotemporal Event Forecasting from Incomplete Hyperlocal Price Data",
    "authors": ["Xuchao Zhang", "Liang Zhao", "Arnold P Boedihardjo", "Chang-Tien Lu", "Naren Ramakrishnan"],
    "venue": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM,",
    "year": 2017
  }],
  "id": "SP:eb93a8dd8e8b36a7379442676cdbef029dd5eb22",
  "authors": [{
    "name": "Ruirui Li",
    "affiliations": []
  }, {
    "name": "Jyun-Yu Jiang",
    "affiliations": []
  }, {
    "name": "Wei Wang",
    "affiliations": []
  }],
  "abstractText": "Identifying and recommending potential new customers for local businesses are crucial to the survival and success of local businesses. A key component to identifying the right customers is to understand the decision-making process of choosing a business over the others. However, modeling this process is an extremely challenging task as a decision is influenced by multiple factors. These factors include but are not limited to an individual’s taste or preference, the location accessibility of a business, and the reputation of a business from social media. Most of the recommender systems lack the power to integrate multiple factors together and are hardly extensible to accommodate new incoming factors. In this paper, we introduce a unified framework, CORALS, which considers the personal preferences of different customers, the geographical influence, and the reputation of local businesses in the customer recommendation task. To evaluate the proposed model, we conduct a series of experiments to extensively compare with 12 state-of-the-art methods using two real-world datasets. The results demonstrate that CORALS outperforms all these baselines by a significant margin in most scenarios. In addition to identifying potential new customers, we also break down the analysis for different types of businesses to evaluate the impact of various factors that may affect customers’ decisions. This information, in return, provides a great resource for local businesses to adjust their advertising strategies and business services to attract more prospective customers.",
  "title": "CORALS: Who are My Potential New Customers? Tapping into the Wisdom of Customers' Decisions"
}