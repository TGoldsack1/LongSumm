{
  "sections": [{
    "text": "CCSConcepts •General and reference→Performance; • Software and its engineering → Runtime environments; Application specific development environments;\nKeywords Software inefficiency detection, PMU, debug registers, sampling, profiling\nACM Reference Format: ShashaWen, Xu Liu, John Byrne, andMilind Chabbi. 2018.Watching\nfor Software Inefficiencies with Witch . In ASPLOS ’18: 2018 Architectural Support for Programming Languages and Operating\nWork done while at Hewlett Packard Labs.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS ’18, March 24–28, 2018, Williamsburg, VA, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery. ACM ISBN 978-1-4503-4911-6/18/03. . . $15.00 https://doi.org/10.1145/3173162.3177159\nSystems, March 24–28, 2018, Williamsburg, VA, USA. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3173162.3177159"
  }, {
    "heading": "1 Introduction",
    "text": "Large, layered, production software is complex due to a hierarchy of component libraries and sophisticated control flow. Even the high-performance computing (HPC) software achieves only 5-15% of peak performance on modern supercomputers [15, 17, 63]. Inefficiencies inherent in complex software [8, 30, 32, 35, 71, 76, 84, 85] significantly contribute to this abysmal performance. Software inefficiencies may arise during design (e.g., inappropriate choice of algorithms and data-structures), implementation (e.g., developers’ inattention to performance and use of heavyweight APIs), or translation (e.g., detrimental compiler optimizations and lack of tuning for an architecture). Inefficiencies, whatever their origin, often manifest as computations whose results may not be used [4, 69], recomputation of already computed values [83], unnecessary data movement [8, 36, 48, 50, 82], and excessive synchronization [5, 78]. Inefficiencies involving the memory subsystem are particularly egregious because of limited bandwidth shared by multiple cores and high access latencies. Repeated initialization, register spill and restore on hot paths, lack of inlining hot functions, missed optimization opportunities due to aliasing, computing and storing already computed or sparingly changing values, and contention and false sharing [22, 44, 46, 47] (in multi-threaded codes), are some of the common prodigal uses of the memory subsystem. Although compiler literature is rich with optimization to eliminate inefficiencies, in practice, layers of abstractions, dynamic libraries, multi-lingual components, aggregate types, aliasing, and combinatorial explosion of execution paths handicap optimizing compilers in delivering top application performance. Additionally, algorithmic and data structural deficiencies also appear as useless memory operations [8, 30, 35, 71, 84, 85]. Coarse-grained profilers such as VTune [27], HPCToolkit [1], gprof [21], Oracle Solaris Studio [66], Oprofile [65], Perf [40], and CrayPAT [14] identify execution “hotspots”. They attribute measurements such as CPU cycles, stalls, arithmetic intensity, and cache misses, obtained from hardware performance monitoring units (PMUs) to\n1 void loop_regs_scan(struct loop *loop , ...){ 2 ... 3 ▶ last_set =(rtx *) xcalloc(regs ->num , sizeof (rtx)); 4 /* Scan the loop , recording register usage */ 5 for (each instruction in loop){ 6 ... 7 if(GET_CODE (PATTERN (insn)) == SET || ...) 8 count_one_set (..., last_set ,...); 9 ... 10 if (end of basic block) 11 ▶ memset(last_set ,0,regs ->num*sizeof(rtx)); 12 } ... }\nListing 1. Dead stores in SPEC CPU2006 gcc due to an inappropriate data structure. The function iterates over the basic blocks in a loop scanning for the registers used. Line 3 allocates and zero initializes a 16K-element 132KB array representing the virtual registers. The loop body accesses only a few (<2) array elements since basic blocks are typically small. At the end of each basic block (Line 11) the code zero initializes the same array for the use in the next basic block. Line 11 is repeatedly involved in dead stores.\nthe source code. On the positive side, they introduce little runtime overhead and do not materially perturb execution. On the negative side, hotspots fail to distinguish efficient vs. inefficient resource usage. The SPEC CPU2006 [73] gcc code, shown in Listing 1, repeatedly zero initializes a 132KB array, most of which is already zero. None of these profilers detects this as wasted work. Ironically, a hotspot may have no further optimization scope (e.g., a highly optimized linear algebra library); and conversely, a code region acclaimed by a profiler with high arithmetic intensity (a goodness metric) may perform useless computations [83]. Fine-grained profilers such as DeadSpy [8], RedSpy [82], RVN [83], Toddler [60], and Cachetor [58], analyze dynamic instructions with specific objectives—detect useless computation or data movement. They can identify inefficiencies not detected by coarse-grained profilers. In Listing 1, they can pinpoint the source code location that re-initializes an already initialized array and quantify the wasted work. On the positive side, they offer visibility into wasted work. On the negative side, they significantly slow execution down (10-80×) and consume enormous (6-100×) extra memory.\nDespite their effectiveness, the high overhead of finegrained inefficiency detection tools has kept them away from wide adoption. They have remained isolated to small research communities or performance experts. There is a need tomake such tools more available to the developer community so that inefficiency detection can be made commonplace—run with each code check-in to isolate inefficiencies at the earliest. We developed Witch—a lightweight inefficiencydetection framework—to address this issue. Witch combines the best of both worlds—low overhead of coarse-grained profilers and inefficiency detection of fine-grained profilers. Our key observation is that an important class of inefficiency detection schemes, explored previously via fine-grained profilers [8, 48, 82], requires monitoring consecutive accesses to the same memory location. For example, detecting repeated initialization—a dead write [8]—requires monitoring store after store without an intervening load to the same location.\nWitch samples addresses accessed by a program using hardware PMUs.Witch intercepts the subsequent access(es) to the sampled memory locations using hardware debug registers. The result is (1) the ability to observe consecutive accesses to the same memory location to detect myriad inefficiencies, and (2) no code or binary instrumentation and hence low overhead. We show the benefit of this concept by building various inefficiency-detection tools (witchcraft) atop Witch. There are various challenges in making it practical, which we detail and address in Section 4 and 5.\nThe idea generalizes to detect other kinds of inefficiencies— updating a location with a value already present at the location (aka silent store [36, 37]) and loading an unchanged value from memory that was previously loaded [3, 60, 72] (poor register usage). Sharing addresses sampled by one thread with another thread enables buildingWitch-based tools for multi-threaded programs. In this paper, we restrict ourselves to describing theWitch framework and three tools that detect inefficiencies in a thread of execution. We make the following contributions: 1. Develop a lightweight framework,Witch, suitable for a\nclass of tools that requires observing a program’s consecutive accesses to the same memory location. 2. Develop a sampling scheme to overcome hardware limitations, which works exceptionally well in practice. 3. Develop inefficiency-detection tools atopWitch, which are at least an order of magnitude faster than the state-ofthe-art exhaustive-instrumentation tools with the same capabilities. Our tools require negligible extra memory. 4. Overcome practical challenges in implementing these tools and demonstrate the accuracy of our tools in comparison with the state-of-the-art. 5. Demonstrate the utility of our tools on large code bases to pinpoint inefficiencies and show up to 10× speedup. Section 2 covers the related work and motivates our work; Section 3 offers the background; Section 4 overviews the design of Witch; Section 5 and 6, respectively, describe implementation details ofWitch and its client witchcraft tools; Section 7 performs evaluation, Section 8 explores case studies, and Section 9 offers our conclusions."
  }, {
    "heading": "2 Related Work and Motivation",
    "text": "There is a vast literature in detecting and eliminating software inefficiencies. We classify these techniques into hardware and software approaches. The hardware approaches [36, 37, 42, 43, 54, 55, 86] introduce new hardware components to detect and eliminate computations whose results are never used or elide memory operations that do not change the contents of their target memory cells. Our focus is on software approaches, which do not need any hardware modification. Classic compiler optimizations such as value numbering [68], constant propagation [81], and common subexpression elimination [13] eliminate several inefficiencies.\nRecently, static analysis has been used in detecting performance bugs [59, 64]. Static analysis, typically, suffers from limitations related to aliasing, optimization scope, and input and context insensitivities. A thorough literature review of static analysis is not pertinent. The dynamic analysis addresses the limitation of static analysis. Chabbi and Mellor-Crummey [8] show that dead writes are a common symptom of myriad inefficiencies. Their tool, DeadSpy, tracks every memory operation to identify store operations that are never loaded (dead) before a subsequent store (kill) to the same location. DeadSpy associates pairs of instructions involved in a dead store (dead-kill pair) with their calling contexts and source code locations to guide manual optimizations. Using DeadSpy, the authors identify inefficiencies arising from inappropriate data structure choice, optimization inhibiting code shape, inattention to performance, and poor compiler code generation. They improve the performance of several systems by eliminating dead writes. DeadSpy’s exhaustive monitoring typically introduces more than 28× slowdown and consumes more than 9× extra memory on average.\nWen et al. perform fine-grained monitoring to track symbolically equivalent [83] and result-equivalent [82] computations. Their tools detect inefficiencies in both CPU- and memory-bound operations. They detect inefficiencies arising from redundant computation, missed inlining opportunities, layers of abstractions, and redundant stores. With exhaustively monitoring, their tools incur 40-280× runtime overhead. By periodically enabling and disabling monitoring (bursty sampling [24]), they bring it down to a manageable 12× slowdown and 9× memory bloat.\nToddler [60] focuses on identifying repetitive memory load sequences across loop iterations at the cost of 10× slowdown. LDoctor [72] reduces Toddler’s overhead using a combination of ad-hoc sampling and static analysis techniques. However, it only analyzes a small number of suspicious loops identified by profiling, and hence does not work for systematically detecting inefficiencies in the whole program.\nUnlike these approaches,Witch, without the need of any prior knowledge of the program, monitors fully optimized native binaries and all their dynamic dependencies and typically incurs negligible runtime overhead (< 5%) and memory overhead (< 5%). Witch is the first lightweight measurement framework that employs PMUs and hardware debug registers to detect program inefficiencies. Neither the inefficiency detection nor the use of PMUs or debug registers is novel in itself, but their combined application is.\nTools Based on Hardware Debug Registers: Erickson et al. [18] use hardware debug registers [31, 51] to detect data races in the Windows kernel. Jiang et al. [29] extend it to the Linux. They sample memory access instructions and set watchpoints to detect conflicting accesses. They use code breakpoints to intercept random instructions and use\nthem to monitor memory accesses for a time window. Liu et al. [45] developed DoubleTake, which uses debug registers to identify buffer overflow, use after free, and memory leaks. Pesterev et al. developed DProf [67], which combines PMU and hardware debug registers to capture the data flow across runtime objects. DProf suffers from limited debug registers; it runs a program multiple times to achieve higher coverage. These approaches focus on detecting the presence or absence of a bug; they are not concerned with quantifying the frequency of a bug or prioritizing the importance of a bug, which become necessary in performance analysis tools.Witch addresses these quantification and attribution problems necessary for performance tools. Kasikci et al. [34] describe a spatially unbiased sampling scheme to trace cold code for code coverage. In contrast, Witch develops a temporally unbiased sampling scheme to monitor memory locations. Kasikci et al. dynamically rewrite the first instruction of every basic block with the int 3 breakpoint instruction, which causes a trap; there is no hardware limit on how many blocks they can monitor. Breakpoints set in hot code regions drive their sampling, and they throttle too frequently trapping breakpoints. In contrast, Witch does not modify the binary (not even at runtime), it uses the PMU as its sampling engine, but it has to workaround the limited number of debug registers."
  }, {
    "heading": "3 Background and Terminology",
    "text": "In this section, we present the background necessary to understandWitch. Expert readers may skip this section.\nHardware Performance Monitoring Units (PMU): CPU’s PMUs offer a programmable way to count hardware events such as loads, stores, CPU cycles, etc. A PMU can trigger an overflow interrupt once a threshold number of events accumulate. A profiler, running in the address space of the monitored program, can handle the interrupt and attribute the measurement “appropriately”. We refer to a PMU counter overflow as a “sample”. Intel SandyBridge and its successors support Precise Event-Based Sampling (PEBS) [25]. A PMU captures a snapshot of the user-visible register state including the program counter (PC) and the effective address (EA) accessed by the instruction on an event overflow. AMD Instruction-Based Sampling (IBS) [16] and PowerPC Marked Event Sampling (MRK) [75] offer commensurate capabilities.\nHardware Debug Registers: Hardware debug registers [31, 51] enable trapping the CPU execution for debugging when the PC reaches an address (breakpoint) or an instruction accesses a designated address (watchpoint). One can program debug registers with different addresses, widths, and conditions that will cause the CPU to trap on reaching the programmed conditions. Today’s x86 processors have four debug registers. If used for the break-on-data-access (store,\nor load-or-store), on x86 processors, the trap occurs after the instruction execution. Hence, if a store instruction results in a trap, the contents of the target memory will contain the results of the store operation.\nLinux Perf_events: Linux offers a standard interface to program and sample PMUs using the perf_event_open system call [39] and the associated ioctl calls. The Linux kernel can deliver a signal to the thread whose PMU event overflows. The user code can mmap a circular buffer into which the kernel keeps appending the PMU data on each sample. Linux 2.6.33 and its successors incorporate the debug registers in the perf_event interface, however, the support has several limitations, which we discuss and fix in our work. We implementWitch on Intel processors with the PEBS facility. It is straightforward to extendWitch to work on AMD with IBS and PowerPC with MRK.\nCall Path Profiling: Call path profiling [23] is a profiling technique where runtime events (e.g., cache misses) are attributed to the full call path seen at the time of the event. Call path profiling offers insightful details in complex applications with deep call chains. The calling context of an event is a set of active procedure frames when the event happens. A calling context begins at a process or thread entry function such as main and ends at the instruction pointer (IP) of the instruction that triggers the event. The alternative, flat profiling, merely attributes events to the leaf function involved in the event, which introduces ambiguities when the same leaf function (e.g., memset) can be invoked from multiple contexts.\nTerminology: A watchpoint is a software abstraction of a debug register to monitor a data access. An address is monitored if we set a watchpoint at that address. A watchpoint can be set to trap on write (W_TRAP) or trap on read-or-write (RW_TRAP). A watchpoint exception (aka trigger) is a synchronous CPU trap caused when an instruction accesses a monitored address. A PMU sample is a CPU interrupt caused when an event counter overflows. Both PMU samples and watchpoint exceptions are handled via the Linux signals."
  }, {
    "heading": "4 Methodology and Design",
    "text": "We want to answer the following questions: 1) Do consecutive store operations to a memory location have an intervening load? 2) Do consecutive stores to a memory location store the same value? 3) Do consecutive loads from a memory location load the same value? 4) Is a cacheline accessed by one thread immediately accessed by another thread? Summary: PMU samples that include the effective address accessed in a sample provide the knowledge of the addresses accessed in an execution. Given this effective address, a hardware debug register allows us to keep an eye on (watch) a location and recognize what the program subsequently does to such location. Since the hardware can monitor a small\nnumber of locations at a time, reservoir sampling [80] allows monitoring a subset of previously seen addresses without any temporal bias. Finally, we scale the measurements taken for a few monitored samples in a calling context to other unmonitored samples in the same calling context; the scaling is based on the observation that the code behavior in a calling context typically remains the same. Details: Precise PMU samples driveWitch. Client tools subscribe to PMU events of their choice. On each PMU sample, the client obtains the memory address M accessed in the sample. Clients subscribe to a watchpoint at the sampled address in the signal handler and continue their execution.1 When the program accesses M next time, a CPU trap happens. Witch handles the watchpoint exception, captures information associated with the trap, associates any information given by the client at the watchpoint subscribe time, and gives control to the client tool for appropriate actions.\nWe use our dead store detection tool—DeadCraft, shown in Figure 1—as a running example to illustrate our methodology. The ideas generalize to any tool built atopWitch. A store followed by another store to the same address is an instance of a dead store. A store followed by a load to the same address is not a dead store. A software instrumentation tool such as DeadSpy [8] maintains a large shadow memory where it stores the last operation performed on each byte of the original program. A write→write transition in a shadow byte indicates an instance of a dead write.\n1A client may set a watchpoint at an address derived from the sampled address or any other address instead of the sampled address itself.\nDeadCraft mimics the behavior of DeadSpy but on a subset of addresses seen in PMU samples. DeadCraft samples the PMU store events at a chosen frequency. Let the address accessed in a PMU sample beM and let the calling context where the sample happens be Cwatch . In the PMU overflow handler, Witch offers the triplet ⟨Cwatch ,M,AccessType⟩ to DeadCraft. DeadCraft memorizes the tuple and in-turn asksWitch to set a RW_TRAPwatchpointW atM . The normal execution continues.W traps when the program accessesM next time; we defer discussing another sample happening before the trap to Section 4.1. Let the address accessed in the trap be M and let its calling context be Ctrap . Witch handles the trap and offers the triplet ⟨Ctrap ,M,AccessType⟩ to DeadCraft. If a load causes a trap, DeadCraft treats it as a useful operation and disables the watchpoint. If a store causes a trap, however, DeadCraft infers the store seen in the context Cwatch as a dead store. It attributes a “unit” of dead store to the calling context pair ⟨Cwatch ,Ctrap⟩. Since dead stores can happen only on store instructions, and since every store instruction is sampled at a frequency proportional to its occurrence, transitively, we would detect dead writes at a frequency proportional to their occurrence, if we had infinite debug registers."
  }, {
    "heading": "4.1 Challenge with Samples Intervening Accesses",
    "text": "Hardware can monitor only a small number of addresses at a time since they have only a handful of debug registers. The scenario of two accesses to the same memory separated by a large distance, where many PMU samples occur in the intervening time, complicates matters.\nConsider the dead store example in Listing 2. Assume the loop index variables i and j are in registers, the sampling period is 10K stores, and the number of debug register is one. The first sample happens in the i loop when accessing array[10K]. DeadCraft sets a watchpoint to monitor &array[10K] since a debug register is available. The second sample happens when accessing array[20K]. Since the watchpoint armed for address &array[10K] is still active, there is no room to monitor &array[20K]. A naive “replace the oldest watchpoint” scheme cannot detect any dead stores in this code. In such scheme, when the j loop begins, the only active watchpoint would be the last sampled address &array[100K] in the i loop. The PMU continues delivering samples in the j loop. At j=10K, the scheme replaces the last watchpoint on &array[100K] with &array[10K], which would not be accessed again. At the end of the j loop not a single watchpoint would have triggered, and hence no dead store detected. The same problem exists for more than one debug register. A slightly smarter strategy is to flip a coin to decide whether or not to set a watchpoint on a sample. This strategy fails because the survival probability of an older sample becomes minuscule if a large number of samples happen between consecutive accesses to the same location."
  }, {
    "heading": "1 for(int i = 1; i <= 100K; i++){",
    "text": "2 array[i] = 0; 3 } 4 for(int j = 1; j <= 100K; j++){ 5 array[j] = j; 6 }\nListing 2. Long distance inefficiencies: All (say 4) watchpoints will be armed when sampling at 10K store in the first four samples taken in the i loop. A naive replacement will not trigger a single watchpoint due tomany samples taken in the i loop before reaching the j loop.Witch ensures each sample equal probability to survive.\nMonitoring a new sample may help detect a new, previously unseen problem whereas continuing to monitor an old, already-armed address may help detect a problem separated by many intervening operations. We should detect both. But, we do not know when in the future a watchpoint may trap, if at all. Our solution strikes a balance between new vs. old by being unbiased in choosing among the previously accessed addresses (reservoir sampling [80]), and we rely on multiple such unbiased samples taken over a repetitive execution to capture both scenarios. We first show our approach for a single debug register and then generalize it for an arbitrary but finite number of debug registers. On the first sample, S1, if the debug register is unarmed, Witch sets the watchpoint with 1.0 probability. The second sample, S2, replaces the previously armed watchpoint (sample S1) with 1/2 probability and installs itself. Thus, at the end of S2, both S1 and S2 have equal (1/2) probability of being monitored. The third sample, S3, replaces the previously armed watchpoint with 1/3 probability to install itself. Since the previously armed watchpoint is S1 or S2 with 1/2 probability each, they each survive with 1/3 probability. The kth sample Sk since the last time a debug register was empty, replaces the previously armed watchpoint with 1/k probability. The previously armed watchpoint could be any one of {S1, S2, . . . , Sk−1} with 1/k−1 probability each. At the end of kth sample, the probability of monitoring any sampled address Si , 1 ≤ i ≤ (k − 1) of the prior (k − 1) samples is: Pr [monitoring Si ] =Pr [Si survived in Sk−1] × Pr [ not retaining Sk ]\n= 1 k − 1 × k − 1 k = 1 k = Pr [monitoring Sk ]\nAny time a watchpoint traps and the client chooses to disarm the watchpoint, and the probability is reset to 1.0, which ensures that the immediately next sample is monitored. Naturally, if every watchpoint triggers before the next sample, we will monitor every address seen in every sample.\nIn a system with N debug registers, on a new sample, we populate any unused debug register as long as we find one. If no debug register is freed up in a window of N consecutive samples, there will be no room for the (N + 1)th sample. We install the sample SN+1 with N/N+1 probability. If the choice is to install SN+1, we randomly choose one of the N debug registers and replace it with SN+1. It follows that at the end of SN+1, the probability of monitoring any sample Si , 1 ≤ i ≤ N + 1, is N/N+1.\nThe sample Sk , k > N , since the last time a debug register was empty, replaces one of the surviving N samples with N/k probability. It follows that at the end of Sk , every sample has the same N/k probability of being monitored. Anytime when a watchpoint traps and the client chooses to disarm the watchpoint, the probability resets to 1.0. Our technique maintains only a count of previous samples—not a log of all previous samples—which needs O (1) memory.\nAdversary Sample: If a “never-again-to-be-accessed” address α finds a place in a watchpoint, it can affect the subsequent samples. If no watchpoint has triggered for H samples when α is sampled, the expected number of samples before α will be replaced is 1.7H , which follows from the sum of harmonic series. The number of debug registers does not influence α . The number of consecutive PMU samples that are not monitored form a “blindspot” window; the longer the window is, the larger the probability of missing bugs. In our experience, many software in practice often have very short windows. For example, in the SPEC CPU2006 [73] reference benchmarks, on an Intel Haswell machine, we found the largest blind-spot to be, typically, extremely small (< 0.02% of the total samples in a program), and the worst case was 0.5% of the total samples in the mcf benchmark."
  }, {
    "heading": "4.2 Challenges with Proportional Attribution",
    "text": "Consider the code in Listing 3. For brevity, line numbers represent contexts. 25% PMU samples will be attributed to each of Line 3, 7, 8, and 11. If the outer loop executes 1K times and if the sampling period is 10K store operations, each of these lines will get approximately 10K PMU samples. The number of sampled dead writes should be 10K for each line pair ⟨3, 11⟩, ⟨11, 3⟩, ⟨7, 8⟩, and ⟨8, 7⟩. That is, 25% each. This expectation in quantification is not preserved with our sampling scheme because of a mixture of sparse monitoring (lines 3 and 11) and dense monitoring (lines 7 and 8). As soon as a watchpoint traps on Line 7, a debug register frees up; every subsequent PMU sample in the k loop will find a free debug register. Hence, there will be a disproportionately large number of dead writes recorded for the line pairs ⟨7, 8⟩ and ⟨8, 7⟩ compared to rest.\nWe solve this problem with a context-sensitive approximation. The code behavior is typically same in a calling context; hence, an observation made by monitoring an address accessed in a calling context can approximately represent other unmonitored samples occurring in the same calling context. If in a sequence of N samples occurring in a calling context C , only one sample is monitored through a debug register, we scale the observation made for the monitored sample by N to approximate the behavior of the remaining N − 1 unmonitored samples taken at C . In this scheme, in a sequence of ten PMU samples taken at line 3, only one is monitored through a debug register, and that address leads to a dead"
  }, {
    "heading": "1 for( ... many iterations ...){",
    "text": ""
  }, {
    "heading": "2 for(int i = 1; i <= 100K; i++){",
    "text": "3 array[i] = 0; 4 } 5 // p and q alias to the same location 6 for(int k = 1; k <= 100K; k++){ 7 *p = 0; // dead write 8 *q = 0; 9 } 10 for(int j = 1; j <= 100K; j++){ 11 array[j] = 0; 12 } 13 }\nListing 3. 100K stores in the i loop are dead by the overwriting j loop, but only a few watchpoints survive between these two loops. 100K writes to *p are also dead but trigger many more watchpoints at *q. Witch applies a proportional attribution heuristic by accounting the samples taken in a context.\nwrite with line 11, we scale and record number of dead writes between lines ⟨3, 11⟩ as ten. Implementation: Every PMU sample increments a metric µ (C ) in the calling context C where it happens. Another metric η(C ) catches up with µ (C ) each time a watchpoint set in C traps. Both metrics are initially zero. Assume we set a watchpoint W in calling context Cwatch , and it traps in a calling context, say Ctrap ; Ctrap can be Cwatch .( µ (C ) − η(C ) ) ≥ 1 is the number of samples thatW is rep-\nresenting. Assume the sampling period (threshold) is P . If the trapping instruction is a store withM-bytes of overlap over the monitored address range set in W , we approximate and attribute ( µ (C ) − η(C ) ) × P ×M bytes of “waste” to the ordered pair ⟨Cwatch ,Ctrap⟩. Conversely, if the trapping instruction is a load withM-bytes of overlap over the monitored address range set in Cwatch , we approximate and attribute ( µ (C ) −η(C ) ) ×P ×M bytes of “use” to the ordered pair ⟨Cwatch ,Ctrap⟩. In either case, we update η(C ) = µ (C ). Both use and waste metrics are additive—they accumulate overtime for the attributions happening in the same calling context pairs. Thus, the total inefficiency (dead-writes) is:\nD̂ = ∑ i ∑ j waste in ⟨Ci ,Cj ⟩∑\ni ∑ j waste in ⟨Ci ,Cj ⟩ + ∑ i ∑ j use in ⟨Ci ,Cj ⟩\n(1)\nThe metric is similar to the “deadness” D metric described in [6]; instead of deriving the metric by measuring every load and store, we are approximating. Equation 1 is an optional feature available for the clients ofWitch; not all clients need this kind of proportional attribution.\nIn Listing 3, when a watchpoint traps for the first time on Line 11(= Ctrap ), and if there were 10 PMU samples accumulated at the source Line 3 (= Cwatch ), we attribute 10 × 10K × 4 bytes = 400K bytes of dead writes to the line pair ⟨3, 11⟩. This scheme allows the dead writes metric to catchup with the PMU samples, resulting in proportional attribution. Thus, even though we have very few watchpoints, we use PMU samples in a context to approximate the dead writes in that context. If multiple watchpoints were simultaneously set from the same calling context at different addresses, we proportionally distribute the samples among them.\nFigure 2 shows Witch’s attribution of dead writes in a more complex scenario, which perfectly matches our expectation of 50%:33%:17% dead writes to a:b:x. Without proportional attribution, we noticed a biased attribution of 5%:2%:93%. With random sampling, rather than our equal probability sampling, 100% samples get attributed to the line pair ⟨16, 17⟩."
  }, {
    "heading": "4.3 Limitations",
    "text": "Witch employs Monte-Carlo experiments to approximately model real-world observations and suffers from the limitations of any sampling system. Insufficient samples can result in overestimation or underestimation.Witch cannot monitor register-to-register operations. Witch cannot hide the deficiencies of the underlying PMU used to drive its sampling: on some Intel architectures, sporadically, the shadow sampling effect [11, 38, 61] may hide a short latency store behind a long latency store. This behavior can bias the samples to favor long latency stores.\nWitch can simultaneously monitor only as manymemory locations as the number of debug registers. This physical constraint often is not a problem in practice as we show in our evaluation. However, an adversary may be able to construct a program where the effects of limited registers can be more pronounced.\nWitch’s context-sensitive attribution is an optional feature available for its tools. It approximates the behavior of one monitored sample in a context to many samples taken in the same context. If very few monitored samples in a context are used to approximate the behavior of a large number of samples with different traits in that context, it can result in noticeable overestimation or underestimation.\nLike any profiler, our tools detect only dynamic instances of inefficiencies. False positives or false negatives can happen based on the kind of tools built atop Witch. A dead write detection tool has false negatives (can miss dead writes in an execution) but it has no false positives (all reported dead writes are dead writes). The performance benefit of using debug registers overweighs the downside of a small number of potential false negatives. Developer investigation or post-processing is necessary to make optimization choices—not all reported inefficiencies need be eliminated. Only high-frequency inefficiency spots are interesting; eliminating a long tail of insignificant inefficiencies that do not add up to a significant fraction is impractical and probably ineffective. Our investigation shows that only a few calling contexts contribute to most of the measured inefficiencies; for example, in SPEC CPU2006 benchmarks, fewer than five contexts, typically, contributed to over 90% of dead writes."
  }, {
    "heading": "5 Design and Implementation",
    "text": "We implement Witch in the open-source HPCToolkit [1] performance analysis tools suite. HPCToolkit works onmultilingual, multi-threaded, and multi-process, fully optimized applications on multiple programming models such as MPI and OpenMP. On a PMU sample, HPCToolkit’s profiler, hpcrun, walks the sampled thread’s call stack using an onthe-fly binary analysis technique and attributes the measurements to the sampled call path. hpcrun introduces negligible runtime overhead (∼3%) and consumes only a fewmegabytes of memory space for its metrics data when sampling at ∼200 samples/second/thread [77].\nPMU Sampling: Although the clients ofWitch can sample any precise PMU event to set a watchpoint, on Intel processors, typically, we use MEM_UOPS_RETIRED:ALL_STORES and MEM_UOPS_RETIRED:ALL_LOADS to drive PMU sampling. These events offer the address accessed in a sample.\nWatchpoint Registration: Witch automatically discovers the number of hardware debug registers supported on the platform. When a client wants to monitor an address,Witch uses the Linux perf_event interface to register a watchpoint event. The event is a HW_BREAKPOINT perf event (a PERF_TYPE_SOFTWARE event category). Witch registers a signal handler to capture watchpoint exceptions that the Linux perf_event interface raises when the event overflows. Witch sets the sample_period to 1 for its HW_BREAKPOINT\nevents, which ensures that the trap signal is delivered immediately after accessing the monitored address.\nPrecise PC of aWatchpoint: Some clients need the precise instruction pointer of the instruction triggering the watchpoint, for example, to distinguish a load from a store when a RW_TRAP watchpoint triggers. The BREAKPOINT event in Linux perf_event is not a PMU event and hence the Intel PEBS support, which otherwise provides the precise register state, is unavailable for a watchpoint. Although the watchpoint causes a trap immediately after the instruction execution, the PC seen in the signal handler context (contextPC) is one ahead of the actual PC (precisePC) that causes the trap. In the x86 variable instruction set ISA, it is non-trivial to derive the precisePC, even though it is just one instruction before the contextPC. A software solution is to find the function enclosing contextPC and disassemble every instruction till we reach the contextPC. This solution may fail with linear disassembly due to 1) data embedded in instruction and 2) missing function bounds [77]. Furthermore, it can be time-consuming if the function body is large. Our solution depends on the Last Branch Record (LBR) facility [25] provided by Intel Nehalem and its successors, which is exposed through the Linux perf_event interface. LBR tracks taken branches throughout CPU execution and continuously records the <from:to> pairs of instruction pointers in a fixed-size in-CPU circular buffer. Witch exploits the LBR facility by modifying the perf_event implementation inside the Linux kernel. Linux perf_event already has the facility to construct the precise PC by disassembling the instructions starting from the “to” field of the last entry in the LBR until the disassembly reaches the contextPC. Disassembling a basic block is “feather light” compared to full function disassembly. We reuse this component with PERF_TYPE_SOFTWARE to construct the precisePC when a watchpoint trap event happens. The kernel makes the precisePC available toWitch’s watchpoint exception handler in the ring buffer associated with the event on each watchpoint trap. This reduces ∼5% runtime overhead. Fast Watchpoint Replacement: Witch requires frequently disabling a watchpoint, closing all the kernel resources (perf_event file descriptor and an mmaped ring buffer) associated with the watchpoint, and recreating the same for another watchpoint. We enhance the kernel perf_event ioctl interface with an additional flag PERF_EVENT_IOC_MODIFY_ATTRIBUTES. This flag allows perf_event users to update the address and the access length associated with an already installed watchpoint. As a result, the user code can continue to reuse all the kernel resources associated with the previous perf_event file descriptor. Although Witch is functionally correct without this support, we found it useful to optimize this use case (∼5% overhead reduction). This change is being contributed to the Linux kernel as of this writing.\nStackAddresses: Clients ofWitchmay set a watchpoint on the stack in one function that returns, and another function invocation may overwrite the previous stack frame. Such situation will cause the watchpoint to trap, andWitch has no problem for such normal call-return sequence. If there is a redundancy in a callee, e.g., write to a variable in a callee that is frequent not read before returning to the caller, Witch can easily detect it.\nSetting a watchpoint on the application stack address has a corner case. On a PMU sample, the profiler’s overflow signal handler, by default, shares the same stack as the application thread. In Figure 3(a), assume M is the sampled stack address. Assume we set a watchpoint at M. If the next PMU sample is taken with a shallower stack (Figure 3(b)), and the signal stack frame overwrites M; it spuriously triggers the watchpoint. Similarly, one watchpoint exception handler stack frame may trap another watchpoint. We avoid this problem by establishing a separate signalhandler stack frame for both PMU signal handler and watchpoint exception handler using the Linux sigaltstack facility [41]. The sigaltstack facility allows each thread in a process to define an alternate signal stack in a userdesignated memory region. We use alternate stack to handle PMU and watchpoint signals as shown in Figure 3(c). All other signals continue to use the default stack unless specified otherwise by the application.\n6 Witchcraft: Client Tools of Witch We have already discussed the dead store detection client in the previous sections as a running example. In this section, we elaborate twomore clients that use theWitch framework to pinpoint different kinds of inefficiencies.\n6.1 SilentCraft: Silent Store Detection Updating a location with a value already present at the location is a silent store. Silent stores are useless since they do not change system state. Our prior work, RedSpy [82], shows that useless computations that store their results into memory often show up as silent stores. Here, we devise SilentCraft, a silent store detection client that mimics RedSpy.\nSilentCraft samples PMU store events. On each PMU sample, SilentCraft remembers the contents (value) of the memory location accessed in the sampled address. SilentCraft, then, arms a W_TRAP watchpoint W . SilentCraft disregards the loads that may intervene between two store operations. Hence loads do not trigger a watchpoint trap. SilentCraft also associates the calling context Cwatch of the sample point with the watchpointW . The next store operation (say in context Ctrap ), overlapping the same memory address, triggers a watchpoint exception. SilentCraft obtains the precise PC and the address accessed in the watchpoint fromWitch and compares the current contents of the memory location with the previously recorded value. The comparison is limited to the bytes that overlap between a) the sampled address and its access length and b) and trapped address and its access length. If all overlapping bytes are same, SilentCraft marks the calling context pair ⟨Cwatch ,Ctrap⟩ with proportional units of silent stores. Proportionality computation follows the previously discussed proportional attribution heuristic. To identify opportunities for approximate computation, for the floating-point operations, SilentCraft performs approximate equality check within a user-specified precision level. SilentCraft infers that a datum is a floating-point value by disassembling the instruction accessing the address.\nSilentCraft quantifies the store redundancy R̂ in an execution analogous to DeadCraft (Equation 1); two consecutive stores with unchanged values (approximately the same for floating point values) contribute to the “waste” and contribute to the “use” otherwise.\n6.2 LoadCraft: Load-after-load Detection We developed a new tool—LoadCraft—that detects a load followed by another load from the same location where the value remains unchanged between the two loads. It ignores intervening stores to the same address that may change the value and revert it to the original value before a load. Not all load-load redundancies can be eliminated. Since machines have a small number of registers, they often spill values to memory to be read back later. Unfavorable algorithms and data structures often show up as load-load redundancies that shed light on domain-specific optimization opportunities.\nLoadCraft samples PMU load events. The rest of the functionality is similar to that of SilentCraft, except that it requests a watchpoint for load access on the monitored location. Witch uses RW_TRAP because x86 machines do not\noffer a trap-on-load watchpoint. If a watchpoint triggers on a store operation, Witch merely drops it. LoadCraft quantifies the load redundancy L̂ in an execution analogous to DeadCraft (Equation 1), where two consecutive loads with (approximately) unchanged values contribute to the “waste” and different values contribute to the “use”."
  }, {
    "heading": "6.3 Witchcrafts on Multi-threading",
    "text": "Debug registers and PMUs are per CPU core and virtualized for each software thread. All the previously discussed Witch tools work on multi-threaded codes; they, however, track intra-thread inefficiencies only. If a thread T1 configures a watchpoint at address M , a trap occurs only in T1; other threads remain unaffected whether they accessM or not. Sharing addresses accessed by one thread with another thread allows building several tools for multi-threaded applications. AtopWitch, we have developed Feather [9]—a tool to detect false sharing in parallel programs."
  }, {
    "heading": "6.4 Discussion",
    "text": "Developers can only reason about inefficiencies at instruction, source line, or data-type granularities. Hence, in all tools we discussed, if a dynamic instruction writesM bytes, either all M bytes contribute to the inefficiency metric or none. In the three tools we developed, we made the following implementation decision: if the monitored element of a SIMD instruction instance is found to be wasteful (useful), we approximate that all elements in the SIMD instruction instance as wasteful (useful). Other tools are free to make a different choice. Currently,Witch is implemented to work on the native code such as C/C++/Fortran applications. The basic idea extends to a managed language but requires runtime support to map JIT-generated instruction to the source code."
  }, {
    "heading": "6.5 Presentation",
    "text": "HPCToolkit maintains all sampled call paths in a compact calling context tree (CCT) format [2]. HPCViewer, the graphical interface, enables navigating the CCT and the corresponding source code ordered by the monitored metrics. A topdown view shows a call path C starting from main to a leaf function with the breakdown of metrics at each level.Witch tools discussed here need to attribute metrics to calling context pairs ⟨Cwatch ,Ctrap⟩. Merely attributing a metric to two independent contexts loses the association between two related contexts during postmortem inspection. To maintain a correlation between a source context (e.g., dead) and target (killing) context, Witch appending a copy of the target calling context to a source calling context. For example, if a store in context main->A->B is overwritten by another store in context main->C->D, DeadCraft constructs a synthetic calling context: main->A->B->KILLED_BY->main->C->D. The dead write metrics will be attributed to the leaf of this call\nchain. These synthetic call chains make it easy to visually navigate the CCT and focus on top redundancy pairs. Figure 2 in Section 4 depicts this scheme."
  }, {
    "heading": "7 Evaluation",
    "text": "We evaluate Witch on a 2-socket, 18-core Intel Xeon E52699 v3 (Haswell) CPU clocked at 2.30GHz running Linux 4.8.0. The machine has 128GB DDR3 RAM. Simultaneous multi-threading (SMT) facility is not used in our experiments. All experiments use GCC v5.4.1 tools with -O3 and profileguided optimization (PGO) to ensure the highest level of optimization. DeadCraft and SilentCraft use the PMU event MEM_UOPS_RETIRED:ALL_STORES whereas LoadCraft uses MEM_UOPS_RETIRED:ALL_LOADS. In our experiments, we use the nearest prime number for the shown sampling intervals, which is the recommended method in PMU sampling. The raw data from our experiments are available online [7].\nTwo aspects are critically important in evaluatingWitch: accuracy and overhead compared to the exhaustive instrumentation techniques. We use SPEC CPU2006 reference benchmarks for this aspect of evaluation. Accuracy: For accuracy, we need to answer three questions: (1) how accurate are the results compared to exhaustive monitoring, (2) how does the accuracy vary with sampling rates, and (3) how stable are the sampled results from one run to another. The quantitative metric of dead writes is the percent of\ndead stores D̂ (bytes overwritten without reading) as described in Equation 1, which we compare against the groundtruth dead stores D from DeadSpy [8, 10]. We compare the percent of silent stores R̂ from SilentCraft against the ground-truth exhaustive monitoring metric R from RedSpy [10, 82]. No prior tool exists to compare against LoadCraft; hence we implemented an exhaustive load-load value redundancy detection tool called LoadSpy. We compare the percent of silent loads L̂ from LoadCraft against the ground-truth exhaustive monitoring metric L from LoadSpy. RedSpy also performs redundancy detection in registers, which we disabled for our evaluation. To assess the accuracy of our sampling clients against the ground-truth, we disable the bursty sampling used by RedSpy. SilentCraft, LoadCraft, RedSpy, and LoadSpy use 1% precision when comparing floating point values. Figure 4 compares the total redundancies found by different sampling vs. exhaustive monitoring tools. The error bars represent the metric values at different sampling rates for Witch tools, i.e., 100K (high), 500K, 1M, 5M, 10M, and 100M (low) events per PMU interrupt. Clearly, the sampling rate, when chosen with some care, does not significantly affect the results. The sampling tools are highly accurate in almost all cases. There are, however, some exceptions. DeadCraft and SilentCraft on hmmer and calculix suffer from shadow sampling effects [11, 38, 61], where high\nTable 1 Pin 100M 10M 5M 1M 500K mean STDEV Pin 100M 10M 5M 1M 500K mean STDEV Pin 100M 10M 5M 1M 500K mean STDEV astar-1 rivers.cfg 15.69 7.75 8.91 9.05 8.2 7.96 8.4 0.58 7.55 4.72 5.18 5.13 4.9 4.95 5.0 0.19 86.61 82.17 82.12 82.07 82.17 82.13 82.1 0.04 astar-2 BigLakes2048.cfg 17.08 7.97 10.55 9.08 9.37 9.16 9.2 0.92 10 6.74 8.55 7.34 8.87 8.63 8.0 0.93 87.57 81.55 79.59 80.2 80.16 80.19 80.3 0.72 bwaves 3.1 3.87 3.55 2.77 3.24 3.47 3.4 0.41 19.59 20.04 16.47 16.33 14.3 15.25 16.5 2.18 85.53 93.86 94.97 94.05 92.21 92.21 93.5 1.22 bzip2-1 chicken.jpg 30 16.95 18.02 16.2 17.97 18.95 19.57 18.1 1.28 8.83 10.25 14.19 14.15 13.5 13.11 13.0 1.62 65.52 66.95 62.45 62.35 63.13 63.13 63.6 1.91 bzip2-2 input.source 280 22.59 25.45 23.95 23.33 22.55 22.77 23.6 1.16 12.78 17.85 16.61 17.21 17.34 17.37 17.3 0.44 70.29 69.26 67.86 67.36 68.45 68.45 68.3 0.71 bzip2-3 text.html 280 18.28 20.59 19.18 18.73 18.62 18.54 19.1 0.85 15.87 21.13 22.71 22.25 23.21 22.83 22.4 0.80 77.18 70.11 72.56 71.12 71.63 71.63 71.4 0.89 bzip2-4 input.combined 200 21.97 21.28 22.59 22.82 22.85 22.16 22.3 0.65 12.53 20.64 16.35 16.95 16.68 17.28 17.6 1.74 69.91 67.7 67.49 67.79 68.06 68.32 67.9 0.32 bzip2-5 input.program 280 20.89 23.27 22.56 20.55 22.02 21.41 22.0 1.05 8.43 10 9.67 10.07 10.03 10.47 10.0 0.28 65.93 64.69 65.01 65.25 65.24 66.11 65.3 0.53 bzip2-6 liberty.jpg 30 13.34 8.82 10.08 10.54 10.81 10.93 10.2 0.86 21.41 28.99 32.2 32.67 32.97 32.54 31.9 1.64 74.93 77.83 74.85 74.75 74.88 74.88 75.4 1.34 cactusADM benchADM.par 0.2 0.11 0.14 0.13 0.2 0.22 0.2 0.05 25.95 20.97 19.93 22.18 17.58 17.59 19.7 2.05 75.39 80.4 80.47 79.73 79.94 79.94 80.1 0.32 calculix -i hyperviscoplastic 2.0 1.97 2.05 2.04 1.97 1.91 2.0 0.06 31.05 55.94 56.72 57.24 57.45 57.61 57.0 0.68 87.01 86.93 86.79 86.48 85.55 86.6 0.60 dealII 23 23.8 27.27 25.91 25.78 26.08 26.32 26.3 0.59 29.66 30.3 31.88 31.11 31.6 31.47 31.3 0.61 83.92 77.95 78.6 78.48 76.86 76.86 77.8 0.85 gamess-1 triazolium.config 6.59 8.78 7.19 7.47 7.2 7.11 7.6 0.70 40.94 44.29 43.84 44.54 44.38 44.78 44.4 0.35 70.88 79.64 81.77 82.52 82.74 83.08 82.0 1.38 gamess-2 h2ocu2+.gradient.config 11.81 14.75 12.7 13.72 13.73 13.56 13.7 0.73 29.87 37.19 48.52 46.88 46.81 45.34 44.9 4.48 78.43 84.8 86.22 86.67 85.7 85.7 85.8 0.70 gamess-3 cytosine.2.config 10.53 13.6 11.41 12 11.54 11.69 12.0 0.90 31.11 32.73 34.2 32.08 32.04 31.9 32.6 0.96 75.13 76.91 77.37 77.55 77.92 78.01 77.6 0.44 gcc-1 g23.i -o g23.s 66.47 72.97 74.82 75.02 75.34 75.25 74.7 0.98 85.71 84.06 84.83 83.16 83.83 83.56 83.9 0.62 84.62 87.4 81.96 83.58 82.46 82.46 83.6 2.22 gcc-2 scilab.i -o scilab.s 45.68 50.21 59 61.3 62.47 62.13 59.0 5.11 64.14 65.32 69.18 65.42 65.82 66.08 66.4 1.60 74.43 75.34 78.92 79.45 78.59 78.59 78.2 1.62 gcc-3 expr2.i -o expr2.s 78.04 86.25 84.66 84.75 85.15 85.42 85.2 0.64 89.42 88.88 89.57 90.52 90.44 90.38 90.0 0.71 83.03 76 82.23 80.47 81.36 81.36 80.3 2.47 gcc-4 expr.i -o expr.s 77.44 87.23 84.52 85.51 84.98 85.1 85.5 1.05 89.81 88.76 90.5 90.29 90.28 90.01 90.0 0.70 84.06 82.8 82.35 83.94 82.97 82.97 83.0 0.58 gcc-5 cp-decl.i -o cp-decl.s 75.48 84.18 86.93 86.56 86.19 86.15 86.0 1.07 88.14 88.18 91.02 90.44 89.5 89.69 89.8 1.07 81.97 88.69 82.26 81.73 83.77 82.27 83.7 2.87 gcc-6 166.i -o 166.s 65.58 78.74 78.01 78.17 78.75 78.25 78.4 0.34 83.24 82.31 85.13 84.15 84.65 84.29 84.1 1.07 81.88 76.69 79.79 81.77 81.15 80.43 80.0 1.98 gcc-7 200.i -o 200.s 49.8 72.35 66.27 66.12 67.17 67.03 67.8 2.59 69.75 69.29 70.17 73.39 71.73 71.69 71.3 1.58 78.11 84.07 78.46 79.29 78.41 79.03 79.9 2.39 gcc-8 s04.i -o s04.s 81.22 87.8 88.51 88.13 88.42 88.2 88.2 0.28 91.64 91.2 92.17 91.73 91.77 91.79 91.7 0.35 84.09 78.62 81.1 80.36 76.76 76.76 78.7 2.00 gcc-9 c-typeck.i -o c-typeck.s 83.29 87.62 89.07 88.69 89.09 89.06 88.7 0.63 91.31 93.63 92.04 92.03 92.03 91.85 92.3 0.74 81.13 81.67 80.66 81.11 81.34 81.34 81.2 0.37 GemsFDTD 2.4 0.65 0.49 0.54 0.78 0.48 0.6 0.13 25.3 33.08 33.72 36.44 46.36 49.33 39.8 7.54 76.15 76.72 79.95 83.25 93.9 93.9 85.5 7.97 gobmk-1 trevord.tst 26.8 25.66 27.12 27.26 27.39 26.8 0.70 31.6 31.72 31.5 31.7 31.8 31.7 0.12 67.36 67.48 66.7 67.22 67 67.2 0.31 gobmk-2 trevorc.tst 27.74 28.56 26.22 28.38 28.16 27.8 0.94 34.76 34.49 32.78 34.13 33.39 33.9 0.81 69.26 68.4 67.02 67.53 67.32 67.9 0.91 gobmk-3 13x13.tst 27.74 27.95 27.35 27.53 27.99 27.7 0.27 33.95 34.23 33.39 34.04 33.49 33.8 0.36 68.07 69.42 68.42 69.11 68.77 68.8 0.54 gobmk-4 score2.tst 39.42 36.63 37.04 36.72 36.85 37.3 1.18 39.92 42.48 39.51 40.02 40.05 40.4 1.18 80 77.87 77.84 77.72 77.66 78.2 1.00 gobmk-5 nngs.tst 27.14 27.17 27.16 27.06 27.03 27.1 0.06 30.7 32.47 31.47 31.82 31.93 31.7 0.65 67.97 67.36 68.53 68.07 68.11 68.0 0.42 gromacs -silent -deffnm gromacs -nice 02.2 2.65 2.96 2.98 2.93 2.95 2.9 0.14 9.89 8.98 8.5 8.22 8.49 8.2 8.5 0.31 59.43 58.98 58.44 58.2 57.67 58.61 58.4 0.49 h264ref-1 -d foreman_ref_encoder_baseline.cfg6.53 6.45 7.78 6.78 7.17 19.18 9.5 5.45 25.06 14.33 19.65 16.68 17.65 17.24 17.1 1.92 87.35 85.04 85.73 85.95 86.15 86.15 85.8 0.46 h264ref-2 -d foreman_ref_encoder_main.cfg39.84 42.64 42.2 42.17 42.18 43.17 42.5 0.44 87.02 86.47 86.77 87.18 87.09 86.89 86.9 0.28 92.76 92.62 91.71 91.43 91.69 91.53 91.8 0.47 h264ref-3 -d sss_encoder_main.cfg 41.4 43.91 43.73 43.61 45.98 45.74 44.6 1.16 89.82 89.41 90.37 90.01 90.11 90.05 90.0 0.35 92.93 93.25 93.37 93.16 93.23 93.2 0.16 hmmer-1 --fixed 0 --mean 500 --num 500000 --sd 350 --seed 0 retro.hmm67.9 28.27 28.36 28.49 28.25 27.54 28.2 0.37 37.84 19.81 19.54 20 19.7 19.8 19.8 0.17 91.44 72.36 72.07 71.85 72.26 72.26 72.2 0.20 hmmer-2 nph3.hmm swiss41 69.4 31.93 29.35 29.26 28.15 27.07 29.2 1.81 38.67 24.02 24.83 24.24 24.64 22.78 24.1 0.81 91.98 69.1 69.87 69.89 70.22 70.94 70.0 0.67 lbm 3000 reference.dat 0 0 100_100_130_ldc.of0.4 0.13 0.48 0.22 0.06 0 0.2 0.19 99.9 100 100 99.98 99.99 99.98 100.0 0.01 99.96 100 99.98 99.99 99.99 99.98 100.0 0.01 leslie3d 8.8 11 12.36 14.13 11.5 12.32 12.3 1.19 41.62 40.64 39.82 39.93 39.66 40.7 40.2 0.48 76.75 83.67 83.5 82.07 83.33 83.04 83.1 0.63 libquantum 1397 8 5.9 0.06 0.12 0.11 0.11 0.1 0.1 0.02 8.5 1.09 1.06 1.25 0.82 0.77 1.0 0.20 68.53 71.2 72.45 72.43 71.94 71.94 72.0 0.51 mcf inp.in 49.6 36.63 38.6 37.17 36.98 37.12 37.3 0.76 37.41 22.33 21.4 21.87 21.46 22.11 21.8 0.40 90.51 87.26 89.11 89.09 89.32 89.41 88.8 0.89 milc 9.6 15.57 15.25 14.37 11.06 19.02 15.1 2.85 18.18 14.49 17.34 20.99 9.58 18.47 16.2 4.36 71.13 73.89 76.51 76.7 83.39 77.15 77.5 3.52 namd --input namd.input --iterations 38 --output namd.out1.3 1.49 1.37 1.29 1.31 1.28 1.3 0.09 40.69 29.39 29.34 29.68 29.67 29.69 29.6 0.17 84.99 84.45 84.76 84.58 85 85.24 84.8 0.32 omnetpp omnetpp.ini 22.8 15.48 17.97 18.25 19.83 20.73 18.5 2.01 19.85 16.3 16.38 18.11 17.41 17.11 17.1 0.75 69.43 75.09 74.86 75.13 75.32 75.32 75.1 0.19 perlbench-1 -I./lib splitmail.pl 1600 12 26 16 45007.07 21.86 24.59 25.1 25.09 24.9 24.3 1.38 43 14.67 21.26 19.44 18.8 17.7 18.4 2.44 80.11 77.82 76.65 77.55 79.99 79.1 78.2 1.32 perlbench-2 -I./lib diffmail.pl 4 800 10 17 19 30017.6 17.86 20.61 24.75 28.11 29.62 24.2 4.95 49.41 34.05 30.43 33 27.81 31.6 31.4 2.42 71.83 75.88 74.66 74.57 74.57 74.3 1.49 perlbench-3 -I./lib checkspam.pl 2500 5 25 11 150 1 1 1 122.88 23.74 39.91 39.86 41.09 42.34 37.4 7.70 56.34 29.18 28.3 27.82 27.02 22.72 27.0 2.52 78.58 78.54 78.63 79.64 79.64 79.0 0.58 povray SPEC-benchmark-ref.ini 12.3 7.61 10.58 16.02 17.94 26.45 15.7 7.29 27.35 23.25 19.99 20.21 19.7 18.93 20.4 1.66 69.1 76.9 75.8 75.2 75.03 75.06 75.6 0.79 sjeng ref.txt 31.54 34.03 34.21 33.8 34.04 33.5 1.12 29.28 31.35 31.2 30.49 30.67 30.6 0.82 83.25 83.91 84.06 83.66 83.66 83.7 0.31 soplex-1 -s1 -e -m45000 pds-50.mps 37.48 38.04 37.7 37.27 37.18 36.67 37.4 0.52 41.03 38.13 39.88 40.62 40.19 40.85 39.9 1.08 80.07 77.99 77.53 77.6 77.81 78.2 1.06 soplex-2 -m3500 ref.mps 6.34 7.71 6.44 5.66 5.1 5.38 6.1 1.05 9.71 3.32 6.37 7.37 7.02 7.31 6.3 1.70 82.51 71.33 71.2 71.26 70.84 70.84 71.1 0.24 sphinx ctlfile . args.an4 16.5 19.24 18.06 18.45 18.67 18.6 18.6 0.43 45.78 45.65 45.18 45.19 41.52 40.84 43.7 2.30 94.82 96.92 96.89 97.05 97.04 97.04 97.0 0.08 tonto 5.8 2.69 2.88 2.65 2.8 2.94 2.8 0.12 29.98 18.72 20.2 20.16 20.37 20.37 20.0 0.70 71.4 75.17 74.7 74.82 75.16 75.89 75.1 0.46 wrf 15.5 13.1 12.61 12.64 13.36 12.8 12.9 0.32 38.94 40.1 39.47 39.48 39.58 37.47 39.2 1.01 79.04 78.52 79.69 79.24 79.35 79.5 79.3 0.45 Xalan -v t5.xml xalanc.xsl 9.12 10.11 11.61 13.22 -0.01 8.8 5.17 15.84 16.22 15.85 14.9 15.38 15.6 0.51 88.99 89.84 90.92 90.87 90.94 90.3 0.87 zeusmp 7.4 11.11 9.53 9.53 9.02 9.37 9.7 0.81 33.63 59.1 59.92 58.71 58.81 59.74 59.3 0.55 67.39 86.7 87.06 87.54 88.14 88.32 87.6 0.69\n% D\nea d\nBy te\ns\n0\n18\n36\n54\n72\n90\nas ta\nr1\nas ta\nr2\nbw av es bz ip 2- 1 bz ip 2- 2 bz ip 2- 3 bz ip 2- 4 bz ip 2- 5 bz ip 2- 6 ca ct us AD M ca lc ul ix de al II ga m es s1 ga m es s2 ga m es s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9 G em sF D TD go bm k1 go bm k2 go bm k3 go bm k4 go bm k5 gr om ac s h2 64 re f1 h2 64 re f2 h2 64 re f3 hm m er -1 hm m er -2 lb m le sl ie 3d lib qu an tu m m cf m ilc na m d om ne tp p pe rlb en ch -1 pe rlb en ch -2 pe rlb en ch -3 po vr ay sj en g so pl ex -1 so pl ex -2 sp hi nx to nt o w rf Xa la n ze us m p\nDeadSpy (exhaustive) DeadCraft (sampling)\n1\n(a) Dead Store\nTable 1 Pin 500K 1M 5M 10M 100M mean STDEV 500K 1M 5M 10M 100M mean STDEV 500K 1M 5M 10M 100M mean STDEV 500K 1M 5M 10M 100M mean STDEV Pin 500K 1M 5M 10M 100M mean STDEV 500K 1M 5M astar-1 rivers.cfg 15.69 8.45 8.57 8.91 9.26 9.98 9.0 0.62 8.07 8.41 8.85 8.12 11.39 9.0 1.39 7.97 8.35 9.23 8.31 9.65 8.7 0.71 7.96 8.2 9.05 8.91 7.75 8.4 0.58 7.55 4.74 4.58 4.96 4.25 4.95 4.7 0.30 4.95 4.84 5 astar-2 BigLakes2048.cfg 17.08 9.93 9.66 10.11 9.61 8.59 9.6 0.59 9.42 9.75 9.47 10.18 11.01 10.0 0.66 9.08 9 9.78 9.31 11.28 9.7 0.94 9.16 9.37 9.08 10.55 7.97 9.2 0.92 10 8.28 8.16 7.96 8.39 9.23 8.4 0.49 8.43 8.24 8.65 bwaves 3.1 2.62 2.91 2.52 2.96 3.05 2.8 0.23 2.82 3.12 2.81 3.14 2.92 3.0 0.16 3.07 3.2 3.09 3.28 2.6 3.0 0.26 3.47 3.24 2.77 3.55 3.87 3.4 0.41 19.59 15.61 15.14 16.52 14.43 15.33 15.4 0.76 15.91 13.76 15.36 bzip2-1 chicken.jpg 30 16.95 18.49 17.93 17.53 14.91 15.8 16.9 1.51 19.07 18.89 18.11 17.04 14.76 17.6 1.77 19.49 19.79 18.44 14.39 11.78 16.8 3.53 19.57 18.95 17.97 16.2 18.02 18.1 1.28 8.83 13.66 13.94 14.29 13.21 9.71 13.0 1.86 13.29 13.98 14.01 bzip2-2 input.source 280 22.59 22.11 22.16 22.53 23.06 21.93 22.4 0.45 22.54 22.63 23.05 23 22.57 22.8 0.25 22.72 22.97 22.99 22.09 26.71 23.5 1.83 22.77 22.55 23.33 23.95 25.45 23.6 1.16 12.78 17.91 18.21 17.85 15.83 17.1 17.4 0.96 17.56 17.29 17.52 bzip2-3 text.html 280 18.28 17.46 17.68 17.47 17.32 18.93 17.8 0.66 17.78 18.33 19.25 19.48 20.35 19.0 1.01 18.16 18.16 19.16 18.55 20.19 18.8 0.86 18.54 18.62 18.73 19.18 20.59 19.1 0.85 15.87 23.06 23.35 22.2 21.59 23.32 22.7 0.78 23.09 23.27 21.2 bzip2-4 input.combined 200 21.97 21.41 21.18 22.02 20.58 19.62 21.0 0.91 21.72 22.37 22.67 24.18 26.53 23.5 1.92 22.1 22.34 22.19 23.09 24.06 22.8 0.83 22.16 22.85 22.82 22.59 21.28 22.3 0.65 12.53 17.58 17.88 16.54 16.73 15.45 16.8 0.96 17.44 16.69 16.66 bzip2-5 input.program 280 20.89 20.98 20.96 20.76 20.95 21.68 21.1 0.35 21.14 21.33 20.27 20.63 18.9 20.5 0.96 21.49 21.47 20.5 21.9 21.17 21.3 0.52 21.41 22.02 20.55 22.56 23.27 22.0 1.05 8.43 10.49 10.29 10.33 10.15 10.53 10.4 0.15 10.31 10.05 10.15 bzip2-6 liberty.jpg 30 13.34 10.03 9.97 10.61 10.19 12.89 10.7 1.23 10.56 10.83 9.79 9.59 7.57 9.7 1.28 10.56 11.02 9.63 10.45 8.02 9.9 1.18 10.93 10.81 10.54 10.08 8.82 10.2 0.86 21.41 33.36 32.51 33.6 33.44 33.16 33.2 0.42 32.53 32.27 32.3 cactusADM benchADM.par 0.2 0.15 0.1 0.12 0.13 0.05 0.1 0.04 0.18 0.15 0.12 0.11 0.23 0.2 0.05 0.2 0.13 0.11 0.16 0.05 0.1 0.06 0.22 0.2 0.13 0.14 0.11 0.2 0.05 25.95 16.09 19.94 21.16 19.1 19.46 19.2 1.88 18.79 19.88 21.29 calculix -i hyperviscoplastic 2.0 1.83 1.83 1.88 1.91 2.06 1.9 0.09 1.88 1.89 1.9 2 2.44 2.0 0.24 1.92 1.91 1.95 1.98 1.6 1.9 0.15 1.91 1.97 2.04 2.05 1.97 2.0 0.06 31.05 56.95 56.8 56.53 58.77 56.63 57.1 0.93 57.46 57.77 59.09 dealII 23 23.8 26.26 26.47 26.31 26.03 27.02 26.4 0.37 26.16 26.63 25.88 25.76 24.15 25.7 0.94 27.02 25.67 26.29 25.11 27.46 26.3 0.96 26.32 26.08 25.78 25.91 27.27 26.3 0.59 29.66 32.3 33.04 31.87 32.27 31.36 32.2 0.62 32.03 32.21 31.59 gamess-1 triazolium.config 6.59 7.09 6.98 7.22 6.87 8.65 7.4 0.73 7.18 7.21 7.08 6.83 6.41 6.9 0.33 7.01 7.14 7.14 6.93 5.96 6.8 0.50 7.11 7.2 7.47 7.19 8.78 7.6 0.70 40.94 44.66 44.66 44.66 44.53 43.45 44.4 0.53 44.66 44.43 44.39 gamess-2 h2ocu2+.gradient.config 11.81 13.64 13.56 13 14.8 12.19 13.4 0.96 13.8 13.23 13.46 14.39 11.96 13.4 0.90 13.6 13.87 12.95 12.81 13.67 13.4 0.47 13.56 13.73 13.72 12.7 14.75 13.7 0.73 29.87 45.76 46.31 48.45 45.24 44.44 46.0 1.51 45.77 45.92 45.05 gamess-3 cytosine.2.config 10.53 11.52 11.43 11.77 11.78 15.36 12.4 1.68 11.62 11.52 11.59 12.05 11.43 11.6 0.24 11.5 11.85 11.16 11.68 12.35 11.7 0.44 11.69 11.54 12 11.41 13.6 12.0 0.90 31.11 31.84 32.15 31.85 31.78 31.48 31.8 0.24 31.81 31.93 31.19 gcc-1 g23.i -o g23.s 66.47 75.08 74.83 74.43 74.98 74.05 74.7 0.43 75.27 74.96 75.58 75.48 78.99 76.1 1.66 75.47 75.51 75.37 75.57 71.37 74.7 1.84 75.25 75.34 75.02 74.82 72.97 74.7 0.98 85.71 83.55 85.66 84.3 84.73 81.53 84.0 1.55 83.54 83.99 84.07 gcc-2 scilab.i -o scilab.s 45.68 61.95 61.23 61.4 63.05 65.54 62.6 1.77 62.05 61.47 62.48 63.29 50.48 60.0 5.34 61.3 61.87 62.3 64.7 57.24 61.5 2.70 62.13 62.47 61.3 59 50.21 59.0 5.11 64.14 66.97 67.5 68.61 64.88 56.52 64.9 4.87 66.9 66.68 67.41 gcc-3 expr2.i -o expr2.s 78.04 85.84 85.47 85.27 84.31 88.09 85.8 1.40 85.92 85.77 85.71 85.05 82.82 85.1 1.29 86.29 85.68 84.88 85.17 87.75 86.0 1.14 85.42 85.15 84.75 84.66 86.25 85.2 0.64 89.42 90.84 90.91 90.24 90.79 89.52 90.5 0.59 90.61 90.68 90.08 gcc-4 expr.i -o expr.s 77.44 86.04 85.64 84.61 84.75 83.65 84.9 0.94 85.27 86.39 84.51 85.61 83.98 85.2 0.94 85.15 85.19 84.58 84.56 87.01 85.3 1.00 85.1 84.98 85.51 84.52 87.23 85.5 1.05 89.81 90.94 90.92 90.87 89.61 90.06 90.5 0.61 90.83 90.43 90.17 gcc-5 cp-decl.i -o cp-decl.s 75.48 86.77 86.47 86.61 85.73 86.25 86.4 0.40 86.43 86.39 85.84 86.33 87.8 86.6 0.73 86.98 85.88 86.23 85.87 87.64 86.5 0.77 86.15 86.19 86.56 86.93 84.18 86.0 1.07 88.14 90.26 90.45 89.23 90.24 87.11 89.5 1.40 89.57 89.69 89.73 gcc-6 166.i -o 166.s 65.58 79.39 79.32 77.73 77.34 81.06 79.0 1.49 78.96 78.49 78.96 77.6 76.29 78.1 1.13 78.63 78.78 77.39 75.86 77.53 77.6 1.18 78.25 78.75 78.17 78.01 78.74 78.4 0.34 83.24 85.24 84.87 82.96 83.98 83.85 84.2 0.90 84.22 85.11 84.42 gcc-7 200.i -o 200.s 49.8 66.64 66.46 66.25 65.56 67.96 66.6 0.88 66.28 66.65 65.86 65.22 64.67 65.7 0.80 66.57 66.97 66.37 66.45 63.55 66.0 1.38 67.03 67.17 66.12 66.27 72.35 67.8 2.59 69.75 73.47 72.72 73.15 72.59 72.83 73.0 0.36 72.03 72.6 73.36 gcc-8 s04.i -o s04.s 81.22 88.51 88.61 87.86 87.73 86.8 87.9 0.73 88.18 88.21 89.52 88.03 86.86 88.2 0.94 89.44 88.54 88.64 87.41 87.16 88.2 0.94 88.2 88.42 88.13 88.51 87.8 88.2 0.28 91.64 92.05 92.28 92.36 92.75 92.37 92.4 0.25 92.38 91.8 92.06 gcc-9 c-typeck.i -o c-typeck.s 83.29 89.6 89.44 89.3 89.03 89.32 89.3 0.21 89.23 89.52 88.82 88.87 91.68 89.6 1.18 89.53 89.33 89.21 88.65 92.07 89.8 1.33 89.06 89.09 88.69 89.07 87.62 88.7 0.63 91.31 92.95 92.56 92.46 92.54 91.72 92.4 0.45 92.44 92.06 91.84 GemsFDTD 2.4 0.63 0.39 0.22 0.32 0.62 0.4 0.18 0.45 0.59 0.31 0.48 0.5 0.5 0.10 0.55 0.55 0.37 0.57 0.59 0.5 0.09 0.48 0.78 0.54 0.49 0.65 0.6 0.13 25.3 36.6 33.76 33.17 29.21 30.83 32.7 2.84 35.1 34.66 34.65 gobmk-1 trevord.tst 26.7 27.46 27.15 28.32 25.65 27.1 0.98 26.63 26.71 27.46 27.7 29.29 27.6 1.07 27.12 27.19 27.33 27.84 28.43 27.6 0.55 27.39 27.26 27.12 25.66 26.8 26.8 0.70 31.4 30.98 32.68 31.7 34.22 32.2 1.29 31.59 31.44 32.07 gobmk-2 trevorc.tst 27.21 27.23 27.8 27.72 26.56 27.3 0.50 27.99 27.94 28.28 28.75 27.44 28.1 0.48 27.98 28.21 28.86 27.66 30.05 28.6 0.95 28.16 28.38 26.22 28.56 27.74 27.8 0.94 32.74 32.85 32.26 34.03 31.03 32.6 1.08 33.5 33.2 33.05 gobmk-3 13x13.tst 26.55 27.21 27.61 28.39 26.46 27.2 0.80 27.96 27.91 28.55 28.49 26.32 27.8 0.90 28.08 27.86 29.08 27.85 26.94 28.0 0.76 27.99 27.53 27.35 27.95 27.74 27.7 0.27 32.84 33.64 33.27 31.11 36.4 33.5 1.91 33.77 34.2 32.74 gobmk-4 score2.tst 36.97 36.9 36.09 36.42 32.87 35.9 1.70 36.67 36.56 35.89 38.29 38.88 37.3 1.26 36.64 36.92 37.78 37.24 37.73 37.3 0.50 36.85 36.72 37.04 36.63 39.42 37.3 1.18 40.51 40.47 39.94 39.22 36.91 39.4 1.49 40.27 40.22 41.12 gobmk-5 nngs.tst 26.83 26.89 26.98 27.27 24.44 26.5 1.15 26.92 26.68 27.23 26.75 27.23 27.0 0.26 27.14 26.89 26.75 27.35 26.25 26.9 0.42 27.03 27.06 27.16 27.17 27.14 27.1 0.06 31.68 31.72 30.87 31.66 28.88 31.0 1.22 31.45 31.5 30.99 gromacs -silent -deffnm gromacs -nice 02.2 3.17 2.94 2.93 2.85 3.13 3.0 0.14 2.87 2.9 2.95 2.75 2.65 2.8 0.12 2.93 2.99 3.21 3.03 2.7 3.0 0.18 2.95 2.93 2.98 2.96 2.65 2.9 0.14 9.89 8.65 8.61 8.75 8.27 8.84 8.6 0.22 8.68 8.67 8.71 h264ref-1 -d foreman_ref_encoder_baseline.cfg6.53 34.37 26.47 14.06 7.11 9.84 18.4 11.62 28.86 24.57 7.01 27.33 7.97 19.1 10.76 27 7.33 6.83 6.12 9.25 11.3 8.85 19.18 7.17 6.78 7.78 6.45 9.5 5.45 25.06 18 18.53 17.02 17.87 16.7 17.6 0.75 17.87 17 17.47 h264ref-2 -d foreman_ref_encoder_main.cfg39.84 42.57 42.27 42.62 42 46.72 43.2 1.96 42.39 42.76 42.2 41.48 46.13 43.0 1.81 42.97 42.99 40.37 43.76 40.18 42.1 1.66 43.17 42.18 42.17 42.2 42.64 42.5 0.44 87.02 88.1 87.69 87.14 86.45 86.49 87.2 0.73 86.9 86.85 86.88 h264ref-3 -d sss_encoder_main.cfg 41.4 46.15 44.52 44.06 43.71 45.68 44.8 1.05 45.98 46.27 44.69 43.78 44.86 45.1 1.01 46.23 45.79 44.03 43.83 42.21 44.4 1.62 45.74 45.98 43.61 43.73 43.91 44.6 1.16 89.82 90.43 90.46 89.99 90.12 90.85 90.4 0.34 90 90.04 89.94 hmmer-1 --fixed 0 --mean 500 --num 500000 --sd 350 --seed 0 retro.hmm67.9 30.62 27.98 29.86 30.08 29.38 29.6 1.00 48 28.64 28.73 28.9 28.82 32.6 8.60 25.57 29.15 28.55 29.8 30.23 28.7 1.84 27.54 28.25 28.49 28.36 28.27 28.2 0.37 37.84 19.76 18.99 19.45 19.47 18.64 19.3 0.44 19.26 18.93 19.58 hmmer-2 nph3.hmm swiss41 69.4 34.99 29.23 29.12 28.79 29.96 30.4 2.59 33.19 30.78 30.3 29.83 29.68 30.8 1.43 28.7 33.03 29.2 28.94 29.4 29.9 1.79 27.07 28.15 29.26 29.35 31.93 29.2 1.81 38.67 23.28 25.24 24.6 24.47 22.28 24.0 1.18 22.25 24.38 24.21 lbm 3000 reference.dat 0 0 100_100_130_ldc.of0.4 0.11 0.21 0.32 0.1 0.8 0.3 0.29 0.04 0.02 0.25 0.35 0.13 0.2 0.14 0.03 0 0.12 0.3 0.67 0.2 0.28 0 0.06 0.22 0.48 0.13 0.2 0.19 99.9 99.98 100 99.98 100 100 100.0 0.01 99.98 100 100 leslie3d 8.8 12.2 11.26 11.82 11.52 12.95 12.0 0.66 11.82 10.66 14.39 12.4 13.82 12.6 1.51 10.8 10.57 14.16 12.26 12.27 12.0 1.44 12.32 11.5 14.13 12.36 11 12.3 1.19 41.62 38.72 39.48 40.34 40.27 39.94 39.8 0.67 38.89 39.89 40.75 libquantum 1397 8 5.9 0.09 0.08 0.09 0.07 -0.01 0.1 0.04 0.11 0.08 0.08 0.12 0.06 0.1 0.02 0.09 0.1 0.1 0.11 0.06 0.1 0.02 0.1 0.11 0.11 0.12 0.06 0.1 0.02 8.5 0.63 0.64 0.66 0.87 1.09 0.8 0.20 0.69 0.71 1.01 mcf inp.in 49.6 37.42 36.77 36 37.32 34.37 36.4 1.26 37.5 37.73 35.5 37.19 35.93 36.8 0.99 37.43 37.34 37.92 37.77 35.76 37.2 0.86 37.12 36.98 37.17 38.6 36.63 37.3 0.76 37.41 22.54 21.34 21.6 21.36 18.68 21.1 1.44 21.53 21.4 21.49 milc 9.6 19.28 14.83 15.06 15.61 15.76 16.1 1.81 20.42 16.88 14.65 14.8 12.3 15.8 3.04 19.33 14.37 14.26 14.49 11.45 14.8 2.84 19.02 11.06 14.37 15.25 15.57 15.1 2.85 18.18 15.69 17.88 17.68 17.27 16.5 17.0 0.90 15.58 17.86 17.49 na d --input namd.input --iterations 38 --output namd.out1.3 1.27 1.27 1.2 1.35 1.4 1.3 0.08 1.29 1.29 1.22 1.37 1.24 1.3 0.06 1.27 1.3 1.32 1.47 1.46 1.4 0.09 1.28 1.31 1.29 1.37 1.49 1.3 0.09 40.69 29.54 29.58 29.71 29.29 29.55 29.5 0.15 29.73 29.51 29.53 o netpp omnetpp.ini 22.8 19.81 18.57 17.71 16.8 18.18 18.2 1.11 20.26 19.55 17.1 17.52 16.74 18.2 1.57 19.91 19.31 17.33 17.79 18.46 18.6 1.06 20.73 19.83 18.25 17.97 15.48 18.5 2.01 19.85 16.73 16.72 16.57 16.64 16.21 16.6 0.21 17.06 17.25 17.19 perlbench-1 -I./lib splitmail.pl 1600 12 26 16 45007.07 24.76 25.67 27.41 25.34 27.2 26.1 1.17 26.09 25.65 25.98 27.7 24.7 26.0 1.08 27.73 17.02 26.2 24.36 28.23 24.7 4.55 24.9 25.09 25.1 24.59 21.86 24.3 1.38 43 21.25 21.08 20.64 20.02 23.04 21.2 1.13 17.33 17.67 20.49 perlbench-2 -I./lib diffmail.pl 4 800 10 17 19 30017.6 31.26 26.1 25.51 19.58 20.66 24.6 4.69 30.44 29.68 22.54 17.64 23.88 24.8 5.31 28.45 25.58 19.05 24.34 18.29 23.1 4.35 29.62 28.11 24.75 20.61 17.86 24.2 4.95 49.41 34.57 34.3 33.42 34.05 34.62 34.2 0.49 31.2 31.63 28.34 perlbench-3 -I./lib checkspam.pl 2500 5 25 11 150 1 1 1 122.88 31.7 40.69 40.21 27.41 25.61 33.1 7.05 37.34 41.59 40.81 27.53 26.21 34.7 7.34 42.92 40.68 40.3 34.51 20.99 35.9 8.88 42.34 41.09 39.86 39.91 23.74 37.4 7.70 56.34 33.22 32.45 33.64 32.76 32.66 32.9 0.48 27.93 28.24 27.18 povray SPEC-benchmark-ref.ini 12.3 25.4 20.43 12.63 18.03 9.52 17.2 6.29 28.99 18.07 14.92 10.14 10.21 16.5 7.76 26.56 23.12 16.52 12.41 10.93 17.9 6.76 26.45 17.94 16.02 10.58 7.61 15.7 7.29 27.35 19.25 20 19.56 19.49 19.53 19.6 0.27 19.13 19.6 19.91 sjeng ref.txt 34.24 33.78 34.1 33.21 33.31 33.7 0.46 33.65 33.32 33.86 33.12 35.52 33.9 0.95 34.32 33.79 34.41 34.02 33.87 34.1 0.27 34.04 33.8 34.21 34.03 31.54 33.5 1.12 30.37 30.65 30.38 31.39 30.44 30.6 0.43 30.41 30.85 30.81 soplex-1 -s1 -e -m45000 pds-50.mps 37.48 35.87 35.95 36.67 35.84 35.13 35.9 0.55 36.37 36.89 37.26 37.08 31.67 35.9 2.36 36.86 36.74 36.43 36.8 38.29 37.0 0.73 36.67 37.18 37.27 37.7 38.04 37.4 0.52 41.03 40.07 39.85 38.66 41.25 37.87 39.5 1.31 39.93 40.31 40.55 soplex-2 -m3500 ref.mps 6.34 5.07 5.08 5.08 5.2 4.21 4.9 0.40 4.91 5.33 5.05 4.61 3.32 4.6 0.78 5.4 5.29 5.56 5.53 4.87 5.3 0.28 5.38 5.1 5.66 6.44 7.71 6.1 1.05 9.71 6.16 7.73 7.53 6.98 8.35 7.4 0.83 7.67 7.58 7.08 sphinx ctlfile . args.an4 16.5 18.65 18.11 18.45 18.47 18.84 18.5 0.27 18.49 18.56 17.94 19.14 17.91 18.4 0.51 18.57 18.43 19.11 17.38 14.21 17.5 1.96 18.6 18.67 18.45 18.06 19.24 18.6 0.43 45.78 41.3 45.81 42.42 46.02 42.51 43.6 2.16 41.11 41.97 41.51 tonto 5.8 2.77 2.74 2.71 2.76 3 2.8 0.12 2.76 2.68 2.9 2.67 2.66 2.7 0.10 2.8 2.78 2.85 2.7 3.67 3.0 0.40 2.94 2.8 2.65 2.88 2.69 2.8 0.12 29.98 21.59 21.46 21.3 21.35 22.23 21.6 0.38 21.33 21.45 21.26 wrf 15.5 12.68 14.36 12.01 13.7 14.72 13.5 1.14 11.86 12.36 12.3 12.75 12.29 12.3 0.32 13.26 11.34 11.92 12.55 14.11 12.6 1.09 12.8 13.36 12.64 12.61 13.1 12.9 0.32 38.94 39.5 38.23 39.27 38.13 39 38.8 0.62 40.05 38.37 38.77 Xalan -v t5.xml xalanc.xsl 10.15 9.32 8.72 9.01 11.75 9.8 1.22 12.68 9.39 10.01 9.11 10.72 10.4 1.43 14.75 9.61 9.53 9.21 11.54 10.9 2.33 15.24 13.22 11.61 10.11 9.12 11.9 2.44 13.22 14.13 14.67 14.45 15.79 14.5 0.93 14.95 16.76 15.02 zeusmp 7.4 8.67 9.77 9.67 8.62 8.37 9.0 0.65 8.79 9.08 9 9.15 9.68 9.1 0.33 9.45 9.1 9.12 9.53 11.23 9.7 0.88 9.37 9.02 9.53 9.53 11.11 9.7 0.81 33.63 58.16 58.43 58.15 59.66 60.95 59.1 1.22 58.36 59.16 58.65\n% S\nile nt\ns to\nre d\nby te\ns\n0\n25\n50\n75\n100\nas ta\nr1\nas ta\nr2\nbw av es bz ip 2- 1 bz ip 2- 2 bz ip 2- 3 bz ip 2- 4 bz ip 2- 5 bz ip 2- 6 ca ct us AD M ca lc ul ix de al II ga m es s1 ga m es s2 ga m es s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9 G em sF D TD go bm k1 go bm k2 go bm k3 go bm k4 go bm k5 gr om ac s h2 64 re f1 h2 64 re f2 h2 64 re f3 hm m er -1 hm m er -2 lb m le sl ie 3d lib qu an tu m m cf m ilc na m d om ne tp p pe rlb en ch -1 pe rlb en ch -2 pe rlb en ch -3 po vr ay sj en g so pl ex -1 so pl ex -2 sp hi nx to nt o w rf Xa la n ze us m p\nRedSpy (exhaustive) SilentCraft (sampling)\n% D\nea d\nBy te\ns\n0\n18\n36\n54\n72\n90\nas ta\nr1\nas ta\nr2\nbw av es bz ip 2- 1 bz ip 2- 2 bz ip 2- 3 bz ip 2- 4 bz ip 2- 5 bz ip 2- 6 ca ct us AD M ca lc ul ix de al II ga m es s1 ga m es s2 ga m es s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9 G em sF D TD go bm k1 go bm k2 go bm k3 go bm k4 go bm k5 gr om ac s h2 64 re f1 h2 64 re f2 h2 64 re f3 hm m er -1 hm m er -2 lb m le sl ie 3d lib qu an tu m m cf m ilc na m d om ne tp p pe rlb en ch -1 pe rlb en ch -2 pe rlb en ch -3 po vr ay sj en g so pl ex -1 so pl ex -2 sp hi nx to nt o w rf Xa la n ze us m p\nDeadSpy (exhaustive) DeadCraft (sampling)\n% S\nile nt\nlo ad\ned b\nyt es\n0\n25\n50\n75\n100\nas ta\nr1\nas ta\nr2\nbw\nav es\nbz\nip 2- 1\nbz\nip 2- 2\nbz\nip 2- 3\nbz\nip 2- 4\nbz\nip 2- 5\nbz\nip 2- 6\nca\nct\nus\nAD M\nca\nlc ul ix de al II\nga\nm\nes s1\nga\nm\nes s2\nga\nm\nes s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9\nG\nem\nsF D TD\ngo\nbm k1\ngo\nbm k2\ngo\nbm k3\ngo\nbm k4\ngo\nbm k5\ngr\nom ac s\nh2\n64\nre f1\nh2\n64\nre f2\nh2\n64\nre f3\nhm\nm er -1\nhm\nm er -2 lb m\nle\nsl ie 3d\nlib\nqu\nan tu m m cf m ilc na m d\nom\nne tp p\npe\nrlb\nen ch -1\npe\nrlb\nen ch -2\npe\nrlb\nen ch -3 po vr ay sj en g\nso\npl ex -1\nso\npl ex -2 sp hi nx to nt o w rf Xa la n\nze\nus m p\nLoadSpy (exhaustive) LoadCraft (sampling)\n1\n(b) Silent Store\n% S\nile nt\ns to\nre d\nby te\ns\n0\n25\n50\n75\n100\nas ta\nr1\nas ta\nr2\nbw av es bz ip 2- 1 bz ip 2- 2 bz ip 2- 3 bz ip 2- 4 bz ip 2- 5 bz ip 2- 6 ca ct us AD M ca lc ul ix de al II ga m es s1 ga m es s2 ga m es s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9 G em sF D TD go bm k1 go bm k2 go bm k3 go bm k4 go bm k5 gr om ac s h2 64 re f1 h2 64 re f2 h2 64 re f3 hm m er -1 hm m er -2 lb m le sl ie 3d lib qu an tu m m cf m ilc na m d om ne tp p pe rlb en ch -1 pe rlb en ch -2 pe rlb en ch -3 po vr ay sj en g so pl ex -1 so pl ex -2 sp hi nx to nt o w rf Xa la n ze us m p\nRedSpy (exhaustive) SilentStore (sampling)\n% S\nile nt\nlo ad\ned b\nyt es\n2\n5\n75\n100\nas ta\nr1\nas ta\nr2\nbw av es bz ip 2- 1 bz ip 2- 2 bz ip 2- 3 bz ip 2- 4 bz ip 2- 5 bz ip 2- 6 ca ct us AD M ca lc ul ix de al II ga m es s1 ga m es s2 ga m es s3 gc c1 gc c2 gc c3 gc c4 gc c5 gc c6 gc c7 gc c8 gc c9 G em sF D TD go bm k1 go bm k2 go bm k3 go bm k4 go bm k5 gr om ac s h2 64 re f1 h2 64 re f2 h2 64 re f3 hm m er -1 hm m er -2 lb m le sl ie 3d lib qu an tu m m cf m ilc na m d om ne tp p pe rlb en ch -1 pe rlb en ch -2 pe rlb en ch -3 po vr ay sj en g so pl ex -1 so pl ex -2 sp hi nx to nt o w rf Xa la n ze us m p\nLoadSpy (exhaustive) LoadCraft (sampling)\n2\n(c) Silent Load\nFigure 4.Witch tools vs. instrumentation tools on SPEC CPU2006. Error bars capture different sampling rates. Ground truth instrumentation data is unavailable for gobmk, sjeng, and Xalan since they ran out of memory. The benchmarks with multiple inputs (e.g., bzip2) appear multiple times with different numerical suffixes.\nlatency stores hide low latency stores. GemsFDTD, perlbench, and zeusmp have many small inefficiencies scattered all over the code, leading to inaccuracies in SilentCraft. We ran each benchmark 10 times at 5M sampling rate (not shown) and the maximum standard deviations were 2.27%, 1.89%, and 0.77% for DeadCraft, SilentCraft, and LoadCraft respectively, which proves the run-to-run sampling stability.\nlbm has ∼100% silent stores and silent loads, but it has negligible dead stores. lbm is a floating point code, which simulates incompressible fluids in 3D. One iteration updates the values in an array that are loaded in the next iteration. The difference between the values produced in adjacent iterations is less than our predefined 1% threshold. Hence, LoadCraft treats these loads as redundant ones. Similarly, SilentCraft treats the stores to be approximately the same.\nTo assess the effectiveness of reservoir sampling, we vary the number of debug registers from one to four and compare the redundancy metrics against the ground truth. Figure 5 shows that the number of debug registers has little practical influence in DeadCraft on the quality of results except h264ref, which shows better results with four debug registers. The online compendium [7] corroborates this observation on SilentCraft and LoadCraft.\nTo assess the effectiveness of our proportional attribution based on samples taken in a context, we compared the accuracy with and without this feature at different sampling\nSession 4A: Memory 1 ASPLOS’18, March 24–28, 2018, Williamsburg, VA, USA\nrates and also with different number of debug registers with all three tools (not shown); we also compared it against the ground truth. In general, the feature did not make significantly positive or negative impact. GemsFDTD and perl were exceptions, where having the feature improved the accuracy.\nTo further understand the accuracy, we compared the rank ordering and percentage contribution of the top N redundancy pairs between DeadSpy and DeadCraft; we chose N to add up to 90% of redundancy observed in execution. No single metric suffices to compare this type of complex data. We used edit distance and set difference of the top N contexts and also compared weights at each position. Our measurements [7] show that only a handful of context pairs account for the majority of redundancies and their rank ordering and individual weights match the exhaustive monitoring. Overhead: Table 1 shows the runtime slowdown and memory bloat of sampling vs. exhaustive monitoring. Slowdown (memory bloat) is the ratio of the runtime (peak memory usage) under monitoring to the runtime (peak memory usage) of the corresponding native execution. We show the average values for the same benchmark with multiple inputs. We used the sampling period of one in 5M stores and one in 10M loads (since loads are more common), which we found to be highly effective. Two critical things to observe about the sampling tools are 1) their overheads are at least an order of magnitude less than the exhaustive instrumentation tools, and 2) they introduce negligible overhead. Deep recursive codes such as xalanbmk, sjeng, and gombk incur higher space and time overheads; and their instrumentation counterparts do not run to completion. Recursive codes with inefficiencies (e.g., SilentCraft on gobmk and LoadCraft on xalanbmk) exacerbate memory bloat due to large calling context trees. Codes with a very small memory footprint (e.g., povray) show higher memory bloat because of some basic pre-allocated data structures used in our tools.\nLoadCraft has higher overhead compared to the other two tools since 1) loads are more common than stores, 2) a high fraction of loading the same value leads to more watchpoint traps and inefficiency reporting cost, 3) most PMU samples find a free debug register and incur the cost of arming it, and finally 4) LoadCraft sets the RW_TRAPwatchpoint (x86 does not support break on load watchpoint), which triggers a spurious exception on a store. Table 2 shows the geometric mean and median of the slowdown and memory bloat at different sampling periods in SPEC CPU2006."
  }, {
    "heading": "8 Case Studies",
    "text": "The lightweight nature ofWitch tools allowed us to apply it on an array of benchmark suites—SPEC CPU2006 [73], SPEC OMP2012 [74], NERSC Trinity [57], Rodinia [62], and STAMP [56] and full applications—NWChem [79], Caffe [28], GNU Binutils [20], and Kallisto RNA sequencing [52]. Table 3 summarizes the new performance bugs found by our tools (denoted by✓ prefix) and confirms previously found performance issues [8, 82]. In this section, we describe four case studies covering the analyses by the three Witch tools."
  }, {
    "heading": "8.1 NWChem-6.3",
    "text": "NWChem [79] is a production computational chemistry package, which implements several quantum mechanics and molecular mechanics methods. NWChem consists of six million lines of code written primarily in Fortran and C and parallelized with MPI [53]. We use the QM-CC aug-cc-pvdz input and eight MPI processes in our studies.\nDeadCraft reports that more than 60% of memory stores are dead. Figure 6 shows the full calling contexts of the top (94% contribution to total dead writes) dead and killing store pair in the call of function dfill, which zeroes the array work2. With the given input, calls to dfill repeat more than 200K times, resulting in writing 500GB data that are never used. With further analysis, we identified that the size of work2 was larger than necessary, and the zero initialization was unnecessary, leading to the dead and killing writes in the same location. We eliminate this unnecessary initialization, yielding a 1.43× speedup. This bug, which was hiding in the large code base, is now fixed.Witch incurs only 6% runtime overhead whereas the fine-grained profiler, DeadSpy, incurs > 10× slowdown identifying the same problem."
  }, {
    "heading": "8.2 Caffe-1.0",
    "text": "We apply SilentCraft on the deep learning framework Caffe [28]. We study the OpenMP C++ CPU version, which uses Intel MKL [26] to parallelize its computation kernels. We use the CIFAR-10 dataset to train the CIFAR network with 0.9 momentum, 4e-3 weight decay, 1e-3 learning rate, 128 batch size. We run Caffe with eight threads.\nSilentCraft attributes 25% of total memory stores as redundant in a loop nest belonging to a major computation kernel in pooling and normalization layers (Listing 4). The memory stores to the array bottom_diff (Line 8) account for 17% of total silent stores. A large portion of elements in\nBenchmark\nas ta\nr\nbw av\nes\nbz ip\n2\nca ct\nus A\nD M\nca lc\nul ix\nde al\nII\nga m\nes s\ngc c\nG em\nsF D\nTD\ngo bm\nk\ngr om\nac s\nh2 64\nre f\nhm m\ner\nlb m le sli\ne3 d\nlib qu\nan tu\nm\nm cf m ilc na m\nd\nom ne\ntp p\npe rl\nbe nc\nh\npo vr\nay\nsj en\ng\nso pl\nex\nsp hi\nnx 3\nto nt\no\nw rf\nxa la\nnc bm\nk\nze us\nm p\nG eo\nM ea\nn\nM ed\nia n\nOriginal Time (second) 139 303 64 371 635 246 50 24 297 71 317 138 160 342 215 173 221 458 318 182 65 101 367 86 423 408 312 158 360 Original Memory Usage (MB) 232 875 562 664 118 795 22 459 831 30 16 38 16 411 125 95 1677 681 48 171 400 7 176 279 44 36 695 421 512\nDead store\nSlowdown (times) DeadSpy 22.65 31.70 32.32 29.93 33.40 54.70 40.00 43.57 26.36 - 26.95 59.67 52.01 28.86 31.70 27.87 21.69 14.61 22.18 37.80 71.91 71.13 - 26.45 26.16 34.85 24.60 - 19.95 32.48 30.82 DeadCraft 1.01 1.00 1.00 1.04 1.01 1.01 1.07 1.02 1.03 1.00 1.02 1.01 1.02 1.00 1.01 1.00 1.00 1.01 1.00 1.01 1.03 1.02 1.00 1.00 1.01 1.02 1.04 1.19 1.00 1.02 1.01 Memory bloat\n(times) DeadSpy 6.47 6.40 6.27 5.84 8.53 14.70 8.20 32.26 6.12 - 9.80 11.93 20.65 6.06 6.25 9.49 6.00 6.26 6.55 6.84 45.75 38.66 - 17.54 7.48 16.76 6.70 - 6.03 9.87 7.16 DeadCraft 1.03 1.01 1.02 1.00 1.07 1.01 1.30 1.03 1.00 2.06 1.38 1.30 1.62 1.01 1.07 1.11 1.00 1.01 1.14 1.04 1.02 2.56 1.09 1.05 1.15 1.64 1.01 5.36 1.02 1.23 1.05\nSilent store\nSlowdown (times) RedSpy 16.33 17.62 23.75 45.64 26.17 23.60 33.00 200.07 41.24 - 25.60 101.66 26.66 14.53 39.43 23.67 10.76 10.94 16.94 27.97 59.71 67.50 - 23.60 16.46 29.32 33.00 - 27.66 29.10 26.42 SilentCraft 1.01 1.01 1.00 1.05 1.01 1.00 1.05 1.03 1.02 1.00 1.02 1.02 1.03 1.00 1.01 1.03 1.00 1.00 1.02 1.01 1.04 1.03 1.00 1.02 1.00 1.01 1.04 1.13 1.00 1.02 1.01 Memory bloat\n(times)\nRedSpy 5.35 5.42 5.26 5.06 8.24 12.50 9.80 34.84 5.22 - 10.08 9.95 15.57 5.15 5.46 6.35 5.08 5.31 5.97 6.42 67.76 51.87 - 3.73 6.53 17.33 5.90 - 5.20 8.58 6.16 SilentCraft 1.03 1.01 1.02 1.00 1.07 1.01 1.31 1.03 1.01 2.05 1.55 1.33 1.64 1.01 1.07 1.11 1.00 1.01 1.14 1.04 1.02 2.78 1.09 1.04 1.15 1.64 1.01 5.17 1.01 1.24 1.04\nSilent load\nSlowdown (times) LoadSpy 30.00 87.70 53.00 123.00 75.30 81.30 100.00 51.80 69.60 - 39.80 185.00 95.30 15.10 98.60 36.80 26.90 26.90 46.10 36.00 82.00 156.00 - 31.20 60.10 54.10 81.90 - 51.00 58.66 57.10 LoadCraft 1.04 1.00 2.16 1.69 1.09 1.00 1.12 1.04 1.08 1.00 1.06 1.04 1.04 1.00 1.09 1.01 1.00 1.02 1.58 1.00 1.51 1.05 1.00 1.00 1.01 1.05 1.10 1.86 1.08 1.13 1.04 Memory bloat\n(times) LoadSpy 6.80 6.50 4.00 5.90 8.49 14.40 12.00 50.10 6.30 - 12.60 18.40 23.90 6.20 4.90 50.20 6.09 6.40 6.80 8.20 184.001051.00 - 13.20 9.30 36.70 5.20 - 6.30 13.52 8.35 LoadCraft 1.03 1.01 1.03 1.00 1.08 1.01 1.36 1.03 1.01 2.01 1.43 1.36 1.64 1.01 1.07 1.11 1.00 1.01 1.13 1.04 1.02 3.55 1.14 1.05 1.19 1.87 1.02 24.93 1.02 1.33 1.05\nTable 1. Runtime slowdown (×) and memory bloat (×) over native execution: Witch (DeadCraft, SilentCraft, LoadCraft) vs. exhaustive monitoring tools (DeadSpy, RedSpy, LoadSpy).\nGeoMean DeadCraft SilentCraft LoadCraft\nTime overhead Space Overhead Time overhead Space Overhead Time overhead Space Overhead\n100M 1.01 1.11 1.01 1.11 1.07 1.14 10M 1.01 1.17 1.01 1.17 1.16 1.27\n5M 1.02 1.21 1.02 1.22 1.21 1.35 1M 1.05 1.40 1.05 1.39 1.43 1.61 500K 1.08 1.50 1.08 1.50 1.74 1.74"
  }, {
    "heading": "1M 1.05/1.03 1.40/1.05 1.06/1.03 1.39/1.04 1.48/1.27 1.66/1.07",
    "text": "top_diff are zeroes; hence the same values overwrite the existing values in the samememory location of bottom_diff. The iteration over all the elements of bottom_diff in the four-level nested loop amplifies the fraction of silent stores. We optimize this code by introducing a check for the value in top_diff. If it is a zero, we bypass a division, an addition, and a memory store. This optimization speeds up the pooling layer by 1.16× the normalization layers by 1.34×. We observe\n1 for (int n = 0; n < top[0]->num(); ++n) { 2 for (int c = 0; c < channels_; ++c) { 3 for (int ph = 0; ph < pooled_height_; ++ph) { 4 for (int pw = 0; pw < pooled_width_; ++pw) { 5 ... 6 for (int h = hstart; h < hend; ++h) { 7 for (int w = wstart; w < wend; ++w) { 8 ▶ bottom_diff[h * width_ + w] += 9 top_diff[ph * pooled_width_ + pw] /\npool_size;"
  }, {
    "heading": "10 }}}}",
    "text": ""
  }, {
    "heading": "11 ...}}",
    "text": "Listing 4. Silent stores to array bottim_diff in Caffe.\n1.03× speedup for the entire program. We further relax the check for the absolute value in top_diff with a small delta 1e-7 rather than 0. If it is smaller, we bypass the computation for approximate results. This optimization, with less than\n1 bfd_boolean lookup_address_in_function_table (struct comp_unit *unit , bfd_vma addr , ...) {\n2 ... 3 for (each_func = unit ->function_table; ...) { 4 for (arange = &each_func ->arange; ...) { 5 ▶ if (addr >= arange ->low && addr < arange ->high){ 6 if (! best_fit || ... ) { 7 best_fit = each_func; 8 best_fit_len = arange ->high - arange ->low; 9 }}}} 10 . . . 11 }\nListing 5. Redundant loads in binutils-2.27 dwarf2.c file. Linear searches load same the values from same locations.\n2% accuracy loss, yields 1.16× and 2.23× speedups for the pooling and normalization layers, respectively. The entire program obtains a 1.06× speedup."
  }, {
    "heading": "8.3 GNU Binutils-2.27",
    "text": "GNU Binutils [20] is a collection of binary tools used by many binary analysis tools such as Pin [49] and commandline tools such as objdump [19]. Disassembling an object file containing many functions using objdump with -d -S -l flags (map assembly to symbol and source lines) is unusually slow. We profile objdump in binutils-2.27 with LoadCraft by disassembling the LULESH-2.0 [33] binary, which contains many functions. LoadCraft identifies 96% of the loads in the program as loading the same value from the same location. The top contributor is the Line 5 (Listing 5) in the function lookup_address_in_function_tablewith 70% redundant loads attributed to it. The function performs a linear scan over the addresses covered by each line of each function, maintained as a linked list, looking for the best match for a given address range.\nWhen repeatedly called for different addresses in an object file containing many functions linear search is a poor choice of algorithm. We replace the linked list with a sorted array and perform a binary search over it. This solution speeds up the execution by 10×. This problem is fixed in the latest binutils. Pinpointing that the code always loads the same values from the same location raised a red flag, clearly indicating an algorithmic deficiency."
  }, {
    "heading": "8.4 SPEC OMP2012 367.imagick",
    "text": "SPEC OMP2012 367.imagick [74] is an OpenMP software to manipulate bitmap images. With the ref input and eight threads, LoadCraft reports that more than 99% of total memory loads are redundant and 85% of the redundant loads are associated with the loop nests shown in Listing 6.\nThe loop body has six memory loads for different fields of pixel and kernel_pixels. Each of the loads is often redundant with a load in a prior iteration. We find that the fields red, green, and blue of kernel_pixels[u] are mostly zeros. For optimization, we introduce a conditional check on kernel_pixels[u]. If it is zero, we skip the computation, which saves a memory load from address k, a multiplication, and a memory load to the field of pixel. This optimization yields a 1.6× speedup."
  }, {
    "heading": "1 for (y=0; y < (ssize_t) image ->rows; y++) {",
    "text": ""
  }, {
    "heading": "2 for (x=0; x < (ssize_t) image ->columns; x++) {",
    "text": ""
  }, {
    "heading": "3 for (v=0; v < (ssize_t) width; v++) {",
    "text": ""
  }, {
    "heading": "4 for (u=0; u < (ssize_t) width; u++) {",
    "text": ""
  }, {
    "heading": "5 ▶ pixel.red +=(*k)*kernel_pixels[u].red;",
    "text": ""
  }, {
    "heading": "6 ▶ pixel.green +=(*k)*kernel_pixels[u].green;",
    "text": ""
  }, {
    "heading": "7 ▶ pixel.blue +=(*k)*kernel_pixels[u].blue;",
    "text": ""
  }, {
    "heading": "8 k++;",
    "text": ""
  }, {
    "heading": "9 }}}}",
    "text": "Listing 6. Redundant loads to different fields of structure pixel and array kernel_pixels in 367.imagick"
  }, {
    "heading": "8.5 Discussion on Other Optimizations",
    "text": "Many algorithmic deficiencies show up as useless loads and stores. While hotspots may indicate where a large fraction of time is spent, they do not indicate the usefulness of the work. Such defects stand out when profiled with our tools.\nWe presented a subset of programs where we found inefficiencies using witchcraft. Kallisto-0.43 [52] is an important RNA-sequencing software where LoadCraft found more than 98% redundant loads. The problem was a large, linear-probing hash-table with excessive hash collisions. We fixed Kallisto by reducing the load factor on the hash table and gained 4.1× speedup. Vacation is a STAMP [56] transactional memory benchmark, where we found unnecessary calls to a hash-table lookup of an item that was already found in the previous line of the code. Memoizing the result of the previous lookup resulted in 1.3× speedup. The results from our tools showed us that SPEC CPU2006 lbm is an excellent candidate for approximate computing; we applied loop perforation [70] to lbm and obtained 1.25x speedup with insignificant (7.7e-5%) accuracy loss."
  }, {
    "heading": "9 Conclusions",
    "text": "Fine-grained execution monitoring via binary instrumentation introduces heavy slowdown and memory bloat. Witch, open sourced at [7], is a lightweight monitoring framework, which employs PMU sampling in conjunction with hardware debug registers to monitor sampled memory addresses. We overcome the problem of limited hardware debug registers with a novel sampling technique.Witch-based tools show high measurement accuracy. Low overhead combined with rich contextual attribution makes our tools attractive to developers in pinpointing inefficiencies in large, complex, production software. We demonstrate the effectiveness of our tools by identifying inefficiencies in several complex, parallel software projects that were the subject of optimization for decades, and we tune them with the guidance from Witch to gain significant speedups."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers and our shepherd Shan Lu for the detailed comments on the paper. We appreciate the Ph.D. student Probir Roy from College of William and Mary for helping study Caffe 1.0. We also thank Dr. Aarthi Muthusamy for sketching the titleWitch icon."
  }],
  "year": 2018,
  "references": [{
    "title": "2010",
    "authors": ["L. Adhianto", "S. Banerjee", "M. Fagan", "M. Krentel", "G. Marin", "J. Mellor- Crummey", "N.R. Tallent"],
    "venue": "HPCToolkit: Tools for Performance Analysis of Optimized Parallel Programs. Concurrency Computation : Practice Expererience 22, 6 ",
    "year": 2010
  }, {
    "title": "Exploiting Hardware Performance Counters with Flow and Context Sensitive Profiling",
    "authors": ["Glenn Ammons", "Thomas Ball", "James R. Larus"],
    "venue": "In SIGPLAN Conference on Programming Language Design and Implementation. ACM, NY, NY,",
    "year": 1997
  }, {
    "title": "Interprocedural Load Elimination for Dynamic Optimization of Parallel Programs",
    "authors": ["R. Barik", "V. Sarkar"],
    "venue": "In 2009 18th International Conference on Parallel Architectures and Compilation Techniques",
    "year": 2009
  }, {
    "title": "Dynamic Dead-instruction Detection and Elimination",
    "authors": ["J. Adam Butts", "Guri Sohi"],
    "venue": "In Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems",
    "year": 2002
  }, {
    "title": "Barrier Elision for Production Parallel Programs",
    "authors": ["Milind Chabbi", "Wim Lavrijsen", "Wibe de Jong", "Koushik Sen", "John Mellor-Crummey", "Costin Iancu"],
    "venue": "In Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2015). ACM,",
    "year": 2015
  }, {
    "title": "Call Paths for Pin Tools",
    "authors": ["Milind Chabbi", "Xu Liu", "John Mellor-Crummey"],
    "venue": "In Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization. Article",
    "year": 2014
  }, {
    "title": "DeadSpy: a tool to pinpoint program inefficiencies",
    "authors": ["Milind Chabbi", "John Mellor-Crummey"],
    "venue": "In Proceedings of the 10th International Symposium on Code Generation and Optimization",
    "year": 2012
  }, {
    "title": "Featherlight On-thefly False Sharing Detection",
    "authors": ["Milind Chabbi", "Shasha Wen", "Xu Liu"],
    "venue": "In Proceedings of the 23th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2018)",
    "year": 2018
  }, {
    "title": "Taming Hardware Event Samples for FDO Compilation",
    "authors": ["Dehao Chen", "Neil Vachharajani", "Robert Hundt", "Shih-wei Liao", "Vinodha Ramasamy", "Paul Yuan", "Wenguang Chen", "Weimin Zheng"],
    "venue": "In Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO ’10)",
    "year": 2010
  }, {
    "title": "2013",
    "authors": ["P Colella", "DT Graves", "ND Keen", "TJ Ligocki", "DF Martin", "PW Mc-Corquodale", "D Modiano", "PO Schwartz", "TD Sternberg", "B Van Straalen"],
    "venue": "Chombo Software Package for AMR Applications - Design Document. Lawrence Berkeley National Laboratory Technical Report LBNL-6616E ",
    "year": 2013
  }, {
    "title": "Eliminating redundancies in sum-of-product array computations",
    "authors": ["Steven J Deitz", "Bradford L Chamberlain", "Lawrence Snyder"],
    "venue": "In Proceedings of the 15th International Conference on Supercomputing",
    "year": 2001
  }, {
    "title": "Cray Performance Analysis Tools. In Tools for High Performance Computing",
    "authors": ["Luiz DeRose", "Bill Homer", "Dean Johnson", "Steve Kaufmann", "Heidi Poxon"],
    "year": 2008
  }, {
    "title": "Toward a new metric for ranking high performance computing systems",
    "authors": ["Jack Dongarra", "Michael A Heroux"],
    "venue": "Sandia Report,",
    "year": 2013
  }, {
    "title": "Instruction-Based Sampling: A New Performance Analysis Technique for AMD Family 10h Processors. https://pdfs.semanticscholar.org/5219/  4b43b8385ce39b2b08ecd409c753e0efafe5.pdf",
    "authors": ["Paul J. Drongowski"],
    "year": 2007
  }, {
    "title": "Potential of a modern vector supercomputer for practical applications: performance evaluation of SX-ACE",
    "authors": ["Ryusuke Egawa", "Kazuhiko Komatsu", "Shintaro Momose", "Yoko Isobe", "Akihiro Musa", "Hiroyuki Takizawa", "Hiroaki Kobayashi"],
    "venue": "The Journal of Supercomputing",
    "year": 2017
  }, {
    "title": "Effective Data-race Detection for the Kernel",
    "authors": ["John Erickson", "Madanlal Musuvathi", "Sebastian Burckhardt", "Kirk Olynyk"],
    "venue": "In Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation (OSDI’10)",
    "year": 2010
  }, {
    "title": "Gprof: A call graph execution profiler",
    "authors": ["Susan L. Graham", "Peter B. Kessler", "Marshall K. McKusick"],
    "venue": "In Proceedings of the 1982 SIG- PLAN Symposium on Compiler Construction. ACM Press,",
    "year": 1982
  }, {
    "title": "Assessing cache false sharing effects by dynamic binary instrumentation",
    "authors": ["Stephan M. Günther", "Josef Weidendorfer"],
    "venue": "In WBIA ’09: Proceedings of theWorkshop on Binary Instrumentation and Applications. ACM,",
    "year": 2009
  }, {
    "title": "Call Path Profiling",
    "authors": ["Robert J. Hall"],
    "venue": "In Proceedings of the 14th International Conference on Software Engineering (ICSE ’92)",
    "year": 1992
  }, {
    "title": "Bursty tracing: A framework for low overhead temporal profiling",
    "authors": ["M. Hirzel", "T. Chilimbi"],
    "venue": "In ACM Workshop on Feedback-Directed and Dynamic Optimization",
    "year": 2001
  }, {
    "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
    "authors": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"],
    "year": 2014
  }, {
    "title": "DRDDR: A Lightweight Method to Detect Data Races in Linux Kernel",
    "authors": ["Yunyun Jiang", "Yi Yang", "Tian Xiao", "Tianwei Sheng", "Wenguang Chen"],
    "venue": "The Journal of Supercomputing 72,",
    "year": 2016
  }, {
    "title": "Understanding and Detecting Real-world Performance Bugs",
    "authors": ["Guoliang Jin", "Linhai Song", "Xiaoming Shi", "Joel Scherpelz", "Shan Lu"],
    "venue": "In Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’12)",
    "year": 2012
  }, {
    "title": "Some Requirements for Architectural Support of Software Debugging",
    "authors": ["Mark Scott Johnson"],
    "venue": "In Proceedings of the First International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS I). ACM,",
    "year": 1982
  }, {
    "title": "CatchMe if You Can: Performance Bug Detection in the Wild",
    "authors": ["Milan Jovic", "Andrea Adamoli", "andMatthias Hauswirth"],
    "venue": "In Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications (OOPSLA ’11)",
    "year": 2011
  }, {
    "title": "Exploring Traditional and Emerging Parallel Programming  Session 4A: Memory 1 ASPLOS’18",
    "authors": ["Ian Karlin", "Abhinav Bhatele", "Jeff Keasler", "Bradford L. Chamberlain", "Jonathan Cohen", "Zachary DeVito", "Riyaz Haque", "Dan Laney", "Edward Luke", "Felix Wang", "David Richards", "Martin Schulz", "Charles Still"],
    "venue": "March 24–28,",
    "year": 2013
  }, {
    "title": "Efficient Tracing of Cold Code via Bias-free Sampling",
    "authors": ["Baris Kasikci", "Thomas Ball", "George Candea", "John Erickson", "Madanlal Musuvathi"],
    "venue": "In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC’14)",
    "year": 2014
  }, {
    "title": "Finding Latent Performance Bugs in Systems Implementations",
    "authors": ["Charles Killian", "Karthik Nagaraj", "Salman Pervez", "Ryan Braud", "James W. Anderson", "Ranjit Jhala"],
    "venue": "In Proceedings of the Eighteenth ACM SIG- SOFT International Symposium on Foundations of Software Engineering (FSE ’10)",
    "year": 2010
  }, {
    "title": "On the Value Locality of Store Instructions",
    "authors": ["K.M. Lepak", "M.H. Lipasti"],
    "venue": "In Proceedings of 27th International Symposium on Computer Architecture (IEEE Cat",
    "year": 2000
  }, {
    "title": "Silent Stores for Free",
    "authors": ["Kevin M. Lepak", "Mikko H. Lipasti"],
    "venue": "In Proceedings of the 33rd Annual ACM/IEEE International Symposium on Microarchitecture (MICRO 33)",
    "year": 2000
  }, {
    "title": "Performance Analysis Guide for Intel Core i7 Processor and Intel Xeon 5500 processors, Version 1.0. https://software.intel.com/sites/products/collateral/hpc/vtune/ performance_analysis_guide.pdf",
    "authors": ["Levinthal", "David"],
    "year": 2009
  }, {
    "title": "Exceeding the Dataflow Limit via Value Prediction",
    "authors": ["Mikko H. Lipasti", "John Paul Shen"],
    "venue": "In Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture (MICRO 29). IEEE Computer Society,",
    "year": 1996
  }, {
    "title": "Value Locality and Load Value Prediction",
    "authors": ["Mikko H. Lipasti", "Christopher B. Wilkerson", "John Paul Shen"],
    "venue": "In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII). ACM,",
    "year": 1996
  }, {
    "title": "False Sharing Analysis for Multithreaded Programs",
    "authors": ["Chien-Lung Liu"],
    "year": 2009
  }, {
    "title": "DoubleTake: Fast and Precise Error Detection via Evidence-based Dynamic Analysis",
    "authors": ["Tongping Liu", "Charlie Curtsinger", "Emery D. Berger"],
    "venue": "In Proceedings of the 38th International Conference on Software Engineering (ICSE ’16)",
    "year": 2016
  }, {
    "title": "Cheetah: Detecting False Sharing Efficiently and Effectively",
    "authors": ["Tongping Liu", "Xu Liu"],
    "venue": "In Proceedings of the 2016 International Symposium on Code Generation and Optimization (CGO ’16)",
    "year": 2016
  }, {
    "title": "Predator: Predictive False Sharing Detection",
    "authors": ["Tongping Liu", "Chen Tian", "Hu Ziang", "Emery D. Berger"],
    "venue": "In Proceedings of 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP’14). ACM,",
    "year": 2014
  }, {
    "title": "Pinpointing data locality bottlenecks with low overhead",
    "authors": ["X. Liu", "J. Mellor-Crummey"],
    "venue": "IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)",
    "year": 2013
  }, {
    "title": "Pin: Building Customized ProgramAnalysis Tools withDynamic Instrumentation",
    "authors": ["Chi-Keung Luk", "Robert Cohn", "Robert Muth", "Harish Patil", "Artur Klauser", "Geoff Lowney", "Steven Wallace", "Vijay Janapa Reddi", "Kim Hazelwood"],
    "venue": "In Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI  ’05)",
    "year": 2005
  }, {
    "title": "Pinpointing and Exploiting Opportunities for Enhancing Data Reuse",
    "authors": ["Gabriel Marin", "John Mellor-Crummey"],
    "venue": "In IEEE Intl. Symposium on Performance Analysis of Systems and Software (ISPASS ’08)",
    "year": 2008
  }, {
    "title": "Guidelines for Creating a Debuggable Processor",
    "authors": ["R.E. McLear", "D.M. Scheibelhut", "E. Tammaru"],
    "venue": "In Proceedings of the First International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS I). ACM,",
    "year": 1982
  }, {
    "title": "Near-optimal RNA-Seq quantification. https://github.com/makaho/kallisto",
    "authors": ["Pall Melsted", "Harold Pimentel", "Lior Pachter"],
    "year": 2014
  }, {
    "title": "Doppelganger: A Cache for Approximate Computing",
    "authors": ["Joshua SanMiguel", "Jorge Albericio", "AndreasMoshovos", "Natalie Enright Jerger"],
    "venue": "In Proceedings of the 48th International Symposium on Microarchitecture (MICRO-48). ACM,",
    "year": 2015
  }, {
    "title": "Load Value Approximation",
    "authors": ["Joshua San Miguel", "Mario Badr", "Natalie Enright Jerger"],
    "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47). IEEE Computer Society,",
    "year": 2014
  }, {
    "title": "Quantitative Comparison of Hardware Transactional Memory for Blue Gene/Q, zEnterprise EC12, Intel Core, and POWER8",
    "authors": ["Takuya Nakaike", "Rei Odaira", "Matthew Gaudet", "Maged M. Michael", "Hisanobu Tomari"],
    "venue": "In Proceedings of the 42Nd Annual International Symposium on Computer Architecture (ISCA ’15)",
    "year": 2015
  }, {
    "title": "Cachetor: Detecting Cacheable Data to Remove Bloat",
    "authors": ["Khanh Nguyen", "Guoqing Xu"],
    "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE",
    "year": 2013
  }, {
    "title": "Caramel: Detecting and Fixing Performance Problems that Have Nonintrusive Fixes",
    "authors": ["Adrian Nistor", "Po-Chun Chang", "Cosmin Radoi", "Shan Lu"],
    "venue": "In Proceedings of the 37th International Conference on Software Engineering - Volume",
    "year": 2015
  }, {
    "title": "Toddler: Detecting performance problems via similar memory-access patterns",
    "authors": ["A. Nistor", "L. Song", "D. Marinov", "S. Lu"],
    "venue": "In 2013 35th International Conference on Software Engineering (ICSE)",
    "year": 2013
  }, {
    "title": "Establishing a Base of Trust with Performance Counters for Enterprise Workloads",
    "authors": ["Andrzej Nowak", "Ahmad Yasin", "Avi Mendelson", "Willy Zwaenepoel"],
    "venue": "In Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC ’15)",
    "year": 2015
  }, {
    "title": "Static Detection of Asymptotic Performance Bugs in Collection Traversals",
    "authors": ["Oswaldo Olivo", "Isil Dillig", "Calvin Lin"],
    "venue": "In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’15)",
    "year": 2015
  }, {
    "title": "Locating Cache Performance Bottlenecks Using Data Profiling",
    "authors": ["Aleksey Pesterev", "Nickolai Zeldovich", "Robert T. Morris"],
    "venue": "In Proceedings of the 5th European Conference on Computer Systems (EuroSys ’10)",
    "year": 2010
  }, {
    "title": "Global Value Numbers and Redundant Computations",
    "authors": ["B.K. Rosen", "M.N. Wegman", "F.K. Zadeck"],
    "venue": "In Proceedings of the 15th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages",
    "year": 1988
  }, {
    "title": "Architecture-Level Power Optimization—What Are the Limits",
    "authors": ["John S. Seng", "Dean M. Tullsen"],
    "venue": "J. Instruction-Level Parallelism",
    "year": 2005
  }, {
    "title": "Managing Performance vs. Accuracy Tradeoffs with Loop Perforation",
    "authors": ["Stelios Sidiroglou-Douskos", "Sasa Misailovic", "Henry Hoffmann", "Martin Rinard"],
    "venue": "In Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering (ESEC/FSE ’11)",
    "year": 2011
  }, {
    "title": "Software Performance Antipatterns",
    "authors": ["Connie U. Smith", "Lloyd G. Williams"],
    "venue": "In Proceedings of the 2Nd International Workshop on Software and Performance (WOSP ’00)",
    "year": 2000
  }, {
    "title": "Performance Diagnosis for Inefficient Loops",
    "authors": ["L. Song", "S. Lu"],
    "venue": "39th International Conference on Software Engineering (ICSE)",
    "year": 2017
  }, {
    "title": "2007",
    "authors": ["SPEC Corporation"],
    "venue": "SPEC CPU2006 Benchmark Suite. http: //www.spec.org/cpu2006. ",
    "year": 2007
  }, {
    "title": "2015",
    "authors": ["SPEC Corporation"],
    "venue": "SPEC OMP2012 Benchmark Suite. https: //www.spec.org/omp2012/. ",
    "year": 2015
  }, {
    "title": "J",
    "authors": ["M. Srinivas", "B. Sinharoy", "R.J. Eickemeyer", "R. Raghavan", "S. Kunkel", "T. Chen", "W. Maron", "D. Flemming", "A. Blanchard", "P. Seshadri"],
    "venue": "W. Kellington, A.Mericas, A. E. Petruski, V. R. Indukuru, and S. Reyes. 2011. IBM POWER7 performance modeling, verification, and evaluation. IBM JRD 55, 3 ",
    "year": 2011
  }, {
    "title": "EntomoModel: Understanding and Avoiding Performance Anomaly Manifestations",
    "authors": ["C. Stewart", "K. Shen", "A. Iyengar", "J. Yin"],
    "venue": "IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems",
    "year": 2010
  }, {
    "title": "Binary Analysis for Measurement and Attribution of Program Performance",
    "authors": ["Nathan R. Tallent", "John Mellor-Crummey", "Michael W. Fagan"],
    "venue": "In Proceedings of the 2009 ACM PLDI. ACM, NY, NY,",
    "year": 2009
  }, {
    "title": "Analyzing Lock Contention in Multithreaded Applications",
    "authors": ["Nathan R. Tallent", "John M. Mellor-Crummey", "Allan Porterfield"],
    "venue": "In Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP ’10)",
    "year": 2010
  }, {
    "title": "E",
    "authors": ["M. Valiev", "E.J. Bylaska", "N. Govind", "K. Kowalski", "T.P. Straatsma", "H.J.J. Van Dam", "D. Wang", "J. Nieplocha"],
    "venue": "Apra, T.L. Windus, and W.A. de Jong. 2010. NWChem: A comprehensive and scalable opensource solution for large scale molecular simulations. Computer Physics Communications 181, 9 ",
    "year": 2010
  }, {
    "title": "Random Sampling with a Reservoir",
    "authors": ["Jeffrey S. Vitter"],
    "venue": "ACM Trans. Math. Softw. 11,",
    "year": 1985
  }, {
    "title": "Constant Propagation with Conditional Branches",
    "authors": ["Mark N. Wegman", "F. Kenneth Zadeck"],
    "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS) 13,",
    "year": 1991
  }, {
    "title": "RedSpy: Exploring Value Locality in Software. In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS ’17)",
    "authors": ["ShashaWen", "Milind Chabbi", "Xu Liu"],
    "year": 2017
  }, {
    "title": "Runtime Value Numbering: A Profiling Technique to Pinpoint Redundant Computations. In Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT) (PACT ’15)",
    "authors": ["Shasha Wen", "Xu Liu", "Milind Chabbi"],
    "venue": "IEEE Computer Society,",
    "year": 2015
  }, {
    "title": "Finding Low-utility Data Structures",
    "authors": ["Guoqing Xu", "Nick Mitchell", "Matthew Arnold", "Atanas Rountev", "Edith Schonberg", "Gary Sevitsky"],
    "venue": "In Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’10)",
    "year": 2010
  }, {
    "title": "Understanding Database Performance Inefficiencies in Real-world Web Applications",
    "authors": ["Cong Yan", "Alvin Cheung", "Junwen Yang", "Shan Lu"],
    "venue": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM ’17)",
    "year": 2017
  }, {
    "title": "RFVP: Rollbackfree Value Prediction with Safe-to-approximate Loads",
    "authors": ["Amir Yazdanbakhsh", "Gennady Pekhimenko", "Bradley Thwaites", "Hadi Esmaeilzadeh", "Onur Mutlu", "Todd C Mowry"],
    "venue": "ACM Transactions on Architecture and Code Optimization (TACO) 12,",
    "year": 2016
  }],
  "id": "SP:30868ee94d410fef27d3d00e423a330480eea4e3",
  "authors": [{
    "name": "Shasha Wen",
    "affiliations": []
  }, {
    "name": "Xu Liu",
    "affiliations": []
  }, {
    "name": "John Byrne",
    "affiliations": []
  }, {
    "name": "Milind Chabbi",
    "affiliations": []
  }],
  "abstractText": "Inefficiencies abound in complex, layered software. A variety of inefficiencies show up as wasteful memory operations. Many existing tools instrument every load and store instruction to monitor memory, which significantly slows execution and consumes enormously extra memory. Our lightweight framework, Witch, samples consecutive accesses to the same memory location by exploiting two ubiquitous hardware features: the performance monitoring units (PMU) and debug registers. Witch performs no instrumentation. Hence, witchcraft—tools built atopWitch—can detect a variety of software inefficiencies while introducing negligible slowdown and insignificant memory consumption and yet maintaining accuracy comparable to exhaustive instrumentation tools.Witch allowed us to scale our analysis to a large number of code bases. Guided by witchcraft, we detected several performance problems in important code bases; eliminating these inefficiencies resulted in significant speedups. CCSConcepts •General and reference→Performance; • Software and its engineering → Runtime environments; Application specific development environments;",
  "title": "Watching for Software Inefficiencies withWitch"
}