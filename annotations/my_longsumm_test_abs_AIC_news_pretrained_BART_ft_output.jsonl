{
    "0": "BEAT: A set of practical asynchronous BFT protocols for wide-area (WAN) blockchains Gog et al., CCS\u201918  Asynchronous blockchains (BFTs) have recently gained in popularity and protection from DDoS attacks and state machine replication (SMR). This paper introduces BEAT, a set of five protocols that are designed to meet different goals (e.g., different application scenarios). The five protocols are:  BEAT  An implementation of BEAT on Amazon EC2 (92 instances)  An evaluation of HoneyBadgerBFT (70), the most recent BFT protocol to be evaluated  A reference to the full BEAT set here:  ( Enlarge )  In part, this is due to the fact that the use of expensive threshold cryptography (specifically threshold encryptions and threshold signatures) has been hard to achieve (as has been argued in various works, e.g. , see our previous post on the topic here  ), and partially synchronous BFT  A combination of these two approaches allows for building permissioned blockchains where the distributed ledgers know each other identities but may not trust one another. Asynchronous BFT ensures liveness of the protocol without depending on any timing assumptions, which is prudent when the network is controlled by an adversary. (See our previous piece on PBFT here )  There are still a number of challenges and opportunities associated with implementing BFT, as well as new opportunities arising with the rise of blockchains. The Hyperledger umbrella has become a global open-source project under the Linux Foundation, now with more than 250 members. Meanwhile, there are also new opportunities for asynchronous B FT protocols, as a result of the Internet\u2019s role as a communications medium, and the proliferation of gateways (IPs) to the Internet . It was shown in  [70] that a fully synchronous PBFT protocol would achieve zero throughput against an adversarial asynchronous scheduler, and that partially synchronized PBFT would be unable to cope with a DDoS attack mounted over the Internet. Enter BEAT! BEAT is comprised of five different bFT protocols, all of which can be implemented on top of each other. There are application scenarios in which each of the five protocols can be used, and different trade-offs can be achieved between application-level functionality and performance. For example, in the EC2 case, there is a large number of jobs that require multiple replica servers, and each replica server has limited computation power. The following table shows the performance of different BEAT instances (highlighted in blue) over a one-week period (July-September 2018):  In the table below, we can see that in this case, the baseline ( PBFT) achieved around 80-90% throughput, and in the second category (BEAT), the performance was significantly worse (between 60-80%). The table below shows the trade-off (latency) and throughput (throughput) of two competing BFTs:  In this example, BEAT (left) and BEAT-All (right), both of which use threshold-encoders and use different combinations of these to achieve lower latency and lower throughput. Note that BEAT uses both threshold-and-signals, whereas PBFT uses only one (per application) and uses both (per server) the second (per user) to guarantee liveness. Here we have a situation in which a server is communicating with multiple other servers, but only one of its servers is synchronous (and therefore it can provide liveness to the rest of the servers). In this situation, and given the latency and throughput",
    "1": "This paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a result of closed-loop policies that react to the actions of other agents. Each policy is designed to capture a different high-level behavior, such as following a lane, changing lanes, or turning at an intersection. At each timestep of the model\u2019s run, it estimates the distribution over the policies that other traffic agents\u2019 actions may be, using information from the observed history of states of nearby cars. It also uses changepoint detection to estimate which policy a given vehicle was executing at each point in its history of actions, and then infer the likelihood of each potential intention of the vehicle. At the last step, it uses the policy that maximizes the expected reward given the sampled outcomes of the behavioral prediction system, and executes the policy with the maximum expected reward value. \u201cDecision-making for autonomous cars is hard due to uncertainty on the continuous state of nearby vehicles and, in particular, due to their discrete potential intentions\u2026\u201d  Previous approaches have employed hand-tuned heuristics and numerical optimization to account for the future actions of interacting traffic agents, but these methods do not scale up to real-world scenarios. In contrast, this paper presents a fully integrated behavioral anticipation and decision making approach that combines the benefits of partially observable Markov decision process (POMDP) solvers to partially incorporate the effects of other traffic agent actions with the uncertainty of the interaction between vehicles. The key to this work is the use of behavioral anticipation to estimate the probability of actions taken by each traffic agent, in addition to the known policy options available for our vehicle. Approach  The proposed in this paper is a combination of two approaches:  The first approach uses a changepoint-based behavioral prediction approach that equates the trajectory of actions of each traffic vehicle with the changepoint of its neighbors. This approach is made tractable by considering only a finite set of a priori known policies. The second approach, based on behavioral anticipation, considers the potential actions that each vehicle could take given the policy available to it and the policy options it has available for it. The choice of policies is made by taking the policy as a whole and combining it with the policy for the current vehicle, the policy of the other vehicles, and any other policies that our vehicle may have available, to produce the final policy for our car. The system proceeds in two stages:  (1) Estimate the distribution of actions that other vehicles may be taking. (2) Sample policies from the distribution to obtain high-likelihood actions for each participating vehicle. (3) Find out which policies are most likely to be followed by our vehicle, and (4) execute the policy based on those policies. During the sampling phase, we use a Bayesian changepoint detector (based on the policy distribution of other vehicles\u2019 trajectories) to determine which of the policy trajectories of our vehicle was being executed at each time step t and t-1. This allows us to infer which policy was being followed, and to use this information to compute the changepoints of other trajectories. The changepoints are then used to compute a statistical test to detect anomalous actions (changing lanes, driving in the wrong direction, etc). The authors use the results of this phase to evaluate the performance of the proposed system online, using traffic data from our autonomous vehicle platform. Results  The key takeaway is that given a set of trajectories and trajectories, it is possible to accurately predict which policies our vehicle will follow given an understanding of the trajectory. This insight forms the basis for the proposed behavioral model",
    "2": "The paper presents a new dataset of 20K commonsense narratives and 200K explanations based on the hypothesis that a thief caused the mess at home (see Figure 1 for an example). In addition, a new conditional generation task, Abductive NLI (abductive reasoning in natural language), is introduced which requires the user to choose between one of the two explanations given an observation and a given pair of hypotheses, given a pair of hypothesis and a set of questions (given the question and the question). Questions  Given a question (question), answer the question with the most plausible explanation while the answers to the questions are selected using a binary cross-entropy scoring function which aims to minimise the discrepancy between the answers provided by the hypothesis and the facts stated in the question. Abduction is the only logical operation which introduces any new ideas, which contrasts with other types of inference such as entailment which focuses on inferring only such information that is already provided in the premise. This shift from logic-based to language-based reasoning draws inspirations from a significant body of work on language based entailment (including work by Bowman et al., 2015, and Williams and Wojciechowski, 2018). Approach  Most previous work on abductive reasoning has focused on formal logic, which has proven to be too rigid and hard to generalize to the full complexity of natural language. The main change proposed in this work is the use of language as the representation medium, with the paradigm shift from formal to language based reasoning. Two tasks are considered:  Question Answering - multiple choice task for choosing the most likely explanation  Conditional generation task for explaining observations given a given hypothesis (AbductiveNLI). Conditional Generation - conditional generation of the hypothesis  Dataset  Created a new multi-choice question answering dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, two new tasks are introduced:  AbductIVE NLI - Multiple choice task to choose the \u201cmore likely explanation\u201d given an hypothesis (given a question and pair of question-answering questions)  Abduction NLG - Conditional reasoning task to generate the hypothesis for the conditional generation  Notes  There has been a lot of work in this field recently (see \u201c Language-based Inference in Natural Language\u201d and \u201cLanguage-based Extrapolation\u201d) but little attention has been paid to the application to the natural language domain. This work aims to investigate the viability of language- based abductive inference and reasoning in the context of language and to provide new insights on the reasoning capabilities of deep pre-trained language models. One of the main observations made in the paper is that the current state-of-the-art models on the task of conditional generation NLI are sub-par when it comes to reasoning about natural language and perform poorly on the question answering task. This gap between human performance on NLI and BERT (based on BERT model) is much wider when the model is trained on the more difficult task of choosing the more likely explanation. The authors believe that this gap could be due to the fact that the models are trained on a task where the task is to predict the most \u201cbelievable\u201d (i.e., plausible) hypothesis and not just the average or \u201cexplanatory\u201d hypothesis. They propose two variants of the proposed models:  BERT: Standard BERT based model where the question is a question with a predefined set of question answering questions and the answers are generated by a binary classifier using a soft-attention scoring function  NLI",
    "3": "MIPS (Maximum Inner Product Search) - solving a K-MIPS problem in sublinear time Gog et al., CIFAR\u201918  Yesterday we looked at some of the amazing properties of word vectors with word embeddings , today\u2019s paper introduces a very similar problem, Maximum Cosine Similarity Search (MIPS). The problem of predicting the top-K most likely class labels for a given data point, given the context of the few previous words in a vocabulary, can be cast as solving a \u201cKMIPS\u201d problem. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. Finding the k most likely words is not always easy. In many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of \u201cthe exact KMIPS vectors\u201d The examples in the above examples motivate research on approximate MIPS algorithms. If we can obtain large speedups over a full \u201cloosely supervised\u201d search without sacrificing too much on precision, it will have a direct impact on such large-scale applications. The authors start out by training a softmax model to predict the softmax probabilities for all the words in the vocabulary, and then use that to rank the likelihood of specific words. The softmax is a non-linear function that allows the model to compute a matrix of softmax scores for each hidden layer representation, and from these scores an inner product score is obtained for the vocabulary words. Given a vocabulary word (e.g. \u201chorse\u201d) and a hidden representation  , an embedding dictionary  is constructed, where  is the set of all the possible words and  is a fixed size vector  . Each column  in the dictionary is a point, and  denotes the embedding of the vocabulary word in the hidden layer space  . An inner product  is computed between the column  and the hidden representation, to yield a score  for each word  . In the final layer  , the number of columns is  and  represents the predicted embedding for that word  To mark the points in the vector space, an addition is added, such that  now we have  . Given a set of k words,  ,  , and  , we can compute the following matrix  . The matrix  has as many columns as there are words in  The columns  are represented by  , with  representing the inner product scores  . Let  be the points and  represent the word vectors  . We want to predict which of the k words from the vocabulary is the most likely to be chosen as the correct label for  . To compute this matrix, we need to know which of  the k word vectors have the highest inner product, and which of them has the lowest. The paper introduces an extremely simple approach to this task, based on a k-means clustering algorithm. Specifically, we propose to train a spherical kmeans (i.e., k-MI PS problem) model, and after having reduced the MIPS problem to  , train this model to solve approximate K-PIPS. The k-Means model  The basic idea is to use locality-sensitive hashing (LSH) to reduce the time of solving MIPS. Localized LHS is a very common technique to solve MIPS, and has been used by famous Convolutional Neural Networks such as GoogLeNet  A variant of LHS called as Local",
    "4": "FatClique: Understanding lifecycle management and expanding a datacenter\u2019s network Gog et al., HotCloud\u201918  Previously on The Morning Paper we\u2019ve looked at some of the characteristics of datacentre topologies, and how they can be optimised for varying degree of flexibility (e.g. flexibility to add more capacity, flexibility to reconfigure at different points in the lifecycle). This paper introduces a new dimension, life cycle management complexity (LMC), which attempts to understand the complexity of deploying a topology and expanding it. It attempts to devise metrics that indicate the time-scale and scale of changes needed to support the needs of a constantly increasing set of services over time, and to provide a measure of how much effort and resources need to be spent on managing this lifecycle. The key to understanding is this:  \u2026 the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels, switch-types, and cable trays, are proprietary and change over time. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle Management, but as far as we know, no prior work has addressed this topic with this prior work. In part, this is due to the fact that intuitions about lifcycle management are developed over time and with operations experience, and these lessons are not made available universally. In this paper, the authors identify the following key factors:  The number of switches required to deploy a data center topology  The amount of effort needed to pack a set of different types of switches into homogeneous racks into a single unit (i.e. a \u201cmasterpiece\u201d of switches, switches, transformers, routers, and associated cables). The need to choose among different type of switch types, and the number of different kinds of cables to connect to them  How many switch-related components are needed to package a given set of switches? This can vary from one provider to another, and depends on a number of factors related to the particular case. For instance, in a typical commercial data center there are three types:  Number of switches in the topology determines how \u201chard\u201d it is to pack switches into racks. This number determines the \u201cneed to\u201d to be able to efficiently place switches in racks in a space efficient manner. Number of racks determines the amount of \u201cclearing\u201d required to pack individual switches, for example, a large number of racks is likely to require the use of three or more of them. Wiring complexity is related to both these factors as well, and can be influenced by the choice of switch type, and also by the type of cables used in the racks. Increasing the total number of links (\u201cconnections\u201d) influences the \u2018need to re-wire\u201d factor, as increasing the number means that the overall amount of re-wiring needs to go up. The authors identify factors such as \u201cwiring patterns\u201d and \u201cpatch panels\u201d, and propose a new class of topologies called FatClique, which combines the benefits of both of these factors with the benefits (and the benefits) of reducing the overall number of switch component requirements, making it comparable to, but better than, other topologies. A key insight from the analysis is that Closings and Expanders are both susceptible to the effects of time-cycle management, and that the effects (and benefits) from these effects are especially prominent at the edges, where the edges are the most sensitive to the timing of connection failures and connection degradation.",
    "5": "The in-memory key-value store (KVS) at scale Li et al., SOSP\u201917  Key-value stores, or KVSs, are a key component in many data centers. Performance of the KVS is the key factor in the overall system efficiency. A significant amount of research has been done over the years to improve KVS performance. Today\u2019s paper introduces KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct access to the main host memory. It achieves 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems. This is equivalent to the throughput of tens of CPU cores. It also reduces the average power usage by 3x and reduces the latency of the connection between the NIC and the host memory by up to 10%. The key to this success is the use of two RDMA-capable NICs, one for the CPU and another for the servers. The CPU is the original bottleneck in KVS until recently, but as the network bandwidth to the CPU has grown, this has become a non-trivial factor in performance. The RDMA abstraction has been largely replaced by RDMA since the introduction of RDMA in the mid-2000s, but primitives for accessing the main memory of the CPU have been somewhat limited. This paper exploits two new RDMA technologies:  Remote Direct Memory Access (RDMA) \u2013 a term coined to refer to the RDMA interface  RDMA bypass bypassing the CPU  In-memory RDMA (or RDMA) can be used to access the CPU directly, but is expensive and doesn\u2019t provide full access to main memory. The authors exploit this by using two common RDMA techniques, one-sided RDMA to bypass CPU and remote RDMA access to KVS servers\u2019 main memory, and another-side RDMA on the servers themselves. This enables access to a shared key value hash table among distributed clients and speeds up their accesses to the table. At the heart of the paper is a discussion on the benefits of using RDMA for KVS, and the challenges presented by moving the workload away from the CPU to the network in this manner. RDMA is an important factor in moving workloads from the network to the data center, and as we\u2019ve seen before, its benefits are limited if the network itself is not fast enough. To address this issue, the authors propose to use two common approaches:  The first approach (more familiar to regular RDMA users) is to use the CPU as the link point between the server and the in- memory RDMA server. This approach degrades PUT (per-time access time) for transactional operations due to high communication and PUT overhead. The second approach (achieving near linear scalability with multiple NICs) exploits the second-level of abstraction provided by the programmable networking layer of the RDDMA abstraction and uses it to support in-network processing. With two NICs in a server, a typical KVS can achieve up to 180 M operations a second, beating the previous best of up to 1.2 billion (1.7 billion) with a single CPU core. Note that the figure in the table below is for the table created by a vanilla Memcached implementation. You\u2019ll find out more details of the hardware involved in the implementation in the video below. The full paper can be found on the link-base at  [url]",
    "6": "The paper presents a generalized framework for clustering networks based on higher-order connectivity patterns. This framework provides mathematical guarantees on the optimisation of obtained clusters and scales to networks with billions of edges. Link to the code  Approach  Given a network of nodes  , the goal is to find a cluster of nodes S such that the nodes participate in many instances of M (with respect to M) and that the set S minimizes the incidence of cutting (when only a subset of the nodes from M are in S). The set S should minimise the following ratio:  Joining the nodes in S should result in low incidence of cut-offs. Avoiding cutting instances  In practice, it is difficult to determine which subset of nodes from S should be in S given the landscape. The paper uses the following objective function:  Where  S is the set of all the nodes S and  M is the number of node instances in S that reside in S. To identify clusters, the paper uses an objective function called Equation 1 (equation 1) which asks:  The most common clustering pattern is chosen and the nodes are connected in such a way that their average number of connections is small (i.e., few connections between them are fewest). This is known as clustering by factorization. Networks with large number of nodes are more susceptible to falling into one of the two problems (see Equation 2). The paper shows that information propagation units in neuronal networks and in transport networks exhibit higher order organization. Network motif  The idea of a \u201cnetwork of building blocks\u201d is derived from the fact that small subgraphs are building blocks for understanding the behavior of the brain. Given a small set of nodes and a set of connections (connected to the same node), the task is to cluster the nodes according to the given network motif so that the connections between a node and its neighbors are close to each other and avoid cutting out any nodes that are not part of S. This requires finding a cluster with low incidence (low incidence) of cutting instances, but high incidence (high incidence). The key to this objective function is the following equation:  Let H = (H - H), where H is the fraction of nodes that belong in S and H denotes how many nodes should be connected to S. The answer is given in Equation 3 and Equation 4  Given H, compute H(H) where H and H are the nodes of S and S are the edges of H.  H is a factorized graph and H is an element-wise product of H and S. Given H (here, H) and S (here in S), compute the following factorizations:  Equation 5  For each node in S, compute the average H, H, and S-factorized by the factorization factorization  H = H (H, S)  H - H, S, and H-factor  H-style factorizations are those nodes that have high H-frequency interactions with other nodes and the type of H that they belong to (here H-type interactions are those with H- or H-like interactions). For S-style interactions, compute a factorization of H with respect to H such that H = M(H), M(M), and S(S)  The final factorization is the combination of H, M, and the remaining factorizations of S, S.  This factorization can be thought of as the \u201ccluster-of-h\u201d factorization which means that the cluster consists of nodes connected to H (and also to the nodes that don\u2019t belong to H).",
    "7": "The basic idea is to introduce a curriculum into the GAN training procedure. One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution. In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10. This is a nice applied paper where the core idea is quite simple and explained clearly. They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them. Lots of good deep learning voodoo in this paper. They found that the progressive scheme helps the models to overall improve (in terms of model stability) and reduces total training time by about a factor of 2. They mainly use the following changes to their models:  (1) They now use the Wasserstein loss (a derivative of the Jensen-Shannon divergence that was previously used) to train the generator. Previously, the divergence was calculated based on the difference between the input image and the target image. This has been changed to  (2) Consider the following examples:  Let's start out with the standard WGAN loss that we saw in the original GAN paper (i.e. WGAN-GP). Then we can add the following modifications to our WGAN:  We now use a 1x1 convolutional loss that penalizes the difference in image quality between the target and the generated image. We now add a 2x2 convolution that normalizes the image quality to the expected value (subject to the learning rate and learning rate of the discriminator). We now have 3 input images per layer and 3 target images per layers. The first 2 images (high-level) are generated using the standard GAN loss. The 3rd and 4th image (shown in the figure below) are created using the progressive loss. Note that the 3rd image is an example of the final generated image for the test set. In this example, the target images are the images of CelebA (high level image) and LSUN (low level image). The white circles indicate the different activations of the different layers (e.g. 2.5, 4.0). The blue circles indicate activations from different layers in the training set. The following examples show the order in which the layers are added:  Inception score  This metric is introduced to evaluate the quality of the generated samples. It is based on a combination of the mean squared error (measured by the squared error with margin) and the variance squared error  Measuring the variation of generated samples is difficult due to the fact that they are generated from a uniform distribution, which makes them hard to interpret. So the paper introduces a very simple and effective method to try and improve their image quality. They start with low-level examples and gradually add layers, such that the images become progressively higher in resolution. They achieve an inception score of 8.80 in unsupervised mode. (That should probably be higher than the 8.4 achieved by the model in \" Inception with Convolutional Autoencoders\" in the paper). They also experiment with least-squares learning with their GAN, and note that it helps the model to deal with higher levels of detail. Overall, one of my favorite paper this year: it's original, tackles an important problem, proposes a simple and elegant solution to it, and is easy to understand.",
    "8": "The paper introduces a new model class called as Convexified Convolutional Neural Networks (CCNNs) which captures the effect of parameter sharing in a convex manner by using a low-rank matrix for the CNN parameters. Benefits  Sparsity  Non-linearity  Each non-linear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing that the same filter is applied to each patch. Caution  The model is hard to train, because the activation function of a CNN is nonlinear. In order to address this issue, the authors propose to reduce the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our previous work on RKHS filters, in which we developed this step for fully connected neural networks. Contribution  The authors show that the model can be trained in a layer-wise manner, i.e. that the parameters of the network are shared across all the layers, in such a way that the input-to-output mapping (IRM) transformation is non-zero and the learning rate is stochastic. They train the network on MNIST, CIFAR-10, and handwritten MNIST datasets and show that it converges to the state-of-the-art in terms of generalization error. They also show that their model is able to outperform SVMs, networks trained with backpropagation, stacked denoising auto-encoders, fully connected networks, and SVMs and other baseline methods. Notes  The paper mentions two possible reasons for their model being different:  One, that the inputs are not directly from the training set, but from the pre-trained models. Two, that they use a different learning rate for each layer (i.e., not one per layer, but multiple layers) and that there is a difference in learning rate between the \"best\" and \"average\" learning rate of the different layers. The paper also shows that the variance of the variance between the best and the average learning rate (based on the number of layers) is lower for the two-layer model, which supports the idea of \"best learning rate\"",
    "9": "The paper presents some general characteristics that intelligent machines should possess, and a roadmap to develop such intelligent machines in small, realistic steps. Characteristics  Communication and Learning  The intelligent agent should be able to communicate with humans, preferably using language as the medium. Such a communication medium can be used to teach the machine basic tasks like spelling, grammar, vocabulary etc. A simulated environment to educate the agent to acquire these skills is suggested. The environment should be interactive and the agent should not be isolated from the outside world. Skills to Learn  Skills to learn should come from both the environment and the external environment. For example, the environment could provide a few examples to the agent so that the agent knows how to differentiate between them. The agent should also know how to distinguish between real words from the examples provided by the environment. Algorithmic Types  Learning of Algorithms  Given a learning environment and a learning agent, the following algorithms should be considered:  Simple - Assign a small number of facts and skills to the learning agent  Simple + Select from a set of known algorithms and use their weights as learning weights  Regular - Choose from a fixed set of chosen algorithms  Regular + select from a random set of candidate algorithms  General - Pick a learning algorithm and use its weights as a learning weight  L1 penalty  L2 penalty  Simultaneously - Choose an algorithm that maximizes the L1-likelihood of a given input (and use L2 as the learning weight)  Multi-threaded - Use multiple threads to transfer data and skills  Long-Term Memory  Use a long-term memory to store both the learned skills and the data  Simulated Environment  Environment  Agent  Learner  Start with a simple environment and teach the basics using natural language. This environment should encourage the learner to acquire \u201ccommon sense\u201d and \u201creasoning\u201d  Teacher  Provides a short-term supervised environment and an intermediate target which should lead to the true learning environment. The learner\u2019s goal is to master the basics so that it can access the full potential of the learning environment in a much smaller time-frame. The teacher  Provides an intermediate goal which encourages the Learner to explore the environment more and more and learns from the mistakes made by the Teacher  The intermediate goal should be a smaller version of the final machine which should act like a stand-alone intelligent system albeit one that will be initially very limited in what it can do. Future Work  The Learner should access the intermediate target and learn from it in a way that it learns from its mistakes and biases. Further, the Teacher should provide more information about the tasks and how to solve them. This information should be incorporated into the training and the Learning Agent  The Learning Agent should be free to use whatever programming language it likes and to interact with the Teacher whenever the Teacher is not around. It should be possible for the Teacher to provide additional information (for example, spelling or grammar) or to supervise the learning Agent. The Teacher should be aware of this and should not expect too much from the Teacher as it is a natural instinct to teach by example and not from pre-programming. Eventually, the Agent should acquire enough knowledge to perform many tasks independently and profitably. The Agent should have access to a large amount of data which should be stored in its own internal memory. The memory should be large enough that it should support many learning agents and not require copying across tasks. Learning  Learners should start out by memorizing facts and symbols. Then they learn to associate symbols and numbers (from the environment) and use these to generate new facts and numbers. They use the new knowledge to",
    "10": "The paper introduces a new methodology that provides a large scale supervised reading comprehension data set which allows to train a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure. Link to the implementation  Approach  Traditional approaches to machine reading and comprehension based on hand engineered grammars or information extraction methods of detecting predicate argument triples that can later be queried as a relational database. This approach fails to manage transition from synthetic data to real environments as such closed worlds \u201cexpect to capture the complexity, richness, and noise\u201d of natural language. Attention Based Attention Mechanism  Recent developments for incorporating attention mechanisms into recurrent neural network architectures allow a model to focus on aspects of a document that it believes will help it answer a question, and also allows us to visualise its inference process. Attentive and Impatient Readers  The attention mechanism is just one instantiation of a very general idea which can be further exploited. However, the incorporation of world knowledge and multi-document queries will also require the development of attention and embedding mechanisms whose complexity to query does not scale linearly with the data set size. The paper proposes to train the models using a combination of recurrent and attention based neural networks. The recurrent neural networks are adapted to take into account the fact that the attention mechanism cannot be directly incorporated into the model\u2019s architecture. Attention mechanism is incorporated via an attention scoring mechanism which allows the model to select the salient features of the input document such that these are the salient points in the document and use these to score the document as a feature vector. Impatiently  An attention scoring system is used where an attention score is assigned to each document point and a score between 0 and 1 indicates how much of the document is of interest to the model  Impatient  A patient reading model is trained to answer the question \u201cwhat\u201d  The document is fed to the attention scoring network and the score between 1 (high score) and 5 (low score) indicate how well the model understood the document  Attention scoring is based on a binary cross-entropy scoring function which aims to measure the degree of agreement between the document points and the question points  Paraphrasing (corresponding to the entities)  A paraphrasing sentence  A news story  The paper uses as input two news stories from CNN and DailyMail.com as the main source of information for the attention score. Two attention scoring models are trained and the results are presented below. Note that the datasets used for training and validation is much smaller than the one used in the state-of-the-art (SoTA) paper which uses 763 tokens and 27 entities and is comparable to the results of the Behavioural Imputation Corpus (BOC) paper . Dataset Generation  The main goal of the paper was to generate a large enough dataset to be able to test the proposed attention based models on which the models can learn to answer natural language questions. Two datasets were used to this end:  CNN Data  A short, short (shortly, short-term) version of each news story was converted to context query answer triplets using simple entity detection and anonymisation algorithms. The second dataset consisted of a longer, longer (more complex and longer) version that doubled the length of the original document triplet triplet and used as the context answer triplet. Data was post processed using a custom anonymisation algorithm which took as input a news story and an entity embedding vector and used the vectors to classify the entities in the embeddings. The input to the network was the embedding vectors and the output was a vector",
    "11": "RPCValet: NI-Driven Tail-Aware Balancing of s-scale RPCs \u2013 Daglis et al. In 2019 Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)  Today\u2019s paper choice is part of an ongoing theme in the \u201cTail at Scale\u201d challenge in the datacenter space: dealing with the tail of online services at scale. Tail-tolerant computing is one of the major ongoing challenges in the  hardware-driven solutions space, as long-tail events are rare and rooted in convoluted hardware-software interactions. A key contributor to the well-known \u201c Tail at Scale challenge\u2026 is the deployment of online service software stacks in numerous communicating tiers, where the interactions between a services software stacks take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity. Not only does each incoming request result in a wide fan-out of inter-tier RPCs, each one lies directly on the critical path between the user and the service, and thus bound the slowest interaction a user may have with the service. The trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant. It would be nice if we could get rid of the notion of \u201cuser-level\u201d protocols, and replace them with user-friendly user-terminated ones (e.g., InfiniBand/RDMA), but this seems to be a long-term trend\u2026  As it seems to us to be more practical to abandon user-level protocols in favour of integrated Network Interfaces (NIs). We are seeing a shift away from POSIX sockets and TCP/IP to lean towards user-specific protocols such as InfiniBands and RDMA. The net result of this is that the fundamental approach of communication latency will lower bound in the foreseeable future, and hence lower bound of speed-of-light propagation in the tail. The fundamental challenge exacerbates the difficulty in dealing with tail at scale as the number of concurrent requests increases. In response to this, we\u2019re seeing the emergence of \u2018inter-scale\u2019 (i.e., large-scale) NIs, and a shift towards more inter-scale communication between small and medium-scale NIs. The convergence of these two technologies will see a significant reduction in the time taken to send a request to the server, and an improvement in the average response time (latency). With a few more cores on the network (and more importantly, more compute power to compute them), the challenges of tail-tolerance will be even greater. We\u2019d like to get to the bottom of this as soon as possible, but it looks like this won\u2019t be possible\u2026  In \u2018tuck at scale\u2019 we have the following challenges:  We have a need to find a solution to the tail-latency problem we can both understand and control  Online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, \u201cshort\u201d RPCs. This means that a single \u201csingle-queue\u201d request could be one of a series of long, intermediate requests, causing the need for faster inter-server communication. We want to know which of these short requests are the most important, and therefore the most impactful, so that we can intelligently and efficiently handle",
    "12": "Read Atomic Multi-Partition (RAMP) transactions that enforce atomic visibility while providing scalability, two-phase locking, and guaranteed commit despite partial failures Gog et al., SIGMOD\u201914  With growing amounts of data and unprecedented query volume, distributed databases increasingly split their data across multiple servers, or partitions, such that no one partition contains an entire copy of the database. This strategy succeeds in allowing near-unlimited scalability for operations that access single partitions. But for the many other applications that access multiple partitions and need to communicate across serversoften synchronously in order to provide correct behavior, it\u2019s not so easy. Designing systems and algorithms that tolerate these communication delays is a difficult task but is key to maintaining scalability. In this work, we address a largely underserved class of applications requiring multi-partition, atomically visible transactional access: cases where all or none of each transactions effects should be visible. The status quo for these types of atomic transactions provides an uncomfortable choice between algorithms that deliver consistent results but are slow and unavailable under failure, and those that provide fast and scalable results but don\u2019t provide proper semantics for operations on arbitrary sets of data items. Read Atomic (RA) isolation \u2013 all-or-nothing visibility of updates  Read Atomic  The core idea in this work is to use the ACID isolation effects of ACID atomicity to mediate atomic visibility of transactions\u2019 updates. This differs from uses of atomicity (e.g., linearizability) to denote serializability or ACID-induced atomicity in the context of transactions. Consider a set of transactions where either all (or many) of the transactions are observed by all other transactions, or none (and only those that cause no (or few) transactions to be observed by other transactions. At issue is the following scenario:  Suppose we have a database containing two sets of operations, one for reads and another for updates to the database, where reads can come from either one or more transactions. For the reads that require atomic visibility, we have the following trade-offs to consider:  Primary indexing (indexing on the database)  Key constraint enforcement (keeping the database up to date)  Primary foreign key (and related to it)  Store updates in the database  Support for materialized view maintenance  Any of the above three use cases can be implemented using the following approaches:  Read-Atomized RamP transactions (read-atomized RAMP)  RAMP transactions are non-serializable transactions that guarantee atomic visibility and scalability because they use multi-party locking  Non-serialized RAP transactions (non-atomic RAP) transactions are transactions where the transactions effects are not visible to other transactions and only visible to others (and hence can be seen by all or many other transactions). In this way, the effects of single-party atomic transactions can be modelled as a single-phase phenomenon. The authors show that under certain conditions (and in the presence of multiple transactions), RAP can provide high scalability (up to 100%) and low overheating (0.5-10%) if all parties involved in the transaction are involved in some way (readers, database operators, or both). If not involved in any way, then the effects can be masked (or masked) by the other parties (or even masked entirely). The authors develop three variants of RAMP transactions that achieve the above objectives:  Single-party RAMP (single-party)  RAMP Transactions that avoid the two phase locking problem  Ramp-agnostic (RPN) transactions  RPN transactions that don\ufffd",
    "13": "The paper presents a framework for processing uncertain data using a DAG-based data processing system called UP-MapReduce that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. Uncertainty propagation  Data should be represented as probability distributions or estimated values with error bounds rather than exact values. In many cases performing computations on uncertain data as if it were exact leads to incorrect results. This paper proposes and evaluates an approach for tackling this challenge in DAG based data processing systems. The approach is based on techniques such as Differential Analysis (DA) and Monte Carlo simulation which help programmers to propagate uncertainties through the nodes of the DAG safely and accurately. Datasets  Data is being produced and collected at a tremendous pace. There is an urgent need to process an exploding body of data with uncertainties (suspected values from sensors in IoT). Data uncertainties also arise in many other contexts such as probabilistic modeling, machine learning, approximate storage and sampling-based approximation. For example, data collected using sensors are always estimates but there can be uncertainties the difference between the estimated and true values due to sensor inaccuracies. Failure to properly account for this uncertainty may lead to incorrect result. For some applications, including AI/ML, trend analysis and image processing, it can reduce execution time by 2.3x and up to 5x in some cases. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains. The framework  DAGs are directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. At the nodes, the inputs are composed of continuous and discrete functions that are differentiable and continuous-differentiable functions. For discrete functions, the input distribution and the locations of the discontinuities (discontinuities) are chosen from a Gaussian distribution which determines the appropriate computation method to use for the given discrete function. For the discrete function inputs, the choice of computation is made by taking the gaussian distribution and comparing it with the distribution of the discrete functions and choosing the appropriate method from the set of all the inputs  Given the variables  , we know which of the continuous function inputs is the most likely to be at a low or high risk of being at low risk given the distribution  We want the distribution to be a mixture of low and high risk, i.e., a mixture that gives a misleading impression of accuracy. To achieve this, we use a combination of DA (differential analysis based on Monte Carlo simulations) and uncertainty propagation  We use the following approach:  We start from a continuous function input (say from sensors) and compute the following function outputs:  The function outputs are then propagated through the following steps:  Let\u2019s start out with some continuous function outputs (per node) from the (continuous and discrete) component  We select a random gaussian function to propagate the uncertainty through the continuous component (d) of the (d-dimensional) component by comparing it to  The uncertainty propagation algorithm uses two different approaches:  First, we compute the probability distribution (probabilities) using the (Continuous, Differential) part-of-the-constant-differential-approximation approach  The second approach uses the (Monte Carlo) part of the same approach  We then use the difference of the probabilities as the uncertainty distribution  The change of the confidence interval  The difference of confidence intervals  The confidence intervals are chosen by comparing the confidence intervals between  and  The first part uses the squared error",
    "14": "VFDT: A Decision Tree Learning System for Data Mining \u2013 Klein et al. 2003  This paper describes and evaluates VFDT, an anytime system that builds decision trees using constant memory and constant time per example. It uses Hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. It does not store any examples in main memory, requiring space proportional to the size of the main memory (or parts thereof) or at all. It can learn an anytime-to-use model that is ready to use at any point in time (and hence can store any statistics or data points associated with any part of the tree) and can output a model which is identical to a batch version of the data points. The main reasons for including such data points is that they come from an open-ended data stream, and that the current KDD systems dealing with such data volumes are not equipped to cope with them. Ideally, we would like to have K DD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information. Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists\u2026 However, the available algorithms of this type have significant shortcomings from the KDD point of view. Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode. They are sensitive to the example ordering, potentially never recovering from an unfavorable set of early examples. Others produce the same model as the batch version, but at a high cost in terms of time and memory. Klein et. al. tackle these challenges head-on, developing a tree learning algorithm that learns to balance the above three main constraints: time, memory and sample size. Their decision tree, VF DT, combines the benefits of online learning with the benefits (a) of batch learning, (b) data dredging avoidance, and (c) reduced overfitting if the sample size is small enough to avoid overfitting and data overfitting. The basic idea is to let the tree grow exponentially with the amount of data it has to learn, while letting the model grow in size (i.e., the tree size and/time per example). To achieve this, the tree\u2019s size and timestamps are both controlled by a factor H (in this paper, H is defined as the number of nodes in the tree). The probability that a given node will produce an H-indexed node at any given timestep decreases with the more examples it has  We also describe how the decision-tree learning system can incorporate tens of thousands of examples per second using off-the-shelf hardware. The system is evaluated on a continuous access data set generated by the University of Washington main campus. In this data set, there are 10 million (millions) of unique classes of classes and categories. Each class is represented by a unique identifier, and each unique identifier is denoted by a 1-dimensional vector (denoted here as  ). The number of unique identifiers is represented as  , where  represents the unique identifier for that class and  denotes the set of all the other classes in the dataset  The input to the time-ordered vector  is a sequence of classes,  ,  , and  . The first  of these vectors is the first example in the class  , the second  of the class,  and  the third  . Each example  is fed to the decision tree as it comes in, and the tree  learns a binary tree-like structure",
    "15": "The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of trajectories. Standard IRL approaches aim to learn reward functions that \u201cjustify\u201d the demonstration policy and hence those approaches cannot outperform the \u201cnear-optimal\u201d demonstration policy. The main flaw of current inverse RL approaches is that when the learning agent does not have access to the true reward function, it can fall prey to the effects of the imitation learning approach when the policy is not directly from the user\u2019s own intentions. In contrast, the proposed IRL approach aims to learn a parameterized reward function that explains the ranking demonstrations, allowing the evaluation to focus on the features correlated with the true intention and not just the quantity. Approach  Approach  Given a trajectory, start with a trajectory T, rank the trajectories according to their similarity with each other. Assume that the trajectory T-1, T-2, and T-3 are a sequence of trajectory trajectories T1, \u2026 T-4, where T=0.5 is the sequence of states corresponding to the trajectory and T=1 if the trajectory is unknown to the user. These trajectories are ranked using a binary classification loss function which aims to predict the reward for trajectories that are better than the one-hot trajectory. The reward function is trained by learning from observations where the input trajectory is the sum of rewards of all trajectories (from the training set) and the target reward is a sum of the rewards of the user chooses from a set of trajectory options. The user chooses the trajectory from a stable distribution. Given the trajectory, the reward function predicts which of the two given trajectory would be the better reward function. The sum of reward functions (corresponding to the two trajectories) is trained jointly with the user and the training objective. The two reward functions are trained jointly and then the user selects the preferred trajectory from the training dataset. The chosen trajectory and the chosen reward function are trained together with a learning agent which is trained separately from the learning the policy from the other trajectory. A learning agent is trained to predict both the reward functions and the preference between the two. Then, given a trajectory and a trajectory (given a trajectory), the model learns to predict which reward function would be better and how to use it to improve the other reward functions. The model is trained end-to-end using a stochasticity term which means that it learns to use the extrapolated reward function to estimate the difference between the expected reward and the generated reward function (from all the other trajectories). Results  Environments  Mujoco (Atari), Atari  Atari  Demonstrations  The proposed approach outperforms the baselines Behaviour Cloning from Observations (Biotic Cloning) and Imitation Learning (RL). In terms of reward extrapolation, the key observation is that the proposed approach is robust to noise and that the model does not overfit (in terms of performance) to the information provided by the training data. The experimental results are presented in the figure below. Ensemble of networks used to train and train the model  Ensemble  Model  Given an observation  , the model generates a trajectory  from a distribution  consisting of  trajectories  . The model predicts the reward  for each trajectory  and  . This prediction is fed to the model as a hidden parameter  . In practice, the model uses the following model:  A trajectory  is the trajectory which is expected to receive the highest reward  A reward function  from  , which",
    "16": "The paper proposes a framework that uses diverse sub-optimal world models to decompose complex task solutions into simpler modular sub-tasks. This framework jointly learns the required modular subpolicies as well as a controller to coordinate them. Link to the code  Approach  The main idea in this paper is to use the agent to learn a gating controller that selects from a set of models such that each model primitive is only relatively better at predicting the next states within a certain region of the environment space. This assumption is based on the fact that successor representations decouple the state transitions from the rewards (representing the task or goals) and can only transfer across tasks with the same environment dynamics. In this setting, the agent must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning (multi-task learning), where the agent jointly trains on multiple task environments. Not only is this setting different, but also non-incremental, which allows the problem of discovering common structures between tasks easier, and allows the methods to ignore the issue of catastrophic forgetting (in which the agent forgets how to solve previous tasks after learning to solve new tasks)  Motivation  The authors hypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agent\u2019s solution increases if it can capture such structure. The world is complex and learning models consistent enough to plan with is not only hard, but planning with such one-step models is suboptimal  The requirement that these models be good predictors of the world state is unnecessary. A key ingredient of our proposal is the idea of a transition model that can predict future sensory data given the agents current actions. The assumption that a subset of model primitives are useful across a range of tasks and environments is non-negligible  It assumes that the following assumption:  That successor representations can decouple from the state transition model and act as a predictor for the agent  That the agent can accurately predict future state transitions given current state transitions  That transitions are transfer-dependent  The region of specialization in the transition model (region of specialization) is the set of all the possible states that the agent could be in given the current state and the current actions  This area is referred to as the \u201ctermmodel region\u201d  Since the termmodels are models that predict the next state (and related subtasks) for a certain set of inputs, they are assumed to be relatively well-tuned  It can be shown that a good transition model can be obtained by learning a permutation of the following permutation:  The permutation term is defined in the form of  where  is the permutation factor    and  is a learning rate   The number of permutation factors is represented as  The model is trained to predict for each input the probability of being in  a given region of  the transition state  The objective is to predict the transition from  to  from  The task of decomposition is to compress the input into smaller and smaller modules according to  . Modules  The agent is trained in a bottom-up manner by learning the required modules in a top-down manner, in such a way that the number of modules grows monotonically with the size of the input. For a given source task,  , the following modules are learned:  Modules 1, 2, 3, 4, 5, and 6  The modules are chosen based on their size  The regions of specialization are defined based on  ,  , and  . Each module is assigned a unique label  which",
    "17": "The paper introduces a new training procedure for generative models where the model is trained to predict the probability of a given data distribution (i.e. the data distribution is fed into a discriminative model, e.g. a GAN) trained to estimate the accuracy of the discriminator (e.g., a DGAN). In this paper, the authors employ the concept of a two-player game in which a discriminator is pitted against an adversary in the form of G.  The adversarial process is modelled as a game between two teams of counterfeiters (G and D). One player, called as G, tries to produce fake samples and use them to evade detection, while the other player (here represented as D) tries to detect the counterfeit samples and correct the mistake caused by the fake samples. The paper demonstrates the potential of the framework through a quantitative evaluation of the generated samples. Questions for discussion  Why don't they use the reverse KL term for L1 units? It seems like they are using a KL term here for L2 units, which is equivalent to using two KL units (one for G and one for D). The paper also mentions that KL units should be used in place of KL units in D, since they seem to have a better gradient (more specifically, their gradient is lower with D). How effective is the method of training G and D? (This seems like a big question)  Why not just train G to predict (and test) D and then test both of them on the real data and then fine-tune G later on? (see Equation 5)  What justifies training two models simultaneously? (See Equation 10)  How do you mesh them together? Both models should be able to outperform each other in terms of accuracy? Generative models typically have a single hidden layer, while D models usually have multiple hidden layers. In the generative model setting, it would be ideal to train D to predict whether a sample came from the training data distribution or not (see Section 4.2.2 in the paper). But it is difficult to get rid of the hidden layers, and hence the name \"adversarial nets\". The paper proposes to train both models jointly using the backpropagation and dropout algorithms, with the last hidden layer being a MSE between the first and second hidden layer. The training procedure thus allows for fine-tuning of G to improve upon D, while ensuring that D does not make a mistake. Notes  The idea in itself is very simple and straight-forward to add to any existing model, but I find it hard to believe that it would not be easier to implement in practice as well. It would be interesting to see if the authors can add an auxiliary network to predict z given the training dataset. The auxiliary network could be trained after the model has finished training, for example, to take care of updating the weights of D after it has run. This is similar to the idea in the wake-sleep algorithm, but with the advantage that it can be trained for a fixed number of epochs (up to the point where D has run out of training, i.e., after p(x, c) has been fed into the model). One can train a family of conditional models to model all conditionals of x by training a pair of models (p(x | c) and training them jointly with each other. The family of models could be composed of models that share parameters, where p is a deterministic MP-DBM stochastic model, and c is a random noise model. I think one could also train a second network (d) to",
    "18": "An empirical analysis of Zcash\u2019s transactions and associated with its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. Contribution  Since the introduction of Bitcoin in 2008, cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. Despite the broader positive potential of blockchain (i.e., the public decentralized ledger underlying almost all cryptocurrencies) is still unclear, despite the growing number of legitimate users there are today still many people using these cryptocurrencies for less legitimate purposes. These range from the purchase of drugs or other illicit goods on so-called dark markets such as Dream Market, to the payments from victims in ransomware attacks such as WannaCry, with many other crimes in between. Given the growing awareness that most cryptocurrencies do not have strong anonymity guarantees, a number of alternative cryptocurrencies or other privacy-enhancing techniques have been deployed with the goal of improving on these guarantees. The most notable cryptocurrencies that fall into this category are Dash, Monero, and Zcash  At the time of this writing all have a market capitalization of over 1 billion USD, although this figure is notoriously volatile, so should be taken with a grain of salt. Even within this category of privacy- enhanced cryptocurrencies, and despite its relative youth, and relatively youth, Zcash stands somewhat on its own. The reason for this is that Zcash is backed by a body of recent, highly regarded research on pseudonymous ad-dresses, and comes with seemingly combining strong privacy guarantees with a relatively small number of transactions. Indeed, the original papers cryptographically prove the security of the \u201cshielded pool\u201d privacy feature  in which users can spend shielded coins without revealing which coins they have spent. It does require, however, that all newly generated coins pass through the shielded pool before being spent further, thus ensuring that all coins have been shielded at least once. This requirement led the Zcash developers to conclude that the anonymity for users spending shielded coins is in fact all generated coins, and thus that the strategies that other cryptocurrencies use for anonymity are \u201csmall\u201d and \u201cimplemented\u201d in comparison to Zcash and  other cryptocurrencies in terms of transaction privacy. The main question this paper aims to address is this:  How much of the overall transactions in Zcash are really \u2018transparent\u2019? And, to what extent does this \u201ctransparency\u201d extend beyond the use of pseudonymous senders and receiveers, and the amount of the total amount of coins that have been spent? The main answer is that the vast majority of transactions are not transparent, meaning that they reveal the pseudonymous addresses of both the sender and the recipient (e.g., \u201ccoin_id\u201d) along with the amount being sent and the transaction amount. In fact, the authors found that the majority of exchanges typically engage in transactions that are essentially the same as transactions in Bitcoin in the transparent part of the blockchain, meaning it does not engage with the \u2018public part\u2019 part at all. How Zcash has evolved  The study begins by introducing a general analysis of how Zcash transactions have evolved over time, and shows that the main privacy features of the current Zcash model are:  A Shielded Pool  Shielded pool is a place in the blockchain where users can send and receive coins in pseudonymous transactions using only pseudonyms as identifiers.",
    "19": "Learning to learn voting rules based on the preferences of individuals Kahng et al., ICML\u201919  This is the third post in a mini-series on machine learning and social choice, and it\u2019s going strong! The question of which voting rule to use is one of the central questions in computational social choice (and in social choice theory more broadly). The central premise is that, in the context of virtual democracy, certain statistical considerations should guide the choice of voting rule, yet one might hope that it would still operate on the \u201ctrue\u201d predictions of the voters, yet still maintain the integrity of the voting rule. Indeed, Borda count is also compelling in terms of usability and explainability, and from a statistical viewpoint, explaining why it is attractive\u2026 which voting rules have the property that their output on the true preferences coincide with their \u201cdeeper\u201d estimates thereof? Our theoretical and empirical results identify two possible reasons for this:  The first is that the input to the system is a collection of (say) 100 pairwise comparisons of two food donations, where in each comparison, the voter is provided information about the type of the donation, as well as seven relevant features of the two alternatives that are being compared, e.g. the distance between the donor and the recipient, and when the recipient last received a donation. The second is that at runtime, the predicted preferences of each voter are used to compute a ranking over the alternatives, and this ranking is combined with the true preference scores of the other voters, to produce a single voting rule that aggregates the vote on the given dilemma. In short, given a set of (100) pair-of-choice examples, the model is trained to predict the voting preference scores for each of those examples, and a voting rule is applied to those predictions, ensuring that their true preferences align with the predictions of other voting rules (i.e., the voting rules are robust to noisy estimates). Building on the work of Noothigattu et al. (2018), who presented a model of voting preferences in a recent paper, \u201cAutomating Decisions with Automated Decision Making , Automated Empirical Evidence ,\u201d with the following structure:  We start out by collecting the voting preferences of the respondents (members of the public) on a variety of different dilemmas. We use these as a proxy for the voting process, and use the models to learn models of the actual voting process. The models are trained on a dataset of candidate preferences, and then we use those models to compute the voting score of each of the candidates (referred to as the \u2018puppeteer\u2019). At runtime, we combine the predicted voting score with the real voting score for each candidate, and produce the final voting decision by summing the results of those two processes. The paper aims to show that voting rules should be robust to prediction errors, in that the output of voting rules that operate on true preferences are likely to coincide with the output on noisy estimates. Specifically, the authors show that the classic Borda Count rule is robust to both noisy estimates and true preferences, whereas voting rules belonging to the wide family of pair-wisemajority consistent rules are not. The main results in this paper are in the following table, and the table below. You\u2019ll find it interesting to note that the figure below shows the effect of using noisy estimates in the case of the proposed voting rule:  In this case, the true voting preference score for a candidate is the score of the candidate whose preference ranking was predicted based on a combination of all the other voting votes, and only the preference"
}