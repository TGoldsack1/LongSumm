{
    "0": "the paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. approach given a motif m , the framework aims to find a cluster of the set of nodes s such that nodes of s participate in many instances of m and avoid cutting instances of m (that is only a subset of nodes in instances of m appears in s). mathematically, the aim is to minimise the motif conductance metric given as cutm(s, s\u2019) / min[volm(s), volm(s\u2019)] where s\u2019 is complement of s, cutm(s, s\u2019) = number of instances",
    "1": "kv-direct: high-performance in-memory key-value store with programmable nic li et al., sosp\u201917 we\u2019ve seen some pretty impressive in-memory datastores in past editions of the morning paper, including farm , ramcloud , and drtm . but nothing that compares with kv-direct: with 10 programmable nic cards in a commodity server, we achieve 1.22 billion kv operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. check out the bottom line in this comparison table from the evaluation: ( enlarge ) in addition to sheer speed,",
    "2": "this paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. only a finite set of a priori known policies are considered. bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. a statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. evidence",
    "3": "`update 2015/11/23: since i first wrote this note, i became involved in the next iterations of this work, which became v2 of the arxiv manuscript. the notes below were made based on v1.` this paper considers the problem of maximum inner product search (mips). in mips, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve mips using hashing methods",
    "4": "proposes a novel, end-to-end architecture for generating short email responses. single most important benchmark of its success is that it is deployed in inbox by gmail and assists with around 10% of all mobile responses. . challenges in deploying smart reply in a user-facing product responses must always be of high quality. ensured by constructing a target response set to select responses from. the likelihood of choosing the responses must be maximised. ensured by normalising the responses and enforcing diversity. the system should not add latency to emails. ensured by using a triggering model to decide if the email",
    "5": "they suggest a new method to train gans. they start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. each new block of convolutions is slowly blended in, instead of being added from one batch to the next. combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new celeba-hq dataset (celeba in high resolution). they also suggest a new scoring method based on the approximated wasserstein distance between real and generated image patches. according to that score, their progressive",
    "6": "the paper presents the task of abductive nlp (pronounced as alpha nlp) where the model needs to perform abductive reasoning. abductive reasoning is the inference to the most plausible explanation. even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. a new dataset called as abstractive reasoning in narrative text (art) consisting of 20k narrative contexts and 200k explanations is also provided. the dataset models the task as multiple-choice questions to make the evaluation process easy. task setup given a pair of observations o1 and o2 and two hypothesis",
    "7": "in this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. succinctly, they propose to use rkhs and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. hence, on experiments on mnist and cifar-10, they show that they can outperform smaller cnns by \u201cconvexifying\u201d them. they note that their method doesn\u2019t work with max pooling or very deep cnns with lots of bells and",
    "8": "an empirical analysis of anonymity in zcash kappos et al., usenix security\u201918 as we\u2019ve seen before, in practice bitcoin offers little in the way of anonymity . zcash on the other hand was carefully designed with privacy in mind. it offers strong theoretical guarantees concerning privacy. so in theory users of zcash can remain anonymous. in practice though it depends on the way those users interact with zcash. today\u2019s paper choice, \u2018an empirical analysis of anonymity in zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the",
    "9": "the paper looks at the problem of learning structured exploration policies for training rl agents. structured exploration consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters. to encourage exploration, noise can be added to the policy at each time step t. but the noise added in such a manner does not have any notion of temporal coherence. another issue is that if the policy is represented by a simple distribution (say parameterized unimodal gaussian), it can not model complex time-correlated stochastic processes. the paper proposes to condition the policy on per-episode random variables (z) which are sampled",
    "10": "build a supervised reading comprehension data set using news corpus. compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. reading comprehension estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. dataset generation use online newspapers (cnn and dailymail) and their matching summaries. parse summaries and bullet points into cloze style questions. generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. data anonymized and randomised using coreference systems,",
    "11": "the paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. adversarial net two models - generative model(g) and discriminative model(d) both are multi-layer perceptrons. g takes as input a noise variable z and outputs data sample x(=g(z)). d takes as input a data sample x and predicts whether it came from true data or from g. g tries to minimise log(1-d(g(z))) while d tries to maximise the probability",
    "12": "the paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. the framework is called as model primitive hierarchical reinforcement learning (mphrl). idea instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). each model primitive will be good in only a small part of the state",
    "13": "statiscal foundations of virtual democracy kahng et al., icml\u201919 this is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently. consider a food bank that receives donations of food and distributes it to those in need. the goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. we could hard code a set of",
    "14": "rpcvalet: ni-driven tail-aware balancing of \u00b5s-scale rpcs daglis et al., asplos\u201919 last week we learned about the [increased tail-latency sensitivity of microservices based applications with high rpc fan-outs. seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. today\u2019s paper choice, rpcvalet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s). the net result of rapid advancements in",
    "15": "mining high-speed data streams \u2013 domingos & hulten 2000 this paper won a \u2018test of time\u2019 award at kdd\u201915 as an \u2018outstanding paper from a past kdd conference beyond the last decade that has had an important impact on the data mining community.\u2019 here\u2019s what the test-of-time committee have to say about it: this paper proposes a decision tree learner for data streams, the hoeffding tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. this work constitutes a significant step",
    "16": "beat: asynchronous bft made practical duan et al., ccs\u201918 reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. that\u2019s the world of byzantine fault tolerance (bft). we\u2019ve looked at practical bft (pbft) and honeybadger on previous editions of the morning paper. today\u2019s paper, beat, builds on top of honeybadger to offer bft with even better latency and throughput. asynchronous bft protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (wan) environments, as these asynchronous protocols",
    "17": "scalable atomic visibility with ramp transactions \u2013 bailis et al. 2014 ramp transactions came up last week as part of the secret sauce in coordination avoidance in database systems that contributed to a 25x improvement on the tpc-c benchmark. so what exactly are ramp transactions and why might we need them? as soon as you partition your database across multiple servers, things start to get interesting. we\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026 the status quo for these multi-partition atomic transactions",
    "18": "uncertainty propagation in data processing systems manousakis et al., socc\u201918 when i\u2019m writing an edition of the morning paper, i often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. there are three levels of takeaway from today\u2019s paper choice: if you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. if",
    "19": "understanding lifecycle management complexity of datacenter topologies zhang et al., nsdi\u201919 there has been plenty of interesting research on network topologies for datacenters, with clos-like tree topologies and expander based graph topologies both shown to scale using widely deployed hardware. this research tends to focus on performance properties such as throughput and latency, together with resilience to failures. important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure. the great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible"
}