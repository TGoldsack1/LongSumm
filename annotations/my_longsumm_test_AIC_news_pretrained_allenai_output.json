{
    "0": "asynchronous environments. and extensible, consisting of five asynchronous protocols that are designed to meet different goals (e.g., different performance scenarios). due to modularity in its design, features of these protocols can be mixed to achieve even more meaningful trade-offs between functionality and performance for various applications. through a 92-instance, five-continent deployment of amazon ec2, we show that beat on amazon ec2, all our beat instances significantly outperform, in terms of both latency .",
    "1": "paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. each policy captures a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. we first employ bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. this work extends our previous multipolicy system [11] by incorporating behavioral anticipation into decision-making to evaluate sampled potential vehicle interactions. .",
    "2": "abductive abductive reasoning. has been considered to be at the core of how people interpret and read between the lines in natural language inference and generation. we present the first study that investigates the viability of language-based abductive we introduce a challenge dataset, art, that consists of over 20k commonsense narrative contexts and 200k explanations. based on this dataset, we conceptualize two new tasks \u2013 (i) abductive a conditional generation task for explaining given observations in natural language. on abductive nli, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. our analysis leads to new insights .",
    "3": "search (mips) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. solutions based on locality-sensitive (lsh) as well as tree-based solutions have been investigated in the recent literature, to perform approximate mips in sublinear time. in this paper, we compare these to another extremely simple approach for solving approximate mips, based on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. this simple method also yields more robust retrievals when the query is corrupted by noise. the maximum inner product search (mips) problem has recently received increased attention, as it arises naturally in many large scale tasks. .",
    "4": "most recent designs have focused on performance properties such as latency and throughput. in this paper, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. motivated by this, we design a new class of topologies, fatclique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics. over the past decade, there has been a long line of work on designing datacenter topologies 35, 31, management [30, management .",
    "5": "modern kvs goes beyond the traditional object-caching workload . data centers, shifting the bottleneck of most kvs from the network to the cpu. rdma-capable nic partly alleviates the problem, but the primitives provided by rdma abstraction are rather limited. meanwhile, programmable in data centers. recent years have witnessed a rapid increase of network bandwidth in data centers, recent years .",
    "6": "networks are a fundamental tool for understanding and modeling complex systems in physics, biology, and social science. many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. however, organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. this framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. the framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns. organization of complex networks .",
    "7": "new layers that model increasingly fine details as training progresses. this both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., images at 1024. we also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised cifar10. additionally, et al., et al., et al., 2017; et al., 2017; et al., 2017; et al., et .",
    "8": "convexified convolutional neural networks have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. as the actual performance is determined by some combination of the cnn architecture along with the optimization algorithm. .",
    "9": "the development of intelligent machines is one of the biggest unsolved challenges in computer science. in the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose communication, as a prerequisite to be pursued all at once, in the last article, we propose an alternative approach: from performing menial jobs for us to helping the advancement of basic and applied research. given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines. still, .",
    "10": "large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and understanding documents that they have seen, but until now large scale training and test datasets have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2]. supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets, .",
    "11": "modern online services come with stringent quality requirements in terms of response time tail latency. because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, \u03bcs-scale aggravating the need for faster inter-server communication. in reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level and new architectures with fully integrated network interfaces (nis). such architectures offer a unique opportunity for a new ni-driven approach to balancing rpcs among the cores of manycore server cpus, yielding major tail latency improvements .",
    "12": "databases can provide scalability by ensuring atomic visibility: either or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components that tolerate these communication delays is a difficult task but is key to maintaining scalability [17, 29, 35]. in this work, we address a largely underserved class of applications requiring near-unlimited operations. .",
    "13": "results. unfortunately, and machine learning algorithms. performing computations on uncertain data as if it were exact leads to incorrect results. sensors in iot, sampling-based approximate computations and use it to modify ten applications, including ai/ml, image processing and trend analysis applications to be expressed as directed acyclic graphs (dags) of side-effect free computation nodes, with data flowing through the edges for processing. the frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data .",
    "14": "many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. mining these continuous data streams brings unique opportunities, but also new challenges. this paper describes and evaluates vfdt, an anytime system that builds decision trees using constant memory and constant time per example. vfdt can incorporate tens of thousands of examples per second using off-the-shelf hardware. it uses hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. we study vfdt\u2019s properties and demonstrate its utility through an extensive set of experiments on synthetic data. we apply vfdt to mining the continuous stream of web access data .",
    "15": "novel reward-learning-from-observation (t-rex), trajectory-ranked reward functions from a set of potentially poor demonstrations. when combined with deep reinforcement learning, t-rex outperforms state-of-the-art imitation learning and irl methods on multiple atari and mujoco benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. we also demonstrate that t-rex is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time. due to advantages .",
    "16": "learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. this paper presents a framework for using diverse suboptimal world models to understand the importance and robustness of different elements in the framework and limitations to this approach. in the lifelong learning setting, we want our agent to solve a series of related tasks after learning to solve new tasks in a sequential learning setting. our work takes a step towards solutions for such incremental settings. .",
    "17": "new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model g that estimates the probability that a sample came from the training data rather than g. the training procedure for g is to maximize the probability of d making a mistake. this framework corresponds to a minimax two-player in the space of arbitrary functions g and d, a unique solution exists, with g recovering the training data distribution and d equal to 1 2 everywhere. in the case where g and d are defined by multilayer perceptrons, the entire system can be trained with backpropagation. to detect the counterfeit currency. competition .",
    "18": "derived from bitcoin, zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. in this paper, we investigate all facets of anonymity in zcash\u2019s transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. we conclude that while it is possible to use zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. since the introduction of bitcoin in 2008 [34], cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. .",
    "19": "virtual democracy is an approach to automating by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. one of the key questions is which aggregation or voting rule \u2014 to use; we offer a novel statistical viewpoint that provides guidance. specifically, we seek voting rules that are robust to prediction errors, in that their output on people\u2019s true preferences is likely to coincide with their output on noisy estimates thereof. we prove that the classic borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwisemajority consistent rules is not. because people can express a sensible opinion about a new dilemma arises. but what if we could predict the preferences of voters \u2014 instead of explicitly asking them each time \u2014 and then aggregate those predicted preferences to arrive at a decision? this paradigm is exactly the idea behind the work of noothigattu ethical decisions. ."
}