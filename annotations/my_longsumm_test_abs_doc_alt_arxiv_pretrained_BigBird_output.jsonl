{
    "0": "in this paper, we show that there is a fundamental trade - off between the amount of privacy that can be shared by an adversary and the speed with which it can retrieve the data that belongs to the adversary.<n> we prove that this tradeoff leads to significant benefits in terms of both security and speed : in our design ( which is based on fault tolerance ) we are able to achieve significantly more security than is possible with fault - tolerant protocols that only support secure storage ; and we can achieve much faster recovery times for our protocols compared to those that do not support recovery of data from adversaries that belong to a class of classes that we refer to as the  adversary class ''<n> [ 1 ] the problem of sharing secret information in a secure way is of fundamental importance in cryptography and information theory @xcite and has attracted a great deal of attention in the last few years ( see for a review on this subject _<n> http://arxiv.org/abs/cond-mat/0406484 __  <n> ]. in order to guarantee the security of the information shared in an exchange of quantum resources ( e.g. the quantum bits ( qubits ), see ref.@xmath0 for an overview of how to share qubit information efficiently on a quantum computer and refs.[4][e]for an introduction to quantum computing see the introduction of [5 [e5[e4[5 ], see reference [6[6 [4 [8[i[8 [[[n[b[43 s[50[a[f[2[22 ][[42 t[21] in [42e[45[in [43[s [b [50 [n [45] [in[62[r[52] we extend [f [ 42 [ [eb [ [d[v[x[we [] to [. [... [ 46[re[ [ in  [ to! [ '[@e [ 43[] ]) we have to solve [ e[(e  to our [ we also [ one [ based []. [ for [ 41 [ the [ @ [ a [ address [ and [ that [ with [ f[the [ which  we  the other [ as [ye [vi[g  in which we support [ further  for the others  that ]] ...  a  based  which [ ( [ is [ from [i  with the most [<n> the algorithm [ while [ it  is  among [ additionally [ of  from the error [ whose [ they  while the database [ among the...... to which has [([.  of our[is [, [ se  one  has  our  whose  it is not [ who is the].  and our algorithm to be the message [ are [ also to have [], [ but [x  also  was [ this [ - [ make the protocols [ x[se  this th[... the graph [is the storage of a further to... that is that has the entire [forward [ storage and a. we do [ p[vi [se [4]  others [ both [ cryptography [ more  they are not the whole [ error of all [ graph and... and levi[ye  who has a... we to all the software  ( the] and that was the function [ these [ each of these com  will  as we will also the[it is to we is also we has to support the we find that the results [2].... with a]. and is for any other that does not ",
    "1": "operate reliably in realworld traffic, an autonomous car must evaluate consequences of its potential actions by repeating uncertain intentions of other traffic participants.<n> this paper presents an integrated behavioral anticipation and decision-making approach that models both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of another vehicle that captures the high-level intention of driving along a particular lane or over a permutation of the lanes of a specific intersection ; and that is able to make decisions based on these coupled interactions between cars in a tractable manner : it is not necessary to consider expensive collection of training data or use heuristics to determine which policy is more likely to be executed by the other car than the policy of our car in the presence of uncertainty over the potential of each other s actions in that particular scenario ( e.g. changing the direction of an intersection or turning the lane in which we are going to change a direction to avoid a stop at a given stop sign ) <n> it does not need to know the full history of all other participants in any scenario to estimate the likelihood that each of their actions are anomalous and can be used to compute the expected reward of those actions based only on their observed behavior and not their actual actions as they would in situations where they are expected to execute known closed - loop policies in their respective directions ( i.e. turning of two lanes or changing a lane as we would do the same in our case ( p, v t = p = @xt | p(t @ @  p _ p @ p ( @ z ] @<n> pt p_t z = zt + p@x @ a @_ @ ( z(p = a = g @ x @ v @ = dx  @ + @ b! p p + a p z @ m @p @ g = b = x = ( a z + z p. p+ p) @t  z (  p b @ - p x dz = v = m = + b p v + +  + x + g + ( b z z) p * p - z+ z@ p d = d  - @ [ p g p a v p= p [ z. z_ z b ( x ( g) = [ @ * @ d @ j = * z v ( dt b(x p] p[t ( v) + d) zp   = = j + v. @( z+1 p... p i = k +t xt a b + [  b. b b) * * d ps @... z] x ( p; p to p y  ( + = c = 0 p<n> z * + j  to z - v * b@ z [ a. d + * g ln p-t g.  x. + m  d p is p",
    "2": "this paper presents the first study that investigates the viability of language-based inference for abductive narratives.<n> the study consists of two parts : ( i ) a comprehensive study of the best performing model on the most plausible scenario for a narrative ; and ( ii) two new experiments, one on multiple scenarios and the other on conditional reasoning for natural language ( cn)ference in contexts of interest to abductively narrative ( a.k.a.non-commutative algebra <n> n.c.f.)and to ctive inference in context of non - causally connected sets _ ( c.v.n.w.h.e.,_the best model achieving 91.4% accuracy on a dataset of 200 observations of human beings in the presence of a linear adversary that has been pre- and post - trained as a neural model in order to achieve high accuracies on both scenarios ( b.i.t.g.an example of how this model fails to perform well in a scenario where the adversary is a highly connected component of an n- or c- type of set s structure in such a way as to make it appear as if the set is composed of several components of different types of sets that are all closely related to each other and that have a high level of similarity to the observed scenario ( j.j.s.d.the model achieves 68.9% accuracy below the performance of 68.3% accuracy for an observation in which state-[e[a ] that is not used to select a state of state ( e.(e(i for state(a for the state for t a  state in [e in an independent state to choose a [a[i as state ii(b(([[b[n](ii] as the]).[ii(s]) and [b.] in one of [i?(n ]) ], while [(t ]](v [v(d(h) in (b (a) to obtain (ii) with a) that we are a, b, [ b) and b(g) while b-) for [ii (c   ii. b[(j!(@b ii ), while (i)... b_(p.b] ( p, ii, which is the b2(w) as b]... ii. ( iii) .... [ ii] with the] and a] to be a......) we introduce the[s) (... ( [...] while the... [. i....[d) which) is ( v.. ii ii_a] for two) the [[2b]),... ii[g]) [] ]..], which are the results of b; b ( 2, a[] [ ( ( the ii-(the] is to [ [_b...(... in... to..., ( in b@e)) of...; [2[@i] we aim ( @e]. in this is in two, while while we introduced ( which we have the @b [, we make ( we will ( to introduce ( for which [) are ( this are to see the  for all the analysis of that ([in [;... with ( that [ i and we... that  to find ( d.2... for... while  in that to which ((in] of ( g.ii in all ( while... we  - ( two. @)<n> [ e, the ( with [ that... which ",
    "3": "in this paper, we compare two very simple approaches for approximate k-means clustering. the first one is based on tree-based vector representations and the second one on spherical kmeans : we propose to train spherical _ k_means after having reduced distance to each leaf of the dataset ( the set of candidate vectors to be clustered ) and we show that this simple approach yields much higher speedups for same precision than current state-of - the - art methods and also yields more robust retrieval when corrupted noise is present ( this is a common situation in large scale applications where we are asked to perform a full linear search within only a few milliseconds ; this problem is common in many applications such as online search and recommendation and it has been addressed in the literature ( e.g. by kim and lee ( 2012 ] and more recently by the authors in arxiv:1211.4321 ). <n> [ theorem]acknowledgement [ 1]algorithm [ 2]axiom[ 3]claim [ 4]conclusion [ 5]condition [ 6]corollary [ 7]criterion [ 8]definition [ 9]example [ 10]exercise [ 11]lemma [ 12]notation [ 13]problem [ 14]proposition [ 15]remark [ 16]solution [ 17]summary [ 18]assumption [ 19]appendix [ 20] # 1#2#3#4#1#2#1 # 3#1  # 2#1_#1#1 - # # 4#1 ( # 2)[4#1(#1]#1-#1 [ 3 -#1 2 -#2] - 2 s # 5 - 0 - 1 - ( 2, 2) to solve the top - @x[n ( 0] to find the number of all the vector points for all to the best points as a top of a vector for the corresponding to all ( top to a cluster as all as the @ @ ( 1 to top in all for a number in @ - to ( t to x - x and to search for any clusters as  to any vector with a small clusters in any other clusters of any small vector that to  ( ( n and all clusters to even the other vector and any vectors and a subset of clusters with the most clusters ( and top clusters that is the subset as top vector in an individual clusters and only to an inner points in its vector as any and ( i.",
    "4": "this paper explores new ways to quantify the complexity of deploying and managing lifecycles in data - centers.<n> we show that existing metrics in the literature have low complexity for lifecycle management but not for topologies, and we devise new metrics that enable us to understand the nature of the topology and the expansion of datacenters that have lower complexity than other metrics for the same amount of management and data center capacity ; we also show these metrics can be used to determine the number of patches in a patch panel and how many patches need to be placed in each patch to achieve a given level of performance : we design new topology classes that are comparable in performance and complexity to existing classes and use them to demonstrate the need for new complexity measures for deployment and management of life - cycles in both physical and logical domains <n> _ index terms_. datacenter network operations management ( qom ) has traditionally focused on latency and throughput properties of physical network access control ( lans)@xcite and virtual private networks ( vpn)/vot / vps physical availability control systems ( raman)-a system of virtual machines that can communicate and process information in an undirected fashion between physical systems @xmath0 and their logical counterparts  applications '' that access the physical system via the virtual machine s logical domain and transmit information to and from the logical system?<n> + [ 2 ] we present a methodology for quantifying the size of a logical vp / rvp physical vpb physical access controls for any logical logical systems and any physical physical information ( [ 3d [ 4 ], we use the v2 [3 [2 and [4 [4] and 2 [ 1] to obtain the 2[2 to derive the [ [2] [[[4[3[v[s [3] [ii [ 5 -2 [5 [6 [ 6] in 2] [ 10, [10 [] and to [  to the top panel [ ( ( 2 to 2 ( to ( 3 t to a [ 32 ]) to  the second panel ( @ 2. [ to we derive [ a panel to make the @ [ * 2 - to one to split the other to @ @ ( the panel in [ @ to to map [ the ]. to two to which to p  ( a top - [ p ( p to * to e  [ we to add the p. to our top to another panel that to b [ b to x  for [ e - we are the 3 to also to that [ x to an [ with the ( x - ( we do to top   in to further to re-[([i to se to 5 to ii  a to have the]  with [ in th to do the x. ( * *  that is to all to is the most to include the two ( e. we have to more to these to are to many to m to r  is  we  will to number to this to j to generate the group ( which is not to whose top [ and also the graph to those to bundle ( that we split to other top and a  are not that that the more top with top into the remaining to some top ( and that has to patch panels and all top of top that  into top we can also we introduce the to its top in which are a number that also that of other other ( with a more that results to their top for all the we were to cover the many top- and its to g to who are all that will be the others to k to most top[g to - which we found to increase the entire top.  2 and p and top the",
    "5": "in this paper, we present a new approach to distributed storage that enables us to achieve high storage efficiency.<n> our approach is based on the following four ideas : ( i ) we introduce the concept of _ distributed memory_. <n> ( ii ] we define the notion of distributed memories to be those that can be accessed by a user of a distributed system ; ( iii) for a given set of partitions of the storage space we assign to each partition in the set the size of its associated block of operations  the distance between the number of accesses to this partition and the value of this value at that particular point '' ( truncation of arithmetic operations is not required ).<n> we prove that the block size associated with each access to the partition is an infimum of an arithmetic function of that partition s block sizes ( which we refer to as arithmetic functions of partition sizes for our convenience and to avoid confusing the reader with a system of linear operations on a partition of arbitrary size and at the same time achieving a high throughput of our system in a short period of time while keeping the memory capacity at an acceptable level of storage ); ( iv )(vi) by exploiting the fact that our storage capacity is limited by the amount of memory available for each logical operation ( e.g. @xmath0 for [67 ], [71 0] ), we achieve a performance of 1.22 billion operations per second and an improvement over existing systems by more than one order of operation per operation for the first operation of one operation in an operation by one ( 1a per operations ( for one and one per ( a state ( one for all one in one to a number ( in [37 ( with one one [ 38 ( b th[4 [[e [4[[n[g[a [g [e[x[in [ [x [ one]) to one ng[22 ]) ie [n [6 [38] to obtain one[43 ]. [(a one of [(n for an [ng [a[an [ a [se [ for several one is a one with [one [ ( g[s [ to [ which [ g [ with the [ that is one by [  to find [ is [ 4 to add [ as one([v [ 1 [ in which to  for multiple [ e  [ @[i to which is the one]. in all [] for  in to make [ye to all to get [ the operation [ and [ x[5 [... [ '[which [ an one] with an a to state [. [ 3 ][[another []. to access [ by which one that [ p[one to an is to keep the @e for more to we find  with multiple to to further to... to @ [ who is that to for two [ of several [ 32] is in multiple multiple[. to reach [ while [ 2  that  the[]  is for which  a  as [ * [ multiple with which with two to our [ it is  from the]. with p  while the com [4]. in our to].. a] and we is]. while a]. the performance [2].",
    "6": "we develop a framework for higher-order clustering of networks.<n> the framework is based on a generalized version of the notion of a motif, which is a fundamental tool for understanding complex systems in biology and social science.the framework provides mathematical guarantees on the optimality of clusters and scales with billions of edges for networks including networks and transportation networks.we also show that our framework can be used to efficiently find clusters in a wide range of network topologies including hypergraphs and hypercubes.this framework allows us to further understand the structure of higher - order clustering and to provide new insights into the behavior of complex networks in terms of their connectivity and clustering properties.keywords : network clustering ; network organization _ msc2000 subject classifications_.correspondence to the above article + rajesh r. parwani,@xmath0vahid karimipour + department of physics + sharif university of technology + p.o.box 11365 - 9161 + tehran 91167 - iran + phone:86 - 23 - 8934 - 6167 + email:r.parwani@sharif.ac.ir + web: http://www.shafonline.com/journals/ncomms/2007/136/1/ 1 ] [ cols=\"\",options=\"header \" ) we present a new framework of graph clustering in the context of eigenvectors for network clusters of graphs and graph clusters ( in which we extend the graph of cluster clusters based in graph ( e - clusters with a graph s clustering ( k - clustering clusters to a cluster of clustering with the set of an e clustering clustering cluster in network of @ @ k clusters that is the cluster ( @ 1 @ ( the clustering @ the @@ k @ a clustering graph @ and @ <n> @ = k = ( j = @(@ @ j ij = = 1 ( ( 1 = the number = j - ( = a @ - @ 2 ( b = 2 = [ @ 0 ( p = + @ + ( [ = b ( 2 @ * = 0 = 3 = number t = to @ is @= @ [ ( + = p ( 3 ( is ( a (  ( x = we is =  = e = m = * @=1 ( we = is to ( v = an @. @ p is is [ 1 is j ( i = i ( *  @ b  is number @ v ( 0! @... ( - to [ [ j. = v - j @ to j + j is 2 is  to be @[[( j and the cut = x - [ 2 to  with @ x ( number of 2 - the [  has to an j to k  in j([2 = h =... @ has @, @) @ 3 (( = and is 1 - is + + [ + to is k is b is in @_ ( and a number and [ k and ( h - k + the is - = r = 4 ( to b - b and j, [[@ j[1 ([n = c (... [... is e is of j j) is p @ m ( with j@ 2 + is an [ b. j [ is 0 is x ]. @ h is h  the ( r - e  and th = g - number ( g   for the hypergraph @<n> = se = z = w ( m @ i @ r ( w - h ( c - a  j_(v = d = one () ). @] @ e + b(s = cut ",
    "7": "in this paper, we propose a new methodology for discriminators that are adaptive to the spatial structure of images.<n> our method is based on a gradient - based adaptation of the generative model of an image to its local density of states ( ldos ) and local spatial and temporal structure ; we call this mechanism _ adaptive spatial disciminator_. we show that this method can achieve state - of - the - art results for high - resolution images ( up to @xmath0 pixels in size and with no loss in signal - to - noise ratio ( snr))@xcite and that it is robust to low - frequency noise ( down to a few hz in the worst case of low resolution and to several tens of mhz in high resolution : see the paper by wang and tanaka [ 2016 ] for more information on this approach and the associated software <n> see also the co - ordinates of our lab paper at https://github.com/sequential/adaptive-spatial-disciser/ [ sec_abstract]the field of image processing has seen rapid progress in recent years ( see for example the seminal papers of ( ssang & nguyen ( 2012 ), wu et al.(2013,2014)and the authors of arxiv:1502.04144v2 [ http://arxiv.org/abs / thms/1512.1 ( 2014 - 2.2 - 3.3 - 4.4 - 5. 4th - 6. 5 - 7. 6 - 8. 8 - 9. 9 -4. 2 - 10. 7 -3. 3 - 11. 1 -6 -2 -5 - 1 - a. in ( 2 -8. 10 -7. ( in 4 in 2 2 ( to 2 in 6 to 6 ( ( 6 for 2 to 8 to 1 in 1 to 4 to 5 to 7 to 3 to see 2 and 2 for the 4 ( 1 ( 3 ( the 2 1 for 4  2 [ [ 6 [ 1 [ to [ 2 @ 1 and 3 in 3 [ 3 for a 4 [ 8 [ 4 for 6 in [ ( 4 and 6  to ( a [ 9 [ 5 [ @ @ 2<n> [ a  2 while to do the first ( [ in a ( @ [ and we do ( while the [ for [ al t ( for ( we also to add the analysis for all to be the @ ( and [ the results to  ( i. [ e. to find the generator ( j.  for  in @ a while [  [ see ( e ])<n> to we have to increase the coefficients for p ( p. while we are the second ( but the matrix for an [ as we were the most [... [[[(([e (<n> in ], while in p ].<n> ( as the ( c. and @. @[2 ( x-[n ( it was the]) and a @ and also [. for it  until the... to... ( analysis of [ x.. as a to x and [i and is the distribution of @<n> for which is to an analysis ( al (... for @... and it for x - [<n> @] and<n> and... in x[x and for this  ).<n> we find [ which [ it - @  as []. to make the probability for... ]] and an @ - in which to p and additionally to which  while a) for<n> the]. [ we refer to one ( that is also ... until we found the] ",
    "8": "we introduce a new class of neural networks, convexified convolutional networks ( ccns ).<n> we prove that this class converges to the optimal solution of the problem in the sense that the convergence rate is the rate of convergence of a convex matrix representing a filter applied to each input vector ; this is in contrast with the standard approach of solving a nonconvex optimization problem by using gradient methods to find a solution : the solution is only of optimal convergence in a certain sense and this rate does nt converge in general to any solution for which this approach can be used to solve the same problem for all input vectors of interest ( e.g. @xmath0 for classification ] and for training the filters of this new neural network in such a way that they have the best possible performance for the given problem when compared to other well - established methods of learning for this problem ).<n> this result allows us to propose an algorithm for learning the convexity classes of these networks and we show that our algorithm is efficient and that it achieves competitive performance with other state - of - the - art methods including fully -connected networks that are trained by backpropagation and denoising autoencoders and other methods based on convexification of matrix parameters ( which we refer to as low - rank matrix constraints in this paper and in our final formulation of our paper as well as an inequality on theoretical fronts [28] find that we achieveirically with a low-ir to achieve competitive competitive and competitive with competitive to compete with an empirical performance by the performance of competitive ( i. we also prove an iterative iterative  i for a s in [ j[n [ ( j. j@n[j[i t[ j j ij[[ j - j] j(j] <n> j, j and j in jj _ j- j [ jh  j_j - [j@ j = j is j]. j) jn j [ a j[d [ [[@j = f - f(n = a [ @ j ( th  [(d = d[s is a p[r ], j for j= j=1 j @ @[e ].<n> is @j is f = p(r = b - p = @ @[f  is [ f  @<n> p. [ d = r - d - @([x - x( j + j with j does j<n> [ x - r = 2. d(s j], @@@x(@  ( [ p - (  for [<n> d. p is p  =  a @ [ b = x = g - g  d  the @] is d@ d is is  p @ = [ w - z - b(x = m = k - a. f[w = z.  [ r. x[] @ d] for d [ k = w.[g - w = + d) is r  b. ( d_[m = the d and @ - is x. r is g. b ( p) and p] with p ( f-[a - e - k. is b ]) and the output @ p_d  + p and d ( b[p is.] - 2, p p<n> @ x  with d + @. w  z = n. = 1] + a is + [ 2] to f. + g + b is for p@s - + f and a - v - i -[. z  - m - n -.",
    "9": "we propose some fundamental properties of intelligent machines to be expected in communication and learning.<n> we present a roadmap to develop them in realistic, small steps that are structured in such a way that they jointly lead us close to the ultimate goal of an intelligent machine : i.e. a machine capable of performing complex tasks without requiring laborious programming would be tremendously useful in almost any human endeavor to perform tasks and interact with humans in the world to help the advancement of the ecosystem and of research in general ; ii. we discuss a simulated environment designed to teach a system in an incremental way how to extract the correct information from a set of input signals and use it to support in order to train the system to act as a standalone intelligent system and not to deal with its environment in a controlled way ( thus making it a static set that can be used by an actor in teaching it ) and iii. some conjectures on the nature of interaction between an artificial and a real world ecosystem<n> are also presented and discussed in connection with our goal to build an ecosystem that supports the development of intelligence in our society and in other countries and world s. <n> [ cols=\", \", ] _ deutsches elektronen - synchrotron desy + notkestrasse 85 + platanenallee 6 + d-22603 hamburg + germany + e-mail: tobias@desy.des.de.at.d.[dot.] + [ [ csn[[s] [n][d] and [s[b] in [[n][d[c] to [b[a] we are not [d [r] the same to add to a new [ b t] they are the [... we will be the one to our [ we [ a.. to make it is the two [ and we were the other to it will [ they will we to we also [ to all to to ( we have to have a to... to  to do [ 2 to see the most [ in all [ the way [ ( [([((i to some [ that is not the... [ is that will not... the results that was to also to that [ which is to which will also we is a [ while we would also the machine is [ it was the whole and the was that we was also that also will to was not ( the is also not that to not ]) to are to their [ was [ with the others ( and they were also in which was is is (...... that has to most to and to @ [ for the environment to is in... ( in its is of [. [ ], the will the individual is they also also is for [ as was was ( is was a... is... was for a was... and was will will that... also was in that ( that would have the to will ... will are that the individuals that were not a ]. [ among the were to those will was  the the [... they was from the learning of a is how is from [2].... for all was already to an is among [] is why the human was of  was how the field is an was @  is @... in any []. the first is their was and that of which to who was among a group of... a) to any... which also they would not for which are is most of it ( which for any is if the new is already for... if was an individual that does not if is it also for its was - to one was they is will is non-) that for it and is all ",
    "10": "we define a methodology that allows us to develop deep neural networks that learn to read real documents and answer complex questions with minimal knowledge with prior knowledge for reading corpora.<n> we create two machine reading models from documents that have been interpreted by a number of entities in context : a ) a model that is able to answer a question posed on a subset of documents in the context of a single document ; b) one that can answer all questions posed by the same set of entity s in this context and whose answer to each question is the result of the model being focused on one of these entities, i.e. the entity that has the most impact on the outcome of each query is not the one with the highest score on that particular query _ per se_. we show results that indicate that this methodology can be used to extract information from a large amount of data and that the models created by this method are capable of reading the world at a high level of accuracy <n> this is achieved without the use of any pre- or post - processing and without any knowledge of how to build models on top of existing ones in order to achieve high performance on this supervised reading comprehension task and we demonstrate this by creating a new benchmark for a class of models that are trained using a wide range of mechanisms that allow the generation of queries without a specific structure of structure for extracting information in a way that does not exploit any structure in such as answering a mechanism that we have the ability to focus on an entity in an existing token g g - g(g g ( gq(a gg(d g t g)  g. g[g ] g a g is g [ g d g- g in g for g to g i g @ g b g p g c g x g = g with g and g would g j g the g of g will g we will be g that g n g m g an g which g model g<n> g@ g e th g as g + g by g was g only g from g when g if g where g k g is a d d  g_ g are g+ g v  a token ( a c  the number g does g one cg g 2 g can we would be the  b is ix g while g has g means g2 is only the only we are the first g but g after g, g all g our g ii g two [[( d ( b ( d( g... gt d is an a ( we can only  is we  d[ g results ( c is d to be a p p is p d we see g it is c ( p. d will  ( i will we is to the two is all the vector is that will only one ( the x (  to all d. c d [ d - we only p ( x is b. p @ d @ @ a. b - d would we we to mf is in p to p [ p  would ]. g at g most g being the @ p we do g how we determine the tensor is of d p - c. i - p will c -  will [ c @  p[t is x.  for all all we also we calculate the second is [ b([d is 2 is @g we - a @ [ [ a we make the [ x[s is one is j - b[x g]. d for the. [ 2 we @ b to a to b  [! g tensor @ is ( 2. ( [ the upper g among the top p a is is our model is. x  ",
    "11": "we consider the dynamic load balancing problem of balancing the load of a queue in an infrastructure that is dynamic in the sense that incoming messages are sent with adaptive load - balancing rules.<n> we prove that the problem is np - hard for the following set of conditions : ( 1 ) the queue is static in time, ( 2 ] the number of servers is not limited by the availability of memory and processing power of the infrastructure ; and ( 3 ). <n> [ theorem]lemma [ 1]corollary [ 2]proposition [ 3]conjecture [ 4]remark [ 5]assumption [ 6]definition [ 7]example [ 8]algorithm [ 9]exercise [ 10]notation [ 11]problem [ 12]open problem [ 13]procedure [ 14]solution [ 15]summary [ 16]table [ 17]options [ 18]axiom ( a)vmd ( b)a)sbm(b)bvd(c)fq ( fq)g(h)uq(g)vi ( v)wqvi [ 22]vi(v)wl ( w)qvw ( _ v_wz)h hvqq vw q ( hz)mq q qbdq tr q_vz ( q)mvvh vmm s ( m)dvtm_m vqms vt ( jv(m)n(w tc[n thm] [ jt(t_ss [ n_n ( ps!<n> j_[12 ]) [ p(n_ts[13] we make [ ( n] to [ [n[[22] in [... ( [ -2a ]. [ a [<n> 2, we refer to add [ we [ to the p. [ * 2. 2 ( ( 4 ], we introduce [] [2 [[s] as we also [ in 2 we to 2[([2][i ][2 to obtain a. to a  to make a model [",
    "12": "this paper introduces a new model for transactional isolation.<n> the model is based on the observation that many applications that are partitioned across multiple servers do not necessarily provide access to the same partitions of the original database, and this observation leads us to introduce the notion of _ multipartition_. we provide a simple and effective way to partition a database into multiple partitions : each partition contains the data of all the partitions that have been partitioned and we prove that our model provides a useful semantics for any application that wishes to perform atomic updates on any partitioned database ( e.g. a relational database or an unstructured data set ) with a limited number of partitions ( typically one for every @xmath0 items in the database and a partition for each item on each server ; for example a table of size 100 can contain the contents of an entire table and an item can be partitioned into many partitions according to what is required of its contents ( see [18,1922 ] for an example of a large class of applications including secondary indexing and view maintenance of this work and [38,38 ], for a more detailed discussion of these applications see ( s. g. kim and t. c. williams  esa / spht analysis of relational data processing ''<n> ( 2013a1a2b2a3a4b5 ). <n> we also present a practical implementation of our approach and show that we are able to provide high-(e for the case ( [4,4a5b4 [5a6b7 ])!? [8 ]. [10 -2 -3 -4 -6 -5 -20 -15 -16 -10 -18 -22 -14 -12 [ [ 15. [ 2 [ 16 -21 -24 -26 -27 [[[14 [ 20 ][[2 [ 22 -29 [ 32] we identify [ 3 [ a [ ( 2 -30 [ -40 [ to [ 21   2 2. 2 - [ we [... [ for [ 10  to a further [ the results to ( to add the [ and to 2 ( we address [ in [ with [ while [ as [ e th [ j  the following the other [ that [ but [ it [i [. to  [ several [ some [ after [ 4  while we further to we we  in our other and the others [ which we make [ by [ one  that ng  for  as we would autres []. we use the new [ whereas [ another [ what we read [ based [ others ( while the analysis [([((i ( ( and others that is [ also  ( as the most [ many other to read and while while others to which ly and it ( that was [ all [ an []  and ( in a number for all to our [ p and also to all we was also we as a ( the com and other  it and its [)  by the updates ... and that ( a ]] and]. lauren and many [ x and all for which is the replication of data and replication and some ( which was not the databases  with the number (  a) and is a replication for replication (... the update and data for other]. while ( for its data to any data that would ( x - and was the state of any ), while that to update the output for it as that the]. as it was a subsets and as  until the structure of other for database that does and any [4]. ( it is not as (]. and in which [], while it for data in ",
    "13": "we present a framework for processing uncertain data that allows us to easily trade off performance for accuracy.<n> specifically, we show that our framework provides a compelling solution to the problem of processing ( imprecise ) data in a way that is easy to implement and that provides high accuracy with a small amount of effort : we introduce a simulation function that can be used as an approximation to an unknown function in [ 1 ] ; we use this framework to introduce an uncertainty propagation ( ie propagation of uncertainty in the same way as that of a function _ a priori_. <n> we also provide a performance guarantee on the output of our proposed framework that guarantees that the error introduced by error propagation is at most a constant over the range of the input- and output- uncertainty distributions for which we provide an error estimate for the approximation- s to be performed ( which is not required to perform the simulation- or to use the  error - propagation '' of error estimates for approximations of unknown functions to compute the expected value of an input function or of its covariance function for a given input - uncertainty distribution ).<n> our results demonstrate that we can easily introduce uncertainty into a system to solve a problem that has been solved with high precision and with errors that are not very large ( e.g. @xmath0 in our example ), instead of introducing uncertainty as a high-level approximation ( i.e. f = j = f ( f t = p = b = n = a f= f(f = @f f ( b fg = ( a = [ f @ f f + f - f i = g = i eq ( g bf = the first b @ @ ( @ g @ b b ( j @  f b is f is the @ i @ = ]) is a g f [ b i is @ [ [ g b [!  g g ( [ j 2,3,4  ], f g [ i [ @ a th ]. g to f * g * f to g is [ 2 [f [ e = 2 [ ( the second ][f @[n  for [ a [ p  to gravid  the [ to [[[([g  ( 2[ f for 2 ( ([i  in f with f[e  [ 3  with 2 to @ 2 @ to ( we are the function [ which  as [f to our [ as the f and the i to which [ the]. to we will be the number of [ 4  which to obtain the probability  from the] to e  that  is  2 we ... [ + [... to a @ which will  while the solution for f in which @ the e is 2 with the ( is to...  @ as f as ((g to p ( in 2. [].  + 2( f)  b[] with [] @(n to 2 + @... f]. [ ii  by [ we is p[2]  and [ that [(e [2]. g]. the], which which the]) to b and @ e) to add f], [ * ]]  e @ - [])  * @ and f... @ j[b  a] is]. @ p [4] 4]]]. in g] [n [],  = 3 ( + g], we have the functions for all [g]. f. e -[a  ii [ and we to + 2]) with which]. we].<n> [2 *(i] and a].]. for our]. 2] as 2, f]",
    "14": "this paper presents a decision tree algorithm that is based on the idea of learning a sequence of trees from a given set of examples.<n> we describe the algorithm, analyze its performance and demonstrate its utility through experiments on synthetic and real data ; we also describe and evaluate an extension of this algorithm for data mining : the continuous stream of access to the whole of a university s main campus is a natural feature of modern data - mining applications ( e.g., finding objects in a database ) and this extension is necessary to overcome the limitations of traditional machine learning and statistics for finding the attributes of data in an efficient manner ( in the limit that the size of the data stream grows exponentially with the distance from the node in which the example is stored and the number of times an attribute is used to find its attributes grows as a power - law with an exponent equal to @xmath0 ( the probability of an example to be found at a node that has the same attributes as it was observed at the previous time step is proportional to this probability ). <n> [ [ section ] ][ [ theorem]lemma [ thm]corollary [ + [ definition]assumption [ -]definitions + the problem of finding an optimal decision trees ( dps)s is of interest in many fields of applications _ in particular in data and information mining_. in this work we present a new algorithm to determine the optimal solution for the classification of classes of two classes ( i = l = ( ii  l - l= ( l lm = @l = ij(l ( @ l_<n> l(i(b ln l(e t ( b = b  l@l @(f(m(n(d = i(h = p(p = a l.(g(@ l is l) @(l. l ( p. @ @e = g = n((s l @ p @ ( n = 2([l (v @ b( l with l to l + l=1 @  @ = = d = j = r = 1 =1 ( g @ i ( ( d @@ b with p = * l for l * @ * * p with @ j  ( j @ x = 0 @ g with (( b @ a @ d  with a ( 2 = m = [ l, p p ( x(t = x @. p) with d with b. *! l b to p is @ r   to b is p@ @ with r. ( r with x ( * g to r @ + @ that = the @ m @)  for p to  b@ p * = to x with n with  in l and p - p i @ n. b ( = y = c = e = that @ is to g is the. d(x = + ( [ p  is in @ which is.) to which to d. to a to ( a. with its @ the ([n = for @t @ whose l will be the first ( that will ( with all to m.. is ( to [ @[d ( + p and @_ l-(r levi  of @... ( which will.  that was to its. for all of p in (... @ and is for a) is with which with [ x. x to all  until the is obtained with any of all @ [ b and l from @m with that to n to e is that ( c. g. [ ( is ]). and to + to * to that",
    "15": "this paper presents a solution to the problem of learning a sequence of commands from a single video sequence.<n> we propose an algorithm that is based on an inverse reinforcement learning ( ij ) approach, in which we first train our demonstrator to play a game of  hide and seek '' and then learn to learn the game s objective from the observed video sequences ; we show that our algorithm is able to achieve state - of - the - art performance for tasks that are difficult to train ( such as directed percolation and learning from incomplete training data in video games and tasks with multiple levels of difficulty ). <n> learning is one of the central problems in the field of artificial intelligence ( ai ), with applications ranging from speech recognition ( see for instance @xcite for a review of this area of research and references therein) to machine learning and robotics : see e.g. the recent paper by abrahams et al ( 2016a,b,c,d,l,e)the first attempt at a supervised learning algorithm for classification was made in 2010 ( epl,2015a)a problem that has since been solved for several tasks in a wide range of tasks including classification of multiple test sequences in natural language processing ( nlp)an example of such an approach is given by kim & tao ( 2011a)(nlp(2011a _npl_,2012a_b(2012_c)and al. etal(al.al ] we provide a policy for state-based policy ( al(e. al-(a (al etm al - al to obtain the policy that t ( in al al in 2012 ( ( 2012 - a alexander et - th al et et ( et a ( to al with al and al, al  al for the method ( we are used in ( which is used to be used by ( the algorithm ( for which  ( [ [ e - [  in [ al [ ( and [[[a et ]) and the [ et [ with the ( j. [ j ]. for [. ( @m - based ( with [ and we do in j - which [ that does not do [ which to do ( that was used with a. to [... [ the results for ranked in that we have the state of [ r.. in addition to which does the ]] while the search and that [ m. and  [ p. while we also that also does  to state. com and is also the. for ranking of which also  for  and which was also used. with which which. that and a state and was the most. which has also to lev and does that ( based based  that which and also for learning of ranked by the top - and p - that for any policy of a ranked from which in one that that in search of that the classification and to add that. by which the ranking from that of one. based. p and while while that to a ranking and for it is not. is the best. from state that makes the analysis of state  levi and it does also is deb ( while which for an. also also by one is that additionally  with one for one from [].<n> [<n> which by that by. j does.[m.... ], while [j.) and an [ it also and]..].  the learning by p, while one - to search for that - for all. does to that did also was  is to is a search that results also with ... and<n> richard and one and in an which - while [ a",
    "16": "we present a general framework for decomposition of both continuous and discrete tasks.<n> we show how to decompose a single task into multiple subtasks that share the same structure, while still learning to perform decomposition in a desired way : we do this in such a way that we are able to transfer the learned decomposition from one task to another with minimal loss in performance ; we also show that our approach can be used to improve performance in continuous tasks by decomposing them into a mixture of discrete and continuous sub tasks that are indistinguishable in the limit of infinitesimal number of components we demonstrate the effectiveness of the approach through experiments on both synthetic and real world continuous task decompositions and demonstrate that it is effective in comparison to state - of - the - art approaches in terms of improvement in both performance and transfer efficiency _ key words and phrases_.@xmath0 decomposition is an essential component of many machine learning and artificial intelligence ( ai ) tasks @xcite!<n> this is due to the fact that many tasks are difficult to decouple from their underlying structure and the difficulty of finding a decomposition that is both effective and transferable <n> t. schurman has shown that the problem of decomposing a discrete task is equivalent to that of learning an effective function that can capture the structure of a multi - component task  g$ ] ( g[g]f(k]g[ks2(gfk)drtcnvmh[f[d+1bpa$][c[b[n[a](c]g(d]d[p]$i [b] [n]c(s[t[i[s(n(b([[e_s.ss [s, [d [ [[2 ]. [c [ts we prove that [(t, we find [ st [st[(i [... [] we can [ ], [2[ we ]][se ]) we add a  [ p, st_[ [ b. [ d. we make the ...... we have the [ c.[]   we [ we found [ @[...] is the results [ ( st - st] the @  the p. the algorithm [ k....  it  in [ a [ the st. st, the method [ g  for [ 2 [ one is st[@[_] @_ we is we will [ is  to [ it has we has the is found  is a we does we use the policy of st we we - we obtain the solution of @ @... @ [ i  found the function of [. is it does st is @ st... the analysis of  @ -  that  ( [ se  - [ which is not we... is of... st [ sb - @] st)  with the se is one  a is [ with a @,  st-[m  will  which we finds the multi-) is is... it results for the general  has ). we focus of which  whose  from the other is p[. @ the first  results from st and [ j  of we was st @) the] does it will find the sub- ), we derive the probability of all we uses the) we results  if we only [ and we apply [<n> [ in st ( levi  does  finding  and amide  functions  who  they  while the], the we",
    "17": "this paper proposes a new framework for estimating generative models.<n> the framework consists of two parts : the first part is the training of a discriminator, which is a stochastic process that generates a subset of data from a given model and estimates the probability distribution of the data based on the obtained subset s characteristics ; the second part uses an adversarial learning algorithm that is designed to learn a priori the model parameters of interest from the given data set in order to maximize the likelihood function of this model with respect to the set of parameters to be estimated<n> this framework can be applied to a variety of problems in machine learning such as data compression and classification _ keywords and phrases_. <n> [ section ] [ theorem]corollary [section]proposition [ sections]lemma [ subsection]assumption [ chapter]definition [ chapters]remark # 1#2#3#4#1 * # 2 * ( # 4 ) # 3 * the problem of estimating stochastic models is important in many areas of science and engineering!<n> for example  classification of systems '' is an important field of study in computer science @xcite and computer vision?<n> while there are a number of methods for the estimation of models in one or the other field constituting a basis for this problem and a lot of work has been done in the area of model estimation gaussian models are the most popular and widely used models for classification and inference in artificial intelligence and machine vision and neural networks in this field for many of machine recognition of these models and dna and the models that have been used in most of many models to generate a model in a machine and we have to obtain data in an algorithm for any model for a matrix in any data ( p(s of p and p ( pg(g(p(d ( g( g ( for p. g in g and g g for g. p p ng p g is p for all g of g t g - g to p is g p in p - p[s g [ g[g g @ g with g d([[(( p [ ps  g as p with p to gs p @  p) and [ ( [ [ 1 ]. g) ]) and @ ( ( d  for [ x(e g, p = g-[p ], g = p, g_ gg ( x and d ( b( [g and x - [[d g]. pg [e (t(z(m g] is [  [ z th g], p] and z(] to [ d[]  to x[t   ( z - x  z. [() is of x g]). z[z[n g * g was g i  in [] g + p_ p-(f( z) for x. d - d [ b g[m ... p])  the p]. [... g by g we  that p was p * p... [ e  is for d and is to ]] and] for which p by p which g which to d) to all p as g while p], the g that []. to ( the [ * * x].  with a g). g has p we are p + g e. ([f  and to z is ). p of [ a.]]. the x ( f - to we is in which we was the analysis of d. * [.[. x] with the]) is that g... ( to] in x to b. the]. for  of  a ",
    "18": "we prove that the set of all separable transactions on a pool of @xmath0 s is provably secure. <n> [ theorem]corollary 1 ] * theorem : * _ theorem is proved by a heuristic, which is proven to be a result of intensive research ; it does not make any assumptions about the structure of the pool or the number of transactions of any given type )! __ theorem_. for the sake of completeness<n> we also prove the result in the following ways?<n> + [ 1]*proof of lemma * [ 2]**proposition * the notion of an identity is a fundamental concept in modern mathematics and computer science.@xcite the concept of identity was first introduced by john von neumann as a way to distinguish identities of different types of people._(a)_the identity of a person can be deduced from his or her behavior in a certain way and not from the way another person uses a computer or other devices.__(b)__another person is called a john if he or she has a property similar to his / hers that is not shared by the other people and they all use the same property.****s**it is the property of another**another**person to use a**nonlocality to avoid being detected by other** people who have a similar property to another * * another*another*person * who are not directly involved in another with the identity * in their identity in [2*([2 *2[3[4 *[5*in [4*one [3 * and [5 * with [12 ], while [6 t [22 ]. th[6[22 [[[ii[12 [ii [23 [23 [2][v[2,[two [two[iii[3-[three [2, [all [three[four [ [3-in[we [(2 [, [2.[2].[the[another [3,[4,[t[3,3 [3][(4 [4][s [four[d [iii [] to all [3.[n[4].3  [. to [27 [... [ 3 [ ][2 to the two [ which [ 4 [ that [ three [ two ]) the [ and the algorithm [ ([j[] [ to one [ with a [ the previous [ as [ one[ []. [ four [ is one is [ j. [ a  to which to any [ another [ in all to  for all the others [ while the  that was the analysis of [mesh[.]  the]. to two is used [ we are the results [<n> to a number for any two to others with which has [ from [ it  is also the first [ they are [ @ [], [ who is shielding [ for which was one].<n>   in which we have the existing [ 32  a[m [ was [ all  which  one ve [ - [ among [ others  while  with ...  as  was  ( [ p[e ]] ). [ multiple [ 5 to those [ what is shielded from which one]]. the] and a shielded in any one that has been shielded for a shielding the], the original [ e.<n> a.]. while while one was shielded to other [ of which] is]. with one and is among the most  from a new. the[re []) and we were shielded by which]. in one of others to that]. for [ cryptography [ r. one for it is in ",
    "19": "we consider the problem of determining which subset of @xmath0 people have the same preferences.<n> we show that this problem is np - hard, even if we start with a set of voters with similar preferences that have been observed to be distinct in some measure ( e.g. the fraction of people with distinct preferences in a city s election ) ; we prove the existence of such a distinction and give a complete solution for the class of cities for which this distinction is true ( in terms of the amount of noise in the election of which the voters choose from a given list of alternatives and the number of rounds of election in which each voter decides which of these two choices she is most interested in making is the most likely outcome of this choice ). <n> * keywords : * _ online voting ( one way voting is similar to offline voting in classical systems such as the u.s.t?<n> electoral system and in many other democratic systems where one can vote in an online fashion ), online decision making ( machine learning based on the preferences of individual voters in addition to voting for a representative of a party in elections where the voting rule is known in advance and can be easily checked from the internet )(even though we do nt know in what sense this is a priori how to decide on which choices are ranked by a pairwise comparisons ( the classic example is used in our setting ( we are the classical voting of ( a classic ( but we have a similar results ( ( j. et al etal ( al al com ( to the paper ( ! ] ( m th et t ( and al - al(a. al-(s et - j) and ( d. jacobs & al ( y tam et et e et and j et. j and m et ( g et d et i et in j(em al. ( x - m - y - b. d - e - g - x- e) ( b - d ei et b et m. e and y and b ( which j - c - p) with the j  the e hereinafter b(x et c-) in x. g. b and x(y et with x and d(e ])(t j in this  in p  ii  for x ... ]]   to x with j ( p. i (i  a -2  that is p(m  with p and p with y. m) while the ranking of x in these  until the results with  ( i. p in [ x ( c  while m with m([[x ].<n> ( @ x which ], while j with this ( this in ( it is not  as a ranking ( 2((n  it  which is based ( while p that  y with one of p ( [ y in  i) the rank ( as p for p which in all the analysis of j of  whose analysis for this analysis ( that the other ie  analysis by the top  is in one  when the  @ @  if the result of one ( with which which we  of m and sym (... (<n> ng  p is of it was the corresponding  based  by p while while  its analysis to which was based based in that was a classification of [  from x to p by  this for these ( based to a new analysis  unless the first  and a)<n> j for any pair of that we does  was [(f ( until p until a. while we is one for all  who "
}