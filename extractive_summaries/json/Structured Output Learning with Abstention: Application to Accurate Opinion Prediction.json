{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Up until recent years, opinion analysis in reviews has been commonly handled as a supervised polarity (positive vs. negative) classification problem. However, understanding the grounds on which an opinion is formed is of highest interest for decision makers. Aligned with this goal, the emerging field of aspect-based sentiment analysis (Pontiki\n1LTCI, Telecom ParisTech, Paris, France. Correspondence to: Alexandre Garcia <algarcia@enst.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\net al., 2016) has evolved towards a more involved machine learning task where opinions are considered to be structured objects—typically hierarchical structures linking polarities to aspects and relying on different units of analysis (i.e. sentence-level and review-level) as in (Marcheggiani et al., 2014). While this problem has attracted a growing attention from the structured output prediction community, it has also raised an unprecedented challenge: the human interpretation of opinions expressed in the reviews is subjective and the opinion aspects and their related polarities are sometimes expressed in an ambiguous way and difficult to annotate (Clavel & Callejas, 2016; Marcheggiani et al., 2014). In this context, the prediction error should be flexible and should integrate this subjectivity so that, for example, mistakes on one aspect do not interfere with the prediction of polarity.\nIn order to address this issue, we propose a novel framework called Structured Output Learning with Abstention (SOLA) which allows for abstaining from predicting parts of the structure, so as to avoid providing erroneous insights about the object to be predicted, therefore increasing reliability. The new approach extends the principles of learning with abstention recently introduced for binary classification (Cortes et al., 2016) and generalizes surrogate least-square loss approaches to Structured Output Prediction recently studied in (Brouard et al., 2016; Ciliberto et al., 2016; Osokin et al., 2017). The main novelty comes from the introduction of an asymmetric loss, based on embeddings of desired outputs and outputs predicted with abstention in the same space. Interestingly, similarly to the case of Output Kernel Regression (Brouard et al., 2016) and appropriate inner productbased losses (Ciliberto et al., 2016), the approach relies on a simple surrogate formulation, namely a least-squares formulation followed by the resolution of a new pre-image problem. The paper is organized as follows. Section 2 introduces the problem to solve and the novel framework, SOLA. Section 3 provides statistical guarantees about the excess risk in the framework of Least Squares Surrogate Loss while section 4 is devoted to the pre-image developed for hierarchical output structures. Section 5 presents the numerical experiments and Section 6 draws a conclusion."
  }, {
    "heading": "2. Structured Output Labeling with Abstention",
    "text": "Let X be the input sample space. We assume a target graph structure of interest, G = (V = {ν1, . . . , νd}, E : V × V → {0, 1}) where V is the set of vertices and E is the edge relationship between vertices. A legal labeling or assignment of G is a d-dimensional binary vector, y ∈ {0, 1}d, that also satisfies some properties induced by the graph structure, i.e. by E . We call Y the subset of {0, 1}d that contains all possible legal labelings of G. Given G, the goal of Structured Output Labeling is to learn a function f : X → Y that predicts a legal labeling ŷ given some input x. Let us emphasize that x does not necessarily share the same structure G with the outputs objects. For instance, in Supervised Opinion Analysis, the inputs are reviews in natural language described by a sequence of feature vectors, each of them representing a sentence. Extending Supervised Classification with Abstention (Cortes et al., 2016), Structured Output Learning with Abstention aims at learning a pair of functions (h, r) from X to Y H,R ⊂ {0, 1}d × {0, 1}d composed of a predictor h that predicts the label of each component of the structure and an abstention function r that determines on which components of the structure G to abstain from predicting a label. If we note Y? ⊂ {0, 1, a}d, the set of legal labelings with abstention where a denotes the abstention label, then the abstention-aware predictive model fh,r : X → Y? is defined from h and r as follows:\nfh,r(x)T = [fh,r1 (x), . . . , f h,r d (x)],\nfh,ri (x) = 1h(x)i=11r(x)i=1 + a1r(x)i=0. (1)\nNow, assuming we have a random variable (X,Y ) taking its values in X × Y and distributed according to a probability distribution D. Learning the predictive model raises the issue of designing an appropriate abstention-aware loss function to define a learning problem as a risk minimization task. Given the relationship in Eq. (1), a risk on fh,r can be converted into a risk on the pair (h, r) using an abstentionaware loss ∆a : YH,R × Y → R+:\nR(h, r) = Ex,y∼D ∆a(h(x), r(x), y). (2)\nIn this paper, we propose a family of abstention-aware losses that both generalizes the abstention-aware loss in the binary classification case (see (Cortes et al., 2016)) and extends the scope of hierarchical losses previously proposed by (CesaBianchi et al., 2006) for Hierarchical Output Labeling tasks. An abstention-aware loss is required to deal asymmetrically with observed labels which are supposed to be complete and predicted labels which may be incomplete due to partial abstention. We thus propose the following general form for the ∆a function:\n∆a(h(x), r(x), y) = 〈ψwa(y), Cψa(h(x), r(x))〉, (3)\nrelying on a bounded linear operator (a rectangular matrix) C : Rp → Rq and two bounded feature maps: ψa : YH,R → Rp devoted to outputs with abstention and ψwa : Y → Rq , devoted to outputs without abstention. The three ingredients of the loss ∆a must enable the loss to be non negative. This is the case for the following examples.\nIn Binary classification with abstention, we have Y = {0, 1} and the abstention-aware loss ∆bina is defined by :\n∆bina (h(x), r(x), y) =  1 if y 6= h(x) and r(x) = 1 0 if y = h(x) and r(x) = 1 c if r(x) = 0 ,\nwhere c ∈ [0, 0.5] is the rejection cost; with r(x) = 0, in case of abstention and 1, otherwise. This can be written with the corresponding functions ψwa and ψa defined as:\nψwa(y) =\n( y\n1− y\n) , C = ( 0 1 c 1 0 c ) ,\nψa(h(x), r(x)) =  h(x)r(x)(1− h(x))r(x) 1− r(x)  . H-loss (hierarchical loss): now we assume that the target structure G is a hierarchical binary tree. Then, E is now the set of directed edges, reflecting a parent relationship among nodes (each node except the root has one parent). Regarding the labeling, we impose the following property : if an oriented pair (νi, νj) ∈ E , then yi ≥ yj , meaning that a child node cannot be greater that his parent node. The Hloss (Cesa-Bianchi et al., 2006) which measures the length of the common path from the root to the leaves between these assignments is defined as follows:\n∆H(h(x), y) = d∑ i=1 ci1h(x)i 6=yi1h(x)p(i)=yp(i) ,\nwhere p(i) is the index of the parent of i according to the set of edges E , and ci is a set of positive constants nonincreasing on paths from the root to the leaves.\nSuch a loss can be rewritten under the form: ∆H(h(x), y) = 〈ψwa(y), Cψwa(h(x))〉\nψwa(z) = ( z Gz ) , C = ( −2diag(c) diag(c) diag(c) 0 ) ,\nG is the adjacency matrix of the underlying binary tree structure and c the vector of weights defined above. The case of the Hamming loss can also be recovered by choosing:\nψwa(y) =\n( y\n1− y\n) , ψa(h(x), r(x)) = ( 1− h(x) h(x) ) ,\nC = I2d,\nwhere I2d is the 2d identity matrix.\nAbstention-aware H-loss (Ha-loss): By mixing the H-loss and the abstention-aware binary classification loss, we get the novel Ha-loss which we define as follows:\n∆Ha(h(x), r(x), y) = d∑ i=1 cAi1{fh,ri =a,f h,r p(i)\n=yp(i)}︸ ︷︷ ︸ abstention cost\n(4)\n+ cAci1{fh,ri 6=yi,f h,r p(i) =a}︸ ︷︷ ︸ abstention regret + ci1{fh,ri 6=yi,f h,r p(i) =yp(i),a 6=fh,ri }︸ ︷︷ ︸ misclassification cost ,\nwhere cAi and cAci can be chosen as constants or be function of the predictions. Thus, we have designed this loss so it is adapted to hierarchies where some nodes are known to be hard to predict whereas their children are easy to predict. In this case, the abstention choice can be used at a particular node to pay the cost cA for predicting its child. If this prediction is still a mistake, the price cAci is additionally paid and acts as a regret cost penalizing the unnecessary abstention chosen at the parent. Acting on cA and cAc provides a way to control the number of abstentions not only through the risk taken by predicting a given node but also its children. For sake of space, the dot product representation with ψwa and ψa of this loss is detailed in the supplementary material."
  }, {
    "heading": "2.1. Empirical risk minimization for SOLA",
    "text": "The goal of SOLA is to learn a pair (h, r) from a i.i.d. (training) sample drawn from a probability distribution D that minimizes the true risk:\nR(h, r) = Ex,y∼D ∆a(h(x), r(x), y), = Ex,y∼D 〈ψwa(y), Cψa(h(x), r(x))〉.\nWe notice that this risk can be rewritten as an expected valued over the input variables only:\nR(h, r) = Ex 〈Ey|xψwa(y), Cψa(h(x), r(x))〉.\nThis pleads for considering the following surrogate problem:\n• Step 1: we define g∗(x) = Ey|xψwa(y) = ming∈(X→Rq) Ex,y‖ψwa(y)− g(x)‖2︸ ︷︷ ︸\nsurrogate risk\n. g∗ is then the\nminimizer of a square surrogate risk.\n• Step 2: we solve the following pre-image or decoding problem:\n(ĥ(x), r̂(x)) = arg min (yh,yr)∈YH,R\n〈g∗(x), Cψa(yh, yr)〉,\nSolving directly the problem above raises some difficulties:\n• In practice, as usual, we do not know the expected value of ψwa(y) conditioned on x: Ey|xψwa(y) needs to be estimated from the training sample {(xi, yi), i = 1, . . . , n}. This simple regression problem is referred to as the learning step and will be solved in the next subsection.\n• The complexity of the arg min problem will depend on some properties of ψa. We will refer to this problem as the pre-image and show how to solve it practically at a later stage.\nThese pitfalls, common to all structured output learning problems, can be overcome by substituting a surrogate loss to the target loss and proceeding in two steps:\n1. Solve the surrogate penalized empirical problem (learning phase):\nmin g\n1\nn n∑ i=1 ‖ψwa(yi)− g(xi)‖2 + λΩ(g), (5)\nwhere Ω is a penalty function and λ a positive parameter. Thus, get a minimizer ĝ which is an estimate of Ey|xψwa(y).\n2. Solve the pre-image or decoding problem:\n(ĥ(x), r̂(x)) =\narg min (h(x),r(x))∈YH,R\n〈ĝ(x), Cψa(h(x), r(x))〉. (6)"
  }, {
    "heading": "2.2. Estimation of the conditional density Ey|xψwa(y) from training data",
    "text": "We choose to solve this problem inH ⊂ F(X ,Rq), a vectorvalued Reproducing Kernel Hilbert Space associated to an operator-valued kernel K : X × X → L(Rq). For the sake of simplicity, K is chosen as a decomposable operatorvalued kernel with identity: K(x, x′) = Ik(x, x′) where k is a positive definite kernel on X and I is the q × q identity matrix. The penalty is chosen as Ω(g) = ‖g‖2H. This choice leads to the ridge regression problem:\narg min g∈H n∑ i=1 ‖g(xi)− ψwa(yi)‖2 + λ‖g‖2H, (7)\nthat admits a unique and well known closed-form solution (Micchelli & Pontil, 2005; Brouard et al., 2016).\nAs ĝ(x) is only needed at the prediction stage, within the preimage to solve, it is important to emphasize the dependency of ĝ(x) on the feature vectors ψwa(yi):\nĝ(x) = n∑ i=1 αi(x)ψwa(yi), (8)\nwhere α(x) is the following vector:\nα(x) = Kx(K + λIqn) −1, (9)\nwhere Kx = [K(x, x1), . . . ,K(x, xn)]. K is the qn × qn block matrix such that Ki,j = K(xi, xj) and Iqn is the identity matrix of the same size. αi(x) is the block i of α(x)."
  }, {
    "heading": "3. Learning guarantee for structured losses with abstention",
    "text": "In this section, we give some statistical guarantees when learning predictors in the framework previously described. To this end, we build on recent results in the framework of Least Squares Loss Surrogate (Ciliberto et al., 2016) that are extended to abstention-aware prediction.\nTheorem 1. Given the definition of ∆a in (3), let us denote (h, r), the pair of predictor and reject functions associated to the estimate ĝ obtained by solving the learning problem stated in Eq. (7):\n(h(x), r(x)) = arg min (yh,yr)∈YH,R\n〈Cψa(yh, yr), ĝ(x)〉.\nIts true risk with respect to ∆a writes as:\nR(h, r) = Ex〈Cψa(h(x), r(x)),Ey|xψwa(y)〉.\nThe optimal predictor (h∗, r∗) is defined as:\n(h∗(x), r∗(x)) = arg min (yh,yr)∈YH,R 〈Cψa(yh, yr),Ey|xψwa(y)〉.\nThe excess risk of an abstention aware predictor (h, r): R(h, r)−R(h?, r?) is linked to the estimation error of the conditional density Ey|xψwa(y) by the following inequality:\nR(h, r)−R(h?, r?) ≤ 2cl √ L(ĝ)− L(Ey|xψwa(y)),\n(10) where L(g) = Ex,y‖ψwa(y) − g(x)‖2, and cl = ‖C‖maxyh,yr∈YH,R ‖ψa(yh, yr)‖Rp .\nThe full proof is given in the Supplements. Close to the one in (Ciliberto et al., 2016), it is extended by taking the sup of the norm of ψa over YH,R. Moreover when the problem (7) is solved by Kernel Ridge Regression, (Ciliberto et al., 2016) have shown the universal consistency and have obtained a generalization bound that still holds in our case since it relies on the result of Theorem 1 only. As a consequence the excess risk of predictors built in the SOLA framework is controlled by the risk suffered at the learning step for which we use off the shelf vector valued regressors with their own convergence guarantees.\nIn the following, we specifically study the pre-image problem in the SOLA framework for a class of output structures that we detail hereafter."
  }, {
    "heading": "4. Pre-image for hierarchical structures with Abstention",
    "text": "In what follows we focus on a class of structured outputs that can be viewed as hierarchical objects for which we show how to solve the pre-image problems involved for a large class of losses."
  }, {
    "heading": "4.1. Hierarchical output structures",
    "text": "Definition 1. A HEX graph G = (V,Eh, Ee) is a graph consisting of a set of nodes V = {v1, . . . , vn}, directed edges Eh ⊂ V × V , and undirected edges Ee ⊂ V × V , such that the subgraph Gh = (V,Eh) is a directed acyclic graph (DAG) and the subgraph Ge = (V,Ee) has no self loop. Definition 2. An assignment (state) y ∈ {0, 1}d of labels V in a HEX graph G = (V,Eh, Ee) is legal if for any pair of nodes labeled (y(i), y(j)) = (1, 1), (vi, vj) /∈ Ee and for any pair (y(i), y(j)) = (0, 1), (vi, vj) /∈ Eh. Definition 3. The state space SG ⊆ {0, 1}d of graph G is the set of all legal assignments of G.\nThus a HEX graph can be described by a pair of (1) a directed graph over a set of binary nodes indicating that any child can be labeled 1 only if its parent is also labeled 1 and (2) an undirected graph of exclusions such that two nodes linked by an edge cannot be simultaneously labeled 1. Note that HEX graphs can represent any type of binary labeled graph sinceEh andEe can be empty sets. In previous works, they have been used to model some coarse to fine ontology through the hierarchy Gh while incorporating some prior known labels exclusions encoded by Ge (Deng et al., 2014; BenTaieb & Hamarneh, 2016)\nWhile the output data we consider consists of HEX graph assignments , our predictions with abstention (h(x), r(x)) belong to another spaceYH,R ⊆ {0, 1}d×{0, 1}d for which we do not restrict h(x) to belong to Y but rather allow for other choices detailed in the next section."
  }, {
    "heading": "4.2. Efficient solution for the preimage problem",
    "text": "The complexity of the preimage problem is due to two aspects: i) the space in which we search the solution (YH,R) can be hard to explore; and ii) the ψa function can lead to high dimensional representations for which the minimization problem is harder.\nThe pre-image problem involves a minimization over a constrained set of binary variables. For a large class of abstention-aware predictors we propose a branch-and-bound formulation for which a nearly optimal initialization point can be obtained in a polynomial time. Following the line given by the form of our abstention aware predictor fh,r defined in Section 2, we consider losses involving binary interaction between the predict function h(x) and the reject\nfunction r(x), and suppose that there exists a rectangular\nmatrix M such that ψa(h(x), r(x)) = M  h(x)r(x) h(x)⊗ r(x)  where ⊗ is the Kronecker product between vectors. Such a class takes as special cases the examples presented in Section 2. We state the following linearization theorem under binary interaction hypothesis:\nTheorem 2. Let lha be an abstention-aware loss defined by its output mappings ψwa, ψa and the corresponding cost matrix C.\nIf the ψa mapping is a linear function of the binary interactions of h(x) and r(x) i.e. there exists a matrix M such that ∀(h(x), r(x)) ∈ YH,R ψa(h(x), r(x)) =\nM  h(x)r(x) h(x)⊗ r(x) , then there exists a bounded linear operator A and a vector b such that ∀ψx ∈ Rp the pre-image problem:\n(ĥ(x), r̂(x)) = arg min (yh,yr)∈YH,R\n〈ψa(yh, yr), ψx〉,\nhas the same solutions as the linear program:\nĥ(x), r̂(x) = arg min (yh,yr)∈YH,R\n[yTh y T r c T ]MTψx\ns.t. A yhyr c  ≤ b. Where c is a d2 dimensional vector constrained to be equal to yh ⊗ yr.\nThe proof is detailed in the supplementary material.\nThe problem above still involves a minimization over the structured binary set YH,R. Such a set of solutions encodes some predefined constraints:\n• Since the objects we intend to predict are HEX graph assignments, the vectors of the output space y ∈ Y should satisfy the hierachical constraint : yi ≤ yp(i) with p(i) the index of the parent of i according to the hierarchy. When predicting with abstention we relax this condition since we suppose that a descendant node can take the value yi = 1 if its parent was active yp(i) = 1 or if we abstained from predicting it rp(i) = 0. Such a condition is equivalent to the constraint\nyirp(i) ≤ yp(i)rp(i). (11)\n• A second condition we used in practice is the restriction of the use of abstention for two consecutive nodes: structured abstention at a layer must be used in order\nto reveal a subsequent prediction which is known to be easy. Such a condition can be encoded through the inequality:\nri + rp(i) ≤ 1. (12)\nIn our experiments, the structured space YH,R has been chosen as the set of binary vectors (h(x), r(x)) ∈ YH,R that respect the two above conditions. These choices are motivated by our application but note that any subset of {0, 1}d × {0, 1}d can be built in a similar way by adding\nsome inequality constraints: AYH,R  h(x)r(x) h(x)⊗ r(x)  ≤ bYH,R . Consequently, the YH,R constraints can be added to the previous minimization problem to build the canonical form:\n(ĥ(x), r̂(x)) = arg min (yh,yr)\n[yTh y T r c T ]MTψx\ns.t. Acanonical yhyr c  ≤ bcanonical, (yh, yr) ∈ {0, 1}d × {0, 1}d,\nwhere Acanonical = (\nA AYH,R\n) and bcanonical = ( b\nbYH,R\n) .\nThe complexity of the problem above is linked to some properties of the Acanonical operator. (Goh & Jaillet, 2016) have shown that in the case of the minimization of the H-loss with hierarchical constraints, the linear operator Acanonical satisfies the property of total unimodularity (Schrijver, 1998) which is a sufficient condition for the problem above to have the same solutions as its continuous relaxation leading to a polynomial time algorithm. In the more general case of the Ha-loss, solving such an integer program is NP-hard and the optimal solution can be obtained using a branch-and-bound algorithm. When implementing this type of approach, the choice of the initialization point can strongly influence the convergence time. As in practical applications, we expect the number of abstentions to remain low, such a point can be chosen as the solution of the original prediction problem without abstention (Goh & Jaillet, 2016). Moreover since the abstention mechanism should modify only a small subset of the predictions, we expect this solution to be close to the abstention aware one."
  }, {
    "heading": "5. Numerical Experiments",
    "text": "We study three subtasks of opinion mining, namely sentencebased aspect prediction, sentence-based joint prediction of aspects and polarities (possibly with abstention) and full review-based star rating. We show that these tasks can be linked using a hierarchical graph similar to the probabilistic\nmodel of (Marcheggiani et al., 2014) and exploit the abstention mechanism to build a robust pipeline: based on the opinion labels available at the sentence-level, we build a two-stage predictor that first predicts the aspects and polarities at the sentence level, before deducing the corresponding review-level values."
  }, {
    "heading": "5.1. Parameterization of the Ha-loss",
    "text": "In all our experiments, we rely on the expression of the Haloss presented in 4. The linear programming formulation of the pre-image problem used in the branch-and-bound solver is derived in the supplementary material and involves a decomposition similar to the one described in Section 2 for the H-loss. Implementing the Ha-loss requires choosing the weights ci, cAi and cAci. We first fix the ci weights in the following way :\nc0 = 1 ci = cp(i)\n|siblings(i)| ∀i ∈ {1, . . . , d}.\nHere, 0 is assumed to be the index of the root node. This weighting scheme has been commonly used in previous studies (Rousu et al., 2006; Bi & Kwok, 2012) and is related to the minimization of the Hamming Loss on a vectorized representation of the graph assignment. As far as the abstention weights cAi and cAci are concerned, making an exhaustive analysis of all the possible choices is impossible due to the number of parameters involved. Therefore, our experiments focus on weighting schemes built in the following way:\ncAi = KAci\ncAci = KAcci\nThe effect of the choices of KA and KAc will be illustrated below on the opinion prediction task. We also ran a set of experiments on a hierarchical classification task of MRI images from the IMAGECLEF2007 dataset reusing the setting of (Dimitrovski et al., 2008) where we show the results obtained for different ci weighting schemes. The settings and the results have been placed in the supplementary material."
  }, {
    "heading": "5.2. Learning with Abstention for aspect-based opinion mining",
    "text": "We test our model on the problem of aspect-based opinion mining on a subset of the TripAdvisor dataset released in (Marcheggiani et al., 2014). It consists of 369 hotel reviews for a total of 4856 sentences with predefined train and test sets. In addition to the review-level star ratings, the authors gathered the opinion annotations at the sentence-level for a set of 11 predefined aspects and their corresponding polarity. Similarly to them, we discard the “NOT RELATED” aspect and consider the remaining 10 aspects with the 3 different\npolarities (positive, negative or neutral) for each. We propose a graphical representation of the opinion structure at the sentence level (see Fig. 1). Objects in the output space y ∈ Y consist of trees of depth 3 where the first node is the root, the second layer is made of aspect labels and the third one is the polarities corresponding to each aspect. The corresponding assignments are encoded by a binary matrix y ∈ Y where y is the concatenation of the vectors indicating the presence of each aspect (depth 2) and the ones indicating the polarity.\nAn example of y encoding is displayed in Fig.1. Based on the recent results of (Conneau et al., 2017), we focus on the InferSent representation to encode our inputs. This dense sentence embedding corresponds to the inner representation of a deep neural network trained on a natural language inference task and has been shown to give competitive results in other natural language processing tasks.\nWe test our model on 3 different subtasks. In Exp1, we first apply our model (H Regression InferSent) to the task of opinion aspect prediction and compare it against two baselines and the original results of (Marcheggiani et al., 2014). In Exp2, we test our method and baselines on the problem of joint aspect and polarity prediction in order to assess the ability of the hierarchical predictor to take advantage of the output structure. On this task we additionally illustrate the behavior of abstention when varying the constants KA and KAc . In Exp3, we illustrate the use abstention as a mean to build a robust pipeline on the task of star rating regression based on a sentence-level opinion predictor.\nExp1. Aspect prediction. In this first task, we aim at predicting the different aspects discussed in each sentence. This problem can be cast as a multilabel classification problem where the target is the first column of the output objects y for which we devise two baselines. The first relies on a logistic regression model (Logistic Regression InferSent) trained separately for each aspect. The second baseline (Linear chain Conditional Random Fields (CRF) (Sutton et al., 2012) InferSent) is inspired by the work of (Marcheggiani et al., 2014) who built a hierarchical CRF model based on a handcrafted sparse feature set including one-hot word encoding, POS tags and sentiment vocabulary. Since the\noptimization via Gibbs sampling of their model relies on the sparsity of the feature set, we could not directly use it with our dense representation. Linear chain CRF InferSent takes advantage of our input features while remaining computationally tractable. One linear chain is trained for each node of the output structures and the chain encodes the dependency between successive sentences.\nTable 5.2 below shows the results in terms of micro-averaged F1 (µ-F1) score obtained on the task of aspect prediction. The three methods using InferSent give significantly better\nmethod µ-F1 H Regression InferSent 0.59 Logistic Regression InferSent 0.60 Linear chain CRF InferSent 0.59 Linear chain CRF sparse features Marcheggiani et al. 0.49 Hierarchical CRF sparse features Marcheggiani et al. 0.49\nTable 1. Experimental results on the TripAdvisor dataset for the aspect prediction task.\nresults than (Marcheggiani et al., 2014). Consequently, the next experiments will not consider them. Even though H Regression was trained in order to predict the whole structure, it obtains results similar to logistic regression and linear chain CRF.\nExp2. Joint polarity and aspect prediction with abstention. We take as output objects the assignments of the graph described (Fig. 1) and build an adapted abstention mechanism. Our intuition is that in some cases, the polarity might be easier to predict than the aspect to which it is linked. This can typically happen when some vocabulary linked to the current aspect has been unseen during the training or is implicit whereas the polarity vocabulary is correctly recognized. An example is the sentence \" We had great views over the East River\" where the aspect \"Location\" is implicit and where the \"views\" could mislead the predictor and result in a prediction of the aspect \"Other\". In such a case, (Marcheggiani et al., 2014) underline that the interannotator agreement is low. For this reason, we want that our classifier allows multiple candidates for aspect prediction while providing the polarity corresponding to them. We illustrate this behavior by running two sets of experiments in which we do not allow the predictor to abstain on the polarity.\nIn the first experiment, we want to analyze the influence of the parameterization of the Ha-loss. Following the parameterization of cAi and cAci previously proposed, we generated some predictions with varying values of KA ∈ [0, 0.5] and KAc ∈ {0.25, 0.5, 0.75}. We displayed the Hamming loss between the true labels and the predictions as a function\nof the mean number of aspects on which the predictor abstained (Fig. 2) and handle two cases : modified : in the left figure, all nodes except the one on which we abstained were used to compute the Hamming loss. In the right one, all nodes except the aspect on which we abstained and their corresponding polarity were used to compute the Hamming loss. The HStrict results correspond to a predictor for which\nthe original hierarchical constraint is forced: y(i) ≤ yp(i) and the three other curves have been obtained with the generalized constraint hypothesis y(i)rp(i) ≤ yp(i)rp(i).\nWe additionally ran our model H Regression without abstention and our two baselines logistic regression for which we measured a similar Hamming loss of 0.03 (corresponding to 0 abstention on the left Figure 2). Concerning the micro-averaged F1 score, the H Regression retrieved a score of 0.54 being slightly above the logistic regression which scored 0.53 and the linear chain CRF with 0.52.\nTwo conclusions can be raised. Firstly, the value of KAc and the choice of the hypothesis HStrict have little to no influence on the scores computed in the two cases previously described. Secondly, increasing the number of abstentions on aspects helps reducing the number of errors counted on the aspects nodes when the predictor abstains on less than 3 labels. After this point, the quality of the overall prediction decreases since the error rate on the remaining aspects selected for abstention is less than the one on the polarity labels\nSubsequently, we examine the Hamming loss on the polarity predictions situated after an aspect node to understand the influence of the cAc coefficients and the relaxation of the HStrict hypothesis in Figure 3. The orange curve gives the best score when the mean number of abstentions is between 2 and 4 per sentence. The only difference with the Hstrict hypothesis is the ability to predict the polarity of an aspect candidate for abstention even if the predictor function does not select it. This behavior is made possible by the fact that\nour prediction does not respect the Y constraints but instead belong to the more flexible space YH,R Finally we show how abstention can be used to build a robust pipeline for star-rating regression.\nExp3. Star rating regression at the review level based on sentence level predictions. In the last round of experiments, we show that abstention can be used as a way to build a robust intermediate representation for the task of opinion rating regression (Wang et al., 2011) which consists in predicting the overall average star rating given by each reviewer on a subset of six predefined aspects. The figure below illustrates the different elements involved in our problem. The procedure is split in two steps. Firstly, we learn\na sentence-level opinion predictor that takes advantage of the available annotations. This step corresponds to the one studied in the previous experiment. Then a vector-valued regressor (star regressor in Figure 4) is built. It takes as input the component-wise average of the sentence level opinion representations, and intends to predict the star ratings at the review level. For each of the five overall aspects a separate Ridge Regressor is trained based on the true labels available. Once learned, the regressors take as input the prediction of the first step in a pipelined way\nSimilarly to (Marcheggiani et al., 2014), we rescale the star ratings on a (-1,0,1) scale and report the macro-averaged mean average error on the test-set in Table 5.2 below under\nthe column MAE text level. We additionally include the MAE error measured on polarity predictions at the sentence level counted when the underlying aspect predicted is a true positive. The first row is our oracle: the sentence-level\nopinion representations are assumed to be known on the test set and fed in the text-level opinion regressors to find back the star ratings. The Hierarchical CRF line corresponds to the best results reported by (Marcheggiani et al., 2014) on the two tasks. H Regression is our model without abstention used as a predictor of the sentence-level representation in the pipeline shown in Fig 4. Finally for the H Regression with abstention, we used as a sentence-level representation : ya = h(x)−(1−r(x)). Since the only non-zero components of (1− r(x)) correspond to aspects on which we abstained, subtracting them from the original prediction results in a reduction of the confidence of the regressor for these aspects and biasing the corresponding polarity predictions towards 0. H Regression strongly outperforms Hierarchical CRF on both tasks. We do not report the score for H Regression with abstention since it is dependent on the number of abstentions but show that it improves the results of the H Regression model on the text-level prediction task. The significance of the scores has been assessed with a Wilcoxon rank sum test (p-value 10−6)."
  }, {
    "heading": "6. Conclusion",
    "text": "The novel framework, Structured Learning with Abstention, extends two families of approaches: learning with abstention and least-squares surrogate structured prediction. It is important to notice that beyond ridge regression, any vectorvalued regression model that writes as (8) is eligible. This is typically the case of Output Kernel tree-based methods (Geurts et al., 2006). Also, SOLA has here been applied to opinion analysis but it could prove suitable for more complex structure-labeling problems. Concerning Opinion Analysis, we have shown that abstention can be used to build a robust representation for star rating in a pipeline framework. One extension of our work would consist in learning how to abstain by jointly predicting the aspects and polarity at the sentence and text level."
  }, {
    "heading": "Acknowledgements",
    "text": "This work has been funded by the french ministry of research and by the chair Machine Learning for Big Data of Télécom ParisTech."
  }],
  "year": 2018,
  "references": [{
    "title": "Topology Aware Fully Convolutional Networks for Histology Gland Segmentation, pp. 460–468",
    "authors": ["A. BenTaieb", "G. Hamarneh"],
    "year": 2016
  }, {
    "title": "Hierarchical multilabel classification with minimum bayes risk",
    "authors": ["W. Bi", "J.T. Kwok"],
    "venue": "In Data Mining (ICDM),",
    "year": 2012
  }, {
    "title": "Input output kernel regression: supervised and semi-supervised structured output prediction with operator-valued kernels",
    "authors": ["C. Brouard", "M. Szafranski", "F. d’Alché Buc"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Hierarchical classification: combining bayes with svm",
    "authors": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"],
    "venue": "In Proceedings of the 23rd international conference on Machine learning,",
    "year": 2006
  }, {
    "title": "Sentiment analysis: from opinion mining to human-agent interaction",
    "authors": ["C. Clavel", "Z. Callejas"],
    "venue": "IEEE Transactions on affective computing,",
    "year": 2016
  }, {
    "title": "Supervised learning of universal sentence representations from natural language inference data",
    "authors": ["A. Conneau", "D. Kiela", "H. Schwenk", "L. Barrault", "A. Bordes"],
    "venue": "CoRR, abs/1705.02364,",
    "year": 2017
  }, {
    "title": "Boosting with abstention",
    "authors": ["C. Cortes", "G. DeSalvo", "M. Mohri"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Hierchical annotation of medical images",
    "authors": ["I. Dimitrovski", "D. Kocev", "S. Loskovska", "S. Džeroski"],
    "venue": "In Proceedings of the 11th International Multiconference - Information Society IS",
    "year": 2008
  }, {
    "title": "Kernelizing the output of tree-based methods",
    "authors": ["P. Geurts", "L. Wehenkel", "F. d’Alché-Buc"],
    "venue": "In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML",
    "year": 2006
  }, {
    "title": "Structured Prediction by Conditional Risk Minimization",
    "authors": ["C.Y. Goh", "P. Jaillet"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Hierarchical multi-label conditional random fields for aspect-oriented opinion mining",
    "authors": ["D. Marcheggiani", "O. Täckström", "A. Esuli", "F. Sebastiani"],
    "venue": "In ECIR,",
    "year": 2014
  }, {
    "title": "On learning vector-valued functions",
    "authors": ["C.A. Micchelli", "M. Pontil"],
    "venue": "Neural computation,",
    "year": 2005
  }, {
    "title": "On structured prediction theory with calibrated convex surrogate losses",
    "authors": ["A. Osokin", "F.R. Bach", "S. Lacoste-Julien"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
    "authors": ["M. Pontiki", "D. Galanis", "H. Papageorgiou", "I. Androutsopoulos", "S. Manandhar", "Mohammad", "A.-S", "M. Al-Ayyoub", "Y. Zhao", "B. Qin", "O De Clercq"],
    "venue": "In Proceedings of the 10th international workshop on semantic evalua-",
    "year": 2016
  }, {
    "title": "Kernel-based learning of hierarchical multilabel classification models",
    "authors": ["J. Rousu", "C. Saunders", "S. Szedmak", "J. Shawe-Taylor"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2006
  }, {
    "title": "Theory of linear and integer programming",
    "authors": ["A. Schrijver"],
    "year": 1998
  }, {
    "title": "Latent aspect rating analysis without aspect keyword supervision",
    "authors": ["Y. Lu", "C. Zhai"],
    "venue": "Machine Learning,",
    "year": 2012
  }],
  "id": "SP:437c5cf8ff3aba82e08ddb8f32308a3dad611749",
  "authors": [{
    "name": "Alexandre Garcia",
    "affiliations": []
  }, {
    "name": "Slim Essid",
    "affiliations": []
  }, {
    "name": "Chloé Clavel",
    "affiliations": []
  }, {
    "name": "Florence d’Alché-Buc",
    "affiliations": []
  }],
  "abstractText": "Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor.",
  "title": "Structured Output Learning with Abstention: Application to Accurate Opinion Prediction"
}