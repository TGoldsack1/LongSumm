{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2094–2099, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015).\nNotably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also\nnote that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE).\nTo remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise.\nThis paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers.\nThe BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently. “Incorrect” target words for the BNNJM can be generated in the same way as NCE generates noise\n2094\nfor the NNJM. We present a novel noise distribution based on translation probabilities to train the NNJM and the BNNJM efficiently."
  }, {
    "heading": "2 Neural Network Joint Model",
    "text": "Let T = t|T |1 be a translation of S = s |S| 1 . The NNJM (Devlin et al., 2014) defines the following probability,\nP (T |S) = ∏|T | i=1 P ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 ) (1) where target word ti is affiliated with source word sai . Affiliation ai is derived from the word alignments using heuristics1. To estimate these probabilities, the NNJM uses m source context words and n− 1 target history words as input to a neural network and performs estimation of unnormalized probabilities p (ti|C) before normalizing over all words in the target vocabulary V ,\nP (ti|C) = p(ti|C)Z(C) Z (C) = ∑ ti′∈V p (ti′|C) (2)\nwhere C stands for source and target context words as in Equation 1.\nThe NNJM can be trained on a word-aligned parallel corpus using standard MLE, but the cost of normalizing over the entire vocabulary to calculate the denominator in Equation 2 is quite large. Devlin et al. (2014)’s self-normalization technique can avoid normalization cost during decoding, but not during training.\nNCE can be used to train NNLM-style models (Vaswani et al., 2013) to reduce training times. NCE creates a noise distribution q (ti), selects k noise samples ti1, ..., tik for each ti and introduces a random variable v which is 1 for training examples and 0 for noise samples,\nP (v = 1, ti|C) = 11+k · p(ti|C)Z(C) P (v = 0, ti|C) = k1+k · q (ti) .\nNCE trains the model to distinguish training data from noise by maximize the conditional likelihood,\nL = log P (v = 1|C, ti) + k∑\nj=1\nlog P (v = 0|C, tik).\nThe normalization cost can be avoided by using p (ti|C) as an approximation of P (ti|C).2\n1If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word.\n2The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015)."
  }, {
    "heading": "3 Binarized NNJM",
    "text": "In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source context words sai+(m−1)/2ai−(m−1)/2 and target history words ti−1i−n+1 ,\nP ( v|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .\nThe BNNJM is learned by a feedforward neural network with m + n inputs{ s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti } and two outputs for v = 1/0. Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden layers. Thus, the hidden layers can be used to learn the difference between correct target words and noise in the BNNJM, while in the NNJM the hidden layers just contain information about context words and only the output layer can be used to discriminate between the training data and noise, giving the BNNJM more power to learn this classification problem.\nWe can use the BNNJM probability in translation as an approximation for the NNJM as below,\nP ( ti|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1 ) ≈ P ( v = 1|sai+(m−1)/2ai−(m−1)/2, t i−1 i−n+1, ti ) .\nAs a binary classifier, the gradient for a single example in the BNNJM can be calculated efficiently by MLE without it being necessary to calculate the softmax over the full vocabulary. On the other hand, we need to create “positive” and “negative” examples for the classifier. Positive examples can be extracted directly from the word-aligned parallel corpus as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti 〉 ; Negative examples can be generated for each positive example in the same way that NCE generates noise data as〈 s ai+(m−1)/2 ai−(m−1)/2, t i−1 i−n+1, ti ′ 〉 , where ti′ ∈ V \\ {ti}."
  }, {
    "heading": "4 Noise Sampling",
    "text": ""
  }, {
    "heading": "4.1 Unigram Noise",
    "text": "Vaswani et al. (2013) adopted the unigram probability distribution (UPD) to sample noise for train-\ning NNLMs with NCE,\nq (ti′) = occur(ti ′)∑ ti ′′∈V occur(ti′′)\nwhere occur (ti′) stands for how many times ti′ occurs in the training corpus."
  }, {
    "heading": "4.2 Translation Model Noise",
    "text": "In this paper, we propose a noise distribution specialized for translation models, such as the NNJM or BNNJM.\nFigure 2 gives a Chinese-to-English parallel sentence pair with word alignments to demonstrate the intuition behind our method. Focusing on sai=“安排”, this is translated into ti =“arrange”. For this positive example, UPD is allowed to sample any arbitrary noise, such as ti′ = “banana”. However, in this case, noise ti′ = “banana” is not useful for model training, as constraints on possible translations given by the phrase table ensure that “安排” will never be translated into “banana”. On the other hand, noise ti′ = “arranges” and “arrangement” are both possible translations of “安排” and therefore useful training data, that we would like our model to penalize.\nBased on this intuition, we propose the use of another noise distribution that only uses ti′ that are possible translations of sai , i.e., ti ′ ∈ U (sai) \\ {ti}, where U (sai) contains all target words aligned to sai in the parallel corpus.\nBecause U (sai) may be quite large and contain many wrong translations caused by wrong alignments, “banana” may actually be included in U (“安排”). To mitigate the effect of uncommon examples, we use a translation probability distribution (TPD) to sample noise ti′ from U (sai) \\ {ti} as follows,\nq (ti′|sai) = align(sai ,ti′)∑\nti ′′∈U(sai )\nalign(sai ,ti′′)\nwhere align (sai , ti ′) is how many times ti′ is aligned to sai in the parallel corpus. Note that ti could be unaligned, in which case we assume that it is aligned to a special null word. Noise for unaligned words is sampled according to the TPD of the null word. If several target/source words are aligned to one source/target word, we\nchoose to combine these target/source words as a new target/source word.3"
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Setting",
    "text": "We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively.\nFor each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nThe word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples.\nThe number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate\n3The processing for multiple alignments helps sample more useful negative examples for TPD, and had little effect on the translation performance when UPD was used as the noise distribution for the NNJM and the BNNJM in our preliminary experiments.\n4http://sourceforge.net/projects/mecab/files/ 5http://hlt.fbk.eu/en/irstlm 6Both the NNJM and the BNNJM had one hidden layer, 100 hidden nodes, input embedding dimension 50, output embedding dimension 50. A small set of training data was used as validation data. The training process was stopped when validation likelihood stopped increasing.\nthe whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples."
  }, {
    "heading": "5.2 Results and Discussion",
    "text": "Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2.\nWe can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the training process significantly, with a small improvement in performance. But for the BNNJM, using different noise distributions affects translation performance significantly. The BNNJM with UPD does not improve over the baseline system, likely due to the small number of noise samples used in training the BNNJM, while the BNNJM with TPD achieves good performance, even better than the NNJM with TPD on the Chinese-to-English and French-to-English translation tasks.\nFrom Table 2, the NNJM does not improve translation performance significantly on the FE task. Note that the baseline BLEU for the FE\n7The decoding time for the NNJM and the BNNJM were similar, since the NNJM trained by NCE uses p (ti|C) as an approximation of P (ti|C) without normalization and the BNNJM only needs to be normalized over two output neurons.\ntask is lower than CE and JE tasks, indicating that learning is harder for the FE task than CE and JE tasks. The validation perplexities of the NNJM with UPD for CE, JE and FE tasks are 4.03, 3.49 and 8.37. Despite these difficult learning circumstances and lack of large gains for the NNJM, the BNNJM improves translations significantly for the FE task, suggesting that the BNNJM is more robust to difficult translation tasks that are hard for the NNJM.\nTable 3 gives Chinese-to-English translation examples to demonstrate how the BNNJM (with TPD) helps to improve translations over the NNJM (with TPD). In this case, the BNNJM helps to translate the phrase “该 移动 持续 到” better. Table 4 gives translation scores for these two translations calculated by the NNJM and the BNNJM. Context words are used for predictions but not shown in the table.\nAs can be seen, the BNNJM prefers T2 while the NNJM prefers T1. Among these predictions, the NNJM and the BNNJM predict the translation for “到” most differently. The NNJM clearly predicts that in this case “到” should be translated into “to” more than “until”, likely because this example rarely occurs in the training corpus. However, the BNNJM prefers “until” more than “to”, which\ndemonstrates the BNNJM’s robustness to less frequent examples."
  }, {
    "heading": "5.3 Analysis for JE Translation Results",
    "text": "Finally, we examine the translation results to explore why the BNNJM with TPD did not outperform the NNJM with TPD for the JE translation task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words.\nFirst, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) Ti contains J individual words, Wij ∈Words (Ti) To (Wij) is how many times Wij occurs in Ti and Ro (Wij) is how many times Wij occurs in Ri.\nThe general 1-gram translation accuracy (Papineni et al., 2002) is calculated as,\nPg =\nI∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))\nI∑ i=1 J∑ j=1 To(Wij)\nThis general 1-gram translation accuracy does not distinguish word frequency.\nWe use a modified 1-gram translation accuracy that weights infrequent words more heavily,\nPc =\nI∑ i=1 J∑ j=1 min(To(Wij),Ro(Wij))· 1 Occur(Wij)\nI∑ i=1 J∑ j=1 To(Wij)\nwhere Occur (Wij) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to Pc.\nTable 5 shows Pg and Pc for different translation tasks. It can be seen that the BNNJM improves infrequent word translation quality similarly for all translation tasks, but improves general translation quality less for the JE task than the other translation tasks. We conjecture that the reason why the BNNJM is less useful for frequent word translations on the JE task is the fact that the JE parallel corpus has less accurate function word alignments than other language pairs, as the\ngrammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM."
  }, {
    "heading": "6 Related Work",
    "text": "Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers.\nMauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses real-valued vector representations of words and shares features, so we train one classifier and can use the similarity information between words."
  }, {
    "heading": "7 Conclusion",
    "text": "This paper proposes an alternative to the NNJM, the BNNJM, which learns a binary classifier that takes both the context and target words as input and combines all useful information in the hidden layers. We also present a novel noise distribution based on translation probabilities to train the BNNJM efficiently. With the improved noise sampling method, the BNNJM can achieve comparable performance with the NNJM and even improve the translation results over the NNJM on Chineseto-English and French-to-English translations.\n8Infrequent words are usually content words and frequent words are usually function words."
  }],
  "year": 2015,
  "references": [{
    "title": "When and why are log-linear models self-normalizing",
    "authors": ["Jacob Andreas", "Dan Klein"],
    "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association",
    "year": 2015
  }, {
    "title": "Joint language and translation modeling with recurrent neural networks",
    "authors": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044–",
    "year": 2013
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Fast and robust neural network joint models for statistical machine translation",
    "authors": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Compu-",
    "year": 2014
  }, {
    "title": "Overview of the patent machine translation task at the NTCIR-9 workshop",
    "authors": ["Isao Goto", "Bin Lu", "Ka Po Chow", "Eiichiro Sumita", "Benjamin K Tsou."],
    "venue": "Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting, pages 559–578.",
    "year": 2011
  }, {
    "title": "Minimum translation modeling with recurrent neural networks",
    "authors": ["Yuening Hu", "Michael Auli", "Qin Gao", "Jianfeng Gao."],
    "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29.",
    "year": 2014
  }, {
    "title": "Statistical significance tests for machine translation evaluation",
    "authors": ["Philipp Koehn."],
    "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.",
    "year": 2004
  }, {
    "title": "Extending statistical machine translation with discriminative and trigger-based lexicon models",
    "authors": ["Arne Mauser", "Saša Hasan", "Hermann Ney."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2009
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["Franz Josef Och."],
    "venue": "Proceedings of",
    "year": 2003
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
    "year": 2002
  }, {
    "title": "Continuous space translation models for phrase-based statistical machine translation",
    "authors": ["Holger Schwenk."],
    "venue": "Proceedings of International Conference on Computational Linguistics : Posters, pages 1071– 1080.",
    "year": 2012
  }, {
    "title": "Translation modeling with bidirectional recurrent neural networks",
    "authors": ["Martin Sundermeyer", "Tamer Alkhouli", "Joern Wuebker", "Hermann Ney."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."],
    "venue": "Advances in neural information processing systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Decoding with largescale neural language models improves translation",
    "authors": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2013
  }, {
    "title": "Efficient subsampling for training complex language models",
    "authors": ["Puyang Xu", "Asela Gunawardana", "Sanjeev Khudanpur."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128–1136.",
    "year": 2011
  }, {
    "title": "An improved Chinese word segmentation system with conditional random field",
    "authors": ["Hai Zhao", "Chang-Ning Huang", "Mu Li."],
    "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165.",
    "year": 2006
  }],
  "id": "SP:aadb0d9057dac2d545eafda6a4f28806bc1a1292",
  "authors": [{
    "name": "Jingyi Zhang",
    "affiliations": []
  }, {
    "name": "Masao Utiyama",
    "affiliations": []
  }, {
    "name": "Eiichro Sumita",
    "affiliations": []
  }, {
    "name": "Graham Neubig",
    "affiliations": []
  }, {
    "name": "Satoshi Nakamura",
    "affiliations": []
  }],
  "abstractText": "The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.",
  "title": "A Binarized Neural Network Joint Model for Machine Translation"
}