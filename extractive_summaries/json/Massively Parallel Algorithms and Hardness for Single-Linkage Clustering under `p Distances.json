{
  "sections": [{
    "heading": "1. Introduction",
    "text": ""
  }, {
    "heading": "1.1. Single-linkage clustering",
    "text": "Single-Linkage Clustering is one of the oldest methods for clustering multi-dimensional vectors based on the nearestneighbor rule and has been studied since 1951, see e.g. (Zahn, 1971). It can be used for hierarchical clustering and is one of the cornerstone techniques in data mining (see e.g. Chapter 17 of a classic text on information retrieval by Manning, Raghavan and Schütze (Manning et al., 2008)). Applications of Single-Linkage Clustering include reconstruction of semantic relationships from word embeddings such as Word2Vec (Malak & East, 2016), phylogenetic tree reconstruction (Gower & Ross, 1969), etc.\n1Department of Computer Science, Indiana University, Bloomington, Indiana, United States. Correspondence to: Grigory Yaroslavstev <grigory@grigory.us>, Adithya Vadapalli <avadapal@iu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nWe consider the problem of constructing a Single-Linkage Clustering for large-scale data. Given a dataset consisting of n real-valued d-dimensional vectors v1, . . . , vn ∈ Rd the goal of Single-Linkage Clustering is to construct a partition of these vectors into k clusters C1, . . . , Ck such that the smallest distance between two vectors in different clusters is maximized. Formally, for i 6= j let the single-linkage distance between two clusters Ci and Cj under `p distance be dp(Ci, Cj) = minva∈Ci,vb∈Cj ‖va − vb‖p where ‖x‖p = ( ∑ i |xi|p)1/p is the standard p-norm. Then in the k-Single-Linkage Clustering (k-SLC) problem under `p distance we aim to find a partition into k clusters that maximizes mini 6=j dp(Ci, Cj). It is well-known that k-SLC can be constructed from the Minimum Spanning Tree (MST) of the underlying metric by taking as clusters connected components resulting from removal of k−1 longest MST edges (see Figure 1 for an example).\nNote that with this approach once the MST is constructed it can be used to compute k-SLC for any value of k. Furthermore, it induces a hierarchical clustering structure that is often desirable in practice. According to Manning, Raghavan and Schütze (Manning et al., 2008) the main impediment to this approach in practice that motivates the use of various heuristics is that for large-scale data no practically feasible techniques are currently known for constructing an exact MST. Our work overcomes this challenge by leveraging two observations: 1) inexact but close to optimum solutions can suffice in practice due to the fact that real-valued data always contains rounding errors, 2) while exact MST algorithms are very sequential, approximate solutions can be computed in parallel on a distributed cluster.\nMassively Parallel Algorithms and Hardness for Single-Linkage Clustering under `p Distances m m ac hi ne s\nR Rounds\n≤ s bits sent/received\nFigure 2. MPC model of computation"
  }, {
    "heading": "1.2. Massively parallel computation",
    "text": "We present analysis of performance of our algorithms in the Massively Parallel Computation model (MPC) which is the most commonly used theoretical model of computation on synchronous large-scale data processing platforms such as MapReduce and Spark. As we demonstrate through experiments in Spark this model accurately reflects performance of our algorithms on real data. MPC model has attracted a lot of interest recently. It has emerged through a sequence of papers (Feldman et al., 2008; Karloff et al., 2010; Goodrich et al., 2011; Beame et al., 2013; Andoni et al., 2014) and has been analyzed extensively (Fish et al., 2015; Roughgarden et al., 2016). While several variations of this basic model exist here we follow the strictest known version of the model used in (Andoni et al., 2014) and hence our algorithmic results hold in other versions as well.\nIn the MPC model we are given access to m identical processors with local RAM space s on each. For an input of size n the total space available to all processors is m · s = Õ(n). The computation is performed in synchronous rounds. In each round each machine: 1) performs a local computation on its data (under its local space restriction of s), 2) sends and receives messages of total length at most s to other machines which are received before the next round begins1 (see Figure 2). Furthermore, we assume that the most time/space-efficient known algorithm for local subproblems (in our case almost linear-time and space) is used on each machine during the round.\nIn this setup the key complexity measure of performance in such computation is the number of rounds it takes to complete it as other characteristics such as time and communication depend directly on it. The parameter s is set to nα for some fixed constant α < 1, see (Karloff et al., 2010; Andoni et al., 2014) for more details. In\n1Note that restriction of s on the total length of received messages follows from the local space constraint assuming there is no computation performed on the fly on incoming data.\nthis setting of parameters sorting can be done in O(1) rounds (Goodrich et al., 2011) while sparse graph connectivity takes O(log n)(Rastogi et al., 2013; Kiveris et al., 2014) which is conjectured to be optimal (Karloff et al., 2010; Beame et al., 2013; Rastogi et al., 2013; Roughgarden et al., 2016). It is folklore that an O(log n)-round algorithm for MST in sparse graphs can be obtained via a simulation of Boruvka’s algorithm in MPC. We use these facts extensively in this paper."
  }, {
    "heading": "1.3. Our results and previous work",
    "text": "While scalable algorithms with provable guarantees for other popular clustering methods such as k-means and kmedian are known (Bahmani et al., 2012; Balcan et al., 2013) we are not aware of any such algorithms for SingleLinkage Clustering 2. Also despite the fact that scalable heuristics exist for k-SLC and MST computation for vector data, e.g. (Jin et al., 2015), the only MPC algorithm with provable guarantees in this area that we are aware of is (Andoni et al., 2014)3. For other recent work on geometric data structures and algorithms in the MPC model see (Agarwal et al., 2016; Nath et al., 2016) and results on distributed constructions of coresets (Agarwal et al., 2005; Indyk et al., 2014; Bateni et al., 2014).\nIn (Andoni et al., 2014) it is shown that a (1 + )- approximate MST under `2 can be constructed in O(1) rounds of MPC for constant dimension. However, while the overall cost of the MST is a good approximation to the optimum the length of any given edge can be arbitrarily distorted. This makes it impossible to directly use this algorithm of for the Single-Linkage Clustering problem. For example, consider an input corresponding to a set of points on the line shown in Figure 3 and k = 2. In this case a (1 + )-approximate MST would not necessarily lead to a (1 + )-approximate clustering as any such clustering would have to have clusters {1, . . . , n− 1} and {n} which are at distance 100 from each other. Moreover, the algorithm of (Andoni et al., 2014) will indeed introduce edges of length Ω( n) into its approximate MST between the first n − 1 points if run on this example. Hence for the MST constructed this way the basic approach of removing the longest edge to obtain a 2-SLC will result in two clusters which are at distance 1 with a very large probability.\n2With the exception of recent work of (Derakhshan et al., 2017) who consider a more general graph metric setting and hence get results which are inherently different from our work as representation of the metric requires Θ(n2) space\n3For general graph metrics an MST algorithm in MPC is given in (Karloff et al., 2010). In our case using this algorithm directly would imply a quadratic increase in space since our graph is implicitly given by n2 distances between the vectors and hence constructing the graph explicitly is infeasible under the overall space restriction.\nIn this paper we show how to overcome this difficulty and give a different family of algorithms which allow to compute an approximate Single-Linkage Clustering under various distance metrics. While in (Andoni et al., 2014) only `2 metric is considered here we further extend this framework so that it also applies to `1 and `∞ with similar performance guarantees. Perhaps most interestingly, while an arbitrarily good MST approximation can be computed inO(1) rounds of MPC (for fixed dimension) our algorithms for k-SLC run in O(log n) rounds. As it turns out, such an increase is likely to be necessary. We justify it through a number of hardness results. Our results show that even for k = 2 assuming two most popular conjectures in the MPC literature regarding complexity of sparse connectivity no o(log n)round algorithm can compute k-SLC for sufficiently large dimension of the data with better than some fixed constantfactor approximation that depends on the distance metric used. See Table 1 for a summary of these results4.\nIn order to complete the picture of approximability of kSLC under the most frequently used `p distances we also give algorithms and hardness results under Hamming distance (commonly referred to as `0). In contrast to other distances studied in this paper we are able to completely resolve approximability of the k-SLC problem for constantdimensional data in this case. As we show, there exists an exact algorithm for d = O(1) that runs in O(log n) rounds of MPC while under Conjecture 3.1 no algorithm running in o(log n) rounds can obtain better than 2-approximation even for d = 2. See Table 1 for details."
  }, {
    "heading": "1.4. Our techniques",
    "text": "`1, `2, `∞ Our algorithms under `1, `2 and `∞ all share the same high-level structure: we tackle the problem of the input having O(n2) edges by first constructing a sparsifier that only has O(n log n) edges and then run an MST algorithm on this sparsifier. In order to construct a sparsifier we execute a (1+ )-approximate MST algorithmO(log n) times and collect all edges of the MSTs constructed in these executions. We then run an exact O(log n)-round exact MST algorithm on this set of O(n log n) edges and output clusters resulting from removing k−1 longest edges of\n4While our algorithms work in the most restricted known version of MPC model, our hardness results also hold in more relaxed versions for which hardness of sparse connectivity is conjectured, see (Roughgarden et al., 2016) for further details. Furthermore, in hardness results for `0 and `1 that require dimension d = Ω(n) the result holds for O(1)-sparse vectors, i.e. the overall input size is still O(n) words.\nthe resulting MST. Note that the executions of the (1 + )- approximate MST algorithm can be done in parallel and hence it is the second step that introduces O(log n) rounds into the overall complexity of the algorithm. Our algorithms under `1, `2 and `∞ are given in Section 2. Assuming the same high-level structure this approach is unlikely to be improved as there are no known algorithms for solving MST in sparse graphs in o(log n) rounds.\nHardness In fact, we make the above observation formal by giving reductions from two most popular problems conjectured to require Ω(log n) rounds in the MPC model: sparse connectivity (Conjecture 3.1) and a stronger “one cycle vs. two cycles” problem (Conjecture 3.2). Our reductions follow the same general strategy – we introduce a vector vi ∈ Rn for each vertex in the input graph. This vector is initially set to be ei, the i-th standard unit vector. Then for each edge (i, j) adjacent to the vertex i we update the coordinate j of the vector by adding a carefully chosen value ξ. This ensures that the for pairs of points which are connected by an edge the distance between their correponding vectors is different from the distance between points which are not connected by an edge. The parameter ξ is then chosen to maximize the ratio of distances in these two cases. Details are given in Section 3.\n`0 Under `0 (Hamming distance) we can’t construct a (1 + )-approximate MST using (Andoni et al., 2014) and hence our algorithms and hardness results are quite different. Using sorting as a primitive we construct an auxiliary graph and then run an O(log n)-round connectivity algorithm on it d times. This way we obtain an exact MST and hence an exact k-SLC for any value of k. Details are given in Section A. Our hardness reduction in this case is also quite different as we construct a hard instance by creating a set of points in 2D instead of using high-dimensional vectors. Hence our result rules out an o(log n)-round 2- approximation even for d = 2. See Section 3.2 for details."
  }, {
    "heading": "1.5. Experimental results",
    "text": "We implemented our algorithm (for `2 distances) in Java on Apache Spark and empirically evaluated the performance. The largest datasets we used were the SIFT10M and HIGGS datasets from the UCI ML repository which has been used widely in literature (≈ 11 × 107). Note that storage of the n2 adjacency would take nearly 960TB of memory and hence building a complete graph locally is infeasible. We observed speedups of several orders of magnitude compared to our benchmark sequential Prim’s algorithm when using 200 reducers. We remark that the speedup is not just due to the parallelism in our algorithm but also due to the use of approximation which is helpful even if the algorithm is executed locally. See Section 4."
  }, {
    "heading": "2. Algorithms",
    "text": "At a high level our k-SLC algorithm for `2 is very simple and can be described as follows:\nAlgorithm 1 Simplified k-SLC Algorithm for `2 Input: vectors v1, . . . , vn ∈ Rd E′ = ∅ Repeat O(log n) times sequentially:\nE = set of edges of a (1 + )-approximate MST E′ = E ∪ E′\nRun Boruvka’s MST algorithm on E′ and remove k − 1 longest edges to obtain the clustering.\nIn order for the above algorithm to produce an approximate k-SLC it is important however that the MST constructed during sequential repetitions obeys certain properties. As we show below, for `2 these properties hold for the algorithm of (Andoni et al., 2014). Furthermore, in order to extend this approach to `1, `∞ a more detailed analysis is required.\n2.1. Partition-based algorithm for `1, `2, `∞\nTheorem 2.1. For each of the three metrics `1, `2 and `∞ for any constants 0 < η ≤ 3, 0 < α < 1/2 such that η = Ω(s 2α−1 2d ) there exists an O(log n)-round MPC algorithm that computes (1+η)-approximate k-Single-Linkage Clustering for any constant dimension d given as an input set of vectors v1, . . . , vn ∈ Rd. The algorithm works simultaneously for all values of k under these metrics. The algorithm is randomized and produces correct result with high probability. Given access to machines with RAM space s it uses Õ(n/s) machines and time at most Õ(s) per round on each machine.\nIn this section we describe a generic partition-based algorithm, Algorithm 2.1, that is used to prove the above theorem. We also give analysis of its approximation guarantee.\nAlgorithm 2.1 relies on (a, b, c)-distance-preserving partitions and uses Algorithm 3 which we describe in Section B.\nWe start by recalling standard definitions of distance preserving hierarchical partitions. Let M(S, ρ) be a metric space with distance function ρ. For S′ ⊆ S we denote its diameter as ∆(S′) = supx,y∈S′ ρ(x, y). A deterministic hierarchical partition P with L levels is defined as a sequence P = (P0, . . . , PL) where PL = {S} and each level P` is a subdivision of P`+1. For a partition Pi we call its parts cells. The diameter at level i is defined as ∆(Pi) = maxC∈Pi ∆(C). The degree of a cell C ∈ P` is deg(C) = |{C ′ ∈ P`−1 : C ′ ⊆ C}|. The degree of a hierarchical partition is the maximum degree of any of its cells. The unique cell at level ` containing a point x is denoted as C`(x). We say that a partition is indexable if this cell can be computed based on x and `. A randomized hierarchical partition is a distribution over deterministic hierarchical partitions. Definition 2.1 (Distance-preserving partition). For parameters a ∈ (0, 1), b, c ∈ R+ and γ > 1 a randomized hierarchical partition P of a metric space with L levels is (a, b, c)-distance-preserving with approximation γ if the degree of all deterministic partitions in its support is at most c and the following properties are satisfied for ∆` = γa L−`∆(S):\n1. (Bounded diameter) For every deterministic partition P = (P0, . . . , PL) in the support of P and for all ` ∈ {0, . . . , L} it holds that ∆(P`) ≤ ∆`. 2. (Probability of cutting an edge) For every x, y ∈ S and for all ` ∈ {0, . . . , L}:\nPr P∼P\n[C`(x) 6= C`(y)] ≤ b ρ(x, y)\n∆` .\nLet M(S, ρ) be a metric space and w : S × S → R+ be a weight function w(x, y) = ρ(x, y). We think of w as representing weights of edges in a complete graph. Let MSTi(w) denote the weight of the i-th Minimum Spanning Tree edge of this graph sorted in non-decreasing order.\nAlgorithm 2 Partition-based Distributed k-SLC Algorithm Input: vectors v1, . . . , vn ∈ Rd, parameters η, α, p E = ∅ Set a = s−α/d, b = poly(d), c = sα, L = O(log1/a n)\nSet = min (\nη 6c1Lb , η3c2 ) Repeat O(log n) times sequentially:\nSample partition P with L levels from (a, b, c)-distance-preserving family wP Execute unit step Algorithm 3 for each cell in P with parameter E′ = set of edges output in the previous step E = E ∪ E′\nRun Boruvka’s MST algorithm on E and remove k − 1 longest edges to obtain the clustering.\nLetw+ : S×S → R+ be a random family of functions that satisfies that for each x, y it holds that w(x, y) ≤ w+(x, y) and E[w+(x, y)] ≤ (1 + γ)w(x, y) for some fixed γ > 0. Note that the weights given by this random family to different pairs might be correlated with each other. Definition 2.2 (Crossing edge). For a partition (C1, . . . , Ct) of S we say that a pair of points (x, y) crosses this partition if x ∈ Ci and y ∈ Cj for i 6= j. Definition 2.3 (Cut-preserving spanning tree). We say that T is an α-cut-preserving spanning tree for w : S × S → R+ if for every partition (C1, C2) of S there exists an edge in T that crosses this partition and is at most α times longer than the shortest such edge with respect to w.\nAs we show below Algorithm 2.1 can be seen as performing the following experiment: draw k functionsw1, . . . , wk i.i.d at random from the family w+. Compute a (1 + δ)cut-preserving spanning tree Ti for each wi. Then for each (x, y) ∈ S×S define w′i(x, y) = w(x, y) if (x, y) is in this spanning tree and w′i(x, y) = +∞ otherwise. Then for all (x, y) ∈ S × S define w̄k(x, y) = minki=1 w′i(x, y). The final run of Boruvka’s MST algorithm is then executed on w̄k.\nIndeed, random family of functionsw+ satisfying the properties described above is constructed by Algoirthm 2.1 as follows from a result (Andoni et al., 2014) given. It is important to note that cut-preserving spanning tree computations for random function samples from this family required above can be also performed as guaranteed by the following lemma: Lemma 2.2 ((Andoni et al., 2014), Lemmas 3.4 and 3.13). Given access to an (a, b, c)-distance-preserving partition with L levels and approximation γ for M(S, ρ) there exists an MPC algorithm that runs inO(1) rounds and constructs a random family of weight functions wP which satisfies:\nρ(i, j) ≤ wP (i, j) and E[wP (i, j)] ≤ (1 + c1 Lb) ρ(i, j).\nFurthermore, execution of unit step Algorithm 3 for all cells in this partition for a random function w∗ sampled from wP produces a (1 + c2 )-cut-preserving spanning tree T for w∗.\nLet w(i, j) = ‖vi − vj‖2, w+ = wP and let γ = c1 d and δ = c2 .\nLemma 2.3. Let n = |S|. There is a large enough constant c > 0 such that if k = c log n then for all i it holds that:\nPr w1,...,wk\n[MSTi(w̄k) ≥ (1+2γ)(1+δ)MSTi(w)] ≤ n−Ω(1).\nProof. Fix (x, y) ∈ S × S and let ∆(x, y) = w+(x, y) − w(x, y). Because ∆(x, y) ≥ 0 and E[∆(x, y)] ≤ γw(x, y) with probability at least 1/2 it holds that ∆(x, y) ≤ 2γw(x, y) by Markov inequality. If k = c log n then with probability 1 − 1/nc there exists i such that wi(x, y) − w(x, y) ≤ 2γw(x, y). By a union bound over all n2 pairs (x, y) with probability 1−1/nc−2 for each such pair a corresponding index exists. Below we refer to this event as E and condition on it.\nProposition 2.4. Let (C1, . . . , Ct) be an arbitrary partition of S. Let (x∗, y∗) ∈ S × S be the closest w.r.t w pair of points that belong to different parts of this partition. Then conditioned on the event E there exists a pair of points (x′, y′) that crosses this partition and:\nw(x∗, y∗) ≤ w̄k(x′, y′) ≤ (1 + 2γ)(1 + δ)w(x∗, y∗).\nProof. First, consider the case when t = 2 and consider any partition (C1, C2) of S. Let (x∗, y∗) be the shortest edge that crosses this partition, i.e. (x∗, y∗) := argminx∈C1,y∈C2 w(x, y). Conditioned on E there exists i such that wi(x∗, y∗) ≤ (1 + 2γ)w(x∗, y∗). Furthermore, there exists an edge (x′, y′) in the (1 + δ)-cutpreserving spanning tree Ti constructed for wi that has length w′i(x ′, y′) = wi(x ′, y′) ≤ (1 + δ)wi(x∗, y∗) ≤ (1 + 2γ)(1 + δ)w(x∗, y∗). On the other hand, because wi ≥ w for every pair (x, y) that crosses the partition (C1, C2) it holds that wi(x, y) ≥ w(x∗, y∗). Combining these two facts we conclude that in Ti there exists some edge (x′, y′) that crosses the cut and satisfies w(x∗, y∗) ≤ w′i(x\n′, y′) ≤ (1+2γ)(1+δ)w(x∗, y∗). By definition of w̄k the same holds for it as well, i.e. w(x∗, y∗) ≤ w̄k(x′, y′) ≤ (1 + 2γ)(1 + δ)w(x∗, y∗).\nNow suppose t > 2. For i = 1, . . . , t define a family of cuts (Si, Ti) where Si = Ci and Ti = ∪j 6=iCj . Let (x∗i , y ∗ i ) be the shortest pair crossing the cut (Si, Ti). If (x∗, y∗) is the shortest edge that crosses (C1, . . . , Ct) then we have w(x∗, y∗) = mini w(x∗i , y ∗ i ). Let i\n∗ = argmini w(x ∗ i , y ∗ i ). Then using the argument above for t = 2 there exists (x′, y′) such that x′ ∈ Si∗ , y ∈ Ti∗ and:\nw(x∗, y∗) = w(x∗i∗ , y ∗ i∗)\n≤ w̄k(x′, y′) ≤ (1 + 2γ)(1 + δ)w(x∗i∗ , y∗i∗) = (1 + 2γ)(1 + δ)w(x∗, y∗).\nGiven Proposition 2.4 the rest of the proof is the same as analysis of approximate Kruskal’s algorithm in (Indyk, 2000), we give the proof here for completeness. Since edges output by Kruskal’s algorithm are produced in the order of non-decreasing weight MSTi is the i-th edge that is output. Consider executions of Kruskal’s algorithm on weights w and w̄k. Let the edges output by the former execution be e1, . . . , en−1 in order. Let the edges output by the latter execution be e′1, . . . , e ′ n−1.\nTo prove Lemma 2.3 it suffices to show that conditioned on E it holds that w(ei) ≤ w(e′i) ≤ (1 + 2γ)w(ei) for all i. The first inequality here essentially follows from the fact that the weight of the i-th MST edge is a monotone function of the weights and w ≤ w̄k.\nThe i-th edge in Kruskal’s algorithm is constructed by joining two closest clusters among n − i + 1 clusters constructed so far. Let these clusters in the execution of Kruskal’s algorithm on w̄k be denoted as C1, . . . , Cn−i+1. The key observation is that there exists an index i∗ ≤ i such that endpoints of the edge ei∗ belong to different parts of the partition C1, . . . , Cn−i+1. Indeed, edges e1, . . . , ei form a forest and thus having all such edges be inside C1, . . . , Cn−i+1 would be a contradiction.\nLet (x∗, y∗) be the closest w.r.t to w pair of points in different parts of the partition C1, . . . , Cn−i+1. By applying Proposition 2.4 to ei∗ there exists a pair of points (x′, y′) whose endpoints belong to different parts of the partition C1, . . . , Cn−i+1 and w̄k(x′, y′) ≤ (1 + 2γ)w(x∗, y∗). Putting everything together we have:\nw(e′i) ≤ w̄k(e′i) w ≤ w̄k\n≤ w̄k(x′, y′) ≤ (1 + 2γ)(1 + δ)w(x∗, y∗) Proposition 2.4 ≤ (1 + 2γ)(1 + δ)w(ei∗) ≤ (1 + 2γ)(1 + δ)w(ei)\nThe second inequality follows because e′i is shortest edge w.r.t w̄k that crosses (C1, . . . , Cn−i+1). The last inequality follows because i∗ ≤ i, edge weights are non-decreasing.\nPutting everything together we obtain analysis of approximation guaranteed by Algorithm 2.1.\nTheorem 2.5. For η ≤ 3 and p = 1, 2,∞ Algorithm 2.1 constructs a spanning tree forw(i, j) = ‖vi−vj‖p for each t its t-th longest edge (x, y) has weight w(x, y) ≤ (1 + η)MSTk(w). This guarantee holds with high probability over the randomness used in Algorithm 2.1.\nProof. Note that takingw+ = wP forw(i, j) = ‖vi−vj‖p where p = 1, 2,∞ satisfies conditions of Lemma 2.3 by Lemma 2.2. Hence our algorithm constructs a function w̄k with properties required for Lemma 2.3. Since c1 Lb ≤ η/6 and c2 ≤ η/3 we can set δ = η/6 and γ = η/3 in Lemma 2.3 and hence for η ≤ 3:\nPr [E1] ≥ Pr [E2] ≥ 1− 1\npoly(n) .\nwhere E1 is the event that MSTi(w̄k) ≤ (1 + η)MSTi(w) and E2 is the event that MSTi(w̄k) ≤ (1 + 2γ)(1 + δ)MSTi(w).\nAfter w̄k is constructed by running Boruvka’s algorithm on it we find an MST exactly and hence the approximation guarantee for each of the MST edges follows."
  }, {
    "heading": "2.2. Solve-and-Sketch framework and unit step",
    "text": "We use Solve-and-Sketch (SAS) framework of (Andoni et al., 2014) for computing an approximate minimum spanning tree. SAS framework works with a partition P = (P0, . . . , PL) of the input M(S, ρ), sampled from a randomized (a, b, c)-partition P . Then SAS algorithm proceeds through L levels, and in level ` a unit step algorithm Au is executed in each cellC of the partition P`, with input the union of the outputs of the unit steps applied to the children of C. The unit step also outputs a subset of the edges of a spanning tree in addition to the input for the next level. Once the unit step has been executed for the root cell of partition at level PL (and hence also for all other cells) the computation is complete. We give the description of the unit step algorithm below (Algorithm 3).\nDefinition 2.4 (δ-covering). Let M = (S, ρ) be a metric space and let δ > 0 . A set S′ ⊆ S is a δ-covering if for any point x ∈ S, there is a point y ∈ S′ such that ρ(x, y) ≤ δ.\n3. Hardness of k-SLC"
  }, {
    "heading": "3.1. Hardness under `1 and `2",
    "text": "The following two conjectures are widely used in the MPC literature (Karloff et al., 2010; Beame et al., 2013; Rastogi et al., 2013; Roughgarden et al., 2016). Note that the second conjecture is stronger and hence can potentially be used to get stronger hardness results.\nConjecture 3.1 (Sparse connectivity hardness). If s = nα for a constant α < 1 then solving connectivity on an input\nAlgorithm 3 Unit Step at Level `, Input: Cell C ∈ P`, a collection V (C) of points in C, and a partition Q = {Q1, . . . Qk} of V (C) into previously computed connected components. Output: V ′ ⊆ V , an 2∆`-covering for C, the partition Q(V ′) induced by Q on V ′. θ := 0 while k > 1 and θ ≤ ∆` do\nLet τ = min i,j i 6=j minu∈Qi,v∈Qj ρ(u, v) Find u ∈ Qi and v ∈ Qj for some i and j such that i 6= j and ρ(u, v) ≤ (1 + )τ . θ := ρ(u, v) if θ ≤ ∆` then\nOutput tree edge (u, v). Merge Qi and Qj and update Q and k.\nend if end while\ngraph with n vertices and O(n) edges requires Ω(log n) rounds of MPC.\nConjecture 3.2 (One cycle vs. two cycles hardness). If s = nα for a constant α < 1 then distinguishing the following two instances requires Ω(log n) rounds of MPC: 1) a cycle on n vertices, 2) two cycles on n/2 vertices each.\nTheorem 3.3. No o(log n)-round MPC algorithm can achieve approximation for 2-SLC:\n1. Better than ( √ 2 + √\n2 − ) under `2 for d = Ω(log n/ 2) under Conjecture 3.2. 2. Better than 3 under `1 for O(1)-sparse vectors and d = Ω(n) under Conjecture 3.2. 3. Better than ( √\n2 − ) under `2 for d = Ω(log n/ 2) under Conjecture 3.1. 4. Better than 2 under `1 for O(1)-sparse vectors and d = Ω(n) under Conjecture 3.1.\nProof. We give proof of Part 1 here, other proofs are similar and are deferred to Appendix E. Given an instance of the “one cycle vs. two cycles problem” we reduce it to the 2-SLC problem as follows:\n1. Create a vector v′i ∈ Rn for each vertex where v′i = ei and ei is the i-th standard unit vector. 2. For each edge (a, b) in the input graph update the corresponding vectors as v′a = v ′ a+ξeb and v ′ b = v ′ b+ξea\nwhere ξ = 1√ 2 . 3. Apply Johnson-Lindenstrauss transform to v′1, . . . , v ′ n\nto construct v1, . . . , vn ∈ Rd where d = O(log n/ 2).\nNote that the above reduction can be performed in only a constant number of MPC rounds. Indeed, Step 1 can be done locally by partitioning vectors between machines and to perform Step 2 we can send each edge (a, b) to the ma-\nchines holding vectors va and vb. For Step 3 note that for each i we have vi = Mv′i where M is the JohnsonLindenstrauss matrix and each v′i has at most 3 non-zero entries. Hence, all vi can be computed in one round of MPC with O(log n/ 2) communication per vector.\nProposition 3.4. If (i, j) is an edge in the input graph then ‖v′i − v′j‖2 = √ 2( √ 2− √ 2), otherwise ‖v′i − v′j‖2 = 2.\nProof. Indeed, if there is an edge (i, j) in the input then there exist two other edges (i, i′) and (j, j′) and hence, the non-zero entries of v′i and v ′ j are as follows: v ′ ii = 1, v ′ ii′ = ξ, v′ij = ξ, v ′ jj = 1, v ′ jj′ = ξ, v ′ ji = ξ. Hence ‖v′i − v′j‖2 =√\n2(1− ξ)2 + 2ξ2. On the other hand, if there is no edge (i, j) then there exist four edges (i, i′), (i, i′′), (j, j′) and (j, j′′) and non-zero entries of v′i and v ′ j are: v ′ ii = 1, v ′ ii′ = ξ, v′ii′′ = ξ, v ′ jj = 1, v ′ jj′ = ξ, v ′ jj′′ = ξ. Hence ‖v′i −\nv′j‖2 = √ 2 + 4ξ2. Maximum of the ratio √ 2+4ξ2√ 2(1−ξ)2+2ξ2 is achieved when ξ = 1/ √ 2 and equals √ 2 + √ 2.\nBy Proposition 3.4, if the input graph is one cycle then the cost of 2-SLC of v′1, . . . , v ′ n equals √ 2 √ 2− √\n2, otherwise it is 2. As Johnson-Lindenstrauss transform preserves all pairwise distances up to a multiplicative (1 ± ) factor with high probability the same is true for the cost of 2-SLC of v1, . . . , vn up to ± error. This completes the proof.\n3.2. Hardness of Hamming k-SLC\nTheorem 3.5. No algorithm for computing Hamming kSLC cost for d = 2 in o(log n) rounds of MPC can achieve better than 2-approximation under Conjecture 3.1.\nProof. Let G(V,E) be an instance of sparse connectivity. Our reduction to Hamming 2-SLC constructs an input set of 2-dimensional vectors as follows: 1) for each vertex i ∈ V create a vector (i, i), 2) or each edge (i, j) ∈ E create a vector (i, j). Clearly this reduction can be performed in a constant number of rounds of MPC and the resulting instance has |V |+ |E| = O(n) many vectors. We will show that if the input graph is connected the cost of Hamming 2-SLC of the input equals 1 and the cost is 2 otherwise. Indeed, note that the distances between resulting vectors are always either 1 or 2. If G is connected then it is easy to construct a connected spanning subgraph in the resulting Hamming graph where each edge has cost 1. Indeed, consider a subgraph that for each edge (i, j) in the input graph contains two edges: one between vectors (i, i) and (i, j) and another between vectors (j, j) and (i, j). Clearly, if the input graph is connected then this is a connected spanning subgraph. Hence the Hamming MST cost of the con-\nstructed point set equals |V |+ |E|−1 and the Hamming 2- SLC cost equals 1. On the other hand, if G is disconnected then consider any partitioning (S, T ) of G into connected components. Clearly, any two vectors representing vertices belonging to different parts of this partition in our reduction are at distance 2 from each other. This implies that the Hamming MST cost is at least |V |+ |E| and the Hamming 2-SLC cost is 2."
  }, {
    "heading": "4. Experiments",
    "text": "Small datasets Four standard clustering datasets used in the literature were taken for experimental evaluation: 1) Image dataset, d = 3, n = 34112 (house images, https://cs.joensuu.fi/sipu/datasets/), 2) KDDCUP04Bio dataset , d = 10, n = 145751 (preprocessed to select 10 numerical dimensions out of 74, accessed via the link above), 3) Shuttle data set from the UCI ML repository, d = 9, n = 43500. 4) US Census dataset from the UCI ML repository, d = 8, n = 2548285.\nDue to page limitations here, we only show plots for the largest Census dataset. Other plots are deferred to Appendix D. Figure 4 shows dependence of speedup as a function of approximation. We observe a dramatic increase in the speedup at around approximation 1.26 due to the fact the local inputs start to fit in L2-cache. Figure 5 shows dependence of approximation on k for the census data.\nLarge datasets In order to test scalability, we took the largest real-valued vector datasets from the UCI ML repository: SIFT10M and HIGGS. Both the datasets have approximately 11 million entries. Thus, constructing the full matrix of distances in memory is clearly infeasible as the size of this matrix would be roughly 960TB in both cases5. Dimension reduction for this data was done using PCA for d = 3. Results are given in Table 2."
  }, {
    "heading": "4.1. Experimental setup",
    "text": "We implemented Algorithm 2.1 in Java on Apache Spark 2.0.2 for Hadoop 2.7.3. Experiments were performed on two different setups:\nGoogle Cloud Dataproc (GCD) platform on two cluster 5Assuming 8-byte double-precision arithmetic.\nconfigurations: 1) single-core 1 master / 7 worker (1m/7w) cluster, 2) dual-core 1 master / 3 worker (1m/3w) cluster. Each core had an Intel Xeon E5 processor at 2.2–2.6 GHz and 3.75GB RAM + 10GB HDD space. Due to the limitations of the free tier access on GCD the total number of cores in a cluster is limited to 8, which is still sufficient to demonstrate at least an order of magnitude speedup over the benchmark sequential algorithm. This setup was used for the small datasets.\nLocal Simulation with 200 reducers on a Dell XPS13 Laptop with an Intel core I5 processor and 8GB RAM. This setup was used for the large datasets."
  }, {
    "heading": "5. Acknowledgements",
    "text": "This research was supported by NSF Award 1657477. The authors would like to thank Alexandr Andoni, Aleksandar Nikolov and Krzysztof Onak for multiple discussions of (Andoni et al., 2014) and its relationship to the singlelinkage clustering problem which led to this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Geometric approximation via coresets",
    "authors": ["Agarwal", "Pankaj K", "Har-Peled", "Sariel", "Varadarajan", "Kasturi R"],
    "venue": "Combinatorial and Computational Geometry (MSRI publication),",
    "year": 2005
  }, {
    "title": "Parallel algorithms for geometric graph problems",
    "authors": ["Andoni", "Alexandr", "Nikolov", "Aleksandar", "Onak", "Krzysztof", "Yaroslavtsev", "Grigory"],
    "venue": "In Symposium on Theory of Computing,",
    "year": 2014
  }, {
    "title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions",
    "authors": ["Arya", "Sunil", "Mount", "David M", "Netanyahu", "Nathan S", "Silverman", "Ruth", "Wu", "Angela Y"],
    "venue": "J. ACM,",
    "year": 1998
  }, {
    "title": "Distributed k-means and k-median clustering on general communication topologies",
    "authors": ["Balcan", "Maria-Florina", "Ehrlich", "Steven", "Liang", "Yingyu"],
    "venue": "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Communication steps for parallel query processing",
    "authors": ["Beame", "Paul", "Koutris", "Paraschos", "Suciu", "Dan"],
    "venue": "In Proceedings of the 32nd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,",
    "year": 2013
  }, {
    "title": "On distributing symmetric streaming computations",
    "authors": ["Feldman", "Jon", "S. Muthukrishnan", "Sidiropoulos", "Anastasios", "Stein", "Clifford", "Svitkina", "Zoya"],
    "venue": "In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2008
  }, {
    "title": "On the computational complexity of mapreduce",
    "authors": ["Fish", "Benjamin", "Kun", "Jeremy", "Lelkes", "Ádám Dániel", "Reyzin", "Lev", "Turán", "György"],
    "venue": "In Distributed Computing - 29th International Symposium,",
    "year": 2015
  }, {
    "title": "Sorting, searching, and simulation in the mapreduce framework",
    "authors": ["Goodrich", "Michael T", "Sitchinava", "Nodari", "Zhang", "Qin"],
    "venue": "In Algorithms and Computation - 22nd International Symposium,",
    "year": 2011
  }, {
    "title": "Minimum spanning trees and single linkage cluster analysis",
    "authors": ["Gower", "John C", "Ross", "GJS"],
    "venue": "Applied statistics,",
    "year": 1969
  }, {
    "title": "High-dimensional Computational Geometry",
    "authors": ["Indyk", "Piotr"],
    "venue": "PhD thesis, Stanford University,",
    "year": 2000
  }, {
    "title": "A scalable hierarchical clustering algorithm using spark",
    "authors": ["Jin", "Chen", "Liu", "Ruoqian", "Zhengzhang", "Hendrix", "William", "Agrawal", "Ankit", "Choudhary", "Alok"],
    "venue": "In Big Data Computing Service and Applications (BigDataService),",
    "year": 2015
  }, {
    "title": "A model of computation for mapreduce",
    "authors": ["Karloff", "Howard J", "Suri", "Siddharth", "Vassilvitskii", "Sergei"],
    "venue": "In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2010
  }, {
    "title": "Connected components in mapreduce and beyond",
    "authors": ["Kiveris", "Raimondas", "Lattanzi", "Silvio", "Mirrokni", "Vahab S", "Rastogi", "Vibhor", "Vassilvitskii", "Sergei"],
    "venue": "In Proceedings of the ACM Symposium on Cloud Computing,",
    "year": 2014
  }, {
    "title": "Spark GraphX in action",
    "authors": ["Malak", "Michael S", "East", "Robin"],
    "year": 2016
  }, {
    "title": "Introduction to information retrieval",
    "authors": ["Manning", "Christopher D", "Raghavan", "Prabhakar", "Schütze", "Hinrich"],
    "year": 2008
  }, {
    "title": "Finding connected components in map-reduce in logarithmic rounds",
    "authors": ["Rastogi", "Vibhor", "Machanavajjhala", "Ashwin", "Chitnis", "Laukik", "Sarma", "Anish Das"],
    "venue": "In 29th IEEE International Conference on Data Engineering,",
    "year": 2013
  }, {
    "title": "Shuffles and circuits: (on lower bounds for modern parallel computation)",
    "authors": ["Roughgarden", "Tim", "Vassilvitskii", "Sergei", "Wang", "Joshua R"],
    "venue": "In Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,",
    "year": 2016
  }, {
    "title": "Graph-theoretical methods for detecting and describing gestalt clusters",
    "authors": ["Zahn", "Charles T"],
    "venue": "Computers, IEEE Transactions on,",
    "year": 1971
  }],
  "id": "SP:577d3b33df9c9009763a157e9b22433cde4b22cf",
  "authors": [{
    "name": "Grigory Yaroslavtsev",
    "affiliations": []
  }, {
    "name": "Adithya Vadapalli",
    "affiliations": []
  }],
  "abstractText": "We present first massively parallel (MPC) algorithms and hardness of approximation results for computing Single-Linkage Clustering of n input d-dimensional vectors under Hamming, `1, `2 and `∞ distances. All our algorithms run in O(log n) rounds of MPC for any fixed d and achieve (1 + )-approximation for all distances (except Hamming for which we show an exact algorithm). We also show constant-factor inapproximability results for o(log n)-round algorithms under standard MPC hardness assumptions (for sufficiently large dimension depending on the distance used). Efficiency of implementation of our algorithms in Apache Spark is demonstrated through experiments on the largest available vector datasets from the UCI machine learning repository exhibiting speedups of several orders of magnitude.",
  "title": "Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under `p Distances"
}