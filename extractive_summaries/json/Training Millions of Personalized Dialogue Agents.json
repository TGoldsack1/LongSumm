{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775–2779 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2775"
  }, {
    "heading": "1 Introduction",
    "text": "End-to-end dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016). One of their main advantages is that they can rely on large data sources of existing dialogues to learn to cover various domains without requiring any expert knowledge. However, the flip side is that they also exhibit limited engagement, especially in chit-chat settings: they lack consistency and do not leverage proactive engagement strategies as (even partially) scripted chatbots do.\nZhang et al. (2018) introduced the PERSONACHAT dataset as a solution to cope with this issue. This dataset consists of dialogues between pairs of agents with text profiles, or personas, attached to\neach of them. As shown in their paper, conditioning an end-to-end system on a given persona improves the engagement of a dialogue agent. This paves the way to potentially end-to-end personalized chatbots because the personas of the bots, by being short texts, could be easily edited by most users. However, the PERSONA-CHAT dataset was created using an artificial data collection mechanism based on Mechanical Turk. As a result, neither dialogs nor personas can be fully representative of real user-bot interactions and the dataset coverage remains limited, containing a bit more than 1k different personas.\nIn this paper, we build a very large-scale persona-based dialogue dataset using conversations previously extracted from REDDIT1. With simple heuristics, we create a corpus of over 5 million personas spanning more than 700 million conversations. We train persona-based end-to-end dialogue models on this dataset. These models outperform their counterparts that do not have access to personas, confirming results of Zhang et al. (2018). In addition, the coverage of our dataset seems very good since pre-training on it also leads to state-of-the-art results on the PERSONA-CHAT dataset."
  }, {
    "heading": "2 Related work",
    "text": "With the rise of end-to-end dialogue systems, personalized trained systems have started to appear. Li et al. (2016) proposed to learn latent variables representing each speaker’s bias/personality in a dialogue model. Other classic strategies include extracting explicit variables from structured knowledge bases or other symbolic sources as in (Ghazvininejad et al., 2017; Joshi et al., 2017; Young et al., 2017). Still, in the context of per-\n1https://www.reddit.com/r/datasets/ comments/3bxlg7/\nsonal chatbots, it might be more desirable to condition on data that can be generated and interpreted by the user itself such as text rather than relying on some knowledge base facts that might not exist for everyone or a great variety of situations. PERSONA-CHAT (Zhang et al., 2018) recently introduced a dataset of conversations revolving around human habits and preferences. In their experiments, they showed that conditioning on a text description of each speaker’s habits, their persona, improved dialogue modeling.\nIn this paper, we use a pre-existing REDDIT data dump as data source. REDDIT is a massive online message board. Dodge et al. (2015) used it to assess chit-chat qualities of generic dialogue models. Yang et al. (2018) used response prediction on REDDIT as an auxiliary task in order to improve prediction performance on natural language inference problems."
  }, {
    "heading": "3 Building a dataset of millions of persona-based dialogues",
    "text": "Our goal is to learn to predict responses based on a persona for a large variety of personas. To that end, we build a dataset of examples of the following form using data from REDDIT:\n• Persona: [“I like sport”, “I work a lot”] • Context: “I love running.” • Response: “Me too! But only on weekends.”\nThe persona is a set of sentences representing the personality of the responding agent, the context is the utterance that it responds to, and the response is the answer to be predicted."
  }, {
    "heading": "3.1 Preprocessing",
    "text": "As in (Dodge et al., 2015), we use a preexisting dump of REDDIT that consists of 1.7 billion comments. We tokenize sentences by padding all special characters with a space and splitting on whitespace characters. We create a dictionary containing the 250k most frequent tokens. We truncate comments that are longer than 100 tokens."
  }, {
    "heading": "3.2 Persona extraction",
    "text": "We construct the persona of a user by gathering all the comments they wrote, splitting them into sentences, and selecting the sentences that satisfy the following rules: (i) each sentence must contain between 4 and 20 words or punctuation marks, (ii) it contains either the word I or my, (iii) at least\none verb, and (iv) at least one noun, pronoun or adjective.\nTo handle the quantity of data involved, we limit the size of a persona to N sentences for each user. We compare four different setups for persona creation. In the rules setup, we select up to N random sentences that satisfy the rules above. In the rules + classifier setup, we filter with the rules then score the resulting sentences using a bag-of-words classifier that is trained to discriminate PERSONACHAT persona sentences from random comments. We manually tune a threshold on the score in order to select sentences. If there are more than N eligible persona sentences for a given user, we keep the highest-scored ones. In the random from user setup, we randomly select sentences uttered by the user while keeping the sentence length requirement above (we ignore the other rules). The random from dataset baseline refers to random sentences from the dataset. They do not necessarily come from the same user. This last setup serves as a control mechanism to verify that the gains in prediction accuracy are due to the user-specific information contained in personas.\nIn the example at the beginning of this section, the response is clearly consistent with the persona. There may not always be such an obvious relationship between the two: the discussion topic may not be covered by the persona, a single user may write contradictory statements, and due to errors in the extraction process, some persona sentences may not represent a general trait of the user (e.g. I am feeling happy today)."
  }, {
    "heading": "3.3 Dataset creation",
    "text": "We take each pair of successive comments in a thread to form the context and response of an example. The persona corresponding to the response is extracted using one of the methods of Section 3.2. We split the dataset randomly between training, validation and test. Validation and test sets contain 50k examples each. We extract personas using training data only: test set responses cannot be contained explicitly in the persona.\nIn total, we select personas covering 4.6m users in the rule-based setups and 7.2m users in the random setups. This is a sizable fraction of the total 13.2m users of the dataset; depending on the persona selection setup, between 97 and 99.4 % of the training set examples are linked to a persona."
  }, {
    "heading": "4 End-to-end dialogue models",
    "text": "We model dialogue by next utterance retrieval (Lowe et al., 2016), where a response is picked among a set of candidates and not generated."
  }, {
    "heading": "4.1 Architecture",
    "text": "The overall architecture is depicted in Fig. 1. We encode the persona and the context using separate modules. As in Zhang et al. (2018), we combine the encoded context and persona using a 1-hop memory network with a residual connection, using the context as query and the set of persona sentences as memory. We also encode all candidate responses and compute the dot-product between all those candidate representations and the joint representation of the context and the persona. The predicted response is the candidate that maximizes the dot product.\nWe train by passing all the dot products through a softmax and maximizing the log-likelihood of the correct responses. We use mini-batches of training examples and, for each example therein, all the responses of the other examples of the same batch are used as negative responses."
  }, {
    "heading": "4.2 Context and response encoders",
    "text": "Both context and response encoders share the same architecture and word embeddings but have different weights in the subsequent layers. We train three different encoder architectures.\nBag-of-words applies two linear projections separated by a tanh non-linearity to the word embeddings. We then sum the resulting sentence representation across all positions in the sentence and divide the result by √ n where n is the length of the sequence.\nLSTM applies a 2-layer bidirectional LSTM. We use the last hidden state as encoded sentence.\nTransformer is a variation of an End-to-end Memory Network (Sukhbaatar et al., 2015) introduced by Vaswani et al. (2017). Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in dialogues (Yang et al., 2018). Here we use only its encoding module. We subsequently average the resulting representation across all positions in the sentence, yielding a fixed-size representation."
  }, {
    "heading": "4.3 Persona encoder",
    "text": "The persona encoder encodes each persona sentence separately. It relies on the same word embeddings as the context encoder and applies a linear layer on top of them. We then sum the representations across the sentence.\nWe deliberately choose a simpler architecture than the other encoders for performance reasons as the number of personas encoded for each batch is an order of magnitude greater than the number of training examples. Most personas are short sentences; we therefore expect a bag-of-words representation to encode them well."
  }, {
    "heading": "5 Experiments",
    "text": "We train models on the persona-based dialogue dataset described in Section 3.3 and we evaluate its accuracy both on the original task and when transferring onto PERSONA-CHAT."
  }, {
    "heading": "5.1 Experimental details",
    "text": "We optimize network parameters using Adamax with a learning rate of 8e−4 on mini-batches of size 512. We initialize embeddings with FastText word vectors and optimize them during learning.\nREDDIT LSTMs use a hidden size of 150; we concatenate the last hidden states for both directions and layers, resulting in a final representation of size 600. Transformer architectures on reddit use 4 layers with a hidden size of 300 and 6 attention heads, resulting in a final representation of size 300. We use Spacy for part-of-speech tagging in order to verify the persona extraction rules. We distribute the training by splitting each batch across 8 GPUs; we stop training after 1 full epoch, which takes about 3 days.\nPERSONA-CHAT We used the revised version of the dataset where the personas have been rephrased, making it a harder task. The dataset being only a few thousands samples, we had to reduce the architecture to avoid overfitting for the models trained purely on PERSONA-CHAT. 2 layers, 2 attention heads, a dropout of 0.2 and keeping the size of the word embeddings to 300 units yield the highest accuracy on the validation set.\nIR Baseline As basic baseline, we use an information retrieval (IR) system that ranks candidate responses according to a TF-IDF weighted exactmatch similarity with the context alone."
  }, {
    "heading": "5.2 Results",
    "text": "Impact of personas We report the accuracy of the different architectures on the reddit task in Table 1. Conditioning on personas improves the prediction performance regardless of the encoder architecture. Table 2 gives some examples of how the persona affects the predicted answer.\nInfluence of the persona extraction In Table 3, we report precision results for several persona extraction setups. The rules setup improves the results somewhat, however adding the persona classifier actually degrades the results. A possible interpretation is that the persona classifier is trained only on the PERSONA-CHAT revised personas, and that this selection might be too narrow and lack di-\nversity. Increasing the maximum persona size also improves the prediction performance.\nTransfer learning We compare the performance of transformer models trained on REDDIT and on PERSONA-CHAT on both datasets. We report results in Table 4. This architecture provides a strong improvement over the results of (Zhang et al., 2018), jumping from 35.4% hits@1 to 42.1%. Pretraining the model on REDDIT and then fine-tuning on PERSONA-CHAT pushes this score to 60.7%, largely improving the state of the art. As expected, fine-tuning on PERSONA-CHAT reduces the performance on REDDIT. However, directly testing on PERSONA-CHAT the model trained on REDDIT without fine-tuning yields a very low result. This could be a consequence of a discrepancy\nbetween the style of personas of the two datasets."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper shows how to create a very large dataset for persona-based dialogue. We show that training models to align answers both with the persona of their author and the context improves the predicting performance. The trained models show promising coverage as exhibited by the stateof-the-art transfer results on the PERSONA-CHAT dataset. As pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems. Future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning end-to-end goal-oriented dialog",
    "authors": ["Antoine Bordes", "Y-Lan Boureau", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1605.07683.",
    "year": 2016
  }, {
    "title": "Evaluating prerequisite qualities for learning end-to-end dialog systems",
    "authors": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1511.06931.",
    "year": 2015
  }, {
    "title": "A knowledge-grounded neural conversation model",
    "authors": ["Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley."],
    "venue": "CoRR, abs/1702.01932.",
    "year": 2017
  }, {
    "title": "Personalization in goal-oriented dialog",
    "authors": ["Chaitanya K Joshi", "Fei Mi", "Boi Faltings."],
    "venue": "arXiv preprint arXiv:1706.07503.",
    "year": 2017
  }, {
    "title": "A persona-based neural conversation model",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "CoRR, abs/1603.06155.",
    "year": 2016
  }, {
    "title": "On the evaluation of dialogue systems with next utterance classification",
    "authors": ["Ryan Lowe", "Iulian V Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau."],
    "venue": "arXiv preprint arXiv:1605.05414.",
    "year": 2016
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C Courville", "Joelle Pineau."],
    "venue": "AAAI, volume 16, pages 3776–3784.",
    "year": 2016
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."],
    "venue": "Proceedings of NIPS.",
    "year": 2015
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "CoRR, abs/1706.03762.",
    "year": 2017
  }, {
    "title": "A networkbased end-to-end trainable task-oriented dialogue system",
    "authors": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."],
    "venue": "arXiv preprint arXiv:1604.04562.",
    "year": 2016
  }, {
    "title": "Learning semantic textual similarity from conversations",
    "authors": ["Yinfei Yang", "Steve Yuan", "Daniel Cer", "Sheng-yi Kong", "Noah Constant", "Petr Pilar", "Heming Ge", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil."],
    "venue": "CoRR, abs/1804.07754.",
    "year": 2018
  }, {
    "title": "Augmenting end-to-end dialog systems with commonsense knowledge",
    "authors": ["Tom Young", "Erik Cambria", "Iti Chaturvedi", "Minlie Huang", "Hao Zhou", "Subham Biswas."],
    "venue": "arXiv preprint arXiv:1709.05453.",
    "year": 2017
  }, {
    "title": "Personalizing dialogue agents: I have a dog, do you have pets too? CoRR, abs/1801.07243",
    "authors": ["Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston"],
    "year": 2018
  }],
  "id": "SP:dde89e64a7f375b90e1cc594142940f4161e1592",
  "authors": [{
    "name": "Pierre-Emmanuel Mazaré",
    "affiliations": []
  }, {
    "name": "Samuel Humeau",
    "affiliations": []
  }, {
    "name": "Martin Raison",
    "affiliations": []
  }, {
    "name": "Antoine Bordes",
    "affiliations": []
  }],
  "abstractText": "Current dialogue systems are not very engaging for users, especially when trained end-toend without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in (Zhang et al., 2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from (Zhang et al., 2018) and achieving state-of-the-art results.",
  "title": "Training Millions of Personalized Dialogue Agents"
}