{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 420–425 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n420"
  }, {
    "heading": "1 Introduction",
    "text": "Recently, more and more attention from both academia and industry is paying to building nontask-oriented chatbots that can naturally converse with humans on any open domain topics. Existing approaches can be categorized into generationbased methods (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Serban et al., 2017; Xing et al., 2018) which synthesize a response with natural language generation techniques, and retrievalbased methods (Hu et al., 2014; Lowe et al., 2015; Yan et al., 2016; Zhou et al., 2016; Wu et al., 2017) which select a response from a pre-built index. In this work, we study response selection for retrieval-based chatbots, not only because retrieval-based methods can return fluent and informative responses, but also because they have been successfully applied to many real products such as the social-bot XiaoIce from Microsoft (Shum et al., 2018) and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017).\n∗ Corresponding Author\nA key step to response selection is measuring the matching degree between a response candidate and an input which is either a single message (Hu et al., 2014) or a conversational context consisting of multiple utterances (Wu et al., 2017). While existing research focuses on how to define a matching model with neural networks, little attention has been paid to how to learn such a model when few labeled data are available. In practice, because human labeling is expensive and exhausting, one cannot have large scale labeled data for model training. Thus, a common practice is to transform the matching problem to a classification problem with human responses as positive examples and randomly sampled ones as negative examples. This strategy, however, oversimplifies the learning problem, as most of the randomly sampled responses are either far from the semantics of the messages or the contexts, or they are false negatives which pollute the training data as noise. As a result, there often exists a significant gap between the performance of a model in training and the same model in practice (Wang et al., 2015; Wu et al., 2017).1\nWe propose a new method that can effectively leverage unlabeled data for learning matching models. To simulate the real scenario of a retrieval-based chatbot, we construct an unlabeled data set by retrieving response candidates from an index. Then, we employ a weak annotator to provide matching signals for the unlabeled inputresponse pairs, and leverage the signals to supervise the learning of matching models. The weak annotator is pre-trained from large scale humanhuman conversations without any annotations, and thus a Seq2Seq model becomes a natural choice. Our approach is compatible with any matching models, and falls in a teacher-student framework\n1The model performs well on randomly sampled data, but badly on human labeled data.\n(Hinton et al., 2015) where the Seq2Seq model transfers the knowledge from human-human conversations to the learning process of the matching models. Broadly speaking, both of (Hinton et al., 2015) and our work let a neural network supervise the learning of another network. An advantage of our method is that it turns the hard zero-one labels in the existing learning paradigm to soft (weak) matching scores. Hence, the model can learn a large margin between a true response with a true negative example, and the semantic distance between a true response and a false negative example is short. Furthermore, due to the simulation of real scenario, harder examples can been seen in the training phase that makes the model more robust in the testing.\nWe conduct experiments on two public data sets, and experimental results on both data sets indicate that models learned with our method can significantly outperform their counterparts learned with the random sampling strategy.\nOur contributions include: (1) proposal of a new method that can leverage unlabeled data to learn matching models for retrieval-based chatbots; and (2) empirical verification of the effectiveness of the method on public data sets."
  }, {
    "heading": "2 Approach",
    "text": ""
  }, {
    "heading": "2.1 The Existing Learning Approach",
    "text": "Given a data setD = {xi, (yi,1, . . . , yi,n)}Ni=1 with xi a message or a conversational context and yi,j a response candidate of xi, we aim to learn a matching modelM(·, ·) from D. Thus, for any new pair (x, y),M(x, y) measures the matching degree between x and y.\nTo obtain a matching model, one has to deal with two problems: (1) how to defineM(·, ·); and (2) how to perform learning. Existing work focuses on Problem (1) where state-of-the-art methods include dual LSTM (Lowe et al., 2015), MultiView LSTM (Zhou et al., 2016), CNN (Yan et al., 2016), and Sequential Matching Network (Wu et al., 2017), but adopts a simple strategy for Problem (2): ∀xi, a human response is designated as yi,1 with a label 1, and some randomly sampled responses are treated as (yi,2, . . . , yi,n) with labels 0. M(·, ·) is then learned by maximizing the following objective: ∑N i=1 ∑n j=1 [ri,j log(M(xi, yi,j)) + (1− ri,j) log(1−M(xi, yi,j))] ,\n(1)\nwhere ri,j ∈ {0, 1} is a label. While matching accuracy can be improved by carefully designing M(·, ·) (Wu et al., 2017), the bottleneck becomes the learning approach which suffers obvious problems: most of the randomly sampled yi,j are semantically far from xi which may cause an undesired decision boundary at the end of optimization; some yi,j are false negatives. As hard zero-one labels are adopted in Equation (1), these false negatives may mislead the learning algorithm. The problems remind us that besides good architectures of matching models, we also need a good approach to learn such models from data."
  }, {
    "heading": "2.2 A New Learning Method",
    "text": "As human labeling is infeasible when training complicated neural networks, we propose a new method that can leverage unlabeled data to learn a matching model. Specifically, instead of random sampling, we construct D by retrieving (yi,2, . . . , yi,n) from an index (yi,1 is the human response of xi). By this means, some yi,j are true positives, and some are negatives but semantically close to xi. After that, we employ a weak annotatorG(·, ·) to indicate the matching degree of every (xi, yi,j) in D as weak supervision signals. Let sij = G(xi, yi,j), then the learning approach can be formulated as:\nargmin M(·,·) N∑ i=1 n∑ j=1 max(0,M(xi, yi,j)−M(xi, yi,1)+ s′i,j),\n(2)\nwhere s′ij is a normalized weak signal defined as max(0,\nsi,j si,1 − 1). The normalization here elimi-\nnates bias from different xi. Objective (2) encourages a large margin between the matching of an input and its human response and the matching of the input and a negative response judged by G(·, ·) (as will be seen later, si,jsi,1 > 1). The learning approach simulates how we build a matching model in a retrievalbased chatbot: given {xi}, some response candidates are first retrieved from an index. Then human annotators are hired to judge the matching degree of each pair. Finally, both the data and the human labels are fed to an optimization program for model training. Here, we replace the expensive human labels with cheap judgment from G(·, ·).\nWe define G(·, ·) as a sequence-to-sequence architecture (Vinyals and Le, 2015) with an attention mechanism (Bahdanau et al., 2015), and pre-train it with large amounts of human-human conversa-\ntion data. The Seq2Seq model can capture the semantic correspondence between an input and a response, and then transfer the knowledge to the learning of a matching model in the optimization of (2). sij is then defined as the likelihood of generating yi,j from xi:\nsij = ∑ k log[p(wyi,j ,k, |xi, wyi,j ,l<k)], (3)\nwhere wyi,j ,k is the k-th word of yi,j and wyi,j ,l<k is the word sequence before wyi,j ,k.\nSince negative examples are retrieved by a search engine, the oversimplification problem of the negative sampling approach can be partially mitigated. We leverage a weak annotator to assign a score for each example to distinguish false negative examples and true negative examples. Equation (2) turns the hard zero-one labels in Equation (1) to soft matching degrees, and thus our method encourages the model to be more confident to classify a response with a high si,j score as a negative one. In this way, we can avoid false negative examples and true negative examples are treated equally during training, and update the model toward a correct direction.\nIt is noteworthy that although our approach also involves an interaction between a generator and a discriminator, it is different from the GANs (Goodfellow et al., 2014) in principle. GANs try to learn a better generator via an adversarial process, while our approach aims to improve the discriminator with supervision from the generator, which also differentiates it from the recent work on transferring knowledge from a discriminator to a generative visual dialog model (Lu et al., 2017). Our approach is also different from those semi-supervised approaches in the teacher-student framework (Dehghani et al., 2017a,b), as there are no labeled data in learning."
  }, {
    "heading": "3 Experiment",
    "text": "We conduct experiments on two public data sets: STC data set (Wang et al., 2013) for single-turn response selection and Douban Conversation Corpus (Wu et al., 2017) for multi-turn response selection. Note that we do not test the proposed approach on Ubuntu Corpus (Lowe et al., 2015), because both training and test data in the corpus are constructed by random sampling."
  }, {
    "heading": "3.1 Implementation Details",
    "text": "We implement our approach with TensorFlow. In both experiments, the same Seq2Seq model is exploited which is trained with 3.3 million inputresponse pairs extracted from the training set of the Douban data. Each input is a concatenation of consecutive utterances in a context, and the response is the next turn ({u<i}, ui). We set the vocabulary size as 30, 000, the hidden vector size as 1024, and the embedding size as 620. Optimization is conducted with stochastic gradient descent (Bottou, 2010), and is terminated when perplexity on a validation set (170k pairs) does not decrease in 3 consecutive epochs. In optimization of Objective (2), we initialize M(·, ·) with a model trained under Objective (1) with the (random) negative sampling strategy, and fix word embeddings throughout training. This can stabilize the learning process. The learning rate is fixed as 0.1."
  }, {
    "heading": "3.2 Single-turn Response Selection",
    "text": "Experiment settings: in the STC (stands for Short Text Conversation) data set, the task is to select a proper response for a post in Weibo2. The training set contains 4.8 million post-response (true response) pairs. The test set consists of 422 posts with each one associated with around 30 responses labeled by human annotators in “good” and “bad”. In total, there are 12, 402 labeled pairs in the test data. Following (Wang et al., 2013, 2015), we combine the score from a matching model with TF-IDF based cosine similarity using RankSVM whose parameters are chosen by 5-fold cross validation. Precision at position 1 (P@1) is employed as an evaluation metric. In addition to the models compared on the data in the existing literatures, we also implement dual LSTM (Lowe et al., 2015) as a baseline. As case studies, we learn a dual LSTM and an CNN (Hu et al., 2014) with the proposed approach, and denote them as LSTM+WS (Weak Supervision) and CNN+WS, respectively. When constructing D, we build an index with the training data using Lucene3 and retrieve 9 candidates (i.e., {yi,2, . . . , yi,n}) for each post with the inline algorithm of the index. We form a validation set by randomly sampling 10 thousand posts associated with the responses from D (human response is positive and others are treated as negative).\nResults: Table 1 reports the results. We can see 2http://weibo.sina.com 3https://lucenenet.apache.org/\nthat CNN and LSTM consistently get improved when learned with the proposed approach, and the improvements over the models learned with random sampling are statistically significant (ttest with p-value < 0.01). LSTM+WS even surpasses the best performing model, DeepMatchtree, reported on this data. These results indicate the usefulness of the proposed approach in practice. One can expect improvements to models like DeepMatchtree with the new learning method. We leave the verification as future work."
  }, {
    "heading": "3.3 Multi-turn Response Selection",
    "text": "Experiment settings: Douban Conversation Corpus contains 0.5 million context-response (true response) pairs for training and 1000 contexts for test. In the test set, every context has 10 response candidates, and each of the response has a label “good” or “bad” judged by human annotators. Mean average precision (MAP) (BaezaYates et al., 1999), mean reciprocal rank (MRR) (Voorhees, 1999), and precision at position 1 (P@1) are employed as evaluation metrics. We copy the numbers reported in (Wu et al., 2017) for the baseline models, and learn LSTM, Multi-View, and SMN with the proposed approach. We build an index with the training data, and retrieve 9 candidates with the method in (Wu et al., 2017) for each context when constructing D. 10 thousand pairs are sampled from D as a validation set.\nResults: Table 2 reports the results. Consistent with the results on the STC data, every model (+WS one) gets improved with the new learning approach, and the improvements are statistically significant (t-test with p-value < 0.01)."
  }, {
    "heading": "3.4 Discussion",
    "text": "Ablation studies: we first replace the weak supervision s′i,j in Equation (2) with a constant selected from {0.1, 0.2, . . . , 0.9} on validation, and denote the models as model+const. Then, we keep\neverything the same as our approach but replace D with a set constructed by random sampling, denoted as model+WSrand. Table 3 reports the results. We can conclude that both the weak supervision and the strategy of training data construction are important to the success of the proposed learning approach. Training data construction plays a more crucial role, because it involves more true positives and negatives with different semantic distances to the positives into learning.\nDoes updating the Seq2Seq model help? It is well known that Seq2Seq models suffer from the “safe response” (Li et al., 2016a) problem, which may bias the weak supervision signals to high-frequency responses. Therefore, we attempt to iteratively optimize the Seq2Seq model and the matching model and check if the matching model can be further improved. Specifically, we update the Seq2Seq model every 20 mini-batches with the policy-based reinforcement learning approach proposed in (Li et al., 2016b). The reward is defined as the matching score of a context and a response given by the matching model. Unfortunately, we do not observe significant improvement on the matching model. The result is attributed to two factors: (1) it is difficult to significantly im-\nprove the Seq2Seq model with a policy gradient based method; and (2) eliminating “safe response” for Seq2Seq model cannot help a matching model to learn a better decision boundary.\nHow the number of response candidates affects learning: we vary the number of {yi,j}nj=1 in D in {2, 5, 10, 20} and study how the hyperparameter influences learning. We study with LSTM on the STC data and SMN on the Douban data. Table 4 reports the results. We can see that as the number of candidates increases, the performance of the the learned models becomes better. Even with 2 candidates (one from human and the other from retrieval), our approach can still improve the peformance of matching models."
  }, {
    "heading": "4 Conclusion and Future Work",
    "text": "Previous studies focus on architecture design for retrieval-based chatbots, but neglect the problems brought by random negative sampling in the learning process. In this paper, we propose leveraging a Seq2Seq model as a weak annotator on unlabeled data to learn a matching model for response selection. By this means, we can mine hard instances for matching model and give them scores with a weak annotator. Experimental results on public data sets verify the effectiveness of the new learning approach. In the future, we will investigate how to remove bias from the weak supervisors, and further improve the matching model performance with a semi-supervised approach."
  }, {
    "heading": "Acknowledgment",
    "text": "Yu Wu is supported by Microsoft Fellowship Scholarship and AdeptMind Scholarship. This work is supported by the National Natural Science Foundation of China (Grand Nos. 61672081,U1636211,61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001)."
  }],
  "year": 2018,
  "references": [{
    "title": "Modern information retrieval, volume 463",
    "authors": ["Ricardo Baeza-Yates", "Berthier Ribeiro-Neto"],
    "year": 1999
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR .",
    "year": 2015
  }, {
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["Léon Bottou."],
    "venue": "Proceedings of COMPSTAT’2010, Springer, pages 177–186.",
    "year": 2010
  }, {
    "title": "Fidelity-weighted learning",
    "authors": ["Mostafa Dehghani", "Arash Mehrjou", "Stephan Gouws", "Jaap Kamps", "Bernhard Schölkopf."],
    "venue": "arXiv preprint arXiv:1711.02799 .",
    "year": 2017
  }, {
    "title": "Avoiding your teacher’s mistakes: Training neural networks with controlled weak supervision",
    "authors": ["Mostafa Dehghani", "Aliaksei Severyn", "Sascha Rothe", "Jaap Kamps."],
    "venue": "arXiv preprint arXiv:1711.00313 .",
    "year": 2017
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Advances in neural information processing systems. pages 2672–2680.",
    "year": 2014
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."],
    "venue": "arXiv preprint arXiv:1503.02531 .",
    "year": 2015
  }, {
    "title": "Convolutional neural network architectures for matching natural language sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "Advances in Neural Information Processing Systems. pages 2042–2050.",
    "year": 2014
  }, {
    "title": "Alime assist: An intelligent assistant for creating an innovative e-commerce experience",
    "authors": ["Feng-Lin Li", "Minghui Qiu", "Haiqing Chen", "Xiongwei Wang", "Xing Gao", "Jun Huang", "Juwei Ren", "Zhongzhou Zhao", "Weipeng Zhao", "Lei Wang"],
    "year": 2017
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for",
    "year": 2016
  }, {
    "title": "Deep reinforcement learning for dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,",
    "year": 2016
  }, {
    "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."],
    "venue": "SIGDIAL .",
    "year": 2015
  }, {
    "title": "Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model",
    "authors": ["Jiasen Lu", "Anitha Kannan", "Jianwei Yang", "Devi Parikh", "Dhruv Batra."],
    "venue": "Advances in Neural Information Processing Systems. pages 313–323.",
    "year": 2017
  }, {
    "title": "A deep architecture for matching short texts",
    "authors": ["Zhengdong Lu", "Hang Li."],
    "venue": "Advances in Neural Information Processing Systems. pages 1367–1375.",
    "year": 2013
  }, {
    "title": "End-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intel-",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "AAAI. pages 3295–3301.",
    "year": 2017
  }, {
    "title": "Neural responding machine for short-text conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. pages 1577–1586.",
    "year": 2015
  }, {
    "title": "From eliza to xiaoice: Challenges and opportunities with social chatbots",
    "authors": ["Heung-Yeung Shum", "Xiaodong He", "Di Li."],
    "venue": "arXiv preprint arXiv:1801.01957 .",
    "year": 2018
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "NAACL HLT",
    "year": 2015
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "arXiv preprint arXiv:1506.05869 .",
    "year": 2015
  }, {
    "title": "The TREC-8 question answering track report",
    "authors": ["Ellen M. Voorhees."],
    "venue": "Proceedings of The Eighth Text REtrieval Conference, TREC 1999, Gaithersburg, Maryland, USA, November 17-19, 1999.",
    "year": 1999
  }, {
    "title": "A dataset for research on short-text conversations",
    "authors": ["Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013,",
    "year": 2013
  }, {
    "title": "Syntax-based deep matching of short texts",
    "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."],
    "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence.",
    "year": 2015
  }, {
    "title": "Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots",
    "authors": ["Yu Wu", "Wei Wu", "Chen Xing", "Ming Zhou", "Zhoujun Li."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Compu-",
    "year": 2017
  }, {
    "title": "Topic aware neural response generation",
    "authors": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."],
    "venue": "AAAI 2017. pages 3351–3357.",
    "year": 2017
  }, {
    "title": "Hierarchical recurrent attention network for response generation",
    "authors": ["Chen Xing", "Wei Wu", "Yu Wu", "Ming Zhou", "Yalou Huang", "Wei-Ying Ma."],
    "venue": "AAAI-18 .",
    "year": 2018
  }, {
    "title": "Learning to respond with deep neural networks for retrievalbased human-computer conversation system",
    "authors": ["Rui Yan", "Yiping Song", "Hua Wu."],
    "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Infor-",
    "year": 2016
  }, {
    "title": "Multi-view response selection for humancomputer conversation",
    "authors": ["Xiangyang Zhou", "Daxiang Dong", "Hua Wu", "Shiqi Zhao", "Dianhai Yu", "Hao Tian", "Xuan Liu", "Rui Yan."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
    "year": 2016
  }],
  "id": "SP:9d6689e20f9e999199d3af3ebe7c1607df9cfa4c",
  "authors": [{
    "name": "Yu Wu",
    "affiliations": []
  }, {
    "name": "Wei Wu",
    "affiliations": []
  }, {
    "name": "Zhoujun Li",
    "affiliations": []
  }, {
    "name": "Ming Zhou",
    "affiliations": []
  }],
  "abstractText": "We propose a method that can leverage unlabeled data to learn a matching model for response selection in retrieval-based chatbots. The method employs a sequence-tosequence architecture (Seq2Seq) model as a weak annotator to judge the matching degree of unlabeled pairs, and then performs learning with both the weak signals and the unlabeled data. Experimental results on two public data sets indicate that matching models get significant improvements when they are learned with the proposed method.",
  "title": "Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots"
}