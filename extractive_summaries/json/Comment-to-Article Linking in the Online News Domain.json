{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2015 Conference, pages 245–249, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "User comments on news articles and other online content provide a communication channel between journalists and their audience, which has previously replaced prevalent one-way reporting from journalists to their readers. Therefore, several user groups in media business now rely on online commenting to build and maintain their reputation and broaden their readers and customer base. To achieve this, however, it is essential to foster high quality discussions in online commenting forums because quality and tone of comments are shown to influence the readers’ attitudes to online news content (Anderson et al., 2013; Diakopoulos and Naaman, 2011; Santana, 2014). In the present set up of online forums, comments are difficult to organize, read and engage with, which affects the quality of discussion and the usefulness of comments for the interested parties. One problem with comments in their current form is their detachment from the original article. Placed at the end of the article without clear reference to the parts of the article that triggered them, comments are hard to put into the context from which they originated, and this makes them difficult to interpret and evaluate. Comment-article linking is also necessary in more complex systems for information extraction from comments\nsuch as comment summarization (Hu et al., 2008; Khabiri et al., 2011; Lu et al., 2009; Ma et al., 2012; Llewellyn et al., 2014). Such systems rely on identifying relevant comments and those that link to the articles are good candidates. In this paper we report the results of our experiments in comment-article linking. Specifically, the task is to bring together readers’ comments with online news article segments that comments refer to. We compare the performance of text similarity measures to that of more elaborate topic modeling methods such as the ones proposed by Sil et al. (2011) and Das et al. (2014) and demonstrate that comparable linking results can be achieved by simpler text similarity methods. Given the weak lexical overlap between comments and source articles, we also investigate the effect of alternative representations of comments and news article texts on the results of commentarticle linking with similarity metrics. We analyze the performance of the similarity method using terms, i.e., sequences of words which have all a meaning in a domain (de Bessé et al., 1997), and show that term based similarity linking outperforms similarity linking based on words. The paper starts with defining the linking task and the pre-processing steps we perform on the article and comments (Section 2). Then we provide the description of our linking approach (Section 3). In Section 4 we report our experimental results. We summarize the paper in Section 5."
  }, {
    "heading": "2 Task and Pre-processing",
    "text": ""
  }, {
    "heading": "2.1 The task",
    "text": "For the linking task we assume a news article A is divided into n segments S(A) = s1, ..., sn. The article A is also associated with a set of comments C(A) = c1, ..., cl. The task is to link comments c ∈ C(A) with article segments s ∈ S(A). We express the strength of link between a comment c and an article segment s as their linking score (Score). A comment c and an article segment s are linked if and only if their Score exceeds a threshold, which we experimentally optimized. Score has the range [0, 1], 0 indicating no linking and 1 defining a strong link."
  }, {
    "heading": "2.2 Pre-processing",
    "text": "First, we split the news article into segments. To compare results with existing data sets and exist-\n245\ning contributions, we comply with segmentation approaches used in previous work (Sil et al., 2011; Das et al., 2014). We treat each article sentence as a segment and group each comment into a single unit regardless of the number of sentences it contains. Then each sentence-comment pair is preprocessed before it is analyzed for linking. The example in Table 2.2 illustrates the outputs of the pre-processing pipeline. The pre-processing includes tokenization1 and lemmatization (step 2) in in Table 2.2, where an original article sentence is shown in step 1)). Next, we use either words with stop-word removal (step 3)) or terms (shown in 4) where each term is split by a semicolon) to represent the article sentence and also each comment. Terms are extracted using the freely available term extraction tool Tilde’s Wrapper System for CollTerm (TWSC)2 (Pinnis et al., 2012). We also record named entities (NEs) (shown in 5)) extracted from either article segments or comments."
  }, {
    "heading": "3 Method",
    "text": "This work investigates a simple method for linking comments and news article sentences using a linear combination of similarity scores as computed through a number of different similarity metrics (features). However, some comments directly quote article segments verbatim, therefore explicitly linking comments to article segments. To account for this, we consider a comment and an article sentence linked if their quotation score (quoteScore) exceeds a threshold. Otherwise, a similarity score is computed and articles are linked if their similarity score is above a threshold. The following paragraphs describe how features and thresholds are computed. Each metric is computed based on the comment c ∈ C(A) and a segment s ∈ S(A) as input. We pair every segment from S(A) with every comment from C(A). With this set up we are able to link one-to-many comments with one segment and also one-to-many segments with a particular comment, which implements an n to m commentsegment linking schema."
  }, {
    "heading": "3.1 Quotation Based Linking",
    "text": "We link all comments including quotes to the article sentences they quote. To determine whether a segment is quoted in the comment, we compute quoteScore = len(quote)/len(S) with len 3. len returns the number of words of the given input\n1For shallow analysis we use the OpenNLP tools: https://opennlp.apache.org.\n2TWSC uses POS-tag grammars to detect word collocations producing NP-like word sequences that we refer to as terms. Terms are extracted from the original version of the sentences, but words in the terms are replaced with their lemmas.\n3For this feature the original version, i.e., without preprocessing, of article segment and comment are used.\nand quote is a place holder for consecutive news article words found in the same order within the comment. If the quoteScore exceeds an experimentally set threshold of 0.5 (50% of consecutive article segment words are found in the same order within the comment), then the segment is regarded as quoted in the comment, the commentsegment pair is linked, their linking Score is set to quoteScore and no further linking features are considered. However, qualitative observations on random data portions have shown that only sentences longer than 10 words render meaningful quote scores, so we add this as an additional constraint."
  }, {
    "heading": "3.2 Similarity Linking",
    "text": ""
  }, {
    "heading": "3.2.1 Similarity Feature Extraction",
    "text": "If a comment does not contain a quote as described above, we compute the following features to obtain the value of the similarity score without considering the quote feature:\n• Cosine: The cosine similarity (Salton and Lesk, 1968) computes the cosine angle between two vectors. We fill the vectors with terms/word frequencies extracted from the article segment/comment.\n• Dice: dice =\n2 ∗ len(I(S, C)) len(S) + len(C)\n(1)\nwhere I(S, C) is the intersection set between the terms/words in the segment and in the comment. len returns the number of entries in the given set.\n• Jaccard:\njaccard = len(I(S, C)) len(U(S, C))\n(2)\nwhere U(S, C) is the union set between the terms/words in the segment and comment.\n• NE overlap:\nNEoverlap = len(I(S, C)) len(U(S, C))\n(3)\nwhere I(S, C) is the intersection set between the named entities (NEs) in the segment and in the comment and U(S, C) is the NEs union set.\n• DISCO 1 + DISCO 2: DISCO (DIStributionally similar words using CO-occurrences) assumes words with similar meaning occur in similar context (Kolb, 2009). Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences. DISCO computes two different similarities between words: DISCO1 and DISCO2. In DISCO1 when two words are directly compared for exact similarity DISCO simply retrieves their word vectors from the large text collections and computes the similarity according to Lin’s information theoretic measure (Lin, 1998). DISCO2 compares words based on their sets of distributional similar words."
  }, {
    "heading": "3.2.2 Computing Similarity Linking Score",
    "text": "Using a linear function, we combine the scores of each of these features (cosine to DISCO) to produce a final similarity score for a commentsegment pair:\nScore = n∑ i=1 featurei ∗ weighti (4)\nwhere weighti is the weight associated with the ith feature. The weights are trained based on linear regression using the Weka package and the training data described in the following section."
  }, {
    "heading": "3.2.3 Training Data",
    "text": "Obtaining training data requires manual effort and human involvement and is thus very expensive, while resulting in relatively small training data sets. We therefore automatically assemble training data by using comments with article quotes as a training data set. As outlined above, in addition to original comment text, many comments include a brief quotation from the article, therefore directly indicating which article segments have triggered the comments. The set of comments with quotes linked to the article segments they quote are used as our training data. To gather the training data, we downloaded 3,362 news articles along with their comments from The\nGuardian news paper web site4 over a period of two months (June-July 2014). The Guardian provides for each topic (e.g., business, politics, art, etc.) a specific RSS feed URL. We manually collected RSS feeds for the topics: politics, health, education, business, society, media, science, thenorthener, law, world-news, scotland-news, money and environment. Using an in-house tool, we visited the news published through the RSS feeds every 30 minutes, downloaded the article content and also recorded the news URL. Every recorded news URL was re-visited after a week (the time we found sufficient for an article to attract commenters) to obtain its comments. Articles contained between 1 and 6,223 comments, averaging 425.95 (median 231.5) comments per article. Each article was split into sentences and for each of these sentences (containing at least 10 words) it was determined whether it is quoted in any of the comments as described above. In case the quoteScore was above 0.5 for a sentencecomment pair, the pair was included in the training set. Using this process we have extracted 43,300 sentence-comment pairs to use for training. For each pair, the similarity features listed in Section 3.2.1 were extracted. The quoteScore was used as the expected outcome. We also included 43,300 negative samples into the training data in order to present linear regression with the behavior of the features for wrong sentence-comment links. The negative samples were created by pairing every sentence containing at least 10 words of article X with every comment of article Y . In this way we pair comments with sentences of another article that have not originally triggered the comments. Similar to the positive samples, the quote score was taken as the expected outcome. However, unlike the positive samples, the quoteScore threshold of 0.5 was not applied for the negative samples."
  }, {
    "heading": "4 Evaluation",
    "text": ""
  }, {
    "heading": "4.1 Test Data",
    "text": "In this study, we use the AT corpus (Das et al., 2014) to test the above linking method. The AT data set consists of articles with comments downloaded from the technology news website Ars Technica (AT). In this data set there are 501 articles. Each article contains between 8 and 132 sentences with an average of 38. Each article has between 2 and 59 linked comments with an average of 6.3. As reported in Das et al. (2014), two annotators mapped comments to article sentences; however, the agreement between annotators cannot be assessed from the available data set due to the lack of double annotations.\n4http://theguardian.com"
  }, {
    "heading": "4.2 State-of-the art",
    "text": "The combined quotation and similarity-based linking investigated here is compared to the stateof-the-art SCTM method described in Das et al. (2014). SCTM (Specific Correspondence Topic Model that admits multiple topic vectors per article-comment pair) is an LDA-based topic modeling method that takes into account the multiplicity of topics in comments and articles. Their baseline is Corr-LDA, which Das et al. (2014) deem unsuitable since it is restricted to using only a single topic vector per article-comment pair. Evaluation on the same AT test data set allows for a direct comparison of our results to those of SCTM and Corr-LDA. Another recently proposed linking approach is reported in (Sil et al., 2011). However, it does not match the performance of its simple tf ∗ idf based baseline, so we do not consider this method in our evaluations."
  }, {
    "heading": "4.3 Results",
    "text": "Table 4.3 shows the performance of the automated linking task using quotation and similarity metrics (Metrics) on the AT data.5 The table shows the results for both term and word based representation of article segments (first two rows). Both results were obtained with the experimentally determined Score >= 0.5. The results in the table show that representation of article segments and comment texts as terms is superior to the bag-ofwords representation for the comment-article linking task as it achieves substantially higher score in precision with a similar recall value. We also combined terms with words by merging the term list with the bag of words and used them to compute the metrics. The results are shown in the 3rd row. Compared to the word only variant, Metricsword, we see a substantial improvement in the precision and a slight one in the recall score. However, compared to the term only variant, Metricsterm, the precision score is still low indicating that terms only are indeed the better choice for representing article segments and comments for the linking task.\nThe results in Table 4.3 show that the state-ofthe-art baseline SCTM outperforms the Metrics regarding the overall F1 score due to higher recall. However, this difference in F1 score is small. The\n5Note that the testing data does not contain any comment that quotes an article sentence as specified in our quote feature. This means all the results are achieved through the other features – cosine to Disco features.\nprecision of Metricsterm based similarity is substantially higher than that of the SCTM method at the expense of recall. Higher precision may be preferable to higher recall for the linking task as including wrong links in order to have higher coverage is noisier and therefore more disturbing for both human and automatic processing of comment-article links than leaving relevant comments unlinked. These results suggest that term based similarity linking is performing almost as well as the SCTM method overall, and if increasing precision over recall is favored for the comment-article linking task, it even could be a preferred method for this task."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper we report initial experiments on linking reader comments to the relevant segments in the articles – a task which has multiple applications in organization and retrievability of information from online commenting forums. Linking between articles and comments implies capturing similarity between a comment and related article segments. In Das et al. (2014) the similarity is defined as similarity in topic. The claim is that multiple topics occurring in a comment and article need to be modeled in order to establish successful links. In this work our aim was to investigate how well known similarity metrics combined with a quotation heuristic perform on the linking task, and how their performance compares to refined topic similarity modeling proposed in previous work. The results showed that the overall performance of combined quote and similarity metrics is comparable to that of topic modeling method despite substantial domain difference between training and testing data sets. The bias of the quote and similarity method is towards precision and in topic modeling towards the recall. We also found that linking using similarity based on terms, i.e., specialized word sequences that have meaning in a domain, achieves better results than linking based on words. This is not surprising given a low lexical overlap between comments and article segments. The fact that terms achieved good results indicates that it is worth exploring further representations that abstract away from lexical items. This will be one of our immediate future studies. Furthermore, we plan to also address the recall problem by investigating clustering methods to group “similar” comments and link these groups instead of the single comments. Finally, we will investigate how the linking task can be used for summarizing news comments."
  }, {
    "heading": "Acknowledgements",
    "text": "The research leading to these results has received funding from the European Union - Seventh Framework Program (FP7/2007-2013) under grant agreement n610916 SENSEI"
  }, {
    "heading": "Appendix – MultiLing Linking Task",
    "text": "We also participated in the linking task organized by MultiLing 2015. Similar to the task described in Section 2.1 the linking task within MultiLing was to link a comment to an article segment (sentence). However, unlike the task described above the comment was not treated as one unit, but split into sentences. This allowed to link parts of the comment (sentences) to article sentences and leave some out. Although the MultiLing linking task setup defined this freedom within the comments we continued treating the entire comment as one unit. More precisely, when our linking approach found a link between a sentence in the comment and an article sentence it also linked all the remaining sentences within the comment to the article sentence. The evaluation was performed with English and Italian data. Each participant was allowed to submit two runs. Our runs differed in how we set a threshold for linking similarity. The first run was set to a lower threshold (i.e. the Score in equation 4 was set to 0.3). Anything below this threshold was not linked. In the second run the threshold was set to 0.5. For English both our runs were considered. However, for Italian there has been some problems in the submission, so that our second run with the threshold 0.5 was not considered. Our results for English are that using our second run we obtained better results compared to all other 8 system submissions. With this set-up we achieved 89% precision. Our first run (run with the 0.3 threshold) achieved 82% precision. With this score it became the 5th system. For Italian our first run got the 6th position scoring 89% precision. Since our first run also did not perform well on the English data, it is likely that the performance on the Italian data would have been better could the second run be submitted."
  }],
  "year": 2015,
  "references": [{
    "title": "The nasty effect: Online incivility and risk perceptions of emerging technologies",
    "authors": ["Ashley A. Anderson", "Dominique Brossard", "Dietram A. Scheufele", "Michael A. Xenos", "Peter Ladwig."],
    "venue": "Journal of Computer-Mediated Communication.",
    "year": 2013
  }, {
    "title": "Going beyond Corr-LDA for detecting specific comments on news & blogs",
    "authors": ["Mrinal Kanti Das", "Trapit Bansal", "Chiranjib Bhattacharyya."],
    "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pages 483–492. ACM.",
    "year": 2014
  }, {
    "title": "Glossary of terms used in terminology",
    "authors": ["Bruno de Bessé", "Blaise Nkwenti-Azeh", "Juan C. Sager."],
    "venue": "Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication, 4:117– 156(39).",
    "year": 1997
  }, {
    "title": "Towards quality discourse in online news comments",
    "authors": ["Nicholas Diakopoulos", "Mor Naaman."],
    "venue": "In",
    "year": 2011
  }, {
    "title": "Comments-oriented document summarization: understanding documents with readers’ feedback",
    "authors": ["Meishan Hu", "Aixin Sun", "Ee-Peng Lim."],
    "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in informa-",
    "year": 2008
  }, {
    "title": "Summarizing user-contributed comments",
    "authors": ["Elham Khabiri", "James Caverlee", "Chiao-Fang Hsu."],
    "venue": "ICWSM.",
    "year": 2011
  }, {
    "title": "Experiments on the difference between semantic similarity and relatedness",
    "authors": ["Peter Kolb."],
    "venue": "Proceedings of the 17th Nordic Conference on Computational Linguistics-NODALIDA09.",
    "year": 2009
  }, {
    "title": "Automatic retrieval and clustering of similar words",
    "authors": ["Dekang Lin."],
    "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 2, pages 768–774. As-",
    "year": 1998
  }, {
    "title": "Summarizing newspaper comments",
    "authors": ["Clare Llewellyn", "Claire Grover", "Jon Oberlander."],
    "venue": "Eighth International AAAI Conference on Weblogs and Social Media.",
    "year": 2014
  }, {
    "title": "Rated aspect summarization of short comments",
    "authors": ["Yue Lu", "ChengXiang Zhai", "Neel Sundaresan."],
    "venue": "Proceedings of the 18th international conference on World wide web, pages 131–140. ACM.",
    "year": 2009
  }, {
    "title": "Topic-driven reader comments summarization",
    "authors": ["Zongyang Ma", "Aixin Sun", "Quan Yuan", "Gao Cong."],
    "venue": "Proceedings of the 21st ACM international conference on Information and knowledge management, pages 265–274. ACM.",
    "year": 2012
  }, {
    "title": "Term extraction, tagging, and mapping tools for under-resourced languages",
    "authors": ["Mārcis Pinnis", "Nikola Ljubešić", "Dan Ştefănescu", "Inguna Skadiņa", "Marko Tadić", "Tatiana Gornostay."],
    "venue": "Proceedings of the 10th Conference on Terminology and Knowledge En-",
    "year": 2012
  }, {
    "title": "Computer evaluation of indexing and text processing",
    "authors": ["G. Salton", "E.M. Lesk"],
    "venue": "Journal of the ACM, volume 15, pages 8–36, New York, NY, USA. ACM Press.",
    "year": 1968
  }, {
    "title": "Virtuous or vitriolic",
    "authors": ["Arthur D. Santana."],
    "venue": "Journalism Practice, 8(1):18–33.",
    "year": 2014
  }, {
    "title": "Supervised matching of comments with news article segments",
    "authors": ["Dyut Kumar Sil", "Srinivasan H Sengamedu", "Chiranjib Bhattacharyya."],
    "venue": "Proceedings of the 20th ACM international conference on Information and knowledge management, pages 2125–",
    "year": 2011
  }],
  "id": "SP:bf2a1102b8b7d02e32013e86f48890f54fb8f477",
  "authors": [{
    "name": "Ahmet Aker",
    "affiliations": []
  }, {
    "name": "Emina Kurtic",
    "affiliations": []
  }, {
    "name": "Mark Hepple",
    "affiliations": []
  }, {
    "name": "Rob Gaizauskas",
    "affiliations": []
  }, {
    "name": "Giuseppe Di Fabbrizio",
    "affiliations": []
  }],
  "abstractText": "Online commenting to news articles provides a communication channel between media professionals and readers offering a crucial tool for opinion exchange and freedom of expression. Currently, comments are detached from the news article and thus removed from the context that they were written for. In this work, we propose a method to connect readers’ comments to the news article segments they refer to. We use similarity features to link comments to relevant article segments and evaluate both word-based and term-based vector spaces. Our results are comparable to state-of-theart topic modeling techniques when used for linking tasks. We demonstrate that article segments and comments representation are relevant to linking accuracy since we achieve better performances when similarity features are computed using similarity between terms rather than words.",
  "title": "Comment-to-Article Linking in the Online News Domain"
}