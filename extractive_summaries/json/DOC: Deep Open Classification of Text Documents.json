{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2911–2916 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "A key assumption made by classic supervised text classification (or learning) is that classes appeared in the test data must have appeared in training, called the closed-world assumption (Fei and Liu, 2016; Chen and Liu, 2016). Although this assumption holds in many applications, it is violated in many others, especially in dynamic or open environments. For example, in social media, a classifier built with past topics or classes may not be effective in classifying future data because new topics appear constantly in social media (Fei et al., 2016). This is clearly true in other domains too, e.g., self-driving cars, where new objects may appear in the scene all the time.\nIdeally, in the text domain, the classifier should classify incoming documents to the right existing classes used in training and also detect those documents that don’t belong to any of the existing classes. This problem is called open world classification or open classification (Fei and Liu, 2016). Such a classifier is aware what it does and does\nnot know. This paper proposes a novel technique to solve this problem.\nProblem Definition: Given the training data D = {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi is the i-th document, and yi ∈ {l1, l2, . . . , lm} = Y is xi’s class label, we want to build a model f(x) that can classify each test instance x to one of them training or seen classes inY or reject it to indicate that it does not belong to any of them training or seen classes, i.e., unseen. In other words, we want to build a (m + 1)-class classifier f(x) with the classes C = {l1, l2, . . . , lm, rejection}.\nThere are some prior approaches for open classification. One-class SVM (Schölkopf et al., 2001; Tax and Duin, 2004) is the earliest approach. However, as no negative training data is used, oneclass classifiers work poorly. Fei and Liu (2016) proposed a Center-Based Similarity (CBS) space learning method (Fei and Liu, 2015). This method first computes a center for each class and transforms each document to a vector of similarities to the center. A binary classifier is then built using the transformed data for each class. The decision surface is like a “ball” encircling each class. Everything outside the ball is considered not belonging to the class. Our proposed method outperforms this method greatly. Fei et al. (2016) further added the capability of incrementally or cumulatively learning new classes, which connects this work to lifelong learning (Chen and Liu, 2016) because without the ability to identify novel or new things and learn them, a system will never be able to learn by itself continually.\nIn computer vision, Scheirer et al. (2013) studied the problem of recognizing unseen images that the system was not trained for by reducing open space risk. The basic idea is that a classifier should not cover too much open space where there are few or no training data. They proposed to reduce the half-space of a binary SVM classifier\n2911\nwith a positive region bounded by two parallel hyperplanes. Similar works were also done in a probability setting by Scheirer et al. (2014) and Jain et al. (2014). Both approaches use probability threshold, but choosing thresholds need prior knowledge, which is a weakness of the methods. Dalvi et al. (2013) proposed a multi-class semisupervised method based on the EM algorithm. It has been shown that these methods are poorer than the method in (Fei and Liu, 2016).\nThe work closest to ours is that in (Bendale and Boult, 2016), which leverages an algorithm called OpenMax to add the rejection capability by utilizing the logits that are trained via closed-world softmax function. One weak assumption of OpenMax is that examples with equally likely logits are more likely from the unseen or rejection class, which can be examples that are hard to classify. Another weakness is that it requires validation examples from the unseen/rejection class to tune the hyperparameters. Our method doesn’t make these weak assumptions and performs markedly better.\nOur proposed method, called DOC (Deep Open Classification), uses deep learning (Goodfellow et al., 2016; Kim, 2014). Unlike traditional classifiers, DOC builds a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than softmax to reduce the open space risk. It reduces the open space risk further for rejection by tightening the decision boundaries of sigmoid functions with Gaussian fitting. Experimental results show that DOC dramatically outperforms state-of-the-art existing approaches from both text classification and image classification domains."
  }, {
    "heading": "2 The Proposed DOC Architecture",
    "text": "DOC uses CNN (Collobert et al., 2011; Kim, 2014) as its base and augments it with a 1-vsrest final sigmoid layer and Gaussian fitting for\nclassification. Note: other existing deep models like RNN (Williams and Zipser, 1989; Schuster and Paliwal, 1997) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) can also be adopted as the base. Similar to RNN, CNN also works on embedded sequential data (using 1D convolution on text instead of 2D convolution on images). We choose CNN because OpenMax uses CNN and CNN performs well on text (Kim, 2014), which enables a fairer comparison with OpenMax."
  }, {
    "heading": "2.1 CNN and Feed Forward Layers of DOC",
    "text": "The proposed DOC system (given in Fig. 1) is a variant of the CNN architecture (Collobert et al., 2011) for text classification (Kim, 2014)1. The first layer embeds words in document x into dense vectors. The second layer performs convolution over dense vectors using different filters of varied sizes (see Sec. 3.4). Next, the max-over-time pooling layer selects the maximum values from the results of the convolution layer to form a kdimension feature vector h. Then we reduce h to a m-dimension vector d = d1:m (m is the number of training/seen classes) via 2 fully connected layers and one intermediate ReLU activation layer:\nd = W ′(ReLU(Wh + b)) + b′, (1)\nwhere W ∈ Rr×k, b ∈ Rr, W ′ ∈ Rm×r, and b′ ∈ Rm are trainable weights; r is the output dimension of the first fully connected layer. The output layer of DOC is a 1-vs-rest layer applied to d1:m, which allows rejection. We describe it next."
  }, {
    "heading": "2.2 1-vs-Rest Layer of DOC",
    "text": "Traditional multi-class classifiers (Goodfellow et al., 2016; Bendale and Boult, 2016) typically use softmax as the final output layer, which does not have the rejection capability since the probability of prediction for each class is normalized across all training/seen classes. Instead, we build a 1-vs-rest layer containing m sigmoid functions for m seen classes. For the i-th sigmoid function corresponding to class li, DOC takes all examples with y = li as positive examples and all the rest examples y 6= li as negative examples.\nThe model is trained with the objective of summation of all log loss of the m sigmoid functions\n1https://github.com/alexander-rakhlin/ CNN-for-Sentence-Classification-in-Keras\non the training data D.\nLoss = m∑\ni=1 n∑ j=1 −I(yj = li) log p(yj = li)\n−I(yj 6= li) log(1− p(yj = li)), (2)\nwhere I is the indicator function and p(yj = li) = Sigmoid(dj,i) is the probability output from ith sigmoid function on the jth document’s ithdimension of d.\nDuring testing, we reinterpret the prediction of m sigmoid functions to allow rejection, as shown in Eq. 3. For the i-th sigmoid function, we check if the predicted probability Sigmoid(di) is less than a threshold ti belonging to class li. If all predicted probabilities are less than their corresponding thresholds for an example, the example is rejected; otherwise, its predicted class is the one with the highest probability. Formally, we have\nŷ = {\nreject, if Sigmoid(di) < ti,∀li ∈ Y; arg maxli∈Y Sigmoid(di), otherwise.\n(3)\nNote that although multi-label classification (Huang et al., 2013; Zhang and Zhou, 2006; Tsoumakas and Katakis, 2006) may also leverage multiple sigmoid functions, Eq. 3 forbids multiple predicted labels for the same example, which is allowed in multi-label classification. DOC is also related to multi-task learning (Huang et al., 2013; Caruana, 1998), where each label li is related to a 1-vs-rest binary classification task with shared representations from CNN and fully connected layers. However, Eq. 3 performs classification and rejection based on the outputs of these binary classification tasks.\nComparison with OpenMax: OpenMax builds on the traditional closed-world multi-class classifier (softmax layer). It reduces the open space for each seen class, which is weak for rejecting unseen classes. DOC’s 1-vs-rest sigmoid layer provides a reasonable representation of all other classes (the rest of seen classes and unseen classes), and enables the 1 class forms a good boundary. Sec. 3.5 shows that this basic DOC is already much better than OpenMax. Below, we improve DOC further by tightening the decision boundaries more."
  }, {
    "heading": "2.3 Reducing Open Space Risk Further",
    "text": "Sigmoid function usually uses the default probability threshold of ti = 0.5 for classification of\neach class i. But this threshold does not consider potential open space risks from unseen (rejection) class data. We can improve the boundary by increasing ti. We use Fig. 2 to illustrate. The x-axis represents di and y-axis is the predicted probability p(y = li|di). The sigmoid function tries to push positive examples (belonging to the i-th class) and negative examples (belonging to the other seen classes) away from the y-axis via a high gain around di = 0, which serves as the default decision boundary for di with ti = 0.5. As demonstrated by those 3 circles on the right-hand side of the y-axis, during testing, unseen class examples (circles) can easily fill in the gap between the y-axis and those dense positive (+) examples, which may reduce the recall of rejection and the precision of the i-th seen class prediction. Obviously, a better decision boundary is at di = T , where the decision boundary more closely “wrap” those dense positive examples with the probability threshold ti 0.5 .\nTo obtain a better ti for each seen class i-th, we use the idea of outlier detection in statistics:\n1. Assume the predicted probabilities p(y = li|xj , yj = li) of all training data of each class i follow one half of the Gaussian distribution (with mean µi = 1), e.g., the three positive points in Fig. 2 projected to the y-axis (we don’t need di). We then artificially create the other half of the Gaussian distributed points (≥ 1): for each existing point p(y = li|xj , yj = li), we create a mirror point 1 + (1 − p(y = li|xj , yj = li) (not a probability) mirrored on the mean of 1.\n2. Estimate the standard deviation σi using both the existing points and the created points.\n3. In statistics, if a value/point is a certain number (α) of standard deviations away from the mean, it is considered an outlier. We thus set the probability threshold ti = max(0.5, 1 − ασi). The commonly used number for α is 3, which also works well in our experiments.\nNote that due to Gaussian fitting, different class li can have a different classification threshold ti."
  }, {
    "heading": "3 Experimental Evaluation",
    "text": ""
  }, {
    "heading": "3.1 Datasets",
    "text": "We perform evaluation using two publicly available datasets, which are exactly the same datasets used in (Fei and Liu, 2016).\n(1) 20 Newsgroups2 (Rennie, 2008): The 20 newsgroups data set contains 20 non-overlapping classes. Each class has about 1000 documents.\n(2) 50-class reviews (Chen and Liu, 2014): The dataset has Amazon reviews of 50 classes of products. Each class has 1000 reviews. Although product reviews are used, we do not do sentiment classification. We still perform topic-based classification. That is, given a review, the system decides what class of product the review is about.\nFor every dataset, we keep a 20000 frequent word vocabulary. Each document is fixed to 2000- word length (cutting or padding when necessary)."
  }, {
    "heading": "3.2 Test Settings and Evaluation Metrics",
    "text": "For a fair comparison, we use exactly the same settings as in (Fei and Liu, 2016). For each class in each dataset, we randomly sampled 60% of documents for training, 10% for validation and 30% for testing. Fei and Liu (2016) did not use a validation set, but the test data is the same 30%. We use the validation set to avoid overfitting. For openworld evaluation, we hold out some classes (as unseen) in training and mix them back during testing. We vary the number of training classes and use 25%, 50%, 75%, or 100% classes for training and all classes for testing. Here using 100% classes for training is the same as the traditional closedworld classification. Taking 20 newsgroups as an example, for 25% classes, we use 5 classes (we randomly choose 5 classes from 20 classes for 10 times and average the results, as in (Fei and Liu, 2016)) for training and all 20 classes for testing (15 classes are unseen in training). We use macro F1-score over 5 + 1 classes (1 for rejection) for\n2http://qwone.com/˜jason/20Newsgroups/\nevaluation. Please note that examples from unseen classes are dropped in the validation set."
  }, {
    "heading": "3.3 Baselines",
    "text": "We compare DOC with two state-of-the-art methods published in 2016 and one DOC variant.\ncbsSVM: This is the latest method published in NLP (Fei and Liu, 2016). It uses SVM to build 1-vs-rest CBS classifiers for multiclass text classification with rejection option. The results of this system are taken from (Fei and Liu, 2016).\nOpenMax: This is the latest method from computer vision (Bendale and Boult, 2016). Since it is a CNN-based method for image classification, we adapt it for text classification by using CNN with a softmax output layer, and adopt the OpenMax layer3 for open text classification. When all classes are seen (100%), the result from softmax is reported since OpenMax layer always performs rejection. We use default hyperparameter values of OpenMax (Weibull tail size is set to 20).\nDOC(t = 0.5): This is the basic DOC (t = 0.5). Gaussian fitting isn’t used to choose each ti.\nNote that (Fei and Liu, 2016) compared with several other baselines. We don’t compare with them as it was shown that cbsSVM was superior."
  }, {
    "heading": "3.4 Hyperparameter Setting",
    "text": "We use word vectors pre-trained from Google News4 (3 million words and 300 dimensions). For the CNN layers, 3 filter sizes are used [3, 4, 5]. For each filter size, 150 filters are applied. The dimension r of the first fully connected layer is 250.\n3https://github.com/abhijitbendale/ OSDN\n4https://code.google.com/archive/p/ word2vec/"
  }, {
    "heading": "3.5 Result Analysis",
    "text": "The results of 20 newsgroups and 50-class reviews are given in Tables 1 and 2, respectively. From the tables, we can make the following observations:\n1. DOC is markedly better than OpenMax and cbsSVM in macro-F1 scores for both datasets in the 25%, 50%, and 75% settings. For the 25% and 50% settings (most test examples are from unseen classes), DOC is dramatically better. Even for 100% of traditional closed-world classification, it is consistently better too. DOC(t = 0.5) is better too.\n2. For the 25% and 50% settings, DOC is also markedly better than DOC(t = 0.5), which shows that Gaussian fitting finds a better probability threshold than t = 0.5 when many unseen classes are present. In the 75% setting (most test examples are from seen classes), DOC(t = 0.5) is slightly better for 20 newsgroups but worse for 50-class reviews. DOC sacrifices some recall of seen class examples for better precision, while t = 0.5 sacrifices the precision of seen classes for better recall. DOC(t = 0.5) is also worse than cbsSVM for 25% setting for 50-class reviews. It is thus not as robust as DOC.\n3. For the 25% and 50% settings, cbsSVM is also markedly better than OpenMax."
  }, {
    "heading": "4 Conclusion",
    "text": "This paper proposed a novel deep learning based method, called DOC, for open text classification. Using the same text datasets and experiment settings, we showed that DOC performs dramatically better than the state-of-the-art methods from both the text and image classification domains. We also believe that DOC is applicable to images.\nIn our future work, we plan to improve the cumulative or incremental learning method in (Fei et al., 2016) to learn new classes without training on all past and new classes of data from scratch. This will enable the system to learn by self to achieve continual or lifelong learning (Chen and Liu, 2016). We also plan to improve model performance during testing (Shu et al., 2017)."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported in part by grants from National Science Foundation (NSF) under grant no. IIS-1407927 and IIS-1650900."
  }],
  "year": 2017,
  "references": [{
    "title": "Towards open set deep networks",
    "authors": ["Abhijit Bendale", "Terrance E Boult."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 1563–1572.",
    "year": 2016
  }, {
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Learning to learn, Springer, pages 95–133.",
    "year": 1998
  }, {
    "title": "Mining topics in documents: standing on the shoulders of big data",
    "authors": ["Zhiyuan Chen", "Bing Liu."],
    "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 1116–1125.",
    "year": 2014
  }, {
    "title": "Lifelong Machine Learning",
    "authors": ["Zhiyuan Chen", "Bing Liu."],
    "venue": "Morgan & Claypool Publishers.",
    "year": 2016
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Exploratory learning",
    "authors": ["Bhavana Dalvi", "William W Cohen", "Jamie Callan."],
    "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 128–143.",
    "year": 2013
  }, {
    "title": "Social media text classification under negative covariate shift",
    "authors": ["Geli Fei", "Bing Liu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2015).",
    "year": 2015
  }, {
    "title": "Breaking the closed world assumption in text classification",
    "authors": ["Geli Fei", "Bing Liu."],
    "venue": "Proceedings of NAACL-HLT . pages 506–514.",
    "year": 2016
  }, {
    "title": "Learning cumulatively to become more knowledgeable",
    "authors": ["Geli Fei", "Shuai Wang", "Bing Liu."],
    "venue": "Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD2016).",
    "year": 2016
  }, {
    "title": "Learning precise timing with lstm recurrent networks",
    "authors": ["Felix A Gers", "Nicol N Schraudolph", "Jürgen Schmidhuber."],
    "venue": "Journal of machine learning research 3(Aug):115–143.",
    "year": 2002
  }, {
    "title": "Deep Learning",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."],
    "venue": "MIT Press. http://www. deeplearningbook.org.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Multi-task deep neural network for multi-label learning",
    "authors": ["Yan Huang", "Wei Wang", "Liang Wang", "Tieniu Tan."],
    "venue": "Image Processing (ICIP), 2013 20th IEEE International Conference on. IEEE, pages 2897–2900.",
    "year": 2013
  }, {
    "title": "Multi-class open set recognition using probability of inclusion",
    "authors": ["Lalit P Jain", "Walter J Scheirer", "Terrance E Boult."],
    "venue": "European Conference on Computer Vision. Springer, pages 393–409.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882 .",
    "year": 2014
  }, {
    "title": "Toward open set recognition",
    "authors": ["Walter J Scheirer", "Anderson de Rezende Rocha", "Archana Sapkota", "Terrance E Boult."],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(7):1757–1772.",
    "year": 2013
  }, {
    "title": "Probability models for open set recognition",
    "authors": ["Walter J Scheirer", "Lalit P Jain", "Terrance E Boult."],
    "venue": "IEEE transactions on pattern analysis and machine intelligence 36(11):2317–2324.",
    "year": 2014
  }, {
    "title": "Estimating the support of a high-dimensional distribution",
    "authors": ["Bernhard Schölkopf", "John C Platt", "John Shawe-Taylor", "Alex J Smola", "Robert C Williamson."],
    "venue": "Neural computation 13(7):1443–1471.",
    "year": 2001
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["Mike Schuster", "Kuldip K Paliwal."],
    "venue": "IEEE Transactions on Signal Processing 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Lifelong learning crf for supervised aspect extraction",
    "authors": ["Lei Shu", "Hu Xu", "Bing Liu."],
    "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2017, short paper).",
    "year": 2017
  }, {
    "title": "Support vector data description",
    "authors": ["David MJ Tax", "Robert PW Duin."],
    "venue": "Machine learning 54(1):45–",
    "year": 2004
  }, {
    "title": "Multi-label classification: An overview",
    "authors": ["Grigorios Tsoumakas", "Ioannis Katakis."],
    "venue": "International Journal of Data Warehousing and Mining 3(3).",
    "year": 2006
  }, {
    "title": "A learning algorithm for continually running fully recurrent neural networks",
    "authors": ["Ronald J Williams", "David Zipser."],
    "venue": "Neural computation 1(2):270–280.",
    "year": 1989
  }, {
    "title": "Multilabel neural networks with applications to functional genomics and text categorization",
    "authors": ["Min-Ling Zhang", "Zhi-Hua Zhou."],
    "venue": "IEEE transactions on Knowledge and Data Engineering 18(10):1338– 1351.",
    "year": 2006
  }],
  "id": "SP:902ac0b06eade35045723b0bf116b4509b415357",
  "authors": [{
    "name": "Lei Shu",
    "affiliations": []
  }, {
    "name": "Hu Xu",
    "affiliations": []
  }, {
    "name": "Bing Liu",
    "affiliations": []
  }],
  "abstractText": "Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called openworld classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.",
  "title": "DOC: Deep Open Classification of Text Documents"
}