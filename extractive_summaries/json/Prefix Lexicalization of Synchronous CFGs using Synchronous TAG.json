{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1160–1170 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1160"
  }, {
    "heading": "1 Introduction",
    "text": "Greibach normal form (GNF; Greibach, 1965) is an important construction in formal language theory which allows every context-free grammar (CFG) to be rewritten so that the first character of each rule is a terminal symbol. A grammar in GNF is said to be prefix lexicalized, because the prefix of every production is a lexical item. GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Schützenberger (Shamir, 1967; Chomsky and Schützenberger, 1963; Autebert et al., 1997). Other applications of prefix lexicalization include proving coverage of parsing algorithms (Gray and Harrison, 1972) and decidability of equivalence problems (Christensen et al., 1995).\nBy using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task. Using a prefix lexicalized grammar ensures that target sentences can be generated from left to right, which allows the use of beam search to constrain their decoder’s search space as it performs a left-to-right traversal of translation hypotheses. To achieve these results,\nnew grammars had to be heuristically constrained to include only prefix lexicalized productions, as there is at present no way to automatically convert an existing SCFG to a prefix lexicalized form.\nThis work investigates the formal properties of prefix lexicalized synchronous grammars as employed by Watanabe et al. (2006) and Siahbani et al. (2013), which have received little theoretical attention compared to non-synchronous prefix lexicalized grammars. To this end, we first prove that SCFG is not closed under prefix lexicalization. Our main result is that there is a method for prefix lexicalizing an SCFG by converting it to an equivalent grammar in a different formalism, namely synchronous tree-adjoining grammar (STAG) in regular form. Like the GNF transformation for CFGs, our method at most cubes the grammar size, but we show empirically that the size increase is only quadratic for grammars used in existing NLP tasks. The rank is at most doubled, and we maintain O(n3k) parsing complexity for grammars of rank k. We conclude that although SCFG does not have a prefix lexicalized normal form like GNF, our conversion to prefix lexicalized STAG offers a practical alternative."
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 SCFG",
    "text": "An SCFG is a tuple G = (N,Σ, P, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and P is a finite set of synchronous rules of the form\n(1) 〈A1 → α1, A2 → α2〉\nfor some A1, A2 ∈ N and strings α1, α2 ∈ (N ∪ Σ)∗.1 Every nonterminal which appears in α1\n1A variant formalism exists which requires thatA1 = A2; this is called syntax-directed transduction grammar (Lewis and Stearns, 1968) or syntax-directed translation schemata (Aho and Ullman, 1969). This variant is weakly equivalent to SCFG, but SCFG has greater strong generative capacity (Crescenzi et al., 2015).\nA 1\nA↓ 2a\nB 2\nB↓ 1b\nA\ncA∗\nB d 〈 〉 〈 〉\n〈 A A\na A ↓ 1\nc ,\nB 1\nb B\nd 〉\nFigure 1: An example of synchronous rewriting in an STAG (left) and the resulting tree pair (right).\nmust be linked to exactly one nonterminal in α2, and vice versa. We write these links using numerical annotations, as in (2).\n(2) 〈A→ A 1 B 2 , B → B 2 A 1 〉\nAn SCFG has rank k if no rule in the grammar contains more than k pairs of linked nodes.\nIn every step of an SCFG derivation, we rewrite one pair of linked nonterminals with a rule from P , in essentially the same way we would rewrite a single nonterminal in a non-synchronous CFG. For example, (3) shows linked A and B nodes being rewritten using (2): (3) 〈X 1 A 2 , B 2 Y 1 〉 ⇒ 〈X 1 A 2 B 3 , B 3 A 2 Y 1 〉\nNote how the 1 and 2 in rule (2) are renumbered to 2 and 3 during rewriting, to avoid an ambiguity with the 1 already present in the derivation.\nAn SCFG derivation is complete when it contains no more nonterminals to rewrite. A completed derivation represents a string pair generated by the grammar."
  }, {
    "heading": "2.2 STAG",
    "text": "An STAG (Shieber, 1994) is a tuple G = (N,Σ, T, S) where N is a finite nonterminal alphabet, Σ is a finite terminal alphabet, S ∈ N is a distinguished nonterminal called the start symbol, and T is a finite set of synchronous tree pairs of the form\n(4) 〈t1, t2〉\nwhere t1 and t2 are elementary trees as defined in Joshi et al. (1975). A substitution site is a leaf node marked by ↓ which may be rewritten by another tree; a foot node is a leaf marked by ∗ that may be used to rewrite a tree-internal node. Every substitution site in t1 must be linked to exactly one nonterminal in t2, and vice versa. As in SCFG, we write these links using numbered annotations; rank is defined for STAG the same way as for SCFG.\nIn every step of an STAG derivation, we rewrite one pair of linked nonterminals with a tree pair from T , using the same substitution and adjunction operations defined for non-synchronous TAG. For example, Figure 1 shows linked A and B nodes being rewritten and the tree pair resulting from this operation. See Joshi et al. (1975) for details about the underlying TAG formalism."
  }, {
    "heading": "2.3 Terminology",
    "text": "We use synchronous production as a cover term for either a synchronous rule in an SCFG or a synchronous tree pair in an STAG.\nFollowing Siahbani et al. (2013), we refer to the left half of a synchronous production as the source side, and the right half as the target side; this terminology captures the intuition that synchronous grammars model translational equivalence between a source phrase and its translation into a target language. Other authors refer to the two halves as the left and right components (Crescenzi et al., 2015) or, viewing the grammar as a transducer, the input and the output (Engelfriet et al., 2017).\nWe call a grammar ε-free if it contains no productions whose source or target side produces only the empty string ε."
  }, {
    "heading": "2.4 Synchronous Prefix Lexicalization",
    "text": "Previous work (Watanabe et al., 2006; Siahbani et al., 2013) has shown that it is useful for the target side of a synchronous grammar to start with a terminal symbol. For this reason, we define a synchronous grammar to be prefix lexicalized when the leftmost character of the target side2 of every synchronous production in that grammar is a terminal symbol.\nFormally, this means that every synchronous rule in a prefix lexicalized SCFG (PL-SCFG) is\n2All of the proofs in this work admit a symmetrical variant which prefix lexicalizes the source side instead of the target. We are not aware of any applications in NLP where sourceside prefix lexicalization is useful, so we do not address this case.\nof the form\n(5) 〈A1 → α1, A2 → aα2〉\nwhereA1, A2 ∈ N , α1, α2 ∈ (N∪Σ)∗ and a ∈ Σ. Every synchronous tree pair in a prefix lexicalized STAG (PL-STAG) is of the form\n(6)\n〈 A1\nα1\n, A2\naα2 〉 whereA1, A2 ∈ N , α1, α2 ∈ (N∪Σ)∗ and a ∈ Σ."
  }, {
    "heading": "3 Closure under Prefix Lexicalization",
    "text": "We now prove that the class SCFG is not closed under prefix lexicalization.\nTheorem 1. There exists an SCFG which cannot be converted to an equivalent PL-SCFG.\nProof. The SCFG in (7) generates the language L = {〈aibjci, bjai〉| i ≥ 0, j ≥ 1}, but this language cannot be generated by any PL-SCFG:\n(7)\n〈S → A 1 , S → A 1 〉 〈A→ aA 1 c, A→ A 1 a〉 〈A→ bB 1 , A→ bB 1 〉 〈A→ b, A→ b〉 〈B → bB 1 , B → bB 1 〉 〈B → b, B → b〉\nSuppose, for the purpose of contradiction, that some PL-SCFG does generate L; call this grammar G. Then the following derivations must all be possible in G for some nontermials U, V,X, Y :\ni) 〈U 1 , V 1 〉 ⇒∗ 〈bkU 1 bm, bnV 1 bp〉, where k +m = n+ p and n ≥ 1\nii) 〈X 1 , Y 1 〉 ⇒∗ 〈aqX 1 cq, arY 1 as〉, where q = r + s and r ≥ 1\niii) 〈S 1 , S 1 〉 ⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉, where α1, ..., α4 ∈ (N ∪ Σ)∗\niv) 〈X 1 , Y 1 〉 ⇒∗ 〈α5U 1 α6, α7V 1 α8〉, where α5, α6, α8 ∈ (N ∪ Σ)∗, α7 ∈ Σ(N ∪ Σ)∗\ni and ii follow from the same arguments used in the pumping lemma for (non-synchronous) context free languages (Bar-Hillel et al., 1961): strings in L can contain arbitrarily many as, bs, and cs, so there must exist some pumpable cycles\nwhich generate these characters. In i, k + m = n + p because the final derived strings must contain an equal number of bs, and n ≥ 1 because G is prefix lexicalized; in ii the constraints on q, r and s follow likewise from L. iii follows from the fact that, in order to pump on the cycle in ii, this cycle must be reachable from the start symbol. iv follows from the fact that a context-free production cannot generate a discontinuous span. Once the cycle in i has generated a b, it is impossible for ii to generate an a on one side of the b and a c on the other. Therefore i must always be derived strictly later than ii, as shown in iv.\nNow we obtain a contradiction. Given that G can derive all of i through iv, the following derivation is also possible: (8)\n〈S 1 , S 1 〉 ⇒∗ 〈α1X 1 α2, bα3Y 1 α4〉 ⇒∗ 〈α1aqX 1 cqα2, bα3arY 1 asα4〉 ⇒∗ 〈α1aqα5U 1 α6cqα2, bα3arα7V 1 α8asα4〉 ⇒∗ 〈α1aqα5bkU 1 bmα6cqα2,\nbα3a rα7b nV 1 bpα8a sα4〉\nBut since n, r ≥ 1, the target string derived this way contains an a before a b and does not belong to L.\nThis is a contradiction: if G is a PL-SCFG then it must generate i through iv, but if so then it also generates strings which do not belong to L. Thus no PL-SCFG can generate L, and SCFG must not be closed under prefix lexicalization.\nThere also exist grammars which cannot be prefix lexicalized because they contain cyclic chain rules. If an SCFG can derive something of the form 〈X 1 , Y 1 〉 ⇒∗ 〈xX 1 , Y 1 〉, then it can generate arbitrarily many symbols in the source string without adding anything to the target string. Prefix lexicalizing the grammar would force it to generate some terminal symbol in the target string at each step of the derivation, making it unable to generate the original language where a source string may be unboundedly longer than its corresponding target. We call an SCFG chain-free if it does not contain a cycle of chain rules of this form. The remainder of this paper focuses on chain-free grammars, like (7), which cannot be converted to PL-SCFG despite containing no such cycles."
  }, {
    "heading": "4 Prefix Lexicalization using STAG",
    "text": "We now present a method for prefix lexicalizing an SCFG by converting it to an STAG.\nSXA\n〉\n〈 SXA\nSXA\n〉\nTheorem 2. Given a rank-k SCFG G which is εfree and chain-free, an STAGH exists such thatH is prefix lexicalized and L(G) = L(H). The rank of H is at most 2k, and |H| = O(|G|3).\nProof. Let G = (N,Σ, P, S) be an ε-free, chainfree SCFG. We provide a constructive method for prefix lexicalizing the target side of G.\nWe begin by constructing an intermediate grammar GXA for each pair of nonterminals X,A ∈ N \\ {S}. For each pair X,A ∈ N \\ {S}, GXA will be constructed to generate the language of sentential forms derivable from 〈X 1 , A 1 〉 via a target-side terminal leftmost derivation (TTLD). A TTLD is a derivation of the form in Figure 2, where the leftmost nonterminal in the target string is expanded until it produces a terminal symbol as the first character. We write 〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉 to mean that 〈X 1 , A 1 〉 derives 〈u, v〉 by way of a TTLD; in this notation, LXA = {〈u, v〉|〈X 1 , A 1 〉 ⇒∗TTLD 〈u, v〉} is the language of sentential forms derivable from 〈X 1 , A 1 〉 via a TTLD.\nGiven X,A ∈ N \\ {S} we formally define GXA as an STAG over the terminal alphabet ΣXA = N ∪ Σ and nonterminal alphabet NXA = {YXA|Y ∈ N}, with start symbol SXA. NXA contains nonterminals indexed by XA to ensure that two intermediate grammars GXA and GY B do not interact as long as 〈X,A〉 6= 〈Y,B〉.\nGXA contains four kinds of tree pairs: 3\n• For each rule in G of the form 〈X → α1, A→ aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗, we add a tree pair of the form in Figure 3(a).\n• For each rule in G of the form 〈Y → α1, B → aα2〉, a ∈ Σ, αi ∈ (N∪Σ)∗, Y,B ∈ N \\ {S}, we add a tree pair of the form in Figure 3(b).\n• For each rule in G of the form 〈Y → α1Z 1 β1, B → C 1 α2〉, Y, Z,B,C ∈ N \\ {S}, αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the form in Figure 3(c).\nAs a special case, if Y = Z we collapse the root node and adjunction site to produce a tree pair of the following form:\n(9)\n〈 ZXA 1\nα1ZXA ∗ β1\n, CXA\nα2BXA ↓ 1 〉 • For each rule in G of the form 〈X → α1Y 1 β1, A→ C 1 α2〉, Y,C ∈ N , αi, βi ∈ (N ∪ Σ)∗, we add a tree pair of the form in Figure 3(d).\n3In all cases, we assume that symbols inN (notNXA) retain any links they bore in the original grammar, even though they belong to the terminal alphabet in GXA and therefore do not participate in rewriting operations. In the final constructed grammar, these symbols will belong to the nonterminal alphabet again, and the links will function normally.\nFigure 4 gives a concrete example of constructing an intermediate grammar tree pair on the basis of an SCFG rule.\nLemma 1. GXA generates the language LXA.\nProof. This can be shown by induction over derivations of increasing length. The proof is straightforward but very long, so we provide only a sketch; the complete proof is provided in the supplementary material.\nAs a base case, observe that a tree of the shape in Figure 3(a) corresponds straightforwardly to the derivation\n(10) 〈X 1 , A 1 〉 ⇒ 〈α1, aα2〉\nwhich is a TTLD starting from 〈X,A〉. By construction, therefore, every TTLD of the shape in (10) corresponds to some tree in GXA of shape 3(a); likewise every derivation inGXA comprising a single tree of shape 3(a) corresponds to a TTLD of the shape in (10).\nAs a second base case, note that a tree of the shape in Figure 3(b) corresponds to the last step of a TTLD like (11):\n(11) 〈X 1 , A 1 〉 ⇒∗TTLD 〈uY 1 v,B 1 w〉 ⇒ 〈uα1v, aα2w〉\nIn the other direction, the last step of any TTLD of the shape in (11) will involve some rule of the shape 〈Y → α1, B → aα2〉; by construction GXA must contain a corresponding tree pair of shape 3(b).\nTogether, these base cases establish a one-toone correspondence between single-tree derivations in GXA and the last step of a TTLD starting from 〈X,A〉.\nNow, assume that the last n steps of every TTLD starting from 〈X,A〉 correspond to some derivation over n trees in GXA, and vice versa. Then the last n + 1 steps of that TTLD will also correspond to some n+ 1 tree derivation in GXA, and vice versa.\nTo see this, consider the step n+ 1 steps before the end of the TTLD. This step may be in the middle of the derivation, or it may be the first step of the derivation. If it is in the middle, then this step must involve a rule of the shape\n(12) 〈Y → α1Z 1 β1, B → C 1 α2〉\nThe existence of such a rule in G implies the existence of a corresponding tree in GXA of the shape in Figure 3(c). Adding this tree to the existing n-tree derivation yields a new n + 1 tree derivation corresponding to the last n + 1 steps of the TTLD.4 In the other direction, if the n+ 1th tree5 of a derivation in GXA is of the shape in Figure 3(c), then this implies the existence of a production in G of the shape in (12). By assumption the first n trees of the derivation in GXA correspond to some TTLD in G; by prepending the rule from (12) to this TTLD we obtain a new TTLD of length n + 1 which corresponds to the entire n + 1 tree derivation in GXA.\nFinally, consider the case where the TTLD is only n + 1 steps long. The first step must involve a rule of the form\n(13) 〈X → α1Y 1 β1, A→ C 1 α2〉\nThe existence of such a rule implies the existence of a corresponding tree in GXA of the shape in Figure 3(d). Adding this tree to the derivation which corresponds to the last n steps of the TTLD yields a new n+1 tree derivation corresponding to the entire n+ 1 step TTLD. In the other direction, if the last tree of an n + 1 tree derivation in GA is of the shape in Figure 3(d), then this implies the\n4It is easy to verify by inspection of Figure 3 that whenever one rule from G can be applied to the output of another rule, then the tree pairs in GXA which correspond to these rules can compose with one another. Thus we can add the new tree to the existing derivation and be assured that it will compose with one of the trees that is already present.\n5Although trees in GXA may contain symbols from the nonterminal alphabet of G, these symbols belong to the terminal alphabet in GXA. Only nonterminals in NXA will be involved in this derivation, and by construction there is at most one such nonterminal per tree. Thus a well-formed derivation structure in GXA will never branch, and we can refer to the n+ 1th tree pair as the one which is at depth n in the derivation structure.\nexistence of a production inG of the shape in (13). By assumption the first n trees of the derivation in GXA correspond to some TTLD inG; by prepending the rule from (13) to this TTLD we obtain a new TTLD of length n + 1 which corresponds to the entire n+ 1 tree derivation in GXA.\nTaken together, these cases establish a one-toone correspondence between derivations in GXA and TTLDs which start from 〈X,A〉; in turn they confirm that GXA generates the desired language LXA.\nOnce we have constructed an intermediate grammar GXA for each X,A ∈ N \\ {S}, we obtain the final STAG H as follows:\n1. Convert the input SCFG G to an equivalent STAG. For each rule 〈A1 → α1, A2 → α2〉, where Ai ∈ N , αi ∈ (N ∪ Σ)∗, create a tree pair of the form\n(14)\n〈 A1\nα1\n, A2\nα2 〉 where each pair of linked nonterminals in the original rule become a pair of linked substitution sites in the tree pair. The terminal and nonterminal alphabets and start symbol are unchanged. Call the resulting STAG H .\n2. For all X,A ∈ N \\ {S}, add all of the tree pairs from the intermediate grammar GXA to the new grammar H . Expand N to include the new nonterminal symbols in NXA.\n3. For every X,A ∈ N , in all tree pairs where the target tree’s leftmost leaf is labeled with A and this node is linked to anX , replace this occurrence of A with SXA. Also replace the linked node in the source tree.\n4. For every X,A ∈ N , let RXA be the set of all tree pairs rooted in SXA, and let TXA be the set of all tree pairs whose target tree’s leftmost leaf is labeled with SXA. For every 〈s, t〉 ∈ TXA and every 〈s′, t′〉 ∈ RXA, substitute or adjoin s′ and t′ into the linked SXA nodes in s and t, respectively. Add the derived trees to H .\n5. For all X,A ∈ N , let TXA be defined as above. Remove all tree pairs in TXA from H .\n6. For all X,A ∈ N , let RXA be defined as above. Remove all tree pairs in RXA from H .\nWe now claim that H generates the same language as the original grammar G, and all of the target trees in H are prefix lexicalized.\nThe first claim follows directly from the construction. Step 1 merely rewrites the grammar in a new formalism. From Lemma 1 it is clear that steps 2–3 do not change the generated language: the set of string pairs generable from a pair of SXA nodes is identical to the set generable from 〈X,A〉 in the original grammar. Step 4 replaces some nonterminals by all possible alternatives; steps 5– 6 then remove the trees which were used in step 4, but since all possible combinations of these trees have already been added to the grammar, removing them will not alter the language.\nThe second claim follows from inspection of the tree pairs generated in Figure 3. Observe that, by construction, for all X,A ∈ N every target tree rooted in SXA is prefix lexicalized. Thus the trees created in step 4 are all prefix lexicalized variants of non-lexicalized tree pairs; steps 5–6 then remove the non-lexicalized trees from the grammar.\nFigure 5 gives an example of this transformation applied to a small grammar. Note how A nodes at the left edge of the target trees end up rewritten as SAA nodes, as per step 4 of the transformation."
  }, {
    "heading": "5 Complexity & Formal Properties",
    "text": "Our conversion generates a subset of the class of prefix lexicalized STAGs in regular form, which we abbreviate to PL-RSTAG (regular form for TAG is defined in Rogers 1994). This section discusses some formal properties of PL-RSTAG.\nGenerative Capacity PL-RSTAG is weakly equivalent to the class of ε-free, chain-free SCFGs: this follows immediately from the proof that our transformation does not change the language generated by the input SCFG. Note that every TAG in regular form generates a context-free language (Rogers, 1994).\nAlignments and Reordering PL-RSTAG generates the same set of reorderings (alignments) as SCFG. Observe that our transformation does not cause nonterminals which were linked in the original grammar to become unlinked, as noted for example in Figure 4. Thus subtrees which are gener-\n〈S → B 2 cA 1 , S → A 1 cB 2 〉 〈A→ B 2 cA 1 , A→ A 1 cB 2 〉 〈A→ a, A→ a〉 〈B → b, B → b〉\n〈 S B ↓ 1 c SAA\na\n,\nS\nSAA\na\nc B ↓ 1\n〉 〈 A B ↓ 1 c SAA\nAAA 2\na\n,\nA\nSAA\na AAA ↓ 2\nc B ↓ 1 〉 〈 S\nB ↓ 1 c SAA\nAAA 2\na\n,\nS\nSAA\na AAA ↓ 2\nc B ↓ 1 〉 〈 A B ↓ 1 c SAA\na\n,\nA\nSAA\na\nc B ↓ 1\n〉 〈 AAA 1\nB ↓ 2 c AAA∗\n, AAA\nc B ↓ 2 AAA ↓ 1 〉 〈\nAAA\nB ↓ 1 c AAA∗\n, AAA\nc B ↓ 1\n〉 〈 B\nb\n, B\nb\n〉 〈 A\na\n, A\na 〉\nFigure 5: An SCFG and the STAG which prefix lexicalizes it. Non-productive trees have been omitted.\nGrammar |G| |H| % of G prefix lexicalized log|G|(|H|) Siahbani and Sarkar (2014a) (Zh-En) 18.5M 23.6T 63% 1.84 Example (7) 6 14 66% 1.47 ITG (10000 translation pairs) 10,003 170,000 99.97% 1.31\nTable 1: Grammar sizes before and after prefix lexicalization, showing O(n2) size increase instead of the worst case O(n3). |G| and |H| give the grammar size before and after prefix lexicalization; log|G| |H| is the increase as a power of the initial size. We also show the percentage of productions which are already prefix lexicalized in G.\nated by linked nonterminals in the original grammar will still be generated by linked nonterminals in the final grammar, so no reordering information is lost or added.6 This result holds despite the fact that our transformation is only applicable to chainfree grammars: chain rules cannot introduce any reorderings, since by definition they involve only a single pair of linked nonterminals.\nGrammar Rank If the input SCFG G has rank k, then the STAG H produced by our transformation has rank at most 2k. To see this, observe that the construction of the intermediate grammars increases the rank by at most 1 (see Figure 3(b)). When a prefix lexicalized tree is substituted at the left edge of a non-lexicalized tree, the link on the substitution site will be consumed, but up to k+ 1 new links will be introduced by the substituting tree, so that the final tree will have rank at most 2k.\nIn the general case, rank-k STAG is more powerful than rank-k SCFG; for example, a rank-4 SCFG is required to generate the reordering in 〈S → A 1 B 2 C 3 D 4 , S → C 3 A 1 D 4 B 2 〉 (Wu, 1997), but this reordering is captured by the\n6Although we consume one link whenever we substitute a prefix lexicalized tree at the left edge of an unlexicalized tree, that link can still be remembered and used to reconstruct the reorderings which occurred between the two sentences.\nfollowing rank-3 STAG:〈 S X\nA ↓ 1 X 2\nC ↓ 3\n, S\nC ↓ 3 A ↓ 1 X ↓ 2 〉 〈\nX\nB ↓ 1 X∗ D ↓ 2\n, X\nD ↓ 2 B ↓ 1 〉 For this reason, we speculate that it is possible to further transform the grammars produced by our lexicalization in order to reduce their rank, but the details of this transformation remain as future work.\nThis potentially poses a solution to an issue raised by Siahbani and Sarkar (2014b). On a Chinese-English translation task, they find that sentences like (15) involve reorderings which cannot be captured by a rank-2 prefix lexicalized SCFG: (15) Tā bǔchōng shuō , liánhé zhèngfǔ mùqián zhuàngkuàng wěndı̀ng ...\nHe added that the coalition government is now in stable condition ...\nIf rank-k PL-RSTAG is more powerful than rank-k\nSCFG, using a PL-RSTAG here would permit capturing more reorderings without using grammars of higher rank.\nParse Complexity Because the grammar produced is in regular form, each side can be parsed in time O(n3) (Rogers, 1994), for an overall parse complexity of O(n3k), where n is sentence length and k is the grammar rank.\nGrammar Size and Experiments If H is the PL-RSTAG produced by applying our transformation to an SCFG G, then H contains O(|G|3) elementary tree pairs, where |G| is the number of synchronous productions in G. When the set of nonterminalsN is small compared to |G|, a tighter bound is given by O(|G|2|N |2).\nTable 1 shows the actual size increase on a variety of grammars: here |G| is the size of the initial grammar, |H| is the size after applying our transformation, and the increase is expressed as a power of the original grammar size. We apply our transformation to the grammar from Siahbani and Sarkar (2014a), which was created for a ChineseEnglish translation task known to involve complex reorderings that cannot be captured by PL-SCFG (Siahbani and Sarkar, 2014b). We also consider the grammar in (7) and an ITG (Wu, 1997) containing 10,000 translation pairs, which is a grammar of the sort that has previously been used for word alignment tasks (cf Zhang and Gildea 2005). We always observe an increase within O(|G|2) rather than the worst-case O(|G|3), because |N | is small relative to |G| in most grammars used for NLP tasks.\nWe also investigated how the proportion of prefix lexicalized rules in the original grammar affects the overall size increase. We sampled grammars with varying proportions of prefix lexicalized rules from the grammar in Siahbani and Sarkar (2014a); Table 2 shows the result of lexicalizing these samples. We find that the worst case size increase occurs when 50% of the original grammar is already prefix lexicalized. This is because the size increase depends on both the number of prefix lexicalized trees in the intermediate grammars (which grows with the proportion of lexicalized rules) and the number of productions which need to be lexicalized (which shrinks as the proportion of prefix lexicalized rules increases). At 50%, both factors contribute appreciably to the grammar size, analogous to how the function f(x) = x(1− x) takes\nits maximum at x = 0.5."
  }, {
    "heading": "6 Applications",
    "text": "The LR decoding algorithm from Watanabe et al. (2006) relies on prefix lexicalized rules to generate a prefix of the target sentence during machine translation. At each step, a translation hypothesis is expanded by rewriting the leftmost nonterminal in its target string using some grammar rule; the prefix of this rule is appended to the existing translation and the remainder of the rule is pushed onto a stack, in reverse order, to be processed later. Translation hypotheses are stored in stacks according to the length of their translated prefix, and beam search is used to traverse these hypotheses and find a complete translation. During decoding, the source side is processed by an Earley-style parser, with the dot moving around to process nonterminals in the order they appear on the target side.\nSince the trees on the target side of our transformed grammar are all of depth 1, and none of these trees can compose via the adjunction operation, they can be treated like context-free rules and used as-is in this decoding algorithm. The only change required to adapt LR decoding to use a PL-RSTAG is to make the source side use a TAG parser instead of a CFG parser; an Earley-style parser for TAG already exists (Joshi and Schabes, 1997), so this is a minor adjustment.\nCombined with the transformation in Section 4, this suggests a method for using LR decoding without sacrificing translation quality. Previously, LR decoding required the use of heuristically generated PL-SCFGs, which cannot model some reorderings (Siahbani and Sarkar, 2014a). Now, an SCFG tailored for a translation task can be transformed directly to PL-RSTAG and used for decod-\ning; unlike a heuristically induced PL-SCFG, the transformed PL-RSTAG will generate the same language as the original SCFG which is known to handle more reorderings.\nNote that, since applying our transformation may double the rank of a grammar, this method may prove prohibitively slow. This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar."
  }, {
    "heading": "7 Related Work",
    "text": "Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993). Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs. Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs).\nOther extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always the target side that will be prefix lexicalized.\nLexicalization of synchronous grammars was addressed by Zhang and Gildea (2005), but they consider lexicalization rather than prefix lexicalization, and they only consider SCFGs of rank 2. They motivate their results using a word alignment task, which may be another possible application for our lexicalization.\nAnalogous to our closure result, Aho and Ullman (1969) prove that SCFG does not admit a normal form with bounded rank like Chomsky normal form.\nBlum and Koch (1999) use intermediate grammars like our GXAs to transform a CFG to GNF. Another GNF transformation (Rosenkrantz, 1967) is used by Schabes and Waters (1995) to define Tree Insertion Grammars (which are also weakly equivalent to CFG).\nWe rely on Rogers (1994) for the claim that\nour transformed grammars generate context-free languages despite allowing wrapping adjunction; an alternative proof could employ the results of Swanson et al. (2013), who develop their own context-free TAG variant known as osTAG.\nKaeshammer (2013) introduces the class of synchronous linear context-free rewriting systems to model reorderings which cannot be captured by a rank-2 SCFG. In the event that rank-k PL-RSTAG is more powerful than rank-k SCFG, our work can be seen as an alternative approach to the same problem.\nFinally, Nesson et al. (2008) present an algorithm for reducing the rank of an STAG on-the-fly during parsing; this presents a promising avenue for proving a smaller upper bound on the rank increase caused by our transformation."
  }, {
    "heading": "8 Conclusion and Future Work",
    "text": "We have demonstrated a method for prefix lexicalizing an SCFG by converting it to an equivalent STAG. This process is applicable to any SCFG which is ε- and chain-free. Like the original GNF transformation for CFGs our construction at most cubes the grammar size, though when applied to the kinds of synchronous grammars used in machine translation the size is merely squared. Our transformation preserves all of the alignments generated by SCFG, and retains properties such as O(n3k) parsing complexity for grammars of rank k. We plan to verify whether rank-k PL-RSTAG is more powerful than rank-k SCFG in future work, and to reduce the rank of the transformed grammar if possible. We further plan to empirically evaluate our lexicalization on an alignment task and to offer a comparison against the lexicalization due to Zhang and Gildea (2005)."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors wish to thank the anonymous reviewers for their helpful comments. The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC RGPIN-2018-06437 and RGPAS-2018-522574) to the second author. We dedicate this paper to the memory of Prof. Aravind Joshi; a short hallway conversation with him at ACL 2014 was the seed for this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "On the covering and reduction problems for contextfree grammars",
    "authors": ["James N. Gray", "Michael A. Harrison."],
    "venue": "Journal of the ACM 19(4):675– 698. https://doi.org/10.1145/321724. 321732.",
    "year": 1972
  }, {
    "title": "A new normal-form theorem for context-free phrase structure grammars",
    "authors": ["Sheila A. Greibach."],
    "venue": "Journal of the ACM 12(1):42–52. https://doi.org/",
    "year": 1965
  }, {
    "title": "Tree adjunct grammars",
    "authors": ["Aravind Joshi", "Leon Levy", "Masako Takahashi."],
    "venue": "Journal of Computer and System Sciences 10(1):136– 163.",
    "year": 1975
  }, {
    "title": "Tree-adjoining grammars",
    "authors": ["Aravind Joshi", "Yves Schabes."],
    "venue": "G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, Vol. 3: Beyond Words, Springer-Verlag New York, Inc., New York, NY, USA, chapter 2, pages 69–124.",
    "year": 1997
  }, {
    "title": "Synchronous linear context-free rewriting systems for machine translation",
    "authors": ["Miriam Kaeshammer."],
    "venue": "M. Carpuat, L. Specia, and D. Wu, editors, Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Transla-",
    "year": 2013
  }, {
    "title": "Treeadjoining grammars are not closed under strong lexicalization",
    "authors": ["Marco Kuhlmann", "Giorgio Satta."],
    "venue": "Computational Linguistics 38(3):617– 629. https://doi.org/10.1162/COLI_a_ 00090.",
    "year": 2012
  }, {
    "title": "Syntax-directed transduction",
    "authors": ["Philip M. Lewis", "Richard E. Stearns."],
    "venue": "Journal of the ACM 15(3):465–488. https://doi.org/10. 1145/321466.321477.",
    "year": 1968
  }, {
    "title": "Strong lexicalization of tree adjoining grammars",
    "authors": ["Andreas Maletti", "Joost Engelfriet."],
    "venue": "The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Vol-",
    "year": 2012
  }, {
    "title": "Optimal k-arization of synchronous tree-adjoining grammar",
    "authors": ["Rebecca Nesson", "Giorgio Satta", "Stuart M. Shieber."],
    "venue": "Proceedings of ACL-08: HLT . Association for Computational Linguistics, Columbus, Ohio, pages 604–612.",
    "year": 2008
  }, {
    "title": "Capturing CFLs with tree adjoining grammars",
    "authors": ["James Rogers."],
    "venue": "Proceedings of the 32nd Annual Meeting of the Association for Computational",
    "year": 1994
  }, {
    "title": "Matrix equations and normal forms for context-free grammars",
    "authors": ["Daniel J. Rosenkrantz."],
    "venue": "Journal of the Association for Computing Machinery 14(3):501–507.",
    "year": 1967
  }, {
    "title": "Lexicalized context-free grammars",
    "authors": ["Yves Schabes", "Richard C. Waters."],
    "venue": "L. Schubert, editor, 31st Annual Meeting of the Association for Computational Linguistics, 22-26 June 1993, Ohio State University, Columbus, Ohio, USA, Proceed-",
    "year": 1993
  }, {
    "title": "Tree insertion grammar: Cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced",
    "authors": ["Yves Schabes", "Richard C. Waters."],
    "venue": "Computational Linguistics 21(4):479–513.",
    "year": 1995
  }, {
    "title": "A representation theorem for algebraic and context-free power series in noncommuting variables",
    "authors": ["Eliahu Shamir."],
    "venue": "Information and Control 11(1/2):239–254. https://doi.org/10. 1016/S0019-9958(67)90529-3.",
    "year": 1967
  }, {
    "title": "Restricting the weakgenerative capacity of synchronous tree-adjoining grammars",
    "authors": ["Stuart M. Shieber."],
    "venue": "Computational Intelligence 10:371– 385. https://doi.org/10.1111/j. 1467-8640.1994.tb00003.x.",
    "year": 1994
  }, {
    "title": "Efficient left-to-right hierarchical phrase-based translation with improved reordering",
    "authors": ["Maryam Siahbani", "Baskaran Sankaran", "Anoop Sarkar."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-",
    "year": 2013
  }, {
    "title": "Expressive hierarchical rule extraction for left-to-right translation",
    "authors": ["Maryam Siahbani", "Anoop Sarkar."],
    "venue": "Proceedings of the 11th Biennial Conference of the Association for Machine Translation in the Americas (AMTA-2014)., Vancouver,",
    "year": 2014
  }, {
    "title": "Two improvements to left-to-right decoding for hierarchical phrase-based machine translation",
    "authors": ["Maryam Siahbani", "Anoop Sarkar."],
    "venue": "A. Moschitti, B. Pang, and W. Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Nat-",
    "year": 2014
  }, {
    "title": "A context free TAG variant",
    "authors": ["Ben Swanson", "Elif Yamangil", "Eugene Charniak", "Stuart M. Shieber."],
    "venue": "Proceedings of the 51st Annual Meeting of",
    "year": 2013
  }, {
    "title": "Left-to-right target generation for hierarchical phrase-based translation",
    "authors": ["Taro Watanabe", "Hajime Tsukada", "Hideki Isozaki."],
    "venue": "N. Calzolari, C. Cardie, and P. Isabelle, editors, ACL 2006, 21st International Conference on Computational Linguistics and",
    "year": 2006
  }, {
    "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora",
    "authors": ["Dekai Wu."],
    "venue": "Computational Linguistics 23(3):377–403.",
    "year": 1997
  }, {
    "title": "Stochastic lexicalized inversion transduction grammar for alignment",
    "authors": ["Hao Zhang", "Daniel Gildea."],
    "venue": "K. Knight, H. T. Ng, and K. Oflazer, editors, ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings",
    "year": 2005
  }],
  "id": "SP:6f8bb77ac01f63e6e98711567538edfe80586586",
  "authors": [{
    "name": "Logan Born",
    "affiliations": []
  }, {
    "name": "Anoop Sarkar",
    "affiliations": []
  }, {
    "name": "Simon Fraser",
    "affiliations": []
  }],
  "abstractText": "We show that an ε-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar’s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.",
  "title": "Prefix Lexicalization of Synchronous CFGs using Synchronous TAG"
}