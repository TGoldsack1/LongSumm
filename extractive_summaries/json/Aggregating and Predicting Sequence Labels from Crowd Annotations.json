{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 299–309 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1028"
  }, {
    "heading": "1 Introduction",
    "text": "Many important problems in Natural Language Processing (NLP) may be viewed as sequence labeling tasks, such as part-of-speech (PoS) tagging, named-entity recognition (NER), and Information Extraction (IE). As with other machine learning tasks, automatic sequence labeling typically requires annotated corpora on which to train predictive models. While such annotation was traditionally performed by domain experts, crowdsourcing has become a popular means to acquire large labeled datasets at lower cost, though annotations from laypeople may be lower quality than those from domain experts (Snow et al., 2008). It\n1 Soure code and biomedical abstract data: www.github.com/thanhan/seqcrowd-acl17, www.byronwallace.com/EBM_abstracts_data\nis therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of “reference standard” consensus labels. While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems (Sheshadri and Lease, 2013), far less work has explored crowdbased annotation of sequences (Finin et al., 2010; Hovy et al., 2014; Rodrigues et al., 2014).\nIn this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2). For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model.\nWhen human-level accuracy in tagging of sequences is not crucial, automatic labeling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective. Given a training set of crowd labels, how can we best predict sequences in unannotated text? Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by Yang et al. (2010)? We investigate both directions in this work.\nOur approach is to augment existing sequence labeling models such as HMMs (Rabiner and Juang, 1986) and LSTMs (Hochreiter and Schmidhuber, 1997; Lample et al., 2016) by introducing an explicit ”crowd component”. For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables. For the LSTM, we introduce a vector representation for each annotator. In\n299\nboth cases, the crowd component models both the noise from labels and the label quality from each annotator. We find that principled combination of the “crowd component” with the “sequence component” yields strong improvement.\nFor evaluation, we consider two practical applications in two text genres: NER in news and IE from medical abstracts. Recognizing namedentities such as people, organizations or locations can be viewed as a sequence labeling task in which each label specifies whether each word is Inside, Outside or Beginning (IOB) a namedentity. For this task, we consider the English portion of the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), using crowd labels collected by Rodrigues et al. (2014).\nFor the IE application, we use a set of biomedical abstracts that describe Randomized Controlled Trials (RCTs). The crowdsourced annotations comprise labeled text spans that describe the patient populations enrolled in the corresponding RCTs. For example, an abstract may contain the text: we recruited and enrolled diabetic patients. Identifying these sequences is useful for downstream systems that process biomedical literature, e.g., clinical search engines (Huang et al., 2006; Schardt et al., 2007; Wallace et al., 2016).\nContributions. We present a systematic investigation and evaluation of alternative methods for handling and utilizing crowd labels for sequential annotation tasks. We consider both how to best aggregate sequential crowd labels (Task 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community."
  }, {
    "heading": "2 Related Work",
    "text": "We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations.\nSequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission\nmodel from a hidden state to an observation and a transition model from a hidden state to the next hidden state. Later work focused on discriminative models such as Maximum Entropy Models (Chieu and Ng, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). These were able to achieve strong predictive performance by exploiting arbitrary features, but they may not be the best choice for label aggregation. Also, compared to the simple HMM model, discriminative sequentially structured models require more complex optimization and are generally more difficult to extend. Here we argue for the generative HMMs for our first task of aggregating crowd labels. The generative nature of HMMs is a good fit for existing crowd modeling techniques and also enables very efficient parameter estimation.\nIn addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels.\nFor Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al., 2011; Kim, 2014; Zhang and Wallace, 2015) and LSTMs (Lample et al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’.\nCrowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods.\nHowever, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF\nmodel from multiple labels (although the identities of the annotators or workers were not used). Rodrigues et al. (2014) extended this approach to account for worker identities, providing a joint ”crowd-CRF” model. They collected a dataset of crowdsourced labels for a portion of the CoNLL 2003 dataset. Using this, they showed that their model outperformed Dredze et al. (2009)’s model and other baselines. However, due to the technical difficulty of the joint approach with CRFs, they resorted to strong modeling assumptions. For example, their model assumes that for each word, only one worker provides the correct answer while all others label the word completely randomly. While this assumption captures some aspects of label quality, it is potentially problematic, such as for ‘easy words’ labeled correctly by all workers.\nMore recently, ? proposed HMM models for aggregating crowdsourced discourse segmentation labels. However, they did not consider the general sequence labeling setting. Their method includes task-specific assumptions, e.g., that discourse segment lengths follow some empirical distribution estimated from data. In the absence of a gold standard, they evaluated by checking that workers accuracies are consistent and by comparing their two models to each other. We include their approach along with Rodrigues et al. (2014) as a baseline in our evaluation."
  }, {
    "heading": "3 Methods",
    "text": "We present our Task 1 HMM approach in Section 3.1 and our Task 2 LSTM approach in Section 3.2."
  }, {
    "heading": "3.1 HMMs with Crowd Workers",
    "text": "Model: We first define a standard HMM with hidden states hi, observations vi, transition parameter vectors τ hi and emission parameter vectors Ωhi :\nhi+1|hi ∼ Discrete(τ hi) (1) vi|hi ∼ Discrete(Ωhi) (2)\nThe discrete distributions here are governed by Multinomials. In the context of our task, vi is the word at position i and hi is the true, latent class of vi (e.g., entity or non-entity).\nFor the crowd component, assume there are n classes, and let lij be the label for word i provided by worker j. Further, let C(j) be the confusion matrix for worker j, i.e., C(j)k is a vector of size n in which element k′ is the probability of worker j\nproviding the label k′ for a word of true class k:\nlij |hi ∼ Discrete(C(j)hi ) (3) Figure 1 shows the factor graph of this model, which we call HMM-Crowd. Note that we assume that individual crowdworker labels are conditionally independent given the (hidden) true label.\nA common problem with crowdsourcing models is data sparsity. For workers who provide only a few labels, it is hard to derive a good estimate of their confusion matrices. This is exacerbated when the label distribution is imbalanced, e.g., most words are not part of a named entity, concentrating the counts in a few confusion matrix entries. Solutions for this problem include hierarchical models of ‘worker communities’ (Venanzi et al., 2014) or correlations between confusion matrix entries (Nguyen et al., 2016). Although effective, these methods are also quite computationally expensive. For our models, to keep parameter estimation efficient, we use a simpler solution of ‘collapsing’ the confusion matrix into a ‘confusion vector’. For worker j, instead of having the n × n matrix C(j), we use the n × 1 vector C′(j) where C′(j)k is the probability of worker j labeling a word with true class k correctly. We also smooth the estimate of C′ with prior counts as in (Liu and Wang, 2012; Kim and Ghahramani, 2012). Learning: We use the Expectation Maximization (EM) algorithm (Dempster et al., 1977) to learn the parameters (τ ,Ω,C′), given the observations (all the words V and all the worker labels L).\nIn the E-step, given the current estimates of the parameters, we take a forward and a backward\npass in the HMM to infer the hidden states, i.e. to calculate p(hi|V,L) and p(hi, hi+1|V,L) for all appropriate i. Let α(hi) = p(hi, v1:i, l1:i) where v1:i are the words from position 1 to i and l1:i are the crowd labels for these words from all workers. Similarly, let β(hi) = p(vi+1:n, li+1:n|hi). We have the recursions:\nα(hi) = ∑\nhi−1\np(vi|hi)p(hi|hi−1) ∏\nj\np(lij |hi)α(hi−1)\n(4)\nβ(hi) = ∑\nhi+1\np(hi+1|hi)p(vi+1|hi+1)\n∏\nj\np(li+1,j |hi+1)β(hi+1) (5)\nThese are the standard α and β recursions for HMMs augmented with the crowd model: the product ∏ j over the workers j who have provided labels for word i (or i + 1). The posteriors can then be easily evaluated: p(hi|V,L) ∝ α(hi)β(hi) and p(hi, hi+1|V,L) ∝ α(hi)p(hi+1|hi)p(vi+1|hi+1)β(hi+1)\nIn the standard M-step, the parameters are estimated using maximum likelihood. However, we found a Variational Bayesian (VB) update procedure for the HMM parameters similar to (Johnson, 2007; Beal, 2003) provides some improvement and stability. We first define the Dirichlet priors over the transition and emission parameters:\np(τ hi) = Dir(at) (6)\np(Ωhi) = Dir(ae) (7)\nWith these priors, the variational M-step updates the parameters as follows2:\nτ h′|h = exp{Ψ(Eh′|h + at)} exp{Ψ(Eh + nat)}\n(8)\nΩv|h = exp{Ψ(Ev|h + ae)} exp{Ψ(Eh +mae)}\n(9)\nwhere Ψ is the Digamma function, n is the number of states and m is the number of observations. E denotes the expected counts, calculated from the posteriors inferred in the E-step. Eh′|h is the expected number of times the HMM transitioned from state h to state h′, where the expectation is with respect to the posterior distribution p(hi, hi+1|V,L) that we infer in the E step:\nEh′|h = ∑\ni\np(hi = h, hi+1 = h ′|V,L) (10)\n2See Beal (2003) for the derivation and Johnson (2007) for further discussion for the Variational Bayesian approach.\nSimilarly, Eh is the expected number of times the HMM is at state h: Eh = ∑ i p(hi = h|V,L) and Ev|h is the expected number of times the HMM emits the observation v from the state h: Ev|h =∑\ni,vi=v p(hi = h|V,L).\nFor the crowd parameters C′(j), we use the (smoothed) maximum likelihood estimate:\nC ′(j) k =\nE (j) k|k + ac\nE (j) k + nac\n(11)\nwhere ac is the smoothing parameter and E (j) k|k is the expected number of times that worker j correctly labeled a word of true class k as k while E(j)k is the expected total number of words belonging to class k worker j has labeled. Again, the expectation in E is taken under the posterior distributions that we infer in the E step."
  }, {
    "heading": "3.2 Long Short Term Memory with Crowds",
    "text": "For Task 2, we extend the LSTM architecture (Hochreiter and Schmidhuber, 1997) for NER (Lample et al., 2016) to account for noisy crowdsourced labels (this can be easily adapted to other sequence labeling tasks). In this model, the sentence input is first fed into an LSTM block (which includes character- and word-level bi-directional LSTM units). The LSTM block’s output then becomes input to a (fully connected) hidden layer, which produces a vector of tags scores for each word. This tag score vector is the word-level prediction, representing the likelihood of the word being from each tag. All the tags scores are then fed into a ‘CRF layer’ that ‘connects’ the word-level predictions in the sentence and produces the final output: the most likely sequence of tags.\nWe introduce a crowd representation in which a worker vector represents the noise associated with her labels. In other words, the parameters in the original architecture learns the correct sequence labeling model while the crowd vectors add noise to its predictions to ‘explain’ the lower quality of the labels. We assume a perfect worker has a zero vector as her representation while an unreliable worker is represented by a large magnitude vector. At test time, we ignore the crowd component and make predictions by feeding the unlabeled sentence into the original LSTM architecture. At train time, an example consists of the labeled sentence and the ID of the worker who provided the labels. Worker IDs are mapped to vector representations and incorporated into the LSTM architecture.\nWe propose two strategies for incorporating the crowd vector into the LSTM: (1) adding the crowd vector to the tags scores and (2) concatenating the crowd vector to the output of the LSTM block.\nLSTM-Crowd. The first strategy is illustrated in Figure 2. We set the dimension of the crowd vectors to be equal to the number of tags and the addition is element-wise. In this strategy, the crowd vectors have a nice interpretation: the tagconditional noise for the worker. This is useful for worker evaluation and intelligent task routing (i.e. assigning the right work to the right worker).\nLSTM-Crowd-cat. The second strategy is illustrated in Figure 3. We set the crowd vectors to be additional inputs for the Hidden Layer (along with the LSTM block output). In this way, we are free to set the dimension of the crowd vectors and we have a more flexible model of worker noise. This comes with a cost of reduced interpretability and additional parameters in the hidden layer.\nFor both strategies, the crowd vectors are randomly initialized and learned in the same LSTM architecture using Back Propagation (Rumelhart et al., 1985) and Stochastic Gradient Descent (SGD) (Bottou, 2010)."
  }, {
    "heading": "4 Evaluation Setup",
    "text": ""
  }, {
    "heading": "4.1 Datasets & Tuning",
    "text": "NER. We use the English portion of the CoNLL2003 dataset (Tjong Kim Sang and De Meulder, 2003), which includes over 21,000 annotated sentences from 1,393 news articles split into 3 sets: train, validation and test. We also use crowd labels collected by Rodrigues et al. (2014) for 400 articles in the train set3. For Task 1 (aggregating crowd labels), to avoid overfitting, we split these 400 articles into 50% validation and 50% test4. For Task 2 (predicting sequences on unannotated text), we follow Rodrigues et al. (2014) in using the CoNLL validation and test sets.\nBiomedical IE. We use 5,000 medical paper abstracts describing randomized control trials (RCTs) involving people. Each abstract is annotated by roughly 5 Amazon Mechanical Turk workers. Annotators were asked to mark all text spans in a given abstract which identify the population enrolled in the clinical trial. The annotations are therefore binary: inside or outside a span. In addition to annotations collected from laypeople via Mechanical Turk, we also use gold annotations by medical students for a small set of 200 abstracts, which we split into 50% validation and 50% test. For Task 1, we run methods being compared on all 5,000 abstracts, but we evaluate them only using the validation/test set. For Task 2, the validation and test sets are held out. Table 1 presents key statistics of datasets used.\nTuning: In all experiments, validation set results are used to tune the models hyper-parameters. For HMM-Crowd, we have a smoothing parameter and two Dirichlet priors. For our two LSTMs, we have a L2 regularization parameter. For LSTMCrowd-cat, we also have the crowd vector dimen-\n3http://www.fprodrigues.com/software/ crf-ma-sequence-labeling-with-multiple-annotators/\n4Rodrigues et al. (2014)’s results on the ‘training set’ are not directly comparable to ours since they do not partition the crowd labels into validation and test sets.\nsion. For each hyper-parameter, we consider a few (less then 5) different parameter settings for light tuning. We report results achieved on the test set."
  }, {
    "heading": "4.2 Baselines",
    "text": "Task 1. For aggregating crowd labels, we consider the following baselines:\n• Majority Voting (MV) at the token level. Rodrigues et al. (2014) show that this generally performs better than MV at the entity level.\n• Dawid and Skene (1979) weighted voting at the token level. We tested both a popular public implementation5 of Dawid-Skene and our own and found that ours performed better (likely due to smoothing), so we report it.\n• MACE (Hovy et al., 2013), using the authors’ public implementation6.\n• Dawid-Skene then HMM. We propose a simple heuristic to aggregate sequential crowd labels: (1) use Dawid and Skene (1979) to induce consensus labels from individual crowd labels; (2) train a HMM using the input text and consensus labels; and then (3) use the trained HMM to predict and output labels for the input text. We also tried using a CRF or LSTM as the sequence labeler but found the HMM performed best. This is not surprising: CRFs and LSTM are good at predicting unseen sequences, whereas the predictions here are on the seen training sequences.\n• Rodrigues et al. (2014)’s CRF with Multiple Annotators (CRF-MA). We use the source code provided by the authors.\n• ?’s Interval-dependent (ID) HMM using the authors’ source code7. Since they assume binary labels, we can only apply this to the biomedical IE task.\nFor non-sequential aggregation baselines, we evaluate majority voting (MV) and Dawid and Skene (1979) as perhaps the most widely known and used in practice. A recent benchmark evaluation of aggregation methods for (non-sequential) crowd labels found that classic Dawid-Skene was the most consistently strong performing method\n5https://github.com/ipeirotis/Get-Another-Label 6http://www.isi.edu/publications/licensed-sw/mace/ 7https://academiccommons.columbia.edu/catalog/ac:199939\namong those considered, despite its age, while majority voting was often outperformed by other methods (Sheshadri and Lease, 2013).\nDawid and Skene (1979) models a confusion matrix for each annotator, using EM estimation of these matrices as parameters and the true token labels as hidden variables. This is roughly equivalent to our proposed HMM-Crowd model (Section 3), but without the HMM component.\nTask 2. To predict sequences on unannotated text when trained on crowd labels, we consider two broad approaches: (1) directly train the model on all individual crowd annotations; and (2) induce consensus labels via Task 1 and train on them.\nFor approach (1), we report as baselines:\n• Rodrigues et al. (2014)’s CRF-MA\n• Lample et al. (2016)’s LSTM trained on all crowd labels (ignoring worker IDs)\nFor approach (2), we report as baselines:\n• Majority Voting (MV) then Conditional Random Field (CRF). We train the CRF using the CRF Suite package (Okazaki, 2007) with the same features as in Rodrigues et al. (2014), who also report this baseline.\n• Lample et al. (2016)’s LSTM trained on Dawid-Skene (DS) consensus labels."
  }, {
    "heading": "4.3 Metrics",
    "text": "NER. We use the CoNLL 2003 metrics of entitylevel precision, recall and F1. The predicted entity must match the gold entity exactly (i.e. no partial credit is given for partial matches).\nBiomedical IE. The above metrics are overly strict for the biomedical IR task, in which annotated sequences are typically far longer than named-entities. We therefore ‘relax’ the metric to credit partial matches as follows. For each predicted positive contiguous text span, we calculate:\nPrecision = # true positive words\n# words in this predicted span\nFor example, for a predicted span of 10 words, if 6 words are truly positive, the Precision is 60%. We evaluate this ‘local’ precision for each predicted span and then take the average as the ‘global’ precision. Similarly, for each gold span, we calculate:\nRecall = # words in a predicted span # words in this gold span\nThe recall scores for all the gold spans are again averaged to get a global recall score.\nFor the biomedical IE task, because we have gold labels for only a small set of 200 abstracts, we create 100 bootstrap re-samples of the (predicted and gold) spans and perform the evaluation for each re-sample. We then report the mean and standard deviation over these 100 re-samples."
  }, {
    "heading": "5 Evaluation Results",
    "text": ""
  }, {
    "heading": "5.1 Named-Entity Recognition (NER)",
    "text": "Table 2 presents Task 1 results for aggregating crowd labels. For the non-sequential aggregation baselines, we see that Dawid and Skene (1979) outperforms both majority voting and MACE (Hovy et al., 2013). For sequential methods, our heuristic ‘Dawid-Skene then HMM’ method performs surprisingly well, nearly as well as HMM-Crowd. However, we will see that this heuristic does not work as well for biomedical IR.\nRodrigues et al. (2014)’s CRF-MA achieves the highest Precision of all methods, but surprisingly the lowest F1. We use their public implementation but observe different results from what they report (we observed similar results when using all the crowd data without validation/test split as they do). We suspect their released source code may be optimized for Task 2, though we could not reach the authors to verify this.\nTable 3 reports NER results for Task 2: predicting sequences on unannotated text when trained on crowd labels. Results for Rodrigues et al. (2014)’s CRF-MA are reproduced using their public implementation and match their reported results. While CRF-MA outperforms ‘Majority Vote then CRF’ as the authors reported, and achieves the highest Recall of all methods, its F1 results are generally not competitive with other methods.\nMethods based on Lample et al. (2016)’s LSTM generally outperform the CRF methods. Adding a crowd component to the LSTM yields marked improvement of 2.5-3 points F1 vs. the LSTM trained on individual crowd annotations or consensus MV annotations. LSTM-Crowd (trained on individual labels) and ‘HMM-Crowd then LSTM’ (LSTM trained on HMM consensus labels) offer different paths to achieving comparable, best results."
  }, {
    "heading": "5.2 Biomedical Information Extraction (IE)",
    "text": "Tables 4 and 5 present Biomedical IE results for Tasks 1 and 2, respectively. We were unable to run Rodrigues et al. (2014)’s CRF-MA public implementation on the Biomedical IE dataset (due to an ‘Out of Memory Error’ with 8gb max heapsize).\nFor Task 1, Majority Vote achieves nearly 92% Precision but suffers from very low Recall. As with NER, HMM-Crowd achieves the highest Recall and F1, showing 2 points F1 improvement here over non-sequential Dawid and Skene (1979). In contrast with the NER results, our heuristic ‘Dawid-Skene then HMM’ performs much worse for Biomedical IE. In general, we expect heuristics to be less robust than principled methods.\nFor Task 2, as with NER, we again see that LSTM-Crowd (trained on individual labels) and ‘HMM-Crowd then LSTM’ (LSTM trained on HMM consensus labels) offer different paths to achieving fairly comparable results. While LSTM-Crowd-cat again achieves slightly lower F1, simply training Lample et al. (2016)’s LSTM directly on all crowd labels performs much better than seen earlier with NER, likely due to the relatively larger size of this dataset (see Table 1). To further investigate, we study the performances of these LSTM models as a function of training data available. In Figure 4, we see that as the amount of training data decreases, our crowd-augmented LSTM models produce greater relative improvement compared to the original LSTM architecture.\nTable 6 presents an example from Task 1 of a sentence with its gold span, annotations and the outputs from Dawid-Skene and HMM-Crowd. Dawid-Skene aggregates labels based only on the crowd labels while our HMM-Crowd combines that with a sequence model. HMM-Crowd is able to return the longer part of the correct span."
  }, {
    "heading": "6 Conclusions and Future Work",
    "text": "Given a dataset of crowdsourced sequence labels, we presented novel methods to: (1) aggregate sequential crowd labels to infer a best single set of consensus annotations; and (2) use crowd annotations as training data for a model that can predict sequences in unannotated text. We evaluated our approaches on two datasets representing different domains and tasks: general NER and biomedical IE. Results showed that our methods show improvement over strong baselines.\nWe expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as POS tagging and chunking (Hovy et al., 2014). Our methods also provide an estimate of each worker’s label quality, which can be transfered between tasks and is useful for error analysis and intelligent task routing (Bragg et al., 2014). We also plan to investigate extension of the crowd component in our HMM method with hierarchical models, as well as a fully-Bayesian approach."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the reviewers for their valuable comments. This work is supported in part by by National Science Foundation grant No. 1253413 and the National Cancer Institute (NCI) of the National Institutes of Health (NIH), award number UH2CA203711. Any opinions, findings, and conclusions or recommendations expressed by the authors are entirely their own and do not represent those of the sponsoring agencies."
  }],
  "year": 2017,
  "references": [{
    "title": "Variational algorithms for approximate Bayesian inference",
    "authors": ["Matthew James Beal."],
    "venue": "University of London United Kingdom.",
    "year": 2003
  }, {
    "title": "Learning to predict from crowdsourced data",
    "authors": ["Wei Bi", "Liwei Wang", "James T. Kwok", "Zhuowen Tu."],
    "venue": "Uncertainty in Artificial Intelligence.",
    "year": 2014
  }, {
    "title": "Nymble: a high-performance learning name-finder",
    "authors": ["Daniel M Bikel", "Scott Miller", "Richard Schwartz", "Ralph Weischedel."],
    "venue": "Proceedings of the fifth conference on Applied natural language processing. Association for",
    "year": 1997
  }, {
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["Léon Bottou."],
    "venue": "Proceedings of COMPSTAT’2010, Springer, pages 177–186.",
    "year": 2010
  }, {
    "title": "Parallel task routing for crowdsourcing",
    "authors": ["Jonathan Bragg", "Andrey Kolobov", "Mausam Mausam", "Daniel S Weld."],
    "venue": "Second AAAI Conference on Human Computation and Crowdsourcing.",
    "year": 2014
  }, {
    "title": "Named entity recognition: a maximum entropy approach using global information",
    "authors": ["Hai Leong Chieu", "Hwee Tou Ng."],
    "venue": "Proceedings of the 19th international conference on Computational linguisticsVolume 1. Association for Computational Linguis-",
    "year": 2002
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Maximum likelihood estimation of observer errorrates using the em algorithm",
    "authors": ["Alexander Philip Dawid", "Allan M Skene."],
    "venue": "Applied statistics pages 20–28.",
    "year": 1979
  }, {
    "title": "Maximum likelihood from incomplete data via the em algorithm",
    "authors": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin."],
    "venue": "Journal of the royal statistical society. Series B (methodological) pages 1–38.",
    "year": 1977
  }, {
    "title": "Sequence learning from data with multiple labels",
    "authors": ["Mark Dredze", "Partha Pratim Talukdar", "Koby Crammer."],
    "venue": "ECML-PKDD 2009 workshop on Learning from Multi- Label Data.",
    "year": 2009
  }, {
    "title": "Early gains matter: A case for preferring generative over discriminative crowdsourcing models",
    "authors": ["Paul Felt", "Eric Ringger", "Kevin Seppi", "Robbie Haertel."],
    "venue": "Conference of the North American Chapter of the Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "Annotating named entities in twitter data with crowdsourcing",
    "authors": ["Tim Finin", "Will Murnane", "Anand Karandikar", "Nicholas Keller", "Justin Martineau", "Mark Dredze."],
    "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language",
    "year": 2010
  }, {
    "title": "A fully bayesian approach to unsupervised part-of-speech tagging",
    "authors": ["Sharon Goldwater", "Tom Griffiths."],
    "venue": "Annual meeting-association for computational linguistics. Citeseer, volume 45, page 744. http://aclweb.org/anthology/P07-1094.",
    "year": 2007
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Learning whom to trust with mace",
    "authors": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguis-",
    "year": 2013
  }, {
    "title": "Experiments with crowdsourced re-annotation of a pos tagging data set",
    "authors": ["Dirk Hovy", "Barbara Plank", "Anders Søgaard."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Associ-",
    "year": 2014
  }, {
    "title": "PICO as a Knowledge Representation for Clinical Questions",
    "authors": ["Xiaoli Huang", "Jimmy Lin", "Dina Demner-Fushman."],
    "venue": "AMIA 2006 Symposium Proceedings. pages 359–363.",
    "year": 2006
  }, {
    "title": "Estimation of discourse segmentation labels from crowd data",
    "authors": ["Ziheng Huang", "Jialu Zhong", "Rebecca J. Passonneau."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
    "year": 2015
  }, {
    "title": "Why doesn’t em find good hmm pos-taggers? In EMNLP-CoNLL",
    "authors": ["Mark Johnson."],
    "venue": "pages 296–305. http://aclweb.org/anthology/D07-1031.",
    "year": 2007
  }, {
    "title": "A convex formulation for learning from crowds",
    "authors": ["Hiroshi Kajino", "Yuta Tsuboi", "Hisashi Kashima."],
    "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence.",
    "year": 2012
  }, {
    "title": "Bayesian classifier combination",
    "authors": ["Hyun-Chul Kim", "Zoubin Ghahramani."],
    "venue": "International conference on artificial intelligence and statistics. pages 619–627.",
    "year": 2012
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pages 1746–",
    "year": 2014
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "Proceedings of the eighteenth international conference on machine learning, ICML.",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Truelabel+ confusions: A spectrum of probabilistic models in analyzing multiple ratings",
    "authors": ["Chao Liu", "Yi-min Wang."],
    "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12). pages 225–232.",
    "year": 2012
  }, {
    "title": "Variational inference for crowdsourcing",
    "authors": ["Qiang Liu", "Jian Peng", "Alex T Ihler."],
    "venue": "Advances in Neural Information Processing Systems. pages 692– 700.",
    "year": 2012
  }, {
    "title": "A correlated worker model for grouped, imbalanced and multitask data",
    "authors": ["An T Nguyen", "Byron C Wallace", "Matthew Lease."],
    "venue": "Uncertainty in Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "Crfsuite: a fast implementation of conditional random fields (crfs)",
    "authors": ["Naoaki Okazaki."],
    "venue": "http://www.chokkan.org/software/crfsuite/.",
    "year": 2007
  }, {
    "title": "An introduction to hidden markov models",
    "authors": ["Lawrence Rabiner", "B Juang."],
    "venue": "ieee assp magazine 3(1):4–16.",
    "year": 1986
  }, {
    "title": "Learning from crowds",
    "authors": ["Vikas C Raykar", "Shipeng Yu", "Linda H Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni", "Linda Moy."],
    "venue": "Journal of Machine Learning Research 11(Apr):1297–1322.",
    "year": 2010
  }, {
    "title": "Sequence labeling with multiple annotators",
    "authors": ["Filipe Rodrigues", "Francisco Pereira", "Bernardete Ribeiro."],
    "venue": "Machine learning 95(2):165–181.",
    "year": 2014
  }, {
    "title": "Learning internal representations by error propagation",
    "authors": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams."],
    "venue": "Technical report, DTIC Document.",
    "year": 1985
  }, {
    "title": "Utilization of the PICO framework to improve searching PubMed for clinical questions",
    "authors": ["Connie Schardt", "Martha B Adams", "Thomas Owens", "Sheri Keitz", "Paul Fontelo."],
    "venue": "BMC medical informatics and decision making 7(1):16.",
    "year": 2007
  }, {
    "title": "Square: A benchmark for research on computing crowd consensus",
    "authors": ["Aashish Sheshadri", "Matthew Lease."],
    "venue": "First AAAI Conference on Human Computation and Crowdsourcing.",
    "year": 2013
  }, {
    "title": "Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks",
    "authors": ["Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Ng"],
    "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pro-",
    "year": 2008
  }, {
    "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition, pages 142–147",
    "authors": ["Erik F Tjong Kim Sang", "Fien De Meulder."],
    "venue": "http://aclweb.org/anthology/W030419.",
    "year": 2003
  }, {
    "title": "Community-based bayesian aggregation models for crowdsourcing",
    "authors": ["Matteo Venanzi", "John Guiver", "Gabriella Kazai", "Pushmeet Kohli", "Milad Shokouhi."],
    "venue": "Proceedings of the 23rd international conference on World wide web. ACM, pages",
    "year": 2014
  }, {
    "title": "Extracting pico sentences from clinical trial reports using supervised distant supervision",
    "authors": ["Byron C Wallace", "Joël Kuiper", "Aakash Sharma", "Mingxi Brian Zhu", "Iain J Marshall."],
    "venue": "Journal of Machine Learning Research 17(132):1–25.",
    "year": 2016
  }, {
    "title": "Collecting high quality overlapping labels at low cost",
    "authors": ["Hui Yang", "Anton Mityagin", "Krysta M Svore", "Sergey Markov."],
    "venue": "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval.",
    "year": 2010
  }, {
    "title": "A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for sentence classification",
    "authors": ["Ye Zhang", "Byron Wallace."],
    "venue": "arXiv preprint arXiv:1510.03820 . 309",
    "year": 2015
  }],
  "id": "SP:cc5e436b09dcc34bba32086d50e505204f06e6c1",
  "authors": [{
    "name": "An T. Nguyen",
    "affiliations": []
  }, {
    "name": "Byron C. Wallace",
    "affiliations": []
  }, {
    "name": "Junyi Jessy Li",
    "affiliations": []
  }, {
    "name": "Ani Nenkova",
    "affiliations": []
  }, {
    "name": "Matthew Lease",
    "affiliations": []
  }],
  "abstractText": "Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a model that can predict sequences in unannotated text. For aggregation, we propose a novel Hidden Markov Model variant. To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory. We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts. Results show improvement over strong baselines. Our source code and data are available online1.",
  "title": "Aggregating and Predicting Sequence Labels from Crowd Annotations"
}