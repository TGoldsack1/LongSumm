{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1118–1127 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1118"
  }, {
    "heading": "1 Introduction",
    "text": "Building a chatbot that can naturally and consistently converse with human-beings on opendomain topics draws increasing research interests in past years. One important task in chatbots is response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation. Besides playing a critical role in retrieval-based chatbots (Ji et al., 2014), response selection models have been used in automatic evaluation of dialogue generation\n∗Equally contributed. †Work done as a visiting scholar at Baidu. Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn.\n(Lowe et al., 2017) and the discriminator of GANbased (Generative Adversarial Networks) neural dialogue generation (Li et al., 2017).\nEarly studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013). Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance (Zhou et al., 2016; Wu et al., 2017). The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn (Lee et al., 2006; Traum and Heeman, 1996). Figure 1 illustrates semantic connectivities between segment pairs across context and response. As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for example the lexical overlap of words “packages”-“package” and phrases “debian package manager”-“debian pack-\nage manager”. (2) latent dependencies upon which segments are semantically/functionally related to each other. Such as the word “it” in the response, which refers to “dpkg” in the context, as well as the phrase “its just reassurance” in the response, which latently points to “what packages are installed on my system”, the question that speaker A wants to double-check.\nPrevious studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection (Wu et al., 2017). However, existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns. Moreover, Recurrent Neural Networks (RNN) are conveniently used for encoding texts, which is too costly to use for capturing multi-grained semantic representations (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017). As an alternative, we propose to match a response with multi-turn context using dependency information based entirely on attention mechanism. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017), which addresses the issue of sequence-to-sequence generation only using attention, and we extend the key attention mechanism of Transformer in two ways:\nself-attention By making a sentence attend to itself, we can capture its intra word-level dependencies. Phrases, such as “debian package manager”, can be modeled with wordlevel self-attention over word-embeddings, and sentence-level representations can be constructed in a similar way with phraselevel self-attention. By hierarchically stacking self-attention from word embeddings, we can gradually construct semantic representations at different granularities.\ncross-attention By making context and response attend to each other, we can generally capture dependencies between those latently matched segment pairs, which is able to provide complementary information to textual relevance for matching response with multi-turn context.\nWe jointly introduce self-attention and crossattention in one uniform neural matching network, namely the Deep Attention Matching Network\n(DAM), for multi-turn response selection. In practice, DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word. Each utterance in context and response are matched based on segment pairs at different granularities, considering both textual relevance and dependency information. In this way, DAM generally captures matching information between the context and the response from word-level to sentence-level, important matching features are then distilled with convolution & maxpooling operations, and finally fused into one single matching score via a single-layer perceptron.\nWe test DAM on two large-scale public multiturn response selection datasets, the Ubuntu Corpus v1 and Douban Conversation Corpus. Experimental results show that our model significantly outperforms the state-of-the-art models, and the improvement to the best baseline model on R10@1 is over 4%. What is more, DAM is expected to be convenient to deploy in practice because most attention computation can be fully parallelized (Vaswani et al., 2017). Our contributions are two-folds: (1) we propose a new matching model for multi-turn response selection with selfattention and cross-attention. (2) empirical results show that our proposed model significantly outperforms the state-of-the-art baselines on public datasets, demonstrating the effectiveness of selfattention and cross-attention."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Conversational System",
    "text": "To build an automatic conversational agent is a long cherished goal in Artificial Intelligence (AI) (Turing, 1950). Previous researches include taskoriented dialogue system, which focuses on completing tasks in vertical domain, and chatbots, which aims to consistently and naturally converse with human-beings on open-domain topics. Most modern chatbots are data-driven, either in a fashion of information-retrieval (Ji et al., 2014; Banchs and Li, 2012; Nio et al., 2014; Ameixa et al., 2014) or sequence-generation (Ritter et al., 2011). The retrieval-based systems enjoy the advantage of informative and fluent responses because it searches a large dialogue repository and selects\ncandidate that best matches the current context. The generation-based models, on the other hand, learn patterns of responding from dialogues and can directly generalize new responses."
  }, {
    "heading": "2.2 Response Selection",
    "text": "Researches on response selection can be generally categorized into single-turn and multi-turn. Most early studies are single-turn that only consider the last utterance for matching response (Wang et al., 2013, 2015). Recent works extend it to multiturn conversation scenario, Lowe et al.,(2015) and Zhou et al.,(2016) use RNN to read context and response, use the last hidden states to represent context and response as two semantic vectors, and measure their relevance. Instead of only considering the last states of RNN, Wu et al.,(2017) take hidden state at each time step as a text segment representation, and measure the distance between context and response via segment-segment matching matrixes. Nevertheless, matching with dependency information is generally ignored in previous works."
  }, {
    "heading": "2.3 Attention",
    "text": "Attention has been proven to be very effective in Natural Language Processing (NLP) (Bahdanau et al., 2015; Yin et al., 2016; Lin et al., 2017) and other research areas (Xu et al., 2015). Recently, Vaswani et al.,(2017) propose a novel sequenceto-sequence generation network, the Transformer,\nwhich is entirely based on attention. Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized. Previous works on attention mechanism show the superior ability of attention to capture semantic dependencies, which inspires us to improve multi-turn response selection with attention mechanism."
  }, {
    "heading": "3 Deep Attention Matching Network",
    "text": ""
  }, {
    "heading": "3.1 Problem Formalization",
    "text": "Given a dialogue data set D = {(c, r, y)Z}NZ=1, where c = {u0, ..., un−1} represents a conversation context with {ui}n−1i=0 as utterances and r as a response candidate. y ∈ {0, 1} is a binary label, indicating whether r is a proper response for c. Our goal is to learn a matching model g(c, r) with D, which can measure the relevance between any context c and candidate response r."
  }, {
    "heading": "3.2 Model Overview",
    "text": "Figure 2 gives an overview of DAM, which generally follows the representation-matchingaggregation framework to match response with multi-turn context. For each utterance ui = [wui,k] nui−1 k=0 in a context and its response candidate r = [wr,t]nr−1t=0 , where nui and nr stand for the numbers of words, DAM first looks up a shared word embedding table and represents ui and r as sequences of word embeddings, namely U0i =\n[e0ui,0, ..., e 0 ui,nui−1 ] and R0 = [e0r,0, ..., e0r,nr−1] respectively, where e ∈ Rd denotes a d-dimension word embedding.\nA representation module then starts to construct semantic representations at different granularities for ui and r. Practically, L identical layers of self-attention are hierarchically stacked, each lth self-attention layer takes the output of the l − 1th layer as its input, and composites the input semantic vectors into more sophisticated representations based on self-attention. In this way, multigrained representations of ui and r are gradually constructed, denoted as [Uli]Ll=0 and [R\nl]Ll=0 respectively.\nGiven [U0i , ...,ULi ] and [R0, ...,RL], utterance ui and response r are then matched with each other in a manner of segment-segment similarity matrix. Practically, for each granularity l ∈ [0...L], two kinds of matching matrixes are constructed, i.e., the self-attention-match Mui,r,lself and cross-attention-match Mui,r,lcross , measuring the relevance between utterance and response with textual information and dependency information respectively.\nThose matching scores are finally merged into a 3D matching image Q1. Each dimension of Q represents each utterance in context, each word in utterance and each word in response respectively. Important matching information between segment pairs across multi-turn context and candidate response is then extracted via convolution with max-pooling operations, and further fused into one matching score via a single-layer perceptron, representing the matching degree between the response candidate and the whole context.\nSpecifically, we use a shared component, the Attentive Module, to implement both selfattention in representation and cross-attention in matching. We will discuss in detail the implementation of Attentive Module and how we used it to implement both self-attention and cross-attention in following sections."
  }, {
    "heading": "3.3 Attentive Module",
    "text": "Figure 3 shows the structure of Attentive Module, which is similar to that used in Transformer (Vaswani et al., 2017). Attentive Module has three input sentences: the query sentence, the key sentence and the value sentence, namely Q = [ei] nQ−1 i=0 ,K = [ei] nK−1 i=0 ,V = [ei] nV−1 i=0 respec-\n1We refer to it as Q because it is like a cube.\ntively, where nQ, nK and nV denote the number of words in each sentence and ei stands for a ddimension embedding, nK is equal to nV . The Attentive Module first takes each word in the query sentence to attend to words in the key sentence via Scaled Dot-Product Attention (Vaswani et al., 2017), then applies those attention results upon the value sentence, which is defined as:\nAtt(Q,K) = [ softmax(\nQ[i] · KT√ d ) ]nQ−1 i=0 (1)\nVatt = Att(Q,K) · V ∈ RnQ×d (2)\nwhere Q[i] is the ith embedding in the query sentence Q. Each row of Vatt, denoted as Vatt[i], stores the fused semantic information of words in the value sentence that possibly have dependencies to the ith word in query sentence. For each i, Vatt[i] and Q[i] are then added up together, compositing them into a new representation that contains their joint meanings. A layer normalization operation (Ba et al., 2016) is then applied, which prevents vanishing or exploding of gradients. A feed-forward network FFN with RELU (LeCun et al., 2015) activation is then applied upon the normalization result, in order to further process the fused embeddings, defined as:\nFFN(x) = max(0, xW1 + b1)W2 + b2 (3)\nwhere, x is a 2D-tensor in the same shape of query sentence Q and W1, b1,W2, b2 are learnt parameters. This kind of activation is empirically useful in other works, and we also adapt it in our model. The result FFN(x) is a 2D-tensor that has the same shape as x, FFN(x) is then residually added (He et al., 2016) to x, and the fusion result is then normalized as the final outputs. We refer to the whole Attentive Module as:\nAttentiveModule(Q,K,V) (4)\nAs described, Attentive Module can capture dependencies across query sentence and key sentence, and further use the dependency information to composite elements in the query sentence and the value sentence into compositional representations. We exploit this property of the Attentive Module to construct multi-grained semantic representations as well as match with dependency information."
  }, {
    "heading": "3.4 Representation",
    "text": "Given U0i or R0, the word-level embedding representations for utterance ui or response r, DAM takes U0i ro R0 as inputs and hierarchically stacks the Attentive Module to construct multi-grained representations of ui and r, which is formulated as:\nUl+1i = AttentiveModule(U l i,U l i,U l i) (5) Rl+1 = AttentiveModule(Rl,Rl,Rl) (6)\nwhere l ranges from 0 to L − 1, denoting the different levels of granularity. By this means, words in each utterance or response repeatedly function together to composite more and more holistic representations, we refer to those multi-grained representations as [U0i , ...,ULi ] and [R0, ...,RL] hereafter."
  }, {
    "heading": "3.5 Utterance-Response Matching",
    "text": "Given [Uli]Ll=0 and [R\nl]Ll=0, two kinds of segmentsegment matching matrixes are constructed at each level of granularity l, i.e., the self-attention-match Mui,r,lself and cross-attention-match M ui,r,l cross . M ui,r,l self is defined as:\nMui,r,lself = {U l i[k] T · Rl[t]}nui×nr (7)\nin which, each element in the matrix is the dotproduct of Uli[k] and Rl[t], the kth embedding in Uli and the tth embedding in Rl, reflecting the textual relevance between the kth segment in ui and tth segment in r at the lth granularity. The crossattention-match matrix is based on cross-attention, which is defined as:\nŨ l\ni = AttentiveModule(U l i,R l,Rl) (8)\nR̃ l = AttentiveModule(Rl,Uli,U l i) (9)\nMui,r,lcross = {Ũ l i[k] T · R̃l[t]}nui×nr (10)\nwhere we use Attentive Module to make Uli and Rl crossly attend to each other, constructing two\nnew representations for both of them, written as Ũ l i and R̃ l respectively. Both Ũ l i and R̃ l\nimplicitly capture semantic structures that cross the utterance and response. In this way, those inter-dependent segment pairs are close to each other in representations, and dot-products between those latently inter-dependent pairs could get increased, providing dependency-aware matching information."
  }, {
    "heading": "3.6 Aggregation",
    "text": "DAM finally aggregates all the segmental matching degrees across each utterance and response into a 3D matching image Q, which is defined as:\nQ = {Qi,k,t}n×nui×nr (11)\nwhere each pixel Qi,k,t is formulated as:\nQi,k,t =\n[Mui,r,lself [k, t]] L l=0 ⊕ [Mui,r,lcross [k, t]]Ll=0\n(12)\n⊕ is concatenation operation, and each pixel has 2(L + 1) channels, storing the matching degrees between one certain segment pair at different levels of granularity. DAM then leverages twolayered 3D convolution with max-pooling operations to distill important matching features from the whole image. The operation of 3D convolution with max-pooling is the extension of typical 2D convolution, whose filters and strides are 3D cubes2. We finally compute matching score g(c, r) based on the extracted matching features fmatch(c, r) via a single-layer perceptron, which is formulated as:\ng(c, r) = σ(W3fmatch(c, r) + b3) (13)\nwhere W3 and b3 are learnt parameters, and σ is sigmoid function that gives the probability if r is a proper candidate to c. The loss function of DAM is the negative log likelihood, defined as:\np(y|c, r) = g(c, r)y + (1− g(c, r))(1− y) (14) L(·) = − ∑ (c,r,y)∈D log(p(y|c, r)) (15)"
  }, {
    "heading": "4 Experiment",
    "text": "2https://www.tensorflow.org/api docs/python/tf/nn/conv3d"
  }, {
    "heading": "4.1 Dataset",
    "text": "We test DAM on two public multi-turn response selection datasets, the Ubuntu Corpus V1 (Lowe et al., 2015) and the Douban Conversation Corpus (Wu et al., 2017). The former one contains multiturn dialogues about Ubuntu system troubleshooting in English and the later one is crawled from a Chinese social networking on open-domain topics. The Ubuntu training set contains 0.5 million multiturn contexts, and each context has one positive response that generated by human and one negative response which is randomly sampled. Both validation and testing sets of Ubuntu Corpus have 50k contexts, where each context is provided with one positive response and nine negative replies. The Douban corpus is constructed in a similar way to the Ubuntu Corpus, except that its validation set contains 50k instances with 1:1 positive-negative ratios and the testing set of Douban corpus is consisted of 10k instances, where each context has 10 candidate responses, collected via a tiny invertedindex system (Lucene3), and labels are manually annotated."
  }, {
    "heading": "4.2 Evaluation Metric",
    "text": "We use the same evaluation metrics as in previous works (Wu et al., 2017). Each comparison model is asked to select k best-matched response from n available candidates for the given conversation context c, and we calculate the recall of the true positive replies among the k selected ones as the main evaluation metric, denoted as Rn@k = ∑k i=1 yi∑n i=1 yi\n, where yi is the binary label for each candidate. In addition to Rn@k, we use MAP (Mean Average Precision) (Baeza-\n3https://lucenent.apache.org/\nYates et al., 1999), MRR (Mean Reciprocal Rank) (Voorhees et al., 1999), and Precision-at-one P@1 especially for Douban corpus, following the setting of previous works (Wu et al., 2017)."
  }, {
    "heading": "4.3 Comparison Methods",
    "text": "RNN-based models : Previous best performing\nmodels are based on RNNs, we choose representative models as baselines, including SMNdynamic(Wu et al., 2017), Multiview(Zhou et al., 2016), DualEncoderlstm and DualEncoderbilstm (Lowe et al., 2015), DL2R (Yan et al., 2016), Match-LSTM (Wang and Jiang, 2017) and MV-LSTM (Pang et al., 2016), where SMNdynamic achieves the best scores against all the other published works, and we take it as our stateof-the-art baseline.\nAblation : To verify the effects of multi-grained representation, we setup two comparison models, i.e., DAMfirst and DAMlast, which dispense with the multi-grained representations in DAM, and use representation results from the 0th layer and Lth layer of self-attention instead. Moreover, we setup DAMself and DAMcross, which only use self-attention-match or cross-attention-match respectively, in order to examine the effectiveness of both self-attention-match and cross-attention-match."
  }, {
    "heading": "4.4 Model Training",
    "text": "We copy the reported evaluation results of all baselines for comparison. DAM is implemented in tensorflow4, and the used vocabularies, word em-\n4https://www.tensorflow.org. Our code and data will be available at https://github.com/baidu/Dialogue/DAM\nbedding sizes for Ubuntu corpus and Douban corpus are all set as same as the SMN (Wu et al., 2017). We consider at most 9 turns and 50 words for each utterance (response) in our experiments, word embeddings are pre-trained using training sets via word2vec (Mikolov et al., 2013), similar to previous works. We use zero-pad to handle the variable-sized input and parameters in FFN are set to 200, same as word-embedding size. We test stacking 1-7 self-attention layers, and reported our results with 5 stacks of self-attention because it gains the best scores on validation set. The 1st convolution layer has 32 [3,3,3] filters with [1,1,1] stride, and its max-pooling size is [3,3,3] with [3,3,3] stride. The 2nd convolution layer has 16 [3,3,3] filters with [1,1,1] stride, and its maxpooling size is also [3,3,3] with [3,3,3] stride. We tune DAM and the other ablation models with adam optimizer (Le et al., 2011) to minimize loss function defined in Eq 15. Learning rate is initialized as 1e-3 and gradually decreased during training, and the batch-size is 256. We use validation sets to select the best models and report their performances on test sets."
  }, {
    "heading": "4.5 Experiment Result",
    "text": "Table 1 shows the evaluation results of DAM as well as all comparison models. As demonstrated, DAM significantly outperforms other competitors on both Ubuntu Corpus and Douban Conversation Corpus, including SMNdynamic, which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context. Besides, both the performances of DAMfirst and DAMself decrease a lot compared with DAM, which shows the effectiveness of self-attention and cross-attention. Both DAMfirst and DAMlast underperform DAM, which demonstrates the benefits of using multigrained representations. Also the absence of self-attention-match brings down the precision, as shown in DAMcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection.\nOne notable point is that, while DAMfirst is able to achieve close performance to SMNdynamic, it is about 2.3 times faster than SMNdynamic in our implementation as it is very simple in computation. We believe that DAMfirst is more suitable to the scenario that has limitations in computation time or memories but requires high precise, such\nas industry application or working as an component in other neural networks like GANs."
  }, {
    "heading": "5 Analysis",
    "text": "We use the Ubuntu Corpus for analyzing how selfattention and cross-attention work in DAM from both quantity analysis as well as visualization."
  }, {
    "heading": "5.1 Quantity Analysis",
    "text": "We first study how DAM performs in different utterance number of context. The left part in Figure 4 shows the changes of R10@1 on Ubuntu Corpus across contexts with different number of utterance. As demonstrated, while being good at matching response with long context that has more than 4 utterances, DAM can still stably deal with short context that only has 2 turns.\nMoreover, the right part of Figure 4 gives the comparison of performance across different contexts with different average utterance text length and self-attention stack depth. As demonstrated, stacking self-attention can consistently improve matching performance for contexts having different average utterance text length, implying the stability advantage of using multi-grained semantic representations. The performance of matching short utterances, that have less than 10 words, is obviously lower than the other longer ones. This is because the shorter the utterance text is, the fewer information it contains, and the more difficult for selecting the next utterance, while stacking self-attention can still help in this case. However for long utterances like containing more than 30 words, stacking self-attention can significantly improve the matching performance, which means that the more information an utterance contains, the more stacked self-attention it needs to capture its intra semantic structures.\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se\nturn 0\nre sp\non se\nself−attention−match in stack 0\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se\nturn 0\nre sp\non se\nself−attention−match in stack 2\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se\nturn 0\nre sp\non se\nself−attention−match in stack 4\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se\nturn 0\nre sp\non se\ncross−attention−match in stack 4\nno clue what\ndo you\nneed it\nfor its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nno clu e wh at do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\nre sp\non se\nself−attention of response in stack 3\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nsystem i.1\ndont see.1\na path\nis the list\nbeing held\nsomewhere else\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se\n. turn 0\ntu rn\n0 self−attention of turn 0 in stack 3\nno clue what\ndo you\nneed it\nfor its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew he re el se . turn 0 re\nsp on\nse\nattention of response over turn 0 in stack 4\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nsystem i.1\ndont see.1\na path\nis the list\nbeing held\nsomewhere else\nno clu e wh at do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\ntu rn\n0\nattention of turn 0 over response in stack 4\nself-attention cross-attention\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior−match in stack 0\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior−match in stack 2\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior−match in stac 4\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nposterior−match in stack 4\nno clue what\ndo you\nneed it\nfor its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nno cl ue w ha t do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\nre sp\non se\nself−attention of response in stack 3\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nsystem dont\na path\nis the list\nbeing held\nsomewhere else\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\n.\nturn 0\ntu rn\n0\nself−attention of turn 0 in stack 3\nno clue what\ndo you\nneed it\nfor its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\n.\nturn 0\nre\nsp\non se\nattention of response over turn 0 in stack 4\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nsystem dont\na path\nis the list\nbeing held\nsomewhere else\nno cl ue w ha t do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\ntu rn\n0\nattention of turn 0 over response in stack 4\nprior-match posterior-match\nself-attention cross-attention\nno clue hat do\nyou need\nit for. its\njust reassurance\nas i\ndont kno\nthe debain\npackage anager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior atch in stack 0\nno clue hat do\nyou need\nit for. its\njust reassurance\nas i\ndont kno\nthe debain\npackage anager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior atch in stack 2\nno clue hat do\nyou need\nit for. its\njust reassurance\nas i\ndont kno\nthe debain\npackage anager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nprior−match in stac 4\nno clue what\ndo you\nneed it\nfor. its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em do nt a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\nturn 0\nre sp\non se\nposterior−match in stack 4\nno clue what\ndo you\nneed it\nfor its\njust reassurance\nas i\ndont know\nthe debain\npackage manager\nno cl ue w ha t do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\nre sp\non se\nself−attention of espo e in stack 3\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nystem dont\na path\nis the list\nbeing held\nsomewhere else\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\n.\nt\ntu rn\n0\nself−atte tion of turn 0 in stack 3\nno clue what\ndo you\nneed it\nfor its\njust rea surance\nas i\ndont know\nthe debain\npackage manager\nhi i am lo ok in\ng to se e w ha t\npa ck\nag es\nar e\nin st\nal le d on m\ny sy\nst em i.1 do nt se e. 1 a pa th is th e lis t be in g he ld so m ew\nhe re\nel se\n.\nturn 0\nre\nsp\non se\nattention of esponse over turn 0 in stack 4\nhi i\nam looking\nto see\nwhat packages\nare installed\non my\nsystem dont\na path\nis the list\nbeing held\nsomewhere else\nno cl ue w ha t do yo u ne ed it fo r. its ju st\nre as\nsu ra\nnc e\nas i do nt kn ow th e de ba in pa ck ag e m an ag\ner\nresponse\ntu rn\n0\nattention of turn 0 over response in stack 4\nprior-match posterior-match\nself-attention cross-attention\ns l - tention-match in stack 0 lf-a tention-match in stack 2 self-attention-match in stack 4 cross-attention-match in stack 4 self-attention-match cross-attention-match\nFigure 5: Visualization of self-attention-match, cross-attention-match as well as the distribution of self-attention and crossattention in matching response with the first utterance in Figure 1. Each c lored grid represents the matching degree or attention score between two words. The deeper the color is, the more important this grid is."
  }, {
    "heading": "5.2 Visualization",
    "text": "We study the case in Figure 1 for analyzing in detail how self-attention and cross-attention work. Practically, we apply a softmax operation over self-attention-match and cross-attention-match, to examine the variance of dominating matching pairs during stacking self-attention or applying cross-attention. Figure 5 gives the visualization results of the 0th, 2nd and 4th self-attention-match matrixes, the 4th cross-attention-match matrix, as well as the distribution of self-attention and crossattention in the 4th layer in matching response with the first utterance (turn 0) due to space limitation. As demonstrated, important matching pairs in selfattention-match in stack 0 are nouns, verbs, like “package” and “packages”, those are similar in topics. However matching scores between prepositions or pronouns pairs, such as “do” and “what”, become more important in self-attention-match in stack 4. The visualization results of self-attention show the reason why matching between prepositions or pronouns matters, as demonstrated, selfattention generally capture the semantic structure of “no clue what do you need package manager” for “do” in response and “what packages are installed” for “what” in utterance, making segments surrounding “do” and “what” close to each other in representations, thus increases their dot-product results.\nAlso as shown in Figure 5, self-attentionmatch and cros -attention-match capture complementary information in matching utterance with response. Words like “reassurance” and “its” in response significantly get larger matching scores in cross-attention-match compared with self-attention-match. According to the visualization of cross-attention, “reassurance” generally depends on “system” “don’t” and “held” in utterance, which makes it close to words like “list”, “installed” or “held” of utterance. Scores of crossattention-match trend to centralize on several segments, which probably means that those segments in response generally capture structure-semantic information across utterance and response, amplifying their matching scores against the others."
  }, {
    "heading": "5.3 Error Analysis",
    "text": "To understand the limitations of DAM and where the future improvements might lie, we analyze 100 strong bad cases from test-set that fail in R10@5. We find two major kinds of bad cases: (1) fuzzycandidate, where response candidates are basically proper for the conversation context, except for a few improper details. (2) logical-error, where response candidates are wrong due to logical mismatch, for example, given a conversation context A: “I just want to stay at home tomorrow.”, B: “Why not go hiking? I can go with\nyou.”, response candidate like “Sure, I was planning to go out tomorrow.” is logically wrong because it is contradictory to the first utterance of speaker A. We believe generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution extends the attention mechanism of Transformer in two ways: (1) using stacked selfattention to harvest multi-grained semantic representations. (2) utilizing cross-attention to match with dependency information. Empirical results on two large-scale datasets demonstrate the effectiveness of self-attention and cross-attention in multi-turn response selection. We believe that both self-attention and cross-attention could benefit other research area, including spoken language understanding, dialogue state tracking or seq2seq dialogue generation. We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work."
  }, {
    "heading": "Acknowledgement",
    "text": "We gratefully thank the anonymous reviewers for their insightful comments. This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505)."
  }],
  "year": 2018,
  "references": [{
    "title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles",
    "authors": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."],
    "venue": "International Conference on Intelligent Virtual Agents, pages 13–21. Springer.",
    "year": 2014
  }, {
    "title": "Layer normalization",
    "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton."],
    "venue": "arXiv preprint arXiv:1607.06450.",
    "year": 2016
  }, {
    "title": "Modern information retrieval, volume 463",
    "authors": ["Ricardo Baeza-Yates", "Berthier Ribeiro-Neto"],
    "year": 1999
  }, {
    "title": "Neural machine translation by jointly",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"],
    "year": 2015
  }, {
    "title": "Iris: a chatoriented dialogue system based on the vector space model",
    "authors": ["Rafael E Banchs", "Haizhou Li."],
    "venue": "Proceedings of the ACL 2012 System Demonstrations, pages 37–42. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
    "year": 2016
  }, {
    "title": "An information retrieval approach to short text conversation",
    "authors": ["Zongcheng Ji", "Zhengdong Lu", "Hang Li."],
    "venue": "arXiv preprint arXiv:1408.6988.",
    "year": 2014
  }, {
    "title": "On optimization methods for deep learning",
    "authors": ["Quoc V Le", "Jiquan Ngiam", "Adam Coates", "Abhik Lahiri", "Bobby Prochnow", "Andrew Y Ng."],
    "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, pages",
    "year": 2011
  }, {
    "title": "Deep learning",
    "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."],
    "venue": "Nature, 521(7553):436–444.",
    "year": 2015
  }, {
    "title": "Complexity of dependencies in discourse: Are dependencies in discourse more complex than in syntax",
    "authors": ["Alan Lee", "Rashmi Prasad", "Aravind Joshi", "Nikhil Dinesh", "Bonnie Webber."],
    "venue": "Proceedings of the 5th International Workshop on Treebanks",
    "year": 2006
  }, {
    "title": "Adversarial learning for neural dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."],
    "venue": "EMNLP, pages 372–381.",
    "year": 2017
  }, {
    "title": "A structured self-attentive sentence embedding",
    "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "international conference on learning representations.",
    "year": 2017
  }, {
    "title": "Towards an automatic turing test: Learning to evaluate dialogue responses",
    "authors": ["Ryan Lowe", "Michael Noseworthy", "Iulian V Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."],
    "venue": "ACL, pages 372–381.",
    "year": 2017
  }, {
    "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Vlad Serban", "Joelle Pineau."],
    "venue": "annual meeting of the special interest group on discourse and dialogue, pages",
    "year": 2015
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Developing non-goal dialog system based on examples of drama television",
    "authors": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."],
    "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355–",
    "year": 2014
  }, {
    "title": "Text matching as image recognition",
    "authors": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng."],
    "venue": "AAAI, pages 2793–2799.",
    "year": 2016
  }, {
    "title": "Data-driven response generation in social media",
    "authors": ["Alan Ritter", "Colin Cherry", "William B Dolan."],
    "venue": "In Proc. EMNLP, pages 583–593. Association for Computational Linguistics.",
    "year": 2011
  }, {
    "title": "Dialogue act modeling for automatic tagging and recognition",
    "authors": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer"],
    "year": 2000
  }, {
    "title": "Utterance units in spoken dialogue",
    "authors": ["David R Traum", "Peter A Heeman."],
    "venue": "Workshop on Dialogue Processing in Spoken Language Systems, pages 125–140.",
    "year": 1996
  }, {
    "title": "Computing machinery and intelligence",
    "authors": ["Alan M Turing."],
    "venue": "Mind, 59(236):433–460.",
    "year": 1950
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems, pages 6000–6010.",
    "year": 2017
  }, {
    "title": "The trec-8 question answering track report",
    "authors": ["Ellen M Voorhees"],
    "venue": "Trec, pages 77–82.",
    "year": 1999
  }, {
    "title": "A dataset for research on short-text conversations",
    "authors": ["Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen."],
    "venue": "EMNLP, pages 935–945.",
    "year": 2013
  }, {
    "title": "Syntax-based deep matching of short texts",
    "authors": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."],
    "venue": "International Joint Conferences on Artificial Intelligence.",
    "year": 2015
  }, {
    "title": "Machine comprehension using match-lstm and answer pointer",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "international conference on learning representations.",
    "year": 2017
  }, {
    "title": "Sequential match network: A new architecture for multi-turn response selection in retrieval-based chatbots",
    "authors": ["Yu Wu", "Wei Wu", "Ming Zhou", "Zhoujun Li."],
    "venue": "ACL, pages 372–381.",
    "year": 2017
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "International Conference on Machine",
    "year": 2015
  }, {
    "title": "Learning to respond with deep neural networks for retrievalbased human-computer conversation system",
    "authors": ["Rui Yan", "Yiping Song", "Hua Wu."],
    "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Infor-",
    "year": 2016
  }, {
    "title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs",
    "authors": ["Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou."],
    "venue": "Transactions of the Association of Computational Linguistics, 4(1):259–272.",
    "year": 2016
  }, {
    "title": "Multi-view response selection for human-computer conversation",
    "authors": ["Xiangyang Zhou", "Daxiang Dong", "Hua Wu", "Shiqi Zhao", "Dianhai Yu", "Hao Tian", "Xuan Liu", "Rui Yan."],
    "venue": "EMNLP, pages 372–381.",
    "year": 2016
  }],
  "id": "SP:50e96030bcab00028a7a772bd5137b8259e20bfc",
  "authors": [{
    "name": "Xiangyang Zhou",
    "affiliations": []
  }, {
    "name": "Lu Li",
    "affiliations": []
  }, {
    "name": "Daxiang Dong",
    "affiliations": []
  }, {
    "name": "Yi Liu",
    "affiliations": []
  }, {
    "name": "Ying Chen",
    "affiliations": []
  }, {
    "name": "Wayne Xin Zhao",
    "affiliations": []
  }, {
    "name": "Dianhai Yu",
    "affiliations": []
  }, {
    "name": "Hua Wu",
    "affiliations": []
  }],
  "abstractText": "Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.",
  "title": "Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network"
}