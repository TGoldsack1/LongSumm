{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Support vector machines (SVMs) and Boosting have been two mainstream learning approaches during the past decade. The former (Cortes & Vapnik, 1995) roots in the statistical learning theory (Vapnik, 1995) with the central idea of searching a large margin separator, i.e., maximizing the smallest distance from the instances to the classification boundary in a RKHS (reproducing kernel Hilbert space). It is noteworthy that there is also a long history of applying margin theory to explain the latter (Freund & Schapire, 1995; Schapire et al., 1998), due to its tending to be empirically resistant to over-fitting (Reyzin & Schapire, 2006; Wang et al., 2011; Zhou, 2012).\n1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nRecently, the margin theory for Boosting has finally been defended (Gao & Zhou, 2013), and has disclosed that the margin distribution rather than a single margin is more crucial to the generalization performance. It suggests that there may still exist large space to further enhance for SVMs. Inspired by this recognition, (Zhang & Zhou, 2014; 2016) proposed a binary classification method to optimize margin distribution by characterizing it through the firstand second-order statistics, which achieves quite satisfactory experimental results. Later, (Zhou & Zhou, 2016) extends the idea to an approach which is able to exploit unlabeled data and handle unequal misclassification cost. A brief summary of this line of early research can be found in (Zhou, 2014).\nAlthough it has been shown that for binary classification, optimizing the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously can get superior performance, it still remains open for multi-class classification. Moreover, the margin for multiclass classification is much more complicated than that for binary class classification, which makes the resultant optimization be a difficult non-differentiable non-convex programming. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine) to solve this problem efficiently. For optimization, we relax mcODM into a series of convex quadratic programming (QP), and extend the Block Coordinate Descent (BCD) algorithm (Tseng, 2001) to solve the dual of each QP. The sub-problem of each iteration of BCD is also a QP. By exploiting its special structure, we derive a sorting algorithm to solve it which is much faster than general QP solvers. We further provide a generalization error bound based on Rademacher complexity, and further present the analysis of the relationship between generalization error and margin distribution for multi-class classification. Extensive experiments on twenty two data sets show the superiority of our method to all four versions of multi-class SVMs.\nThe rest of this paper is organized as follows. Section 2 introduces some preliminaries. Section 3 formulates the problem. Section 4 presents the proposed algorithm. Section 5 discusses some theoretical analyses. Section 6 reports on our experimental studies and empirical observations. Finally Section 7 concludes with future work."
  }, {
    "heading": "2. Preliminaries",
    "text": "We denote by X ∈ Rd the instance space and Y = [k] the label set, where [k] = {1, . . . , k}. Let D be an unknown (underlying) distribution over X × Y . A training set S = {(x1, y1), (x2, y2), . . . , (xm, ym)} ∈ (X × Y)m is drawn identically and independently (i.i.d.) according to distribution D. Let ϕ : X 7→ H be a feature mapping associated to some positive definite kernel κ. For multi-class classification setting, the hypothesis is defined based on k weight vectors w1, . . . ,wk ∈ H, where each weight vector wl, l ∈ Y defines a scoring function x 7→ w⊤l ϕ(x) and the label of instance x is the one resulting in the largest score, i.e., h(x) = argmaxl∈Y w ⊤ l ϕ(x). This decision function naturally leads to the following definition of the margin for a labeled instance (x, y):\nγh(x, y) = w ⊤ y ϕ(x)−max l ̸=y w⊤l ϕ(x).\nThus h misclassifies (x, y) if and only if it produces a negative margin for this instance.\nGiven a hypothesis set H of functions mapping X to Y and the labeled training set S, our goal is to learn a function h ∈ H such that the generalization error R(h) = E(x,y)∼D[1h(x)̸=y] is small."
  }, {
    "heading": "3. Formulation",
    "text": "To design optimal margin distribution machine for multiclass classification, we need to understand how to optimize the margin distribution. (Gao & Zhou, 2013) proved that, to characterize the margin distribution, it is important to consider not only the margin mean but also the margin variance. Inspired by this idea, (Zhang & Zhou, 2014; 2016) proposed the optimal margin distribution machine for binary classification, which characterizes the margin distribution according to the first- and second-order statistics, that is, maximizing the margin mean and minimizing the margin variance simultaneously. Specifically, let γ̄ denote the margin mean, and the optimal margin distribution machine can be formulated as:\nmin w,γ̄,ξi,ϵi Ω(w)− ηγ̄ + λ m m∑ i=1 (ξ2i + ϵ 2 i )\ns.t. γh(xi, yi) ≥ γ̄ − ξi, γh(xi, yi) ≤ γ̄ + ϵi, ∀i,\nwhere Ω(w) is the regularization term to penalize the model complexity, η and λ are trading-off parameters, ξi and ϵi are the deviation of the margin γh(xi, yi) to the margin mean. It’s evident that ∑m i=1(ξ 2 i + ϵ 2 i )/m is exactly the margin variance.\nBy scaling w which doesn’t affect the final classification results, the margin mean can be fixed as 1, then the de-\nviation of the margin of (xi, yi) to the margin mean is |γh(xi, yi) − 1|, and the optimal margin distribution machine can be reformulated as\nmin w,ξi,ϵi\nΩ(w) + λ\nm m∑ i=1 ξ2i + µϵ 2 i (1− θ)2\ns.t. γh(xi, yi) ≥ 1− θ − ξi, γh(xi, yi) ≤ 1 + θ + ϵi, ∀i.\nwhere µ ∈ (0, 1] is a parameter to trade off two different kinds of deviation (larger or less than margin mean). θ ∈ [0, 1) is a parameter of the zero loss band, which can control the number of support vectors, i.e., the sparsity of the solution, and (1− θ)2 in the denominator is to scale the second term to be a surrogate loss for 0-1 loss.\nFor multi-class classification, let the regularization term Ω(w) = ∑k l=1 ∥wl∥2H/2 and combine with the definition of margin, and we arrive at the formulation of mcODM,\nmin wl,ξi,ϵi\n1\n2 k∑ l=1 ∥wl∥2H + λ m m∑ i=1 ξ2i + µϵ 2 i (1− θ)2 (1)\ns.t. w⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≥ 1− θ − ξi,\nw⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≤ 1 + θ + ϵi, ∀i.\nwhere λ, µ and θ are the parameters for trading-off described previously."
  }, {
    "heading": "4. Optimization",
    "text": "Due to the max operator in the second constraint, mcODM is a non-differentiable non-convex programming, which is quite difficult to solve directly.\nIn this section, we first relax mcODM into a series of convex quadratic programming (QP), which can be much easier to handle. Specifically, at each iteration, we recast the first constraint as k − 1 linear inequality constraints:\nw⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, l ̸= yi,\nand replace the second constraint with\nw⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi,\nwhere Mi = maxl ̸=yi w̄ ⊤ l ϕ(xi) and w̄l is the solution to the previous iteration. Then we can repeatedly solve the following convex QP problem until convergence:\nmin wl,ξi,ϵi\n1\n2 k∑ l=1 ∥wl∥2H + λ m m∑ i=1 ξ2i + µϵ 2 i (1− θ)2 (2)\ns.t. w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, ∀l ̸= yi,\nw⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi, ∀i.\nIntroduce the lagrange multipliers ζli ≥ 0, l ̸= yi for the first k − 1 constraints and βi ≥ 0 for the last constraint respectively, the Lagrangian function of Eq. 2 leads to\nL(wl, ξi, ϵi, ζ l i , βi)\n= 1\n2 k∑ l=1 ∥wl∥2H + λ m m∑ i=1 ξ2i + µϵ 2 i (1− θ)2\n− m∑ i=1 ∑ l ̸=yi ζli(w ⊤ yiϕ(xi)−w ⊤ l ϕ(xi)− 1 + θ + ξi)\n+ m∑ i=1 βi(w ⊤ yiϕ(xi)−Mi − 1− θ − ϵi),\nBy setting the partial derivations of variables {wl, ξi, ϵi} to zero, we have\nwl = m∑ i=1 δyi,l ∑ s̸=yi ζsi − (1− δyi,l)ζli − δyi,lβi ϕ(xi), ξi = m(1− θ)2\n2λ\n∑ l ̸=yi ζli , ϵi = m(1− θ)2 2λµ βi. (3)\nwhere δyi,l equals 1 when yi = l and 0 otherwise. We further simplify the expression of wl as\nwl = m∑ i=1 (αli − δyi,lβi)ϕ(xi), (4)\nby defining αli ≡ −ζli for ∀l ̸= yi and α yi i ≡ ∑ s̸=yi\nζsi and substituting Eq. 4 and Eq. 3 into the Lagrangian function, then we have the following dual problem\nmin αli,α yi i ,βi\n1\n2 k∑ l=1 ∥wl∥2H + m(1− θ)2 4λ m∑ i=1 (αyii ) 2\n+ m(1− θ)2\n4λµ\nm∑ i=1 β2i + (1− θ) m∑ i=1 ∑ l ̸=yi αli\n+ (Mi + 1 + θ) m∑ i=1 βi (5)\ns.t. k∑\nl=1\nαli = 0, ∀i,\nαli ≤ 0, ∀i,∀l ̸= yi, βi ≥ 0, ∀i.\nThe objective function in Eq. 5 involves m(k + 1) variables in total, so it is not easy to optimize with respect to all the variables simultaneously. Note that all the constraints can be partitioned into m disjoint sets, and the i-th set only involves α1i , . . . , α k i , βi, so the variables can be divided into m decoupled groups and an efficient block coordinate descent algorithm (Tseng, 2001) can be applied.\nSpecifically, we sequentially select a group of k + 1 variables α1i , . . . , α k i , βi associated with instance xi to minimize, while keeping other variables as constants, and repeat this procedure until convergence.\nAlgorithm 1 below details the kenrel mcODM.\nAlgorithm 1 Kenrel mcODM 1: Input: Data set S. 2: Initialize α⊤ = [α11, . . . , α k 1 , . . . , α 1 m, . . . , α k m] and\nβ⊤ = [β1, . . . , βm] as zero vector. 3: while α and β not converge do 4: for i = 1, . . . ,m do 5: Mi ← maxl ̸=yi ∑m j=1(α l j − δyj ,lβj)κ(xj ,xi). 6: end for 7: Solve Eq. 5 by block coordinate descent method. 8: end while 9: Output: α, β."
  }, {
    "heading": "4.1. Solving the sub-problem",
    "text": "The sub-problem in step 7 of Algorithm 1 is also a convex QP with k + 1 variables, which can be accomplished by some standard QP solvers. However, by exploiting its special structure, i.e., only a small quantity of cross terms are involved, we can derive an algorithm to solve this subproblem just by sorting, which can be much faster than general QP solvers.\nNote that all variables except α1i , . . . , α k i , βi are fixed, so we have the following sub-problem:\nmin αli,α yi i ,βi ∑ l ̸=yi A 2 (αli) 2 + ∑ l ̸=yi Blα l i + D 2 (αyii ) 2 −Aαyii βi\n+Byiα yi i +\nE 2 β2i + Fβi\ns.t. k∑\nl=1\nαli = 0, (6)\nαli ≤ 0, ∀l ̸= yi, βi ≥ 0.\nwhere A = κ(xi,xi), Bl = ∑ j ̸=i κ(xi,xj)(α l j −\nδyj ,lβj)+1− θ for ∀l ̸= yi, Byi = ∑ j ̸=i κ(xi,xj)(α yi j − δyj ,yiβj), D = A + m(1−θ)2 2λ , E = A + m(1−θ)2\n2λµ and F ≡Mi + 1 + θ −Byi .\nThe KKT conditions of Eq. 6 indicate that there are scalars ν, ρl and η such that\nk∑ l=1 αli = 0, (7) αli ≤ 0, ∀l ̸= yi, (8)\nβi ≥ 0, (9) ρlα l i = 0, ρl ≥ 0, ∀l ̸= yi, (10)\nAαli +Bl − ν + ρl = 0, ∀l ̸= yi, (11) ηβi = 0, η ≥ 0, (12) −Aαyii + Eβi + F − η = 0, (13) Dαyii −Aβi +Byi − ν = 0. (14)\nAccording to Eq. 8, Eq. 10 and Eq. 11 are equivalent to\nAαli +Bl − ν = 0, if αli < 0, ∀l ̸= yi, (15) Bl − ν ≤ 0, if αli = 0, ∀l ̸= yi. (16)\nIn the same way, Eq. 12 and Eq. 13 are equivalent to\n−Aαyii + Eβi + F = 0, if βi > 0, (17) −Aαyii + F ≥ 0, if βi = 0. (18)\nThus KKT conditions turn to Eq. 7 - Eq. 9 and Eq. 14 - Eq. 18. Note that\nαli ≡ min ( 0,\nν −Bl A\n) , ∀l ̸= yi, (19)\nsatisfies KKT conditions Eq. 8 and Eq. 15 - Eq. 16 and βi ≡ max ( 0,\nAαyii − F E\n) , (20)\nsatisfies KKT conditions Eq. 9 and Eq. 17 - Eq. 18. By substituting Eq. 20 into Eq. 14, we obtain\nDαyii +Byi − ν = max ( 0, A\nE (Aαyii − F )\n) . (21)\nLet’s consider the following two cases in turn.\nCase 1: Aαyii ≤ F , according to Eq. 20 and Eq. 21, we have βi = 0 and α yi i = ν−Byi D . Thus, A ν−Byi D ≤ F , which implies that ν ≤ Byi + DFA . Case 2: Aαyii > F , according to Eq. 20 and Eq. 21, we have βi = Aα yi i −F E > 0 and α yi i = Eν−AF−EByi DE−A2 . Thus, A Eν−AF−EByi\nDE−A2 > F , which implies that ν > Byi + DF A .\nThe remaining task is to find ν such that Eq. 7 holds. With Eq. 7 and Eq. 19, it can be shown that\nν =\nAByi D + ∑ l:αli<0\nBl A D + |{l|α l i < 0}| , Case 1, (22)\nν =\nAEByi+A 2F DE−A2 + ∑ l:αli<0 Bl\nAE DE−A2 + |{l|α l i < 0}|\n, Case 2. (23)\nIn both cases, the optimal ν takes the form of (P +∑ l:αli<0 Bl)/(Q+ |{l|αli < 0}|), where P and Q are some\nconstants. (Fan et al., 2008) showed that it can be found by sorting {Bl} for ∀l ̸= yi in decreasing order and then sequentially adding them into an empty set Φ, until\nν∗ = P +\n∑ l∈Φ Bl\nQ+ |Φ| ≥ max l ̸∈Φ Bl. (24)\nNote that the Hessian matrix of the objective function of Eq. 6 is positive definite, which guarantees the existence and uniqueness of the optimal solution, so only one of Eq. 22 and Eq. 23 can hold. We can first compute ν∗ according to Eq. 24 for Case 1, and then check whether the constraint of ν is satisfied. If not, we further compute ν∗ for Case 2. Algorithm 2 summarizes the pseudo-code for solving the sub-problem.\nAlgorithm 2 Solving the sub-problem 1: Input: Parameters A, B = {B1, . . . , Bk}, D,E, F . 2: Initialize B̂ ← B, then swap B̂1 and B̂yi , and sort\nB̂\\{B̂1} in decreasing order. 3: i← 2, ν ← AByi/D. 4: while i ≤ k and ν/(i− 2 +A/D) < B̂i do 5: ν ← ν + B̂i. 6: i← i+ 1. 7: end while 8: if ν ≤ Byi +DF/A then 9: αli ← min(0, (ν −Bl)/A), l ̸= yi.\n10: αyii ← (ν −Byi)/D. 11: βi ← 0. 12: else 13: i← 2, ν ← (AEB̂1 +A2F )/(DE −A2). 14: while i ≤ k and ν/(i− 2+AE/(DE−A2)) < B̂i do 15: ν ← ν + B̂i. 16: i← i+ 1. 17: end while 18: αli ← min(0, (ν −Bl)/A), l ̸= yi. 19: αyii ← (Eν −AF − EByi)/(DE −A2). 20: βi ← (Aαyii − F )/E. 21: end if 22: Output: α1i , . . . , αki , βi."
  }, {
    "heading": "4.2. Speedup for linear kernel",
    "text": "In section 4.1, the proposed method can efficiently deal with kernel mcODM. However, the computation of Mi in step 5 of Algorithm 1 and the computation of parameters B̄l in Algorithm 2 both involve the kernel matrix, whose inherent computational cost takes O(m2) time, so it might be computational prohibitive for large scale problems.\nWhen linear kernel is used, these problems can be alleviated. According to Eq. 4, w is spanned by the training instance so it lies in a finite dimensional space under this\ncircumstance. By storing w1, . . . ,wk explicitly, the computational cost of Mi = maxl ̸=yi w ⊤ l xi can be much less. Moreover, note that B̄l = ∑ j ̸=i x ⊤ i xj(α\nl j − δyj ,lβj) =∑m\nj=1 x ⊤ i xj(ᾱ l j−δyj ,lβ̄j)−x⊤i xi(ᾱli−δyi,lβ̄i) = w⊤l xi−\nA(αli − δyi,lβi), so B̄l can also be computed efficiently."
  }, {
    "heading": "5. Analysis",
    "text": "In this section, we study the statistical property of mcODM. To present the generalization bound of mcODM, we need to introduce the following loss function Φ,\nΦ(z) = 1z≤0 + (z − 1 + θ)2\n(1− θ)2 10<z≤1−θ,\nγh,θ(x, y) = w ⊤ y ϕ(x)−max l∈Y {w⊤l ϕ(x)− (1− θ)1l=y},\nwhere 1(·) is the indicator function that returns 1 when the argument holds, and 0 otherwise. As can be seen, γh,θ(x, y) is a lower bound of γh(x, y) and Φ(γh,θ(x, y)) = Φ(γh(x, y)).\nTheorem 1. Let H = {(x, y) ∈ X × [k] 7→ w⊤y ϕ(x)| ∑k l=1 ∥wl∥2H ≤ Λ2} be the hypothesis space of mcODM, where ϕ : X 7→ H is a feature mapping induced by some positive definite kernel κ. Assume that S ⊆ {x : κ(x,x) ≤ r2}, then for any δ > 0, with probability at least 1 − δ, the following generalization bound holds for any h ∈ H ,\nR(h) ≤ 1 m m∑ i=1 Φ(γh(xi, yi)) + 16rΛ 1− θ\n√ 2πk\nm + 3 √ ln 2δ 2m .\nProof. Let H̃θ be the family of hypotheses mapping X × Y 7→ R defined by H̃θ = {(x, y) 7→ γh,θ(x, y) : h ∈ H}, with McDiarmid inequality (McDiarmid, 1989), yields the following inequality with probability at least 1− δ,\nE[Φ(γh,θ(x, y))] ≤ 1\nm m∑ i=1 Φ(γh,θ(xi, yi))\n+ 2RS(Φ ◦ H̃θ) + 3 √ ln 2δ 2m ,∀h ∈ H̃θ.\nNote that Φ(γh,θ) = Φ(γh), R(h) = E[1γh(x,y)≤0] ≤ E[1γh,θ(x,y)≤0] ≤ E[Φ(γh,θ(x, y))] and Φ(z) is 21−θ - Lipschitz function, by using Talagrand’s lemma (Mohri et al., 2012), we have\nR(h) ≤ 1 m m∑ i=1 Φ(γh(xi, yi)) + 4RS(H̃θ) 1− θ + 3 √ ln 2δ 2m .\nAccording to Theorem 7 of (Lei et al., 2015), we have RS(H̃θ) ≤ 4rΛ √ 2πk/m and proves the stated result.\nTheorem 1 shows that we can get a tighter generalization bound for smaller rΛ and smaller θ. Since γ ≤ 2rΛ, so the former can be viewed as an upper bound of the margin. Besides, 1 − θ is the lower bound of the zero loss band of mcODM. This verifies that better margin distribution (i.e., larger margin mean and smaller margin variance) can yield better generalization performance, which is also consistent with the work of (Gao & Zhou, 2013)."
  }, {
    "heading": "6. Empirical Study",
    "text": "In this section, we empirically evaluate the effectiveness of our method on a broad range of data sets. We first introduce the experimental settings and compared methods in Section 6.1, and then in Section 6.2, we compare our method with four versions of multi-class SVMs, i.e., mcSVM (Weston & Watkins, 1999; Crammer & Singer, 2001; 2002), one-versus-all SVM (ovaSVM), one-versusone SVM (ovoSVM) (Ulrich, 1998) and error-correcting output code SVM (ecocSVM) (Dietterich & Bakiri, 1995). In addition, we also study the influence of the number of classes on generalization performance and margin distribution in Section 6.3. Finally, the computational cost is presented in Section 6.4."
  }, {
    "heading": "6.1. Experimental Setup",
    "text": "We evaluate the effectiveness of our proposed methods on twenty two data sets. Table 1 summarizes the statistics of these data sets. The data set size ranges from 150 to more than 581,012, and the dimensionality ranges from 4 to more than 62,061. Moreover, the number of class ranges from 3 to 1,000, so these data sets cover a broad range of properties. All features are normalized into the interval [0, 1]. For each data set, eighty percent of the instances are randomly selected as training data, and the rest are used as testing data. For each data set, experiments are repeated for 10 times with random data partitions, and the average accuracies as well as the standard deviations are recorded.\nmcODM is compared with four versions of multi-class SVMs, i.e., ovaSVM, ovoSVM, ecocSVM and mcSVM. These four methods can be roughly classified into two groups. The first group includes the first three methods by converting the multi-class classification problem into a set of binary classification problems. Specially, ovaSVM consists of learning k scoring functions hl : X 7→ {−1,+1}, l ∈ Y , each seeking to discriminate one class l ∈ Y from all the others, as can be seen it need train k SVM models. Alternatively, ovoSVM determines the scoring functions for all the combinations of class pairs, so it need train k(k − 1)/2 SVM models. Finally, ecocSVM is a generalization of the former two methods. This technique assigns to each class l ∈ Y a code word with length c, which serves as a signature for this class. After training\nc binary SVM models h1(·), . . . , hc(·), the class predicted for each testing instance is the one whose signatures is the closest to [h1(x), . . . , hc(x)] in Hamming distance. The weakness of these methods is that they may produce unclassifiable regions and their computational costs are usually quite large in practice, which can be observed in the following experiments. On the other hand, mcSVM belongs to the second group. It directly determines all the scroing functions at once, so the time cost is usually less than the former methods. In addition, the unclassifiable regions are also resolved.\nFor all the methods, the regularization parameter λ for mcODM or C for binary SVM and mcSVM is selected by 5-fold cross validation from [20, 22, . . . , 220]. For mcODM, the regularization parameters µ and θ are both selected from [0.2, 0.4, 0.6, 0.8]. For ecocSVM, the exhaustive codes strategy is employed, i.e., for each class, we construct a code of length 2k−1 − 1 as the signature. All the selections for parameters are performed on training sets."
  }, {
    "heading": "6.2. Results",
    "text": "Table 2 summarizes the detailed results on twenty two data sets. As can be seen, the overall performance of our method is superior or highly competitive to the other compared methods. Specifically, mcODM performs significantly better than mcSVM/ovaSVM/ovoSVM/ecocSVM on 17/19/18/17 over 22 data sets respectively, and achieves the best accuracy on 20 data sets. In addition, as can be seen, in comparing with other four methods which don’t consider margin distribution, the win/tie/loss counts show that mcODM is always better or comparable, almost never worse than it."
  }, {
    "heading": "6.3. Influence of the Number of Classes",
    "text": "In this section we study the influence of the number of classes on generalization performance and margin distribution, respectively."
  }, {
    "heading": "6.3.1. GENERALIZATION PERFORMANCE",
    "text": "Figure 1 plots the generalization performance of all the five methods on data set segment, and similar observation can be found for other data sets. As can be seen, when the number of classes is less than four, all methods perform quite well. However, as the fifth class is added, the generalization performance of other four methods decreases drastically. This might be attributable to the introduction of some noisy data, which SVM-style algorithms are very sensitive to since they optimize the minimum margin. On the other hand, our method considers the whole margin distribution, so it can be robust to noise and relatively more stable."
  }, {
    "heading": "6.3.2. MARGIN DISTRIBUTION",
    "text": "Figure 2 plots the frequency histogram of margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the number of classes increases from two to seven. As can be seen, when the number of classes is less than four, all methods can achieve good margin distribution, whereas with the increase of the number of classes, the other two methods begin to produce negative margins. At the same time, the distribution of our method becomes\n“sharper”, which prevents the instance with small margin, so our method can still perform relatively well as the number of classes increases, which is also consistent with the observation from Figure 1."
  }, {
    "heading": "6.4. Time Cost",
    "text": "We compare the single iteration time cost of our method with mcSVM, ovaSVM, ovoSVM on all the data sets except aloi, on which ovoSVM could not return results in 48 hours. All the experiments are performed with MATLAB 2012b on a machine with 8×2.60 GHz CPUs and 32GB main memory. The average CPU time (in seconds) on each data set is shown in Figure 3. The binary SVM used in ovaSVM, ovoSVM and mcSVM are both implemented by the LIBLINEAR (Fan et al., 2008) package. It can be seen that for small data sets, the efficiency of all the methods are similar, however, for data sets with more than ten classes, e.g., sector and rcv1, mcSVM and mcODM, which learn all the scroing functions at once, are much faster than ovaSVM and ovoSVM, owing to the inefficiency of binarydecomposition as discussed in Section 6.1. Note that LIBLINEAR are very fast implementations of binary SVM and mcSVM, and this shows that our method is also computationally efficient."
  }, {
    "heading": "7. Conclusions",
    "text": "Recent studies disclose that for binary class classification, maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. However, it remains open to the influence of margin distribution for multi-class classification. We try to answer this question in this paper. After maximizing the margin mean and minimizing the margin variance simultaneously, the resultant optimization is a difficult non-differentiable non-convex programming. We propose mcODM to solve this problem efficiently. Extensive experiments on twenty two data sets validate the superiority of our method to four versions of multi-class SVMs. In the future it will be interesting to extend mcODM to more general learning settings, i.e., multilabel learning and structured learning."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by the NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Authors want to thank reviewers for helpful comments, and thank Dr. Wei Gao for reading a draft."
  }],
  "year": 2017,
  "references": [{
    "title": "On the algorithmic implementation of multiclass kernel-based vector machines",
    "authors": ["K. Crammer", "Y. Singer"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2001
  }, {
    "title": "On the learnability and design of output codes for multiclass problems",
    "authors": ["K. Crammer", "Y. Singer"],
    "venue": "Machine Learning,",
    "year": 2002
  }, {
    "title": "Solving multiclass learning problems via error-correcting output codes",
    "authors": ["T.G. Dietterich", "G. Bakiri"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 1995
  }, {
    "title": "Liblinear: A library for large linear classification",
    "authors": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
    "authors": ["Y. Freund", "R.E. Schapire"],
    "venue": "In Proceedings of the 2nd European Conference on Computational Learning Theory,",
    "year": 1995
  }, {
    "title": "On the doubt about margin explanation of boosting",
    "authors": ["W. Gao", "Zhou", "Z.-H"],
    "venue": "Artificial Intelligence,",
    "year": 2013
  }, {
    "title": "On the method of bounded differences",
    "authors": ["C. McDiarmid"],
    "venue": "Surveys in Combinatorics,",
    "year": 1989
  }, {
    "title": "Foundations of Machine Learning",
    "authors": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"],
    "year": 2012
  }, {
    "title": "How boosting the margin can also boost classifier complexity",
    "authors": ["L. Reyzin", "R.E. Schapire"],
    "venue": "In Proceedings of 23rd International Conference on Machine Learning,",
    "year": 2006
  }, {
    "title": "Boosting the margin: a new explanation for the effectives of voting methods",
    "authors": ["R.E. Schapire", "Y. Freund", "P.L. Bartlett", "W.S. Lee"],
    "venue": "Annuals of Statistics,",
    "year": 1998
  }, {
    "title": "Convergence of a block coordinate descent method for nondifferentiable minimization",
    "authors": ["P. Tseng"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 2001
  }, {
    "title": "Pairwise classification and support vector machines",
    "authors": ["Ulrich", "H.G. Kreel"],
    "venue": "Advances in Kernel Methods: Support Vector Machines,",
    "year": 1998
  }, {
    "title": "The Nature of Statistical Learning",
    "authors": ["V. Vapnik"],
    "year": 1995
  }, {
    "title": "A refined margin analysis for boosting algorithms via equilibrium margin",
    "authors": ["L.W. Wang", "M. Sugiyama", "C. Yang", "Zhou", "Z.-H", "J. Feng"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Multi-class support vector machines",
    "authors": ["J. Weston", "C. Watkins"],
    "venue": "In Proceedings of the European Symposium on Artificial Neural Networks, Brussels, Belgium,",
    "year": 1999
  }, {
    "title": "Large margin distribution machine",
    "authors": ["T. Zhang", "Zhou", "Z.-H"],
    "venue": "In Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
    "year": 2014
  }, {
    "title": "Optimal margin distribution machine",
    "authors": ["T. Zhang", "Zhou", "Z.-H"],
    "venue": "CoRR, abs/1604.03348,",
    "year": 2016
  }, {
    "title": "Large margin distribution learning with cost interval and unlabeled data",
    "authors": ["Zhou", "Y.-H", "Z.-H"],
    "venue": "IEEE Transactions on Knowledge and Data Engineering,",
    "year": 2016
  }, {
    "title": "Ensemble Methods: Foundations and Algorithms",
    "authors": ["Zhou", "Z.-H"],
    "year": 2012
  }, {
    "title": "Large margin distribution learning",
    "authors": ["Zhou", "Z.-H"],
    "venue": "Proceedings of the 6th IAPR International Workshop on Artificial Neural Networks in Pattern Recognition,",
    "year": 2014
  }],
  "id": "SP:bc5cc0640814684349e25e8e873c17dd2d0d8318",
  "authors": [{
    "name": "Teng Zhang",
    "affiliations": []
  }, {
    "name": "Zhi-Hua Zhou",
    "affiliations": []
  }],
  "abstractText": "Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the firstand second-order statistics can achieve superior performance. It still remains open for multiclass classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.",
  "title": "Multi-Class Optimal Margin Distribution Machine"
}