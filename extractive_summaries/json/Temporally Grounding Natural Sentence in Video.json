{
  "sections": [{
    "text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1884–1894 Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics\n1884"
  }, {
    "heading": "1 Introduction",
    "text": "Given an image/video and a language query, image/video grounding aims to localize a spatial region in the image (Plummer et al., 2015; Yu et al., 2017, 2018) or a specific frame in the video (Zhou et al., 2018) which semantically corresponds to the language query. Grounding has broad applications, such as text based image retrieval (Chen et al., 2017; Ma et al., 2015), description generation (Wang et al., 2018a; Rohrbach et al., 2017;\n∗ Work done while Zhenfang Chen was a Research Intern with Tencent AI Lab.\n† Corresponding authors.\nWang et al., 2018b), and question answer (Gao et al., 2018; Ma et al., 2016). Recently, promising progress has been made in image grounding (Yu et al., 2018; Chen et al., 2018c; Zhang et al., 2018) which heavily relies on fine-grained annotations in the form of region-sentence pairs. Fine-grained annotations for video grounding are more complicated and labor-intensive as one may need to annotate a spatio-temporal tube (i.e., label the spatial region in each frame) in a video which semantically corresponds to one language query.\nTo avoid the intensive labor involved in dense annotations, (Huang et al., 2018) and (Zhou et al., 2018) considered the problem of weaklysupervised video grounding where only aligned video-sentence pairs are provided without any fine-grained regional annotations. However, they both ground only a noun or pronoun in a static frame of the video. As illustrated in Fig. 1, it is difficult to distinguish the target dog (denoted by the green box) from other dogs (denoted by the red boxes) if we attempt to ground only the noun “dog” in one single frame of the video. The main reason is that the textual description of “dog” is not sufficiently expressive and the visual appearance in one single frame cannot characterize the spatio-temporal dynamics (e.g., the action and movements of the “dog”).\nIn this paper, we introduce a novel task, referred to as weakly-supervised spatio-temporally grounding sentence in video (WSSTG). Specifically, given a natural sentence and a video, we aim to localize a spatio-temporal tube (i.e., a sequence of bounding boxes), referred to as an instance, in the video which semantically matches the given sentence (see Fig. 1). During training, we do not rely on any fine-grained regional annotations. Compared with existing weaklysupervised video grounding problems (Zhou et al., 2018; Huang et al., 2018), our proposed WSSTG task has the following two advantages and challenges. First, we aim to ground a natural sentence instead of just a noun or pronoun, which is more comprehensive and flexible. As illustrated in Fig. 1, with a detailed description like “lying on the grass and then it stands up”, the target dog (denoted by green boxes) can be localized without ambiguity. However, how to comprehensively capture the semantic meaning of a sentence and ground it in a video, especially in a weaklysupervised manner, poses a challenge. Second, compared with one bounding box in a static frame, a spatio-temporal tube (denoted by a sequence of green bounding boxes in Fig. 1) presents the temporal movements of “dog”, which can characterize its visual dynamics and thereby semantically match the given sentence. However, how to exploit and model the spatio-temporal characteristics of the tubes as well as their complicated relationships with the sentence poses another challenge.\nTo handle the above challenges, we propose a novel model realized within the multiple instance learning framework (Karpathy and Fei-Fei, 2015; Tang et al., 2017, 2018). First, a set of instance proposals are extracted from a given video. Features of the instance proposals and the sentence are then encoded by a novel attentive interactor that exploits their fine-grained relationships to generate semantic matching behaviors. Finally, we propose a diversity loss, together with a ranking loss, to train the whole model. During testing, the instance proposal which exhibits the strongest semantic matching behavior with the given sentence is selected as the grounding result.\nTo facilitate our proposed WSSTG task, we contribute a new grounding dataset, called VIDsentence, by providing sentence descriptions for the instances of the ImageNet video object detection dataset (VID) (Russakovsky et al., 2015).\nSpecifically, 7, 654 instances of 30 categories from 4, 381 videos in VID are extracted. For each instance, annotators are asked to provide a natural sentence describing its content. Please refer to Sec. 4 for more details about the dataset.\nOur main contributions can be summarized as follows. 1) We tackle a novel task, namely weakly-supervised spatio-temporally video grounding (WSSTG), which localizes a spatiotemporal tube in a given video that semantically corresponds to a given natural sentence, in a weakly-supervised manner. 2) We propose a novel attentive interactor to exploit fine-grained relationships between instances and the sentence to characterize their matching behaviors. A diversity loss is proposed to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones during training. 3) We contribute a new dataset, named as VID-sentence, to serve as a benchmark for the novel WSSTG task. 4) Extensive experimental results are analyzed, which illustrate the superiority of our proposed method."
  }, {
    "heading": "2 Related Work",
    "text": "Grounding in Images/Videos. Grounding in images has been popular in the research community over the past decade (Kong et al., 2014; Matuszek et al., 2012; Hu et al., 2016; Wang et al., 2016a,b; Li et al., 2017; Cirik et al., 2018; Sadeghi and Farhadi, 2011; Zhang et al., 2017; Xiao et al., 2017; Chen et al., 2019, 2018a). In recent years, researchers also explore grounding in videos. Yu and Siskind (2015) grounded objects in constrained videos by leveraging weak semantic constraints implied by a sequence of sentences. Vasudevan et al. (2018) grounded objects in the last frame of stereo videos with the help of text, motion cues, human gazes and spatial-temporal context. However, fully supervised grounding requires intensive labor for regional annotations, especially in the case of videos.\nWeakly-Supervised Grounding. To avoid the intensive labor involved in regional annotations, weakly-supervised grounding has been proposed where only image-sentence or videosentence pairs are needed. It was first studied in the image domain (Zhao et al., 2018; Rohrbach et al., 2016). Later, given a sequence of transcriptions and their corresponding video clips as well as their temporal alignment, Huang et al. (2018)\ngrounded nouns/pronouns in specific frames by constructing a visual grounded action graph. The work closest to ours is (Zhou et al., 2018), in which the authors grounded a noun in a specific frame by considering object interactions and loss weighting given one video and one text input. In this work, we also focus on grounding in a videotext pair. However, different from (Zhou et al., 2018) whose text input consists of nouns/pronouns and output is a bounding box in a specific frame, we aim to ground a natural sentence and output a spatio-temporal tube in the video."
  }, {
    "heading": "3 Method",
    "text": "Given a natural sentence query q and a video v, our proposed WSSTG task aims to localize a spatio-temporal tube, referred to as an instance, p = {bt}Tt=1 in the video sequence, where bt represents a bounding box in the t-th frame and T denotes the total number of frames. The localized instance should semantically correspond to the sentence query q. As WSSTG is carried out in a weakly-supervised manner, only aligned videosentence pairs {v, q} are available with no finegrained regional annotations during training. In this paper, we cast the WSSTG task as a multiple instance learning problem (Karpathy and FeiFei, 2015). Given a video v, we first generate a set of instance proposals by an instance generator (Gkioxari and Malik, 2015). We then identify which instance semantically matches the natural sentence query q.\nWe propose a novel model for handling the WSSTG task. It consists of two components,\nnamely an instance generator and an attentive interactor (see Fig. 2). The instance generator links bounding boxes detected in each frame into instance proposals (see Sec. 3.1). The attentive interactor exploits the complicated relationships between instance proposals and the given sentence to yield their matching scores (see Sec. 3.2). The proposed model is optimized with a ranking loss Lrank and a novel diversity loss Ldiv (see Sec. 3.3). Specifically, Lrank aims to distinguish aligned video-sentence pairs from the unaligned ones, while Ldiv targets strengthening the matching behaviors between reliable instance-sentence pairs and penalizing the unreliable ones from the aligned video-sentence pairs."
  }, {
    "heading": "3.1 Instance Extraction",
    "text": "Instance Generation. As shown in Fig. 2, the first step of our method is to generate instance proposals. Similar to (Zhou et al., 2018), the region proposal network from Faster-RCNN (Ren et al., 2015) is used to detect frame-level bounding boxes with corresponding confidence scores, which are then linked to produce spatio-temporal tubes.\nLet bt denote a detected bounding box at time t and bt+1 denote another box at time t+ 1. Following (Gkioxari and Malik, 2015), we define the linking score sl between bt and bt+1 as\nsl(bt, bt+1) = sc(bt) + sc(bt+1) + λ · IoU(bt, bt+1), (1)\nwhere sc(b) is the confidence score of b, IoU(bt, bt+1) is the intersection-over-union (IoU) of bt and bt+1, and λ is a balancing scalar which is set to 0.2 in our implementation.\nAs such, one instance proposal pn can be viewed as a path {bnt }Tt=1 over the whole video sequence with energy E(pn) given by\nE(pn) = 1\nT − 1 T−1∑ t=1 sl(b n t , b n t+1). (2)\nWe identify the instance proposal with the maximal energy by the Viterbi algorithm (Gkioxari and Malik, 2015). We keep the identified instance proposal and remove all the bounding boxes associated with it. We then repeat the above process until there is no bounding box left. This results in a set of instance proposals P = {pn}Nn=1, with N being the total number of proposals.\nFeature Representation. Since an instance proposal consists of bounding boxes in consecutive video frames, we use I3D (Carreira and\nZisserman, 2017) and Faster-RCNN to generate the RGB sequence feature I3D-RGB, the flow sequence feature I3D-Flow, and the frame-level RoI pooled feature, respectively. Note that it is not effective to encode each bounding box as an instance proposal may include thousands of bounding boxes. We therefore evenly divide each instance proposal into tp segments and average the features within each segment. tp is set to 20 for all our experiments. We concatenate all three kinds of visual features before feeding it into the following attentive interactor. Taking each segment as a time step, each proposal p is thereby represented as Fp ∈ Rtp×dp , a sequence of dp dimensional concatenated visual features at each step."
  }, {
    "heading": "3.2 Attentive Interactor",
    "text": "With the instance proposals from the video and the given sentence query, we propose a novel attentive interactor to characterize the matching behaviors between each proposal and the sentence query. Our attentive interactor consists of two coupling components, namely interaction and matching behavior characterization (see Fig. 3).\nBefore diving into the details of the interactor, we first introduce the representation of the query sentence q. We represent each word in q using the 300-dimensional word2vec (Mikolov et al., 2013) and omit words that are not in the dictionary. In this way, each sentence q is represented as Fq ∈ Rtq×dq , where tq is the total number of words in the sentence and dq denotes the dimension of the\nword embedding."
  }, {
    "heading": "3.2.1 Interaction",
    "text": "Given the sequential visual features Fp ∈ Rtp×dp of one candidate instance and the sequential textual features Fq ∈ Rtq×dq of the query sentence, we propose an interaction module to exploit their complicated matching behaviors in a finegrained manner. First, two long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) are utilized to encode the instance proposal and sentence, respectively:\nhpt = LSTMp(f p t ,h p t−1), hqt = LSTMq(f q t ,h q t−1),\n(3)\nwhere fpt and f q t are the t-th row representations in Fp and Fq, respectively. Due to the natural characteristics of LSTM, hpt and h q t , as the yielded hidden states, encode and aggregate the contextual information from the sequential representation, and thereby yield more meaningful and informative visual features Hp = {hpt } tp t=1 and sentence representations Hq = {hqt} tq t=1.\nDifferent from (Rohrbach et al., 2016; Zhao et al., 2018) which used only the last hidden state hqtq as the feature embedding for the query sentence, we generate visually guided sentence features Hqp = {hqpt } tp t=1 by exploiting their fine-grained relationships based on Hq and Hp. Specifically, given the i-th visual feature hpi , an attention mechanism (Xu et al., 2015) is used to adaptively summarize Hq = {hqt} tq t=1 with respect to hpi :\nei,j = w T tanh (Wqhqj + W phpi + b1) + b2, (4) ai,j = exp(ei,j)∑tq\nj′=1 exp(ei,j′) , (5)\nhqpi = tq∑ j=1 ai,jh q j , (6)\nwhere Wq ∈ RK×Dq , Wp ∈ RK×Dp , b1 ∈ RK are the learnable parameters that map visual and sentence features to the same K-dimension space. w ∈ RK and b2 ∈ R work on the coupled textual and visual features and yield their affinity scores. With respect to Wphpi in Eq. (4), the generated visually guided sentence feature hqpi pays more attention on the words more correlated with hpi by adaptively summarizing Hq = {h q t} tq t=1.\nOwning to the attention mechanism in Eqs. (4- 6), our proposed interaction module makes each\nvisual feature interact with all the sentence features and attentively summarize them together. As such, fine-grained relationships between the visual and sentence representations are exploited."
  }, {
    "heading": "3.2.2 Matching Behavior Characterization",
    "text": "After obtaining a set of visually guided sentence features Hqp = {hqpt } tp t=1, we characterize the fine-grained matching behaviors between the visual and sentence features. Specifically, the matching behavior between the i-th visual and sentence features is defined as\nsi(h p i ,h qp i ) = φ(h p i ,h qp i ). (7)\nThe instantiation of φ can be realized by different approaches, such as multi-layer perceptron (MLP), inner-product, or cosine similarity. In this paper, we use cosine similarity between hpi and hqpi for simplicity. Finally, we define the matching behavior between an instance proposal p and the sentence q as\ns(q, p) = 1\ntp tp∑ i=1 si(h p i ,h qp i ). (8)"
  }, {
    "heading": "3.3 Training",
    "text": "For the WSSTG task, since no regional annotations are available during the training, we cannot optimize the framework in a fully supervised manner. We, therefore, resort to MIL to optimize the proposed network based on the obtained matching behaviors of the instance-sentence pairs. Specifically, our objective function is defined as\nL = Lrank + β Ldiv, (9)\nwhere Lrank is a ranking loss, aiming at distinguishing aligned video-sentence pairs from the unaligned ones. Ldiv is a novel diversity loss, which is proposed to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones from the aligned videosentence pair. β is a scalar which is set to 1 in all our experiments. Ranking Loss. Assume that {v, q} is a semantically aligned video-sentence pair. We define the visual-semantic matching score S between v and q as\nS(v, q) = max s(q, pn) , n = 1, ..., N , (10)\nwhere pn is the n-th proposal generated from the video v, s(q, pn) is the matching behavior computed by Eq. (8), and N is the total number of instance proposals.\nSuppose that v′ and q′ are negative samples that are not semantically correlated with q and v, respectively. Inspired by (Karpathy and Fei-Fei, 2015), we define the ranking loss as\nLrank = ∑ v 6=v′ ∑ q 6=q′ [max(0, S(v, q′)− S(v, q) + ∆)+\nmax(0, S(v′, q)− S(v, q) + ∆)], (11)\nwhere ∆ is a margin which is set to 1 in all our experiments. Lrank directly encourages the matching scores of aligned video-sentence pairs to be larger than those of unaligned pairs. Diversity Loss. One limitation of the ranking loss defined in Eq. (11) is that it does not consider the matching behaviors between the sentence and different instance proposals extracted from an aligned video. A prior for video grounding is that only a few instance proposals in the paired video are semantically aligned to the query sentence, while most of the other instance proposals are not. Thus, it is desirable to have a diverse distribution of the matching behaviors {s(q, pn)}Nn=1.\nTo encourage a diverse distribution of {s(q, pn)}Nn=1, we propose a diversity loss Ldiv to strengthen the matching behaviors between reliable instance-sentence pairs and penalize the unreliable ones during training. Specifically, we first normalize {s(q, pn)}Nn=1 by softmax\ns′(q, pn) = exp(s(q, pn))∑N\nn′=1 exp(s(q, pn′)) , (12)\nand then penalize the entropy of the distribution of {s′(q, pn)}Nn=1 by defining the diversity loss as\nLdiv = − N∑\nn=1\ns′(q, pn)log(s ′(q, pn)). (13)\nNote that the smaller Ldiv is, the more diverse {s(q, pn)}Nn=1 will be, which implicitly encourages the matching scores of semantically aligned instance-sentence pairs being larger than those of the misaligned pairs."
  }, {
    "heading": "3.4 Inference",
    "text": "Given a testing video and a query sentence, we extract candidate instance proposals, and characterize the matching behavior between each instance proposal and the sentence by the proposed attentive interactor. The instance with the strongest matching behavior is deemed the result of the WSSTG task.\nA red bus is making a turn on the road A red bus is making a turn on the road\nA brown and white dog is lying on the grass and then standing upA large elephant runs i the water from left to right\nA red bus is making a turn on the road\nbrown and white dog is lying on the grass and then standing up\nlarge elephant runs in the water from left to rig t"
  }, {
    "heading": "4 VID-sentence Dataset",
    "text": "A main challenge for the WSSTG task is the lack of suitable datasets. Existing datasets like TACoS (Regneri et al., 2013) and YouCook (Das et al., 2013) are unsuitable as they do not provide spatio-temporal annotations for target instances in the videos, which are necessary for the WSSTG task for evaluation. To the best of our knowledge, the most suitable existing dataset is the Personsentence dataset provided by (Yamaguchi et al., 2017), which is used for spatio-temporal person search among videos. However, this dataset is too simple for the WSSTG task since it contains only people in the videos. To this end, we contribute a new dataset by annotating videos in ImageNet video object detection dataset (VID) (Russakovsky et al., 2015) with sentence descriptions. We choose VID as the visual materials for two primary reasons. First, it is one of the largest video detection datasets containing videos of diverse categories in complicated scenarios. Second, it provides dense bounding-box annotations and instance IDs which help avoid labor-intensive annotations for spatio-temporal regions of the validation/testing set. VID-sentence Annotation. With 30 categories, VID contains 3826, 555 and 937 videos for training, validation and testing respectively. We first divide videos in training and validation sets1 into trimmed videos based on the provided instance IDs, and delete videos less than 9 frames. As such, there remain 9, 029 trimmed videos in total. In each trimmed video, one instance is identified as a sequence of bounding boxes. A group of annotators are asked to provide sentence descriptions for the target instances. Each target instance is\n1Testing set is omitted as its spatial-temporal annotations are unavailable\nannotated with one sentence description. An instance is discarded if it is too difficult to provide a unique and precise description. After annotation, there are 7, 654 videos with sentence descriptions. We randomly select 6, 582 videos as the training set, and evenly split the remaining videos into the validation and testing sets (i.e., each contains 536 videos). Some examples from the VID-sentence dataset are shown in Fig. 4. Dataset Statistics. To summarize, the created dataset has 6, 582/536/536 spatiotemporal instances with descriptions for training/validation/testing. It covers all 30 categories in VID, such as “car”, “monkey” and “watercraft”. The size of the vocabulary is 1, 823 and the average length of the descriptions is 13.2. Table 1 shows the statistics of our constructed VID-sentence dataset. Compared with the Person-sentence dataset, our VID-sentence dataset has a similar description length but includes more instances and categories.\nIt is important to note that, although VID provides regional annotations for the training set, these annotations are not used in any of our experiments since we focus on weakly-supervised spatio-temporal video grounding."
  }, {
    "heading": "5 Experiments",
    "text": "In this section, we first compare our method with different kinds of baseline methods on the created VID-sentence dataset, followed by the ablation study. Finally, we show how well our model generalizes on the Person-sentence dataset."
  }, {
    "heading": "5.1 Experimental Settings",
    "text": "Baseline Models. Existing weakly-supervised video grounding methods (Huang et al., 2018; Zhou et al., 2018) are not applicable to the WSSTG task. Huang et al. (2018) requires temporal alignment between a sequence of transcription descriptions and the video segments to ground a noun/pronoun in a certain frame, while Zhou et al. (2018) mainly grounds nouns/pronouns in specific frames of videos. As such, we develop three baselines based on DVSA (Karpathy and Fei-Fei, 2015), GroundeR (Rohrbach et al., 2016), and a variant frame-level method modified from (Zhou et al., 2018) for performance comparisons. Following recent grounding methods like (Rohrbach et al., 2016; Chen et al., 2018b), we use the last hidden state of an LSTM encoder as the sentence\nembedding for all the baselines. Since DVSA and GroundeR are originally proposed for image grounding, in order to adapt to video, we consider three methods to encode visual features Fp ∈ Rtp×dp including averaging (Avg), NetVLAD (Arandjelovic et al., 2016), and LSTM. For the variant baseline modified from (Zhou et al., 2018), we densely predict each frame to generate a spatio-temporal prediction. Implementation Details. Similar to (Zhou et al., 2018), we use the region proposal network from Faster-RCNN pretrained on MSCOCO (Lin et al., 2014) to extract frame-level region proposals. For each video, we extract 30 bounding boxes for each frame and link them into 30 spatio-temporal tubes with the method (Gkioxari and Malik, 2015). We map the word embedding to 512-dimension before feeding it to the LSTM encoder. Dimension of the hidden state of all the LSTMs is set to 512. Batch size is 16, i.e., 16 videos with total 480 instance proposals and 16 corresponding sentences. We construct positive and negative video-sentence pairs for training within a batch for efficiency, i.e., roughly 16 positive pairs and 240 negative pairs for the triplet construction. SGD is used to optimize the models with a learning rate of 0.001 and momentum of 0.9. We train all the models with 30 epochs. Please refer to supplementary materials for more details. Evaluation Metric. We use the bounding box localization accuracy for evaluation. An output instance is considered as “accurate” if the overlap between the detected instance and the groundtruth is greater than a threshold η. The definition of the overlap is the same as (Yamaguchi et al., 2017), i.e., the average overlap of the bounding boxes in annotated frames. η is set to 0.4, 0.5, 0.6 for extensive evaluations."
  }, {
    "heading": "5.2 Performance Comparisons",
    "text": "Table 2 shows the performance comparisons between our model and the baselines. We additionally show the performance of randomly choosing an instance proposal and the upper bound perfor-\nmance of choosing the instance proposal of the largest overlap with the ground-truth.\nThe results suggest that, 1) models with NetVLAD (Arandjelovic et al., 2016) perform the worst. We suspect that models based on NetVLAD are complicated and the supervisions are too weak to optimize the models sufficiently well. 2) Models with LSTM embedding achieve only comparable performances compared with models based on simple averagingf. It is mainly due to the fact that the power of LSTM has not been fully exploited. 3) The variant method of (Zhou et al., 2018) performs better than both DVSA and GroundeR with various kinds of visual encoding techniques, indicating its power for the task. 4) Our model achieves the best results, demonstrating its effectiveness, showing that our model is better at characterizing the matching behaviors between the query sentence and the visual instances in the video.\nTo compare the methods qualitatively, we show an exemplar sample in Fig. 5. Compared with GroundeR+LSTM and DVSA+LSTM, our method identifies a more accurate instance from the candidate instance proposals. Moreover, the instances generated by our method are more temporally consistent compared with the modified frame-level method (Zhou et al., 2018). This can be attributed to the exploitation of the temporal information during instance generation and attentive interactor in our model."
  }, {
    "heading": "5.3 Ablation Study",
    "text": "To verify the contributions of the proposed attentive interactor and diversity loss, we perform the following ablation study. To be specific, we compare the full method with three variants, includ-\ning: 1) removing both the attentive interactor and diversity loss, which is equivalent to the DVSA model using LSTM for encoding both the visual features and sentence features, termed as Base; 2) Base+Div, which is formed by introducing the diversity loss; 3) Base+Int with the attentive interactor module.\nTable 3 shows the corresponding results. Compared with Base, both the diversity loss and attentive interactor constantly improve the performance. Moreover, to show the effectiveness of the proposed attentive interactor, we visualize the adaptive weight a in Eq. (5). As shown in Fig. 6,\nour method adaptively pays more attention to the words that match the instance such as the “puppy” in all three segments and the “hand” in segment with ID 2. To show the effectiveness of the diversity loss, we divide instance proposals in the testing set into 10 groups based on their IoU scores with the ground-truth and then calculate the average matching behaviors of each group, predicted by counterparts with and without the diversity loss. As shown in Fig. 7, the proposed diversity loss Ldiv penalizes the matching behaviors of the instances of lower IoU with ground-truth while strengthens instances of higher IoU."
  }, {
    "heading": "5.4 Experiments on Person-sentence Dataset",
    "text": "We further evaluate our model and the baseline methods on the Person-sentence dataset (Yamaguchi et al., 2017). We ignore the bounding box annotations in the training set and carry out experiments for the proposed WSSTG task. For fair comparisons, all experiments are conducted on the visual feature extractor provided by (Carreira and Zisserman, 2017).\nTable 4 shows the results. Similarly, the proposed attentive interactor model (without the diversity loss) outperforms all the baselines. Moreover, the diversity loss further improves the performance. Note that the improvement of our model on this dataset is more significant than that on the VID-sentence dataset. The reason might be that the upper bound performance of the Personsentence is much higher than that of the VIDsentence (77.9 for Person-sentence versus 47.6 for VID-sentence on average). This also suggests that the created VID-sentence dataset is more challenging and more suitable as a benchmark dataset."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we introduced a new task, namely weakly-supervised spatio-temporally grounding natural sentence in video. It takes a sentence and a video as input and outputs a spatio-temporal tube from the video, which semantically matches the sentence, with no reliance on spatio-temporal annotations during training. We handled this task based on the multiple instance learning framework. An attentive interactor and a diversity loss were proposed to learn the complicated relationships between the instance proposals and the sentence. Extensive experiments showed the effectiveness of our model. Moreover, we contributed a new dataset, named as VID-sentence, which can serve as a benchmark for the proposed task."
  }],
  "year": 2019,
  "references": [{
    "title": "Netvlad: Cnn architecture for weakly supervised place recognition",
    "authors": ["Relja Arandjelovic", "Petr Gronat", "Akihiko Torii", "Tomas Pajdla", "Josef Sivic."],
    "venue": "CVPR, pages 5297–5307.",
    "year": 2016
  }, {
    "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
    "authors": ["Joao Carreira", "Andrew Zisserman."],
    "venue": "CVPR, pages 4724–4733.",
    "year": 2017
  }, {
    "title": "Temporally grounding natural sentence in video",
    "authors": ["Jingyuan Chen", "Xinpeng Chen", "Lin Ma", "Zequn Jie", "Tat-Seng Chua."],
    "venue": "EMNLP.",
    "year": 2018
  }, {
    "title": "Localizing natural language in videos",
    "authors": ["Jingyuan Chen", "Lin Ma", "Xinpeng Chen", "Zequn Jie", "Jiebo Luo."],
    "venue": "AAAI.",
    "year": 2019
  }, {
    "title": "Amc: Attention guided multi-modal correlation learning for image search",
    "authors": ["Kan Chen", "Trung Bui", "Chen Fang", "Zhaowen Wang", "Ram Nevatia."],
    "venue": "CVPR, pages 6203–6211.",
    "year": 2017
  }, {
    "title": "Knowledge aided consistency for weakly supervised phrase grounding",
    "authors": ["Kan Chen", "Jiyang Gao", "Ram Nevatia."],
    "venue": "arXiv preprint arXiv:1803.03879.",
    "year": 2018
  }, {
    "title": "Real-time referring expression comprehension by single-stage grounding network",
    "authors": ["Xinpeng Chen", "Lin Ma", "Jingyuan Chen", "Zequn Jie", "Wei Liu", "Jiebo Luo."],
    "venue": "arXiv: 1812.03426.",
    "year": 2018
  }, {
    "title": "Using syntax to ground referring expressions in natural images",
    "authors": ["Volkan Cirik", "Taylor Berg-Kirkpatrick", "LouisPhilippe Morency."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
    "authors": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso."],
    "venue": "CVPR.",
    "year": 2013
  }, {
    "title": "Motion-appearance co-memory networks for video question answering",
    "authors": ["Jiyang Gao", "Runzhou Ge", "Kan Chen", "Ram Nevatia."],
    "venue": "arXiv preprint arXiv:1803.10906.",
    "year": 2018
  }, {
    "title": "Finding action tubes",
    "authors": ["Georgia Gkioxari", "Jitendra Malik."],
    "venue": "CVPR, pages 759–768.",
    "year": 2015
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Natural language object retrieval",
    "authors": ["Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell."],
    "venue": "CVPR, pages 4555– 4564.",
    "year": 2016
  }, {
    "title": "Finding “it”: Weakly-supervised reference-aware visual grounding in instructional videos",
    "authors": ["De-An Huang", "Shyamal Buch", "Lucio Dery", "Animesh Garg", "Li Fei-Fei", "Juan Carlos Niebles."],
    "venue": "CVPR.",
    "year": 2018
  }, {
    "title": "Deep visualsemantic alignments for generating image descriptions",
    "authors": ["Andrej Karpathy", "Li Fei-Fei."],
    "venue": "CVPR, pages 3128–3137.",
    "year": 2015
  }, {
    "title": "What are you talking about? text-to-image coreference",
    "authors": ["Chen Kong", "Dahua Lin", "Mohit Bansal", "Raquel Urtasun", "Sanja Fidler."],
    "venue": "CVPR, pages 3558– 3565.",
    "year": 2014
  }, {
    "title": "Deep attribute-preserving metric learning for natural language object retrieval",
    "authors": ["Jianan Li", "Yunchao Wei", "Xiaodan Liang", "Fang Zhao", "Jianshu Li", "Tingfa Xu", "Jiashi Feng."],
    "venue": "MM, pages 181–189.",
    "year": 2017
  }, {
    "title": "Microsoft coco: Common objects in context",
    "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick."],
    "venue": "ECCV, pages 740– 755.",
    "year": 2014
  }, {
    "title": "Learning to answer questions from image using convolutional neural network",
    "authors": ["Lin Ma", "Zhengdong Lu", "Hang Li."],
    "venue": "AAAI.",
    "year": 2016
  }, {
    "title": "Multimodal convolutional neural networks for matching image and sentence",
    "authors": ["Lin Ma", "Zhengdong Lu", "Lifeng Shang", "Hang Li."],
    "venue": "ICCV.",
    "year": 2015
  }, {
    "title": "A joint model of language and perception for grounded attribute learning",
    "authors": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox."],
    "venue": "arXiv preprint arXiv:1206.6423.",
    "year": 2012
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
    "authors": ["Bryan A Plummer", "Liwei Wang", "Chris M Cervantes", "Juan C Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik."],
    "venue": "ICCV, pages 2641–2649.",
    "year": 2015
  }, {
    "title": "Grounding action descriptions in videos",
    "authors": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal."],
    "venue": "Transactions of the Association for Computational Linguistics, 1:25–36.",
    "year": 2013
  }, {
    "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
    "authors": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun."],
    "venue": "NIPS, pages 91–99.",
    "year": 2015
  }, {
    "title": "Grounding of textual phrases in images by reconstruction",
    "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Ronghang Hu", "Trevor Darrell", "Bernt Schiele."],
    "venue": "ECCV, pages 817–834.",
    "year": 2016
  }, {
    "title": "Generating descriptions with grounded and co-referenced people",
    "authors": ["Anna Rohrbach", "Marcus Rohrbach", "Siyu Tang", "Seong Joon Oh", "Bernt Schiele."],
    "venue": "arXiv preprint arXiv:1704.01518, 3.",
    "year": 2017
  }, {
    "title": "Imagenet large scale visual recognition challenge",
    "authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"],
    "year": 2015
  }, {
    "title": "Recognition using visual phrases",
    "authors": ["Mohammad Amin Sadeghi", "Ali Farhadi."],
    "venue": "CVPR, pages 1745–1752.",
    "year": 2011
  }, {
    "title": "Pcl: Proposal cluster learning for weakly supervised object detection",
    "authors": ["Peng Tang", "Xinggang Wang", "Song Bai", "Wei Shen", "Xiang Bai", "Wenyu Liu", "Alan Loddon Yuille."],
    "venue": "IEEE transactions on pattern analysis and machine intelligence.",
    "year": 2018
  }, {
    "title": "Multiple instance detection network with online instance classifier refinement",
    "authors": ["Peng Tang", "Xinggang Wang", "Xiang Bai", "Wenyu Liu."],
    "venue": "CVPR.",
    "year": 2017
  }, {
    "title": "Object referring in videos with language and human gaze",
    "authors": ["Arun Balajee Vasudevan", "Dengxin Dai", "Luc Van Gool."],
    "venue": "arXiv preprint arXiv:1801.01582.",
    "year": 2018
  }, {
    "title": "Reconstruction network for video captioning",
    "authors": ["Bairui Wang", "Lin Ma", "Wei Zhang", "Wei Liu."],
    "venue": "CVPR.",
    "year": 2018
  }, {
    "title": "Bidirectional attentive fusion with context gating for dense video captioning",
    "authors": ["Jingwen Wang", "Wenhao Jiang", "Lin Ma", "Wei Liu", "Yong Xu."],
    "venue": "CVPR.",
    "year": 2018
  }, {
    "title": "Learning deep structure-preserving image-text embeddings",
    "authors": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik."],
    "venue": "CVPR, pages 5005–5013.",
    "year": 2016
  }, {
    "title": "Structured matching for phrase localization",
    "authors": ["Mingzhe Wang", "Mahmoud Azab", "Noriyuki Kojima", "Rada Mihalcea", "Jia Deng."],
    "venue": "ECCV, pages 696–711.",
    "year": 2016
  }, {
    "title": "Weakly-supervised visual grounding of phrases with linguistic structures",
    "authors": ["Fanyi Xiao", "Leonid Sigal", "Yong Jae Lee."],
    "venue": "arXiv preprint arXiv:1705.01371.",
    "year": 2017
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "ICML, pages 2048–2057.",
    "year": 2015
  }, {
    "title": "Spatio-temporal person retrieval via natural language queries",
    "authors": ["Masataka Yamaguchi", "Kuniaki Saito", "Yoshitaka Ushiku", "Tatsuya Harada."],
    "venue": "ICCV.",
    "year": 2017
  }, {
    "title": "Sentence directed video object codetection",
    "authors": ["Haonan Yu", "Jeffrey Mark Siskind."],
    "venue": "arXiv preprint arXiv:1506.02059.",
    "year": 2015
  }, {
    "title": "Mattnet: Modular attention network for referring expression comprehension",
    "authors": ["Licheng Yu", "Zhe Lin", "Xiaohui Shen", "Jimei Yang", "Xin Lu", "Mohit Bansal", "Tamara L Berg."],
    "venue": "CVPR.",
    "year": 2018
  }, {
    "title": "A joint speakerlistener-reinforcer model for referring expressions",
    "authors": ["Licheng Yu", "Hao Tan", "Mohit Bansal", "Tamara L Berg."],
    "venue": "CVPR, volume 2.",
    "year": 2017
  }, {
    "title": "Grounding referring expressions in images by variational context",
    "authors": ["Hanwang Zhang", "Yulei Niu", "Shih-Fu Chang."],
    "venue": "CVPR, pages 4158–4166.",
    "year": 2018
  }, {
    "title": "Discriminative bimodal networks for visual localization and detection with natural language queries",
    "authors": ["Yuting Zhang", "Luyao Yuan", "Yijie Guo", "Zhiyuan He", "IAn Huang", "Honglak Lee."],
    "venue": "CVPR.",
    "year": 2017
  }, {
    "title": "Weakly supervised phrase localization with multi-scale anchored transformer network",
    "authors": ["Fang Zhao", "Jianshu Li", "Jian Zhao", "Jiashi Feng."],
    "venue": "CVPR, pages 5696–5705.",
    "year": 2018
  }, {
    "title": "Weakly-supervised video object grounding from text by loss weighting and object interaction",
    "authors": ["Luowei Zhou", "Nathan Louis", "Jason J Corso."],
    "venue": "BMVC.",
    "year": 2018
  }],
  "id": "SP:232f78d3bd242d278ce39ae761fd6d6a5be37704",
  "authors": [{
    "name": "Zhenfang Chen",
    "affiliations": []
  }, {
    "name": "Lin Ma",
    "affiliations": []
  }, {
    "name": "Wenhan Luo",
    "affiliations": []
  }, {
    "name": "Kwan-Yee K. Wong",
    "affiliations": []
  }],
  "abstractText": "In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatiotemporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train the proposed attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. Moreover, we also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Extensive experimental results demonstrate the superiority of our model over the baseline approaches. Our code and the constructed VID-sentence dataset are available at: https://github.com/ JeffCHEN2017/WSSTG.git.",
  "title": "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video"
}