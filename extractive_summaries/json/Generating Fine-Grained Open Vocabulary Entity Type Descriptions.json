{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 877–888 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n877"
  }, {
    "heading": "1 Introduction",
    "text": "Broad-coverage knowledge graphs such as Freebase, Wikidata, and NELL are increasingly being used in many NLP and AI tasks. For instance, DBpedia and YAGO were vital for IBM’s Watson! Jeopardy system (Welty et al., 2012). Google’s Knowledge Graph is tightly integrated into its search engine, yielding improved responses for entity queries as well as for question answering. In a similar effort, Apple Inc. is building an inhouse knowledge graph to power Siri and its next generation of intelligent products and services.\nDespite being rich sources of factual knowledge, cross-domain knowledge graphs often lack a succinct textual description for many of the existing entities. Fig. 1 depicts an example of a concise entity description presented to a user. Descriptions of this sort can be beneficial both to humans and in downstream AI and natural language processing tasks, including question answering (e.g., Who\nis Roger Federer?), named entity disambiguation (e.g., Philadelphia as a city vs. the film or even the brand of cream cheese), and information retrieval, to name but a few.\nAdditionally, descriptions of this sort can also be useful to determine the ontological type of an entity – another challenging task that often needs to be addressed in cross-domain knowledge graphs. Many knowledge graphs already provide ontological type information, and there has been substantial previous research on how to predict such types automatically for entities in knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), or even in unstructured text (Snow et al., 2006; Bansal et al., 2014; Tandon et al., 2015). However, most such work has targeted a fixed inventory of types from a given target ontology, many\nof which are more abstract in nature (e.g., human or artifact). In this work, we consider the task of generating more detailed open vocabulary descriptions (e.g., Swiss tennis player) that can readily be presented to end users, generated from facts in the knowledge graph.\nApart from type descriptions, certain knowledge graphs, such as Freebase and DBpedia, also provide a paragraph-length textual abstract for every entity. In the latter case, these are sourced from Wikipedia. There has also been research on generating such abstracts automatically (Biran and McKeown, 2017). While abstracts of this sort provide considerably more detail than ontological types, they are not sufficiently concise to be grasped at a single glance, and thus the onus is put on the reader to comprehend and summarize them.\nTypically, a short description of an entity will hence need to be synthesized just by drawing on certain most relevant facts about it. While in many circumstances, humans tend to categorize entities at a level of abstraction commonly referred to as basic level categories (Rosch et al., 1976), in an information seeking setting, however, such as in Fig. 1, humans naturally expect more detail from their interlocutor. For example, occupation and nationality are often the two most relevant properties used in describing a person in Wikidata, while terms such as person or human being are likely to be perceived as overly unspecific. However, choosing such most relevant and distinctive attributes from the set of available facts about the entity is non-trivial, especially given the diversity of different kinds of entities in broad-coverage knowledge graphs. Moreover, the generated text should be coherent, succinct, and non-redundant.\nTo address this problem, we propose a dynamic memory-based generative network that can generate short textual descriptions from the available factual information about the entities. To the best of our knowledge, we are the first to present neural methods to tackle this problem. Previous work has suggested generating short descriptions using predefined templates (cf. Section 4). However, this approach severely restricts the expressivity of the model and hence such templates are typically only applied to very narrow classes of entities. In contrast, our goal is to design a broad-coverage open domain description generation architecture.\nIn our experiments, we induce a new benchmark dataset for this task by relying on Wikidata, which\nhas recently emerged as the most popular crowdsourced knowledge base, following Google’s designation of Wikidata as the successor to Freebase (Tanon et al., 2016). With a broad base of 19,000 casual Web users as contributors, Wikidata is a crucial source of machine-readable knowledge in many applications. Unlike DBpedia and Freebase, Wikidata usually contains a very concise description for many of its entities. However, because Wikidata is based on user contributions, many new entries are created that still lack such descriptions. This can be a problem for downstream tools and applications using Wikidata for background knowledge. Hence, even for Wikidata, there is a need for tools to generate fine-grained type descriptions. Fortunately, we can rely on the entities for which users have already contributed short descriptions to induce a new benchmark dataset for the task of automatically inducing type descriptions from structured data."
  }, {
    "heading": "2 A Dynamic Memory-based Generative Network Architecture",
    "text": "Our proposed dynamic memory-based generative network consists of three key components: an input module, a dynamic memory module, and an output module. A schematic diagram of these are given in Fig. 2."
  }, {
    "heading": "2.1 Input Module",
    "text": "The input to the input module is a set of N facts F = {f1, f2, . . . , fN} pertaining to an entity. Each of these input facts are essentially (s, p, o) triples, for subjects s, predicates p, and objects o. Upon being encoded into a distributed vector representation, we refer to them as fact embeddings.\nAlthough many different encoding schemes can be adopted to obtain such fact embeddings, we opt for a positional encoding as described by Sukhbaatar et al. (2015), motivated in part by the considerations given by Xiong et al. (2016). For completeness, we describe the positional encoding scheme here. We encode each fact fi as a vector fi =∑J j=1 lj◦wij, where ◦ is an element-wise multiplication, and lj is a column vector with the structure lkj = (1 − jJ ) − (k/d)(1 − 2 j J ), with J being the number of words in the factual phrase, wij as the embedding of the j-th word, and d as the dimensionality of the embedding. Details about how these factual phrases are formed for our data are\ngiven in Section 3.3. Thus, the output of this module is a concatenation of N fact embeddings F = [f1; f2; . . . ; fN]."
  }, {
    "heading": "2.2 Dynamic Memory Module",
    "text": "The dynamic memory module is responsible for memorizing specific facts about an entity that will be useful for generating the next word in the output description sequence. Intuitively, such a memory should be able to update itself dynamically by accounting not only for the factual embeddings but also for the current context of the generated sequence of words.\nTo begin with, the memory is initialized as m(0) = max(0,WmF+ bm). At each time step t, the memory module attempts to gather pertinent contextual information by attending to and summing over the fact embeddings in a weighted manner. These attention weights are scalar values informed by two factors: (1) how much information from a particular fact is used by the previous memory state m(t−1), and (2) how much information of a particular fact is invoked in the current context of the output sequence h(t−1). Formally,\nxi (t) = [|fi − h(t−1)|; |fi −m(t−1)|], (1)\nzi (t) = W2 tanh(W1xi (t) + b1) + b2, (2)\na (t) i = exp(zi (t))∑N\nk=1 exp(zk (t))\n, (3)\nwhere |.| is the element-wise absolute difference\nand [; ] denotes the concatenation of vectors. Having obtained the attention weights, we apply a soft attention mechanism to extract the current context vector at time t as\nc(t) = N∑ i=1 a (t) i fi. (4)\nThis newly obtained context information is then used along with the previous memory state to update the memory state as follows:\nC(t) = [m(t−1); c(t);h(t−1)] (5)\nm(t) = max(0,WmC (t) + bm) (6)\nSuch updated memory states serve as the input to the decoder sequence of the output module at each time step."
  }, {
    "heading": "2.3 Output Module",
    "text": "The output module governs the process of repeatedly decoding the current memory state so as to emit the next word in an ordered sequence of output words. We rely on GRUs for this.\nAt each time step, the decoder GRU is presented as input a glimpse of the current memory state m(t) as well as the previous context of the output sequence, i.e., the previous hidden state of the decoder h(t−1). At each step, the resulting output of the GRU is concatenated with the context vector ci(t) and is passed through a fully connected\nlayer and finally through a softmax layer. During training, we deploy teacher forcing at each step by providing the vector embedding of the previous correct word in the sequence as an additional input. During testing, when such a signal is not available, we use the embedding of the predicted word in the previous step as an additional input to the current step. Formally,\nh(t) = GRU([m(t);w(t−1)],h(t−1)), (7)\nh̃(t) = tanh(Wd[h (t); c(t)] + bd), (8)\nŷ(t) = Softmax(Woh̃(t) + bo), (9)\nwhere [; ] is the concatenation operator, w(t−1) is vector embedding of the previous word in the sequence, and ŷ(t) is the probability distribution for the predicted word over the vocabulary at the current step."
  }, {
    "heading": "2.4 Loss Function and Training",
    "text": "Training this model amounts to picking suitable values for the model parameters θ, which include the matrices W1, W2, Wm, Wd, Wo and the corresponding bias terms b1, b2, bm, bd, and bo as well as the various transition and output matrices of the GRU.\nTo this end, if each of the training instances has a description with a maximum of M words, we can rely on the categorical cross-entropy over the entire output sequence as the loss function:\nL(θ) = − M∑ t=1 |V|∑ j=1 y (t) j log(ŷ (t) j ). (10)\nwhere y(t)j ∈ {0, 1} and |V| is the vocabulary size. We train our model end-to-end using Adam as the optimization technique."
  }, {
    "heading": "3 Evaluation",
    "text": "In this section, we describe the process of creating our benchmark dataset as well as the baseline methods and the experimental results."
  }, {
    "heading": "3.1 Benchmark Dataset Creation",
    "text": "For the evaluation of our method, we introduce a novel benchmark dataset that we have extracted from Wikidata and transformed to a suitable format. We rely on the official RDF exports of Wikidata, which are generated regularly (Erxleben et al., 2014), specifically, the RDF dump dated\n2016-08-01, which consists of 19,768,780 entities with 2,570 distinct properties. A pair of a property and its corresponding value represents a fact about an entity. In Wikidata parlance, such facts are called statements. We sample a dataset of 10K entities from Wikidata, and henceforth refer to the resulting dataset as WikiFacts10K. Our sampling method ensures that each entity in WikiFacts10K has an English description and at least 5 associated statements. We then transform each extracted statement into a phrasal form by concatenating the words of the property name and its value. For example, the (subject, predicate, object) triple (Roger Federer, occupation, tennis player) is transformed to ’occupation tennis player’. We refer to these phrases as the factual phrases, which are embedded as described earlier. We randomly divide this dataset into training, validation, and test sets with a 8:1:1 ratio. We have made our code and data available1 for reproducibility and to facilitate further research in this area."
  }, {
    "heading": "3.2 Baselines",
    "text": "We compare our model against an array of baselines of varying complexity. We experiment with some variants of our model as well as several other state-of-the-art models that, although not specifically designed for this setting, can straightforwardly be applied to the task of generating descriptions from factual data.\n1. Facts-to-sequence Encoder-Decoder Model. This model is a variant of the standard sequence-to-sequence encoderdecoder architecture described by Sutskever et al. (2014). However, instead of an input sequence, it here operates on a set of fact embeddings {f1, f2, . . . , fN}, which are emitted by the positional encoder described in Section 2.1. We initialize the hidden state of the decoder with a linear transformation of the fact embeddings as h(0) = WF + b, where F = [f1; f2; . . . ; fN] is the concatenation of N fact embeddings.\nAs an alternative, we also experimented with a sequence encoder that takes a separate fact embedding as input at each step and initializes the decoder hidden state with the final hidden state of the encoder. However, this approach did not yield us better results.\n1https://github.com/kingsaint/Open-vocabulary-entitytype-description\nTable 1: Automatic evaluation results of different models. For a detailed explanation of the baseline models, please refer to Section 3.2. The best performing model for each column is highlighted in boldface.\nModel B-1 B-2 B-3 B-4 ROUGE-L METEOR CIDEr Facts-to-seq 0.404 0.324 0.274 0.242 0.433 0.214 1.627 Facts-to-seq w. Attention 0.491 0.414 0.366 0.335 0.512 0.257 2.207 Static Memory 0.374 0.298 0.255 0.223 0.383 0.185 1.328 DMN+ 0.281 0.234 0.236 0.234 0.275 0.139 0.912 Our Model 0.611 0.535 0.485 0.461 0.641 0.353 3.295\n2. Facts-to-sequence Model with Attention Decoder. The encoder of this model is identical to the one described above. The difference is in the decoder module that uses an attention mechanism.\nAt each time step t, the decoder GRU receives a context vector c(t) as input, which is an attention weighted sum of the fact embeddings. The attention weights and the context vectors are computed as follows:\nx(t) = [w(t−1);h(t−1)] (11)\nz(t) = Wx(t) + b (12)\na(t) = softmax(z(t)) (13) c(t) = max(0, N∑ i=1 a (t) i fi) (14)\nAfter obtaining the context vector, it is fed to the GRU as input:\nh(t) = GRU([w(t−1); c(t)],h(t−1)) (15)\n3. Static Memory Model. This is a variant of our model in which we do not upgrade the memory dynamically at each time step. Rather, we use the initial memory state as the input to all of the decoder GRU steps.\n4. Dynamic Memory Network (DMN+). We consider the approach proposed by Xiong et al. (2016), which supersedes Kumar et al. (2016). However, some minor modifications are needed to adapt it to our task. Unlike the bAbI dataset, our task does not involve any question. The presence of a question is imperative in DMN+, as it helps to determine the initial state of the episodic memory module. Thus, we prepend an interrogative phrase such as ”Who is” or ”What is” to every entity name. The question module of the DMN+ is hence presented with a question such as\n”Who is Roger Federer?” or ”What is Star Wars?”. Another difference is in the output module. In DMN+, the final memory state is passed through a softmax layer to generate the answer. Since most answers in the bAbI dataset are unigrams, such an approach suffices. However, as our task is to generate a sequence of words as descriptions, we use a GRU-based decoder sequence model, which at each time step receives the final memory state m(T ) as input to the GRU. We restrict the number of memory update episodes to 3, which is also the preferred number of episodes in the original paper."
  }, {
    "heading": "3.3 Experimental Setup",
    "text": "For each entity in the WikiFacts10K dataset, there is a corresponding set of facts expressed as factual phrases as defined earlier. Each factual phrase in turn is encoded as a vector by means of the positional encoding scheme described in Section 2.1. Although other variants could be considered, such as LSTMs and GRUs, we apply this standard fact encoding mechanism for our model as well as all our baselines for the sake of uniformity and fair comparison. Another factor that makes the use of a sequence encoder such as LSTMs or GRUs less suitable is that the set of input facts is essentially unordered without any temporal correlation between facts.\nWe fixed the dimensionality of the fact embeddings and all hidden states to be 100. The vocabulary size is 29K. Our models and all other baselines are trained for a maximum of 25 epochs with an early stopping criterion and a fixed learning rate of 0.001.\nTo evaluate the quality of the generated descriptions, we rely on the standard BLEU (B-1, B-2, B-3, B-4), ROUGE-L, METEOR and CIDEr metrics, as implemented by Sharma et al. (2017). Of course, we would be remiss not to point out that these metrics are imperfect. In general, they tend\nto be conservative in that they only reward generated descriptions that overlap substantially with the ground truth descriptions given in Wikidata. In reality, it may of course be the case that alternative descriptions are equally appropriate. In fact, inspecting the generated descriptions, we found that our method often indeed generates correct alternative descriptions. For instance, Darius Kaiser is described as a cyclist, but one could also describe him as a German bicycle racer. Despite their shortcomings, the aforementioned metrics have generally been found suitable for comparing supervised systems, in that systems with significantly higher scores tend to fare better at learning to reproduce ground truth captions."
  }, {
    "heading": "3.4 Results",
    "text": "The results of the experiments are reported in Table 1. Across all metrics, we observe that our model obtains significantly better scores than the alternatives.\nA facts-to-seq model exploiting our positional fact encoding performs adequately. With an additional attention mechanism (Facts-to-seq w. Attention), the results are even better. This is on account of the attention mechanism’s ability to reconsider the attention distribution at each time step using the current context of the output sequence. The results suggest that this enables the model to more flexibly focus on the most pertinent parts of the input. In this regard, such a model thus resembles our approach. However, there are important differences between this baseline and our model. Our model not only uses the current context of the output sequence, but also memorizes how information of a particular fact has been used thus far, via the dynamic memory module. We conjecture that the dynamic memory module thereby facilitates generating longer description sequences more accurately by better tracking which parts have been attended to, as is empirically corroborated by the comparably higher BLEU scores for longer n-grams.\nThe analysis of the Static Memory approach amounts to an ablation study, as it only differs from our full model in lacking memory updates. The divergence of scores between the two variants suggests that the dynamic memory indeed is vital for more dynamically attending to the facts by taking into account the current context of the output sequence at each step. Our model needs to dynam-\nically achieve different objectives at different time points. For instance, it may start off looking at several properties to infer a type of the appropriate granularity for the entity (e.g., village), while in the following steps it considers a salient property and emits the corresponding named entity for it as well as a suitable preposition (e.g., in China).\nFinally, the poor results of the DMN+ approach show that a naı̈ve application of a state-of-theart dynamic memory architecture does not suffice to obtain strong results on this task. Indeed, the DMN+ is even outperformed by our Facts-to-seq baseline. This appears to stem from the inability of the model to properly memorize all pertinent facts in its encoder.\nAnalysis. In Figure 3, we visualize the attention distribution over facts. We observe how the model shifts its focus to different sorts of properties while generating successive words.\nTable 2 provides a representative sample of the generated descriptions and their ground truth counterparts. A manual inspection reveals five distinct patterns. The first case is that of exact matches with the reference descriptions. The second involves examples on which there is a high overlap of words between the ground truth and generated descriptions, but the latter as a whole is incorrect because of semantic drift or other challenges. In some cases, the model may have never seen a word or named entity during training (e.g., Hypocrisy), or their frequency is very limited in the training set. While it has been shown that GRUs with an attention mechanism are capable of learning to copy random strings from the input (Gu et al., 2016), we conjecture that a dedicated copy mechanism may help to mitigate this problem, which we will explore in future research. In other cases, the model conflates semantically related concepts, as is evident from examples such as a film being described as a filmmaker and a polo player as a water polo player. Next, the third group involves generated descriptions that are more specific than the ground truth, but correct, while, in the fourth group, the generated outputs generalize the descriptions to a certain extent. For example, American musician and pianist is generalized as American musician, since musician is a hypernym of pianist. Finally, the last group consists of cases in which our model generated descriptions that are factually accurate and may be deemed appropriate despite diverging from the\nreference descriptions to an extent that almost no overlapping words are shared with them. Note that such outputs are heavily penalized by the metrics considered in our evaluation."
  }, {
    "heading": "4 Related Work",
    "text": "Type Prediction. There has been extensive work on predicting the ontological types of entities in large knowledge graphs (Neelakantan and Chang, 2015; Miao et al., 2016; Kejriwal and Szekely, 2017; Shimaoka et al., 2017), in semistructured resources such as Wikipedia (Ponzetto and Strube, 2007; de Melo and Weikum, 2010), as well as in text (Del Corro et al., 2015; Yaghoobzadeh and Schütze, 2015; Ren et al.,\n2016). However, the major shortcoming of these sorts of methods, including those aiming at more fine-grained typing, is that they assume that the set of candidate types is given as input, and the main remaining challenge is to pick the correct one(s). In contrast, our work yields descriptions that often indicate the type of entity, but typically are more natural-sounding and descriptive (e.g. French Impressionist artist) than the oftentimes abstract ontological types (such as human or artifact) chosen by type prediction methods.\nA separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text (Hearst, 1992; Snow et al., 2006), possibly also induc-\ning taxonomies from them (Poon and Domingos, 2010; Velardi et al., 2013; Bansal et al., 2014). However, these methods typically just need to select existing spans of text from the input as the output description.\nText Generation from Structured Data. Research on methods to generate descriptions for entities has remained scant. Lebret et al. (2016) take Wikipedia infobox data as input and train a custom form of neural language model that, conditioned on occurrences of words in the input table, generates biographical sentences as output. However, their system is limited to a single kind of description (biographical sentences) that tend to share a common structure. Wang et al. (2016) focus on the problem of temporal ordering of extracted facts. Biran and McKeown (2017) introduced a template-based description generation framework for creating hybrid concept-to-text and text-to-text generation systems that produce descriptions of RDF entities. Their framework can be tuned for new domains, but does not yield a broad-coverage multi-domain model. Voskarides et al. (2017) first create sentence templates for specific entity relationships, and then, given a new relationship instance, generate a description by selecting the best template and filling the template slots with the appropriate entities from the knowledge graph. Kutlak et al. (2013) generates referring expressions by converting property-value pairs to text using a hand-crafted mapping scheme. Wiseman et al. (2017) considered the related task of mapping tables with numeric basketball statistics to natural language. They investigated an extensive array of current state-of-the-art neural pointer methods but found that template-based models outperform all neural models on this task by a significant margin. However, their method requires specific templates for each domain (for example, basketball games in their case). Applying template-based methods to cross-domain knowledge bases is highly challenging, as this would require too many different templates for different types of entities. Our dataset contains items of from a large number of diverse domains such as humans, books, films, paintings, music albums, genes, proteins, cities, scientific articles, etc., to name but a few.\nChen and Mooney (2008) studied the task of taking representations of observations from a sports simulation (Robocup simulator) as input, e.g. pass(arg1=purple6, arg2=purple3), and gen-\nerating game commentary. Liang et al. (2009) learned alignments between formal descriptions such as rainChance(time=26-30,mode=Def) and natural language weather reports. Mei et al. (2016) used LSTMs for these sorts of generation tasks, via a custom coarse-to-fine architecture that first determines which input parts to focus on.\nMuch of the aforementioned work essentially involves aligning small snippets in the input to the relevant parts in the training output and then learning to expand such input snippets into full sentences. In contrast, in our task, alignments between parts of the input and the output do not suffice. Instead, describing an entity often also involves considering all available evidence about that entity to infer information about it that is often not immediately given. Rather than verbalizing facts, our method needs a complex attention mechanism to predict an object’s general type and consider the information that is most likely to appear salient to humans from across the entire input.\nThe WebNLG Challenge (Gardent et al., 2017) is another task for generating text from structured data. However, this task requires a textual verbalization of every triple. On the contrary, the task we consider in this work is quite complementary in that a verbalization of all facts one-by-one is not the sought result. Rather, our task requires synthesizing a short description by carefully selecting the most relevant and distinctive facts from the set of all available facts about the entity. Due to these differences, the WebNLG dataset was not suitable for the research question considered by our paper.\nNeural Text Summarization. Generating entity descriptions is related to the task of text summarization. Most traditional work in this area was extractive in nature, i.e. it selects the most salient sentences from a given input text and concatenates them to form a shorter summary or presents them differently to the user (Yang et al., 2017). Abstractive summarization goes beyond this in generating new text not necessarily encountered in the input, as is typically necessary in our setting. The surge of sequence-to-sequence modeling of text via LSTMs naturally extends to the task of abstractive summarization by training a model to accept a longer sequence as input and learning to generate a shorter compressed sequence as a summary.\nRush et al. (2015) employed this idea to generate a short headline from the first sentence of a text. Subsequent work investigated the use of\narchitectures such as pointer-generator networks to better cope with long input texts (See et al., 2017). Recently, Liu et al. (2018) presented a model that generates an entire Wikipedia article via a neural decoder component that performs abstractive summarization of multiple source documents. Our work differs from such previous work in that we do not consider a text sequence as input. Rather, our input are a series of entity relationships or properties, as reflected by our facts-to-sequence baselines in the experiments. Note that our task is in certain respects also more difficult than text summarization. While regular neural summarizers are often able to identify salient spans of text that can be copied to the output, our input is of a substantially different form than the desired output.\nAdditionally, our goal is to make our method applicable to any entity with factual information that may not have a corresponding Wikipedia-like article available. Indeed, Wikidata currently has 46 million items, whereas the English Wikipedia has only 5.6 million articles. Hence, for the vast majority of items in Wikidata, no corresponding Wikipedia article is available. In such cases, a summarization baseline will not be effective.\nEpisodic Memory Architectures. A number of neural models have been put forth that possess the ability to interact with a memory component. Recent advances in neural architectures that combine memory components with an attention mechanism exhibit the ability to extract and reason over factual information. A well-known example is the End-To-End Memory Network model by Sukhbaatar et al. (2015), which may make multiple passes over the memory input to facilitate multi-hop reasoning. These have been particularly successful on the bAbI test suite of artificial comprehension tests (Weston et al., 2015), due to their ability to extract and reason over the input.\nAt the core of the Dynamic Memory Networks (DMN) architecture (Kumar et al., 2016) is an episodic memory module, which is updated at each episode with new information that is required to answer a predefined question. Our approach shares several commonalities with DMNs, as it is also endowed with a dynamic memory of this sort. However, there are also a number of significant differences. First of all, DMN and its improved version DMN+ (Xiong et al., 2016) assume sequential correlations between the sentences and\nrely on them for reasoning purposes. To this end, DMN+ needs an additional layer of GRUs, which is used to capture sequential correlations among sentences. Our model does not need any such layer, as facts in a knowledge graph do not necessarily possess any sequential interconnections. Additionally, DMNs assume a predefined number of memory episodes, with the final memory state being passed to the answer module. Unlike DMNs, our model uses the dynamic context of the output sequence to update the memory state. The number of memory updates in our model flexibly depends on the length of the generated sequence. DMNs also have an additional question module as input, which guides the memory updates and also the output, while our model does not leverage any such guiding factor. Finally, in DMNs, the output is typically a unigram, whereas our model emits a sequence of words."
  }, {
    "heading": "5 Conclusion",
    "text": "Short textual descriptions of entities facilitate instantaneous grasping of key information about entities and their types. Generating them from facts in a knowledge graph requires not only mapping the structured fact information to natural language, but also identifying the type of entity and then discerning the most crucial pieces of information for that particular type from the long list of input facts and compressing them down to a highly succinct form. This is very challenging in light of the very heterogeneous kinds of entities in our data.\nTo this end, we have introduced a novel dynamic memory-based neural architecture that updates its memory at each step to continually reassess the relevance of potential input signals. We have shown that our approach outperforms several competitive baselines. In future work, we hope to explore the potential of this architecture on further kinds of data, including multimodal data (Long et al., 2018), from which one can extract structured signals. Our code and data is freely available.2"
  }, {
    "heading": "Acknowledgments",
    "text": "This research is funded in part by ARO grant no. W911NF-17-C-0098 as part of the DARPA SocialSim program.\n2https://github.com/kingsaint/ Open-vocabulary-entity-type-description"
  }],
  "year": 2018,
  "references": [{
    "title": "Structured learning for taxonomy induction with belief propagation",
    "authors": ["Mohit Bansal", "David Burkett", "Gerard de Melo", "Dan Klein."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2014
  }, {
    "title": "Domainadaptable hybrid generation of RDF entity descriptions",
    "authors": ["Or Biran", "Kathleen McKeown."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 -",
    "year": 2017
  }, {
    "title": "Learning to sportscast: A test of grounded language acquisition",
    "authors": ["David L. Chen", "Raymond J. Mooney."],
    "venue": "Proceedings of the 25th International Conference on Machine Learning. ACM, New York, NY, USA, ICML ’08, pages 128–135.",
    "year": 2008
  }, {
    "title": "MENTA: Inducing multilingual taxonomies from Wikipedia",
    "authors": ["Gerard de Melo", "Gerhard Weikum."],
    "venue": "Jimmy Huang, Nick Koudas, Gareth Jones, Xindong Wu, Kevyn Collins-Thompson, and Aijun An, editors, Proceedings of the 19th ACM Conference",
    "year": 2010
  }, {
    "title": "FINET: Context-aware fine-grained named entity typing",
    "authors": ["Luciano Del Corro", "Abdalghani Abujabal", "Rainer Gemulla", "Gerhard Weikum."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
    "year": 2015
  }, {
    "title": "Introducing Wikidata to the Linked Data Web",
    "authors": ["Fredo Erxleben", "Michael Günther", "Markus Krötzsch", "Julian Mendez", "Denny Vrandečić."],
    "venue": "Peter Mika, Tania Tudorache, Abraham Bernstein, Chris Welty, Craig A. Knoblock, Denny Vrandečić,",
    "year": 2014
  }, {
    "title": "The webnlg challenge: Generating text from rdf data",
    "authors": ["Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini."],
    "venue": "Proceedings of the 10th International Conference on Natural Language Generation.",
    "year": 2017
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
    "year": 2016
  }, {
    "title": "Automatic acquisition of hyponyms from large text corpora",
    "authors": ["Marti A. Hearst."],
    "venue": "COLING.",
    "year": 1992
  }, {
    "title": "Supervised typing of big graphs using semantic embeddings",
    "authors": ["Mayank Kejriwal", "Pedro Szekely."],
    "venue": "CoRR abs/1703.07805. http://arxiv.org/abs/1703.07805.",
    "year": 2017
  }, {
    "title": "Ask me anything: Dynamic memory networks for natural language processing",
    "authors": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."],
    "venue": "Maria Flo-",
    "year": 2016
  }, {
    "title": "Generation of referring expressions in large domains",
    "authors": ["Roman Kutlak", "Kees van Deemter", "Christopher Stuart Mellish"],
    "year": 2013
  }, {
    "title": "Generating text from structured data with application to the biography domain",
    "authors": ["Rémi Lebret", "David Grangier", "Michael Auli."],
    "venue": "CoRR abs/1603.07771. http://arxiv.org/abs/1603.07771.",
    "year": 2016
  }, {
    "title": "Learning semantic correspondences with less supervision",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural",
    "year": 2009
  }, {
    "title": "Generating Wikipedia by summarizing long sequences",
    "authors": ["Peter Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer."],
    "venue": "CoRR abs/1801.10198. http://arxiv.org/abs/1801.10198.",
    "year": 2018
  }, {
    "title": "Video captioning with multi-faceted attention",
    "authors": ["Xiang Long", "Chuang Gan", "Gerard de Melo."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL) 6:173–184. https://transacl.org/ojs/index.php/tacl/article/view/1289.",
    "year": 2018
  }, {
    "title": "What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment",
    "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."],
    "venue": "Proceedings of NAACL.",
    "year": 2016
  }, {
    "title": "Relevance of unsupervised",
    "authors": ["Jeremie Zumer"],
    "year": 2017
  }, {
    "title": "Neural architectures for",
    "authors": ["Sebastian Riedel"],
    "year": 2017
  }, {
    "title": "Semantic taxonomy induction from heterogenous evidence",
    "authors": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."],
    "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meet-",
    "year": 2006
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."],
    "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-",
    "year": 2014
  }, {
    "title": "Knowlywood: Mining activity knowledge from Hollywood narratives",
    "authors": ["Niket Tandon", "Gerard de Melo", "Abir De", "Gerhard Weikum."],
    "venue": "Proceedings of CIKM 2015.",
    "year": 2015
  }, {
    "title": "From Freebase to Wikidata: The great migration",
    "authors": ["Thomas Pellissier Tanon", "Denny Vrandečić", "Sebastian Schaffert", "Thomas Steiner", "Lydia Pintscher."],
    "venue": "World Wide Web Conference.",
    "year": 2016
  }, {
    "title": "OntoLearn reloaded: A graphbased algorithm for taxonomy induction",
    "authors": ["Paola Velardi", "Stefano Faralli", "Roberto Navigli."],
    "venue": "Computational Linguistics 39(3):665–707. https://doi.org/10.1162/COLI a 00146.",
    "year": 2013
  }, {
    "title": "Generating descriptions of entity relationships",
    "authors": ["Nikos Voskarides", "Edgar Meij", "Maarten de Rijke."],
    "venue": "ECIR 2017: 39th European Conference on Information Retrieval. Springer, LNCS.",
    "year": 2017
  }, {
    "title": "Summary generation for temporal extractions",
    "authors": ["Yafang Wang", "Zhaochun Ren", "Martin Theobald", "Maximilian Dylla", "Gerard de Melo."],
    "venue": "Proceedings of 27th International Conference on Database and Expert Systems Applications (DEXA 2016).",
    "year": 2016
  }, {
    "title": "A comparison of hard filters and soft evidence for answer typing in Watson",
    "authors": ["Chris Welty", "J. William Murdock", "Aditya Kalyanpur", "James Fan."],
    "venue": "Philippe Cudré-Mauroux, Jeff Heflin,",
    "year": 2012
  }, {
    "title": "Towards aicomplete question answering: A set of prerequisite toy tasks",
    "authors": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."],
    "venue": "CoRR abs/1502.05698. http://arxiv.org/abs/1502.05698.",
    "year": 2015
  }, {
    "title": "Challenges in data-to-document generation",
    "authors": ["Sam Wiseman", "Stuart Shieber", "Alexander Rush."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "Dynamic memory networks for visual and textual question answering",
    "authors": ["Caiming Xiong", "Stephen Merity", "Richard Socher."],
    "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning -",
    "year": 2016
  }, {
    "title": "Corpus-level fine-grained entity typing using contextual information",
    "authors": ["Yadollah Yaghoobzadeh", "Hinrich Schütze."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
    "year": 2015
  }, {
    "title": "HiText: Text reading with dynamic salience marking",
    "authors": ["Qian Yang", "Yong Cheng", "Sen Wang", "Gerard de Melo."],
    "venue": "Proceedings of WWW 2017 (Digital Learning Track). ACM.",
    "year": 2017
  }],
  "id": "SP:db1551a91b2231696df3baffe8ccd5c39d3bb444",
  "authors": [{
    "name": "Rajarshi Bhowmik",
    "affiliations": []
  }, {
    "name": "Gerard de Melo",
    "affiliations": []
  }],
  "abstractText": "While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graph entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.",
  "title": "Generating Fine-Grained Open Vocabulary Entity Type Descriptions"
}