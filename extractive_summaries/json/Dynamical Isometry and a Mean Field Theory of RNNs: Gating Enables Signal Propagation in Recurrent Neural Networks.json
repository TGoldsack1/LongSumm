{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986; Elman, 1990) have found widespread use across a variety of domains from language modeling (Mikolov et al., 2010; Kiros et al., 2015; Jozefowicz et al., 2016) and machine translation (Bahdanau et al., 2014) to speech recogni-\n*Equal contribution 1Google 2Google Brain. Correspondence to: Minmin Chen <minminc@google.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ntion (Graves et al., 2013) and recommendation systems (Hidasi et al., 2015; Wu et al., 2017). However, RNNs as originally proposed are difficult to train and are rarely used in practice. Instead, variants of RNNs - such as Long ShortTerm Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Chung et al., 2014) - that feature various forms of “gating” perform significantly better than their vanilla counterparts. Often, these models must be paired with techniques such as normalization layers (Ioffe & Szegedy, 2015b; Ba et al., 2016) and gradient clipping (Pascanu et al., 2013) to achieve good performance.\nA rigorous explanation for the remarkable success of gated recurrent networks remains illusive (Jozefowicz et al., 2015; Greff et al., 2017). Recent work (Collins et al., 2016) provides empirical evidence that the benefits of gating are mostly rooted in improved trainability rather than increased capacity or expressivity. The problem of disentangling trainability from expressivity is widespread in machine learning since state-of-the-art architectures are nearly always the result of sparse searches in high dimensional spaces of hyperparameters. As a result, we often mistake trainability for expressivity. Seminal early work (Glorot & Bengio; Bertschinger et al.) showed that a major hindrance to trainability was the vanishing and exploding of gradients.\nRecently, progress has been made in the feed-forward setting (Schoenholz et al., 2017; Pennington et al., 2017; Yang & Schoenholz, 2017) by developing a theory of both the forward-propagation of signal and the backwardpropagation of gradients. This theory is based on studying neural networks whose weights and biases are randomly distributed. This is equivalent to studying the behavior of neural networks after random initialization or, equivalently, to studying the prior over functions induced by a particular choice of hyperparameters (Lee et al., 2017). It was shown that randomly initialized neural networks are trainable if three conditions are satisfied: (1) the size of the output of the network is finite for finite inputs, (2) the output of the network is sensitive to changes in the input, and (3) gradients neither explode nor vanish. Moreover, neural networks achieving dynamical isometry, i.e. having input-output Jacobian matrices that are well-conditioned, were shown to\ntrain orders of magnitude faster than networks that do not.\nIn this work, we combine mean field theory and random matrix theory to extend these results to the recurrent setting. We will be particularly focused on understanding the role that gating plays in trainability. As we will see, there are a number of subtleties that must be addressed for (gated) recurrent networks that were not present in the feed-forward setting. To clarify the discussion, we will therefore contrast vanilla RNNs with a gated RNN cell, that we call the minimalRNN, which is significantly simpler than LSTMs and GRUs but implements a similar form of gating. We expect the framework introduced here to be applicable to more complicated gated architectures.\nThe first main contribution of this paper is the development of a mean field theory for forward propagation of signal through vanilla RNNs and minimalRNNs. In doing so, we identify a theory of the maximum timescale over which signal can propagate in each case. Next, we produce a random matrix theory for the end-to-end Jacobian of the minimalRNN. As in the feed-forward setting, we establish that the duality between the forward propagation of signal and the backward propagation of gradients persists in the recurrent setting. We then show that our theory is indeed predictive of trainability in recurrent neural networks by comparing the maximum trainable number of steps of RNNs with the timescale predicted by the theory. Overall, we find remarkable alignment between theory and practice. Additionally, we develop a closed-form initialization procedure for both networks and show that on a variety of tasks RNNs initialized to be dynamically isometric are significantly easier to train than those lacking this property.\nCorroborating the experimental findings of Collins et al. (2016), we show that both signal propagation and dynamical isometry in vanilla RNNs is far more precarious than in the case of the minimalRNN. Indeed the vanilla RNN achieves dynamical isometry only if the network is initialized with orthogonal weights at the boundary between order-and-chaos, a one-dimensional line in parameter space. Owing to its gating mechanism, the minimalRNN on the other hand enjoys a robust multi-dimensional subspace of good initializations which all enable dynamical isometry. Based on these insights, we conjecture that more complex gated recurrent neural networks also benefit from the similar effects."
  }, {
    "heading": "2. Related Work",
    "text": "Identity and Orthogonal initialization schemes have been identified as a promising approach to improve trainability of deep neural networks (Le et al., 2015; Mishkin & Matas, 2015). Additionally, Arjovsky et al. (2016); Hyland & Rätsch (2017); Xie et al. (2017) advocate going beyond initialization to constrain the transition matrix to be orthog-\nonal throughout the entire learning process either through re-parametrisation or by constraining the optimization to the Stiefel manifold (Wisdom et al., 2016). However, as was pointed out in Vorontsov et al. (2017), strictly enforcing orthogonality during training may hinder training speed and generalization performance. While these contributions are similar to our own, in the sense that they attempt to construct networks that feature dynamical isometry, it is worth noting that orthogonal weight matrices do not guarantee dynamical isometry. This is due to the nonlinear nature of deep neural networks as shown in Pennington et al. (2017). In this paper we continue this trend and show that orthogonality has little impact on the conditioning of the Jacobian (and so trainability) in gated RNNs.\nThe notion of “edge of chaos” initialization has been explored previously especially in the case of recurrent neural networks. Bertschinger et al.; Glorot & Bengio propose edge-of-chaos initialization schemes that they show leads to improved performance. Additionally, architectural innovations such as batch normalization (Ioffe & Szegedy, 2015a), orthogonal matrix initialization (Saxe et al., 2013), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015) all share a common goal of stabilizing gradients and improving training dynamics.\nThere is a long history of applying mean field-like approaches to understand the behavior of neural networks. Indeed several pieces of seminal work used statistical physics (Derrida & Pomeau; Sompolinsky et al., 1988) and Gaussian Processes (Neal, 2012) to show that neural networks exhibit remarkable regularity as the width of the network gets large. Mean field theory also has long been used to study Boltzmann machines (Ackley et al.) and sigmoid belief networks (Saul et al., 1996). More recently, there has been a revitalization of mean field theory to explore questions of trainability and expressivity in fully-connected networks and residual networks (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018). Our approach will closely follow these later contributions and extend many of their techniques to the case of recurrent networks with gating. Beyond mean field theory, there have been several attempts in understanding signal propagation in RNNs, e.g., using Gers̆gorin circle theorem (Zilly et al., 2016) or time invariance (Tallec & Ollivier, 2018)."
  }, {
    "heading": "3. Theory and Critical Initialization",
    "text": "We begin by developing a mean field theory for vanilla RNNs and discuss the notion of dynamical isometry. Afterwards, we move on to a simple gated architecture to explain the role of gating in facilitating signal propagation in RNNs."
  }, {
    "heading": "3.1. Vanilla RNN",
    "text": "Vanilla RNNs are described by the recurrence relation,\net = Wht 1 + V xt + b ht = (et). (1)\nHere xt 2 RM is the input, et 2 RN is the pre-activation, and ht 2 RN is the hidden state after applying an arbitrary activation function : R ! R. For the purposes of this discussion we set = tanh. Furthermore, W 2 RN⇥N and V 2 RN⇥M are weight matrices that multiply the hidden state and inputs respectively and b 2 RN is a bias.\nNext, we apply mean-field theory to vanilla RNNs following a similar strategy introduced in (Poole et al., 2016; Schoenholz et al., 2017). At the level of mean-field theory, vanilla RNNs will prove to be intimately related to feed-forward networks and so this discussion proceeds analogously. For a more detailed discussion, see these earlier studies.\nConsider two sequences of inputs {xt1} and {xt2}, described by the covariance matrix Rt 2 R2⇥2 with Rtab = 1 M E[xa · xb], a, b 2 {1, 2}. To simplify notation, we assume the input sequences have been standardized so that Rt11 = R t 22 = R independent of time. This allows us to write Rt = R⌃t, where ⌃t is a matrix whose diagonal terms are 1 and whose off-diagonal terms are the cosine similarity between the inputs at time t. These sequences are then passed into two identical copies of an RNN to produce two corresponding pre-activation sequences {et1} and {et2}. As in Poole et al. (2016) we let the weights and biases be Gaussian distributed so that Wij ⇠ N (0, 2w/N), Vij ⇠ N (0, 2v/M), and bi ⇠ N (µb, 2b )\n1, and we consider the wide network limit, N !1. As in the fully-connected setting, we would like to invoke the Central Limit Theorem (CLT) to conclude that the pre-activations of hidden states are jointly Gaussian distributed. Unfortunately, the CLT is violated in the recurrent setting as ht 1 is correlated with W due to weight sharing between steps of the RNN.\nTo make progress, we proceed by developing the theory of signal propagation for RNNs with untied weights. This allows for several simplifications, including the application of the CLT to conclude that etia are jointly Gaussian distributed,\n[eti1, e t j2] T ⇠ N (µb1, q t ij), i, j 2 {1, · · · , N}\nwhere the covariance matrix qt 2 R2⇥2 is independent of neuron index, i. We explore the ramifications of this approximation by comparing simulations of RNNs with tied and untied weights. Overall, we will see that while ignoring weight tying leads to quantitative differences between theory and experiment, it does not change the qualitative picture that emerges. See figs. 1 and 2 for verification.\nWith this approximation in mind, we will now quantify how the pre-activation hidden states {et1} and {et2} evolve by\n1in practice we will set µb = 0 for vanillaRNN.\nderiving the recurrence relation of the covariance matrix qt from the recurrence on et in eq. (1). Using identical arguments to Poole et al. (2016) one can show that,\nqt = 2w Z Dqt 1z (z) (z) > + 2vR⌃ t + 2bI. (2)\nwhere z = [z1, z2]>, and Z\nDqz = 1\n2⇡ p |q|\nZ dze (z µb1) T q 1(z µb1) (3)\nis a Gaussian measure with covariance matrix q. By symmetry, our normalization allows us to define qt11 = qt22 = qt to be the magnitude of the pre-activation hidden state and ct = qt12/q\nt to be the cosine similarity between the hidden states. We will be particularly concerned with understanding the dynamics of the cosine similarity, ct.\nIn feed-forward networks, the inputs dictate the initial value of the cosine similarity, c0 and then the evolution of ct is determined solely by the network architecture. By contrast in recurrent networks, inputs perturb ct at each timestep. Analyzing the dynamics of ct for arbitrary ⌃t is therefore challenging, however significant insight can be gained by studying the off-diagonal entries of eq. (2) for ⌃t = ⌃ independent of time. In the case of time-independent ⌃t, as t ! 1 both qt ! q⇤ and ct ! c⇤ where q⇤ and c⇤ are fixed points of the variance of the pre-activation hidden state and the cosine-similarity between pre-activation hidden states respectively. As was discussed previously (Poole et al., 2016; Schoenholz et al., 2017), the dynamics of qt are generally uninteresting provided q⇤ is finite. We therefore choose to normalize the hidden state such that q0 = q⇤ which implies that qt = q⇤ independent of time.\nIn this setting it was shown in Schoenholz et al. (2017) that in the vicinity of a fixed point, the off-diagonal term in eq. (2) can be expanded to lowest order in ✏t = c⇤ ct to give the linearized dynamics, ✏t = c⇤✏t 1 where\nc⇤ = 2 w Z Dq⇤z 0(z1) 0(z2). (4)\nThese dynamics have the solution ✏t = t t0c⇤ ✏t0 where t0 is the time when ct is close enough to c⇤ for the linear approximation to be valid. If c⇤ < 1 it follows that ct approaches c⇤ exponentially quickly over a timescale ⌧ = 1/ log c⇤ and c⇤ is called a stable fixed point. When ct gets too close to c⇤ to be distinguished from it to within numerical precision, information about the initial inputs has been lost. Thus, ⌧ sets the maximum timescale over which we expect the RNN to be able to remember information. If c⇤ > 1 then ct gets exponentially farther from c⇤ over time and c⇤ is an unstable fixed point. In this case, for the activation function considered here, another fixed point that\nis stable will emerge. Note that c⇤ is independent of ⌃ and so the dynamics of ct near c⇤ do not depend on ⌃.\nIn vanilla fully-connected networks c⇤ = 1 is always a fixed point of ct, but it is not always stable. Indeed, it was shown that these networks exhibit a phase transition where c⇤ = 1 goes from being a stable fixed point to an unstable one as a function of the network’s hyperparameters. This is known as the order-to-chaos transition and it occurs exactly when 1 = 1. Since ⌧ = 1/ log( 1), signal can propagate infinitely far at the boundary between order and chaos. Comparing the diagonal and off-diagonal entries of eq. (2), we see that in recurrent networks, c⇤ = 1 is a fixed point only when ⌃12 = 1, and in this case the discussion is identical to the feed-forward setting. When ⌃12 < 1, it is easy to see that c⇤ < 1 since if ct = 1 at some time t then ct+1 = 1 2vR(1 ⌃12)/q\n⇤ < 1. We see that in recurrent networks noise from the inputs destroys the ordered phase and there is no ordered-to-chaos critical point. As a result we should expect the maximum timescale over which memory may be stored in vanilla RNNs to be fundamentally limited by noise from the inputs.\nThe end-to-end Jacobian of a vanilla RNN with untied weights is in fact formally identical to the input-output Jacobian of a feedforward network, and thus the results from (Pennington et al., 2017) regarding conditions for dynamical isometry apply directly. In particular, dynamical isometry is achieved with orthogonal state-to-state transition matrices W , tanh non-linearities, and small values of q⇤. Perhaps surprisingly, these conclusions continue to be valid if the assumption of untied weights is relaxed. To understand why this is the case, consider the example of a linear network. For untied weights, the end-to-end Jacobian is given by Ĵ = QT t=1 Wt, while for tied weights the Jacobian is given by J = W T . It turns out that as N !1 there is sufficient self-averaging to overcome the dependencies induced by weight tying and the asymptotic singular value distributions of Ĵ and J are actually identical (Haagerup & Larsen, 2000)."
  }, {
    "heading": "3.2. MinimalRNN",
    "text": ""
  }, {
    "heading": "3.2.1. MEAN-FIELD THEORY",
    "text": "To study the role of gating, we introduce the minimalRNN which is simpler than other gated RNN architectures but nonetheless features the same gating mechanism. A sequence of inputs xt 2 RM , is first mapped to the hidden space through x̃t = (xt)2. From here on, we refer to x̃t 2 RN as the inputs to minimalRNN. The minimalRNN\n2 (·) here can be any highly flexible functions such as a feed-forward network. In our experiments, we take (·) to be a fully connected layer with tanh activation, that is, (xt) = tanh(Wxx t).\nis then described by the recurrence relation,\net = Wht 1 + V x̃t + b ut = (et) (5)\nht = ut ht 1 + (1 ut) x̃t\nwhere et 2 RN is the pre-activation to the gating function, ut 2 RN the update gate and ht 2 RN the hidden state. The minimalRNN retains the most essential gate in LSTMs (Jozefowicz et al., 2015; Greff et al., 2017) and achieves competitive performance. The simplified update of this cell on the other hand, enables us to pinpoint the role of gating in a more controlled setting.\nAs in the previous case we consider two sequences of inputs to the network, {x̃t1} and {x̃t2}. We take Wij ⇠ N (0, 2w/N), Vij ⇠ N (0, 2v/N) and bi ⇠ N (µb, 2b ). By analogy to the vanilla case, we can make the mean field approximation that the etia are jointly Gaussian distributed with covariance matrix qt ij 2 R2⇥2. Here,\nqt = 2wQ t 1 + 2vR t + 2bI (6)\nwhere we have defined Qt as the second-moment matrix with Qtab = E[htiahtib]. 3 As in the vanilla case, Rt is the covariance between inputs so that Rtab = 1 N E[x̃a · x̃b].\nWe note that Rt is fixed by the input, but it remains for us to work out Qt. We find that (see SI section B),\nQt = Qt 1 Z Dqtz (z) (z) > (7)\n+ Rt Z Dqtz (1 (z))(1 (z)) >\nHere we assume that the expectation factorizes so that ht 1 and ut are approximately independent. We believe this approximation becomes exact in the N !1 limit.\nWe choose to normalize the data in a similar manner to the vanilla case so that Rt11 = Rt22 = R independent of time. An immediate consequence of this normalization is that Qt11 = Qt22 = Qt and qt11 = qt22 = qt. We then write Ct = Qt12/Q\nt and ct = qt12/qt as the cosine similarities between the hidden states and the pre-activations respectively. With this normalization, we can work out the mean-field recurrence relation characterizing the covariance matrix for the minimalRNN. This analysis can be done by deriving the recurrence relation for either Qt or qt. We will choose to study the dynamics of qt, however the two are trivially related by eq. (6). In SI section C, we analyze the dynamics of the diagonal term in the recurrence relation and prove that there is always a fixed point at some q⇤. In SI section D, we compute the depth scale over which qt approaches q⇤. However, as in the case of the vanillaRNN, the dynamics of q⇤ are generally uninteresting.\n3ht will be centered under mean field approximation if h0 is initialized with mean zero.\nWe now turn our attention to the dynamics of the cosine similarity between the pre-activations, ct. As in the case of vanilla RNNs, we note that qt approaches q⇤ quickly relative to the dynamics of ct. We therefore choose to normalize the hidden state of the RNN so that Q0 = Q⇤ in which case both Qt = Q⇤ and qt = q⇤ independent of time. From eq. (6) and (7) it follows that the cosine similarity of the pre-activation evolves as,\nct = ⇥ ct 1 + ( 2w\n2 v)⇢\nt 1⇤ Z\nDqt 1z (z1) (z2)\n2 2w⇢ t 1 Z Dqt 1z (z1) + 2 w⇢ t 1 + 2v⇢ t (8)\nwhere we have defined ⇢t = R⌃t12/q⇤. As in the case of the vanilla RNN, we can study the behavior of ct in the vicinity of a fixed point, c⇤. By expanding eq. (8) to lowest order in ✏t = c⇤ ct we arrive at a linearized recurrence relation that has an exponential solution ✏t+1 = c⇤✏t where here,\nc⇤ = Z Dq⇤z (z1) (z2) (9)\n+ q⇤c⇤ + ( 2w\n2 v)R⌃12 Z Dq⇤z 0(z1) 0(z2).\nThe discussion above in the vanilla case carries over directly to the minimalRNN with the appropriate replacement of c⇤ . Unlike in the case of the vanilla RNN, here we see that c⇤ itself depends on ⌃12.\nAgain c⇤ = 1 is a fixed point of the dynamics only when ⌃12 = 1. In this case, the minimalRNN experiences an order-to-chaos phase transition when 1 = 1 at which point the maximum timescale over which signal can propagate goes to infinity. Similar to the vanilla RNN, when ⌃12 < 1, we expect that the phase transition will be destroyed and the maximum duration of signal propagation will be severely limited. However, in a significant departure from the vanilla case, when µb ! 1 we notice that (z + µb) ! 1, and 0(z +µb)! 0 for all z. Considering eq. (9) we notice that in this regime c⇤ ! 1 independent of ⌃12. In other words, gating allows for arbitrarily long term signal propagation in recurrent neural networks independent of ⌃12.\nWe explore agreement between our theory and MC simulations of the minimalRNN in fig. 1. In this set of experiments, we consider inputs such that ⌃t12 = 0 for t < 10 and ⌃t12 = 1 for t 10. Fig. 1 (a,c,d) show excellent quantitative agreement between our theory and MC simulations. In fig. 1 (a,b) we compare the MC simulations of the minimalRNN with and without weight tying. While we observe that for many choices of hyperparameters the untied weight approximation is quite good (particularly when c⇤ ⇡ 1), deeper into the chaotic phase the quantitative agreement between breaks down. Nonetheless, we observe that the untied approximation describes the qualitative behavior of the real\nminimalRNN overall. In fig. 1 (e) we plot the timescale for signal propagation for ⌃12 = 1, 0.99, and 0 for the minimalRNN with identical choices of hyperparameters. We see that while ⌧ ! 1 as µb gets large independent of ⌃12, a critical point at µb = 0 is only observed when ⌃12 = 1."
  }, {
    "heading": "3.2.2. DYNAMICAL ISOMETRY",
    "text": "In the previous subsection, we derived a quantity 1 that defines the boundary between the ordered and the chaotic phases of forward propagation. Here we show that it also defines the boundary between exploding and vanishing gradients. To see this, consider the Jacobian of the state-to-state transition operator,\nJt = @ht+1\n@ht = Dut + D 0(et) (ht 1 zt)W , (10)\nwhere Dx denotes a diagonal matrix with x along its diagonal. We can compute the expected norm-squared of back-propagated error signals, which measures the growth or shrinkage of gradients. It is equal to the mean-squared singular value of the Jacobian (Poole et al., 2016; Schoenholz et al., 2017) or the first moment of JtJTt ,\n1\nN E[tr(JtJTt )] = E[(ut1)2] + 2wE[ 0(et1)2(ht 11 zt1)2],\n(11) where we have used the fact that the elements of ut, ht and zt are i.i.d. Since we assume convergence to the fixed point, these distributions are independent of t and it is easy to see that 1N E[tr(JtJ T t )] = 1. The variance of backpropagated error signals through T time steps is therefore 1 N E[tr(JJ\nT )] = T1 . As such, the constraint 1 = 1 defines the boundary between phases of exponentially exploding and exponentially vanishing gradient norm (variance). Note that unlike in the case of forward signal propagation, in the case of backpropagation this is independent of ⌃.\nAs argued in (Pennington et al., 2017; 2018), controlling the variance of back-propagated gradients is necessary but not sufficient to guarantee trainability, especially for very deep networks. Beyond the first moment, the entire distribution of eigenvalues of JJT (or of singular values of J) is relevant. Indeed, it was found in (Pennington et al., 2017; 2018) that enabling dynamical isometry, namely the condition that all singular values of J are close to unity, can drastically improve training speed for very deep feed-forward networks.\nFollowing (Pennington et al., 2017; 2018), we use tools from free probability theory to compute the variance 2JJT of the limiting spectral density of JJT ; however, unlike previous work, in our case the relevant matrices are not symmetric and therefore we must invoke tools from nonHermitian free probability, see (Cakmak, 2012) for a review. As in previous section, we make the simplifying assumption that the weights are untied, relying on the same motivations\ngiven in section 3.1. Using these tools, an un-illuminating calculation reveals that,\n2JJT = 2T 1\n✓ 1 + T 2(µ1 s1)µ2 + 21 + 2 2\n21\n◆ , (12)\nwhere,\n1 = µ1 + µ2 (13)\nµ1 =\nZ Dz 2( p q⇤z + µb)\n21 = µ 2 1 +\nZ Dz 4( p q⇤z + µb)\nµ2 = 2 w(Q ⇤ + R)\nZ Dz [ 0( p q⇤z + µb)] 2\n22 = µ 2 2 + 4 w((Q ⇤)2 + R2)\nZ Dz [ 0( p q⇤z + µb)] 4\nand s1 is the first term in the Taylor expansion of the Stransform of the eigenvalue distribution of WWT (Pennington et al., 2018). For example, for Gaussian matrices, s1 = 1 and for orthogonal matrices s1 = 0.\nSome remarks are in order about eq. (12). First, we note the duality between the forward and backward signal propagation (eq. (9) and eq. (13)). For critical initializations, 1 = 1, so 2JJT does not grow exponentially, but it still grows linearly with T . This situation is entirely analogous to the feed-forward analysis of (Pennington et al., 2017; 2018). In the case of the vanilla RNN, the coefficient of the linear term is proportion to q⇤, and can only be reduced by taking the weight and bias variances ( 2w, 2b )! (1, 0). A crucial difference in the minimalRNN is that the coefficient of the linear term can be made arbitrarily small by simply adjusting the bias mean µb to be positive, which will send µ2 ! 0 and µ1 ! 1 independent of ⌃. Therefore the conditions for dynamical isometry decouple from the weight\nand bias variances, implying that trainability can occur for a higher-dimensional, more robust, slice of parameter space. Moreover, the value of s1 has no effect on the capacity of the minimalRNN to achieve dynamical isometry. We believe these are fundamental reasons why gated cells such as the minimalRNN perform well in practice.\nAlgorithm 1 describes the procedure to find 2w, 2v and 2b to achieve 1 condition for minimalRNN. Given 2w, 2v , 2b , we then construct the weight matrices and biases accordingly. Q⇤ is used to initialize the h0 to avoid transient phase.\nAlgorithm 1 Critical initialization for minimalRNNs Require: q⇤, µb, R\n1: E[u2] R Dz 2( p q⇤z + µb) 2: E[(1 u)2] R Dz (1 ( p q⇤z + µb)) 2 3: Q⇤ R · E[(1 u)2]/(1 E[u2]) (eq.(7)) 4: E[u02] R Dz[ 0( p q⇤z + µb)]2 5: 2w (1 E[u2])/(Q⇤ + R)/E[u02] (eq.(13)) 6: 2b 0 7: 2v (q ⇤ Q⇤ 2w 2 b )/R"
  }, {
    "heading": "4. Experiments",
    "text": "Having established a theory for the behavior of random vanilla RNNs and minimalRNNs, we now discuss the connection between our theory and trainability in practice. We begin by corroborating the claim that the maximum timescale over which memory can be stored in a RNN is controlled by the timescale ⌧ identified in the previous section. We will then investigate the role of dynamical isometry in speeding up learning.\n6⌧ 6⌧ 6⌧ 6⌧"
  }, {
    "heading": "4.1. Trainability",
    "text": "Dataset. To verify the results of our theoretical calculation, we consider a task that is reflective of the theory above. To that end, we constructed a sequence dataset for training RNNs from MNIST (LeCun et al., 1998). Each of the 28⇥ 28 digit image is flattened into a vector of 784 pixels and sent as the first input to a RNN. We then send T random inputs xt ⇠ N (0, 2x), 0 < t < T into the RNN varying T between 10 and 1000 steps. As the only salient information about the digit is in the first layer, the network will need to propagate information through T layers to accurately identify the MNIST digit. The random inputs are drawn independently for each example and so this is a regime where ⌃t = 0 for all t > 0.\nWe then performed a series of experiments on this task to make connection with our theory. In each case we experimented with both tied and untied weights. The result are shown in fig. 2. In the case of untied weights, we observe strong quantitative agreement between our theoretical prediction for ⌧ and the maximum depth T where the network is still trainable. When the weights of the network are tied, we observe quantitative deviations between our thoery and experiments, but the overall qualitative picture remains.\nWe train vanilla RNNs for 103 steps (around 10 epochs) varying w 2 [0.5, 1.5] while fixing v = 0.025. The results of this experiment are shown in fig. 2 (a-b). We train minimalRNNs for 102 steps (around 1 epoch) fixing v = 1.39. We perform three different experiments here: 1)\nvarying µb 2 [ 4, 8] with w = 6.88 shown in fig. 2 (c-d), 2) varying w 2 [0.5, 10] with µb = 4 shown in fig. 2 (e-f), 3) varying w 2 [0.5, 10] with µb = 6 shown in fig. 2 (g-h). Comparing fig. 2(a,b) with fig. 2(c,d, g,h), the minimalRNN with large depth T is trainable over a much wider range of hyperparameters than the vanillaRNN despite the fact that the network was trained for an order of magnitude less time."
  }, {
    "heading": "4.2. Critical initialization",
    "text": "Dataset. To study the impact of critical initialization on training speed, we constructed a more realistic sequence dataset from MNIST. We unroll the pixels into a sequence of T inputs, each containing 784/T pixels. We tested T = 196 and T = 784 to vary the difficulty of the tasks.\nNote that we are more interested in the training speed of these networks under different initialization conditions than the test accuracy. We compare the convergence speed of vanilla RNN and minimalRNN under four initialization conditions: 1) critical initialization with orthogonal weights (solid blue); 2) critical initialization with Gaussian distributed weights (sold red); 3) off-critical initialization with orthogonal weights (dotted green); 4) off-critical initialization with Gaussian distributed weights (dotted black).\nWe fix 2b to zero in all settings. Under critical initialization, 2w and 2v are carefully chosen to achieve 1 = 1 as defined in eqn.(4) for vanilla RNN and eqn.(13) (detailed in algorithm 1) for minimalRNN respectively. When testing networks off criticality, we employ a common initialization\nprocedure in which, 2w = 1.0 and 2v = 1.0.\nFigure 3 summarizes our findings: there is a clear difference in training speed between models trained with critical initialization compared with models initialized far from criticality. We observe two orders of magnitude difference in training speed between a critical and off-critical initialization for vanilla RNNs. While a critically initialized model reaches a test accuracy of 90% after 750 optimization steps, the offcritical nework takes over 16,000 updates. A similar trend was observed for the minimalRNN. This difference is even more pronounced in the case of the longer sequence with T = 784. Both vanilla RNNs and minimalRNNs initialized off-criticality failed at task. The well-conditioned minimalRNN trains a factor of three faster than the vanilla RNN. As predicted above, the difference in training speed between orthogonal and Gaussian initialization schemes is significant for vanilla RNNs but is insignificant for the minimalRNN. This is corroborated in fig. 3 (b,d) where the distribution of the weights has no impact on the training speed."
  }, {
    "heading": "5. Language modeling",
    "text": "We compare the minimalRNN against more complex gated RNNs such as LSTM and GRU on the Penn Tree-Bank corpus (Marcus et al., 1993). Language modeling is a difficult task, and competitive performance is often achieved by more complicated RNN cells. We show that the minimalRNN achieves competitive performance despite its simplicity.\nWe follow the precise setup of (Mikolov et al., 2010; Zaremba et al., 2014), and train RNNs of two sizes: a small configuration with 5M parameters and a medium-sized configuration with 20M parameters 4. We report the perplexity on the validation and test sets. We focus our comparison on single layer RNNs, however we also report perplexities for multi-layer RNNs from the literature for reference. We\n4The hidden layer size of these networks are adjusted accordingly to reach the target model size.\nfollow the learning schedule of Zaremba et al. (2014) and (Jozefowicz et al., 2015). We review additional hyperparameter ranges in section F of the supplementary material.\nTable 1 summarizes our results. We find that single layer RNNs perform on par with their multi-layer counterparts. Despite being a significantly simpler model, the minimalRNN performs comparably to GRUs. Given the closed-form critical initialization developed here that significantly boosts convergence speed, the minimalRNN might be a favorable alternative to GRUs. There is a gap in perplexity between the performance of LSTMs and minimalRNNs. We hypothesize that this is due to the removal of an independent gate on the input. The same strategy is employed in GRUs and may cause a conflict between keeping longer-range memory and updating new information as was originally pointed out by Hochreiter & Schmidhuber (1997)."
  }, {
    "heading": "6. Discussion",
    "text": "We have developed a theory of signal propagation for random vanilla RNNs and a simple gated RNNs. We demonstrate rigorously that the theory predicts trainability of these networks and gating mechanisms allow for a significantly larger trainable region. We are planning to extend the theory to more complicated RNN cells as well as RNNs with multiple layers."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Jascha Sohl-Dickstein and Greg Yang for helpful discussions and Ashish Bora for many contributions to early stages of this project."
  }],
  "year": 2018,
  "references": [{
    "title": "Unitary evolution recurrent neural networks",
    "authors": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"],
    "venue": "arXiv preprint arXiv:1409.0473,",
    "year": 2014
  }, {
    "title": "Non-hermitian random matrix theory for mimo channels. Master’s thesis, Institutt for elektronikk og telekommunikasjon",
    "authors": ["Cakmak", "Burak"],
    "year": 2012
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"],
    "venue": "arXiv preprint arXiv:1412.3555,",
    "year": 2014
  }, {
    "title": "Capacity and trainability in recurrent neural networks",
    "authors": ["Collins", "Jasmine", "Sohl-Dickstein", "Jascha", "Sussillo", "David"],
    "year": 2016
  }, {
    "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity",
    "authors": ["A. Daniely", "R. Frostig", "Y. Singer"],
    "year": 2016
  }, {
    "title": "Finding structure in time",
    "authors": ["Elman", "Jeffrey L"],
    "venue": "Cognitive science,",
    "year": 1990
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"],
    "venue": "In ICASSP,",
    "year": 2013
  }, {
    "title": "Lstm: A search space odyssey",
    "authors": ["Greff", "Klaus", "Srivastava", "Rupesh K", "Koutnı́k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "Jürgen"],
    "venue": "IEEE transactions on neural networks and learning systems,",
    "year": 2017
  }, {
    "title": "Brown’s spectral distribution measure for r-diagonal elements in finite von neumann algebras",
    "authors": ["Haagerup", "Uffe", "Larsen", "Flemming"],
    "venue": "Journal of Functional Analysis,",
    "year": 2000
  }, {
    "title": "How to start training: The effect of initialization and architecture",
    "authors": ["Hanin", "Boris", "Rolnick", "David"],
    "venue": "arXiv preprint arXiv:1803.01719,",
    "year": 2018
  }, {
    "title": "On the selection of initialization and activation function for deep neural networks",
    "authors": ["Hayou", "Soufiane", "Doucet", "Arnaud", "Rousseau", "Judith"],
    "venue": "arXiv preprint arXiv:1805.08266,",
    "year": 2018
  }, {
    "title": "Deep Residual Learning for Image Recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "ArXiv e-prints,",
    "year": 2015
  }, {
    "title": "Session-based recommendations with recurrent neural networks",
    "authors": ["Hidasi", "Balázs", "Karatzoglou", "Alexandros", "Baltrunas", "Linas", "Tikk", "Domonkos"],
    "venue": "arXiv preprint arXiv:1511.06939,",
    "year": 2015
  }, {
    "title": "Long short-term memory",
    "authors": ["Hochreiter", "Sepp", "Schmidhuber", "Jürgen"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Learning unitary operators with help from u (n)",
    "authors": ["Hyland", "Stephanie L", "Rätsch", "Gunnar"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"],
    "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["Jozefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"],
    "venue": "arXiv preprint arXiv:1602.02410,",
    "year": 2016
  }, {
    "title": "Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach",
    "authors": ["R. Karakida", "S. Akaho", "Amari", "S.-i"],
    "year": 2018
  }, {
    "title": "Skip-thought vectors. In Advances in neural information processing",
    "authors": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"],
    "year": 2015
  }, {
    "title": "A simple way to initialize recurrent networks of rectified linear units",
    "authors": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"],
    "venue": "arXiv preprint arXiv:1504.00941,",
    "year": 2015
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Deep neural networks as gaussian processes",
    "authors": ["Lee", "Jaehoon", "Bahri", "Yasaman", "Novak", "Roman", "Schoenholz", "Samuel S", "Pennington", "Jeffrey", "Sohl-Dickstein", "Jascha"],
    "venue": "arXiv preprint arXiv:1711.00165,",
    "year": 2017
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"],
    "venue": "Computational linguistics,",
    "year": 1993
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Mikolov", "Tomas", "Karafiát", "Martin", "Burget", "Lukas", "Cernockỳ", "Jan", "Khudanpur", "Sanjeev"],
    "venue": "In Interspeech,",
    "year": 2010
  }, {
    "title": "All you need is a good init",
    "authors": ["Mishkin", "Dmytro", "Matas", "Jiri"],
    "venue": "arXiv preprint arXiv:1511.06422,",
    "year": 2015
  }, {
    "title": "Bayesian learning for neural networks, volume 118",
    "authors": ["Neal", "Radford M"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice",
    "authors": ["Pennington", "Jeffrey", "Schoenholz", "Sam", "Ganguli", "Surya"],
    "year": 2017
  }, {
    "title": "The emergence of spectral universality in deep networks",
    "authors": ["Pennington", "Jeffrey", "Schoenholz", "Samuel S", "Ganguli", "Surya"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "Exponential expressivity in deep neural networks through transient chaos",
    "authors": ["B. Poole", "S. Lahiri", "M. Raghu", "J. Sohl-Dickstein", "S. Ganguli"],
    "year": 2016
  }, {
    "title": "Mean field theory for sigmoid belief networks",
    "authors": ["Saul", "Lawrence K", "Jaakkola", "Tommi", "Jordan", "Michael I"],
    "venue": "Journal of artificial intelligence research,",
    "year": 1996
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"],
    "venue": "arXiv preprint arXiv:1312.6120,",
    "year": 2013
  }, {
    "title": "A correspondence between random neural networks and statistical field theory",
    "authors": ["Schoenholz", "Samuel S", "Pennington", "Jeffrey", "SohlDickstein", "Jascha"],
    "venue": "arXiv preprint arXiv:1710.06570,",
    "year": 2017
  }, {
    "title": "Chaos in random neural networks",
    "authors": ["H. Sompolinsky", "A. Crisanti", "H.J. Sommers"],
    "venue": "Phys. Rev. Lett., 61:259–262,",
    "year": 1988
  }, {
    "title": "Random walks: Training very deep nonlinear feed-forward networks with smart initialization",
    "authors": ["Sussillo", "David", "Abbott", "LF"],
    "venue": "CoRR, vol. abs/1412.6558,",
    "year": 2014
  }, {
    "title": "Can recurrent neural networks warp time",
    "authors": ["Tallec", "Corentin", "Ollivier", "Yann"],
    "venue": "arXiv preprint arXiv:1804.11188,",
    "year": 2018
  }, {
    "title": "On orthogonality and learning recurrent networks with long term dependencies",
    "authors": ["Vorontsov", "Eugene", "Trabelsi", "Chiheb", "Kadoury", "Samuel", "Pal", "Chris"],
    "venue": "arXiv preprint arXiv:1702.00071,",
    "year": 2017
  }, {
    "title": "Full-capacity unitary recurrent neural networks",
    "authors": ["Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John", "Le Roux", "Jonathan", "Atlas", "Les"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Recurrent recommender networks",
    "authors": ["Wu", "Chao-Yuan", "Ahmed", "Amr", "Beutel", "Alex", "Smola", "Alexander J", "Jing", "How"],
    "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,",
    "year": 2017
  }, {
    "title": "All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation",
    "authors": ["Xie", "Di", "Xiong", "Jiang", "Pu", "Shiliang"],
    "venue": "arXiv preprint arXiv:1703.01827,",
    "year": 2017
  }, {
    "title": "Deep mean field theory: Layerwise variance and width variation as methods to control gradient explosion, 2018",
    "authors": ["Yang", "Greg", "Schoenholz", "Sam S"],
    "year": 2018
  }, {
    "title": "Mean field residual networks: On the edge of chaos",
    "authors": ["Yang", "Greg", "Schoenholz", "Samuel S"],
    "venue": "arXiv preprint arXiv:1712.08969,",
    "year": 2017
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"],
    "venue": "arXiv preprint arXiv:1409.2329,",
    "year": 2014
  }, {
    "title": "Recurrent highway networks",
    "authors": ["Zilly", "Julian Georg", "Srivastava", "Rupesh Kumar", "Koutnı́k", "Jan", "Schmidhuber", "Jürgen"],
    "venue": "arXiv preprint arXiv:1607.03474,",
    "year": 2016
  }],
  "id": "SP:fa24befaa2c67cbc576211a5d7328ce15429e716",
  "authors": [{
    "name": "Minmin Chen",
    "affiliations": []
  }, {
    "name": "Jeffrey Pennington",
    "affiliations": []
  }, {
    "name": "Samuel S. Schoenholz",
    "affiliations": []
  }],
  "abstractText": "Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improved training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.",
  "title": "Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks"
}