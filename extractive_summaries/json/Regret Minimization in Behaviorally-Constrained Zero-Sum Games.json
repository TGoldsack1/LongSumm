{
  "sections": [{
    "heading": "1. Introduction",
    "text": "No-regret learning algorithms have become a powerful tool for solving large-scale zero-sum extensive-form games (EFGs) (Bowling et al., 2015; Brown et al., 2015). This has largely been facilitated by the counterfactual-regret minimization (CFR) algorithm (Zinkevich et al., 2007) and its newer variants (Lanctot et al., 2009; Sandholm, 2010; Bowling et al., 2015; Brown and Sandholm, 2015; Brown et al., 2017; Brown and Sandholm, 2017). This framework works by defining a notion of regret local to an information\n1Carnegie Mellon University, Pittsburgh PA 15213 USA. Correspondence to: Gabriele Farina <gfarina@cs.cmu.edu>, Christian Kroer <ckroer@cs.cmu.edu>, Tuomas Sandholm <sandholm@cs.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nset, and instantiating a standard regret minimizer at each information set in order to minimize local regret. Zinkevich et al. (2007) prove that this scheme of local regret minimization leads to a Nash equilibrium in two-player zero-sum extensive-form games of perfect recall. The framework works with any regret-minimizing algorithm, but in practice variants of the regret matching algorithm have been dominant (Hart and Mas-Colell, 2000; Bowling et al., 2015; Brown and Sandholm, 2015; Brown et al., 2015). We investigate the extension of regret-matching+ (RM+) (Tammelin et al., 2015), an even faster regretmatching algorithm, to more general regret-minimization problems over (finitely-generated) convex polytopes. We use these results to instantiate RM+ for linearly constrained simplexes, which in turn allows us to model and solve behaviorally-constrained EFGs (which are EFGs with additional linear constraints on the simplexes at each information set). An important special case of this framework is behaviorally-perturbed EFGs, which can be used to compute Nash equilibrium refinements.1\nNash equilibrium refinements are motivated by major deficiencies in the Nash equilibrium solution concept: Nash equilibria provide no guarantees on performance in information sets that are reached with probability zero in equilibrium, beyond not giving up more utility than the value of the game. Thus, if an opponent makes a mistake, Nash equilibrium is not guaranteed to capitalize on it, but may instead give back up to all of that utility (Miltersen and Sørensen, 2010). This is especially relevant when Nash equilibria are used as a solution concept for playing against imperfect opponents. Equilibrium refinements ameliorate this issue by introducing further constraints on behavior in information sets that are reached with probability zero. We will be interested in equilibrium concepts\n1The idea of certain kinds of behavioral perturbations to CFR has been suggested by Neller and Lanctot (2013). They suggest that at every information set, with small probability , a player will make a random move. However, they provide no results of what the refinement consequences are (i.e., what kind of refinement this would lead to), and it is unclear whether the proposed method actually leads to refinements. In contrast, we establish a connection to (approximate) EFPEs in this paper. Furthermore, Neller and Lanctot cite Miltersen and Sørensen (2010) which is about quasi-perfect equilibrium (where a player assumes she will not make errors in the future), while the modeling of Neller and Lanctot makes errors at future information sets as well.\nthat achieve this through the notion of perturbations or trembling hands (Selten, 1975). At each decision-point, a player is assumed to tremble with some small probability, and a Nash equilibrium is then computed in this perturbed game. A refinement is then a limit point of the sequence of Nash equilibria achieved as the probability of trembles is taken to zero. In quasi-perfect equilibria, players take into account only the trembles of their opponents (van Damme, 1984), whereas in an extensive-form perfect equilibrium (EFPE), players take into account mistakes made both by themselves and opponents (Selten, 1975).\nWe compare our algorithm for perturbed EFGs to state-ofthe-art large-scale zero-sum EFG-solving algorithms: the standard CFR+ algorithm (Tammelin et al., 2015) and the excessive gap technique (EGT) (Nesterov, 2005a) instantiated with a state-of-the art smoothing function (Nesterov, 2005b; Hoda et al., 2010; Kroer et al., 2015; 2017b). We find that our perturbed variant of CFR+ converges (in the perturbed game) at the same rate as those algorithms converge while ours leads to orders of magnitude more refined strategies. Our algorithm also converges at the same rate in the unperturbed game, almost until the point where the imposed behavioral constraints necessarily prevent further convergence."
  }, {
    "heading": "2. Related work",
    "text": "No-regret algorithms have a long history in EFG solving. Gordon (2006) developed the Lagrangian Hedging algorithm, which can be used to find a Nash equilibrium in EFGs. However, it suffers from a drawback: it requires projection onto the strategy space at each iteration. Zinkevich et al. (2007) developed CFR, which avoids projection while maintaining the same convergence rate. It has since been extended in a number of ways. Lanctot et al. (2009) showed how to incorporate sampling in CFR. Brown and Sandholm (2015) showed how to achieve greater pruning in CFR, thereby reducing the iteration costs. CFR+ is a state-of-the-art variant of CFR (Tammelin et al., 2015), which has vastly superior practical performance compared to standard CFR, though it is not known to be stronger from a theoretical perspective. Gordon et al. (2008) shows how no-regret algorithms can also be utilized for computing extensive-form correlated equilibria in EFGs.\nPolynomial-time algorithms have been proposed for computing certain equilibrium refinements in two-player zerosum perfect-recall EFGs. Miltersen and Sørensen (2010) develop a linear program (LP) for computing quasi-perfect equilibria by choosing a sufficiently small perturbation to realization plans. Farina and Gatti (2017) develop a similar approach for EFPE computation, but rely on perturbations to behavioral strategies of players. These approaches rely on solving modified variants of the sequence-form LP for\ncomputing Nash equilibrium (von Stengel, 1996) in EFGs. These algorithms are of theoretical interest only and do not work in practice. They require rational numbers of precision n log n bits, where n is the number of sequences in the game. Another issue is that LP algorithms do not scale to large EFGs even when just finding Nash equilibria, and in practice CFR-based or EGT-based approaches are used to achieve scalability. Kroer et al. (2017a) recently showed how smoothing functions for first-order methods such as EGT can be extended to games with perturbations.\nJohanson et al. (2007) consider robust strategies that arise from assuming that the opponent will randomize between playing a Nash equilibrium and a strategy within some model of opponent behavior. Johanson and Bowling (2009) consider a similar model-biased Nash equilibrium approach on games where an independent model is used at each information set. Ganzfried and Sandholm (2011) develop an opponent modeling approach that adds opponentmodeled constraints across information sets. Our approach provides a principled framework for solving model-biased games that use general constraints on per-information set behavioral strategies. Constraints across information sets currently require the much less scalable LP approach."
  }, {
    "heading": "3. Preliminaries",
    "text": "We briefly introduce several of the basic concepts we use in the rest of the paper. We denote by R+ and R− the set of non-negative and non-positive reals, respectively."
  }, {
    "heading": "3.1. Normal-Form Games",
    "text": "Definition 1. A two-player zero-sum normal-form game (for the rest of the paper, simply normal-form game or NFG) is a tuple (A1, A2, u) where A1 represents the finite set of actions that player 1 can play,A2 represents the finite set of actions that player 2 can play, and u : A1×A2 → R is the payoff function for player 1, mapping the pair of actions (a1, a2) of the players into the payoff for player 1. The corresponding payoff for player 2 is given by −u(a1, a2).\nUsually, the payoff function u is given as a matrix U , called the payoff matrix of the game. The rows of U represent the actions {a1,1, . . . , a1,n} = A1 of player 1, while the columns of U represent the actions {a2,1, . . . , a2,m} = A2 of player 2. At the intersection of the i-th row and the j-th column is the payoff for the action pair (a1,i, a2,j). Definition 2. A mixed strategy π for player i ∈ {1, 2} is a probability mass function over the set Ai.\nWhen players play according to mixed strategies π1 and π2 respectively, the expected payoff is given by\nEπ1,π2(u) = ∑ a1∈A1 ∑ a2∈A2 π1(a1)π2(a2)u(a1, a2). (1)"
  }, {
    "heading": "3.2. Generalized Normal-Form Games",
    "text": "Telgarsky (2011) and Abernethy et al. (2011) propose a generalization of the concept of normal-form games, which conveniently allows us to remove all expectation operators, making the notation lighter and more legible. In this generalization, players select deterministic strategies from a convex compact set. For a normal-form game, this set is the space of all mixed strategies. Definition 3. A two-player zero-sum generalized normalform game Γ = (X ,Y, u) is a tuple defined by a pair of convex and compact action spaces X ⊆ Rn, Y ⊆ Rm, one for each player, as well as a biaffine utility function u : X ×Y → R. The utility function u(x, y) maps the pair of actions (x, y) ∈ X × Y of the players into the payoff for player 1, while the corresponding payoff for player 2 is given by −u(x, y). Observation 1. Any normal-form game can be mapped to an instance of a generalized normal-form game. Given Γ = (A1, A2, u), where |A1| = n and |A2| = m, the set of all mixed strategies for player 1 forms the n-dimensional simplex X = ∆n, while the set of all mixed strategies for player 2 forms the m-dimensional simplex Y = ∆m. Let U be the payoff matrix associated with Γ. Using Equation 1, we conclude that Γ is equivalent to the generalized two-player zero-sum normal-form game Γ∗ = (X ,Y, u∗), where u∗(x, y) = x>Uy for all (x, y) ∈ X × Y ."
  }, {
    "heading": "3.3. Extensive-Form Games",
    "text": "Definition 4. A two-player zero-sum extensive-form game with imperfect information and perfect recall Γ is a tuple (H,Z,A, P, fc, I1, I2, u) composed of: • H: a finite set of possible sequences (or histories) of\nactions, such that the empty sequence ∅ ∈ H , and every prefix z of h in H is also in H .\n• Z ⊆ H: the set of terminal histories, i.e. those sequences that are not a proper prefix of any sequence.\n• A: a function mapping h ∈ H \\ Z to the set of available actions at non-terminal history h.\n• P : the player function, mapping each non-terminal history h ∈ H \\Z to {1, 2, c}, representing the player who takes action after h. If P (h) = c, the player is chance.\n• fc: a function assigning to each h ∈ H \\ Z such that P (h) = c a probability mass function over A(h).\n• Ii, for i ∈ {1, 2}: partition of {h ∈ H : P (h) = i} with the property that A(h) = A(h′) for each h, h′\nin the same set of the partition. For notational convenience, we will writeA(I) to meanA(h) for any of the h ∈ I , where I ∈ Ii. Ii is the information partition of player i, while the sets in Ii are called the information sets of player i.\n• u: utility function mapping z ∈ Z to the utility (a real number) gained by player 1 when the history is reached. The corresponding utility for player 2 is given by −u(z).\nWe further assume that all players can recall their previous actions and the corresponding information sets.\nIn the rest of the paper, we will use the more relaxed term extensive-form game, or EFG, to mean a two-player zerosum extensive-form game with imperfect information and perfect recall. Observation 2. Extensive-form games can be represented as generalized NFGs, for example, via the normal form representation or sequence form representation (Romanovskii, 1962; Koller et al., 1996; von Stengel, 1996)."
  }, {
    "heading": "3.4. Regret and Regret Minimization",
    "text": "Suppose there exists an iterative algorithm which, at each step t = 1, . . . , T , computes a strategy xt ∈ X for player 1, and plays a (generalized) normal-form game (X ,Y, u) against player 2 using such strategy. Let yt be the strategy used by player 2 at step t. The average external regret of player 1 up to step T against action x̂ ∈ X is\nR̄T1 (x̂) = 1\nT T∑ t=1 u(x̂, yt)− u(xt, yt).\nThe case for player 2 is symmetrical. A regret-minimizing scheme is a function that assigns, for each sequence of past actions x1, y1, . . . , xt−1, yt−1, an action xt such that lim supT→∞maxx̂∈X R̄ T 1 (x̂) ≤ 0.\nRegret-matching (RM) (Hart and Mas-Colell, 2000) is a regret-minimizing scheme for normal-form games, based on Blackwell’s approachability theorem (Blackwell, 1956). The following theorem, a proof of which is given by CesaBianchi and Lugosi (2006), characterizes the convergence rate of RM. Theorem 1. Given a normal-form game (A1, A2, u), the maximum average external regret for player 1 at iteration T , when player 1 plays according to the regret-matching algorithm, is\nmax x̂ R̄T1 (x̂) ≤ γ √ |A1|√ T ,\nwhere γ .= maxx,y u(x, y)−minx,y u(x, y).\nRegret-matching+ is an extension of RM, and converges significantly faster in practice. Tammelin et al. (2015; Lemma 2) proved that the convergence rate of RM+ is the same as that of RM above."
  }, {
    "heading": "3.5. Nash Equilibria and Refinements",
    "text": "We now review the needed solution concepts from game theory. We mostly focus on generalized normal-form\ngames, allowing a compact presentation of concepts pertaining both normal-form games and extensive-form games.\nDefinition 5 (Approximate best response). Given a generalized normal-form game (X ,Y, u) and a strategy y ∈ Y , we say that x ∈ X is an -best response to y for player 1 if u(x, y) + ≥ u(x̂, y) for all x̂ ∈ X . Symmetrically, given x ∈ X , we say that y ∈ Y is an -best response to x for player 2 if −u(x, y) + ≥ −u(x, ŷ) for all ŷ ∈ Y . Definition 6 (Approximate Nash equilibrium). Given a generalized normal-form game (X ,Y, u), the strategy pair (x, y) ∈ X × Y is a -Nash equilibrium for the game if x is an -best response to y for player 1, and y is an -best response to x for player 2.\nDefinition 7 (Nash equilibrium). Given a generalized normal-form game (X ,Y, u), a Nash equilibrium for the game is a 0-Nash equilibrium.\nThere exists a well-known relationship between regret and approximate Nash equilibria (Definition 6), as summarized in the next theorem.\nTheorem 2. In a zero-sum game, if the average external regrets of the players up to step T are such that\nR̄T1 (x̂) ≤ 1, R̄T2 (ŷ) ≤ 2 for all actions x̂ ∈ X , ŷ ∈ Y , then the strategy pair\n(x̄T , ȳT ) . =\n( 1\nT T∑ i=1 xi, 1 T T∑ i=1 yi\n) ∈ X × Y\nis an ( 1 + 2)-Nash equilibrium.\nTheorem 2 basically says that if there exists an iterative algorithm able to progressively propose strategies so that the maximum average external regret go to zero, then recovering a Nash equilibrium is straightforward, and just a matter of averaging the individual strategies proposed.\nWe now turn to the class of perturbed games (Selten, 1975). Intuitively, a perturbation restricts the set of playable strategies by enforcing a lower bound on the probability of playing each action. We recall the definition and some of the properties of game perturbations, starting from the normalform case. We focus on player 1, but remark that the same definitions hold symmetrically for player 2 as well.\nDefinition 8. Let Γ = (A1, A2, u) be an NFG and let Γ∗ = (∆|A1|,∆|A2|, u\n∗) be its generalized NFG representation (see Observation 1). A perturbation is a function p : A1 ∪ A2 → R+ such that ∑ a∈A1 p(a) < 1 and∑\na∈A2 p(a) < 1. The corresponding perturbed NFG Γp is the generalized NFG where each action a must be played with probability at least p(a). Formally, Γp = (X̃p, Ỹp, u∗) where X̃p = { x ∈ ∆|A1| : xa ≥ p(a)∀a ∈ A1 } . Ỹp is defined analogously.\nIn the case of extensive-form games, a perturbation for player 1 assigns a lower-bound on each action playable by the player. More precisely: Definition 9. Let Γ = (H,Z,A, P, fc, I1, I2, u) be an extensive-form game. A perturbation is a function p mapping each pair (I, a) where I ∈ I1 ∪ I2 and a ∈ A(I) to a non-negative real, such that∑\na∈A(I) p(I, a) < 1 ∀ I ∈ I1 ∪ I2.\nThe corresponding perturbed EFG Γp is the analogous game where each action a at each information set I has to be played with probability at least p(I, a).\nPerturbations play an important role in equilibrium refinement, as they form the basis for the concept of equilibrium perfection (Selten, 1975). In this paper we only focus on the case of extensive-form perfect equilibria (EFPEs). Definition 10. A strategy pair (x, y) ∈ X ×Y is an EFPE of Γ if it is a limit point of a sequence {(xp, yp)}p→0 where (xp, yp) is a Nash equilibrium of the perturbed game Γp.\nIntuitively, an EFPE is an equilibrium refinement that takes into account an imperfect ability to deterministically commit to a single action."
  }, {
    "heading": "4. Generalized Normal-Form Games over Finitely-Generated Convex Polytopes",
    "text": "In this section, we show how to adapt a regret-minimization algorithm to handle generalized normal-form games played on finitely-generated convex polytopes. The key insight is that when the action space is a finitely-generated convex polytope, the generalized game can be cast back as a normal-form game, i.e. a generalized normal-form game played over simplexes, and “solved” by a regretminimization algorithm; subsequently, the solution for the normal-form game gets mapped back into the polytope. This is achieved by constructing new simplex action spaces for the players, where each point in a simplex denotes a convex combination of weights on the vertices of that players’ finitely-generated convex polytopal action space. Theorem 3. Let Γ = (X ,Y, u) be a generalized normalform game played on the finitely-generated convex polytopes X and Y . There exists a regret-minimizing scheme for player 1 in Γ.\nProof. Let {b1, . . . , bn} be a convex basis for X , and {c1, . . . , cm} be a convex basis for Y; also, let B = (b1 | · · · | bn) and C = (c1 | · · · | cm) be the basis matrices for X and Y , respectively. We construct a generalized normalform game Γ∗ = (∆n,∆m, u∗), where\nu∗(x, y) . = u(Bx,Cy)\nfor all x ∈ ∆n, y ∈ ∆m. Of course Bx ∈ X , Cy ∈ Y for all x and y, so the definition is valid. Let f∗ be any of regret-minimizing schemes for normal-form games (e.g., RM or RM+). We construct a regret-minimizing scheme f for Γ such that, at each iteration t = 1, 2, . . . ,\nf(x1, y1, . . . , xt−1, yt−1) = Bf ∗(x∗1, y ∗ 1 , . . . , x ∗ t−1, y ∗ t−1),\nwhere x∗, y∗ denotes the coordinates of x, y with respect to the basis of X ,Y , respectively; note that this definition is well-defined since the coordinates are guaranteed to belong to ∆n,∆m2. The regret induced by this scheme is\nR̄T1 (x̂) = 1\nT T∑ t=1 u(x̂, yt)− u(xt, yt)\n= 1\nT T∑ t=1 u∗(x̂∗, y∗t )− u∗(x∗t , y∗t ).\nNotice that the last expression is exactly the average regret for player 1 up to iteration T against action x̂∗ in Γ∗. Since f∗ is a regret-minimizing scheme, the average regret against any action converges to zero, meaning that lim supT→∞ R̄ T 1 (x̂) ≤ 0 for each x̂, i.e. lim supT→∞maxx̂∈X R̄ T 1 (x̂) ≤ 0. This proves that f is a regret-minimizing scheme for Γ, concluding the proof.\nAnother way to think about the construction above is that at each iteration, we compute the regret for not playing each of the “strategies” forming the vertices of the polytope, and updating the next strategy by taking a convex combination of the vertices, in a way proportional to the regret against them.\nAlgorithm 1 represents an instantiation of the construction given in the proof of Theorem 3, where the regretminimizing scheme for the normal-form game was chosen to be RM+. A careful analysis of the construction also reveals that the convergence bound for RM+ carries over, as expressed by Theorem 4. At time t, RM+ projects the cumulative regret rt−1 onto the non-negative orthant Rn+; the projection is equal to the vector [rt−1]+, where [a]+i . = max{0, ai}.\nTheorem 4. Given a generalized normal-form game (X ,Y, u) with finitely generated X and Y , the maximum average external regret for player 1 at iteration T , when player 1 plays according to Algorithm 1, is bounded by\nmax x̂∈X R̄T1 (x̂) ≤ γ √ |X |√ T\nwhere γ .= maxx,y u(x, y) − minx,y u(x, y), and |X | denotes the number of vertices of X .\n2Passage to coordinates might not be unique. In this case, any coordinate vector will do, as long as the choice is deterministic.\nAlgorithm 1 RM+ algorithm for generalized normal-form games played over finitely-generated convex polytopes.\n1: procedure REGRET-MATCHING+(Γ) . Γ = (X ,Y, u), and B is a fixed convex basis for X . note: this reflects the point of view of player 1 2: r0 ← (0, . . . , 0)> ∈ Rn 3: x̄← (0, . . . , 0)> ∈ Rn 4: for t = 1, 2, 3, . . . do 5: if rt−1 ∈ Rn− then 6: xt ← any action ∈ X̃p 7: else\n8: Λt−1 ← n∑ i=1 [rt−1] + i 9: xt ← B [rt−1]+\nΛt−1 10: play action xt 11: observe yt ∈ Y played by opponent\n12: rt ← rt−1 + u(b1, yt)− u(xt, yt)... u(bn, yt)− u(xt, yt)   + 13: x̄← t− 1 t x̄+ 1 t xt\n. x̄ contains the average strategy for player 1"
  }, {
    "heading": "5. Behavioral Constraints and Perturbations",
    "text": "Behavioral constraint are linear constraints on the simplexes at each information set. In order to obtain a regret minimizer for a behaviorally-constrained EFG, we could try to cast the game as a generalized NFG by means of the normal-form or sequence form representation (see Observation 2). However, the number of vertices of this representation is exponential, and therefore it does not work well with Theorem 4. Counterfactual Regret (CFR, Zinkevich et al. 2007) solves this problem, by defining a regretminimizing scheme that runs in polynomial time in the size of the game. Intuitively, CFR minimizes a variant of instantaneous regret, called immediate counterfactual regret, at each information set separately, and later combines the strategies computed at each information set. It requires simplex regret minimizers for each information set. If we have a finite number of them, each information set can be modeled as a finitely-generated convex polytope. We can then use Theorem 4 to get regret minimizers for each information set. Perturbations can be handled as a special case.\nTheorem 5 below shows that CFR+ instantiated with such regret minimizers for each behaviorally-constrained information set converges to an equilibrium of the constrained EFG. For this approach to be practical, we need the set of vertices for each information set to be of manageable\nsize, as reflected in the dependence on maxI∈I √ |QI | in Theorem 5, where |QI | is the number of vertices in the behaviorally-constrained simplex at information set I .\nTheorem 5. Let (H,Z,A, P, fc, I1, I2, u) be an extensive-form game; let QI ⊆ ∆|A(I)| represent the behaviorally-constrained strategy space at information set I , for all I ∈ I1 ∪ I2. The maximum average external regret for player 1 in the constrained game at iteration T , when player 1 plays according to CFR+, is bounded by\nR̄T1 ≤ γ|I1| √\nmaxI∈I1 |QI |√ T ,\nwhere γ .= maxx,y u(x, y)−minx,y u(x, y)."
  }, {
    "heading": "6. Perturbed Normal-Form Games",
    "text": "Section 4 established that, in general, the problem of finding an approximate Nash equilibrium for player 1 in the generalized normal-form game Γ = (X ,Y, u), where X is a convex polytope generated by n vectors, can be solved via regret-matching.\nWe now specialize this result for the specific case of perturbed normal-form games. The following holds:\nProposition 6. Let Γ = (A1, A2, u) be a normal-form game, where A1 = {a1, . . . , an}, and let p be a perturbation for player 1. Let Γp = (X̃p, Ỹp, u∗) be the generalized normal-form game corresponding to the perturbation (Definition 8). Then the perturbed action space X̃p is a finitely generated convex polytope of dimension n, a basis of which is given by the columns of the following invertible matrix:\nBp . =  τp + p(a1) p(a1) · · · p(a1) p(a2) τp + p(a2) · · · p(a2) ... ... . . . ...\np(an) p(an) · · · τp + p(an)  where τp . = 1− p(a1)− · · · − p(an).\nThis means that Algorithm 1 is applicable and provides a regret-minimizing scheme. We remark that when computing the instantaneous regrets (Algorithm 1, Line 12), it is important to remember these values have to be computed against the basis {b1, . . . , bn} of X̃p. However, computing the (expected) utility of the game when player 1 plays according to a mixed strategy is usually more expensive than the same task when player 1 plays a deterministic action. For this reason, we express the instantaneous regret calculation against {b1, . . . , bn} in terms of instantaneous regrets against the pure actions {e1, . . . , en} in the unperturbed game. In particular, by using the fact that the utility\nfunction is biaffine, we can write, for each i ∈ {1, . . . , n},\nu(bi, yt) = u τpei + n∑ j=1 p(aj)ej , yt  = τpu(ei, yt) +\nn∑ j=1 p(aj)u(ej , yt),\nso that, by introducing φt,i . = u(ei, ut)− u(xt, yt) and the corresponding vector φt . = (φt,1, . . . , φt,n) >, we have\nrt = rt−1 + τpφt + 1 p(a1)... p(an)  > φt.\nThis allows us to compute the regret update in terms of the instantaneous regret against {e1, . . . , en} in the unperturbed game, without introducing any overhead from an asymptotic point of view.\nThe maximum average external regret for player 1 at iteration T is given by Theorem 4; in this case |X̃p| = |A1|."
  }, {
    "heading": "7. Perturbed Extensive-Form Games",
    "text": "As discussed in Section 5, it is possible to use CFR in conjunction with any regret-minimizing scheme for generalized NFGs, in order to define a regret-minimizing scheme able to support any behaviorally-perturbed EFG (thus including the restricted case of perturbed EFGs). In Algorithm 2, we propose an implementation of CFR+, i.e. CFR instantiated with the RM+ algorithm able to handle perturbed EFGs. Algorithm 2 assumes that we are given a perturbation p of the extensive-form game, X Ip denotes the perturbed simplex for information set I , and τp(I) is as in Proposition 6.\nThe following theorem characterizes the convergence guarantee of the proposed algorithm. Theorem 7. Let (H,Z,A, P, fc, I1, I2, u) be an extensive-form game; let p be the perturbation applied to the game. The maximum average external regret for player 1 in the perturbed game at iteration T , when player 1 plays according to Algorithm 2, is bounded by\nR̄T1 ≤ γ|I1| √\nmaxI∈I1 |A(I)|√ T ,\nwhere γ .= maxx,y u(x, y)−minx,y u(x, y).\nProof. Follows as a corollary of Theorem 5.\nNotice that the bound provided by Theorem 5 is the same provided by the original CFR algorithm proposed by Zinkevich et al. (2007). In other words, our modification does not impair the convergence and speed guarantees given by the original algorithm.\nAlgorithm 2 Regret minimization algorithm for perturbed extensive-form games.\n1: procedure REGRET-MATCH+-INFOSET(I, t) . we assume A(I) = {a1, . . . , an}. 2: if rIt−1 ∈ Rn− then 3: xIt ← any action ∈ X̃ Ip 4: else\n5: ΛI ← n∑ i=1 [ rIt−1 ]+ i\n6: xIt ← p(I, a1)... p(I, an) + τp(I)[rIt−1]+ ΛI\n1: procedure TRAVERSE(h, i, t, π1, π2) . assume h belongs to information set I 2: if h ∈ Z then 3: return u(h) 4: if P (h) = c then . chance node 5: sample a ∼ fc(h) 6: return TRAVERSE(ha, t, π1, π2) 7: else if P (h) = 2 then\n8: vIt ← TRAVERSE(ha1, t, π1, y I t,1π2)\n... TRAVERSE(han, t, π1, y I t,nπ2)  9: else . player 1’s turn\n10: REGRET-MATCH-INFOSET(I, t)\n11: vIt ← TRAVERSE(ha1, t, x I t,1π1, π2)\n... TRAVERSE(han, t, x I t,nπ1, π2)  12: v̄ ← (xIt )>vIt 13: φIt ← π2(vIt − 1 v̄)\n14: rIt ← rIt−1 + τp(I)φIt + 1 p(I, a1)... p(I, an)  > φIt  +\n15: x̄I ← x̄I + π1xIt 16: return v̄\n1: procedure CFR+(Γ) . Γ = (H,Z,A, P, fc, I1, I2, u) 2: for all I ∈ I1 do 3: rI0 ← (0, . . . , 0)> ∈ R|A(I)| 4: x̄I ← (0, . . . , 0)> ∈ R|A(I)| 5: for t = 1, 2, 3, . . . do 6: play according to strategy xt 7: observe strategy yt played by opponent 8: TRAVERSE(∅, t, 1, 1) 9: for all I ∈ I1 do\n10: x̄I ← ( x̄I/ ∑|A(I)| i=1 x̄ I i ) . x̄ contains the average strategy for player 1."
  }, {
    "heading": "8. Experimental Evaluation",
    "text": "We conducted experiments to investigate the practical performance of our perturbed-regret-minimization approach when used to instantiate the CFR and CFR+ algorithms for computing approximate EFPE in EFGs. We compare these algorithms to state-of-the-art Nash-equilibrium-finding algorithms: EGT (Nesterov, 2005a) on an unperturbed polytope using the state-of-the-art smoothing technique by Kroer et. al. (Kroer et al., 2017b), CFR (Zinkevich et al., 2007) and CFR+ (Tammelin et al., 2015). We conducted the experiments on Leduc hold’em poker (Southey et al., 2005), a widely-used benchmark in the imperfectinformation game-solving community. In our variant, Leduc k, the deck consists of k pairs of cards 1 . . . k, for a total deck size of 2k. We experiment on the standard Leduc game where k = 3 and a larger game where k = 5. Each player initially pays one chip to the pot, and is dealt a single private card. After a round of betting, a community card is dealt face up. After a subsequent round of betting, if neither player has folded, both players reveal their private cards. If either player pairs their card with the community card they win the pot. Otherwise, the player with the highest private card wins. In the event that both players have the same private card, they draw and split the pot. We consider k ∈ {3, 5}. We test our approach on games subject to different uniform perturbations p(I, a) = ξ for all information sets I and actions a ∈ A(I), for ξ ∈ {0.1, 0.05, 0.01, 0.005, 0.001}. Figure 1 reports on convergence to Nash equilibrium. The x-axis shows the number of tree traversals performed. We use tree traversals rather than iterations because EGT requires more tree traversals than CFR+ per iteration. The y-axis shows the sum of player regrets in the full (unperturbed) game. For both Leduc 3 and 5, we find that the ξ perturbations have only a small effect on overall convergence rate until convergence within the perturbed polytope, at which point the regret in the unperturbed game stops decreasing, as expected. Until bottoming out, the convergence is almost identical for all CFR+ algorithms. This shows that our approach can be utilized in practice: there is no substantial loss of convergence rate. Later in the run once the perturbed algorithms have bottomed out, there is a tradeoff between exploitability in the full game and refinement (i.e., better performance in low-probability information sets).\nThe second set of experiments, Figure 2, investigates the improvement that our perturbation approach achieves compared to standard Nash equilibrium solutions in terms of equilibrium refinement. Our measure of refinement is the maximum regret at any information set, conditioned on reaching that information set. As discussed, convergence to a Nash Equilibrium does not guarantee that this measure goes to zero. Again, the x-axis shows the number\nof tree traversals performed. The y-axis shows the maximum regret at any individual information set. Both unperturbed CFR+ and EGT perform badly in both games with respect to this measure of refinement. In Leduc 5, both have maximum regret two orders of magnitude worse than the perturbed approach. In Leduc 3, EGT still does as poorly. CFR+ does slightly better, but is still worse than our stronger refinements by more than an order of magnitude. The maximum regret one can possibly cause in an information set in either Leduc game is 23, so CFR+ and unperturbed EGT also do poorly in that sense.\nIn contrast to this, we find that our ξ-perturbed solution concepts converge to a strategy with low regret at every information set. The choice of ξ is important: for ξ = 0.001, the smallest perturbation, we see that it takes a long time to converge at low-probability information sets, whereas we converge reasonably quickly for ξ = 0.01 or ξ = 0.005; for ξ = 0.1 and ξ = 0.05 the perturbations are too large, and we end up converging with relatively high regret (due to being forced to play every action with probability ξ). Thus, within this set of experiments, ξ ∈ [0.005, 0.01] seems to be the ideal amount of perturbation."
  }, {
    "heading": "9. Discussion",
    "text": "We extended the RM and RM+ regret minimization algorithms to finitely-generated convex polytopes, and specialized our results to linearly-constrained simplexes and behaviorally-perturbed EFGs. We then showed how this allows us to compute an approximate EFPE. Our experi-\nments showed that this approach leads to much stronger strategies for information sets reached with low probability, while maintaining the strong convergence rate of CFR+.\nOur experiments raise an interesting question. Across both games, we see that the maximum information set regret goes down much faster for larger amounts of perturbation, but then it bottoms out earlier than for smaller perturbations (as expected). To get the best of both large and small perturbations, it may be possible to decrease the perturbations over time, leading to faster convergence rate, while never bottoming out. However, this has a number of challenges associated with it. Most importantly, we need a variant of RM or RM+ that can handle a slowly expanding feasible set within the simplex. This would also require decreasing the perturbations at the correct rate; if decreased too quickly, it is unlikely that we will converge to a refinement, and if decreased too slowly, we might still bottom out.\nThe CFR algorithms have been shown to work well with a number of other techniques, notably sampling (Lanctot et al., 2009) and abstraction (Lanctot et al., 2012; Kroer and Sandholm, 2016). It would be both theoretically and practically interesting to see how well our refinement approach works in conjunction with these techniques."
  }, {
    "heading": "Acknowledgments",
    "text": "This material is based on work supported by NSF grant IIS1617590 and ARO award W911NF-17-1-0082."
  }],
  "year": 2017,
  "references": [{
    "title": "Blackwell approachability and no-regret learning are equivalent",
    "authors": ["Jacob Abernethy", "Peter L Bartlett", "Elad Hazan"],
    "venue": "In Conference on Learning Theory (COLT),",
    "year": 2011
  }, {
    "title": "An analog of the minmax theorem for vector payoffs",
    "authors": ["David Blackwell"],
    "venue": "Pacific Journal of Mathematics,",
    "year": 1956
  }, {
    "title": "Heads-up limit hold’em poker is solved",
    "authors": ["Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin"],
    "venue": "Science,",
    "year": 2015
  }, {
    "title": "Regret-based pruning in extensive-form games",
    "authors": ["Noam Brown", "Tuomas Sandholm"],
    "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Reduced space and faster convergence in imperfect-information games via pruning",
    "authors": ["Noam Brown", "Tuomas Sandholm"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Dynamic thresholding and pruning for regret minimization",
    "authors": ["Noam Brown", "Christian Kroer", "Tuomas Sandholm"],
    "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2017
  }, {
    "title": "Prediction, learning, and games",
    "authors": ["Nicolo Cesa-Bianchi", "Gábor Lugosi"],
    "venue": "Cambridge university press,",
    "year": 2006
  }, {
    "title": "Extensive-form perfect equilibrium computation in two-player games",
    "authors": ["Gabriele Farina", "Nicola Gatti"],
    "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2017
  }, {
    "title": "Game theorybased opponent modeling in large imperfect-information games",
    "authors": ["Sam Ganzfried", "Tuomas Sandholm"],
    "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
    "year": 2011
  }, {
    "title": "No-regret learning in convex games",
    "authors": ["Geoffrey J. Gordon", "Amy Greenwald", "Casey Marks"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2008
  }, {
    "title": "No-regret algorithms for online convex programs",
    "authors": ["Geoffrey J. Gordon"],
    "venue": "NIPS, 19:489,",
    "year": 2006
  }, {
    "title": "A simple adaptive procedure leading to correlated",
    "authors": ["Sergiu Hart", "Andreu Mas-Colell"],
    "venue": "equilibrium. Econometrica,",
    "year": 2000
  }, {
    "title": "Smoothing techniques for computing Nash equilibria of sequential games",
    "authors": ["Samid Hoda", "Andrew Gilpin", "Javier Peña", "Tuomas Sandholm"],
    "venue": "Mathematics of Operations Research,",
    "year": 2010
  }, {
    "title": "Data biased robust counter strategies",
    "authors": ["Michael Johanson", "Michael Bowling"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2009
  }, {
    "title": "Computing robust counter-strategies",
    "authors": ["Michael Johanson", "Martin Zinkevich", "Michael Bowling"],
    "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2007
  }, {
    "title": "Efficient computation of equilibria for extensive two-person games",
    "authors": ["Daphne Koller", "Nimrod Megiddo", "Bernhard von Stengel"],
    "venue": "Games and Economic Behavior,",
    "year": 1996
  }, {
    "title": "Imperfect-recall abstractions with bounds in games",
    "authors": ["Christian Kroer", "Tuomas Sandholm"],
    "venue": "In Proceedings of the ACM Conference on Economics and Computation",
    "year": 2016
  }, {
    "title": "Faster first-order methods for extensive-form game solving",
    "authors": ["Christian Kroer", "Kevin Waugh", "Fatma Kılınç-Karzan", "Tuomas Sandholm"],
    "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),",
    "year": 2015
  }, {
    "title": "Smoothing method for approximate extensiveform perfect equilibrium",
    "authors": ["Christian Kroer", "Gabriele Farina", "Tuomas Sandholm"],
    "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
    "year": 2017
  }, {
    "title": "Theoretical and practical advances on smoothing for extensive-form games",
    "authors": ["Christian Kroer", "Kevin Waugh", "Fatma Kilinc-Karzan", "Tuomas Sandholm"],
    "venue": "In Proceedings of the ACM Conference on Economics and Computation",
    "year": 2017
  }, {
    "title": "Monte Carlo sampling for regret minimization in extensive games",
    "authors": ["Marc Lanctot", "Kevin Waugh", "Martin Zinkevich", "Michael Bowling"],
    "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2009
  }, {
    "title": "No-regret learning in extensive-form games with imperfect recall",
    "authors": ["Marc Lanctot", "Richard Gibson", "Neil Burch", "Martin Zinkevich", "Michael Bowling"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2012
  }, {
    "title": "Computing a quasi-perfect equilibrium of a two-player game",
    "authors": ["Peter Bro Miltersen", "Troels Bjerre Sørensen"],
    "venue": "Economic Theory,",
    "year": 2010
  }, {
    "title": "An introduction to counterfactual regret minimization",
    "authors": ["Todd N. Neller", "Marc Lanctot"],
    "venue": "Tutorial,",
    "year": 2013
  }, {
    "title": "Excessive gap technique in nonsmooth convex minimization",
    "authors": ["Yurii Nesterov"],
    "venue": "SIAM Journal of Optimization,",
    "year": 2005
  }, {
    "title": "Smooth minimization of non-smooth functions",
    "authors": ["Yurii Nesterov"],
    "venue": "Mathematical Programming,",
    "year": 2005
  }, {
    "title": "Reduction of a game with complete memory to a matrix game",
    "authors": ["I. Romanovskii"],
    "venue": "Soviet Mathematics,",
    "year": 1962
  }, {
    "title": "The state of solving large incompleteinformation games, and application to poker",
    "authors": ["Tuomas Sandholm"],
    "venue": "AI Magazine,",
    "year": 2010
  }, {
    "title": "Reexamination of the perfectness concept for equilibrium points in extensive games",
    "authors": ["Reinhard Selten"],
    "venue": "International Journal of Game Theory,",
    "year": 1975
  }, {
    "title": "bluff: Opponent modelling in poker",
    "authors": ["Finnegan Southey", "Michael Bowling", "Bryce Larson", "Carmelo Piccione", "Neil Burch", "Darse Billings", "Chris Rayner. Bayes"],
    "venue": "In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI),",
    "year": 2005
  }, {
    "title": "Solving heads-up limit Texas hold’em",
    "authors": ["Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling"],
    "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),",
    "year": 2015
  }, {
    "title": "A relation between perfect equilibria in extensive form games and proper equilibria in normal form games",
    "authors": ["Eric van Damme"],
    "venue": "International Journal of Game Theory,",
    "year": 1984
  }, {
    "title": "Efficient computation of behavior strategies",
    "authors": ["Bernhard von Stengel"],
    "venue": "Games and Economic Behavior,",
    "year": 1996
  }, {
    "title": "Regret minimization in games with incomplete information",
    "authors": ["Martin Zinkevich", "Michael Bowling", "Michael Johanson", "Carmelo Piccione"],
    "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2007
  }],
  "id": "SP:9d0eca1cb28b992fe8f1df4ad1d86e1981d62747",
  "authors": [{
    "name": "Gabriele Farina",
    "affiliations": []
  }, {
    "name": "Christian Kroer",
    "affiliations": []
  }, {
    "name": "Tuomas Sandholm",
    "affiliations": []
  }],
  "abstractText": "No-regret learning has emerged as a powerful tool for solving extensive-form games. This was facilitated by the counterfactual-regret minimization (CFR) framework, which relies on the instantiation of regret minimizers for simplexes at each information set of the game. We use an instantiation of the CFR framework to develop algorithms for solving behaviorally-constrained (and, as a special case, perturbed in the Selten sense) extensive-form games, which allows us to compute approximate Nash equilibrium refinements. Nash equilibrium refinements are motivated by a major deficiency in Nash equilibrium: it provides virtually no guarantees on how it will play in parts of the game tree that are reached with zero probability. Refinements can mend this issue, but have not been adopted in practice, mostly due to a lack of scalable algorithms. We show that, compared to standard algorithms, our method finds solutions that have substantially better refinement properties, while enjoying a convergence rate that is comparable to that of state-of-the-art algorithms for Nash equilibrium computation both in theory and practice.",
  "title": "Regret Minimization in Behaviorally-Constrained Zero-Sum Games"
}