{
  "sections": [{
    "text": "1Departments of Statistics and Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA 2Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA. Correspondence to: Aaditya Ramdas <aramdas@eecs.berkeley.edu>, Tijana Zrnic <tijana@eecs.berkeley.edu>, Martin J. Wainwright <wainwrig@eecs.berkeley.edu>, Michael I. Jordan <jordan@eecs.berkeley.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s)."
  }, {
    "heading": "1. Introduction",
    "text": "It is now commonplace in science and technology to make thousands or even millions of related decisions based on data analysis. As a simplified example, to discover which genes may be related to diabetes, we can formulate the decision-making problem in terms of hypotheses that take the form “gene X is not associated with diabetes,” for many different genes X, and test for which of these null hypotheses can be confidently rejected by the data. As first identified by Tukey in a seminal 1953 manuscript (1953), the central difficulty when testing a large number of null hypotheses is that several of them may appear to be false, purely by chance. Arguably, we would like the set of rejected null hypothesesR to have high precision, so that most discovered genes are indeed truly correlated with diabetes and further investigations are not fruitless. Unfortunately, separately controlling the false positive rate for each individual test actually does not provide any guarantee on the precision. This motivated the development of procedures that can provide guarantees on an error metric called the false discovery rate (FDR) (Benjamini & Hochberg, 1995), defined as:\nFDR ≡ E [FDP(R)] = E [ |H0 ∩R| |R| ] ,\nwhereH0 is the unknown set of truly null hypotheses, and 0/0 ≡ 0. Here the FDP represents the ratio of falsely rejected nulls to the total number of rejected nulls, and since the set of discoveries R is data-dependent, the FDR takes an expectation over the underlying randomness. The evidence from a hypothesis test can typically be summarized in terms of a p-value, and so offline multiple testing algorithms take a set of p-values {Pi} as their input, and a target FDR level α ∈ (0, 1), and produce a rejected set R that is guaranteed to have FDR ≤ α. Of course, one also desires a high recall, or equivalently a low false negative rate, but without assumptions on many uncontrollable factors like the frequency and strength of signals, additional guarantees on the recall are impossible.\nWhile the offline paradigm previously described is the classical setting for multiple decision-making, the corresponding online problem is emerging as a major area of its own. For example, large information technology companies run thou-\nsands of A/B tests every week of the year, and decisions about whether or not to reject the corresponding null hypothesis must be made without knowing the outcomes of future tests; indeed, future null hypotheses may depend on the outcome of the current test. The current standard of setting all thresholds αk to a fixed quantity such as 0.05 does not provide any control of the FDR. Hence, the following hypothetical scenario is entirely plausible: a company conducts 1000 tests in one week, each with a target false positive rate of 0.05; it happens to make 80 discoveries in total of which 50 are accidental false discoveries, ending up with an FDP of 5/8. Such uncontrolled error rates can have severe financial and social consequences.\nThe first method for online control of the FDR was the alpha-investing algorithm of Foster and Stine (2008), later extended to generalized alpha-investing (GAI) algorithms by Aharoni and Rosset (2014). Recently, Javanmard and Montanari (2017) proposed variants of GAI algorithms that control the FDR (as opposed to the modified FDR controlled in the original paper (Foster & Stine, 2008)), including a new algorithm called LORD. The GAI++ algorithms by Ramdas et al. (2017) improved the earlier GAI algorithms (uniformly), and the improved LORD++ (henceforth LORD) method arguably represents the current state-of-the-art in online multiple hypothesis testing.\nThe current paper’s central contribution is the derivation and analysis of a powerful new class of online FDR algorithms called “SAFFRON” (Serial estimate of the Alpha Fraction that is Futilely Rationed On true Null hypotheses). As an instance of the GAI framework, the SAFFRON method starts off with an error budget, referred to as alphawealth, that it allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery. However, unlike earlier work in the online setting, SAFFRON is an adaptive method, meaning that it is based on an estimate of the proportion of true nulls. In the offline setting, adaptive methods were proposed by Storey (2002; 2004), who showed that they are more powerful than the Benjamini-Hochberg (BH) procedure (1995) under independence assumptions; this advantage usually increases with the proportion of non-nulls and the signal strength. Thus, the SAFFRON method can be viewed as an online analogue of Storey’s adaptive version of the BH procedure. As shown in Figure 1, our simulations show that SAFFRON demonstrates the same types of advantages over its non-adaptive counterparts, such as LORD and alpha-investing. Furthermore, the ideas behind SAFFRON’s derivation can provide a natural template for the design and analysis of a suite of other adaptive online methods.\nThe rest of this paper is organized as follows. In Section 2, we derive the SAFFRON algorithm from first principles, leaving the precise statement and the proof of a central tech-\nnical lemma for Section 3. In Section 4, we investigate the practical choice of tuning parameters, and demonstrate the effectiveness of our recommended choice using simulations. We end with a summary in Section 5."
  }, {
    "heading": "2. Deriving the SAFFRON Algorithm",
    "text": "Before deriving the SAFFRON algorithm, it is useful to recap a few concepts. By definition of a p-value, if the hypothesis Hi is truly null, then the corresponding p-value is stochastically larger than the uniform distribution (“superuniformly distributed,” for short), meaning that:\nIf the null hypothesis Hi is true, then Pr{Pi ≤ u} ≤ u for all u ∈ [0, 1].\n(1)\nFor any online FDR procedure, let the rejected set after t steps be denoted byR(t). More precisely, this set consists of all p-values among the first t ones for which the indicator for rejection is equal to 1; i.e., Rj : = 1 {Pj ≤ αj} = 1, for all j ≤ t. While we have already defined the classical FDP and FDR in the introduction, several authors, including Foster and Stine (2008), have considered a modified FDR, defined as:\nmFDR(t) : = E [ |H0 ∩R(t)| ] E [|R(t)|] . (2)\nIn the sequel, we provide guarantees for both mFDR and FDR. Our guarantees on mFDR hold under the following weakening of (1). Define the filtration formed by the sequence of sigma-fields F t : = σ(R1, . . . , Rt), and let αt : = ft(R1, . . . , Rt−1), where ft is an arbitrary function of the first t−1 indicators for rejection. Then, we say that the null p-values are conditionally super-uniformly distributed if the following holds:\nIf the null hypothesis Ht is true, then Pr { Pt ≤ αt ∣∣ F t−1} ≤ αt. (3)"
  }, {
    "heading": "2.1. An Oracle FDP Estimate and a Naive Overestimate",
    "text": "To understand the motivation behind the new procedure, it is necessary to expand on an perspective on existing online FDR procedures, recently suggested by Ramdas et al. (2017). We begin by defining an oracle estimate of the FDP as:\nFDP∗(t) : =\n∑ j≤t,j∈H0 αj\n|R(t)| .\nThe word oracle indicates that FDP∗ cannot be calculated by the scientist, since H0 is unknown. Intuitively, the numerator ∑ j≤t,j∈H0 αj overestimates the number of false discoveries, and FDP∗(t) overestimates the FDP, as formalized in the claim below:\nProposition 1. If the null p-values are conditionally superuniformly distributed (3), then we have:\n(a) E [ ∑ j≤t,j∈H0 αj ] ≥ E [ |H0 ∩R(t)| ] ;\n(b) If FDP∗(t) ≤ α for all t ∈ N, then mFDR(t) ≤ α for all t ∈ N.\nFurther, if the null p-values are independent of each other and of the non-nulls, and {αt} is a monotone function of past rejections, then:\n(c) E [FDP∗(t)] ≥ E [FDP(t)] ≡ FDR(t) for all t ∈ N;\n(d) The condition FDP∗(t) ≤ α for all t ∈ N implies that FDR(t) ≤ α for all t ∈ N.\nTo clarify, the word monotone means that αt is a coordinatewise non-decreasing function of the vector R1, . . . , Rt−1. Proposition 1 follows from the results of Ramdas et al. (2017), and we prove it in Subsection 3.1 for completeness. Even though FDP∗(t) cannot be directly calculated and used, Proposition 1 is a useful way to identify what would be ideally possible.\nOne natural way to convert FDP∗(t) to a truly empirical overestimate of FDP(t) is to define:\nF̂DPLORD(t) : =\n∑ j≤t αj\n|R(t)| .\nSince it is trivially true that F̂DPLORD(t) ≥ FDP∗(t), we immediately obtain that Proposition 1 also holds with FDP∗(t) replaced by F̂DPLORD(t). The subscript LORD is used because Ramdas et al. (2017) point out that their variant of the LORD algorithm of Javanmard and Montanari (2017) can be derived by simply assigning αj in an online fashion to ensure that the condition F̂DPLORD(t) ≤ α is met for all times t."
  }, {
    "heading": "2.2. A Better Estimate of the Alpha-Wealth Spent on Testing Nulls",
    "text": "The main drawback of F̂DPLORD is that if the underlying (unknown) truth is such that the proportion of non-nulls (true signals) is non-negligible, then F̂DPLORD(t) is a very crude and overly conservative overestimate of FDP∗(t), and hence also of the true unknown FDP. With this drawback in mind, and knowing that we would expect non-nulls to typically have smaller p-values, we propose the following novel estimator:\nF̂DPSAFFRON(λ)(t) ≡ F̂DPλ(t) : = ∑ j≤t αj 1{Pj>λj} 1−λj\n|R(t)| ,\nwhere {λj}∞j=1 is a predictable sequence of user-chosen parameters in the interval (0, 1). Here the term predictable means that λj is a deterministic function of the information available from time 1 to j − 1, which will be formalized later. For simplicity, when λj is chosen to be a constant for all j, we will drop the subscript and just write λ, and we will consider λ = 1/2 as our default choice. SAFFRON is based on the idea that the numerator of F̂DPλ is a much better estimator of the quantity ∑ j≤t,j∈H0 αj than LORD’s\nnaive estimate ∑ j≤t αj .\nSo as to provide some intuition for why we expect F̂DPλ to be a fairly tight estimate of FDP∗, note that 1{Pj>λj}1−λj has a unit expectation whenever Pj is uniformly distributed (null), but would typically have a much smaller expectation whenever Pj is stochastically much smaller than uniform (non-null). The following theorem shows that, even though F̂DPλ(t) is not necessarily always larger than FDP∗(t), a direct analog of Proposition 1 is nonetheless valid. In order to state this claim formally, we need to slightly modify the assumption (3). As before, denote by Rj : = 1 {Pj ≤ αj} the indicator for rejection, and let Cj := 1 {Pj ≤ λj} be the indicator for candidacy. Accordingly, we refer to the p-values for which Cj = 1 as candidates. Moreover, we let αt : = ft(R1, . . . , Rt−1, C1, . . . , Ct−1), where ft denotes an arbitrary function of the first t − 1 indicators for rejection and candidacy, and define the filtration generated from sigma-fields F t : = σ(R1, . . . , Rt, C1, . . . , Ct). With respect to this filtration, we introduce a conditional superuniformity condition on the null p-values similar to (3):\nIf the null hypothesis Ht is true, then Pr { Pt ≤ αt ∣∣ F t−1} ≤ αt, (4) which can be rephrased as:\nE [ 1 {Pt > αt}\n1− αt ························ ∣∣∣∣ F t−1] ≥ 1 ≥ E [1 {Pt ≤ αt}αt························ ∣∣∣∣ F t−1] .\nNote that again marginal super-uniformity (1) implies this condition, provided that the p-values are independent.\nTheorem 1. If the null p-values are conditionally superuniformly distributed (4), then we have:\n(a) E [ ∑ j≤t,j∈H0 αj 1{Pj>λj} 1−λj ] ≥ E [ |H0 ∩R(t)| ] ;\n(b) The condition F̂DPλ(t) ≤ α for all t ∈ N implies that mFDR(t) ≤ α for all t ∈ N.\nFurther, if the null p-values are independent of each other and of the non-nulls, and {αt} is a monotone function of the vector R1, ..., Rt−1, C1, ..., Ct−1, then we additionally have:\n(c) E [ F̂DPλ(t) ] ≥ E [FDP(t)] ≡ FDR(t) for all t ∈ N;\n(d) The condition F̂DPλ(t) ≤ α for all t ∈ N implies that FDR(t) ≤ α for all t ∈ N.\nThe proof of this theorem is given in Section 3.2, and is based on a “reverse super-uniformity lemma” that is discussed in the next section. This lemma, though of a technical nature, may be of independent interest in deriving new algorithms. The statements on mFDR control allow SAFFRON to be used in place of LORD in applications in which p-values are not independent, but are conditionally super-uniformly distributed, such as the MAB-FDR framework (based on multi-armed bandits) proposed by Yang et al. (2017)."
  }, {
    "heading": "2.3. The SAFFRON Algorithm for Constant λ",
    "text": "We now present the SAFFRON algorithm at a high level. For simplicity, we consider the constant λ setting, which performs well in experiments, though it may be a useful direction for future work to construct good heuristics for time-varying sequences {λj}∞j=1.\n1. Given a target FDR level α, the user first picks a constant λ ∈ (0, 1), an initial wealth W0 < (1− λ)α, and a positive non-increasing sequence {γj}∞j=1 of summing to one. For example, given a parameter s > 1, we might pick γj ∝ j−s for some s > 1.\n2. We use the term “candidates” to refer to p-values smaller than λ, since SAFFRON will never reject a p-value larger than λ. Recalling the indicator for candidacy Ct : = 1 {Pt ≤ λ}, and denoting by τj be the time of the j-th rejection (and setting τ0 = 0), define the candidates after the j-th rejection as Cj+ = Cj+(t) = ∑t−1 i=τj+1 Ci.\n3. SAFFRON begins by allocation α1 = min{γ1W0, λ},\nand then at time t = 2, 3, . . ., it allocates:\nαt : = min{λ, α̃t}, where α̃t : = W0γt−C0++ ((1− λ)α−W0)γt−τ1−C1+ + ∑ j≥2 (1− λ)αγt−τj−Cj+ .\nIn words, SAFFRON starts off with an alpha-wealth W0 < (1−λ)α, never loses wealth when testing candidate p-values, gains wealth of (1− λ)α on every rejection except the first. If there is a significant fraction of non-nulls, and the signals are fairly strong, then SAFFRON may make more rejections than LORD.\nTo clarify, SAFFRON guarantees FDR control for any λ ∈ (0, 1) and any chosen sequence {γj}∞j=1, but the algorithm’s power, or ability to detect signals, varies as a function of these parameters. Given the minimal nature of our assumptions, there is no universally optimal constant or sequence: specifically, we do not make assumptions on the frequency of true signals, or on how strong they are, or on their order, all of which are factors that affect the power. We discuss reasonable default choices in the experimental section."
  }, {
    "heading": "2.4. Relationship to Other Procedures",
    "text": "Here, we compare SAFFRON to existing procedures in the literature, emphasizing commonalities that allow us to give a unified view of seemingly disparate algorithms.\nAlpha-investing. Even though the motivation that we have presented for SAFFRON relates it to the LORD algorithm, we find it interesting that the original alpha-investing algorithm of Foster and Stine (2008) is recovered by choosing λj = αj in F̂DPλ, and attempting to ensure that F̂DPλ(t) ≤ α for all times t ∈ N. In order to see this fact, first note that with this choice of λj , the indicator 1 {Pj > λj} simply indicates when the j-th hypothesis is not rejected. Consequently, the numerator of F̂DPλ reads as ∑ j≤t αj 1−αj 1 {j /∈ R(t)}. Hence, ensuring that F̂DPλ(t) ≤ α at all times t ∈ N, is equivalent to ensuring that ∑ j≤t αj 1−αj 1 {j /∈ R(t)} never exceeds α(|R(t)| ∨ 1), which, in the language of alpha-investing, is equivalent to ensuring that the algorithm’s wealth never becomes negative.1 Just as Ramdas et al. (2017) were able to reinterpret and rederive LORD in terms of a particular estimate of the FDP, the current work allows us to reinterpret and rederive alpha-investing in terms of SAFFRON’s FDP. However, our simulations demonstrate that despite this similarity, SAFFRON with λj = 1/2 is typically a more powerful algorithm than both LORD and alpha-investing.\n1Recall that the alpha-investing algorithm starts off with an alpha-wealth of α, reduces its alpha-wealth by αj\n1−αj after tests\nthat fail to reject, and increase the wealth by α on rejections.\nStorey-BH. In offline multiple testing, where all n pvalues are immediately available, the Benjamini-Hochberg (BH) procedure (1995) is a classical method for guaranteeing FDR control. BH estimates the FDP of the rejection set R(s) := {i : Pi ≤ s} by F̂DPBH(s) : = n·s|R(s)| , which is a conservative estimate of the oracle FDP∗BH(s) : = |H0|·s |R(s)| (details in Supplementary Material). For independent pvalues, Storey et al. (2002; 2004) improved the BH method, by picking a constant λ ∈ (0, 1), and calculating:\nF̂DPStBH(s) : = n · s · π̂0 |R(s)| ,\nwhere π̂0 is an estimate of the unknown proportion of nulls π0 = |H0|/n computed as:\nπ̂0 : = 1 + ∑n i=1 1 {Pi > λ} n(1− λ) .\nThen, this procedure, which we refer to as “Storey-BH,” calculates ŝStBH : = max{s : F̂DPStBH(s) ≤ α} and rejects the setR(ŝStBH) which satisfies the bound FDR ≤ α. Procedures such as Storey-BH are known in the multiple testing literature as adaptive procedures, since they automatically adapt to the unknown proportion of nulls.\nReturning to the setting of online FDR, what matters is not the proportion of nulls π0, but instead a running estimate of the amount of alpha-wealth that was spent testing nulls thus far; this difference arises because, unlike the offline setting where all p-values are compared to the same level ŝ, different p-values have to pass different thresholds αi. In light of the above discussion, it should be apparent that Storey-BH is to BH as SAFFRON is to LORD. In the offline context, Storey-BH is called an “adaptive method” (it is adaptive to the unknown null proportion) and in this sense, SAFFRON can be seen as an adaptive online FDR method.\nAccumulation tests. Note that E [2I(P > 1/2)] ≥ 1 for null p-values (with equality when they are exactly uniformly distributed, simply because ∫ 1 0\n2I(p > 1/2)dp = 1). One may actually use any non-decreasing function h such that∫ 1 0 h(p)dp in the formula for F̂DPλ. Such accumulation functions were studied (Li & Barber, 2017) in the (offline) context of ordered testing, and may seamlessly be transferred to the online setting considered here, yielding mFDR control using the same proof. In initial experiments, the use of other functions does not seem to yield any advantage, and under some additional assumptions in the offline ordered testing setting, the aforementioned authors argued that the step function (1− λ)−1I(I > λ) is asymptotically optimal for power. In this light, SAFFRON can also be seen as an online analog of adaptive SeqStep (Lei & Fithian, 2016), which is a variant of selective SeqStep (Barber & Candès, 2015) and SeqStep (Li & Barber, 2017)."
  }, {
    "heading": "3. Proof of Theorem 1 Using a Reverse Super-Uniformity Lemma",
    "text": "In this section, we present a lemma that is central to the proof of FDR control for SAFFRON. We then use this lemma to prove Proposition 1 and Theorem 1. Let us first recall and set up some preliminary notation. In what follows, αt, λt are random variables in (0, 1) that always satisfy αt ≤ λt. We denote the indicator for rejection at the t-th step by Rt : = 1 {Pt ≤ αt}, and recall that since only p-values smaller than λt are candidates for rejection, we had earlier defined the indicator for candidacy as Ct : = 1 {Pt ≤ λt}. If we denote C̄t = 1 − Ct, then it is clear that RtC̄t = 0, since Rt = 1 implies C̄t = 0 and C̄t = 1 implies Rt = 0, and it is also possible for Rt and C̄t to both equal 0. Also let R1:t : = {R1, . . . , Rt} and C1:t : = {C1, . . . , Ct}. As before, we consider the filtration F t : = σ(R1:t, C1:t). In what follows, we insist that the sequences {αt}∞t=1 and {λt}∞t=1 are predictable, meaning that they are functions of the information available from time 1 to t − 1 only; specifically, we insist that αt, λt are measurable with respect to the sigma-field F t−1. We will also require that the {αt} sequence is monotone, meaning that αt = ft(R1:t−1, C1:t−1) for some coordinatewise nondecreasing function ft : {0, 1}2(t−1) → [0, λt]. The proof that SAFFRON as described in Subsection 2.3 satisfies this requirement is given in the Supplementary Material.\nRecall the definition (4) of conditional super-uniformity, as well as its equivalent rephrased form in the line after definition (4). Lemma 1 guarantees that for independent p-values, this statement holds true more generally.\nLemma 1. Assume that the p-values P1, P2, . . . are independent and let g : {0, 1}T → R be any coordinatewise non-decreasing function. Then, for any index t ≤ T such that Ht ∈ H0, we have:\nE [ ft(R1:t−1, C1:t−1)1 {Pt > λt}\n(1− λt)g(R1:T ) ∣∣∣∣ F t−1] ≥ E [ ft(R1:t−1, C1:t−1)\ng(R1:T ) ∣∣∣∣ F t−1] ≥ E [ 1 {Pt ≤ ft(R1:t−1, C1:t−1)}\ng(R1:T ) ∣∣∣∣ F t−1] . Proof. The second inequality is a consequence of superuniformity lemmas from past work (Ramdas et al., 2017; Javanmard & Montanari, 2017), so we only prove the first inequality. At a high level, the proof strategy is inverted, and we will hallucinate a vector with one element being set to 1, instead of being set to 0 in the aforementioned works.\nLetting P1:T = (P1, . . . , PT ) be the original vector of p-values, we define a “hallucinated” vector of p-values P̃ t→11:T : = (P̃1, . . . , P̃T ) that equals P1:T , except that the\nt-th component is set to one:\nP̃i = { 1 if i = t Pi if i 6= t.\nDefine hallucinated candidate and rejection indicators as C̃i = 1 { P̃i ≤ λi } and R̃i =\n1 { P̃i ≤ fi(R̃1:i−1, C̃1:i−1) } respectively. Let\nR1:T = (R1, . . . , RT ) and R̃t→11:T = (R̃1, . . . , R̃T ) denote the vector of rejections using P1:T and P̃ t→11:T , respectively. Similarly, let C1:T = (C1, . . . , CT ) and C̃t→11:T = (C̃1, . . . , C̃T ) denote the vector of candidates using P1:T and P̃ t→11:T , respectively.\nBy construction, we have the following properties:\n1. R̃i = Ri and C̃i = Ci for all i < t, hence fi(R1:i−1, C1:i−1) = fi(R̃1:i−1, C̃1:i−1) for all i ≤ t.\n2. R̃t = C̃t = 0, and hence R̃i ≤ Ri for all i ≥ t, due to monotonicity of the functions fi.\nHence, on the event {Pt > λt}, we have Rt = R̃t = 0 and Ct = C̃t = 0, and hence also R1:T = R̃t→11:T . This allows us to conclude that:\nft(R1:t−1, C1:t−1)1 {Pt > λt} (1− λt)g(R1:T ) =\nft(R1:t−1, C1:t−1)1 {Pt > λt} (1− λt)g(R̃t→11:T ) .\nSince R̃t→11:T is independent of Pt, we may take conditional expectations to obtain:\nE [ ft(R1:t−1, C1:t−1)1 {Pt > λt}\n(1− λt)g(R1:T ) ∣∣∣∣ F t−1] = E [ ft(R1:t−1, C1:t−1)1 {Pt > λt}\n(1− λt)g(R̃t→11:T )\n∣∣∣∣∣ F t−1 ]\n(i) ≥ E\n[ ft(R1:t−1, C1:t−1)\ng(R̃t→11:T )\n∣∣∣∣∣ F t−1 ]\n(ii) ≥ E [ ft(R1:t−1, C1:t−1)\ng(R1:T ) ∣∣∣∣ F t−1] , where inequality (i) follows by taking an expectation only with respect to Pt by invoking the conditional superuniformity property (4); and inequality (ii) follows because g(R1:T ) ≥ g(R̃t→11:T ) since Ri ≥ R̃i for all i by monotonicity of the online FDR rule. This concludes the proof of the lemma.\nWe now proceed to using the above lemma to prove Proposition 1 and Theorem 1."
  }, {
    "heading": "3.1. Proof of Proposition 1",
    "text": "Statement (a) is proved by noting that for any time t ∈ N, we have:\nE [ |H0 ∩R(t)| ] = ∑ j≤t,j∈H0 E [1 {Pj ≤ αj}]\n≤ ∑\nj≤t,j∈H0 E [αj ] ,\nwhere the inequality follows after taking iterated expectations by conditioning on F j−1, and then applying the conditional super-uniformity property (3).\nIf we have FDP∗(t) : = 1|R(t)| ∑\nj≤t,j∈H0 αj ≤ α, as assumed\nin statement (b), then it follows that:\n∑ j≤t,j∈H0 E [αj ] = E  ∑ j≤t,j∈H0 αj  ≤ αE [|R(t)|] ,\nusing linearity of expectation and the assumption on FDP∗(t). Using part (a) and rearranging yields the inequality mFDR(t) : = E[|H0∩R(t)|]\nE[|R(t)|] ≤ α, which concludes the proof of part (b).\nIf, in addition, the null p-values are independent of each other and of the non-nulls and the sequence {αt} is monotone, we can use the following argument to prove claims (c) and (d). These claims establish that the procedure controls the FDR at any time t ∈ N. Still assuming the inequality FDP∗(t) ≤ α, we have:\nFDR(t) = E [ |H0 ∩R(t)| |R(t)| ] =\n∑ j≤t,j∈H0 E [ 1 {Pj ≤ αj} |R(t)| ························ ]\n≤ ∑ j≤t,j∈H0 E [ αj |R(t)| ············ ] = E [FDP∗(t)] ≤ α,\nwhere the first inequality follows after taking iterated expectations by conditioning on F j−1, and then applying the super-uniformity lemma (Ramdas et al., 2017), the following equality uses linearity of expectation, and the final inequality follows by the assumption on FDP∗(t). This concludes the proof of both statements (c) and (d)."
  }, {
    "heading": "3.2. Proof of Theorem 1",
    "text": "First note that, for any time t ∈ N, we have: E [ |H0 ∩R(t)| ] = ∑ j≤t,j∈H0 E [1 {Pj ≤ αj}]\n(i) ≤ ∑\nj≤t,j∈H0 E [αj ]\n(ii)\n≤ E  ∑ j≤t,j∈H0 αj 1 {Pj > λj} 1− λj  , where inequality (i) first uses the law of iterated expectations by conditioning onF j−1, and then both (i) and (ii) apply the conditional super-uniformity property (4), which concludes the proof of part (a). To prove part (b), we drop the condition j ∈ H0 from the last expectation, and use the assumption\nthat F̂DPλ(t) : = ∑ j≤t αj 1{Pj>λj} 1−λj |R(t)| ≤ α to obtain:\nE  ∑ j≤t,j∈H0 αj 1 {Pj > λj} 1− λj  ≤ αE [|R(t)|] . Combining this inequality with the result of part (a), and rearranging the terms, we reach the conclusion that mFDR(t) ≤ α, as desired. Under the independence and monotonicity assumptions of parts (c, d), we have\nFDR(t) = E [ |H0 ∩R(t)| |R(t)| ] =\n∑ j≤t,j∈H0 E [ 1 {Pj ≤ αj} |R(t)| ························ ]\n(iii) ≤ ∑ j≤t,j∈H0 E [ αj |R(t)| ············ ] (iv)\n≤ ∑ j≤t,j∈H0 E [ αj1 {Pj > λj} (1− λj)|R(t)| ····························· ] ,\nwhere inequality (iii) first uses iterated expectations by conditioning on F j−1, and then both (iii) and (iv) apply Lemma 1. Assuming that the inequality F̂DPλ(t) ≤ α holds, it follows that:∑ j≤t,j∈H0 E [ αj1 {Pj > λj} (1− λj)|R(t)| ····························· ] (v) ≤ E [∑ j≤t αj1 {Pj > λj} (1− λj)|R(t)| ········································ ]\n(vi) = E [ F̂DPλ(t) ] (vii)\n≤ α,\nwhere inequality (v) follows by linearity of expectation and summing over a larger set of indices; equality (vi) simply uses the definition of F̂DPλ(t), and inequality (vii) follows by the assumption, hence proving parts (c,d)."
  }, {
    "heading": "4. Numerical Simulations",
    "text": "In this section, we provide the results of some numerical experiments that compare the performance of SAFFRON with current state-of-the-art algorithms for online FDR control, namely the aforementioned LORD and alpha-investing procedures.2 For each method, we provide empirical evaluations of its power while ensuring that the FDR remains below a chosen value. We only run simulations since for real data, we would not know the ground truth and hence which discoveries are true or false.\nThe following two subsections separately analyze two experimental settings - one in which the p-values are computed from Gaussian observations, and another in which the pvalues under the alternative are drawn from a beta distribution. In both cases, SAFFRON outperforms the competing algorithms, with the exact level of performance depending on the choice of sequence {γj}. All experiments use a target FDR of α = 0.05 and estimate the FDR and power by averaging over 200 independent trials. As previously mentioned, the constant sequence λj = 1/2 for all j was found to be particularly successful, so this is our default choice, and we drop the index for simplicity."
  }, {
    "heading": "4.1. Testing with Gaussian Observations",
    "text": "We use the simple experimental setup of testing the mean of a Gaussian distribution with T = 1000 components. More precisely, for each index i ∈ {1, . . . , T}, the null hypothesis takes the form Hi : µi = 0. The observations consist of independent Gaussian variates Zi ∼ N(µi, 1), which are converted into one-sided p-values using the transform Pi = Φ(−Zi), where Φ is the standard Gaussian CDF. The motivation for one-sided conversion lies in A/B testing, where one wishes to detect larger effects, not smaller. The parameter µi is chosen according to a mixture model:\nµi = { 0 with probability 1− π1 F1 with probability π1,\nwhere the random variable F1 is of the form N(µc, 1) for some constant µc. We ran simulations for µc ∈ {2, 3}, thus seeing how changing the distance of the alternative mean to the null mean affects the performance of SAFFRON.\nIn what follows, we compare SAFFRON’s achieved power and FDR to those of LORD and alpha-investing. The constant infinite sequence γj ∝ log(j∨2)je√log j , where the proportionality constant is determined so that the sequence sums to one, was shown to be asymptotically optimal for testing Gaussian means via the LORD method in the paper (Javanmard & Montanari, 2017). Since SAFFRON loses wealth only when\n2The code for all simulations described in this section is available at: https://github.com/tijana-zrnic/SAFFRONcode\ntesting non-candidates whereas LORD loses wealth at every step, it is expected to behave more conservatively and not use up its wealth at the same rate, conditioned on both using the same sequence {γj}. For this reason, informally speaking, it can reuse this leftover wealth, hence the sequence {γj} chosen for SAFFRON is more aggressive, in the sense that more wealth is concentrated around the beginning of the sequence. In particular, we choose sequences of the form γj ∝ j−s, where the parameter s > 1 controls the aggressiveness of the procedure; the greater the constant s, the more wealth is concentrated around small values of j. We also consider these sequences for LORD, thus observing the difference in performance resulting from using a more aggressive sequence in the regime of a finite sequence of pvalues. Figures showing the power and FDR of SAFFRON and LORD by varying the aggressiveness of sequence {γj} are in the Supplementary Material.\nIn Figure 2 we consider F1 = N(2, 1), and compare the level of performance of alpha-investing, SAFFRON and LORD, the latter two using the highest performing sequence chosen among six possible sequences. Figure 1 demonstrates the same comparison for a similar but somewhat easier testing problem, with F1 = N(3, 1). Experiments indicate that increasing the fraction of non-null hypotheses allows SAFFRON to achieve a faster increase of power than LORD, thus performing considerably better than both LORD and the alpha-investing procedure in settings with a great number of non-null observations."
  }, {
    "heading": "4.2. Testing with Beta Alternatives",
    "text": "In this setting we generate the p-value sequence according to the following model:\nPi ∼ { Unif[0, 1], with probability 1− π1 Beta(m,n), with probability π1,\nwhere i ∈ [T ] and T = 1000, as before. Again we compare the performance of SAFFRON, alpha-investing and LORD in terms of the achieved power with the FDR controlled under a chosen level. For LORD, the asymptotically optimal sequence {γj} was derived in the paper (Javanmard & Montanari, 2017) and is of the form γj ∝ ( 1j log j) 1/m for m < 1 and n ≥ 1. As in the Gaussian case, for SAFFRON and additionally for LORD we consider the sequence γj ∝ j−s with varying s, which, unlike the aforementioned sequence, does not depend on the parameters of the distribution. Please refer to the Supplementary Material for plots of achieved power and FDR of SAFFRON and LORD obtained by varying the sequence. For the particular distribution of the observed p-values we choose m = 0.5 and n = 5.\nFigure 3 compares the performance of SAFFRON, LORD and alpha-investing, the first two using the highest performing sequence {γj} chosen among six considered sequences, as in the setting with Gaussian tests. Although simulations show SAFFRON performing similarly to LORD and alphainvesting for small fractions of non-null hypotheses, it significantly outperforms its competitors in terms of power and using up available wealth with a higher number of signals."
  }, {
    "heading": "5. Summary",
    "text": "This paper introduces SAFFRON, a new algorithmic framework for online mFDR and FDR control. We show empirically that SAFFRON is more powerful than existing algorithms. SAFFRON is based on a novel reverse superuniformity lemma that allows us to estimate the fraction of alpha-wealth that an algorithm spends on testing null hypotheses. One may interpret SAFFRON as an adaptive version of LORD, just as Storey-BH is an adaptive version of the Benjamini-Hochberg algorithm. Lastly, the derivation of SAFFRON is rather different from that of earlier generalized alpha-investing (GAI) algorithms, and as such provides a template for the derivation of new algorithms."
  }],
  "year": 2018,
  "references": [{
    "title": "Generalized α-investing: definitions, optimality results and application to public databases",
    "authors": ["E. Aharoni", "S. Rosset"],
    "venue": "Journal of the Royal Statistical Society, Series B (Statistical Methodology),",
    "year": 2014
  }, {
    "title": "Controlling the false discovery rate via knockoffs",
    "authors": ["R.F. Barber", "E. Candès"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
    "authors": ["Y. Benjamini", "Y. Hochberg"],
    "venue": "Journal of the Royal Statistical Society, Series B (Methodological),",
    "year": 1995
  }, {
    "title": "α-investing: a procedure for sequential control of expected false discoveries",
    "authors": ["D. Foster", "R. Stine"],
    "venue": "Journal of the Royal Statistical Society, Series B (Statistical Methodology),",
    "year": 2008
  }, {
    "title": "Online rules for control of false discovery rate and false discovery exceedance",
    "authors": ["A. Javanmard", "A. Montanari"],
    "venue": "The Annals of Statistics,",
    "year": 2017
  }, {
    "title": "Power of ordered hypothesis testing",
    "authors": ["L. Lei", "W. Fithian"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Accumulation tests for fdr control in ordered hypothesis testing",
    "authors": ["A. Li", "R.F. Barber"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2017
  }, {
    "title": "Online control of the false discovery rate with decaying memory",
    "authors": ["A. Ramdas", "F. Yang", "M. Wainwright", "M. Jordan"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "A direct approach to false discovery rates",
    "authors": ["J. Storey"],
    "venue": "Journal of the Royal Statistical Society, Series B (Statistical Methodology),",
    "year": 2002
  }, {
    "title": "Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: a unified approach",
    "authors": ["J. Storey", "J. Taylor", "D. Siegmund"],
    "venue": "Journal of the Royal Statistical Society, Series B (Statistical Methodology),",
    "year": 2004
  }, {
    "title": "The Problem of Multiple Comparisons: Introduction and Parts A, B, and C",
    "authors": ["J. Tukey"],
    "venue": "Princeton University,",
    "year": 1953
  }, {
    "title": "Multi-A(rmed)/B(andit) testing with online FDR control",
    "authors": ["F. Yang", "A. Ramdas", "K. Jamieson", "M.J. Wainwright"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2017
  }],
  "id": "SP:757cb069775e07913f4d7db122bcdd382930c847",
  "authors": [{
    "name": "Aaditya Ramdas",
    "affiliations": []
  }, {
    "name": "Tijana Zrnic",
    "affiliations": []
  }, {
    "name": "Martin J. Wainwright",
    "affiliations": []
  }, {
    "name": "Michael I. Jordan",
    "affiliations": []
  }],
  "abstractText": "In the online false discovery rate (FDR) problem, one observes a possibly infinite sequence of pvalues P1, P2, . . . , each testing a different null hypothesis, and an algorithm must pick a sequence of rejection thresholds α1, α2, . . . in an online fashion, effectively rejecting the k-th null hypothesis whenever Pk ≤ αk. Importantly, αk must be a function of the past, and cannot depend on Pk or any of the later unseen p-values, and must be chosen to guarantee that for any time t, the FDR up to time t is less than some pre-determined quantity α ∈ (0, 1). In this work, we present a powerful new framework for online FDR control that we refer to as “SAFFRON”. Like older alphainvesting algorithms, SAFFRON starts off with an error budget (called alpha-wealth) that it intelligently allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery. However, unlike older methods, SAFFRON’s threshold sequence is based on a novel estimate of the alpha fraction that it allocates to true null hypotheses. In the offline setting, algorithms that employ an estimate of the proportion of true nulls are called “adaptive”, hence SAFFRON can be seen as an online analogue of the offline Storey-BH adaptive procedure. Just as Storey-BH is typically more powerful than the Benjamini-Hochberg (BH) procedure under independence, we demonstrate that SAFFRON is also more powerful than its non-adaptive counterparts such as LORD. Departments of Statistics and Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA. Correspondence to: Aaditya Ramdas <aramdas@eecs.berkeley.edu>, Tijana Zrnic <tijana@eecs.berkeley.edu>, Martin J. Wainwright <wainwrig@eecs.berkeley.edu>, Michael I. Jordan <jordan@eecs.berkeley.edu>. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).",
  "title": "SAFFRON: an Adaptive Algorithm for Online Control of the False Discovery Rate"
}