{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1828–1836, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006).\nAlthough state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly\n∗Corresponding author: Yang Liu.\none source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007).\nInstead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly.\nHowever, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-English (see Section 5). Although posterior decoding can potentially address this problem, Liang et al. (2006) find that many-to-many alignments occur infrequently because posteriors are sharply peaked around the Viterbi alignments. We believe that this happens because their model imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of alignment posteriors (see Section 2).\nIn this work, we propose a general framework for imposing agreement constraints in joint training of unidirectional models. The central idea is to use the expectation of a loss function, which measures the disagreement between two models, to replace the original probability of agreement. This allows for many possible ways to quantify agreement. Experiments on Chinese-English translation show that our approach outperforms two state-ofthe-art baselines significantly.\n1828"
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 Asymmetric Alignment Models",
    "text": "Given a source-language sentence e ≡ eI1 = e1, . . . , eI and a target-language sentence f ≡ fJ1 = f1, . . . , fJ , a source-to-target translation model (Brown et al., 1993; Vogel et al., 1996) can be defined as\nP (f |e; θ1) = ∑ a1 P (f ,a1|e; θ1) (1)\nwhere a1 denotes the source-to-target alignment and θ1 is the set of source-to-target translation model parameters.\nLikewise, the target-to-source translation model is given by\nP (e|f ; θ2) = ∑ a2 P (e,a2|f ; θ2) (2)\nwhere a2 denotes the target-to-source alignment and θ2 is the set of target-to-source translation model parameters.\nGiven a training set D = {〈f (s), e(s)〉}Ss=1, the two models are trained independently to maximize the log-likelihood of the training data for each direction, respectively:\nL(θ1) = S∑\ns=1\nlogP (f (s)|e(s); θ1) (3)\nL(θ2) = S∑\ns=1\nlogP (e(s)|f (s); θ2) (4)\nOne key limitation of these generative models is that they are asymmetric: each target word is restricted to be aligned to exactly one source word (including the empty cept) in the sourceto-target direction and vice versa. This is undesirable because most real-world word alignments are symmetric, in which one-to-one, oneto-many, many-to-one, and many-to-many links are usually mixed. See Figure 1(a) for example. Therefore, a number of heuristic symmetrization methods such as intersection, union, and growdiag-final have been proposed to combine asym-\nmetric alignments (Och and Ney, 2003; Koehn and Hoang, 2007)."
  }, {
    "heading": "2.2 Alignment by Agreement",
    "text": "Rather than using heuristic symmetrization methods, Liang et al. (2006) propose a principled approach to jointly training of the two models via enforcing agreement:\nJ(θ1,θ2)\n= S∑\ns=1\nlogP (f (s)|e(s); θ1) +\nlogP (e(s)|f (s); θ2) + log ∑ a P (a|f (s), e(s); θ1)×\nP (a|e(s), f (s); θ2) (5) Note that the last term in Eq. (5) encourages the two models to agree on asymmetric alignments. While this strategy significantly improves alignment accuracy, the joint model is prone to generate one-to-one alignments because it imposes a hard constraint on agreement: the two models must share the same alignment when estimating the parameters by calculating the products of alignment posteriors. In Figure 1(b), the two oneto-one alignments are almost identical except for one link. This makes the posteriors to be sharply peaked around the Viterbi alignments (Liang et al., 2006). As a result, the lack of many-to-many alignments limits the benefits of joint training to end-to-end machine translation."
  }, {
    "heading": "3 Generalized Agreement for Bidirectional Alignment",
    "text": "Our intuition is that the agreement between two alignments can be defined as a loss function, which enables us to consider various ways of quantification (Section 3.1) and even to incorporate the dependency between alignments and other latent structures such as phrase segmentations (Section 3.2)."
  }, {
    "heading": "3.1 Agreement between Word Alignments",
    "text": "The key idea of generalizing agreement is to leverage loss functions that measure the difference between two unidirectional alignments. For example, the last term in Eq. (5) can be re-written as\n∑ a P (a|f (s), e(s); θ1)P (a|e(s), f (s); θ2)\n= ∑ a1 ∑ a2 P (a1|f (s), e(s); θ1)×\nP (a2|e(s), f (s); θ2)× δ(a1,a2) (6)\nNote that the last term in Eq. (6) is actually the expected value of agreement:\nEa1|f (s),e(s);θ1 [ Ea2|e(s),f (s);θ2 [ δ(a1,a2) ]] (7)\nOur idea is to replace δ(a1,a2) in Eq. (6) with an arbitrary loss function ∆(a1,a2) that measures the difference between a1 and a2. This gives the new joint training objective with generalized agreement:\nJ(θ1,θ2)\n= S∑\ns=1\nlogP (f (s)|e(s); θ1) +\nlogP (e(s)|f (s); θ2)− log ∑ a1 ∑ a2 P (a1|f (s), e(s); θ1)×\nP (a2|e(s), f (s); θ2)× ∆(a1,a2) (8)\nObviously, Liang et al. (2006)’s training objective is a special case of our framework. We refer to its loss function as hard matching:\n∆HM(a1,a2) = 1− δ(a1,a2) (9)\nWe are interested in developing a soft version of the hard matching loss function because this will help to produce many-to-many symmetric alignments. For example, in Figure 1(c), the two alignments share most links but still allow for disagreed links to capture one-to-many and many-toone links. Note that the union of the two asymmetric alignments is almost the same with the goldstandard alignment in this example.\nWhile there are many possible ways to define a soft matching loss function, we choose the difference between disagreed and agreed link counts because it is easy and efficient to calculate during search:\n∆SM(a1,a2) = |a1 ∪ a2| − 2|a1 ∩ a2| (10)\nCE EC"
  }, {
    "heading": "3.2 Agreement between Word Alignments and Phrase Segmentations",
    "text": "Our framework is very general and can be extended to include the agreement between word alignment and other latent structures such as phrase segmentations.\nThe words in a Chinese sentence often constitute phrases that are translated as units in English and vice versa. Inspired by the alignment consistency constraint widely used in translation rule extraction (Koehn et al., 2003), we make the following assumption to impose a structural agreement constraint between word alignment and phrase segmentation: source words in one source phrase should be aligned to target words belonging to the same target phrase and vice versa.\nFor example, consider the C→ E alignment in Figure 2. We segment Chinese and English sentences into phrases, which are sequences of consecutive words. Since “2002” and “APEC” belong to the same English phrase, their counterparts on the Chinese side should also belong to one phrase.\nWhile this assumption can potentially improve the correlation between word alignment and phrase-based translation, a question naturally arises: how to segment sentences into phrases? Instead of leveraging chunking, we treat phrase segmentation as a latent variable and train the\njoint alignment and segmentation model from unlabeled data in an unsupervised way.\nFormally, given a target-language sentence f ≡ fJ1 = f1, . . . , fJ , we introduce a latent variable b ≡ bJ1 = b1, . . . , bJ to denote a phrase segmentation. Each label bj ∈ {B, I,E, S}, where B denotes the beginning word of a phrase, I denotes the internal word, E denotes the ending word, and S denotes the one-word phrase. Figure 2 shows the label sequences for the sentence pair.\nWe use a first-order HMM to model phrase segmentation of a target sentence:\nP (f ; λ1) = ∑ b1 P (f ,b1; λ1) (11)\nSimilarly, the hidden Markov model for the phrase segmentation of the source sentence can be defined as\nP (e; λ2) = ∑ b2 P (e,b2; λ2) (12)\nThen, we can combine word alignment and phrase segmentation and define the joint training objective as\nJ(θ1,θ2,λ1,λ2)\n= S∑\ns=1\nlogP (f (s)|e(s); θ1) +\n1: procedure VITERBIEM(D) 2: Initialize Θ(0) 3: for all k = 1, . . . ,K do 4: Ĥ(k) ← SEARCH(D,Θ(k−1)) 5: Θ(k) ← UPDATE(D, Ĥ(k)) 6: end for 7: return Ĥ(K),Θ(K) 8: end procedure\nAlgorithm 1: A Viterbi EM algorithm for learning the joint word alignment and phrase segmentation model from bilingual corpus. D is a bilingual corpus, Θ(k) is the set of model parameters at the k-th iteration, H(k) is the set of Viterbi latent variables at the k-th iteration.\nlogP (e(s)|f (s); θ2) + logP (f (s); λ1) + logP (e(s); λ2)− log E(f (s), e(s),θ1,θ2,λ1,λ2) (13)\nwhere the expected loss is given by\nE(f (s), e(s),θ1,θ2,λ1,λ2) = ∑ a1 ∑ a2 ∑ b1 ∑ b2 P (a1|f (s), e(s); θ1)×\nP (a2|e(s), f (s); θ2)× P (b1|f (s); λ1)× P (b2|e(s); λ2)× ∆(a1,a2,b1,b2) (14)\nWe define a new loss function segmentation violation to measure the degree that an alignment violates phrase segmentations.\n∆SV(a1,a2,b1,b2)\n= J−1∑ j=1 β(a1, j,b1,b2) + I−1∑ i=1 β(a2, i,b2,b1)\n(15)\nwhere β(a1, j,b1,b2) evaluates whether two links l1 = (j, aj) and l2 = (j + 1, aj+1) violate the phrase segmentation:\n1. fj and fj+1 belong to one phrase but eaj and eaj+1 belong to two phrases, or\n2. fj and fj+1 belong to two phrases but eaj and eaj+1 belong to one phrase.\nThe β function returns 1 if there is violation and 0 otherwise.\n1: procedure SEARCH(D, Θ) 2: Ĥ← ∅ 3: for all s ∈ {1, . . . , S} do 4: â1 ← ALIGN(f (s), e(s),θ1) 5: â2 ← ALIGN(e(s), f (s),θ2) 6: b̂1 ← SEGMENT(f (s),λ1) 7: b̂2 ← SEGMENT(e(s),λ2) 8: h0 ← 〈â1, â2, b̂1, b̂2〉 9: ĥ←HILLCLIMB(f (s), e(s),h0,Θ)\n10: Ĥ← Ĥ ∪ {ĥ} 11: end for 12: return Ĥ 13: end procedure\nAlgorithm 2: A search algorithm for finding the Viterbi latent variables. â1 and â2 denote Viterbi alignments, b̂1 and b̂2 denote Viterbi segmentations. They form a starting point h0 for the hill climbing algorithm, which keeps changing alignments and segmentations until the model score does not increase. ĥ is the final set of Viterbi latent variables for one sentence.\nIn Figure 2, we use “+” to label words that do not violate the phrase segmentations and “-” to label violations.\nIn practice, we combine the two loss functions to enable word alignment and phrase segmentation to benefit each other in a joint search space:\n∆SM+SV(a1,a2,b1,b2) = ∆SM(a1,a2) + ∆SV(a1,a2,b1,b2) (16)"
  }, {
    "heading": "4 Training",
    "text": "Liang et al. (2006) indicate that it is intractable to train the joint model. For simplicity and efficiency, they exploit a simple heuristic procedure that leverages the product of posterior marginal probabilities. The intuition behind the heuristic is that links on which two models disagree should be discounted because the products of the marginals are small (Liang et al., 2006).\nUnfortunately, it is hard to develop a similar heuristic for our model that allows for arbitrary loss functions. Alternatively, we resort to a Viterbi EM algorithm, as shown in Algorithm 1. The algorithm takes the training data D = {〈f (s), e(s)〉}Ss=1 as input (line 1). We use Θ(k) = 〈θ(k)1 ,θ(k)2 ,λ(k)1 ,λ(k)2 〉 to denote the set of model parameters at the k-th iteration. After initializing the model parameters (line 2), the algorithm alternates between searching for the Viterbi alignments\nand segmentations Ĥ(k) using the SEARCH procedure (line 4) and updating model parameters using the UPDATE procedure (line 5). The algorithm terminates after running for K iterations.\nIt is challenging to search for the Viterbi alignments and segmentations because of complicated structural dependencies. As shown in Algorithm 2, our strategy is first to find Viterbi alignments and segmentations independently using the ALIGN and SEGMENT procedures (lines 4-7), which then serve as a starting point for the HILLCLIMB procedure (lines 8-9).\nFigure 3 shows three operators we use in the HILLCLIMB procedure. The MOVE operator moves a link in an alignment, the MERGE operator merges two phrases into one phrase, and the SPLIT operator splits one phrase into two smaller phrases. Note that each operator can be further divided into two variants: one for the source side and another for the target side."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Setup",
    "text": "We evaluate our approach on Chinese-English alignment and translation tasks.\nThe training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English\nword alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002).\nWe used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders.\nWe compared our approach with two state-ofthe-art generative alignment models:\n1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM,\n2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM.\nFor GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we trained joint HMMs using the default setting. The hyperparameter of posterior decoding was optimized on the development set.\nWe used first-order HMMs for both word alignment and phrase segmentation. Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterations. Note that the Chinese-to-English and English-toChinese alignments are generally non-identical but share many links (see Figure 1(c)). Then, we used the grow-diag-final heuristic to generate symmetric alignments."
  }, {
    "heading": "5.2 Comparison with GIZA++ and",
    "text": "BERKELEY\nTable 1 shows the comparison of our approach with GIZA++ and BERKELEY in terms of AER and BLEU. GIZA++ trains two asymmetric models independently and uses the grow-diagfinal (i.e., GDF) for symmetrization. BERKELEY\n1http://nlp.csai.tsinghua.edu.cn/˜ly/systems/TsinghuaAlig ner/TsinghuaAligner.html\ntrains two models jointly with the hard-matching (i.e., HM) loss function and uses posterior decoding for symmetrization.\nFor our approach, we distinguish between two variants:\n1. Imposing agreement between word alignments (i.e., word-word) that uses the soft matching loss function (i.e., SM) (see Section 3.1);\n2. Imposing agreement between word alignments and phrase segmentations (i.e., wordword, word-phrase) that uses both the soft matching and segmentation violation loss functions (i.e., SM+SV) (see Section 3.2).\nWe used the grow-diag-final heuristic for symmetrization.\nFor the alignment evaluation, we find that our approach achieves higher AER scores than the two baseline systems. One possible reason is that links in the intersection of two symmetric alignments or two symmetric models agree usually correspond to sure links in the gold-standard annotation. Our approach loosens the hard constraint on agreement\nand makes the posteriors less peaked around the Viterbi alignments.\nFor the translation evaluation, we used the phrase-based system Moses to report BLEU scores on the NIST 2008 test set. We find that both the two variants of our approach significantly outperforms the two baselines (p < 0.01)."
  }, {
    "heading": "5.3 Results on (Hierarchical) Phrase-based Translation",
    "text": "Table 2 shows the results on phrase-based and hierarchical phrase-based translation systems. We find that our approach systematically outperforms GIZA++ and BERKELEY on all NIST datasets.\nIn particular, generalizing the agreement to model the discrepancy between word alignment and phrase segmentation is consistently beneficial for improving translation quality, suggesting that it is important to introduce structural constraints into word alignment to increase the correlation between alignment and translation.\nWhile “SM+SV” improves over “SM” significantly on phrase-based translation, the margins on the hierarchical phrase-based system are relatively smaller. One possible reason is that the “SV”\nloss function can better account for phrase-based rather than hierarchical phrase-based translation. It is possible to design new loss functions tailored to hierarchical phrase-based translation.\nWe also find that the BLEU scores of BERKELEY on hierarchical phrase-based translation are much lower than those on phrase-based translation. This might result from the fact that BERKELEY is prone to produce one-to-one alignments, which are not optimal for hierarchical phrasebased translation."
  }, {
    "heading": "5.4 Agreement Evaluation",
    "text": "Table 3 compares how well two asymmetric models agree with each other among GIZA++, BERKELEY and our approach. We use F1 score to measure the degree of agreement:\n2|AC→E ∩ AE→C | |AC→E |+ |AE→C | (17)\nwhere AC→E is the set of Chinese-to-English alignments on the training data and AE→C is the set of English-to-Chinese alignments.\nIt is clear that independent training leads to low agreement and joint training results in high agreement. BERKELEY achieves the highest value of agreement because of the hard constraint."
  }, {
    "heading": "6 Related Work",
    "text": "This work is inspired by two lines of research: (1) agreement-based learning and (2) joint modeling of multiple NLP tasks."
  }, {
    "heading": "6.1 Agreement-based Learning",
    "text": "The key idea of agreement-based learning is to train a set of models jointly by encouraging them to agree on the hidden variables (Liang et al., 2006; Liang et al., 2008). This can also be seen as a particular form of posterior constraint or posterior regularization (Graça et al., 2007; Ganchev et al., 2010). The agreement is prior knowledge and\nindirect supervision, which helps to train a more reasonable model with biased guidance.\nWhile agreement-based learning provides a principled approach to training a generative model, it constrains that the sub-models must share the same output space. Our work extends (Liang et al., 2006) to introduce arbitrary loss functions that can encode prior knowledge. As a result, Liang et al. (2006)’s model is a special case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees."
  }, {
    "heading": "6.2 Joint Modeling of Multiple NLP Tasks",
    "text": "It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013).\nAmong them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding."
  }, {
    "heading": "7 Conclusion",
    "text": "We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viterbi EM algorithm to train the joint model. Experiments on Chinese-English translation show that joint training with generalized agreement achieves\nsignificant improvements over two baselines for (hierarchical) phrase-based MT systems. In the future, we plan to investigate more loss functions to account for syntactic constraints."
  }, {
    "heading": "Acknowledgments",
    "text": "Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808), the National Natural Science Foundation of China (No. 61331013 and No. 61432013), and Samsung R&D Institute of China. Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme. We sincerely thank the reviewers for their valuable suggestions. We also thank Yue Zhang, Meng Zhang and Shiqi Shen for their insightful discussions."
  }],
  "year": 2015,
  "references": [{
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."],
    "venue": "Computational Linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Joint parsing and alignment with weakly synchronized grammars",
    "authors": ["David Burkett", "John Blitzer", "Dan Klein."],
    "venue": "Proceedings of NAACL-HLT 2010.",
    "year": 2010
  }, {
    "title": "On jointly recognizing and aligning bilingual named entities",
    "authors": ["Yufeng Chen", "Chengqing Zong", "Keh-Yih Su."],
    "venue": "Proceedings of ACL 2010.",
    "year": 2010
  }, {
    "title": "Hierarchical phrase-based translation",
    "authors": ["David Chiang."],
    "venue": "Computational Linguistics, 33(2):201–228.",
    "year": 2007
  }, {
    "title": "Scalable inference and training of context-rich syntactic translation models",
    "authors": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer."],
    "venue": "Proceedings of COLING-ACL 2006, pages 961–968,",
    "year": 2006
  }, {
    "title": "Posterior regularization for structured latent variable models",
    "authors": ["Kuzmann Ganchev", "João Graça", "Jennifer Gillenwater", "Ben Taskar."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2010
  }, {
    "title": "Expectation maximization and posterior constraints",
    "authors": ["Joao V Graça", "Kuzman Ganchev", "Ben Taskar."],
    "venue": "Proceedings of NIPS 2007.",
    "year": 2007
  }, {
    "title": "Factored translation models",
    "authors": ["Philipp Koehn", "Hieu Hoang."],
    "venue": "Proceedings of EMNLPCoNLL 2007, pages 868–876, Prague, Czech Republic, June.",
    "year": 2007
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu."],
    "venue": "Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton, Canada, May.",
    "year": 2003
  }, {
    "title": "Alignment by agreement",
    "authors": ["Percy Liang", "Ben Taskar", "Dan Klein."],
    "venue": "Proceedings of HLT-NAACL 2006, pages 104–111, New York City, USA, June.",
    "year": 2006
  }, {
    "title": "Agreement-based learning",
    "authors": ["Percy Liang", "Dan Klein", "Michael I. Jordan."],
    "venue": "Advances in Neural Information Processing Systems (NIPS).",
    "year": 2008
  }, {
    "title": "Joint parsing and translation",
    "authors": ["Yang Liu", "Qun Liu."],
    "venue": "Proceedings of COLING 2010.",
    "year": 2010
  }, {
    "title": "Treeto-string alignment template for statistical machine translation",
    "authors": ["Yang Liu", "Qun Liu", "Shouxun Lin."],
    "venue": "Proceedings of COLING/ACL 2006.",
    "year": 2006
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz J. Och", "Hermann Ney."],
    "venue": "Computational Linguistics, 29(1):19–51.",
    "year": 2003
  }, {
    "title": "Bleu: a methof for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of ACL 2002.",
    "year": 2002
  }, {
    "title": "Srilm - an extensible language modeling toolkit",
    "authors": ["Andreas Stolcke."],
    "venue": "Proceedings of ICSLP 2002.",
    "year": 2002
  }, {
    "title": "Hmm-based word alignment in statistical translation",
    "authors": ["Stephan Vogel", "Hermann Ney", "Christoph Tillmann."],
    "venue": "Proceedings of COLING 1996.",
    "year": 1996
  }, {
    "title": "Joint word alignment and bilingual named entity recognition using dual decomposition",
    "authors": ["Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning."],
    "venue": "Proceedings of ACL 2013.",
    "year": 2013
  }, {
    "title": "Joint tokenization and translation",
    "authors": ["Xinyan Xiao", "Yang Liu", "Young-Sook Hwang", "Qun Liu", "Shouxun Lin."],
    "venue": "Proceedings of COLING 2010.",
    "year": 2010
  }, {
    "title": "Integrated phrase segmentation and alignment algorithm for statistical machine translation",
    "authors": ["Ying Zhang", "Stephan Vogel", "Alex Waibel."],
    "venue": "Proceedings of Natural Language Processing and Knowledge Engineering, 2003. IEEE.",
    "year": 2003
  }],
  "id": "SP:5c18187c38f93529885b489673aac0b09e78ac27",
  "authors": [{
    "name": "Chunyang Liu",
    "affiliations": []
  }, {
    "name": "Yang Liu",
    "affiliations": []
  }, {
    "name": "Huanbo Luan",
    "affiliations": []
  }, {
    "name": "Maosong Sun",
    "affiliations": []
  }, {
    "name": "Heng Yu",
    "affiliations": []
  }],
  "abstractText": "While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.",
  "title": "Generalized Agreement for Bidirectional Word Alignment"
}