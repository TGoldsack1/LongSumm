{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Submodular Functions (Fujishige, 2005) are a special class of set functions, which have rich structures and a lot of links\n1Tencent AI Lab 2State Key Lab of CAD&CG, Zhejiang University. Correspondence to: Weizhong Zhang <zhangweizhongzju@gmail.com>, Bin Hong <hongbinzju@gmail.com>, Lin Ma <forest.linma@gmail.com>, Wei Liu <wl2223@columbia.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwith convex functions. They arise naturally in many domains, such as clustering (Narasimhan & Bilmes, 2007), image segmentation (Kolmogorov & Zabin, 2004; Cevher et al., 2009), document summarization (Lin & Bilmes, 2011a), etc. Most of these applications can be finally deduced to a Submodular Function Minimization (SFM) problem:\nmin A⊆V F (A), (SFM)\nwhere F (A) is a submodular function defined on a set V . The problem of SFM has been extensively studied for several decades in the literatures (Edmonds, 1970; Lovász, 1983; McCormick, 2005; Wu et al., 2016; Ene et al., 2017), in which many algorithms have been developed from the perspectives of combinatorial optimization and convex optimization. The most well-known conclusion is that SFM is solvable in strongly polynomial time (Iwata et al., 2001). Unfortunately, due to the high-degree polynomial dependence, the applications of submodular functions on the large scale problems remain challenging, such as image segmentation (Cevher et al., 2009) and speech analysis (Lin & Bilmes, 2011b), which both involve a huge number of variables.\nScreening (El Ghaoui et al., 2012) is an emerging technique, which has been proved to be effective in accelerating large-scale sparse model training. It is motivated by the well-known feature of sparse models that a significant portion of the coefficients in the optimal solutions of them (resp. their dual problems) are zeros, that is, the corresponding features (resp. samples) are irrelevant with the final learned models. Screening methods aim to quickly identify these irrelevant features and/or samples and remove them from the datasets before or during the training process. Thus, the problem size can be reduced dramatically, leading to substantial savings in the computational cost. The framework of these methods is given in Algorithm 1. Since screening methods are always independent of the training algorithms, they can be integrated with all the algorithms flexibly. In the recent few years, specific screening methods for most of the traditional sparse models have been developed, such as Lasso (Tibshirani et al., 2012; Wang et al., 2013; Wang & Ye, 2015), sparse logistic regression (Wang et al., 2014), multi-task learning (Ndiaye et al., 2015) and SVM (Ogawa et al., 2013; Zhang et al., 2017). Empirical studies indicate that the speedups they achieved can be orders of magnitudes.\nAlgorithm 1 Framework of screening in sparse learning 1: Estimate the dual (resp. primal) optimum of the sparse\nmodel. 2: Based on the estimation above, infer which components\nof the primal (resp. dual ) optimum are zeros from the KKT conditions. 3: Remove the features (resp. samples) corresponding to the identified components. 4: Train the model on the reduced dataset.\nThe binary attribute (each element in V must be either in or not in the optimal solution) of SFM motivates us to introduce the key idea of screening into SFM to accelerate its optimization process. The most intuitive approach is to identify the elements that are guaranteed to be included or excluded in the minimizer A∗ of SFM prior to or during actually solving it. Then, by fixing the identified active elements and removing the inactive ones, we just need to solve a small-scale problem. However, we note that existing screening methods are all developed for convex models and they cannot be applied to SFM directly. The reason is that they all heavily depend on KKT conditions (see Algorithm 1), which do not exist in SFM problems.\nIn this paper, to improve the efficiency of SFM algorithms, we propose a novel Inactive and Active Element Screening (IAES) framework for SFM, which consists of two kinds of screening rules, i.e., Inactive Elements Screening (IES) and Active Elements Screening (AES). As we analyze above, the major challenge in developing IAES is the absence of KKT conditions. We bypass this obstacle by carefully studying the relationship between SFM and convex optimization, which can be regarded as another form of KKT conditions. We find that SFM is closely related to a particular convex primal and dual problem pair Q-P and Q-D (see Section 2), that is, the minimizer of SFM can be obtained from the positive components of the optimum of problem Q-P. Hence, the proposed IAES identifies the active and inactive elements by estimating the lower and upper bounds of the components of the optimum of problem Q-P. Thus, one of our major technical contributions is a novel framework (Section 3)—developed by carefully studying the strong convexity of the corresponding primal and dual objective functions, the structure of the base polyhedra, and the optimality conditions of the SFM problem—for deriving accurate optimum estimation of problem Q-P. We integrate IAES with the solver for problems Q-P and Q-D. As the solver goes on, and the estimation becomes more and more accurate, IAES can identify more and more elements. By fixing the active elements and removing the inactive ones, the problem size can be reduced gradually. IAES is safe in the sense that it would never sacrifice any accuracy on the final output. To the best of our knowledge, IAES is the first screening\nmethod in the domain of SFM or even combinatorial optimization. Moreover, compared with the screening methods for sparse models, an outstanding feature of IAES is that it has no theoretical limit in reducing the problem size. That is, we can finally reduce the problem size to zero, leading to substantial savings in the computational cost. The reason is that as the optimization proceeds, our estimation will be accurate enough to infer the affiliations of all the elements with the optimizer A∗. While in sparse models, screening methods can never reduce the problem size to zero since the features (resp. samples) with nonzero coefficients in the primal (resp. dual) optimum can never be removed. Experiments (see Section 4) on both synthetic and real datasets demonstrate the significant speedups gained by IAES. For the convenience of presentation, we postpone the detailed proofs of theoretical results in the main text to the supplementary materials.\nNotations: We consider a set V = {1, ..., p}, and denote its power set by 2V , which is composed of 2p subsets of V . |A| is the cardinality of a set A. A∪B and A∩B are the union and intersection of the sets A and B, respectively. A ⊆ B means that A is a subset of B, potentially being equal to B. Moreover, for w ∈ Rp and α ∈ R, we let [w]k be the k-th component of w and {w ≥ α} (resp. {w > α}) be the weak (resp. strong) α-sup-level set of w defined as {k : k ∈ V, [w]k ≥ α} (resp. {k : k ∈ V, [w]k > α}). At last, for s ∈ Rp, we define a set function by s(A) = ∑ k∈A[s]k."
  }, {
    "heading": "2. Basics and Motivations",
    "text": "This section is composed of two parts: a) briefly review some basics of submodular functions, SFM, and their relations with convex optimization; b) motivate our screening method IAES.\nThe followings are the definitions of submodular function, submodular polyhedra and base polyhedra, which play an important role in submodular analysis. Definition 1. [Submodular Function (McCormick, 2005)] A set function F : 2V → R is submodular if and only if for all subsets A,B ⊆ V we have:\nF (A) + F (B) ≥ F (A ∪B) + F (A ∩B).\nDefinition 2. [Submodular and Base Polyhedra (Fujishige, 2005)] Let F be a submodular function such that F (∅) = 0. The submodular polyhedra P (F ) and the base polyhedra B(F ) are defined as:\nP (F )={s∈ Rp : ∀A ⊆ V, s(A) ≤ F (A)}, B(F )={s∈ Rp : s(V ) = F (V ),∀A ⊆ V, s(A) ≤ F (A)}.\nBelow we give the definition of Lovász extension, which works as the bridge that connects submodular functions and convex functions.\nDefinition 3. [Lovász Extension (Fujishige, 2005)] Given a set-function F such that F (∅) = 0, the Lovász extension f : Rp → R is defined as follows: for w ∈ Rp, order the components in a decreasing order [w]j1 ≥ ... ≥ [w]jp , and define f(w) through the equation below,\nf(w) = p∑ k=1 [w]jk ( F ({j1, ..., jk})− F ({j1, ..., jk−1}) ) .\nLovász extension f(w) is convex if and only if F is submodular (see (Fujishige, 2005)).\nWe focus on the generic submodular function minimization problem SFM defined in Section 1 and denote its minimizer as A∗. To reveal the relationship between SFM and convex optimization and finally motivate our method, we need the following theorems. Theorem 1. Let ψ1, ..., ψp be p convex functions on R, ψ∗1 , ..., ψ ∗ p be their Fenchel-conjugates (Borwein & Lewis, 2010), and f be the Lovász extension of a submodular function F . Denote the subgradient of ψk(·) by ∂ψk(·). Then, the followings hold: (i) The problems below are dual of each other:\nmin w∈Rp f(w) + p∑ j=1 ψj([w]j), (P)\nmax s∈B(F ) − p∑ j=1 ψ∗j (−[s]j). (D)\n(ii) The pair (w∗, s∗) is optimal for problems (P) and (D) if and only if{\n(a): [s]∗k ∈ −∂ψk([w]∗k),∀k ∈ V, (b): w∗ ∈ NB(F )(s∗),\n(Opt)\nwhere NB(F )(s∗) is the normal cone (see Chapter 2 of (Borwein & Lewis, 2010)) of B(F ) at s∗.\nWhen ψj(·) is differentiable, we consider a sequence of set optimization problems parameterized by α ∈ R:\nmin A⊆V F (A) + ∑ j∈A ∇ψj(α), (SFM’)\nwhere ∇ψj(·) is the gradient of ψk(·). The problem SFM’ has tight connections with the convex optimization problem P (see the theorem below). Theorem 2. [Submodular function minimization from the proximal problem, Proposition 8.4 in (Bach et al., 2013)] Under the same assumptions in Theorem 1, if ψj(·) is differentiable for all j ∈ V and w∗ is the unique minimizer of problem P, then for all α ∈ R, the minimal minimizer of problem SFM’ is {u > α} and the maximal minimizer is {u ≥ α}, that is, for any minimizers A∗α we have:\n{w∗ > α} ⊆ A∗α ⊆ {w∗ ≥ α}. (1)\nBy choosing ψj(x) = 12x 2 and α = 0 in SFM’, combining Theorems 1 and 2, we can see that SFM can be reduced to the following primal and dual problems, one is a quadratic optimization problem and the other is equivalent to finding the minimum norm point in the base polytope B(F ):\nmin w∈Rp\nP (w) := f(w) + 1\n2 ‖w‖22, (Q-P)\nmax s∈B(F ) D(s) := −1 2 ‖s‖22. (Q-D)\nAccording to (1), we can define two index sets:\nE := {j ∈ V : [w]∗j > 0}, and G := {j ∈ V : [w]∗j < 0},\nwhich imply that\n(i): j ∈ E ⇒ j ∈ A∗, (R1) (ii): j ∈ G ⇒ j /∈ A∗. (R2)\nWe call the j-th element active if j ∈ E and the ones in G inactive.\nSuppose that we are given two subsets of E and G, by rules R1 and R2, we can see that many affiliations between A∗ and the elements of V can be deduced. Thus, we have less unknowns to solve in SFM and its size can be dramatically reduced. We formalize this idea in Lemma 1. Lemma 1. Given two subsets Ĝ ⊆ G and Ê ⊆ E , the followings hold:\n(i): Ê ⊆ A∗, and for all j ∈ Ĝ we have j /∈ A∗.\n(ii): The problem SFM can be reduced to the following scaled problem:\nmin C⊆V/(Ê∪Ĝ)\nF̂ (C) := F (Ê ∪ C)− F (Ê), (scaled-SFM)\nwhich is also an SFM problem.\n(iii): A∗ can be recovered by A∗ = Ê ∪ C∗, where C∗ is the minimizer of scaled-SFM.\nLemma 1 indicates that, if we can identify the active set Ê and inactive set Ĝ, we only need to solve a scaled problem scaled-SFM, which may have much smaller size than the original problem SFM, to exactly recover the optimal solution A∗ without sacrificing any accuracy.\nHowever, since w∗ is unknown, we cannot directly apply rules R1 and R2 to identify the active set Ê and inactive set Ĝ. Inspired by the ideas in the gap safe screening methods ((Fercoq et al., 2015; Ndiaye et al., 2016; Shibagaki et al., 2016)) for convex problems, we can first estimate the region W that contains w∗ and then relax the rules R1 and R2 to the practicable versions. Specifically, we first denote\nÊ := {j ∈ V : min w∈W [w]j > 0}, (2)\nĜ := {j ∈ V : max w∈W [w]j < 0}. (3)\nIt is obvious that Ê ⊆ E and Ĝ ⊆ G. Hence, the rules R1 and R2 can be relaxed as follows:\n(i): j ∈ Ê ⇒ j ∈ A∗, (R1’) (ii): j ∈ Ĝ ⇒ j /∈ A∗. (R2’)\nIn view of the rules R1’ and R2’, we sketch the development of IAES as follows: Step 1: Derive the estimationW such that w∗ ∈ W . Step 2: Develop IAES via deriving the detailed screening rules R1’ and R2’."
  }, {
    "heading": "3. The Proposed Element Screening Method",
    "text": "In this section, we first present the accurate optimum estimation by carefully studying the strong convexity of the functions P (w) andD(s), the optimality conditions of SFM and its relationship with the convex problem pair (see Section 3.1). Then, in Section 3.2, we develop our inactive and active element screening rules IES and AES step by step. At last, in Section 3.3, we develop the screening framework IAES by an alternating application of IES and AES."
  }, {
    "heading": "3.1. Optimum Estimation",
    "text": "Let Ê and Ĝ be the active and inactive sets identified by the previous IAES steps (before applying IAES for the first time, they are ∅). From Lemma 1, we know that the problem SFM then can be reduced to the following scaled problem:\nmin C⊆V̂\nF̂ (C) := F (Ê ∪ C)− F (Ê),\nwhere V̂ = V/(Ê ∪ Ĝ). The second term−F (Ê) at the right side of the equation above is added to make F̂ (∅) = 0. Thus, the corresponding problems Q-P and Q-D then become:\nmin ŵ∈Rp̂\nP̂ (ŵ) := f̂(ŵ) + 1\n2 ‖ŵ‖22, (Q-P’)\nmax ŝ∈B(F̂ ) D̂(ŝ) := −1 2 ‖ŝ‖22, (Q-D’)\nwhere f̂(ŵ) is the Lovász extension of F̂ and p̂ = |V/(Ê ∪ Ĝ)|. Now, we turn to estimate the minimizer ŵ∗ of the problem Q-P’. The result is presented in the theorem below.\nTheorem 3. For any ŵ ∈ domP̂ (ŵ), ŝ ∈ B(F̂ ) and C ⊆ V̂ , we denote the dual gap as G(ŵ, ŝ) = P̂ (ŵ) − D̂(ŝ), and then we have\nŵ∗ ∈ W = B ∩ Ω ∩ P,\nwhere B = { w : ‖w − ŵ‖ ≤ √ 2G(ŵ, ŝ) } , Ω = { w :\nF̂ (V̂ ) − 2F̂ (C) ≤ ‖w‖1 ≤ ‖ŝ‖1 } , and P = { w :\n〈w,1〉 = −F̂ (V̂ ) } .\nFrom the theorem above, we can see that the estimation W is the intersection of three sets: the ball B, the `1-norm equipped spherical shell Ω and the planeP . As the optimizer goes on, the dual gapG(ŵ, ŝ) becomes smaller, and F̂ (V̂ )− 2F̂ (C) and ‖ŵ‖1 would converge to ‖ŵ∗‖1 (see Chapter 7 of (Bach et al., 2013)). Thus, the volumes of B and Ω become smaller and smaller during the optimization process, and the estimationW would be more and more accurate."
  }, {
    "heading": "3.2. Inactive and Active Element Screening",
    "text": "We now turn to develop the screening rules IES and AES based on the estimation of the optimum ŵ∗.\nFrom (2) and (3), we can see that, to develop the screening rules we need to solve two problems: minw∈W [w]j and maxw∈W [w]j . However, since W is highly non-convex and has a complex structure, it is very hard to solve these two problems efficiently. Hence, we rewrite the estimation W asW = (B ∩ P) ∩ (B ∩ Ω), and develop two different screening rules on B ∩ P and B ∩ Ω, respectively."
  }, {
    "heading": "3.2.1. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ P",
    "text": "Given the estimation B ∩ P , we derive the screening rules by solving the following problems\nmin w∈B∩P [w]j and max w∈B∩P [w]j .\nWe show that both of the two problems above admit closedform solutions. Lemma 2. Given the estimation ball B, the plane P and the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, for all j ∈ [p̂] we denote\nbj = 2 (∑ i 6=j [ŵ]i + F̂ (V̂ )− (p̂− 1)[ŵ]j ) ,\ncj = (∑ i 6=j [ŵ]i + F̂ (V̂ ) )2 − (p̂− 1) ( 2G(ŵ, ŝ)− [ŵ]2j ) .\nThen the followings hold:\n(i): min w∈B∩P [w]j = [w] min j :=\n−bj − √ b2j − 4p̂cj\n2p̂ ,\n(ii): max w∈B∩P [w]j = [w] max j :=\n−bj + √ b2j − 4p̂cj\n2p̂ .\nWe are now ready to present the active and inactive screening rules AES-1 and IES-1. Theorem 4. Given the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, we have\n(i): The active element screening rule takes the form of\n[w]minj > 0⇒ j ∈ A∗,∀j ∈ V/(Ê ∪ Ĝ). (AES-1)\n(ii): The inactive element screening rule takes the form of\n[w]maxj < 0⇒ j /∈ A∗,∀j ∈ V/(Ê ∪ Ĝ). (IES-1)\n(iii): The active and inactive sets Ê and Ĝ can be updated by\nÊ ← Ê ∪∆Ê , (4) Ĝ ← Ĝ ∪∆Ĝ, (5)\nwhere ∆Ê and ∆Ĝ are the newly identified active and inactive sets defined as\n∆Ê := {j ∈ V/(Ê ∪ Ĝ) : [w]minj > 0},\n∆Ĝ := {j ∈ V/(Ê ∪ Ĝ) : [w]maxj < 0}.\nFrom the theorem above, we can see that our rules AES-1 and IES-1 are safe in the sense that the detected elements are guaranteed to be included or excluded in A∗."
  }, {
    "heading": "3.2.2. INACTIVE AND ACTIVE ELEMENT SCREENING BASED ON B ∩ Ω",
    "text": "We now derive the second screening rule pair based on the estimation B ∩ Ω.\nDue to the high non-convexity and complex structure of B ∩ Ω, directly solving problems minw∈B∩Ω[w]j and maxw∈B∩Ω[w]j is time consuming. Notice that, to derive IAS and IES, we only need to judge whether the inequalities minw∈B∩Ω[w]j > 0 and maxw∈B∩Ω[w]j < 0 are satisfied or not, instead of calculating minw∈B∩Ω[w]j and maxw∈B∩Ω[w]j . Hence, we only need to infer the hypotheses { w : w ∈ B, [w]j ≤ 0 } ∩ Ω = ∅ and{\nw : w ∈ B, [w]j ≥ 0 } ∩ Ω = ∅ are true or false. Thus, from the formulation of Ω (see Theorem 3), the problems boil down to calculating the minimum and the maximum of ‖w‖1 with { w : w ∈ B, [w]j ≥ 0 } or{\nw : w ∈ B, [w]j ≤ 0 }\n, which admit closed-form solutions. The results are presented in the lemma below.\nLemma 3. Given the estimation ball B and the active and inactive sets Ê and Ĝ, which are identified in the previous IAES steps, then the followings hold:\n(i): ∀j ∈ p̂, if |[ŵ]j | > √\n2G(ŵ, ŝ), then the element j can be identified by rule AES-1 or IES-1 to be active or inactive. (ii): ∀j ∈ p̂, if 0 < [ŵ]j ≤ √ 2G(ŵ, ŝ), we have\nmin w∈B,[w]j≤0\n‖w‖1 < ‖ŵ‖1,\nmax w∈B,[w]j≤0\n‖w‖1\n= ‖ŵ‖1−2[ŵ]j+ √ 2p̂G(ŵ,ŝ), if [ŵ]j− √ 2G(ŵ,ŝ) p̂ <0,\n‖ŵ‖1−[ŵ]j+ √ p̂−1 √ 2G(ŵ,ŝ)−[ŵ]2j , otherwise.\n(iii): ∀j ∈ p̂, if − √ 2G(ŵ, ŝ) ≤ [ŵ]j < 0, we have\nmin w∈B,[w]j≥0\n‖w‖1 < ‖ŵ‖1,\nmax w∈B,[w]j≥0\n‖w‖1\n= ‖ŵ‖1+2[ŵ]j+ √ 2p̂G(ŵ,ŝ), if [ŵ]j+ √ 2G(ŵ,ŝ) p̂ >0,\n‖ŵ‖1+[ŵ]j+ √ p̂−1 √ 2G(ŵ,ŝ)−[ŵ]2j , otherwise.\nWe are now ready to present the second active and inactive screening rule pair AES-2 and IES-2. From the lemma above, we can see that the element j with |[ŵ]j | >√\n2G(ŵ, ŝ) can be screened by rules AES-1 and IES-1. Hence, we now only need to consider the cases when |[ŵ]j | ≤ √ 2G(ŵ, ŝ).\nTheorem 5. Given a set C ⊆ V̂ and the active and inactive sets Ê and Ĝ identified in the previous IAES steps, then,\n(i): The active element screening rule takes the form of{ 0 < [ŵ]j ≤ √ 2G(ŵ, ŝ)\nmaxw∈B,[w]j≤0 ‖w‖1 < F̂ (V̂ )− 2F̂ (C)\n⇒j ∈ A∗,∀j ∈ V/(Ê ∪ Ĝ). (AES-2)\n(ii): The inactive element screening rule takes the form of{ − √\n2G(ŵ, ŝ) ≤ [ŵ]j < 0 maxw∈B,[w]j≥0 ‖w‖1 < F̂ (V̂ )− 2F̂ (C)\n⇒j /∈ A∗,∀j ∈ V/(Ê ∪ Ĝ). (IES-2)\n(iii): The active and inactive sets Ê and Ĝ can be updated by\nÊ ← Ê ∪∆Ê , (6) Ĝ ← Ĝ ∪∆Ĝ, (7)\nwhere ∆Ê and ∆Ĝ are the newly identified active and inactive sets defined as\n∆Ê := { j ∈ V/(Ê ∪ Ĝ) : 0 < [ŵ]j ≤ √ 2G(ŵ, ŝ),\nmax w∈B,[w]j≤0\n‖w‖1 < F̂ (V̂ )− 2F̂ (C) } ,\n∆Ĝ := { j ∈ V/(Ê ∪ Ĝ) : − √ 2G(ŵ, ŝ) ≤ [ŵ]j < 0,\nmax w∈B,[w]j≥0\n‖w‖1 < F̂ (V̂ )− 2F̂ (C) } .\nTheorem 5 verifies the safety of AES-2 and IES-2."
  }, {
    "heading": "3.3. The Proposed IAES Framework by An Alternating Execution of AES and IES",
    "text": "To reinforce the capability of the proposed screening rules, we develop a novel framework IAES in Algorithm 2, which\napplies the active element screening rules (AES-1 and AES2) and the inactive element screening rules (IES-1 and IES2) in an alternating manner during the optimization process. Specifically, we integrate our screening rules AES-1, AES-2, IES-1 and IES-2 with the optimization algorithm A for the problems Q-P’ and Q-D’. During the optimization process, we trigger the screening rules AES-1, AES-2, IES-1 and IES-2 every time when the dual gap is 1− ρ times smaller than itself in the last triggering of IAES. As the solver A goes on, the volumes of Ω and B would decrease to zeros quickly, IAES can thus identify more and more inactive and active elements.\nCompared with the existing screening methods for convex sparse models, an appealing feature of IAES is that it has no theoretical limit in identifying the inactive and active elements and reducing the problem size. The reason is that, in convex sparse models, screening models can never rule out the features and samples whose corresponding coefficients in the optimal solution are nonzero. While in our case, as the optimizer A goes on, our estimation will be accurate enough for us to infer the affiliation of each element with A∗. Hence, we can finally identify all the inactive and active elements and the problem size can be reduced to zero. This nice feature can lead to significant speedups in the computation time.\nRemark 1. The setC in Algorithm 2 is updated by choosing one of the super-level sets of ŵ with the smallest value F̂ (C). It is free to obtain it. The reason is that most of the existing methods A for the problems Q-P’ and Q-D’ need to calculate f̂(ŵ) in each iteration, in which they need to calculate the value F̂ at all of the super-level sets of ŵ (see the greedy algorithm in (Bach et al., 2013) for details).\nRemark 2. The algorithm A can be all the methods for the problems Q-P’ and Q-D’, such as minimum-norm point algorithm (Wolfe, 1976) and conditional gradient descent (Dunn & Harshbarger, 1978). Although some algorithms only update s, in IAES, we can update w in each iteration by letting w = −s and refining it by the algorithm named pool adjacent violators (Best & Chakravarti, 1990).\nRemark 3. Due to Lemma 1 and the safety of AES-1, AES-2, IES-1 and IES-2, we can see that IAES would never sacrifice any accuracy.\nRemark 4. Although step 14 in Algorithm 2 may increase the dual gap slightly, it is worthwhile because of the reduced problem size. This is verified by the speedups gained by IAES in the experiments.\nRemark 5. The parameter ρ in Algorithm 2 controls the frequency how often we trigger IAES. The larger value, the higher frequency to trigger IAES but more computational time consumed by IAES. In our experiment, we set ρ = 0.5 and it achieves a good performance.\nAlgorithm 2 Inactive and Active Element Screening 1: Input: an optimization algorithm A for problems (Q-\nP’) and (Q-D’), > 0, 0 < ρ < 1. 2: Initialize: Ê = Ĝ = ∅, C = ∅, g = ∞, choose ŝ ∈ B(F ) and ŵ = −ŝ. 3: repeat 4: Run A on problems (Q-P’) and (Q-D’) to update ŵ, ŝ and C. 5: if dual gap G(ŵ, ŝ) < ρg then 6: Run the active element screening rules AES-1 and AES-2 based on (ŵ, ŝ) and C. 7: Update the active set Ê by (4) and (6). 8: Run the inactive element screening rules IES-1 and IES-2 based on (ŵ, ŝ) and C. 9: Update the inactive set Ĝ by (5) and (7).\n10: if V/(Ê ∪ Ĝ) = ∅ then 11: Return: Ê . 12: else 13: Update F̂ , Q-P’, Q-D’ according to Ê and Ĝ. 14: Update ŵ and ŝ by:\nŵ← [ŵ]V/(Ê∪Ĝ),\nŝ← arg max s∈B(F̂ ) 〈ŵ, s〉.\n15: Update g ← G(ŵ, ŝ). 16: end if 17: end if 18: until G(ŵ, ŝ) < . 19: Return: Ê ∪ {ŵ > 0}."
  }, {
    "heading": "4. Experiments",
    "text": "We evaluate IAES through numerical experiments on both synthetic and real datasets by two measurements. The first one is the rejection ratios of IAES over iterations: mi+nim∗+n∗ , where mi and ni are the numbers of the active and inactive elements identified by IAES after the i-th iteration, and m∗ and n∗ are the numbers of the active and inactive elements in A∗. We notice that in our experiments m∗ + n∗ = p, so the rejection ratio presents the problem size reduced by IAES. The second measurement is speedup, i.e., the ratio of the running times of the solver without IAES and with IAES. We set the accuracy tolerance to be 10−6. Recall that, IAES can be integrated with all the solvers for the problems Q-P and Q-D. In this experiment, we use one of the most widely used algorithms minimum-norm point algorithm (MinNorm) (Wolfe, 1976) as the solver. The function F (A) varies according to the datasets, whose detailed definitions will be given in the subsequent subsections.\nWe write the code in Matlab and perform all the computations on a single core of Intel(R) Core(TM) i7-5930K 3.50GHz, 32GB MEM."
  }, {
    "heading": "4.1. Experiments on Synthetic Datasets",
    "text": "We perform experiments on a synthetic dataset named twomoons with different sample sizes (see Figure 2 for an example). All the data points are sampled from two different semicircles. Specifically, each point can be represented as x = ci + γ ∗ [cos(θi), sin(θi)], where i = 1, 2 stands for the two semicircles, c1 = [−0.5, 1], c2 = [0.5,−1], γ is generated from a normal distribution N(2, 0.52), and θ1 and θ2 are sampled from two uniform distributions [−π2 , π 2 ] and [π2 , 3π 2 ], respectively. We first sample p data points from these two semicircles with equal probability. Then, we randomly choose p0 = 16 samples and label each of them as positive if it is from the first semicircle and otherwise label it as negative. We generate five datasets by varying the sample size p in [200, 400, 600, 800, 1000]. We perform semi-supervised clustering on each dataset and the objective function F (A) is defined as:\nF (A) = I(fA, fV/A)− ∑ j∈A log ηj − ∑ j∈V/A log(1− ηj),\nwhere I(fA, fV/A) is the mutual information between two Gaussian processes with a Gaussian kernel k(x, y) = exp(−α‖x− y‖2), α = 1.5, and ηj ∈ {0, 1} if j is labeled and otherwise ηj = 12 (see Chapter 6 of (Bach et al., 2013) for more details). The kernel matrix is dense with the size p× p, leading to a big computational cost when p is large.\nFigure 1 displays the rejection ratios of IAES on two-moons. We can see that IAES can find the active and inactive elements incrementally during the optimization process. It can\nfinally identify almost all of the elements and reduce the problem size to nearly zero in no more than 400 iterations, which is consistent with our theoretical analysis in Section 3.3. Figure 3 visualizes the screening process of IAES on two-moons when p = 400. It shows that, during the optimization process, IAES identifies the elements that are easy to be classified first and then identifies the rest.\nTable 1 reports the running time of MinNorm without and with AES (AES-1 + AES-2), IES (IES-1 + IES-2) and IAES for solving the problem SFM on two-moons. We can see that the speedup of IAES can be up to 10 times. In all the datasets, IAES is significantly faster than MinNorm, MinNorm with AES or IES. At last, we can see that the time costs of AES, IES and IAES are negligible."
  }, {
    "heading": "4.2. Experiments on Real Datasets",
    "text": "In this experiment, we evaluate the performance of IAES on an image segmentation task. We use five images (included in the supplemental material) in (Rother et al., 2004) to evaluate IAES. The objective function F (A) is the sum of the unary potentials for all individual pixels and the pairwise potentials of a 8-neighbor grid graph:\nF (A) = u(A) + ∑\ni∈A,j∈V/A\nd(i, j),\nwhere V presents all the pixels, u ∈ RV is the unary potential derived from the Gaussian Mixture model (Rother et al., 2004), and d(i, j) = exp{−‖xi − xj‖2} (xi and xj are the values of two pixels) if i, j are neighbors, otherwise d(i, j) = 0. Table 3 provides the statistics of the resulting image segmentation problems, including the numbers of the pixels and the edges in the 8-neighbor grid graph.\nThe rejection ratios in Figure 4 show that IAES can identify the active and inactive elements during the optimization process incrementally until all of them are identified. This implies that IAES can lead to a significant speedup in the time cost.\nTable 2 reports the detailed time cost of MinNorm without and with AES, IES and IAES for solving the image segmentation problems. We can see that IAES leads to significant speedups, which are up to 30.7 times. In addition, we notice that the speedup gained by AES is small. The reason is that AES is used to identify the pixels of the foreground, which is a small region in the image, and thus the problem size cannot be reduced dramatically even if all the active elements are identified.\nAt last, from Table 2, we can also see that the speedup we achieve is supper-additive (speedup of AES + speedup of IES < speedup of IAES). This can usually be expected, which comes from the super linear computational complexity of each iteration in MinNorm, leading to a super-additive saving in the computational cost. We notice that the speedup we achieve on some of the two-moon datasets is not superadditive. The reason is that we cannot identify a lot of\nelements in the early stage (Figure 1). Thus, the early stage takes up too much time cost."
  }, {
    "heading": "5. Conclusion",
    "text": "In this paper, we proposed a novel safe element screening method IAES for SFM to accelerate its optimization process by simultaneously identifying the active and inactive elements. Our major contribution is a novel framework for accurately estimating the optimum of the corresponding primal problem of SFM developed by carefully studying the strong convexity of the primal and dual problems, the structure of the base polyhedra, and the optimality conditions of SFM. To the best of our knowledge, IAES is the first screening method in the fields of SFM and even combinatorial optimization. The extensive experimental results demonstrate that IAES can achieve significant speedups."
  }],
  "year": 2018,
  "references": [{
    "title": "Active set algorithms for isotonic regression; a unifying framework",
    "authors": ["M.J. Best", "N. Chakravarti"],
    "venue": "Mathematical Programming,",
    "year": 1990
  }, {
    "title": "Convex analysis and nonlinear optimization: theory and examples",
    "authors": ["J. Borwein", "A.S. Lewis"],
    "venue": "Springer Science & Business Media,",
    "year": 2010
  }, {
    "title": "Sparse signal recovery using markov random fields",
    "authors": ["V. Cevher", "M.F. Duarte", "C. Hegde", "R. Baraniuk"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Conditional gradient algorithms with open loop step size rules",
    "authors": ["J.C. Dunn", "S. Harshbarger"],
    "venue": "Journal of Mathematical Analysis and Applications,",
    "year": 1978
  }, {
    "title": "Submodular functions, matroids, and certain polyhedra",
    "authors": ["J. Edmonds"],
    "venue": "Combinatorial structures and their applications,",
    "year": 1970
  }, {
    "title": "Safe feature elimination in sparse supervised learning",
    "authors": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"],
    "venue": "Pacific Journal of Optimization,",
    "year": 2012
  }, {
    "title": "Decomposable submodular function minimization: discrete and continuous",
    "authors": ["A. Ene", "H. Nguyen", "L.A. Végh"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Mind the duality gap: safer rules for the lasso",
    "authors": ["O. Fercoq", "A. Gramfort", "J. Salmon"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Submodular functions and optimization, volume 58",
    "authors": ["S. Fujishige"],
    "year": 2005
  }, {
    "title": "A combinatorial strongly polynomial algorithm for minimizing submodular functions",
    "authors": ["S. Iwata", "L. Fleischer", "S. Fujishige"],
    "venue": "Journal of the ACM (JACM),",
    "year": 2001
  }, {
    "title": "What energy functions can be minimized via graph cuts",
    "authors": ["V. Kolmogorov", "R. Zabin"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2004
  }, {
    "title": "A class of submodular functions for document summarization",
    "authors": ["H. Lin", "J. Bilmes"],
    "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
    "year": 2011
  }, {
    "title": "Optimal selection of limited vocabulary speech corpora",
    "authors": ["H. Lin", "J. Bilmes"],
    "venue": "In Twelfth Annual Conference of the International Speech Communication Association,",
    "year": 2011
  }, {
    "title": "Submodular functions and convexity",
    "authors": ["L. Lovász"],
    "venue": "In Mathematical Programming The State of the Art,",
    "year": 1983
  }, {
    "title": "Submodular function minimization",
    "authors": ["S.T. McCormick"],
    "venue": "Handbooks in operations research and management science,",
    "year": 2005
  }, {
    "title": "Local search for balanced submodular clusterings",
    "authors": ["M. Narasimhan", "J. Bilmes"],
    "venue": "In Proceedings of the 20th International Joint Conference on Artifical Intelligence,",
    "year": 2007
  }, {
    "title": "Gap safe screening rules for sparse multi-task and multi-class models",
    "authors": ["E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Gap safe screening rules for sparse-group lasso",
    "authors": ["E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Safe screening of non-support vectors in pathwise svm computation",
    "authors": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Grabcut: Interactive foreground extraction using iterated graph cuts",
    "authors": ["C. Rother", "V. Kolmogorov", "A. Blake"],
    "venue": "In ACM transactions on graphics (TOG),",
    "year": 2004
  }, {
    "title": "Simultaneous safe screening of features and samples in doubly sparse modeling",
    "authors": ["A. Shibagaki", "M. Karasuyama", "K. Hatano", "I. Takeuchi"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Strong rules for discarding predictors in lasso-type problems",
    "authors": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2012
  }, {
    "title": "Multi-layer feature reduction for tree structured group lasso via hierarchical projection",
    "authors": ["J. Wang", "J. Ye"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Lasso screening rules via dual polytope projection",
    "authors": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A safe screening rule for sparse logistic regression",
    "authors": ["J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Finding the nearest point in a polytope",
    "authors": ["P. Wolfe"],
    "venue": "Mathematical Programming,",
    "year": 1976
  }, {
    "title": "Constrained submodular minimization for missing labels and class imbalance in multi-label learning",
    "authors": ["B. Wu", "S. Lyu", "B. Ghanem"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Scaling up sparse support vector machines by simultaneous feature and sample reduction",
    "authors": ["W. Zhang", "B. Hong", "W. Liu", "J. Ye", "D. Cai", "X. He", "J. Wang"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }],
  "id": "SP:b656a739aa41190e45a7ed115e811172e65041df",
  "authors": [{
    "name": "Weizhong Zhang",
    "affiliations": []
  }, {
    "name": "Bin Hong",
    "affiliations": []
  }, {
    "name": "Lin Ma",
    "affiliations": []
  }, {
    "name": "Wei Liu",
    "affiliations": []
  }, {
    "name": "Tong Zhang",
    "affiliations": []
  }],
  "abstractText": "Submodular functions are discrete analogs of convex functions, which have applications in various fields, including machine learning and computer vision. However, in large-scale applications, solving Submodular Function Minimization (SFM) problems remains challenging. In this paper, we make the first attempt to extend the emerging technique named screening in large-scale sparse learning to SFM for accelerating its optimization process. We first conduct a careful studying of the relationships between SFM and the corresponding convex proximal problems, as well as the accurate primal optimum estimation of the proximal problems. Relying on this study, we subsequently propose a novel safe screening method to quickly identify the elements guaranteed to be included (we refer to them as active) or excluded (inactive) in the final optimal solution of SFM during the optimization process. By removing the inactive elements and fixing the active ones, the problem size can be dramatically reduced, leading to great savings in the computational cost without sacrificing any accuracy. To the best of our knowledge, the proposed method is the first screening method in the fields of SFM and even combinatorial optimization, thus pointing out a new direction for accelerating SFM algorithms. Experiment results on both synthetic and real datasets demonstrate the significant speedups gained by our approach.",
  "title": "Safe Element Screening for Submodular Function Minimization"
}