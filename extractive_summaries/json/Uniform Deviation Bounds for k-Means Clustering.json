{
  "sections": [{
    "text": "⇣ m 12 ⌘ com-\npared to the previously known O ⇣ m 14 ⌘\nrate. We further show that this rate also depends on the kurtosis — the normalized fourth moment which measures the “tailedness” of the distribution. We also provide improved rates under progressively stronger assumptions, namely, bounded higher moments, subgaussianity and bounded support of the underlying distribution."
  }, {
    "heading": "1. Introduction",
    "text": "Empirical risk minimization — i.e. the training of models on a finite sample drawn i.i.d from an underlying distribution — is a central paradigm in machine learning. The hope is that models trained on the finite sample perform provably well even on previously unseen samples from the underlying distribution. But how many samples m are required to guarantee a low approximation error ✏? Uniform deviation bounds provide the answer. Informally, they are the worst-case difference across all possible models between the empirical loss of a model and its expected loss. As such, they determine how many samples are required to achieve a fixed error in terms of the loss function. In this paper, we consider the popular k-Means clustering problem and provide uniform deviation bounds based on weak assumptions on the underlying data generating distribution.\n1Department of Computer Science, ETH Zurich. Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nRelated work. Traditional Vapnik-Chervonenkis theory provides tools to obtain uniform deviation bounds for binary concept classes such as classification using halfspaces (Vapnik & Chervonenkis, 1971). While these results have been extended to provide uniform deviation bounds for sets of continuous functions bounded in [0, 1] (Haussler, 1992; Li et al., 2001), these results are not readily applied to kMeans clustering as the underlying loss function in k-Means clustering is continuous and unbounded.\nIn their seminal work, Pollard (1981) shows that k-Means clustering is strongly consistent, i.e., that the optimal cluster centers on a random sample converge almost surely to the optimal centers of the distribution under a weak assumption. This has sparked a long line of research on cluster stability (Ben-David et al., 2006; Rakhlin & Caponnetto, 2007; Shamir & Tishby, 2007; 2008) which investigates the convergence of optimal parameters both asymptotically and for finite samples. The vector quantization literature offers insights into the convergence of empirically optimal quantizers for k-Means in terms of the objective: A minimax rate of O ⇣ m 12 ⌘\nis known if the underlying distribution has bounded support (Linder et al., 1994; Bartlett et al., 1998). A better rate of O m\n1 may be achieved for finite support (Antos et al., 2005) or under both bounded support and regularity assumptions (Levrard et al., 2013). Ben-David (2007) provides a uniform convergence result for center based clustering under a bounded support assumption and Telgarsky & Dasgupta (2013) prove uniform deviation bounds for k-Means clustering if the underlying distribution satisfies moment assumptions (see Section 4).\nEmpirical risk minimization with fat-tailed losses has been studied in Mendelson et al. (2014), Mendelson (2014), Grünwald & Mehta (2016) and Dinh et al. (2016). Dinh et al. (2016) provide fast learning rates for k-Means clustering but under stronger assumptions than the ones considered in this paper. Guarantees similar to uniform deviation bounds can be obtained using importance sampling in the context of coreset construction (Bachem et al., 2015; Lucic et al., 2016; 2017).\nOur contributions. We provide a new framework to obtain uniform deviation bounds for unbounded loss functions. We apply it to k-Means clustering and provide uniform deviation bounds with a rate of O ⇣ m 12 ⌘ for finite samples\nunder weak assumptions. In contrast to prior work, our bounds are all scale-invariant and hold for any set of k cluster centers (not only for a restricted solution set). We show that convergence depends on the kurtosis of the underlying distribution, which is the normalized fourth moment and measures the “tailedness” of a distribution. If bounded higher moments are available, we provide improved bounds that depend upon the normalized higher moments and we sharpen them even further under the stronger assumptions of subgaussianity and bounded support.\n2. Problem statement for k-Means We first focus on uniform deviation bounds for k-Means clustering, and defer the (more technical) framework for unbounded loss functions to Section 5. We consider the d-dimensional Euclidean space and define\nd(x,Q)\n2\n= min q2Q kx qk2 2\nfor any x 2 Rd and any finite set Q ⇢ Rd. Furthermore, slightly abusing the notation, for any x, y 2 Rd, we set d(x, y) 2\n= d(x, {y})2 = kx yk2 2 .\nStatistical k-Means. Let P be any distribution on Rd with µ = E\nP [x] and 2 = E P ⇥ d(x, µ) 2 ⇤ 2 (0,1). For any set\nQ ⇢ Rd of k 2 N cluster centers, the expected quantization error is given by E\nP\n⇥ d(x,Q) 2 ⇤ . The goal of the statistical\nk-Means problem is to find a set of k cluster centers such that the expected quantization error is minimized.\nEmpirical k-Means. Let X denote a finite set of points in Rd. The goal of the empirical k-Means problem is to find a set Q of k cluster centers in Rd such that the empirical quantization error X (Q) is minimized, where\nX (Q) = 1 |X | X\nx2X d(x,Q)\n2\n.\nEmpirical risk minimization. In practice, the underlying data distribution P in statistical learning problems is often unknown. Instead, one is only able to observe independent samples from P . The empirical risk minimization principle advocates that finding a good solution on a random sample X\nm also provides provably good solutions to certain statistical learning problems if enough samples m are used.\nUniform deviation bounds. For k-Means, such a result may be shown by bounding the deviation between the expected loss and the empirical error, i.e.,\nX\nm\n(Q) E P ⇥ d(x,Q) 2 ⇤ ,\nuniformly for all possible clusterings Q 2 Rd⇥k. If this difference is sufficiently small for a given m, one may then solve the empirical k-Means problem on X\nm and obtain provable guarantees on the expected quantization error.\n3. Uniform deviation bounds for k-Means A simple approach would be to bound the deviation by an absolute error ✏, i.e., to require that\nX\nm\n(Q) E P ⇥ d(x,Q) 2 ⇤  ✏ (1)\nuniformly for a set of possible solutions (Telgarsky & Dasgupta, 2013). However, in this paper, we provide uniform deviation bounds of a more general form: For any distribution P and a sample of m = f(✏, , k, d, P ) points, we require that with probability at least 1 X\nm\n(Q) E P ⇥ d(x,Q) 2 ⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤\n(2) uniformly for all Q 2 Rd⇥k. The terms on the right-hand side may be interpreted as follows: The first term based on the variance 2 corresponds to a scale-invariant, additive approximation error. The second term is a multiplicative approximation error that allows the guarantee to hold even for solutions Q with a large expected quantization error. Similar additive error terms were recently explored by Bachem et al. (2016; 2017) in the context of seeding for k-Means.\nThere are three key reasons why we choose (2) over (1): First, (1) is not scale-invariant and may thus not hold for classes of distributions that are equal up to scaling. Second, (1) may not hold for an unbounded solution space, e.g. Rd⇥k. Third, we can always rescale P to unit variance and restrict ourselves to solutions Q with E\nP\n⇥ d(x,Q) 2 ⇤  2.\nThen, (2) implies (1) for a suitable transformation of P .\nImportance of scale-invariance. If we scale all the points in a data set X and all possible sets of solutions Q by some > 0, then the empirical quantization error is scaled by\n2. Similarly, if we consider the random variable x where x ⇠ P , then the expected quantization error is scaled by\n2. At the same time, the k-Means problem remains the same: an optimal solution of the scaled problem is simply a scaled optimal solution of the original problem. Crucially, however, it is impossible to achieve the guarantee in (1) for distributions that are equal up to scaling: Suppose that (1) holds for some error tolerance ✏, and sample size m with probability at least 1 . Consider a distribution P and a solution Q 2 Rd⇥k such that with probability at least\na < X\nm\n(Q) E P ⇥ d(x,Q) 2 ⇤ .\nfor some a > 0.1 For > 1p a✏ , let ˜P be the distribution of the random variable x where x ⇠ P and let ˜X\nm consist of m samples from ˜P . Defining ˜Q = { q | q 2 Q}, we have with probability at least\n˜X m\n⇣ ˜\nQ ⌘ E\n˜ P\nh d ⇣ x, ˜ Q ⌘ 2 i > a 2 > ✏\n1For example, let P be a nondegenerate multivariate normal distribution and Q consist of k copies of the origin.\nwhich contradicts (1) for the distribution ˜P and the solution ˜Q. Hence, (1) cannot hold for both P and its scaled transformation ˜P .\nUnrestricted solution space One way to guarantee scaleinvariance would be require that\nX\nm\n(Q) E P ⇥ d(x,Q) 2 ⇤  ✏ 2 (3)\nfor all Q 2 Rd⇥k. However, while (3) is scale-invariant, it is also impossible to achieve for all solutions Q as the following example shows. For simplicity, consider the 1-Means problem in 1-dimensional space and let P be a distribution with zero mean. Let X\nm denote m independent samples from P and denote by µ̂ the mean of X\nm . For any finite m, suppose that µ̂ 6= 0 with high probability2 and consider a solution Q consisting of a single point q 2 R. We then have\n(4)\nX\nm\n({q}) E P ⇥ d(x, {q})2 ⇤\n= X\nm\n({µ̂}) + d(µ̂, q)2 2 d(0, q)2\n= X\nm\n({µ̂}) 2 + q2 2qµ̂+ µ̂2 q2\n= X\nm\n({µ̂}) 2 + µ̂2 2qµ̂\nSince µ̂ 6= 0 with high probability, clearly this expression diverges as q ! 1 and thus (3) cannot hold for arbitrary solutions Q 2 Rd⇥k. Intuitively, the key issue is that both the empirical and the statistical error become unbounded as q ! 1. Previous approaches such as Telgarsky & Dasgupta (2013) solve this issue by restricting the solution space from Rd⇥k to solutions that are no worse than some threshold. In contrast, we allow the deviation between the empirical and the expected quantization error to scale with E\nP\n⇥ d(x,Q) 2 ⇤ .\nArbitrary distributions. Finally, we show that we either need to impose assumptions on P or equivalently make the relationship between m, ✏ and in (2) depend on the underlying distribution P . Suppose that there exists a sample size m 2 N, an error tolerance ✏ 2 (0, 1) and a maximal failure probability 2 (0, 1) such that (2) holds for any distribution P . Let P be the Bernoulli distribution on {0, 1} ⇢ R with P [x = 1] = p for p 2 ( 1m , 1). By design, we have µ = p,\n2 = p(1 p) and E P ⇥ d(x, 1) 2 ⇤ = (1 p). Furthermore,\nwith probability at least , the set X m of m independent samples from P consists of m copies of a point at one. Hence, (2) implies that with probability at least 1\nX\nm\n(1) E P ⇥ d(x, 1) 2 ⇤  ✏E P ⇥ d(x, 1) 2 ⇤\nsince 2  E P ⇥ d(x, 1) 2 ⇤ . However, with probability at least , we have X m\n(1) = 0 which would imply 1  ✏ and thus lead to a contradiction with ✏ 2 (0, 1).\n2For example, if P is the standard normal distribution.\n4. Key results for k-Means In this section, we present our main results for k-Means and defer the analysis and proofs to Sections 6."
  }, {
    "heading": "4.1. Kurtosis bound",
    "text": "Similar to Telgarsky & Dasgupta (2013), the weakest assumption that we require is that the fourth moment of d(x, µ) for x 2 P is bounded.3 Our results are based on the kurtosis of P which we define as\nˆ M\n4\n=\nE P ⇥ d(x, µ) 4 ⇤\n4\n.\nThe kurtosis is the normalized fourth moment and is a scaleinvariant measure of the “tailedness” of a distribution. For example, the normal distribution has a kurtosis of 3, while more heavy tailed distributions such as the t-Student distribution or the Pareto distribution have a potentially unbounded kurtosis. A natural interpretation of the kurtosis is provided by Moors (1986). For simplicity, consider a data set with unit variance. Then, the kurtosis may be restated as the shifted variance of d(x, µ)2, i.e.,\nˆ M\n4\n= Var d(x, µ)\n2\n+ 1.\nThis provides a valuable insight into why the kurtosis is relevant for our setting: For simplicity, suppose we would like to estimate the expected quantization error E\nP\n⇥ d(x, µ) 2 ⇤\nby the empirical quantization error X m ({µ}) on a finite sample X\nm .4 Then, the kurtosis measures the dispersion of d(x, µ)2 around its mean E\nP\n⇥ d(x, µ) 2 ⇤ and provides a\nbound on how many samples are required to achieve an error of ✏. While this simple example provides the key insight for the trivial solution Q = {µ}, it requires a non-trivial effort to extend the guarantee in (2) to hold uniformly for all solutions Q 2 Rd⇥k. With the use of a novel framework to learn unbounded loss functions (presented in Section 5), we are able to provide the following guarantee for k-Means. Theorem 1 (Kurtosis). Let ✏, 2 (0, 1) and k 2 N. Let P be any distribution on Rd with kurtosis ˆM\n4\n< 1. For\nm 12800\n⇣ 8 + ˆ M\n4\n⌘\n✏\n2\n✓ 3 + 30k(d+ 4) log 6k + log 1 ◆\nlet X = {x 1 , x 2 , . . . , x m } be m independent samples from P . Then, with probability at least 1 , for all Q 2 Rd⇥k X (Q) EP ⇥ d(x,Q) 2\n⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤ .\n3While our random variables x 2 P are potentially multivariate, it suffices to consider the behavior of the univariate random variable d(x, µ) for the assumptions in this section.\n4This is a hypothetical exercise as EP ⇥ d(x, µ) 2 ⇤ = 1 by design. However, it provides an insight to the importance of the kurtosis.\nThe proof is provided in Section 6.1. The number of sufficient samples\nm 2 O\nˆ M\n4\n✏\n2\n✓ dk log k + log 1\n◆!\nis linear in the kurtosis ˆM 4 and the dimensionality d, nearlinear in the number of clusters k and 1\n, and quadratic in 1\n✏\n. Intuitively, the bound may be interpreted as follows:\n⌦\n⇣ ˆ\nM4 ✏ 2\n⌘ samples are required such that the guarantee holds\nfor a single solution Q 2 Rd⇥k. Informally, a generalization of the Vapnik Chervonenkis dimension for k-Means clustering may be bounded by O(dk log k) and measures the “complexity” of the learning problem. The multiplicative dk log k + log 1\nterm intuitively extends the guarantee uniformly to all possible Q 2 Rd⇥k. Comparison to Telgarsky & Dasgupta (2013). While we require a bound on the normalized fourth moment, i.e., the kurtosis, Telgarsky & Dasgupta (2013) consider the case where all unnormalized moments up to the fourth are uniformly bounded by some M , i.e.,\nE P ⇥ d(x, µ) l ⇤  M, 1  l  4.\nThey provide uniform deviation bounds for all solutions Q such that either X (Q)  c or EP ⇥ d(x,Q) 2 ⇤  c for some c > 0. To compare our bounds, we consider a data set with unit variance and restrict ourselves to solutions Q 2 Rd⇥k with an expected quantization error of at most the variance, i.e., E\nP\n⇥ d(x,Q) 2 ⇤  2 = 1. Consider the deviation\n= sup\nQ2Rd⇥k:E P [d(x,Q) 2 ]1\nX (Q) EP ⇥ d(x,Q) 2 ⇤ .\nTelgarsky & Dasgupta (2013) bound this deviation by\n2 O s M 2\np m\n✓ dk log(Mdm) + log 1 ◆ + r 1\nm\n2\n! .\nIn contrast, our bound in Theorem 1 implies\n2 O\n0 @ s ˆ M 4\nm\n✓ dk log k + log 1\n◆1\nA .\nThe key difference lies in how scales with the sample size m. While Telgarsky & Dasgupta (2013) show a rate of 2 O ⇣ m 14 ⌘ , we improve it to 2 O ⇣ m 12 ⌘ ."
  }, {
    "heading": "4.2. Bounded higher moments",
    "text": "The tail behavior of d(x, µ) may be characterized by the moments of P . For p 2 N, consider the standardized p-th moment of P , i.e.,\nˆ M\np\n=\nE P [d(x, µ) p ]\np\n.\nTheorem 2 provides an improved uniform deviation bound if P has bounded higher moments. Theorem 2 (Moment bound). Let ✏ 2 (0, 1), 2 (0, 1) and k 2 N. Let P be any distribution on Rd with finite p-th order moment bound ˆM\np < 1 for p 2 {4, 8, . . . ,1}. For m max ⇣ 3200m1\n✏\n2 ,\n8 8 p ⌘ with\nm\n1\n= p ✓ 4 + ˆ M\np\n4 p ◆✓ 3 + 30k(d+ 4) log 6k + log 1 ◆\nlet X = {x 1 , x 2 , . . . , x m } be m independent samples from P . Then, with probability at least 1 , for all Q 2 Rd⇥k X (Q) EP ⇥ d(x,Q) 2\n⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤ .\nThe proof is provided in Section 6.2. Compared to the previous bound based on the kurtosis, Theorem 2 requires\nm 2 ⌦\n0\n@p ˆ M p\n4 p\n✏\n2\n✓ dk log k + log 1 ◆ + ✓ 1 ◆ 8 p\n1\nA\nsamples. With higher order moment bounds, it is easier to achieve high probability results since the dependence on 1 is only of O ⇣ 1 8 p ⌘ compared to near linear for a kurtosis\nbound. The quantity ˆM p 4 p may be interpreted as a bound on the kurtosis ˆM 4 based on the higher order moment ˆM p since\nHoelder’s inequality implies that ˆM 4  ˆM p 4 p . While the result only holds for p 2 {8, 12, 16, . . . ,1}, it is trivially extended to p0 8: Consider Theorem 2 with p = 4 j p 0\n4\nk\nand note that by Hoelder’s inequality ˆM p 4 p  ˆM p 0 4 p 0 .\nComparison to Telgarsky & Dasgupta (2013). Again, we consider distributions P that have unit variance and we restrict ourselves to solutions Q 2 Rd⇥k with an expected quantization error of at most the variance, i.e., E P ⇥ d(x,Q) 2 ⇤  2 = 1. Telgarsky & Dasgupta (2013) require that there exists a bound M\nE P ⇥ d(x, µ) l ⇤  M, 1  l  p.\nThen, for m sufficiently large, is of\nO\n0\n@\ns M 8 p\nm\n1 4 p\n✓ dk ln(M 4 p dm) + ln 1 ◆ + 2 p 4\nm\n3 4 2 p\n✓ 1 ◆ 4 p\n1\nA .\nIn contrast, we obtain that, for m sufficiently large,\n2 O\n0\nB@\nvuutp ˆM p 4 p\nm\n✓ dk log k + log 1\n◆ 1\nCA.\nWhile Telgarsky & Dasgupta (2013) only show a rate of O ⇣ m 12 ⌘ as p ! 1, we obtain a 2 O ⇣ m 12 ⌘\nrate for all higher moment bounds."
  }, {
    "heading": "4.3. Subgaussianity",
    "text": "If the distribution P is subgaussian, then all its moments ˆ M\np are bounded. By optimizing p in Theorem 2, we are able to show the following bound. Theorem 3 (Subgaussian bound). Let ✏ 2 (0, 1), 2 (0, 1) and k 2 N. Let P be any distribution on Rd with µ = E P [x] and\n8t > 0 : P [d(x, µ) > t ]  a exp ✓ t 2\np b\n◆\nfor some a > 1, b > 0. Let m 3200m1 ✏ 2 with\nm\n1\n= p ✓ 4 + abp 2\n4\n◆✓ 3 + 30k(d+ 4) log 6k + log 1 ◆ .\nand p = 9 + 3 log 1 . Let X = {x 1 , x 2 , . . . , x m } be m independent samples from P . Then, with probability at least 1 , for all Q 2 Rd⇥k X (Q) EP ⇥ d(x,Q) 2\n⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤ .\nThe proof is provided in Section 6.3. In O(·) notation,\nm 2 O ab log\n3\n1\n✏\n2\n✓ dk log k + log 1\n◆!\nsamples are hence sufficient. This result features a polylogarithmic dependence on 1\ncompared to the polynomial dependence for the bounds based on bounded higher moments. The sufficient sample size further scales linearly with the (scale-invariant) subgaussianity parameters a and b. For example, if P is a one-dimensional normal distribution of any scale, we would have a = 2 and b = 1."
  }, {
    "heading": "4.4. Bounded support",
    "text": "The strongest assumption that we consider is if the support of P is bounded by a hypersphere in Rd with diameter R > 0. This ensures that almost surely d(x, µ)  R and hence ˆM\n4\n R4\n4 . This allows us to obtain Theorem 4. Theorem 4 (Bounded support). Let ✏ 2 (0, 1), 2 (0, 1) and k 2 N. Let P be any distribution on Rd, with µ = E P [x] and 2 = E P ⇥ d(x, µ) 2 ⇤ 2 (0,1), whose support is contained in a d-dimensional hypersphere of diameter R > 0. For\nm 12800\n⇣ 8 + R 4\n4\n⌘\n✏\n2\n✓ 3 + 30k(d+ 4) log 6k + log 1 ◆\nlet X = {x 1 , x 2 , . . . , x m } be m independent samples from P . Then, with probability at least 1 , for all Q 2 Rd⇥k X (Q) EP ⇥ d(x,Q) 2\n⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤ .\nThe proof is provided in Section 6.4. Again, the sufficient sample size scales linearly with the kurtosis bound R 4\n4 . However, the bound is only logarithmic in 1 ."
  }, {
    "heading": "5. Framework for unbounded loss functions",
    "text": "To obtain the results presented in Section 4, we propose a novel framework to uniformly approximate the expected values of a set of unbounded functions based on a random sample. We consider a function family F mapping from an arbitrary input space X to R 0 and a distribution P on X . We further require a generalization of the VapnikChervonenkis dimension to continuous, unbounded functions5 — the pseudo-dimension. Definition 1 (Haussler (1992); Li et al. (2001)). The pseudodimension of a set F of functions from X to R 0, denoted by Pdim(F), is the largest d0 such there is a sequence x\n1\n, . . . , x\nd 0 of domain elements from X and a sequence r\n1\n, . . . , r\nd 0 of reals such that for each b 1 , . . . , b d 0 2 {above, below}, there is an f 2 F such that for all i = 1, . . . , d 0, we have f(x i ) r i () b i = above.\nSimilar to the VC dimension, the pseudo-dimension measures the cardinality of the largest subset of X that can be shattered by the function family F . Informally, the pseudodimension measures the richness of F and plays a critical role in providing a uniform approximation guarantee across all f 2 F . With this notion, we are able to state the main result in our framework. Theorem 5. Let ✏ 2 (0, 1), 2 (0, 1) and t > 0. Let F be a family of functions from X to R 0 with Pdim(F ) = d < 1. Let s : X ! R 0 be a function such that s(x) sup\nf2F f(x) for all x 2 X . Let P be any distribution on X and for\nm 200t ✏ 2\n✓ 3 + 5d+ log 1 ◆ ,\nlet x 1 , x 2 , . . . , x 2m be 2m independent samples from P .\nThen, if\nE P ⇥ s(x) 2 ⇤  t and P\n\" 1\n2m\n2mX\ni=1\ns(x\ni\n)\n2\n> t # \n4\n,\n(5) it holds with probability at least 1 that\n1\nm\nmX\ni=1\nf(x\ni ) E P [f(x)]  ✏, 8f 2 F . (6)\nApplying Theorem 5 to a function family F requires three steps: First, one needs to bound the pseudo-dimension of\n5The pseudo-dimension was originally defined for sets of functions mapping to [0, 1] (Haussler, 1992; Li et al., 2001). However, it is trivially extended to unbounded functions mapping to R 0.\nF . Second, it is necessary to find a function s : X ! R 0 such that\nf(x)  s(x), 8x 2 X and 8f 2 F .\nIdeally, such a bound should be as tight as possible. Third, one needs to find some t > 0 and a sample size\nm 200t ✏ 2\n✓ 3 + 5d+ log 1 ◆\nsuch that\nE P h s(x) 2 i  t and P\n\" 1\n2m\n2mX\ni=1\ns(x\ni\n)\n2\n> t # \n4\n.\nFinding such a bound usually entails examining the tail behavior of s(x)2 under P . Furthermore, it is evident that a bound t may only be found if E\nP\nh s(x) 2 i is bounded\nand that assumptions on the distribution P are required. In Section 6, we will see that for k-Means a function s(x) with E\nP\nh s(x) 2 i < 1 may be found if the kurtosis of P is\nbounded.\nWe defer the proof of Theorem 5 to Section B of the Supplementary Materials and provide a short proof sketch that captures the main insight. The proof is based on symmetrization, the bounding of covering numbers and chaining — common techniques in the empirical process literature (Pollard, 1984; Li et al., 2001; Boucheron et al., 2013; Koltchinskii, 2011; van der Vaart & Wellner, 1996). The novelty lies in considering loss functions f(·) and cover functions s(·) in Theorem 5 that are potentially unbounded.\nProof sketch. Our proof is based on a double sampling approach. Let x\nm+1\n, x\nm+2\n, . . . , x\n2m be an additional m independent samples from P and let\n1\n,\n2\n, . . . ,\nm be independent random variables uniformly sampled from { 1, 1}. Then, we show that, if E\nP\nh s(x) 2 i  t, the probability\nof (6) not holding may be bounded by the probability that there exists a f 2 F such that\n1\nm\nmX\ni=1\ni\n(f(x\ni ) f(x i+m ))\n> ✏. (7)\nWe first provide the intuition for a single function f 2 F and then show how we extend it to all f 2 F . While the function f(x) is not bounded, for a given sample x\n1\n, x\n2\n, . . . , x\n2m , each f(x\ni ) is contained within [0, s(x i )]. Given the sample x\n1\n, x\n2\n, . . . , x\n2m , the random variable i (f(x i ) f(x i+m )\nis bounded in 0±max (s(x i ), s(x i+m )) and has zero mean. Hence, given independent samples x\n1\n, x\n2\n, . . . , x\n2m , the probability of (7) occurring for a single f 2 F can be\nbounded using Hoeffding’s inequality by\n2 exp\n2m✏\n2\n1\nm\nP m\ni=1\nmax (s(x\ni\n), s(x\ni+m\n))\n2\n!\n 2 exp m✏\n2\n1\n2m\nP 2m\ni=1\ns(x\ni\n)\n2\n! .\nBy (5), with probability at least 1 4 , we have 1\n2m\nP 2m\ni=1\ns(x\ni\n) 2  t and we hence require m 2 ⌦ ⇣ t log 1\n✏\n2\n⌘\nsamples to guarantee that (7) does not hold for a single f 2 F with probability at least 1\n2\n.\nTo bound the probability that there exists any f 2 F such that (7) holds, we show in Lemma 5 (see Section B of the Supplementary Materials) that, given independent samples x\n1\n, x\n2\n, . . . , x\n2m\n,\nP \" 9f 2 F : 1\nm\nmX\ni=1\ni(f(xi) f(xi+m)) > ✏\n#\n 4 16e2 Pdim(F) e ✏\n2 m\n200 1 2m P2m i=1 s(xi) 2 .\nThe key difficulty in proving Lemma 5 is that the functions f 2 F are not bounded uniformly in [0, 1]. To this end, we provide in Lemma 4 a novel result that bounds the size of ✏-packings of F if the functions f 2 F are bounded in expectation. Based on Lemma 5, we then prove the main claim of Theorem 5.\n6. Analysis for k-Means In order to apply Theorem 5 to k-Means clustering, we require a suitable family F , an upper bound s(x) and a bound on E\nP\nh s(x) 2 i . We provide this in Lemma 1 and defer\nbounding 1 2m\nP 2m\ni=1\ns(x\ni\n) 2 to the proofs of Theorems 2-4. Lemma 1 (k-Means). Let k 2 N. Let P be any distribution on Rd with µ = E\nP [x], 2 = E P ⇥ d(x, µ) 2 ⇤ 2 (0,1) and\nbounded kurtosis ˆM 4 . For any x 2 Rd and any Q 2 Rd⇥k, define\nf\nQ\n(x) =\nd(x,Q)\n2\n1\n2\n2\n+\n1\n2\nE P [d(x,Q) 2 ]\n(8)\nas well as the function family F = f\nQ\n(·) | Q 2 Rd⇥k\n. Let\ns(x) =\n4 d(x, µ)\n2\n2\n+ 8.\nWe then have\nPdim(F)  6k(d+ 4) log 6k, (9)\nf\nQ\n(x)  s(x) (10)\nfor any x 2 Rd and Q 2 Rd⇥k and\nE P h s(x) 2 i = 128 + 16 ˆ M 4 . (11)\nThe proof of Lemma 1 is provided in Section C of the Supplementary Materials. The definition of f\nQ (x) in (8) is motivated as follows: If we use Theorem 5 to guarantee\nmX\ni=1\nf(x\ni ) E P [f(x)]  ✏ 8f 2 F . (12)\nthen this implies X (Q) EP ⇥ d(x,Q) 2\n⇤  ✏ 2 2 + ✏ 2 E P ⇥ d(x,Q) 2 ⇤\n(13) as is required by Theorems 2-4. Lemma 1 further shows that E [s(x)]2 is bounded if and only if the kurtosis of P is bounded. This is the reason why a bounded kurtosis is the weakest assumption on P that we require in Section 4.\nWe now proceed to prove Theorems 2-4 by applying Theorem 5 and examining the tail behavior of 1\n2m\nP 2m\ni=1\ns(x\ni\n) 2."
  }, {
    "heading": "6.1. Proof of Theorem 1 (kurtosis bound)",
    "text": "The bound based on the kurtosis follows easily from Markov’s inequality.\nProof. We consider the choice t = 4 ⇣ 128 + 16 ˆ M\n4\n⌘ / .\nBy Markov’s inequality and linearity of expectation, we then have by Lemma 1 that\nP \" 1\n2m\n2mX\ni=1\ns(x\ni\n)\n2\n> t #  E ⇥ s(x) 2 ⇤\nt\n=\n4\n.\nFurthermore, E P h s(x) 2 i  t. Hence, we may apply Theo-\nrem 5 to obtain that for\nm 12800\n⇣ 8 + ˆ M\n4\n⌘\n✏\n2\n✓ 3 + 30k(d+ 4) log 6k + log 1 ◆ ,\nit holds with probability at least 1 that 1\nm\nmX\ni=1\nf(x\ni\n) E [f(x)]  ✏ 8f 2 F .\nThis implies the main claim and thus concludes the proof."
  }, {
    "heading": "6.2. Proof of Theorem 2 (higher order moment bound)",
    "text": "We prove the result by bounding the higher moments of 1\n2m\nP 2m\ni=1\ns(x\ni\n) 2 using the Marcinkiewicz-Zygmund inequality and subsequently applying Markov’s inequality.\nProof. Hoelder’s inequality implies\nˆ M\n4\n=\nE P ⇥ d(x, µ) 4 ⇤\n4\n EP [d(x, µ) p ]\n4 p\n4\n ˆM p\n4 p\nHence, by Lemma 1 we have that E P ⇥ s(x) 2 ⇤  128 +\n16\nˆ M\np\n4 p Since s(x)2 0 for all x 2 Rd, we have\ns(x) 2 E P ⇥ s(x) 2 ⇤  max s(x) 2 ,E P ⇥ s(x) 2 ⇤\n max ✓ s(x) 2 , 128 + 16 ˆ M\np\n4 p\n◆\n 128\n+ 16max\n✓ ˆ\nM\np\n4 p\n, 2\nd(x, µ)\n4\n4\n◆ .\n(14)\nThis implies that\n(15)\nE P h s(x) 2 E P ⇥ s(x) 2 ⇤ p 4 i\n 256 p 4 + 32 p 4 max\n✓ ˆ\nM\np\n, 2\np 4 E P [d(x, µ) p ]\np\n◆\n 256 p 4 + 32 p 4 max\n⇣ ˆ\nM\np\n, 2\np 4 ˆ M\np\n⌘\n 256 p 4 + 64 p 4 ˆ M\np\n.\nWe apply a variant of the Marcinkiewicz-Zygmund inequality (Ren & Liang, 2001) to the zero-mean random variable s(x) 2 E P ⇥ s(x) 2 ⇤ to obtain\n(16)\nE P\n2\n4 1\n2m\n2mX\ni=1\ns(x\ni\n) 2 E P ⇥ s(x) 2 ⇤\np 4\n3\n5\n ✓\np 4 4 p 2m\n◆ p 4\nE P h s(x) 2 E P ⇥ s(x) 2 ⇤ p 4 i\n ✓\np 4 4 p 2m\n◆ p 4 ⇣\n256\np 4 + 64 p 4 ˆ M\np\n⌘\nFor u > 0, the Markov inequality implies\n(17)\nP \" 1\n2m\n2mX\ni=1\ns(x\ni\n) 2 E P ⇥ s(x) 2 ⇤ > u\n#\n ✓\np 4 4u p 2m\n◆ p 4 ⇣\n256\np 4 + 64 p 4 ˆ M\np\n⌘\n ✓\np 4 4u p 2m\n◆ p 4\n2max ⇣ 256 p 4 , 64 p 4 ˆ M\np\n⌘\n 2 ✓\np 4 4u p 2m max\n✓ 256, 64 ˆ M\np\n4 p\n◆◆ p 4\n 2 ✓\np 4 u p 2m\n✓ 64 + 16 ˆ M\np\n4 p\n◆◆ p 4\n.\nFor u = (p 4) ✓ 64 + 16 ˆ M\np\n4 p ◆ , we thus have\nP \" 1\n2m\n2mX\ni=1\ns(x\ni\n) 2 E P ⇥ s(x) 2 ⇤ > u #  2m p 8\n(18)\nSince m 8 8 p , this implies\n(19)P \" 1\n2m\n2mX\ni=1\ns(x\ni\n) 2 E P ⇥ s(x) 2 ⇤ > u # \n4\nIt holds that\nu+E P ⇥ s(x) 2 ⇤ = (p 4) ✓ 64+16 ˆ M p 4 p ◆ +128+16 ˆ M p 4 p\n p ✓ 64 + 16 ˆ M\np\n4 p\n◆\n(20)\nWe set t = p ✓ 64 + 16 ˆ M\np\n4 p ◆ and thus have\n(21)P \" 1\n2m\n2mX\ni=1\ns(x\ni\n)\n2\n> t # \n4\nIn combination with E P h s(x) 2 i  t by Lemma 1, we may\nthus apply Theorem 5. Since m 3200m1 ✏ 2 with\nm\n1\n= p ✓ 4 + ˆ M\np\n4 p ◆✓ 3 + 30k(d+ 4) log 6k + log 1 ◆\nit holds with probability at least 1 that 1\nm\nmX\ni=1\nf(x\ni\n) E [f(x)]  ✏ 8f 2 F .\nThis implies the main claim and thus concludes the proof."
  }, {
    "heading": "6.3. Proof of Theorem 3 (subgaussianity)",
    "text": "Under subgaussianity, all moments of d(x, µ) are bounded. We show the result by optimizing over p in Theorem 2.\nProof. For p 2 {4, 8, . . . ,1}, we have\n(22)\nˆ M\np = E P\n d(x, µ) p\n=\nZ 1\n0\nP  d(x, µ) > u 1 p du\n Z 1\n0\na exp\nu\n2 p p b\n! du.\nLet u(t) = b p 4 t p 2 which implies du/dt = b p 4 p\n2\nt\np 2 1. Hence,\nˆ M\np\n ab p 4 p\n2\nZ 1\n0\ne t t\np 2 1 dt.\nBy the definition of the gamma function and since p is even, we have\nZ 1\n0\ne t t\np 2 1 dt =\n⇣ p\n2\n⌘ = ⇣ p\n2\n1 ⌘ ! ⇣ p\n2\n⌘ p 2 1\nHence, for p 2 {4, 8, . . . ,1}, we have\nˆ M\np\n4 p  1\n4\na\n4 p\nbp 2  1 4 abp 2 .\nLet p⇤ = 4 ⌃ 5\n4\n+\n3\n4\nlog\n1 ⌥ which implies\np⇤ 5 + 3 log 1\n8 log 48 log 8\nand thus 8 8 p⇤  48. We instantiate Theorem 2 with\nthe p⇤th-order bound ˆMp⇤ of P . Since 8\n8 p⇤  48, the\nminimum sample size is thus\n3200p⇤ ✏ 2\n✓ 4 + abp⇤ 2\n4\n◆✓ 3 + 30k(d+ 4) log 6k + log 1 ◆ .\nThe main claim finally holds since p⇤  p = 9 + 3 log 1 ."
  }, {
    "heading": "6.4. Proof of Theorem 4 (bounded support)",
    "text": "Proof. Let t = 128 + 64R4/ 4. Since the support of P is bounded, we have s(x)  t for all x 2 Rd. This implies that E\nP\nh s(x) 2 i  t and that 1\n2m\nP 2m\ni=1\ns(x\ni\n) 2  t almost surely. The result then follows from Theorem 5."
  }, {
    "heading": "7. Conclusion",
    "text": "We have presented a framework to uniformly approximate the expected value of unbounded functions on a sample. With this framework we are able to provide theoretical guarantees for empirical risk minimization in k-Means clustering if the kurtosis of the underlying distribution is bounded. In particular, we obtain state-of-the art bounds on the sufficient number of samples to achieve a given uniform approximation error. If the underlying distribution fulfills stronger assumptions, such as bounded higher moments, subgaussianity or bounded support, our analysis yields progressively better bounds. We conjecture that Theorem 5 can be applied to other related problems such as hard and soft Bregman clustering, likelihood estimation of Gaussian mixture models, as well as nonparametric clustering problems. However, such results do not follow immediately and require additional arguments beyond the scope of this paper."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing."
  }],
  "year": 2017,
  "references": [{
    "title": "Individual convergence rates in empirical vector quantizer design",
    "authors": ["Antos", "András", "L Gyorfi", "Gyorgy", "Andras"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2005
  }, {
    "title": "Coresets for nonparametric estimation - the case of DP-means",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Krause", "Andreas"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Fast and provably good seedings for k-means",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "S. Hamed", "Krause", "Andreas"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Distributed and provably good seedings for k-means in constant rounds",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Krause", "Andreas"],
    "venue": "In To appear in International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "The minimax distortion redundancy in empirical quantizer design",
    "authors": ["Bartlett", "Peter L", "Linder", "Tamás", "Lugosi", "Gábor"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1998
  }, {
    "title": "A framework for statistical clustering with constant time approximation algorithms for k-median and kmeans clustering",
    "authors": ["Ben-David", "Shai"],
    "venue": "Machine Learning,",
    "year": 2007
  }, {
    "title": "A sober look at clustering stability",
    "authors": ["Ben-David", "Shai", "Von Luxburg", "Ulrike", "Pál", "Dávid"],
    "venue": "In International Conference on Computational Learning Theory (COLT),",
    "year": 2006
  }, {
    "title": "Concentration inequalities: A nonasymptotic theory of independence",
    "authors": ["Boucheron", "Stéphane", "Lugosi", "Gábor", "Massart", "Pascal"],
    "year": 2013
  }, {
    "title": "Fast learning rates with heavy-tailed losses",
    "authors": ["Dinh", "Vu C", "Ho", "Lam S", "Nguyen", "Binh", "Duy"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Fast rates with unbounded losses",
    "authors": ["Grünwald", "Peter D", "Mehta", "Nishant A"],
    "venue": "arXiv preprint arXiv:1605.00252,",
    "year": 2016
  }, {
    "title": "Geometric approximation algorithms, volume 173",
    "authors": ["Har-Peled", "Sariel"],
    "venue": "American Mathematical Society Boston,",
    "year": 2011
  }, {
    "title": "Decision theoretic generalizations of the pac model for neural net and other learning applications",
    "authors": ["Haussler", "David"],
    "venue": "Information and Computation,",
    "year": 1992
  }, {
    "title": "Probability inequalities for sums of bounded random variables",
    "authors": ["Hoeffding", "Wassily"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1963
  }, {
    "title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems",
    "authors": ["Koltchinskii", "Vladimir"],
    "venue": "Lecture Notes in Mathematics. Springer,",
    "year": 2011
  }, {
    "title": "Fast rates for empirical vector quantization",
    "authors": ["Levrard", "Clément"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2013
  }, {
    "title": "Improved bounds on the sample complexity of learning",
    "authors": ["Li", "Yi", "Long", "Philip M", "Srinivasan", "Aravind"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 2001
  }, {
    "title": "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding",
    "authors": ["Linder", "Tamás", "Lugosi", "Gábor", "Zeger", "Kenneth"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1994
  }, {
    "title": "Strong coresets for hard and soft bregman clustering with applications to exponential family mixtures",
    "authors": ["Lucic", "Mario", "Bachem", "Olivier", "Krause", "Andreas"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2016
  }, {
    "title": "Training mixture models at scale via coresets",
    "authors": ["Lucic", "Mario", "Faulkner", "Matthew", "Krause", "Andreas", "Feldman", "Dan"],
    "venue": "To appear in Journal of Machine Learning Research (JMLR),",
    "year": 2017
  }, {
    "title": "Learning without concentration for general loss functions",
    "authors": ["Mendelson", "Shahar"],
    "venue": "arXiv preprint arXiv:1410.3192,",
    "year": 2014
  }, {
    "title": "Learning without concentration",
    "authors": ["Mendelson", "Shahar"],
    "venue": "In International Conference on Computational Learning Theory (COLT), pp",
    "year": 2014
  }, {
    "title": "The meaning of kurtosis: Darlington reexamined",
    "authors": ["Moors", "Johannes J A"],
    "venue": "The American Statistician,",
    "year": 1986
  }, {
    "title": "Strong consistency of k-means clustering",
    "authors": ["Pollard", "David"],
    "venue": "The Annals of Statistics,",
    "year": 1981
  }, {
    "title": "Convergence of stochastic processes",
    "authors": ["Pollard", "David"],
    "year": 1984
  }, {
    "title": "Stability of k-means clustering",
    "authors": ["Rakhlin", "Alexander", "Caponnetto", "Andrea"],
    "venue": "Advances in Neural Information Processing Systems (NIPS),",
    "year": 2007
  }, {
    "title": "On the best constant in Marcinkiewicz-Zygmund inequality",
    "authors": ["Ren", "Yao-Feng", "Liang", "Han-Ying"],
    "venue": "Statistics & Probability Letters,",
    "year": 2001
  }, {
    "title": "On the density of families of sets",
    "authors": ["Sauer", "Norbert"],
    "venue": "Journal of Combinatorial Theory, Series A,",
    "year": 1972
  }, {
    "title": "Cluster stability for finite samples",
    "authors": ["Shamir", "Ohad", "Tishby", "Naftali"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS), pp",
    "year": 2007
  }, {
    "title": "Model selection and stability in k-means clustering",
    "authors": ["Shamir", "Ohad", "Tishby", "Naftali"],
    "venue": "In International Conference on Computational Learning Theory (COLT),",
    "year": 2008
  }, {
    "title": "Moment-based uniform deviation bounds for k-means and friends",
    "authors": ["Telgarsky", "Matus J", "Dasgupta", "Sanjoy"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Weak convergence and empirical processes: with applications to statistics",
    "authors": ["van der Vaart", "Aad W", "Wellner", "Jon A"],
    "year": 1996
  }, {
    "title": "On the uniform convergence of relative frequencies of events to their probabilities",
    "authors": ["Vapnik", "Vladimir N", "Chervonenkis", "Alexey Ya"],
    "venue": "Theory of Probability & Its Applications,",
    "year": 1971
  }],
  "id": "SP:8a25bda607d7c8c815e77efa18172b1e290dd4e9",
  "authors": [{
    "name": "Olivier Bachem",
    "affiliations": []
  }, {
    "name": "Mario Lucic",
    "affiliations": []
  }, {
    "name": "S. Hamed Hassani",
    "affiliations": []
  }, {
    "name": "Andreas Krause",
    "affiliations": []
  }],
  "abstractText": "Uniform deviation bounds limit the difference between a model’s expected loss and its loss on a random sample uniformly for all models in a learning problem. In this paper, we provide a novel framework to obtain uniform deviation bounds for unbounded loss functions. As a result, we obtain competitive uniform deviation bounds for k-Means clustering under weak assumptions on the underlying distribution. If the fourth moment is bounded, we prove a rate of O ⇣",
  "title": "Uniform Deviation Bounds for k-Means Clustering"
}