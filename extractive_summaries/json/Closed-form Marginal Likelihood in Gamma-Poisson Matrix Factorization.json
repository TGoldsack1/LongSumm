{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The Gamma-Poisson (GaP) model is a probabilistic matrix factorization model which was introduced in the field of text information retrieval (Canny, 2004; Buntine & Jakulin, 2006). In this field, a corpus of text documents is typically represented by an integer-valued matrix V of size F × N , where each column vn represents a document as a so-called “bag of words”. Given a vocabulary of F words (or in practice semantic stems), the matrix entry vfn is the number of occurrences of word f in the document n. GaP is a generative model described by a dictionary of “topics” or “patterns” W (a non-negative matrix of size F ×K) and a nonnegative “activation” or “score” matrix H (of size K×N ), as follows:\nH ∼ ∏ k,n Gamma(hkn|αk, βk), (1)\nV|H ∼ ∏ f,n Poisson (vfn|[WH]fn) , (2)\n1IRIT, Université de Toulouse, CNRS, France. Correspondence to: Louis Filstroff <louis.filstroff@irit.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhere we use the shape and rate parametrization of the Gamma distribution, i.e., Gamma(x|α, β) = βα Γ(α)x α−1e−βx. The dictionary W is treated as a free deterministic variable.\nThough this generative model takes its origins in text information retrieval, it has found applications (with variants) in other areas such as image reconstruction (Cemgil, 2009), collaborative filtering (Ma et al., 2011; Gopalan et al., 2015) or audio signal processing (Virtanen et al., 2008).\nDenoting α = [α1, . . . , αK ]T , β = [β1, . . . , βK ]T , and treating the shape parameters αk as fixed hyperparameters, maximum joint likelihood estimation (MJLE) in GaP amounts to the minimization of\nCJL(W,H,β) def = − log p(V,H|W,β) (3) = DKL(V|WH) +Rα(H,β) + cst (4)\nwhere DKL(·|·) is the generalized Kullback-Leibler (KL) divergence defined by\nDKL(V|V̂) = ∑ f,n ( vfn log ( vfn v̂fn ) − vfn + v̂fn ) (5)\nand\nRα(H,β) =∑ k,n [(1− αk) log(hkn) + βkhkn]−N ∑ k αk log βk.\n(6)\nEquation (4) shows that MJLE is tantamount to penalized KL non-negative matrix factorization (NMF) (Lee & Seung, 2000) and may be addressed using alternating majorization-minimization (Canny, 2004; Févotte & Idier, 2011; Dikmen & Févotte, 2012).\nAs explained in Dikmen & Févotte (2012), MJLE can be criticized from a statistical point of view. Indeed, the number of estimated parameters grows with the number of samples N (this is because H has as many columns as V). To overcome this issue, they have instead proposed to consider maximum marginal likelihood estimation (MMLE), in which H is treated as a latent variable over which the\njoint likelihood is integrated. In other words, MMLE relies on the minimization of\nCML(W,β) def = − log p(V|W,β) (7) = − log ∫\nH p(V|H,W,β)p(H|β)dH. (8)\nWe emphasize that MMLE treats the dictionary W as a free deterministic variable. This is in contrast with fully Bayesian approaches where W is given a prior, and where estimation revolves around the posterior p(W,H|V). For instance, Buntine & Jakulin (2006) place a Dirichlet prior on the columns of W, while Cemgil (2009) considers independent Gamma priors. Zhou et al. (2012), Zhou & Carin (2015) set a Dirichlet prior on the columns of W and a Gamma-based non-parametric Bayesian prior on H, which allows for possible rank estimation.\nDikmen & Févotte (2012) assumed that a closed-form expression of CML was not available. Besides, they proposed variational and Monte Carlo Expectation-Maximization (MCEM) algorithms based on a complete set formed by H and a set of other latent components C that will later be defined. In their experiments, they found MMLE to be robust to over-specified values of K, while MJLE clearly overfit. This intriguing (and advantageous) behavior was left unexplained. In this paper, we provide the following contributions:\n• We provide a computable closed-form expression of CML. The expression is tedious to compute for large F and K, as it involves combinatorial operations, but is workable for reasonably dimensioned problems.\n• We show that the proposed closed-form expression reveals a penalization term on the columns of W that explains the “self-regularization” effect observed in Dikmen & Févotte (2012).\n• We show that the marginalization of H allows to derive a new MCEM algorithm with favorable properties.\nThe rest of the paper is organized as follows. Section 2 introduces preliminary material (composite form of GaP, useful probability distributions). In Section 3, we propose two new parameterizations of the GaP model in which H has been marginalized. This yields a closed-form expression of CML which is discussed in Section 4. Finally, a new MCEM algorithm is introduced in Section 5 and is compared to the MCEM algorithms proposed in Dikmen & Févotte (2012) on synthetic and real data."
  }, {
    "heading": "2. Preliminaries",
    "text": ""
  }, {
    "heading": "2.1. Composite structure of GaP",
    "text": "GaP can be written as a composite model, thanks to the superposition property of the Poisson distribution (Févotte & Cemgil, 2009):\nhkn ∼ Gamma(αk, βk) (9) ckn|hkn ∼ ∏ f Poisson(cfkn|wfkhkn) (10)\nvn = ∑ k ckn (11)\nThe vectors ckn = [c1kn, . . . , cFkn]T of size F and which sum up to vn are referred to as components. In the remainder, C will denote the F ×K ×N tensor with coefficients cfkn."
  }, {
    "heading": "2.2. Negative Binomial and Negative Multinomial distributions",
    "text": "In this section, we introduce two probability distributions that will be used later in the article."
  }, {
    "heading": "2.2.1. NEGATIVE BINOMIAL DISTRIBUTION",
    "text": "A discrete random variable X is said to have a negative binomial (NB) distribution with parameters α > 0 (called the dispersion or shape parameter) and p ∈ [0, 1] if, for all c ∈ N, the probability mass function (p.m.f.) of X is given by:\nP(X = c) = Γ(α+ c)\nΓ(α) c! (1− p)αpc. (12)\nIts variance is αp(1−p)2 , which is larger than its mean, αp 1−p . It is therefore a suitable distribution to model over-dispersed count data. Indeed, it offers more flexibility than the Poisson distribution where the variance and the mean are equal.\nThe NB distribution can be obtained via a Gamma-Poisson mixture, that is:\nP(X = c) = ∫ R+ Poisson(c|λ)Gamma(λ|α, β)dλ (13)\n= NB ( α, 1\nβ + 1\n) . (14)"
  }, {
    "heading": "2.2.2. NEGATIVE MULTINOMIAL DISTRIBUTION",
    "text": "The negative multinomial (NM) distribution (Sibuya et al., 1964) is the multivariate generalization of the NB distribution. It is parametrized by a dispersion parameter α > 0 and a vector of event probabilities p = [p1, . . . , pF ]T , where 0 ≤ pf ≤ 1 and ∑ f pf ≤ 1. Denoting p0 =\n1− ∑ f pf , and for all (c1, . . . , cF ) ∈ NF , the p.m.f. of the NM distribution is given by\nP(X1 = c1, . . . , XF = cF ) = Γ(α+\n∑ f cf )\nΓ(α) ∏ f cf ! pα0 ∏ f p cf f ,\n(15) with expectation given by\nE(X) = α [ p1 p0 , . . . , pF p0 ]T . (16)\nIn particular, the NM distribution arises in the following Gamma-Poisson mixture, as detailed in the next proposition.\nProposition 1. Let X = [X1, . . . , XF ]T be a random vector whose entries are independent Poisson random variables. Assume that each variable Xf is governed by the parameterwfλ, where λ is itself a Gamma random variable with parameters (α, β). Then the joint probability distribution of X is a NM distribution with dispersion parameter α and event probabilities\np = [ w1∑\nf wf + β , . . . , wF∑ f wf + β\n]T . (17)\nProof. P(X = [c1, . . . , cF ]T )\n= ∫ R+ P(X1 = c1, . . . , XF = cF |λ)p(λ)dλ\n= ∫ R+ ∏ f (wfλ) cf e−wfλ cf !  βα Γ(α) λα−1e−βλdλ\n= ∏ f w cf f cf !  βα Γ(α) Γ(α+ ∑ f cf )(∑ f wf + β )α+∑f cf\n= Γ(α+\n∑ f cf )\nΓ(α) ∏ f cf !\n( β∑\nf wf + β )α∏ f ( wf∑ f wf + β )cf\nThe NM distribution can also be obtained with an alternative generative process, as shown in the following proposition.\nProposition 2. Let Y = [Y1, . . . , YF ]T be a random vector following a multinomial distribution with number of trials L and event probabilities p = [ w1∑\nf wf , . . . , wF∑ f wf ]T . As-\nsume that L is a NB random variable with dispersion parameter α and probability p = ∑ f wf∑\nf wf+β . Then the ran-\ndom vectors Y and X (as defined in Proposition 1) have the same distribution.\nProof. P(Y = [c1, . . . , cF ]T )\n= P(Y = [c1, . . . , cF ]T |L)× P(L) = L!∏ f cf ! ∏ f ( wf∑ f wf )cf\n× Γ(α+ L) Γ(α)L!\n( β∑\nf wf + β\n)α( ∑ f wf∑\nf wf + β\n)L\nNoting that L = ∑ f cf completes the proof."
  }, {
    "heading": "3. New formulations of GaP",
    "text": "We now show how GaP can be rewritten free of the latent variables H in two different ways."
  }, {
    "heading": "3.1. GaP as a composite NM model",
    "text": "Theorem 1. GaP can be rewritten as follows:\nckn ∼ NM αk,[ w1k∑ f wfk + βk , . . . , wFk∑ f wfk + βk ]T (18)\nvn = ∑ k ckn (19)\nProof. Combining Equations (9)-(11) with Proposition 1 completes the proof.\nGaP may thus be interpreted as a composite model in which the kth component has a NM distribution with parameters governed by wk (the kth column of W), αk and βk. Using straightforward computations, the data expectation can be expressed as\nE(vn) = ∑ k E(ckn) (20)\n= ∑ k αk βk wk. (21)"
  }, {
    "heading": "3.2. GaP as a composite multinomial model",
    "text": "Theorem 2. GaP can be rewritten as follows:\nLkn ∼ NB ( αk, ∑ f wfk∑\nf wfk + βk\n) (22)\nckn ∼ Mult Lkn,[ w1k∑ f wfk , . . . , wFk∑ f wfk ]T (23) vn =\n∑ k ckn (24)\nwhere “Mult” refers to the multinomial distribution.\nProof. Combining Equations (9)-(11) with Proposition 2 completes the proof.\nTheorem 2 states that another interpretation of GaP consists in modeling the data as a sum of k independent multinomial distributions, governed individually by wk and whose number of trials is random, following a NB distribution governed by wk, αk and βk.\nA special case of the reformulation of GaP offered by Theorem 2 is given by Buntine & Jakulin (2006) using a different reasoning, when it is assumed that ∑ f wfk = 1 (a common assumption in the field of text information retrieval, where the columns of W are interpreted as discrete probability distributions). Theorem 2 provides a more general result as it applies to any non-negative matrix W."
  }, {
    "heading": "4. Closed-form marginal likelihood",
    "text": ""
  }, {
    "heading": "4.1. Analytical expression",
    "text": "Until now, it was assumed that the marginal likelihood in the GaP model was not available analytically. However, the new parametrization offered by Theorem 1 allows to obtain a computable analytical expression of the marginal likelihood CML. Denote by C the set of all “admissible” components, i.e.,\nC = {C ∈ NF×K×N | ∀(f, n), ∑\nk cfkn = vfn}. (25)\nBy marginalization of C, we may write\np(V|W,β) = ∑ C∈C p(C|W,β) (26)\n= ∑ C∈C ∏ k,n p(ckn|W,β). (27)\nUsing Equation (18) we obtain\np(V|W,β) =∑ C∈C ∏ k,n [ Γ( ∑ f cfkn + αk) Γ(αk) ∏ f cfkn! ( βk∑ f wfk + βk )αk\n× ∏ f\n( wfk∑\nf wfk + βk )cfkn . (28) Introducing the notations\nΩα(C) = ∏ k,n\nΓ( ∑ f cfkn + αk)\nΓ(αk) ∏ f cfkn!\n(29)\nand pfk = wfk∑ f wfk + βk\n(30)\nwe may rewrite Eq. (28) as\np(V|W,β) = ∏ k (1− ∑ f pfk) Nαk  × ∑ C∈C Ωα(C)∏ f,k p ∑ n cfkn fk\n . (31) Equation (31) is a computable closed-form expression of the marginal likelihood. It is free of H and in particular of the integral that appears in Equation (8). However the expression (31) is still semi-explicit because it involves a sum over the set of all admissible components C. C is countable set with cardinality #C = ∏ f,n ( vfn+K−1 K−1 ) . It is straightforward to construct but challenging to compute in large dimension, and for large values of vfn.\nThe sum over all the matrices in the set C expresses the convolution of the (discrete) probability distributions of the K components. Unfortunately, the distribution of the sum of independent negative multinomial variables of different event probabilities is not available in closed form.\nAs already known from Dikmen & Févotte (2012), the value of the marginal likelihood is unchanged when the scales of the columns of W and the rates β are changed accordingly. Let Λ be a non-negative diagonal matrix of size K, it can easily be derived from Equation (31) that\np(V|WΛ,βΛ) = p(V|W,β). (32)\nWe therefore have a scaling invariance between W and β, and as such, we may fix β to arbitrary values and leave W free. Thus, we will treat β as a constant in the following and drop it from the arguments of CML."
  }, {
    "heading": "4.2. Self-regularization",
    "text": "Dikmen & Févotte (2012) empirically studied the properties of MMLE. In particular, they observed the self-ability of the estimator to regularize the number of columns of W. For example, one experiment consisted in generating synthetic data according to the GaP model, with a ground-truth number of componentsK?. MMLE was run withK > K? and they noticed that the estimated W contained K −K? empty columns. As such, the estimator was able to recover the ground-truth dimensionality. In contrast, MJLE used all K dimensions and overfit the data. They were unable to give a theoretical justification of the observed phenomenon, but provided a first insight thanks to a Laplace approximation of p(V|W). The closed-form expression (31) offers\na deeper understanding of this phenomenon, as explained next.\nUsing Equations (31) and (30) and treating β as a constant, the negative log-likelihood can be expressed as\n1\nN CML(W) =\n− 1 N log ∑ C∈C Ωα(C) ∏ f,k p ∑ n cfkn fk  (33) + ∑ k αk log(||wk||1 + βk) + cst, (34)\nwhere cst = − ∑ k αk log βk.\nThe negative log-likelihood reveals two terms. The first term, Equation (33), captures the interaction between data V (through C) and the parameter W (through the event probabilities pfk = wfk/(‖wk‖1 +βk)). The second term, Equation (34), only depends on the parameter W and can be interpreted as a group-regularization term. The nonconvex and sharply peaked function f(x) = ∑ k log(xk + b) is known to be sparsity-inducing (Candès et al., 2008). As such, the term (34) will promote sparsity of the norms of the columns of W. When a norm ||wk||1 is set to zero for some k, the whole column wk is set to zero because of the non-negativity constraint. This gives a formal explanation of the ability of MMLE to automatically prune columns of W, without any explicit sparsity-inducing prior at the modeling stage (recall that W is a deterministic parameter without a prior)."
  }, {
    "heading": "5. MCEM algorithms form MMLE",
    "text": ""
  }, {
    "heading": "5.1. Expectation-Maximization",
    "text": "We now turn to the problem of optimizing Equation (8) by leveraging on the results of Section 4. Despite obtaining a closed-form expression, the direct optimization of the marginal likelihood remains difficult. However, the structure of GaP makes Expectation-Maximization (EM) a natural option (Dempster et al., 1977). Indeed, GaP involves observed variables V and latent variables C and H. As such, we can derive several EM algorithms based on various choices of the complete set. More precisely, we consider three possible choices that each define a different algorithm, as follows.\nEM-CH. The complete set is {C,H} and EM consists in the iterative minimization w.r.t W of the functional defined by QCH(W|W̃) = − ∫\nC,H log p(C,H|W)p(C,H|V, W̃)dCdH,\n(35)\nwhere W̃ is the current estimate. Note that V does not need to be included in the complete set because we have V = ∑ k Ck. This corresponds to the general formulation of EM in which the relation between the complete set and the data is a many-to-one mapping and slightly differs from the more usual one where the complete set is formed by the union of data and a hidden set (Dempster et al., 1977; Févotte et al., 2011).\nEM-H. The complete set is {V,H} and EM consists in the iterative minimization of\nQH(W|W̃) = − ∫\nH log p(V,H|W)p(H|V, W̃)dH. (36)\nEM-C. The complete set is merely {C} and EM consists in the iterative minimization of\nQC(W|W̃) = − ∫\nC log p(C|W)p(C|V, W̃)dC. (37)\nEM-CH and EM-H have been considered in Dikmen & Févotte (2012). EM-C is a new proposal that exploits the results of Section 4. In all three cases, the posteriors of the latent variables involved – p(C,H|V, W̃), p(H|V, W̃) and p(C|V, W̃) – are untractable and neither are the integrals involved in Equations (35), (36) and (37). To overcome this problem, we resort to Monte Carlo EM (MCEM) (Wei & Tanner, 1990) as described in the next section."
  }, {
    "heading": "5.2. Monte Carlo E-step",
    "text": "MCEM consists in using a Monte Carlo (MC) approximation of the integrals in Equations (35), (36) and (37) based on samples drawn from the posterior distributions p(C,H|V, W̃), p(H|V, W̃) and p(C|V, W̃). These can be obtained by Gibbs sampling of the joint posterior p(C,H|V, W̃), which also returns samples from the marginals p(H|V, W̃) and p(C|V, W̃) at convergence. The Gibbs sampler can easily be derived because the conditional distributions p(H|C,V, W̃) = p(H|C, W̃) and p(C|H,V, W̃) are available in closed form.\nAt iteration j + 1, Gibbs sampling writes\nh (j+1) kn ∼ Gamma(αk + ∑ f c (j) fkn, βk + ∑ f w̃fk) (38)\nc(j+1)fn ∼ Mult ( vfn, [ ρ (j+1) f1 , . . . , ρ (j+1) fK ]T) (39)\nwhere cfn denotes the vector [cf1n, . . . , cfKn]T of size K and\nρ (j+1) fk = w̃fkh (j+1) kn∑\nk w̃fkh (j+1) kn\n. (40)\nNote that c(j+1)fn only needs to be sampled when vfn 6= 0, since c(j+1)fn = [0, . . . , 0] T when vfn = 0."
  }, {
    "heading": "5.3. M-step",
    "text": "Given a set of J samples {H(j),C(j)} drawn from p(C,H|V, W̃) (after burn-in), minimization of the MC approximation of QCH in Equation (35) yields\nwMCEM-CHfk =\n∑ j,n c\n(j) fkn∑\nj,n h (j) kn\n, (41)\nas shown by Dikmen & Févotte (2012). They also show that the following multiplicative update decreases the MC approximation of QH in Equation (36) at every iteration\nwMCEM-Hfk = w̃fk\n∑ j,n h (j) knvfn[W̃H\n(j)]−1fn∑ j,n h (j) kn . (42)\nWe now derive the new update for EM-C. The MC approximation of QC in Equation (37) is given by:\nQ̂C(W|W̃) def = − 1\nJ J∑ j=1 log p(C(j)|W). (43)\nReplacing p(C(j)|W) by its expression given by Equation (18), we obtain:\nQ̂C(W|W̃) = 1\nJ ∑ j,k,n\nαk log ∑\nf\nwfk + βk  + ∑ f c (j) fkn ( log (∑ f wfk + βk ) − log (wfk) ) + cst\n . (44)\nThe minimization of Q̂C w.r.t. W leads toK linear systems of equations that we need to solve for each column wk:\nAkwk = bk. (45)\nThe matrix Ak ∈ RF×F is defined by:\nafg = JNαk + ∑ j,f,n c (j) fkn  δfg −∑ j,n c (j) fkn, (46)\nwhere δfg is the Kronecker symbol, i.e. δfg = 1 if and only if f = g, and zero otherwise. The vector bk ∈ RF×1 is defined by:\nbfk = βk ∑ j,n c (j) fkn. (47)\nThe matrix Ak appears to be the sum of a diagonal matrix with a rank-1 matrix and can be inverted analytically thanks\nto the Sherman-Morrison formula (Sherman & Morrison, 1950). This results in the new closed-form update\nwMCEM-Cfk = 1\nJN βk αk ∑ j,n c (j) fkn. (48)\nIn the common case where βkαk is equal to a constant γ for all k, we can write:\n∑ k wMCEM-Cfk = ∑ k βk αk\n∑ j,n c (j) fkn\nJN = γ\n∑ j,n vfn\nJN = γvf\n(49) where vf = N−1 ∑ n vfn is the empirical mean of the data for the feature f . Equation (49) implies that the rows of the estimate WMCEM-C at every iteration sum to a constant. This behavior is illustrated on Figure 1."
  }, {
    "heading": "6. Experimental results",
    "text": "We now compare the three MCEM algorithms proposed for MMLE in the GaP model, first using synthetic toy datasets, then real-world data. Python implementations of the three algorithms are available from the first author website."
  }, {
    "heading": "6.1. Experiments with synthetic data",
    "text": "We generate a dataset of N = 100 samples according to the GaP model, with the following parameters:\nW?1 =  0.638 0.075 0.009 0.568 0.044 0.126 0.309 0.231  , α? = β? = 1. (50) The columns of W?1 have been generated from a Dirichlet distribution (with parameters 1). The generated dataset (of size 4× 100) is denoted by V1.\nWe proceed to estimate the dictionary W using hyperparemeters K = K? + 1 = 3, αk = βk = 1 with MCEM-C, MCEM-H and MCEM-CH. The algorithms are run for 500 iterations. 300 Gibbs samples are generated at each iteration, with the first 150 samples being discarded for burn-in (this proves to be enough in practice), leading to J = 150. The Gibbs sampler at EM iteration i + 1 is initialized with the last sample obtained at EM iteration i (warm restart). The algorithms are initialized from the same deterministic starting point given by\nwfk = 1\nK βk αk vf , (51)\nas suggested by Equation (49).\nFigure 2 displays the negative log-likelihood CML(W), computed thanks to the derivations of Section 4, and Figure 3-(a) displays the norm of the three columns of the iterates, both w.r.t. CPU time in seconds. The three algorithms have almost identical computation times, most of the computational burden residing in the Gibbs sampling procedure that is common to the three algorithms. Moreover, the three algorithms converge to the same point, with MCEM-C converging marginally faster than the other two in this case.\nThen we proceed to generate a second dataset V2 according to the GaP model, with now W?2 = 100 ×W?1. The\nexpectation of V2 is thus a hundred times the expectation of V1. We apply the exact same experimental protocol to V2 as we did for V1, except that the algorithms are now run for a larger number of 2000 iterations. In this case, because of the combinatorial nature of #C, it is impossible to compute the likelihood in reasonable time. The norms of the columns of the iterates are displayed on Figure 3-(b). As we can see, MCEM-C clearly outperforms the other two algorithms in this scenario. This behavior has been consistently found when estimating dictionaries from datasets with sufficiently large values. This drastic difference in convergence is unexplained at this stage. However, we conjecture that this is linked to the over-dispersion of the data (i.e. when the variance is greater than the mean), which increases with the scale of W. This will be studied in future work."
  }, {
    "heading": "6.2. Experiments with the NIPS dataset",
    "text": "Finally, we consider the NIPS dataset which contains word counts from a collection of articles published at the NIPS conference.1 The number of articles is N = 1, 500 and the number of unique terms (appearing at least 10 times after tokenization and removing stop-words) is F = 12, 419. The matrix V is quite sparse as 96% of its coefficients are zeros. This saves a large amount of computational effort, because we only need to sample cfn for pairs (f, n) such that vfn is non-zero. Moreover, the count values range from 0 to 132.\nWe applied MCEM-C and MCEM-CH with K = 10 and αk = βk = 1. The algorithms are run for 1, 000 iterations. 250 Gibbs samples are generated in each iteration with the first half being discarded for burn-in (i.e. J = 125). The Gibbs sampler at iteration i + 1 is again initialized with warm restart. The algorithms are initialized using Equation (51). MCEM-H results in similar performance than MCEM-CH and is not reported here.\nFigure 4 shows that the column norms of the iterates w.r.t. CPU time in seconds. The difference in convergence speed between the two algorithms is again striking. MCEM-C efficiently explores the parameter space in the first iterations and converges dramatically faster than MCEM-CH. The algorithms here converge to different solutions, which confirms the non-convexity of CML(W). Other runs confirmed that MCEM-C is consistently faster, and also that the two algorithms do not always converge to the same solution.\nMCEM-C still takes a few hours to converge. We implemented stochastic variants of the EM algorithm such as SAEM (Kuhn & Lavielle, 2004), however this did not result in significant improvements in our case. Finally, note that large-scale implementations are possible using for example on-line EM (Cappé & Moulines, 2009)."
  }, {
    "heading": "7. Conclusion",
    "text": "In this paper, we have shown how the Gamma-Poisson model can be rewritten free of the latent variables H. This new parametrization enabled us to come up with a closedform expression of the marginal likelihood, which revealed a penalization term explaining the “self-regularization” effect described in Dikmen & Févotte (2012). We then proceeded to compare three MCEM algorithms for the task of maximizing the likelihood, and the algorithm taking advantage of the marginalization H has been empirically proven to have better convergence properties.\nIn this work, we treated α as a fixed hyperparameter. Fu-\n1https://archive.ics.uci.edu/ml/datasets/ bag+of+words\nture work will focus on lifting this hypothesis, and on designing algorithms to estimate both W and α. We will also look into carrying a similar analysis in other probabilistic matrix factorization models, such as the GammaExponential model (Dikmen & Févotte, 2011)."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Benjamin Guedj, Vı́ctor Elvira and Jérôme Idier for fruitful discussions related to this work. This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program under grant agreement No 681839 (project FACTORY)."
  }],
  "year": 2018,
  "references": [{
    "title": "Discrete component analysis",
    "authors": ["W. Buntine", "A. Jakulin"],
    "venue": "In Subspace, Latent Structure and Feature Selection,",
    "year": 2006
  }, {
    "title": "Enhancing sparsity by reweighted l1 minimization",
    "authors": ["E.J. Candès", "M.B. Wakin", "S.P. Boyd"],
    "venue": "Journal of Fourier analysis and applications,",
    "year": 2008
  }, {
    "title": "GaP: a factor model for discrete data",
    "authors": ["J. Canny"],
    "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
    "year": 2004
  }, {
    "title": "On-line expectation– maximization algorithm for latent data models",
    "authors": ["O. Cappé", "E. Moulines"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2009
  }, {
    "title": "Maximum likelihood from incomplete data via the EM algorithm",
    "authors": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"],
    "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),",
    "year": 1977
  }, {
    "title": "Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation",
    "authors": ["O. Dikmen", "C. Févotte"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2011
  }, {
    "title": "Maximum marginal likelihood estimation for nonnegative dictionary learning in the gamma-poisson model",
    "authors": ["O. Dikmen", "C. Févotte"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2012
  }, {
    "title": "Nonnegative matrix factorizations as probabilistic inference in composite models",
    "authors": ["C. Févotte", "A.T. Cemgil"],
    "venue": "In Proceedings of the European Signal Processing Conference (EUSIPCO),",
    "year": 1913
  }, {
    "title": "Algorithms for nonnegative matrix factorization with the β-divergence",
    "authors": ["C. Févotte", "J. Idier"],
    "venue": "Neural computation,",
    "year": 2011
  }, {
    "title": "Efficient Markov chain Monte Carlo inference in composite models with space alternating data augmentation",
    "authors": ["C. Févotte", "O. Cappé", "A.T. Cemgil"],
    "venue": "In Proceedings of the IEEE Workshop on Statistical Signal Processing (SSP),",
    "year": 2011
  }, {
    "title": "Scalable recommendation with hierarchical poisson factorization",
    "authors": ["P. Gopalan", "J.M. Hofman", "D.M. Blei"],
    "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI),",
    "year": 2015
  }, {
    "title": "Coupling a stochastic approximation version of EM with an MCMC procedure",
    "authors": ["E. Kuhn", "M. Lavielle"],
    "venue": "ESAIM: Probability and Statistics,",
    "year": 2004
  }, {
    "title": "Algorithms for non-negative matrix factorization",
    "authors": ["D.D. Lee", "H.S. Seung"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2000
  }, {
    "title": "Probabilistic factor models for web site recommendation",
    "authors": ["H. Ma", "C. Liu", "I. King", "M.R. Lyu"],
    "venue": "In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval,",
    "year": 2011
  }, {
    "title": "Adjustment of an inverse matrix corresponding to a change in one element of a given matrix",
    "authors": ["J. Sherman", "W.J. Morrison"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1950
  }, {
    "title": "Negative multinomial distribution",
    "authors": ["M. Sibuya", "I. Yoshimura", "R. Shimizu"],
    "venue": "Annals of the Institute of Statistical Mathematics,",
    "year": 1964
  }, {
    "title": "Bayesian extensions to non-negative matrix factorisation for audio signal modelling",
    "authors": ["T. Virtanen", "A.T. Cemgil", "S. Godsill"],
    "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2008
  }, {
    "title": "A Monte Carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms",
    "authors": ["G.C. Wei", "M.A. Tanner"],
    "venue": "Journal of the American statistical Association,",
    "year": 1990
  }, {
    "title": "Negative binomial process count and mixture modeling",
    "authors": ["M. Zhou", "L. Carin"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }, {
    "title": "Betanegative binomial process and Poisson factor analysis",
    "authors": ["M. Zhou", "L. Hannah", "D. Dunson", "L. Carin"],
    "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2012
  }],
  "id": "SP:d39fef5bab55d3bc65c92faca815cc62646aed38",
  "authors": [{
    "name": "Louis Filstroff",
    "affiliations": []
  }, {
    "name": "Alberto Lumbreras",
    "affiliations": []
  }, {
    "name": "Cédric Févotte",
    "affiliations": []
  }],
  "abstractText": "We present novel understandings of the GammaPoisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties.",
  "title": "Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization"
}