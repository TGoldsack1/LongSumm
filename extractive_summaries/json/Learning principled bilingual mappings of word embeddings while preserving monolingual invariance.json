{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289–2294, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Bilingual word embeddings have attracted a lot of attention in recent times (Zou et al., 2013; Kočiský et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and Søgaard, 2015; Luong et al., 2015; Wick et al., 2016). A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015).\nThe first method to learn bilingual word embedding mappings was proposed by Mikolov et al. (2013b), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. Subsequent work has proposed alternative optimization objectives to learn\nbetter mappings. Xing et al. (2015) incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection. Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space.\nBeyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language. Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b), both through the neighbor retrieval method (Dinu et al., 2015) and the training itself (Lazaridou et al., 2015). We leave the study of non-linear transformation and other additions for further work.\nIn this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them. Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.\n2289"
  }, {
    "heading": "2 Learning bilingual mappings",
    "text": "Let X and Z denote the word embedding matrices in two languages for a given bilingual dictionary so that their ith row Xi∗ and Zi∗ are the word embeddings of the ith entry in the dictionary. Our goal is to find a linear transformation matrix W so that XW best approximates Z, which we formalize minimizing the sum of squared Euclidean distances following Mikolov et al. (2013b):\narg min W\n∑\ni\n‖Xi∗W − Zi∗‖2\nAlternatively, this is equivalent to minimizing the (squared) Frobenius norm of the residual matrix:\narg min W\n‖XW − Z‖2F\nConsequently, W will be the so called leastsquares solution of the linear matrix equation XW = Z. This is a well-known problem in linear algebra and can be solved by taking the MoorePenrose pseudoinverse X+ = ( XTX )−1 XT as W = X+Z, which can be computed using SVD."
  }, {
    "heading": "2.1 Orthogonality for monolingual invariance",
    "text": "Monolingual invariance is needed to preserve the dot products after mapping, avoiding performance degradation in monolingual tasks (e.g. analogy). This can be obtained requiring W to be an orthogonal matrix (W TW = I). The exact solution under such orthogonality constraint is given by W = V UT , where ZTX = UΣV T is the SVD factorization of ZTX (cf. Appendix A). Thanks to this, the optimal transformation can be efficiently computed in linear time with respect to the vocabulary size. Note that orthogonality enforces an intuitive property, and as such it could be useful to avoid degenerated solutions and learn better bilingual mappings, as we empirically show in Section 3."
  }, {
    "heading": "2.2 Length normalization for maximum cosine",
    "text": "Normalizing word embeddings in both languages to be unit vectors guarantees that all training instances contribute equally to the optimization goal. As long as W is orthogonal, this is equivalent to maximizing the sum of cosine similarities for the dictionary\nentries, which is commonly used for similarity computations:\narg min W\n∑\ni\n∥∥∥∥ Xi∗ ‖Xi∗‖ W − Zi∗‖Zi∗‖ ∥∥∥∥ 2\n= arg max W\n∑\ni\ncos (Xi∗W,Zi∗)\nThis last optimization objective coincides with Xing et al. (2015), but their work was motivated by an hypothetical inconsistency in Mikolov et al. (2013b), where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by Xing et al. (2015) is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to Xing et al. (2015), who introduce orthogonality only to ensure that unit length is preserved after mapping."
  }, {
    "heading": "2.3 Mean centering for maximum covariance",
    "text": "Dimension-wise mean centering captures the intuition that two randomly taken words would not be expected to be semantically similar, ensuring that the expected product of two random embeddings in any dimension and, consequently, their cosine similarity, is zero. As long as W is orthogonal, this is equivalent to maximizing the sum of dimensionwise covariance for the dictionary entries:\narg min W\n‖CmXW − CmZ‖2F\n= arg max W\n∑\ni\ncov (XW∗i, Z∗i)\nwhere Cm denotes the centering matrix This equivalence reveals that the method proposed by Faruqui and Dyer (2014) is closely related to our framework. More concretely, Faruqui and Dyer (2014) use Canonical Correlation Analysis (CCA) to project the word embeddings in both languages to a shared vector space. CCA maximizes\nthe dimension-wise covariance of both projections (which is equivalent to maximizing the covariance of a single projection if the transformations are constrained to be orthogonal, as in our case) but adds an implicit restriction to the two mappings, making different dimensions have the same variance and be uncorrelated among themselves1:\narg max A,B\n∑\ni\ncov (XA∗i, ZB∗i)\ns.t. ATXTCmXA = BTZTCmZB = I\nTherefore, the only fundamental difference between both methods is that, while our model enforces monolingual invariance, Faruqui and Dyer (2014) do change the monolingual embeddings to meet this restriction. In this regard, we think that the restriction they add could have a negative impact on the learning of the bilingual mapping, and it could also degrade the quality of the monolingual embeddings. Our experiments (cf. Section 3) show empirical evidence supporting this idea."
  }, {
    "heading": "3 Experiments",
    "text": "In this section, we experimentally test the proposed framework and all its variants in comparison with related methods. For that purpose, we use the translation induction task introduced by Mikolov et al. (2013b), which learns a bilingual mapping on a small dictionary and measures its accuracy on predicting the translation of new words. Unfortunately, the dataset they use is not public. For that reason, we use the English-Italian dataset on the same task provided by Dinu et al. (2015)2. The dataset contains monolingual word embeddings trained with the word2vec toolkit using the CBOW method with negative sampling (Mikolov et al., 2013a)3. The English embeddings were trained on a 2.8 billion word corpus (ukWaC + Wikipedia + BNC), while the 1.6 billion word corpus itWaC was used to train the Italian\n1While CCA is typically defined in terms of correlation (thus its name), correlation is invariant to the scaling of variables, so it is possible to constrain the canonical variables to have a fixed variance, as we do, in which case correlation and covariance become equivalent\n2http://clic.cimec.unitn.it/˜georgiana. dinu/down/\n3The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10\nembeddings. The dataset also contains a bilingual dictionary learned from Europarl, split into a training set of 5,000 word pairs and a test set of 1,500 word pairs, both of them uniformly distributed in frequency bins. Accuracy is the evaluation measure.\nApart from the performance of the projected embeddings in bilingual terms, we are also interested in the monolingual quality of the source language embeddings after the projection. For that purpose, we use the word analogy task proposed by Mikolov et al. (2013a), which measures the accuracy on answering questions like “what is the word that is similar to small in the same sense as biggest is similar to big?” using simple word vector arithmetic. The dataset they use consists of 8,869 semantic and 10,675 syntactic questions of this type, and is publicly available4. In order to speed up the experiments, we follow the authors and perform an approximate evaluation by reducing the vocabulary size according to a frequency threshold of 30,000 (Mikolov et al., 2013a). Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves. With these settings, we obtain a coverage of 64.98%.\nWe implemented the proposed method in Python using NumPy, and make it available as an open source project5. The code for Mikolov et al. (2013b) and Xing et al. (2015) is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of Xing et al. (2015) (postprocessing instead of constrained training). As for the method by Faruqui and Dyer (2014), we used their original implementation in Python and MATLAB6, which we extended to cover cases where the dictionary contains more than one entry for the same word.\n4https://code.google.com/archive/p/ word2vec/\n5https://github.com/artetxem/vecmap 6https://github.com/mfaruqui/\ncrosslingual-cca"
  }, {
    "heading": "3.1 Results of our framework",
    "text": "The rows in Table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by Mikolov et al. (2013b) (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, incrementally, mean centering. In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.\nThe results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have. The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance."
  }, {
    "heading": "3.2 Comparison to other work",
    "text": "Table 2 shows the results for our best performing configuration in comparison to previous work. As discussed before, (Mikolov et al., 2013b) and (Xing et al., 2015) were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.\nAs it can be seen, the method by Xing et al. (2015) performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what they report in their paper. Moreover, thanks to the orthogonality constraint their monolingual performance in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings.\nSince Faruqui and Dyer (2014) take advantage of\nCCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions). This beats the method by Xing et al. (2015) in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality.\nIn any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimensionwise mean centering that achieves the best accuracy in the word translation induction task. Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for Faruqui and Dyer (2014).\nWhen compared to Xing et al. (2015), our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to Mikolov et al. (2013b) comes solely from the orthogonality constraint, and not from solving any inconsistency.\nIt should be noted that the implementation by Faruqui and Dyer (2014) also length-normalizes the word embeddings in a preprocessing step. Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by Faruqui and Dyer (2014), as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves. However, our model performs considerably better than any configuration from Faruqui and Dyer (2014) in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing,\nbut also have a negative impact."
  }, {
    "heading": "4 Conclusions",
    "text": "This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation. Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of Xing et al. (2015) and Faruqui and Dyer (2014). It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods. In the future, we would like to study non-linear mappings (Lu et al., 2015) and the additional techniques in (Lazaridou et al., 2015)."
  }, {
    "heading": "Acknowledgments",
    "text": "This research was partially supported by the European Commision (QTLeap FP7-ICT-2013-10610516), a Google Faculty Award, and the Spanish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P). Mikel Artetxe enjoys a doctoral grant from the Spanish Ministry of Education, Culture and Sports."
  }, {
    "heading": "A Proof of solution under orthogonality",
    "text": "Constraining W to be orthogonal (W TW = I), the original minimization problem can be reformulated as follows (cf. Section 2.1):\narg min W\n∑\ni\n‖Xi∗W − Zi∗‖2\n= arg min W\n∑\ni\n( ‖Xi∗W‖2 + ‖Zi∗‖2 − 2Xi∗WZTi∗ )\n= arg max W\n∑\ni\nXi∗WZTi∗\n= arg max W\nTr ( XWZT )\n= arg max W\nTr ( ZTXW )\nIn the above expression, Tr(·) denotes the trace operator (the sum of all the elements in the main diagonal), and the last equality is given by its cyclic\nproperty. At this point, we can take the SVD of ZTX as ZTX = UΣV T , so Tr ( ZTXW ) = Tr ( UΣV TW ) = Tr ( ΣV TWU ) . Since V T , W and U are orthogonal matrices, their product V TWU will also be an orthogonal matrix. In addition to that, given that Σ is a diagonal matrix, its trace after an orthogonal transformation will be maximal when the values in its main diagonal are preserved after the mapping, that is, when the orthogonal transformation matrix is the identity matrix. This will happen when V TWU = I in our case, so the optimal solution will be W = V UT ."
  }],
  "year": 2016,
  "references": [{
    "title": "Deep multilingual correlation for improved word embeddings",
    "authors": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
    "year": 2015
  }, {
    "title": "Bilingual word representations with monolingual quality in mind",
    "authors": ["Min-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "NAACL Workshop on Vector Space Modeling for NLP, pages 151–159.",
    "year": 2015
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1309.4168.",
    "year": 2013
  }, {
    "title": "Minimally-constrained multilingual embeddings via artificial code-switching",
    "authors": ["Michael Wick", "Pallika Kanani", "Adam Pocock."],
    "venue": "Thirtieth AAAI conference on Artificial Intelligence (AAAI).",
    "year": 2016
  }, {
    "title": "Normalized word embedding and orthogonal transform for bilingual word translation",
    "authors": ["Chao Xing", "Dong Wang", "Chao Liu", "Yiye Lin."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:",
    "year": 2015
  }, {
    "title": "Learning translation models from monolingual continuous representations",
    "authors": ["Kai Zhao", "Hany Hassan", "Michael Auli."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
    "year": 2015
  }, {
    "title": "Bilingual word embeddings for phrase-based machine translation",
    "authors": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398.",
    "year": 2013
  }],
  "id": "SP:9a2eed5f8175275af0d55d4aed39afc8e2b2acf2",
  "authors": [{
    "name": "Mikel Artetxe",
    "affiliations": []
  }, {
    "name": "Gorka Labaka",
    "affiliations": []
  }, {
    "name": "Eneko Agirre",
    "affiliations": []
  }],
  "abstractText": "Mapping word embeddings of different languages into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task.",
  "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance"
}