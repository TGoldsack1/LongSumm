{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2814–2819 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Automatically analyzing and generating natural language requires capturing not only what is said, but also how to say it. Consider the sentences “anybody hurt?” and “is someone wounded?”. The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used. Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation.\nIn this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation. Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job. We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input. While ultimately we would like systems to preserve the formality of the source, this is a\nchallenging task that requires not only automatically inferring the formality of the source, but also understanding how formality differs across languages and cultures. As a first step, we therefore limit our study to the scenario where the expected output formality is given to the MT system as an additional input.\nWe first select a formality model providing the most accurate scores on intrinsic formality datasets. We compare existing lexical formality models and novel variants based on inducing formality dimensions or subspaces in vector space models. We then turn to machine translation and show that a lexical formality model can have a positive impact when used to control the formality of machine translation output. When the expected formality matches the reference, we obtain improvement of translation quality evaluated by automatic metrics (BLEU). A human assessment also verified the effectiveness of our proposed system in generating translations at diverse levels of formality."
  }, {
    "heading": "2 Formality-Sensitive MT",
    "text": "Our goal is to provide systems with the ability to generate language across a range of formality style. We propose a Formality-Sensitive Machine Translation (FSMT) scenario where the system takes two inputs: (1) text in the source language to be translated, and (2) a desired formality level capturing the intended audience of the translation. We propose to implement it as n-best re-ranking within a standard phrase-based MT architecture. Unlike domain adaptation approaches, which aim to produce domain-specific or potentially formality-specific systems, our goal is to obtain a single system trained on diverse data which can adaptively produce output for a range of styles.\nWe therefore introduce a formality-scoring fea-\n2814\nture for re-ranking. For each translation hypothesis h, given the formality level ` as a parameter:\nf(h; `) = |Formality(h)− `|\nwhere Formality(h) is the sentence-level formality score for h. f(h; `), along with standard model features, is fed into a standard re-ranking model. When training the re-ranking model, the parameter ` is set to the actual formality score of the reference translation for each instance. At test time, ` is provided by the user. The re-scoring weights help promote candidate sentences whose formality scores approach the expected level."
  }, {
    "heading": "3 Formality Modeling",
    "text": "The FSMT system requires quantifying the formality level of a sentence. Following prior work, we define sentence-level formality based on lexical formality scores (Brooke et al., 2010; Pavlick and Nenkova, 2015). We conduct an empirical comparison of existing techniques that can be adapted as lexical formality models, and introduce a sentence-level formality scheme based on weighted average."
  }, {
    "heading": "3.1 Lexical Formality",
    "text": "State-of-the-art lexical formality models (Brooke et al., 2010; Brooke and Hirst, 2014) are based on vector space models of word meaning, and a set of pre-selected seed words that are representative of formal and informal language.\nSimDiff Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity. Intuitively, w is more likely formal if it is semantically closer to formal seed words than to informal seed words. Formally, given a formal word set Sf and an informal word set Si, SimDiff scores a word w by\nscore(w) = 1 |Sf | ∑ v∈Sf cos(ew, ev)− 1|Si| ∑ v∈Si cos(ew, ev)\nTurning this difference into a formality score requires further manipulation. A neutral word r has to be manually selected to anchor the midpoint of the formality score range. In other words, the final formality score for r is enforced to be zero:\nFormality(w) = score(w)− score(r) normalizer(w, r)\nThe neutral word is typically selected from function words. We select “at” because it appears in nearly every document and appears with nearly equivalent probabilities in formal/informal corpora. Finally, a normalizer which is maximized among the whole vocabulary ensures that scores cover the entire [−1, 1] range.\nInstead of using cosine diff as the score function score(w), other standard techniques can be also applied under this framework.\nSVM As an alternative to the model proposed by Brooke and Hirst (2014), we propose to train an Support Vector Machine (SVM) model to find a hyperplane that separates formal and informal words and define the score function as the distance to the hyperplane.\nFormality Subspace Another category of methods consists in identifying a subspace that captures formality within the original vector space. Lexical scores can then simply be obtained by projecting word representations onto the formality subspace. One example is training a Principal Component Analysis (PCA) model on word representations of all seeds. This method is based on the assumption that representative formal/informal words principally vary along the direction of formality. Alternatively, inspired by DENSIFIER (Rothe et al., 2016), we can learn a subspace that aims at separating words in Sf vs. words in Si and grouping words in the same set."
  }, {
    "heading": "3.2 From Word to Sentence Formality",
    "text": "While previous work scored sentence by averaging word scores (Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), we propose a weighted average scheme for word sequences W to downgrade the formality contribution of neutral words:\nFormality(W ) =∑ wi∈W |Formality(wi)| · Formality(wi)∑\nwi∈W |Formality(wi)|"
  }, {
    "heading": "3.3 Evaluation",
    "text": "Before evaluating our FSMT framework, we evaluate the formality models at the sentence level. Lahiri (2015) and Pavlick and Tetreault (2016) collected 5-way human scores for 11,263 sentences in the genres of blog, email, answers and news. Following Pavlick and Tetreault (2016), we averaged human scores for each sentence as the\ngold standard. As in prior work, the score quality was evaluated by the Spearman correlation.\nA large mixed-topic corpus is required to train vector space models. As suggested by Brooke et al. (2010), we used the ICWSM 2009 Spinn3r dataset (English tier-1) which consists of about 1.6 billion words (Burton et al., 2009). We also compared the term-document association model Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and the term-term association model word2vec (W2V) (Mikolov et al., 2013). We used the same 105 formal seeds and 138 informal seeds as Brooke et al. (2010).\nFollowed Brooke et al. (2010), to achieve best performance, we used a small dimensionality (10) for training LSA and word2vec. In practice, we normalized the LSA word vectors to make them have unit length for SVM and PCA, but did not applied it to word2vec. This suggests that the magnitude of LSA word vectors is harmful for formality modeling.\nWe also compared formality models based on word representations to a baseline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (Pavlick and Nenkova, 2015). This method requires language examples of diverse formality. Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (Cieri et al., 2004), Switchboard (Godfrey et al., 1992), SBCSAE (Bois et al., 2000- 2005), CallHome1, CallFriend2, BOLT SMS/Chat (Song et al., 2014) and NPS Chatroom (Forsythand and Martell, 2007). As the formal counterpart, we extracted comparable size of text from Europarl (Koehn, 2005). This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments).\nTable 1 shows that all models based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance). The baseline method based on unigram models was outperformed by 0.1+ point. So we select DENSIFIER-LSA as a representative for our FSMT system.\n1https://catalog.ldc.upenn.edu/ LDC97S42\n2https://talkbank.org/access/CABank/ CallFriend/"
  }, {
    "heading": "4 Evaluation of the FSMT System",
    "text": "Set-up We evaluate this approach on a French to English translation task. Two parallel FrenchEnglish corpora are used: (1) MultiUN (Eisele and Chen, 2010), which is extracted from the United Nations website, and can be considered to be formal text; (2) OpenSubtitles2016 (Lison and Tiedemann, 2016), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations. Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments). Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles. Moses (Koehn et al., 2007) is used to build our phrase-based MT system. We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic. We used 4-gram language models, trained using KenLM (Heafield, 2011). Model weights were tuned using batch MIRA (Cherry and Foster, 2012).\nWe used constant size n=1000 for n-best lists in all experiments. The re-ranking is a log-linear model trained using batch MIRA. 4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011).\nFSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different\n3http://www.statmt.org/moses/?n=Moses. Baseline\n4https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/ nbest-rescore\nformality for each source sentence. Since such references are not available, we construct three sets of test data where instances are divided according to the formality level of the available reference translation. The formality distribution in the tuning set shows that 97% reference translations fall into the range of [−0.6, 0.6]. We therefore set three formality bins – informal [−1,−0.2), neutral formality [−0.2, 0.2], and formal (0.2, 1] – and split the test set into these bins. We use DENSIFIER-LSA and training setting described above to translate the entire test set three times, with three different formality levels: low (-0.4), neutral (0) and high (0.4)."
  }, {
    "heading": "4.1 Automatic Evaluation",
    "text": "We first report standard automatic evaluation results using the BLEU score to compare FSMT output given different desired formality level on each bins (See Table 2).\nThe best BLEU scores for each formality level are obtained when the level of formality given as input to the MT system matches the nature of the text being translated, as can be seen in the scores along the diagonal in Table 2. Comparing with the baseline system, which produces the top translation from each n-best list, translation quality improves by +0.5 BLEU on informal text, +0.3 BLEU on neutral text, and remains constant on formal text. The impact increases with the distance to formal language increases. This can be explained by the fact that more formal sentences tend to be longer, and the impact of alternate lexical choice for a small number of words per sentence is smaller in longer sentences. In addition, the formal sentences are mostly drawn from UN data which is sufficiently different from the other genres in the heterogeneous training corpus that the informal examples do not affect baseline per-\nformance on formal data."
  }, {
    "heading": "4.2 Human Assessment",
    "text": "Automatic evaluation is limited to comparing output to a single reference: lower BLEU scores conflate translation errors and stylistic mismatch. Therefore, we conduct a human study of the formality vs. the quality.\nWe conducted a manual evaluation of the output of our FSMT system taking low/high formality levels (-0.4/0.4) as parameters. 42 translation pairs were randomly selected and were annotated by 15 volunteers. For each pair of segments, the volunteers were asked to select the segment that would be more appropriate in a formal setting (e.g., a job interview) than in a casual setting (e.g., chatting with friends). A default option of “neither of them is more formal or hard to say” was also available.\nBy majority voting, 20 pairs were annotated as “N”, indicating the two translations has no distinctions w.r.t. formality. For example, “A: how can they do this” vs. “B: how can they do that”. Given that the translations were restricted to the nbest list, not all sentences could be translated into stylistically different language.\nOf the remaining 21 pairs where annotators judged one output more formal than the other, in all but one case the translation produced by our FSMT system with high formality level parameter was judged to be more formal. Overall this indicates that our formality scoring and ranking procedure are effective.\nTo determine whether re-ranking based on formality might have a detrimental effect on quality, we also had annotators rate the fluency and adequacy of the segments. Inspired by Graham et al. (2013), annotators were first asked to assess fluency without a reference and separately adequacy with a reference. Both assessments used a sliding scale. Each segment was evaluated by an average of 7 annotators. After rescaling the ratings into the [0, 1] range, we observed a 0.75 level of fluency for informal translations and 0.70 for formal ones. This slight difference fits our expectation that more casual language may feel more fluent while more formal language may feel more stilted. The adequacy ratings were 0.65 and 0.64 for informal and translations respectively, indicating that adjusting the level of formality had minimal effect on the adequacy of the result.\nSome examples are listed in Table 3. Occa-\nsionally, the n-best list had no translation hypotheses with diverse formality, so the FSMT system dropped necessary words, appended inessential words, or selected improper or even incorrect words to fit the target formality level. In the case of ’how do you do’, the translation that was meant to be more casual was rated more formal. Because the system measures formality on the lexical level, it was not able to recognize this idiomatically formal phrase made up of words that are not inherently formal. Despite these issues, most of the output were formality-variant translations of the same French source segment, as expected."
  }, {
    "heading": "5 Conclusion",
    "text": "We presented a framework for formality-sensitive machine translation, where a system produces translations at a desired formality level. Our evaluation shows the effectiveness of this system in controlling language formality without loss in translation quality."
  }],
  "year": 2017,
  "references": [{
    "title": "Hybrid models for lexical acquisition of correlated styles",
    "authors": ["Julian Brooke", "Graeme Hirst."],
    "venue": "IJCNLP, pages 82–90. Asian Federation of Natural Language Processing / ACL.",
    "year": 2013
  }, {
    "title": "Supervised ranking of co-occurrence profiles for acquisition of continuous lexical attributes",
    "authors": ["Julian Brooke", "Graeme Hirst."],
    "venue": "COLING, pages 2172–2183. ACL.",
    "year": 2014
  }, {
    "title": "Automatic acquisition of lexical formality",
    "authors": ["Julian Brooke", "Tong Wang", "Graeme Hirst."],
    "venue": "COLING (Posters), pages 90–98. Chinese Information Processing Society of China.",
    "year": 2010
  }, {
    "title": "The ICWSM 2009 Spinn3r dataset",
    "authors": ["Kevin Burton", "Akshay Java", "Ian Soboroff."],
    "venue": "Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM 2009), San Jose, CA.",
    "year": 2009
  }, {
    "title": "Batch tuning strategies for statistical machine translation",
    "authors": ["Colin Cherry", "George F. Foster."],
    "venue": "HLT-NAACL, pages 427–436. The Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "The fisher corpus: a resource for the next generations of speech-to-text",
    "authors": ["Christopher Cieri", "David Miller", "Kevin Walker."],
    "venue": "LREC. European Language Resources Association.",
    "year": 2004
  }, {
    "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "ACL (Short Papers), pages 176– 181. The Association for Computer Linguistics.",
    "year": 2011
  }, {
    "title": "Indexing by latent semantic analysis",
    "authors": ["Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman."],
    "venue": "JASIS, 41(6):391–407.",
    "year": 1990
  }, {
    "title": "A simple, fast, and effective reparameterization of IBM model 2",
    "authors": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."],
    "venue": "HLT-NAACL, pages 644– 648. The Association for Computational Linguistics.",
    "year": 2013
  }, {
    "title": "Multiun: A multilingual corpus from united nation documents",
    "authors": ["Andreas Eisele", "Yu Chen."],
    "venue": "LREC. European Language Resources Association.",
    "year": 2010
  }, {
    "title": "Lexical and discourse analysis of online chat dialog",
    "authors": ["Eric N. Forsythand", "Craig H. Martell."],
    "venue": "ICSC, pages 19–26. IEEE Computer Society.",
    "year": 2007
  }, {
    "title": "SWITCHBOARD: Telephone speech corpus for research and development",
    "authors": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel."],
    "venue": "Proceedings of IEEE International Conference on Speech, and Signal Processing, volume 1, pages 517–520.",
    "year": 1992
  }, {
    "title": "Continuous measurement scales in human evaluation of machine translation",
    "authors": ["Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel."],
    "venue": "LAW@ACL, pages 33–41. The Association for Computer Linguistics.",
    "year": 2013
  }, {
    "title": "Language as Social Semiotic: The Social Interpretation of Language and Meaning",
    "authors": ["Michael AK Halliday."],
    "venue": "Edward Arnold, London.",
    "year": 1978
  }, {
    "title": "KenLM: faster and smaller language model queries",
    "authors": ["Kenneth Heafield."],
    "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom.",
    "year": 2011
  }, {
    "title": "Europarl: A Parallel Corpus for Statistical Machine Translation",
    "authors": ["Philipp Koehn."],
    "venue": "Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.",
    "year": 2005
  }, {
    "title": "SQUINKY! A corpus of sentence-level formality, informativeness, and implicature",
    "authors": ["Shibamouli Lahiri."],
    "venue": "CoRR, abs/1506.02306.",
    "year": 2015
  }, {
    "title": "Interrater agreement on sentence formality",
    "authors": ["Shibamouli Lahiri", "Xiaofei Lu."],
    "venue": "CoRR, abs/1109.0069.",
    "year": 2011
  }, {
    "title": "Opensubtitles2016: Extracting large parallel corpora from movie and TV subtitles",
    "authors": ["Pierre Lison", "Jörg Tiedemann."],
    "venue": "LREC. European Language Resources Association (ELRA).",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "CoRR, abs/1301.3781.",
    "year": 2013
  }, {
    "title": "The theory and practice of translation",
    "authors": ["EugeneA. Nida", "R Taber Charles."],
    "venue": "Helps for Translators, volume 8. United Bible Societies.",
    "year": 1969
  }, {
    "title": "Inducing lexical style properties for paraphrase and genre differentiation",
    "authors": ["Ellie Pavlick", "Ani Nenkova."],
    "venue": "HLT-NAACL, pages 218–224. The Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "An empirical analysis of formality in online communication",
    "authors": ["Ellie Pavlick", "Joel R. Tetreault."],
    "venue": "TACL, 4:61–74.",
    "year": 2016
  }, {
    "title": "Ultradense word embeddings by orthogonal transformation",
    "authors": ["Sascha Rothe", "Sebastian Ebert", "Hinrich Schütze."],
    "venue": "HLT-NAACL, pages 767–777. The Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Collecting natural SMS and chat conversations",
    "authors": ["Zhiyi Song", "Stephanie Strassel", "Haejoong Lee", "Kevin Walker", "Jonathan Wright", "Jennifer Garland", "Dana Fore", "Brian Gainor", "Preston Cabe", "Thomas Thomas", "Brendan Callahan", "Ann Sawyer"],
    "year": 2014
  }, {
    "title": "Introducing Language and Society",
    "authors": ["Peter Trudgill."],
    "venue": "Penguin, London.",
    "year": 1992
  }],
  "id": "SP:0d5bc86d613fd7ae3bc0aa073c88ade3f79b9aae",
  "authors": [{
    "name": "Xing Niu",
    "affiliations": []
  }, {
    "name": "Marianna Martindale",
    "affiliations": []
  }],
  "abstractText": "Stylistic variations of language, such as formality, carry speakers’ intention beyond literal meaning and should be conveyed adequately in translation. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.",
  "title": "A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output"
}