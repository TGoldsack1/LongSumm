{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2860–2865 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "NLP research relies heavily on annotated datasets for training and evaluation. The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.\nWhen collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (Hovy et al., 2006), methods for aggregating annotations (Passonneau and Carpenter, 2014), and intuitive user interfaces (Stenetorp et al., 2012).\nCrowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al.,\n2008; Callison-Burch and Dredze, 2010; Graham et al., 2017), although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (CallisonBurch and Dredze, 2010; Hoßfeld et al., 2014). In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (Koehn and Monz, 2006; Bojar et al., 2016).\nWhen crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a “HIT” (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes. The order of items is generally fixed across all annotators of an individual HIT (Snow et al., 2008; Graham et al., 2017).\nIn this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals’ assessment of an item. Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).\nPossible explanations for sequence effects include:\nGambler’s fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution. In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True). So if they assign 0 to an item,\n2860\nthey may approach the next item with a prior belief that it is more likely to be a 1 than a 0. Chen et al. (2016) showed evidence for the gambler’s fallacy in decisions of loan officers, asylum judges, and baseball umpires.\nSequential contrast effects: A high quality item may raise the bar for the next item. On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear)\nAssimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item. If they focus on similarities between the previous and current instance, the annotations show an assimilation effect (Geiselman et al., 1984; Damisch et al., 2006). Anchoring effects may decrease as people gain experience and expertise in the task (Wilson et al., 1996)."
  }, {
    "heading": "2 Methodology",
    "text": "We test whether the annotation of an instance is correlated with the annotation on previous instances, conditioned on control variables such as the gold standard (i.e. expert annotations1), based on the following linear model:\nYi,t = β0 + β1Yi,t−1 + β2Gold + η (1)\nwhere Yi,t is the annotation given by an annotator i to an instance t, and η is white Gaussian noise with zero mean. We use linear regression for continuous data and logistic regression for binary data.2 If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive β2 value) and bias term (β0), with β1 set to zero. When we use the ground truth as a control, if β1 is non-zero, it is evidence of mistakes being made by annotators due to sequential bias. A positive value of β1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler’s fallacy. Accordingly, we test the statistical significance of\n1For the Machine Translation dataset described in Section 3.3, we use the mean of at least fifteen crowd workers as a proxy for expert annotations.\n2η is not included in the case of logistic regression\nthe β1 6= 0 to determine whether sequencing effects are present in crowdsourced text corpora."
  }, {
    "heading": "3 Experiments",
    "text": "We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation."
  }, {
    "heading": "3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering",
    "text": "First, we examine the recognising textual entailment (“RTE”) and event temporal ordering (“TEMPORAL”) datasets from Snow et al. (2008). In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first. With the TEMPORAL dataset, they are shown two sentences describing events, and asked to indicate which of the two events occurred first. Both datasets include both expert annotations and crowdsourced annotations constructed using Amazon Mechanical Turk (“MTurk”). On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order. For both tasks, each HIT was annotated by 10 workers.\nResults We use logistic regression on worker labels against labels on the previous instance in the current HIT, with the expert judgements as a control variable. We also add an additional control, namely the percentage of True labels assigned by the worker overall, which accounts for the overall annotator bias. To calculate this, we use scores by the worker excluding the current score, to avoid giving the model any information about the current instance.\nAs shown in Table 1, over all workers (“All”), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks. One possibility\nis that this is biased by opportunistic workers who assign the same label to all instances in the HIT, for which we would not expect any sequential bias effects. When we exclude these workers (“Moderate”), the autocorrelation increases, and is highly statistically significant. We also show results for workers with at least 60% accuracy when compared to expert annotations (“Good”), and observe a similar effect."
  }, {
    "heading": "3.2 Affective text analysis",
    "text": "In the affective text analysis task (“AFFECTIVE”), annotators are asked to rate news headlines for anger, disgust, fear, joy, sadness, and surprise on a continuous scale of 0–100. Besides these emotions, they are asked to rate sentences for (emotive) valence, i.e., how strongly negative or positive they are (−100 to +100). In this dataset, there are 100 headlines divided into 10 HITs, with 10 workers annotating each HIT (Snow et al., 2008). We test for autocorrelation of scores of each aspect individually, controlling for the expert scores and worker correlation with the expert scores. We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5.\nResults For individual emotions, we do not observe any significant autocorrelation (p ≥ 0.05). As there are only 1000 annotations per emotion, we also look at results when combining data for all aspects. Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2). Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects."
  }, {
    "heading": "3.3 Machine Translation Adequacy",
    "text": "When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output. In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100- point sliding scale which is initialised at the mid point. There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration. They are given no other instructions or\nguidelines. In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire data from WMT 2013 (Bojar et al., 2013). The dataset consists of 12 HITS of 100 sentence pairs each; each HIT is annotated by at least 15 workers.\nHITs are designed to include quality control items to filter out poor quality scores. In addition to 70 MT system translations, each HIT contains degraded versions of 10 of these translations, 10 reference translations by a human expert corresponding to 10 of these translations, and repeats of another 10 translations. Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations. Workers who submitted scores of clearly bad quality were rejected. For the remaining workers, the Wilcoxon rank-sum test is used to test whether the score difference between the repeat judgements is less than the score difference between translations and the corresponding degraded versions. We divide these workers into “good” and “moderate” based on the threshold of p < 0.05.\nTo eliminate differences due to different internal scales, every individual worker’s scores are standardised by subtracting the mean and dividing by the standard deviation of their scores. Following Graham et al. (2015), we use the average of standardised scores of at least 15 good workers as the ground truth.\nWe refer to the final dataset as “MTadeq”.\nResults As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control. We also controlled for worker correlation with mean score, and position of the sentence in the HIT, but these were not significant and did not affect the autocorrelation. As seen in Table 3, we see a small but significant positive autocorrelation for good workers. The bias is much stronger with\nbad (rejected) workers. An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task. Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT. As shown in Table 4, for good and moderate workers, the bias is stronger in the first group of sentences annotated, decreases in the second, and is much smaller in the last. This could be because workers are familiarising themselves with the task earlier on, and calibrating their scale. There is no such trend with bad quality scores, possibly because the workers are not putting in sufficient effort to produce accurate scores.\nNext we assess the impact of the bias in the worst case situation. We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence. As shown in Table 5 we can see that the sentences in the “low” partition and the “high” partition have a difference of 0.18, which is highly significant;3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation. The bias remains even when we increase the number of workers and use the average score, as all workers scored the translations in the same order. This shows that the mean is also affected by\n3p < 0.001 using Welch’s two-sample t-test\nsequence bias. Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system’s computed score by ordering a HIT such that the system’s output is seen consistently immediately after a bad (or good) output."
  }, {
    "heading": "4 Discussion and Conclusions",
    "text": "We have shown significant sequence effects across several independent crowdsourced datasets: a negative autocorrelation in the RTE and TEMPORAL datasets, and a positive autocorrelation in the MTadeq dataset. The negative autocorrelation can be attributed either to sequential contrast effects or the gambler’s fallacy. These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects. It is also possible that the dataset is too small to obtain statistical significance.\nMT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale. We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect. Since the ordering of the systems is random, system scores obtained by averaging scores of all sentences translated by the system would be unbiased, assuming a sufficiently large sample of sentences. Thus we do not expect sequential bias to have a marked effect on system rankings or other macro-level conclusions on the basis of this data. However, the scores of in-\ndividual translations remain biased, which augurs poorly for the use of these annotations at the sentence level, such as when used in error analysis or for training automatic metrics.\nSequence problems can be easily addressed by adequate randomisation — providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data. In this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased.\nThis study has shown that sequence bias is real, and can distort evaluation and annotation exercises with crowd-workers. We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006). Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their valuable feedback. This work was supported in part by the Australian Research Council."
  }],
  "year": 2017,
  "references": [{
    "title": "Findings of the 2013 Workshop on Statistical Machine Translation",
    "authors": ["Ondřej Bojar", "Christian Buck", "Chris Callison-Burch", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."],
    "venue": "Pro-",
    "year": 2013
  }, {
    "title": "Findings of the 2016 Conference on Machine Translation",
    "authors": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."],
    "venue": "Proceedings of the First Conference on Machine Translation, pages 131–",
    "year": 2016
  }, {
    "title": "Creating speech and language data with Amazon’s Mechanical Turk",
    "authors": ["Chris Callison-Burch", "Mark Dredze."],
    "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language",
    "year": 2010
  }, {
    "title": "Decision-making under the gambler’s fallacy: Evidence from asylum judges, loan officers, and baseball umpires",
    "authors": ["Daniel Chen", "Tobias J. Moskowitz", "Kelly Shue."],
    "venue": "The Quarterly Journal of Economics.",
    "year": 2016
  }, {
    "title": "Olympic medals as fruits of comparison? Assimilation and contrast in sequential performance judgments",
    "authors": ["Lysann Damisch", "Thomas Mussweiler", "Henning Plessner."],
    "venue": "Journal of Experimental Psychology: Applied, 12(3):166.",
    "year": 2006
  }, {
    "title": "Context effects on the perceived physical attractiveness of faces",
    "authors": ["R Edward Geiselman", "Nancy A Haight", "Lori G Kimata."],
    "venue": "Journal of Experimental Social Psychology, 20(5):409–424.",
    "year": 1984
  }, {
    "title": "Can machine translation systems be evaluated by the crowd alone",
    "authors": ["Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel."],
    "venue": "Natural Language Engineering, 23(1):330.",
    "year": 2017
  }, {
    "title": "Accurate evaluation of segment-level machine translation metrics",
    "authors": ["Yvette Graham", "Nitika Mathur", "Timothy Baldwin."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Best practices and recommendations for crowdsourced QoE — lessons learned",
    "authors": ["Tobias Hoßfeld", "Matthias Hirth", "Judith Redi", "Filippo Mazza", "Pavel Korshunov", "Babak Naderi", "Michael Seufert", "Bruno Gardlo", "Sebastian Egger", "Christian Keimel"],
    "year": 2014
  }, {
    "title": "Ontonotes: The 90% solution",
    "authors": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60,",
    "year": 2006
  }, {
    "title": "Contrast effects and judgments of physical attractiveness: When beauty becomes a social problem",
    "authors": ["Douglas T. Kenrick", "Sara E. Gutierres."],
    "venue": "Journal of Personality and Social Psychology, 38(1):131.",
    "year": 1980
  }, {
    "title": "Manual and automatic evaluation of machine translation between European languages",
    "authors": ["Philipp Koehn", "Christof Monz."],
    "venue": "Proceedings of the Workshop on Statistical Machine Translation, pages 102– 121, New York, USA.",
    "year": 2006
  }, {
    "title": "The benefits of a model of annotation",
    "authors": ["J. Rebecca Passonneau", "Bob Carpenter."],
    "venue": "Transactions of the Association of Computational Linguistics, 2(1):311–326.",
    "year": 2014
  }, {
    "title": "Computational modelling of structural priming in dialogue",
    "authors": ["David Reitter", "Frank Keller", "Johanna D Moore."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 121–124, New",
    "year": 2006
  }, {
    "title": "Cheap and fast — but is it good?: Evaluating non-expert annotations for natural language tasks",
    "authors": ["Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Y. Ng"],
    "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language",
    "year": 2008
  }, {
    "title": "BRAT: A web-based tool for NLP-assisted text annotation",
    "authors": ["Pontus Stenetorp", "Sampo Pyysalo", "Goran Topić", "Tomoko Ohta", "Sophia Ananiadou", "Jun’ichi Tsujii"],
    "venue": "In Proceedings of the Demonstrations at the 13th Conference of the European Chap-",
    "year": 2012
  }, {
    "title": "A new look at anchoring effects: basic anchoring and its antecedents",
    "authors": ["Timothy D. Wilson", "Christopher E. Houston", "Kathryn M. Etling", "Nancy Brekke."],
    "venue": "Journal of Experimental Psychology: General, 125(4):387.",
    "year": 1996
  }],
  "id": "SP:40d1f8f2982b25ffad2b2e672f8e6d3a6528bb1c",
  "authors": [{
    "name": "Nitika Mathur",
    "affiliations": []
  }, {
    "name": "Timothy Baldwin",
    "affiliations": []
  }, {
    "name": "Trevor Cohn",
    "affiliations": []
  }],
  "abstractText": "Manual data annotation is a vital component of NLP research. When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where annotations of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.",
  "title": "Sequence Effects in Crowdsourced Annotations"
}