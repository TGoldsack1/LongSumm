{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider the problem of regularized empirical risk minimization (ERM) of linear predictors. Let a1, . . . , an ∈ Rd be the feature vectors of n data samples, φi : R → R be a convex loss function associated with the linear prediction aTi x, for i = 1, . . . , n, and g : Rd → R be a convex regularization function for the predictor x ∈ Rd. ERM amounts to solving the following convex optimization problem:\nmin x∈Rd\n{ P (x)\ndef = 1n ∑n i=1 φi(a T i x) + g(x) } . (1)\nThis formulation covers many well-known classification and regression problems. For example, logistic regression is obtained by setting φi(z) = log(1 + exp(−biz)) where bi ∈ {±1}. For linear regression problems, the loss\n1Department of Computer Science, The University of Chicago, Chicago, Illinois 60637, USA. 2Microsoft Research, Redmond, Washington 98052, USA. Correspondence to: Jialei Wang <jialei@uchicago.edu>, Lin Xiao <lin.xiao@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nfunction is φi(z) = (1/2)(z − bi)2, and we get ridge regression with g(x) = (λ/2)‖x‖22 and the elastic net with g(x) = λ1‖x‖1 + (λ2/2)‖x‖22. LetA = [a1, . . . , an]T be the n by d data matrix. Throughout this paper, we make the following assumptions: Assumption 1. The functions φi, g and matrix A satisfy:\n• Each φi is δ-strongly convex and 1/γ-smooth where γ > 0 and δ ≥ 0, and γδ ≤ 1;\n• g is λ-strongly convex where λ ≥ 0; • λ+ δµ2 > 0, where µ = √ λmin(ATA).\nThe strong convexity and smoothness mentioned above are with respect to the standard Euclidean norm, denoted as ‖x‖ = √ xTx. (See, e.g., Nesterov (2004, Sections 2.1.1 and 2.1.3) for the exact definitions.) We allow δ = 0, which simply means φi is convex. Let R = maxi{‖ai‖} and assuming λ > 0, then R2/(γλ) is a popular definition of condition number for analyzing complexities of different algorithms. The last condition above means that the primal objective function P (x) is strongly convex, even if λ = 0.\nThere have been extensive research activities in recent years on developing efficiently algorithms for solving problem (1). A broad class of randomized algorithms that exploit the finite sum structure in the ERM problem have emerged as very competitive both in terms of theoretical complexity and practical performance. They can be put into three categories: primal, dual, and primal-dual.\nPrimal randomized algorithms work with the ERM problem (1) directly. They are modern versions of randomized incremental gradient methods (e.g., Bertsekas, 2012; Nedic & Bertsekas, 2001) equipped with variance reduction techniques. Each iteration of such algorithms only process one data point ai with complexity O(d). They includes SAG (Roux et al., 2012), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), which all achieve the iteration complexity O ( (n+R2/(γλ)) log(1/ǫ) ) to find an ǫoptimal solution. In fact, they are capable of exploiting the strong convexity from data, meaning that the condition number R2/(γλ) in the complexity can be replaced by the more favorable oneR2/(γ(λ+δµ2/n)). This improvement can be achieved without explicit knowledge of µ from data.\nDual algorithms solve Fenchel dual of (1) by maximizing\nD(y) def = 1n ∑n i=1 −φ∗i (yi)− g∗ ( − 1n ∑n i=1 yiai ) (2) using randomized coordinate ascent algorithms. (Here φ∗i and g∗ denotes the conjugate functions of φi and g.) They include SDCA (Shalev-Shwartz & Zhang, 2013), Nesterov (2012) and Richtárik & Takáč (2014). They have the same complexity O ( (n+R2/(γλ)) log(1/ǫ) ) , but cannot exploit strong convexity, if any (when δµ2 > 0), from data.\nPrimal-dual algorithms solve the convex-concave saddle point problem minxmaxy L(x, y) where\nL(x, y) def= 1n ∑n i=1 ( yi〈ai, x〉 − φ∗i (yi) ) + g(x). (3)\nIn particular, SPDC (Zhang & Xiao, 2015) achieves an accelerated linear convergence rate with iteration complexity O ( (n+ √ nR/ √ γλ) log(1/ǫ) ) , which is better than the aforementioned non-accelerated complexity when R2/(γλ) > n. Lan & Zhou (2015) developed dual-free variants of accelerated primal-dual algorithms, but without considering the linear predictor structure in ERM. Balamurugan & Bach (2016) extended SVRG and SAGA to solving saddle point problems.\nAccelerated primal and dual randomized algorithms have also been developed. Nesterov (2012), Fercoq & Richtárik (2015) and Lin et al. (2015b) developed accelerated coordinate gradient algorithms, which can be applied to solve the dual problem (2). Allen-Zhu (2016) developed an accelerated variant of SVRG. Acceleration can also be obtained using the Catalyst framework (Lin et al., 2015a). They all achieve the same O ( (n+ √ nR/ √ γλ) log(1/ǫ) ) complexity. A common feature of accelerated algorithms is that they require good estimate of the strong convexity parameter. This makes hard for them to exploit strong convexity from data because the minimum singular value µ of the data matrix A is very hard to estimate in general.\nIn this paper, we show that primal-dual algorithms are capable of exploiting strong convexity from data if the algorithm parameters (such as step sizes) are set appropriately. While these optimal setting depends on the knowledge of the convexity parameter µ from the data, we develop adaptive variants of primal-dual algorithms that can tune the parameter automatically. Such adaptive schemes rely critically on the capability of evaluating the primal-dual optimality gaps by primal-dual algorithms.\nA major disadvantage of primal-dual algorithms is that the required dual proximal mapping may not admit closedform or efficient solution. We follow the approach of Lan & Zhou (2015) to derive dual-free variants of the primal-dual algorithms customized for ERM problems with the linear predictor structure, and show that they can also exploit strong convexity from data with correct choices of parameters or using an adaptation scheme.\nAlgorithm 1 Batch Primal-Dual (BPD) Algorithm input: parameters τ , σ, θ, initial point (x̃(0) = x(0), y(0))\nfor t = 0, 1, 2, . . . do y(t+1) = proxσf∗ ( y(t) + σAx̃(t) )\nx(t+1) = proxτg ( x(t) − τAT y(t+1) )\nx̃(t+1) = x(t+1) + θ(x(t+1) − x(t)) end for"
  }, {
    "heading": "2. Batch primal-dual algorithms",
    "text": "We first study batch primal-dual algorithms, by considering a “batch” version of the ERM problem (1),\nminx∈Rd { P (x) def = f(Ax) + g(x) } . (4)\nwhere A ∈ Rn×d. We make the following assumptions: Assumption 2. The functions f , g and matrix A satisfy:\n• f is δ-strongly convex and 1/γ-smooth where γ > 0 and δ ≥ 0, and γδ ≤ 1;\n• g is λ-strongly convex where λ ≥ 0; • λ+ δµ2 > 0, where µ = √ λmin(ATA).\nUsing conjugate functions, we can derive the dual of (4) as\nmaxy∈Rn { D(y) def = −f∗(y)− g∗(−AT y) } , (5)\nand the convex-concave saddle point formulation is\nmin x∈Rd max y∈Rn\n{ L(x, y) def= g(x) + yTAx− f∗(y) } . (6)\nWe consider the primal-dual first-order algorithm proposed by Chambolle & Pock (2011; 2016) for solving the saddle point problem (6), given in Algorithm 1, where proxψ(·), for any convex function ψ : Rn ∪ {∞}, is defined as\nproxψ(β) = arg min α∈Rn\n( ψ(α) + (1/2)‖α− β‖2 ) .\nAssuming that f is smooth and g is strongly convex, Chambolle & Pock (2011; 2016) showed that Algorithm 1 achieves accelerated linear convergence rate if λ > 0. However, they did not consider the case where additional or the sole source of strong convexity comes from f(Ax). In the following theorem, we show how to set the parameters τ , σ and θ to exploit both sources of strong convexity to achieve fast linear convergence.\nTheorem 1. Suppose Assumption 2 holds and (x⋆, y⋆) is the unique saddle point of L defined in (6). Let L = ‖A‖ =√ λmax(ATA). If we set the parameters in Algorithm 1 as\nσ = 1L\n√ λ+δµ2\nγ , τ = 1 L\n√ γ\nλ+δµ2 , (7)\nand θ = max{θx, θy} where\nθx = ( 1− δ(δ+2σ) µ2 L2 ) 1 1+τλ , θy = 1 1+σγ/2 , (8)\nthen we have (\n1 2τ + λ 2 ) ‖x(t) − x⋆‖2 + γ4 ‖y(t) − y⋆‖2 ≤ θtC,\nL(x(t), y⋆)− L(x⋆, y(t)) ≤ θtC,\nwhereC = (\n1 2τ + λ 2\n) ‖x(0)−x⋆‖2+ ( 1 2σ+ γ 4 ) ‖y(0)−y⋆‖2.\nThe proof of Theorem 1 is given in Appendices B and C. Here we give a detailed analysis of the convergence rate. Substituting σ and τ in (7) into the expressions for θy and θx in (8), and assuming γ(λ+ δµ2) ≪ L2, we have\nθx ≈ 1− γδµ 2\nL2\n( 2 √ γ(λ+δµ2) L + γδ )−1 − λL √ γ λ+δµ2 ,\nθy = 1 1+ √ γ(λ+δµ2)/(2L)\n≈ 1− √ γ(λ+δµ2)\n2L .\nSince the overall condition number of the problem is L2 γ(λ+δµ2) , it is clear that θy is an accelerated convergence rate. Next we examine θx in two special cases.\nThe case of δµ2 = 0 but λ > 0. In this case, we have τ = 1L √ γ λ and σ = 1 L √ λ γ , and thus\nθx= 1 1+ √ γλ/L ≈ 1− √ γλ L , θy= 1 1+ √ γλ/(2L) ≈ 1− √ γλ 2L . Therefore we have θ = max{θx, θy} ≈ 1 − √ λγ 2L . This indeed is an accelerated convergence rate, recovering the result of Chambolle & Pock (2011; 2016).\nThe case of λ = 0 but δµ2 > 0. In this case, we have τ = 1Lµ √ γ δ and σ = µ L √ δ γ , and\nθx = 1− γδµ 2 L2 · 12√γδµ/L+γδ , θy ≈ 1− √ γδµ 2L .\nNotice that 1γδ L2\nµ2 is the condition number of f(Ax). Next we assume µ≪ L and examine how θx varies with γδ.\n• If γδ ≈ µ2L2 , meaning f is badly conditioned, then\nθx ≈ 1− γδµ 2 L2 · 13√γδµ/L = 1− √ γδµ 3L .\nBecause the overall condition number is 1γδ L2\nµ2 , this is an accelerated linear rate, and so is θ = max{θx, θy}.\n• If γδ ≈ µL , meaning f is mildly conditioned, then\nθx ≈ 1− µ 3 L3 1 2(µ/L)3/2+µ/L ≈ 1− µ2L2 .\nThis represents a half-accelerated rate, because the overall condition number is 1γδ L2 µ2 ≈ L 3 µ3 .\n• If γδ = 1, i.e., f is a simple quadratic function, then\nθx ≈ 1− µ 2 L2 1 2µ/L+1 ≈ 1− µ2 L2 .\nThis rate does not have acceleration, because the overall condition number is 1γδ L2 µ2 ≈ L 2 µ2 .\nAlgorithm 2 Adaptive Batch Primal-Dual (Ada-BPD) input: problem constants λ, γ, δ, L and µ̂ > 0, initial\npoint (x(0), y(0)), and adaptation period T . Compute σ, τ , and θ as in (7) and (8) using µ = µ̂ for t = 0, 1, 2, . . . do y(t+1) = proxσf∗ ( y(t) + σAx̃(t) )\nx(t+1) = proxτg ( x(t) − τAT y(t+1) ) x̃(t+1) = x(t+1) + θ(x(t+1) − x(t)) if mod(t+ 1, T ) == 0 then\n(σ, τ, θ) = BPD-Adapt ( {P (s), D(s)}t+1s=t−T )\nend if end for\nAlgorithm 3 BPD-Adapt (simple heuristic) input: previous estimate µ̂, adaption period T , primal and\ndual objective values {P (s), D(s)}ts=t−T if P (t) −D(t) < θT (P (t−T ) −D(t−T )) then µ̂ := √ 2µ̂ else µ̂ := µ̂/ √ 2 end if Compute σ, τ , and θ as in (7) and (8) using µ = µ̂\noutput: new parameters (σ, τ, θ)\nIn summary, the extent of acceleration in the dominating factor θx (which determines θ) depends on the relative size of γδ and µ2/L2, i.e., the relative conditioning between the function f and the matrix A. In general, we have full acceleration if γδ ≤ µ2/L2. The theory predicts that the acceleration degrades as the function f gets better conditioned. However, in our numerical experiments, we often observe acceleration even if γδ gets closer to 1.\nAs explained in Chambolle & Pock (2011), Algorithm 1 is equivalent to a preconditioned ADMM. Deng & Yin (2016) characterized various conditions for ADMM to obtain linear convergence, but did not derive the convergence rate for the case we consider in this paper."
  }, {
    "heading": "2.1. Adaptive batch primal-dual algorithms",
    "text": "In practice, it is often very hard to obtain a good estimate of the problem-dependent constants, especially µ =√ λmin(ATA), in order to apply the algorithmic parameters specified in Theorem 1. Here we explore heuristics that can enable adaptive tuning of such parameters, which often lead to much improved performance in practice.\nA key observation is that the convergence rate of the BPD algorithm changes monotonically with the overall convexity parameter λ + δµ2, regardless of the extent of acceleration. In other words, the larger λ + δµ2 is, the faster the convergence. Therefore, if we can monitor the progress of\nAlgorithm 4 BPD-Adapt (robust heuristic) input: previous rate estimate ρ > 0, ∆ = δµ̂2, period T ,\nconstants c < 1 and c > 1, and {P (s), D(s)}ts=t−T Compute new rate estimate ρ̂ = P\n(t)−D(t) P (t−T )−D(t−T )\nif ρ̂ ≤ c ρ then ∆ := 2∆, ρ := ρ̂ else if ρ̂ ≥ c ρ then ∆ := ∆/2, ρ := ρ̂ else ∆ := ∆ end if σ = 1L √ λ+∆ γ , τ = 1 L √ γ λ+∆\nCompute θ using (8) or set θ = 1 output: new parameters (σ, τ, θ)\nthe convergence and compare it with the predicted convergence rate in Theorem 1, then we can adjust the estimated parameters to exploit strong convexity from data. More specifically, if the observed convergence rate is slower than the predicted rate, then we should reduce the estimate of µ; otherwise we should increase µ for faster convergence.\nWe formalize the above reasoning in Algorithm 2 (called Ada-BPD). This algorithm maintains an estimate µ̂ of the true constant µ, and adjust it every T iterations. We use P (t) and D(t) to represent the primal and dual objective values at P (x(t)) and D(y(t)), respectively. We give two implementations of the tuning procedure BPD-Adapt: Algorithm 3 is a simple heuristic for tuning the estimate µ̂, where the increasing and decreasing factor √ 2 can be changed to other values larger than 1. Algorithm 4 is a more robust heuristic. It does not rely on the specific convergence rate θ established in Theorem 1. Instead, it simply compares the current estimate of objective reduction rate ρ̂ with the previous estimate ρ. It also specifies a non-tuning range of changes in ρ, specified by the interval [c, c].\nThe capability of accessing both the primal and dual objective values allows primal-dual algorithms to have good estimate of the convergence rate, which enables effective tuning heuristics. Automatic tuning of primal-dual algorithms have also been studied by, e.g., Malitsky & Pock (2016) and Goldstein et al. (2013), but with different goals."
  }, {
    "heading": "3. Randomized primal-dual algorithm",
    "text": "In this section, we come back to the ERM problem and consider its saddle-point formulation in (3). Due to its finite sum structure in the dual variables yi, we can develope randomized algorithms to exploit strong convexity from data. In particular, we extend the stochastic primal-dual coordinate (SPDC) algorithm by Zhang & Xiao (2015). SPDC is\nAlgorithm 5 Adaptive SPDC (Ada-SPDC) input: parameters σ, τ , θ > 0, initial point (x(0), y(0)),\nand adaptation period T . Set x̃(0) = x(0) for t = 0, 1, 2, . . . do pick k ∈ {1, . . . , n} uniformly at random for i ∈ {1, . . . , n} do\nif i == k then y (t+1) k = proxσφ∗k ( y (t) k + σa T k x̃ (t) ) else y (t+1) i = y (t) i\nend if end for\nx(t+1) = proxτg\n( x(t)− τ ( u(t)+ (y\n(t+1) k −y (t) k )ak\n))\nu(t+1) = u(t) + 1n (y (t+1) k − y (t) k )ak x̃(t+1) = x(t+1) + θ(x(t+1) − x(t)) if mod(t+ 1, T · n) = 0 then\n(τ, σ, θ) = SPDC-Adapt ( {P (t−sn), D(t−sn)}Ts=0 )\nend if end for\na special case of the Ada-SPDC algorithm in Algorithm 5, by setting the adaption period T = ∞ (no adaption). The following theorem is proved in Appendix E. Theorem 2. Suppose Assumption 1 holds. Let (x⋆, y⋆) be the saddle point of the function L defined in (3), and R = max{‖a1‖, . . . , ‖an‖}. If we set T = ∞ in Algorithm 5 (no adaption) and let\nτ = 14R\n√ γ\nnλ+δµ2 , σ = 1 4R\n√ nλ+δµ2\nγ , (9)\nand θ = max{θx, θy} where\nθx = ( 1− τσδµ22n(σ+4δ) ) 1 1+τλ , θy = 1+((n−1)/n)σγ/2 1+σγ/2 , (10)\nthen we have (\n1 2τ + λ 2\n) E [ ‖x(t) − x⋆‖2 ] + γ4E [ ‖y(t) − y⋆‖2 ] ≤ θtC,\nE [ L(x(t), y⋆)− L(x⋆, y(t)) ] ≤ θtC,\nwhereC = (\n1 2τ + λ 2\n) ‖x(0)−x⋆‖2+ ( 1 2σ+ γ 4 ) ‖y(0)−y⋆‖2.\nThe expectation E[·] is taken with respect to the history of random indices drawn at each iteration.\nBelow we give a detailed discussion on the expected convergence rate established in Theorem 2.\nThe cases of σµ2 = 0 but λ > 0. In this case we have τ = 14R √ γ nλ and σ = 1 4R √ nλ γ , and\nθx = 1 1+τλ = 1− 11+4R√n/(λγ) ,\nθy = 1+((n−1)/n)σγ/2 1+σγ/2 = 1− 1n+8R√n/(λγ) .\nHence θ = θy . These recover the parameters and convergence rate of the standard SPDC (Zhang & Xiao, 2015).\nThe cases of σµ2 > 0 but λ = 0. In this case we have τ = 14Rµ √ γ δ and σ = µ 4R √ δ γ , and\nθx = 1− τσδµ 2 2n(σ+4δ) = 1− γδµ2 32nR2 · 1√γδµ/(4R)+4γδ . θy = 1− 1n+8nR/(µ√γδ) ≈ 1− √ γδµ 8nR ( 1 + √ γδµ 8R )−1 .\nSince the objective is R2/γ-smooth and δµ2/n-strongly convex, θy is an accelerated rate if √ γδµ 8R ≪ 1 (otherwise θy ≈ 1− 1n ). For θx, we consider different situations:\n• If µ ≥ R, then we have θx ≈ 1− √ γδµ nR , which is an\naccelerated rate. So is θ = max{θx, θy}. • If µ < R and γδ ≈ µ2R2 , then θx ≈ 1− √ γδµ nR , which\nrepresents an accelerated rate. The iteration complexity of SPDC is Õ( nR\nµ √ γδ ), which is better than that of\nSVRG in this case, which is Õ( nR 2\nγδµ2 ).\n• If µ < R and γδ ≈ µR , then we get θx ≈ 1− µ2\nnR2 . This is a half-accelerated rate, because in this case SVRG requires Õ(nR 3\nµ3 ) iterations, versus Õ( nR2 µ2 ) for SPDC.\n• If µ < R and γδ ≈ 1, meaning the φi’s are well conditioned, then we get θx ≈ 1 − γδµ 2 nR2 ≈ 1 − µ2\nnR2 , which is a non-accelerated rate. The corresponding iteration complexity is the same as SVRG."
  }, {
    "heading": "3.1. Parameter adaptation for SPDC",
    "text": "The SPDC-Adapt procedure called in Algorithm 5 follows the same logics as the batch adaption schemes in Algorithms 3 and 4, and we omit the details here. One thing we emphasize here is that the adaptation period T is in terms of epochs, or number of passes over the data. In addition, we only compute the primal and dual objective values after each pass or every few passes, because computing them exactly usually need to take a full pass of the data.\nUnlike the batch case where the duality gap decreases monotonically, the duality gap for randomized algorithms can fluctuate wildly. So instead of using only the two end values P (t−Tn) − D(t−Tn) and P (t) − D(t), we can use more points to estimate the convergence rate through a linear regression. Suppose the primaldual objective values for the last T + 1 passes are (P (0), D(0)), (P (1), D(1)), . . . , (P (T ), D(T )), and we need to estimate ρ (rate per pass) such that\nP (t)−D(t) ≈ ρt ( P (0)−D(0) ) , t = 1, . . . , T.\nWe can turn it into a linear regression problem after taking logarithm and obtain the estimate ρ̂ through\nlog(ρ̂) = 112+22+···+T 2 ∑T t=1 t log P (t)−D(t) P (0)−D(0) .\nAlgorithm 6 Dual-Free BPD Algorithm input: parameters σ, τ , θ > 0, initial point (x(0), y(0))\nSet x̃(0) = x(0) and v(0) = (f∗)′(y(0)) for t = 0, 1, 2, . . . do v(t+1) = v (t)+σAx̃(t)\n1+σ , y (t+1) = f ′(v(t+1))\nx(t+1) = proxτg ( x(t) − τAT y(t+1) )\nx̃(t+1) = x(t+1) + θ(x(t+1) − x(t)) end for"
  }, {
    "heading": "4. Dual-free Primal-dual algorithms",
    "text": "Compared with primal algorithms, one major disadvantage of primal-dual algorithms is the requirement of computing the proximal mapping of the dual function f∗ or φ∗i , which may not admit closed-formed solution or efficient computation. This is especially the case for logistic regression, one of the most popular loss functions used in classification.\nLan & Zhou (2015) developed “dual-free” variants of primal-dual algorithms that avoid computing the dual proximal mapping. Their main technique is to replace the Euclidean distance in the dual proximal mapping with a Bregman divergence defined over the dual loss function itself. We show how to apply this approach to solve the structured ERM problems considered in this paper. They can also exploit strong convexity from data if the algorithmic parameters are set appropriately or adapted automatically."
  }, {
    "heading": "4.1. Dual-free BPD algorithm",
    "text": "First, we consider the batch setting. We replace the dual proximal mapping (computing y(t+1)) in Algorithm 1 with\ny(t+1)=argmin y\n{ f∗(y)−yTAx̃(t)+ 1σD(y, y(t)) } , (11)\nwhere D is the Bregman divergence of a strictly convex kernel function h, defined as\nDh(y, y(t)) = h(y)− h(y(t))− 〈∇h(y(t)), y − y(t)〉. Algorithm 1 is obtained in the Euclidean setting with h(y) = 12‖y‖2 and D(y, y(t)) = 12‖y−y(t)‖2. Here we use f∗ as the kernel function, and show that it allows us to compute y(t+1) in (11) very efficiently. The following lemma explains the details (Cf. Lan & Zhou, 2015, Lemma 1). Lemma 1. Let the kernel h ≡ f∗ in the Bregman divergence D. If we construct a sequence of vectors {v(t)} such that v(0) = (f∗)′(y(0)) and for all t ≥ 0,\nv(t+1) = v (t)+σAx̃(t)\n1+σ , (12)\nthen the solution to problem (11) is y(t+1) = f ′(v(t+1)).\nProof. Suppose v(t) = (f∗)′(y(t)) (true for t = 0), then\nD(y, y(t)) = f∗(y)− f∗(y(t))− v(t)T (y − y(t)).\nThe solution to (11) can be written as\ny(t+1)= argmin y\n{ f∗(y)−yTAx̃(t)+ 1σ ( f∗(y)−v(t)T y )}\n= argmin y\n{( 1 + 1σ ) f∗(y)− ( Ax̃(t) + 1σv (t) )T y }\n= argmax y\n{( v(t)+σAx̃(t)\n1+σ\n)T y − f∗(y) }\n= argmax y\n{ v(t+1) T y − f∗(y) } = f ′(v(t+1)),\nwhere in the last equality we used the property of conjugate function when f is strongly convex and smooth. Moreover,\nv(t+1) = (f ′)−1(y(t+1)) = (f∗)′(y(t+1)),\nwhich completes the proof.\nAccording to Lemma 1, we only need to provide initial points such that v(0) = (f∗)′(y(0)) is easy to compute. We do not need to compute (f∗)′(y(t)) directly for any t > 0, because it is can be updated as v(t) in (12). Consequently, we can update y(t) in the BPD algorithm using the gradient f ′(v(t)), without the need of dual proximal mapping. The resulting dual-free algorithm is given in Algorithm 6. Theorem 3. Suppose Assumption 2 holds and let (x⋆, y⋆) be the unique saddle point of L defined in (6). If we set the parameters in Algorithm 6 as\nτ = 1L\n√ γ\nλ+δµ2 , σ = 1 L\n√ γ(λ+ δµ2), (13)\nand θ = max{θx, θy} where\nθx = ( 1− τσδµ2(4+2σ) ) 1 1+τλ , θy = 1 1+σ/2 , (14)\nthen we have (\n1 2τ + λ 2 ) ‖x(t) − x⋆‖2 + 12D(y⋆, y(t)) ≤ θtC,\nL(x(t), y⋆)− L(x⋆, y(t)) ≤ θtC,\nwhere C = (\n1 2τ + λ 2\n) ‖x(0) − x⋆‖2 + ( 1 σ+ 1 2 ) D(y⋆, y(0)).\nTheorem 3 is proved in Appendices B and D. Assuming γ(λ+ δµ2) ≪ L2, we have\nθx ≈ 1− γδµ 2 16L2 − λ2L √ γ λ+δµ2 , θy ≈ 1− √ γ(λ+δµ2) 4L .\nAgain, we gain insights by consider the special cases:\n• If δµ2 = 0 and λ > 0, then θy ≈ 1 − √ γλ 4L and θx ≈\n1− √ γλ 2L . So θ = max{θx, θy} is an accelerated rate.\n• If δµ2 > 0 and λ = 0, then θy ≈ 1 − √ γδµ2\n4L and\nθx ≈ 1− γδµ 2 16L2 . Thus θ = max{θx, θy} ≈ 1− γδµ2\n16L2 is not accelerated. This conclusion does not depends on the relative sizes of γδ and µ2/L2, and it is the major difference from the Euclidean case in Section 2.\nAlgorithm 7 Adaptive Dual-Free SPDC (ADF-SPDC) input: parameters σ, τ , θ > 0, initial point (x(0), y(0)),\nand adaptation period T .\nSet x̃(0) = x(0) and v(0)i = (φ ∗ i ) ′(y(0)i ) for i = 1, . . . , n for t = 0, 1, 2, . . . do\npick k ∈ {1, . . . , n} uniformly at random for i ∈ {1, . . . , n} do\nif i == k then\nv (t+1) k =\nv (t) k +σa T k x̃ (t)\n1+σ , y (t+1) k = φ ′ k(v (t+1) k )\nelse v (t+1) i = v (t) i , y (t+1) i = y (t) i\nend if end for\nx(t+1) = proxτg\n( x(t)− τ ( u(t)+ (y\n(t+1) k −y (t) k )ak\n))\nu(t+1) = u(t) + 1n (y (t+1) k − y (t) k )ak x̃(t+1) = x(t+1) + θ(x(t+1) − x(t)) if mod(t+ 1, T · n) = 0 then\n(τ, σ, θ) = SPDC-Adapt ( {P (t−sn), D(t−sn)}Ts=0 )\nend if end for\nIf both δµ2 > 0 and λ > 0, then the extent of acceleration depends on their relative size. If λ is on the same order as δµ2 or larger, then accelerated rate is obtained. If λ is much smaller than δµ2, then the theory predicts no acceleration."
  }, {
    "heading": "4.2. Dual-free SPDC algorithm",
    "text": "Following the same approach, we can derive an Adaptive Dual-Free SPDC algorithm, given in Algorithm 7. On related work, Shalev-Shwartz & Zhang (2016) and (Shalev-Shwartz, 2016) introduced dual-free SDCA.\nThe following theorem characterizes the choice of algorithmic parameters that can exploit strong convexity from data to achieve linear convergence (proof given in Appendix F). Theorem 4. Suppose Assumption 1 holds. Let (x⋆, y⋆) be the saddle point of L in (3) andR=max{‖a1‖, . . . , ‖an‖}. If we set T = ∞ in Algorithm 7 (non adaption) and let\nσ = 14R √ γ(nλ+ δµ2), τ = 14R √ γ nλ+δµ2 , (15)\nand θ = max{θx, θy} where\nθx = ( 1− τσδµ2n(4+2σ) ) 1 1+τλ , θy = 1+((n−1)/n)σ/2 1+σ/2 ,\n(16) then we have (\n1 2τ + λ 2\n) E [ ‖x(t) − x⋆‖2 ] + γ4E [ D(y⋆, y(t)) ] ≤ θtC,\nE [ L(x(t), y⋆)− L(x⋆, y(t)) ] ≤ θtC,\nwhere C = (\n1 2τ + λ 2\n) ‖x(0) − x⋆‖2 + ( 1 σ+ 1 2 ) D(y⋆, y(0)).\nNow we discuss the results of Theorem 4 in further details.\nThe cases of σµ2 = 0 but λ > 0. In this case we have τ = 14R √ γ nλ and σ = 1 4R √ nγλ, and\nθx = 1− 1 1+4R √ n/(λγ) , θy = 1− 1 n+8R √ n/(λγ) .\nThe rate is the same as for SPDC in Zhang & Xiao (2015).\nThe cases of σµ2 > 0 but λ = 0. In this case we have τ = 14Rµ √ γ δ and σ = µ 4R √ δγ, thus\nθx = 1− τσδµ 2 2n(σ+4) = 1− γδµ2 32nR2 · 1√γδµ/(4R)+4 , θy = 1+((n−1)/n)σ/2\n1+σ/2 = 1− 1n+8nR/(µ√γδ) .\nWe note that the primal function now is R2/γ-smooth and δµ2/n-strongly convex. We discuss the following cases:\n• If √γδµ > R, then we have θx ≈ 1 − √ γδµ\n8nR and θy ≈ 1− 1n . Therefore θ = max{θx, θy} ≈ 1− 1n .\n• Otherwise, we have θx ≈ 1 − γδµ 2\n64nR2 and θy is of the same order. This is not an accelerated rate, and we have the same iteration complexity as SVRG.\nFinally, we give concrete examples of how to compute the initial points y(0) and v(0) such that v(0)i = (φ ∗ i ) ′(y(0)i ).\n• For squared loss, φi(α) = 12 (α − bi)2 and φ∗i (β) = 1 2β 2 + biβ. So v (0) i = (φ ∗ i ) ′(y(0)i ) = y (0) i + bi. • For logistic regression, we have bi ∈ {1,−1} and φi(α) = log(1 + e\n−biα). The conjugate function is φ∗i (β) = (−biβ) log(−biβ) + (1 + biβ) log(1 + biβ) if biβ ∈ [−1, 0] and +∞ otherwise. We can choose y (0) i =− 12bi and v (0) i =0 such that v (0) i =(φ ∗ i ) ′(y(0)i ).\nFor logistic regression, we have δ = 0 over the full domain of φi. However, each φi is locally strongly convex in bounded domain (Bach, 2014): if z ∈ [−B,B], then we know δ = minz φi′′(z) ≥ exp(−B)/4. Therefore it is well suitable for an adaptation scheme similar to Algorithm 4 that do not require knowledge of either δ or µ."
  }, {
    "heading": "5. Preliminary experiments",
    "text": "We present preliminary experiments to demonstrate the effectiveness of our proposed algorithms. First, we consider batch primal-dual algorithms for ridge regression over a synthetic dataset. The data matrix A has sizes n = 5000 and d = 3000, and its entries are sampled from multivariate normal distribution with mean zero and covariance matrix Σij = 2|i−j|/2. We normalize all datasets\nsuch that ai = ai/ (maxj ‖aj‖), to ensure the maximum norm of the data points is 1. We use ℓ2-regularization g(x) = (λ/2)‖x‖2 with three choices of parameter λ: 1/n, 10−2/n and 10−4/n, which represent the strong, medium, and weak levels of regularization, respectively.\nFigure 1 shows the performance of four different algorithms: the primal accelerated gradient (Primal AG) algorithm (Nesterov, 2004) using λ as strong convexity parameter, the BPD algorithm (Algorithm 1) that uses the same λ and µ2δ = 0, the optimal BPD algorithm (Opt-BPD) that uses µ2δ = λmin(A\nTA) n ≈ 0.022n computed from data,\nand the Ada-BPD algorithm (Algorithm 2) with the robust adaptation heuristic (Algorithm 4) with T = 10, c = 0.95 and c = 1.5. As expected, the performance of Primal-AG is very similar to that of BPD, and Opt-BPD has the fastest convergence. The Ada-BPD algorithm can partially exploit strong convexity from data without knowledge of µ.\nNext we compare DF-SPDC (Algorithm 5 without adaption) and ADF-SPDC (Algorithm 7 with adaption) against several state-of-the-art randomized algorithms for ERM: SVRG (Johnson & Zhang, 2013), SAGA (Defazio et al., 2014) Katyusha (Allen-Zhu, 2016) and the standard SPDC method (Zhang & Xiao, 2015). For SVRG and Katyusha (an accelerated variant of SVRG), we choose the variance reduction period as m = 2n. The step sizes of all algorithms are set as their original paper suggested. For\nAda-SPDC and ADF-SPDC, we use the robust adaptation scheme with T = 10, c = 0.95 and c = 1.5.\nWe first compare these randomized algorithms for ridge regression over the cpuact data from the LibSVM website (https://www.csie.ntu.edu.tw/˜cjlin/libsvm/). The results are shown in Figure 2. With relatively strong regularization λ = 1/n, all methods perform similarly as predicted by theory. When λ becomes smaller, the nonaccelerated algorithms (SVRG and SAGA) automatically exploit strong convexity from data, so they become faster than the non-adaptive accelerated methods (Katyusha, SPDC and DF-SPDC). The adaptive accelerated method, ADF-SPDC, has the fastest convergence. This indicates that our theoretical results, which predict no acceleration in this case, may be further improved.\nFinally we compare these algorithms for logistic regression on the rcv1 dataset (from LibSVM website) and another synthetic dataset with n = 5000 and d = 500, generated similarly as before but with covariance matrix Σij = 2\n|i−j|/100. For the standard SPDC, we compute the coordinate-wise dual proximal mapping using a few steps of scalar Newton’s method to high precision. The dualfree SPDC algorithms only use gradients of the logistic function. The results are presented in Figure 3. For both datasets, the strong convexity from data is very weak, and the accelerated algorithms performs better."
  }],
  "year": 2017,
  "references": [{
    "title": "Katyusha: Accelerated variance reduction for faster sgd",
    "authors": ["Allen-Zhu", "Zeyuan"],
    "venue": "ArXiv e-print",
    "year": 2016
  }, {
    "title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression",
    "authors": ["Bach", "Francis"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Stochastic variance reduction methods for saddle-point problems",
    "authors": ["Balamurugan", "Palaniappan", "Bach", "Francis"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS)",
    "year": 2016
  }, {
    "title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
    "authors": ["Bertsekas", "Dimitri P"],
    "venue": "Optimization for Machine Learning,",
    "year": 2012
  }, {
    "title": "A first-order primal-dual algorithm for convex problems with applications to imaging",
    "authors": ["Chambolle", "Antonin", "Pock", "Thomas"],
    "venue": "Journal of Mathematical Imaging and Vision,",
    "year": 2011
  }, {
    "title": "On the ergodic convergence rates of a first-order primal–dual algorithm",
    "authors": ["Chambolle", "Antonin", "Pock", "Thomas"],
    "venue": "Mathematical Programming, Series A,",
    "year": 2016
  }, {
    "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
    "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "On the global and linear convergence of the generalized alternating direction method of multipliers",
    "authors": ["Deng", "Wei", "Yin", "Wotao"],
    "venue": "Journal of Scientific Computing,",
    "year": 2016
  }, {
    "title": "Accelerated, parallel, and proximal coordinate descent",
    "authors": ["Fercoq", "Oliver", "Richtárik", "Peter"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "Adaptive primal-dual hybrid gradient methods for saddle-point problems",
    "authors": ["Goldstein", "Tom", "Li", "Min", "Yuan", "Xiaoming", "Esser", "Ernie", "Baraniuk", "Richard"],
    "venue": "arXiv preprint arXiv:1305.0546,",
    "year": 2013
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "An optimal randomized incremental gradient method",
    "authors": ["Lan", "Guanghui", "Zhou", "Yi"],
    "venue": "arXiv preprint arXiv:1507.02000,",
    "year": 2015
  }, {
    "title": "A universal catalyst for first-order optimization",
    "authors": ["Lin", "Hongzhou", "Mairal", "Julien", "Harchaoui", "Zaid"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "An accelerated randomized proximal coordinate gradient method and its application to regularized empirical risk minimization",
    "authors": ["Lin", "Qihang", "Lu", "Zhaosong", "Xiao"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2015
  }, {
    "title": "A first-order primal-dual algorithm with linesearch",
    "authors": ["Malitsky", "Yura", "Pock", "Thomas"],
    "venue": "arXiv preprint arXiv:1608.08883,",
    "year": 2016
  }, {
    "title": "Incremental subgradient methods for nondifferentiable optimization",
    "authors": ["Nedic", "Angelia", "Bertsekas", "Dimitri P"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2001
  }, {
    "title": "Introductory Lectures on Convex Optimization: A Basic Course",
    "authors": ["Nesterov", "Yurii"],
    "year": 2004
  }, {
    "title": "Efficiency of coordinate descent methods on huge-scale optimization problems",
    "authors": ["Nesterov", "Yurii"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2012
  }, {
    "title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
    "authors": ["Richtárik", "Peter", "Takáč", "Martin"],
    "venue": "Mathematical Programming,",
    "year": 2014
  }, {
    "title": "A stochastic gradient method with an exponential convergence rate for finite training sets",
    "authors": ["Roux", "Nicolas L", "Schmidt", "Mark", "Bach", "Francis"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "SDCA without duality, regularization, and individual convexity",
    "authors": ["Shalev-Shwartz", "Shai"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Stochastic dual coordinate ascent methods for regularized loss minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
    "authors": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"],
    "venue": "Mathematical Programming,",
    "year": 2016
  }, {
    "title": "A proximal stochastic gradient method with progressive variance reduction",
    "authors": ["Xiao", "Lin", "Zhang", "Tong"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization",
    "authors": ["Zhang", "Yuchen", "Xiao", "Lin"],
    "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
    "year": 2015
  }],
  "id": "SP:c6baf181bf67bc1f5283f3ce28070705c7c7085a",
  "authors": [{
    "name": "Jialei Wang",
    "affiliations": []
  }, {
    "name": "Lin Xiao",
    "affiliations": []
  }],
  "abstractText": "We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems and solved by primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution. In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization. We also present dual-free variants of adaptive primal-dual algorithms that do not need the dual proximal mapping, which are especially suitable for logistic regression.",
  "title": "Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms"
}