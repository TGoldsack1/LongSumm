{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1492–1502 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation.\nLearning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing\n(FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017). Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016). But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other (Lluı́s et al., 2013; Peng et al., 2017). Of particular interest is the approach of Peng et al., where three kinds of semantic graphs are jointly learned on the same input, using parallel annotations. However, as new annotation efforts cannot be expected to use the same original texts as earlier efforts, the utility of this approach is limited.\nWe propose an extension to Peng et al.’s formulation which addresses this limitation by considering disjoint resources, each containing only a single kind of annotation. Moreover, we consider structurally divergent formalisms, one dealing with semantic spans and the other with semantic\n1492\ndependencies. We experiment on frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2010), a span-based semantic role labeling (SRL) task (§2.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM; Flickinger et al., 2012) task (§2.2). See Figure 1 for an example sentence with gold FrameNet annotations, and author-annotated DM representations.\nOur joint inference formulation handles missing annotations by treating the structures that are not present in a given training example as latent variables (§3).1 Specifically, semantic dependencies are treated as a collection of latent variables when training on FrameNet examples.\nUsing this latent variable formulation, we present an approach for relating spans and dependencies, by explicitly scoring affinities between pairs of potential spans and dependencies. Because there are a huge number of such pairs, we limit our consideration to only certain pairs—our design is inspired by the head rules of Surdeanu et al. (2008). Further possible span-dependency pairs are pruned using an `1-penalty technique adapted from sparse structure learning (§5). Neural network architectures are used to score framesemantic structures, semantic dependencies, as well as cross-task structures (§4).\nTo summarize, our contributions include: • using a latent variable formulation to ex-\ntend cross-task scoring techniques to scenarios where datasets do not overlap; • learning cross-task parts across structurally\ndivergent formalisms; and • using an `1-penalty technique to prune the\nspace of cross task parts. Our approach results in a new state-of-the-art in frame-semantic parsing, improving prior work by 0.8% absolute F1 points (§6), and achieves competitive performance on semantic dependency parsing. Our code is available at https://github.com/Noahs-ARK/ NeurboParser."
  }, {
    "heading": "2 Tasks and Related Work",
    "text": "We describe the two tasks addressed in this work—frame-semantic parsing (§2.1) and semantic dependency parsing (§2.2)—and discuss how\n1Following past work on support vector machines with latent variables (Yu and Joachims, 2009), we use the term “latent variable,” even though the model is not probabilistic.\ntheir structures relate to each other (§2.3)."
  }, {
    "heading": "2.1 Frame-Semantic Parsing",
    "text": "Frame-semantic parsing is a span-based task, under which certain words or phrases in a sentence evoke semantic frames. A frame is a group of events, situations, or relationships that all share the same set of participant and attribute types, called frame elements or roles. Gold supervision for frame-semantic parses comes from the FrameNet lexicon and corpus (Baker et al., 1998).\nConcretely, for a given sentence, x, a framesemantic parse y consists of:\n• a set of targets, each being a short span (usually a single token2) that evokes a frame; • for each target t, the frame f that it evokes;\nand • for each frame f , a set of non-overlapping ar-\ngument spans in the sentence, each argument a = (i, j, r) having a start token index i, end token index j and role label r.\nThe lemma and part-of-speech tag of a target comprise a lexical unit (or LU). The FrameNet ontology provides a mapping from an LU ` to the set of possible frames it could evoke, F`. Every frame f ∈ F` is also associated with a set of roles, Rf under this ontology. For example, in Figure 1, the LU “fall.v” evokes the frame MOTION DIRECTIONAL. The roles THEME and PLACE (which are specific to MOTION DIRECTIONAL), are filled by the spans “Only a few books” and “in the reading room” respectively. LOCATIVE RELATION has other roles (PROFILED REGION, ACCESSIBILITY, DEIXIS, etc.) which are not realized in this sentence.\nIn this work, we assume gold targets and LUs are given, and parse each target independently, following the literature (Johansson and Nugues, 2007; FitzGerald et al., 2015; Yang and Mitchell, 2017; Swayamdipta et al., 2017, inter alia). Moreover, following Yang and Mitchell (2017), we perform frame and argument identification jointly. Most prior work has enforced the constraint that a role may be filled by at most one argument span, but following Swayamdipta et al. (2017) we do not impose this constraint, requiring only that arguments for the same target do not overlap.\n296.5% of targets in the training data are single tokens."
  }, {
    "heading": "2.2 Semantic Dependency Parsing",
    "text": "Broad-coverage semantic dependency parsing (SDP; Oepen et al., 2014, 2015, 2016) represents sentential semantics with labeled bilexical dependencies. The SDP task mainly focuses on three semantic formalisms, which have been converted to dependency graphs from their original annotations. In this work we focus on only the DELPHIN MRS (DM) formalism.\nEach semantic dependency corresponds to a labeled, directed edge between two words. A single token is also designated as the top of the parse, usually indicating the main predicate in the sentence. For example in Figure 1, the left-most arc has head “Only”, dependent “few”, and label arg1. In semantic dependencies, the head of an arc is analogous to the target in frame semantics, the destination corresponds to the argument, and the label corresponds to the role. The same set of labels are available for all arcs, in contrast to the frame-specific roles in FrameNet."
  }, {
    "heading": "2.3 Spans vs. Dependencies",
    "text": "Early semantic role labeling was span-based (Gildea and Jurafsky, 2002; Toutanova et al., 2008, inter alia), with spans corresponding to syntactic constituents. But, as in syntactic parsing, there are sometimes theoretical or practical reasons to prefer dependency graphs. To this end, Surdeanu et al. (2008) devised heuristics based on syntactic head rules (Collins, 2003) to transform PropBank (Palmer et al., 2005) annotations into dependencies. Hence, for PropBank at least, there is a very direct connection (through syntax) between spans and dependencies.\nFor many other semantic representations, such a direct relationship might not be present. Some semantic representations are designed as graphs from the start (Hajič et al., 2012; Banarescu et al., 2013), and have no gold alignment to spans. Conversely, some span-based formalisms are not annotated with syntax (Baker et al., 1998; He et al., 2015),3 and so head rules would require using (noisy and potentially expensive) predicted syntax.\nInspired by the head rules of Surdeanu et al. (2008), we design cross-task parts, without relying\n3 In FrameNet, phrase types of arguments and their grammatical function in relation to their target have been annotated. But in order to apply head rules, the internal structure of arguments (or at least their semantic heads) would also require syntactic annotations.\non gold or predicted syntax (which may be either unavailable or error-prone) or on heuristics."
  }, {
    "heading": "3 Model",
    "text": "Given an input sentence x, and target t with its LU `, denote the set of valid frame-semantic parses (§2.1) as Y(x, t, `), and valid semantic dependency parses as Z(x).4 We learn a parameterized function S that scores candidate parses. Our goal is to jointly predict a frame-semantic parse and a semantic dependency graph by selecting the highest scoring candidates:\n(ŷ, ẑ) = arg max (y,z)∈Y(x,t,`)×Z(x) S(y, z,x, t, `). (1)\nThe overall score S can be decomposed into the sum of frame SRL score Sf, semantic dependency score Sd, and a cross-task score Sc:\nS(y, z,x, t, `) = Sf(y,x, t, `) + Sd(z,x)\n+Sc(y, z,x, t,`). (2)\nSf and Sc require access to the target and LU, in addition to x, but Sd does not. For clarity, we omit the dependence on the input sentence, target, and lexical unit, whenever the context is clear. Below we describe how each of the scores is computed based on the individual parts that make up the candidate parses. Frame SRL score. The score of a framesemantic parse consists of • the score for a predicate part, sf (p) where\neach predicate is defined as a combination of a target t, the associated LU, `, and the frame evoked by the LU, f ∈ F`; • the score for argument parts, sf (a), each as-\nsociated with a token span and semantic role fromRf .\nTogether, this results in a set of frame-semantic parts of size O(n2 |F`| |Rf |).5 The score for a frame semantic structure y is the sum of local scores of parts in y:\nSf(y) = ∑\nyi∈y sf(yi). (3)\nThe computation of sf is described in §4.2. 4For simplicity, we consider only a single target here; handling of multiple targets is discussed in §6. 5With pruning (described in §6) we reduce this to a number of parts linear in n. Also, |F`| is usually small (averaging 1.9), as is |Rf | (averaging 9.5).\nincludes include.v Inclusion Evidence to support this argument Total …\nFigure 2: An example of cross-task parts from the FrameNet 1.5 development set. We enumerate all unlabeled semantic dependencies from the first word of the target (includes) to any token inside the span. The red bolded arc indicates the prediction of our model.\nSemantic dependency score. Following Martins and Almeida (2014), we consider three types of parts in a semantic dependency graph: semantic heads, unlabeled semantic arcs, and labeled semantic arcs. Analogous to Equation 3, the score for a dependency graph z is the sum of local scores:\nSd(z) = ∑\nzj∈z sd(zj), (4)\nThe computation of sd is described in §4.3. Cross task score. In addition to task-specific parts, we introduce a set C of cross-task parts. Each cross-task part relates an argument part from y to an unlabeled dependency arc from z. Based on the head-rules described in §2.3, we consider unlabeled arcs from the target to any token inside the span.6 Intuitively, an argument in FrameNet would be converted into a dependency from its target to the semantic head of its span. Since we do not know the semantic head of the span, we consider all tokens in the span as potential modifiers of the target. Figure 2 shows examples of crosstask parts. The cross-task score is given by\nSc(y, z) = ∑\n(yi,zj)∈(y×z)∩C sc(yi, zj). (5)\nThe computation of sc is described in §4.4. In contrast to previous work (Lluı́s et al., 2013; Peng et al., 2017), where there are parallel annotations for all formalisms, our input sentences contain only one of the two—either the span-based frame SRL annotations, or semantic dependency graphs from DM. To handle missing annotations, we treat semantic dependencies z as latent when\n6Most targets are single-words (§2.1). For multi-token targets, we consider only the first token, which is usually content-bearing.\ndecoding frame-semantic structures.7 Because the DM dataset we use does not have target annotations, we do not use latent variables for frame semantic structures when predicting semantic dependency graphs. The parsing problem here reduces to\nẑ = arg max z∈Z Sd(z), (6)\nin contrast with Equation 1 ."
  }, {
    "heading": "4 Parameterizations of Scores",
    "text": "This section describes the parametrization of the scoring functions from §3. At a very high level: we learn contextualized token and span vectors using a bidirectional LSTM (biLSTM; Graves, 2012) and multilayer perceptrons (MLPs) (§4.1); we learn lookup embeddings for LUs, frames, roles, and arc labels; and to score a part, we combine the relevant representations into a single scalar score using a (learned) low-rank multilinear mapping. Scoring frames and arguments is detailed in §4.2, that of dependency structures in §4.3, and §4.4 shows how to capture interactions between arguments and dependencies. All parameters are learned jointly, through the optimization of a multitask objective (§5). Tensor notation. The order of a tensor is the number of its dimensions—an order-2 tensor is a matrix and an order-1 tensor is a vector. Let ⊗ denote tensor product; the tensor product of two order-2 tensors A and B yields an order-4 tensor where (A ⊗B)i,j,k,l = Ai,jBk,l. We use 〈·, ·〉 to denote inner products."
  }, {
    "heading": "4.1 Token and Span Representations",
    "text": "The representations of tokens and spans are formed using biLSTMs followed by MLPs.\nContextualized token representations. Each token in the input sentence x is mapped to an embedding vector. Two LSTMs (Hochreiter and Schmidhuber, 1997) are run in opposite directions over the input vector sequence. We use the concatenation of the two hidden representations at each position i as a contextualized word embedding for each token:\nhi = [−→ h i; ←− h i ] . (7)\n7Semantic dependency parses over a sentence are not constrained to be identical for different frame-semantic targets.\nSpan representations. Following Lee et al. (2017), span representations are computed based on boundary word representations and discrete length and distance features. Concretely, given a target t and its associated argument a = (i, j, r) with boundary indices i and j, we compute three features φt(a) based on the length of a, and the distances from i and j to the start of t. We concatenate the token representations at a’s boundary with the discrete features φt(a). We then use a two-layer tanh-MLP to compute the span representation:\ngspan(i, j) = MLPspan ( [hi;hj ;φt(a)] ) . (8)\nThe target representation gtgt(t) is similarly computed using a separate MLPtgt, with a length feature but no distance features."
  }, {
    "heading": "4.2 Frame and Argument Scoring",
    "text": "As defined in §3, the representation for a predicate part incorporates representations of a target span, the associated LU and the frame evoked by the LU. The score for a predicate part is given by a multilinear mapping:\ngpred(f) = gfr(f)⊗ gtgt(t)⊗ glu(`) (9a) sf(p) = 〈 W,gpred(f) 〉 , (9b)\nwhere W is a low-rank order-3 tensor of learned parameters, and gfr(f) and glu(`) are learned lookup embeddings for the frame and LU.\nA candidate argument consists of a span and its role label, which in turn depends on the frame, target and LU. Hence the score for argument part, a = (i, j, r) is given by extending definitions from Equation 9:\ngarg(a) = gspan(i, j)⊗ grole(r), (10a) sf(a) = 〈 W⊗U,gpred(f)⊗ garg(a) 〉 , (10b)\nwhere U is a low-rank order-2 tensor of learned parameters and grole(r) is a learned lookup embedding of the role label."
  }, {
    "heading": "4.3 Dependency Scoring",
    "text": "Local scores for dependencies are implemented with two-layer tanh-MLPs, followed by a final linear layer reducing the represenation to a single scalar score. For example, let u = i→j denote an unlabeled arc (ua). Its score is:\ngua(u) = MLPua ( [hi;hj ] ) (11a)\nsd(u) = w ua · gua(u), (11b)\nwhere wua is a vector of learned weights. The scores for other types of parts are computed similarly, but with separate MLPs and weights."
  }, {
    "heading": "4.4 Cross-Task Part Scoring",
    "text": "As shown in Figure 2, each cross-task part c consists of two first-order parts: a frame argument part a, and an unlabeled dependency part, u. The score for a cross-task part incorporates both:\nsc (c) = 〈 W⊗U⊗V,gpred(f)⊗ garg(a)\n⊗ wua ⊗ gua(u) 〉 , (12)\nwhere V is a low-rank order-2 tensor of parameters. Following previous work (Lei et al., 2014; Peng et al., 2017), we construct the parameter tensors W, U, and V so as to upper-bound their ranks."
  }, {
    "heading": "5 Training and Inference",
    "text": "All parameters from the previous sections are trained using a max-margin training objective (§5.1). For inference, we use a linear programming procedure, and a sparsity-promoting penalty term for speeding it up (§5.2)."
  }, {
    "heading": "5.1 Max-Margin Training",
    "text": "Let y∗ denote the gold frame-semantic parse, and let δ (y,y∗) denote the cost of predicting y with respect to y∗. We optimize the latent structured hinge loss (Yu and Joachims, 2009), which gives a subdifferentiable upper-bound on δ:\nL (y∗) = max (y,z)∈Y×Z {S (y, z) + δ (y,y∗)}\n−max z∈Z {S (y∗, z)} .\n(13)\nFollowing Martins and Almeida (2014), we use a weighted Hamming distance as the cost function, where, to encourage recall, we use costs 0.6 for false negative predictions and 0.4 for false positives. Equation 13 can be evaluated by applying the same max-decoding algorithm twice—once with cost-augmented inference (Crammer et al., 2006), and once more keeping y∗ fixed. Training then aims to minimize the average loss over all training instances.8\nAnother potential approach to training a model on disjoint data would be to marginalize out the\n8We do not use latent frame structures when decoding semantic dependency graphs (§3). Hence, the loss reduces to structured hinge (Tsochantaridis et al., 2004) when training on semantic dependencies.\nlatent structures and optimize the conditional loglikelihood (Naradowsky et al., 2012). Although max-decoding and computing marginals are both NP-hard in general graphical models, there are more efficient off-the-shelf implementations for approximate max-decoding, hence, we adopt a max-margin formulation."
  }, {
    "heading": "5.2 Inference",
    "text": "We formulate the maximizations in Equation 13 as 0–1 integer linear programs and use AD3 to solve them (Martins et al., 2011). We only enforce a non-overlapping constraint when decoding FrameNet structures, so that the argument identification subproblem can be efficiently solved by a dynamic program (Kong et al., 2016; Swayamdipta et al., 2017). When decoding semantic dependency graphs, we enforce the determinism constraint (Flanigan et al., 2014), where certain labels may appear on at most one arc outgoing from the same token. Inference speedup by promoting sparsity. As discussed in §3, even after pruning, the number of within-task parts is linear in the length of the input sentence, so the number of cross-task parts is quadratic. This leads to potentially very slow inference. We address this problem by imposing an `1 penalty on the cross-task part scores:\nL ( y∗ ) + λ ∑\n(yi,zj)∈C\n∣∣sc(yi, zj) ∣∣, (14)\nwhere λ is a hyperparameter, set to 0.01 as a practical tradeoff between efficiency and development set performance. Whenever the score for a crosstask part is driven to zero, that part’s score no longer needs to be considered during inference. It is important to note that by promoting sparsity this way, we do not prune out any candidate solutions. We are instead encouraging fewer terms in the scoring function, which leads to smaller, faster inference problems even though the space of feasible parses is unchanged.\nThe above technique is closely related to a line of work in estimating the structure of sparse graphical models (Yuan and Lin, 2007; Friedman et al., 2008), where an `1 penalty is applied to the inverse covariance matrix in order to induce a smaller number of conditional dependencies between variables. To the best of our knowledge, we are the first to apply this technique to the output of neural scoring functions. Here, we are interested in learn-\ning sparse graphical models only because they result in faster inference, not because we have any a priori belief about sparsity. This results in roughly a 14× speedup in our experiments, without any significant drop in performance."
  }, {
    "heading": "6 Experiments",
    "text": "Datasets. Our model is evaluated on two different releases of FrameNet: FN 1.5 and FN 1.7,9 using splits from Swayamdipta et al. (2017). Following Swayamdipta et al. (2017) and Yang and Mitchell (2017), each target annotation is treated as a separate training instance. We also include as training data the exemplar sentences, each annotated for a single target, as they have been reported to improve performance (Kshirsagar et al., 2015; Yang and Mitchell, 2017). For semantic dependencies, we use the English DM dataset from the SemEval 2015 Task 18 closed track (Oepen et al., 2015).10 DM contains instances from the WSJ corpus for training and both in-domain (id) and out-of-domain (ood) test sets, the latter from the Brown corpus.11 Table 1 summarizes the sizes of the datasets. Baselines. We compare FN performance of our joint learning model (FULL) to two baselines: BASIC: A single-task frame SRL model, trained using a structured hinge objective. NOCTP: A joint model without cross-task parts.\nIt demonstrates the effect of sharing parameters in word embeddings and LSTMs (like in FULL). It does not use latent semantic dependency structures, and aims to minimize the sum of training losses from both tasks.\nWe also compare semantic dependency parsing performance against the single task model by Peng\n9https://FN.icsi.berkeley.edu/ fndrupal/\n10http://sdp.delph-in.net/. The closed track does not have access to any syntactic analyses. The impact of syntactic features on SDP performance is extensively studied in Ribeyre et al. (2015).\n11Our FN training data does not overlap with the DM test set. We remove the 3 training sentences from DM which appear in FN test data.\net al. (2017), denoted as NeurboParser (BASIC). To ensure fair comparison with our FULL model, we made several modifications to their implementation (§6.3). We observed performance improvements from our reimplementation, which can be seen in Table 5. Pruning strategies. For frame SRL, we discard argument spans longer than 20 tokens (Swayamdipta et al., 2017). We further pretrain an unlabeled model and prune spans with posteriors lower than 1/n2, with n being the input sentence length. For semantic dependencies, we generally follow Martins and Almeida (2014), replacing their feature-rich pruner with neural networks. We observe that O(n) spans/arcs remain after pruning, with around 96% FN development recall, and more than 99% for DM.12"
  }, {
    "heading": "6.1 Empirical Results",
    "text": "FN parsing results. Table 2 compares our full frame-semantic parsing results to previous systems. Among them, Täckström et al. (2015) and Roth (2016) implement a two-stage pipeline and use the method from Hermann et al. (2014) to predict frames. FitzGerald et al. (2015) uses the\n12On average, around 0.8n argument spans, and 5.7n unlabeled dependency arcs remain after pruning.\nsame pipeline formulation, but improves the frame identification of Hermann et al. (2014) with better syntactic features. open-SESAME (Swayamdipta et al., 2017) uses predicted frames from FitzGerald et al. (2015), and improves argument identification using a softmax-margin segmental RNN. They observe further improvements from product of experts ensembles (Hinton, 2002).\nThe best published FN 1.5 results are due to Yang and Mitchell (2017). Their relational model (REL) formulates argument identification as a sequence of local classifications. They additionally introduce an ensemble method (denoted as ALL) to integrate the predictions of a sequential CRF. They use a linear program to jointly predict frames and arguments at test time. As shown in Table 2, our single-model performance outperforms their REL model, and is on par with their ALL model. For a fair comparison, we build an ensemble (FULL, 2×) by separately training two models, differing only in random seeds, and averaging their part scores. Our ensembled model outperforms previous best results by 0.8% absolute.\nTable 3 compares our frame identification results with previous approaches. Hermann et al. (2014) and Hartmann et al. (2017) use distributed word representations and syntax features. We follow the FULL LEXICON setting (Hermann et al., 2014) and extract candidate frames from the offi-\n13Our comparison to Hermann et al. (2014) is based on their updated version: http://www.aclweb.org/ anthology/P/P14/P14-1136v2.pdf. Ambiguous frame identification results by Yang and Mitchell (2017) and Hartmann et al. (2017) are 75.7 and 73.8. Their ambiguous lexical unit sets are different from the one extracted from the official frame directory, and thus the results are not comparable to those in Table 3.\ncial directories. The Ambiguous setting compares lexical units with more than one possible frames. Our approach improves over all previous models under both settings, demonstrating a clear benefit from joint learning.\nWe observe similar trends on FN 1.7 for both full structure extraction and for frame identification only (Table 4). FN 1.7 extends FN 1.5 with more consistent annotations. Its test set is different from that of FN 1.5, so the results are not directly comparable to Table 2. We are the first to report frame-semantic parsing results on FN 1.7, and we encourage future efforts to do so as well.\nSemantic dependency parsing results. Table 5 compares our semantic dependency parsing performance on DM with the baselines. Our reimplementation of the BASIC model slightly improves performance on in-domain test data. The NOCTP model ties parameters from word embeddings and LSTMs when training on FrameNet and DM, but does not use cross-task parts or joint prediction. NOCTP achieves similar in-domain test performance, and improves over BASIC on outof-domain data. By jointly predicting FrameNet\nstructures and semantic dependency graphs, the FULL model outperforms the baselines by more than 0.6% absolute F1 scores under both settings.\nPrevious state-of-the-art results on DM are due to the joint learning model of Peng et al. (2017), denoted as NeurboParser (FREDA3). They adopted a multitask learning approach, jointly predicting three different parallel semantic dependency annotations. Our FULL model’s in-domain test performance is on par with FREDA3, and improves over it by 0.6% absolute F1 on out-ofdomain test data. Our ensemble of two FULL models achieves a new state-of-the-art in both indomain and out-of-domain test performance."
  }, {
    "heading": "6.2 Analysis",
    "text": "Error type breakdown. Similarly to He et al. (2017), we categorize prediction errors made by the BASIC and FULL models in Table 6. Entirely missing an argument accounts for most of the errors for both models, but we observe fewer errors by FULL compared to BASIC in this category. FULL tends to predict more arguments in general, including more incorrect arguments.\nSince candidate roles are determined by frames, frame and role errors are highly correlated. Therefore, we also show the role errors when frames are correctly predicted (parenthesized numbers in the second row). When a predicted argument span matches a gold span, predicting the semantic role is less challenging. Role errors account for only around 13% of all errors, and half of them are due to mispredictions of frames. Performance by argument length. Figure 3 plots dev. precision and recall of both BASIC and FULL against binned argument lengths. We ob-\nserve two trends: (a) FULL tends to predict longer arguments (averaging 3.2) compared to BASIC (averaging 2.9), while keeping similar precision;14 (b) recall improvement in FULL mainly comes from arguments longer than 4."
  }, {
    "heading": "6.3 Implementation Details",
    "text": "Our implementation is based on DyNet (Neubig et al., 2017).15 We use predicted part-of-speech tags and lemmas using NLTK (Bird et al., 2009).16\nParameters are optimized with stochastic subgradient descent for up to 30 epochs, with `2 norms of gradients clipped to 1. We use 0.33 as initial learning rate, and anneal it at a rate of 0.5 every 10 epochs. Early stopping is applied based on FN development F1. We apply logarithm with base 2 to all discrete features, e.g., log2(d+1) for distance feature valuing d. To speed up training, we randomly sample a 35% subset from the FN exemplar instances each epoch. Hyperparameters. Each input token is represented as the concatenation a word embedding vector, a learned lemma vector, and a learned vector for part-of speech, all updated during training. We use 100-dimensional GloVe (Pennington et al., 2014) to initialize word embeddings. We apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special UNK symbol with probability α1+#(w) , with #(w) being the count of w in the training set. We follow the default parameters initialization procedure by DyNet, and an `2\n14Average gold span length is 3.4 after discarding those longer than 20.\n15https://github.com/clab/dynet 16http://www.nltk.org/\npenalty of 10−6 is applied to all weights. See Table 7 for other hyperparameters. Modifications to Peng et al. (2017). To ensure fair comparisons, we note two implementation modifications to Peng et al.’s basic model. We use a more recent version (2.0) of the DyNet toolkit, and we use 50-dimensional lemma embeddings instead of their 25-dimensional randomly-initialized learned word embeddings."
  }, {
    "heading": "7 Conclusion",
    "text": "We presented a novel multitask approach to learning semantic parsers from disjoint corpora with structurally divergent formalisms. We showed how joint learning and prediction can be done with scoring functions that explicitly relate spans and dependencies, even when they are never observed together in the data. We handled the resulting inference challenges with a novel adaptation of graphical model structure learning to the deep learning setting. We raised the state-ofthe-art on DM and FrameNet parsing by learning from both, despite their structural differences and non-overlapping data. While our selection of factors is specific to spans and dependencies, our general techniques could be adapted to work with more combinations of structured prediction tasks. We have released our implementation at https://github.com/ Noahs-ARK/NeurboParser."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Kenton Lee, Luheng He, and Rowan Zellers for their helpful comments, and the anonymous reviewers for their valuable feedback. This work was supported in part by NSF grant IIS1562364."
  }],
  "year": 2018,
  "references": [{
    "title": "Frame semantic tree kernels for social network extraction from text",
    "authors": ["Apoorv Agarwal", "Sriramkumar Balasubramanian", "Anup Kotalwar", "Jiehan Zheng", "Owen Rambow."],
    "venue": "Proc. of EACL.",
    "year": 2014
  }, {
    "title": "The Berkeley FrameNet project",
    "authors": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."],
    "venue": "Proc. ACL.",
    "year": 1998
  }, {
    "title": "Abstract meaning representation for sembanking",
    "authors": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."],
    "venue": "Proc. LAW-ID.",
    "year": 2013
  }, {
    "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
    "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."],
    "venue": "” O’Reilly Media, Inc.”.",
    "year": 2009
  }, {
    "title": "Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing",
    "authors": ["Yun-Nung Chen", "William Yang Wang", "Alexander I Rudnicky."],
    "venue": "Proc. of ASRU-IEEE.",
    "year": 2013
  }, {
    "title": "Head-driven statistical models for natural language parsing",
    "authors": ["Michael Collins."],
    "venue": "Computational Linguistics 29(4):589–637.",
    "year": 2003
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proc. ICML.",
    "year": 2008
  }, {
    "title": "Minimal recursion semantics: An introduction",
    "authors": ["Ann Copestake", "Dan Flickinger", "Ivan A. Sag", "Carl Pollard."],
    "venue": "Research on Language & Computation 3(4):281–332.",
    "year": 2005
  }, {
    "title": "Annotation tools and knowledge representation for a textto-scene system",
    "authors": ["Bob Coyne", "Alex Klapheke", "Masoud Rouhizadeh", "Richard Sproat", "Daniel Bauer."],
    "venue": "Proc. of COLING.",
    "year": 2012
  }, {
    "title": "Online passive-aggressive algorithms",
    "authors": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer."],
    "venue": "JMLR 7:551–585.",
    "year": 2006
  }, {
    "title": "Probabilistic frame-semantic parsing",
    "authors": ["Dipanjan Das", "Nathan Schneider", "Desai Chen", "Noah A. Smith."],
    "venue": "Proc. of NAACL.",
    "year": 2010
  }, {
    "title": "Frame semantics",
    "authors": ["Charles Fillmore."],
    "venue": "Linguistics in the morning calm pages 111–137.",
    "year": 1982
  }, {
    "title": "Semantic role labeling with neural network factors",
    "authors": ["Nicholas FitzGerald", "Oscar Täckström", "Kuzman Ganchev", "Dipanjan Das."],
    "venue": "Proc. of EMNLP.",
    "year": 2015
  }, {
    "title": "A discriminative graph-based parser for the abstract meaning representation",
    "authors": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."],
    "venue": "Proc. ACL.",
    "year": 2014
  }, {
    "title": "DeepBank: A dynamically annotated treebank of the Wall Street Journal",
    "authors": ["Daniel Flickinger", "Yi Zhang", "Valia Kordoni."],
    "venue": "Proc. of TLT . pages 85–96.",
    "year": 2012
  }, {
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "authors": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani."],
    "venue": "Biostatistics 9(3):432–441.",
    "year": 2008
  }, {
    "title": "Automatic labeling of semantic roles",
    "authors": ["Daniel Gildea", "Daniel Jurafsky."],
    "venue": "Computational Linguistics 28(3):245–288.",
    "year": 2002
  }, {
    "title": "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence",
    "authors": ["Alex Graves."],
    "venue": "Springer.",
    "year": 2012
  }, {
    "title": "Out-of-domain FrameNet semantic role labeling",
    "authors": ["Silvana Hartmann", "Ilia Kuznetsov", "Teresa Martin", "Iryna Gurevych."],
    "venue": "Proc. of EACL.",
    "year": 2017
  }, {
    "title": "Deep semantic role labeling: What works and whats next",
    "authors": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proc. of ACL.",
    "year": 2017
  }, {
    "title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
    "authors": ["Luheng He", "Mike Lewis", "Luke S. Zettlemoyer."],
    "venue": "Proc. of EMNLP.",
    "year": 2015
  }, {
    "title": "Semantic frame identification with distributed word representations",
    "authors": ["Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev."],
    "venue": "Proc. of ACL.",
    "year": 2014
  }, {
    "title": "Training products of experts by minimizing contrastive divergence",
    "authors": ["Geoffrey E. Hinton."],
    "venue": "Neural Computation 14(8):1771–1800.",
    "year": 2002
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "Proc. of ACL.",
    "year": 2015
  }, {
    "title": "LTH: Semantic structure extraction using nonprojective dependency trees",
    "authors": ["Richard Johansson", "Pierre Nugues."],
    "venue": "Proc. of SemEval.",
    "year": 2007
  }, {
    "title": "Segmental recurrent neural networks",
    "authors": ["Lingpeng Kong", "Chris Dyer", "Noah A. Smith."],
    "venue": "Proc. of ICLR.",
    "year": 2016
  }, {
    "title": "Frame-semantic role labeling with heterogeneous annotations",
    "authors": ["Meghana Kshirsagar", "Sam Thomson", "Nathan Schneider", "Jaime Carbonell", "Noah A. Smith", "Chris Dyer."],
    "venue": "Proc. ACL.",
    "year": 2015
  }, {
    "title": "End-to-end neural coreference resolution",
    "authors": ["Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proc. of EMNLP.",
    "year": 2017
  }, {
    "title": "Low-rank tensors for scoring dependency structures",
    "authors": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Proc. ACL.",
    "year": 2014
  }, {
    "title": "Joint arc-factored parsing of syntactic and semantic dependencies",
    "authors": ["Xavier Lluı́s", "Xavier Carreras", "Lluı́s Màrquez"],
    "venue": "TACL",
    "year": 2013
  }, {
    "title": "Priberam: A turbo semantic parser with second order features",
    "authors": ["André F.T. Martins", "Mariana S.C. Almeida."],
    "venue": "Proc. of SemEval.",
    "year": 2014
  }, {
    "title": "Dual decomposition with many overlapping components",
    "authors": ["André F.T. Martins", "Noah A. Smith", "Pedro M.Q. Aguiar", "Mário A.T. Figueiredo."],
    "venue": "Proc. of EMNLP.",
    "year": 2011
  }, {
    "title": "Improving NLP through marginalization of hidden syntactic structure",
    "authors": ["Jason Naradowsky", "Sebastian Riedel", "David A. Smith."],
    "venue": "Proc. EMNLP.",
    "year": 2012
  }, {
    "title": "DyNet: The dynamic neural network toolkit",
    "authors": ["Ji", "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin"],
    "year": 2017
  }, {
    "title": "SemEval 2015 task 18: Broad-coverage semantic dependency parsing",
    "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinkova", "Dan Flickinger", "Jan Hajic", "Zdenka Uresova."],
    "venue": "Proc. of SemEval.",
    "year": 2015
  }, {
    "title": "Towards comparability of linguistic graph banks for semantic parsing",
    "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinková", "Dan Flickinger", "Jan Hajič", "Angelina Ivanova", "Zdeňka Urešová."],
    "venue": "Proc. of LREC.",
    "year": 2016
  }, {
    "title": "SemEval 2014 task 8: Broad-coverage semantic dependency parsing",
    "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajic", "Angelina Ivanova", "Yi Zhang."],
    "venue": "Proc. of SemEval.",
    "year": 2014
  }, {
    "title": "The proposition bank: An annotated corpus of semantic roles",
    "authors": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."],
    "venue": "Computional Linguistics 31(1):71– 106.",
    "year": 2005
  }, {
    "title": "Deep multitask learning for semantic dependency parsing",
    "authors": ["Hao Peng", "Sam Thomson", "Noah A. Smith."],
    "venue": "Proc. of ACL.",
    "year": 2017
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proc. of EMNLP.",
    "year": 2014
  }, {
    "title": "Because Syntax does Matter: Improving Predicate-Argument Structures Parsing Using Syntactic Features",
    "authors": ["Corentin Ribeyre", "Éric Villemonte De La Clergerie", "Djamé Seddah."],
    "venue": "Proc. of NAACL.",
    "year": 2015
  }, {
    "title": "Improving frame semantic parsing via dependency path embeddings",
    "authors": ["Michael Roth."],
    "venue": "Book of Abstracts of the 9th International Conference on Construction Grammar.",
    "year": 2016
  }, {
    "title": "Using semantic roles to improve question answering",
    "authors": ["Dan Shen", "Mirella Lapata."],
    "venue": "Proc. of EMNLP-CoNLL.",
    "year": 2007
  }, {
    "title": "Deep multi-task learning with low level tasks supervised at lower layers",
    "authors": ["Anders Søgaard", "Yoav Goldberg."],
    "venue": "Proc. of ACL.",
    "year": 2016
  }, {
    "title": "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies",
    "authors": ["Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Lluı́s Màrquez", "Joakim Nivre"],
    "venue": "In Proc. of CoNLL",
    "year": 2008
  }, {
    "title": "Frame-semantic parsing with softmax-margin segmental RNNs and a syntactic scaffold",
    "authors": ["Swabha Swayamdipta", "Sam Thomson", "Chris Dyer", "Noah A. Smith."],
    "venue": "arXiv:1706.09528.",
    "year": 2017
  }, {
    "title": "Efficient inference and structured learning for semantic role labeling",
    "authors": ["Oscar Täckström", "Kuzman Ganchev", "Dipanjan Das."],
    "venue": "TACL 3:29–41.",
    "year": 2015
  }, {
    "title": "A global joint model for semantic role labeling",
    "authors": ["Kristina Toutanova", "Aria Haghighi", "Christopher D. Manning."],
    "venue": "Computational Linguistics 34(2):161– 191.",
    "year": 2008
  }, {
    "title": "Support vector machine learning for interdependent and structured output spaces",
    "authors": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun."],
    "venue": "Proc. of ICML.",
    "year": 2004
  }, {
    "title": "A joint sequential and relational model for frame-semantic parsing",
    "authors": ["Bishan Yang", "Tom Mitchell."],
    "venue": "Proc. of EMNLP.",
    "year": 2017
  }, {
    "title": "Learning structural SVMs with latent variables",
    "authors": ["Chun-Nam John Yu", "Thorsten Joachims."],
    "venue": "Proc. of ICML.",
    "year": 2009
  }, {
    "title": "Model selection and estimation in the gaussian graphical model",
    "authors": ["Ming Yuan", "Yi Lin."],
    "venue": "Biometrika 94(1):19–35.",
    "year": 2007
  }],
  "id": "SP:f7f2117d09d294201982d3eff6c959bf2982d96e",
  "authors": [{
    "name": "Hao Peng",
    "affiliations": []
  }, {
    "name": "Sam Thomson",
    "affiliations": []
  }, {
    "name": "Swabha Swayamdipta",
    "affiliations": []
  }, {
    "name": "Noah A. Smith",
    "affiliations": []
  }, {
    "name": "Paul G. Allen",
    "affiliations": []
  }],
  "abstractText": "We present a new approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap. We handle such “disjoint” data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly. Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser.",
  "title": "Learning Joint Semantic Parsers from Disjoint Data"
}