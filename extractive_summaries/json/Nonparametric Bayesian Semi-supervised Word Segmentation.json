{
  "sections": [{
    "text": "Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semisupervised framework (JESS-CM; Suzuki and Isozaki (2008)). We confirmed that it can appropriately segment non-standard texts like those in Twitter and Weibo and has nearly state-of-the-art accuracy on standard datasets in Japanese, Chinese, and Thai."
  }, {
    "heading": "1 Introduction",
    "text": "For any unsegmented language, especially East Asian languages such as Chinese, Japanese and Thai, word segmentation is almost an inevitable first step in natural language processing. In fact, it is becoming increasingly important lately because of the growing interest in processing user-generated media, such as Twitter and blogs. Texts in such media are often written in a colloquial style that contains many new words and expressions that are not present in any existing dictionaries. Since such words are theoretically infinite in number, we need to leverage unsupervised learning to automatically identify them in corpora.\nFor this purpose, ordinary supervised learning is clearly unsatisfactory; even hand-crafted dictionar-\nies will not suffice because functional expressions more complex than simple nouns need to be recognized through their relationship with other words in text, which also might be unknown in advance. Previous studies of this issue used character and word information in the framework of supervised learning (Kruengkrai et al., 2009; Sun et al., 2009; Sun and Xu, 2011). However, they\n(1) did not explicitly model new words, or (2) did not give a seamless combination with dis-\ncriminative classifiers (e.g., they just used a threshold to discriminate between known and unknown words).\nIn contrast, unsupervised word segmentation methods (Goldwater et al., 2006; Mochihashi et al., 2009) use nonparametric Bayesian generative models for word generation to infer the “words” only from observations of raw input strings. These methods work quite well and have been used not only for tokenization but also for machine translation (Nguyen et al., 2010), speech recognition (Lee and Glass, 2012; Heymann et al., 2014), and even robotics (Nakamura et al., 2014).\nHowever, from a practical point of view, such purely unsupervised approaches do not suffice. Since they only aim to maximize the probability of the language model on the observed set of strings, they sometimes yield word segmentations that are\n179\nTransactions of the Association for Computational Linguistics, vol. 5, pp. 179–189, 2017. Action Editor: Masaaki Nagata. Submission batch: 10/2016; Revision batch: 12/2016; Published 6/2017.\nc©2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\ndifferent from human standards on low frequency words.\nTo solve this problem, this paper describes a novel combination of a nonparametric Bayesian generative model (NPYLM; Mochihashi et al. (2009)) and a discriminative classifier (CRF; Lafferty et al. (2001)). This combination is based on a semisupervised framework called JESS-CM (Suzuki and Isozaki, 2008), and it requires a nontrivial exchange of information between these two models. In this approach, the generative and discriminative models will “teach each other” and yield a novel log-linear model for word segmentation.\nExperiments on standard datasets of Chinese, Japanese, and Thai indicate that this hybrid model achieves nearly state-of-the-art accuracy on standard corpora, and, thanks to our nonparametric Bayesian model of infinite vocabulary it can accurately segment non-standard texts like those in Twitter and Weibo (the Chinese equivalent of Twitter) without any human intervention.\nThis paper is organized as follows. Section 2 introduces NPYLM which will be leveraged in the framework of JESS-CM, described in Section 3. Section 4 introduces our model, NPYCRF, and the necessary exchange of information, while Section 5 is devoted to experiments on datasets in Chinese, Japanese, and Thai. We analyze the results and discuss future directions of research on semisupervised learning in Section 6 and conclude in Section 7."
  }, {
    "heading": "2 Unsupervised Word Segmentation",
    "text": "To acquire new words from an observation consisting of raw strings, a generative model of words can be extremely useful for word segmentation. Goldwater et al. (2006) showed that a bigram hierarchical Dirichlet process (HDP) model based on Gibbs sampling can effectively find “words” in small corpora. In extending this work, Mochihashi et al. (2009) proposed a nested Pitman-Yor language model (NPYLM), a hierarchical Bayesian language model, where character n-grams (actually, ∞-grams (Mochihashi and Sumita, 2008)) are embedded in word n-grams, and an efficient dynamic programming algorithm for inference exists. Conceptually, NPYLM posits that an infinite number of spellings,\ni.e., “words”, are probabilistically generated from character n-grams, and a word unigram is drawn using the character n-grams as the base measure. Then bigram and trigram distributions are hierarchically generated and the final string is yielded from the “word” n-grams, as shown in Figure 2.\nPractically, NPYLM can be considered as a hierarchical smoothing of the Bayesian n-gram language model, HPYLM (Teh, 2006). In HPYLM, the predictive distribution of a word w=wt given a history h=wt−(n−1) · · ·wt−1 is expressed as\np(w|h) = c(w|h)−d·thw θ+c(h) + θ+d·th · θ+c(h) ·p(w|h′) (1)\nwhere c(w|h) denotes the observed counts, θ and d are model parameters, and thw and th·= ∑ w thw are latent variables estimated in the model. The probability of w given h is recursively interpolated using a shorter history h′ = wt−(n−2) · · ·wt−1. If h is already empty at the unigram level, NPYLM employs a back-off distribution using character n-grams for p(w|h′):\np0(w) = p(c1 · · · ck) (2) = ∏k i=1 p(ci|c1 · · · ci−1) . (3)\nIn this way, NPYLM can assign appropriate probabilities to every possible sequence of segmentation and learn the word and character n-grams at the same time by using a single generative model (Mochihashi et al., 2009).\nSemi-Markov view of NPYLM NPYLM formulates unsupervised word segmentation as learning with a semi-Markov model (Figure 3). Here, each\nnode corresponds to an inside probability α[t][k]1 that equals the probability of a substring ct1 = c1 · · · ct with the last k characters ctt−k+1 being a word. This inside probability can be computed recursively as follows:\nα[t][k] =\nL∑\nj=1\np(ctt−k+1|ct−kt−k−j+1) · α[t−k][j] (4)\nHere, 1≤L≤ t−k is the maximum allowed length of a word. With these inside probabilities, we can make use of Markov Chain Monte Carlo (MCMC) method with an efficient forward filtering-backward sampling algorithm (Scott, 2002), namely a “stochastic Viterbi” algorithm to iteratively sample “words” from raw strings in a completely unsupervised fashion, while avoiding local minima.\nProblems and Beyond Unsupervised word segmentation with NPYLM works surprisingly well for many languages (Mochihashi et al., 2009); however, it has certain issues. First, since it optimizes the performance of the language model, its segmentation does not always conform to human standards and depends on subtle modeling decisions. For example, NPYLM often separates inflectional suffixes in Japanese like “る” in “見–る” from the rest of the verb, when it is actually a part of the verb itself. Second, it can produce deficient segmentations for low-frequency words and the beginning or ending of a string where the available information comes from only one direction. These issues can be alleviated by using naı̈ve semi-supervised learning method (Mochihashi et al., 2009) that simply\n1While we consider only bigrams in this paper for simplicity, the theory can be naturally extended to higher-order ngrams. However, it requires quite a complicated implementation, and the expected gain in performance will not be large, even if we use trigrams (Mochihashi et al., 2009).\nadds n-gram counts from supervised segmentations in advance. However, this solution is not perfect because these supervised counts will eventually be overwhelmed by the unsupervised counts, because the overall objective function remains unsupervised.\nTo resolve this issue, we must resort to an explicit semi-supervised learning framework that combines both discriminative and generative models. We used JESS-CM (Suzuki and Isozaki, 2008), currently the best such framework for this purpose, which we will briefly introduce below."
  }, {
    "heading": "3 Integration with a Discriminative Model",
    "text": "JESS-CM (Joint probability model Embedding style Semi-Supervised Conditional Model) is a semisupervised learning framework that outperforms other generative and log-linear models (Druck and McCallum, 2010). In JESS-CM, the probability of a label sequence y given an input sequence x is written as follows:\np(y|x) ∝ pDISC(y|x; Λ) pGEN(y,x; Θ)λ0 (5) where pDISC and pGEN are respectively the discriminative and generative models, and Λ and Θ are their corresponding parameters. Equation (5) is the product of the experts, where each expert works as a “constraint” to the other with a relative geometrical interpolation weight 1 :λ0. If we take pDISC to be a log-linear model like CRF (Lafferty et al., 2001):\npDISC(y|x) ∝ exp (∑K k=1 λkfk(y,x) ) , (6)\nEquation (5) can be also expressed as a loglinear model with a new “feature function” log pGEN(y,x): p(y|x) ∝ exp ( λ0 log pGEN(y,x) + ∑K k=1 λkfk(y,x) )\n= exp (Λ · F (y,x)) . (7)\nHere, the parameter Λ = (λ0, λ1, · · · , λK) includes the interpolation weight λ0 and F (y,x)=(log pGEN(y,x), f1(y,x), · · · , fK(y,x)).\nJESS-CM interleaves the optimization of Λ and Θ to maximize the objective function p(Yl,Xu|Xl; Λ,Θ) = p(Yl|Xl; Λ) · p(Xu; Θ) (8)\nwhere 〈Xl, Yl〉 is the labeled dataset and Xu is the unlabeled dataset.\nSuzuki and Isozaki (2008) conducted semisupervised learning on a combination of a CRF and an HMM, as shown in Figure 4. Since CRF and HMM have the same Markov model structure, they interpolate two weights\n∑K k=1 λkfk(yt, yt−1,x) and (9)\nλ0 log pGEN(yt|yt−1,x) (10) on the corresponding path, altenately\n• fixing Θ and optimizing Λ of CRF on 〈Xl, Yl〉, and\n• fixing Λ and optimizing Θ of HMM on Xu until convergence, and thereby iteratively maximizing the two terms in (8).\nThrough this optimization, pDISC and pGEN will “teach each other” to make the feature log pGEN more accurate, and further rectified by pDISC with respect to the labeled data. Note that the interpolation weight λ0 is automatically computed through this process."
  }, {
    "heading": "4 Connecting Two Worlds: NPYCRF",
    "text": "We wish to integrate NPYLM and CRF, applying semi-supervised learning via JESS-CM. Note that Suzuki and Isozaki (2008) implicitly assumed that the discriminative and generative models have the same structure as shown in Figure 4. Since NPYLM is a semi-Markov model as described in Section 2, a naı̈ve approach would be to combine it with a semiMarkov CRF (Sarawagi and Cohen, 2005) as the discriminative model.\nHowever, this strategy does not work well for two reasons: First, since a semi-Markov CRF is a model for transitions between segments, it cannot deal with character-level transitions and thus performs suboptimally on its own. In fact, our preliminary supervised word segmentation experiments showed a F1 measure of around 95%, whereas a\ncharacter-wise Markov CRF achieves >99%. Second, the semi-Markov CRF was originally designed to chunk at most a few words (Sarawagi and Cohen, 2005). However, in word segmentation of Japanese, for example, we often encounter long proper nouns or Katakana sequences that are more than ten characters, requiring a huge amount of memory even for a small dataset.\nIn this paper we instead transparently exchange information between the Markov model (CRF) on characters and the semi-Markov model (NPYLM) on words to perform a semi-supervised learning on different model structures. Called NPYCRF, this unified statistical model makes good use of the discriminative model (CRF) from the labeled data and the generative model (NPYLM) from the unlabeled data."
  }, {
    "heading": "4.1 CRF→NPYLM",
    "text": "To convert from a CRF to NPYLM, we can easily translate Markov potentials into semi-Markov potentials as shown in Andrew (2006) for the supervised learning case.\nConsider the situation depicted in Figure 5. Here we can see that the potential of the substring “東京 都” (Tokyo prefecture) in the semi-Markov model (left) corresponds to the sum of the potentials in the Markov model (right) along the path shown in bold. Here, we introduce binary hidden states in the Markov model for each character, similarly to the BI tags used in supervised learning, where state 1 represents the beginning of a word and state 0 represents a continuation of the word.\nMathematically, we define γ[a, b) as the sum of the potentials along a U-shaped path over an interval [a, b) (a<b) as shown in Figure 5, which begins with state 1 and ends with (but does not include) 1.\nUsing this notation, the potential that corresponds to α[t][k] is γ[t− k+1, t + 1) covering ct−k+1 · · · ct, and thus the forward recursion of the inside probability α[t][k] that incorporates the information from the CRF can be written as follows, instead of (4):\nα[t][k] =\nL∑\nj=1\nexp [ λ0 log p(c t t−k+1|ct−kt−k−j+1) + γ[t−k+1, t+1) ] · α[t−k][j]. (11)\nBackward sampling can be performed in a similar fashion. In this way, we can incorporate information from the character-wise discriminative model (CRF) into the language model segmentation of NPYLM."
  }, {
    "heading": "4.2 NPYLM→CRF",
    "text": "On the other hand, translating the information from the semi-Markov to Markov model, i.e., translating a potential from the word-based language model into the character-wise discriminative classifier, is not trivial. However, as we describe below, it is actually possible to do so by extending the technique proposed in Andrew (2006).\nNote that for the inference of CRF, from the standard theory of log-linear models we only have to compute its gradient with respect to the expectation of each feature in the current model. This reduces the problem to a computation of the marginal probability of each path, which can be derived within the framework of semi-Markov models as follows: Semi-Markov feature λ0 . Following the line of argument presented in the Section 4.1, the potential with respect to the semi-Markov feature weight λ0 that is associated with the word transition ct−kt−k−j+1 → ctt−k+1, shown in Figure 6, can be expressed as an expectation using the standard forward-backward formula:\np(ctt−k+1, c t−k t−k−j+1|s) = α[t−k][j]β[t][k] ·\nexp [ λ0 log p(c t t−k+1|ct−kt−k−j+1) + γ[t−k+1, t+1) ]\n/Z(s) (12)\nHere, Z(s) is a normalizing constant associated with each input string s, and β[t][k] is a backward proba-\nbility similar to (11) computed by\nβ[t][k] =\nL∑\nj=1\nexp [ λ0 log p(c t+j t+1|ctt−k+1) γ[t+1, t+j+2) ] · β[t+j][j] . (13)\nMarkov features λ1, · · · , λK . Note that the features associated with label bigrams in our binary CRF can be divided into four types: 1-1,1-0,0-1, and 0-0, as shown in Figure 7.\nCase 1-1: As shown in Figure 8(a), this case means that a word of length 1 begins at time t, which is equivalent to the probability of substring ctt being a word:\np(zt=1, zt+1=1|s) = p(ctt|s). (14) Here, p(ckℓ |s) is the marginal probability of a substring cℓ · · · ck being a word, which can be derived from equation (12):\np(ckℓ |s) = ∑\nj\np(ckℓ , c ℓ−1 ℓ−j |s)\n= ∑\nj\nα[ℓ−1][j] · β[k][k−ℓ+1] ·\nexp [ λ0 log p(c k ℓ |cℓ−1ℓ−j ) + γ[ℓ, k+1) ] /Z(s)\n= β[k][k−ℓ+1] Z(s) · ∑\nj\nexp [ λ0 log p(c k ℓ |cℓ−1ℓ−j )\n+ γ[ℓ, k+1) ] α[ℓ−1][j]\n= α[k][k−ℓ+1] · β[k][k−ℓ+1]\nZ(s) (15)\nCase 1-0: As shown in Figure 8(b), this case means that a word begins at time t and has length at least 2. Since we do not know the endpoint of this word, we can obtain the probability p(zt = 1, zt+1=0) by marginalizing over the endpoint j (· · · means values all 0):\nwhere p(ct+j−1t |s) is obtained from (15).\nCase 0-1: Similarly, as shown in Figure 8(c) this case means that a word of length at least 2 begins before time t and ends at time t. Therefore, we can marginalize over the start point of a possible word to obtain the marginal probability:\np(zt=0, zt+1=1|s) = ∑\nj=1\np(zt−j=1, · · · , zt=0, zt+1=1|s) (17)\n= ∑\nj=1\np(ctt−j |s) . (18)\nCase 0-0: In principle, this means that a word begins before time t and ends later than (and including) time t+1. Therefore, we can marginalize over both the start and end time of a possible word spanning [t, t+1] to obtain:\np(zt=0, zt+1=0|s) = ∑\nj=1\n∑\nk=1\np(ct+kt−j |s) . (19)\nHowever, in fact we can avoid this nested computation because the probability of p(zt, zt+1) over the possible values of zt and zt+1 must sum to 1. We can therefore simply calculate it as follows (Andrew, 2006):\np(zt=0, zt+1=0|s) = 1−p(1, 1)−p(1, 0)−p(0, 1) (20)\nwhere p(x, y) means p(zt=x, zt+1=y|s)."
  }, {
    "heading": "4.3 Inference",
    "text": "Finally, we obtain the inference algorithm for NPYCRF as a variant of the MCMC-EM algorithm (Wei\nand Tanner, 1990) shown in Figure 9.2 In learning of a NPYLM, we add the CRF potentials as described in Section 4.1, and sample a possible segmentation from the posterior through Forward filteringBackward sampling to update the model parameters. On the basis of this improved language model, the CRF weights are then optimized by incorporating language model features as explained in Section 4.2. We iterate this process until convergence.\nNote that we first have to learn an unsupervised segmentation in Step 2 before training the CRF. Since our inference algorithm includes an optimization of CRF and thus is not a true MCMC, the learning of word segmentation after the supervised information will be severely constrained and likely to get stuck in local minima.\nIn practice, we found that the EM-style batch learning of CRF described above often fails because our objective function is non-convex. Therefore, we switched to ADF below (Sun et al., 2014), an adaptive stochastic gradient descent that yields state-ofthe-art accuracies for natural language processing problems including word segmentation. In this case, Λ in Figure 9 was optimized with each minibatch through the labeled data 〈Xl, Yl〉, while incorporating information from the unlabeled data Xu by the language model.\nBecause of its heavy computational demands,\n1: Add 〈Yl, Xl〉 to NPYLM. 2: Optimize Λ on 〈Yl, Xl〉. (pure CRF) 3: for j = 1 · · ·M do 4: for i = randperm(1 · · ·N) do 5: if j > 1 then 6: Remove customers of X(i)u from NPYLM Θ 7: end if 8: Draw segmentations of X(i)u from NPYCRF 9: Add customers of X(i)u to NPYLM Θ 10: end for 11: Optimize Λ of NPYCRF on 〈Yl, Xl〉. 12: end for\nFigure 9: Basic learning algorithm for NPYCRF. X(i)u denotes the i-th sentence in the unlabeled data Xu. We can also iterate steps 4 to 10 several times until Θ approximately converges, before updating Λ.\n2It is possible to fix NPYLM and just use this as a feature to CRF: this amounts to running only the first iteration (j =1) of the EM algorithm. However, it still requires NPYLM→CRF conversion in Section 4.2, and we found that the performance is not optimal while slightly better than plain CRF.\nwe parallelized the NPYLM sampling over several processors and because of the possible correlation of segmentations within the samples, used the Metropolis-Hastings algorithm to correct them. The acceptance rate in our experiments was over 99%.\nFor decoding, we can simply find a Viterbi path in the integrated semi-Markov model while fixing all the sampled segmentations on the unlabeled data."
  }, {
    "heading": "5 Experiments",
    "text": "We conducted experiments on several corpora of unsegmented languages: Japanese, Chinese, and Thai. The corpora included standard corpora as well as text from Twitter and its equivalent, Weibo, in Chinese."
  }, {
    "heading": "5.1 Data",
    "text": "Chinese For Chinese, we first used a standard dataset from the SIGHAN Bakeoff 2005 (Emerson, 2005) for the labeled and test data, and Chinese gigaword version 2 (LDC2009T14) for the unlabeled data. We chose the MSR subset of SIGHAN Bakeoff written in simplified Chinese together with the provided training and test splits, which contain about 87K/40K sentences, respectively. For the unlabeled data, i.e., a collection of raw strings, we used a random subset of 880K sentences from Chinese gigaword with all spaces removed. We chose this size to be about 10 times larger than the labeled data, considering current computational requirements. We used the part from the Xinhua news agency 2004 and split the data into sentences at the end-of-sentence character “。”.\nBecause the MSR and Xinhua datasets were compiled from newspapers, to meet our objective on informal text we conducted further experiments using\n3This is the total number of sentences in the experiment: the actual number of unsupervised sentences is this set minus the different number of supervised sentences.\nthe Leiden Weibo corpus4 from Weibo, a Twitter equivalent in China. From this dataset, we used the sentences that have exact correspondence between the provided segmented-unsegmented pair, yielding about 880K sentences. Since we did not know how much supervision would be necessary for a decent performance, we conducted experiments with different amounts of labeled data: 10K, 20K, 40K and 880K(all). Note that the final case amounts to complete supervision, an ideal situation that is not likely in practice.\nJapanese Word segmentation accuracies around 99% have already been reported for newspaper domains in Japanese (Kudo et al., 2004). Therefore, we only conducted experiments on segmenting Twitter text. In addition to our random Twitter crawl in April 2014, we used a corpus of Japanese Twitter text compiled by the Tokyo Metropolitan University5. This corpus is actually very small, 944 sentences. It mainly targets transfer learning and is segmented according to BCCWJ (Basic Corpus of Contemporary\n4http://www.leidenweibocorpus.nl/openaccess.php 5https://github.com/tmu-nlp/TwitterCorpus\nWritten Japanese) standards from the National Institute of Japanese Language (Maekawa, 2007). Therefore, for the labeled data we used the “core” subset of BCCWJ consisting of about 59K sentences plus 500 random sentences from the Twitter dataset. We used the remaining 444 sentences for testing. For the unlabeled data, we used a random crawl of 600K Japanese sentences collected from Twitter in MarchApril, 2014.\nThai Unsegmented languages, such as Thai, Lao, Myanmar, and Kumer, are also prevalent in South East Asia and are becoming increasingly important targets of natural language processing. Thus we also conducted an experiment on Thai, using the standard InterBEST 2009 dataset (Kosawat, 2009). Since it is reported that the “novel” subset of InterBEST has relatively low precision, we used this part with a random split of 10K sentences for supervised learning, 30K sentences for unsupervised learning, and a further 10K sentences for testing."
  }, {
    "heading": "5.2 Training Settings",
    "text": "Because Sun et al. (2012) report increased accuracy with three tags, {B,I,E}6, we also tried these tags in place of the binary tags described in Section 4.2. This modification resulted in 6 possible transitions out of 32 = 9 transitions, whose computation follows from the binary case in Section 4.2. We used normal priors of truncated N(1, σ2) and N(0, σ2) for λ0 and λ1 · · ·λK , respectively, and fixed the CRF regularization parameter C to 1.0, and σ to 1.0 by preliminary experiments on the same data.\nFor the feature templates, we followed Sun et al. (2012). In addition to those templates, we used character type bigrams, where the ‘character type’ was defined by Unicode blocks (like Hiragana or CJK Unified Ideographs for Chinese and Japanese) or Unicode character categories (Thai).\nTo reduce computations by restricting the search space appropriately, we employed a Negative Binomial generalized linear model on string features (Uchiumi et al., 2015) to predict the maximum length of a possible word for each character position in the training data. Therefore, the upper limit of L in (11) and (13) was Lt for each position t, obtained\n6The B, I, and E tags mean the beginning, internal part, and end of a word, respectively.\nfrom this statistical model trained on labeled segmentations. We observed that this prediction made the computation several times faster than, for example, using a fixed threshold in Japanese where quite long words are occasionally encountered."
  }, {
    "heading": "5.3 Experimental results",
    "text": "Chinese Tables 2 and 3 show IV (in-vocabulary) and OOV (out-of-vocabulary) precision and Fmeasure, computed against segmented tokens. The results for standard newspaper text indicate that NPYCRF is basically comparable in performance to state-of-the-art supervised neural networks (Chen et al., 2015; Zhang et al., 2016) that require hand tuning of hyperparameters or model architectures. Figure 10 shows some of the learned words in the testset of the Bakeoff MSR corpus. As shown in Table 3, NPYCRF also yields higher precision than supervised learning on non-standard text like Weibo, which is the main objective for this study. Contrary to ordinary supervised learning, we can see that NPYCRF effectively learns many “new words” from the large amount of unlabeled data thanks to the generative model, while observing human standards of segmentation by the discriminative model. Note that in Weibo segmentation, complete supervision is not\navailable in practice. In fact, we realized that the Weibo segmentations were given automatically by an existing classifier, and contain many inappropriate segmentations, while NPYCRF finds much “better” segmentations.\nFigure 11 compares the results of CRF, NPYLM, and NPYCRF with the gold segmentation. While proverbs like “眼高手低” (wide vision without action) are correctly captured from the unlabeled data by NPYLM, it is sometimes broken by CRF through integration. In another case, the name of a person is properly connected because of the information provided by the CRF. This comparison shows that there is still room for improvement in NPYCRF. Section 6 discusses future research directions for improvements. Japanese and Thai Figure 12 shows an example of the analysis of Japanese Twitter text. Shaded words are those that are not contained in labeled data (BCCWJ core) but were found by NPYCRF. Many segmentations, including new words, are correct. We expect NPYCRF would perform better with more unlabeled data that are easily obtained.\nTables 4 and 5 show the segmentation accuracies of the Twitter data in Japanese and novel data in\nThai. While there are no publicly available results for these data (the InterBEST testset is closed during competition), NPYCRF achieved better accuracies than vanilla supervised segmentation based on CRF. Considering that many new words were found in Figure 12, for example, we believe NPYCRF is quite competitive thanks to its ability to learn the infinite vocabulary, which it inherits from NPYLM."
  }, {
    "heading": "6 Analysis",
    "text": "As shown in Figure 11, NPYCRF makes good use of NPYLM but sometimes ignores its prediction by falling back to CRF, yielding suboptimal performance. This is mainly because the geometric interpolation weight λ0 is always constant and does not vary according to the input. For example, even if the substring to segment is very rare in the labeled data, NPYCRF trusts the supervised classifier (CRF) with a constant rate of 1/(1+λ0) in the log probability domain. To alleviate this problem,\nit is necessary to change λ0 depending on the input string in a log-linear framework.7 While this might be achieved through Density Ratio estimation framework (Sugiyama et al., 2012; Tsuboi et al., 2009), we believe it is a general problem of semisupervised learning and is beyond the scope of this paper.\nThis issue also affects the estimation of λ0 as a scalar: that is, we found that λ0 often fluctuates during training because Λ (which includes λ0) is estimated using only limited 〈Xl, Yl〉. In practice, we terminated the EM algorithm in Figure 9 early after a few iterations. Therefore, with a more adaptive semi-supervised learning framework, we expect that NPYCRF will achieve higher accuracy than the current performance."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we presented a hybrid generative/discriminative model of word segmentation, leveraging a nonparametric Bayesian model for unsupervised segmentation. By combining CRF and NPYLM within the semi-supervised framework of JESS-CM, our NPYCRF not only works as well as the state-of-the-art neural segmentation without hand tuning of hyperparameters on standard corpora, but also appropriately segments non-standard texts found in Twitter and Weibo, for example, by automatically finding “new words” thanks to a nonparametric model of infinite vocabulary.\nWe believe that our model lays the foundation for developing a methodology of combining nonparametric Bayesian models and discriminative classifiers, as well as providing an example of semisupervised learning on different model structures, i.e. Markov and semi-Markov models for word segmentation."
  }, {
    "heading": "Acknowledgments",
    "text": "We are deeply grateful to Jun Suzuki (NTT CS Labs) for important discussions leading to this research, Xu Sun (Peking University) for details of his experiments in Chinese. We would also like to thank anonymous reviewers and the action editor,\n7This is reminiscent of context-dependent Bayesian smoothing of MacKay (1994) in the probability domain, as opposed to the fixed Jelinek-Mercer smoothing (Goodman, 2001).\nespecially the editors-in-chief for the thorough comments for the final manuscript."
  }],
  "year": 2017,
  "references": [{
    "title": "A Hybrid Markov/Semi-Markov Conditional Random Field for Sequence Segmentation",
    "authors": ["Galen Andrew."],
    "venue": "EMNLP 2006, pages 465–472.",
    "year": 2006
  }, {
    "title": "Gated Recursive Neural Network for Chinese Word Segmentation",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."],
    "venue": "ACL 2015, pages 1744–1753.",
    "year": 2015
  }, {
    "title": "HighPerformance Semi-Supervised Learning using Discriminatively Constrained Generative Models",
    "authors": ["Gregory Druck", "Andrew McCallum."],
    "venue": "ICML 2010, pages 319–326.",
    "year": 2010
  }, {
    "title": "The Second International Chinese Word Segmentation Bakeoff",
    "authors": ["Tom Emerson."],
    "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.",
    "year": 2005
  }, {
    "title": "Contextual Dependencies in Unsupervised Word Segmentation",
    "authors": ["Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson."],
    "venue": "Proceedings of ACL/COLING 2006, pages 673–680.",
    "year": 2006
  }, {
    "title": "A Bit of Progress in Language Modeling, Extended Version",
    "authors": ["Joshua T. Goodman."],
    "venue": "Technical Report MSR–TR–2001–72, Microsoft Research.",
    "year": 2001
  }, {
    "title": "Iterative Bayesian Word Segmentation for Unsupervised Vocabulary Discovery from Phoneme Lattices",
    "authors": ["Jahn Heymann", "Oliver Walter", "Reinhold Häb-Umbach", "Bhiksha Raj."],
    "venue": "ICASSP 2014, pages 4057–4061.",
    "year": 2014
  }, {
    "title": "InterBEST 2009: Thai Word Segmentation Workshop",
    "authors": ["Krit Kosawat."],
    "venue": "Proceedings of 2009 Eighth International Symposium on Natural Language Processing (SNLP2009), Thailand.",
    "year": 2009
  }, {
    "title": "A word and charactercluster hybrid model for Thai word segmentation",
    "authors": ["Canasai Kruengkrai", "Kiyotaka Uchimoto", "Junichi Kazama", "Kentaro Torisawa", "Hiroshi Isahara", "Chuleerat Jaruskulchai."],
    "venue": "Eighth International Symposium on Natural Lanugage",
    "year": 2009
  }, {
    "title": "Applying Conditional Random Fields to Japanese Morphological Analysis",
    "authors": ["Taku Kudo", "Kaoru Yamamoto", "Yuji Matsumoto."],
    "venue": "EMNLP 2004, pages 230–237.",
    "year": 2004
  }, {
    "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "Proc. of ICML 2001, pages 282–289.",
    "year": 2001
  }, {
    "title": "A Nonparametric Bayesian Approach to Acoustic Model Discovery",
    "authors": ["Chia-ying Lee", "James Glass."],
    "venue": "ACL 2012, pages 40–49.",
    "year": 2012
  }, {
    "title": "A Hierarchical Dirichlet Language Model",
    "authors": ["David J.C. MacKay", "L. Peto."],
    "venue": "Natural Language Engineering, 1(3):1–19.",
    "year": 1994
  }, {
    "title": "Kotonoha and BCCWJ: Development of a Balanced Corpus of Contemporary Written Japanese",
    "authors": ["Kikuo Maekawa."],
    "venue": "Corpora and Language Research: Proceedings of the First International Conference on Korean Language, Literature, and Culture, pages 158–",
    "year": 2007
  }, {
    "title": "The Infinite Markov Model",
    "authors": ["Daichi Mochihashi", "Eiichiro Sumita."],
    "venue": "Advances in Neural Information Processing Systems 20 (NIPS 2007), pages 1017– 1024.",
    "year": 2008
  }, {
    "title": "Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling",
    "authors": ["Daichi Mochihashi", "Takeshi Yamada", "Naonori Ueda."],
    "venue": "Proceedings of ACL-IJCNLP 2009, pages 100–108.",
    "year": 2009
  }, {
    "title": "Mutual Learning of an Object Concept and Language Model Based on MLDA and NPYLM",
    "authors": ["Tomoaki Nakamura", "Takayuki Nagai", "Kotaro Funakoshi", "Shogo Nagasaka", "Tadahiro Taniguchi", "Naoto Iwahashi."],
    "venue": "2014 IEEE/RSJ International Conference on Intel-",
    "year": 2014
  }, {
    "title": "Nonparametric Word Segmentation for Machine Translation",
    "authors": ["ThuyLinh Nguyen", "Stephan Vogel", "Noah A. Smith."],
    "venue": "COLING 2010, pages 815–823.",
    "year": 2010
  }, {
    "title": "SemiMarkov Conditional Random Fields for Information Extraction",
    "authors": ["Sunita Sarawagi", "William W. Cohen."],
    "venue": "Advances in Neural Information Processing Systems 17 (NIPS 2004), pages 1185–1192.",
    "year": 2005
  }, {
    "title": "Bayesian Methods for Hidden Markov Models",
    "authors": ["Steven L. Scott."],
    "venue": "Journal of the American Statistical Association, 97:337–351.",
    "year": 2002
  }, {
    "title": "Density Ratio Estimation in Machine Learning",
    "authors": ["Masashi Sugiyama", "Taiji Suzuki", "Takafumi Kanamori."],
    "venue": "Cambridge University Press.",
    "year": 2012
  }, {
    "title": "Enhancing Chinese Word Segmentation using Unlabeled Data",
    "authors": ["Weiwei Sun", "Jia Xu."],
    "venue": "EMNLP 2011, pages 970–979.",
    "year": 2011
  }, {
    "title": "A Discriminative Latent Variable Chinese Segmenter with Hybrid Word/Character Information",
    "authors": ["Xu Sun", "Yaozhong Zhang", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Jun’ichi Tsujii"],
    "venue": "NAACL",
    "year": 2009
  }, {
    "title": "Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection",
    "authors": ["Xu Sun", "Houfeng Wang", "Wenjie Li."],
    "venue": "ACL 2012, pages 253–262.",
    "year": 2012
  }, {
    "title": "Feature-Frequency-Adaptive Online Training for Fast and Accurate Natural Language Processing",
    "authors": ["Xu Sun", "Wenjie Li", "Houfeng Wang", "Qin Lu."],
    "venue": "Computational Linguistics, 40(3):563–586.",
    "year": 2014
  }, {
    "title": "Semi-Supervised Sequential Labeling and Segmentation Using GigaWord Scale Unlabeled Data",
    "authors": ["Jun Suzuki", "Hideki Isozaki."],
    "venue": "ACL:HLT 2008, pages 665–673.",
    "year": 2008
  }, {
    "title": "A Hierarchical Bayesian Language Model based on Pitman-Yor Processes",
    "authors": ["Yee Whye Teh."],
    "venue": "Proceedings of ACL/COLING 2006, pages 985–992.",
    "year": 2006
  }, {
    "title": "Direct Density Ratio Estimation for Large-scale Covariate Shift Adaptation",
    "authors": ["Yuta Tsuboi", "Hisashi Kashima", "Shohei Hido", "Steffen Bickel", "Masashi Sugiyama."],
    "venue": "Information and Media Technologies, 4(2):529–546.",
    "year": 2009
  }, {
    "title": "Inducing Word and Part-of-speech with Pitman-Yor Hidden Semi-Markov Models",
    "authors": ["Kei Uchiumi", "Hiroshi Tsukahara", "Daichi Mochihashi."],
    "venue": "ACLIJCNLP 2015, pages 1774–1782.",
    "year": 2015
  }, {
    "title": "A Monte Carlo Implementation of the EM Algorithm and the Poor Man’s Data Augmentation Algorithms",
    "authors": ["Greg C.G. Wei", "Martin A. Tanner."],
    "venue": "Journal of the American Statistical Association, 85(411):699– 704.",
    "year": 1990
  }, {
    "title": "Transition-Based Neural Word Segmentation",
    "authors": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."],
    "venue": "ACL 2016.",
    "year": 2016
  }],
  "id": "SP:8f9dcc15ea1f08533eba82bc2e4fcd94a7e7980a",
  "authors": [{
    "name": "Ryo Fujii",
    "affiliations": []
  }, {
    "name": "Ryo Domoto",
    "affiliations": []
  }, {
    "name": "Daichi Mochihashi",
    "affiliations": []
  }],
  "abstractText": "This paper presents a novel hybrid generative/discriminative model of word segmentation based on nonparametric Bayesian methods. Unlike ordinary discriminative word segmentation which relies only on labeled data, our semi-supervised model also leverages a huge amounts of unlabeled text to automatically learn new “words”, and further constrains them by using a labeled data to segment non-standard texts such as those found in social networking services. Specifically, our hybrid model combines a discriminative classifier (CRF; Lafferty et al. (2001) and unsupervised word segmentation (NPYLM; Mochihashi et al. (2009)), with a transparent exchange of information between these two model structures within the semisupervised framework (JESS-CM; Suzuki and Isozaki (2008)). We confirmed that it can appropriately segment non-standard texts like those in Twitter and Weibo and has nearly state-of-the-art accuracy on standard datasets in Japanese, Chinese, and Thai.",
  "title": "Nonparametric Bayesian Semi-supervised Word Segmentation"
}