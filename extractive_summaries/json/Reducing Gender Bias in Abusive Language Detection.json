{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2799"
  }, {
    "heading": "1 Introduction",
    "text": "Automatic detection of abusive language is an important task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination. As more and more people freely express their opinions in social media, the amount of textual contents produced every day grows almost exponentially, rendering it difficult to effectively moderate user content. For this reason, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.\nAlthough many works already tackled on training machine learning models to automatically detect abusive language, recent works have raised concerns about the robustness of those systems. Hosseini et al. (2017) have shown how to easily cause false predictions with adversarial examples in Google’s API, and Dixon et al. (2017) show that\nclassifiers can have unfair biases toward certain groups of people.\nWe focus on the fact that the representations of abusive language learned in only supervised learning setting may not be able to generalize well enough for practical use since they tend to overfit to certain words that are neutral but occur frequently in the training samples. To such classifiers, sentences like “You are a good woman” are considered “sexist” probably because of the word “woman.”\nThis phenomenon, called false positive bias, has been reported by Dixon et al. (2017). They further defined this model bias as unintended, “a model contains unintended bias if it performs better for comments containing some particular identity terms than for comments containing others.”\nSuch model bias is important but often unmeasurable in the usual experiment settings since the validation/test sets we use for evaluation are already biased. For this reason, we tackle the issue of measuring and mitigating unintended bias. Without achieving certain level of generalization ability, abusive language detection models may not be suitable for real-life situations.\nIn this work, we address model biases specific to gender identities (gender bias) existing in abusive language datasets by measuring them with a generated unbiased test set and propose three reduction methods: (1) debiased word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias."
  }, {
    "heading": "2 Related Work",
    "text": "So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd-\nsourcing or user moderation (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018; Wulczyn et al., 2017). Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system (Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). However, these works do not explicitly address any model bias in their models.\nAddressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. Bolukbasi et al. (2016) is one of the first works to point out the gender stereotypes inside word2vec (Mikolov et al., 2013) and propose an algorithm to correct them. Caliskan et al. (2017) also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. Dixon et al. (2017) is one of the first works that point out existing “unintended” bias in abusive language detection models. Kiritchenko and Mohammad (2018) compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems. Zhao et al. (2018) shows the effectiveness of measuring and correcting gender biases in coreference resolution tasks. We later show how we extend a few of these works into ours."
  }, {
    "heading": "3 Datasets",
    "text": "3.1 Sexist Tweets (st)\nThis dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as “feminazi.” The tweets were then annotated by experts based on criteria founded in critical race theory. The original dataset also contained a relatively small number of “racist” label tweets, but we only retain “sexist” samples to focus on gender biases. Waseem and Hovy (2016); Waseem (2016), the creators of the dataset, describe “sexist” and “racist” languages as specific subsets of abusive language.\n3.2 Abusive Tweets (abt)\nRecently, Founta et al. (2018) has published a large scale crowdsourced abusive tweet dataset with 60K tweets. Their work incrementally and iteratively investigated methods such as boosted sampling and exploratory rounds, to effectively annotate tweets through crowdsourcing. Through such systematic processes, they identify the most relevant label set in identifying abusive behaviors in Twitter as {None, Spam,Abusive,Hateful} resulting in 11% as ’Abusive,’ 7.5% as ’Hateful’, 22.5% as ’Spam’, and 59% as ’None’. We transform this dataset for a binary classification problem by concatenating ’None’/’Spam’ together, and ’Abusive’/’Hateful’ together."
  }, {
    "heading": "4 Measuring Gender Biases",
    "text": ""
  }, {
    "heading": "4.1 Methodology",
    "text": "Gender bias cannot be measured when evaluated on the original dataset as the test sets will follow the same biased distribution, so normal evaluation set will not suffice. Therefore, we generate a separate unbiased test set for each gender, male and female, using the identity term template method proposed in Dixon et al. (2017).\nThe intuition of this template method is that given a pair of sentences with only the identity terms different (ex. “He is happy” & “She is happy”), the model should be able to generalize well and output same prediction for abusive language. This kind of evaluation has also been performed in SemEval 2018: Task 1 Affect In Tweets (Kiritchenko and Mohammad, 2018) to measure the gender and race bias among the competing systems for sentiment/emotion analysis.\nUsing the released code1 of Dixon et al. (2017), we generated 1,152 samples (576 pairs) by filling the templates with common gender identity pairs (ex. male/female, man/woman, etc.). We created templates (Table 2) that contained both neutral and offensive nouns and adjectives inside the vocabu-\n1https://github.com/conversationai/ unintended-ml-bias-analysis\nlary (See Table 3) to retain balance in neutral and abusive samples.\nFor the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in Dixon et al. (2017) which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where T = {male, female}.\nFPED = ∑ t∈T |FPR− FPRt|\nFNED = ∑ t∈T |FNR− FNRt|\nSince the classifiers output probabilities, equal error rate thresholds are used for prediction decision.\nWhile the two AUC scores show the performances of the models in terms of accuracy, the equality difference scores show them in terms of fairness, which we believe is another dimension for evaluating the model’s generalization ability."
  }, {
    "heading": "4.2 Experimental Setup",
    "text": "We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) (Park\nand Fung, 2017), Gated Recurrent Unit (GRU) (Cho et al., 2014), and Bidirectional GRU with self-attention (α-GRU) (Pavlopoulos et al., 2017), but with a simpler mechanism used in Felbo et al. (2017). Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters:\n1. CNN: Convolution layers with 3 filters with the size of [3,4,5], feature map size=100, Embedding Size=300, Maxpooling, Dropout=0.5\n2. GRU: hidden dimension=512, Maximum Sequence Length=100, Embedding Size=300, Dropout=0.3\n3. α-GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3\nWe also compare different pre-trained embeddings, word2vec (Mikolov et al., 2013) trained on Google News corpus, FastText (Bojanowski et al., 2017)) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged."
  }, {
    "heading": "4.3 Results & Discussions",
    "text": "Tables 4 and 5 show the bias measurement experiment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word embeddings can provide prior knowledge of words.\nHowever, the equality difference scores tended to be larger when pre-trained embeddings were\nused, especially in the st dataset. This confirms the result of Bolukbasi et al. (2016). In all experiments, direction of the gender bias was towards female identity words. We can infer that this is due to the more frequent appearances of female identities in “sexist” tweets and lack of negative samples, similar to the reports of Dixon et al. (2017). This is problematic since not many NLP datasets are large enough to reflect the true data distribution, more prominent in tasks like abusive language where data collection and annotation are difficult.\nOn the other hand, abt dataset showed significantly better results on the two equality difference scores, of at most 0.04. Performance in the generated test set was better because the models successfully classify abusive samples regardless of the gender identity terms used. Hence, we can assume that abt dataset is less gender-biased than the st dataset, presumably due to its larger size, balance in classes, and systematic collection method.\nInterestingly, the architecture of the models also influenced the biases. Models that “attend” to certain words, such as CNN’s max-pooling or αGRU’s self-attention, tended to result in higher false positive equality difference scores in st dataset. These models show effectiveness in catching not only the discriminative features for classification, but also the “unintended” ones causing the model biases."
  }, {
    "heading": "5 Reducing Gender Biases",
    "text": "We experiment and discuss various methods to reduce gender biases identified in Section 4.3."
  }, {
    "heading": "5.1 Methodology",
    "text": "Debiased Word Embeddings (DE) (Bolukbasi et al., 2016) proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.\nGender Swap (GS) We augment the training data by identifying male entities and swapping them with equivalent female entities and vice-versa. This simple method removes correlation between gender and classification decision and has proven to be effective for correcting gender biases in coreference resolution task (Zhao et al., 2018).\nBias fine-tuning (FT) We propose a method to use transfer learning from a less biased corpus to reduce the bias. A model is initially trained with a larger, less-biased source corpus with a same or similar task, and fine-tuned with a target corpus with a larger bias. This method is inspired by the fact that model bias mainly rises from the imbalance of labels and the limited size of data samples. Training the model with a larger and less biased dataset may regularize and prevent the model from over-fitting to the small, biased dataset."
  }, {
    "heading": "5.2 Experimental Setup",
    "text": "Debiased word2vec Bolukbasi et al. (2016) is compared with the original word2vec (Mikolov et al., 2013) for evaluation. For gender swapping data augmentation, we use pairs identified through crowd-sourcing by Zhao et al. (2018).\nAfter identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset.\nBased on this criterion and results from Section 4.3, we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments."
  }, {
    "heading": "5.3 Results & Discussion",
    "text": "Table 6 shows the results of experiments using the three methods proposed. The first rows are the baselines without any method applied. We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores. Meanwhile, fine-tuning bias with a larger, less biased source dataset helped to decrease the equality difference scores and greatly improve the AUC scores from the generated unbiased test set. The latter improvement shows that the model significantly reduced errors on the unbiased set in general.\nTo our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of “attending” model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less\nthan 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important “unbiased” features."
  }, {
    "heading": "6 Conclusion & Future Work",
    "text": "We discussed model biases, especially toward gender identity terms, in abusive language detection. We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence. Also, we found our proposed methods can reduce gender biases up to 90-98%, improving the robustness of the models.\nAs shown in Section 4.3, some classification performance drop happens when mitigation methods. We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time. Some previous works (Beutel et al.; Zhang et al., 2018) employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Although those approaches are not directly comparable to our methods, it would be interesting to explore adversarial training to tackle this problem in the future.\nAlthough our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST, and 16248016 of Hong Kong Research Grants Council."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep learning for hate speech detection in tweets",
    "authors": ["Pinkesh Badjatiya", "Shashank Gupta", "Manish Gupta", "Vasudeva Varma."],
    "venue": "Proceedings of the 26th International Conference on World Wide Web Companion, pages 759–760. International World",
    "year": 2017
  }, {
    "title": "Data decisions and theoretical implications when adversarially learning fair representations. FAT/ML 2018: 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning",
    "authors": ["Alex Beutel", "Jilin Chen", "Zhe Zhao", "Ed H Chi"],
    "year": 2018
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics Volume 5, Issue",
    "year": 2017
  }, {
    "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
    "authors": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai."],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Semantics derived automatically from language corpora contain human-like biases",
    "authors": ["Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan."],
    "venue": "Science, 356(6334):183–186.",
    "year": 2017
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "EMNLP2014.",
    "year": 2014
  }, {
    "title": "Measuring and mitigating unintended bias in text classification",
    "authors": ["Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman."],
    "venue": "AAAI.",
    "year": 2017
  }, {
    "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
    "authors": ["Bjarke Felbo", "Alan Mislove", "Anders Søgaard", "Iyad Rahwan", "Sune Lehmann."],
    "venue": "EMNLP2017.",
    "year": 2017
  }, {
    "title": "Large scale crowdsourcing and characterization of twitter",
    "authors": ["Antigoni-Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis"],
    "year": 2018
  }, {
    "title": "Deceiving google’s perspective api built for detecting toxic comments",
    "authors": ["Hossein Hosseini", "Sreeram Kannan", "Baosen Zhang", "Radha Poovendran."],
    "venue": "arXiv preprint arXiv:1702.08138.",
    "year": 2017
  }, {
    "title": "Examining gender and race bias in two hundred sentiment analysis systems",
    "authors": ["Svetlana Kiritchenko", "Saif M Mohammad."],
    "venue": "Proceedings of the 7th Joint Conference on Lexical and Computational Semantics(*SEM), New Orleans, USA.",
    "year": 2018
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "One-step and twostep classification for abusive language detection on twitter",
    "authors": ["Ji Ho Park", "Pascale Fung."],
    "venue": "ALW1: 1st Workshop on Abusive Language Online to be held at the annual meeting of the Association of Computational Linguistics (ACL) 2017.",
    "year": 2017
  }, {
    "title": "Deeper attention to abusive user content moderation",
    "authors": ["John Pavlopoulos", "Prodromos Malakasiotis", "Ion Androutsopoulos."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1125–1135.",
    "year": 2017
  }, {
    "title": "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter",
    "authors": ["Zeerak Waseem."],
    "venue": "Proceedings of the first workshop on NLP and computational social science, pages 138– 142.",
    "year": 2016
  }, {
    "title": "Hateful symbols or hateful people? predictive features for hate speech detection on twitter",
    "authors": ["Zeerak Waseem", "Dirk Hovy."],
    "venue": "Proceedings of the NAACL student research workshop, pages 88–93.",
    "year": 2016
  }, {
    "title": "Ex machina: Personal attacks seen at scale",
    "authors": ["Ellery Wulczyn", "Nithum Thain", "Lucas Dixon."],
    "venue": "Proceedings of the 26th International Conference on World Wide Web, pages 1391–1399. International World Wide Web Conferences Steering Committee.",
    "year": 2017
  }, {
    "title": "Mitigating unwanted biases with adversarial learning",
    "authors": ["Brian Hu Zhang", "Blake Lemoine", "Margaret Mitchell."],
    "venue": "Proceedings of AAAI/ACM Conference on Ethics and Society(AIES) 2018.",
    "year": 2018
  }, {
    "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
    "authors": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang."],
    "venue": "NAACL 2018.",
    "year": 2018
  }],
  "id": "SP:192a943ad1895f0d77af7931135e5292f18a6649",
  "authors": [{
    "name": "Ji Ho Park",
    "affiliations": []
  }, {
    "name": "Jamin Shin",
    "affiliations": []
  }, {
    "name": "Pascale Fung",
    "affiliations": []
  }],
  "abstractText": "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, “You are a good woman” was considered “sexist” when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure gender biases on models trained with different abusive language datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios.",
  "title": "Reducing Gender Bias in Abusive Language Detection"
}