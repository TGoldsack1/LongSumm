{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1207–1216, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Chinese word segmentation (CWS) is generally accepted to be a necessary first step in most Chinese NLP tasks because Chinese sentences are written in continuous sequences of characters with no explicit delimiters (e.g., the spaces in English). Many studies have been conducted in this area, resulting in extensive investigation of the problem of\nCWS using machine learning techniques in recent years. However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data. Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive. Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS.\nTo address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years. These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora). In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed. However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers. This practice means that these methods can typically yield high performance in certain specialized domains, but they lack generalizability. In contrast with these methods, we propose to leverage bilingual unlabeled data, i.e., a Chinese-English corpus with sentence alignment. Because English sentences\n1207\nare naturally segmented, extracting information from a bilingual corpus is a much more objective task. As the example presented in Fig 1 shows, the English sentences that correspond to Chinese text can easily help guide better segmentation, and thus, the learning of segmenting information from bilingual data is a very promising approach.\nIn this paper, to obtain high-quality segmenting information from bilingual unlabeled data, we leverage multilevel features using the following steps: first, we integrate character-level features calculated using a conditional random field (CRF) model, which is used to capture the monolingual grammars. Then, we employ a statistical aligner to perform character-based alignment. Given the results of this character-based alignment, we apply several phrase-level features to extract explicit and implicit segmenting information: (1) we use two types of English-Chinese co-occurrence features (one-to-many and many-to-many) to learn the explicit segmenting information of the English sentences, (2) we use the transliteration similarity feature to detect out-of-vocabulary (OOV) words using a phrase-based translation model, and (3) we employ a neural network to calculate the semantic gap between the Chinese and English words to ensure that the Chinese segmentation follows the semantic meanings of the corresponding English sentences as closely as possible. Finally, we employ another phrase-based translation model to perform a sentence-level calculation of the translation probability of the Chinese segmentation and its corresponding English sentences. After obtaining these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original super-\nvised CWS model, which was trained on a standard manually segmented corpus.\nIn fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilingual segmenting information, which allows our semi-supervised CWS model to be very efficient at other NLP tasks and endows it with higher generalizability.\nOur evaluation also shows that our method significantly outperforms the state-of-the-art monolingual and bilingual semi-supervised approaches."
  }, {
    "heading": "2 Related Work",
    "text": "First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS."
  }, {
    "heading": "2.1 Monolingual Supervised and Semi-supervised CWS Methods",
    "text": "Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et\nal., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods.\nTo address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model. (Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias."
  }, {
    "heading": "2.2 Bilingual Semi-supervised CWS Methods",
    "text": "Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT. However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy. Thus, they sometimes ignore the standard grammars during\nsegmentation in favor of satisfying the needs of SMT, thereby causing these methods to be rather unsuitable for other NLP tasks. By contrast, we propose to use various types of features to capture syntactic and semantic information and a cascaded log-linear model to maintain balance between the monolingual grammars and the bilingual knowledge."
  }, {
    "heading": "3 Multilevel Features",
    "text": "In this section, we describe the three levels of features used in our approach. We propose to use character-level features to capture monolingual grammars and phrase-level and sentence-level features to obtain bilingual segmenting information. Moreover, we describe a cascaded log-linear model by proposing both inner and outer log-linear models."
  }, {
    "heading": "3.1 Character-level Feature",
    "text": "The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model’s effectiveness in detecting OOV words.\nIn this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003). For the jth character cj in the sentence cJ1 = c1...cJ , the score can be calculated as follows:\nfCRF (j) = ∑ k λkfk(yj−1, yj , c J 1 , j) (1)\nwhere fk(yj−1, yj , cJ1 , j) is a feature function and λk is a learned weight that corresponds to the feature fk. j represents the index of the character in the sentence. yj−1 and yj represent the tags of the previous and current characters, respectively.\nWe do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003)."
  }, {
    "heading": "3.2 Phrase-level Features",
    "text": "In this section, we first describe English-Chinese character-based alignment. Then, we propose several phrase-level features to obtain explicit and implicit segmenting information from the characterbased alignment. Finally, we describe the inner log-linear model that is used to combine the character-level and phrase-level features."
  }, {
    "heading": "3.2.1 English-Chinese Character-based Alignment",
    "text": "To avoid introducing omissions and mistakes into the linguistic information in the initial segmentations of the bilingual data, we perform a statistical character-based alignment: First, every Chinese character in the bitexts is separated by white spaces so that individual characters are recognized as unique /words0 or alignment targets. Then, they are associated with English words using a statistical word aligner.\nBy representing the English and Chinese sentences as eI1 = e1e2...eI and c J 1 = c1c2...cJ , respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, c J 1 , a K 1 >\nTo obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars."
  }, {
    "heading": "3.2.2 Features Obtained from the Character-based Alignment",
    "text": "Given the English-Chinese character-based alignment aK1 , we extract several phrase-level features to optimize the segmentation. For the jth character in cJ1 , we assume that one of the segmentations of the substring cj1 can be represented as wN+11 = w1w2w3...wN+1 = c j1 1 c j2 j1+1\n...cjjN+1. Then, we calculate the scores of each Chinese word wn = c jn jm\n(jm = jn−1 + 1) in wN+11 using the following features.\nEnglish-Chinese One-to-Many Alignment To evaluate the probability that a sequence of Chinese characters cjnjm = cjmcjm+1...cjn should be combined into a word wn based on the corresponding English sentence, we integrate the feature of English-Chinese one-to-many alignment (one English word is aligned with multiple Chinese characters). First, for any English word ei in eI1, the phrase tuple < ei, c jn jm\n> can be defined as an aligned One-to-Many phrase tuple if it satisfies the following conditions:\n(1) < i, jm > ∈ aK1 , < i, jn > ∈ aK1 (2) ∀j′ /∈ [jm, jn], < i, j′ >/∈ aK1 1http://www.phontron.com/pialign/\n(3) ∀i′ 6= i§∀j′ ∈ [jm, jn], < i′, j′ >/∈ aK1 Then, for any phrase tuple < ei, c jn jm > that satisfies these conditions, the span < i, jm, jn > is defined as a One-to-Many span and as a member of the set AOne.\nThus, for each span < i, jm, jn >, the One-toMany score can be calculated as follows:\ns(< i, jm, jn >) = { t(cjnjm |ei) if < i, jm, jn >∈ AOne 0 else\n(2)\nwhere t(cjnjm |ei) represents the translation probability of the phrase tuple cjnjm |ei.\nFinally, the score for the feature of EnglishChinese One-to-Many alignment for wn = c jn jm\nis derived as follows:\nfOne−to−Many(n) = argmax i∈[1,I] s(< i, jm, jn >) (3)\nEnglish-Chinese Many-to-Many Alignment The second phrase-level feature, called EnglishChinese Many-to-Many Alignment (multiple English words are aligned with multiple Chinese characters), is used to evaluate the probability that a space should be inserted between cn and cn+1. Similar to One-to-Many alignment, for any sequence of English words ei2i1 and the Chinese word wn = c jn jm , the phrase tuple < ei2i1 , c jn j1\n> is defined as an aligned Many-to-Many phrase tuple if it satisfies the following conditions:\n(1) j1 ≤ jm, and j1 is the beginning character of a word in wn1\n(1) < i1, j1 >∈ aK1 , < i2, jm >∈ aK1 (2) ∀j′ /∈ [j1, jm], ∀i′ ∈ [i1, i2], < i′, j′ >/∈ aK1 (3) ∀j′ ∈ [j1, jm], ∀i′ /∈ [i1, i2], < i′, j′ >/∈ aK1 Then, for any phrase tuple< ei2i1 , c jn jm > that satisfies these conditions, the span < i1, i2, j1, jn > is defined as a Many-to-Many span and as a member of the set AMany.\nThus, for each span < i1, i2, j1, jn >, the Many-to-Many score can be calculated as follows:\ns(< i1, i2, j1, jn >) =  t(cjnj1 |ei2i1) if < i1, i2, j1, jn >∈ AMany\n0 else\n(4)\nwhere t(cjnj1 |ei2i1) represents the translation probability of the phrase tuple < ei2i1 , c jn j1 >.\nFinally, the score for the feature of EnglishChinese Many-to-Many alignment for wn = c jn jm is derived as follows:\nfMany−to−Many(n) = argmax i1∈[1,I]i2∈[i1,I]j1≤jm s(i1, i2, j1, jn)\n(5)\nTransliteration Feature To account for named entities (NEs), which suffer from sparsity and thus make it difficult to calculate the probabilities discussed above, we introduce a transliteration feature to evaluate the similarities between the pronunciations of Chinese and English words because many NEs are translated via transliteration. To perform this task, we first introduce an initial NE dictionary and convert each dictionary item—for example, we convert ”Ow d/Alice” into ”ai l i s i/a l i c e” —by transforming the Chinese word into its pronunciation (represented by the function Fpy(·)) and splitting the English word into its constituent letters (represented by the function Flet(·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary.\nSpecifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary. The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables. A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language. Moses (Koehn et al., 2007) is used as a decoder. Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.\nGiven these two phrase-based translation models, we calculate each span < i, jm, jn > in AOne for the Chinese word wn using the following formula:\nStr(< i, jm, jn >) = Sch−en(< i, jm, jn >) +Sen−ch(< i, jm, jn >) (6)\nwhere Sch−en(<i, jm, jn>) = DLev(Fletei, PTch−en(Fpy(c jn jm\n))) means that the pronunciation conversion in the Chinese-English direction is performed as follows: First, the English word ei is split into its constituent letters; Second, the\nsequence of Chinese characters cjnjm is converted into its pronunciation; Third, this pronunciation is input into the Chinese-English phrase-based translation model, and the corresponding translation result is obtained; And finally, the Levenshtein distance between the English letters and the translation result is returned. Sen−ch(<i, jm, jn>) can be calculated in exactly the same way. We set any span that does not belong to AOne to zero, and the transliteration feature score of a word wn = c jn jm is derived as follows:\nftransliteration(n) = argmax i∈[1,I] Str(< i, jm, jn >) (7)\nEnglish-Chinese semantic gap feature To guarantee that the semantic meanings of the Chinese segmentation match those of the corresponding English sentences as closely as possible, we propose to use a feature based on the EnglishChinese semantic gap to ensure the retention of semantic meaning during the segmentation process.\nFirst, we pre-train word embeddings using the open-source toolkit Word2Vec (Mikolov et al., 2013) on the Chinese (segmented using characterlevel features only) and English sentences separately, thereby obtaining the vocabularies Vch and Ven and their corresponding embedding matrixes Lch ∈ Rn×|Vch| andLen ∈ Rn×|Ven|. Given a Chinese word wn with an index i in the vocabulary, it is then straightforward to retrieve the word’s vector representation via simple multiplication with a binary vector d that is equal to zero at all positions except that with index i:\nXi = Lchdi ∈ Rn (8)\nBecause the word embeddings for the two languages (Lch and Len) are learned separately and located in different vector spaces, we suppose that a transformation exists between these two semantic embedding spaces. Thus, we collect all the One-to-Many phrase tuples < e1, c j2 j1 > that satisfy e1 ∈ Ven and cj2j1 ∈ Vch from the entire corpus of bilingual data. Then, we insert the word embedding tuple of each One-to-Many phrase tuple into the set Aembed. Let us consider a word embedding tuple < ps, pt > in Aembed as an example. We define a bidirectional semantic distance using the parameter θ as follows:\nEsem(ps, pt; θ) = Esem(ps|pt, θ) + Esem(pt|ps, θ) (9)\nHere, Esem(ps|pt, θ) = Esem(pt, f(W chenps + bchen)) represents the transformation of ps and is performed as follows: We first multiply a parameter matrix W chen by ps, and after adding a bias term bchen, we apply an element-wise activation function f = tanh(·). Finally, we calculate their Euclidean distance:\nEsem(ps|pt, θ) = 1 2 ||pt − f(W chenps + bchen)||2 (10)\nEsem(pt|ps, θ) can be calculated in exactly the same way.\nGiven the definition of the semantic distance of each word-embedding tuple inAembed, we wish to minimize the following objective function:\nJ = ∑\n<ps,pt>∈Aembed Esem(ps, pt; θ) (11)\nWe apply the Stochastic Gradient Descent (SGD) algorithm to optimize each parameter and ultimately obtain the optimized parameters θ∗.\nUsing θ∗, we can calculate the semantic gap for any possible span for wn, such as < i, jm, jn >, as follows:\nSgap(< i, jm, jn >) =  1 Esem(p′s|p′t,θ∗) if ei ∈ Ven cjnjm ∈ Vch\n< i, jm, jn >∈ AOne 0 else\n(12)\nwhere p′s and p′t are the word vector representation of ei and c jn jm\n, respectively. Thus, the semantic gap feature score of the word wn = c jn jm\nis derived as follows:\nfsem(wn) = argmax i∈[1,I] Sgap(< i, jm, jn >) (13)"
  }, {
    "heading": "3.2.3 Normalization and the Inner Log-Linear Model",
    "text": "Because the output scores of each sub-model described above are not probabilistic and they vary by orders of magnitude, we must first normalize\nthe output scores of each sub-model. After normalization, the scores have means and standard deviations of zero. We represent the normalization function by Norm(·).\nThus, for the substring cj1 (j ∈ [1, J)) in cJ1 of the sentence tuple <eI1, c J 1 , a K 1 >, assuming that one of its candidate segmentations is wN+11 = w1w2w3...wN+1 = c j1 1 c j2 j1+1\n...cjjN+1, the feature score of the inner log-linear model is derived as follows:\nfinner = ∑\nj′∈[1,j] Norm(fCRF (j\n′))+\nλ1 ∑ n∈[1,N+1] ( ∑ k Norm(fk(n))) (14)\nwhere fk(n) represents the phrase-level features. Then, we tune the weight λ1 from 0 to 1 in equal\nincrements of 0.1 to optimize its value."
  }, {
    "heading": "3.3 Sentence-level Features",
    "text": "In this section, we describe the sentence-level features calculated using the phrase-based translation model and the outer log-linear model that is used to combine the sentence-level features with the features in the inner log-linear model."
  }, {
    "heading": "3.3.1 Features Obtained from the Phrase-based Translation Model",
    "text": "Let us consider the last character cJ in cJ1 and assume that its candidate segmentation (according to the inner log-linear model only) is wN+11 = w1w2w3...wN+1. We now add a sentence-level feature to incorporate into the inner log-linear model. This sentence-level feature is obtained using a phrase-based translation model. We segment the Chinese sentences from the bilingual unlabeled data using character-level features only and train a phrase-based translation model on the bilingual data that is similar to the phrase-based translation model used for the transliteration features.\nUnlike the usage of the phrase-based translation model in the case of the transliteration features, here, we input both the source and target sentences and achieve the output of translation probability. Thus, we perform a force decoding for the sentence tuple <wN+11 , e I 1> and obtain the set of decoding paths P(wN+11 ), where each element acts as a decoding path that can translate wN+11 into eI1. Finally, we define the sentence-level feature score of <wN+11 , e I 1> as follows:\nfsent(w N+1 1 ) = argmax\np(wN+11 )∈P (w N+1 1 )\nFtrans(p(w N+1 1 ))\n(15)\nwhere Ftrans(·) returns the translation score of the given decoding path based on the phrase-based translation model."
  }, {
    "heading": "3.3.2 The Outer Log-Linear Model",
    "text": "Finally, we normalize the sentence-level features in a manner similar to that described previously and construct the outer log-linear model by combining the inner log-linear model and the sentencelevel features as follows:\nfouter = finner + λ2Norm(fsent(wN+11 )) (16)\nThen, we also tune the weight λ2 from 0 to 1 in equal increments of 0.1 to optimize its value."
  }, {
    "heading": "3.3.3 Decoder",
    "text": "A traditional viterbi beam search procedure is applied in the decoder to seek the segmented sequence with the highest score. Given a sentence tuple < eI1, c J 1 , a K 1 >, the decoding procedure will proceed in a left-right fashion using a dynamic programming approach. At each position j in the sequence cJ1 , we maintain a vector of size N to store the top N candidate segmentations of subsequence cj1 which are scored using the inner loglinear model (j ∈ [1, J)) or the outer log-linear model (j = J). Finally, we return the best segmentation."
  }, {
    "heading": "4 Justifying the Original CWS Model",
    "text": "We justify the original CWS model (the CRFbased model trained on manually segmented data) using the new CRF model trained on the segmentation of unlabeled bilingual data. To avoid overweakening the influence of the small-scale manually segmented data, we again utilized a log-linear model to balance their weights. The formula can be described as follows:\nfnew mono = ∑ k1 λk1fk1(yj−1, yj , c J 1 , j)\n+θ3 ∑ k2 λk2fk2(yj−1, yj , c J 1 , j)\n(17)\nwhere θ3 represents the weights of the second CRF model, which are set via minimum error rate training using the developing dataset, and λki (i\n=1, 2) represents the learned weights of the features of the CRF models."
  }, {
    "heading": "5 The Datasets",
    "text": "In this paper, we conduct our experiments on the corpus of People’s daily of 1998 (from January to June) as the standard (manually segmented) training corpus, the corpus of Bakeoff-2 CWS evaluation as the developing and testing dataset. As the corpus of Bakeoff-2 is made up of several sets provided by different organizations, we only select two sets whose segmenting standards are similar to the training corpus. For each set, we take 3000 sentences as the developing dataset and the others as the testing dataset. The statistics of every set and the standard training corpus are shown in Table 1.\nMoreover, the bilingual unlabeled data is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,215,000 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous."
  }, {
    "heading": "6 Experiments",
    "text": "In our evaluation, the F-score was used as the accuracy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data).\nInitially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by\nsegmenting the bilingual unlabeled dataset using character-level features only, the inner log-linear model (which includes character-level and phraselevel features) and the outer log-linear model (the full SLBD approach). After applying these three segmentations using the different sub-models, we trained the new CRF models on the results of the three segmentations to justify the original CWS model. The evaluation results for the supervised CWS methods and the sub-models are presented in Table 2.\nIt can be seen that we achieved significant improvement in performance when we combined the character-level and phrase-level features in the inner log-linear model, demonstrating that the proposed phrase-level features can be used to efficiently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.\nNext, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (S&X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sentences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natural, we used the XIN CMN portion of Chinese Gigaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 million words, more than ten times\nthe number of words in the bilingual unlabeled dataset. The testing data was the set of AS only. The evaluation is summarized in Table 3.\nThe results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.\nFinally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014). The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we propose a cascaded log-linear model to involve learning three levels of bilingual linguistic features to semi-supervisedly learn a new CWS model. Different from other monolingual and bilingual semi-supervised approaches, we employ various types of features to capture both monolingual grammars and bilingual segmenting information, which allows our model to be very efficient at other NLP tasks and endows it with higher generalizability. The evaluation shows that our method significantly outperforms the state-of-the-art monolingual and bilingual semi-supervised approaches."
  }],
  "year": 2015,
  "references": [{
    "title": "Combination of machine learning methods for optimum chinese word segmentation",
    "authors": ["Masayuki Asahara", "Kenta Fukuoka", "Ai Azuma", "ChooiLing Goh", "Yotaro Watanabe", "Yuji Matsumoto", "Takahashi Tsuzuki."],
    "venue": "Proceedings of The Fourth",
    "year": 2005
  }, {
    "title": "Optimizing Chinese word segmentation for machine translation performance",
    "authors": ["Pi-Chuan Chang", "Michel Galley", "Christopher D Manning."],
    "venue": "Proceedings of WMT, pages 224-232. Association for Computational Linguistics.",
    "year": 2008
  }, {
    "title": "Unsupervised Tokenization for Machine Translation",
    "authors": ["Tagyoung Chung", "Daniel Gildea."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2. Association for Computational Lin-",
    "year": 2009
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "Proc. 18th International Conf. on Machine Learning.",
    "year": 2001
  }, {
    "title": "Semi-supervised learning for natural language",
    "authors": ["Percy Liang"],
    "venue": "Master.s thesis",
    "year": 2005
  }, {
    "title": "Bilingually motivated domain-adapted word segmentation for statistical machine translation",
    "authors": ["Yanjun Ma", "Andy Way."],
    "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for",
    "year": 2009
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Mikolov", "Tomas"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "An Unsupervised Model for Joint Phrase Alignment and Extraction",
    "authors": ["Graham Neubig", "Taro Watanabe"],
    "venue": "Proceedings of ACL 2011.",
    "year": 2011
  }, {
    "title": "Machine Translation without Words through Substring Alignment",
    "authors": ["Graham Neubig", "Taro Watanabe"],
    "venue": "Proceedings of ACL 2012.",
    "year": 2012
  }, {
    "title": "Semi-supervised conditional random fields for improved sequence segmentation and labeling",
    "authors": ["Feng Jiao", "Shaojun Wang", "Chi-Hoon Lee."],
    "venue": "Proceedings of ACL, pages 209-216, Sydney, Australia.",
    "year": 2006
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Koehn", "Philipp", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-",
    "year": 2003
  }, {
    "title": "Moses: Open source toolkit for statistical machine translation",
    "authors": ["Koehn", "Philipp"],
    "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics,",
    "year": 2007
  }, {
    "title": "Improved statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Proceedings of ACL, pages 440-447.",
    "year": 2000
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["Och", "Franz Josef."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, 2003: 160-167.",
    "year": 2003
  }, {
    "title": "Chinese segmentation and new word detection using conditional random fields",
    "authors": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."],
    "venue": "Proceedings of the 20th international conference on Computational Linguistics.",
    "year": 2004
  }, {
    "title": "SRILM-an extensible language modeling toolkit",
    "authors": ["Stolcke", "Andreas."],
    "venue": "INTERSPEECH. 2002",
    "year": 2002
  }, {
    "title": "Enhancing chinese word segmentation using unlabeled data",
    "authors": ["Weiwei Sun", "Jia Xu."],
    "venue": ". In Proceedings of EMNLP 2011.",
    "year": 2011
  }, {
    "title": "Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection",
    "authors": ["Xu Sun", "Houfeng Wang", "Wenjie Li."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistic-",
    "year": 2012
  }, {
    "title": "UM-Corpus: a large EnglishChinese parallel corpus for statistical machine translation",
    "authors": ["Tian", "Liang"],
    "venue": "In Proceedings of the 9th International Conference on Language Resources and Evaluation. ELRA Reykjavik,",
    "year": 2014
  }, {
    "title": "Enhancing statistical machine translation with character alignment",
    "authors": ["Ning Xi", "Guangchao Tang", "Xinyu Dai", "Shujian Huang", "Jiajun Chen."],
    "venue": "Proceedings of ACL, pages 285-290. Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Do we need Chinese word segmentation for statistical machine translation",
    "authors": ["Jia Xu", "Richard Zens", "Hermann Ney."],
    "venue": "Proceedings of the Third SIGHAN Workshop on Chinese Language Learning, pages 122õ128.",
    "year": 2004
  }, {
    "title": "Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation",
    "authors": ["Jia Xu", "Jianfeng Gao", "Kristina Toutanova", "Hermann Ney."],
    "venue": "Proceedings of Coling 2008.",
    "year": 2008
  }, {
    "title": "Chinese word segmentation as character tagging",
    "authors": ["Nianwen Xue."],
    "venue": "Computational Linguistics and Chinese Language Processing, pages 29õ48.",
    "year": 2003
  }, {
    "title": "Chinese segmentation with a word-based perceptron algorithm",
    "authors": ["Zhang", "Clark"],
    "venue": "Proceedings of ACL",
    "year": 2007
  }, {
    "title": "A unified character-based tagging framework for chinese word segmentation",
    "authors": ["Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu."],
    "venue": "ACM Transactions on Asian Language Information Processing, 9(2):5:1-5:32, June.",
    "year": 2010
  }, {
    "title": "Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging",
    "authors": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Compu-",
    "year": 2013
  }, {
    "title": "Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation",
    "authors": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso"],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computa-",
    "year": 2013
  }, {
    "title": "Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints",
    "authors": ["Xiaodong Zeng", "Derek F. Wong"],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.",
    "year": 2014
  }],
  "id": "SP:14e7c7932bf8717a51e155e4ed224359be70d24f",
  "authors": [{
    "name": "Wei Chen",
    "affiliations": []
  }, {
    "name": "Bo Xu",
    "affiliations": []
  }],
  "abstractText": "This paper presents a bilingual semisupervised Chinese word segmentation (CWS) method that leverages the natural segmenting information of English sentences. The proposed method involves learning three levels of features, namely, character-level, phrase-level and sentence-level, provided by multiple submodels. We use a sub-model of conditional random fields (CRF) to learn monolingual grammars, a sub-model based on character-based alignment to obtain explicit segmenting knowledge, and another sub-model based on transliteration similarity to detect out-of-vocabulary (OOV) words. Moreover, we propose a sub-model leveraging neural network to ensure the proper treatment of the semantic gap and a phrase-based translation sub-model to score the translation probability of the Chinese segmentation and its corresponding English sentences. A cascaded log-linear model is employed to combine these features to segment bilingual unlabeled data, the results of which are used to justify the original supervised CWS model. The evaluation shows that our method results in superior results compared with those of the state-of-the-art monolingual and bilingual semi-supervised models that have been reported in the literature.",
  "title": "Semi-supervised Chinese Word Segmentation based on Bilingual Information"
}