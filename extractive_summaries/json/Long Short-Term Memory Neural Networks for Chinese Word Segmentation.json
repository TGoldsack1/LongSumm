{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1197–1206, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional RandomFields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert\n∗Corresponding author.\net al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the following sentence needs the information of the long distance collocation. 冬天 (winter)，能 (can) 穿 (wear) 多少 (amount) 穿 (wear) 多少 (amount)；夏天 (summer)，能 (can)穿 (wear)多 (more)少 (little)穿 (wear)多 (more)少 (little)。 Without the word “夏天 (summer)” or “冬天 (winter)”, it is difficult to segment the phrase “能 穿多少穿多少”. Therefore, we usually need utilize the non-local information for more accurate word segmentation. However, it does not work by simply increasing the context window size. As reported in (Zheng et al., 2013), the performance drops smoothly when the window size is larger than 3. The reason is that the number of its parameters is so large that the trained network has\n1197\noverfitted on training data. Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window. In order to address this problem, we propose a neural model based on Long Short-Term Memory Neural Network (LSTM) (Hochreiter and Schmidhuber, 1997) that explicitly model the previous information by exploiting input, output and forget gates to decide how to utilize and update the memory of pervious information. Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over a long distance, hence, capturing the potential useful long-distance information. We evaluate our model on three popular benchmark datasets (PKU, MSRA and CTB6), and the experimental results show that our model achieves the state-of-the-art performance with the smaller context window size (0,2). The contributions of this paper can be summarized as follows.\n• We first introduce the LSTM neural network for Chinese word segmentation. The LSTM can capture potential long-distance dependencies and keep the previous useful information in memory, which avoids the limit of the size of context window.\n• Although there are relatively few researches of applying dropout method to the LSTM, we investigate several dropout strategies and find that dropout is also effective to avoid the overfitting of the LSTM.\n• Despite Chinese word segmentation being a specific case, our model can be easily generalized and applied to the other sequence labeling tasks."
  }, {
    "heading": "2 Neural Model for Chinese Word Segmentation",
    "text": "Chinese word segmentation is usually regarded as character-based sequence labeling. Each character is labeled as one of {B, M, E, S} to indicate the segmentation. {B, M, E} represent Begin, Middle, End of a multi-character segmentation respectively, and S represents a Single character segmentation. The neural model is usually characterized by three specialized layers: (1) a character embedding\nlayer; (2) a series of classical neural network layers and (3) tag inference layer. An illustration is shown in Figure 1. The most common tagging approach is based on a local window. The window approach assumes that the tag of a character largely depends on its neighboring characters. Given an input sentence c(1:n), a window of size k slides over the sentence from character c(1) to c(n), where n is the length of the sentence. As shown in Figure 1, for each character c(t)(1 ≤ t ≤ n), the context characters (c(t−2),c(t−1),c(t),c(t+1),c(t+2)) are fed into the lookup table layer when the window size k is 5. The characters exceeding the sentence boundaries are mapped to one of two special symbols, namely “start” and “end” symbols. The character embeddings extracted by the lookup table layer are then concatenated into a single vector x(t) ∈ RH1 , where H1 = k × d is the size of layer 1. Then x(t) is fed into the next layer which performs linear transformation followed by an element-wise activation function g such as sigmoid function σ(x) = (1+e−x)−1 and hyperbolic tangent function ϕ(x) = ex−e−x\nex+e−x here.\nh(t) = g(W1x(t) + b1), (1)\nwhereW1 ∈ RH2×H1 , b1 ∈ RH2 , h(t) ∈ RH2 . H2 is a hyper-parameter which indicates the number of hidden units in layer 2. Given a set of tags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed:\ny(t) = W2h(t) + b2, (2)\nwhere W2 ∈ R|T |×H2 , b2 ∈ R|T |. y(t) ∈ R|T | is the score vector for each possible tag. In Chinese word segmentation, the most prevalent tag set T j T is {B, M, E, S} as mentioned above. To model the tag dependency, a transition score Aij is introduced to measure the probability of jumping from tag i ∈ T to tag j ∈ T (Collobert et al., 2011). Although this model works well for Chinese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window. Some useful long distance information is neglected."
  }, {
    "heading": "3 Long Short-Term Memory Neural Network for Chinese Word Segmentation",
    "text": "In this section, we introduce the LSTM neural network for Chinese word segmentation."
  }, {
    "heading": "3.1 Character Embeddings",
    "text": "The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings (Bengio et al., 2003; Collobert and Weston, 2008). Formally, in Chinese word segmentation task, we have a character dictionary C of size |C|. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ C is represented as a real-valued vector (character embedding) vc ∈ Rd where d is the dimensionality of the vector space. The character embeddings are then stacked into an embeddingmatrixM ∈ Rd×|C|. For a character c ∈ C, the corresponding character embedding vc ∈ Rd is retrieved by the lookup table layer. And the lookup table layer can be regarded as a simple projection layer where the character embedding for each context character is achieved by table lookup operation according to its index."
  }, {
    "heading": "3.2 LSTM",
    "text": "The long short term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is an extension of the recurrent neural network (RNN).\nThe RNN has recurrent hidden states whose output at each time is dependent on that of the previous time. More formally, given a sequence x(1:n) = (x(1), x(2), . . . , x(t), . . . , x(n)), the RNN updates its recurrent hidden state h(t) by\nh(t) = g(Uh(t−1) +Wx(t) + b), (3)\nwhere g is a nonlinear function as mentioned above. Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of upper layer of neural network at current time step t. The definitions of the gates, cell update and output are as follows:\ni(t) = σ(Wixx(t) + Wihh(t−1) + Wicc(t−1)), (4) f(t) = σ(Wfxx(t) + Wfhh(t−1) + Wfcc(t−1)), (5) c(t) = f(t) ⊙ c(t−1) + i(t) ⊙ ϕ(Wcxx(t) + Wchh(t−1)), (6) o(t) = σ(Woxx(t) +Wohh(t−1) +Wocc(t)), (7)\nh(t) = o(t) ⊙ ϕ(c(t)), (8)\nwhereσ andϕ are the logistic sigmoid function and hyperbolic tangent function respectively; i(t), f(t), o(t) and c(t) are respectively the input gate, forget gate, output gate, and memory cell activation vector at time step t, all of which have the same size as the hidden vector h(t) ∈ RH2 ; the parameter matrices W s with different subscripts are all square matrices; ⊙ denotes the element-wise product of the vectors. Note that Wic, Wfc and Woc are diagonal matrices."
  }, {
    "heading": "3.3 LSTM Architectures for Chinese Word Segmentation",
    "text": "To fully utilize the LSTM, we propose four different structures of neural network to select the effective features via memory units. Figure 3 illustrates our proposed architectures.\nLSTM-1 The LSTM-1 simply replace the hidden neurons in Eq. (1) with LSTM units (See Figure 3a).\nThe input of the LSTM unit is from a window of context characters. For each character, c(t), (1 ≤ t ≤ n), the input of the LSTM unit x(t),\nx(t) = v(t−k1)c ⊕ · · · ⊕ v(t+k2)c , (9)\nis concatenated from character embeddings of c(t−k1):(t+k2), where k1 and k2 represent the numbers of characters from left and right contexts respectively. The output of the LSTM unit is used in final inference function (Eq. (11) ) after a linear transformation.\nLSTM-2 The LSTM-2 can be created by stacking multiple LSTM hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next (See Figure 3b). Here we use two LSTM layers. Specifically, input of the upper LSTM layer takes h(t) from the lower LSTM layer without any transformation. The input of the first layer is same to LSTM-1, and the output of the second layer is as same operation as LSTM-1.\nLSTM-3 The LSTM-3 is a extension of LSTM1, which adopts a local context of LSTM layer as input of the last layer (See Figure 3c). For each time step t, we concatenate the outputs of a window of the LSTM layer into a vector ĥ(t),\nĥ(t) = h(t−m1) ⊕ · · · ⊕ h(t+m2), (10)\nwherem1 andm2 represent the lengths of time lags before and after current time step.Finally, ĥ(t) is used in final inference function (Eq. (11) ) after a linear transformation.\nLSTM-4 The LSTM-4 (see Figure 3d) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers. The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer. The final layer adopts a local context of upper LSTM layer as input."
  }, {
    "heading": "3.4 Inference at Sentence Level",
    "text": "To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014) introduced the transition score Aij for measuring the probability of jumping from tag i ∈ T to tag j ∈ T . For a input sentence c(1:n) with a tag sequence y(1:n), a sentencelevel score is then given by the sum of tag transition scores and network tagging scores:\ns(c(1:n), y(1:n), θ) = n∑ t=1 ( Ay(t−1)y(t) + y (t) y(t) ) , (11)\nwhere y(t) y(t) indicates the score of tag y(t), and y(t) is computed by the network as in Eq. (2). The parameter set of our model θ = {M,A,Wic,Wfc,Woc,Wix,Wfx,Wox,Wih,Wfh, Woh,Wcx,Wch}."
  }, {
    "heading": "4 Training",
    "text": ""
  }, {
    "heading": "4.1 Max-Margin criterion",
    "text": "We use the Max-Margin criterion to train our model. Intuitively, the Max-Margin criterion provides an alternative to probabilistic, likelihood based estimation methods by concentrating directly on the robustness of the decision boundary of a model (Taskar et al., 2005). We use Y (xi) to denote the set of all possible tag sequences for a given sentence xi and the correct tag sequence for xi is yi. The parameter set of our model is θ. We first define a structured margin loss ∆(yi, ŷ) for predicted tag sequence ŷ:\n∆(yi, ŷ) = n∑ t η1{y(t)i ̸= ŷ(t)}, (12)\nwhere n is the length of sentence xi and η is a discount parameter. The loss is proportional to the number of characters with incorrect tags in the proposed tag sequence. For a given training instance (xi, yi),the predicted tag sequence ŷi ∈ Y (xi) is the one with the highest score:\nŷi = argmax y∈Y (xi) s(xi, y, θ), (13)\nwhere the function s(·) is sentence-level score and defined in equation (11). Given a set of training setD, the regularized objective function is the loss function J(θ) including\na l2-norm term:\nJ(θ) = 1 |D| ∑ (xi,yi)∈D li(θ) + λ 2 ∥θ∥22, (14)\nwhere li(θ) = max(0, s(xi, ŷi, θ) + ∆(yi, ŷi) − s(xi, yi, θ)). To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows:\nθt,i = θt−1,i − α√∑t τ=1 g 2 τ,i gt,i, (15)\nwhere α is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997)."
  }, {
    "heading": "4.2 Dropout",
    "text": "Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivastava et al., 2014). When dropping a unit out, we temporarily remove it from the network, alongwith all its incoming and outgoing connections. In the simplest case, each unit is omitted with a fixed probability p independent of other units, namely dropout rate, where p is also chosen on development set."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 Datasets",
    "text": "We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model. The PKU\nand MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (Xue et al., 2005), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism. These datasets are commonly used by previous state-of-the-art models and neural network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10%\nsentences as development set for PKU and MSRA datasets. For CTB6 dataset, we divide the training, development and test sets according to (Yang and Xue, 2012) All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag. For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1score and out-of-vocabulary (OOV) word recall."
  }, {
    "heading": "5.2 Hyper-parameters",
    "text": "Hyper-parameters of neural model impact the performance of the algorithm significantly. According to experiment results, we choose the hyperparameters of our model as showing in Figure 1. The minibatch size is set to 20. Generally, the number of hidden units has a limited impact on the performance as long as it is large enough. We found that 150 is a good trade-off between speed and model performance. The dimensionality of character embedding is set to 100 which achieved the best performance. All these hyperparameters are chosen according to their average performances on three development sets. For the context lengths (k1, k2) and dropout strategy, we give detailed analysis in next section."
  }, {
    "heading": "5.3 Dropout and Context Length",
    "text": "We first investigate the different dropout strategies, including dropout at different layers and with different dropout rate p. As a result, we found that it is a good trade-off between speed and model performance to drop the input layer only with dropout rate pinput = 0.2. However, it does not show any significant improvement to dropout on hidden LSTM layers.\nDue to space constraints, we just give the performances of LSTM-1 model on PKU dataset with different context lengths (k1, k2) and dropout rates in Figure 4 and Table 2. From Figure 4, we can see that 20% dropout converges slightly slower than the one without dropout, but avoids overfitting. 50% or higher dropout rate seems to be underfitting since its training error is also high. Table 2 shows that the LSTM-1 model performs consistently well with the different context length, but the LSTM-1 model with short context length saves computational resource, and gets more efficiency. At the meanwhile, the LSTM-1 model with context length (0,2) can receive the same or better performance than that with context length (2,2), which shows that the LSTM model can well model the pervious information, and it is more robust for its insensitivity of window size variation. We employ context length (0,2) with the 20% dropout rate in the following experiments to balance the tradeoff between accuracy and efficiency."
  }, {
    "heading": "5.4 Model Selection",
    "text": "We also evaluate the our four proposed models with the hyper-parameter settings in Table 1. For LSTM-3 and LSTM-4 models, the context window length of top LSTM layer is set to (2,0). For LSTM-2 and LSTM-4,the number of upper hidden LSTM layer is set to 100. We use PKU dataset to select the best model. Figure 5 shows the results of the fourmodels on PKUdevelopment set from first epoch to 60-th epoch. We see that the LSTM-1 is the fastest one to converge and achieves the best\nperformance. The LSTM-2 (two LSTM layers) get worse, which shows the performance seems not to benefit from deep model. The LSTM-3 and LSTM-4 models do not converge, which could be caused by the complexity of models. The results on PKU test set are also shown in Table 3, which again show that the LSTM-1 achieves the best performance. Therefore, in the rest of the paper we will give more analysis based on the LSTM-1with hyper-parameter settings as showing in Table 1."
  }, {
    "heading": "5.5 Experiment Results",
    "text": "In this section, we give comparisons of the LSTM1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets. We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2). Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initialize the character lookup table instead of random initialization. Inspired by (Pei et al., 2014), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters. Table 5 shows the performances with additional pre-trained and bigram character embeddings. Again, the performances boost significantly as a result. Moreover, when we use bigram embeddings only, which means we do close test without pre-training the embeddings on other extra corpus, our model still perform competitively compared\n1http://code.google.com/p/word2vec/\nwith previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as unlabeled data or other knowledge. Despite our model only uses simple bigram features, it outperforms previous state-of-the-art models which use more complex features. Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed\na lot. On PKU dataset, it takes about 3 days to train themodel (last row of Table 5) usingCPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python."
  }, {
    "heading": "6 Related Work",
    "text": "Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long-\ndistance features. Though our model use smaller context window size (0,2), it still outperforms the previous neural models with context window size (2,2). Besides, our model can also be easily generalized and applied to other sequence labeling tasks. Although our model achieves state-of-the-art performance, it only makes use of previous context. The future context is also useful for Chinese word segmentation. In future work, wewould like to adopt the bidirectional recurrent neural network (Schuster and Paliwal, 1997) to process the sequence in both directions."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200)."
  }],
  "year": 2015,
  "references": [{
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin."],
    "venue": "The Journal of Machine Learning Research, 3:1137–1155.",
    "year": 2003
  }, {
    "title": "A maximum entropy approach to natural language processing",
    "authors": ["A.L. Berger", "V.J. Della Pietra", "S.A. Della Pietra."],
    "venue": "Computational Linguistics, 22(1):39–71.",
    "year": 1996
  }, {
    "title": "Gated recursive neural network for Chinese word segmentation",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."],
    "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics.",
    "year": 2015
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of ICML.",
    "year": 2008
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "The Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal ofMachine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "The second international Chinese word segmentation bakeoff",
    "authors": ["T. Emerson."],
    "venue": "Proceedings of",
    "year": 2005
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning.",
    "year": 2001
  }, {
    "title": "Multi-timescale long short-term memory neural network for modelling sentences and documents",
    "authors": ["PengFei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-",
    "year": 2015
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur."],
    "venue": "INTERSPEECH.",
    "year": 2010
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Maxmargin tensor neural network for chinese word segmentation",
    "authors": ["Wenzhe Pei", "Tao Ge", "Chang Baobao."],
    "venue": "Proceedings of ACL.",
    "year": 2014
  }, {
    "title": "Chinese segmentation and new word detection using conditional random fields",
    "authors": ["F. Peng", "F. Feng", "A. McCallum."],
    "venue": "Proceedings of the 20th international conference on Computational Linguistics.",
    "year": 2004
  }, {
    "title": "Deep learning for character-based information extraction",
    "authors": ["Yanjun Qi", "Sujatha G Das", "Ronan Collobert", "Jason Weston."],
    "venue": "Advances in Information Retrieval, pages 668–674. Springer.",
    "year": 2014
  }, {
    "title": "online) subgradient methods for structured prediction",
    "authors": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin A Zinkevich."],
    "venue": "Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).",
    "year": 2007
  }, {
    "title": "Bidirectional recurrent neural networks",
    "authors": ["Mike Schuster", "Kuldip K Paliwal."],
    "venue": "Signal Processing, IEEE Transactions on, 45(11):2673–2681.",
    "year": 1997
  }, {
    "title": "Parsing with compositional vector grammars",
    "authors": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "In Proceedings of the ACL conference. Citeseer.",
    "year": 2013
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Enhancing Chinese word segmentation using unlabeled data",
    "authors": ["Weiwei Sun", "Jia Xu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979.",
    "year": 2011
  }, {
    "title": "Generating text with recurrent neural networks",
    "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.",
    "year": 2011
  }, {
    "title": "Sequence to sequence learningwith neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."],
    "venue": "Advances in Neural Information Processing Systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "A conditional random field word segmenter for sighan bakeoff 2005",
    "authors": ["Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning."],
    "venue": "Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume",
    "year": 2005
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for",
    "year": 2010
  }, {
    "title": "Revisiting recurrent neural networks for robust asr",
    "authors": ["Oriol Vinyals", "Suman V Ravuri", "Daniel Povey."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4085–4088. IEEE.",
    "year": 2012
  }, {
    "title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus",
    "authors": ["Naiwen Xue", "Fei Xia", "Fu-Dong Chiou", "Martha Palmer."],
    "venue": "Natural language engineering, 11(2):207–238.",
    "year": 2005
  }, {
    "title": "Chinese word segmentation as character tagging",
    "authors": ["N. Xue."],
    "venue": "Computational Linguistics and Chinese Language Processing, 8(1):29–48.",
    "year": 2003
  }, {
    "title": "Chinese comma disambiguation for discourse analysis",
    "authors": ["Yaqin Yang", "Nianwen Xue."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, pages 786–794. Association for Compu-",
    "year": 2012
  }, {
    "title": "Chinese segmentation with a word-based perceptron algorithm",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "ACL.",
    "year": 2007
  }, {
    "title": "Exploring representations from unlabeled data with co-training for Chinese word segmentation",
    "authors": ["Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur."],
    "venue": "Proceedings of the 2013 Conference",
    "year": 2013
  }, {
    "title": "Deep learning for chinese word segmentation and pos tagging",
    "authors": ["Xiaoqing Zheng", "Hanyang Chen", "TianyuXu."],
    "venue": "EMNLP, pages 647–657. 1206",
    "year": 2013
  }],
  "id": "SP:63869c1b3b4890e22101bc491b459e9d9b355f9f",
  "authors": [{
    "name": "Xinchi Chen",
    "affiliations": []
  }, {
    "name": "Xipeng Qiu",
    "affiliations": []
  }, {
    "name": "Chenxi Zhu",
    "affiliations": []
  }, {
    "name": "Pengfei Liu",
    "affiliations": []
  }, {
    "name": "Xuanjing Huang",
    "affiliations": []
  }],
  "abstractText": "Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features aremostly extracted from a local context. Thesemethods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information inmemory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods.",
  "title": "Long Short-Term Memory Neural Networks for Chinese Word Segmentation"
}