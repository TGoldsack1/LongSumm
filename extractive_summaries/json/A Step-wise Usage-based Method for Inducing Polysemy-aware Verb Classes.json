{
  "sections": [{
    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "A verb plays a primary role in conveying the meaning of a sentence. Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP.\nVerb classes are one such lexical resource. Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior. Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009).\nThere have also been many attempts to automatically acquire verb classes with the goal of ei-\nther adding frequency information to an existing resource or of inducing similar verb classes for other languages. Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013). This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses. Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008).\nIn this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy.\nOur novel contributions are summarized as follows:\n• induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,\n• explicitly deal with verb polysemy, • discover effective features for each of the\nclustering steps, and\n• quantitatively evaluate a soft clustering of verbs.\n1030"
  }, {
    "heading": "2 Related Work",
    "text": "As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous. A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points. As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance.\nAmong these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods. Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions. They evaluated their result with a gold-standard test set, where a single class is assigned to a verb. Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features. Parisien and Stevenson (2011) extended their model by adding semantic features. They tried to account for verb learning by children and did not evaluate the resultant verb classes. Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985). All of the above methods considered verbs to be monosemous and did not deal with verb polysemy.\nOur approach also uses Bayesian methods, but is designed to capture verb polysemy.\nWe summarize a few studies that consider polysemy of verbs in the rest of this section.\nMiyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy. Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007). Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives.\nThe most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999). However, the verb itself is still represented as a single data point. After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering. They considered multiple classes only in the gold-standard data used for their evaluations. We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993).\nLapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb. These models are approximated to condition not\non verbs but on subcategorization frames. As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy. They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models. Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet. This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle. Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure.\nMaterna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013). LDA-frames are probabilistic semantic frames automatically induced from a raw corpus. He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles. Both of these are represented as a probabilistic distribution of words across verbs. He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012). He did not evaluate the resulting frames as verb classes.\nIn sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods."
  }, {
    "heading": "3 Our Approach",
    "text": ""
  }, {
    "heading": "3.1 Overview",
    "text": "Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches. Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost. For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011). Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce seman-\ntic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013). However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.\nIn this paper, we propose a two-step approach for inducing semantic frames and verb classes. First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)). To do that, we induce verb-specific semantic frames by clustering verb uses. Then, we induce verb classes by clustering these verbspecific semantic frames across verbs. An interesting point here is that we can use exactly the same method for these two clustering steps.\nOur procedure to automatically induce verb classes from verb uses is summarized as follows:\n1. induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and\n2. induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.\nEach of these two steps is described in the following sections in detail."
  }, {
    "heading": "3.2 Inducing Verb-specific Semantic Frames",
    "text": "We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014). Our semantic frames consist of case slots, each of which consists of word instances that can be filled. The procedure for inducing these semantic frames is as follows:\n1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses,\n2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and\n1In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU. To reach 1,000 iterations, which are reported to be optimum, it would take three months.\n3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames.\nThese three steps are briefly described below."
  }, {
    "heading": "3.2.1 Extracting Predicate-argument Structures from a Raw Corpus",
    "text": "We apply dependency parsing to a large raw corpus. We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases.\nThen, we extract predicate-argument structures from the dependency parses. Dependents that have the following dependency relations to a verb are extracted as arguments:\nnsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗\nIn this process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns.\nPredicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb."
  }, {
    "heading": "3.2.2 Constructing Initial Frames from Predicate-argument Structures",
    "text": "To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames. These initial frames are the input of the subsequent clustering process. For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures.\nFor each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation. We select an argument in the following order by considering the degree of effect on the verb sense:3\ndobj, ccomp, nsubj, prep ∗, iobj. Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame. After this process, we discard minor initial frames that occur fewer than 10 times.\n2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple preposi-\ntional phrases, one of them is randomly selected."
  }, {
    "heading": "3.2.3 Clustering Method",
    "text": "We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.\nWe calculate the posterior probability of a cluster cj given an initial frame fi as follows:\nP (cj |fi) ∝ { n(cj) N+α · P (fi|cj) cj ̸= new\nα N+α · P (fi|cj) cj = new,\n(1)\nwhere N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj . α is a hyperparameter that determines how likely it is for a new cluster to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi.\nP (fi|cj) is defined based on the DirichletMultinomial distribution as follows:\nP (fi|cj) = ∏ w∈V P (w|cj)count(fi,w), (2)\nwhere V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi. The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information. Here we experiment with both representations and compare the results.\nP (w|cj) is defined as follows:\nP (w|cj) = count(cj , w) + β∑ t∈V count(cj , t) + |V | · β , (3)\nwhere count(cj , w) is the current number of w in the cluster cj , and β is a hyper-parameter of Dirichlet distribution. For a new cluster, this probability is uniform (1/|V |).\nWe regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame. In this way, semantic frames for each verb are acquired.\nWe use Gibbs sampling to realize this clustering."
  }, {
    "heading": "3.3 Inducing Verb Classes from Semantic Frames",
    "text": "To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic\nframes. We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb. This is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames. We regard each output cluster as a verb class this time.\nFor the features, w, in equation (2), we try the two representations again: slot-only features and slot-word pair features. The representation using only slots corresponds to the consideration of only syntactic argument patterns. The other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information. We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps."
  }, {
    "heading": "4 Experiments and Evaluations",
    "text": "We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes. Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations. These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation. Finally, we discuss the results of our full experiments."
  }, {
    "heading": "4.1 Experimental Settings",
    "text": "We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus.\nTo prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information. Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences. From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words. We focused on verbs whose frequency in the web corpus was more than 1,000. There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions. We extracted 2,032,774,982 predicate-argument structures.\nWe also used the English Gigaword corpus (LDC2011T07; English Gigaword Fifth Edition). This corpus consists of approximately 180 million sentences, which totaling four billion words.\nThere were 7,356 verbs after applying the same frequency threshold as the web corpus. We extracted 423,778,278 predicate-argument structures from this corpus.\nWe set the hyper-parameters α in (1) and β in (3) to 1.0. The cluster assignments for all the components were initialized randomly. We took 100 samples for each input frame and selected the cluster assignment that has the highest probability."
  }, {
    "heading": "4.2 Evaluation Metrics",
    "text": "To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)). However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4 We propose a normalized version of modified purity and inverse purity. This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013).\nTo measure the precision of a clustering, a normalized version of modified purity is defined as follows. Suppose K is the set of automatically induced clusters and G is the set of gold classes. Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class. Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb. Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes. The core idea of purity is that each cluster Ki is associated with its most prevalent gold class. In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity. The normalized modified purity (nmPU) can then be written as follows:\nnmPU = 1 N ∑ i s.t. |Ki|>1 max j δKi(Ki ∩ Gj), (4)\nδKi(Ki ∩ Gj) = ∑\nv∈Ki∩Gj civ, (5)\n4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings.\nwhere N denotes the total number of verbs, |Ki| denotes the number of positive components in Ki, and civ denotes the v-th component of Ki. δKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩Gj , given by summing up the values in Ki. In case of evaluating a hard clustering, this is equal to |Ki ∩ Gj | because all the values of civ are equal to 1.\nAs usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering:\nniPU = 1 N ∑ j max i δGj (Ki ∩ Gj). (6)\nFinally, we use the harmonic mean (F1) of nmPU and niPU as a single measure of clustering quality."
  }, {
    "heading": "4.3 Type-level Multi-class Evaluations",
    "text": "We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997). It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous. The average number of verb classes per verb is 2.24. An excerpt from this data is shown in Table 1.\nAs our baselines, we adopt two previously proposed methods. We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003). They used the information bottleneck (IB) method for assigning probabilities of classes to each verb. Note that Korhonen et al. (2003) actually hardened the clusterings and left\nthe evaluations of soft clusterings for their future work. For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies. To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities. That is, classes that have a higher class attribute probability than the threshold are output for each verb. We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10.\nThe other baseline is LDA-frames (Materna, 2012). We use the induced LDA-frames that are\n5http://ilexir.co.uk/applications/valex/\navailable on the web site.6 This frame data was induced from the BNC and consists of 1,200 frames and 400 semantic roles. Again, we set a threshold for frame attribute probabilities.\nWe report results using our methods with four feature combinations (slot-only (S) and slot-word pair (SW) features each used for both the framegeneration and verb-class clustering steps) for both the Gigaword and web corpora. Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.\nWe can see that “web/SW-S” achieved the best performance and obtained a higher F1 than the baselines by more than nine points. “Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs. Interestingly, this result indicates that slot distributions are more effective than lexical information in slotword pairs for inducing verb classes similar to the gold standard. This result is consistent with expectations, given a gold standard based on Levin’s verb classes, which are organized according to the syntactic behavior of verbs. The use of slot-word pairs for verb class induction generally merged too many frames into each class, apparently due to accidental word overlaps across verbs.\nThe verb classes induced from the web corpus achieved a higher F1 than those from the Gigaword corpus. This can be attributed to the larger size of the web corpus. The employment of this kind of huge corpus is enabled by our scalable method.\n6http://nlp.fi.muni.cz/projekty/lda-frames/ 7Although we do not think that the classes with very small attribute probabilities are meaningful, the F1 scores for lower thresholds than 0.01 converged to about 66 in the case of LDA-frames."
  }, {
    "heading": "4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes",
    "text": "Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective. However, we wish to compare our method with previous work on the induction of a predominant (monosemous) class for each verb.\nTo output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb. Then, we apply clustering to these frames across verbs. For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW).\nWe evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003). This data contains 110 verbs and 33 classes. We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.\nAs we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes. It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense. When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity.\nFor baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012). The\nclusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon. To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb. This hardening process is exactly the same as Korhonen et al. (2003). Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8\nTable 3 lists accuracies of baseline methods and our methods. Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation. More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes.\nFrom the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases. This result is different from that of multi-class evaluations in Section 4.3. We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames."
  }, {
    "heading": "4.5 Token-level Multi-class Evaluations",
    "text": "We conduct token-level multi-class evaluations using 119 verbs, which appear 100 or more times in sections 02-21 of the SemLink WSJ corpus. These 119 verbs cover 102 VerbNet classes, and 48 of them are polysemous in the sense of being in more than one VerbNet class. Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes. We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances. Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes. We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1). It is not necessary to normalize these metrics because the clustering of these instances is hard.\n8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.\nFor clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3). The results of these methods are obtained by averaging five runs. For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs. Table 4 lists accuracies of these methods for the two corpora. We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).\nModi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus. We also conducted the above evaluation against FrameNet frames for 75 verbs.9 We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy."
  }, {
    "heading": "4.6 Full Experiments and Discussions",
    "text": "We finally induce verb classes from the semantic frames of 1,667 verbs, which appear at least once in sections 02-21 of the WSJ corpus. Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features. We ended with 38,481 semantic frames and 699 verb classes from the Gigaword\n9Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes.\ncorpus, and 61,903 semantic frames and 840 verb classes from the web corpus. It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.\nExamples of verb classes and semantic frames induced from the web corpus are shown in Table 5 and Table 6. While there are many classes with consistent meanings, such as “Class 4” and “Class 16,” some classes have mixed meanings. For instance, “Class 2” consists of the semantic frames “need:2” and “say:2.” These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement. To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way."
  }, {
    "heading": "5 Conclusion",
    "text": "We presented a step-wise unsupervised method for inducing verb classes from instances in gigaword corpora. This method first clusters predicateargument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes. Both clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process. The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10\n10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/\nFrom the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points. This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation. This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs.\nAs applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation. For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus. It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011)."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by Kyoto University John Mung Program and JST CREST. We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."
  }],
  "year": 2014,
  "references": [{
    "title": "Exchangeability and related topics",
    "authors": ["David Aldous."],
    "venue": "École d’Été de Probabilités de Saint-Flour XIII ―1983, pages 1–198.",
    "year": 1985
  }, {
    "title": "Latent Dirichlet allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "the Journal of Machine Learning Research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Modelling polysemy in adjective classes by multi-label classification",
    "authors": ["Gemma Boleda", "Sabine Schulte im Walde", "Toni Badia."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
    "year": 2007
  }, {
    "title": "Investigations into the role of lexical semantics in word sense disambiguation",
    "authors": ["Hoa Trang Dang."],
    "venue": "Ph.D. thesis, University of Pennsylvania.",
    "year": 2004
  }, {
    "title": "Generating typed dependency parses from phrase structure parses",
    "authors": ["Marie-Catherine de Marneffe", "Bill MacCartney", "Christopher D. Manning."],
    "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449–",
    "year": 2006
  }, {
    "title": "Large-scale dictionary construction for foreign language tutoring and interlingual machine translation",
    "authors": ["Bonnie J. Dorr."],
    "venue": "Machine Translation, 12(4):271–322.",
    "year": 1997
  }, {
    "title": "Classifying French verbs using French and English lexical resources",
    "authors": ["Ingrid Falk", "Claire Gardent", "Jean-Charles Lamirel."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 854–863.",
    "year": 2012
  }, {
    "title": "A weakly-supervised approach to argumentative zoning of scientific documents",
    "authors": ["Yufan Guo", "Anna Korhonen", "Thierry Poibeau."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 273–283.",
    "year": 2011
  }, {
    "title": "A general feature space for automatic verb classification",
    "authors": ["Eric Joanis", "Suzanne Stevenson", "David James."],
    "venue": "Natural Language Engineering, 14(3):337–367.",
    "year": 2008
  }, {
    "title": "A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis",
    "authors": ["Daisuke Kawahara", "Sadao Kurohashi."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, pages 176–183.",
    "year": 2006
  }, {
    "title": "Inducing example-based semantic frames from a massive amount of verb uses",
    "authors": ["Daisuke Kawahara", "Daniel W. Peterson", "Octavian Popescu", "Martha Palmer."],
    "venue": "Proceedings of the 14th Conference of the European Chapter of the Associa-",
    "year": 2014
  }, {
    "title": "VerbNet: A BroadCoverage, Comprehensive Verb Lexicon",
    "authors": ["Karin Kipper-Schuler."],
    "venue": "Ph.D. thesis, University of Pennsylvania.",
    "year": 2005
  }, {
    "title": "Clustering polysemic subcategorization frame distributions semantically",
    "authors": ["Anna Korhonen", "Yuval Krymolowski", "Zvika Marx."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 64–71.",
    "year": 2003
  }, {
    "title": "A large subcategorization lexicon for natural language processing applications",
    "authors": ["Anna Korhonen", "Yuval Krymolowski", "Ted Briscoe."],
    "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 345–352.",
    "year": 2006
  }, {
    "title": "Verb class disambiguation using informative priors",
    "authors": ["Mirella Lapata", "Chris Brew."],
    "venue": "Computational Linguistics, 30(1):45–73.",
    "year": 2004
  }, {
    "title": "English verb classes and alternations: A preliminary investigation",
    "authors": ["Beth Levin."],
    "venue": "The University of Chicago Press.",
    "year": 1993
  }, {
    "title": "Disambiguating Levin verbs using untagged data",
    "authors": ["Jianguo Li", "Chris Brew."],
    "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing.",
    "year": 2007
  }, {
    "title": "Which are the best features for automatic verb classification",
    "authors": ["Jianguo Li", "Chris Brew."],
    "venue": "Proceedings of ACL-08: HLT, pages 434–442.",
    "year": 2008
  }, {
    "title": "Learning syntactic verb frames using graphical models",
    "authors": ["Thomas Lippincott", "Anna Korhonen", "Diarmuid Ó Séaghdha."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 420–429.",
    "year": 2012
  }, {
    "title": "Combining lexical resources: mapping between PropBank and VerbNet",
    "authors": ["Edward Loper", "Szu-Ting Yi", "Martha Palmer."],
    "venue": "Proceedings of the 7th International Workshop on Computational Linguistics.",
    "year": 2007
  }, {
    "title": "LDA-frames: An unsupervised approach to generating semantic frames",
    "authors": ["Jiřı́ Materna"],
    "venue": "In Proceedings of the 13th International Conference CICLing 2012,",
    "year": 2012
  }, {
    "title": "Parameter estimation for LDAframes",
    "authors": ["Jiřı́ Materna"],
    "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2013
  }, {
    "title": "Supervised learning of a probabilistic lexicon of verb semantic classes",
    "authors": ["Yusuke Miyao", "Jun’ichi Tsujii"],
    "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2009
  }, {
    "title": "Unsupervised induction of frame-semantic representations",
    "authors": ["Ashutosh Modi", "Ivan Titov", "Alexandre Klementiev."],
    "venue": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7.",
    "year": 2012
  }, {
    "title": "Markov chain sampling methods for Dirichlet process mixture models",
    "authors": ["Radford M. Neal."],
    "venue": "Journal of computational and graphical statistics, 9(2):249– 265.",
    "year": 2000
  }, {
    "title": "Learning verb alternations in a usage-based Bayesian model",
    "authors": ["Christopher Parisien", "Suzanne Stevenson."],
    "venue": "Proceedings of the 32nd Annual Meeting of the Cognitive Science Society.",
    "year": 2010
  }, {
    "title": "Generalizing between form and meaning using learned verb classes",
    "authors": ["Christopher Parisien", "Suzanne Stevenson."],
    "venue": "Proceedings of the 33rd Annual Meeting of the Cognitive Science Society.",
    "year": 2011
  }, {
    "title": "Improved lexical acquisition through DPP-based verb clustering",
    "authors": ["Roi Reichart", "Anna Korhonen."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862–872.",
    "year": 2013
  }, {
    "title": "Type level clustering evaluation: New measures and a POS induction case study",
    "authors": ["Roi Reichart", "Omri Abend", "Ari Rappoport."],
    "venue": "Proceedings of the 14th Conference on Computational Natural Language Learning, pages 77–87.",
    "year": 2010
  }, {
    "title": "The effect of corpus size on case frame acquisition for discourse analysis",
    "authors": ["Ryohei Sasano", "Daisuke Kawahara", "Sadao Kurohashi."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the",
    "year": 2009
  }, {
    "title": "Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences",
    "authors": ["Sabine Schulte im Walde", "Christian Hying", "Christian Scheible", "Helmut Schmid."],
    "venue": "Proceedings of ACL-08: HLT, pages 496–",
    "year": 2008
  }, {
    "title": "Experiments on the automatic induction of German semantic verb classes",
    "authors": ["Sabine Schulte im Walde."],
    "venue": "Computational Linguistics, 32(2):159–194.",
    "year": 2006
  }, {
    "title": "Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing",
    "authors": ["Lei Shi", "Rada Mihalcea."],
    "venue": "Computational Linguistics and Intelligent Text Processing, pages 100–111. Springer.",
    "year": 2005
  }, {
    "title": "Metaphor identification using verb and noun clustering",
    "authors": ["Ekaterina Shutova", "Lin Sun", "Anna Korhonen."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002–1010.",
    "year": 2010
  }, {
    "title": "Detecting polysemy in hard and soft cluster analyses of German preposition vector spaces",
    "authors": ["Sylvia Springorum", "Sabine Schulte im Walde", "Jason Utt."],
    "venue": "Proceedings of the 6th International Joint Conference on Natural Language Processing, pages",
    "year": 2013
  }, {
    "title": "Semisupervised verb class discovery using noisy features",
    "authors": ["Suzanne Stevenson", "Eric Joanis."],
    "venue": "Proceedings of the 7th Conference on Natural Language Learning, pages 71–78.",
    "year": 2003
  }, {
    "title": "An effective discourse parser that uses rich linguistic information",
    "authors": ["Rajen Subba", "Barbara Di Eugenio."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computa-",
    "year": 2009
  }, {
    "title": "Improving verb clustering with automatically acquired selectional preferences",
    "authors": ["Lin Sun", "Anna Korhonen."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638–647.",
    "year": 2009
  }, {
    "title": "Automatic classification of English verbs using rich syntactic features",
    "authors": ["Lin Sun", "Anna Korhonen", "Yuval Krymolowski."],
    "venue": "Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 769–774.",
    "year": 2008
  }, {
    "title": "Diathesis alternation approximation for verb clustering",
    "authors": ["Lin Sun", "Diana McCarthy", "Anna Korhonen."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Short Papers, pages 736–741.",
    "year": 2013
  }, {
    "title": "Exploiting a verb lexicon in automatic semantic role labelling",
    "authors": ["Robert Swier", "Suzanne Stevenson."],
    "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages",
    "year": 2005
  }, {
    "title": "Hierarchical Dirichlet processes",
    "authors": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei."],
    "venue": "Journal of the American Statistical Association, 101(476).",
    "year": 2006
  }, {
    "title": "The information bottleneck method",
    "authors": ["Naftali Tishby", "Fernando C. Pereira", "William Bialek."],
    "venue": "Proceedings of the 37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377.",
    "year": 1999
  }, {
    "title": "A Bayesian approach to unsupervised semantic role induction",
    "authors": ["Ivan Titov", "Alexandre Klementiev."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22.",
    "year": 2012
  }, {
    "title": "Unsupervised and constrained Dirichlet process mixture models for verb clustering",
    "authors": ["Andreas Vlachos", "Anna Korhonen", "Zoubin Ghahramani."],
    "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages",
    "year": 2009
  }, {
    "title": "One sense per collocation",
    "authors": ["David Yarowsky."],
    "venue": "Proceedings of the Workshop on Human Language Technology, pages 266–271.",
    "year": 1993
  }],
  "id": "SP:61b92df5708a6120f4e946402f06765ea715a35a",
  "authors": [{
    "name": "Daisuke Kawahara",
    "affiliations": []
  }, {
    "name": "Daniel W. Peterson",
    "affiliations": []
  }, {
    "name": "Martha Palmer",
    "affiliations": []
  }],
  "abstractText": "We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora. Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames. By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering. In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words. The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.",
  "title": "A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes"
}