{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A wide range of data mining, machine learning and social network analysis problems can be modeled as graph mining tasks on large graphs. The ability to analyze layers of connectivity is useful to understand the hierarchical structure of the input data and the role of nodes in different networks. A commonly used technique for this task is the k-core decomposition: a k-core of a graph is a maximal subgraph where every node has induced degree at least k. k-core decomposition has many real world applications from understanding dynamics in social networks (Bhawalkar et al., 2012) to graph visualization (Alvarez-Hamelin et al., 2005), from describing protein functions based on protein-protein networks (Altaf-Ul-Amin et al., 2006) to computing network centrality measures (Healy et al., 2006). k-core is also widely used as a sub-routine for community detection algorithms (Chester et al., 2012; Mitzenmacher et al., 2015) or for finding dense clusters in graphs (Lee et al., 2010; Mitzenmacher et al., 2015). As a graph theoretic tool k-core decomposition has been used to solve the densest subgraph\n1Google Research. Correspondence to: Hossein Esfandiari <esfandiari@googol.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nproblem (Lee et al., 2010; Bahmani et al., 2012; Epasto et al., 2015; Esfandiari et al., 2015).\nk-core is often use a feature in machine learning systems with applications in network analysis, spam detection and biology. Furthermore, in comparison with other densitybased measure as the densest subgraph, it has the advantage to assign a score to every node in the network. Finally, the k-core decomposition induce a hierarchical clustering on the entire network. For many applications in machine learning and in data mining, it is important to be able to compute it efficiently on large graphs.\nIn the past decade, with increasing size of data sets available in various applications, the need for developing scalable algorithms has become more important. By definition, the process of computing k-core decomposition is sequential: in order to find the k-core, one can keep removing all nodes of degree less than k from the remaining graph until there is no such a node. As a result, computing k-cores for big graphs in distributed systems is a challenging task. In fact, while k-core decomposition has been studied extensively in the literature and many efficient decentralized and streaming heuristics have been developed for this problem (Montresor et al., 2013; Sarayuce et al., 2015), nevertheless developing a distributed or a streaming algorithm with provable guarantees for k-core decomposition problem remains an unsolved problem. One difficulty in tackling the problem is that simple non-adaptive sampling techniques used for similar problems as densest subgraph (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) do not work here (See Related Work for details). In this paper, we tackle this problem and present the first parallel and streaming algorithm for this problem with provable approximation guarantee. We do so by defining an approximate notion of k-core, and providing an adaptive space-efficient sketching technique that can be used to compute an approximate k-core decomposition efficiently. Roughly speaking, a 1− -approximate k-core is an induced subgraph that includes the k-core, and such that the induced degree of every node is at least (1− )k.\nOur Contributions. As a foundation to all our results, we provide a powerful sketching technique to compute a 1− - approximate k-core for all k simultaneously. Our sketch is adaptive in nature and it is based on a novel iterative\nedge sampling strategy. In particular, we design a sketch of size Õ(n) that can be constructed in O(log n) rounds of sampling1.\nWe then show the first application of our sketching technique in designing a parallel algorithm for computing the k-core decomposition. More precisely, we present a MapReducebased algorithm to compute a 1 − approximate k-core decomposition of a graph in O(log n) rounds of computations, where the load of each machine is Õ(n), for any ∈ (0, 1].\nMoreover, we show that one can implement our sketch for k-core decomposition in a streaming setting in one pass using only Õ(n) space. In particular, we present a one-pass streaming algorithm for 1− -approximate k-core decomposition of graphs with Õ(n) space.\nFinally, we show experimentally the efficiency and accuracy of our sketching algorithm on few real world networks.\nRelated Work. The k-core decomposition problem is related to the densest subgraph problem. Streaming and turnstile algorithms for the densest subgrpah problem have been studied extensively in the past (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016). While these problems are related, the theoretical results known for the densest subgraph problem are not directly applicable to the k-core decomposition problem.\nThere are two types of algorithms for the densest subgraph problem in the streaming. First type of algorithms simulates the process of iteratively removing vertices with small degrees (Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016). All of these results are based on the fact that we only need logarithmic rounds of probing to find a 1/2 approximation of the densest subgraph (Bahmani et al., 2012). However, this can not be used to 1− approximate the k-coreness numbers.\nThe second type of algorithms do a (non-adaptive) single pass and use uniform samplings of edges (Esfandiari et al., 2015; Mitzenmacher et al., 2015; McGregor et al., 2015). These results are based on the fact that the density of the optimum solution is proportional to the sampling rate with high probability, where the probability of failure is exponentially small. There are two obstacles toward applying this approach to approximating a k-core decomposition. First, by using uniform sampling it is not possible to obtain a (1− ) approximation of the coreness number for nodes of constant degree (unless we do not sample all edges with probability one). Second, in order to achieve Õ(n) space, we can only sample Õ(1) edges per vertex. Hence, the\n1In the paper, we use the notation Õ(·) to denote the fact that poly-logarithmic factors are ignored.\nprobability that the degree of a vertex in the sampled is not proportional to the sampling rate, is not exponentially small anymore. Therefore it is not possible to union bound over exponentially many feasible solutions. To overcome this issue, we analyze the combinatorial correlation between feasible solutions and wisely pick polynomially many feasible solutions that approximate all of the feasible solutions. To the best of our knowledge this is the first work that analyzes the combinatorial correlation of different feasible solution on a graph.\nIn recent years the k-core decomposition problem received a lot of attention (Bhawalkar et al., 2012; Montresor et al., 2013; Aksu et al., 2014; Sarayuce et al., 2015; Zhang et al., 2017), nevertheless we do not know of any previous distributed algorithms with small bounded memory and number of rounds. A recent related paper is (Sarayuce et al., 2015) where the authors present a streaming algorithm for the kcore decomposition problem. While the authors report good empirical results for their algorithm, they do not provide a guarantee for this problem, e.g., they do not prove an upper bound on the memory complexity of this algorithm. Finally we note that Monteresor et al. (Montresor et al., 2013) provide a distributed algorithm for this problem in a vertexcentric model. Although their model is different from our, more classic, MapReduce setting and their bound on the number of rounds is linear instead we achieve a logarithmic bound."
  }, {
    "heading": "2. Preliminaries",
    "text": "In this section, we introduce the main definitions and the computational models that we consider in the paper. We start by defining k-core and by introducing the concept of approximate k-core. Then we describe the MapReduce and streaming models.\nApproximate k-core. Let G = (V,E) be a graph with |V | = n nodes and |E| = m edges. Let H be a subgraph of G, for any node v ∈ G we denote by d(v) the degree of the node in G and for any node v ∈ H we denote by dH(v) the degree of v in the subgraph induced by H . A k-core is a maximal subgraph H ⊆ G such that ∀v ∈ H we have dH(v) ≥ k. Note that for any k the k-core is unique and it may be possibly disconnected. We say that a vertex v has coreness number k if it belongs to the k-core but it does not belong to the (k + 1)-core. We denote the coreness number of node i in the graph G with CG(i)(we drop the subscript notation when the graph is clear from the context).\nWe define the core labeling for a graph G as the labeling where every vertex v is labeled with its coreness number. It is wroth noting that this labeling is unique and that it defines a hierarchical decomposition of G.\nIn this paper we are interested in computing a good approxi-\nmation of the core labeling for a graph G efficiently in the MapReduce and in the streaming model. For this reason, we introduce the concept of 1− approximate k-core. We define a 1 − approximation to the k-core of G to be a subgraph H of G that contains the k-core of G and such that ∀v ∈ H we have dH(v) ≥ (1 − )k. In other words, a 1− approximation to the k-core of G is a subgraph of the (1− )k-core of G and supergraph of the k-core of G. In Figure 1 we present the 3-core for a small graph and a 2 3 -approximate 3-core.\nSimilarly, a 1− approximate core-labeling of a graph G is a labeling of the vertices in G, where each vertex is labelled with a number between its coreness number and its coreness number multiplied by 11− .\nIn the paper we often refer to the classic greedy algorithm (Matula & Beck, 1983)(also known as peeling algorithm) to compute the coreness number. The algorithm works as follows: nodes are removed from the graph iteratively. In particular, in iteration i of the algorithm all nodes with degree smaller or equal to i are removed iteratively and they are assigned coreness number i. It is possible to show that the algorithm computes the correct coreness number of all nodes in the graph and it can be implemented in linear time.\nMapReduce model. Here we briefly recall the main aspect of the model by Karloff et al. (Karloff et al., 2010) of the MapReduce framework (Dean & Ghemawat, 2010).\nIn the MapReduce model, the computation happens in parallel in several rounds. In each round, data is analyzed on each machine in parallel and then the output of the computations are shuffled between machines. The model has two main restrictions, one on the total number of machines and another on the memory available on each machine. More specifically, given an input of size N , and a small constant > 0, in the model there are N1− machines, each with N1− memory available. Note that, the total amount of memory available to the entire system is O(N2−2 ).\nThe efficiency of an algorithm is measured by the number of the “rounds” needed by the algorithm to terminate. Classes of algorithms of particular interest are the ones that run in a constant or poly-logarithmic number of rounds.\nStreaming We also analyze the approximate core labelling problem in the streaming model (Munro & Paterson, 1980). In this model the input consists of an undirected graph G = (V,E) and the input is presented as a stream of edges. The goal of our algorithm is to obtain a good approximation of the core labelling at the end of the stream using only small memory (Õ(n)).\n3. Sketching k-Cores In this section we present a sketch to compute an approximate core labelling that uses only O(npolylog(n)) space. Compared with previous sketching for similar problems (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) our sketching samples different area of the graphs with different, carefully selected, probabilities.\nThe main idea behind the sketch is to sample edges more aggressively in denser areas of the graph and less aggressively in sparser areas. More specifically, the algorithm works as follows: we start by sampling edges with some small probability, p, so that the resulting sampled graph, H , is sparse. We then compute the coreness numbers for the vertices in H . The key observation is that if a vertex has logarithmic coreness number inH we can precisely estimate its coreness number in the input graph G. Furthermore we can show that if a vertex has large enough coreness number in the input graph G it will have at least logarithmic coreness number in H . So using this technique we can detect efficiently all nodes with sufficiently high coreness number. To compute the coreness numbers of the rest of the node in the graph, we first remove from the graph the nodes for which we have a good estimation and then we iterate the same approach. In particular we double our sample probability p and sample edges again. Interestingly, we can show that by sampling edges adaptively, we can iteratively estimate the coreness of all nodes in the graph by analyzing only sparse subgraphs.\nWe are now ready to describe our sketching algorithm in details. We start by describing a basic subroutine that estimates a modified version of the coreness number. We dubbed the subroutine ExclusiveCorenessLabeling. The subroutine takes as input a subgraph, H , and a subset of the vertices Λ ⊆ H and it runs a modified version of the classic peeling algorithm (Matula & Beck, 1983) to compute the coreness number. The main difference between ExclusiveCorenessLabeling and the peeling algorithm in (Matula & Beck, 1983) is that we do not compute labels for nodes in Λ and we do not remove them from the subgraph H . The pseudocode for ExclusiveCorenessLabeling is presented in Algorithm 1.\nDuring the execution of the algorithm we use subroutine ExclusiveCorenessLabeling to compute a labelling for the\nAlgorithm 1 ExclusiveCorenessLabeling(H,Λ) 1: Input: A graph H with n vertices and a set Λ ⊆ VH . 2: Initialize Γ = VH \\ Λ 3: Initialize l← 0 4: while Γ 6= ∅ do 5: while minv∈Γ(dH(v)) ≤ l do 6: Let v ← argminv∈Γ(dH(v)) 7: Set lv ← l 8: Remove v from Γ 9: Remove v from H 10: end whilel← l + 1 11: end while\nsubset of the nodes in H for which we do not have already a good estimate of the coreness number.\nNow we can formally present our algorithm, we start by sampling the graph G with p ∈ O ( logn 2n ) . In this way, we obtain a sparse graph H0. Then we run ExclusiveCorenessLabeling with H = H0 and Λ = ∅ to obtain a labeling of the nodes in H0. Let l0(i) be the label of vertex i in this labeling. If a vertex i has l0(i) ≥ C log n, for a specific constant C > 0, we can estimate its coreness number in G precisely. Intuitively this is true because we are sampling the edges independently so we can use concentration results to bound its coreness number. Hence, in the first round of our algorithm we can compute a precise estimate of the coreness number for all nodes i with l0(i) ≥ C log n.\nIn the rest of the execution of our algorithm we can recurse on the remaining nodes. To do so, we add the nodes with a good estimate to the set Λ and we remove from G the edges in the subgraph induced by the nodes in Λ. Then we increase the sampling probability p by 2 and sample G again. Similarly we obtain a new subgraph H1 and we run ExclusiveCorenessLabeling with H = H1 and Λ equal to the current Λ. So we obtain a labeling l1 for the nodes in H1\\Λ. and also in this case if a vertex i has l1(i) ≥ C log n, for a specific constant C > 0, we can estimate its coreness number in G precisely.\nWe iterate this algorithm for log n steps. In the remaining of the section we first present pseudocode of our sketching algorithm(Algorithm 2) then we show that at the end of the execution of the algorithm we have a good estimation of the coreness number for all nodes in G. Finally we argue that in every iteration the graphs Hi are sparse so the algorithm uses only small memory at any point in time.\nWe start by providing the pseudocode of the algorithm in Algorithm 2.\nWe are now ready to prove the main properties of our sketching technique. We start by stating two technical lemma whose proofs follow from application of concentration bounds and are deferred to the full version. The main goal of the lemma is to relate the degree of a vertex v in a\nAlgorithm 2 A sketch based algorithm to compute 1 − O( ) approximate core-labeling.\n1: Input: A graph G with n vertices and parameter ∈ (0, 1]. 2: Initialize Λ← ∅ 3: Initialize p0 ← 12 logn 2n 4: for j = 0 to logn do 5: Let Hj be a subgraph of G with the edges sampled independently with probability pj 6: Run Exclusive Core Labeling(Hj ,Λ) and denote the label of vertex i on Hj by lj(i) 7: for i ∈ Hj do 8: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 9: // Node i has sufficiently high degree to estimate its\ncoreness number. 10: if lj(i) ≤ 48 logn 2 then 11: Set the label of vertex i to (1− ) lj(i)\npj\n12: Add i to Λ 13: else 14: Set the label of vertex i to 2(1− )n 2j−1 15: Add i to Λ 16: end if 17: end if 18: end for 19: Remove from G the edges of G induced by Λ 20: pj+1 ← 2pj 21: end for\nsubgraph of G and the sampled subgraph H .\nLemma 3.1. Let G be a graph and let ∈ (0, 1] and δ ∈ (0, 1) be two arbitrary numbers. Let f(n) be a function of n such that f(n) ≥ 6 log n δ\n2 and let H be a subgraph of G that contains each edge of G independently with probability p ≥ 6 log n δ\n2f(n) . Then for all v ∈ G the following statements holds, with probability 1 − δ3n2 : (i) If dG(v) ≥ f(n) we have |dH(v)− pdG(v)| ≤ dH(v), (ii) If dG(v) < f(n) we have dH(v) < 2pf(n).\nLemma 3.2. Let G be a graph and let ∈ (0, 1] and δ ∈ (0, 1) be two arbitrary numbers. Let f(n) be a function of n such that f(n) ≥ 6 log n δ\n2 and let H be a subgraph of G that contains each edge of G independently with probability p ≥ 6 log n δ\n2f(n) . For all v ∈ G the following statements holds, with probability 1 − δ3n2 : (i) If dH(v) ≥ 2pf(n) we have |dH(v) − pdG(v)| ≤ dH(v), (ii) If dH(v) < 2pf(n) we have dG(v) ≤ 2(1 + )f(n). In addition, in the first case we have dG(v) ≥ 2(1− )f(n). Furthermore, if the graph is directed the same claims hold for the in-degree(d−(v)) and the out-degree(d+(v)) of a node v.\nIn the remaining of this section we assume that Lemma 3.1 and Lemma 3.2 hold and using this assumption we prove the main properties of our algorithm.\nWe start by comparing the labels computed by Algorithm 1 with the coreness number of its input graph. Recall that we denote the coreness number of node i in the graph G with CG(i).\nLemma 3.3. Let H = (V,E) be an arbitrary graph and let Λ ⊆ V be an arbitrary set of vertices. Let Ĥ = (Λ, Ê) be an arbitrary graph on the set of vertices Λ, and let H ′ = H ∪ Ĥ . Let lv be the label computed by ExclusiveCorenessLabeling(H,Λ). Then for each vertex v ∈ V \\ Λ we have: (i) lv ≥ CH′(v), (ii) if CH′(v) ≤ minu∈Λ ( CH′(u) ) , we have lv = CH′(v).\nProof. By definition of coreness number, if we iteratively remove all vertices with degree CH′(v)− 1 from H ′, vertex v is not removed from the graph. Furthermore note that Algorithm 1 does not remove any vertex with degree more than CH′(v) − 1 unless CH′(v) − 1 < l. Thus, we have lv ≥ CH′(v) as desired.\nNote that, if we set Λ = ∅, Algorithm 1 acts as the greedy algorithm that computes the coreness numbers. Moreover notice that if CH′(v) ≤ minu∈Λ ( CH′(u) ) , the classic peeling algorithm does not removed any of the vertices in Λ until it does not consider nodes with degree smaller or equal than l. Therefore we have lv ≥ CH′(v) which proves the second statement of the theorem.\nWe are now ready to state the two main Lemma proving the quality of the solution computed by our sketching technique.\nLemma 3.4. For all 0 ≤ j ≤ log n such that pj ≤ 1 and for any node v added to Λ in round j we have with probability 1− 13n that: C(v) < 2(1 + ) n 2j−1 .\nFurthermore for all 0 ≤ j ≤ log n such that pj < 1 we have with probability 1− 13n that: C(v) ≥ 2(1− ) n 2j . Lemma 3.5. Algorithm 2 computes a 1− 2 approximate core labeling, with probability 1− 23n .\nThe proofs of the Lemma is presented in the full version.\nNow we give a lemma that bounds the total number of edges used in sketches H0, H2, . . . ,Hρ. Lemma 3.6. The number of edges in ∪ρi=0Hi produced by Algorithm 2 is upper bounded by O ( n log2 n 2 ) , with proba-\nbility 1− 1n .\nProof. In the proof, we assume that the statement of Lemma 3.4 holds, and the statements of Lemma 3.1 and 3.2 hold for Hj,k and Hj,v for all choices of j and k and v.\nConsider an arbitrary 0 ≤ j ≤ log n. From Lemma 3.4, we have that for any v ∈ Hj \\ (∪j−1i=0 Λj) the coreness number of v is bounded by 2(1 + ) n2j−1 . Now consider an orientation of the edges of Hj where every edge is oriented to its endpoint of smallest core number, breaking the ties in such a way that the in-degree of every node v, d−(v) is upperbounded by C(v)2. Furthermore note\n2Note that such an orientation exists, in fact it can be obtained\nthat every edge in Hj is incident to a node of coreness number at most 2(1 + ) n2j−1 , so using Lemma 3.1 we have that in-degree of every node in Hj is bounded by 2(1 + )2 n2j−1 p j = 48 (1+ ) 2\n2 log n. So summing over all the in-degrees we get that the number of edges in Hj is bounded by 48 (1+ ) 2\n2 n log n. We conclude the proof by noticing that there are at most log n different Hj so the total memory used is 48 (1+ ) 2 2 n log 2 n.\nPutting together Lemma 3.5 and Lemma 3.6 we get the main theorem of this section.\nTheorem 3.7. Algorithm 2 computes a 1− 2 approximate core labeling and the total spaced used by the algorithm is O ( n log2 n 2 ) , with probability 1− 2n ."
  }, {
    "heading": "4. MapReduce and Streaming Algorithms",
    "text": "In this section we show how to compute our sketch efficiently using a MapReduce or a streaming algorithm."
  }, {
    "heading": "4.1. MapReduce algorithm",
    "text": "Here, we show how to use implement the sketch introduced in Section 3 in the MapReduce model. In this way we obtain an efficient MapReduce algorithm for dense graphs3.\nRecall that the main limitation of the MapReduce model is on the number of machines and on the available memory on each machine. Our algorithm runs for 2 log n rounds.4 In the first round of MapReduce, the edges are sampled in parallel with probability p0 = 12 logn 2n . In this way, we obtain a graph H0 that we analyze in the second round in a single machines(note that we can do it because from Lemma 3.6 we know that for all i the number of edges inHi is bounded by O ( n log2 n 2 ) . At the end of the second round,\nwe obtain the labeling for the nodes with high coreness number and we add them to the set Λ0. In the third round we send the set Λ0 to all the machined and we sample in parallel the edges in |E|\\Λ0 with probability 2p0 in a round of MapReduce. In this way, we obtain H1 that in the fourth round is analyzed by a single machine to obtain the labelling of few additional nodes that are added to Λ1. By iterating this process for 2 log n rounds, we obtain an approximation of the coreness number for each node. The pseudo-code for the MapReduce algorithm is presented in Algorithm 3.\nby orienting every edges to its endpoint that is first removed by the classic peeling algorithm used to compute the coreness number.\n3It is important to note that we only use polylogarithmic memory for each machine so our algorithm works also in more restrictive parallel models as the massively parallel model (Andoni et al., 2014; Im et al., 2017)\n4The algorithm can be implemented using logn MapReduce rounds, but for simplicity, here we present a 2 logn rounds version.\nAlgorithm 3 A MapReduce algorithm to compute 1 − O( )- approximate core-labeling.\n1: Input: A graph G with n vertices and parameter ∈ (0, 1]. 2: Initialize Λ← ∅ 3: Initialize p0 ← 12 logn 2n 4: for j = 0 to logn do 5: // First round of MapReduce 6: Send Λ to all machines 7: Let E′ be the set of edges of G that are not contained in the graph induced by Λ on G 8: Sample with probability pj in parallel using n machines\nthe edges in E′\n9: // Second round of MapReduce 10: Send all the sampled edge to a single machine 11: Let Hj be the sampled subgraph of G 12: Run Exclusive Core Labeling(Hj ,Λ) and denote the label of vertex i on Hj by lj(i) 13: for i ∈ Hj do 14: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 15: // Node i has sufficiently high degree to estimate its coreness number. 16: if lj(i) ≤ 48 logn 2 then 17: Set the label of vertex i to (1− ) lj(i)\npj\n18: Add i to Λ 19: else 20: Set the label of vertex i to 2(1− )n 2j−1 21: Add i to Λ 22: end if 23: end if 24: end for 25: pj+1 ← 2pj 26: end for\nBy Theorem 3.7 presented in the previous section we obtain the following corollary.\nCorollary 4.1. Let G = (V,E) be a graph such that |E| ∈ Ω(|V |1+γ), for some constant γ > 0. Then there is an algorithm that computes w.h.p. an approximate core-labeling of the graph in the MapReduce model using O(log n) rounds of MapReduce."
  }, {
    "heading": "4.2. Semi-streaming algorithms",
    "text": "Next we show an application of our sketch in the streaming setting. We consider the setting where edges are only added to the graph. The main idea behind our streaming algorithm is to maintain at any point in time the sketch presented in Section 3, which requires only Õ(n) space. In the remaining of the section we describe how we can maintain the sketching in streaming.\nWhen an edge is added to G, we check by sampling if it is H0. In this case in H0, we recompute the labeling of H0 and if one of the endpoints of the edge is added to Λ0, we update the rest of the sketch to reflect this change. Then, if both endpoints of the edge are not contained in Λ0, we check by sampling if the edge is contained in H1. Also in this case, if it is inH1, we recompute the labeling ofH1 and\nAlgorithm 4 A streaming algorithm to compute 1 − O( ) approximate core-labeling.\n1: Input: Stream of edges and parameter ∈ (0, 1]. 2: Initialize Λi ← ∅, ∀i 3: Initialize p0 ← 12 logn 2n 4: Insertion of (u, v) 5: r ← random number from [0, 1] 6: for j = 0 to logn do 7: if v /∈ ∪i<jΛi or u /∈ ∪i<jΛi then 8: if r ≤ pj then 9: Add (u, v) to Hj\n10: Run Exclusive Core Labeling(Hj ,Λi) and denote the label of vertex i on Hj by lj(i) 11: for i ∈ Hj do 12: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 13: if lj(i) ≤ 48 logn 2 then 14: Set the label of vertex i to (1− ) lj(i)\npj\n15: Add i to Λj 16: else 17: Set the label of vertex i to 2(1− )n 2j−1 18: Add i to Λj 19: end if 20: end if 21: end for 22: for j′ = j + 1 to logn do 23: Remove from Hj′ any edge induced by Λj 24: end for 25: end if 26: else 27: Break 28: end if 29: pj+1 ← 2pj 30: end for\nmodify the sketch accordingly. We continue this procedure until both endpoints of the edge are contained in Λ. Notice that, by inserting edges ∪i≤jΛis may only grow. Hence if at some point both endpoints of an edge (u, v) are in ∪i≤jΛi, by inserting more edge u and v remain in ∪i≤jΛi.\nThe pseudo-code for the streaming algorithm is presented in Algorithm 4(Note that here for simplicity we recompute the core labels after the insertion of an edge (u, v). However, one might recurse over the neighborhood of u and v and update the core labels locally).\nBy Theorem 3.7 presented in the previous section we obtain the following corollary.\nCorollary 4.2. There exists a streaming algorithm that computes w.h.p. an (1 − )-approximate core-labeling of the input graph using O ( n log2 n 2 ) space."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we analyze the performances of our sketch in practice. First we describe our datasets. Next we discuss the implementations of our sketch presented in Section 3 and\nour MapReduce algorithm. Then we study the scalability and the accuracy of our sketch. In particular, we analyze the trade-off between quality of the approximation and space used by the sketch.\nDatasets. We apply our sketch to eight real-world graphs available in the SNAP, Stanford Large Network Dataset Library (Leskovec & Sosič, 2016): Enron (Klimt & Yang, 2004), Epinions (Richardson et al., 2003), Slashdot (Leskovec et al., 2009), Twitter (McAuley & Leskovec, 2012), Amazon (Yang & Leskovec, 2015), Youtube (Yang & Leskovec, 2015), LiveJournal (Yang & Leskovec, 2015), Orkut (Yang & Leskovec, 2015) with respectively 36692, 75879, 82168, 81306, 334863, 1134890, 3997962 and 3072441 nodes and 183831, 508837, 948464, 1768149, 925872, 2987624, 34681189 and 117185083 edges.\nImplementation details. In order to have an efficient implementation of our sketch, we modify Algorithm 2 slightly. More specifically, we change line 14 to “if lj(i) ≥ T ∨pj = 1” where T is a parameter of our implementation. Furthermore, we also modify line 22 to “pj+1 ← M · pj , where M is modifiable multiplicative factor (that in Algorithm 2 is fixed to 2). We also slightly modify our MapReduce algorithm to remove iteratively in parallel all nodes with degree smaller than 3 before sending the remaining graph to a single machine.\nMetrics. To study the scalability of the algorithm we implement our MapReduce algorithm in distributed setting and we analyze the running time on different graphs by using a fixed number of machine. To evaluate the quality of our sketch, we consider the quality of the approximation and the space used.\nFor the quality of the approximation, we report the median error and the error at the 60, 70, 80 and 90 percentile of our algorithm. In interest of space, we report the errors only on nodes with coreness number at least 5, because high coreness number are harder to approximate and for almost all the nodes of coreness smaller than 5 we have errors close to 0.\nFor space we consider the maximum size of any sample graph Hi and the sum of their sizes. Note that the first quantity bounds the memory used by our distributed algorithm or a multi-pass streaming algorithm, and the second one bounds the memory used by a single pass streaming algorithm.\nScalability Results. In Figure 2 we present the results of our scalability experiments(In the experiment we fix T = 4 and M = 2). On the x axis we order the graphs based on their number of edges, in the y axis we show the relative running time on different graphs. Note that in the Figure\nthe x axis is in logscale and the y axis is in linear scale so the running time of our algorithm grows sublinearly in the number of edges in the graph proving that our algorithm is able to leverage parallelization to obtain good performance.\nFor comparison we also run a simple iterative algorithm to estimate the k-core number that resembles a simple adaptation of the algorithm presented in (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) for densest subgraph. The adapted algorithm works as follows: it removes all nodes below a threshold T (initially equal to 4) from the graphs in parallel and estimate their coreness number as T . Then when no node with degree smaller than T is left, it iteratively increases T by a multiplicative factor M = 2 and recurse on the remaining graph. Interestingly we observe that this adapted algorithm is an order of magnitude slower than our distributed algorithm and so we could run it only on relatively small graphs like Amazon.5\nAccuracy Results. All the reported number are the average over 3 runs of our algorithm. In all our experiment we either fix T = 3 and vary M or fix M to 2 and vary T . In Table 1 we present the space used in our algorithm when we vary the value of T .\n5Note that the parallel version of the simple iterative algorithm is particularly slow in practice because it needs several parallel rounds to complete.\nThere are few interesting things to note. First, the size of the maximum sampled graph is always significantly smaller than the size of input graph and in some cases it is more than one order of magnitude smaller (for example in the Twitter case). Interestingly, note that the relative size of the maximum sampled graph decrease with the size of the input graph. This suggests that the sketch would be even more effective when applied to a larger graph. Also the total size of the sketch is also smaller than the size of the graph in many cases (for instance, the sketch for Twitter is always smaller than half of the size of the input graph). This implies that we can compute an approximation of the coreness number without processing most of the edges in the input graph.\nIn Figure 3, we report the approximation error of our algorithm. First we note that as T increases the approximation error decreases as predicted by our theorems. It is also interesting to note that the median error is always below 50% and with T ≥ 3 is below 25%. Observe for T ≥ 3, the error at the 90 percentile is below 50%. Overall our sketch provides a good approximation of the coreness numbers.\nNow we focus on the effect of M on our sketch. In Table 2, we present the space used by our algorithm as a function of M . Note that the maximum size of a single sample graph decrease with M , but the total size of the sketch increases (this is due to the increased number of sampled graphs). This suggests that we should use small M in distributed settings where we have tighter space constraint and larger M when\nwe want to design single pass streaming algorithms.\nFinally it is interesting to note that as shown in Figure 4, the quality of the approximation is not very much influenced by the scaling factor M ."
  }, {
    "heading": "6. Conclusions and future works",
    "text": "In this paper we introduce a new sketching technique for computing the core-labeling of a graph. In particular, we design efficient MapReduce and streaming algorithms to approximate the coreness number of all the nodes in a graph efficiently. We also confirm the effectiveness of our sketch via an empirical study. The most interesting open problem in the area is to design a fully dynamic algorithm (Italiano et al., 1999) to maintain the core-labeling of a graph by using only polylog n operations per update(edge addition or deletion)."
  }],
  "year": 2018,
  "references": [{
    "title": "Distributed $k$ -core view materializationand maintenance for large dynamic graphs",
    "authors": ["H. Aksu", "M. Canim", "Y. Chang", "I. Korpeoglu", "Ö. Ulusoy"],
    "venue": "IEEE Trans. Knowl. Data Eng.,",
    "year": 2014
  }, {
    "title": "Development and implementation of an algorithm for detection of protein complexes in large interaction networks",
    "authors": ["M. Altaf-Ul-Amin", "Y. Shinbo", "K. Mihara", "K. Kurokawa", "S. Kanaya"],
    "venue": "BMC Bioinformatics,",
    "year": 2006
  }, {
    "title": "k-core decomposition: a tool for the visualization of large scale networks",
    "authors": ["J.I. Alvarez-Hamelin", "L. Dall’Asta", "A. Barrat", "A. Vespignani"],
    "venue": "CoRR, abs/cs/0504107,",
    "year": 2005
  }, {
    "title": "Parallel algorithms for geometric graph problems",
    "authors": ["A. Andoni", "A. Nikolov", "K. Onak", "G. Yaroslavtsev"],
    "venue": "In Symposium on Theory of Computing,",
    "year": 2014
  }, {
    "title": "New deterministic approximation algorithms for fully dynamic matching",
    "authors": ["S. Bhattacharya", "M. Henzinger", "D. Nanongkai"],
    "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
    "year": 2016
  }, {
    "title": "Preventing unraveling in social networks: The anchored k-core problem",
    "authors": ["K. Bhawalkar", "J.M. Kleinberg", "K. Lewi", "T. Roughgarden", "A. Sharma"],
    "venue": "In Automata, Languages, and Programming - 39th International Colloquium,",
    "year": 2012
  }, {
    "title": "Anonymizing subsets of social networks with degree constrained subgraphs",
    "authors": ["S. Chester", "J. Gaertner", "U. Stege", "S. Venkatesh"],
    "venue": "In International Conference on Advances in Social Networks Analysis and Mining,",
    "year": 2012
  }, {
    "title": "Mapreduce: a flexible data processing",
    "authors": ["J. Dean", "S. Ghemawat"],
    "venue": "tool. Commun. ACM,",
    "year": 2010
  }, {
    "title": "Efficient densest subgraph computation in evolving graphs",
    "authors": ["A. Epasto", "S. Lattanzi", "M. Sozio"],
    "venue": "In Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence,",
    "year": 2015
  }, {
    "title": "Applications of uniform sampling: Densest subgraph and beyond",
    "authors": ["H. Esfandiari", "M. Hajiaghayi", "D.P. Woodruff"],
    "venue": "arXiv preprint arXiv:1506.04505,",
    "year": 2015
  }, {
    "title": "Characterization of graphs using degree cores. In Algorithms and Models for the Web-Graph",
    "authors": ["J. Healy", "J.C.M. Janssen", "E.E. Milios", "W. Aiello"],
    "venue": "Fourth International Workshop,",
    "year": 2006
  }, {
    "title": "Efficient massively parallel methods for dynamic programming",
    "authors": ["S. Im", "B. Moseley", "X. Sun"],
    "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,",
    "year": 2017
  }, {
    "title": "Dynamic graph algorithms",
    "authors": ["G.F. Italiano", "D. Eppstein", "Z. Galil"],
    "venue": "Algorithms and Theory of Computation Handbook,",
    "year": 1999
  }, {
    "title": "A model of computation for mapreduce",
    "authors": ["H.J. Karloff", "S. Suri", "S. Vassilvitskii"],
    "venue": "In Proceedings of the TwentyFirst Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2010
  }, {
    "title": "Introducing the enron corpus",
    "authors": ["B. Klimt", "Y. Yang"],
    "venue": "In CEAS,",
    "year": 2004
  }, {
    "title": "A survey of algorithms for dense subgraph discovery",
    "authors": ["V.E. Lee", "N. Ruan", "R. Jin", "C. Aggarwal"],
    "venue": "In Managing and Mining Graph Data,",
    "year": 2010
  }, {
    "title": "Snap: A general-purpose network analysis and graph-mining library",
    "authors": ["J. Leskovec", "R. Sosič"],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
    "year": 2016
  }, {
    "title": "Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters",
    "authors": ["J. Leskovec", "K.J. Lang", "A. Dasgupta", "M.W. Mahoney"],
    "venue": "Internet Mathematics,",
    "year": 2009
  }, {
    "title": "Smallest-last ordering and clustering and graph coloring algorithms",
    "authors": ["D.W. Matula", "L.L. Beck"],
    "venue": "J. ACM,",
    "year": 1983
  }, {
    "title": "Learning to discover social circles in ego networks. In Advances in Neural Information Processing Systems",
    "authors": ["J.J. McAuley", "J. Leskovec"],
    "venue": "Annual Conference on Neural Information Processing Systems",
    "year": 2012
  }, {
    "title": "Densest subgraph in dynamic graph streams",
    "authors": ["A. McGregor", "D. Tench", "S. Vorotnikova", "H.T. Vu"],
    "venue": "In International Symposium on Mathematical Foundations of Computer Science,",
    "year": 2015
  }, {
    "title": "Distributed k-core decomposition",
    "authors": ["A. Montresor", "F.D. Pellegrini", "D. Miorandi"],
    "venue": "IEEE Trans. Parallel Distrib. Syst.,",
    "year": 2013
  }, {
    "title": "Selection and sorting with limited storage",
    "authors": ["J.I. Munro", "M. Paterson"],
    "venue": "Theor. Comput. Sci.,",
    "year": 1980
  }, {
    "title": "Trust management for the semantic web",
    "authors": ["M. Richardson", "R. Agrawal", "P.M. Domingos"],
    "venue": "In The Semantic Web - ISWC",
    "year": 2003
  }, {
    "title": "Streaming algorithms for k-core decomposition",
    "authors": ["A.E. Sarayuce", "B. Gedik", "G. Jacques-Silva", "Wu", "K.-L", "U.V. Catalyurek"],
    "venue": "PVLDB, pp",
    "year": 2015
  }, {
    "title": "Defining and evaluating network communities based on ground-truth",
    "authors": ["J. Yang", "J. Leskovec"],
    "venue": "Knowl. Inf. Syst.,",
    "year": 2015
  }, {
    "title": "A fast orderbased approach for core maintenance",
    "authors": ["Y. Zhang", "J.X. Yu", "L. Qin"],
    "venue": "IEEE International Conference on Data Engineering,",
    "year": 2017
  }],
  "id": "SP:24345f76cd03d67ec95ec5da60d6b75fa3311287",
  "authors": [{
    "name": "Hossein Esfandiari",
    "affiliations": []
  }, {
    "name": "Silvio Lattanzi",
    "affiliations": []
  }, {
    "name": "Vahab Mirrokni",
    "affiliations": []
  }],
  "abstractText": "The k-core decomposition is a fundamental primitive in many machine learning and data mining applications. We present the first distributed and the first streaming algorithms to compute and maintain an approximate k-core decomposition with provable guarantees. Our algorithms achieve rigorous bounds on space complexity while bounding the number of passes or number of rounds of computation. We do so by presenting a new powerful sketching technique for k-core decomposition, and then by showing it can be computed efficiently in both streaming and MapReduce models. Finally, we confirm the effectiveness of our sketching technique empirically on a number of publicly available graphs.",
  "title": "Parallel and Streaming Algorithms for K-Core Decomposition"
}