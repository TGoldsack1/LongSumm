{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2041–2050 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2041"
  }, {
    "heading": "1 Introduction",
    "text": "The problem of similarity search, also called nearest-neighbor search, consists of finding documents from a large collection of documents, or corpus, which are most similar to a query document of interest. Fast and accurate similarity search is at the core of many information retrieval applications, such as plagiarism analysis (Stein et al., 2007), collaborative filtering (Koren, 2008), content-based multimedia retrieval (Lew et al., 2006) and caching (Pandey et al., 2009). Semantic hashing is an effective approach for fast similarity search (Salakhutdinov and Hinton, 2009; Zhang\n∗ Equal contribution.\net al., 2010; Wang et al., 2014). By representing every document in the corpus as a similaritypreserving discrete (binary) hashing code, the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing codes, i.e., the number of bits that are different between two codes. Given that today, an ordinary PC is able to execute millions of Hamming distance computations in just a few milliseconds (Zhang et al., 2010), this semantic hashing strategy is very computationally attractive.\nWhile considerable research has been devoted to text (semantic) hashing, existing approaches typically require two-stage training procedures. These methods can be generally divided into two categories: (i) binary codes for documents are first learned in an unsupervised manner, then l binary classifiers are trained via supervised learning to predict the l-bit hashing code (Zhang et al., 2010; Xu et al., 2015); (ii) continuous text representations are first inferred, which are binarized as a second (separate) step during testing (Wang et al., 2013; Chaidaroon and Fang, 2017). Because the model parameters are not learned in an end-to-end manner, these two-stage training strategies may result in suboptimal local optima. This happens because different modules within the model are optimized separately, preventing the sharing of information between them. Further, in existing methods, binary constraints are typically handled adhoc by truncation, i.e., the hashing codes are obtained via direct binarization from continuous representations after training. As a result, the information contained in the continuous representations is lost during the (separate) binarization process. Moreover, training different modules (mapping and classifier/binarization) separately often requires additional hyperparameter tuning for each training stage, which can be laborious and timeconsuming.\nIn this paper, we propose a simple and generic neural architecture for text hashing that learns binary latent codes for documents in an end-toend manner. Inspired by recent advances in neural variational inference (NVI) for text processing (Miao et al., 2016; Yang et al., 2017; Shen et al., 2017b), we approach semantic hashing from a generative model perspective, where binary (hashing) codes are represented as either deterministic or stochastic Bernoulli latent variables. The inference (encoder) and generative (decoder) networks are optimized jointly by maximizing a variational lower bound to the marginal distribution of input documents (corpus). By leveraging a simple and effective method to estimate the gradients with respect to discrete (binary) variables, the loss term from the generative (decoder) network can be directly backpropagated into the inference (encoder) network to optimize the hash function.\nMotivated by the rate-distortion theory (Berger, 1971; Theis et al., 2017), we propose to inject data-dependent noise into the latent codes during the decoding stage, which adaptively accounts for the tradeoff between minimizing rate (number of bits used, or effective code length) and distortion (reconstruction error) during training. The connection between the proposed method and ratedistortion theory is further elucidated, providing a theoretical foundation for the effectiveness of our framework.\nSummarizing, the contributions of this paper are: (i) to the best of our knowledge, we present the first semantic hashing architecture that can be trained in an end-to-end manner; (ii) we propose a neural variational inference framework to learn compact (regularized) binary codes for documents, achieving promising results on both unsupervised and supervised text hashing; (iii) the connection between our method and rate-distortion theory is established, from which we demonstrate the advantage of injecting data-dependent noise into the latent variable during training."
  }, {
    "heading": "2 Related Work",
    "text": "Models with discrete random variables have attracted much attention in the deep learning community (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017; Li et al., 2017; Shu and Nakayama, 2017). Some of these structures are more natural choices for language or speech data, which are inherently discrete. More specifically,\n<latexit sha1_base64=\"7fXReuSi2AGXHQbFX8oagcVUXco=\">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit><latexit sha1_base64=\"7fXReuSi2AGXHQbFX8oagcVUXco=\">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit><latexit sha1_base64=\"7fXReuSi2AGXHQbFX8oagcVUXco=\">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit>\nvan den Oord et al. (2017) combined VAEs with vector quantization to learn discrete latent representation, and demonstrated the utility of these learned representations on images, videos, and speech data. Li et al. (2017) leveraged both pairwise label and classification information to learn discrete hash codes, which exhibit state-of-the-art performance on image retrieval tasks.\nFor natural language processing (NLP), although significant research has been made to learn continuous deep representations for words or documents (Mikolov et al., 2013; Kiros et al., 2015; Shen et al., 2018), discrete neural representations have been mainly explored in learning word embeddings (Shu and Nakayama, 2017; Chen et al., 2017). In these recent works, words are represented as a vector of discrete numbers, which are very efficient storage-wise, while showing comparable performance on several NLP tasks, relative to continuous word embeddings. However, discrete representations that are learned in an endto-end manner at the sentence or document level have been rarely explored. Also there is a lack of strict evaluation regarding their effectiveness. Our work focuses on learning discrete (binary) representations for text documents. Further, we employ semantic hashing (fast similarity search) as a mechanism to evaluate the quality of learned binary latent codes."
  }, {
    "heading": "3 The Proposed Method",
    "text": ""
  }, {
    "heading": "3.1 Hashing under the NVI Framework",
    "text": "Inspired by the recent success of variational autoencoders for various NLP problems (Miao et al., 2016; Bowman et al., 2015; Yang et al., 2017; Miao et al., 2017; Shen et al., 2017b; Wang et al., 2018), we approach the training of discrete (binary) latent variables from a generative perspec-\ntive. Let x and z denote the input document and its corresponding binary hash code, respectively. Most of the previous text hashing methods focus on modeling the encoding distribution p(z|x), or hash function, so the local/global pairwise similarity structure of documents in the original space is preserved in latent space (Zhang et al., 2010; Wang et al., 2013; Xu et al., 2015; Wang et al., 2014). However, the generative (decoding) process of reconstructing x from binary latent code z, i.e., modeling distribution p(x|z), has been rarely considered. Intuitively, latent codes learned from a model that accounts for the generative term should naturally encapsulate key semantic information from x because the generation/reconstruction objective is a function of p(x|z). In this regard, the generative term provides a natural training objective for semantic hashing.\nWe define a generative model that simultaneously accounts for both the encoding distribution, p(z|x), and decoding distribution, p(x|z), by defining approximations qφ(z|x) and qθ(x|z), via inference and generative networks, gφ(x) and gθ(z), parameterized by φ and θ, respectively. Specifically, x ∈ Z |V |+ is the bag-of-words (count) representation for the input document, where |V | is the vocabulary size. Notably, we can also employ other count weighting schemes as input features x, e.g., the term frequency-inverse document frequency (TFIDF) (Manning et al., 2008). For the encoding distribution, a latent variable z is first inferred from the input text x, by constructing an inference network gφ(x) to approximate the true posterior distribution p(z|x) as qφ(z|x). Subsequently, the decoder network gθ(z) maps z back into input space to reconstruct the original sequence x as x̂, approximating p(x|z) as qθ(x|z) (as shown in Figure 1). This cyclic strategy, x → z → x̂ ≈ x, provides the latent variable z with a better ability to generalize (Miao et al., 2016).\nTo tailor the NVI framework for semantic hashing, we cast z as a binary latent variable and assume a multivariate Bernoulli prior on z: p(z) ∼ Bernoulli(γ) = ∏l i=1 γ zi i (1 − γi)1−zi , where γi ∈ [0, 1] is component i of vector γ. Thus, the encoding (approximate posterior) distribution qφ(z|x) is restricted to take the form qφ(z|x) = Bernoulli(h), where h = σ(gφ(x)), σ(·) is the sigmoid function, and gφ(·) is the (nonlinear) inference network specified as a multilayer perceptron (MLP). As illustrated in Figure 1, we can obtain\nsamples from the Bernoulli posterior either deterministically or stochastically. Suppose z is a l-bit hash code, for the deterministic binarization, we have, for i = 1, 2, ......, l:\nzi = 1σ(giφ(x))>0.5 = sign(σ(giφ(x)− 0.5) + 1 2 ,\n(1)\nwhere z is the binarized variable, and zi and giφ(x) denote the i-th dimension of z and gφ(x), respectively. The standard Bernoulli sampling in (1) can be understood as setting a hard threshold at 0.5 for each representation dimension, therefore, the binary latent code is generated deterministically. Another strategy to obtain the discrete variable is to binarize h in a stochastic manner:\nzi = 1σ(giφ(x))>µi = sign(σ(giφ(x))− µi) + 1 2 ,\n(2)\nwhere µi ∼ Uniform(0, 1). Because of this sampling process, we do not have to assume a predefined threshold value like in (1)."
  }, {
    "heading": "3.2 Training with Binary Latent Variables",
    "text": "To estimate the parameters of the encoder and decoder networks, we would ideally maximize the marginal distribution p(x) = ∫ p(z)p(x|z)dz. However, computing this marginal is intractable in most cases of interest. Instead, we maximize a variational lower bound. This approach is typically employed in the VAE framework (Kingma and Welling, 2013):\nLvae = Eqφ(z|x) [ log\nqθ(x|z)p(z) qφ(z|x)\n] , (3)\n= Eqφ(z|x)[log qθ(x|z)]−DKL(qφ(z|x)||p(z)),\nwhere the Kullback-Leibler (KL) divergence DKL(qφ(z|x)||p(z)) encourages the approximate posterior distribution qφ(z|x) to be close to the multivariate Bernoulli prior p(z). In this case, DKL(qφ(z|x)|p(z)) can be written in closed-form as a function of gφ(x):\nDKL = gφ(x) log gφ(x)\nγ\n+ (1− gφ(x)) log 1− gφ(x) 1− γ . (4)\nNote that the gradient for the KL divergence term above can be evaluated easily.\nFor the first term in (3), we should in principle estimate the influence of µi in (2) on qθ(x|z) by averaging over the entire (uniform) noise distribution. However, a closed-form distribution does not exist since it is not possible to enumerate all possible configurations of z, especially when the latent dimension is large. Moreover, discrete latent variables are inherently incompatible with backpropagation, since the derivative of the sign function is zero for almost all input values. As a result, the exact gradients of Lvae wrt the inputs before binarization would be essentially all zero.\nTo estimate the gradients for binary latent variables, we utilize the straight-through (ST) estimator, which was first introduced by Hinton (2012). So motivated, the strategy here is to simply backpropagate through the hard threshold by approximating the gradient ∂z/∂φ as 1. Thus, we have:\ndEqφ(z|x)[log qθ(x|z)] ∂φ\n= dEqφ(z|x)[log qθ(x|z)]\ndz\ndz\ndσ(giφ(x))\ndσ(giφ(x))\ndφ\n≈ dEqφ(z|x)[log qθ(x|z)]\ndz\ndσ(giφ(x))\ndφ (5)\nAlthough this is clearly a biased estimator, it has been shown to be a fast and efficient method relative to other gradient estimators for discrete variables, especially for the Bernoulli case (Bengio et al., 2013; Hubara et al., 2016; Theis et al., 2017). With the ST gradient estimator, the first loss term in (3) can be backpropagated into the encoder network to fine-tune the hash function gφ(x).\nFor the approximate generator qθ(x|z) in (3), let xi denote the one-hot representation of ith word within a document. Note that x = ∑ i xi is thus the bag-of-words representation for document x. To reconstruct the input x from z, we utilize a softmax decoding function written as:\nq(xi = w|z) = exp(zTExw + bw)∑|V | j=1 exp(z TExj + bj) , (6)\nwhere q(xi = w|z) is the probability that xi is word w ∈ V , qθ(x|z) = ∏ i q(xi = w|z) and θ = {E, b1, . . . , b|V |}. Note that E ∈ Rd×|V | can be interpreted as a word embedding matrix to be learned, and {bi}|V |i=1 denote bias terms. Intuitively, the objective in (6) encourages the discrete vector z to be close to the embeddings for every word\nthat appear in the input document x. As shown in Section 5.3.1, meaningful semantic structures can be learned and manifested in the word embedding matrix E."
  }, {
    "heading": "3.3 Injecting Data-dependent Noise to z",
    "text": "To reconstruct text data x from sampled binary representation z, a deterministic decoder is typically utilized (Miao et al., 2016; Chaidaroon and Fang, 2017). Inspired by the success of employing stochastic decoders in image hashing applications (Dai et al., 2017; Theis et al., 2017), in our experiments, we found that injecting random Gaussian noise into z makes the decoder a more favorable regularizer for the binary codes, which in practice leads to stronger retrieval performance. Below, we invoke the rate-distortion theory to perform some further analysis, which leads to interesting findings.\nLearning binary latent codes z to represent a continuous distribution p(x) is a classical information theory concept known as lossy source coding. From this perspective, semantic hashing, which compresses an input document into compact binary codes, can be casted as a conventional ratedistortion tradeoff problem (Theis et al., 2017; Ballé et al., 2016):\nmin − log2R(z)︸ ︷︷ ︸ Rate +β ·D(x, x̂)︸ ︷︷ ︸ Distortion , (7)\nwhere rate and distortion denote the effective code length, i.e., the number of bits used, and the distortion introduced by the encoding/decoding sequence, respectively. Further, x̂ is the reconstructed input and β is a hyperparameter that controls the tradeoff between the two terms.\nConsidering the case where we have a Bernoulli prior on z as p(z) ∼ Bernoulli(γ), and x conditionally drawn from a Gaussian distribution p(x|z) ∼ N (Ez, σ2I). Here, E = {ei}|V |i=1, where ei ∈ Rd can be interpreted as a codebook with |V | codewords. In our case, E corresponds to the word embedding matrix as in (6).\nFor the case of stochastic latent variable z, the objective function in (3) can be written in a form similar to the rate-distortion tradeoff:\nminEqφ(z|x) − log qφ(z|x)︸ ︷︷ ︸ Rate + 1 2σ2︸︷︷︸ β ||x− Ez||22︸ ︷︷ ︸ Distortion +C  , (8)\nwhere C is a constant that encapsulates the prior distribution p(z) and the Gaussian distribution normalization term. Notably, the trade-off hyperparameter β = σ−2/2 is closely related to the variance of the distribution p(x|z). In other words, by controlling the variance σ, the model can adaptively explore different trade-offs between the rate and distortion objectives. However, the optimal trade-offs for distinct samples may be different.\nInspired by the observations above, we propose to inject data-dependent noise into latent variable z, rather than to setting the variance term σ2 to a fixed value (Dai et al., 2017; Theis et al., 2017). Specifically, log σ2 is obtained via a one-layer MLP transformation from gφ(x). Afterwards, we sample z′ fromN (z, σ2I), which then replace z in (6) to infer the probability of generating individual words (as shown in Figure 1). As a result, the variances are different for every input document x, and thus the model is provided with additional flexibility to explore various trade-offs between rate and distortion for different training observations. Although our decoder is not a strictly Gaussian distribution, as in (6), we found empirically that injecting data-dependent noise into z yields strong retrieval results, see Section 5.1."
  }, {
    "heading": "3.4 Supervised Hashing",
    "text": "The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing, where a mapping from latent variable z to labels y is learned, here parametrized by a twolayer MLP followed by a fully-connected softmax layer. To allow the model to explore and balance between maximizing the variational lower bound in (3) and minimizing the discriminative loss, the following joint training objective is employed:\nL = −Lvae(θ, φ;x) + αLdis(η; z, y). (9)\nwhere η refers to parameters of the MLP classifier and α controls the relative weight between the variational lower bound (Lvae) and discriminative loss (Ldis), defined as the cross-entropy loss. The parameters {θ, φ, η} are learned end-to-end via Monte Carlo estimation."
  }, {
    "heading": "4 Experimental Setup",
    "text": ""
  }, {
    "heading": "4.1 Datasets",
    "text": "We use the following three standard publicly available datasets for training and evaluation:\n(i) Reuters21578, containing 10,788 news documents, which have been classified into 90 different categories. (ii) 20Newsgroups, a collection of 18,828 newsgroup documents, which are categorized into 20 different topics. (iii) TMC (stands for SIAM text mining competition), containing air traffic reports provided by NASA. TMC consists 21,519 training documents divided into 22 different categories. To make direct comparison with prior works, we employed the TFIDF features on these datasets supplied by (Chaidaroon and Fang, 2017), where the vocabulary sizes for the three datasets are set to 10,000, 7,164 and 20,000, respectively."
  }, {
    "heading": "4.2 Training Details",
    "text": "For the inference networks, we employ a feedforward neural network with 2 hidden layers (both with 500 units) using the ReLU non-linearity activation function, which transform the input documents, i.e., TFIDF features in our experiments, into a continuous representation. Empirically, we found that stochastic binarization as in (2) shows stronger performance than deterministic binarization, and thus use the former in our experiments. However, we further conduct a systematic ablation study in Section 5.2 to compare the two binarization strategies.\nOur model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters. We decay the learning rate by a factor of 0.96 for every 10,000 iterations. Dropout (Srivastava et al., 2014) is employed on the output of encoder networks, with the rate selected from {0.7, 0.8, 0.9} on the validation set. To facilitate comparisons with previous methods, we set the dimension of z, i.e., the number of bits within the hashing code) as 8, 16, 32, 64, or 128."
  }, {
    "heading": "4.3 Baselines",
    "text": "We evaluate the effectiveness of our framework on both unsupervised and supervised semantic hashing tasks. We consider the following unsupervised baselines for comparisons: Locality Sensitive Hashing (LSH) (Datar et al., 2004), Stack Restricted Boltzmann Machines (S-RBM) (Salakhutdinov and Hinton, 2009), Spectral Hashing (SpH) (Weiss et al., 2009), Self-taught Hashing (STH) (Zhang et al., 2010) and Variational Deep Semantic Hashing (VDSH) (Chaidaroon and Fang, 2017).\nFor supervised semantic hashing, we also compare NASH against a number of baselines: Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Semantic Hashing using Tags and Topic Modeling (SHTTM) (Wang et al., 2013) and Supervised VDSH (Chaidaroon and Fang, 2017). It is worth noting that unlike all these baselines, our NASH model is trained end-to-end in one-step."
  }, {
    "heading": "4.4 Evaluation Metrics",
    "text": "To evaluate the hashing codes for similarity search, we consider each document in the testing set as a query document. Similar documents to the query in the corresponding training set need to be retrieved based on the Hamming distance of their hashing codes, i.e. number of different bits. To facilitate comparison with prior work (Wang et al., 2013; Chaidaroon and Fang, 2017), the performance is measured with precision. Specifically, during testing, for a query document, we first retrieve the 100 nearest/closest documents according to the Hamming distances of the corresponding hash codes (i.e., the number of different bits). We then examine the percentage of documents among these 100 retrieved ones that belong to the same label (topic) with the query document (we consider documents having the same label as relevant pairs). The ratio of the number of relevant documents to the number of retrieved documents (fixed value of 100) is calculated as the precision score. The precision scores are further averaged over all test (query) documents."
  }, {
    "heading": "5 Experimental Results",
    "text": "We experimented with four variants for our NASH model: (i) NASH: with deterministic decoder; (ii) NASH-N: with fixed random noise injected to decoder; (iii) NASH-DN: with data-dependent noise injected to decoder; (iv) NASH-DN-S: NASH-DN with supervised information during training."
  }, {
    "heading": "5.1 Semantic Hashing Evaluation",
    "text": "Table 1 presents the results of all models on Reuters dataset. Regarding unsupervised semantic hashing, all the NASH variants consistently outperform the baseline methods by a substantial margin, indicating that our model makes the most effective use of unlabeled data and manage to assign similar hashing codes, i.e., with small Hamming distance to each other, to documents that belong to the same label. It can be also observed that the injection of noise into the decoder networks has improved the robustness of learned binary representations, resulting in better retrieval performance. More importantly, by making the variances of noise adaptive to the specific input, our NASH-DN achieves even better results, compared with NASH-N, highlighting the importance of exploring/learning the trade-off between rate and distortion objectives by the data itself. We observe the same trend and superiority of our NASH-DN models on the other two benchmarks, as shown in Tables 3 and 4.\nAnother observation is that the retrieval results tend to drop a bit when we set the length of hashing codes to be 64 or larger, which also happens for some baseline models. This phenomenon has been reported previously in Wang et al. (2012); Liu et al. (2012); Wang et al. (2013); Chaidaroon and Fang (2017), and the reasons could be twofold: (i) for longer codes, the number of data points that are assigned to a certain binary code decreases exponentially. As a result, many queries may fail to return any neighbor documents (Wang et al., 2012); (ii) considering the size of training data, it is likely that the model may overfit with long hash codes (Chaidaroon and Fang, 2017). However, even with longer hashing codes,\nour NASH models perform stronger than the baselines in most cases (except for the 20Newsgroups dataset), suggesting that NASH can effectively allocate documents to informative/meaningful hashing codes even with limited training data.\nWe also evaluate the effectiveness of NASH in a supervised scenario on the Reuters dataset,\nwhere the label or topic information is utilized during training. As shown in Figure 2, our NASHDN-S model consistently outperforms several supervised semantic hashing baselines, with various choices of hashing bits. Notably, our model exhibits higher Top-100 retrieval precision than VDSH-S and VDSH-SP, proposed by Chaidaroon and Fang (2017). This may be attributed to the fact that in VDSH models, the continuous embeddings are not optimized with their future binarization in mind, and thus could hurt the relevance of learned binary codes. On the contrary, our model is optimized in an end-to-end manner, where the gradients are directly backpropagated to the inference network (through the binary/discrete latent variable), and thus gives rise to a more robust hash function."
  }, {
    "heading": "5.2 Ablation study",
    "text": ""
  }, {
    "heading": "5.2.1 The effect of stochastic sampling",
    "text": "As described in Section 3, the binary latent variables z in NASH can be either deterministically (via (1)) or stochastically (via (2)) sampled. We compare these two types of binarization functions in the case of unsupervised hashing. As illustrated in Figure 3, stochastic sampling shows stronger retrieval results on all three datasets, indicating that endowing the sampling process of latent variables with more stochasticity improves the learned representations."
  }, {
    "heading": "5.2.2 The effect of encoder/decoder networks",
    "text": "Under the variational framework introduced here, the encoder network, i.e., hash function, and decoder network are jointly optimized to abstract semantic features from documents. An interesting question concerns what types of network should be leveraged for each part of our NASH model. In this regard, we further investigate the effect of\nusing an encoder or decoder with different nonlinearity, ranging from a linear transformation to two-layer MLPs. We employ a base model with an encoder of two-layer MLPs and a linear decoder (the setup described in Section 3), and the ablation study results are shown in Table 6.\nIt is observed that for the encoder networks, increasing the non-linearity by stacking MLP layers leads to better empirical results. In other words, endowing the hash function with more modeling capacity is advantageous to retrieval tasks. However, when we employ a non-linear network for the decoder, the retrieval precision drops dramatically. It is worth noting that the only difference between linear transformation and one-layer MLP is whether a non-linear activation function is employed or not.\nThis observation may be attributed the fact that the decoder networks can be considered as a sim-\nilarity measure between latent variable z and the word embeddings Ek for every word, and the probabilities for words that present in the document is maximized to ensure that z is informative. As a result, if we allow the decoder to be too expressive (e.g., a one-layer MLP), it is likely that we will end up with a very flexible similarity measure but relatively less meaningful binary representations. This finding is consistent with several image hashing methods, such as SGH (Dai et al., 2017) or binary autoencoder (Carreira-Perpinán and Raziperchikolaei, 2015), where a linear decoder is typically adopted to obtain promising retrieval results. However, our experiments may not speak for other choices of encoder-decoder architectures, e.g., LSTM-based sequence-to-sequence models (Sutskever et al., 2014) or DCNN-based autoencoder (Zhang et al., 2017)."
  }, {
    "heading": "5.3 Qualitative Analysis",
    "text": ""
  }, {
    "heading": "5.3.1 Analysis of Semantic Information",
    "text": "To understand what information has been learned in our NASH model, we examine the matrix E ∈ Rd×l in (6). Similar to (Miao et al., 2016; Larochelle and Lauly, 2012), we select the 5 nearest words according to the word vectors learned from NASH and compare with the corresponding results from NVDM.\nAs shown in Table 2, although our NASH model contains a binary latent variable, rather than a continuous one as in NVDM, it also effectively group semantically-similar words together in the learned vector space. This further demonstrates that the proposed generative framework manages to bypass the binary/discrete constraint and is able to abstract useful semantic information from documents."
  }, {
    "heading": "5.3.2 Case Study",
    "text": "In Table 5, we show some examples of the learned binary hashing codes on 20Newsgroups\ndataset. We observe that for both 8-bit and 16- bit cases, NASH typically compresses documents with shared topics into very similar binary codes. On the contrary, the hashing codes for documents with different topics exhibit much larger Hamming distance. As a result, relevant documents can be efficiently retrieved by simply computing their Hamming distances."
  }, {
    "heading": "6 Conclusions",
    "text": "This paper presents a first step towards end-to-end semantic hashing, where the binary/discrete constraints are carefully handled with an effective gradient estimator. A neural variational framework is introduced to train our model. Motivated by the connections between the proposed method and rate-distortion theory, we inject data-dependent noise into the Bernoulli latent variable at the training stage. The effectiveness of our framework is demonstrated with extensive experiments.\nAcknowledgments We would like to thank the ACL reviewers for their insightful suggestions. This research was supported in part by DARPA, DOE, NIH, NSF and ONR."
  }],
  "year": 2018,
  "references": [{
    "title": "End-to-end optimization of nonlinear transform codes for perceptual quality",
    "authors": ["Johannes Ballé", "Valero Laparra", "Eero P Simoncelli."],
    "venue": "Picture Coding Symposium (PCS), 2016. IEEE, pages 1–5.",
    "year": 2016
  }, {
    "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
    "authors": ["Yoshua Bengio", "Nicholas Léonard", "Aaron Courville."],
    "venue": "arXiv preprint arXiv:1308.3432 .",
    "year": 2013
  }, {
    "title": "Rate-distortion theory",
    "authors": ["Toby Berger."],
    "venue": "Encyclopedia of Telecommunications .",
    "year": 1971
  }, {
    "title": "Generating sentences from a continuous space",
    "authors": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."],
    "venue": "arXiv preprint arXiv:1511.06349 .",
    "year": 2015
  }, {
    "title": "Hashing with binary autoencoders",
    "authors": ["Miguel A Carreira-Perpinán", "Ramin Raziperchikolaei."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE, pages 557–566.",
    "year": 2015
  }, {
    "title": "Variational deep semantic hashing for text documents",
    "authors": ["Suthee Chaidaroon", "Yi Fang."],
    "venue": "Proceedings of the 40th international ACM SIGIR conference on Research and development in information retrieval. ACM.",
    "year": 2017
  }, {
    "title": "Learning k-way d-dimensional discrete code for compact embedding representations",
    "authors": ["Ting Chen", "Martin Renqiang Min", "Yizhou Sun."],
    "venue": "arXiv preprint arXiv:1711.03067 .",
    "year": 2017
  }, {
    "title": "Stochastic generative hashing",
    "authors": ["Bo Dai", "Ruiqi Guo", "Sanjiv Kumar", "Niao He", "Le Song."],
    "venue": "arXiv preprint arXiv:1701.02815 .",
    "year": 2017
  }, {
    "title": "Locality-sensitive hashing scheme based on p-stable distributions",
    "authors": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S Mirrokni."],
    "venue": "Proceedings of the twentieth annual symposium on Computational geometry. ACM, pages 253–262.",
    "year": 2004
  }, {
    "title": "Neural networks for machine learning, coursera",
    "authors": ["Geoffrey Hinton."],
    "venue": "URL: http://coursera. org/course/neuralnets .",
    "year": 2012
  }, {
    "title": "Binarized neural networks",
    "authors": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio."],
    "venue": "Advances in neural information processing systems. pages 4107–4115.",
    "year": 2016
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole."],
    "venue": "arXiv preprint arXiv:1611.01144 .",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "arXiv preprint arXiv:1312.6114 .",
    "year": 2013
  }, {
    "title": "Skip-thought vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "Advances in neural information processing systems. pages 3294–3302.",
    "year": 2015
  }, {
    "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
    "authors": ["Yehuda Koren."],
    "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pages 426–434.",
    "year": 2008
  }, {
    "title": "A neural autoregressive topic model",
    "authors": ["Hugo Larochelle", "Stanislas Lauly."],
    "venue": "Advances in Neural Information Processing Systems. pages 2708–2716.",
    "year": 2012
  }, {
    "title": "Content-based multimedia information retrieval: State of the art and challenges",
    "authors": ["Michael S Lew", "Nicu Sebe", "Chabane Djeraba", "Ramesh Jain."],
    "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 2(1):1–19.",
    "year": 2006
  }, {
    "title": "Deep supervised discrete hashing",
    "authors": ["Qi Li", "Zhenan Sun", "Ran He", "Tieniu Tan."],
    "venue": "arXiv preprint arXiv:1705.10999 .",
    "year": 2017
  }, {
    "title": "Supervised hashing with kernels",
    "authors": ["Wei Liu", "Jun Wang", "Rongrong Ji", "Yu-Gang Jiang", "Shih-Fu Chang."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, pages 2074–2081.",
    "year": 2012
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables",
    "authors": ["Chris J Maddison", "Andriy Mnih", "Yee Whye Teh."],
    "venue": "arXiv preprint arXiv:1611.00712 .",
    "year": 2016
  }, {
    "title": "Discovering discrete latent topics with neural variational inference",
    "authors": ["Yishu Miao", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "arXiv preprint arXiv:1706.00359 .",
    "year": 2017
  }, {
    "title": "Neural variational inference for text processing",
    "authors": ["Yishu Miao", "Lei Yu", "Phil Blunsom."],
    "venue": "International Conference on Machine Learning. pages 1727–1736.",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Nearest-neighbor caching for contentmatch applications",
    "authors": ["Sandeep Pandey", "Andrei Broder", "Flavio Chierichetti", "Vanja Josifovski", "Ravi Kumar", "Sergei Vassilvitskii."],
    "venue": "Proceedings of the 18th international conference on World wide web. ACM,",
    "year": 2009
  }, {
    "title": "Semantic hashing",
    "authors": ["Ruslan Salakhutdinov", "Geoffrey Hinton."],
    "venue": "International Journal of Approximate Reasoning 50(7):969–978.",
    "year": 2009
  }, {
    "title": "Adaptive convolutional filter generation for natural language understanding",
    "authors": ["Dinghan Shen", "Martin Renqiang Min", "Yitong Li", "Lawrence Carin."],
    "venue": "arXiv preprint arXiv:1709.08294 .",
    "year": 2017
  }, {
    "title": "Baseline needs more love: On simple wordembedding-based models and associated pooling",
    "authors": ["Dinghan Shen", "Guoyin Wang", "Wenlin Wang", "Martin Renqiang Min", "Qinliang Su", "Yizhe Zhang", "Chunyuan Li", "Ricardo Henao", "Lawrence Carin"],
    "year": 2018
  }, {
    "title": "Deconvolutional latent-variable model for text sequence matching",
    "authors": ["Dinghan Shen", "Yizhe Zhang", "Ricardo Henao", "Qinliang Su", "Lawrence Carin."],
    "venue": "AAAI .",
    "year": 2017
  }, {
    "title": "Compressing word embeddings via deep compositional code learning",
    "authors": ["Raphael Shu", "Hideki Nakayama."],
    "venue": "arXiv preprint arXiv:1711.01068 .",
    "year": 2017
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Strategies for retrieving plagiarized documents",
    "authors": ["Benno Stein", "Sven Meyer zu Eissen", "Martin Potthast."],
    "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages",
    "year": 2007
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems. pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Lossy image compression with compressive autoencoders",
    "authors": ["Lucas Theis", "Wenzhe Shi", "Andrew Cunningham", "Ferenc Huszár."],
    "venue": "ICLR .",
    "year": 2017
  }, {
    "title": "Neural discrete representation learning",
    "authors": ["Aaron van den Oord", "Oriol Vinyals"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Hashing for similarity search: A survey",
    "authors": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji."],
    "venue": "arXiv preprint arXiv:1408.2927 .",
    "year": 2014
  }, {
    "title": "Semi-supervised hashing for large-scale search",
    "authors": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang."],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34(12):2393–2406.",
    "year": 2012
  }, {
    "title": "Semantic hashing using tags and topic modeling",
    "authors": ["Qifan Wang", "Dan Zhang", "Luo Si."],
    "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 213–222.",
    "year": 2013
  }, {
    "title": "Topic compositional neural language model",
    "authors": ["Wenlin Wang", "Zhe Gan", "Wenqi Wang", "Dinghan Shen", "Jiaji Huang", "Wei Ping", "Sanjeev Satheesh", "Lawrence Carin."],
    "venue": "AISTATS.",
    "year": 2018
  }, {
    "title": "Spectral hashing",
    "authors": ["Yair Weiss", "Antonio Torralba", "Rob Fergus."],
    "venue": "Advances in neural information processing systems. pages 1753–1760.",
    "year": 2009
  }, {
    "title": "Convolutional neural networks for text hashing",
    "authors": ["Jiaming Xu", "Peng Wang", "Guanhua Tian", "Bo Xu", "Jun Zhao", "Fangyuan Wang", "Hongwei Hao."],
    "venue": "IJCAI. pages 1369–1375.",
    "year": 2015
  }, {
    "title": "Improved variational autoencoders for text modeling using dilated convolutions",
    "authors": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick."],
    "venue": "arXiv preprint arXiv:1702.08139 .",
    "year": 2017
  }, {
    "title": "Self-taught hashing for fast similarity search",
    "authors": ["Dell Zhang", "Jun Wang", "Deng Cai", "Jinsong Lu."],
    "venue": "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 18–25.",
    "year": 2010
  }, {
    "title": "Deconvolutional paragraph representation learning",
    "authors": ["Yizhe Zhang", "Dinghan Shen", "Guoyin Wang", "Zhe Gan", "Ricardo Henao", "Lawrence Carin."],
    "venue": "Advances in Neural Information Processing Systems. pages 4172–4182.",
    "year": 2017
  }],
  "id": "SP:f00e4c76da2261662486a9816109614f39ce8c51",
  "authors": [{
    "name": "Dinghan Shen",
    "affiliations": []
  }, {
    "name": "Qinliang Su",
    "affiliations": []
  }, {
    "name": "Paidamoyo Chapfuwa",
    "affiliations": []
  }, {
    "name": "Wenlin Wang",
    "affiliations": []
  }, {
    "name": "Guoyin Wang",
    "affiliations": []
  }, {
    "name": "Lawrence Carin",
    "affiliations": []
  }, {
    "name": "Ricardo Henao",
    "affiliations": []
  }],
  "abstractText": "Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.",
  "title": "NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing"
}