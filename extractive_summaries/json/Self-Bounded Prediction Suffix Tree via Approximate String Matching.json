{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Prediction suffix trees (PST) provide an elegant and effective tool for sequence prediction tasks such as compression, temporal classification, reinforcement learning, and DNA sequencing (Li & Fu, 2014; Majumdar, 2016; Messias & Whiteson, 2017). The advantage of PSTs over other fixedorder Markov model is that the number of symbols used to predict depends on prediction context through the suffix tree data structure, which provides an efficient way to store and retrieve a set of strings and all their suffixes (Bellemare et al., 2014).\nMany PST algorithms perform exact matching between the suffix of the current sequence and sub-sequences in the previous sequence (Ron et al., 1996). The algorithms then make a prediction based on the previous history of those subsequences. Although the exact matching explicitly models the context where the same pattern occurred, it is potentially\n1Australian National University, Canberra, ACT, Australia 2Data to Decisions CRC, Kent Town, SA, Australia 3Data61 at CSIRO, Canberra, ACT, Australia. Correspondence to: Dongwoo Kim <dongwoo.kim@anu.edu.au>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1\nvulnerable to variation in a sequence such as substitution noise. To illustrate the prediction under variation, we present two symbolic music scores in Figure 1. When a PST model performs a prediction on the first score, it can take advantage of the same pattern between the first and last three symbols, where the latter is the suffix of the current sequence. On the other hand, the model cannot relate the first and last three symbols given the second score because of the minor variation in the suffix with the same length.\nAnother practical assumption made in earlier work is that a shorter suffix has a higher priority than a longer one during prediction (Dekel et al., 2005; Karampatziakis & Kozen, 2009). This might be an appropriate assumption for some domains where the recently observed symbols are more important than the previous symbols, but this might be inappropriate for other domains. For example, a longer suffix would be more important than a suffix of length one to predict the next symbol of a music sheet since the temporal pattern in music is often continued over multiple notes.\nIn this paper, we provide a novel construction of the prediction suffix tree and its online learning algorithm via approximate string matching. In that sense, the proposed algorithm can be robust to small variations in a sequence. We also provide a mechanism to control the importance of different suffixes based on their length. Finally, we derive a selfbounded version of the proposed model that decides the maximum length of suffix based on a trade-off between a confidence of prediction and complexity of algorithm automatically.\nIn the next section, we describe a decision theoretic PST model and how to learn the model parameters. In Section 3, we derive a PST with approximate string matching and its online learning algorithm. We also proof the mistake bound of the proposed algorithm w.r.t. an arbitrary hypothesis. In Section 4, we enhance our model by letting the model adaptively choose the depth of suffix tree. In Section 5 and 6, we verify our approach on synthetic datasets and demonstrate the improved predictive performance of our model on three real-world datasets."
  }, {
    "heading": "2. Background",
    "text": "We start by providing a formal description of a decision theoretic PST model. Assume that an input stream is a sequence of vectors x1,x2, ...(xt ∈ Rn), and an output stream is a sequence of binary symbols y1, y2, ...(yt ∈ {−1,+1}). We will relax the binary assumption in Subsection 5.2. We denote a sub-sequence of output yi, yi+1, ..., yj by y j i . Our goal is to predict the next symbol yt given the binary sequence yt−11 and the next input vector xt\nDekel et al. propose a provably-correct PST algorithm to predict a sequence of symbols. With suffix-closed tree T 1 endowed with a score at each node, the prediction function for symbol yt given yt−11 and xt is\nh(xt,y t−1 1 ) = w · xt + t−1∑ i=1 2−i/2g(yt−1t−i ), (1)\nwhere w ∈ Rn is a weight vector, and g(s) is a score of node s in a suffix tree T . The score of a suffix is zero if the tree does not have the suffix. We then use the sign of this prediction function as a predicted symbol yt. The prediction function looks up scores of all possible suffixes of the input stream up to time t − 1 from suffix tree T\n1T is a suffix-closed if ∀s ∈ T , all suffixes of s are also in T .\nand takes a weighted sum of the scores of suffixes with exponential decaying weight 2−i/2 w.r.t. the length of the suffix. Finally, the weighted score is added to the inner product between weight vector w and input vector x to make a prediction. The magnitude of h represents the confidence in this prediction.\nFigure 2 shows an example of prediction suffix tree with six suffixes therein. The value of node shows the score of a corresponding suffix, e.g., g(−+ +) = 4. Assume that we want to predict the next symbol of sequence y41 = −−++, then, with the prediction function given Eq. 1, the predicted symbol of y5 is sign(2−1/2×(−1)+2−2/2×(4)+2−3/2× (−2)).\nThere are multiple ways to construct the suffix tree and learn the model parameters. In this paper, we focus on the margin-based online learning algorithm and its analysis as per Dekel et al.; Karampatziakis & Kozen. In online learning, the model parameters are updated after each round. At round t, the model makes a prediction given input xt and the previous sequence yt−11 . The predicted symbol ŷt = sign(ht(xt,yt−11 )) is then compared to the revealed correct symbol yt. Finally, the prediction function is updated based on the prediction and true symbol. More formally, the model computes the hinge loss after each round\n`t = max{0, 1− ytht(xt,yt−11 )}. (2)\nThen the weight vector and node scores are updated based on the loss suffered from the prediction. The update rules for w and g for all s ∈ {yt−1t−i } t−1 i=1 are as follows:\nwt+1 = wt + ytτtxt (3)\ngt+1(s) =\n{ gt(s) + yt2\n−|s|/2τt, if s ∈ Tt yt2 −|s|/2τt, otherwise\n(4)\nwhere τt depends on the loss `t and can be interpreted as a learning weight at time t, and Tt is a suffix tree at time t. When the updates happen, the unbounded version of this PST learning algorithm adds suffixes of the currently observed sequence into the suffix tree, resulting in the tree having O(t) depth and O(t2) nodes. The same authors derive a self-bounded PST where the depth of tree is automatically chosen based on the model performance."
  }, {
    "heading": "3. PST with Approximate String Matching",
    "text": "We propose a new prediction suffix tree algorithm with approximate string matching (aPST) where the prediction on next symbol depends on the exact matching as well as approximate matching suffixes. From the previous section, we observe that the PST algorithm has the following properties:\n• The model performs exact matching between the current suffix and a node in the suffix tree where each node\nrepresents sub-sequences of the previously observed sequence.\n• The weight of shorter suffixes is higher than longer suffixes due to the exponential decaying rate.\nAs we have seen in Section 1, the first property prevents the model to take into account similar sub-sequences in the previous sequence, and the second property does not reflect the importance of suffix length. Our new prediction function takes into account all possible suffixes within - Hamming distance from the original suffixes and provides a controllable weighting scheme. Formally, the prediction function of aPST is defined as:\nh(xt,y t−1 1 ) = w >xt + t−1∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\nω(i, k)g(s),\n(5)\nwhere ω : Z+ × Z → R+ controls the contribution of suffix s, and d(s, s′) computes a distance between s and s′ given some distance metric. We use Hamming distance throughout the paper, but the proposed framework may be applied to other distance metrics defined over strings. When = 0, the model performs the exact string matching over the suffix tree. Hamming distance is only defined when the lengths of two sequences are the same (Robinson, 2003). Hence there is no suffix s whose distance k from yt−1t−i is greater than i. We define the contribution function ω as\nω(i, k) = ( (1− ξ)k λ\ni exp(−λ) i!\n) 1 2\n, λ > 0\n1 > ξ > 0 (6)\nwhere λ and ξ are two model parameters. Unlike Eq. 1, where shorter sequences get a higher weight than longer ones, the model imposes different weights on different lengths of suffix. Specifically, the model gives the highest weight to suffixes of length bλc. To reduce the contribution of approximate suffixes, we add an exponentially decaying factor w.r.t the distance from the original suffixes k.\nThe following theorem bounds the cumulative loss of the unbounded aPST online learning method of Algorithm 1 w.r.t. any fixed hypothesis h? which could be chosen in hindsight. To derive the bound we define the norm of the score function g as ||g||2 = ∑ s∈T g(s) 2.\nTheorem 1 (unbounded aPST). Let x1,x2, ...,xT be an input stream and let y1, y2, ..., yT be an output stream, where every ||x||2 ≤ 1 and every yt ∈ {−1,+1}. Let h? = (w?, T ?, g?) be an arbitrary hypothesis such that ||g?||2 < ∞ and which attains the loss values `?1, `?2, ... `?T . Let `1, ..., `T be the sequence of losses attained by the unbounded online learning algorithm with the -Hamming\nAlgorithm 1 Online learning algorithm for unbounded aPST.\n1: Input: T1 = {∅},w1 = 0, λ > 0, ≥ 0, 1 > ξ > 0 2: Set γ = min{−e−λ + eλ(1−ξ), e\nλΓ(1+ ,λ) Γ(1+ ) }\n3: for all t = 1, 2, ..., T do 4: Compute ht(xt,yt−11 ) 5: Predict ŷt = sign(ht(xt,yt−11 )) 6: Receive yt and compute loss `t = max{0, 1− ytht(xt,yt−11 )} 7: Set τt = `t/(||x||2 + 2 + γ) 8: Set dt = t− 1 9: Update weight vector: wt+1 = wt + ytτtxt 10: Update tree: 11: Tt+1 = Tt ∪ {yt−1t−i : 1 ≤ i ≤ dt} 12: gt+1(s) = gt(s) + ytτtω(|s|, d(s,yt−1t−i )) if {s : s ∈ T , d(s,yt−1t−i ) ≤ } 13: gt+1(s) = ytτtω(|s|, d(s,yt−1t−i )) if {s : s /∈ T , d(s,yt−1t−i ) ≤ } 14: end for\ndistance in Algorithm 1. Then it holds that\nT∑ t=1 `2t ≤ ( 3 + γ )( ||w?||2 + ||g?||2 + 1 2 T∑ t=1 (`?t ) 2 ) ,\nwhere γ = min{−e−λ + eλ(1−ξ), e λΓ(1+ ,λ) Γ(1+ ) }.\nProof. Define ∆t = ||wt −w?||2 − ||wt+1 −w?||2 and ∆̂t = ∑ s∈Y? (gt(s)− g?(s))2 − ∑ s∈Y? (gt+1(s)− g?(s))2.\n(7)\nWe prove the theorem by using an upper and lower bound of ∑ t(∆t + ∆̂t). First, expanding ∑ t(∆t + ∆̂t) by the definition gives∑ t (∆t + ∆̂t) = ||w1 −w?||2 − ||wt+1 −w?||2\n+ ∑ s∈Y? { (g1(s)− g?(s))2 − (gt+1(s)− g?(s))2 } . (8)\nSince w1 = 0 and g1(·) = 0, we can obtain a simple upper bound by omitting the negative terms∑\nt\n(∆t + ∆̂t) ≤ ||w?||2 + ||g?||2, (9)\nwhere ||g?||2 = ∑\ns∈Y? g ?(s)2. To derive the lower bound,\nwe first rewrite ∆t as ||wt−w?||2−||(wt+1−wt)+(wt− w?)||2 by adding null term wt −wt. A further derivation gives ∆t = −||wt+1−wt||2− 2(wt+1−wt) · (wt−w?). With the update rule wt+1 = wt + ytτtxt, we can obtain\n∆t = −τ2t ||xt||2 − 2ytτtxt(wt −w?). (10)\nWe manipulate the second term in Eq. 7 in a similar way by adding null term gt(s)− gt(s) to get\n∆̂t = ∑ s∈Y? {( gt(s)− g?(s) )2 − ( (gt+1(s)− gt(s)) + (gt(s)− g?(s)) )2}\n= ∑ s∈Y? { − ( gt+1(s)− gt(s) )2 − 2 ( (gt+1(s)− gt(s))(gt(s)− g?(s)) )} . (11)\nNote that the algorithm updates gt(s) at time t only if s is one of the approximate suffixes of {yt−1t−i } dt i=1 with dt = t− 1. Therefore gt+1(s)− gt(s) would only have non-zero value if suffix s is within -neighbourhood of yt−1t−|s|. Using this fact, ∆̂t can be further simplified as\n∆̂t = dt∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\n−τ2ω(i, k)2 (12)\n− 2 dt∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\nytτtω(i, k)(gt(s)− g?(s)),\nwhere we have used the update rule gt+1(s) = gt(s) + ytτtω(i, k). By adding Eq. 10 and Eq. 12, we have that ∆t + ∆̂t = −τ2t ( ||xt||2 +\ndt∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\nω(i, k)2 )\n− 2τtyt ( wtxt + dt∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\nω(i, k)gt(s)\n)\n+ 2τtyt ( w?xt + dt∑ i=1 ∑ k=0 ∑ s:s∈Tt−1,\nd(s,yt−1t−i )=k\nω(i, k)g?(s) ) .\n(13)\nLet γ = min{−e−λ + eλ(1−ξ), eλΓ(1 + , λ)/Γ(1 + )}. Combining Corollary 3.1 with the definition of ht and h? leads to\n∆t + ∆̂t ≥− τ2t ( ||xt||2 + γ ) − 2τtytht(xt,yt−11 ) + 2τtyth?(xt,y t−1 1 )\n≥− τ2t ( ||xt||2 + γ ) + 2τt(`t − 1) + 2τt(1− `?t ) = Ψt. (14)\nIf we subtract a non-negative term (21/2τt−2−1/2`?t )2 from Ψt, then Ψt is lower bounded by\nΨt ≥− τ2t ( ||xt||2 + 2 + γ ) + 2τt`t − (`?t )2/2\nLet τt = `t/(||x||2 + 2 + γ). Using ||x||2 ≤ 1,\nΨt ≥τt`t − (`?t ) 2\n2 = `2t ||x||2 + 2 + γ − (` ? t ) 2 2\n≥ ` 2 t 3 + γ − (` ? t ) 2 2 (15)\nCombining the sum of lower bounds ∑T t=1(∆t + ∆̂t) with\nthe upper bound on ∑T t=1(∆t + ∆̂t) in Eq. 9 gives us the bound stated in the theorem\n||w?||2 + ||g?||2 ≥ T∑ t=1 ( `2t 3 + γ − (` ? t ) 2 2 ) .\nThe competitive ratio bound obtained by Theorem 1 depends exponentially on λ but also depends on the other parameter and ξ. We plot the contour maps of γ within a sensible range of parameters in Figure 3. The contour suggests a plausible range of parameters to obtain competitive bound against an arbitrary hypothesis. For example, ξ needs to be close to 1 if the algorithm gives more weight on long suffixes. It is also worth noting that if we set to 0, we can obtain the same competitive ratio provided in the original PST algorithm (Dekel et al., 2005) with more flexible Poisson weighting scheme.\nThe following lemmas used to prove Theorem 1 shows an upper bound on the squared sum of ω(i, k) given sequence yt−11 with respect to λ and ξ. Lemma 2. Let ω(i, k) = ((1 − ξ)kλi exp(−λ)/i!)1/2. Given a binary sequence yt−11 and an arbitrary suffix tree T equipped with Hamming distance metric, ∑t−1 i=1 ∑ k=0 ∑ s:s∈T ,d(s,yt−1t−i )=k ω2(i, k) ≤ −e−λ +\neλ(1−ξ)Γ(t,−λ(−2 + ξ))/Γ(t) for all λ > 0, ≥ 0, and 0 < ξ < 1.\nThe proof of the lemma is provided in Appendix A.\nThe following simplification directly follows from the definition of the gamma function. Corollary 2.1. Under the assumption of Lemma 2,∑t−1 i=1 ∑ k=0 ∑ s:s∈T ,d(s,yt−1t−i )=k\nω2(i, k) ≤ −e−λ + eλ(1−ξ) for all λ > 0, ≥ 0, and 0 < ξ < 1.\nThe bound in Lemma 2 depends on the value of λ, t and ξ. We provide an alternative bound depending on λ and .\nLemma 3. Let ω(i, k) = ((1 − ξ)kλi exp(−λ)/i!)1/2. Given a binary sequence yt−11 and an arbitrary suffix tree T equipped with the Hamming distance metric, ∑t−1 i=1 ∑ k=0 ∑ s:s∈T ,d(s,yt−1t−i )=k\nω2(i, k) ≤ eλΓ(1 + , λ)/Γ(1 + ) for all λ > 0, ≥ 0, and 0 < ξ < 1.\nThe proof of the lemma is also provided in Appendix B.\nFinally, combining above lemmas provides the constant ratio used in Theorem 1.\nCorollary 3.1. Under the assumption of Corollary 2.1 and Lemma 3, ∑t−1 i=1 ∑ k=0 ∑ s:s∈T ,d(s,yt−1t−i )=k ω2(i, k) ≤ min{−e−λ + eλ(1−ξ), e λΓ(1+ ,λ) Γ(1+ ) } for all λ > 0, ≥ 0, and 0 < ξ < 1."
  }, {
    "heading": "4. Self-Bounded aPST",
    "text": "The unbounded aPST algorithm relaxes the exact matching condition of PST. However, similarly to the unbounded PST, the depth of a suffix tree in the unbounded aPST also scales linearly in the length of sequence. In this section, we propose a self-bounded enhancement of the aPST algorithm which automatically grows a bounded-depth aPST. In each round, the algorithm decides whether to increase the depth of the suffix tree based on the prediction of next symbol.\nThe self-bounded aPST algorithm is described in Algorithm 2. We introduce tree depth variable dt calculated on every round of online iteration to represent the maximum depth of the suffix tree at time t. The proposed model automatically trades off between the size of suffix tree and confidence of prediction. We set the minimum value of dt to dλ+ e in order to take into account the suffixes around the maximal weight length bλc. Note that the algorithm updates the tree when the loss is greater than 1/2. This relaxed margin prevents the tree growing linearly with respect to the length of observed sequence. The following theorem provides the loss bound of proposed algorithm in exchange for having a relatively small aPST.\nAlgorithm 2 Online learning algorithm for self-bounded aPST.\n1: Input: T = {∅},w1 = 0, P1 = 0, λ > 0, ≥ 0, δ ∈ (0, 1) 2: Set Γ̄ = Γ(1 + , λ)/Γ(1 + ) 3: Set γ = min{−e−λ + eλ(1−ξ), eλΓ̄} 4: for all t = 1, 2, ..., T do 5: Compute ht(xt,yt−11 ) 6: Predict ŷt = sign(ht(xt,yt−11 )) 7: Receive yt and compute loss `t = max{0, 1− ytht(xt,yt−11 )} 8: if ` > 1/2 then 9: Set τt = `t/(||xt||2 + 2 + γ)\n10: Set dt = max{dλ+ e, dt−1} 11: Define function f(d) = edλdd−d\n12: while Γ̄f(dt) > ( (P 2t−1+τt`t)1/2−Pt−1\n2τt )2 do 13: dt = dt + 1 14: end while 15: Set Pt = Pt−1 + 2τt √ Γ̄f(dt) 16: Update weight vector: wt+1 = wt + ytτtxt 17: Update suffix tree: 18: gt+1(s) = gt(s) + ytτtω(|s|, d(s,yt−1t−i )) if {s : s ∈ T , d(s,yt−1t−i ) ≤ } 19: gt+1(s) = ytτtω(|s|, d(s,yt−1t−i )) if {s : s /∈ T , d(s,yt−1t−i ) ≤ } 20: else 21: Set: τt = 0, Pt = Pt−1 22: end if 23: end for\nTheorem 4 (Self-bounded aPST). Let x1,x2, ...,xT be an input stream and let y1, y2, ..., yT be an output stream, where every ||x||2 ≤ 1 and every yt ∈ {−1,+1}. Let h? = (w?, T ?, g?) be an arbitrary hypothesis such that ||g?||2 ≤ ∞ and which attains the loss values `?1, `?2, ... `?T . Let `1, ..., `T be the sequence of losses attained by the self-bounded online learning algorithm with the -Hamming distance in Algorithm 2. Then the sum of square losses on those rounds where `t > 12 is bounded by\n∑ t:`t> 1 2 `2t ≤ λ̄ (1 +√5 2 ||g?||+ ||w?||+ (1 2 T∑ t=1 (`?t ) 2 ) 1 2 )2 ,\nwhere λ̄ = 3 + min{−e−λ + eλ(1−ξ), e λΓ(1+ ,λ) Γ(1+ ) }.\nSee Appendix C for the detailed proof. Here we provide a sketch of proof. Again, the proof starts from the upper bound and lower bound on ∆+∆̂ defined in Theorem 1. The upper bound remains the same. The derivation of the lower bound is the same up to Eq. 13, however, by the definition of h, we need to add null terms consisting scores of the suffixes from dt+ 1 to t−1 in T ? to formulate the lower bound as a\nlinear function of ht and h? as done in Eq. 14. Adding null terms results in a remainder after reformulation. The lower bound on this remainder can be obtained by Chernoff bound since the weights of additional term can be bounded by the sum of Poisson tail distribution. The combination of two lower bounds leads to a new lower bound on ∆ + ∆̂. Given the combination of upper and lower bounds on ∆ + ∆̂ , we further show that if dt satisfies the condition described in Algorithm 2, the sum of squared losses has the lower bound explained in Theorem 4 via mathematical induction."
  }, {
    "heading": "5. Simulation Study",
    "text": "In this section, we compare the performance of aPST on a sequence prediction task to the classical PST (Dekel et al., 2005) and its variant wPST (Karampatziakis & Kozen, 2009) on a synthetic dataset. We start from a sequence motif, which is frequently occurred subsequences of an original sequence. Many sequences observed in real world applications can be rendered by a small number of motifs (Bailey et al., 2006; Ross et al., 2012). It is known that the PSTs are capable of identifying those motifs from a sequence (Majumdar, 2016). Given a sequence motif, we generate a random sequence with some level of noise, and then, compute predictive accuracies of PST algorithms to compare. Throughout this experiments, we focus on a situation where the input stream is unavailable, i.e. xt = 0 for all t in a sequence, since the modelling on the input stream of aPST algorithm remains the same as those of PST. In other words, we measure and compare a sequence memorisation perspective of both models under a noisy environment in order to emphasise the importance of the approximate matching and weighting scheme."
  }, {
    "heading": "5.1. Binary sequence prediction",
    "text": "We first synthesise simple sequence from the motif [−1,−1,+1,+1]. We construct a sequence by repeating the motif multiple times (25, 50, 100, 200 times), and then, we randomly corrupt each entry of input yi via an independent Bernoulli trial with a fixed noise probability. Without noise in a final sequence, all three models can predict perfectly after observing a first few entries. When a sequence is corrupted by some random noise, PST only relies on an input xt if available, while aPST retrieves approximate suffixes to predict the next symbol.\nFor every experiment, we use the first 40% of a sequence to train, the subsequent 20% of the sequence to validate, and the final 40% of sequence to test the models. For both parameter λ and , we test all possible configuration of λ = {2, 4, 6, 8, 10, 12}, ξ = (0.5, 0.7, 0.9, 0.99), and = {0, 1} and choose the best model based on the accuracy of validation set. All the experiments are repeated over 20 times with randomly corrupted entries. We report two quantities: accuracy of prediction on the uncorrupted entries and the final depth of the suffix tree.\nFigure 4 (a) and (b) show the prediction accuracy and tree depth of the three different models with respect to varying proportions of noise. In general, as the proportion of noise increases, the prediction accuracy of both models decrease. The accuracy converges to the random baseline when the sequence is unpredictable, i.e. a half of sequence is corrupted. Aside from this unpredictable case, aPST always outperforms PST while keeping similar tree depths. Although wPST maintains the shallowest tree depth, it shows the lowest performance among all models.\nFigure 4 (c) and (d) show the prediction accuracy and tree depth of the three different models with respect to varying lengths of sequence. The model predicts better when the\nsequence length is longer in general. The tree depth of aPST increases sub-linearly as the length of sequence increases. We can also observe that the variance of accuracy decreases as the sequence length increases with aPST.\nWe present a more comprehensive analysis of the binary sequence prediction and demonstrate the performance on more complex synthetic sequences in Appendix D."
  }, {
    "heading": "5.2. Multi-class sequence prediction",
    "text": "To predict on sequences with multiple symbols, we adopt ideas from (Crammer & Singer, 2001) and maintain trees T (1), ...T (k) for each symbol. The decision at time t is ŷt = arg maxk h (k) t (xt,y t−1 1 ). If the prediction is wrong, we update the tree parameters of predicted symbol and true symbol with a piecewise margin loss defined as `t = maxk{hkt +1−h yt t } if ŷt 6= yt. Hence, different trees might have different depth. Here, we report the maximum depth among the trees.\nNote that there might be a combinatorial number of approximate suffixes if we add all approximate suffixes while updating the tree. To reduce the computational burden, we add the suffixes of the current sequence into the tree if the suffixes are not in the tree and update the approximate suffixes which are already in the suffix tree. Therefore the suffix tree only contains the sub-sequences which have been observed in the past. The prediction still requires to search the approximate sequences over the suffix tree, but it can be done in an efficient way (Ukkonen, 1993; Giegerich & Kurtz, 1997).\nFor the experiment, we generate a random sequence from motif [1, 2, 3, 4, 1, 3]. Again, we inject random noise based on a Bernoulli trial with a fixed probability. The corrupted symbols are then replaced by random symbol with uniform probability over symbols. We follow the same experimental procedure as used in the binary experiments with the same set of parameters except that we tested up to 2.\nFigure 5 shows the result of the synthetic experiments. In general, aPST outperforms both PST and wPST, and the accuracy increases as increases from 0 to 2. Again, this results emphasis the importance of approximate matching of PST under some noise in a sequence."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we conduct some experiments with real datasets to demonstrate the practical effectiveness of the proposed method. We use three datasets: a symbolic music dataset (Walder, 2016) from which we retain midi onset events only, a system call dataset (Hofmeyr et al., 1998), and human activity dataset (Ordónez et al., 2013). The symbolic music dataset contains four sets of midi music dataset from different sources. The models predict a sequence of midi note number, which ranges 0-127. The system call dataset records a set of system call traces made by active processes, which might contain some intrusions of malicious programs. The models predict the next system call given a previous call sequence. The human activity dataset records a sequence of activities from two subjects. The models predict the next activity of each subject given a trajectory of activities.\nWe again compare aPST with PST and wPST in terms of prediction accuracy and tree depth. For every experiment, we use the first 30% of symbols to adjust the model parameters and use remaining 70% to report the model accuracy and tree depth. We use the same set of parameters used in the previous section.\nTable 1, 2, 3 show the accuracies and final tree depth of PST models on music, system call, human activity datasets, respectively. For the music and human activity datasets, aPST outperforms the other models in terms of accuracy while using a slightly larger suffix tree. For the system call dataset, the tree depth of aPST is shallower than those of the other models while having a similar or better accuracies. The performance gain of aPST against the other models are\nsignificant when the sequence lengths are relatively short.\nWe plot the histogram of the best λ values for the music dataset and system call dataset in Figure 6. The graph shows a quite distinctive characteristic of these two datasets. λ values from the musical sequences distribute evenly across range 0 to 8, while λ from system call traces are highly focused on the range [3, 5). We conjecture that the songs have a greater variety of relevant motif lengths than the system call traces which have more static transition patterns (Nikolopoulos & Polenakis, 2014), therefore λ values are adjusted according to the nature of a song."
  }, {
    "heading": "7. Related Work",
    "text": "Variants of the PST algorithm have been developed in different scientific communities in different forms such as the variable length Markov models and context tree weighting (Willems et al., 1995; Helmbold & Schapire, 1997; Bühlmann et al., 1999; Bellemare et al., 2014). Most of these algorithms need an a-priori bound on the maximum number of previous symbols. Apostolico & Bejerano show that the upper bound assumption can be relaxed by a linear time prediction tree construction algorithm where the depth of suffix tree can increase up to the length of a training sequence. Dekel et al. propose an alternative self-bounded PST learning algorithm where the depth of prediction tree is bounded automatically based on a number of mistakes made by the algorithm. Karampatziakis & Kozen combine the idea of self-bounded PST and Winnow algorithm and derive a multiplicative update algorithm for online learning. By using the multiplicative update rules, the suggested algorithm can quickly adopt variation in complex sequence\nwhich exhibit different patterns at various points during life time. Xiao & Eckert further extend the PST to incorporate additional side information. They derive a second order online learning algorithm to take into account the variance of the estimator."
  }, {
    "heading": "8. Conclusion",
    "text": "We have presented the decision theoretic prediction suffix tree with approximate string matching (aPST) by relaxing the exact matching condition of the PST models. The depth of the suffix tree generated by the proposed algorithm scales linearly with the length of the input sequence. To limit the depth of aPST, we proposed self-bounded version of aPST which automatically determines the depth of suffix tree. The loss bounds for both unbounded- and bounded-aPST are analysed. We showed that the applications of aPST to sequence modelling outperform the other PST models under a noisy environment via synthetic datasets. Furthermore, we showed the improved predictive performance of aPST on three real world datasets. Future work on this research will explorer wide range of distance metrics instead of the Hamming distance in order to take into account more complex editing behaviour in sequences."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Minjeong Shin, Cheng Soon Ong and Lexing Xie for instructive discussions on an early draft of this work. We also thank the anonymous reviewers for their detailed and thoughtful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Optimal amnesic probabilistic automata or how to learn and classify proteins in linear time and space",
    "authors": ["A. Apostolico", "G. Bejerano"],
    "venue": "Journal of Computational Biology,",
    "year": 2000
  }, {
    "title": "Meme: discovering and analyzing dna and protein sequence motifs",
    "authors": ["T.L. Bailey", "N. Williams", "C. Misleh", "W.W. Li"],
    "venue": "Nucleic acids research,",
    "year": 2006
  }, {
    "title": "Skip context tree switching",
    "authors": ["M. Bellemare", "J. Veness", "E. Talvitie"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Variable length markov chains",
    "authors": ["P. Bühlmann", "Wyner", "A. J"],
    "venue": "The Annals of Statistics,",
    "year": 1999
  }, {
    "title": "On the algorithmic implementation of multiclass kernel-based vector machines",
    "authors": ["K. Crammer", "Y. Singer"],
    "venue": "Journal of machine learning research,",
    "year": 2001
  }, {
    "title": "The power of selective memory: Self-bounded learning of prediction suffix trees",
    "authors": ["O. Dekel", "S. Shalev-Shwartz", "Y. Singer"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2005
  }, {
    "title": "From ukkonen to mccreight and weiner: A unifying view of linear-time suffix tree construction",
    "authors": ["R. Giegerich", "S. Kurtz"],
    "year": 1997
  }, {
    "title": "Upper bounds on poisson tail probabilities",
    "authors": ["P.W. Glynn"],
    "venue": "Operations research letters,",
    "year": 1987
  }, {
    "title": "Predicting nearly as well as the best pruning of a decision tree",
    "authors": ["D.P. Helmbold", "R.E. Schapire"],
    "venue": "Machine Learning,",
    "year": 1997
  }, {
    "title": "Probability inequalities for sums of bounded random variables",
    "authors": ["W. Hoeffding"],
    "venue": "Journal of the American statistical association,",
    "year": 1963
  }, {
    "title": "Intrusion detection using sequences of system calls",
    "authors": ["S.A. Hofmeyr", "S. Forrest", "A. Somayaji"],
    "venue": "Journal of computer security,",
    "year": 1998
  }, {
    "title": "Learning prediction suffix trees with winnow",
    "authors": ["N. Karampatziakis", "D. Kozen"],
    "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,",
    "year": 2009
  }, {
    "title": "Prediction of human activity by discovering temporal sequence patterns",
    "authors": ["K. Li", "Y. Fu"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2014
  }, {
    "title": "Finding dna motifs: A probabilistic suffix tree approach",
    "authors": ["A. Majumdar"],
    "year": 2016
  }, {
    "title": "Dynamic-depth context tree weighting",
    "authors": ["J.V. Messias", "S. Whiteson"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Detecting malicious code by exploiting dependencies of system-call groups",
    "authors": ["S.D. Nikolopoulos", "I. Polenakis"],
    "venue": "arXiv preprint arXiv:1412.8712,",
    "year": 2014
  }, {
    "title": "Activity recognition using hybrid generative/discriminative models on home environments using binary sensors",
    "authors": ["F.J. Ordónez", "P. de Toledo", "A. Sanchis"],
    "year": 2013
  }, {
    "title": "An introduction to abstract algebra",
    "authors": ["D.J. Robinson"],
    "venue": "Walter de Gruyter,",
    "year": 2003
  }, {
    "title": "The power of amnesia: Learning probabilistic automata with variable memory length",
    "authors": ["D. Ron", "Y. Singer", "N. Tishby"],
    "venue": "Machine Learning,",
    "year": 1996
  }, {
    "title": "Detecting melodic motifs from audio for hindustani classical music",
    "authors": ["J.C. Ross", "T. Vinutha", "P. Rao"],
    "venue": "In ISMIR,",
    "year": 2012
  }, {
    "title": "Approximate string-matching over suffix trees",
    "authors": ["E. Ukkonen"],
    "venue": "In Combinatorial Pattern Matching,",
    "year": 1993
  }, {
    "title": "Symbolic music data version 1.0",
    "authors": ["C. Walder"],
    "venue": "arXiv preprint arXiv:1606.02542,",
    "year": 2016
  }, {
    "title": "The context-tree weighting method: basic properties",
    "authors": ["F.M. Willems", "Y.M. Shtarkov", "T.J. Tjalkens"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1995
  }, {
    "title": "Efficient online sequence prediction with side information",
    "authors": ["H. Xiao", "C. Eckert"],
    "venue": "In Data Mining (ICDM),",
    "year": 2013
  }],
  "id": "SP:f8b61998792be3d53f126bb2561a3565c246f3d0",
  "authors": [{
    "name": "Dongwoo Kim",
    "affiliations": []
  }, {
    "name": "Christian Walder",
    "affiliations": []
  }],
  "abstractText": "Prediction suffix trees (PST) provide an effective tool for sequence modelling and prediction. Current prediction techniques for PSTs rely on exact matching between the suffix of the current sequence and the previously observed sequence. We present a provably correct algorithm for learning a PST with approximate suffix matching by relaxing the exact matching condition. We then present a self-bounded enhancement of our algorithm where the depth of suffix tree grows automatically in response to the model performance on a training sequence. Through experiments on synthetic datasets as well as three real-world datasets, we show that the approximate matching PST results in better predictive performance than the other variants of PST.",
  "title": "Self-Bounded Prediction Suffix Tree via Approximate String Matching"
}