{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In this paper, we focus on the multi-term nonsmooth convex composite optimization\nmin x∈X f(x) + n∑ i=1 gi(x), (1)\nwhere X is a linear space, gi : X → (−∞,+∞] is a proper, lower semicontinuous convex function for all i = 1, · · · , n, and f : X → (−∞,+∞) is a continuous\n1Tencent AI Lab, China 2Sun Yat-sen University, China 3The Chinese University of Hong Kong, China. Correspondence to: Li Shen <mathshenli@gmail.com>, Wei Liu <wliu@ee.columbia.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndifferentiable convex function with its gradient satisfying the inequality that\n1\nL ∥∥∇f(x)−∇f(y)∥∥2 ≤ 〈∇f(x)−∇f(y), x− y〉. (2) The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010), graph-guided fused Lasso (Chen et al., 2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; Dupé et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007). By introducing the multi-term nonsmooth regularization term∑n i=1 gi(x) such as structured sparsity (Huang et al., 2011; Bach et al., 2012; Bach, 2010) and nonnegativity (Chen & Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models. However, due to the multi-term nonsmooth regularization term ∑n i=1 gi(x), the optimization problem (1) is too complicated to be solved even for small n. For n ≤ 2, some existing popular first-order optimization methods are accelerated proximal gradient method (Beck & Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three operator splitting method (Davis & Yin, 2015), and some primal-dual operator splitting methods such as majorized alternating direction method of multiplier (ADMM) (Cui et al., 2016; Lin et al., 2011), fast proximity method (Li & Zhang, 2016), and so on.\nOn the other hand, when n ≥ 3, there also exist some algorithms for solving problem (1). A directly method for (1) is smoothing accelerated proximal gradient (S-APG) proposed by Nesterov (Nesterov, 2005a;b). Then, Yu (Yu, 2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average approximation technique and Nesterov’s acceleration technique, which has been enhanced very recently by Shen et al. (Shen et al., 2017). Their proposed method called APA-APG adopts an adaptive stepsize strategy. However,\nthe above mentioned methods S-APG, PA-APG and its enhanced version APA-APG all need a strict restriction on the nonsmooth functions {gi(x)} that each gi(x) must be Lipschitz continuous. In addition, some primal-dual parallel splitting methods (Briceno-Arias et al., 2011; Combettes & Pesquet, 2007; 2008; Condat, 2013; Vũ, 2013) generalized from traditional operator splitting, such as forward backward splitting method (Chen & Rockafellar, 1997) and Douglas Rachford splitting method (Eckstein & Bertsekas, 1992), can also solve the multi-term nonsmooth convex composite optimization problem (1). Different from prior work, Raguet et al. (Raguet et al., 2013) proposed an efficient primal operator splitting method called generalized forward backward splitting method using the classic forward backward splitting technique, which has shown the superiority over numerous existing primal-dual splitting methods (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) in dealing with variational image restoration problems. All the above mentioned methods for problem (1) with n ≥ 3 share a common feature that they all split the nonsmooth composite term∑n i=1 gi(x) in the Jacobi iteration manner, i.e., parallelly. This is one of the main differences between existing splitting methods and our proposed method in this paper.\nTo split the nonsmooth composite term ∑n i=1 gi(x) more efficiently, we propose a novel operator splitting algorithm to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal mapping of the current function gi(x) uses the proximal mappings of gj(x) for all j < i which have already been computed ahead. In addition, to further improve the algorithm’s efficiency, we leverage the over-relaxation acceleration technique. What’s more, we provide a new strategy that the over-relaxation stepsize can be determined adaptively, ensuring a larger value to accelerate the algorithm. The most important is that the convergence of our proposed GSOS algorithm is established by a newly developed analysis technique. In detail, given an invertible linear operator R, we first argue that the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of problem (1) can be recovered\nby the zero point set [ (R∗)−1SR, ∂g+A◦∇f◦A,NV ]−1 (0). This is fulfilled through adopting the tool of operator optimization theory, in which the composite operator SR, ∂g+A◦∇f◦A,NV is generalized from the definition of the composite monotone operator Sλ,A,B in (Eckstein & Bertsekas, 1992). Next, by unitizing the definition of the -enlargement of maximal monotone (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), we establish a key property for SR, ∂g+A◦∇f◦A,NV , that is, gph ( SR, (∂g+A∗◦∇f◦A)[ ],NV ) ⊆\ngph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ][ ] ) . Based on this observation, we equivalently reformulate the GSOS algorithm as a two-step iterations algorithm. Then, the\nglobal convergence of the proposed GSOS algorithm is easily established based on this reformulation.\nThe closest algorithm to our proposed GSOS algorithm is the generalized forward backward splitting method proposed by Raguet et al. (Raguet et al., 2013). By carefully selecting the scaling matrix H in the forthcoming GSOS algorithm, it is easy to check that GSOS covers the generalized forward backward splitting method as a special case. Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method (Luo & Tseng, 1991; Yuan et al., 2016). Choosing the scaling matrixH suitably, the proposed GSOS algorithm can inherit the advantage of the matrix splitting technique which has shown the efficiency in (Yuan et al., 2016) for coping with a special class of coordinate separable composite optimization problems.\nThe rest of this paper is organized as follows. In Section 2, we first give the definitions of some useful notations which can make the paper much more readable. We also establish some lemmas and propositions based on monotone operator theory (Bauschke & Combettes, 2011), which are the key to the convergence of the GSOS algorithm. In Section 3, we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity. In Section 4, we conduct numerical experiments on overlapping group Lasso and graph-guided fused Lasso problems to evaluate the efficacy of the GSOS algorithm. Finally, we draw conclusions in Section 5."
  }, {
    "heading": "2. Preliminaries and Notations",
    "text": "Let Y = ∏n i=1 Xi be the product space of Xi with Xi = X for all i ∈ {1, 2, · · · , n}. Let V be a linear space and V⊥ be its complementary space with the following definitions V= { y ∈ Y | y1 = · · · = yn } , V⊥= { y ∈ Y |\nn∑ i yi = 0 } .\nLet IX : X → X be the identity map and EY : X → Y be a block linear operator defined as EY =( IX · · · IX )∗ . Let A : Y → X be a linear operator defined as Ay = 1nE ∗ Yy = 1 n ∑n i=1 yi. Hence, its adjoint operator A∗ : X → Y is defined as A∗x = 1nEYx. Let H,R : Y → Y be block lower triangular linear invertible operators satisfying (R∗)−1 = H and H + H∗ 0. Moreover,H is defined as H1,1 0 · · · 0 ... . . . ...\n... Hn−1,1 · · · Hn−1,n−1 0 Hn,1 · · · Hn−1,n Hn,n  , (3) where Hi,j : X → X is a linear operator for all (i, j) ∈ {1, · · · , n}. It is worthwhile to emphasize that Hi,i is also possible to be a lower triangular linear operator satisfying\nHi,i +H∗i,i 0. Next, we abuse the notation ‖ · ‖H which is induced by the inner product 〈·,H·〉 satisfying\n‖ · ‖H : = √ 〈·,H·〉 = √ 〈·,H∗·〉\n= √ 〈·, H+H ∗\n2 ·〉 = ‖ · ‖H+H∗ 2 . (4)\nIn addition, we define the generalized proximal mapping of a proper, lower semicontinuous convex function gi(x) with respect to the invertible linear operatorHi,i.\nDefinition 1 For a given x, the proximal mapping denoted by ProxH−1i,i gi(x) of a proper, lower semicontinuous convex function gi with respect to an invertible linear operator Hi,i satisfying Hi,i + H∗i,i 0 is defined to be the zero point of the following inclusion equation\n0 ∈ ∂gi(·) +Hi,i(· − x). (5)\nMoreover, if Hi,i is symmetric, it can be reformulated as the following convex minimization\nProxHi,igi(x) := arg min y∈X\ngi(y) + 1\n2 ‖y − x‖2Hi,i .\nNext, we recall the definition of -enlargement of monotone operators (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), which is an effective tool for establishing the convergence of the proposed GSOS algorithm.\nDefinition 2 Given a maximal monotone operator T : X ⇒ X, the (≥ 0)-enlargement of T is defined as the set T [ ](x) := { v ∈ Y | 〈w − v, z − x〉 ≥ − for all z ∈\nX, w ∈ T (z) } .\nRecall that f(x) is a gradient Lipschitz convex function satisfying inequality (2). There exits 0 Σ Σ̂ LI such that the following two inequalities hold for any x, x′ ∈ X\nf(x) ≤ f(x′) + 〈∇f(x′), x− x′〉+ 1 2 ‖x− x′‖2 Σ̂ , (6) f(x) ≥ f(x′) + 〈∇f(x′), x− x′〉+ 1 2 ‖x− x′‖2Σ. (7)\nActually, when f(x) is a quadratic function, it holds Σ = Σ̂ directly in inequalities (6) and (7). The following lemma establishes the property of the enlargement of the composite operatorA∗ ◦∇f ◦A with f satisfying inequalities (6)- (7) or (2), which is an essential ingredient for reformulating the GSOS algorithm as a two-step iterations algorithm.\nProposition 1 Assume that f is a gradient Lipschitz continuous convex function satisfying inequality (2). For any x1, x2 ∈ Y , it holds that\n(A∗ ◦ ∇f ◦ A)(x2) ∈ (A∗ ◦ ∇f ◦ A)[ ](x1) (8)\nwith = L4 ‖Ax1−Ax2‖ 2. In addition, if f further satisfies inequalities (6)-(7), it holds that\n(A∗ ◦ ∇f ◦ A)(x2) ∈ (A∗ ◦ ∇f ◦ A)[ ](x1) (9)\nwith = 14‖Ax1 −Ax2‖ 2 2Σ̂−Σ .\nRemark 1 Two comments are made for Proposition 1:\n(1) This proposition gives two types of estimations for in( A∗ ◦∇f ◦A )[ ] in (8) and (9). When f is a quadratic\nfunction, it is easy to check that\n1 4 ‖Ax1 −Ax2‖22Σ̂−Σ ≤ L 4 ‖Ax1 −Ax2‖2\ndue to Σ̂ = Σ LI. When f is a general gradient Lipschitz continuous function, we do not know which estimation for is tighter in (8) and (9).\n(2) The second part of this proposition can be regarded as an intensified version of Lemma 2.2 in (Svaiter, 2014) for a specified composite operator A∗ ◦ ∇f ◦ A. The first part of the proposition coincides with the results by applying Lemma 2.2 in (Svaiter, 2014) for A∗ ◦ ∇f ◦ A.\nNext, we generalize the notation Sλ,T1,T2 in (Eckstein & Bertsekas, 1992) for a given λ > 0 and two maximal monotone operators T1, T2 as SR,T1,T2 for a given invertible linear operatorR defined as\ngphSR, T1, T2 (10) := { (x1 +Ry2, x2 − x1) | y1 ∈ T1(x1),\ny2 ∈ T2(x2), x1 +R∗y1 = x2 −R∗y2 } .\nBy (Eckstein & Bertsekas, 1992), we know that Sλ,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone. However, its generalized operator SR,T1,T2 is not monotone unless the invertible linear operator R reduces to be a constant. Very interesting, it can be shown that its composition with (R∗)−1, i.e., (R∗)−1SR,T1,T2 , is maximal monotone for any invertible linear operatorR.\nLemma 1 For any given invertible linear operator R, operator (R∗)−1SR,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone operators.\nSetting T1 = ∂g + A∗ ◦ ∇f ◦ A, T2 = NV , we obtain SR, ∂g+A∗◦∇f◦A,NV , which is defined as\ngph ( SR,∂g+A∗◦∇f◦A,NV ) (11)\n:= { (x1+Ry2, x2−x1) | y1∈(∂g +A∗ ◦ ∇f ◦ A)(x1),\ny2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .\nBy Lemma 1, we know that (R∗)−1SR, ∂g+A∗◦∇f◦A,NV is maximal monotone due to the maximal monotonicity of ∂g + A∗ ◦ ∇f ◦ A and NV . Hence, given a constant ≥ 0, the enlargement [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ][ ] is well defined. In addition, based on the definition of SR,T1,T2 again, we set T1 = ∂g + (A∗ ◦ ∇f ◦ A)[ ], or T1 = (∂g + A∗ ◦ ∇f ◦ A)[ ] and T2 = NV in (10). Then we have the definition of SR,∂g+(A∗◦∇f◦A)[ ],NV or SR,(∂g+A∗◦∇f◦A)[ ],NV for any given invertible linear operatorR and constant ≥ 0 as follows\ngph ( SR,∂g+(A∗◦∇f◦A)[ ],NV ) (12)\n:= { (x1+Ry2, x2−x1)|y1∈(∂g+(A∗◦∇f ◦A)[ ])(x1),\ny2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } ,\ngph ( SR,(∂g+A∗◦∇f◦A)[ ],NV ) (13)\n:= { (x1+Ry2, x2−x1)|y1∈(∂g +A∗◦∇f ◦A)[ ])(x1),\ny2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .\nIn the proposition below, we will establish the relationships among the above mentioned three operators SR,∂g+(A∗◦∇f◦A)[ ],NV , SR,(∂g+A∗◦∇f◦A)[ ],NV and [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ][ ].\nProposition 2 Given a constant ≥ 0 and an invertible linear operatorR, it holds that\ngph ( SR, ∂g+(A∗◦∇f◦A)[ ],NV ) ⊆ gph ( SR, (∂g+A∗◦∇f◦A)[ ],NV\n) ⊆ gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ][ ] ) .\nIn the following, we establish the relationship between the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of prob-\nlem (1) and [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0), which means that we can recover the solution of problem (1) through [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0).\nLemma 2 Let linear operators H and R satisfy (R∗)−1 = H and H satisfy (3). Denote Ω = [ (R∗)−1SR, (∂g+A∗◦∇f◦A),NV ]−1 (0). It holds that[ ∇f +\nn∑ i=1 ∂gi\n]−1 (0) = ( ETYH∗EY\n)−1ETYH∗(Ω)."
  }, {
    "heading": "3. GSOS Algorithm",
    "text": "In this section, we first propose the Gauss-Seidel operator splitting algorithm for solving the multi-term nonsmooth convex composite problem (1). Then, based on the preliminaries in Section 2, we establish the convergence and iteration complexity of the GSOS algorithm.\nAlgorithm 1 GSOS Algorithm Parameters: Choose σ ∈ (0, 1), a linear operator H satisfying (3) and a starting point z0 ∈ Z . Set θfix1 ∈( − 1, θ1 ] and θfix2 ∈ ( − 1, θ2 ] , where θ1 and θ2 are\ndefined via equations (14a) and (14b), respectively. for k = 0, 1, 2, · · · ,K do xk := EY ( ETYHEY )−1ETYHzk; for i = 1, 2 · · · , n do yki := ProxH−1i,i gi ( H−1i,i [ ∑i j=1Hi,j(2xkj − zkj ) −\n1 n∇f( 1 n ∑n i=1 x k i )− ∑i−1 j=1Hi,jykj ] ) ;\nend for set θadap1k as (14c) and θ adap2 k as (14d); set θk ∈ [θfix1, θadap1k ] ∪ [θfix2, θ adap2 k ]; zk+1 := zk + (1 + θk)(y k − xk);\nend for return ωK := ( ETYH∗EY )−1ETYH∗zK . In Algorithm 1, parameters θ1, θ1, θ adap1 k , θ adap1 k are defined as θ1 = max { θ|(θ − σ)(H+H∗) + LA∗A 0 } ; (14a) θ2 = max { θ | (θ − σ)(H+H∗) (14b) +A∗(2Σ̂− Σ)A 0 } ; θadap1k = σ − L‖A(xk − yk)‖2 ‖xk − yk‖2H+H∗ ; (14c) θadap2k = σ − ‖A(xk − yk)‖2\n2Σ̂−Σ ‖xk − yk‖2H+H∗ . (14d)\nRemark 2 We make some comments on GSOS below.\n(1) For the updating step of xk, we obtain xk = EY (∑K i,j=1Hij )−1∑K j=1 ∑K i=j Hijzkj by using the\nnotations H and EY . Similarly, we have ωk =(∑K i,j=1Hij )−1∑K j=1 ∑j i=iH∗jizkj . Hence, we need\nto compute the inverse of ∑n i,j=1Hi,j . However, if Hi,j is a lower triangular matrix operator, xk and ωk can be obtained easily.\n(2) By the definitions of ProxH−1i,i gi and y k, we need to\nsolve the following inclusion equation\nGki ∈ Hi,iyki + ∂gi(yki ),\nwhere Gki = H −1 i,i [∑i j=1Hi,j(2xkj − zkj ) − 1 n∇f( 1 n ∑n i=1 x k i ) − ∑i−1 j=1Hi,jykj ] . Usually, it is easy to choose a suitable Hi,i such that the solution of the above inclusion equation has a closed form.\n(3) θk is the over-relaxation stepsize for accelerating the GSOS algorithm. If the computations of θadap1k and θadap2k are time consuming, we can set θk = max{θfix1, θfix2}.\n(4) WhenH is a diagonal matrix, i.e.,Hi,j = 0 andHi,i = aiI with some nonnegative constant ai, and the over relaxation stepsize θk is fixed to a smaller region, the GSOS algorithm reduces to the generalized forward backward splitting method in (Raguet et al., 2013).\nIn the following, we reformulate the GSOS algorithm as a two-step iterations algorithm by utilizing monotone optimization theory established in Section 2, which is the key to the convergence of the GSOS algorithm.\nProposition 3 Let g : Y → (−∞,+∞] be the function defined as g(x) = ∑n i=1 gi(xi). Assume that the sequences (xk, yk) and zk are generated by Algorithm 1 with σ ∈ (0, 1). Let vk = (R∗)−1(xk − yk) and zk = yk + R(R∗)−1(zk − xk). Then, for all k ∈ N, there exists k ≥ 0 such that the iterations in Algorithm 1 can be reformulated as the following two-step iterations algorithm: vk ∈ [(R∗)−1SR, ∂g+(A∗◦∇f◦A),NV ] [ k](zk), (15a) θk‖R∗vk‖2R−1 + ‖R ∗vk + zk − zk‖2R−1\n+2 k ≤ σ‖zk − zk‖2R−1 , (15b)\nand zk+1 = zk − (1 + θk)R∗vk.\nRemark 3 Based on Proposition 3, the GSOS algorithm can be regarded as an inexact over-relaxed metric proximal point algorithm for the composite inclusion\n0 ∈ (R∗)−1SR,∂g+A∗◦∇f◦A,NV (z).\nBy Proposition 3 and Lemma 2, we can establish the convergence of the GSOS algorithm based on the relationship\nbetween the two zero point sets [∇f+ n∑ i=1 ∂gi] −1(0) and Ω.\nTheorem 1 Let {(xk, yk, zk)} be the sequence generated by Algorithm 1. We have:\n(i) for any z∗ ∈ [(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0), it holds that\n‖zk+1 − z∗‖2R−1 ≤ ‖z k − z∗‖2R−1 (16)\n− (1− σ)(1 + θk)‖xk − yk‖2R−1 ;\n(ii) zk converges to a point belonging to zero point set [(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0) and ωk converges to a point belonging to [∇f + ∑n i=1 ∂gi] −1 (0), i.e., the optimal solution set\nof problem (1).\nTheorem 1 indicates that ‖xk − yk‖ approaching to zero implies the convergence of the GSOS algorithm. In the theorem below, we measure the convergence rates of two sequences ‖xk − yk‖ and ‖ωk − ωk+1‖.\nTheorem 2 Let zk be the sequence generated by the GSOS algorithm. Then, there exists i ∈ {1, 2, · · · , k} such that\n‖xi − yi‖2 ≤ O (1 k ) , ∥∥ωi+1 − ωi∥∥2 ≤ O(1 k ) .\nDue to the space limit, all proofs of the propositions, lemmas and theorems are placed into the supplementary material."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, we apply the proposed algorithm to the overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010) and graph-guided fused Lasso problems (Chen et al., 2012; Kim & Xing, 2009), which can be formulated as\nmin 1\n2 ‖Sx− b‖2 + K∑ i=1 gi(x). (17)\nFor overlapping group Lasso problem (21), gi(x) = ναi‖xGi‖ andK denotes the number of groups. For graphguided Lasso problem (25), gi(x) = ναij‖xi−xj‖ and K denotes the number of edges in the graph edge set E.\nWe describe the detailed techniques in the experimental implementation for (17). Given a > 12 and a positive definite operator D satisfying D STS, we set\nHi,j = {\n1 K2D, i ≥ j ∈ {1, 2, · · · ,K}; a K2D, i = j ∈ {1, 2, · · · ,K}.\n(18)\nHence, it easy to check that H + H∗ = A∗DA + 2a−1 K2 Diag ( EYD ) 0. Due to the smooth term in overlapping group Lasso (21) is quadratic, the two estimations θ2 and θadap2k in (14b) and (14d) are preferred to be used. By specific H, we obtain ∑K i,j=1Hi,j =\nK(K−1)+2αK 2K2 D and∑K\nj=1 ∑K i=j Hi,jzkj = D K2 ∑K j=1(a+K−j)zkj ,which fur-\nther imply xk = (∑K i,j=1Hi,j )−1∑K j=1 ∑K i=j Hi,jzkj =\n2 ∑K j=1(a+K−j)z k j\nK(K−1)+2aK . Moreover, by the positive definiteness of Hi,i and D, it holds that ∑n j=1 ∑j i=iH∗j,izkj =\nD K2 ∑K j=1(a + j − 1)zkj . Hence, we attain ωk =\n2 ∑K j=1(a+j−1)z k j\nK(K−1)+2aK . In addition, by the definition of H, we reformulate the estimation (14b) for θk as the following form:\nθ = max { θ | EY [ (σ − θ)D − STS ] E∗Y\n+ (2a− 1) ( σ − θ ) Diag(EYD) 0 } .\nDue to a ≥ 12 and the positive definiteness of D, a sufficient condition satisfying the constraint in the above set is{\n(σ − θ)D − STS 0, θ ≤ σ }\n. Hence, we have an alternative estimation for θ as\nθ = max { θ | (σ − θ)D − STS 0, θ ≤ σ } . (19)\nSimilarly, the adaptive stepsize estimation (14d) is reformulated as\nθadapk = σ −\n1 2K2 ∥∥∥∥ K∑ i=1 (xk − yki ) ∥∥∥∥2 STS\nK∑ j=1 K∑ i=j (xk − yki )THij(xk − ykj ) . (20)\nTherefore, the GSOS algorithm can be specified as the following form for solving problem (17).\nAlgorithm 2 GSOS Algorithm for Solving Problem (17) Parameters: Choose σ ∈ (0, 1), positive definite operators D and Hi,j satisfying (18), and a starting point z0 ∈ Z . Set θ as (19) and θfix ∈ ( − 1, θ1 ] .\nfor k = 0, 1, 2, · · · , do xk := 2 ∑K j=1(α+K−j)z k j\nK(K−1)+2αK ; for i = 1, 2 · · · ,K do yki := ProxH−1i,i gi ( H−1i,i [ ∑i j=1Hi,j(2xk − zkj ) −\n1 KS T (Sxk − b)− ∑i−1 j=1Hi,jykj ] ) ;\nend for set θk ∈ [θfixk , θ adap k ], where θ adap k is defined via (20); for j = 1, 2 · · · ,K do zk+1j := z k j + (1 + θk)(y k j − xk);\nend for end for return ωN = 2 ∑K j=1(α+j−1)z N j\nK(K−1)+2αK .\nIn this paper, we compare the proposed GSOS algorithm with four state-of-the-art algorithms below.\n• GFB (Raguet et al., 2013): Generalized Forward Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.\n• PDM (Condat, 2013): A first-order Primal-Dual splitting Method (PDM) (Condat, 2013) for solving jointly the primal and dual formulations of large-scale convex minimization problems involving Lipschitz, proximal and linear composite terms.\n• PA-APG (Yu, 2013): Proximal Average approximated Accelerated Proximal Gradient (PA-APG) algorithm (Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al., 2008) to separate the multi-term nonsmooth function in (1). It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov, 2005b;a).\n• APA-APG (Shen et al., 2017): An enhanced version of PA-APG, which incorporates the Adaptive Proximal Average approximation technique with the Accelerated Proximal Gradient (APA-APG) method to improve the efficiency of the optimization procedure.\nIt is worthwhile to emphasize that PA-APG and APA-APG algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization is Lipschitz continuous. Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided fused Lasso are all exactly Lipschitz continuous, the two efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with the GSOS algorithm to illustrate the efficacy of GSOS. In the implementation, the approximation parameter for PAAPG is set as 1.0e− 5."
  }, {
    "heading": "4.1. Overlapping Group Lasso",
    "text": "In this subsection, we apply the proposed GSOS algorithm to the overlapping group Lasso problem, which takes the following formal definition:\nmin 1\n2 ‖Sx− b‖2 + ν K∑ i=1 αi‖xGi‖, (21)\nwhere S ∈ Rn×d is the sampling matrix, b is the noisy observation vector, G = {G1, · · · ,GK} denotes the set of overlapping groups (Gi ⊂ {1, · · · , d} satisfying ⋃K i=1 Gi =\n{1, · · · , d} and Gi ⋂ Gj 6= ∅ for some i, j), xGi ∈ Rd is a duplication of x with x{1,··· ,d}\\Gi = 0, αi is the weight for the i-th group, and ν is the regularization parameter controlling group sparsity.\nDuring the implementation of Algorithm 2, we need to calculate the generalized proximal mapping of ‖xGi‖ in the updating step of yki . By the positive definiteness of Hi,i, the calculation of yki in Algorithm 2 is equivalent to solving the following problem:\nyki := arg min x\n1 2 ‖x− bk‖2Hi,i + ναi‖xGi‖,\nwhere bk = H−1i,i [∑i j=1Hi,j(2xk − zkj ) − 1 KS\nT (Sxk − b)− ∑i−1 j=1Hi,jykj ] . In the proposition below, given c, diag-\nonal positive definite operatprHi,i and group G, we solve\nx∗ := arg min x\n1 2 ‖x− c‖2Hi,i + ν‖xG‖. (22)\nWhen Hi,i is identity matrix I, (22) has the closed-form solution\nx∗ = { x∗G , i ∈ G, ci, else,\nwhere x∗G = { (1− ν/‖cG‖)cG , ‖cG‖ ≥ t; 0, else.\nProposition 4 Let (Hi,i)G be the subdiagonal matrix of Hi,i with the index set G, and t∗ be the optimal solution of the one-dimensional optimization problem\nmin t≥0\n{ 1\n2\n〈 cG , [ (Hi,i)−1G + 2tI ]−1 cG 〉 + tν2 } . (23)\nHence, the optimal solution of (22) has the following form\nx∗ =\n{ cG − [ I + 2t∗(Hi,i)G ]−1 cG , i ∈ G;\nci, else. (24)\nLike (Chen et al., 2012; Yu, 2013), the entries of sampling matrix S ∈ Rn×d are sampled from an i.i.d. normal distribution, and x ∈ Rd with xj = (−1)j exp−(j−1)/100 and d = 90K + 10. Let ξ be the noise sampled from the standard normal distribution, and the noisy observation satisfies b = Sx + ξ. In addition, we set ν = 1 and αi = 1K2 for each group Gi and the groups {Gi} are overlapped by 10 elements, that is{\nG1 = {1, · · · , 100} G2 = {91, · · · , 190} · · · GK = {d− 99, · · · , d}\n} .\nThe sampling size and the number of groups (n,K) are chosen from the following set\n(n,K) ∈ {\n(1000, 20), (2000, 40), (4000, 60), (4000, 80), (5000, 80), (5000, 100)\n} .\nTo further reduce the computations, in Algorithm 2 we set Hi,i = ‖STS‖I and the over-relaxation stepsize θk as θ in (19). Hence, the compared five solvers GSOS, GFB, PDM, PA-APG and APA-APG have the same computational cost in each iteration. To be fair, all the compared algorithms start with the same initial point. The following six pictures in Figures 1 and 2 display the comparisons of the five solvers for a variety of (n,K). It is apparent that our proposed GSOS algorithm shows great superiorities over the other four solvers. The primal-dual solver PDM is slightly faster than the primal solver GFB. PA-APG is the slowest algorithm, because the prespecified proximal average approximation precision is 1.0e − 5 which leads to a very small stepsize. Also, APA-APG is much faster than the other four solvers at the first 50 iterations. However, it is slowed down since the stepsize used in AP-APG becomes smaller and smaller as the iterations go on."
  }, {
    "heading": "4.2. Graph-Guided Fused Lasso",
    "text": "In this subsection, we perform experiments on graphguided fused Lasso which is formulated as\nmin 1\n2 ‖Sx− b‖2 + ν ∑ (i,j)∈E αij |xi − xj |, (25)\nwhere αij ≥ 0 is the weight for the fused term ‖xi − xj‖ for all (i, j) ∈ E (E is the given graph edge set), and ν is\nthe regularization parameter.\nIn the implementation of Algorithm 2 for tackling graphguided fused Lasso (25), we need to solve the following optimization in the updating step of yk:\nx∗ := arg min x\n1 2 ‖x− b‖2Hi,i + ν|xi − xj |, (26)\nwhereHi,i is a diagonal positive definite matrix, and b and ν are given constants. Let hii and hjj be the i-th and j-th diagonal elements ofHi,i, respectively.\nProposition 5 The optimal solution of (26) takes the following closed-form:\nx∗ =  bl − h −1 ll λ ∗, l = i, bl + h −1 ll λ\n∗, l = j, bl, l 6= i, j,\n(27)\nwhere λ∗ is defined as\nλ∗ =  bi−bj h−1ii +h −1 jj , ∣∣∣ bi−bj h−1ii +h −1 jj ∣∣∣ ≤ ν; sign (bi − bj) ν, ∣∣∣ bi−bj h−1ii +h −1 jj\n∣∣∣ > ν. In the implementation, we use the similar parameter settings of S, ν as above. The dimension parameter pair (n, d) is chosen from the following set\n(n, d) ∈ {\n(2000, 500), (2000, 1000), (5000, 1000), (5000, 2000), (10000, 2000), (10000, 4000)\n} ,\nand the parameter αi = 100/|E|2. Similarly, all the compared algorithms start with the same initial point. The following six pictures in Figures 3 and 4 display the comparisons of the five solvers for six kinds of choices of (n, d). It\nis obvious that the other four solvers GFB, PDM, AP-APG and APA-APG are not as efficient as the proposed GSOS algorithm, which demonstrates that the Gauss-Seidel technique is very useful for addressing nonsmooth optimization. It is worthwhile to point out that the primal solver GFB is faster than the primal-dual solver PDM on graphguided fused Lasso. One possible reason is that the number of nonsmooth terms is too large, which will lead to a large quantity of dual variables introduced in PDM and hence slow down the updating of primal variables."
  }, {
    "heading": "5. Conclusions",
    "text": "In this paper, we proposed a novel first-order algorithm called GSOS for addressing multi-term nonsmooth convex composite optimization. This algorithm inherits the advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated. We found that the GSOS algorithm includes the generalized forward backward splitting method (Raguet et al., 2013) as a special case. In addition, we developed a new technique to establish the global convergence and iteration complexity of the GSOS algorithm. Last, we applied the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared it against several state-of-the-art algorithms. The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness."
  }, {
    "heading": "Acknowledgements",
    "text": "Yuan is supported by NSF-China (61402182)."
  }],
  "year": 2017,
  "references": [{
    "title": "Structured sparsity-inducing norms through submodular functions",
    "authors": ["F.R. Bach"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Structured sparsity through convex optimization",
    "authors": ["F.R. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"],
    "venue": "Statistical Science,",
    "year": 2012
  }, {
    "title": "Convex Analysis and Monotone Operator Theory in Hilbert Space",
    "authors": ["H.H. Bauschke", "P.L. Combettes"],
    "year": 2011
  }, {
    "title": "The proximal average: basic theory",
    "authors": ["H.H. Bauschke", "R. Goebel", "Y. Lucet", "X. Wang"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2008
  }, {
    "title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
    "authors": ["A. Beck", "M. Teboulle"],
    "venue": "SIAM journal on imaging sciences,",
    "year": 2009
  }, {
    "title": "Proximal algorithms for multicomponent image recovery problems",
    "authors": ["L.M. Briceno-Arias", "P.L. Combettes", "J.C. Pesquet", "N. Pustelnik"],
    "venue": "Journal of Mathematical Imaging and Vision,",
    "year": 2011
  }, {
    "title": "ε-enlargements of maximal monotone operators in banach spaces",
    "authors": ["R.S. Burachik", "B.F. Svaiter"],
    "venue": "Set-Valued Analysis,",
    "year": 1999
  }, {
    "title": "Enlargement of monotone operators with applications to variational inequalities",
    "authors": ["R.S. Burachik", "A.N. Iusem", "B.F. Svaiter"],
    "venue": "Set-Valued and Variational Analysis,",
    "year": 1997
  }, {
    "title": "εenlargements of maximal monotone operators: Theory and applications. In Reformulation: nonsmooth, piecewise smooth, semismooth and smoothing methods",
    "authors": ["R.S. Burachik", "C.A. Sagastizábal", "B.F. Svaiter"],
    "year": 1998
  }, {
    "title": "A first-order primal-dual algorithm for convex problems with applications to imaging",
    "authors": ["A. Chambolle", "T. Pock"],
    "venue": "Journal of Mathematical Imaging and Vision,",
    "year": 2011
  }, {
    "title": "Nonnegativity constraints in numerical analysis",
    "authors": ["D. Chen", "R.J. Plemmons"],
    "year": 2015
  }, {
    "title": "Convergence rates in forward–backward splitting",
    "authors": ["G.H.G. Chen", "R.T. Rockafellar"],
    "venue": "SIAM Journal on Optimization,",
    "year": 1997
  }, {
    "title": "An efficient proximal gradient method for general structured sparse learning",
    "authors": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"],
    "year": 2011
  }, {
    "title": "Smoothing proximal gradient method for general structured sparse regression",
    "authors": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2012
  }, {
    "title": "A douglas–rachford splitting approach to nonsmooth convex variational signal recovery",
    "authors": ["P.L. Combettes", "J.C. Pesquet"],
    "venue": "IEEE Journal of Selected Topics in Signal Processing,",
    "year": 2007
  }, {
    "title": "A proximal decomposition method for solving convex variational inverse problems",
    "authors": ["Combettes", "P. L", "J.C. Pesquet"],
    "venue": "Inverse problems,",
    "year": 2008
  }, {
    "title": "Proximal splitting methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering",
    "authors": ["P.L. Combettes", "J.C. Pesquet"],
    "year": 2011
  }, {
    "title": "Primal-dual splitting algorithm for solving inclusions with mixtures of composite, lipschitzian, and parallel-sum type monotone operators. Set-Valued and variational analysis",
    "authors": ["P.L. Combettes", "J.C. Pesquet"],
    "year": 2012
  }, {
    "title": "A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms",
    "authors": ["L. Condat"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 2013
  }, {
    "title": "A three-operator splitting scheme and its optimization applications",
    "authors": ["D. Davis", "W. Yin"],
    "year": 2015
  }, {
    "title": "A proximal iteration for deconvolving poisson noisy images using sparse representations",
    "authors": ["F.X. Dupé", "J.M. Fadili", "J.L. Starck"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2009
  }, {
    "title": "On the douglasłrachford splitting method and the proximal point algorithm for maximal monotone operators",
    "authors": ["J. Eckstein", "D.P. Bertsekas"],
    "venue": "Mathematical Programming,",
    "year": 1992
  }, {
    "title": "Learning with structured sparsity",
    "authors": ["J. Huang", "T. Zhang", "D. Metaxas"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Group lasso with overlap and graph lasso",
    "authors": ["L. Jacob", "G. Obozinski", "J.P. Vert"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2009
  }, {
    "title": "Statistical estimation of correlated genome associations to a quantitative trait network",
    "authors": ["S. Kim", "E.P. Xing"],
    "venue": "PLoS Genet,",
    "year": 2009
  }, {
    "title": "Fast proximity-gradient algorithms for structured convex optimization problems",
    "authors": ["Q. Li", "N. Zhang"],
    "venue": "Applied & Computational Harmonic Analysis,",
    "year": 2016
  }, {
    "title": "Linearized alternating direction method with adaptive penalty for low-rank representation",
    "authors": ["Z. Lin", "R. Liu", "Z. Su"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "On the convergence of a matrix splitting algorithm for the symmetric monotone linear complementarity problem",
    "authors": ["Z.Q. Luo", "P. Tseng"],
    "venue": "SIAM Journal on Control and Optimization,",
    "year": 1991
  }, {
    "title": "Network flow algorithms for structured sparsity",
    "authors": ["J. Mairal", "R. Jenatton", "F.R. Bach", "G.R. Obozinski"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers",
    "authors": ["R.D.C. Monteiro", "B.F. Svaiter"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Excessive gap technique in nonsmooth convex minimization",
    "authors": ["Y. Nesterov"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2005
  }, {
    "title": "Smooth minimization of non-smooth functions",
    "authors": ["Y. Nesterov"],
    "venue": "Mathematical programming,",
    "year": 2005
  }, {
    "title": "Gradient methods for minimizing composite functions",
    "authors": ["Y. Nesterov"],
    "venue": "Mathematical Programming,",
    "year": 2007
  }, {
    "title": "Parallel proximal algorithm for image restoration using hybrid regularization",
    "authors": ["N. Pustelnik", "C. Chaux", "J.C. Pesquet"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2011
  }, {
    "title": "A generalized forwardbackward splitting",
    "authors": ["H. Raguet", "J. Fadili", "G. Peyré"],
    "venue": "SIAM Journal on Imaging Sciences,",
    "year": 2013
  }, {
    "title": "Estimation of simultaneously sparse and low rank matrices",
    "authors": ["E. Richard", "P.A. Savalle", "N. Vayatis"],
    "venue": "arXiv preprint arXiv:1206.6474,",
    "year": 2012
  }, {
    "title": "Adaptive proximal average approximation for composite convex minimization",
    "authors": ["L. Shen", "W. Liu", "J. Huang", "Y.G. Jiang", "S. Ma"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "A family of enlargements of maximal monotone operators",
    "authors": ["B.F. Svaiter"],
    "venue": "Set-Valued Analysis,",
    "year": 2000
  }, {
    "title": "A class of fejér convergent algorithms, approximate resolvents and the hybrid proximalextragradient method",
    "authors": ["B.F. Svaiter"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 2014
  }, {
    "title": "A scalable modular convex solver for regularized risk minimization",
    "authors": ["C.H. Teo", "A. Smola", "S. Vishwanathan", "Q.V. Le"],
    "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,",
    "year": 2007
  }, {
    "title": "Bundle methods for regularized risk minimization",
    "authors": ["C.H. Teo", "S. Vishwanthan", "A.J. Smola", "Q.V. Le"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "A splitting algorithm for dual monotone inclusions involving cocoercive operators",
    "authors": ["B.C. Vũ"],
    "venue": "Advances in Computational Mathematics,",
    "year": 2013
  }, {
    "title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion",
    "authors": ["Y. Xu", "W. Yin"],
    "venue": "Siam Journal on Imaging Sciences,",
    "year": 2013
  }, {
    "title": "Better approximation and faster algorithm using the proximal average",
    "authors": ["Y. Yu"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A matrix splitting method for composite function minimization",
    "authors": ["G. Yuan", "W.S. Zheng", "B. Ghanem"],
    "venue": "arXiv preprint arXiv:1612.02317,",
    "year": 2016
  }, {
    "title": "The composite absolute penalties family for grouped and hierarchical variable selection",
    "authors": ["P. Zhao", "G. Rocha", "B. Yu"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "Accelerated stochastic gradient method for composite regularization",
    "authors": ["W. Zhong", "J.T.Y. Kwok"],
    "venue": "In AISTATS, pp",
    "year": 2014
  }, {
    "title": "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes",
    "authors": ["K. Zhou", "H. Zha", "L. Song"],
    "venue": "In AISTATS,",
    "year": 2013
  }],
  "id": "SP:4061a745797e0dd7bf84b452cab71191623ccde2",
  "authors": [{
    "name": "Li Shen",
    "affiliations": []
  }, {
    "name": "Wei Liu",
    "affiliations": []
  }, {
    "name": "Ganzhao Yuan",
    "affiliations": []
  }, {
    "name": "Shiqian Ma",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity. In addition, we develop a new technique to establish the global convergence of the GSOS algorithm. To be specific, we first reformulate the iterations of GSOS as a twostep iterations algorithm by employing the tool of operator optimization theory. Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation. At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems. Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.",
  "title": "GSOS: Gauss-Seidel Operator Splitting Algorithm for  Multi-Term Nonsmooth Convex Composite Optimization"
}