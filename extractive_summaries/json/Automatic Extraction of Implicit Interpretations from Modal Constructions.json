{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1098–1107, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "People use language to communicate not only facts, but also intentions, uncertain information and points of view. Modality can be broadly defined as a grammatical phenomenon used to express the speaker’s opinion or attitude towards a proposition (Lyons, 1977). Modality has also been defined as “the category of meaning used to talk about possibilities and necessities, essentially, states of affairs beyond the actual.” (Hacquard, 2011). Within computational linguistics, processing modality has proven useful for, among others, recognizing textual entailment (Snow et al., 2006; MacCartney et al., 2006), machine translation (Murata et al., 2005; Baker et al., 2012), and sentiment analysis (Wiebe et al., 2005).\nIn the absence of modality markers, it is understood that the author of a proposition agrees with it (Hengeveld and Mackenzie, 2008). Adding a modality marker—also referred to as cue—casts doubt on the truth of the proposition, e.g., Mary got a new job last week vs. Mary likely got a new job last week. Modality is surprisingly common (Morante\nand Sporleder, 2012), and notoriously difficult to annotate and process automatically (Rubinstein et al., 2013; Vincze et al., 2011). In MEDLINE, 11% of sentences contain speculative language (Light et al., 2004) and in biomedical abstracts, 18% (Vincze et al., 2008). Rubin (2006) reports that 59% of statements in 80 New York Times articles include epistemic modality. Despite modality being ubiquitous, there is not an agreed upon annotation schema.\nIn this paper, we extract implicit interpretations intuitively understood by humans when reading modal constructions. We do not follow any specific theory of modality. Instead, we manipulate modal constructions to automatically generate potential interpretations, and then assign factuality scores to them. Consider statement (1) below:\n1. John likely contracted the disease when a mouse bit him in the Adirondacks.\nEven though likely syntactically attaches to contracted, a natural reading suggests that John contracted the disease is factual; the only bit of uncertain information is how (or when) he contracted the disease. In other words, assuming that the author of statement (1) is truthful, event contracted occurred with AGENT John and THEME the disease, but the MANNER (or TIME) may not have been when a mouse bit him in the Adirondacks.\nA key feature of the work presented in this paper is that the interpretations extracted from modal constructions are not tied to any syntactic or semantic representation. Given modal constructions in plain text, we extract implicit interpretations in plain text, and these interpretations can be processed with any existing NLP pipeline. The main contributions of\n1098\nthis paper are: (1) procedure to automatically generate potential interpretations from modal constructions; (2) annotations assessing the factuality of potential interpretations generated from OntoNotes;1 and (3) experimental results using several features."
  }, {
    "heading": "2 Previous Work",
    "text": "Theoretical works in philosophy and linguistics have studied modality for decades (Palmer, 2001; Jespersen, 1992). Morante and Sporleder (2012) summarize some of these works and related phenomena, e.g., evidentiality, certainty, factuality, subjectivity. There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.) and conditionals (e.g., If the light is on, Sandy is home). Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers such as reporting verbs (e.g., The evidence suggests that he caused the fire), references, or all verbs (Diab et al., 2009). Following these previous works, we focus on modal adverbs.\nBeyond theoretical works, there are many proposals to annotate modality. Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeting modal adverbs. Annotation schemas typically include 3 tasks: identifying modality triggers, their scopes, and sources (Quaresma et al., 2014; Sánchez and Vogel, 2015). Many also classify the modality into several types (epistemic, circumstantial, ability, deontic, etc.) or a fine-grained taxonomy (Rubinstein et al., 2013; Nissim et al., 2013). In this paper, we are not concerned with modeling modality per se, or classifying instances of modality into predefined classes or hierarchies. Instead, we extract implicit interpretations from modal constructions in order to mirror intuitive readings.\nFactBank is probably the best-known corpus for event factuality (Saurı́ and Pustejovsky, 2009). It was created following carefully crafted annotation\n1Available at www.sanders.tech\nguidelines and examples comprising 34 pages.2 The guidelines detail a manual normalization step to “identify the full event that needs to be assessed in terms of its factuality” (p. 12), and the annotation process includes identifying the sources that are assessing factuality (p. 15). de Marneffe et al. (2012) reannotate a subset of FactBank with factuality values from the reader’s perspective—they call it veridicality—using crowdsourcing. Both FactBank and de Marneffe et al. (2012), rely on manual normalization to identify the eventuality whose factuality is being annotated. Instead, we present an automated approach: we manipulate semantic roles and syntactic dependencies deterministically to generate several potential interpretations per modal construction, and then assess their factuality.\nMany other efforts expand on FactBank using crowdsourced annotations, different annotation schemas (usually simpler) or other domains. Prabhakaran et al. (2012) use crowdsourcing to classify propositions into 5 modalities: ability, effort, intention, success and want. Soni et al. (2014) target the factuality of quotes (direct and indirect) in Twitter. Lee et al. (2015) detect events and assess factuality using easy-to-understand short instructions to crowdsource annotations. Unlike us, they annotate factuality at the individual token level, where annotated tokens are deemed events by annotators. Prabhakaran et al. (2015) define and annotate propositional heads with four categories: (1) non-belief propositions, or (2) committed, non-committed or reported belief. Instead of assessing factuality only for propositional heads (usually verbs, one assessment per proposition), we do so for potential interpretations automatically generated by manipulating verbs and their arguments deterministically.\nAll works cited in the previous two paragraphs either manually normalize text prior to assessing factuality—making automation from plain text impossible—or assess factuality for tokens deemed events (ordered, delay, agreed, etc.) or full propositions (a verb and all its arguments). Unlike them, we automatically generate potential interpretations from a single modal construction—or, equivalently, automatically generate several normalizations—and then assess their factuality.\n2https://catalog.ldc.upenn.edu/docs/ LDC2009T23/annotationGuidelines.pdf"
  }, {
    "heading": "3 Terminology and Background",
    "text": "We use the term modal construction to refer to verbargument structures modified by a modal adverb (possibly, probably, etc.). We use the term implicit interpretation, or interpretation to save space, to refer to meaning intuitively understood by humans when reading a modal construction. Potential interpretations are interpretations automatically generated whose factuality has yet to be determined. The factuality of an interpretation is a score indicating its likelihood—whether it is true, false or unknown given the modal construction.\nWe work on top of OntoNotes (Hovy et al., 2006) because it includes text from several genres (news, broadcast and telephone conversations, weblogs, etc.) and includes part-of-speech tags, parse trees, PropBank-style semantic roles and other linguistic information.3 Very briefly, PropBank (Palmer et al., 2005) has two kinds of semantic roles: numbered roles (ARG0, ARG1, etc.), which are defined in verb-specific framesets, and argument modifiers (ARGM-TMP, ARGM-LOC, etc.), we refer the reader to the aforementioned reference, and the guidelines and framesets4 for more details. We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP (Manning et al., 2014)."
  }, {
    "heading": "4 Corpus Creation",
    "text": "We define a two-step procedure to create a corpus of modal constructions and the implicit interpretations intuitively understood by humans when reading them. First, we automatically generate potential interpretations from modal constructions by manipulating syntactic dependencies and semantic roles. Second, we manually score potential interpretations according to their likelihood. These interpretations and scores are later used to learn how to score potential interpretations automatically (Section 6)."
  }, {
    "heading": "4.1 Generating Potential Interpretations",
    "text": "Selecting Modal Constructions. OntoNotes is a large corpus containing 63,918 sentences. Creating a corpus of interpretations for all modal constructions is outside the scope of this paper. In order\n3We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/\n4http://propbank.github.io/\nto alleviate the annotation effort, we focus on selected modal constructions. Specifically, we select verb-argument structures that have one ARGM-ADV or ARGM-MNR role, and that role is one of the following modal adverbs: certainly, clearly, definitely, likely, obviously, possibly, probably, surely, or unlikely. These adverbs are the most frequent that satisfy the above filter. Additionally, we discard verbargument structures with to be as the main verb. These rules retrieve 324 modal constructions. Automatic Normalization. Modal constructions often occur in long multi-clause sentences. In order to identify the eventuality from which potential interpretations should be generated, we automatically normalize the original sentence. Normalizing consists of a battery of deterministic steps implemented using syntactic dependencies and semantic roles. In contrast with previous work (Section 2), our normalization is fully automated. Hereafter, we use verb to refer to the main verb in the modal construction, adverb to the modal adverb, and sem roles to all semantic roles in the modal construction.\n1. Remove adverb. 2. Convert negated verb-argument structures into\ntheir positive counterparts. We follow 3 steps inspired by the rules to form negation proposed by (Huddleston and Pullum, 2002): (a) Remove the negation mark by deleting the\ntoken whose syntactic dependency is neg. (b) Remove auxiliaries, expand contractions,\nand fix third-person singular and past tense. For example (before: after), doesn’t go: goes, didn’t go: went, won’t go: will go. To implement this step, we loop through tokens whose head is the negated verb with dependency aux, and use a list of irregular verbs5 and grammar rules to convert to third-person singular and past tense based on orthographic patterns. (c) Rewrite negatively-oriented polaritysensitive items. For example (before: after), anyone: someone, any longer: still, yet: already. at all: somewhat. We use the correspondences between negatively-oriented and positively-\n5https://en.wikipedia.org/wiki/English_ irregular_verbs\noriented polarity-sensitive items by (Huddleston and Pullum, 2002, pp. 831).\n3. Fix modal verbs and tense. If a modal verb (can, could, may, would, should, must, etc.) has as syntactic head verb, we transform the modal construction into past or future depending on the modal and tense of verb. For example: could go: went, can go: will go, should have gone: went. We use the same grammar rules and list of irregular verbs as in Step (2b). 4. Select relevant tokens. We remove all tokens in the original sentence except verb and tokens belonging to the roles in sem roles. Additionally, we fix phrasal verbs by adding tokens with the part-of-speech tag RP whose syntactic head is verb and dependency type prt (semantic roles in OntoNotes are annotated for verb tokens, missing the preposition when verb is a phrasal verb would inadvertently change meaning). We also add all tokens to the left of verb until we find the first token whose part-of-speech tag does not start with VB, MD, RB or EX (verbs, modals, adverbs and existential there). 5. Generate additional normalizations. If verb is\nfollowed by TO + verb2 (e.g., want to go, like to play, intend to pass), we generate an additional normalization for verb2 after merging the semantic roles of verb and verb2.\nTable 1 exemplifies the automatic normalization step by step with 2 modal constructions. Generating Potential Interpretations in Plain Text. Inspired by the rules Blanco and Sarabi (2016) used to generate interpretations from negation, we generate potential interpretations from modal constructions by toggling off combinations of roles in sem roles. We consider numbered roles (ARG0– ARG5), and argument modifiers (ARGM-) ending in LOC, TMP, MNR, PRP, CAU, EXT, PRD or DIR.\nTable 1 lists some potential interpretations generated from a sample modal construction. The total number of potential interpretations for the 324 selected modal construction is 1,756 (average: 5.4).\nWe recognize that our procedure to generate implicit interpretations is unable to generate some useful interpretations. For example, from This is [a person who]ARG1 [likely]ARGM-ADV [died]verb [on impact versus perhaps freezing to death]ARGM-MNR , we\ngenerate This is a person who died {ARGM-MNR}, which is factual: the only uncertain information is the manner in which the person died. Since we toggle off semantic roles of verb, our procedure is unable to generate A person died on impact and A person died freezing to death; the former interpretation would receive a higher factuality score than the latter. We argue that automation is preferable, and reserve for future work generating interpretations that require splitting semantic roles."
  }, {
    "heading": "4.2 Scoring Potential Interpretations",
    "text": "After automatically generating potential interpretations, we collected manual annotations to determine their factuality. The annotation interface showed the original sentence containing the modal construction, the previous and next sentences as context, and no additional information. Following previous work (Saurı́ and Pustejovsky, 2009; de Marneffe et al., 2012), we found it useful not to restrict answers to yes or no, but to allow for degrees of certainty. Specifically, we asked “Given the 3 sentences above, do you believe that the statement [potential interpretation] below is true?”. Answers are a score ranging from −5 to 5, where −5 indicates Certainly no, 5 indicates Certainly yes, and the scores in between indicate a continuum of certainty (0 indicates unknown).\nAfter pilot annotations, we examined disagreements and defined the following simple guidelines:\n1. Context (previous sentence, target sentence, and next sentence) is taken into account. 2. World knowledge available at the time the original sentence was authored—not new knowledge available after—is taken into account. 3. Semantic roles toggled off are replaced with a semantically related substitute (Turney and Pantel, 2010) for the original role, e.g., give: take, customer: sales associate."
  }, {
    "heading": "5 Corpus Analysis",
    "text": "The total number of modal constructions selected is 324 and the number of potential interpretations automatically generated in 1,756 (average: 5.4 interpretation per modal construction). 39.4% of interpretations are scored with a high degree of certainty. We define high certainty as a score below −3 (interpretation is false) or larger than 3 (interpretation is\ntrue). Importantly, on overage, modal constructions have 2.13 interpretations scored with high certainty, and 1.23 scored 3 or higher. In other words, on average, our procedure generates over 2 interpretation that are either true or false, and over 1 interpretation that is true per modal construction.\nTables 2 and 3 present basic corpus statistics. The percentage of interpretations annotated with a score different than 0 depends greatly on the number of roles toggled off (Table 2): 0: 87.25%, 1: 48.50%, 2: 20.46%, 3: 5.83%. Note that the number of roles toggled off does not significantly affect the mean score of interpretations not scored 0 (Table 2, last 2 columns). Most interpretations have either ARG0 or ARG1 toggled off (Table 3), and the percentages of interpretations not scored zero range from 20% to 32.84% depending on the semantic role. Note that the average score of interpretations scored positively and negatively, however, does not depend on whether a semantic role is toggled off."
  }, {
    "heading": "5.1 Annotation Quality",
    "text": "The annotation guidelines (Section 4.2) to score potential interpretations were defined after examining disagreements in pilot annotations. After defining the guidelines, inter-annotator agreement was 0.92 on 18% of randomly selected interpretations.6 Agreement measures designed for categorical labels are unsuitable, as not all disagreements are equal, e.g., 4 vs. 5, -2 vs. 5. Because of the high agreement and following previous work (Agirre et al., 2012), the rest of interpretations were annotated once."
  }, {
    "heading": "5.2 Annotation Examples",
    "text": "Table 4 presents annotation examples. For each example, we include the original sentence containing a selected modal construction, its context (previous and next sentence) if helpful for scoring, and 2 automatically generated potential interpretations with their annotated scores.\nExample (1) shows that context helps in determining the factuality of potential interpretations (item (1) in the guidelines). After reading the three sen-\n6We set an internal deadline of 3 days after agreeing on the guidelines, and we could annotate 18% of instances in that time.\ntences, it is clear that they are making wild statements, and are hoping to get attention for it. Interpretation 1.1 removes adverb certainly and receives the highest score, 5. Interpretation 1.2 is obtained after toggling off ARG1, and receives the lowest score, −5. This low score is justified by item (3) in our annotation guidelines: replacing wild statements with a semantically (different but) related substitute, e.g., But they chose reasonable statements / good manners to get our attention and that of the international community, yields an unlikely interpretation.\nThe interpretations in Example (2) show again the importance of context, and also exemplify item (2) in the annotation guidelines. Interpretation 2.1, We will find them one day receives a high score (4/5), as given the context (and assuming that Rumsfeld is truthful), it is very likely that they will find the weapons of mass destruction, but it is not guaranteed. Note that annotators are not allowed to use the fact that the weapons were never found (item (2) in the guidelines). In Interpretation 2.2, one day could be replaced with never / at no time or similar constructions, and doing so yields the opposite of the intended meaning (score: −3). A possible descrip-\ntion of these scores could be “almost certainly true” (4 out of 5), and “most probably false” (-3 out of -5). We see scores as a continuum of certainty, but textual description may help understand the examples.\nExample (3) demonstrates the usefulness of the normalization process—specifically, Step 4, selecting relevant tokens—and the importance of replacing roles with semantically related substitutes (item (3) in the guidelines). In interpretation 3.1, {ARG0} will act in the interests of the minority holders, ARG0 can be replaced with a company with several minority holders, yielding a valid interpretation scored 4 (out of 5). Similarly, in interpretation 3.2, A company with a big majority holder will act {ARG1}, ARG1 can be replaced with in the interests of the big majority holder, yielding another valid interpretation also scored 4 (out of 5).\nFinally, Example (4) shows Step 5 in the automatic normalization procedure (Section 4). By creating an additional verb-argument structure, we are able to differentiate between liking to do something (Interpretation 4.1, score 5/5) and actually doing that something (Interpretation 4.2, score 1/5)."
  }, {
    "heading": "6 Learning to Score Potential Interpretations",
    "text": "In order to automatically score potential interpretations, we follow a standard supervised machine learning approach. Each potential interpretation becomes an instance, and we split modal constructions (and their potential interpretations) into training (80%) and test (20%). When splitting, we make sure that the amount of modal constructions for each adverb in each split is proportional, i.e., 80% of modal constructions with each adverb are in the train split and the rest in the test split. Splitting instances randomly would assign interpretations generated from the same modal construction to the train and test splits, and bias the results.\nWe trained a Support Vector Machine (SVM) for regression with RBF kernel using scikit-learn (Pedregosa et al., 2011), which uses LIBSVM (Chang and Lin, 2011). The SVM parameters (C and γ) were tuned using 10-fold cross-validation with the training set, and we report results using the test split."
  }, {
    "heading": "6.1 Feature Selection",
    "text": "The full set of features is detailed in Table 5. Baseline features are simple features characterizing adverb and verb and we do not elaborate on them. Adverb and verb features are extracted from the modal construction (constituent tree and semantic roles) and provide additional information about the modal construction. Interpretation features characterize the potential interpretation whose factuality is being scored, and are also derived from the constituent tree and semantic roles.\nMost adverb and verb features are standard in semantic role labeling (Gildea and Jurafsky, 2002). We include the part-of-speech tags of the parent, and left and right siblings of adverb and verb, as well as their subcategorization, i.e., the concatenation of the sibling’s part-of-speech tags. We also include syntactic path between adverb and verb, and its length. Additionally, we include the common ancestor, i.e., the syntactic node of the lowest common node that is an ancestor of both adverb and verb, and use binary features to indicate whether each semantic role is present in the modal construction.\nFinally, interpretation features characterize the semantic roles toggled off to generate the potential interpretation. We include the number of roles toggled off to generate the potential interpretation, and binary flags indicating which roles. Additionally, for each role toggled off, we include the distance from the verb (number of tokens), whether it occurs before or after the verb, the syntactic path to the verb and the length of the path."
  }, {
    "heading": "7 Experimental Results",
    "text": "Table 6 details results obtained with test instances using several feature combinations derived from\ngold linguistic information (POS tags, parse trees, semantic roles, etc.). Baseline and adverb and verb features, which characterize the modal construction from which potential interpretation are extracted, are virtually useless. They yield Pearson correlations of −0.029 and 0.025 individually, and −0.013 combined. These results suggest that the verb and adverb in the modal construction (word forms, syntactic paths, etc.) are insufficient to rank potential interpretations generated from the modal construction.\nInterpretation features, which capture differences between potential interpretations being scored (number of roles toggled off, roles toggled off, etc.), obtain a modest Pearson correlation of 0.494. Combining interpretation features with other features proved detrimental, Pearson correlations are between 0.463 and 0.468."
  }, {
    "heading": "8 Conclusions",
    "text": "Modality is a pervasive phenomenon used to talk about what is not factual. In this paper, we have presented a methodology to extract implicit interpretations from modal constructions. First, we automatically generate potential interpretations using syntactic dependencies and semantic roles, and then assign to them a factuality score.\nThe most important conclusion of the work presented here is that several interpretations automatically generated from a single modal construction often receive scores indicating high certainty. Indeed, on average, modal constructions have 2.13 interpretations scored lower or equal than −3, or higher or equal than 3. This contrast with previous work, which only assess factuality of one normalization per proposition.\nExperimental results using supervised machine learning and relatively simple features show that the task is challenging but can be automated. We believe better results could be obtained by incorporating features capturing knowledge in the context of the modal construction, including other clauses in the same sentence, and the previous and next sentences. Another extension of the current work is to investigate a similar approach for other modality markers such as nouns (e.g., possibility, chance), adjectives (e.g.necessary, probable, ) and certain verbs (e.g., claim, suggests)."
  }],
  "year": 2016,
  "references": [{
    "title": "Semeval-2012 task 6: A pilot on semantic textual similarity",
    "authors": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."],
    "venue": "Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montréal, Canada,",
    "year": 2012
  }, {
    "title": "Use of Modality and Negation in Semantically-Informed Syntactic MT",
    "authors": ["Kathryn Baker", "Michael Bloodgood", "Bonnie J. Dorr", "Chris Callison-Burch", "Nathaniel W. Filardo", "Christine Piatko", "Lori Levin", "Scott Miller."],
    "venue": "Comput. Linguist., 38(2):411–438, June.",
    "year": 2012
  }, {
    "title": "Automatic generation and scoring of positive interpretations from negated statements",
    "authors": ["Eduardo Blanco", "Zahra Sarabi."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
    "year": 2016
  }, {
    "title": "An analysis of disagreement-provoking factors in the analysis of epistemic modality and evidentiality: the case of english adverbials",
    "authors": ["Marta Carretero", "Juan Rafael Zamorano-Mansilla."],
    "venue": "Proceedings of IWCS 2013 Workshop on Annotation of Modal Meanings in",
    "year": 2013
  }, {
    "title": "Libsvm: A library for support vector machines",
    "authors": ["Chih-Chung Chang", "Chih-Jen Lin."],
    "venue": "ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27, May.",
    "year": 2011
  }, {
    "title": "Did it happen? the pragmatic complexity of veridicality assessment",
    "authors": ["Marie-Catherine de Marneffe", "Christopher D. Manning", "Christopher Potts."],
    "venue": "Comput. Linguist., 38(2):301–333, June.",
    "year": 2012
  }, {
    "title": "Epistemic modality and knowledge attribution in scientific discourse: A taxonomy of types and overview of features",
    "authors": ["Anita de Waard", "Henk Pander Maat."],
    "venue": "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, ACL ’12, pages 47–",
    "year": 2012
  }, {
    "title": "Committed belief annotation and tagging",
    "authors": ["Mona Diab", "Lori Levin", "Teruko Mitamura", "Owen Rambow", "Vinodkumar Prabhakaran", "Weiwei Guo."],
    "venue": "Proceedings of the Third Linguistic Annotation Workshop, pages 68–73, Suntec, Singapore, August.",
    "year": 2009
  }, {
    "title": "Modality and language",
    "authors": ["Kai Von Fintel."],
    "venue": "D. Borchert, editor, Encyclopedia of Philosophy, pages 20–27. Macmillan Reference.",
    "year": 2006
  }, {
    "title": "Automatic labeling of semantic roles",
    "authors": ["Daniel Gildea", "Daniel Jurafsky."],
    "venue": "Comput. Linguist., 28(3):245–288, September.",
    "year": 2002
  }, {
    "title": "Modality",
    "authors": ["Valentine Hacquard."],
    "venue": "C. Maienborn, K. von Heusinger, and P. Portner, editors, Semantics: An International Handbook of Natural Language Meaning, pages 1484–1515. Mouton de Gruyter.",
    "year": 2011
  }, {
    "title": "Functional Discourse Grammar: A Typologically-Based Theory of Language Structure",
    "authors": ["Kees Hengeveld", "J. Lachlan Mackenzie."],
    "venue": "Oxford University Press.",
    "year": 2008
  }, {
    "title": "OntoNotes: the 90% Solution",
    "authors": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."],
    "venue": "NAACL ’06: Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers on XX,",
    "year": 2006
  }, {
    "title": "The Cambridge Grammar of the English Language",
    "authors": ["Rodney D. Huddleston", "Geoffrey K. Pullum."],
    "venue": "Cambridge University Press, April.",
    "year": 2002
  }, {
    "title": "The philosophy of grammar",
    "authors": ["Otto Jespersen."],
    "venue": "University of Chicago Press, Chicago.",
    "year": 1992
  }, {
    "title": "Event detection and factuality assessment with non-expert supervision",
    "authors": ["Kenton Lee", "Yoav Artzi", "Yejin Choi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–1648, Lisbon,",
    "year": 2015
  }, {
    "title": "The language of bioscience: Facts, speculations, and statements in between",
    "authors": ["Marc Light", "Xin Ying Qiu", "Padmini Srinivasan."],
    "venue": "Lynette Hirschman and James Pustejovsky, editors, HLTNAACL 2004 Workshop: BioLINK 2004, Linking Bi-",
    "year": 2004
  }, {
    "title": "Semantics",
    "authors": ["John Lyons."],
    "venue": "Cambridge University Press. Cambridge Books Online.",
    "year": 1977
  }, {
    "title": "Learning to recognize features of valid textual entailments",
    "authors": ["Bill MacCartney", "Trond Grenager", "Marie-Catherine de Marneffe", "Daniel Cer", "Christopher D. Manning."],
    "venue": "Proceedings of the Main Conference on Human Language Technology Conference",
    "year": 2006
  }, {
    "title": "The stanford corenlp natural language processing toolkit",
    "authors": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J Bethard", "David McClosky."],
    "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System",
    "year": 2014
  }, {
    "title": "Modality and negation: An introduction to the special issue",
    "authors": ["Roser Morante", "Caroline Sporleder."],
    "venue": "Comput. Linguist., 38(2):223–260, June.",
    "year": 2012
  }, {
    "title": "Correction of errors in a verb modality corpus for machine translation with a machine-learning method",
    "authors": ["Masaki Murata", "Masao Utiyama", "Kiyotaka Uchimoto", "Hitoshi Isahara", "Qing Ma."],
    "venue": "4(1):18–37, March.",
    "year": 2005
  }, {
    "title": "Cross-linguistic annotation",
    "authors": ["Malvina Nissim", "Paola Pietrandrea", "Andrea Sanso", "Caterina Mauri"],
    "year": 2013
  }, {
    "title": "The Proposition Bank: An Annotated Corpus of Semantic Roles",
    "authors": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."],
    "venue": "Computational Linguistics, 31(1):71–106.",
    "year": 2005
  }, {
    "title": "Mood and Modality",
    "authors": ["F.R. Palmer."],
    "venue": "Cambridge University Press, second edition. Cambridge Books Online.",
    "year": 2001
  }, {
    "title": "Scikit-learn: Machine learning in Python",
    "authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"],
    "year": 2011
  }, {
    "title": "Statistical modality tagging from rule-based annotations and crowdsourcing",
    "authors": ["Vinodkumar Prabhakaran", "Michael Bloodgood", "Mona Diab", "Bonnie Dorr", "Lori Levin", "Christine D. Piatko", "Owen Rambow", "Benjamin Van Durme."],
    "venue": "Proceedings of the Workshop",
    "year": 2012
  }, {
    "title": "A new dataset and evaluation for belief/factuality",
    "authors": ["Werner", "Yorick Wilks", "Janyce Wiebe."],
    "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 82–91, Denver, Colorado, June.",
    "year": 2015
  }, {
    "title": "Conll-2011 shared task: Modeling unrestricted coreference in ontonotes",
    "authors": ["Sameer Pradhan", "Lance Ramshaw", "Mitchell Marcus", "Martha Palmer", "Ralph Weischedel", "Nianwen Xue."],
    "venue": "Proceedings of the Fifteenth Conference on Computational Natural Lan-",
    "year": 2011
  }, {
    "title": "Automatic tagging of modality: identifying triggers and modal value",
    "authors": ["P. Quaresma", "A. Mendes", "I. Hendrickx", "T. Gon?alves"],
    "venue": "In Proceedings of the 10th Joint ACL SIGSEM - ISO Workshop on Interoperable Semantic Annotation",
    "year": 2014
  }, {
    "title": "Identifying certainty in texts",
    "authors": ["Victoria L. Rubin."],
    "venue": "Ph.D. thesis, Syracuse University, Syracuse, NY.",
    "year": 2006
  }, {
    "title": "Toward fine-grained annotation of modality in text",
    "authors": ["Aynat Rubinstein", "Hillary Harner", "Elizabeth Krawczyk", "Daniel Simonson", "Graham Katz", "Paul Portner."],
    "venue": "Proceedings of IWCS 2013 Workshop on",
    "year": 2013
  }, {
    "title": "A hedging annotation scheme focused on epistemic phrases for informal language",
    "authors": ["Liliana Mamani Sánchez", "Carl Vogel."],
    "venue": "Proceedings of the Workshop on Models for Modality Annotation.",
    "year": 2015
  }, {
    "title": "Factbank: a corpus annotated with event factuality",
    "authors": ["Roser Saurı", "James Pustejovsky"],
    "venue": "Language Resources and Evaluation,",
    "year": 2009
  }, {
    "title": "Effectively using syntax for recognizing false entailment",
    "authors": ["Rion Snow", "Lucy Vanderwende", "Arul Menezes."],
    "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Compu-",
    "year": 2006
  }, {
    "title": "Modeling factuality judgments in social media text",
    "authors": ["Sandeep Soni", "Tanushree Mitra", "Eric Gilbert", "Jacob Eisenstein."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 415–420, Balti-",
    "year": 2014
  }, {
    "title": "From frequency to meaning: Vector space models of semantics",
    "authors": ["Peter D. Turney", "Patrick Pantel."],
    "venue": "J. Artif. Int. Res., 37(1):141–188, January.",
    "year": 2010
  }, {
    "title": "The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes",
    "authors": ["Veronika Vincze", "György Szarvas", "Richard Farkas", "György Móra", "János Csirik."],
    "venue": "BMC Bioinformatics, 9((Suppl 11)):S9.",
    "year": 2008
  }, {
    "title": "Linguistic scope-based and biological event-based speculation and negation annotations in the bioscope and genia event corpora",
    "authors": ["Veronika Vincze", "György Szarvas", "György Móra", "Tomoko Ohta", "Richárd Farkas."],
    "venue": "Journal of Biomedical Semantics,",
    "year": 2011
  }, {
    "title": "Annotating expressions of opinions and emotions in language",
    "authors": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."],
    "venue": "Language Resources and Evaluation, 39(2):165–210.",
    "year": 2005
  }],
  "id": "SP:813779418a613d1faecd7b1deb9b4456121a9b7e",
  "authors": [{
    "name": "Jordan Sanders",
    "affiliations": []
  }, {
    "name": "Eduardo Blanco",
    "affiliations": []
  }],
  "abstractText": "This paper presents an approach to extract implicit interpretations from modal constructions. Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations. An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation. Experimental results show that the task is challenging but can be automated.",
  "title": "Automatic Extraction of Implicit Interpretations from Modal Constructions"
}