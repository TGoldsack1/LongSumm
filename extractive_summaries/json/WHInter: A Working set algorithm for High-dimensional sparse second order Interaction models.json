{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In application domains where the number of features exceeds the number of available samples, sparsity-inducing regularisers have a long history of success. Genomic prediction of complex phenotypes, biomedical imaging, astronomy or finance are a few examples. In particular the least squares with `1 regularisation, known as the LASSO (Tibshirani, 1996), has been extensively studied. It enjoys desirable statistical properties, since the number of samples required for exact support recovery of a sparse model scales as the logarithm of the number of features, under some\n1MINES ParisTech, PSL Research University, CBIO-Centre for Computational Biology, 75006 Paris, France 2Institut Curie, PSL Research University, 75005 Paris, France 3INSERM, U900, 75005 Paris, France 4Ecole Normale Supérieure, Department of Mathematics and Applications, 75005 Paris, France. Correspondence to: Jean-Philippe Vert <jean-philippe.vert@mines-paristech.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nassumptions (Wainwright, 2009). It also enjoys practical advantages, notably the interpretability of the learned models and the availability of fast solvers.\nIndeed, a lot of research effort has been devoted to accelerating solvers for sparsity constrained problems in high dimension. A central idea is to exploit the sparsity of the solution to develop algorithms that do not spend too much time on optimising coefficients that will end up being 0. For example, safe screening rules identify features which are guaranteed to be inactive at the optimum so that their corresponding coefficients can be safely zeroed and set aside from the pool of coefficients to update (El Ghaoui et al., 2012; Xiang et al., 2011; Xiang & Ramadge, 2012; Fercoq et al., 2015; Wang et al., 2013; Raj et al., 2016). Dynamic screening rules (Bonnefoy et al., 2015) such as the GAP safe rules (Fercoq et al., 2015) are particularly useful since more and more coefficients can be safely zeroed while the solver approaches the optimal solution. In spite of this, safe rules tend to be conservative, thereby limiting the potential speed-up. To remedy this drawback, new working set heuristics have been proposed. Working set algorithms enjoy great success in practice, as exemplified by the popular GLMNET package (Friedman et al., 2010). They iteratively solve subproblems, either problems restricted to a subset of features in the primal or to a subset of constraints in the dual, until convergence. Working set methods allow to focus coefficient updates on a set of features which can be significantly smaller than that yielded by safe rules. However this comes at a cost, that of checking the optimality conditions for all features at each iteration. BLITZ (Johnson & Guestrin, 2015) is a recently proposed working set algorithm that has been shown to have state-of-the-art performance for `1 regularised problems. Interestingly, the choice of the working sets in BLITZ can be seen as an aggressive use of the GAP safe rules (as noted in Massias et al., 2017) where the size of the working set is chosen to maximise the progress towards convergence. BLITZ can therefore be combined with the GAP safe rules (or the FLEX constraint elimination according to Johnson et al. terminology) at no cost. A direct comparison between BLITZ and the GAP safe rules by Ndiaye et al. (2017) illustrates the effectiveness of the working set approach. Further developments have also focused on coordinate descent (CD) to avoid wasteful\ncoordinate updates, which represent most of the time spent by the solver (Fujiwara et al., 2016; Johnson & Guestrin, 2017).\nThe problem of fitting sparse linear models with two-way interactions has also attracted attention during the past decade. By two-way interactions we mean the entry-wise multiplication between two features; this is for example important in genomics to detect possible epistasis between genes. Surprisingly, very few of these works have links with the aforementioned literature. A majority of them focus on the design of sparsity-inducing penalties which enforce heredity assumptions and apply to moderate-dimensional settings (p < 1, 000) (Radchenko & James, 2010; Bien et al., 2013; Lim & Hastie, 2015; Haris et al., 2016). Heredity assumptions state that an interaction can be included in the model only if one or both of its corresponding main effects are included. We note however that glinternet (Lim & Hastie, 2015) was applied to higher dimensional problems and in particular to a dataset with roughly p = 27, 000 main effects, although the size of the learned model is not specified and the running time for the experiment is not reported by the authors. Interestingly, glinternet uses an active set strategy. Comparatively few works have been devoted to learning sparse regression models with interactions when the number of interactions is higher. Most of them are heuristics which start by selecting main effects and then incorporate interactions generated under the heredity constraint in a possibly iterative fashion. The simplest form of such heuristics consists in fitting a sparse linear model with the main effects only, and then fitting a second sparse linear model on all previously selected main effects and their interactions. This has been used in practice for example by Wu et al. (2009). Iterative refinements have been proposed where the LASSO is fit several times, and each time the set of candidate interactions considered is updated either by subsets, with the interactions between the K most relevant main effects selected at the previous fit (Bickel et al., 2010), or in a greedy fashion, where new interactions are included in the model as soon as a new main effect enters the LASSO path (Shah, 2016). In a similar vein, Hao & Zhang (2014) is based on a greedy model selection procedure instead of several LASSO fits. While these heuristics can deal with higher-dimensional problems than previous methods and enjoy some desirable statistical properties, they do not provide exact solutions and do not enjoy statistical properties as strong as those of the LASSO estimator.\nAn interesting link between the literature on interactions and that of solver acceleration with sparsity inducing norms has been made recently by Nakagawa et al. (2016). In the case where variables are binary or with values in [0, 1], they propose an approach called Safe Pattern Pruning (SPP) which is able to provide the optimal solution of the LASSO with two-way interactions (or possibly higher-order interactions)\nfor fairly high-dimensional problems, with no heredity constraint. Typically, for a problem with 1,000 samples and 10,000 main effects where two-way interactions are considered, SPP can provide solutions for a grid of regularisation parameters within one or two hours on a laptop with one core. SPP relies on the recently developed GAP safe screening rules. More precisely, the authors propose a safe pattern pruning criterion that can safely discard subsets of interactions from the model to speed up convergence. The performance of SPP is however hindered by several factors. One of them is that safe screening rules can be quite conservative even in the sequential setting. This property is inherited and amplified by the SPP criterion which can lead to heavy computations.\nInspired by SPP and the acceleration of solvers for sparsity constrained problems we propose a scalable algorithm, WHInter, to compute the optimal solution of `1-regularised linear problems with two-way interactions. WHInter is a working set method that efficiently delineates working sets among all interactions and main effects thanks to two contributions. First, we introduce a cheap and effective bound to rule out subsets of interactions that are guaranteed to be outside of the working set. Second, the identification of the working set among the remaining features is cast as a variant of the Maximum Inner Product Search (MIPS) problem to alleviate the corresponding computational load. We find that WHInter is up to two orders of magnitude faster than SPP. For example, a problem with roughly 700 samples and 100,000 main effects can be solved for a grid of regularisation parameters in half an hour on a laptop with one core compared to more than 30 hours with SPP. This improvement in the scalability opens up new horizons in several application fields. The rest of the paper is organised as follows. In Section 2, we present useful knowledge and notations used throughout the paper. In Section 3 we describe in details our algorithm and our main contributions. In Section 4, we evaluate WHInter on simulated datasets and finally in Section 5, we report results on a toxicogenomics prediction task."
  }, {
    "heading": "2. Preliminaries",
    "text": ""
  }, {
    "heading": "2.1. Setting and notations",
    "text": "For any integer d ∈ N, we note JdK = {1, . . . , d} and 1d ∈ Rd the d-dimensional vector of 1’s. For any vector u = (u1, . . . ,ud) ∈ Rd, we note ‖u ‖1 = ∑d i=1 |ui |,\n‖u ‖2 = (∑d i=1 u 2 i )1/2 , supp(u) = {i ∈ JdK : ui 6= 0} and ‖u ‖0 = | supp(u) |. For any two vectors u,v ∈ Rd, u v is the vector of entry-wise products, i.e., (u v)i := uivi for i = 1, . . . , d. For any matrix M, we denote by Mi,j its (i, j)-th entry, Mj its j-th column and by mi its i-th row. For any u ∈ Rd and I ⊂ JdK, uI = (ui)i∈I ,\nand similarly, if M is a matrix with d columns, MI is the sub-matrix with | I | columns MI = (Mi)i∈I .\nThroughout the text we consider a design matrix X ∈ {0, 1}n×p corresponding to n samples and p binary features, together with a response vector y ∈ Rn. We define an expanded design matrix Z ∈ {0, 1}n×D, with D = p(p + 1)/2, which contains all p features from X plus the p(p − 1)/2 interaction features. For clarity purposes, we define a symmetric indexing function τ : JpK2 7→ JDK that uniquely assigns to every main effect and interaction an index in the expanded matrix Z such that Zτ(j,k) = Zτ(k,j) := Xj Xk. In particular Zτ(i,i) = Xi Xi = Xi represents the ith main effect. Since X is a binary matrix, the interaction feature Xj Xk corresponds to a logical AND between features Xi and Xj . We organise the main effects and interactions in a simple tree as depicted in Figure 1 so as to reflect the property that ∀(j, k) ∈ JpK2 ,Zτ(j,k) ≤ Xj and Zτ(j,k) ≤ Xk. In the sequel, the set composed of a main effect and its interactions with all other main effects will be referred to as a branch and for any j ∈ JpK, we note branch(j) = {τ(j, k) : k ∈ JpK}.\nWe consider the convex optimisation problem:\nmin (w,b)∈RD×R PZ,λ(w, b) , (1)\nwith\nPZ,λ(w, b) = F (Zw + b1n) + λ ‖w‖1\n= n∑ i=1 fi (ziw + b) + λ ‖w‖1 ,\nwhere λ > 0 is a regularisation parameter and, for any i ∈ JnK, fi : R 7→ [−∞,+∞] is a loss function parametrised by yi and assumed to be convex and differentiable. Table 1 provides examples of classical loss functions in classification and regression. A dual formulation of (1) reads:\nmax θ∈∆Z,λ D(θ) := − n∑ i=1 f∗i (−θi) , (2)\nwhere ∆Z,λ = { θ ∈ Rn : ∣∣Z>θ∣∣ ≤ λ1D ,1>n θ = 0} , (3) and where f∗i is the Fenchel-Legendre transform of the loss fi, i.e., the function f∗i : R 7→ [−∞,+∞] defined by f∗i (u) = supv∈R uv − fi(v). For the derivation of the dual problem, we refer the reader to Johnson & Guestrin (2015, Appendix E). The constraint 1>n θ = 0 comes from the bias term b1n in the primal problem (1). We denote by (w∗, b∗) and θ∗ a set of primal and dual optimal solutions to\nproblems (1) and (2) respectively. Strong duality holds and therefore (w∗, b∗) and θ∗ satisfy Fermat’s rules (Ndiaye et al., 2017):\nθ∗ = −∇F (Zw∗ + b∗1n) , (4)\nand ∀i ∈ JDK , Z>i θ∗ ∈ { {−λ, λ} if w∗i 6= 0 , [−λ, λ] if w∗i = 0 .\n(5)"
  }, {
    "heading": "2.2. Basic working set algorithm",
    "text": "A general strategy to solve (1) is to follow a working set approach, as summarised in Algorithm 1. At each iteration, it solves (1) restricted to a small subset of featuresW called the working set. W is typically chosen as the set of features that violate the optimality condition (5) at the current iteration. In the sequel, we will call such features violating features. The algorithm converges when no violating feature remains, which occurs in a finite number of iterations as shown in Kowalski et al. (2011). When the number of interaction features runs into the billions, Algorithm 1 is not tractable since the delineation of the working set (line 3 in Alg. 1) requires O(p2n) operations at each iteration.\nAlgorithm 1 Working set algorithm\nInput: Z ∈ {0, 1}n×D,y ∈ Rn, λ > 0 Output: w∗, b∗\n1: Set θ ← −∇F (0n),W = ∅. 2: while true do 3: W ′ = { i ∈ JDK : ∣∣Z>i θ∣∣ ≥ λ} 4: if maxi∈W′\n∣∣Z>i θ ∣∣ ≤ λ then Break elseW ←W ′ 5: w∗W , b\n∗ ← argmin wW ,b PZW ,λ(wW , b)\n6: θ ← −∇F (ZWw∗W + b∗1n). 7: end while"
  }, {
    "heading": "3. The WHInter algorithm",
    "text": ""
  }, {
    "heading": "3.1. Overview",
    "text": "WHInter is a working set algorithm that follows the general scheme of Algorithm 1 but implements an efficient strategy to delineate the working set among all main effects and interactions. It is described in Algorithm 2. The identification of the working set (line 3 in Algorithm 1) corresponds to lines 11-18 in Algorithm 2. Instead of scanning through all features to build the working set, WHInter first identifies branches that are guaranteed to contain no violating feature. These branches are identified via the evaluation of a branch bound η(Xj ,Θ ref j ,θ,m ref j ) (line 13), which is presented in Section 3.2 together with the parameters it takes as input. The branch bound is cheap to evaluate since it solely depends on main effects and not on their numerous\nTable 1 – Summary of useful functions for the LASSO and logistic regression: loss function fi, its derivative f ′i , its FenchelLegendre transform f∗i .\nfi(u) f ′ i(u) f ∗ i (u)\nLASSO 12 (yi − u) 2\nu− yi 12 (yi + u) 2 − 12y 2 i\nLogistic regr. log(1 + exp(−yiu)) − uyi log(− u yi ) + (1 + uyi ) log(1 + u yi ) −yi1+exp(yiu)\ninteractions. Moreover, it is designed to efficiently rule out branches thanks to the exploitation of the shared structure among features in a branch, as well as the correlation among dual variables for two sufficiently close points in the optimisation path. In cases where a branch cannot be ruled out, features in the branch are considered one by one to build the working set, which is very computationally expensive. In order to reduce this cost, we cast the problem as a variant of the Maximum Inner Product Search (MIPS) problem, which is described in Section 3.3. If no violating feature is identified then the algorithm has converged. Otherwise, a new candidate solution is obtained by solving problem (1) restricted to the features in the working set (line 20), and the process is repeated until no violating feature remains. While any solver can be used to solve the restricted problem, we implemented in WHInter a coordinate descent approach with safe pruning."
  }, {
    "heading": "3.2. The Branch bound η",
    "text": "As WHInter iterates, it produces candidate solutions (w∗, b∗) and corresponding dual variables θ (lines 20 and 21 of Algorithm 2). For two sufficiently close iterations, or for two problems with sufficiently close regularisation parameters, the candidate solutions are likely to be close to one another, as well as the corresponding dual variables provided that the function F does not vary too quickly. WHInter exploits this intuition to speed up the identification of the working set from an iteration to another or from one problem to another. The following results relate the criteria used to identify the working set for two distinct dual variables (line 3 of Algorithm 1). Lemma 3.1. For any X ∈ {0, 1}n×p, v ∈ Rn+, θ1,θ2 ∈ Rn, j ∈ JpK, I ⊂ JpK and α ∈ R, the following holds:\nmax k∈I ∣∣θ>2 (v Xk) ∣∣ ≤ |α |max\nk∈I\n∣∣θ>1 (v Xk) ∣∣+ ζ(θ2 − αθ1,v) , (6)\nwhere ∀(u,v) ∈ Rn × Rn+ ,\nζ(u,v) = max ( ∑ i:ui>0 uivi,− ∑ i:ui<0 uivi ) .\nThe proof of Lemma 3.1 is provided in Appendix A. It is based on the decomposition θ2 = αθ1 + (θ2 − αθ1), and exploits the tree structure among features in a branch. To exploit Lemma 3.1 in WHInter, we define for α ∈ R and for all (v,θ1,θ2,m) ∈ Rn+ × Rn × Rn × R the function:\nηα (v,θ1,θ2,m) = |α |m+ ζ (θ2 − αθ1,v) , (7)\nand we maintain an active setW ⊂ JDK, a matrix Θref ∈ Rn×p that contains reference dual variables Θrefj ∈ Rn for each branch j ∈ JpK, and the vector mref ∈ Rp defined by:\n∀j ∈ JpK , mrefj = max k∈JpK:τ(j,k)/∈W ∣∣∣Z>τ(j,k)Θrefj ∣∣∣ . (8) We now state our main theorem which allows to identify branches that are guaranteed to not contain any violating feature (line 13 of Algorithm 2):\nTheorem 3.1 (Branch pruning). For any Θref ∈ Rn×p, W ⊂ JpK, j ∈ JpK, let mrefj ∈ R+ be given by (8). Then for any θ ∈ Rn, α ∈ R and λ > 0, if\nηα ( Xj ,Θ ref j ,θ,m ref j ) < λ , (9)\nthen any feature from branch j that belongs to the working set { i ∈ JDK : ∣∣Z>i θ∣∣ ≥ λ} is already inW . This holds in particular if\nηmin := min α∈R ηα\n( Xj ,Θ ref j ,θ,m ref j ) < λ . (10)\nAlgorithm 2 WHInter\nInput: X ∈ {0, 1}n×p, y ∈ Rn, λ1 > · · · > λT . Output: (W,w∗W , b∗)t for each λt\n# Initialisation 1: θ ← −∇F (0n) 2: for j in JpK do 3: Θrefj ← θ 4: end for 5: W,mref ← update W(X,θ, JpK , λ1, ∅) 6: for t = 1 to T do # Pre-Solve 7: w∗W , b\n∗ ← argmin wW ,b PZW ,λt(wW , b)\n8: θ ← −∇F (ZWw∗W + b∗1n). 9: W,mref ← clean W(W, λt,θ,Θref ,mref )\n10: while true do # Branch pruning 11: V ← ∅ 12: for j in JpK do 13: if η(Xj ,Θrefj ,θ,m ref j ) ≥ λt then 14: V ← V ∪ {j} 15: Θrefj ← θ 16: end if 17: end for # Identify the working set 18: W ′,mrefV ← update W(X,θ,V, λt,W) 19: if maxi∈W′\n∣∣Z>i θ ∣∣ ≤ λ then 20: Break 21: else 22: W ←W ′ 23: end if # Solve subproblem 24: w∗W , b\n∗ ← argmin wW ,b PZW ,λt(wW , b)"
  }, {
    "heading": "25: θ ← −∇F (ZWw∗W + b∗1n).",
    "text": "26: W,mref ← clean W(W, λt,θ,Θref ,mref ) 27: end while 28: (W,w∗W , b∗)k ← (W,w∗W , b∗) 29: end for\n30: function clean W(W, λ,θ,Θref ,mref ) 31: for i inW do 32: if ∣∣Z>i θ∣∣ < λ then 33: Remove {i} fromW 34: for b in branch(i) do 35: if mrefb <\n∣∣∣Z>i Θrefb ∣∣∣ then 36: mrefb ←\n∣∣∣Z>i Θrefb ∣∣∣ 37: returnW ,mref\nProof. Take I = {k ∈ JpK : τ(j, k) /∈ W}, v = Xj , θ1 = Θrefj and θ2 = θ in Lemma 3.1. Then if (9) holds, we deduce from (6) that\nmax k∈JpK:τ(j,k)/∈W ∣∣∣Z>τ(j,k)θ ∣∣∣ < λ . This shows that there is no feature i in branch j such that∣∣Z>i θ∣∣ ≥ λ and i is not already inW . The fact that for fixed arguments, the function α→ ηα has a minimum α∗ ∈ R is shown in Appendix B, along with an algorithm to compute it in O (‖Xj ‖0 ln ‖Xj ‖0) operations . Since the statement is true for any α, it is a fortiori true for α∗.\nTheorem 3.1 provides criteria (9) and (10) that can be computed for each branch j, and which if satisfied allow to skip the search for violating variables in the branch. Importantly, the features that are already in the working set W are not taken into account to compute the criterion for a given branch. This subtlety allows to rule out branches even if they already contain features that were previously incorporated in the working set. Note that the reference dual variable for branch j, i.e, Θrefj , is kept unchanged as long as branch j is pruned, and is otherwise updated to the latest dual variable (line 15 of Algorithm 2). As mrefj depends on the reference dual variable instead of the current one, it is solely reevaluated each time the reference residual is updated (line 18 of Algorithm 2) or when a feature from branch j leaves the working set (line 22 of Algorithm 2) .\nCriterion (10) is the most stringent one, and therefore the most efficient one to prune branches, but it takes O (‖Xj ‖0 ln ‖Xj ‖0) operations to compute. In order to balance computational complexity of the bound with its efficacy to prune branches, criterion (9) can be used as an alternative for a specific value of α. One simple choice is to just take α = 1, which leads to the criterion\nη1 ( Xj ,Θ ref j ,θ,m ref j ) = mrefj +ζ ( θ −Θrefj ,Xj ) < λ . (11) Alternatively, a simple heuristic to expect a more efficient pruning is to choose an α that minimises ‖ ( θ − αΘrefj ) Xj ‖2, i.e,\nα`2 = θ> ( Θrefj Xj ) ‖Θrefj Xj ‖22 . (12)\nηα`2 is expected to be more effective than η1 since it is reasonable to expect that ζ ( θ − α`2Θ ref j ,Xj ) is smaller\nthan ζ ( θ −Θrefj ,Xj ) . Overall, computing α = α`2 as in (12) is an O(‖Xj ‖0) operation. Since computing ζ(θ − αΘrefj ,Xj) for a fixed α is also a O(‖Xj ‖0) computation, the total cost of identifying branch j as violated is O(‖Xj ‖0) for criterion (9) with α = 1 or α = α`2 ,\ncompared to O (‖Xj ‖0 ln ‖Xj ‖0) for criterion (10). In Algorithm 2, the notation η refers to a user-defined function among η1, ηα`2 or ηmin."
  }, {
    "heading": "3.3. Updating the working set",
    "text": "When some branches V ⊂ JpK cannot be pruned, the simultaneous updates of the working setW and of mrefV requires scanning through all features in the branches V (lines 5 and 18 in Algorithm 2). In what follows we discuss strategies to make these updates efficient. For that purpose, let us first notice that:\n∀j, k ∈ JpK , ∣∣∣Z>τ(j,k)θ∣∣∣ = ∣∣∣(Xj Xk)> θ∣∣∣\n= ∣∣∣(Xj θ)>Xk∣∣∣\n= ∣∣Q>j Xk∣∣ ,\nwhere for any j ∈ JpK ,Qj = Xj θ. This allows us to write the updates ofW andmrefV as:W ′ =W ∪ { τ(j, k) : j ∈ V, k ∈ JpK , ∣∣Q>j Xk∣∣ ≥ λ} , mrefj = max\nk: |Q>j Xk|<λ ∣∣Q>j Xk∣∣ , ∀j ∈ V .\n(13) This highlights the fact that the updates of the working set W and of mrefV can be cast as particular variants of the Maximum Inner Product Search (MIPS) problem. MIPS aims at finding a vector in a database of probes which maximises the inner product with a given query vector. If we consider X as a set of probes, andQj as a query, then (13) is a variant of MIPS where (i) the set of probe vectors satisfies some constraints and is not known upfront and (ii) the problem is a maximum absolute inner product search. The update of W involves what is sometimes referred to as above-λ-MIPS problems where again, maximum absolute inner products are considered. The interest of casting these updates as variants of MIPS problems is to exploit the ideas developed in the literature for solving these problems efficiently. Teflioudi & Gemulla (2016) and Fontoura et al. (2011) give good overviews of MIPS solvers developed for recommender systems and information retrieval applications respectively. In both cases, the proposed methods rely on two main ideas: (i) adequate indexing techniques or data structures and (ii) pruning criteria which allow to not compute all inner products entirely. Since none of these methods can directly be applied to problem (13) because of its specificities, we propose an appropriate algorithm based on a simple inverted index approach, which we will refer to as IL (standing for Inverted Lists), and which exploits the sparsity of the problem. Another option would be to leverage pruning techniques. We detail such an attempt in Appendix C. However, since our preliminary results with the pruning technique were not conclusive compared to IL\non the simulated and real data, we only focus on the inverted index approach below. IL is detailed in Algorithm 3.\nAlgorithm 3 update W\nInput: X ∈ {0, 1}n×p, θ ∈ Rn, V ⊂ JpK , λ ∈ R, W ⊂ JDK Output: W, mref 1: for j ∈ V do 2: mrefj = 0 3: Set ak = 0 for all k ∈ JpK 4: for each i in supp(Xj) do 5: for each k in supp(xi) do 6: ak = ak + θi 7: end for 8: end for 9: for each k s.t. ak 6= 0 do 10: if mrefj < |ak | < λ then set m ref j = |ak | 11: if |ak | ≥ λ and τ(j, k) /∈ W then add τ(j, k) to W 12: end for 13: end for 14: returnW,mref\nThe inverted lists consist of n lists, one for each dimension, where each list supp(xi) records the indices of the features in X which have a non-zero element for the ith dimension. These inverted lists can be computed once for all when WHInter starts and be reused for all MIPS problems, and therefore building the inverted lists requires a negligible additional computational cost. Algorithm (3) computes inner product following a term-at-a-time (TAAT) scheme (Fontoura et al., 2011), i.e, the inner products are accumulated simultaneously across probes and the contribution of the ith dimension to the inner products is entirely processed before moving to the next one."
  }, {
    "heading": "4. Simulation study",
    "text": "We first test the performances of WHInter on synthetic LASSO datasets. We assess the performances of the different branch pruning bounds presented in 3.2, i.e, ηmin, η1 and ηα`2 , and further compare WHInter to a working set method that uses the bound ζ(θ,Xj) instead of ηα, but is otherwise equivalent to WHInter. We refer to this method as ζ + IL. It is expected to prune less branches than WHInter but does not require to maintain mref . We also compare WHInter to SPP (Nakagawa et al., 2016) and BLITZ (Johnson & Guestrin, 2015). In our experiments, we use a slightly modified, more efficient version of the code provided by the authors of SPP (cf Appendix D). As for BLITZ, since the method is not tailored for interaction problems, we first compute the matrix Z which is fed as input to BLITZ. For this reason we could not solve problems when p is too large (e.g.,\np = 10, 000 in the simulations) since, even in sparse format, storing Z requires too much memory. Importantly, the performances reported for BLITZ do not include the time required to compute Z fromX , which clearly advantages BLITZ compared to the other methods.\nWe simulate five datasets X ∈ {0, 1}n×p with varying number of features and samples: three datasets with p = 1, 000 fixed and n ∈ {300, 1, 000, 10, 000}, and two more with n = 1, 000 fixed and p ∈ {3, 000, 10, 000}. The features are drawn from a Bernoulli distribution with parameter q ∈ [0.1, 0.5] itself drawn from a uniform distribution U[0.1,0.5]. We then randomly pick a set S of 100 features among the main effects and interactions and compute the response as y = ZSw∗S where w ∗ S ∼ N (0|S|, I|S|). In all experiments, the LASSO is solved for a sequence (λt)t∈JT K, T = 100, logarithmically spaced between λmax and max(0.01λmax, λ′) where λmax is the largest value of λ for which at least one feature is selected, and λ′ is the first λi for which 150 features or more are selected in the model. For all methods, the time to compute λmax is included in the total time required to solve the regularisation path. In WHInter, λmax can easily be deduced from the initialisation of mref since λmax = maxj∈JpKm ref j . All algorithms are implemented in C++ and compiled with the -O3 optimisation flag. The experiments are run on a 64-bit machine with Intel Core i7 Processor 2.5 GHz, 16GB of memory and 6MB of cache.\nResults are shown in Figure 2. For n = 1, 000 (Figure 2a), LASSO solutions are computed for 42, 32 and 28 values of λ for p = 1, 000, p = 3, 000 and p = 10, 000 respectively. In these cases smaller values of λ result in model sizes exceeding 150 features. For the remaining settings where p = 1, 000 and n = 300 or n = 10, 000 (Figure 2b),\nLASSO solutions are computed for 34 and all 100 values of λ between λmax and 0.01λmax, respectively. All methods returned the exact same support for all values of λ.\nIn all settings, WHInter is the fastest method. Its better performance compared to ζ+IL highlights the benefit of using reference dual variables even if it implies to maintainmref . The results also show the importance of α, since WHInter with η`2 is always better (×1.2 to ×1.8) than WHInter with η1 for example. Figure 2c confirms that the choice of α has an impact on the pruning efficiency and consequently on the performance. It shows, however, that on this experiment ηmin does not allow to prune many more branches than η`2 . This explains why η`2 tends to outperform ηmin, notably for large n, since the higher computational complexity of ηmin does not sufficiently enhance the pruning. We also notice that SPP is the slowest algorithm, and in particular ζ + IL is ×17 faster than SPP on average. This speed-up is mostly explained by the fact that ζ + IL relies on inverted lists to update the working set while SPP identifies the safe set naively. Overall, WHInter offers a signifiant speed-up of two orders of magnitude or more compared to its safe screening counterpart."
  }, {
    "heading": "5. Results on real world data",
    "text": "We now illustrate the performance of the different algorithms on a real-world problem, where we want to predict the cytotoxic response of 884 lymphoblastoid cell lines split into a train (n = 620) and a test (n = 264) set, and characterized by about 1.2×106 single nucleotide polymorphisms (SNP) that represent their genotypes. The data was released as part of the Dialogue on Reverse Engineering Assessment and Methods 8 (DREAM 8) toxicogenetics challenge (Ed-\nuati et al., 2015). We encode the SNP data as a binary matrix were 1 stand for the presence of a minor allele on one or both copies of the chromosomes. As preprocessing we removed SNP with less than 5% of 1’s and corrected the data for population structure as in Price et al. (2006). To focus on problems of increasing scales, we first considered the SNPs of the smallest chromosome only (chr. 22), then of the largest only (chr. 1) and finally of all chromosomes together. This leads to train matrices with n = 620 and p = 18, 168 SNPs for chromosome 22, p = 89, 027 SNPs for chromosome 1 and p = 1, 166, 836 SNPs for the whole genome.We consider a sequence of regularisation parameter λ logarithmically spaced between λmax and 0.01λmax, and by default stop computations as soon as 150 features or more are selected. This occurs after the 12th, the 11th and the 9th value of λ for chromosome 22, chromosome 1 and all chromosomes respectively. The time required to compute the regularisation paths are shown in Fig. 4. The relative performances of the methods are the same as for the simulations. ηα`2 provides a ×1.4 (resp. ×1.8) speed up compared to using η1 for chromosome 22 (resp. chr. 1). and compared to SPP, there is a ×81 (resp. ×73) speed up for chromosome 22 (resp chr. 1). In the case of the whole genome, we only ran WHInter with ηα`2 which takes two days and a half. While this can seem a lot, we recall that this corresponds to a problem with roughly 680 billion features. We did not run other methods on the whole genome since most of them are expected to take too long.\nOut of curiosity, we also obtained preliminary results concerning the predictive performance of WHInter compared to a LASSO with no interactions on such high-dimensional problems. The results, presented in Figure 3 , suggest that interactions are relevant predictors for this data. For the chromosomes 1 and 22 taken independently, the predictive accuracy of WHInter is better than that of the simple LASSO for almost every value of λ. By contrast, for the whole genome, the LASSO clearly performs better, which may underline statistical issues due to the huge number of variables in this case (Donoho & Tanner, 2009)."
  }, {
    "heading": "6. Discussion",
    "text": "We presented WHInter, a working set algorithm designed to solve large scale `1-penalised linear problems with interaction terms. WHInter implements a new branch pruning bound to efficiently delineate the working set among the many possible interaction variables, and a variant of MIPS solver that provides a further speed up. We showed that WHInter is up to two orders of magnitudes faster than competing approaches. While we presented WHInter for binary data, it could also be used for data rescaled in [0, 1], provided that an appropriate solver is picked for the MIPS problems. As for future work, one could exploit the recent works on approximate MIPS (Shrivastava & Li, 2014; Teflioudi & Gemulla, 2016) to obtain an additional speed up for the computationally intensive updates, and possibly rely on recent post selection-inference (Suzumura et al., 2017) frameworks to characterise the approximate solution obtained."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank our anonymous reviewers for their useful comments as well as Nino Shervashidze for thoughtful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "hierarchical selection of variables in sparse high-dimensional regression",
    "authors": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"],
    "venue": "In Borrow. strength theory powering Appl. Festschrift Lawrence D. Brown, pp. 56–69. Institute of Mathematical Statistics,",
    "year": 2010
  }, {
    "title": "A lasso for hierarchical interactions",
    "authors": ["J. Bien", "J. Taylor", "R. Tibshirani"],
    "venue": "Ann. Stat.,",
    "year": 2013
  }, {
    "title": "Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso",
    "authors": ["A. Bonnefoy", "V. Emiya", "L. Ralaivola", "R. Gribonval"],
    "venue": "IEEE Trans. Signal Process.,",
    "year": 2015
  }, {
    "title": "Observed universality of phase transition in high-dimenisonal geometry, with applications for modern data analysis and signal processing",
    "authors": ["D.L. Donoho", "J. Tanner"],
    "venue": "Philos. Trans. R. Soc. London A Math. Phys. Eng. Sci.,",
    "year": 1906
  }, {
    "title": "Prediction of human population responses to toxic compounds by a collaborative competition",
    "authors": ["D. Ziwei"],
    "venue": "Nat. Biotechnol.,",
    "year": 2015
  }, {
    "title": "Safe feature elimination in sparse supervised learning",
    "authors": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"],
    "venue": "Pacific J. Optim.,",
    "year": 2012
  }, {
    "title": "Mind the Duality Gap: Safer Rules for the Lasso",
    "authors": ["O. Fercoq", "A. Gramfort", "J. Salmon"],
    "venue": "In Proc. 32nd Int. Conf. Mach. Learn.,",
    "year": 2015
  }, {
    "title": "Evaluation Strategies for Top-k Queries over Memory-Resident Inverted Indexes",
    "authors": ["M. Fontoura", "V. Josifovski", "J. Liu", "S. Venkatesan", "X. Zhu", "J.Y. Zien"],
    "venue": "Proc. VLDB Endow.,",
    "year": 2011
  }, {
    "title": "Regularization Paths for Generalized Linear Models via Coordinate Descent",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "venue": "J. Stat. Softw.,",
    "year": 2010
  }, {
    "title": "Fast Lasso Algorithm via Selective Coordinate Descent",
    "authors": ["Y. Fujiwara", "Y. Ida", "H. Shiokawa", "S. Iwamura"],
    "venue": "In Proc. 30th Conf. Artif. Intell.,",
    "year": 2016
  }, {
    "title": "Interaction Screening for UltraHigh Dimensional Data",
    "authors": ["N. Hao", "H.H. Zhang"],
    "venue": "J. Am. Stat. Assoc.,",
    "year": 2014
  }, {
    "title": "Convex Modeling of Interactions With Strong Heredity",
    "authors": ["A. Haris", "D. Witten", "N. Simon"],
    "venue": "J. Comput. Graph. Stat.,",
    "year": 2016
  }, {
    "title": "Blitz: A Principled MetaAlgorithm for Scaling Sparse Optimization",
    "authors": ["T. Johnson", "C. Guestrin"],
    "venue": "In Proc. 32nd Int. Conf. Mach. Learn.,",
    "year": 2015
  }, {
    "title": "StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent",
    "authors": ["T.B. Johnson", "C. Guestrin"],
    "venue": "In Proc. 34th Int. Conf. Mach. Learn.,",
    "year": 2017
  }, {
    "title": "Accelerating ISTA with an active set strategy",
    "authors": ["M. Kowalski", "P. Weiss", "A. Gramfort", "S. Anthoine"],
    "venue": "In OPT 2011 4th Int. Work. Optim. Mach. Learn., pp",
    "year": 2011
  }, {
    "title": "Learning Interactions via Hierarchical Group-Lasso Regularization",
    "authors": ["M. Lim", "T. Hastie"],
    "venue": "J. Comput. Graph. Stat.,",
    "year": 2015
  }, {
    "title": "From safe screening rules to working sets for faster Lasso-type solvers",
    "authors": ["M. Massias", "A. Gramfort", "J. Salmon"],
    "venue": "ArXiv e-prints,",
    "year": 2017
  }, {
    "title": "Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining",
    "authors": ["K. Nakagawa", "S. Suzumura", "M. Karasuyama", "K. Tsuda", "I. Takeuchi"],
    "venue": "In Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.,",
    "year": 2016
  }, {
    "title": "Gap Safe screening rules for sparsity enforcing penalties",
    "authors": ["E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2017
  }, {
    "title": "Principal components analysis corrects for stratification in genome-wide association studies",
    "authors": ["A.L. Price", "N.J. Patterson", "R.M. Plenge", "M.E. Weinblatt", "N.A. Shadick", "D. Reich"],
    "venue": "Nat. Genet.,",
    "year": 2006
  }, {
    "title": "Variable selection using Adaptive Nonlinear Interaction Structures in High dimensions",
    "authors": ["P. Radchenko", "G. James"],
    "venue": "J. Am. Stat. Assoc.,",
    "year": 2010
  }, {
    "title": "Screening Rules for Convex Problems",
    "authors": ["A. Raj", "J. Olbrich", "B. Gärtner", "B. Schölkopf", "M. Jaggi"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Modelling interactions in high-dimensional data with backtracking",
    "authors": ["R.D. Shah"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2016
  }, {
    "title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)",
    "authors": ["A. Shrivastava", "P. Li"],
    "venue": "In Adv. Neural Inf. Process. Syst.,",
    "year": 2014
  }, {
    "title": "Selective Inference for Sparse High-Order Interaction Models",
    "authors": ["S. Suzumura", "K. Nakagawa", "Y. Umezu", "K. Tsuda", "I. Takeuchi"],
    "venue": "In Proc. 34th Int. Conf. Mach. Learn.,",
    "year": 2017
  }, {
    "title": "Exact and Approximate Maximum Inner Product Search with LEMP",
    "authors": ["C. Teflioudi", "R. Gemulla"],
    "venue": "ACM Trans. Database Syst.,",
    "year": 2016
  }, {
    "title": "Regression Selection and Shrinkage via the Lasso",
    "authors": ["R. Tibshirani"],
    "venue": "J. R. Stat. Soc. Ser. B (Statistical Methodol.,",
    "year": 1996
  }, {
    "title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming (Lasso)",
    "authors": ["M.J. Wainwright"],
    "venue": "IEEE Trans. Inf. Theory,",
    "year": 2009
  }, {
    "title": "Lasso screening rules via dual polytope projection",
    "authors": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"],
    "venue": "In Adv. Neural Inf. Process. Syst.,",
    "year": 2013
  }, {
    "title": "Genome-wide association analysis by lasso penalized logistic regression",
    "authors": ["T.T. Wu", "Y.F. Chen", "T. Hastie", "E. Sobel", "K. Lange"],
    "year": 2009
  }, {
    "title": "Learning sparse representations of high dimensional data on large scale dictionaries",
    "authors": ["Z. Xiang", "H. Xu", "P. Ramadge"],
    "venue": "In Adv. Neural Inf. Process. Syst.,",
    "year": 2011
  }, {
    "title": "Fast lasso screening tests based on correlations",
    "authors": ["Z.J. Xiang", "P.J. Ramadge"],
    "venue": "In IEEE Int. Conf. Acoust. Speech Signal Process.,",
    "year": 2012
  }],
  "id": "SP:d13647ade2270f92c597d55d204f951fcc2d30ae",
  "authors": [{
    "name": "Marine Le Morvan",
    "affiliations": []
  }, {
    "name": "Jean-Philippe Vert",
    "affiliations": []
  }],
  "abstractText": "Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics. `1-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features. Here we present WHInter, a working set algorithm to solve large `1-regularised problems with two-way interactions for binary design matrices. The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem. We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.",
  "title": "WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models"
}