{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Our goal is to build a competitor for decision tree and rule learning algorithms in terms of accuracy, interpretability, and computational speed. Decision trees are widely used, particularly in industry, because of their interpretability. Their logical IF-THEN structure allows predictions to be explained to users. However, decision tree algorithms have the serious flaw that they are constructed using greedy splitting from the top down. They also use greedy pruning of nodes. They do not globally optimize any function, instead they are composed entirely of local optimization heuristics. If the algorithm makes a mistake in the splitting near the\n1Massachusetts Institute of Technology, Cambridge, Massachusetts, USA 2Duke University, Durham, North Carolina, USA 3Harvard University, Cambridge, Massachusetts, USA. Correspondence to: Hongyu Yang <hongyuy@mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntop of the tree, it is difficult to undo it, and consequently the trees become long and uninterpretable, unless they are heavily pruned, in which case accuracy suffers. In general, decision tree algorithms are computationally tractable, not particularly accurate, and less sparse and interpretable than they could be. This leaves users with no good alternative if they desire an accurate yet sparse logical classifier.\nSeveral important ingredients provide the underpinning for our method including:\n(i) A principled objective, which is the posterior distribution for the Bayesian Rule List (BRL) model (see Letham et al., 2015). We optimize this objective over rule lists. Our algorithm is called Scalable Bayesian Rule Lists (SBRL).\n(ii) A useful statistical approximation that narrows the search space. We assume that each rule in the rule list contains (“captures”) a number of observations that is bounded below. Because of this approximation, the set of conditions defining each leaf is a frequent pattern. This means the antecedents within the rule list are all frequent patterns. All of the possible frequent patterns can be pre-mined from the dataset using one of the standard frequent pattern mining methods. This leaves us with a much smaller optimization problem: we optimize over the set of possible pre-mined antecedents and their order to create the rule list.\n(iii) High performance language libraries to achieve computational efficiency. Optimization over rule lists is done through repeated low level computations. At every iteration, we make a change to the rule list and need to evaluate the new rule list on the data. High performance calculations (novel to this problem) speed up this evaluation.\n(iv) Computational reuse. When we evaluate a rule list on the data that has been modified from a previous rule list, we need only to change the evaluation of points below the change in the rule list. Thus we can reuse the computation above the change.\n(v) Analytical bounds on BRL’s posterior that are tight enough to be used in practice for screening association\nThrough a series of controlled experiments, we show that SBRL is over two orders of magnitude faster than the previous best code for this problem.\nFor example, Table 1 presents a rule list that we learned for the UCI Mushroom dataset (see Bache & Lichman, 2013). This rule list is a predictive model for whether a mushroom is edible. It was created in about 9 seconds on a laptop and achieves perfect out-of-sample accuracy."
  }, {
    "heading": "2. Review of Bayesian Rule Lists",
    "text": "Scalable Bayesian Rule Lists maximizes the posterior distribution of the Bayesian Rule Lists algorithm. Our training set is {(xi, yi)}ni=1 where the xi ∈ X encode features, and yi are labels, which in our case are binary, either 0 or 1. A Bayesian rule list has the following form:\nif x obeys a1 then y ∼ Binom(θ1), θ1 ∼ Beta(α+ N1) else if x obeys a2 then y ∼ Binom(θ2), θ2 ∼ Beta(α+ N2) ... else if x obeys am then y ∼ Binom(θm), θm ∼ Beta(α+ Nm) else y ∼ Binom(θ0), θ0 ∼ Beta(α+ N0).\nHere, the antecedents {aj}mj=1 are conditions on the x’s that are either true or false, for instance, if x is a patient, aj is true when x’s age is above 60 years old and x has diabetes, otherwise false. The vector α = [α1, α0] has a prior parameter for each of the two labels. Values α1 and α0 are prior parameters, in the sense that each rule’s prediction y ∼ Binomial(θj), and θj |α ∼ Beta(α). The notation Nj is the vector of counts, where Nj,l is the number of observations xi that satisfy condition aj but none of the previous\nconditions a1, ..., aj−1, and that have label yi = l, where l is either 1 or 0. Nj is added to the prior parameters α from the usual derivation of the posterior for the Beta-binomial. The default rule is at the bottom, which makes predictions for observations that are not satisfied by any of the conditions. When an observation satisfies condition aj but not a1, ..., aj−1 we say that the observation is captured by rule j. Formally:\nDefinition 1 Rule j captures observation i, denoted Captr(i) = j, when j = argmin j′ such that aj′(xi) = True.\nBayesian Rule Lists is an associative classification method, in the sense that the antecedents are first mined from the database, and then the set of rules and their order are learned. The rule mining step is fast, and there are fast parallel implementations available. Any frequent pattern mining method will suffice, since the method needs only to produce those conditions with sufficiently high support in the database. The support of antecedent aj is denoted supp(aj), which is the number of observations that obey condition aj . A condition is a conjunction of expressions “feature∈values,” e.g., age∈[40,50] and color=white. The hard part is learning the rule list, which is what this paper focuses on. It is an optimization over subsets of rules and their permutations.\nThe likelihood for the model discussed above is:\nLikelihood = p(y|x, d, α) ∝ ∏m j=0 Γ(Nj,0+α0)Γ(Nj,1+α1) Γ(Nj,0+Nj,1+α0+α1) ,\nwhere d denotes the rules in the list and their order, d = (m, {aj , θj}mj=0). Intuitively, one can see that having more of one class and less of the other class will make the likelihood larger. To see this, note that if Nj,0 is large and Nj,1 is small (or vice versa) the likelihood for rule j is large.\nWe next discuss the prior. It has three terms, one governing the number of rules m in the list, one governing the size cj of each antecedent j, and one governing the choice of antecedent condition aj of rule j given its size. Specifically, cj is the cardinality of antecedent aj , also written |aj |, the number of conjunctive clauses in antecedent aj . E.g, if a is ‘x1=green’ and ‘x2<50’, this has cardinality 2. Notation a<j includes the antecedents before j in the rule list, if there are any, e.g. a<4 = {a1, a2, a3}. c<j includes the cardinalities of the antecedents before j in the rule list. Notation A is the set of pre-mined antecedents. The prior is:\np(d|A, λ, η) = p(m|A, λ) ∏m j=1 p(cj |c<j ,A, η)p(aj |a<j , cj ,A).\nThe first term is the prior for the number of rules in the list. Here, the number of rules m is Poisson, truncated at the\ntotal number of pre-selected antecedents:\np(m|A, λ) = (λ m/m!)∑|A|\nj=0(λ j/j!)\n, m = 0, . . . , |A|,\nwhere λ is a hyper-parameter. The second term in the prior governs the number of conditions in each rule. The size of rule j is cj which is Poisson, truncated to remove values for which no rules are available with that cardinality:\np(cj |c<j ,A, η) = (η cj /cj !)∑\nk∈Rj−1(c<j,A) (ηk/k!)\n, cj ∈ Rj−1(c<j ,A),\nwhere Rj−1(c<j ,A) is the set of cardinalities available after removing the first j−1 rules, and η is a hyperparameter. The third term in the prior governs the choice of antecedent, given that we have determined its size through the second term. We have aj selected from a uniform distribution over antecedents in A of size cj , excluding those in a<j .\np(aj |a<j , cj ,A) ∝ 1, aj ∈ Qcj = {a ∈ A \\ {a1, a2, ..., aj−1} : |a| = cj}.\nAs usual, the posterior is the likelihood times the prior.\np(d|x,y,A, α, λ, η) ∝ p(y|x, d, α)p(d|A, λ, η).\nThis is the full model, and the posterior p(d|x,y,A, α, λ, η) is what we optimize to obtain the best rule lists.\nThe hyperparameter λ is chosen by the user to be the desired size of the rule list, and η is chosen as the desired number of terms in each rule. The parameters α0 and α1 are usually chosen to be 1 to avoid favoring one class label over another.\nGiven the prior parameters λ, η, and α, along with the set of pre-mined rulesA, the algorithm must select which rules from A to use, along with their order."
  }, {
    "heading": "3. Representation",
    "text": "We use an MCMC scheme: at each time t, we choose a neighboring rule list at random by adding, removing, or swapping rules, starting with the basic algorithm of Letham et al. (2015) as a starting point. However, to optimize performance we use a more efficient rule list representation that is amenable to fast computation.\nExpressing computation as bit vectors: The vast majority of the computational time spent constructing rule sets lies in determining which rules capture which observations in a particular rule ordering. The naı̈ve implementation of these operations calls for various set operations – checking whether a set contains an element, adding an element to a set, and removing an element from a set. However, set operations are typically slow, and hardware does little to help with efficiency.\nWe convert all set operations to logical operations on bit vectors, for which hardware support is readily available. The bit vector representation is efficient in terms of both memory and computation. Before beginning the algorithm, for each rule, we compute the bit vector representing the samples for which the rule generates a true value. For a one million sample data set (or more precisely up to 1,048,576 observations) each rule carries with it a 128 KB vector (since a byte consists of 8 bits), which fits comfortably in most L2 caches.\nRepresenting intermediate state as bit vectors: For each rule list we consider, we maintain similarly sized vectors for each rule in the set indicating which (unique) rule in the set captures which observation. Representing the rules and rule lists this way allows us to explore the rule list state space, reusing significant computation. For example, consider a rule list containing n rules. Imagine that we wish to delete rule k from the set. The naı̈ve implementation recomputes the “captures” vector for every rule in the set. Our implementation updates only rules j > k, using logical operators acting upon the rule list “captures” vector for k, and the rule’s “captures” vector for each rule j > k. This shortens the run time of the algorithm in practice by approximately 50%.\nA fast algebra for computational reuse: Our use of bit vectors transforms the large number of set operations (performed in a traditional implementation) into a set of boolean operations on bit vectors. We have custom implementations (discussed in the full version of this work, Yang et al., 2017) for the following: (i) Remove rule k uses boolean operations on bit vectors to redistribute the observations captured by rule k to the rules below it in the list. (ii) Insert rule k shifts the rules below k down one position, determines which observations are captured by the new rule, and removes those observations from the rules below it. (iii) Swap consecutive rules updates only which observations were captured for the two swapped rules. (iv) Generalized swap subroutine updates only observations captured for all rules between the two rules to be swapped. All operations use only bit vector computations.\nAblation study: Having transformed expensive set operations into bit vector operations, we can now leverage both hardware vector instructions and optimized software libraries. We investigated four alternative implementations, each improving efficiency from the previous one. (i) First, we have the original python implementation here for comparison. (ii) Next, we retained our python implementation but converted from set operations to bit operations. (iii) Then, we used the python gmpy library to perform the bit operations. (iv) Finally, we moved the implementation from Python to C, represented the bit vectors as multiprecision integers, used the GMP library, which is faster on\nlarge data sets, and implemented the algebra for computational reuse outlined above. To evaluate how each of these steps improved the computation time of the algorithm, we conducted a controlled experiment where each version of the algorithm (corresponding to the four steps above) was given the same data (the UCI adult dataset, divided into three folds), same set of rules, and same number of MCMC iterations (20,000) to run. We created boxplots for the log10 of the run time over the different folds, which are shown in Figure 1. The final code is over two orders of magnitude faster than the original optimized python code (that of Letham et al., 2015)."
  }, {
    "heading": "4. Bounds",
    "text": "We prove two bounds. First we provide an upper bound on the number of rules in a maximum a posteriori rule list. This allows us to narrow our search space to rule lists below a certain size. Second we provide a constraint that eliminates certain prefixes of rule lists. This prevents our algorithm from searching in regions of the space that provably do not contain the maximum a posteriori rule list."
  }, {
    "heading": "4.1. Upper bound on the number of rules in the list",
    "text": "Given the number of features, the parameter λ for the size of the list, and parameters α0 and α1, we can derive an upper bound for the size of a maximum a posteriori rule list. This formalizes how the prior on the number of rules is strong enough to overwhelm the likelihood.\nWe are considering binary antecedents and binary features (e.g., aj is true if female), so the total number of possible\nAlgorithm 1 Calculating bj’s Initialization: index=0, b0 = 1 for c = 0 to ⌊ P 2 ⌋ do\nfor j = ( P c ) downto 1 do\nindex = index + 1 bindex = j\nend for if c+ c 6= p then\nfor j = ( P P−c ) downto 1 do\nindex = index + 1 bindex = j\nend for end if\nend for\nantecedents of each size can be calculated directly. When creating the upper bound, within the proof, we hypothetically exhaust antecedents from each size category in turn, starting with the smallest sizes. We discuss this further below.\nLet |Qc| be the number of antecedents that remain in the pile that have c logical conditions. The sequence of b’s that we define next is a lower bound for the possible sequence of |Qc|’s. In particular, b0, b1, b2, etc. represents the sequence of sizes of rules that would provide the smallest possible |Qc|’s. Intuitively, the sequence of b’s arises when we deplete the antecedents of size 1, then deplete all of size 2, etc. The number of ways to do this is given by the bindex values, computed as Algorithm 1, where P is the number of features, and b = {b0, b1, ...b2P−1} is a vector of length 2P . We will use b within the theorem below. In our notation, rule list d is defined by the antecedents and the probabilities on the right side of the rules, d = (m, {al, θl}ml=1).\nTheorem 1 The size m∗ of any MAP rule list d∗ (with parameters λ, η, and α = (α0, α1)) obeys m∗ ≤ mmax, where\nmmax = min\n{ 2P − 1,max { m′ ∈ Z+ : λ m′ m′! ≥ Γ(N−+α0)Γ(N++α1)\nΓ(N+α0+α1) m′∏ j=1 bj\n}} .\nWith parameters α0 = 1 and α1 = 1, this reduces to:\nmmax = min\n{ 2P − 1,max { m′ ∈ Z+ : λ m′ m′! ≥ Γ(N−+1)Γ(N++1) Γ(N+2) m′∏ j=1 bj }} .\nThe proof is in the longer version of this paper (Yang et al., 2017). Theorem 1 tends to significantly reduce the size of the space. Without this bound, it is possible that the search would consider extremely long lists, without knowing that they are provably non-optimal."
  }, {
    "heading": "4.2. Prefix Bound",
    "text": "We next provide a bound that eliminates certain regions of the rule space from consideration. Consider a rule list beginning with antecedents a1, .., ap. If the best possible rule list starting with a1, .., ap cannot beat the posterior of the best rule list we have found so far, then we know any rule list starting with a1, .., ap is suboptimal. In that case, we should stop exploring rule lists that start with a1, .., ap. This is a type of branch and bound strategy, in that we have now eliminated (bounded) the entire set of lists starting with a1, .., ap. We formalize this intuition below.\nDenote the rule list antecedents at iteration t by dt = (at1, a t 2, ..., a t mt , a0). The current best posterior probability has value v∗t , that is\nv∗t = max t′≤t Posterior(dt ′ , {(xi, yi)}ni=1).\nLet us consider a rule list with antecedents d = (a1, a2, ...am, a0). Let dp denote a prefix of length p of the rule list d, i.e., dp = (a1, a2, ...ap), where a1, a2, ..., ap are the same as the first p antecedents in d. We want to determine whether a rule list starting with dp could be better than the best we have seen so far.\nDefine Υ(dp, {(xi, yi)}ni=1) as follows:\nΥ(dp, {(xi, yi)}ni=1) := λmax (p,λ)/(max (p,λ))!∑|A|\nj=0(λ j/j!)\n(∏p j=1 p(cj |c<j ,A, η)\n1 |Qcj | ) × (∏m\nj=0 Γ(Nj,0+1)Γ(Nj,1+1)\nΓ(Nj,0+Nj,1+2)\n) ×\nΓ(1+N0− ∑p j=1Nj,0)\nΓ(2+N0− ∑p j=1Nj,0)\nΓ(1+N1− ∑p j=1Nj,1)\nΓ(2+N1− ∑p j=1Nj,1) .\nHere, Nj,0 is the number of points captured by rule j with label 0, and Nj,1 is the number of points captured by rule j with label 1,\nNj,0 = |{i : Captr(i) = j and yi = 0}|, Nj,1 = |{i : Captr(i) = j and yi = 1}|.\nThe result states that for a rule list with prefix dp, if the upper bound on the posterior, Υ(dp), is not as high as the posterior of the best rule list we have seen so far, then dp is a bad prefix, which cannot lead to a MAP solution. It tells us we no longer need to consider rule lists starting with dp.\nTheorem 2 For rule list d = {dp, ap+1, ..., am, a0}, if\nΥ(dp, {(xi, yi)}ni=1) < v∗t ,\nthen for α0 = 1 and α1 = 1, we have\nd 6∈ argmaxd′Posterior(d′, {(xi, yi)}ni=1). (1)\nTheorem 2 is implemented in our code in the following way: for each random restart, the initial rule in the list is checked against the bound of Theorem 2. If the condition Υ(d1) < v ∗ t holds, we throw out this initial rule and choose a new one, because that rule provably cannot be the first rule in an optimal rule list. Theorem 2 provides a substantial computational speedup in finding high quality or optimal solutions. In some cases, it provides a full order of magnitude speedup. The proofs are lengthy and contained in the longer version of this work (Yang et al., 2017)."
  }, {
    "heading": "5. Experiments",
    "text": "We provide a comparison of algorithms along three dimensions: solution quality (AUC - area under the ROC curve), sparsity, and scalability. As baselines, we chose popular algorithms to represent the sets of uninterpretable methods and the set of “interpretable” methods. To represent uninterpretable methods, we chose logistic regression, SVM RBF, random forests (RF), and boosted decision trees (ADA). To represent the class of “interpretable” algorithms, we chose CART, C4.5, RIPPER (Cohen, 1995), CBA (Liu et al., 1998), and CMAR (Li et al., 2001). Other works (see Letham et al., 2015; Wang & Rudin, 2015a) have accuracy/interpretability comparisons to Bayesian Rule Lists and Falling Rule Lists, so our main effort here will be to study the scalability component. Implementation details are in the full version (Yang et al., 2017).\nWe benchmark using publicly available datasets (see Bache & Lichman, 2013) that have interpretable features: the Tic Tac Toe dataset, where the goal is to determine whether the “X” player wins; the Adult dataset, where we aim to predict whether an individual makes over $50K peryear; the Mushroom dataset, where the goal is to predict whether a mushroom is edible; the Nursery dataset, where the goal is to predict whether a child’s application to nursery school will be in either the “very recommended” or “special priority” categories; and the Telco customer churn dataset (see WatsonAnalytics, 2015), where the goal is to predict whether a customer will leave the service provider.\nEvaluations of prediction quality, sparsity, and timing were done using 10-fold cross validation. The prior parameters were fixed at η = 1, and α = (1, 1). For the λ for each dataset, we first let λ be 5 and ran SBRL once with the above parameters. Then we fixed λ at the length of the returned rule list for that dataset. It is possible that the solution quality would increase if SBRL ran for a larger number of iterations. For the purpose of providing a controlled experiment, the number of iterations was fixed at 5,000 for each of the 20 chains of SBRL, which we ran in series on a laptop. Every time SBRL started a new rule list, we checked the initial rule in the list to see whether the upper-bound on its posterior (by Theorem 2) was greater\nthan the best rule list we had found so far. If not, the rule was replaced until the condition was satisfied.\nTic Tac Toe: Each observation in this dataset is a tic tac toe board after the game has finished. If there are 3 X’s in a row, the label of the board is 1, otherwise 0. This should not be a difficult learning problem since there are solutions with perfect accuracy on the training set that generalize to the test set. Figure 2 shows a scatter plot of AUC vs. number of leaves (sparsity), where each triangular marker represents an evaluation of one algorithm, on one fold, with one parameter setting. We tried many different parameter settings for CART (in blue), and many different parameter settings for C4.5 (in gray), none of which were able to achieve points on the efficient frontier defined by SBRL. SBRL’s run time was on average 0.759 (± .02) seconds.\nAdult: For the Adult dataset, results are in Figure 3, Figure 4 and Table 2. Adult contains 45,121 observations and 12 features, where each observation is an individual, and the features are census data, including demographics, income levels, and other financial information. Here, SBRL, which was untuned and forced to be sparse, performed only slightly worse than several of the uninterpretable methods. Its AUC performance dominated those of the CART and C4.5 algorithms. As the scatter plot shows, even if CART were tuned on the test set, it would have performed at around the same level, perhaps slightly worse than SBRL. The timing for SBRL was competitive, at about 18 seconds, where 14 seconds were MCMC iterations. If the chains were computed in parallel rather than in series, it would speed up computation further.\nMushroom: Table 1 contains an SBRL rule list for Mushroom; other results are in Yang et al. (2017). SBRL attains perfect test accuracy on this dataset.\nNursery: Results from the Nursery dataset are shown in Figure 5. A similar story holds as for the previous datasets: SBRL is on the optimal frontier of accuracy/sparsity without tuning and with reasonable run time.\nTelco: Figure 6, Figure 7 and Table 3 show the results for the Telco dataset, which contains 7043 observations and 18 features. Similar observations hold for this dataset. The model from one of the ten folds is provided in Table 4."
  }, {
    "heading": "6. Scalability",
    "text": "We used the USCensus1990 data (see Bache & Lichman, 2013) to test the scalability of SBRL on large datasets. We used 1,000,000 observations and set SBRL’s parameter to extract ≈1000 rules as problem (A) and about 50,000 observations with 50,000 rules as problem (B). The runtime comparison with CART is shown in Table 5. For problem (A), the run times are similar; for (B); SBRL is slower (2.5 hours), which is not prohibitive for important problems. One can see why CART does not perform as well in high dimensions, as it often spends less time on harder problems than on easier ones; details are in (Yang et al.,\nTable 3. Run Time on Telco dataset\nRUN TIME LR SVM RF ADA CART C4.5 RIPPER CBA CMAR SBRL\nMEAN 0.267 7.468 3.703 7.839 0.168 0.250 37.14 8.028 1.679 5.239 MEDIAN 0.272 7.550 3.695 8.726 0.168 0.252 37.63 8.050 1.705 5.271 STD 0.009 0.207 0.183 0.111 0.008 0.017 3.202 0.400 0.161 0.149\n2017)."
  }, {
    "heading": "7. Related Works and Discussion",
    "text": "Rule lists are a type of one-sided decision tree, and any decision tree can be written as a rule list by enumerating the leaves. Thus SBRL is a competitor for CART (Breiman et al., 1984). CART is currently still popular in industry. CART and other decision tree methods (also decision list methods and associative classification methods) form trees from the top down using greedy splitting and greedy pruning (see, e.g., Quinlan, 1983; Clark & Niblett, 1989; Cendrowska, 1987; Rivest, 1987; Quinlan, 1993; Liu et al., 1998; Li et al., 2001; Yin & Han, 2003; Marchand & Sokolova, 2005; Vanhoof & Depaire, 2010; Rudin et al., 2013). Since our work does not use greedy splitting and pruning, it is closer to Bayesian tree models (Dension et al., 1998; Chipman et al., 2002; 2010), which are built greedily from the top down, but then the trees change according to an MCMC scheme, which allows for more exploration of the search space. However even with MCMC, the chains tend to center on local optima. It may be possible to use our techniques to build trees, where one would mine rules and create a globally optimal tree.\nThere are a series of works from the mid-1990’s onwards on finding optimal decision trees using dynamic programming and search techniques (e.g., Bennett & Blue, 1996; Auer et al., 1995; Dobkin et al., 1996; Boros et al., 2000; Garofalakis et al., 2000; Farhangfar et al., 2008), mainly working with fixed depth trees. The number of trees of a\nfixed depth is much larger than the number of rule lists of a fixed depth and are therefore more difficult to optimize. Nijssen & Fromont (2010) use pre-mined rules to form trees, but in a different way than our method. There, the user premines all possible leaves, enumerating all conditions leading to that leaf. (By contrast, in associative classification, we mine only small conjunctions, and their ordered combination creates leaves.) As as result, Nijssen & Fromont (2010) warn about issues related to running out of memory.\nAn extension of this work (CORELS - Angelino et al., 2017) does not provide probabilistic predictions, but is able to provide a certificate of optimality to a globally optimal rule list. This indicates that SBRL is probably also achieving optimality; however, because SBRL is probabilistic, the proof of optimality is much more difficult. To clarify: finding the optimal solution for both methods should be approximately equally difficult, but proving optimality for SBRL is much more difficult. However, there is a clear practical benefit to having probabilistic predictions like those of SBRL. One can post-process CORELS to have probabilistic predictions by computing P (Y = 1|x ∈ leaf) for each leaf, but this is not the same as optimizing likelihood and obtaining these probabilities directly like SBRL.\nThere are several subfields that produce disjunctive normal form (DNF) classifiers rather than rule lists, including rule learning/induction, and associative classification, which stemmed possibly from work in the 1960s (Michalski, 1969), and throughout the 1980’s and 90’s (Cendrowska, 1987; Clark & Niblett, 1989; Cohen, 1995). The vast majority of these techniques are not probabilistic, and aim for covering the positive class without covering the negative class. Rijnbeek & Kors (2010) aim to produce globally op-\ntimal DNF models. There is recent work on probabilistic DNF’s that is similar to SBRL (Wang et al., 2016; 2017).\nTeleo-reactive programs (Nilsson, 1994) use a rule list structure and could benefit from learning this structure from data.\nSBRL aims to produce interpretable models. Interpretability has been a fundamental topic in artificial intelligence for a long time (see Rüping, 2006; Bratko, 1997; Dawes, 1979; Vellido et al., 2012; Giraud-Carrier, 1998; Holte, 1993; Shmueli, 2010; Huysmans et al., 2011; Freitas, 2014). Because the rule lists created by our method are designed to be interpretable, one would probably not want to boost them using AdaBoost to form more complicated models. This contrasts with, for instance, Friedman & Popescu (2008), who linearly combine pre-mined rules.\nRule lists and their variants are currently being used for text processing (King et al., 2017), discovering treatment regimes (Zhang et al., 2015; Lakkaraju & Rudin, 2017; Wang & Rudin, 2015b), and creating medical risk assessments (Letham et al., 2015), among other applications.\nConclusion We finish by stating why/when one would want to use this particular method. SBRL is not meant as a competitor for black box classifiers such as neural networks, support vector machines, gradient boosting or random forests. It is useful when machine learning tools are used as a decision aid to humans, who need to understand the model in order to trust it and make data-driven decisions. SBRL does not use a greedy splitting/pruning procedure like decision tree algorithms (CART, C4.5), which means that it more reliably computes high quality solutions, at the possible expense of additional computation time. Many of the decision tree methods do not compute sparse or interpretable trees, as we have seen with C4.5. Our code is a strict improvement over the original Bayesian Rule Lists algorithm if one is looking for a maximum a posteriori solution. It is faster because of careful use of low-level computations and theoretical bounds.\nCode\nCode for SBRL is available at the following link: https://github.com/Hongyuy/sbrlmod Link to R package SBRL on CRAN: https: //cran.r-project.org/web/packages/sbrl/ index.html"
  }, {
    "heading": "Acknowledgement",
    "text": "The authors would like to acknowledge partial funding provided by NSF, Philips, Wistron, and Siemens."
  }],
  "year": 2017,
  "references": [{
    "title": "Certifiably optimal rule lists for categorical data",
    "authors": ["Angelino", "Elaine", "Larus-Stone", "Nicholas", "Alabi", "Daniel", "Seltzer", "Margo", "Rudin", "Cynthia"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD Conference of Knowledge, Discovery, and Data Mining (KDD),",
    "year": 2017
  }, {
    "title": "Theory and applications of agnostic PAC-learning with small decision trees",
    "authors": ["Auer", "Peter", "Holte", "Robert C", "Maass", "Wolfgang"],
    "year": 1995
  }, {
    "title": "Optimal decision trees",
    "authors": ["Bennett", "Kristin P", "Blue", "Jennifer A"],
    "venue": "Technical report, R.P.I. Math Report No. 214, Rensselaer Polytechnic Institute,",
    "year": 1996
  }, {
    "title": "An implementation of logical analysis of data",
    "authors": ["Boros", "Endre", "Hammer", "Peter L", "Ibaraki", "Toshihide", "Kogan", "Alexander", "Mayoraz", "Eddy", "Muchnik", "Ilya"],
    "venue": "IEEE Transactions on Knowledge and Data Engineering,",
    "year": 2000
  }, {
    "title": "Machine learning: between accuracy and interpretability",
    "authors": ["I. Bratko"],
    "year": 1997
  }, {
    "title": "Classification and Regression Trees",
    "authors": ["Breiman", "Leo", "Friedman", "Jerome H", "Olshen", "Richard A", "Stone", "Charles J"],
    "year": 1984
  }, {
    "title": "PRISM: An algorithm for inducing modular rules",
    "authors": ["J. Cendrowska"],
    "venue": "International Journal of Man-Machine Studies,",
    "year": 1987
  }, {
    "title": "BART: Bayesian additive regression trees",
    "authors": ["Chipman", "Hugh A", "George", "Edward I", "McCulloch", "Robert E"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2010
  }, {
    "title": "The CN2 induction algorithm",
    "authors": ["Clark", "Peter", "Niblett", "Tim"],
    "venue": "Machine Learning,",
    "year": 1989
  }, {
    "title": "Fast effective rule induction",
    "authors": ["Cohen", "William W"],
    "venue": "Proceedings of the Twelfth International Conference on Machine Learning,",
    "year": 1995
  }, {
    "title": "The robust beauty of improper linear models in decision making",
    "authors": ["Dawes", "Robyn M"],
    "venue": "American Psychologist,",
    "year": 1979
  }, {
    "title": "Induction of shallow decision trees",
    "authors": ["Dobkin", "David", "Fulton", "Truxton", "Gunopulos", "Dimitrios", "Kasif", "Simon", "Salzberg", "Steven"],
    "year": 1996
  }, {
    "title": "A fast way to produce optimal fixed-depth decision trees",
    "authors": ["Farhangfar", "Alireza", "Greiner", "Russell", "Zinkevich", "Martin"],
    "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM",
    "year": 2008
  }, {
    "title": "Comprehensible classification models: a position paper",
    "authors": ["Freitas", "Alex A"],
    "venue": "ACM SIGKDD Explorations Newsletter,",
    "year": 2014
  }, {
    "title": "Predictive learning via rule ensembles",
    "authors": ["Friedman", "Jerome H", "Popescu", "Bogdan E"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2008
  }, {
    "title": "Efficient algorithms for constructing decision trees with constraints",
    "authors": ["Garofalakis", "Minos", "Hyun", "Dongjoon", "Rastogi", "Rajeev", "Shim", "Kyuseok"],
    "venue": "In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
    "year": 2000
  }, {
    "title": "Beyond predictive accuracy: what? In Proceedings of the ECML-98 Workshop on Upgrading Learning to Meta-Level: Model Selection and Data Transformation",
    "authors": ["Giraud-Carrier", "Christophe"],
    "year": 1998
  }, {
    "title": "Very simple classification rules perform well on most commonly used datasets",
    "authors": ["Holte", "Robert C"],
    "venue": "Machine Learning,",
    "year": 1993
  }, {
    "title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models",
    "authors": ["Huysmans", "Johan", "Dejaeger", "Karel", "Mues", "Christophe", "Vanthienen", "Jan", "Baesens", "Bart"],
    "venue": "Decision Support Systems,",
    "year": 2011
  }, {
    "title": "Computerassisted keyword and document set discovery from unstructured text",
    "authors": ["King", "Gary", "Lam", "Patrick", "Roberts", "Margaret"],
    "venue": "American Journal of Political Science,",
    "year": 2017
  }, {
    "title": "Learning cost effective and interpretable treatment regimes in the form of rule lists",
    "authors": ["Lakkaraju", "Himabindu", "Rudin", "Cynthia"],
    "venue": "In Proceedings of Artificial Intelligence and Statistics (AISTATS),",
    "year": 2017
  }, {
    "title": "Crafting papers on machine learning",
    "authors": ["P. Langley"],
    "venue": "Proceedings of the 17th International Conference on Machine Learning ICML,",
    "year": 2000
  }, {
    "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model",
    "authors": ["Letham", "Benjamin", "Rudin", "Cynthia", "McCormick", "Tyler H", "Madigan", "David"],
    "venue": "Annals of Applied Statistics,",
    "year": 2015
  }, {
    "title": "CMAR: accurate and efficient classification based on multiple class-association rules",
    "authors": ["Li", "Wenmin", "Han", "Jiawei", "Pei", "Jian"],
    "venue": "In IEEE International Conference on Data Mining,",
    "year": 2001
  }, {
    "title": "Integrating classification and association rule mining",
    "authors": ["Liu", "Bing", "Hsu", "Wynne", "Ma", "Yiming"],
    "venue": "In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining (KDD), pp",
    "year": 1998
  }, {
    "title": "Learning with decision lists of data-dependent features",
    "authors": ["Marchand", "Mario", "Sokolova", "Marina"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "On the quasi-optimal solution of the general covering problem",
    "authors": ["R.S. Michalski"],
    "venue": "In Proceedings of the V International Symposium on Information Processing (FCIP",
    "year": 1969
  }, {
    "title": "Optimal constraint-based decision tree induction from itemset lattices",
    "authors": ["Nijssen", "Siegfried", "Fromont", "Elisa"],
    "venue": "Data Mining and Knowledge Discovery,",
    "year": 2010
  }, {
    "title": "Teleo-reactive programs for agent control",
    "authors": ["Nilsson", "Nils J"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 1994
  }, {
    "title": "Learning Efficient Classification Procedures and Their Application to Chess End Games, pp. 463–482",
    "authors": ["Quinlan", "J. Ross"],
    "year": 1983
  }, {
    "title": "Programs for Machine Learning",
    "authors": ["Quinlan", "J. Ross"],
    "year": 1993
  }, {
    "title": "Finding a short and accurate decision rule in disjunctive normal form by exhaustive search",
    "authors": ["Rijnbeek", "Peter R", "Kors", "Jan A"],
    "venue": "Machine Learning,",
    "year": 2010
  }, {
    "title": "Learning decision lists",
    "authors": ["Rivest", "Ronald L"],
    "venue": "Machine Learning,",
    "year": 1987
  }, {
    "title": "Learning theory analysis for association rules and sequential event prediction",
    "authors": ["Rudin", "Cynthia", "Letham", "Benjamin", "Madigan", "David"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Learning interpretable models",
    "authors": ["Rüping", "Stefan"],
    "venue": "PhD thesis, Universität Dortmund,",
    "year": 2006
  }, {
    "title": "To explain or to predict",
    "authors": ["Shmueli", "Galit"],
    "venue": "Statistical Science,",
    "year": 2010
  }, {
    "title": "Structure of association rule classifiers: a review",
    "authors": ["Vanhoof", "Koen", "Depaire", "Benoı̂t"],
    "venue": "In Proceedings of the International Conference on Intelligent Systems and Knowledge Engineering (ISKE), pp",
    "year": 2010
  }, {
    "title": "Making machine learning models interpretable",
    "authors": ["Vellido", "Alfredo", "Martı́n-Guerrero", "José D", "Lisboa", "Paulo J.G"],
    "venue": "In Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,",
    "year": 2012
  }, {
    "title": "Falling rule lists",
    "authors": ["Wang", "Fulton", "Rudin", "Cynthia"],
    "venue": "In Proceedings of Artificial Intelligence and Statistics (AISTATS),",
    "year": 2015
  }, {
    "title": "Causal falling rule lists",
    "authors": ["Wang", "Fulton", "Rudin", "Cynthia"],
    "venue": "CoRR, abs/1510.05189,",
    "year": 2015
  }, {
    "title": "Bayesian or’s of and’s for interpretable classification",
    "authors": ["Wang", "Tong", "Rudin", "Cynthia", "Doshi", "Finale", "Liu", "Yimin", "Klampfl", "Erica", "MacNeille", "Perry"],
    "venue": "In SIAM International Conference on Data Mining (ICDM),",
    "year": 2016
  }, {
    "title": "A Bayesian framework for learning rule sets for interpretable classification",
    "authors": ["Wang", "Tong", "Rudin", "Cynthia", "Doshi", "Finale", "Liu", "Yimin", "Klampfl", "Erica", "MacNeille", "Perry"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "CPAR: classification based on predictive association rules",
    "authors": ["Yin", "Xiaoxin", "Han", "Jiawei"],
    "venue": "In Proceedings of the 2003 SIAM International Conference on Data Mining (ICDM), pp",
    "year": 2003
  }, {
    "title": "Using decision lists to construct interpretable and parsimonious treatment",
    "authors": ["Zhang", "Yichi", "Laber", "Eric B", "Tsiatis", "Anastasios", "Davidian", "Marie"],
    "venue": "regimes. Biometrics,",
    "year": 2015
  }],
  "id": "SP:296291df7a2052b70e094330bce9005147d70014",
  "authors": [{
    "name": "Hongyu Yang",
    "affiliations": []
  }, {
    "name": "Cynthia Rudin",
    "affiliations": []
  }, {
    "name": "Margo Seltzer",
    "affiliations": []
  }],
  "abstractText": "We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we aim to fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees, with practical running times. The predictions in each leaf are probabilistic.",
  "title": "Scalable Bayesian Rule Lists"
}