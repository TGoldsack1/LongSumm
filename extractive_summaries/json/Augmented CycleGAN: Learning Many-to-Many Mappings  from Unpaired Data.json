{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The problem of learning mappings between domains from unpaired data has recently received increasing attention, especially in the context of image-to-image translation (Zhu et al., 2017a; Kim et al., 2017; Liu et al., 2017). This problem is important because, in some cases, paired information may be scarce or otherwise difficult to obtain. For example, consider tasks like face transfiguration (male to female), where obtaining explicit pairs would be difficult as it would require artistic authoring. An effective unsupervised model may help when learning from relatively few paired examples, as compared to training strictly from the paired examples. Intuitively, forcing inter-domain mappings to be (approximately) invertible by a model of limited capacity acts as a strong regularizer.\nMotivated by the success of Generative Adversarial Networks (GANs) in image generation (Goodfellow et al., 2014; Radford et al., 2015), existing unsupervised mapping meth-\n1Montreal Institute for Learning Algorithms (MILA), Canada. 2Microsoft Research Montreal, Canada. 3CIFAR Fellow. †Work partly done at MSR Montreal. Correspondence to: Amjad Almahairi <amjad.almahairi@umontreal.ca>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nods such as CycleGAN (Zhu et al., 2017a) learn a generator which produces images in one domain given images from the other. Without the use of pairing information, there are many possible mappings that could be inferred. To reduce the space of the possible mappings, these models are typically trained with a cycle-consistency constraint which enforces a strong connection across domains, by requiring that mapping an image from the source domain to the target domain and then back to source will result in the same starting image. This framework has been shown to learn convincing mappings across image domains and proved successful in a variety of related applications (Tung et al., 2017; Wolf et al., 2017; Hoffman et al., 2017).\nOne major limitation of CycleGAN is that it only learns one-to-one mappings, i.e. the model associates each input image with a single output image. We believe that most relationships across domains are more complex, and better characterized as many-to-many. For example, consider mapping silhouettes of shoes to images of shoes. While the mapping that CycleGAN learns can be superficially convincing (e.g. it produces a single reasonable shoe with a particular style), we would like to learn a mapping that can capture diversity of the output (e.g. produces multiple shoes with different styles). The limits of one-to-one mappings are more dramatic when the source domain and target domain substantially differ. For instance, it would be difficult to learn a CycleGAN model when the two domains are descriptive facial attributes and images of faces.\nWe propose a model for learning many-to-many mappings between domains from unpaired data. Specifically, we “augment” each domain with auxiliary latent variables and extend CycleGAN’s training procedure to the augmented spaces. The mappings in our model take as input a sample from the source domain and a latent variable, and output both a sample in the target domain and a latent variable (Fig. 1b). The learned mappings are one-to-one in the augmented space, but many-to-many in the original domains after marginalizing over the latent variables.\nOur contributions are as follows. (i) We introduce the Augmented CycleGAN model for learning many-to-many mappings across domains in an unsupervised way. (ii) We show that our model can learn mappings which produce a diverse set of outputs for each input. (iii) We show that our model can learn mappings across substantially different domains, and we apply it in a semi-supervised setting for mapping between faces and attributes with competitive results."
  }, {
    "heading": "2. Unsupervised Learning of Mappings Between Domains",
    "text": ""
  }, {
    "heading": "2.1. Problem Setting",
    "text": "Given two domains A and B, we assume there exists a mapping, potentially many-to-many, between their elements. The objective is to recover this mapping using unpaired samples from distributions pd(a) and pd(b) in each domain. This can be formulated as a conditional generative modeling task where we try to estimate the true conditionals p(a|b) and p(b|a) using samples from the true marginals. An important assumption here is that elements in domains A and B are highly dependent; otherwise, it is unlikely that the model would uncover a meaningful relationship without any pairing information."
  }, {
    "heading": "2.2. CycleGAN Model",
    "text": "The CycleGAN model (Zhu et al., 2017a) estimates these conditionals using two mappings GAB : A 7→ B and GBA : B 7→ A, parameterized by neural networks, which satisfy the following constraints:\n1. Marginal matching: The output of each mapping should match the empirical distribution of the target domain, when marginalized over the source domain.\n2. Cycle-consistency: Mapping an element from one domain to the other, and then back, should produce a sample close to the original element.\nMarginal matching in CycleGAN is achieved using the generative adversarial networks framework (GAN) (Goodfellow et al., 2014). Mappings GAB and GBA are given by neural networks trained to fool adversarial discriminators DB and\nDA, respectively. Enforcing marginal matching on target domain B, marginalized over source domain A, involves minimizing an adversarial objective with respect to GAB :\nLBGAN(GAB , DB) = E b∼pd(b)\n[ logDB(b) ] +\nE a∼pd(a)\n[ log(1−DB(GAB(a))) ] ,\n(1) while the discriminatorDB is trained to maximize it. A similar adversarial loss LAGAN(GBA, DA) is defined for marginal matching in the reverse direction.\nCycle-consistency enforces that, when starting from a sample a from A, the reconstruction a′ = GBA(GAB(a)) remains close to the original a. For image domains, closeness between a and a′ is typically measured with L1 or L2 norms. When using the L1 norm, cycle-consistency starting from A can be formulated as:\nLACYC(GAB , GBA) = E a∼pd(a) ∥∥GBA(GAB(a))− a∥∥1. (2) And similarly for cycle-consistency starting from B. The full CycleGAN objective is given by:\nLAGAN(GBA, DA) + LBGAN(GAB , DB) + γLACYC(GAB , GBA) + γLBCYC(GAB , GBA),\n(3)\nwhere γ is a hyper-parameter that balances between marginal matching and cycle-consistency.\nThe success of CycleGAN can be attributed to the complementary roles of marginal matching and cycle-consistency in its objective. Marginal matching encourages generating realistic samples in each domain. Cycle-consistency encourages a tight relationship between domains. It may also help prevent multiple items from one domain mapping to a single item from the other, analogous to the troublesome mode collapse in adversarial generators (Li et al., 2017)."
  }, {
    "heading": "2.3. Limitations of CycleGAN",
    "text": "A fundamental weakness of the CycleGAN model is that it learns deterministic mappings. In CycleGAN, and in other similar models (Kim et al., 2017; Yi et al., 2017), the conditionals between domains correspond to delta functions: p̂(a|b) = δ(GBA(b)) and p̂(b|a) = δ(GAB(a)), and cycleconsistency forces the learned mappings to be inverses of each other. When faced with complex cross-domain relationships, this results in CycleGAN learning an arbitrary one-toone mapping instead of capturing the true, structured conditional distribution more faithfully. Deterministic mappings are also an obstacle to optimizing cycle-consistency when the domains differ substantially in complexity, in which case mapping from one domain (e.g. class labels) to the other (e.g. real images) is generally one-to-many. Next, we dis-\ncuss how to extend CycleGAN to capture more expressive relationships across domains."
  }, {
    "heading": "2.4. CycleGAN with Stochastic Mappings",
    "text": "A straightforward approach for extending CycleGAN to model many-to-many relationships is to equip it with stochastic mappings between A and B. Let Z be a latent space with a standard Gaussian prior p(z) over its elements. We define mappings GAB : A × Z 7→ B and GBA : B × Z 7→ A1. Each mapping takes as input a vector of auxiliary noise and a sample from the source domain, and generates a sample in the target domain. Therefore, by sampling different z ∼ p(z), we could in principle generate multiple b’s conditioned on the same a and vice-versa. We can write the marginal matching loss on domain B as:\nLBGAN(GAB , DB) = E b∼pd(b)\n[ logDB(b) ] +\nE a∼pd(a) z∼p(z)\n[ log(1−DB(GAB(a, z))) ] .\n(4) Cycle-consistency starting from A is now given by:\nLACYC(GAB , GBA) = E a∼pd(a)\nz1,z2∼p(z) ∥∥GBA(GAB(a, z1), z2)− a∥∥1 (5)\nThe full training loss is similar to the objective in Eqn. 3. We refer to this model as Stochastic CycleGAN.\nIn principle, stochastic mappings can model multi-modal conditionals, and hence generate a richer set of outputs than deterministic mappings. However, Stochastic CycleGAN suffers from a fundamental flaw: the cycle-consistency in Eq. 5 encourages the mappings to ignore the latent z. Specifically, the unimodality assumption implicit in the reconstruction error from Eq. 5 forces the mapping GBA to be manyto-one when cycling A→ B → A′, since any b generated for a given a must map to a′ = GBA(b, z) ≈ a, for all z. For the cycle B → A → B′, GAB is similarly forced to be many-to-one. The only way for to GBA and GAB to be both many-to-one and mutual inverses is if they collapse to being (roughly) one-to-one. We could possibly mitigate this degeneracy by introducing a VAE-like encoder and exchanging the L1 error in Eq. 5 for a more complex variational bound on conditional log-likelihood. In the next section, we discuss an alternative approach to learning complex, stochastic mappings between domains."
  }, {
    "heading": "3. Approach",
    "text": "In order to learn many-to-many mappings across domains, we propose to learn to map between pairs of items (a, zb) ∈\n1To avoid clutter in notation, we reuse the same symbols of deterministic mappings.\nA× Zb and (b, za) ∈ B × Za, where Za and Zb are latent spaces that capture any missing information when transforming an element from A to B, and vice-versa. For example, when generating a female face (b ∈ B) which resembles a male face (a ∈ A), the latent code zb ∈ Zb can capture female face variations (e.g. hair length or style) independent from a. Similarly, za ∈ Za captures variations in a generated male face independent from the given female face. This approach can be described as learning mappings between augmented spaces A× Zb and B × Za (Figure 1b); hence, we call it Augmented CycleGAN. By learning to map a pair (a, zb) ∈ A × Zb to (b, za) ∈ B × Za, we can (i) learn a stochastic mapping from a to multiple items in B by sampling different zb ∈ Zb, and (ii) infer latent codes za containing information about a not captured in the generated b, which allows for doing proper reconstruction of a. As a result, we are able to optimize both marginal matching and cycle consistency while using stochastic mappings. We present details of our approach in the next sections. 2"
  }, {
    "heading": "3.1. Augmented CycleGAN",
    "text": "Our proposed model has four components. First, the two mappings GAB : A × Zb 7→ B and GBA : B × Za 7→ A, which are the conditional generators of items in each domain. These models are similar to those used in Stochastic CycleGAN. We also have two encoders EA : A×B 7→ Za and EB : A × B 7→ Zb, which enable optimization of cycle-consistency with stochastic, structured mappings. All components are parameterized with neural networks – see Fig. 2. We define mappings over augmented spaces in our model as follows. Let p(za) and p(zb) be standard Gaussian priors over Za and Zb, which are independent from pd(b) and pd(a). Given a pair (a, zb) ∼ pd(a)p(zb), we generate a pair (b̃, z̃a) as follows:\nb̃ = GAB(a, zb), z̃a = EA(a, b̃). (6)\nThat is, we first generate a sample in domain B, then we use it along with a to generate latent code z̃a. Note here that by sampling different zb ∼ p(zb), we can generate multiple b̃’s conditioned on the same a. In addition, given the pair (a, b̃), we can recover information about a which is not captured in b̃, via z̃a. Similarly, given a pair (b, za) ∼ pd(b)p(za), we generate a pair (ã, z̃b) as follows:\nã = GBA(b, za), z̃b = EB(b, ã). (7)\nLearning in Augmented CycleGAN follows a similar approach to CycleGAN – optimizing both marginal matching and cycle-consistency losses, albeit over augmented spaces.\n2Our model captures many-to-many relationships because it captures both one-to-many and many-to-one: one item in A maps to many items in B, and many items in B map to one item in A (cycle). The same is true in the other direction.\nMarginal Matching Loss We adopt an adversarial approach for marginal matching over B × Za where we use two independent discriminators DB and DZa to match generated pairs to real samples from the independent priors pd(b) and p(za), respectively. Marginal matching loss over B is defined as in Eqn 4. Marginal matching over Za is given by:\nLZaGAN(EA, GAB , DZa) = E za∼p(za)\n[ logDZa(za) ] +\nE a∼pd(a) zb∼p(zb)\n[ log(1−DZa(z̃a)) ] ,\n(8)\nwhere z̃a is defined by Eqn 6. As in CycleGAN, the goal of marginal matching over B is to insure that generated samples b̃ are realistic. For latent codes z̃a, marginal matching acts as a regularizer for the encoder, encouraging the marginalized encoding distribution to match a simple prior p(za). This is similar to adversarial regularization of latent codes in adversarial autoencoders (Makhzani et al., 2016). We define similar losses LAGAN(GBA, DA) and LZbGAN(EB , GBA, DZb) for marginal matching over A×Zb.\nCycle Consistency Loss We define two cycle-consistency constraints in Augmented CycleGAN starting from each of the two augmented spaces, as shown in Fig. 2. In cycleconsistency starting from A × Zb, we ensure that given a pair (a, zb) ∼ pd(a)p(zb), the model is able to produce a faithful reconstruction of it after being mapped to (b̃, z̃a). This is achieved with two losses; first for reconstructing a ∼ pd(a):\nLACYC(GAB , GBA, EA) = E a∼pd(a) zb∼p(zb)\n∥∥a′ − a∥∥ 1 ,\nb̃ = GAB(a, zb), z̃a = EA(a, b̃), a ′ = GBA(b̃, z̃a). (9)\nThe second is for reconstructing zb ∼ p(zb):\nLZbCYC(GAB , EB) = E a∼pd(a) zb∼p(zb) ∥∥z′b − zb∥∥1, z′b = EB(a, b̃), b̃ = GAB(a, zb). (10)\nThese reconstruction costs represent an autoregressive decomposition of the basic CycleGAN cycle-consistency cost from Eq. 2, after extending it to the augmented domains. Specifically, we decompose the required reconstruction distribution p(b, za|a, zb) into the conditionals p(b|a, zb) and p(za|a, zb, b).\nJust like in CycleGAN, the cycle loss in Eqn. 9 enforces the dependency of generated samples in B on samples of A. Thanks to the encoder EA, the model is able to reconstruct a because it can recover information loss in generated b̃ through z̃a. On the other hand, the cycle loss in Eqn. 10 enforces the dependency of a generated sample b̃ on the given latent code zb. In effect, it increases the mutual information between zb and b conditioned on a, i.e. I(b, zb|a) (Chen et al., 2016; Li et al., 2017).\nTraining Augmented CycleGAN in the direction A× Zb to B × Za is done by optimizing:\nLBGAN(DB , GAB) + L za GAN(DZa , EA, GAB) + γ1LACYC(GAB , GBA, EA) + γ2L zb CYC(GAB , EB), (11)\nwhere γ1 and γ2 are a hyper-parameters used to balance objectives. We define a similar objective for the direction going from B × Za to A× Zb, and train the model on both objectives simultaneously."
  }, {
    "heading": "3.2. Semi-supervised Learning with Augmented CycleGAN",
    "text": "In cases where we have access to paired data, we can leverage it to train our model in a semi-supervised setting (Fig. 3). Given pairs sampled from the true joint,\ni.e. (a, b) ∼ pd(a, b), we can define a supervision cost for the mapping GAB as follows:\nLASUP(GBA, EA) = E (a,b)∼pd(a,b) ∥∥GBA(b, z̃a)− a∥∥1, (12)\nwhere z̃a = EA(a, b) infers a latent code which can produce a given b via GBA(b, z̃a). We also apply an adversarial regularization cost on the encoder, in the form of Eqn. 8. Similar supervision and regularization costs can be defined for GBA and EB , respectively."
  }, {
    "heading": "3.3. Modeling Stochastic Mappings",
    "text": "We note here some design choices that we found important for training our stochastic mappings. We discuss architectural and training details further in Sec. 5. In order to allow the latent codes to capture diversity in generated samples, we found it important to inject latent codes to layers of the network which are closer to the inputs. This allows the injected codes to be processed with a larger number of remaining layers and therefore capture high-level variations of the output, as opposed to small pixel-level variations. We also found that Conditional Normalization (CN) (Dumoulin et al.; Perez et al., 2017) for conditioning layers can be more effective than concatenation, which is more commonly used (Radford et al., 2015; Zhu et al., 2017b). The basic idea of CN is to replace parameters of affine transformations in normalization layers (Ioffe & Szegedy, 2015) of a neural network with a learned function of the conditioning information. We apply CN by learning two linear functions f and g which take a latent code z as input and output scale and shift parameters of normalization layers in intermediate layers, i.e. γ = f(z) and β = g(z). When activations are normalized over spatial dimensions only, we get Conditional Instance Normalization (CIN), and when they are also normalized over batch dimension, we get Conditional Batch Normalization (CBN)."
  }, {
    "heading": "4. Related Work",
    "text": "There has been a surge of interest recently in unsupervised learning of cross-domain mappings, especially for image translation tasks. Previous attempts for image-to-image translation have unanimously relied on GANs to learn mappings that produce compelling images. In order to constrain learned mappings, some methods have relied on cycleconsistency based constraints similar to CycleGAN (Kim et al., 2017; Yi et al., 2017; Royer et al., 2017), while others relied on weight sharing constraints (Liu & Tuzel, 2016; Liu et al., 2017). However, the focus in all of these methods was on learning conditional image generators that produce single output images given the input image. Notably, Liu et al. (2015) propose to map inputs from both domains into a\nshared latent space. This approach may constrain too much the space of learnable mappings, for example in cases where the domains differ substantially (class labels and images).\nUnsupervised learning of mappings have also been addressed recently in language translation, especially for machine translation (Lample et al., 2017) and text style transfer (Shen et al., 2017). These methods also rely on some notion of cycle-consistency over domains in order to constrain the learned mappings. They rely heavily on the power of the RNN-based decoders to capture complex relationships across domains while we propose to use auxiliary latent variables. The two approaches may be synergistic, as it was recently suggested in (Gulrajani et al., 2016).\nRecently, Zhu et al. (2017b) proposed the BiCycleGAN model for learning multi-modal mappings but in fully supervised setting. This model extends the pix2pix framework in (Isola et al., 2017) by learning a stochastic mapping from the source to the target, and shows interesting diversity in the generated samples. Several modeling choices in BiCycleGAN resemble our proposed model, including the use of stochastic mappings and an encoder to handle multi-modal targets. However, our approach focuses on unsupervised many-to-many mappings, which allows it to handle domains with no or very little paired data."
  }, {
    "heading": "5. Experiments",
    "text": ""
  }, {
    "heading": "5.1. Edges-to-Photos",
    "text": "We first study a one-to-many image translation task between edges (domain A) and photos of shoes (domain B).3 Training data is composed of almost 50K shoe images with corresponding edges (Yu & Grauman, 2014; Zhu et al., 2016; Isola et al., 2017), but as in previous approaches (e.g. (Kim et al., 2017)), we assume no pairing information while training unsupervised models. Stochastic mappings in our Augmented CycleGAN (AugCGAN) model are based on ResNet conditional image generators of (Zhu et al., 2017a), where we inject noise with CIN to all intermediate layers. As baselines, we train: CycleGAN, Stochastic CycleGAN (StochCGAN) and Triangle-GAN (∆-GAN) of (Gan et al., 2017) which share the same architectures and training procedure for fair comparison. 4\nQuantitative Results First, we evaluate conditionals learned by each model by measuring the ability of the model of generating a specific edge-shoe pair from a test set. We follow the same evaluation methodology adopted in (Metz et al., 2016; Xiang & Li, 2017), which opt for an\n3 Public code available at: https://github.com/ aalmah/augmented_cyclegan\n4∆-GAN architecture differs only in the two discriminators, which match conditionals/joints instead of marginals.\ninference-via-optimization approach to estimate the reconstruction error of a specific shoe given an edge. Specifically, given a trained model with mapping GAB and an edgeshoe pair (a, b) in the test set, we solve the optimization task z∗b = arg minzb ‖GAB(a, zb)−b‖1 and compute reconstruction error ‖GAB(a, z∗b ) − b‖1. Optimization is done with RMSProp as in (Xiang & Li, 2017). We show the average errors over a predefined test set of 200 samples in Table 1 for: AugCGAN (unsupervised and semi-supervised with 10% paired data), unsupervised CycleGAN and StochCGAN, and a semi-supervised ∆-GAN, all sharing the same architecture. Our unsupervised AugCGAN model outperforms all baselines including semi-supervised ∆-GAN, which indicates that reconstruction-based cycle-consistency is more effective in learning conditionals than the adversarial approach of ∆-GAN. As expected, adding 10% supervision to AugCGAN improves shoe predictions further. In addition, we evaluate edge predictions given real shoes from test set as well. We report mean squared error (MSE) similar to (Gan et al., 2017), where we normalize over all edge pixels. The ∆-GAN model with our architecture outperforms\nthe one reported in (Gan et al., 2017), but is outperformed by our unsupervised AugCGAN model. Again, adding 10% supervision to AugCGAN reduces MSE even further.\nQualitative Results We qualitatively compare the mappings learned by our model AugCGAN and StochCGAN. Fig. 6 shows generated images of shoes given an edge a ∼ pd(a) (row) and zb ∼ p(zb) (column) from both model, and Fig. 5 shows cycles starting from edges and shoes. Note that here the edges are sampled from the data distribution and not produced by the learnt stochastic mapping GBA. In this case, both models can (i) generate diverse set of shoes with color variations mostly defined by zb, and (ii) perform reconstructions of both edges and shoes.\nWhile we expect our model to achieve these results, the fact that StochCGAN can reconstruct shoes perfectly without an inference model may seem at first surprising. However, this can be explained by the “steganography” behavior of CycleGAN (Chu et al., 2017): the model hides in the generated edge ã imperceptible information about a given shoe b (e.g. its color), in order to satisfy cycle-consistency without being\npenalized by the discriminator on A. A good model of the true conditionals p(b|a), p(a|b) should reproduce the hidden joint distribution and consequently the marginals by alternatively sampling from conditionals. Therefore, we examine the behavior of the models when edges are generated from the model itself (instead of the empirical data distribution). In Fig. 7, we plot multiple generated shoes given an edge generated by the model, i.e. ã, and 5 different zb sampled from p(zb). In StochCGAN, the mapping GBA(ã, zb) collapses to a deterministic function generating a single shoe for every zb. This distinction between behaviour on real and synthetic data is undesirable, e.g. regularization benefits of using unpaired data may be reduced if the model slips into this regime switching style. In AugCGAN, on the other hand, the mapping seem to closely capture the diversity in the conditional distribution of shoes given edges. Furthermore, in Fig. 8, we run a Markov chain by generating from the learned mappings multiple times, starting from a real shoe. Again AugCGAN produces diverse samples while StochCGAN seems to collapse to a single mode.\nWe investigate “steganography” behavior in both AugCGAN and StochCGAN using a similar approach to (Chu et al., 2017), where we corrupt generated edges with noise sampled fromN (0, 2), and compute reconstruction error of shoes. Fig. 4 shows L1 reconstruction error as we increase . AugCGAN seems more robust to corruption of edges than in StochCGAN, which confirms that information is being stored in the latent codes instead of being completely hidden in generated edges."
  }, {
    "heading": "5.2. Male-to-Female",
    "text": "We study another image translation task of translating between male and female faces. Data is based on CelebA dataset (Liu et al., 2015) where we split it into two separate domains using provided attributes. Several key features distinguish this task from other image-translation tasks: (i) there is no predefined correspondence in real data of each domain, (ii) the relationship is many-to-many between domains, as we can map a male to female face, and vice-versa, in many possible ways, and (iii) capturing realistic variations in generated faces requires transformations that go beyond simple color and texture changes. The architecture of stochastic mappings are based on U-NET conditional image generators of (Isola et al., 2017), and again with noise injected to all intermediate layers. Fig. 9 shows results of applying our model to this task on 128 × 128 resolution CelebA images. We can see that our model depicts meaningful variations in generated faces without compromising their realistic appearance. In Fig. 10 we show 64 × 64 generated samples in both domains from our model ((a) and (b)), and compare them to both: (c) our model but with noise injected noise only in last 3 layers of the GAB’s\nnetwork, and (d) StochCGAN with the same architecture. We can see that in Fig. 10-(c) variations are very limited, which highlights the importance of processing latent code with multiple layers. StochCGAN in this task produces almost no variations at all, which highlights the importance of proper optimization of cycle-consistency for capturing meaningful variations. We verify these results quantitatively using LPIPS distance (Zhang et al., 2018), where we average distance between 1000 pairs of generated female faces (10 random pairs from 100 male faces). AugCGAN (Fig. 10-(b)) achieves highest LPIPS diversity score with 0.108 ± 0.003, while AugCGAN with z in low-level layers (Fig. 10-(c)) gets 0.059 +/- 0.001, and finally StochCGAN (Fig. 10-(d)) gets 0.008 +/- 0.000, i.e. severe mode collapse."
  }, {
    "heading": "5.3. Attributes-to-Faces",
    "text": "In this task, we make use of the CelebA dataset in order map from descriptive facial attributes A to images of faces B and vice-versa. We report both quantitative and qualitative results. For the quantitative results, we follow (Gan et al.,\n2017) and test our models in a semi-supervised attribute prediction setting. We let the model train on all the available data without the pairing information and only train with a small amount of paired data as described in Sec. 3.2. We report Precision (P) and normalized Discounted Cumulative Gain (nDCG) as the two metrics for multi-label classification problems. As an additional baseline, we also train a supervised classifier (which has the same architecture as GBA) on the paired subset. The results are reported in Table 3. In Fig. 11, we show some generation obtained from the model in the direction attributes to faces. We can see that the model generates reasonable diverse faces for the same set of attributes."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper we have introduced the Augmented CycleGAN model for learning many-to-many cross-domain mappings in unsupervised fashion. This model can learn stochastic mappings which leverage auxiliary noise to capture multimodal conditionals. Our experimental results verify quantitatively and qualitatively the effectiveness of our approach in image translation tasks. Furthermore, we apply our model in a challenging task of learning to map across attributes and faces, and show that it can be used effectively in a semi-supervised learning setting."
  }, {
    "heading": "Acknowledgements",
    "text": "Authors would like to thank Zihang Dai for valuable discussions and feedback. We are also grateful for ICML anonymous reviewers for their comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
    "authors": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Cyclegan: a master of steganography",
    "authors": ["C. Chu", "A. Zhmoginov", "M. Sandler"],
    "venue": "arXiv preprint arXiv:1712.02950,",
    "year": 2017
  }, {
    "title": "Triangle generative adversarial networks",
    "authors": ["Z. Gan", "L. Chen", "W. Wang", "Y. Pu", "Y. Zhang", "H. Liu", "C. Li", "L. Carin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Pixelvae: A latent variable model for natural images",
    "authors": ["I. Gulrajani", "K. Kumar", "F. Ahmed", "A.A. Taiga", "F. Visin", "D. Vazquez", "A. Courville"],
    "venue": "arXiv preprint arXiv:1611.05013,",
    "year": 2016
  }, {
    "title": "Cycada: Cycleconsistent adversarial domain adaptation",
    "authors": ["J. Hoffman", "E. Tzeng", "T. Park", "Zhu", "J.-Y", "P. Isola", "K. Saenko", "A.A. Efros", "T. Darrell"],
    "venue": "arXiv preprint arXiv:1711.03213,",
    "year": 2017
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International conference on machine learning,",
    "year": 2015
  }, {
    "title": "Image-toimage translation with conditional adversarial networks",
    "authors": ["P. Isola", "Zhu", "J.-Y", "T. Zhou", "A.A. Efros"],
    "venue": "arXiv preprint,",
    "year": 2017
  }, {
    "title": "Learning to discover cross-domain relations with generative adversarial networks",
    "authors": ["T. Kim", "M. Cha", "H. Kim", "J. Lee", "J. Kim"],
    "venue": "arXiv preprint arXiv:1703.05192,",
    "year": 2017
  }, {
    "title": "Unsupervised machine translation using monolingual corpora only",
    "authors": ["G. Lample", "L. Denoyer", "M. Ranzato"],
    "venue": "arXiv preprint arXiv:1711.00043,",
    "year": 2017
  }, {
    "title": "Towards understanding adversarial learning for joint distribution matching",
    "authors": ["C. Li", "H. Liu", "C. Chen", "Y. Pu", "L. Chen", "R. Henao", "Carin", "L. Alice"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Coupled generative adversarial networks",
    "authors": ["Liu", "M.-Y", "O. Tuzel"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2016
  }, {
    "title": "Unsupervised imageto-image translation networks",
    "authors": ["Liu", "M.-Y", "T. Breuel", "J. Kautz"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Deep learning face attributes in the wild",
    "authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"],
    "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
    "year": 2015
  }, {
    "title": "Unrolled generative adversarial networks",
    "authors": ["L. Metz", "B. Poole", "D. Pfau", "J. Sohl-Dickstein"],
    "venue": "arXiv preprint arXiv:1611.02163,",
    "year": 2016
  }, {
    "title": "Film: Visual reasoning with a general conditioning layer",
    "authors": ["E. Perez", "F. Strub", "H. De Vries", "V. Dumoulin", "A. Courville"],
    "venue": "arXiv preprint arXiv:1709.07871,",
    "year": 2017
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "venue": "arXiv preprint arXiv:1511.06434,",
    "year": 2015
  }, {
    "title": "Xgan: Unsupervised imageto-image translation for many-to-many mappings",
    "authors": ["A. Royer", "K. Bousmalis", "S. Gouws", "F. Bertsch", "I. Moressi", "F. Cole", "K. Murphy"],
    "venue": "arXiv preprint arXiv:1711.05139,",
    "year": 2017
  }, {
    "title": "Style transfer from non-parallel text by cross-alignment",
    "authors": ["T. Shen", "T. Lei", "R. Barzilay", "T. Jaakkola"],
    "venue": "arXiv preprint arXiv:1705.09655,",
    "year": 2017
  }, {
    "title": "Adversarial inverse graphics networks: Learning 2d-to3d lifting and image-to-image translation from unpaired supervision",
    "authors": ["Tung", "H.-Y. F", "A.W. Harley", "W. Seto", "K. Fragkiadaki"],
    "venue": "In The IEEE International Conference on Computer Vision (ICCV),",
    "year": 2017
  }, {
    "title": "Unsupervised creation of parameterized avatars",
    "authors": ["L. Wolf", "Y. Taigman", "A. Polyak"],
    "venue": "arXiv preprint arXiv:1704.05693,",
    "year": 2017
  }, {
    "title": "On the effects of batch and weight normalization in generative adversarial",
    "authors": ["S. Xiang", "H. Li"],
    "venue": "networks. stat,",
    "year": 2017
  }, {
    "title": "Dualgan: Unsupervised dual learning for image-to-image translation",
    "authors": ["Z. Yi", "H. Zhang", "Gong", "P. T"],
    "venue": "arXiv preprint arXiv:1704.02510,",
    "year": 2017
  }, {
    "title": "Fine-grained visual comparisons with local learning",
    "authors": ["A. Yu", "K. Grauman"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2014
  }, {
    "title": "The unreasonable effectiveness of deep networks as a perceptual metric",
    "authors": ["R. Zhang", "P. Isola", "A.A. Efros", "E. Shechtman", "O. Wang"],
    "year": 2018
  }, {
    "title": "Generative visual manipulation on the natural image manifold",
    "authors": ["Zhu", "J.-Y", "P. Krähenbühl", "E. Shechtman", "A.A. Efros"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
    "authors": ["Zhu", "J.-Y", "T. Park", "P. Isola", "A.A. Efros"],
    "venue": "arXiv preprint arXiv:1703.10593,",
    "year": 2017
  }, {
    "title": "Toward multimodal imageto-image translation",
    "authors": ["Zhu", "J.-Y", "R. Zhang", "D. Pathak", "T. Darrell", "A.A. Efros", "O. Wang", "E. Shechtman"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }],
  "id": "SP:4a45b8f8decc178305af06d758ac7428a9070fad",
  "authors": [{
    "name": "Amjad Almahairi",
    "affiliations": []
  }, {
    "name": "Sai Rajeswar",
    "affiliations": []
  }, {
    "name": "Alessandro Sordoni",
    "affiliations": []
  }, {
    "name": "Philip Bachman",
    "affiliations": []
  }, {
    "name": "Aaron Courville",
    "affiliations": []
  }],
  "abstractText": "Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.",
  "title": "Augmented CycleGAN: Learning Many-to-Many Mappings  from Unpaired Data"
}