{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1853–1862 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1853"
  }, {
    "heading": "1 Introduction",
    "text": "Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010). Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954). A representative method is skip-gram (SG) (Mikolov et al., 2013a,b), which realizes the hypothesis using a\n∗The first two authors contributed equally to this paper and share the first-authorship.\nshallow neural network model. The other family of methods is count-based, such as GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016a,b), which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.\nBy far, most state-of-the-art embedding methods rely on SGD and negative sampling for optimization. However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples (Chen et al., 2018; Yuan et al., 2016), as shown in Figure 1. Essentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken. Moreover, SGD exhibits dramatic fluctuation and suffers from overshooting on local minimums (Ruder, 2016). These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.\nTo address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning. In contrast to SGD, batch learning does not involve any sampling procedure and computes the gradient over all training samples. As such, it can easily converge to a better optimum in a more stable way. Nevertheless, a well-known\ndifficulty in applying full batch learning lies in the expensive computational cost for large-scale data. Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size. This high complexity is unaffordable in practice, since |V |2 can easily reach billion level or even higher.\nIn this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning. To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embedding, which allows fast optimization with memorization strategies. Specifically, the acceleration is achieved by reformulating the expensive loss over all negative samples using a partition and a decouple operation. By decoupling and caching the bottleneck terms, we succeed to use all samples for each parameter update in a manageable time complexity which is mainly determined by the positive samples. The main contributions of this work are summarized as follows:\n• We present a fine-grained weighted least square loss for learning word embeddings. Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.\n• We propose an efficient and exact optimization algorithm based on full batch gradient optimization. It has a comparable time complexity with SGD, but being more effective and stable due to the consideration of all samples in each parameter update.\n• We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Skip-gram with Negative Sampling",
    "text": "Mikolov et al. (2013a,b) proposed the skip-gram model to learn word embeddings. SG formulates the problem as a predictive task, aiming at predicting the proper context c for a target word w within a local window. To speed up the training process, it applies the negative sampling (Mikolov et al., 2013b) to approximate the full softmax. That is,\neach positive (w, c) pair is trained with n randomly sampled negative pairs (w,wi). The sampled loss function of SG is defined as\nLSGwc =log σ(UwŨ T c )+ n∑ i=1 Ewi∼Pn(w) log σ(−UwŨ T wi)\nwhere Uw and Ũc denote the k-dimensional embedding vectors for word w and context c. Pn(w) is the distribution from which negative context wi is sampled.\nPlenty of research has been done based on SG, such as the use of prior knowledge from another source (Kumar and Araki, 2016; Liu et al., 2015a; Bollegala et al., 2016), incorporating word type information (Cao and Lu, 2017; Niu et al., 2017), character level n-gram models (Bojanowski et al., 2016; Joulin et al., 2016) and jointly learning with topic models like LDA (Shi et al., 2017; Liu et al., 2015b)."
  }, {
    "heading": "2.2 Importance of the Sampling Distribution",
    "text": "Mikolov et al. (2013b) showed that the unigram distribution raised to the 3/4th power as Pn(w) significantly outperformed both the unigram and the uniform distribution. This suggests that the sampling distribution (of negative words) has a great impact on the embedding quality. Furthermore, Chen et al. (2018) and Guo et al. (2018) recently found that replacing the original sampler with adaptive samplers could result in better performance. The adaptive samplers are used to find more informative negative examples during the training process. Compared with the original word-frequency based sampler, adaptive samplers adapt to both the target word and the current state of the model. They also showed that the finegrained samplers not only speeded up the convergence but also significantly improved the embedding quality. Similar observations were also found in other fields like collaborative filtering (Yuan et al., 2016). While being effective, it is proven that negative sampling is a biased approximation and does not converges to the same loss as the full softmax — regardless of how many update steps have been taken (Bengio and Senécal, 2008; Blanc and Rendle, 2017)."
  }, {
    "heading": "2.3 Count-based Embedding Methods",
    "text": "Another line of research is the count-based embedding, such as GloVe (Pennington et al., 2014). GloVe performs a biased MF on the word-context co-occurrence statistics, which is a common ap-\nproach in the field of collaborative filtering (Koren, 2008). However, GloVe only formulates the loss on positive entries of the co-occurrence matrix, meaning that negative signals about wordcontext co-occurrence are discarded. A remedy solution is LexVec (Salle et al., 2016a,b) which integrates negative sampling into MF. Some other methods (Li et al., 2015; Stratos et al., 2015; Ailem et al., 2017) also use MF to approximate the word-context co-occurrence statistics. Although predictive models and count-based models seem different at first glance, Levy and Goldberg (2014) proved that SG with negative sampling is implicitly factorizing a shifted pointwise mutual information (PMI) matrix, which means that the two families of embedding models resemble each other to a certain degree.\nOur proposed method departs from all above methods by using the full batch gradient optimizer to learn from all (positive and negative) samples. We propose a fast learning algorithm to show that such batch learning is not “heavy” even with tens of billions of training examples."
  }, {
    "heading": "3 AllVec Loss",
    "text": "In this work, we adopt the regression loss that is commonly used in count-based models (Pennington et al., 2014; Stratos et al., 2015; Ailem et al., 2017) to perform matrix factorization on word cooccurrence statistics. As highlighted, to retain the modeling fidelity, AllVec eschews using any sampling but optimizes the loss on all positive and negative word-context pairs.\nGiven a word w and a symmetric window of win contexts, the set of positive contexts can be obtained by sliding through the corpus. Let c denote a specific context, Mwc be the number of cooccurred (w, c) pairs in the corpus within the window. Mwc=0 means that the pair (w, c) has never been observed, i.e. the negative signal. rwc is the association coefficient between w and c, which is calculated from Mwc. Specifically, we use r+wc to denote the ground truth value for positive (w, c) pairs and a constant value r−(e.g., 0 or -1) for negative ones since there is no interaction between w and c in negative pairs. Finally, with all positive and negative pairs considered, a regular loss function can be given as Eq.(1), where V is the vocabulary and S is the set of positive pairs. α+wc and α−wc represent the weight for positive and negative\n(w, c) pairs, respectively. L = ∑\n(w,c)∈S α+wc(r + wc − UwŨTc )2︸ ︷︷ ︸\nLP + ∑ (w,c)∈(V×V )\\S\nα−wc(r − − UwŨTc )2︸ ︷︷ ︸\nLN\n(1)\nWhen it comes to r+wc, there are several choices. For example, GloVe applies the log of Mwc with bias terms for w and c. However, research from Levy and Goldberg (2014) showed that the SG model with negative sampling implicitly factorizes a shifted PMI matrix. The PMI value for a (w, c) pair can be defined as\nPMIwc = log P (w, c)\nP (w)P (c) = log MwcM∗∗ Mw∗M∗c (2)\nwhere ‘*’ denotes the summation of all corresponding indexes (e.g., Mw∗= ∑ c∈V Mwc). Inspired by this connection, we set r+wc as the positive point-wise mutual information (PPMI) which has been commonly used in the NLP literature (Stratos et al., 2015; Levy and Goldberg, 2014). Sepcifically, PPMI is the positive version of PMI by setting the negative values to zero. Finally, r+wc is defined as\nr+wc = PPMIwc = max(PMIwc, 0) (3)"
  }, {
    "heading": "3.1 Weighting Strategies",
    "text": "Regarding α+wc, we follow the design in GloVe, where it is defined as\nα+wc =\n{ (Mwc/xmax) ρ Mwc < xmax\n1 Mwc ≥ xmax (4)\nAs for the weight for negative instances α−wc, considering that there is no interaction between w and negative c, we set α−wc as α − c (or α − w), which means that the weight is determined by the word itself rather than the word-context interaction. Note that either α−wc = α − c or α − wc = α − w does not influence the complexity of AllVec learning algorithm described in the next section. The design of α−c is inspired by the frequency-based oversampling scheme in skip-gram and missing data reweighting in recommendation (He et al., 2016). The intuition is that a word with high frequency is more likely to be a true negative context word if there is no observed word-context interactions. Hence, to effectively differentiate the positive and negative examples, we assign a higher weight for the negative examples that have a higher word fre-\nquency, and a smaller weight for infrequent words. Formally, α−wc is defined as\nα−wc = α − c = α0 M δ∗c∑ c∈V M δ ∗c\n(5)\nwhere α0 can be seen as a global weight to control the overall importance of negative samples. α0 = 0 means that no negative information is utilized in the training. The exponent δ is used for smoothing the weights. Specially, δ = 0 means a uniform weight for all negative examples and δ = 1 means that no smoothing is applied."
  }, {
    "heading": "4 Fast Batch Gradient Optimization",
    "text": "Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1). In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product."
  }, {
    "heading": "4.1 Loss Partition",
    "text": "As can be seen, the major computational cost in Eq.(1) lies in the term LN , because the size of (V×V ) \\S is very huge, which typically contains over billions of negative examples. To this end, we show our first key design that separates the loss of negative samples into the difference between the loss on all samples and that on positive samples1. The loss partition serves as the prerequisite for the efficient computation of full batch gradients.\nLN= ∑ w∈V ∑ c∈V α−c (r −−UwŨTc )2− ∑ (w,c)∈S α−c (r −− UwŨTc )2 (6)\nBy replacing LN in Eq.(1) with Eq.(6), we can obtain a new loss function with a more clear structure. We further simplify the loss function by merging the terms on positive examples. Finally, we achieve a reformulated loss\nL = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2\n︸ ︷︷ ︸ LA\n+ ∑\n(w,c)∈S\n(α+wc − α−c )(∆− UwŨTc ) 2\n︸ ︷︷ ︸ L\nP ′\n+C (7)\nwhere ∆ = (α+wcr + wc − α−c r−)/(α+wc − α−c ). It can be seen that the new loss function consists of two components: the loss LA on the whole V ×V training examples and LP ′ on positive examples. The major computation now lies in LA which has\n1The idea here is similar to that used in (He et al., 2016; Li et al., 2016) for a different problem.\na time complexity of O(k|V |2). In the following, we show how to reduce the huge volume of computation by a simple mathematical decouple."
  }, {
    "heading": "4.2 Decouple",
    "text": "To clearly show the decouple operation, we rewrite LA as L̃A by omitting the constant term α−c (r\n−)2. Note that uwd and ũcd denote the d-th element in Uw and Ũc, respectively.\nL̃A = ∑ w∈V ∑ c∈V α−c k∑ d=0 uwdũcd k∑ d′=0 uwd′ ũcd′\n− 2r− ∑ w∈V ∑ c∈V α−c k∑ d=0 uwdũcd\n(8)\nNow we show our second key design that is based on a decouple manipulation for the inner product operation. Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.\nL̃A = k∑ d=0 k∑ d′=0 ∑ w∈V uwduwd′ ∑ c∈V α−c ũcdũcd′\n− 2r− k∑\nd=0 ∑ w∈V uwd ∑ c∈V α−c ũcd\n(9)\nAn important feature in Eq.(9) is that the original inner product terms are disappeared, while in the new equation ∑ c∈V α − c ũcdũcd′ and ∑ c∈V α − c ũcd are “constant” values relative to uwduwd′ and uwd respectively. This means that they can be pre-calculated before training in each iteration. Specifically, we define pwdd′ , p c dd′ , q w d and q c d as the pre-calculated terms\npwdd′ = ∑ w∈V uwduwd′ q w d = ∑ w∈V uwd\npcdd′ = ∑ c∈V α−c ũcdũcd′ q c d = ∑ c∈V α−c ũcd (10) Then the computation of L̃A can be simplified to∑k d=0 ∑k d′=0 p w dd′p c dd′ − 2r−qwd qcd.\nIt can be seen that the time complexity to compute all pwdd′ is O(|V |k2), and similarly, O(|V |k2) for pcdd′ andO(|V |k) for qwd and qcd. With all terms pre-calculated before each iteration, the time complexity of computing L̃A is justO(k2). As a result, the total time complexity of computing LA is decreased toO(2|V |k2+2|V |k+k2) ≈ O(2|V |k2), which is much smaller than the originalO(k|V |2). Moreover, it’s worth noting that our efficient computation for L̃A is strictly equal to its original value, which means AllVec does not introduce any approximation in evaluating the loss function.\nFinally, we can derive the batch gradients for\nuwd and ũcd as ∂L\n∂uwd = k∑ d′=0 uwd′p c dd′ − ∑ c∈I+w Λ · ũcd − r−qcd\n∂L\n∂ũcd = k∑ d′=0 ũcd′p w dd′α − c− ∑ w∈I+c Λ · uwd − r−α−c qwd (11) where I+w denotes the set of positive contexts for w, I+c denotes the set of positive words for c and Λ = (α+wc−α−c )(∆−UwŨTc ). Algorithm 1 shows the training procedure of AllVec.\nAlgorithm 1 AllVec learning Input: corpus Γ, win, α0, δ, iter, learning rate η Output: embedding matrices U and Ũ\n1: Build vocabulary V from Γ 2: Obtain all positive (w, c) and Mwc from Γ 3: Compute all r+wc, α + wc and α − c 4: Initialize U and Ũ 5: for i = 1, ..., iter do 6: for d ∈ {0, .., k} do 7: Compute and store qcd .O(|V |k) 8: for d′ ∈ {0, .., k} do 9: Compute and store pcdd′ .O(|V |k 2) 10: end for 11: end for 12: for w ∈ V do 13: Compute Λ .O(|S|k) 14: for d ∈ {0, .., k} do 15: Update uwd .O(|S|k + |V |k2) 16: end for 17: end for 18: Repeat 6-17 for ũcd .O(2|S|k+2|V |k2) 19: end for"
  }, {
    "heading": "4.3 Time Complexity Analysis",
    "text": "In the following, we show that AllVec can achieve the same time complexity with negative sampling based SGD methods.\nGiven the sample size n, the total time complexity for SG is O((n + 1)|S|k), where n + 1 denotes n negative samples and 1 positive example. Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).\nFor the ease of discussion, we denote c as the average number of positive contexts for a word in the training corpus, i.e. |S| = c|V | (c ≥ 1000 in most cases). We then obtain the ratio\n4|S|k + 4|V |k2\n(n+ 1)|S|k =\n4\nn+ 1 (1 +\nk c ) (12)\nwhere k is typically set from 100 to 300 (Mikolov et al., 2013a; Pennington et al., 2014), resulting in k ≤ c. Hence, we can give the lower and upper bound for the ratio:\n4\nn+1 <\n4|S|k+4|V |k2\n(n+1)|S|k = 4 n+1 (1+ k c )≤ 8 n+1 (13)\nThe above analysis suggests that the complexity of AllVec is same as that of SGD with negative sample size between 3 and 7. In fact, considering that c is much larger than k in most datasets, the major cost of AllVec comes from the part 4|S|k (see Section 5.4 for details), which is linear with respect to the number of positive samples."
  }, {
    "heading": "5 Experiments",
    "text": "We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).\nWord analogy task. The task aims to answer questions like, “a is to b as c is to ?”. We adopt the Google testbed2 which contains 19, 544 such questions in two categories: semantic and syntactic. The semantic questions are usually analogies about people or locations, like “king is to man as queen is to ?”, while the syntactic questions focus on forms or tenses, e.g., “swimming is to swim as running to ?”.\nWord similarity tasks. We perform evaluation on six datasets, including MEN (Bruni et al., 2012), MC (Miller and Charles, 1991), RW (Luong et al., 2013), RG (Rubenstein and Goodenough, 1965), WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001). We compute the spearman rank correlation between the similarity scores calculated based on the trained embeddings and human labeled scores.\nQVEC. QVEC is an intrinsic evaluation metric of word embeddings based on the alignment to features extracted from manually crafted lexical resources. QVEC has shown strong correlation with the performance of embeddings in several semantic tasks (Tsvetkov et al., 2015).\nWe compare AllVec with the following word embedding methods.\n• SG: This is the original skip-gram model with SGD and negative sampling (Mikolov et al., 2013a,b). • SGA: This is the skip-gram model with an\nadaptive sampler (Chen et al., 2018). 2https://code.google.com/archive/p/word2vec/\nFor all baselines, we use the original implementation released by the authors."
  }, {
    "heading": "5.1 Datasets and Experimental Setup",
    "text": "We evaluate the performance of AllVec on four real-world corpora, namely Text83, NewsIR4, Wiki-sub and Wiki-all. Wiki-sub is a subset of 2017 Wikipedia dump5. All corpora have been pre-processed by a standard pipeline (i.e. removing non-textual elements, lowercasing and tokenization). Table 1 summarizes the statistics of these corpora.\nTo obtain Mwc for positive (w, c) pairs, we follow GloVe where word pairs that are xwords apart contribute 1/x to Mwc. The window size is set as win = 8. Regarding α+wc, we set xmax = 100 and ρ = 0.75. For a fair comparison, the embedding size k is set as 200 for all models and corpora. AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like (Bayer et al., 2017; Bradley et al., 2011) second order methods. For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all. The setting is also suggested by Mikolov et al. (2013b). Other detailed hyper-parameters are reported in Table 2."
  }, {
    "heading": "5.2 Accuracy Comparison",
    "text": "We present results on the word analogy task in Table 2. As shown, AllVec achieves the highest total accuracy (Tot.) in all corpora, particu-\n3http://mattmahoney.net/dc/text8.zip 4http://research.signalmedia.co/newsir16/signal-\ndataset.html 5https://dumps.wikimedia.org/enwiki/\nlarly in smaller corpora (Text8 and NewsIR). The reason is that in smaller corpora the number of positive (w, c) pairs is very limited, thus making use of negative examples will bring more benefits. Similar reason also explains the poor accuracy of GloVe in Text8, because GloVe does not consider negative samples. Even in the very large corpus (Wiki-all), ignoring negative samples still results in sub-optimal performance.\nOur results also show that SGA achieves better performance than SG, which demonstrates the importance of a good sampling strategy. However, regardless what sampler (except the full softmax sampling) is utilized and how many updates are taken, sampling is still a biased approach. AllVec achieves the best performance because it is trained on the whole batch data for each parameter update rather than a fraction of sampled data.\nAnother interesting observation is AllVec performs better in semantic tasks in general. The reason is that our model utilizes global co-occurrence statistics, which capture more semantic signals than syntactic signals. While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks. We argue that the main reason is because AllVec can distill useful signals from negative examples, while GloVe simply ignores all negative information. By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies. However, Rekabsaz et al. (2017) argues that these local-window based methods may suffer from the topic shifting issue.\nTable 3 and Table 4 provide results in the word similarity and QVEC tasks. We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples. Interestingly, although GloVe performs well in semantic analogy tasks, it shows extremely worse results in word similarity and QVEC. The reason shall be the same as that it performs poorly in syntactic tasks."
  }, {
    "heading": "5.3 Impact of α−c",
    "text": "In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words. We show the performance change of word analogy tasks on NewsIR in Figure 2 by tuning α0 and δ. Results in other corpora show similar trends thus are omitted due to space limitation.\nTable 2: Results (“Tot.” denotes total accuracy) on the word analogy task.\nCorpus Text8 NewsIR\npara. Sem. Syn. Tot. para. Sem. Syn. Tot.\nSG 1e-4 8 25 47.51 32.26 38.60 1e-5 10 10 70.81 47.48 58.10 SGA 6e-3 - - 48.10 33.78 39.74 6e-3 - - 71.74 48.71 59.20 GloVe 10 15 1 45.11 26.89 34.47 50 8 1 78.79 41.58 58.52 LexVec 1e-4 25 - 51.87 31.78 40.14 1e-5 10 - 76.11 39.09 55.95 AllVec 350 0.75 - 56.66 32.42 42.50 100 0.8 - 78.47 48.33 61.57\nWiki-sub Wiki-all\nSG 1e-5 10 5 72.05 55.88 63.24 1e-5 10 5 73.91 61.91 67.37 SGA 6e-3 - - 73.93 56.10 63.81 6e-3 - - 75.11 61.94 67.92 GloVe 100 8 1 77.22 53.16 64.13 100 8 1 77.38 58.94 67.33 LexVec 1e-5 5 - 75.95 52.78 63.33 1e-5 5 - 76.31 56.83 65.48 AllVec 100 0.75 - 76.66 54.72 64.75 50 0.75 - 77.64 60.96 68.52\nThe parameter columns (para.) for each model are given from left to right as follows. SG: subsampling of frequent words, window size and the number of negative samples; SGA: λ (Chen et al., 2018) that controls the distribution of the rank, the other parameters are the same with SG; GloVe: xmax, window size and symmetric window; LexVec: subsampling of frequent words and the number of negative samples; AllVec: the negative weight α0 and δ. Boldface denotes the highest total accuracy.\nFigure 2(a) shows the impact of the overall weight α0 by setting δ as 0.75 (inspired by the setting of skip-gram). Clearly, we observe that all results (including semantic, syntactic and total accuracy) have been greatly improved when α0 increases from 0 to a larger value. As mentioned before, α0 = 0 means that no negative information is considered. This observation verifies that negative samples are very important for learning good embeddings. It also helps to explain why GloVe performs poorly on syntactic tasks. In addition, we find that in all corpora the optimal results are usually obtained when α0 falls in the range of 50 to 400. For example, in the NewIR corpus as shown, AllVec achieves the best performance when α0 = 100. Figure 2(b) shows the impact of δ with α0 = 100. As mentioned before, δ = 0 denotes a uniform value for all negative words and δ = 1 denotes that no smoothing is applied to word frequency. We can see that the total accuracy is only around 55% when δ = 0. By increasing its value, the performance is gradually improved, achieving the highest score when δ is around 0.8. Further increase of δ will degrade the total accuracy. This analysis demonstrates the effectiveness of the proposed negative weighting scheme."
  }, {
    "heading": "5.4 Convergence Rate and Runtime",
    "text": "Figure 3(a) compares the convergence between AllVec and GloVe on NewsIR. Clearly, AllVec ex-\nhibits a more stable convergence due to its full batch learning. In contrast, GloVe has a more dramatic fluctuation because of the one-sample learning scheme. Figure 3(b) shows the relationship between the embedding size k and runtime on NewsIR. Although the analysis in Section 4.3 demonstrates that the time complexity of AllVec is O(4|S|k + 4|V |k2), the actual runtime shows a near linear relationship with k. This is because 4|V |k2/4|S|k = k/c, where c generally ranges from 1000 ∼ 6000 and k is set from 200 to 300 in practice. The above ratio explains the fact that 4|S|k dominates the complexity, which is linear\nwith k and |S|. We also compare the overall runtime of AllVec and SG on NewsIR and show the results in Table 5. As can be seen, the runtime of AllVec falls in the range of SG-3 and SG-7 in a single iteration, which confirms the theoretical analysis in Section 4.3. In contrast with SG, AllVec needs more iterations to converge. The reason is that each parameter in SG is updated many times during each iteration, although only one training example is used in each update. Despite this, the total run time of AllVec is still in a feasible range. Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.\nIn practice, the runtime of our model in each iteration can be further reduced by increasing the number of parallel workers. Although baseline methods like SG and GloVe can also be parallelized, the stochastic gradient steps in these methods unnecessarily influence each other as there is no exact way to separate these updates for different workers. In other words, the parallelization of SGD is not well suited to a large number of work-\ners. In contrast, the parameter updates in AllVec are completely independent of each other, therefore AllVec does not have the update collision issue. This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words. As such, AllVec can provide a near linear scaling without any approximation since there is no potential conflicts between updates."
  }, {
    "heading": "6 Conclusion",
    "text": "In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation. In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization. Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models. In future, we will extend\nour proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model. Moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process. Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.\nAcknowledgements. This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative. Joemon M.Jose and Xiangnan He are corresponding authors."
  }],
  "year": 2018,
  "references": [{
    "title": "Non-negative matrix factorization meets word embedding",
    "authors": ["Melissa Ailem", "Aghiles Salah", "Mohamed Nadif."],
    "venue": "SIGIR, pages 1081–1084.",
    "year": 2017
  }, {
    "title": "A generic coordinate descent framework for learning from implicit feedback",
    "authors": ["Immanuel Bayer", "Xiangnan He", "Bhargav Kanagal", "Steffen Rendle."],
    "venue": "WWW, pages 1341–1350.",
    "year": 2017
  }, {
    "title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Jean-Sébastien Senécal."],
    "venue": "IEEE Transactions on Neural Networks, pages 713–722.",
    "year": 2008
  }, {
    "title": "Adaptive sampled softmax with kernel based sampling",
    "authors": ["Guy Blanc", "Steffen Rendle."],
    "venue": "arXiv preprint arXiv:1712.00527.",
    "year": 2017
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.04606.",
    "year": 2016
  }, {
    "title": "Joint word representation learning using a corpus and a semantic lexicon",
    "authors": ["Danushka Bollegala", "Mohammed Alsuhaibani", "Takanori Maehara", "Ken-ichi Kawarabayashi."],
    "venue": "AAAI, pages 2690–2696.",
    "year": 2016
  }, {
    "title": "Parallel coordinate descent for l1-regularized loss minimization",
    "authors": ["Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin."],
    "venue": "arXiv preprint arXiv:1105.5379.",
    "year": 2011
  }, {
    "title": "Distributional semantics in technicolor",
    "authors": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."],
    "venue": "ACL, volume 1, pages 136–145.",
    "year": 2012
  }, {
    "title": "Improving word embeddings with convolutional feature learning and subword information",
    "authors": ["Shaosheng Cao", "Wei Lu."],
    "venue": "AAAI, pages 3144–3151.",
    "year": 2017
  }, {
    "title": "Improving negative sampling for word representation using self-embedded features",
    "authors": ["Long Chen", "Fajie Yuan", "Joemon M Jose", "Weinan Zhang."],
    "venue": "WSDM, pages 99–107.",
    "year": 2018
  }, {
    "title": "Co-clustering based classification for outof-domain documents",
    "authors": ["Wenyuan Dai", "Gui-Rong Xue", "Qiang Yang", "Yong Yu."],
    "venue": "SIGKDD, pages 210–219.",
    "year": 2007
  }, {
    "title": "Community evaluation and exchange of word vectors at wordvectors",
    "authors": ["Manaal Faruqui", "Chris Dyer."],
    "venue": "org. In ACL, pages 19–24.",
    "year": 2014
  }, {
    "title": "Placing search in context: The concept revisited",
    "authors": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."],
    "venue": "WWW, pages 406–414.",
    "year": 2001
  }, {
    "title": "Approximating word ranking and negative sampling for word embedding",
    "authors": ["Guibing Guo", "SC Ouyang", "Fajie Yuan."],
    "venue": "IJCAI.",
    "year": 2018
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig S Harris."],
    "venue": "Word, 10(2-3):146–162.",
    "year": 1954
  }, {
    "title": "Fast matrix factorization for online recommendation with implicit feedback",
    "authors": ["Xiangnan He", "Hanwang Zhang", "Min-Yen Kan", "TatSeng Chua."],
    "venue": "SIGIR, pages 549–558.",
    "year": 2016
  }, {
    "title": "Improving word representations via global context and multiple word prototypes",
    "authors": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "ACL, pages 873–882.",
    "year": 2012
  }, {
    "title": "Fasttext",
    "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Hervé Jégou", "Tomas Mikolov."],
    "venue": "zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.",
    "year": 2016
  }, {
    "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
    "authors": ["Yehuda Koren."],
    "venue": "SIGKDD, pages 426–434.",
    "year": 2008
  }, {
    "title": "Incorporating relational knowledge into word representations using subspace regularization",
    "authors": ["Abhishek Kumar", "Jun Araki."],
    "venue": "ACL, volume 2, pages 506–511.",
    "year": 2016
  }, {
    "title": "Swim: A simple word interaction model for implicit discourse relation recognition",
    "authors": ["Wenqiang Lei", "Xuancong Wang", "Meichun Liu", "Ilija Ilievski", "Xiangnan He", "Min-Yen Kan."],
    "venue": "IJCAI, pages 4026–4032.",
    "year": 2017
  }, {
    "title": "Neural word embedding as implicit matrix factorization",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "NIPS, pages 2177–2185.",
    "year": 2014
  }, {
    "title": "A relaxed ranking-based factor model for recommender system from implicit feedback",
    "authors": ["Huayu Li", "Richang Hong", "Defu Lian", "Zhiang Wu", "Meng Wang", "Yong Ge."],
    "venue": "IJCAI, pages 1683– 1689.",
    "year": 2016
  }, {
    "title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective",
    "authors": ["Yitan Li", "Linli Xu", "Fei Tian", "Liang Jiang", "Xiaowei Zhong", "Enhong Chen."],
    "venue": "IJCAI, pages 3650–3656.",
    "year": 2015
  }, {
    "title": "Learning semantic word embeddings based on ordinal knowledge constraints",
    "authors": ["Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu."],
    "venue": "ACL, volume 1, pages 1501–1511.",
    "year": 2015
  }, {
    "title": "Topical word embeddings",
    "authors": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun."],
    "venue": "AAAI, pages 2418–2424.",
    "year": 2015
  }, {
    "title": "Better word representations with recursive neural networks for morphology",
    "authors": ["Thang Luong", "Richard Socher", "Christopher Manning."],
    "venue": "CoNLL, pages 104–113.",
    "year": 2013
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Contextual correlates of semantic similarity",
    "authors": ["George A Miller", "Walter G Charles."],
    "venue": "Language and cognitive processes, 6(1):1–28.",
    "year": 1991
  }, {
    "title": "Improved word representation learning with sememes",
    "authors": ["Yilin Niu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun."],
    "venue": "ACL, volume 1, pages 2049– 2058.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Word embedding causes topic shifting; exploit global context",
    "authors": ["Navid Rekabsaz", "Mihai Lupu", "Allan Hanbury", "Hamed Zamani"],
    "venue": "In SIGIR,",
    "year": 2017
  }, {
    "title": "Contextual correlates of synonymy",
    "authors": ["Herbert Rubenstein", "John B Goodenough."],
    "venue": "Communications of the ACM, 8(10):627–633.",
    "year": 1965
  }, {
    "title": "An overview of gradient descent optimization algorithms",
    "authors": ["Sebastian Ruder."],
    "venue": "arXiv preprint arXiv:1609.04747.",
    "year": 2016
  }, {
    "title": "Enhancing the lexvec distributed word representation model using positional contexts and external memory",
    "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio."],
    "venue": "arXiv preprint arXiv:1606.01283.",
    "year": 2016
  }, {
    "title": "Matrix factorization using window sampling and negative sampling for improved word representations",
    "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio."],
    "venue": "arXiv preprint arXiv:1606.00819.",
    "year": 2016
  }, {
    "title": "Machine learning in automated text categorization",
    "authors": ["Fabrizio Sebastiani."],
    "venue": "ACM computing surveys (CSUR), 34(1):1–47.",
    "year": 2002
  }, {
    "title": "Jointly learning word embeddings and latent topics",
    "authors": ["Bei Shi", "Wai Lam", "Shoaib Jameel", "Steven Schockaert", "Kwun Ping Lai."],
    "venue": "SIGIR, pages 375– 384.",
    "year": 2017
  }, {
    "title": "Model-based word embeddings from decompositions of count matrices",
    "authors": ["Karl Stratos", "Michael Collins", "Daniel Hsu."],
    "venue": "ACL, volume 1, pages 1282–1291.",
    "year": 2015
  }, {
    "title": "Evaluation of word vector representations by subspace alignment",
    "authors": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer."],
    "venue": "EMNLP, pages 2049–2054.",
    "year": 2015
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "ACL, pages 384– 394.",
    "year": 2010
  }, {
    "title": "Lambdafm: learning optimal ranking with factorization machines using lambda surrogates",
    "authors": ["Fajie Yuan", "Guibing Guo", "Joemon M Jose", "Long Chen", "Haitao Yu", "Weinan Zhang."],
    "venue": "CIKM, pages 227–236. ACM.",
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701.",
    "year": 2012
  }],
  "id": "SP:fb05648648a95a03ed424c41ab7584274cce5422",
  "authors": [{
    "name": "∗Xin Xin",
    "affiliations": []
  }, {
    "name": "∗Fajie Yuan",
    "affiliations": []
  }, {
    "name": "Xiangnan He",
    "affiliations": []
  }, {
    "name": "Joemon M.Jose",
    "affiliations": []
  }],
  "abstractText": "Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the onesample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms samplingbased SGD methods with comparable efficiency, especially for small training corpora.",
  "title": "Batch IS NOT Heavy: Learning Word Representations From All Samples"
}