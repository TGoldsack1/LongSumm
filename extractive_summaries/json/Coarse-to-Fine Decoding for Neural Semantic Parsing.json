{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n731\nSemantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders."
  }, {
    "heading": "1 Introduction",
    "text": "Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular\ndecoders which use syntax to dynamically compose various submodels (Rabinovich et al., 2017).\nIn this work, we propose to decompose the decoding process into two stages. The first decoder focuses on predicting a rough sketch of the meaning representation, which omits low-level details, such as arguments and variable names. Example sketches for various meaning representations are shown in Table 1. Then, a second decoder fills in missing details by conditioning on the natural language input and the sketch itself. Specifically, the sketch constrains the generation process and is encoded into vectors to guide decoding.\nWe argue that there are at least three advantages to the proposed approach. Firstly, the decomposition disentangles high-level from low-level semantic information, which enables the decoders to model meaning at different levels of granularity. As shown in Table 1, sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go. Secondly, the model can explicitly share knowledge of coarse structures for the examples that have the same sketch (i.e., basic meaning), even though their actual meaning representations are different (e.g., due to different details). Thirdly, after generating the sketch, the decoder knows what the basic meaning of the utterance looks like, and the model can use it as global context to improve the prediction of the final details.\nOur framework is flexible and not restricted to specific tasks or any particular model. We conduct experiments on four datasets representative of various semantic parsing tasks ranging from logical form parsing, to code generation, and SQL query generation. We adapt our architecture to these tasks and present several ways to obtain sketches from their respective meaning representations. Experimental results show that our framework achieves competitive performance compared\nwith previous systems, despite employing relatively simple sequence decoders."
  }, {
    "heading": "2 Related Work",
    "text": "Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input.\nMore recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Kočiský et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017)\nuse a transition system to generate variable-free queries. Yin and Neubig (2017) design a grammar model for the generation of abstract syntax trees (Aho et al., 2007) in depth-first, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure.\nOur own work also aims to model the structure of meaning representations more faithfully. The flexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-fine methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-fine inference for lexical induction, sketches in our case are abstractions of the final meaning representation.\nThe idea of using sketches as intermediate representations has also been explored in the field of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use SEMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty."
  }, {
    "heading": "3 Problem Formulation",
    "text": "Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations.\nLet x = x1 · · ·x|x| denote a natural language expression, and y = y1 · · · y|y| its meaning representation. We wish to estimate p (y|x), the conditional probability of meaning representation y given input x. We decompose p (y|x) into a twostage generation process:\np (y|x) = p (y|x, a) p (a|x) (1) where a = a1 · · · a|a| is an abstract sketch representing the meaning of y. We defer detailed description of how sketches are extracted to Section 4. Suffice it to say that the extraction amounts to stripping off arguments and variable names in logical forms, schema specific information in SQL queries, and substituting tokens with types in source code (see Table 1).\nAs shown in Figure 1, we first predict sketch a for input x, and then fill in missing details to generate the final meaning representation y by conditioning on both x and a. The sketch is encoded into vectors which in turn guide and constrain the decoding of y. We view the input expression x, the meaning representation y, and its sketch a as sequences. The generation probabilities are factorized as:\np (a|x) = |a|∏ t=1 p (at|a<t, x) (2)\np (y|x, a) = |y|∏ t=1 p (yt|y<t, x, a) (3)\nwhere a<t = a1 · · · at−1, and y<t = y1 · · · yt−1. In the following, we will explain how p (a|x) and p (y|x, a) are estimated."
  }, {
    "heading": "3.1 Sketch Generation",
    "text": "An encoder is used to encode the natural language input x into vector representations. Then, a decoder learns to compute p (a|x) and generate the sketch a conditioned on the encoding vectors.\nInput Encoder Every input word is mapped to a vector via xt = Wxo (xt), where Wx ∈ R n×|Vx| is an embedding matrix, |Vx| is the vocabulary size, and o (xt) a one-hot vector. We use a bi-directional recurrent neural network with long short-term memory units (LSTM, Hochreiter and Schmidhuber 1997) as the input encoder. The encoder recursively computes the hidden vectors at the t-th time step via:\n−→e t = fLSTM (−→e t−1,xt) , t = 1, · · · , |x| (4)\n←−e t = fLSTM (←−e t+1,xt) , t = |x|, · · · , 1 (5)\net = [ −→e t,←−e t] (6)\nwhere [·, ·] denotes vector concatenation, et ∈ Rn, and fLSTM is the LSTM function.\nCoarse Meaning Decoder The decoder’s hidden vector at the t-th time step is computed by dt = fLSTM (dt−1,at−1), where at−1 ∈ Rn is the embedding of the previously predicted token. The hidden states of the first time step in the decoder are initialized by the concatenated encoding vectors d0 = [−→e |x|,←−e 1]. Additionally, we use an attention mechanism (Luong et al., 2015) to learn soft alignments. We compute the attention score for the current time step t of the decoder, with the k-th hidden state in the encoder as:\nst,k = exp{dt · ek}/Zt (7)\nwhere Zt = ∑|x|\nj=1 exp{dt · ej} is a normalization term. Then we compute p (at|a<t, x) via:\nedt = |x|∑ k=1 st,kek (8)\ndattt = tanh ( W1dt +W2e d t ) (9)\np (at|a<t, x) = softmaxat ( Wod att t + bo ) (10)\nwhere W1,W2 ∈ Rn×n, Wo ∈ R|Va|×n, and bo ∈ R|Va| are parameters. Generation terminates once an end-of-sequence token “</s>” is emitted."
  }, {
    "heading": "3.2 Meaning Representation Generation",
    "text": "Meaning representations are predicted by conditioning on the input x and the generated sketch a. The model uses the encoder-decoder architecture to compute p (y|x, a), and decorates the sketch a with details to generate the final output.\nSketch Encoder As shown in Figure 1, a bidirectional LSTM encoder maps the sketch sequence a into vectors {vk}|a|k=1 as in Equation (6), where vk denotes the vector of the k-th time step.\nFine Meaning Decoder The final decoder is based on recurrent neural networks with an attention mechanism, and shares the input encoder described in Section 3.1. The decoder’s hidden states {ht}|y|t=1 are computed via:\nit = { vk yt−1 is determined by ak yt−1 otherwise (11)\nht = fLSTM (ht−1, it)\nwhere h0 = [−→e |x|,←−e 1], and yt−1 is the embedding of the previously predicted token. Apart from using the embeddings of previous tokens, the decoder is also fed with {vk}|a|k=1. If yt−1 is determined by ak in the sketch (i.e., there is a one-toone alignment between yt−1 and ak), we use the corresponding token’s vector vk as input to the next time step.\nThe sketch constrains the decoding output. If the output token yt is already in the sketch, we force yt to conform to the sketch. In some cases, sketch tokens will indicate what information is missing (e.g., in Figure 1, token “flight@1” indicates that an argument is missing for the predicate “flight”). In other cases, sketch tokens will not reveal the number of missing tokens (e.g., “STRING” in DJANGO) but the decoder’s\noutput will indicate whether missing details have been generated (e.g., if the decoder emits a closing quote token for “STRING”). Moreover, type information in sketches can be used to constrain generation. In Table 1, sketch token “NUMBER” specifies that a numeric token should be emitted.\nFor the missing details, we use the hidden vector ht to compute p (yt|y<t, x, a), analogously to Equations (7)–(10)."
  }, {
    "heading": "3.3 Training and Inference",
    "text": "The model’s training objective is to maximize the log likelihood of the generated meaning representations given natural language expressions:\nmax ∑\n(x,a,y)∈D log p (y|x, a) + log p (a|x)\nwhere D represents training pairs. At test time, the prediction for input x is obtained via â = argmaxa′ p (a ′|x) and ŷ = argmaxy′ p (y ′|x, â), where a′ and y′ represent coarse- and fine-grained meaning candidates. Because probabilities p (a|x) and p (y|x, a) are factorized as shown in Equations (2)–(3), we can obtain best results approximately by using greedy search to generate tokens one by one, rather than iterating over all candidates."
  }, {
    "heading": "4 Semantic Parsing Tasks",
    "text": "In order to show that our framework applies across domains and meaning representations, we developed models for three tasks, namely parsing natural language to logical form, to Python source code, and to SQL query. For each of these tasks we describe the datasets we used, how sketches were extracted, and specify model details over and above the architecture presented in Section 3."
  }, {
    "heading": "4.1 Natural Language to Logical Form",
    "text": "For our first task we used two benchmark datasets, namely GEO (880 language queries to a database of U.S. geography) and ATIS (5, 410 queries to a flight booking system). Examples are shown in Table 1 (see the first and second block). We used standard splits for both datasets: 600 training and 280 test instances for GEO (Zettlemoyer and Collins, 2005); 4, 480 training, 480 development, and 450 test examples for ATIS. Meaning representations in these datasets are based on λ-calculus (Kwiatkowski et al., 2011). We use brackets to linearize the hierarchical structure.\nAlgorithm 1 Sketch for GEO and ATIS Input: t: Tree-structure λ-calculus expression t.pred: Predicate name, or operator name Output: a: Meaning sketch\n(count $0 (< (fare $0) 50:do))→(count#1 (< fare@1 ?)) function SKETCH(t)\nif t is leaf then No nonterminal in arguments return “%s@%d” % (t.pred,len(t.args)) if t.pred is λ operator, or quantifier then e.g., count Omit variable information defined by t.pred t.pred← “%s#%d” % (t.pred,len(variable)) for c ← argument in t.args do if c is nonterminal then\nc ← SKETCH(c) else\nc ← “?” Placeholder for terminal return t\nThe first element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments.\nAlgorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol “@” to denote the number of missing arguments in a predicate. For example, we extract “from@2” from the expression “(from $0 dallas:ci)” which indicates that the predicate “from” has two arguments. We use “?” as a placeholder in cases where only partial argument information can be omitted. We also omit variable information defined by the lambda operator and quantifiers (e.g., exists, count, and argmax). We use the symbol “#” to denote the number of omitted tokens. For the example in Figure 1, “lambda $0 e” is reduced to “lambda#2”.\nThe meaning representations of these two datasets are highly compositional, which motivates us to utilize the hierarchical structure of λ-calculus. A similar idea is also explored in the tree decoders proposed in Dong and Lapata (2016) and Yin and Neubig (2017) where parent hidden states are fed to the input gate of the LSTM units. On the contrary, parent hidden states serve as input to the softmax classifiers of both fine and coarse meaning decoders.\nParent Feeding Taking the meaning sketch “(and flight@1 from@2)” as an example, the parent of “from@2” is “(and”. Let pt denote the parent of the t-th time step in the decoder. Compared with Equation (10), we use the vector dattt and the hidden state of its parent dpt to compute the prob-\nability p (at|a<t, x) via:\np (at|a<t, x) = softmaxat ( Wo[d att t ,dpt ] + bo ) where [·, ·] denotes vector concatenation. The parent feeding is used for both decoding stages."
  }, {
    "heading": "4.2 Natural Language to Source Code",
    "text": "Our second semantic parsing task used DJANGO (Oda et al., 2015), a dataset built upon the Python code of the Django library. The dataset contains lines of code paired with natural language expressions (see the third block in Table 1) and exhibits a variety of use cases, such as iteration, exception handling, and string manipulation. The original split has 16, 000 training, 1, 000 development, and 1, 805 test instances.\nWe used the built-in lexical scanner of Python1 to tokenize the code and obtain token types. Sketches were extracted by substituting the original tokens with their token types, except delimiters (e.g., “[”, and “:”), operators (e.g., “+”, and “*”), and built-in keywords (e.g., “True”, and “while”). For instance, the expression “if s[:4].lower() == ’http’:” becomes “if NAME [ : NUMBER ] . NAME ( ) == STRING :”, with details about names, values, and strings being omitted.\nDJANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016), which allows the fine meaning decoder (Section 3.2) to directly copy tokens from the natural language input.\nCopying Mechanism Recall that we use a softmax classifier to predict the probability distribution p (yt|y<t, x, a) over the pre-defined vocabulary. We also learn a copying gate gt ∈ [0, 1] to decide whether yt should be copied from the input or generated from the vocabulary. We compute the modified output distribution via:\ngt = sigmoid(wg · ht + bg) p̃ (yt|y<t, x, a) = (1− gt)p (yt|y<t, x, a)\n+ [yt /∈Vy ]gt ∑\nk:xk=yt\nst,k\n1https://docs.python.org/3/library/ tokenize\nwhere wg ∈ Rn and bg ∈ R are parameters, and the indicator function [yt /∈Vy ] is 1 only if yt is not in the target vocabulary Vy; the attention score st,k (see Equation (7)) measures how likely it is to copy yt from the input word xk."
  }, {
    "heading": "4.3 Natural Language to SQL",
    "text": "The WIKISQL (Zhong et al., 2017) dataset contains 80, 654 examples of questions and SQL queries distributed across 24, 241 tables from Wikipedia. The goal is to generate the correct SQL query for a natural language question and table schema (i.e., table column names), without using the content values of tables (see the last block in Table 1 for an example). The dataset is partitioned into a training set (70%), a development set (10%), and a test set (20%). Each table is present in one split to ensure generalization to unseen tables.\nWIKISQL queries follow the format “SELECT agg op agg col WHERE (cond col cond op cond) AND ...”, which is a subset of the SQL syntax. SELECT identifies the column that is to be included in the results after applying the aggregation operator agg op2 to column agg col. WHERE can have zero or multiple conditions, which means that column cond col must satisfy the constraints expressed by the operator cond op3 and the condition value cond. Sketches for SQL queries are simply the (sorted) sequences of condition operators cond op in WHERE clauses. For example, in Table 1, sketch “WHERE > AND =” has two condition operators, namely “>” and “=”.\nThe generation of SQL queries differs from our previous semantic parsing tasks, in that the table schema serves as input in addition to natural language. We therefore modify our input encoder in order to render it table-aware, so to speak. Furthermore, due to the formulaic nature of the SQL query, we only use our decoder to generate the WHERE clause (with the help of sketches). The SELECT clause has a fixed number of slots (i.e., aggregation operator agg op and column agg col), which we straightforwardly predict with softmax classifiers (conditioned on the input). We briefly explain how these components are modeled below.\nTable-Aware Input Encoder Given a table schema with M columns, we employ the special token “‖” to concatenate its header names\n2agg op ∈ {empty,COUNT,MIN,MAX,SUM,AVG}. 3cond op ∈ {=, <,>}.\nas “‖c1,1 · · · c1,|c1|‖· · · ‖cM,1 · · · cM,|cM |‖”, where the k-th column (“ck,1 · · · ck,|ck|”) has |ck| words. As shown in Figure 2, we use bi-directional LSTMs to encode the whole sequence. Next, for column ck, the LSTM hidden states at positions ck,1 and ck,|ck| are concatenated. Finally, the concatenated vectors are used as the encoding vectors {ck}Mk=1 for table columns.\nAs mentioned earlier, the meaning representations of questions are dependent on the tables. As shown in Figure 2, we encode the input question x into {et}|x|t=1 using LSTM units. At each time step t, we use an attention mechanism towards table column vectors {ck}Mk=1 to obtain the most relevant columns for et. The attention score from et to ck is computed via ut,k ∝ exp{α(et) · α(ck)}, where α(·) is a one-layer neural network, and∑M\nk=1 ut,k = 1. Then we compute the context vector cet = ∑M k=1 ut,kck to summarize the relevant columns for et. We feed the concatenated vectors {[et, cet ]}|x|t=1 into a bi-directional LSTM encoder, and use the new encoding vectors {ẽt}|x|t=1 to replace {et}|x|t=1 in other model components. We define the vector representation of input x as:\nẽ = [ −→̃ e |x|, ←−̃ e 1] (12)\nanalogously to Equations (4)–(6).\nSELECT Clause We feed the question vector ẽ into a softmax classifier to obtain the aggregation operator agg op. If agg col is the k-th table column, its probability is computed via:\nσ(x) = w3 · tanh (W4x+ b4) (13) p (agg col = k|x) ∝ exp{σ([ẽ, ck])} (14)\nwhere ∑M\nj=1 p (agg col = j|x) = 1, σ(·) is a scoring network, and W4 ∈ R2n×m,w3,b4 ∈ R m are parameters.\nWHERE Clause We first generate sketches whose details are subsequently decorated by the fine meaning decoder described in Section 3.2. As the number of sketches in the training set is small (35 in total), we model sketch generation as a classification problem. We treat each sketch a as a category, and use a softmax classifier to compute p (a|x):\np (a|x) = softmaxa (Waẽ+ ba) where Wa ∈ R|Va|×n,ba ∈ R|Va| are parameters, and ẽ is the table-aware input representation defined in Equation (12).\nOnce the sketch is predicted, we know the condition operators and number of conditions in the WHERE clause which follows the format “WHERE (cond op cond col cond) AND ...”. As shown in Figure 3, our generation task now amounts to populating the sketch with condition columns cond col and their values cond.\nLet {ht}|y|t=1 denote the LSTM hidden states of the fine meaning decoder, and {hattt }|y|t=1 the vectors obtained by the attention mechanism as in Equation (9). The condition column cond colyt is selected from the table’s headers. For the k-th column in the table, we compute p (cond colyt = k|y<t, x, a) as in Equation (14), but use different parameters and compute the score via σ([hattt , ck]). If the k-th table column is selected, we use ck for the input of the next LSTM unit in the decoder.\nCondition values are typically mentioned in the input questions. These values are often phrases with multiple tokens (e.g., Mikhail Snitko in Table 1). We therefore propose to select a text span from input x for each condition value condyt rather than copying tokens one by one. Let xl · · ·xr denote the text span from which condyt\nis copied. We factorize its probability as:\np (condyt = xl · · ·xr|y<t, x, a) = p ( l Lyt |y<t, x, a ) p ( r Ryt |y<t, x, a, l Lyt ) p ( l Lyt |y<t, x, a\n) ∝ exp{σ([hattt , ẽl])} p ( r Ryt |y<t, x, a, l Lyt\n) ∝ exp{σ([hattt , ẽl, ẽr])} where l Lyt/ r R yt represents the first/last copying index of condyt is l/r, the probabilities are normalized to 1, and σ(·) is the scoring network defined in Equation (13). Notice that we use different parameters for the scoring networks σ(·). The copied span is represented by the concatenated vector [ẽl, ẽr], which is fed into a one-layer neural network and then used as the input to the next LSTM unit in the decoder."
  }, {
    "heading": "5 Experiments",
    "text": "We present results on the three semantic parsing tasks discussed in Section 4. Our implementation and pretrained models are available at https:// github.com/donglixp/coarse2fine."
  }, {
    "heading": "5.1 Experimental Setup",
    "text": "Preprocessing For GEO and ATIS, we used the preprocessed versions provided by Dong and Lapata (2016), where natural language expressions are lowercased and stemmed with NLTK (Bird et al., 2009), and entity mentions are replaced by numbered markers. We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact. We employed the preprocessed DJANGO data provided by Yin and Neubig (2017), where input expressions are tokenized by NLTK, and quoted strings in the input are replaced with place holders. WIKISQL was preprocessed by the script provided by Zhong et al. (2017), where inputs were lowercased and tokenized by Stanford CoreNLP (Manning et al., 2014).\nConfiguration Model hyperparameters were cross-validated on the training set for GEO, and were validated on the development split for the other datasets. Dimensions of hidden vectors and word embeddings were selected from {250, 300} and {150, 200, 250, 300}, respectively. The dropout rate was selected from {0.3, 0.5}. Label smoothing (Szegedy et al., 2016) was employed for GEO and ATIS. The smoothing parameter was set to 0.1. For WIKISQL, the hidden size of σ(·)\nand α(·) in Equation (13) was set to 64. Word embeddings were initialized by GloVe (Pennington et al., 2014), and were shared by table encoder and input encoder in Section 4.3. We appended 10-dimensional part-of-speech tag vectors to embeddings of the question words in WIKISQL. The part-of-speech tags were obtained by the spaCy toolkit. We used the RMSProp optimizer (Tieleman and Hinton, 2012) to train the models. The learning rate was selected from {0.002, 0.005}. The batch size was 200 for WIKISQL, and was 64 for other datasets. Early stopping was used to determine the number of epochs.\nEvaluation We use accuracy as the evaluation metric, i.e., the percentage of the examples that are correctly parsed to their gold standard meaning representations. For WIKISQL, we also execute generated SQL queries on their corresponding tables, and report the execution accuracy which is defined as the proportion of correct answers."
  }, {
    "heading": "5.2 Results and Analysis",
    "text": "We compare our model (COARSE2FINE) against several previously published systems as well as various baselines. Specifically, we report results with a model which decodes meaning representations in one stage (ONESTAGE) without leveraging sketches. We also report the results of several ablation models, i.e., without a sketch encoder and without a table-aware input encoder.\nTable 2 presents our results on GEO and ATIS. Overall, we observe that COARSE2FINE outperforms ONESTAGE, which suggests that disentangling high-level from low-level information dur-\ning decoding is beneficial. The results also show that removing the sketch encoder harms performance since the decoder loses access to additional contextual information. Compared with previous neural models that utilize syntax or grammatical information (SEQ2TREE, ASN; the second block in Table 2), our method performs competitively despite the use of relatively simple decoders. As an upper bound, we report model accuracy when gold meaning sketches are given to the fine meaning decoder (+oracle sketch). As can be seen, predicting the sketch correctly boosts performance. The oracle results also indicate the accuracy of the fine meaning decoder.\nTable 3 reports results on DJANGO where we observe similar tendencies. COARSE2FINE outperforms ONESTAGE by a wide margin. It is also superior to the best reported result in the literature (SNM+COPY; see the second block in the table). Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle.\nResults on WIKISQL are shown in Table 4. Our model is superior to ONESTAGE as well as to previous best performing systems. COARSE2FINE’s accuracies on aggregation agg op and agg col are 90.2% and 92.0%, respectively, which is comparable to SQLNET (Xu et al., 2017). So the most gain is obtained by the improved decoder of the WHERE clause. We also find that a tableaware input encoder is critical for doing well on this task, since the same question might lead to different SQL queries depending on the table schemas. Consider the question “how many presidents are graduated from A”. The SQL query over table “‖President‖College‖” is “SELECT\nCOUNT(President) WHERE (College = A)”, but the query over table “‖College‖Number of Presidents‖” would be “SELECT Number of Presidents WHERE (College = A)”.\nWe also examine the predicted sketches themselves in Table 5. We compare sketches generated by COARSE2FINE against ONESTAGE. The latter model generates meaning representations without an intermediate sketch generation stage. Nevertheless, we can extract sketches from the output of ONESTAGE following the procedures described in Section 4. Sketches produced by COARSE2FINE are more accurate across the board. This is not surprising because our model is trained explicitly to generate compact meaning sketches. Taken together (Tables 2–4), our results show that better sketches bring accuracy gains on GEO, ATIS, and DJANGO. On WIKISQL, the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE. Performance improvements on this task are mainly due to the fine meaning decoder. We conjecture that by decomposing decoding into two stages, COARSE2FINE can better match table columns and extract condition values without interference from the prediction of condition operators. Moreover, the sketch provides a canonical order of condition operators, which is beneficial for the decoding process (Vinyals et al., 2016; Xu et al., 2017)."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper we presented a coarse-to-fine decoding framework for neural semantic parsing. We first generate meaning sketches which abstract away from low-level information such as arguments and variable names and then predict missing details in order to obtain full meaning representations. The proposed framework can be easily adapted to different domains and meaning representations. Experimental results show that coarseto-fine decoding improves performance across tasks. In the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches.\nAcknowledgments We would like to thank Pengcheng Yin for sharing with us the preprocessed version of the DJANGO dataset. We gratefully acknowledge the financial support of the European Research Council (award number 681760; Dong, Lapata) and the AdeptMind Scholar Fellowship program (Dong)."
  }],
  "year": 2018,
  "references": [{
    "title": "Compilers: principles, techniques, and tools, volume 2",
    "authors": ["Alfred V Aho", "Ravi Sethi", "Jeffrey D Ullman."],
    "venue": "Addison-wesley Reading.",
    "year": 2007
  }, {
    "title": "Tree-structured decoding with doubly-recurrent neural networks",
    "authors": ["David Alvarez-Melis", "Tommi S Jaakkola."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations, Toulon, France.",
    "year": 2017
  }, {
    "title": "Semantic parsing as machine translation",
    "authors": ["Jacob Andreas", "Andreas Vlachos", "Stephen Clark."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, Sofia, Bulgaria.",
    "year": 2013
  }, {
    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
    "authors": ["Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Transactions of the Association of Computational Linguistics, 1:49–62.",
    "year": 2013
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations, San Diego, California.",
    "year": 2015
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of the 2013",
    "year": 2013
  }, {
    "title": "Natural Language Processing with Python",
    "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."],
    "venue": "O’Reilly Media.",
    "year": 2009
  }, {
    "title": "Multilevel coarse-to-fine PCFG parsing",
    "authors": ["Eugene Charniak", "Mark Johnson", "Micha Elsner", "Joseph Austerweil", "David Ellis", "Isaac Haxton", "Catherine Hill", "R. Shrivaths", "Jeremy Moore", "Michael Pozar", "Theresa Vu."],
    "venue": "Proceedings of the Human Lan-",
    "year": 2006
  }, {
    "title": "Learning structured natural language representations for semantic parsing",
    "authors": ["Jianpeng Cheng", "Siva Reddy", "Vijay Saraswat", "Mirella Lapata."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55.",
    "year": 2017
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 33–43, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Transfer learning for neural semantic parsing",
    "authors": ["Xing Fan", "Emilio Monti", "Lambert Mathias", "Markus Dreyer."],
    "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 48–56, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Component-based synthesis for complex apis",
    "authors": ["Yu Feng", "Ruben Martins", "Yuepeng Wang", "Isil Dillig", "Thomas Reps."],
    "venue": "Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, pages 599–612,",
    "year": 2017
  }, {
    "title": "A statistical semantic parser that integrates syntax and semantics",
    "authors": ["Ruifang Ge", "Raymond J. Mooney."],
    "venue": "Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 9–16, Ann Arbor, Michigan.",
    "year": 2005
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1631–1640, Berlin,",
    "year": 2016
  }, {
    "title": "Pointing the unknown words",
    "authors": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 140–149, Berlin, Germany. Asso-",
    "year": 2016
  }, {
    "title": "Neural semantic parsing over multiple knowledge-bases",
    "authors": ["Jonathan Herzig", "Jonathan Berant."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 623– 628, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9:1735– 1780.",
    "year": 1997
  }, {
    "title": "Learning a neural semantic parser from user feedback",
    "authors": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Jayant Krishnamurthy", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
    "year": 2017
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Semantic parsing with semi-supervised sequential autoencoders",
    "authors": ["Tomáš Kočiský", "Gábor Melis", "Edward Grefenstette", "Chris Dyer", "Wang Ling", "Phil Blunsom", "Karl Moritz Hermann."],
    "venue": "Proceedings of the 2016 Conference on Empirical Meth-",
    "year": 2016
  }, {
    "title": "Neural semantic parsing with type constraints for semi-structured tables",
    "authors": ["Jayant Krishnamurthy", "Pradeep Dasigi", "Matt Gardner."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1517–1527, Copen-",
    "year": 2017
  }, {
    "title": "Inducing probabilistic CCG grammars from logical form with higherorder unification",
    "authors": ["Tom Kwiatkowksi", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language",
    "year": 2010
  }, {
    "title": "Scaling semantic parsers with on-the-fly ontology matching",
    "authors": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle,",
    "year": 2013
  }, {
    "title": "Lexical generalization in CCG grammar induction for semantic parsing",
    "authors": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2011
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "Computational Linguistics, 39(2).",
    "year": 2013
  }, {
    "title": "Latent predictor networks for code generation",
    "authors": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiský", "Fumin Wang", "Andrew Senior."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-",
    "year": 2016
  }, {
    "title": "A generative model for parsing natural language to meaning representations",
    "authors": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783–792,",
    "year": 2008
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon,",
    "year": 2015
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Association for Computational Linguistics System Demonstrations, pages",
    "year": 2014
  }, {
    "title": "Learning to generate pseudo-code from source code using statistical machine translation",
    "authors": ["Yusuke Oda", "Hiroyuki Fudaba", "Graham Neubig", "Hideaki Hata", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura."],
    "venue": "Proceedings of the 2015 30th",
    "year": 2015
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532–1543, Doha, Qatar.",
    "year": 2014
  }, {
    "title": "Coarse-to-fine natural language processing",
    "authors": ["Slav Petrov."],
    "venue": "Springer Science & Business Media.",
    "year": 2011
  }, {
    "title": "Grounded unsupervised semantic parsing",
    "authors": ["Hoifung Poon."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933–943, Sofia, Bulgaria.",
    "year": 2013
  }, {
    "title": "Abstract syntax networks for code generation and semantic parsing",
    "authors": ["Maxim Rabinovich", "Mitchell Stern", "Dan Klein."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1139–1149, Vancouver,",
    "year": 2017
  }, {
    "title": "Program Synthesis by Sketching",
    "authors": ["Armando Solar-Lezama."],
    "venue": "Ph.D. thesis, University of California at Berkeley, Berkeley, CA.",
    "year": 2008
  }, {
    "title": "Neural architectures for multilingual semantic parsing",
    "authors": ["Raymond Hendy Susanto", "Wei Lu."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 38–44, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Rethinking the inception architecture for computer vision",
    "authors": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–2826.",
    "year": 2016
  }, {
    "title": "Automated construction of database interfaces: Intergrating statistical and relational learning for semantic parsing",
    "authors": ["Lappoon R. Tang", "Raymond J. Mooney."],
    "venue": "2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Process-",
    "year": 2000
  }, {
    "title": "Lecture 6.5— RMSProp: Divide the gradient by a running average of its recent magnitude",
    "authors": ["T. Tieleman", "G. Hinton"],
    "venue": "Technical report",
    "year": 2012
  }, {
    "title": "Order matters: Sequence to sequence for sets",
    "authors": ["Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur."],
    "venue": "Proceedings of the 4th International Conference on Learning Representations, San Juan, Puerto Rico.",
    "year": 2016
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 2773–2781, Mon-",
    "year": 2015
  }, {
    "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
    "authors": ["Yuk Wah Wong", "Raymond Mooney."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, Prague, Czech Repub-",
    "year": 2007
  }, {
    "title": "Sequence-based structured prediction for semantic parsing",
    "authors": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1341–1350, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "SQLNet: Generating structured queries from natural language without reinforcement learning",
    "authors": ["Xiaojun Xu", "Chang Liu", "Dawn Song."],
    "venue": "arXiv preprint arXiv:1711.04436.",
    "year": 2017
  }, {
    "title": "SQLizer: Query synthesis from natural language",
    "authors": ["Navid Yaghmazadeh", "Yuepeng Wang", "Isil Dillig", "Thomas Dillig."],
    "venue": "Proceedings of the ACM on Programming Languages, 1:63:1–63:26.",
    "year": 2017
  }, {
    "title": "A syntactic neural model for general-purpose code generation",
    "authors": ["Pengcheng Yin", "Graham Neubig."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 440– 450, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, pages 658–666,",
    "year": 2005
  }, {
    "title": "Online learning of relaxed CCG grammars for parsing to logical form",
    "authors": ["Luke Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
    "year": 2007
  }, {
    "title": "Automatically synthesizing SQL queries from input-output examples",
    "authors": ["Sai Zhang", "Yuyin Sun."],
    "venue": "Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering, pages 224–234, Piscataway, NJ.",
    "year": 2013
  }, {
    "title": "Macro grammars and holistic triggering for efficient semantic parsing",
    "authors": ["Yuchen Zhang", "Panupong Pasupat", "Percy Liang."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1214–1223. Associa-",
    "year": 2017
  }, {
    "title": "Type-driven incremental semantic parsing with polymorphism",
    "authors": ["Kai Zhao", "Liang Huang."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2015
  }, {
    "title": "Seq2SQL: Generating structured queries from natural language using reinforcement learning",
    "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher."],
    "venue": "arXiv preprint arXiv:1709.00103.",
    "year": 2017
  }],
  "id": "SP:1ef0ad65055fb8714b5a4a53f2e21825a9b17ba7",
  "authors": [{
    "name": "Li Dong",
    "affiliations": []
  }, {
    "name": "Mirella Lapata",
    "affiliations": []
  }],
  "abstractText": "Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.",
  "title": "Coarse-to-Fine Decoding for Neural Semantic Parsing"
}