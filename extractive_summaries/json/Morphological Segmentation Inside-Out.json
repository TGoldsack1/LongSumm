{
  "sections": [{
    "text": "ar X\niv :1\n91 1.\n04 91\n6v 2\n[ cs\n.C L\n] 1\n2 Fe\nb 20\n21 Appeared in the proceedings of EMNLP 2016 (Austin, November). This version was\nMorphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.1"
  }, {
    "heading": "1 Introduction",
    "text": "In NLP, supervised morphological segmentation has typically been viewed as either a sequence-labeling or a segmentation task (Ruokolainen et al., 2016). In contrast, we consider a hierarchical approach, employing a context-free grammar (CFG). CFGs provide a richer model of morphology: They capture (i) the intuition that words themselves have internal constituents, which belong to different categories, as well as (ii) the order in which affixes are attached. Moreover, many morphological processes, e.g., compounding and reduplication, are best modeled as hierarchical; thus, context-free models are expressively more appropriate.\nThe purpose of morphological segmentation is to decompose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning-bearing units in language.\n1We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well.\nThis work concerns itself with modeling hierarchical structure over these morphemes. Note a simple flat morphological segmentation can also be straightforwardly derived from the CFG parse tree. Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and Çetinoğlu, 2015). In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation. For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le.\nWe make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone. We experimentally show that this model outperforms a semi-Markov model on flat segmentation. (ii) We release the first morphology treebank, consisting of 7454 English word types, each annotated with a full constituency parse."
  }, {
    "heading": "2 The Case For Hierarchical Structure",
    "text": "Why should we analyze morphology hierarchically? It is true that we can model much of morphology with finite-state machinery (Beesley and Karttunen, 2003), but there are, nevertheless, many cases where hierarchical structure appears requisite. For instance, the flat segmentation of the word untestably7→un+test+able+ly is missing important information about how the word was derived. The correct parse [[un[[test]able]]ly], on the other hand, does tell us that this is the order in which the complex form was derived:\ntest able 7−−→testable un 7−→untestable ly 7−→untestably.\nThis gives us insight into the structure of the\nlexicon—we expect that the segment testable exists as an independent word, but ably does not.\nMoreover, a flat segmentation is often semantically ambiguous. There are two potentially valid readings of untestably depending on how the negative prefix un scopes. The correct tree (see Figure 1) yields the reading “in the manner of not able to be tested.” A second—likely infelicitous reading—where the segment untest forms a constituent yields the reading “in a manner of being able to untest.” Recovering the hierarchical structure allows us to select the correct reading; note there are even cases of true ambiguity; e.g., unlockable has two readings: “unable to be locked” and “able to be unlocked.”\nWe also note that theoretical linguists often implicitly assume a context-free treatment of word formation, e.g., by employing brackets to indicate different levels of affixation. Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982; Marvin, 2002)."
  }, {
    "heading": "3 Parsing the Lexicon",
    "text": "A novel component of this work is the development of a discriminative parser (Finkel et al., 2008; Hall et al., 2014) for morphology. The goal is to define a probability distribution over all trees that could arise from the input word, after reversal of orthographic and phonological processes. We employ the simple grammar shown in Table 1. Despite its simplicity, it models the order in which morphemes are attached.\nMore formally, our goal is to map a surface form w (e.g., w=untestably) into its underlying canonical form u (e.g., u=untestablely) and then into a parse tree t over its morphemes. We assume u,w ∈ Σ∗, for some discrete alphabet Σ.2 Note\n2For efficiency, we assume u ∈ Σ|w|+k , k = 5.\nthat a parse tree over the string implicitly defines a flat segmentation given our grammar—one can simply extract the characters spanned by all preterminals in the resulting tree. Before describing the joint model in detail, we first consider its pieces individually."
  }, {
    "heading": "3.1 Restoring Orthographic Changes",
    "text": "To extract a canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016), we restore orthographic changes that occur during word formation. To this end, we define the score function\nscoreη(u, a,w) = exp ( g(u, a,w)⊤η )\n(1)\nwhere a is a monotonic alignment between the strings u and w. The goal is for scoreη to assign higher values to better matched pairs, e.g., (w=untestably, u=untestablely). We refer to Dreyer et al. (2008) for a thorough exposition.\nFor ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002). This requires, however, that the feature function g factors over the topology of the finite-state encoding. Since our model conditions on the word w, the feature function g can extract features from any part of this string. Features on the output string, u, however, are more restricted. In this work, we employ a bigram model over output characters. This implies that each state remembers exactly one character: the previous one. See Cotterell et al. (2014) for details. We can compute the score for two strings u and w using a weighted generalization of the Levenshtein algorithm. Computing the partition function requires a different dynamic program, which runs in O(|w|2 · |Σ|2) time. Note that since |Σ| ≈ 26 (lower case English letters), it takes\nroughly 262 = 676 times longer to compute the partition function than to score a pair of strings.\nOur model includes several simple feature tem-\nplates, including features that fire on individual edit actions as well as conjunctions of edit actions and characters in the surrounding context. See Cotterell et al. (2016) for details."
  }, {
    "heading": "3.2 Morphological Analysis as Parsing",
    "text": "Next, we need to score an underlying canonical form (e.g., u=untestablely) together with a parse tree (e.g., t=[[un[[test]able]]ly]). Thus, we define the parser score with the following function\nscoreω(t, u) = exp\n\n\n∑\nπ∈Π(t)\nf(π, u)⊤ω\n\n (2)\nwhere Π(t) is the set of anchored productions in the tree t. An anchored production π is a grammar rule in Chomsky normal form attached to a span, e.g., Ai,k → Bi,jCj,k. Each π is then assigned a weight by the linear function f(π, u)⊤ω, where the function f extracts relevant features from the anchored production as well as the corresponding span of the underlying form u. This model is typically referred to as a weighted CFG (WCFG) (Smith and Johnson, 2007) or a CRF parser.\nFor f , we define three span features: (i) indicator features on the span’s segment, (ii) an indicator feature that fires if the segment appears in an external corpus3 and (iii) the conjunction of the segment with the label (e.g., PREFIX) of the subtree root. Following Hall et al. (2014), we employ an indicator feature for each production as well as production backoff features."
  }, {
    "heading": "4 A Joint Model",
    "text": "Our complete model is a joint CRF (Koller and Friedman, 2009) where each of\n3We use the Wikipedia dump from 2016-05-01.\nthe above scores are factors. We define the following probability distribution over trees, canonical forms and their alignments to the original word\npθ(t,a, u | w) = (3)\n1\nZθ(w) scoreω(t, u) · scoreη(u, a,w)\nwhere θ = {ω,η} is the parameter vector and the normalizing partition function as\nZθ(w) = ∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n(4)\n∑\nt′∈T (u′)\nscoreω(t ′, u′) · scoreη(u ′, a, w)\nwhere T (u) is the set of all parse trees for the string u. This involves a sum over all possible underlying orthographic forms and all parse trees for those forms.\nThe joint approach has the advantage that it allows both factors to work together to influence the choice of the underlying form u. This is useful as the parser now has access to which words are attested in the language; this helps guide the relatively weak transduction model. On the downside, the partition function Zθ now involves a sum over all strings in Σ|w|+k and all possible parses of each string! Finally, we define the marginal distribution over trees and underlying forms as\npθ(t, u | w) = ∑\na∈A(u,w)\npθ(t, a, u | w) (5)\nwhere A(u,w) is the set of all monotonic alignments between u and w. The marginalized form in eq. (5) is our final model of morphological segmentation since we are not interested in the latent alignments a."
  }, {
    "heading": "4.1 Learning and Inference",
    "text": "We use stochastic gradient descent to optimize the log-probability of the training data ∑N\nn=1 log pθ(t (n), u(n) | w(n)); this requires the computation of the gradient of the partition function ∇θ logZθ. We may view this gradient as an expectation:\n∇θ logZθ(w) = (6)\nE(t,a,u)∼pθ(·|w)\n\n\n∑\nπ∈Π(t)\nf(π, u)⊤ + g(u, a,w)⊤\n\n\nWe provide the full derivation in Appendix A with an additional Rao-Blackwellization step that we make use of in the implementation. While the sum over all underlying forms and trees in eq. (6) may be achieved in polynomial time (using the Bar-Hillel construction), we make use of an importance-sampling estimator, derived by Cotterell et al. (2016), which is faster in practice. Roughly speaking, we approximate the hard-tosample-from distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively. We then reweight the samples using the unnormalized score from pθ. Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016). Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient."
  }, {
    "heading": "4.2 Decoding",
    "text": "We also decode by importance sampling. Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree."
  }, {
    "heading": "5 Related Work",
    "text": "We believe our attempt to train discriminative grammars for morphology is novel. Nevertheless, other researchers have described parsers for morphology. Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation. Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013). Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algo-\nrithm (Baker, 1979). Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014)."
  }, {
    "heading": "6 Morphological Treebank",
    "text": "Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure. A core reason behind this is that—to the best of our knowledge— there are no hierarchically annotated corpora for the task. To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993). We reannotated 7454 English types with a full constituency parse.4 The resource will be freely available for future research."
  }, {
    "heading": "6.1 Annotation Guidelines",
    "text": "The annotation of the morphology treebank was guided by three core principles. The first principle concerns productivity: we exclusively annotate productive morphology. In the context of morphology, productivity refers to the degree that native speakers actively employ the affix to create new words (Aronoff, 1976). We believe that for NLP applications, we should focus on productive affixation. Indeed, this sets our corpus apart from many existing morphologically annotated corpora such as CELEX. For example, CELEX contains warmth7→warm+th, but th is not a productive suffix and cannot be used to create new words. Thus, we do not want to analyze hearth7→hear+th or, in general, allow wug7→wug+th. Second, we annotate for semantic coherence. When there are several candidate parses, we choose the one that is best compatible with the compositional semantics of the derived form.\nInterestingly, multiple trees can be considered valid depending on the linguistic tier of interest. Consider the word unhappier. From a semantic\n4In many cases, we corrected the flat segmentation as well.\nperspective, we have the parse [[un [happy]] er] which gives us the correct meaning “not happy to a greater degree.” However, since the suffix er only attaches to mono- and bisyllabic words, we get [un[[happy] er]] from a phonological perspective. In the linguistics literature, this problem is known as the bracketing paradox (Pesetsky, 1985; Embick, 2015). We annotate exclusively at the syntactic-semantic tier.\nThirdly, in the context of derivational morphology, we force spans to be words themselves. Since derivational morphology—by definition—forms new words from existing words (Lieber and Štekauer, 2014), it follows that each span rooted with WORD or ROOT in the correct parse corresponds to a word in the lexicon. For example, consider unlickable. The correct parse, under our scheme, is [un [[lick] able]]. Each of the spans (lick, lickable and unlickable) exists as a word. By contrast, the parse [[un [lick]] able] contains the span unlick, which is not a word in the lexicon. The span in the segmented form may involve changes, e.g., [un [[achieve] able]], where achieveable is not a word, but achievable (after deleting e) is."
  }, {
    "heading": "7 Experiments",
    "text": "We run a simple experiment to show the empirical utility of parsing words—we compare a WCFG-based canonical segmenter with the semiMarkov segmenter introduced in Cotterell et al. (2016). We divide the corpus into 10 distinct train/dev/test splits with 5454 words for train and 1000 for each of dev and test. We report three evaluation metrics: full form accuracy, morpheme F1 (Van den Bosch and Daelemans, 1999) and average edit distance to the gold segmentation with boundaries marked by a distinguished symbol. For the WCFG model, we also report constituent F1— typical for sentential constituency parsing— as a baseline for future systems. This F1 measures how well we predict the whole tree (not just a segmentation). For all models, we use L2 regularization and run 100 epochs of ADAGRAD (Duchi et al., 2011) with early stopping. We tune the regularization coefficient by grid search considering λ ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}."
  }, {
    "heading": "7.1 Results and Discussion",
    "text": "Table 2 shows the results. The hierarchical WCFG model outperforms the flat semi-Markov model on\nall metrics on the segmentation task. This shows that modeling structure among the morphemes, indeed, does help segmentation. The largest improvements are found under the morpheme F1 metric (≈ 6.5 points). In contrast, accuracy improves by < 1%. Edit distance is in between with an improvement of 0.2 characters. Accuracy, in general, is an all or nothing metric since it requires getting every canonical segment correct. Morpheme F1, on the other hand, gives us partial credit. Thus, what this shows us is that the WCFG gets a lot more of the morphemes in the held-out set correct, even if it only gets a few more complete forms correct. We provide additional results evaluating the entire tree with constituency F1 as a future baseline."
  }, {
    "heading": "8 Conclusion",
    "text": "We presented a discriminative CFG-based model for canonical morphological segmentation and showed empirical improvements on its ability to segment words under three metrics. We argue that our hierarchical approach to modeling morphemes is more often appropriate than the traditional flat segmentation. Additionally, we have annotated 7454 words with a morphological constituency parse. The corpus is available online at\nhttp://ryancotterell.github.io/data/morphological-treebank to allow for exact comparison and to spark future research."
  }, {
    "heading": "Acknowledgements",
    "text": "The first author was supported by a DAAD LongTerm Research Grant and an NDSEG fellowship. The third author was supported by DFG (SCHU 2246/10-1)."
  }, {
    "heading": "A Derivation of Eq. 6",
    "text": "Here we provide the gradient of the log-partition function as an expectation:\n∇θ logZθ(w) = 1\nZθ(w) ∇θZθ(w) (7)\n= 1\nZθ(w) ∇θ\n\n\n∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n∑\nt′∈T (u′)\nscoreω(t ′, u′) · scoreη(u ′, a, w)\n\n\n= 1\nZθ(w)\n∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n∑\nt′∈T (u′)\n∇θ ( scoreω(t ′, u′) · scoreη(u ′, a, w) )\n= 1\nZθ(w)\n∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n∑\nt′∈T (u′)\n(\nscoreη(u ′, a, w) · ∇ωscoreω(t ′, u′)\n+ scoreω(t ′, u′) · ∇ηscoreη(u ′, a, w) )\n= 1\nZθ(w)\n∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n∑\nt′∈T (u′)\nscoreη(u ′, a, w) · scoreω(t ′, u′)\n\n\n∑\nπ∈Π(t′)\nf(π, u′)⊤ + g(u′, a, w)⊤\n\n\n= ∑\nu′∈Σ|w|+k\n∑\na∈A(u′,w)\n∑\nt′∈T (u′)\nscoreη(u ′, a, w) · scoreω(t ′, u′)\nZθ(w)\n\n\n∑\nπ∈Π(t′)\nf(π, u)⊤ + g(u′, a, w)⊤\n\n\n= E(t,a,u)∼pθ(·|w)\n\n\n∑\nπ∈Π(t)\nf(π, u)⊤ + g(u, a,w)⊤\n\n (8)\nThe result above can be further improved through Rao-Blackwellization. In this case, when we sample a tree–underlying form pair (t, u), we marginalize out all alignments that could have given rise to the sampled pair. The final derivation is show below:\n∇θ logZθ(w) = E(t,a,u)∼pθ(·|w)\n\n\n∑\nπ∈Π(t)\nf(π, u)⊤ + g(u, a,w)⊤\n\n\n= E(t,u)∼pθ(·|w)\n\n\n∑\nπ∈Π(t)\nf(π, u)⊤ + ∑\na∈A(u,w)\npθ(a | u,w)g(u, a,w) ⊤\n\n (9)\nThis estimator in eq. (9) will have lower variance than eq. (8)."
  }],
  "year": 2021,
  "references": [{
    "title": "On the use of morphological analysis for dialectal Arabic speech recognition",
    "authors": ["Mohamed Afify", "Ruhi Sarikaya", "Hong-Kwang Jeff Kuo", "Laurent Besacier", "Yuqing Gao."],
    "venue": "INTERSPEECH.",
    "year": 2006
  }, {
    "title": "Word Formation in Generative Grammar",
    "authors": ["Mark Aronoff."],
    "venue": "MIT Press.",
    "year": 1976
  }, {
    "title": "The CELEX lexical data base on CDROM",
    "authors": ["R. Harald Baayen", "Richard Piepenbrock", "Rijn van H"],
    "year": 1993
  }, {
    "title": "Trainable grammars for speech recognition",
    "authors": ["James K. Baker."],
    "venue": "The Journal of the Acoustical Society of America, 65(S1):S132–S132.",
    "year": 1979
  }, {
    "title": "Finitestate Morphology: Xerox Tools and Techniques",
    "authors": ["Kenneth R. Beesley", "Lauri Karttunen."],
    "venue": "CSLI, Stanford.",
    "year": 2003
  }, {
    "title": "Quick training of probabilistic neural nets by importance sampling",
    "authors": ["Yoshua Bengio", "Jean-Sébastien Senécal"],
    "venue": "In AISTATS",
    "year": 2003
  }, {
    "title": "Memory-based morphological analysis",
    "authors": ["Antal Van den Bosch", "Walter Daelemans."],
    "venue": "ACL.",
    "year": 1999
  }, {
    "title": "Adaptor grammars for learning non-concatenative morphology",
    "authors": ["Jan A. Botha", "Phil Blunsom."],
    "venue": "EMNLP, pages 345–356.",
    "year": 2013
  }, {
    "title": "Combining morpheme-based machine translation with postprocessing morpheme prediction",
    "authors": ["Ann Clifton", "Anoop Sarkar."],
    "venue": "ACL.",
    "year": 2011
  }, {
    "title": "Stochastic contextual edit distance and probabilistic FSTs",
    "authors": ["Ryan Cotterell", "Nanyun Peng", "Jason Eisner."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "A joint model of orthography and morphological segmentation",
    "authors": ["Ryan Cotterell", "Tim Vieira", "Hinrich Schütze."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Latent-variable modeling of string transductions with finite-state methods",
    "authors": ["Markus Dreyer", "Jason R. Smith", "Jason Eisner."],
    "venue": "EMNLP.",
    "year": 2008
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "JMLR, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Recurrent neural network grammars",
    "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "The Morpheme: A Theoretical Introduction, volume 31",
    "authors": ["David Embick."],
    "venue": "Walter de Gruyter GmbH & Co KG.",
    "year": 2015
  }, {
    "title": "Efficient, feature-based, conditional random field parsing",
    "authors": ["Jenny Rose Finkel", "Alex Kleeman", "Christopher D. Manning."],
    "venue": "ACL, volume 46, pages 959–967.",
    "year": 2008
  }, {
    "title": "Less grammar, more features",
    "authors": ["David Leo Wright Hall", "Greg Durrett", "Dan Klein."],
    "venue": "ACL, pages 228–237.",
    "year": 2014
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "ACL, pages 1–10.",
    "year": 2015
  }, {
    "title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models",
    "authors": ["Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater."],
    "venue": "NIPS, pages 641–648.",
    "year": 2006
  }, {
    "title": "Bayesian inference for PCFGs via Markov Chain Monte Carlo",
    "authors": ["Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater."],
    "venue": "HLT-NAACL, pages 139–146.",
    "year": 2007
  }, {
    "title": "Probabilistic Graphical Models: Principles and Techniques",
    "authors": ["Daphne Koller", "Nir Friedman."],
    "venue": "MIT press.",
    "year": 2009
  }, {
    "title": "The Oxford Handbook of Derivational Morphology",
    "authors": ["Rochelle Lieber", "Pavol Štekauer."],
    "venue": "Oxford University Press, USA.",
    "year": 2014
  }, {
    "title": "Topics in the Stress and Syntax of Words",
    "authors": ["Tatjana Marvin."],
    "venue": "Ph.D. thesis, Massachusetts Institute of Technology.",
    "year": 2002
  }, {
    "title": "Weighted finite-state transducers in speech recognition",
    "authors": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley."],
    "venue": "Computer Speech & Language, 16(1):69–88.",
    "year": 2002
  }, {
    "title": "Improving morphology induction by learning spelling rules",
    "authors": ["Jason Naradowsky", "Sharon Goldwater."],
    "venue": "IJCAI.",
    "year": 2009
  }, {
    "title": "Morphological segmentation for keyword spotting",
    "authors": ["Karthik Narasimhan", "Damianos Karakos", "Richard Schwartz", "Stavros Tsakalidis", "Regina Barzilay."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Morphology and logical form",
    "authors": ["David Pesetsky."],
    "venue": "Linguistic Inquiry, 16(2):193–246.",
    "year": 1985
  }, {
    "title": "Comparative study of minimally supervised morphological segmentation",
    "authors": ["Teemu Ruokolainen", "Oskar Kohonen", "Kairit Sirts", "StigArne Grönroos", "Mikko Kurimo", "Sami Virpioja."],
    "venue": "Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Disambiguation of morphological structure using a PCFG",
    "authors": ["Helmut Schmid."],
    "venue": "EMNLP, pages 515– 522. Association for Computational Linguistics.",
    "year": 2005
  }, {
    "title": "SMOR: A German computational morphology covering derivation, composition and inflection",
    "authors": ["Helmut Schmid", "Arne Fitschen", "Ulrich Heid."],
    "venue": "LREC.",
    "year": 2004
  }, {
    "title": "A graphbased lattice dependency parser for joint morphological segmentation and syntactic analysis",
    "authors": ["Wolfgang Seeker", "Özlem Çetinoğlu."],
    "venue": "TACL.",
    "year": 2015
  }, {
    "title": "The Syntax of Words",
    "authors": ["Elisabeth Selkirk."],
    "venue": "Number 7 in Linguistic Inquiry Monograph Series. MIT Press.",
    "year": 1982
  }, {
    "title": "Minimallysupervised morphological segmentation using adaptor grammars",
    "authors": ["Kairit Sirts", "Sharon Goldwater."],
    "venue": "TACL, 1:255–266.",
    "year": 2013
  }, {
    "title": "Weighted and probabilistic context-free grammars are equally expressive",
    "authors": ["Noah A. Smith", "Mark Johnson."],
    "venue": "Computational Linguistics, 33(4):477– 491.",
    "year": 2007
  }, {
    "title": "Character-level Chinese dependency parsing",
    "authors": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."],
    "venue": "ACL, pages 1326–1336.",
    "year": 2014
  }],
  "id": "SP:4e97532bf0c0697ce4842db524e885cf02eb7c26",
  "authors": [{
    "name": "Ryan Cotterell",
    "affiliations": []
  }, {
    "name": "Arun Kumar",
    "affiliations": []
  }],
  "abstractText": "Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area."
}