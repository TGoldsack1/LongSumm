{
  "sections": [{
    "heading": "1. Introduction and Background",
    "text": "As machine learning-based models become more widespread and grow in both scale and complexity, methods of interpreting their predictions are increasingly attracting attention from the machine learning community. Some of the applications and benefits of employing these methods outlined in previous work (Ancona et al., 2017) include (1) “debugging” the model to expose ways of model failures not discoverable via conventional test set performance measuring (e.g., data or target leakages);\n1Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands 2Yandex, Moscow, Russia 3Department of Mathematics, Princeton University, Princeton, NJ, USA. Correspondence to: Boris Sharchilev <bshar@yandex-team.ru>, Pavel Serdyukov <pavser@yandex-team.ru>, Maarten de Rijke <derijke@uva.nl>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1Supporting code for the paper is available at https:// github.com/bsharchilev/influence_boosting.\n(2) boosting developer’s trust in the model’s performance in scenarios when on-line evaluation is not available before deployment; and (3) increasing user satisfaction and/or confidence in provided predictions, etc. Various problem setups (Palczewska et al., 2013; Tolomei et al., 2017; Fong & Vedaldi, 2017) and interpretation methods, both model-agnostic (Ribeiro et al., 2016; Lundberg & Lee, 2017) and model-specific (Shrikumar et al., 2017; Tolomei et al., 2017; Sundararajan et al., 2017), have recently been proposed in the literature.\nA common trait shared by the majority of these methods is that they do not provide a way of automatically improving the model, since the model is fixed; the main use-case thus becomes manual analytics by the user or the developer, which is both time and resource-consuming. It is thus desirable to derive a framework for obtaining actionable insights into the model’s behavior allowing us to automatically improve a model’s performance.\nOne such framework has recently been introduced by Koh & Liang (2017); it deals with finding the most influential training objects. They formalize the notion of “influence” via an infinitesimal approximation to leave-one-out retraining: the core question that this work aims to answer is “how would the model’s performance on a test object xtest change if the weight of a training object xtrain is perturbed?” Assuming a smooth parametric model family (e.g., linear models or neural networks), the authors employ the Influence Functions framework from classical statistics (Cook & Weisberg (1980); also see Koh & Liang (2017) for a literature review on the topic) to show that this quantity can be estimated much faster than via straightforward model retraining, which makes their method tractable in a real-world scenario. A natural use-case of such a framework is to consider individual test objects (or groups of them) on which the model performs poorly and either remove the most “harmful” training objects or prioritize a batch of new objects for labeling based on which ones are expected to be the most “helpful,” akin to active learning.\nUnfortunately, the method suggested by Koh & Liang (2017) heavily relies on the smooth parametric nature of the model family. While this is a large class of machine learning models, it is by far not the only one. In particular, decision tree ensembles such as Random Forests (Ho, 1995, RF) and Gradient Boosted Decision Trees (Friedman, 2001, GBDT) are probably the most widely used model family in industry, largely due to their state-of-the-art performance on struc-\ntured and/or multimodal data. Thus, it is important to extend the aforementioned Influence Functions framework to tree ensembles.\nIn this paper, we propose a way of doing so, while focusing specifically on GBDT. We consider two proxy metrics for the informal notion of influence. For the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the trees’ structures) to derive LeafRefit and FastLeafRefit, a well-founded family of approximations to leave-one-out retraining that trade off approximation accuracy for computational complexity. For the second, analogously to the Influence Functions framework, we consider infinitesimal training sample weight perturbations and derive LeafInfluence and FastLeafInfluence, methods for estimating gradients of the model’s predictions with respect to training objects’ weights. From a theoretical perspective, LeafInfluence and FastLeafInfluence allow us to deal with the discontinuous dependency of tree structure on training sample perturbations; from a practical one, they allow us to further reduce computational complexity due to the possibility of precomputing certain derivatives.\nIn our experiments we (1) study the conditions under which our methods, FastLeafRefit and FastLeafInfluence, successfully approximate their proxy metrics, (2) demonstrate our methods’ ability to target training objects which are influential for specific test objects, and (3) show that our algorithms run much faster than straightforward retraining, which makes them applicable in practical scenarios."
  }, {
    "heading": "2. Problem Definition",
    "text": "First, we formally define the problem setup. We consider standard supervised training of a GBDT ensemble2\nF (x;w) := ∑T\nt=1 f t P (x)t (At−1) on a training sample Xtrain. Learning consists of two separate stages: model structure selection and picking the optimal leaf values. The way of choosing the model structure is not important for our work; we refer the interested reader to existing implementations, e.g., Chen & Guestrin (2016); Dorogush et al. (2017). For picking optimal leaf values, we consider two most commonly used formulas:\nGradient: At leaf l at step t, output negative average gradients (calculated at current predictions) over the leaf objects:\nf tG;l(A t−1) := − G\nt l(A t−1)\nHtG;l(A t−1)\n. (1)\nThis is equivalent to minimizing the empirical loss function w.r.t. the current leaf value by doing a single gradient step in function space (Chen & Guestrin, 2016).\nNewton: At leaf l at step t, output the negative total gradient 2Mathematical notations are defined in Table 1.\ndivided by the total second derivative over the leaf objects:\nf tH;l(A t−1) := − G\nt l(A t−1)\nHtH;l(A t−1)\n. (2)\nThis is equivalent to minimizing the empirical loss function w.r.t. the current leaf value by doing a single Newton step in function space (Chen & Guestrin, 2016)."
  }, {
    "heading": "3. Approach",
    "text": "In this section, we describe our approach to efficiently calculating the influence of training points. Since the notion of “influence” is not rigorously defined and partly intuitive, we need to introduce a well-defined, measurable quantity that aims to capture the desired intuition; we refer to it as a proxy for influence. In this work, we follow the general framework of Koh & Liang (2017) and quantify influence through train set perturbations. We consider two proxies that reflect two natural variations of this approach. First, we describe an algorithm for faster exact leave-one-out retraining of GBDT under the assumption that the model structure remains fixed, and explain how to use that framework for estimating the influence of training points on specific test samples. Secondly, we derive an iterative algorithm to compute gradients of GBDT predictions w.r.t. the weights of training sample and analyze the resulting expressions."
  }, {
    "heading": "3.1. Leave-One-Out Retraining",
    "text": "For the first proxy, following Koh & Liang (2017), we quantify the (negative) influence of a training sample xtrain on a model’s prediction on a test sample F (xtest;w) as the change of loss on xtest after retraining the model without xtrain:\nProxy 1. Infgrad(xtrain,xtest) := L(ytest, F (xtest)) − L(ytest, F̂\\xtrain(xtest)), where F̂\\xtrain is the model retrained without xtrain.\nSince, in order to rank the training points according to Infgrad(xtrain,xtest), we would have to compute Proxy 1 for each xtrain, straightforward leave-one-out retraining would be prohibitively expensive even for moderately-sized datasets. Moreover, as mentioned in Section 1, the parametric model framework of Koh & Liang (2017) is not directly applicable here. Thus, a solution tailored specifically for tree ensembles is required."
  }, {
    "heading": "3.1.1. LEAFREFIT",
    "text": "In the problem definition (Section 2) we noted that training each tree requires picking its structure and leaf values. Moreover, these two operations respond to small training set perturbations differently: the tree structure is piecewise constant (i.e., it either stays the same or changes abruptly), whereas leaf values change more smoothly. Thus, a natural assumption to make is:\nAssumption 1. The effect of removing a single training point can be estimated while treating each tree’s structure as fixed.\nUnder Assumption 1, it is thus sufficient to estimate how the leaf values of each tree are going to change. Since selecting optimal feature splits, e.g. via CART (Quinlan, 1986) or C4.5 (Quinlan, 2014) algorithms, is often the computational bottleneck in fitting decision trees, this observation already yields a significant complexity reduction.\nThus, our first algorithm for approximate leave-one-out retraining, LeafRefit, is equivalent to fixing the structure of every tree and fitting leaf values without the removed point. A formal listing of the resulting algorithm is given in Algorithm 1.\nNote that the effect of removing a training object xi is twofold: on each step, we have to (1) remove xi from its leaf (Algorithm 1, line 7) and (2) recalculate the leaf values and record the resulting changes of intermediate predictions for each training object (line 14). Thus, despite improving upon straightforward retraining by not having to search for the optimal tree splits, LeafRefit is still an expensive algorithm. Running it for each training sample has an asymptotic complexity ofO(Tn2); moreover, in practice, for each training step t it involves an expensive routine of recalculating derivatives for each training point.\nAlgorithm 1 LeafRefit 1: Input: training point index to remove i0, sample-to-leaf as-\nsignments {Itl }T,Lt=1,l=1, leaf formula type formula 2: Output: new leaf values {f̂ tl }T,Lt=1,l=1 3: Initialize ∆0i ← 0, A0i ← 0, i = 1 . . . n 4: for t = 1 to T do 5: Ât−1i ← A t−1 i + ∆ t−1 i , i = 1 . . . n 6: for l = 1 to L do 7: Îtl ← Itl \\ {i0} 8: if formula == Gradient then 9: f̂ tl ← fG;tl ({Â t−1 i }i∈Ît\nl )\n10: else 11: f̂ tl ← fN ;tl ({Â t−1 i }i∈Ît\nl )\n12: end if 13: ∆f tl ← f̂ tl − f tl 14: ∆ti ← ∆t−1i + ∆f t l , i ∈ Itl 15: end for 16: end for 17: return {f̂ tl }T,Lt=1,l=1"
  }, {
    "heading": "3.1.2. FASTLEAFREFIT",
    "text": "We seek to limit the number of calculations at each step of LeafRefit. Note that, in LeafRefit, we generally cannot make any use of caching the original first and/or second derivatives, since any ∆t−1i (Algorithm 1, line 14) can be nonzero, which forces us to recompute the derivatives for each object. We build on the intuition that, in practice, a lot of ∆t−1i may be negligible; an extreme example is when training samples can be separated in disjoint cliques, i.e., It1l = I t2 l ∀t1, t2 = 1, . . . , T , l = 1 . . . L. In this case, removing each training point only affects its clique Il0 := I 1 l0\n. Thus, at each training step t, we may select a subset of training samples3 U t whose deltas we take into account, and suppose Ât−1i = A t−1 i ∀i /∈ U t. We refer to U t as the update set. Combining this with caching the originalAt−1i and sums of derivatives in each leaf, we reduce the asymptotic complexity to O(TnC), where C = maxt |U t|, which is a significant reduction if C n. A formal listing of the resulting algorithm, FastLeafRefit, is given in Algorithm 2."
  }, {
    "heading": "3.1.3. SELECTING THE UPDATE SET",
    "text": "In Section 3.1.2, we introduced FastLeafRefit, an approximate algorithm potentially achieving lower complexity than LeafRefit. Its definition, however, allowed for an arbitrary choice of the update set U t telling us which training points’ prediction changes to take into account at boosting step t. It is intuitively clear that different strategies of selecting U t allow us to optimize the trade-off between computational complexity and quality of approximating leave-one-out retraining; thus, FastLeafRefit provides a principled way of obtaining approximations of different rigor to LeafRefit. Natural strategies for selecting the update set include:\nSinglePoint: don’t update any points’ predictions and only 3Methods of selecting U t will be given below.\nAlgorithm 2 FastLeafRefit\nInput: i0, {Itl }T,Lt=1,l=1, {g t i(A t−1 i )} T,n t=1,i=1, {hti(At−1i )} T,n t=1,i=1, {G t l(A\nt−1)}T,Lt=1,l=1, {Htl (At−1)}T,Lt=1,l=1, leaf formula type formula Output: New leaf values {f̂ tl }T,Lt=1,l=1 Initialize ∆0i ← 0, i = 1...n for t = 1 to T do\nU t ← UpdateSet(t) for l = 1 to L do\nU tl ← U t ∩ Itl f̂ tl ← LeafRecalc(t, l, {Itl }T,Lt=1,l=1, {g t i(A t−1 i )} T,n t=1,i=1, {hti(At−1i )} T,n t=1,i=1, G t l(A t−1), Htl (A t−1), U tl , formula) ∆f tl ← f̂ tl − f tl ∆ti ← ∆t−1i + ∆f t l , i ∈ Itl\nend for end for return {f̂ tl }T,Lt=1,l=1\nAlgorithm 3 LeafRecalc\nInput: boosting step t, leaf index l, {Itl } T,L t=1,l=1, {gti(A t−1 i )} T,n t=1,i=1, {hti(A t−1 i )} T,n t=1,i=1, G t l(A\nt−1), Htl (A\nt−1), U tl , leaf formula type formula Output: New leaf value f̂ tl I ← I[i0 ∈ Itl ] ∆ gtj ← gtj(A t−1 j + ∆ t−1 j )− gtj(A t−1 j ), j ∈ U tl\nĜtl ← Gtl(At−1) + ∑\nj∈Utl wj∆ g t j − Iwi0gti0(A t−1 i0 )\nif formula == Gradient then Ĥtl ← ∑ j∈Itl \\{i0}\nwj else\n∆htj ← htj(A t−1 j + ∆ t−1 j )− htj(A t−1 j ), j ∈ U tl Ĥtl ← Htl (At−1)+ ∑\nj∈Utl wj∆h t j−Iwi0hti0(A t−1 i0 )\nend if return − Ĝ t l\nĤtl\nignore the derivatives of i (the index of the training point to be removed) in each leaf, i.e., U t = ∅. Also note that this strategy is equivalent to disregarding dependencies between consecutive trees in GBDT and treating the ensemble like a Random Forest. Its complexity is O(Tn).\nAllPoints: make no approximations and update each point at each step, i.e., U t = {1, . . . , |Xtrain|}. This reduces FastLeafRefit to LeafRefit.\nTopKLeaves(k): this heuristic builds on the observation that, at each step t, each ∆tj , j ∈ Itl increases over ∆ t−1 j by the same amount ∆f tl across the leaf l (see Algorithm 2). ∆f tl ’s magnitude, in turn, is expected to be larger for leaves where ∆t−1j , j ∈ Itl (and, subsequently, ∆gtj) are already large. Informally, the “snowball” effect holds: the larger the change accumulated in the leaf so far, the greater its value will change. Thus, to exploit this intuition, TopKLeaves(k)\nonly updates ∆tj of training points in k leaves with the largest accumulated prediction change so far:\nU t = {i ∈ Itl | l ∈ {Ltj}kj=1}, Lt = argsort [ − ∑\ni∈Itl |∆t−1i |, l = 1 . . . L ] (3) Note: formally, this strategy is still O(Tn2) due to the fact that computing U t according to Eq. 3 takes O(n). In practice, overhead for computing Eq. 3 may be negligible because, firstly, sums of ∆t−1i can be quickly computed in a parallel or vectorized fashion and, secondly, because the complexity of addition is negligible compared to, e.g., calculating derivatives. However, if this still poses a problem, a natural way of getting around it is sampling m training points uniformly from Xtrain and using a sample estimator of Eq. 3. The complexity of FastLeafRefit thus becomes O(Tn[C +m]), which is useful if m n."
  }, {
    "heading": "3.2. Prediction gradients",
    "text": "In the previous sections we introduced LeafRefit and FastLeafRefit, fast methods of estimating the effect of a training sample on the GBDT ensemble, which can then be used to rank training points, e.g., by their influence on a test point of interest. Under Assumption 1, these methods are valid approximations of leave-one-out retraining, which gives them theoretical grounding. However, perhaps surprisingly, our experiments show that Assumption 1 fails quite often: in particular, for the Adult dataset (see Section 4.2), 58% of training points change the structures of at least one tree in the ensemble. Moreover, as shown in Section 4.3, when Assumption 1 is violated, LeafRefit and FastLeafRefit are no longer valid approximations to Proxy 1.\nThe intuition underlying Assumption 1, however, still holds: for a small enough perturbation to the training data, the structure will remain fixed, whereas leaf values will still be changing smoothly. Note that retraining the model without a sample i is equivalent to setting wnewi = w old i + ∆wi; ∆wi = −woldi . This change may be large enough to trigger structural shifts in the ensemble; thus, we need a tool to study a model’s response to smaller (arbitrarily small) perturbations.\nAn obvious choice for such a tool is the derivative of a model’s prediction w.r.t. a sample’s weight, which was also a crucial tool in the Influence Functions framework from classical statistics (Cook & Weisberg, 1980):\nProxy 2. Infgrad(xtrain,xtest) := ∂L(ytest,F (xtest))∂wi(xtrain) , where i(xtrain) is the index of xtrain in Xtrain."
  }, {
    "heading": "3.2.1. LEAFINFLUENCE",
    "text": "As mentioned above, in the setup of Proxy 2 the statement of Assumption 1 is now guaranteed to hold and is no longer an assumption; we may consider the tree structures to be fixed and only study perturbations of leaf values, which smoothly\ndepend on the weights. Using the chain rule\n∂L(y, F (x;w)) ∂wi = ∂L(y, z) ∂z |z=F (x;w) · ∂F (x;w) ∂wi , (4)\nwe can then derive various counterfactuals (e.g., “how would the loss on a test point change if we upweight a training point i?”), similarly to Koh & Liang (2017). Since we have\n∂F (x;w)\n∂wi = T∑ t=1 ∂f tP (x)t(A t−1) ∂wi , (5)\nfor applying Eq. 4 to arbitrary x (for a fixed i) it is necessary and sufficient to calculate ∂f t l (A\nt−1) ∂wi\n, t = 1 . . . T , l = 1 . . . L. Applying Eq. 4 can then be done by running x though a new tree ensemble having {∂f t l (A\nt−1) ∂wi\n}T,Lt=1,l=1 as leaf values.\nExpressions for leaf value derivatives depend on the type of leaf formula:4 Proposition 1. Leaf value derivatives are given by: ∂f tG;l ∂wi = − Itl (i)(f t G;l+g t i)+ ∑ j∈It l wjh t jJ(A t−1)ij Ht G;l and\n∂f tH;l ∂wi = − Itl (i)(h t if t H;l+g t i)+\n∑ j∈It\nl wj(k\nt jf t H;l+h t j)J(A t−1)ij\nHt H;l\n,\n(6)\nwhere Itl (i) := I[i ∈ Itl ] and J(At)ij := ∂Atj(w) ∂wi .\nProof. First, let us derive the desired expression5 for f tG;l(A t−1(w),w):\n∂f tG;l(A t−1(w),w)\n∂wi = − ∂ ∂wi\n[ Gtl(A t−1(w),w)\nHtG;l(A t−1(w),w)\n] =\n= − ∂Gtl(A t−1(w),w) ∂wi HtG;l(A t−1(w),w)\nHtG;l(A t−1(w),w)2\n+\n+\n∂HtG;l(A t−1(w),w)\n∂wi Gtl(A t−1(w),w) HtG;l(A t−1(w),w)2\n(7) Let us calculate the derivatives of Gtl(A\nt−1(w),w) and HtG;l(A t−1(w),w) separately:\n∂Gtl(A t−1(w),w)\n∂wi = = ∑ j∈Itl [ δijg t j(A t−1 j (w)) + wjh t j(A t−1 j (w))J(A t−1)ij ] =\n= Itl (i)g t i(A t−1 i (w)) + ∑ j∈Itl wjh t j(A t−1 j (w))J(A t−1)ij ; ∂HtG;l(A t−1(w),w)\n∂wi = Itl (i)\n4In Proposition 1’s statement, arguments such as w or Ati are dropped for brevity.\n5Throughout this proof, we add w as an extra argument to the functions we study in order to highlight the dependency.\nPlugging this back into Equations 7 and grouping terms with and without Itl (i) separately, we get:\n∂f tG;l(A t−1(w),w)\n∂wi = −Itl (i)\nf tG;l + g t i(A t−1 i (w)) HtG;l(A t−1(w),w) −\n− ∑ j∈Itl wjh t j(A t−1 j (w))J(A t−1)ij\nHtG;l(A t−1(w),w)\n,\nwhich proves the first part of Proposition 1.\nFor the second part, all we have to change is to substitute HtH;l(A t−1(w),w) for HtG;l(A t−1(w),w). Its derivative is given by\n∂HtG;l(A t−1(w),w)\n∂wi = = ∑ j∈Itl [ δijh t j(A t−1 j (w)) + wjk t j(A t−1 j (w))J(A t−1)ij ] =\n= Itl (i)h t i(A t−1 i (w)) + ∑ j∈Itl wjk t j(A t−1 j (w))J(A t−1)ij\nJust like before, plugging it back into Equations 7 and grouping terms containing and not containing Itl (i) separately, we get:\n∂f tH;l(A t−1(w),w)\n∂wi = −Itl (i)\nhti(A t−1 i )f t H;l + g t i(A t−1 i )\nHtH;l(A t−1(w),w)\n−\n− ∑ j∈Itl wj(k t j(A t−1 j )f t H;l + h t j(A t−1 j ))J(A t−1)ij\nHtH;l(A t−1(w),w)\n.\nThis concludes the proof of Proposition 1.\nIt can be seen from Eq. 6 that leaf value derivatives at step t depend on the Jacobi matrix J(At−1)ij . These values, in turn, are connected by a recursive relationship:\nJ(At)ij = J(A t−1)ij + ∂f tP (xj)t ∂wi . (8)\nThus, we can calculate leaf value derivatives in an iterative fashion similar to LeafRefit. A formal listing of the resulting algorithm, LeafInfluence, can be found in Algorithm 4. Besides providing means for analyzing small weight perturbations, two important traits yielding complexity reductions can be seen from Eq. 6:\nA. Using Eq. 6, we can write out ∇wf tl = ( ∂ftl ∂wi )n i=1 in vector form; since computing each ∂f t l\n∂wi involves addition\nand a vector dot product, ∇wf tl can then be expressed via vector addition and matrix/vector product for easy parallelization/vectorization.\nB. The derivatives {gtj , htj , ktj} T,n t=1,j=1 used in Eq. 6 can now be precomputed only once during GBDT training and not for\nAlgorithm 4 LeafInfluence Inputs: training point index i0, sample-to-leaf assignments {Itl } T,L t=1,l=1, {gti(A t−1 i )} T,n t=1,i=1,\n{hti(A t−1 i )} T,n t=1,i=1, {kti(A t−1 i )} T,n t=1,i=1, leaf formula type formula Outputs: leaf value derivatives {∂f t l (A\nt−1) ∂wi\n}T,Lt=1,l=1 J(A0)ij ← 0, i = 1 . . . n, j = 1 . . . n for t = 1 to T do\n∂ftl (A t−1)\n∂wi0 ←/According to Eq. 6/, l = 1 . . . L\nJ(At)ij ←/According to Eq. 8/,i = 1 . . . n, j = 1 . . . n\nend for return {∂f t l (A\nt−1) ∂wi }T,Lt=1,l=1\neach training object i whose influence we want to compute. This contrasts LeafInfluence with LeafRefit and FastLeafRefit, where these derivatives had to be recalculated for each i depending on the values of ∆t−1j , which change for different i."
  }, {
    "heading": "3.2.2. FASTLEAFINFLUENCE",
    "text": "The final step to be made is analogous to the transition from LeafRefit to FastLeafRefit: LeafInfluence is, again, O(Tn2) because it has to compute matrix/vector products with the matrix J(At−1)ij for every t. The same approximation that powers FastLeafRefit can be made here as well: at each step, we can select an update set U t and only take into account the influences of a subset of training objects on At−1. This is equivalent to assuming J(At−1)ij = 0 ∀ j /∈ U t, making J(At−1)ij a sparse matrix with the number of nonzero elements in each row bounded by C := maxt |U t|. Strategies of selecting U t and the resulting asymptotics become the same as described in Section 3.1.3, with the additional benefit of being able to compute the derivatives “off-line.”"
  }, {
    "heading": "4. Experiments",
    "text": ""
  }, {
    "heading": "4.1. Research Questions",
    "text": "The experiments that we conduct can be broadly categorized as serving two purposes: (1) studying the fundamentals of our framework and (2) evaluating its quality in two applied problem setups. For the first part, the research questions that we seek to answer are as follows:\nRQ1. How well do the different methods introduced in Sections 3.1 and 3.2 approximate their respective influence proxies?\nRQ2. Do smaller update sets significantly reduce the runtimes of FastLeafRefit and FastLeafInfluence? Does FastLeafInfluence yield a notable runtime speedup over FastLeafRefit?\nFor the second part, we proceed by considering two ap-\nplied scenarios: (1) classification in the presence of label noise, and (2) classification with train/test domain mismatch. Specifically, the research questions for this part are:\nRQ3. For Scenario 1, do our methods allow to detect noise in general and, more specifically, to identify training objects most harmful for specific test points?\nRQ4. For Scenario 1, how do the proxies and their respective approximations compare in terms of quality?\nRQ5. For Scenario 2, are our methods capable of detecting domain mismatch and, moreover, providing recommendations on how to fix it?"
  }, {
    "heading": "4.2. Datasets and Framework",
    "text": "For our experiments with GBDT, we use CatBoost(cat, 2018) an open-source implementation of GBDT by Yandex6. The datasets used for evaluation are: (1) Adult Data Set (Adult, (dat, 1996)), (2) Amazon Employee Access Challenge dataset (Amazon, (dat, 2013)), (3) the KDD Cup 2009 Upselling dataset (Upselling, (dat, 2009)) and, for the domain mismatch experiment, (4) the Hospital Readmission dataset (Strack et al., 2014). Dataset statistics and corresponding CatBoost parameters can be found in the supplementary material. Since we approach the problem as a search (for influential examples) problem, the main metrics we will be using are ranking metrics - specifically, DCG and NDCG(dcg, 2018) with linear gains."
  }, {
    "heading": "4.3. Proxy Approximation Quality",
    "text": "Here, we evaluate how well do variations of FastLeafRefit and FastLeafInfluence match their respective Proxies 1 and 2. For that, we use the Adult Data Set. For LeafRefit, its validity heavily depends on whether Assumption 1 holds; thus, we split the training points into two disjoint sets based on whether they violate Assumption 1 (Changed in Table 2) or not. We then randomly sample n = 2000 points from both groups to ensure that they are equal in size and, in both of them, for each test object, we rank the train points with respect to their influence on this test object. We then measure NDCG@100 with respect to the relevance labels produced by ground-truth rankings induced by the respective proxies for LeafRefit and FastLeafRefit, Proxy 1 and Proxy 2. Finally, we average the results over the test objects. Results are given in Table 2.\nAnalysis of Table 2 answers our RQ1. Firstly, as expected, LeafRefit and its faster variations only approximate Proxy 1 when Assumption 1 holds. When it does, the quality of FastLeafRefit uniformly increases with the update set size, reaching perfect results for Top64Leaves, which is equivalent to LeafRefit. On the other hand, LeafInfluence approximates Proxy 2 regardless of Assumption 1; the dependency of FastLeafInfluence on the update set is\n6We use the “Plain” mode which disables CatBoost’s conceptual modifications to the standard GBDT scheme.\nanalogous to that of FastLeafRefit. This shows that LeafInfluence is more robust in approximating its corresponding proxy than LeafRefit."
  }, {
    "heading": "4.4. Runtime Comparison",
    "text": "In this section, we compare different variations (update set choices) of FastLeafRefit and FastLeafInfluence in terms of their runtimes. For each dataset used in the study, we randomly pick k = 100 training objects for influence evaluation, calculate the resulting change in the model (new leaf values for FastLeafRefit and leaf value derivatives for FastLeafInfluence), measure the total elapsed wall time and divide the result by k to obtain the average time elapsed per one training object. The results are given in Fig. 1. Firstly, as expected, we observe that smaller update sets considerably reduce the runtimes of our algorithms, with the most radical speedup yielded by SinglePoint due to not having to recalculate any derivatives at all. Secondly, quite naturally, FastLeafInfluence performs much faster than FastLeafRefit, presumably due to vectorization and gradient precomputation (see end of Section 3.2.1). These observations confirm RQ2."
  }, {
    "heading": "4.5. Harmful Object Removal",
    "text": "In this experiment, we consider a particular use-case scenario, classification in the presence of label noise, and evaluate whether our methods are able to identify training objects that are (1) noisy, (2) harmful for specific test objects. In order to do that, we randomly select k training samples,7 flip their labels, and obtain GBDT’s predictions on test data\n7We set k = 4000 for Adult and Amazon, and k = 3500 for Upselling.\nbefore and after noise injection. We then conduct two experiments:\nA. We sort the training points in ascending order of average influence on test objects and measure ROC-AUC of noise detection. In addition to variations of FastLeafRefit and FastLeafInfluence, we also compare against (1) A noise detection method exploiting the problem structure, which scores the training points using GBDT’s prediction in favor of the class opposite to its observed label (Detector), (2) actual loss changes after leave-one-out retraining (Leave-One-Out), and (3) ground-truth binary labels of the train object being noisy (Oracle). The results are given in Fig. 2.\nB. We select n = 50 test points that suffered the largest Logloss increase, thus simulating problematic test objects. For each of these objects, we sort the training points in ascending order of influence and incrementally remove them from the training set in batches of m = 50 objects; on each iteration we measure the relative Logloss reduction both on this given test object and on the whole test set Xtest and, similarly to ranking, calculate DCG using these reductions as gains. Finally, we average these metrics over the n test points. The results are given in Fig. 3a and 3b.\nFirstly, from Fig. 2, we note that all variations of FastLeafRefit and FastLeafInfluence perform strongly on the over-\nall noise detection problem, where they score close to the top-performing Detector. Secondly, our methods greatly outperform their competitors (shown in blue on Fig. 3a) in targeting training objects harmful for a particular test object. These two observations confirm the hypothesis of RQ3. Finally, the two parts on Fig. 3 address RQ4 by clearly showing the way in which larger update sets increase quality: while all approximations score comparably in targeting particular test objects, smaller update sets lead to worsening the overall test quality (except for Upselling); in other words, smaller update sets lead to overfitting the targeted test object. Proper configurations of TopKLeaves, on the other hand, allow to “fix” a specific test object without overfitting it (k=8, 22, 64 for Adult and Amazon)."
  }, {
    "heading": "4.6. Debugging Domain Mismatch",
    "text": "A common issue in the supervised machine learning is domain mismatch. This is a situation, when the joint distribution of points in the test dataset Xtest differs from the one in the labeled training dataset Xtrain. Often in such scenarios, a model fine-tuned on the training dataset fails to produce accurate predictions on the test data. A standard way to cope with this problem is re-weighting Xtrain.\nIn the following experiment we demonstrate how our methods allow to detect domain mismatch and get a hint on how the distribution of points in Xtrain should be modified in order to better match the distribution of points in Xtest. The design of this experiment is a modification of the corresponding use-case of Koh & Liang (2017). We use the same Hospital dataset (see Section 4.2), with each point being a hospital patient represented by 127 features and the goal is to predict the readmission. To introduce domain mismatch we bias the distribution in the training dataset by filtering out a subsample of patients with age ∈ [40; 50) and label y = 1. Originally we had 169/1853 readmitted patients in this group and 2140/20000 overall; after we get 17/1601 in the age ∈ [40; 50) group and 1988/19848 overall. Clearly, the distribution of labels in this specific age group becomes highly biased, while the proportion of positive labels in the whole dataset changes slightly (from 10.7% to 10.0%).\nTraining set Xtrain is naturally split into four parts {Xitrain}4i=1 depending on the value of y and whether age ∈ [40; 50). One would expect that in the modified training dataset, samples with age ∈ [40; 50) and y = 1 are the most (positively) influential, so their removal will be the most harmful for the performance on the test dataset, while the removal of the samples with age ∈ [40; 50) and y = 0 might even be beneficial, since it is the most straightforward way to align the distributions in the test and train datasets. Below we confirm this expectation.\nLet us focus on the subset X0test := {x ∈ Xtest | age(x) ∈ [40; 50)}, since its elements are expected to be the most affected by the introduced domain mismatch. We sample 100 points from every part {Xitrain}4i=1 (or take the whole\npart, if it has < 100 points). For each of the methods FastLeafRefit and FastLeafInfluence with various update sets we compute the influence of the training samples on X0test. Specifically, (a) with FastLeafRefit, for an element x ∈ Xtrain we find the average Logloss reduction on X0test, introduced by removing x; (b) with FastLeafInfluence, for an element x ∈ Xtrain we find the derivative of the average Logloss on X0test with respect to the weightw of x atw = 1.\nTable 3 provides the average influence among the sampled train points with the fixed label y ∈ {0, 1} and the fixed indicator I(age ∈ [40; 50)) ∈ {0, 1}. As expected, with all methods, the train samples of the same type are consistently the most influential, which corresponds to maximal loss increase upon addition. In all cases removal of elements with y = 0 and age ∈ [40; 50) is estimated to be profitable, also confirming the initial expectations. These results allow us to answer RQ5 in the positive."
  }, {
    "heading": "5. Conclusion",
    "text": "In this work, we addressed the problem of finding train objects that exerted the largest influence on the GBDT’s prediction on a particular test object. Building on the Influence Function framework for parametric models, we derived LeafRefit and LeafInfluence, methods for estimating influences based on their respective proxy metrics, Proxies 1 and 2. By utilizing the structure of tree ensembles, we also derived computationally efficient approximations to these methods, FastLeafRefit and FastLeafInfluence. In our experiments, through considering several applied scenarios, we showed the practical applicability of these approaches, as well as their ability to produce actionable insights allowing to improve the existing model."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Anna Veronika Dorogush for valuable commentary and discussions, as well as technical assistance with the CatBoost library."
  }],
  "year": 2018,
  "references": [{
    "title": "A unified view of gradient-based attribution methods for deep neural networks",
    "authors": ["M. Ancona", "E. Ceolini", "C. Öztireli", "M. Gross"],
    "venue": "arXiv preprint arXiv:1711.06104,",
    "year": 2017
  }, {
    "title": "Xgboost: A scalable tree boosting system",
    "authors": ["T. Chen", "C. Guestrin"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "Characterizations of an empirical influence function for detecting influential cases in regression",
    "authors": ["R.D. Cook", "S. Weisberg"],
    "year": 1980
  }, {
    "title": "Fighting biases with dynamic boosting",
    "authors": ["A.V. Dorogush", "A. Gulin", "G. Gusev", "N. Kazeev", "L.O. Prokhorenkova", "A. Vorobev"],
    "venue": "arXiv preprint arXiv:1706.09516,",
    "year": 2017
  }, {
    "title": "Interpretable explanations of black boxes by meaningful perturbation",
    "authors": ["R.C. Fong", "A. Vedaldi"],
    "venue": "arXiv preprint arXiv:1704.03296,",
    "year": 2017
  }, {
    "title": "Greedy function approximation: a gradient boosting machine",
    "authors": ["J.H. Friedman"],
    "venue": "Annals of statistics,",
    "year": 2001
  }, {
    "title": "Random decision forests",
    "authors": ["T.K. Ho"],
    "venue": "In Proceedings of the 3rd International Conference on Document Analysis and Recognition,",
    "year": 1995
  }, {
    "title": "Understanding black-box predictions via influence functions",
    "authors": ["P.W. Koh", "P. Liang"],
    "venue": "arXiv preprint arXiv:1703.04730,",
    "year": 2017
  }, {
    "title": "Consistent feature attribution for tree ensembles",
    "authors": ["S.M. Lundberg", "S. Lee"],
    "venue": "arXiv preprint arXiv:1706.06060,",
    "year": 2017
  }, {
    "title": "Interpreting random forest models using a feature contribution method",
    "authors": ["A. Palczewska", "J. Palczewski", "R.M. Robinson", "D. Neagu"],
    "venue": "In Information Reuse and Integration (IRI),",
    "year": 2013
  }, {
    "title": "Induction of decision trees",
    "authors": ["J.R. Quinlan"],
    "venue": "Machine learning,",
    "year": 1986
  }, {
    "title": "5: programs for machine learning",
    "authors": ["Quinlan", "J.R. C"],
    "year": 2014
  }, {
    "title": "Why should I trust you?: Explaining the predictions of any classifier",
    "authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "Learning important features through propagating activation differences",
    "authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"],
    "venue": "arXiv preprint arXiv:1704.02685,",
    "year": 2017
  }, {
    "title": "Impact of hba1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient",
    "authors": ["B. Strack", "J.P. DeShazo", "C. Gennings", "J.L. Olmo", "S. Ventura", "K.J. Cios", "J.N. Clore"],
    "venue": "records. BioMed research international,",
    "year": 2014
  }, {
    "title": "Axiomatic attribution for deep networks",
    "authors": ["M. Sundararajan", "A. Taly", "Q. Yan"],
    "venue": "arXiv preprint arXiv:1703.01365,",
    "year": 2017
  }, {
    "title": "Interpretable predictions of tree-based ensembles via actionable feature tweaking",
    "authors": ["G. Tolomei", "F. Silvestri", "A. Haines", "M. Lalmas"],
    "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2017
  }],
  "id": "SP:217d17ea1d304981782976aefbe295f0ebb77213",
  "authors": [{
    "name": "Boris Sharchilev",
    "affiliations": []
  }, {
    "name": "Yury Ustinovsky",
    "affiliations": []
  }, {
    "name": "Pavel Serdyukov",
    "affiliations": []
  }, {
    "name": "Maarten de Rijke",
    "affiliations": []
  }],
  "abstractText": "We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model’s predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.1",
  "title": "Finding Influential Training Samples for Gradient Boosted Decision Trees"
}