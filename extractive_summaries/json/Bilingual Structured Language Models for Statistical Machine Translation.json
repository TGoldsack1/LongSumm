{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Many model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by\nintegrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014).\nThis paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic.\nThere is a variety of ways syntax can be used in a PBSMT model. Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it. For example, Cherry (2008) defines soft constraints based on the notion of syntactic cohesion (Section 2). Ge (2010) captures reordering patterns by defining soft constraints based on the currently translated word’s POS tag and the words structurally related to it. On the other hand, target syntax is more challenging to use in PBSMT, since a target-side syntactic model does not have access to the whole target sentence at decoding. Post and Gildea (2008) is one of the few targetside syntactic approaches applicable to PBSMT, but it has been shown not to improve translation. Their approach uses a target side parser as a language model: one of the reasons why it fails is that a parser assumes its input to be grammatical and chooses the most likely parse for it. What we are interested in during translation is how gram-\n2398\nmatical the target sentence actually is. In addition to reordering constraints, source syntax can be used for target-side language modeling. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows:\n• we propose a novel method to adapt monolingual structured language models (Chelba and Jelinek, 2000) (Section 3) to a PBSMT system (Section 4), which does not require an external on-the-fly parser, but only uses the given source-side syntactic analysis to infer structural relations between target words;\n• building on the existing literature, we propose a set of deterministic rules that incrementally build up a parse of a target translation hypothesis based on the source parse (Section 4);\n• we evaluate our models in a series of rescoring experiments and achieve statistically significant improvements of up to 0.7 BLEU for Chinese-English (Section 5).\nBefore describing the models, we motivate our method with a common assumption about crosslingual correspondence (Section 2)."
  }, {
    "heading": "2 Direct correspondence assumption and syntactic cohesion in SMT",
    "text": "Before we apply the syntactic model introduced in Section 3 to the bilingual setting (Section 4), we first explain two widely used assumptions about syntactic correspondence across languages.\nWe take a dependency tree to be a syntactic representation of a sentence and reason about other syntactic assumptions and models in its terms. In this work, we choose a dependency structure over a constituency structure because the former\nis more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W , and we will think of it as a relation between words of W , such that D(w, v) if w is a parent (head) of v (v being a child/modifier). D can be generalized to D∗ which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D∗(w, v) if D(w, v) or if ∃u s.t. D(w, u) ∧ D∗(u, v)). We assume unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = w1, . . . , wn is projective, if for every word pair wi, wj ∈ W s.t. D(wi, wj) it holds that every wk ∈ W s.t. i < k < j or j < k < i is a descendant of wi, i.e., D∗(wi, wk); see Figure 1.\nMost NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistically, as translation equivalence implies semantic equivalence and therefore thematic relations are preserved (Hwa et al., 2002). Thus dependency relations are preserved, as they are defined based on thematic relations between words. On the other hand, there is plenty empirical evidence supporting the violation of DCA under certain conditions (Hwa et al., 2002). For instance, even semantically very close sentences in different languages may have a different number of\n1A dependency parse (a dependency tree analysis of a sentence) is more primitive because every constituency parse can be formalized as a projective dependency parse with labeled relations, but not vice versa (Osborne, 2008).\nwords. Syntactic divergence increases if the two languages are typologically different.\nEven though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments.\nClosely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wi andwj but not bywk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the cohesion principle. For example, if we imagine a situation where a subtree as in Figure 2(b) is translated as a whole with one phrase application (and not word by word), then it does not violate the cohesion principle, although it is internally\nuncohesive. Both our approach and Cherry (2008) implement the idea of conforming the target translation to the source syntactic structure, but in different ways. Approaches like Cherry (2008) define principles that constrain the decoder in order to produce better translations. Our goal is to have a model that allows for a more direct way of evaluation of how well-formed the target translation is. In Section 5 we compare translation performance of the two approaches."
  }, {
    "heading": "3 Structured language models",
    "text": "As discussed in Sections 1 and 2, we would like to test how much a PBSMT can benefit from an additional syntax-based LM. In this section, we describe a syntactic language model, structured LM (SLM) (Chelba and Jelinek, 2000), that we extend to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system.\nThe linguistic intuition behind SLMs is that the structural children of a word do not essentially change its distributional properties but just provide additional specification. In Figure 3(a) the word president has two modifiers: the and former and it follows yesterday (an adjunct) and precedes met (a predicate). This ordering is correct in English. If instead its modifier was a or an entire relative clause, it would not make it incorrect.\nTo capture this observation, (Chelba and Jelinek, 2000) propose a language model where each word wi of a sentence W is predicted by an ordered subset of the words preceding wi. This conditioning subset is selected based on the syntactic properties of the preceding sequence Wi−1: the strong predictors are kept and the weak ones are left out. The strong predictors are the set of exposed heads. Given a subsequence Wi−1 and its associated parseDi−1, exposed heads are the roots of all the disconnected subtrees inDi−1. Note that\na parseDi−1 is not necessarily fully connected and thus a word can have multiple conditioning words.\nFor an example, consider again Figure 3(a). In a left-to-right scenario, when met is generated, a regular n-gram LM conditions it on yesterday the former president, while a SLM conditions it on yesterday president, since these two words are the exposed heads with respect to met (Figure 3(b)). The words the and former are modifiers of president and they get filtered out. Thus we obtain a less specific conditioning history, which may lead to the resulting model being less sparse. Another potential benefit is that SLMs can capture longdistance reordering: If president had as its modifier a relative clause (Figure 3(c)) then a simple n-gram LM would be conditioned on days before (assuming n = 3), while an SLM would condition met on yesterday president.\nSummarizing the ideas of words being conditioned on a structurally defined subset of the preceding sentence, Chelba and Jelinek (2000) formalize the generation process of W as follows:2 Each new word wi is conditioned on a\n2The original model by (Chelba and Jelinek, 2000) is defined in terms of a lexicalized constituency grammar, but as\nsequence of exposed heads Expos(W,D). Then a tag ti is predicted, and the parse Di−i of Wi−1 is extended to Di incorporating wi and ti (where Wi−1 is the prefix of W preceding wi): p(W,D) = |W |∏ i=1 p(wi|Expos(Wi−1, Di−1))\n· p(ti|wi, Expos(Wi−1, Di−1)) · p(Di|wi, ti, Expos(Wi−1, Di−1)).\n(1)\nThey use a shift-reduce parser with reduce-left, reduce-right, and shift operations."
  }, {
    "heading": "4 Bilingual structured language models",
    "text": "In this section, we combine the direct correspondence assumption (Section 2) and SLMs (Section 3), and define bilingual structured language models (BiSLMs) for PBSMT. Structured LMs have been successfully applied in SMT before. Yamada and Knight (2001) use SLMs in a stringto-tree SMT system where a derivation of a targetside parse tree is part of the decoding algorithm, and target syntactic representations are obtained ‘for free’. Yu et al. (2014) use an on-the-fly shiftreduce parser to build an incremental target parse.\nThe approaches sketched above rely on resources that a standard PBSMT system does not have access to by default. Phrase-based decoders do not provide us with a parse of the target sentence, and inferring the parse of a target string with an external parser is computationally expensive and potentially unreliable (see Section 1). Our main insight is that in a bilingual setting one does not need an additional probabilistic target parsing model. We assume that the source parse is given (precomputed) and that the DCA (Section 2) holds, and project the parse deterministically onto the target side via word alignments3. We obtain the following equation:\np(T |S,DS) = |T |∏ i=1 p(ti| Expos(Ti−1,\nProjP(DS , S, Ti−1))),\n(2)\nwhere T is a target sentence, Ti−1 is the sequence in T preceding the i-th target word ti, S is a\nwe discussed in Section 2, constituency parses can be transformed into dependency parses.\n3Phrase-internal word alignments are stored in the phrase table and are available at decoding time, see Section 4.4.\nsource sentence,DS is a source dependency parse, and ProjP is a function that returns a partial target parse DT i−1 by projecting DS onto Ti−1. In words, at each time step iwe predict the next word ti conditioned on the exposed heads of the partial parse of Ti−1 projected from the source side. We limit Expos to returning the four preceding exposed heads.4 Because the function ProjP is deterministic and because we do not have to predict tags for words, Equation 2 is simpler than Equation 1.\nWe first illustrate Equation 2 with an example in Figure 4. Since word alignment is monotonic in Figure 4(a), it is straightforward to project the source dependencies onto the target side. We aim to imitate a monolingual parser in the way we build up our projected parse: Reduce operations should be invoked whenever both of the subtrees involved in the operation are complete, i.e., are not expected to have any more modifiers (Section 4.2). For example, when the target word likes is produced its exposed heads are said and he (Figure 4(b)), since Putin is a modifier of said. Likewise, the exposed heads for women are said likes all Russian (Figure 4(c)).\nIn what follows we discuss how to define ProjP. Compared to projection approaches like (Quirk\n4As written above, we choose the dependency structures over the lexicalized constituency ones because the latter can be mapped to the former. It is thus more likely that a projected dependency tree is still be a well-formed parse, than a projected constituency tree. We decided to work with structural models that are more flexible, but one may also define BiSLM in terms of the more constraining constituency trees and see if the such model has better generalization power.\nand Menezes, 2006), we would like our model to project a source parse incrementally, allowing it to be used in a PBSMT decoder. We think of ProjP as a function that computes the output in two stages: first, it infers from the source parse the dependency relations between target words (Section 4.1), second, it decides how to parse the target sequence, i.e. in which order to assign these dependencies (Section 4.2). Additionally, in Section 4.3 we propose to use additional labelings of target words, and in Section 4.4 we describe some important implementation details."
  }, {
    "heading": "4.1 Dependency graph projection",
    "text": "Adoption of DCA (Section 2) allows to build up a target dependency tree from a source tree by projecting the latter through word alignments. The definition of DCA can be rephrased as requiring a one-to-one correspondence map between words of a sentence pair, allowing one to unambiguously map dependencies: Given a source parse, if t1 is the head of t2, then map(t1) is the head of map(t2). The correspondence relation that we have in PBSMT is the word alignment align: in the most general case, it is a many-to-many correspondence, and the straightforward projection described above can lead to incorrect dependency structures. To overcome these problems, we describe a simple ordered set of projection rules, based on the ones specified by (Quirk and Menezes, 2006) (and we point out if otherwise).\nThe general idea behind this set of rules is to extract a one-to-one function align1−1 from source words to target words from align and use it to project source dependencies as described in the paragraph above (R1 below). We then use additional rules (R2-R4 below) for the target words that are not in align1−1. Given a source sentence S with a parse DS , a target sentence T and word alignment align, align1−1 is extracted as follows: For all ti ∈ T with multiple aligned source words {si1 , si2 , ...} only align1−1(si1) = ti (only leftmost source word is kept, the links from the rest of the source words are removed5). For all si ∈ S with aligned target words {ti1 , ti2 , ...} keep the link only for the leftmost aligned target word: align1−1(si) = ti1 . For example, in Figure 5(b) the link between f0 and e1 is not in align1−1, and in Figure 5(c) the link between f1 and e0 is removed (and the arc from f2 to f1 is not projected).\n5This is an ad-hoc solution, other heuristics could be used.\nThe following rules should be applied in order (as else-if conditions). Given a source sentence S with a parse DS , a target sentence T and word alignment align between them, ti ∈ T is a head of tj ∈ T (i.e. DT (ti, tj)): (R1) if there are sk, sl ∈ S s.t. DS(sk, sl) and align1−1(sk) = ti and align1−1(sl) = tj ; see Figures 5(a)-5(c); (R2) if ∃s ∈ S s.t. align1−1(s) = ti and (s, tj) ∈ align. This rule deals with one-to-many alignments; see Figure 5(d); (R3a) if ∃sk s.t. align1−1(sk) = ti and ∃sl s.t. (sl, tj) ∈ align and and DS(sl, sk), and ti linearly precedes tj . In words: if two target words are in align1−1 but do not get connected via R1, find a source word aligned to the second target word that may get them connected; see Figure 5(e); (R3b) same as R3a, but in case tj precedes ti (i.e., find an additional source word aligned to the first target word; see Figure 5(f)).6 (R4) In case ¬∃s (s, tj) ∈ align (tj is unaligned), we consider two strategies: We simplify the rule of Quirk and Menezes (2006) (dealing with the same situation) by adjoining it to the immediately preceding head. We also consider a strategy whereby the word remains unconnected to any word in the sentence; see Figure 5(g).\n6R3a and R3b differ from the rules proposed in Quirk and Menezes (2006) dealing with the same situation, since we had to adapt it to the left-to-right parsing scenario."
  }, {
    "heading": "4.2 BiSLM parsing procedure",
    "text": "Given an inference procedure for dependency relations between target words (Section 4.1), one can specify in which order the corresponding dependency arcs are assigned to the target sentence. We define an incremental parsing procedure in terms of three operations: shift, left-reduce, and right-reduce. The operations are applied as soon as the sufficient conditions hold: We specify the conditions using the following structural properties. A target subtree is source-complete if all the descendants of align−11−1(root(sub)) (source correspondent of the root of the current subtree) (Section 4.1) have been translated and reduced. A target subtree is complete if it is source-complete and all the target words that are its children through non-projected arcs (through R2 or R4 in Section 4.1) have been translated and reduced. The bilingual parsing operations and the sufficient conditions for them are defined as follows: Shift: after the word is produced it is shifted onto the stack as an elementary subtree. Left-reduce: if a disconnected subtree subi and a disconnected subtree subi−1 immediately preceding it are both complete and DT (root(subi), root(subi−1)), adjoin subi−1 to subi so that root(subi−1) is a modifier of root(subi). Right-reduce: analogous to left-reduce, but DT (root(subi−1), root(subi)).\nIn the case of non-cohesive translation the resulting target dependencies are non-projective. Our definition of left- and right-reduce only produces projective parses. For a non-cohesive translation, certain subtrees will never be sourcecomplete and will never be reduced; see Figure 6(a). Note that this is not a disadvantage\nof our model. Cherry (2008) simply assumes that non-cohesive reordering should be penalized, and our model is able to learn this pattern. We also consider an alternative to incorporating noncohesive alignments by relaxing the definition of completeness for subtrees: A projected subtree sub is weakly source-complete if all descendants of all source word(s) which are aligned to the root of sub have been translated and, only if the definition of reduce applies, reduced; see Figure 6(b)."
  }, {
    "heading": "4.3 Syntactic labeling of tokens",
    "text": "One of the problems with SLMs in general is that at time steps i and j the sets of exposed heads for ti and tj can differ in size, which may imply different predictive power. To this end, we add an additional detail to our model: Each time a reduction occurs, we label the root of the subtree to which another subtree has been adjoined, thus making the conditioning history more specific. We use the following labelings: Reduction labeling: if a subtree is adjoint to sub from the left, then label root(sub) with LR. If it is adjoint from the right, then label it with RR. Reduction POS-labeling: same as in simple reduction labeling, but add the POS tag of the root of the reduced subtree to the label."
  }, {
    "heading": "4.4 Implementation and training",
    "text": "To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling t̂ given the phrase pair: t̂ = arg maxt̄ p(t̄|ē, f̄). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing."
  }, {
    "heading": "5 Experiments",
    "text": "To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for\n7We extract dependency parses from its output based on Collins (1999)\nArabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009)."
  }, {
    "heading": "5.1 Experimental setup",
    "text": "This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences."
  }, {
    "heading": "5.2 Baseline and comparison systems",
    "text": "As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count.\nby the baseline system. Since these features are binary or count-based, we cannot use them directly in rescoring. For that reason we integrated the features into the decoder and tuned the corresponding weights. The results for Chinese-English and Arabic-English translation experiments are presented in Table 1 and 2, respectively. We see that adding the cohesion constraints does not improve performance. This finding is different from, for example, Feng et al. (2010), where they get improvement for Chinese-English: however, we note that their training set is smaller than ours, and their baseline is weaker as it does not contain lexicalized distortion models."
  }, {
    "heading": "5.3 Rescoring experiments",
    "text": "Rescoring with BiSLMs is performed as follows: For the test runs of the baseline system we compute the n = 1000 best translation hypotheses for each source sentence and extract their derivations (sequence of phrase pair applications). Each phrase pair in our implementation is associated with a unique phrase-internal alignment and target POS-sequence. We fully reconstruct wordalignment for each pair of a source sentence and its translation hypothesis. We project a precomputed source parse onto the target side and compute representations of the target sentence to be computed by a BiSLM. For each hypothesis, we take its BiSLM score and its score assigned by the baseline system and compute the final score as a weighted sum of the original baseline score and a length-normalized BiSLM score10, where the weight λ is empirically set to 0.3:\nλ · scoreBiSLM lengthHypothesis + (1− λ) · scoreBaseline (3)"
  }, {
    "heading": "5.3.1 Chinese-English",
    "text": "Our main focus here is Chinese-English, since it has more instances of longer-distance reordering, at which syntax-based models are typically good.\n10Normalization is needed to ensure comparability of scores for translation hypotheses of different lengths, since longer translation hypotheses will have lower scores.\nSLMs by design are good at capturing longerdistance dependencies. We try out several variations of BiSLM. First, we test whether to use a strong or weak definition of a complete subtree (Section 4.2). Second, we investigate whether to adjoin unaligned target words to a preceding head (Section 4.1; unalign-adjoin+/-). Third, we compare several target-side labeling methods (Section 4.3): plain (just target words), reduce (LR or RR) or reduce-POS (LR POS or RR POS, where POS is the tag of the root of the reduced subtree). The rescoring results are presented in Table 4.\nThe results show statistically significant improvement over the baseline of up to 0.7 BLEU (for all of the employed BiSLM variants except one). The rescoring experiments also demonstrate the tendency of the unalign-adjoin- feature value to produce higher scores than unalign-adjoin+. But the other two distinguishing features do not have an effect on BLEU scores. As future work, we are interested in examining if these features produce the same distribution of scores when a BiSLM is fully integrated into the decoder."
  }, {
    "heading": "5.3.2 Arabic-English",
    "text": "We also rescore the n-best lists for the output of the Arabic-English baseline system and results are shown in Table 5. Arabic and English are typologically very different, but the range of reordering is much smaller than for Chinese-English. We expect reordering-related models to have lesser effect on Arabic as compared to Chinese (Carpuat et al., 2010). Experimental results on ArabicEnglish could indicate what kind of translation aspect benefits from BiSLMs. We see that for Arabic-English, just as for the cohesion constraint, BiSLM have little effect on BLEU scores, or even decrease them. This is a weak indication that BiSLMs are better at capturing reordering aspects. As for the varying features defining different BiSLM versions, we again see little effect of the labeling type or subtree completeness definition. On the other hand, we see the opposite pattern for the unalign-adjoin feature, where unalign-adjoin+ is preferred.\nTo gain further insight into the different effect of BiSLM on the two language pairs, we evaluated our experimental output against a reorderingsensitive metric LRscore (Birch et al., 2010). We use the version of LRscore which is an average of the inverse Kendall’s Tau distance and the Hamming distance. In order to compute alignments for test sets which are needed to compute the score we concatenated the parallel text with an additional 250K lines of parallel text from the training data to ensure better generalization of the alignment algorithm (GIZA++). The LRscores of the baseline are compared to the best performing BiSLM system with respect to BLEU, for each of the language pair. The results are provided in Tables 6 and 7.\nAs expected, the scores for Chinese-English are much lower than for Arabic-English, which is consistent with the observation reordering is more difficult for Chinese-English. BiSLM yields larger improvements for Chinese-English suggesting that the proposed model helps addressing difficult reordering problems. While there are also small improvements for Arabic-English the they may be too small to be detectable by BLEU."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper we proposed a novel way to adapt structured language models to phrase-based SMT. Our method requires minimal changes to the PBSMT pipeline. We tried a number of variations of our model and evaluated them in rescoring experiments, resulting in statistically significant improvement for Chinese-English. The model is based on the idea of syntactic transfer (DCA; Section 2) and the positive result indicates its ability to capture syntactic patterns across languages. For Arabic-English, we did not observe any improvements, suggesting that our models indeed mainly improve reordering aspects. Improvements in rescoring are a positive indication that our model may be a strong feature during decoding. As future work, we will fully integrate our model into a PBSMT decoder and evaluate it on other language pairs with different reordering distributions."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the reviewers for their useful comments. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218."
  }],
  "year": 2015,
  "references": [{
    "title": "Cohesive constraints in a beam search phrase-based decoder",
    "authors": ["Nguyen Bach", "Stephan Vogel", "Colin Cherry."],
    "venue": "Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1–4.",
    "year": 2009
  }, {
    "title": "Metrics for mt evaluation: evaluating reordering",
    "authors": ["Alexandra Birch", "Miles Osborne", "Phil Blunsom."],
    "venue": "Machine Translation, 24(1):15–26.",
    "year": 2010
  }, {
    "title": "Improving Arabic-to-English statistical machine translation by reordering post-verbal subjects for alignment",
    "authors": ["Marine Carpuat", "Yuval Marton", "Nizar Habash."],
    "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 178–183. Association",
    "year": 2010
  }, {
    "title": "Discriminative reordering with chinese grammatical relations features",
    "authors": ["Pi-Chuan Chang", "Huihsin Tseng", "Dan Jurafsky", "Christopher D. Manning."],
    "venue": "Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages",
    "year": 2009
  }, {
    "title": "Structured language modeling",
    "authors": ["Ciprian Chelba", "Frederick Jelinek."],
    "venue": "Computer Speech and Language, 14(4):283–332.",
    "year": 2000
  }, {
    "title": "Cohesive phrase-based decoding for statistical machine translation",
    "authors": ["Colin Cherry."],
    "venue": "Proceedings of Association for Computational Linguistics, pages 72–80.",
    "year": 2008
  }, {
    "title": "Head-Driven Statistical Models for Natural Language Parsing",
    "authors": ["Michael Collins."],
    "venue": "Ph.D. thesis, University of Pennsylvania.",
    "year": 1999
  }, {
    "title": "Improving reordering with linguistically informed bilingual n-grams",
    "authors": ["Josep M. Crego", "François Yvon."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 197–205. Association for Computational Lin-",
    "year": 2010
  }, {
    "title": "A source-side decoding sequence model for statistical machine translation",
    "authors": ["Minwei Feng", "Arne Mauser", "Hermann Ney."],
    "venue": "Conference of the Association for Machine Translation in the Americas, Denver, Colorado, USA.",
    "year": 2010
  }, {
    "title": "Phrasal cohesion and statistical machine translation",
    "authors": ["Heidi J. Fox."],
    "venue": "Proceedings the Conference on Empirical Methods in Natural Language Processing, pages 304–311. Association for Computational Linguistics.",
    "year": 2002
  }, {
    "title": "Dependency-based bilingual language models for reordering in statistical machine translation",
    "authors": ["Ekaterina Garmash", "Christof Monz."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2014
  }, {
    "title": "A direct syntax-driven reordering model for phrase-based machine translation",
    "authors": ["Niyu Ge."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages",
    "year": 2010
  }, {
    "title": "Better arabic parsing: Baselines, evaluations, and analysis",
    "authors": ["Spence Green", "Christopher D. Manning."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 394–402. Association for Computational Linguis-",
    "year": 2010
  }, {
    "title": "Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures",
    "authors": ["Jiri Havelka."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 608–615. Association for Com-",
    "year": 2007
  }, {
    "title": "Tuning as ranking",
    "authors": ["Mark Hopkins", "Jonathan May."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1352–1362. Association for Computational Linguistics.",
    "year": 2011
  }, {
    "title": "Evaluating translational correspondence using annotation projection",
    "authors": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Okan Kolak."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 392–399. Associ-",
    "year": 2002
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages",
    "year": 2003
  }, {
    "title": "Moses: Open source toolkit for statistical machine translation",
    "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"],
    "year": 2007
  }, {
    "title": "Mildly non-projective dependency structures",
    "authors": ["Marco Kuhlmann", "Joakim Nivre."],
    "venue": "Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507–514, Sydney, Australia, July. Association for Computational Linguistics.",
    "year": 2006
  }, {
    "title": "Source-side classifier preordering for machine translation",
    "authors": ["Uri Lerner", "Slav Petrov."],
    "venue": "Proceedings of the Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Selective sharing for multilingual dependency",
    "authors": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson"],
    "year": 2012
  }, {
    "title": "Wider context by using bilingual language models in machine translation",
    "authors": ["Jan Niehues", "Teresa Herrmann", "Stephan Vogel", "Alex Waibel."],
    "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198–206. Association",
    "year": 2011
  }, {
    "title": "Computer Intensive Methods for Testing Hypotheses",
    "authors": ["Eric W. Noreen."],
    "venue": "An Introduction. WileyInterscience.",
    "year": 1989
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Computational Linguistics, 29(1):19–51.",
    "year": 2003
  }, {
    "title": "Major constituents and two dependency grammar constraints on sharing in coordination",
    "authors": ["Timothy Osborne."],
    "venue": "Linguistics, 46(6):1109–1165.",
    "year": 2008
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318. Association for",
    "year": 2002
  }, {
    "title": "Parsers as language models for statistical machine translation",
    "authors": ["Matt Post", "Daniel Gildea."],
    "venue": "Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 172–181. Citeseer.",
    "year": 2008
  }, {
    "title": "Dependency treelet translation: The convergence of statistical and example-based machine translation",
    "authors": ["Chris Quirk", "Arul Menezes"],
    "venue": "Machine Translation,",
    "year": 2006
  }, {
    "title": "On some pitfalls in automatic evaluation and significance testing for MT",
    "authors": ["Stefan Riezler", "John T. Maxwell."],
    "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.",
    "year": 2005
  }, {
    "title": "Modelling and optimizing on syntactic n-grams for statistical machine translation",
    "authors": ["Rico Sennrich."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:169–182.",
    "year": 2015
  }, {
    "title": "A new string-to-dependency machine translation algorithm with a target dependency language model",
    "authors": ["Libin Shen", "Jinxi Xu", "Ralph M. Weischedel."],
    "venue": "Proceedings of the Association for Computational Linguistics, pages 577–585.",
    "year": 2008
  }, {
    "title": "Srilm at sixteen: Update and outlook",
    "authors": ["Andreas Stolcke", "Jing Zheng", "Wen Wang", "Victor Abrash."],
    "venue": "Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop, page 5.",
    "year": 2011
  }, {
    "title": "Feature-rich part-ofspeech tagging with a cyclic dependency network",
    "authors": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computa-",
    "year": 2003
  }, {
    "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora",
    "authors": ["Dekai Wu."],
    "venue": "Computational linguistics, 23(3):377–403.",
    "year": 1997
  }, {
    "title": "Improving reordering for statistical machine translation with smoothed priors and syntactic features",
    "authors": ["Bing Xiang", "Niyu Ge", "Abraham Ittycheriah."],
    "venue": "Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,",
    "year": 2011
  }, {
    "title": "A syntaxbased statistical translation model",
    "authors": ["Kenji Yamada", "Kevin Knight."],
    "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.",
    "year": 2001
  }, {
    "title": "A structured language model for incremental treeto-string translation",
    "authors": ["Heng Yu", "Haitao Mi", "Liang Huang", "Qun Liu."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics, pages 1133–1143.",
    "year": 2014
  }, {
    "title": "Syntax augmented machine translation via chart parsing",
    "authors": ["Andreas Zollmann", "Ashish Venugopal."],
    "venue": "Proceedings of the Workshop on Statistical Machine Translation, pages 138–141. Association for Computational Linguistics.",
    "year": 2006
  }],
  "id": "SP:a4f2d2f6f18b2a1b590d3e59e7c0946f1968ecf4",
  "authors": [{
    "name": "Ekaterina Garmash",
    "affiliations": []
  }, {
    "name": "Christof Monz",
    "affiliations": []
  }],
  "abstractText": "This paper describes a novel target-side syntactic language model for phrase-based statistical machine translation, bilingual structured language model. Our approach represents a new way to adapt structured language models (Chelba and Jelinek, 2000) to statistical machine translation, and a first attempt to adapt them to phrasebased statistical machine translation. We propose a number of variations of the bilingual structured language model and evaluate them in a series of rescoring experiments. Rescoring of 1000-best translation lists produces statistically significant improvements of up to 0.7 BLEU over a strong baseline for Chinese-English, but does not yield improvements for ArabicEnglish.",
  "title": "Bilingual Structured Language Models for Statistical Machine Translation"
}