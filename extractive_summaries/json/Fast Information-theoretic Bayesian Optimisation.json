{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Optimisation problems arise in numerous fields ranging from science and engineering to economics and management (Brochu et al., 2010). In classical optimisation tasks, the objective function is usually known and cheap to evaluate (Hennig and Schuler, 2012). However, in many situations, we face another type of tasks for which the above assumptions do not apply. For example, in the cases of clinical trials, financial investments or constructing a sensor network, it is very costly to draw a sample from the latent function underlying the real-world processes (Brochu et al., 2010). The objective functions in such type\n1Department of Engineering Science, University of Oxford, Oxford, UK 2Mind Foundry Ltd., Oxford. Correspondence to: Binxin Ru <robin@robots.ox.ac.uk>, Mark McLeod <mark.mcleod@magd.ox.ac.uk>, Diego Granziol <diego@robots.ox.ac.uk>, Michael A. Osborne <mosb@robots.ox.ac.uk>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nof problems are generally non-convex and their closed-form expressions and derivatives are unknown (Shahriari et al., 2016). Bayesian optimisation is a powerful tool to tackle such optimisation challenges (Brochu et al., 2010).\nA core step in Bayesian optimisation is to define an acquisition function which uses the available observations effectively to recommend the next query location (Shahriari et al., 2016). There are many types of acquisition functions such as Probability of Improvement (PI) (Kushner, 1964), Expected Improvement (EI) (Močkus et al., 1978; Jones et al., 1998) and Gaussian Process Upper Confidence Bound (GP-UCB) (Srinivas et al., 2009). The most recent type is based on information theory and offers a new perspective to efficiently select the sequence of sampling locations based on entropy of the distribution over the unknown minimiser x∗ (Shahriari et al., 2016). The information-theoretic approaches guide our evaluations to locations where we can maximise our learning about the unknown minimum rather than to locations where we expect to obtain lower function values (Hennig and Schuler, 2012). Such methods have demonstrated impressive empirical performance and tend to outperform traditional methods in tasks with highly multimodal and noisy latent functions.\nOne popular information-based acquisition function is Predictive Entropy Search (PES) (Villemonteix et al., 2009; Hennig and Schuler, 2012; Hernández-Lobato et al., 2014) . However, it is very slow to evaluate in comparison with traditional methods like EI, PI and GP-UCB and faces serious constraints in its application. For example, the implementation of PES requires the first and second partial derivatives as well as the spectral density of the Gaussian process kernel function (Hernández-Lobato et al., 2014; Requeima, 2016). This limits our kernel choices. Moreover, PES deals with the input space, thus less efficient in higher dimensional problems (Wang and Jegelka, 2017). The more recent methods such as Output-space Predictive Entropy Search (OPES) (Hoffman and Ghahramani, 2015) and Max-value Entropy Search (MES) (Wang and Jegelka, 2017) improve on PES by focusing on the information content in output space instead of input space. However, current entropy search methods, whether dealing with the minimiser or the minimum value, all involve two separate sampling processes : 1) sampling\nhyperparameters for marginalisation and 2) sampling the global minimum/minimiser for entropy computation. The second sampling process not only contributes significantly to the computational burden of these information-based acquisition functions but also requires the construction of a good approximation for the objective function based on Bochner’s theorem (Hernández-Lobato et al., 2014), which limits the kernel choices to the stationary ones (Bochner, 1959).\nIn view of the limitations of the existing methods, we propose a fast information-theoretic Bayesian optimisation technique (FITBO). Inspired by the Bayesian integration work in (Gunter et al., 2014), the creative contribution of our technique is to approximate any black-box function in a parabolic form: f(x) = η + 1/2g(x)2. The global minimum is explicitly represented by a hyperparameter η, which can be sampled together with other hyperparameters. As a result, our approach has the following three major advantages:\n1. Our approach reduces the expensive process of sampling the global minimum/minimiser to the much more efficient process of sampling one additional hyperparameter, thus overcoming the speed bottleneck of information-theoretic approaches.\n2. Our approach faces fewer constraints on the choice of appropriate kernel functions for the Gaussian process prior.\n3. Similar to MES (Wang and Jegelka, 2017), our approach works on information in the output space and thus is more efficient in high dimensional problems."
  }, {
    "heading": "2. Fast Information-theoretic Bayesian Optimisation",
    "text": "Information-theoretic techniques aim to reduce the uncertainty about the unknown global minimiser x∗ by selecting a query point that leads to the largest reduction in entropy of the distribution p(x∗|Dn) (Hennig and Schuler, 2012). The acquisition function for such techniques has the form (Hennig and Schuler, 2012; Hernández-Lobato et al., 2014):\nαES(x|Dn) = H[p(x∗|Dn)] − Ep(y|Dn,x) [ H [ p ( x∗|Dn ∪ (x, y) )]] . (1)\nPES makes use of the symmetry of mutual information and arrives at the following equivalent acquisition function:\nαPES(x|Dn) = H[p(y|Dn,x)] − Ep(x∗|Dn) [ H [ p(y|Dn,x,x∗) ]] , (2)\nwhere p(y|Dn,x,x∗) is the predictive posterior distribution for y conditioned on the observed data Dn, the test location x and the global minimiser x∗ of the objective function.\nFITBO harnesses the same information-theoretic thinking but measures the entropy about the latent global minimum f∗ = f(x∗) instead of that of the global minimiser x∗. Thus, the acquisition function of FITBO method is the mutual information between the function minimum f∗ and the next query point (Wang and Jegelka, 2017). In other words, FITBO aims to select the next query point which minimises the entropy of the global minimum:\nαFITBO(x|Dn) = H[p(y|Dn,x)] − Ep(f∗|Dn) [ H [ p(y|Dn,x, f∗) ]] . (3)\nThis idea of changing entropy computation from the input space to the output space is also shared by Hoffman and Ghahramani (2015) and Wang and Jegelka (2017). Hence, the acquisition function of the FITBO method is very similar to those of OPES (Hoffman and Ghahramani, 2015) and MES (Wang and Jegelka, 2017).\nHowever, our novel contribution is to express the unknown objective function in a parabolic form f(x) = η+ 1/2g(x)2, thus representing the global minimum f∗ by a hyperparameter η and circumventing the laborious process of sampling the global minimum. FITBO acquisition function can then be reformulated as:\nαFITBO(x|Dn) = H[p(y|Dn,x)] − Ep(η|Dn) [ H [ p(y|Dn,x, η) ]] = H [ ∫ p(y|Dn,x, η)p(η|Dn)dη\n] − ∫ p(η|Dn)H [ p(y|Dn,x, η) ] dη. (4)\nThe intractable integral terms can be approximated by drawing M samples of η from the posterior distribution p(η|Dn) and using a Monte Carlo method (HernándezLobato et al., 2014). The predictive posterior distribution p(y|Dn,x, η) can be turned into a neat Gaussian form by applying a local linearisation technique on our parabolic transformation as described in Section 2.1. Thus, the first term in the above FITBO acquisition function is an entropy of a Gaussian mixture, which is intractable and demands approximation as described in Section 2.3. The second term is the expected entropy of a one-dimensional Gaussian distribution and can be computed analytically because the entropy of a Gaussian has the closed form: H[p(y|Dn,x, η)] = 0.5 log [ 2πe ( vf (x|Dn, η) + σ2n\n)] where the variance vf (x|Dn, η) = Kf (x,x′) and σ2n is the variance of observation noise."
  }, {
    "heading": "2.1. Parabolic Transformation and Predictive Posterior Distribution",
    "text": "Gunter et al. (2014) use a square-root transformation on the integrand in their warped sequential active Bayesian integration method to ensure non-negativity. Inspired by this work, we creatively express any unknown objective function f(x) in the parabolic form:\nf(x) = η + 1/2g(x)2, (5)\nwhere η is the global minimum of the objective function. Given the noise-free observation data Df = {(xi, fi)|i = 1, . . . n} = {Xn, fn}, the observation data on g is Dg = {(xi, gi)|i = 1, . . . n} = {Xn,gn} where gi = √ 2(fi − η) .\nWe impose a zero-mean Gaussian process prior on g(x), g ∼ GP ( 0, k(x,x′) ) , so that the posterior distribution for g conditioned on the observation data Dg and the test point x also follows a Gaussian process:\np(g|Dg,x, η) = GP ( g;mg(·),Kg(·, ·) ) (6)\nwhere\nmg(x) = K(x,Xn)K(Xn,Xn) −1gn,\nKg(x,x ′) = K(x,x′)−K(x,Xn)K(Xn,Xn)−1K(Xn,x′).\nThe parabolic transformation causes the distribution for any f to become a non-central χ2 process, making the analysis intractable. In order to tackle this problem and obtain a posterior distribution p(f |Df ,x, η) that is also Gaussian, we employ a linearisation technique (Gunter et al., 2014).\nWe perform a local linearisation of the parabolic transformation h(g) = η + 1/2g2 around g0 and obtain f ≈ h(g0) + h′(g0)(g − g0) where the gradient h′(g) = g. By setting g0 to the mode of the posterior distribution p(g|Dg,x, η) (i.e. g0 = mg), we obtain an expression for f which is linear in g:\nf(x) ≈ [η + 1/2mg(x)2] +mg(x)[g(x)−mg(x)] = η − 1/2mg(x)2 +mg(x)g(x). (7)\nSince the affine transformation of a Gaussian process remains Gaussian, the predictive posterior distribution for f now has a closed form:\np(f |Df ,x, η) = GP ( f ;mf (·),Kf (·, ·) ) (8)\nwhere mf (x) = η + 1/2mg(x) 2\nKf (x,x ′) = mg(x)Kg(x,x ′)mg(x ′).\nHowever, in real world situations, we do not have access to the true function values but only noisy observations of the function, y(x) = f(x) + , where is assumed to be an independently and identically distributed Gaussian noise with variance σ2n (Rasmussen and Williams, 2006). Given noisy observation data Dn = {(xi, yi)|i = 1, . . . n} = {Xn,yn}, the predictive posterior distribution (8) becomes:\np(y|Dn,x, η) = GP ( y;mf (·),Kf (·, ·) + σ2nδ(·, ·) ) . (9)"
  }, {
    "heading": "2.2. Hyperparameter Treatment",
    "text": "Hyperparameters are the free parameters, such as output scale and characteristic length scales in the kernel function for the Gaussian processes as well as noise variance. We use θ to represent a vector of hyperparameters that includes all the kernel parameters and the noise variance. Recall that we introduce a new hyperparameter η in our model to represent the global minimum. To ensure that η is not greater than the minimum observation ymin, we assume that log(ymin − η) follows a broad normal distribution. Thus the prior for η has the form:\np(η) = 1 (ymin − η) N ( log(ymin − η);µ, σ2 ) . (10)\nThe most popular approach to hyperparameter treatment is to learn hyperparameter values via maximum likelihood estimation (MLE) or maximum a posterior estimation (MAP). However, both MLE and MAP are not desirable as they give point estimates and ignore our uncertainty about the hyperparameters (Hernández-Lobato et al., 2014). In a fully Bayesian treatment of the hyperparameters, we should consider all possible hyperparameter values. This can be done by marginalising the terms in the acquisition function with respect to the posterior p(ψ|Dn) where ψ = {θ, η}:\nαFITBO(x|Dn) = H [ ∫ p(y|Dn,x,ψ)p(ψ|Dn)dψ ]\n− ∫ p(ψ|Dn)H [ p(y|Dn,x,ψ) ] dψ.\nSince complete marginalisation over hyperparameters is analytically intractable, the integral can be approximated using the Monte Carlo method (Hoffman and Ghahramani, 2015; Snoek et al., 2012), leading to the final expression:\nαFITBO(x|Dn)\n= H [ 1 M M∑ j p(y|Dn,x,θ(j), η(j)) ]\n− 1 2M M∑ j log [ 2πe ( vf (x|D,θ(j), η(j)) + σ2n )] .\n(11)"
  }, {
    "heading": "2.3. Approximation for the Gaussian Mixture Entropy",
    "text": "The entropy of a Gaussian mixture is intractable and can be estimated via a number of methods: the Taylor expansion proposed in (Huber et al., 2008), numerical integration and Monte Carlo integration. Of these three, our experimentation revealed that numerical integration (in particular, an adaptive Simpson’s method) was clearly the most performant for our application (see the supplementary material). Note that our Gaussian mixture is univariate.\nA faster alternative is to approximate the first entropy term by matching the first two moments of a Gaussian mixture. The mean and variance of a univariate Gaussian mixture model p(z) = ∑M j 1 MN (z|mj ,Kj) have the analytical form:\nE[z] = M∑ j 1 M mj (12)\nVar(z) = M∑ j 1 M (Kj +m 2 j )− E[z]2. (13)\nBy fitting a Gaussian to the Gaussian mixture, we can obtain a closed-form upper bound for the first entropy term: H[p(z)] ≈ 0.5 log [ 2πe ( Var(z)+σ2n )] , thus further enhancing the computational speed of FITBO approach. However, the moment-matching approach results in a looser approximation than numerical integration (shown in the supplementary material) and we will compare both approaches in our experiments in Section 3."
  }, {
    "heading": "2.4. The Algorithm",
    "text": "The procedures of computing the acquisition function of FITBO are summarised by Algorithm 1. Figure 1 illustrates the sampling behaviour of FITBO method for a simple 1D Bayesian optimisation problem. The optimisation process is started with 3 initial observation data. As more samples are taken, the mean of the posterior distribution for the objective function gradually resembles the objective function and the distribution of η converges to the global minimum."
  }, {
    "heading": "3. Experiments",
    "text": "We conduct a series of experiments to test the empirical performance of FITBO and compare it with other popular acquisition functions. In this section, FITBO denotes the version using numerical integration to estimate the entropy of the Gaussian mixture while FITBO-MM denotes the version using moment matching. In all experiments, we adopt a zero-mean Gaussian process prior with the squared exponential kernel function and use the elliptical slice sampler (Murray et al., 2010) for sampling hyperparameters θ and η. For the implementation of EI, PI, GP-UCB, MES\nAlgorithm 1 FITBO acquisition function 1: Input: a test input x; noisy observation data Dn = {(xi, yi)|i = 1, . . . , n}\n2: Sample hyperparameters and η from p(ψ|Dn): Ψ = {θ(j), η(j)|j = 1, . . . ,M} 3: for j = 1, . . . ,M do 4: Use f(x) = η + 1/2g(x)2 to approximate\np(f |Dn,x,θ(j), η(j)) = GP ( mf (·),Kf (·, ·) ) 5: Compute p(y|Dn,x,θ(j), η(j)) 6: Compute H[p(y|Dn,x,θ(j), η(j))] 7: end for 8: Estimate the entropy of the Gaussian mixture :\nE1(x|Dn) = H [ 1 M ∑M j p(y|Dn,x,θ (j), η(j)) ]\n9: Compute the entropy expectation: E2(x|Dn) = 1M ∑M j H[p(y|Dn,x,θ\n(j), η(j))] = 1\n2M ∑M j log [ 2πe ( vf (x|Dn,θ(j), η(j)) + σ2n )] 10: return αn(x|Dn) = E1(x|Dn)− E2(x|Dn)\nand PES, we use the open source Matlab code by Wang and Jegelka (2017) and Hernández-Lobato et al. (2014). Our Matlab code for FITBO will be available at https: //github.com/rubinxin/FITBO. We use the type of MES method that samples the global minimum f(x∗) from an approximated posterior function f̃(x) = φ(x)Tã where φ(x) is an m-dimensional feature vector and ã is a Gaussian weight vector (Wang and Jegelka, 2017). This is also the minimiser sampling strategy adopted by PES (Hernández-Lobato et al., 2014). The computational complexity of sampling ã from its posterior distribution p(ã|Dn) is O(n2m) when n < m (Hernández-Lobato et al., 2014). Minimising f̃(x) to within ζ accuracy using any grid search or branch and bound optimiser requires O(ζ−d) calls to f̃(x) for d-dimensional input data (Kandasamy et al., 2015). For both PES and MES, we apply their fastest versions which draw only 1 minimum or minimiser sample to estimate the acquisition function."
  }, {
    "heading": "3.1. Runtime Tests",
    "text": "The first set of experiments measure and compare the runtime of evaluating the acquisition functions αn(x|Dn) for methods including GP-UCB, PI, EI, PES, MES, FITBO and FITBO-MM. All the timing tests were performed exclusively on a 2.3 GHz Intel Core i5. The runtime measured excludes the time taken for sampling hyperparameters as well as optimising the acquisition functions. The methodology of the tests can be summarised as follows:\n1. Generate 10 initial observation data from a ddimensional test function and sample a set of M hyperparameters Ψ = {θ(j), η(j)|j = 1, . . . ,M} from the log posterior distribution log p̃(ψ|Dn) using the\nelliptical slice sampler.\n2. Use this set of hyperparameters to evaluate all acquisition functions at 100 test points.\n3. Repeat the procedures 1 and 2 for 100 different initialisations and compute the mean and standard deviation of the runtime taken for evaluating various acquisition functions.\nWe did not include the time for sampling η alone into the runtime of evaluating FITBO and FITBO-MM because η is sampled jointly with other hyperparameters and does not add to the overall sampling burden significantly. In fact, we have tested that sampling η by the elliptical slice sampler adds 0.09 seconds on average when drawing 2 000 samples and 0.93 seconds when drawing 20 000 samples. Note further that we will limit all methods to a fixed number of hyperparameter samples in both runtime tests and performance experiments: this will impart a slight performance penalty to our method, which must sample from a hyperparameter space of one additional dimension.\nThe above tests are repeated for different hyperparameter sample sizes M = 100, 300, 500, 700, 900 and input data of different dimensions d = 2, 4, 6, 8, 10. The results are presented graphically in Figure 2 with the\nevaluation runtime being expressed in the logarithm to the base 10 and the exact numerical results for methods that are very close in runtime are presented in Tables 1 and 2.\nFigure 2 shows that FITBO is significantly faster to evaluate than PES and MES for various hyperparameter sample sizes used and for problems of different input dimensions. Moreover, FITBO even gains a clear speed advantage over EI. The moment matching technique manages to further enhance the speed of FITBO, making FITBO-MM comparable with, if not slightly faster than, simple algorithms like PI and GP-UCB. In addition, we notice that the runtime of evaluating FITBO-MM, EI, PI and GP-UCB tend to remain constant regardless of the input dimensions while the runtime for PES and MES tends to increase with input dimensions at a rate of 10d. Thus, our\napproach is more efficient and applicable in dealing with high-dimensional problems."
  }, {
    "heading": "3.2. Tests with Benchmark Functions",
    "text": "We perform optimisation tasks on three challenging benchmark functions: Branin (defined in [0, 1]2), Eggholder (defined in [0, 1]2) and Hartmann (defined in [0, 1]6). In all tests, we set the observation noise to σ2n = 10\n−3 and resample all the hyperparameters after each function evaluation. In evaluating the optimisation performance of various Bayesian optimisation methods, we use the two common metrics adopted by Hennig and Schuler (2012). The first metric is Immediate regret (IR) which is defined as:\nIR = |f(x∗)− f(x̂n)| (14)\nwhere x∗ is the location of true global minimum and x̂n is the best guess recommended by a Bayesian optimiser after n iterations, which corresponds to the minimiser of the posterior mean. The second metric is the Euclidean distance of an optimiser’s recommendation x̂n from the true global minimiser x∗, which is defined as:\n‖L‖2 = ‖x∗ − x̂n‖. (15)\nWe compute the median IR and the median ‖L‖2 over 40 random initialisations. At each initialisation, all Bayesian optimisation algorithms start from 3 random observation data for Branin-2D and Eggholder-2D problems and from 9 random observation data for Hartmann-6D problem.\nThe results are presented in Figure 3. The plots on the left show the median IR achieved by each approach as more evaluation steps are taken. The plots on the right show the median ‖L‖2 between each optimiser’s recommended global minimiser and the true global minimiser. The error bars indicate one standard deviation.\nIn the case of Branin-2D, FITBO and FITBO-MM lose out to other methods initially but surpass other methods after 50 evaluations. One interesting point we would like to illustrate through the Branin problem is the fundamentally different mechanisms behind information-based approaches like FITBO and improvement-based approaches like EI. As shown in Figure 4, FITBO is much more explorative compared to EI in taking new evaluations because FITBO selects the query points that maximise the information gain about the minimiser instead of those that lead to an improvement over the best function value observed. FITBO successfully finds all three global minimisers but EI quickly concentrates its searches into regions of low function values, missing out one of the global minimisers.\nIn the case of Eggholder-2D which is more complicated and multimodal, FITBO and FITBO-MM perform\nnot as well as other methods in finding lower function values but outperform all competitors in locating the global minimiser by a large margin. One reason is that the function value near the global minimiser of Eggholder-2D rises sharply. Thus, although FITBO and FITBO-MM are able to better identify the location of the true global minimum, they return higher function values than other methods that are trapped in locations of good local minima.\nAs for a higher dimensional problem, Hartmann-6D, FITBO and FITBO-MM outperform all other methods in finding both the lower function value and the location of the global minimum. In all three tasks, FITBO-MM, despite using a looser upper bound of the Gaussian mixture entropy, still manages to demonstrate similar, sometimes better, results compared with FITBO. This shows that the performance of our information-theoretic approach is robust to slightly worse approximation of the Gaussian mixture entropy."
  }, {
    "heading": "3.3. Test with Real-world Problems",
    "text": "Finally, we experiment with a series of real-world optimisation problems. The first problem (Boston) returns the L2 validation loss of a 1-hidden layer neural network (Wang and Jegelka, 2017) on the Boston housing dataset (Bache and Lichman, 2013). The dataset is randomly partitioned into train/validation/test sets and the neural network is trained with Levenberg-Marquardt optimisation. The 2 variables tuned with Bayesian optimisation are the number\nof neurons and the damping factor µ.\nThe second problem (MNIST-SVM) outputs the classification error of a support vector machine (SVM) classifier on the validation set of the MNIST dataset (LeCun et al., 1998). The SVM classifier adopts a radial basis kernel and the 2 variables to optimise are the kernel scale parameter and the box constraint.\nThe third problem (Cancer) returns the cross-entropy loss of a 1-hidden layer neural network (Wang and Jegelka, 2017) on the validation set of the breast cancer dataset (Bache and Lichman, 2013). This neural network is trained with the scaled conjugate gradient method and we use Bayesian optimisation methods to tune the number of neurons, the damping factor µ, the µ−increase factor and the µ−decrease factor.\nWe initialise all Bayesian optimisation algorithms with 3 random observation data and set the observation noise to σ2n = 10\n−3. All experiments are repeated 40 times. In each case, the ground truth is unknown but our aim is to minimise the validation loss. Thus, the corresponding loss functions are used to compare the performance of various Bayesian optimisation algorithms.\nFigure 5 shows the median of the best validation losses achieved by all Bayesian optimisation algorithms after n iterations for the Boston and MNIST-SVM problems. Our FITBO and FITBO-MM perform competitively well compared to their information-theoretic counterparts and\nall information-theoretic methods outperform EI in these real-world applications.\nAs for the Cancer problem (Figure 6), FITBO and FITBO-MM converge to the stable median value of the validation loss at a much faster speed than MES and EI and are almost on par with PES. By examining the mean validation loss shown in the right plot of Figure 6, it is evident that both FITBO and FITBO-MM demonstrate better performance than all other methods on average with FITBO gaining a slight advantage over FITBO-MM. Moreover, the comparable performance of FITBO and FITBO-MM in all three real-world tasks re-affirmed the robustness of our approach to entropy approximation as our moment matching technique, while improving the speed of the algorithm, does not really compromise the performance."
  }, {
    "heading": "4. Conclusion",
    "text": "We have proposed a novel information-theoretic approach for Bayesian optimisation, FITBO. With the creative use of the parabolic transformation and the hyperparameter η, FITBO enjoys the merits of less sampling effort, more flexible kernel choices and much simpler implementation in comparison with other information-based methods like PES and MES. As a result, its computational speed outperforms current information-based methods by a large margin and even exceeds EI to be on par with PI and GP-UCB. While requiring much lower runtime, it still manages to achieve satisfactory optimisation performance which is as good as or even better than PES and MES in a variety of tasks. Therefore, FITBO approach offers a very efficient and competitive alternative to existing Bayesian optimisation approaches."
  }, {
    "heading": "Acknowledgements",
    "text": "We wish to thank Roman Garnett and Tom Gunter for the insightful discussions and Zi Wang for sharing the Matlab implementation of EI, PI, GP-UCB, MES and PES. We would also like to thank Favour Mandanji Nyikosa, Logan Graham, Arno Blaas and Olga Isupova for their helpful comments about improving the paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Lectures on Fourier Integrals: With an Author’s Suppl. on Monotonic Functions, Stieltjes Integrals and Harmonic Analysis. Transl. from the Orig",
    "authors": ["S. Bochner"],
    "year": 1959
  }, {
    "title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
    "authors": ["E. Brochu", "V.M. Cora", "N. De Freitas"],
    "venue": "arXiv preprint arXiv:1012.2599,",
    "year": 2010
  }, {
    "title": "Sampling for inference in probabilisktic models with fast Bayesian quadrature",
    "authors": ["T. Gunter", "M.A. Osborne", "R. Garnett", "P. Hennig", "S.J. Roberts"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Entropy search for informationefficient global optimization",
    "authors": ["P. Hennig", "C.J. Schuler"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Predictive entropy search for efficient global optimization of black-box functions",
    "authors": ["J.M. Hernández-Lobato", "M.W. Hoffman", "Z. Ghahramani"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Output-space predictive entropy search for flexible global optimization",
    "authors": ["M.W. Hoffman", "Z. Ghahramani"],
    "venue": "In the NIPS workshop on Bayesian optimization,",
    "year": 2015
  }, {
    "title": "On entropy approximation for Gaussian mixture random vectors",
    "authors": ["M.F. Huber", "T. Bailey", "H. Durrant-Whyte", "U.D. Hanebeck"],
    "venue": "In Multisensor Fusion and Integration for Intelligent Systems,",
    "year": 2008
  }, {
    "title": "Efficient global optimization of expensive black-box functions",
    "authors": ["D.R. Jones", "M. Schonlau", "W.J. Welch"],
    "venue": "Journal of Global optimization,",
    "year": 1998
  }, {
    "title": "High dimensional Bayesian optimisation and bandits via additive models",
    "authors": ["K. Kandasamy", "J. Schneider", "B. Póczos"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise",
    "authors": ["H.J. Kushner"],
    "venue": "Journal of Basic Engineering,",
    "year": 1964
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Toward global optimization, volume 2, chapter the application of Bayesian methods for seeking the extremum",
    "authors": ["J. Močkus", "V. Tiesis", "A. Žilinskas"],
    "year": 1978
  }, {
    "title": "Gaussian processes for machine learning, volume 1",
    "authors": ["C.E. Rasmussen", "C.K. Williams"],
    "venue": "MIT press Cambridge,",
    "year": 2006
  }, {
    "title": "Integrated predictive entropy search for Bayesian optimization",
    "authors": ["J.R. Requeima"],
    "year": 2016
  }, {
    "title": "Taking the human out of the loop: A review of Bayesian optimization",
    "authors": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"],
    "venue": "Proceedings of the IEEE,",
    "year": 2016
  }, {
    "title": "Practical Bayesian optimization of machine learning algorithms",
    "authors": ["J. Snoek", "H. Larochelle", "R.P. Adams"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2012
  }, {
    "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
    "authors": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"],
    "venue": "arXiv preprint arXiv:0912.3995,",
    "year": 2009
  }, {
    "title": "An informational approach to the global optimization of expensive-toevaluate functions",
    "authors": ["J. Villemonteix", "E. Vazquez", "E. Walter"],
    "venue": "Journal of Global Optimization,",
    "year": 2009
  }, {
    "title": "Max-value entropy search for efficient Bayesian optimization",
    "authors": ["Z. Wang", "S. Jegelka"],
    "venue": "arXiv preprint arXiv:1703.01968,",
    "year": 2017
  }],
  "id": "SP:568993dc961962f55aa0c123f7d3ff45d76713c3",
  "authors": [{
    "name": "Binxin Ru",
    "affiliations": []
  }, {
    "name": "Mark McLeod",
    "affiliations": []
  }, {
    "name": "Diego Granziol",
    "affiliations": []
  }, {
    "name": "Michael A. Osborne",
    "affiliations": []
  }],
  "abstractText": "Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with informationtheoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.",
  "title": "Fast Information-theoretic Bayesian Optimisation"
}