{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2243–2248, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Color descriptions represent a microcosm of grounded language semantics. Basic color terms like “red” and “blue” provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as “greenish blue” or “the color of the rust on my aunt’s old Chevrolet” (Berlin and Kay, 1991). The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems.\nWe consider color description generation as a grounded language modeling problem. We present\nan effective new model for this task that uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997; Graves, 2013) and a Fourier-basis color representation inspired by feature representations in computer vision.\nWe compare our model with LUX (McMahan and Stone, 2015), a Bayesian generative model of color semantics. Our model improves on their approach in several respects, which we demonstrate by examining the meanings it assigns to various unusual descriptions: (1) it can generate compositional color descriptions not observed in training (Fig. 3); (2) it learns correct denotations for underspecified modifiers, which name a variety of colors (“dark”, “dull”; Fig. 2); and (3) it can model non-convex denotations, such as that of “greenish”, which includes both greenish yellows and blues (Fig. 4). As a result, our model also produces significant improvements on several grounded language modeling metrics."
  }, {
    "heading": "2 Model formulation",
    "text": "Formally, a model of color description generation is a probability distribution S(d | c) over sequences of\n2243\ntokens d conditioned on a color c, where c is represented as a 3-dimensional real vector in HSV space.1\nArchitecture Our main model is a recurrent neural network sequence decoder (Fig. 1, left panel). An input color c = (h, s, v) is mapped to a representation f (see Color features, below). At each time step, the model takes in a concatenation of f and an embedding for the previous output token di, starting with the start token d0 = <s>. This concatenated vector is passed through an LSTM layer, using the formulation of Graves (2013). The output of the LSTM at each step is passed through a fully-connected layer, and a softmax nonlinearity is applied to produce a probability distribution for the following token.2 The probability of a sequence is the product of probabilities of the output tokens up to and including the end token </s>.\nWe also implemented a simple feed-forward neural network, to demonstrate the value gained by modeling descriptions as sequences. This architecture (atomic; Fig. 1, right panel) consists of two fully-connected hidden layers, with a ReLU nonlinearity after the first and a softmax output over all full color descriptions seen in training. This model therefore treats the descriptions as atomic symbols rather than sequences.\nColor features We compare three representations:\n• Raw: The original 3-dimensional color vectors, in HSV space.\n1HSV: hue-saturation-value. The visualizations and tables in this paper instead use HSL (hue-saturation-lightness), which yields somewhat more intuitive diagrams and differs from HSV by a trivial reparameterization.\n2Our implementation uses Lasagne (Dieleman et al., 2015), a neural network library based on Theano (Al-Rfou et al., 2016).\n• Buckets: A discretized representation, dividing HSV space into rectangular regions at three resolutions (90×10×10, 45×5×5, 1×1×1) and assigning a separate embedding to each region.\n• Fourier: Transformation of HSV vectors into a Fourier basis representation. Specifically, the representation f of a color (h, s, v) is given by\nf̂jk` = exp [−2πi (jh∗ + ks∗ + `v∗)] f = [ Re{f̂} Im{f̂} ] j, k, ` = 0..2\nwhere (h∗, s∗, v∗) = (h/360, s/200, v/200).\nThe Fourier representation is inspired by the use of Fourier feature descriptors in computer vision applications (Zhang and Lu, 2002). It is a nonlinear transformation that maps the 3-dimensional HSV space to a 54-dimensional vector space. This representation has the property that most regions of color space denoted by some description are extreme along a single direction in Fourier space, thus largely avoiding the need for the model to learn non-monotonic functions of the color representation.\nTraining We train using Adagrad (Duchi et al., 2011) with initial learning rate η = 0.1, hidden layer size and cell size 20, and dropout (Hinton et al., 2012) with a rate of 0.2 on the output of the LSTM and each fully-connected layer. We identified these hyperparameters with random search, evaluating on a held-out subset of the training data.\nWe use random normally-distributed initialization for embeddings (σ = 0.01) and LSTM weights (σ = 0.1), except for forget gates, which are initialized to a constant value of 5. Dense weights use normalized uniform initialization (Glorot and Bengio, 2010)."
  }, {
    "heading": "3 Experiments",
    "text": "We demonstrate the effectiveness of our model using the same data and statistical modeling metrics as McMahan and Stone (2015).\nData The dataset used to train and evaluate our model consists of pairs of colors and descriptions collected in an open online survey (Munroe, 2010). Participants were shown a square of color and asked to write a free-form description of the color in a text box. McMahan and Stone filtered the responses to normalize spelling differences and exclude spam responses and descriptions that occurred\nvery rarely. The resulting dataset contains 2,176,417 pairs divided into training (1,523,108), development (108,545), and test (544,764) sets.\nMetrics We quantify model effectiveness with the following evaluation metrics:\n• Perplexity: The geometric mean of the reciprocal probability assigned by the model to the descriptions in the dataset, conditioned on the respective colors. This expresses the same objective as log conditional likelihood. We follow McMahan and Stone (2015) in reporting perplexity per-description, not per-token as in the language modeling literature.\n• AIC: The Akaike information criterion (Akaike, 1974) is given by AIC = 2` + 2k, where ` is log likelihood and k is the total number of real-valued parameters of the model (e.g., weights and biases, or bucket probabilities). This quantifies a tradeoff between accurate modeling and model complexity.\n• Accuracy: The percentage of most-likely descriptions predicted by the model that exactly match the description in the dataset (recall@1).\nResults The top section of Table 2 shows development set results comparing modeling effectiveness for atomic and sequence model architectures\nand different features. The Fourier feature transformation generally improves on raw HSV vectors and discretized embeddings. The value of modeling descriptions as sequences can also be observed in these results; the LSTM models consistently outperform their atomic counterparts.\nAdditional development set experiments (not shown in Table 2) confirmed smaller design choices for the recurrent architecture. We evaluated a model with two LSTM layers, but we found that the model with only one layer yielded better perplexity. We also compared the LSTM with GRU and vanilla recurrent cells; we saw no significant difference between LSTM and GRU, while using a vanilla recurrent unit resulted in unstable training. Also note that the color representation f is input to the model at every time step in decoding. In our experiments, this yielded a small but significant improvement in perplexity versus using the color representation as the initial state.\nTest set results appear in the bottom section. Our best model outperforms both the histogram baseline (HM) and the improved LUX model of McMahan and Stone (2015), obtaining state-of-the-art results on this task. Improvements are highly significant on all metrics (p < 0.001, approximate permutation test, R = 10,000 samples; Padó, 2006)."
  }, {
    "heading": "4 Analysis",
    "text": "Given the general success of LSTM-based models at generation tasks, it is perhaps not surprising that they yield good raw performance when applied to color description. The color domain, however, has the advantage of admitting faithful visualization of descriptions’ semantics: colors exist in a 3- dimensional space, so a two-dimensional visualization can show an acceptably complete picture of an entire distribution over the space. We exploit this to highlight three specific improvements our model realizes over previous ones.\nWe construct visualizations by querying the model for the probability S(d | c) of the same description for each color in a uniform grid, summing the probabilities over the hue dimension (left crosssection) and the saturation dimension (right crosssection), normalizing them to sum to 1, and plotting the log of the resulting values as a grayscale image.\nFormally, each visualization is a pair of functions (L,R), where\nL(s, `) = log\n[∫ dh S(d | c = (h, s, `))∫\ndc′ S(d | c′)\n]\nR(h, `) = log\n[∫ ds S(d | c = (h, s, `))∫\ndc′ S(d | c′)\n]\nThe maximum value of each function is plotted as white, the minimum value is black, and intermediate values linearly interpolated.\nLearning modifiers Our model learns accurate meanings of adjectival modifiers apart from the full descriptions that contain them. We examine this in Fig. 2, by plotting the probabilities assigned to the bare modifiers “light”, “bright”, “dark”, and “dull”. “Light” and “dark” unsurprisingly denote high and low lightness, respectively. Less obviously, they also exclude high-saturation colors. “Bright”, on the other hand, features both high-lightness colors and saturated colors—“bright yellow” can refer to the prototypical yellow, whereas “light yellow” cannot. Finally, “dull” denotes unsaturated colors in a variety of lightnesses.\nCompositionality Our model generalizes to compositional descriptions not found in the training set. Fig. 3 visualizes the probability assigned to the\nnovel utterance “faded teal”, along with “faded” and “teal” individually. The meaning of “faded teal” is intersective: “faded” colors are lower in saturation, excluding the colors of the rainbow (the V on the right side of the left panel); and “teal” denotes colors with a hue near 180° (center of the right panel).\nNon-convex denotations The Fourier feature transformation and the nonlinearities in the model allow it to capture a rich set of denotations. In particular, our model addresses the shortcoming identified by McMahan and Stone (2015) that their model cannot capture non-convex denotations. The description\n“greenish” (Fig. 4) has such a denotation: “greenish” specifies a region of color space surrounding, but not including, true greens.\nError analysis Table 3 shows some examples of errors found in samples taken from the model. The main type of error the system makes is ungrammatical descriptions, particularly fragments lacking a basic color term (e.g., “robin’s”). Rarer are grammatical but meaningless compositions (“reddish green”) and false descriptions. When queried for its single most likely prediction, argmaxd S(d | c), the result is nearly always an acceptable, “safe” description— manual inspection of 200 such top-1 predictions did not identify any errors."
  }, {
    "heading": "5 Conclusion and future work",
    "text": "We presented a model for generating compositional color descriptions that is capable of producing novel descriptions not seen in training and significantly outperforms prior work at conditional language modeling.3 One natural extension is the use of character-level sequence modeling to capture complex morphology (e.g., “-ish” in “greenish”). Kawakami et al. (2016) build character-level models for predicting colors given descriptions in addition to describing colors. Their model uses a Labspace color representation and uses the color to initialize the LSTM instead of feeding it in at each time step; they also focus on visualizing point predictions of their description-to-color model, whereas we examine the full distributions implied by our color-todescription model.\nAnother extension we plan to investigate is modeling of context, to capture how people describe colors differently to contrast them with other colors via\n3We release our code at https://github.com/ stanfordnlp/color-describer.\npragmatic reasoning (DeVault and Stone, 2007; Golland et al., 2010; Monroe and Potts, 2015)."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Jiwei Li, Jian Zhang, Anusha Balakrishnan, and Daniel Ritchie for valuable advice and discussions. This research was supported in part by the Stanford Data Science Initiative, NSF BCS 1456077, and NSF IIS 1159679."
  }],
  "year": 2016,
  "references": [{
    "title": "A new look at the statistical model identification",
    "authors": ["Hirotugu Akaike."],
    "venue": "IEEE Transactions on Automatic Control, 19(6):716–723.",
    "year": 1974
  }, {
    "title": "Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688",
    "authors": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas"],
    "year": 2016
  }, {
    "title": "Basic color terms: Their universality and evolution",
    "authors": ["Brent Berlin", "Paul Kay."],
    "venue": "University of California Press.",
    "year": 1991
  }, {
    "title": "Managing ambiguities across utterances in dialogue",
    "authors": ["David DeVault", "Matthew Stone."],
    "venue": "Ron Artstein and Laure Vieu, editors, Proceedings of DECALOG 2007: Workshop on the Semantics and Pragmatics of Dialogue.",
    "year": 2007
  }, {
    "title": "Lasagne: First release",
    "authors": ["Sander Dieleman", "Jan Schlüter", "Colin Raffel", "Eben Olson", "Søren Kaae Sønderby", "Daniel Nouri"],
    "year": 2015
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "AISTATS.",
    "year": 2010
  }, {
    "title": "A game-theoretic approach to generating spatial descriptions",
    "authors": ["Dave Golland", "Percy Liang", "Dan Klein."],
    "venue": "EMNLP.",
    "year": 2010
  }, {
    "title": "Generating sequences with recurrent neural networks",
    "authors": ["Alex Graves."],
    "venue": "arXiv preprint arXiv:1308.0850.",
    "year": 2013
  }, {
    "title": "Improving neural networks by preventing co-adaptation of feature detectors",
    "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1207.0580.",
    "year": 2012
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Character sequence models for colorful words",
    "authors": ["Kazuya Kawakami", "Chris Dyer", "Bryan Routledge", "Noah A. Smith."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Computational generation of referring expressions: A survey",
    "authors": ["Emiel Krahmer", "Kees Van Deemter."],
    "venue": "Computational Linguistics, 38(1):173–218.",
    "year": 2012
  }, {
    "title": "Baby talk: Understanding and generating image descriptions",
    "authors": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg"],
    "year": 2011
  }, {
    "title": "A Bayesian model of grounded color semantics",
    "authors": ["Brian McMahan", "Matthew Stone."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:103– 115.",
    "year": 2015
  }, {
    "title": "Midge: Generating image descriptions from computer vision detections",
    "authors": ["Margaret Mitchell", "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg"],
    "year": 2012
  }, {
    "title": "Learning in the Rational Speech Acts model",
    "authors": ["Will Monroe", "Christopher Potts."],
    "venue": "Proceedings of the 20th Amsterdam Colloquium.",
    "year": 2015
  }, {
    "title": "Color survey results",
    "authors": ["Randall Munroe."],
    "venue": "Online at http://blog.xkcd.com/2010/05/03/color-surveyresults.",
    "year": 2010
  }, {
    "title": "User’s guide to sigf: Significance testing by approximate randomisation. http://www.nlpado.de/~sebastian/ software/sigf.shtml",
    "authors": ["Sebastian Padó"],
    "year": 2006
  }, {
    "title": "Shape-based image retrieval using generic Fourier descriptor",
    "authors": ["Dengsheng Zhang", "Guojun Lu."],
    "venue": "Signal Processing: Image Communication, 17(10):825– 848.",
    "year": 2002
  }],
  "id": "SP:0ec9ec9e37131ff28efba99c6cab789d08dfff38",
  "authors": [{
    "name": "Will Monroe",
    "affiliations": []
  }, {
    "name": "Noah D. Goodman",
    "affiliations": []
  }, {
    "name": "Christopher Potts",
    "affiliations": []
  }],
  "abstractText": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fouriertransformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model’s output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (“greenish”), bare modifiers (“bright”, “dull”), and compositional phrases (“faded teal”) not seen in training.",
  "title": "Learning to Generate Compositional Color Descriptions"
}