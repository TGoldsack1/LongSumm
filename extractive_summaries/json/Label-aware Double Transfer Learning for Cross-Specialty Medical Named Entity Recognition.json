{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1–15 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The development of hospital information system and medical informatics drives the leverage of various medical data for a more efficient and intelligent medical care service. Among many kinds of medical data, electronic health records (EHRs) are one of the most valuable and informative data as they contain detailed information about the patients and the clinical practices. EHRs are essential to many intelligent clinical applications, such\n∗Weinan Zhang is the corresponding author.\nas hospital quality control and clinical decision support systems (Wu et al., 2015). Most of EHRs are recorded in an unstructured form, i.e., natural language. Hence, extracting structured information from EHRs using natural language processing (NLP), e.g., named entity recognition (NER) and entity linking, plays a fundamental role in medical informatics (Zhang and Elhadad, 2013). In this paper, we focus on medical NER from EHRs, which is a fundamental task and is widely studied in the research community (Nadeau and Sekine, 2007; Uzuner et al., 2011).\nIn practice, the difficulty of building a universally robust and high-performance medical NER system lies in the variety of medical terminologies and expressions among different departments of specialties and hospitals. However, building separate NER systems for so many specialties comes with a prohibitively high cost. The data privacy issue further discourages the sharing of the data across departments or hospitals, making it more difficult to train a canonical NER system to be applied everywhere. This raises a natural question: if we have sufficient annotated EHRs data in one source specialty, can we distill the knowledge and transfer it to help training models in a related target specialty with few annotations? By transferring the knowledge we can achieve higher performance in target specialties with lower annotation cost and bypass the data sharing concerns. This is commonly referred to as transfer learning (Pan and Yang, 2010).\nCurrent state-of-the-art transfer learning methods for NER are mainly based on deep neural networks, which perform an end-to-end training to distill sequential dependency patterns in the natural language (Ma and Hovy, 2016; Lample et al., 2016). These transfer learning methods include (i) feature representation transfer (Peng and Dredze, 2017; Kulkarni et al., 2016), which normally lever-\n1\nages deep neural networks to learn a close feature mapping between the source and target domains, and (ii) parameter transfer (Murthy et al., 2016; Yang et al., 2017), which performs parameter sharing or joint training to get the target-domain model parameters close to those of the source-domain model. To the best of our knowledge, there is no previous literature working on transfer learning for NER in the medical domain, or even in a larger scope, i.e., medical natural language processing.\nIn this paper, we propose a novel NER transfer learning framework, namely label-aware double transfer learning (La-DTL): (i) We leverage bidirectional long-short term memory (Bi-LSTM) network (Graves and Schmidhuber, 2005) to automatically learn the text representations, based on which we perform a label-aware feature representation transfer. We propose a variant of maximum mean discrepancy (MMD) (Gretton et al., 2012), namely label-aware MMD (La-MMD), to explicitly reduce the domain discrepancy of feature representations of tokens with the same label between two domains. (ii) Based on the learned feature representations from Bi-LSTM, two conditional random field (CRF) models are performed for sequence labeling for source and target domain separately, where parameter transfer learning is performed. Specifically, an upper bound of KL divergence between the source and target domain’s CRF label distributions is added over the emission and transition matrices across the source and target CRF models to explore the shareable parts of the parameters. Both (i) and (ii) have a labelaware characteristic, which will be discussed later. We further argue that label-aware characteristic is crucial for transfer learning in sequence labeling problems, e.g., NER, because only when the corresponding labels are matched, can the “similar” contexts (i.e. feature representation) and model parameters be efficiently borrowed to improve the label prediction.\nExtensive experiments are conducted on 12 cross-specialty medical NER tasks with real-world EHRs. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines, with overall 2.62% to 6.70% absolute F1-score improvement over the state-of-the-art methods. Besides, the promising experimental results on other two non-medical NER scenarios indicate that La-DTL has the potential to be seamlessly adapted to a wide range of\nNER tasks."
  }, {
    "heading": "2 Related Works",
    "text": "Named Entity Recognition (NER) is fundamental in information extraction area which aims at automatic detection of named entities (e.g., person, organization, location and geo-political) in free text (Marrero et al., 2013). Many high-level applications such as entity linking (Moro et al., 2014) and knowledge graph construction (Hachey et al., 2011) could be built on top of an NER system. Traditional high-performance approaches include conditional random fields models (CRFs) (Lafferty et al., 2001), maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and hidden Markov models (HMMs). Recently, many neural network-based models have been proposed (Collobert et al., 2011; Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016), in which few feature engineering works are needed to train a high-performance NER system. The architecture of those neural network-based models are similar, where different neural networks (LSTMs, CNNs) at different levels (char- and word-level) are applied to learn feature representations, and on top of neural networks, a CRF model is employed to make label predictions. Transfer Learning distills knowledge from a source domain to help create a high-performance learner for a target domain. Transfer learning algorithms are mainly categorized into three types, namely instance transfer, feature representation transfer and parameter transfer (Pan and Yang, 2010). Instance transfer normally samples or reweights source-domain samples to match the distribution of the target domain (Chen et al., 2011; Chu et al., 2013). Feature representation transfer typically learns a feature mapping which projects source and target domain data simultaneously onto a common feature space following similar distributions (Zhuang et al., 2015; Long et al., 2015; Shen et al., 2017). Parameter transfer normally involves a joint or constrained training for the models on source and target domains, usually introduce connections between source target parameters via sharing (Srivastava and Salakhutdinov, 2013), initialization (Perlich et al., 2014), or intermodel parameter penalty schemes (Zhang et al., 2016). Transfer Learning for NER Training a highperformance NER system requires expensive and\ntime-consuming manually annotated data. But sufficient labeled data is critical for the generalization of an NER system, especially for neural networkbased models. Thus, transfer learning for NER is a practically important problem. The first group of methods focuses on sharing model parameters but they differ in the training schemes. He and Sun (2017) proposed to train the parameter-shared model with source and target data jointly, while the learning rates for sentences from source domain are re-weighted by the similarity with target domain corpus. Yang et al. (2017) proposed a family of frameworks which share model parameters in hierarchical recurrent networks to handle crossapplication, cross-lingual, and cross-domain transfer in sequence labeling tasks. Differently, Lee et al. (2017) first trained the model with source domain data and then fine-tuned the model with little annotated target domain data.\nDomain adaptation method has been well studied in NER scenarios such as using distributed word representations (Kulkarni et al., 2016) and leveraging rule-based annotators (Chiticariu et al., 2010). Multi-task learning has also been studied to improve performance in multiple NER tasks by transferring meaningful knowledge from other tasks (Collobert et al., 2011; Peng and Dredze, 2016). To take the advantages of both domain adaptation and multi-task learning, Peng and Dredze (2017) proposed a multi-task domain adaptation model."
  }, {
    "heading": "3 Preliminaries",
    "text": "This section briefly introduces bidirectional LSTM, conditional random field and maximum mean discrepancy, which are the building blocks of our transfer learning framework. Bidirectional LSTM Recurrent neural networks (RNNs) are widely used in NLP tasks for their great capability to capture contextual information in sequence data. A widely used variant of RNNs is long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), which incorporates input and forget gates to capture both long and short term dependencies. Furthermore, it will be beneficial if we process the sequence in not only a forward but also a backward way. Thus, bidirectional LSTM (Bi-LSTM) was employed in many previous works (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016) to capture bidirectional information in a sequence. More specifi-\ncally, for token xt (embedding vector) at timestep t in sequence X = (x1,x2, ...,xn), the θbparameterized Bi-LSTM recurrently updates hidden vectors h→t = G f θb (X,h→t−1) and h ← t = Gbθb(X,h ← t+1) produced by a forward LSTM and a backward one, respectively. Then we concatenate h→t and h ← t to ht as the final hidden vector produced by Bi-LSTM:\nht = h → t ⊕ h←t .\nThe representations learned from Bi-LSTM for sequence X is thus denoted as H = (h1,h2, ...,hn). Conditional Random Field The goal of NER is to detect named entities in a sequence X by predicting a sequence of labels y = (y1, y2, ..., yn). Conditional random field (CRF) is widely used to make joint labeling of the tokens in a sequence (Lafferty et al., 2001).\nRecently, Lample et al. (2016) proposed to build a CRF layer on top of a Bi-LSTM so that the automatically learned feature representation H = (h1,h2, ...,hn) of the sequence can be directly fed into the CRF for sequence labeling. For a sequence of labels y, given the hidden vector sequence H, we define its θc-parametrized score function sθc(H,y) as:\nsθc(H,y) =\nn∑\ni=1\nEi,yi +\nn−1∑\ni=1\nAyi,yi+1 ,\nwhere E is the emission score matrix of size n×m (m is the number of unique labels), and is computed by E = HW where W is the label emission parameter matrix; A is the label transition parameter matrix; thus θc = {W,A}. We then define the conditional probability of label sequence y given H by a softmax over all possible label sequences in set Y(H) as:\npθc(y|H) = exp{sθc(H,y)}/Z(H) (1) =exp{sθc(H,y)} / ∑\ny′∈Y(H) exp{sθc(H,y′)},\nwhere θc is omitted for simplification in the following part. The training objective in the CRF layer is to maximize the log-likelihood maxθc log p(y|H). In the label prediction phase, we give the output label sequence y∗ with the highest conditional probability y∗ = argmaxy′∈Y(H) p(y′|H) by dynamic programming (Sutton et al., 2012). Maximum Mean Discrepancy Maximum Mean Discrepancy (Gretton et al., 2012) is a non-"
  }, {
    "heading": "Bi-LSTM",
    "text": "parametric test statistic to measure the distribution discrepancy in terms of the distance between the kernel mean embeddings of two distributions p and q. The MMD is defined in particular function spaces that witness the difference in distributions\nMMD(F , p, q) = sup f∈F (Ex∼p[f(x)]− Ey∼q[f(y)]).\nBy defining the function class F as the unit ball in a universal Reproducing Kernel Hilbert Space (RKHS), denoted by H, it holds that MMD[F , p, q] = 0 if and only if p = q. And then given two sets of samples X = {x1, ..., xm} and Y = {y1, ..., yn} independently and identically distributed (i.i.d.) from p and q on the data space X , the empirical estimate of MMD can be written as the distance between the empirical mean embeddings after mapping to RKHS\nMMD(X,Y ) = ∥∥∥ 1 m m∑\ni=1\nφ(xi)− 1 n\nn∑\nj=1\nφ(yj) ∥∥∥ H , (2)\nwhere φ(·) : X → H is the nonlinear feature mapping that inducesH."
  }, {
    "heading": "4 Methodology",
    "text": "In this section, we present a label-aware double transfer learning (La-DTL) framework and discuss its rationale."
  }, {
    "heading": "4.1 Framework Overview",
    "text": "Figure 1 gives an overview of La-DTL for NER. From bottom up, each input sentence is converted\ninto a sequence of embedding vectors, which are then fed into a Bi-LSTM to sequentially encode contextual information into fixed-length hidden vectors. The embedding and Bi-LSTM layers are shared among source/target domains. With labelaware maximum mean discrepancy (La-MMD) to reduce the feature representation discrepancy between two domains, the hidden vectors are directly fed into source/target domain specific CRF layers to predict the label sequence. We use domain constrained CRF layers to enhance the target domain performance.\nMore formally, let Ds = {(Xsi ,ysi )}N s\ni=1 be the training set of N s samples from the source domain and Dt = {(Xti,yti)} Nt\ni=1 be the training set of N t samples from the target domain, with N t N s. Bi-LSTM encodes a sentence X = (x1,x2, ...,xn) to hidden vectors H = (h1,h2, ...,hn). We occasionally use H(X) to denote the corresponding hidden vectors when feeding X into the Bi-LSTM. CRF decodes hidden vectors H to a label sequence ŷ = (ŷ1, ŷ2, ..., ŷn). Our goal is to improve label prediction accuracy on the target domain Dt by utilizing the knowledge from the source domain Ds:\np(y|X) =p(y|H(X)),\nlog p(y|H) = n∑\ni=1\nEi,yi +\nn−1∑\ni=1\nAyi,yi+1 − logZ(H). (3)\nThus training a transferable model p(y|X) requires both H(X) and p(y|H) to be transferable.\nWe use share word embedding and Bi-LSTM by approaching the feature representation distributions p(h|Ds) and p(h|Dt), i.e., the distributions of Bi-LSTM hidden vectors at each timestep of the sentences from the source and target domains respectively. The rationale behind it lies on the insufficiency of labeled target data. Even though LSTM has high capacity, its generalization ability highly relies on viewing “sufficient” data. Otherwise, LSTM is very likely to overfit the data. Training on both source and target data, the BiLSTM is expected to learn feature representations with high quality. Yosinski et al. (2014) provided a justification of this solution that sharing bottom layers is promising for transfer learning in practice.\nWith the sentences projected onto the same hidden space, the conditional distribution p(hs|Ds) and p(ht|Dt), however, may be distant because\nLSTM hidden vectors contain contextual information which is different across domains. In order to reduce source/target discrepancy, we refine MMD (Gretton et al., 2012) with label constraints, i.e., label-aware MMD (La-MMD). Using La-MMD, the source/target hidden states are pushed to similar distributions to make the feature representation H(X) transfer feasible.\nBased on the hidden vectors from Bi-LSTM, we adopt independent CRF layers for each domain. The rationale lies in the hypotheses that (i) the target domain predictor can better capture target data distribution which could be very unique; (ii) a good predictor trained on the source domain directly could be leveraged to assist the target domain predictor without directly borrowing the source domain training data to bypass the data privacy issue. With respect to the emission and transition score matrices ∑ Ei,yi and ∑ Ayi,yi+1 , we adopt an upper bound between source/target domains, which helps the target domain predictor to be guided by the source domain predictor. Thus p(y|H) is also transferable.\nThere are also other transfer methods, including fine-tuning, sharing parameter directly (without constraints) (He and Sun, 2017; Lee et al., 2017; Yang et al., 2017), etc. However, simply sharing models may dismiss target specific instances."
  }, {
    "heading": "4.2 Learning Objective",
    "text": "The learning objective is to minimize the following loss L with respect to parameters Θ = {θb, θc}:\nL = Lc + α LLa-MMD + β Lp + γ Lr,\nwhere Lc is the CRF loss, LLa-MMD is the LaMMD loss, Lp is the parameter similarity loss on CRF layers, andLr is the regularization term, with α, β, γ as hyperparameters to balance loss terms.\nThe CRF loss is our ultimate objective predicting the label sequence given the input sentence, i.e., we minimize the negative log-likelihood of training samples from both source/target domains:\nLc = − ε Ns\nNs∑\ni=1\nlog p(ysi |Hsi )− 1− ε N t\nNt∑\ni=1\nlog p(yti |Hti),\nwhere H are hidden vectors obtained from BiLSTM, ε is the balance coefficient. The La-MMD loss LLa-MMD and parameter similarity loss Lp are discussed in Section 4.3 and 4.4, respectively. The\nregularization term is to generally control overfitting:\nLr = ‖θb‖22 + ‖θc‖22.\nWe will provide the model convergence and hyperparameter study in Section 5.1."
  }, {
    "heading": "4.3 Bi-LSTM Feature Representation Transfer",
    "text": "To learn transferable feature representations, the maximum mean discrepancy (MMD) which measures the distance between two distributions, has been widely used in domain adaptation scenarios (Long et al., 2015; Rozantsev et al., 2016). Almost all these works focus on reducing the marginal distribution distance between different domain features in an unsupervised manner to make them indistinguishable. However, considering a word is not evenly distributed conditioning on different labels, it may result in that the discriminative property of features from different domains may not be similar, which means that close source and target samples may not have the same label. Different from previous works, we propose label-aware MMD (La-MMD) in Eq. (5) to explicitly reduce the discrepancy between hidden representations with the same label, i.e., the linear combination of the MMD for each label. For each label class y ∈ Yv, where Yv is the set of matched labels in two domains, we compute the squared population MMD between the hidden representations of source/target samples with the same label y:\nMMD2(Rsy,Rty) = 1\n(Nsy )2\nNsy∑\ni,j=1\nk(hsi ,h s j) +\n1\n(N ty)2\nNty∑\ni,j=1\nk(hti,h t j)\n− 2 NsyN ty\nNsy ,N t y∑\ni,j=1\nk(hsi ,h t j), (4)\nwhere Rsy and Rty are sets of hidden representation hs and ht with corresponding number N sy and N t y. Eq. (4) can be easily derived by casting Eq. (2) into inner product form and applying 〈φ(x), φ(y)〉H = k(x, y) where k is the reproducing kernel function (Gretton et al., 2012). For each label class, we compute the MMD loss in a normal manner. After that, we define the La-MMD loss as:\nLLa-MMD = ∑\ny∈Yv µy ·MMD2(Rsy,Rty), (5)\nwhere µy is the corresponding coefficient. The illustration of La-MMD is shown in Figure 2.\nOnce we have applied this La-MMD to our representations learned from Bi-LSTM, the representation distribution of instances with the same label from different domains should be close. Then the standard CRF layer which has a simple linear structure takes these similar representations as input and is likely to give a more transferable label decision for instances with the same label."
  }, {
    "heading": "4.4 CRF Parameter Transfer",
    "text": "Simply sharing the CRF layer is non-promising when source/target data are diversely distributed. According to probability decomposition in Eq. (3), in order to transfer on source/target CRF layers, more specifically, p(y|H), we reduce the KL divergence from pt(y|H) to ps(y|H). But directly reducing DKL(ps(y|H)||pt(y|H)) is intractable, we tend to reduce its upper bound:\nDKL(p s(y|H)||pt(y|H))\n= ∑\ny∈Y(H) ps(y|H) log(p s(y|H) pt(y|H) )\n=−H(ps(y|H))− ∑\ny∈Y(H) ps(y|H) log pt(y|H)\n≤c(‖Ws −Wt‖22 + ‖As −At‖22) 1 2 , (6)\nwhere H(·) is the entropy of distribution (·) and c is a constant. The detailed proof is provided in Appendix A.1. Since c(‖Ws−Wt‖22+‖As−At‖22) is the upper bound of DKL(ps(y|H)‖pt(y|H)),\nwe conduct CRF parameter transfer by minimizing\nLp = ‖Ws −Wt‖22 + ‖A s −At‖22.\nIt turns out that a similar regularization term is applied in our CRF parameter transfer method and the regularization framework (RF) for domain adaptation (Lu et al., 2016). However, RF is proposed to generalize the feature augmentation method in (Daume III, 2007), and these two methods are only discussed from a perspective of the parameter. There is no guarantee that two models having similar parameters yields similar output distributions. In this work, we discuss the model behavior in CRF conditions, and we successfully prove that two CRF models having similar parameters (in Euclidean space) yields similar output distributions. In another word, our method guarantees transferability in the model behavior level, while previous works are limited in parameter level.\nThe CRF parameter transfer is illustrated in Figure 3, which is also label-aware since the L2 constraint is added over parameters corresponding to the same label in two domains, e.g., WsO and W t O."
  }, {
    "heading": "4.5 Training",
    "text": "We train La-DTL in an end-to-end manner with mini-batch AdaGrad (Duchi et al., 2011). One mini-batch contains training samples from both domains, otherwise the computation of LLa-MMD can not be performed. During training, word (and character) embeddings are fine-tuned to adjust real data distribution. During both training and decoding (testing) of CRF layers, we use dynamic programming to compute the normalizer in Eq. (1) and infer the label sequence."
  }, {
    "heading": "5 Experiments",
    "text": "In this section, we evaluate La-DTL1 and other baseline methods on 12 cross-specialty NER problems based on real-world datasets. The experimental results show that La-DTL steadily outperforms other baseline models in all tasks significantly. We also conduct further ablation study and robustness study. We evaluate La-DTL on two more nonmedical NER transfer tasks to validate its general efficacy over a wide range of applications."
  }, {
    "heading": "5.1 Cross-Specialty NER",
    "text": "Datasets We collected a Chinese medical NER (CM-NER) corpus for our experiments. This corpus contains 1600 de-identified EHRs of our affiliated hospital from four different specialties in four departments: Cardiology (500), Respiratory (500), Neurology (300) and Gastroenterology (300), and the research had been reviewed and approved by the ethics committee. Named entities are annotated in the BIOES format (Begin, Inside, Outside, End and Single), with 30 types in total. The statistics of CM-NER is shown in Table 1. Baselines The following methods are compared. For a fair comparison, we implement La-DTL and baselines with the same base model introduced in (Lample et al., 2016) but with different transfer techniques.\n• Non-transfer uses the target domain labeled data only.\n• Domain mask and Linear projection belong to the same framework proposed by Peng and Dredze (2017) but have different implementations at the projection layer, which aims to produce shared feature representations among different domains through a linear transformation.\n• Re-training is proposed by Lee et al. (2017), where an artificial neural networks (ANNs)\n1https://github.com/felixwzh/La-DTL\nis first trained on the source domain and then re-trained on the target domain.\n• Joint-training is a transfer learning method proposed by Yang et al. (2017) where different tasks are trained jointly.\n• CD-learning is a cross-domain learning method proposed by He and Sun (2017), where each source domain training example’s learning rate is re-weighted.\nExperimental Settings We use 23,217 unlabeled clinical records to train the word embeddings (word2vec) at 128 dimensions using skipgram model (Mikolov et al., 2013). The hidden state size is set to be 200 for word-level Bi-LSTM. We evaluate La-DTL for cross-specialty NER with CM-NER in 12 transfer tasks, results shown in Table 2. For each task, we take the whole source domain training set Ds and 10% sentences of the target domain training set Dt as training data. We use the development set in target domain to search hyper-parameters including training epochs. We then take the models to make the prediction in target domain test set and use F1-score as the evaluation metric. Statistical significance has been determined using a randomization version of the paired sample t-test (Cohen, 1995). Results and Discussion From the results of 12 cross-specialty NER tasks shown in Table 2, we find that La-DTL outperforms all the strong baselines in all the 12 cross-specialty transfer learning tasks, with 2.62% to 6.70% F1-score lift over state-of-the-art baseline methods. Meanwhile, Linear projection and Domain mask (Peng and Dredze, 2017) do not perform as good as other three baselines, which may be because such linear transformation methods are likely to weaken the representations. While other three baseline methods all share the whole model between source/target domains but differ in the training schemes and performance.\nTo better understand the transferability of LaDTL, we also evaluate three variants of LaDTL: La-MMD, CRF-L2, and MMD-CRF-L2. La-MMD and CRF-L2 have the same networks and loss function as La-DTL but with different building blocks: La-MMD has β = 0, while CRFL2 has α = 0. In MMD-CRF-L2, we replace La-MMD loss LLa-MMD in La-DTL with a vanilla MMD loss:\nLMMD = MMD2(Rs,Rt),\nwhere Rs and Rt are sets of hidden representation from source and target domain. Results in Table 2 show that: (i) Using La-MMD alone does achieve satisfactory performance since it outperforms the best baseline Joint-training (Yang et al., 2017) in 7 of 12 tasks. And it has a significant improvement over Domain mask and Linear projection methods (Peng and Dredze, 2017), which indicates that using La-MMD to reduce the domain discrepancy of feature representations in sequence tagging tasks is promising. (ii) CRF-L2 is also a promising method when transferring between NER tasks, and it improves the La-MMD method significantly when these two methods are combined to form La-DTL. (iii) Label-aware characteristic is important in sequence labeling problems because there is an obvious performance drop when La-MMD is replaced with a vanilla MMD in La-DTL. But MMD-CRF-L2 still has very competitive performance compared to all the baseline methods. This shows positive empirical evidence that transferring knowledge at both BiLSTM feature representation level and CRF parameter level for NER tasks is better than transferring knowledge at only one of these two levels, as discussed in Section 4.1."
  }, {
    "heading": "Robustness to Target Domain Data Sparsity",
    "text": "We further study the sparsity problem (target domain) of La-DTL in C→R task comparing to Joint-training (Yang et al., 2017) and Non-transfer method. We evaluate La-DTL with different data volume (sampling rate: 10%, 25%, 50%, 100%) on the target domain training set. Results are shown in Figure 4(a). We observe that La-DTL outperforms Joint-training and Non-transfer results under all circumstances, and the improvement of LaDTL is more significant when the sampling rate is lower.\nTo show La-DTL’s convergence and significant improvement over Joint-training, we repeat the 10% sampling rate experiment for 10 times with 10 random seeds. The F1-score on the target domain development set for two methods with a 95% confidence interval is shown in Figure 4(b) where La-DTL outperforms Joint-training method significantly. Hyperparameter Study We study the influence of three key hyperparameters in La-DTL: α, β, and ε in C→R task with 10% target domain sampling rate. We first apply a rough grid search for the three hyperparameters, and the result is (α = 0.02, β = 0.03, ε = 0.3). We then fix two hyperparameters and test the third one in a finer granularity. The results in Figure 5 indicate that setting α ∈ [0.01, 0.04] could better leverage LaMMD and further setting β ∈ [0.03, 0.12] and ε ∈ [0.3, 0.4] yields the best empirical perfor-\nmance. This shows that we need to balance the learning objective of the source and target domains for better transferability."
  }, {
    "heading": "5.2 NER Transfer Experiment on Non-medical Corpus",
    "text": "To show La-DTL could be applied in a wide range of NER transfer learning scenarios, we make experiments on two non-medical NER tasks. Corpora’s details are shown in Table 3. WeiboNER Transfer Following He and Sun (2017); Peng and Dredze (2017), we transfer knowledge from SighanNER (MSR corpus of the sixth SIGHAN Workshop on Chinese language processing) to WeiboNER (a social media NER corpus) (Peng and Dredze, 2015). Results in Table 4 show that La-DTL outperforms all the baseline methods in Chinese social media domain. TwitterNER Transfer Following Yang et al. (2017) we transfer knowledge from CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) to TwitterNER (Ritter et al., 2011). Since the entity types in these two corpora cannot be exactly matched, La-DTL and Joint-training (Yang et al., 2017) can be applied directly in this case while other baselines can not. Because the CRF parameter transfer of La-DTL is label-aware, and Jointtraining simply leverages two independent CRF layers. The results are shown in Table 5, where LaDTL again outperforms Joint-training, indicating that La-DTL could be applied seamlessly to trans-\nfer learning scenarios with mismatched label sets and languages like English."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper, we propose La-DTL, a label-aware double transfer learning framework, to conduct both Bi-LSTM feature representation transfer and CRF parameter transfer with label-aware constraints for cross-specialty medical NER tasks. To our best knowledge, this is the first work on transfer learning for medical NER in cross-specialty scenario. Experiments on 12 cross-specialty NER tasks show that La-DTL provides consistent performance improvement over strong baselines. We further perform a set of experiments on different target domain data size, hyperparameter study and other non-medical NER tasks, where La-DTL shows great robustness and wide efficacy. For future work, we plan to jointly perform NER and entity linking for better cross-specialty media structural information extraction."
  }, {
    "heading": "Acknowledgments",
    "text": "The work done by SJTU is sponsored by Synyi-SJTU Innovation Program, National Natural Science Foundation of China (61632017, 61702327, 61772333) and Shanghai Sailing Program (17YF1428200)."
  }, {
    "heading": "A Appendix",
    "text": ""
  }, {
    "heading": "A.1 Detailed Proof",
    "text": "Recall the bound as in Eq. (6):\nLemma A.1. c1(‖Ws −Wt‖22 + ‖As −At‖22) is the upper bound of (ss(H,y)− st(H,y))2.\nProof of Lemma A.1. ⊗ refers to convolutional product, HW ,HA are mask matrices corresponding to the given hidden vectors H, and c1 is a constant. We have:\n(ss(H,y)− st(H,y))2\n=(\nn∑\ni=1\nEsi,yi + n−1∑\ni=1\nAsyi,yi+1 − n∑\ni=1\nEti,yi − n−1∑\ni=1\nAtyi,yi+1) 2\n=(Ws ⊗HW + As ⊗HA −Wt ⊗HW −At ⊗HA)2\n=((Ws −Wt)⊗HW + (As −At)⊗HA)2\n≤2((Ws −Wt)⊗HW )2 + 2((As −At)⊗HA)2 =2( ∑\ni,j\n(Ws −Wt)i,j ·HWi,j)2 + 2( ∑\np,q\n(As −At)p,q ·HAp,q)2\n≤2( ∑\ni,j\n(Ws −Wt)2i,j · ∑\ni,j\n(HWi,j) 2) + 2(\n∑\np,q\n(As −At)2p,q · ∑\np,q\n(HAp,q) 2)\n=2(‖Ws −Wt‖22 · ‖HW ‖22) + 2(‖As −At‖22 · ‖HA‖22) ≤c1(‖Ws −Wt‖22 + ‖As −At‖22).\nLemma A.2. c(‖Ws −Wt‖22 + ‖As −At‖22) 1 2 is the upper bound of DKL(ps(y|H)||pt(y|H)).\nProof of Lemma A.2. With Lemma. (A.1), we set ε = (c1(‖Ws −Wt‖22 + ‖As − At‖22)) 1 2 ≥ 0 and c = 2c 1 2 1 , and we have:\nss(H,y)− ε ≤ st(H,y) ≤ ss(H,y) + ε, (7)\nlog{ ∑\ny′∈Y(H) exp[ss(H,y′)]} − ε ≤ log{\n∑\ny′∈Y(H) exp[st(H,y′)]} ≤ log{\n∑\ny′∈Y(H) exp[ss(H,y′)]}+ ε.\n(8)\nWith Eq. (7) and Eq. (8), we can derive\n− ∑\ny∈Y(H) ps(y|H) log pt(y|H)\n=− ∑\ny∈Y(H) ps(y|H) log exp[s t(H,y)]∑ y′∈Y(H) exp[s t(H,y′)]\n=− ∑\ny∈Y(H) ps(y|H)\n{ st(H,y)− log{ ∑\ny′∈Y(H) exp[st(H,y′)]}\n}\n≤− ∑\ny∈Y(H) ps(y|H)\n{ ss(H,y)− ε− log{ ∑\ny′∈Y(H) exp[ss(H,y′)]} − ε\n}\n=− ∑\ny∈Y(H) ps(y|H)\n{ log exp[ss(H,y)]∑ y′∈Y(H) exp[s s(H,y′)] −2ε }\n=− ∑\ny∈Y(H) ps(y|H)\n{ log ps(y|H)−2ε }\n=H(ps(y|H)) + 2ε.\nFinally, we have\nDKL(p s(y|H)||pt(y|H))\n= ∑\ny∈Y(H) ps(y|H) log(p s(y|H) pt(y|H) )\n=−H(ps(y|H))− ∑\ny∈Y(H) ps(y|H) log pt(y|H)\n≤−H(ps(y|H)) +H(ps(y|H)) + 2ε =c(‖Ws −Wt‖22 + ‖As −At‖22) 1 2 ."
  }, {
    "heading": "A.2 Case Analysis",
    "text": "In clinical practice, patients with specific diseases would be assigned to different departments, and specialist doctors in their department may pay more attention to the specific disease. When writing a medical chart, these specific diseases and related clinical findings would have a more detailed description. Therefore, some medical terms would have enriched meanings in different departments accordingly. For example, patients with rheumatic heart disease are often treated in the department of Cardiology. The term, “rheumatic”, a modifier, describes and limits the type of “heart disease”. In English, “rheumatic” is an adjective modifying “heart disease”. However, in Chinese, “rheumatic heart disease” can be regarded as two diseases, “rheumatism” and “heart disease”. In the department of Cardiology, “rheumatic heart dis-\nease” is usually mentioned as a single term. While in other departments, “rheumatism” and “heart disease” are mostly two independent named entities in annotated datasets. As such, it is difficult to train an NER model to capture the relationship between “rheumatism” and “heart disease”, and band them as a whole. In the training set of our study, the diagnostic term “rheumatic heart disease” (including synonym) is mentioned for 17 times in Dept. Cardiology, 16 times in Dept. Respiratory, none in Dept. Neurology and 3 times in Dept. Gastroenterology. We use the data from the first 3 departments as source domain training set respectively, and the data from Dept. Gastroenterology as the target domain training set. We test our models on the test set from Dept. Gastroenterology, where “rheumatic heart disease” is mentioned 3 times, and compare the results across models\nwith/without transfer learning. As expected, models with source training data from Dept. Cardiovascular and Respiration correctly predict all these entities, but the model using source data from Dept. Neurology fails and so does a model without transfer learning.\nPatients with pulmonary heart disease were often referred to Dept. Respiratory and Dept. Cardiology. In our training set, “pulmonary heart disease” (including synonym) is labeled for 24 times in Dept. Respiratory and 4 times in Dept. Cardiology. In English, “pulmonary” modified “heart disease”. In Chinese, “pulmonary heart disease” contains body structure “lung” and disease name “heart disease”. The model trained with the source set from both from department of respiratory and cardiology could correctly recognize the relation between lung and heart disease and predict the entity in the test set from Dept. Gastroenterology.\nSimilarly, “coronary atherosclerotic heart disease” contains two disease names, “coronary atherosclerosis” and “heart disease”. Training model using source set from a department where the terms are enriched could improve the performance of recognizing the whole entity."
  }, {
    "heading": "A.3 Medical Experiments Details",
    "text": "The 30 entity types for medical domain are: Symptom, Disease, Examination, Treatment, Laboratory index, Products, Body structure, Frequency, Negative word, Value, Trend, Modification, Temporal word, Noun of locality, Degree modifier, Probability, Object, Organism, Location, Person, Pronoun, Privacy information, Accident, Action, Header, Instrument and material, Nonphysiological structure, Dosage, Scale, and Preposition."
  }, {
    "heading": "Word Bi-LSTM",
    "text": ""
  }, {
    "heading": "A.4 Non-medical Experiments Details",
    "text": "WeiboNER Transfer Both SighanNER and WeiboNER are annotated in the BIO format (Begin, Inside and Outside), but there is one more entity type (geo-political) in WeiboNER. For a fair comparison, we follow Peng and Dredze (2017); He and Sun (2017) to merge geo-political entities and locations in WeiboNER, to match different labeling schemes between WeiboNER and SighanNER. We use the inconsistencies fixed second version of WeiboNER data and word embeddings provided by WeiboNER’s developers (Peng and Dredze, 2015)2 in this experiment.\nTwitterNER Transfer To show that La-DTL could be applied in transfer learning for NER scenario with mismatched\n2 https://github.com/hltcoe/golden-horse\nnamed entity types and languages like English, we conduct this experiment transfer from CoNLL 2003 English NER to TwitterNER. The four entity types in CoNLL 2003 English NER are LOC, PER, ORG, and MISC. The ten entity types in TwitterNER are company, facility, geo-loc, movie, musicartist, other, person, product, sportsteam, and tvshow.\nThe Joint-training method (Yang et al., 2017) separates the CRF layers for each domain to bypass the label mismatch problem. Since our La-DTL is label-aware, we match four pairs of named entities between two CoNLL 2003 English NER and TwitterNER: LOC with geo-loc, PER with person, ORG with company and MISC with other to compute LLa-MMD and Lp, and leave six named entities unmatched. Following Yang et al. (2017), We leverage char-level Bi-LSTM to generate better word representations, concatenate it with pre-trained word embeddings and feed concatenated embeddings to the word-level Bi-LSTM. The framework used for language like English is illustrated in Figure 6.\nWe also convert all characters to lowercase and use the same word embeddings provided by Yang et al. (2017)3. Also, we concatenate the training set and the development set for both domains and sample the same 10% from TwitterNER as (Yang et al., 2017) to be target domain training data. Since Yang et al. (2017) merge training and development set into training data, both Yang et al. (2017) and we report the best performance in the target domain test set.\n3 https://github.com/kimiyoung/transfer"
  }],
  "year": 2018,
  "references": [{
    "title": "Co-training for domain adaptation",
    "authors": ["Minmin Chen", "Kilian Q Weinberger", "John Blitzer."],
    "venue": "Advances in Neural Information Processing Systems 24, pages 2456–2464. Curran Associates, Inc.",
    "year": 2011
  }, {
    "title": "Domain adaptation of rule-based annotators for named-entity recognition tasks",
    "authors": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan."],
    "venue": "Proceedings of the 2010 Conference on Empirical Meth-",
    "year": 2010
  }, {
    "title": "Named entity recognition with bidirectional lstm-cnns",
    "authors": ["Jason Chiu", "Eric Nichols."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:357–370.",
    "year": 2016
  }, {
    "title": "Selective transfer machine for personalized facial action unit detection",
    "authors": ["Wen-Sheng Chu", "Fernando De la Torre", "Jeffery F Cohn."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3515–3522.",
    "year": 2013
  }, {
    "title": "Empirical methods for artificial intelligence, volume 139",
    "authors": ["Paul R Cohen."],
    "venue": "MIT press Cambridge, MA.",
    "year": 1995
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "J. Mach. Learn. Res., 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Frustratingly easy domain adaptation",
    "authors": ["Hal Daume III."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263. Association for Computational Linguistics.",
    "year": 2007
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "J. Mach. Learn. Res., 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks, 18(5):602–610.",
    "year": 2005
  }, {
    "title": "A kernel two-sample test",
    "authors": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Schölkopf", "Alexander Smola."],
    "venue": "J. Mach. Learn. Res., 13:723–773.",
    "year": 2012
  }, {
    "title": "Graph-based named entity linking with wikipedia",
    "authors": ["Ben Hachey", "Will Radford", "James R. Curran."],
    "venue": "Proceedings of the 12th International Conference on Web Information System Engineering, WISE’11, pages 213–226, Berlin, Heidelberg.",
    "year": 2011
  }, {
    "title": "A unified model for cross-domain and semi-supervised named entity recognition in chinese social media",
    "authors": ["Hangfeng He", "Xu Sun."],
    "venue": "AAAI, pages 3216–3222.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Domain adaptation for named entity recognition in online media with word embeddings",
    "authors": ["Vivek Kulkarni", "Yashar Mehdad", "Troy Chevalier."],
    "venue": "arXiv preprint arXiv:1612.00148.",
    "year": 2016
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Transfer learning for named-entity recognition with neural networks",
    "authors": ["Ji Young Lee", "Franck Dernoncourt", "Peter Szolovits."],
    "venue": "arXiv preprint arXiv:1705.06273.",
    "year": 2017
  }, {
    "title": "Learning transferable features with deep adaptation networks",
    "authors": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael Jordan."],
    "venue": "Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine",
    "year": 2015
  }, {
    "title": "A general regularization framework for domain adaptation",
    "authors": ["Wei Lu", "Hai Leong Chieu", "Jonathan Löfgren."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 950–954, Austin, Texas. Associa-",
    "year": 2016
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Named entity recognition: Fallacies, challenges and opportunities",
    "authors": ["Mnica Marrero", "Julin Urbano", "Sonia Snchez-Cuadrado", "Jorge Morato", "Juan Miguel Gmez-Berbs."],
    "venue": "Computer Standards & Interfaces, 35(5):482 – 489.",
    "year": 2013
  }, {
    "title": "Maximum entropy markov models for information extraction and segmentation",
    "authors": ["Andrew McCallum", "Dayne Freitag", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages",
    "year": 2000
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates,",
    "year": 2013
  }, {
    "title": "Entity linking meets word sense disambiguation: a unified approach",
    "authors": ["Andrea Moro", "Alessandro Raganato", "Roberto Navigli."],
    "venue": "Transactions of the Association for Computational Linguistics, 2:231– 244.",
    "year": 2014
  }, {
    "title": "Sharing network parameters for crosslingual named entity recognition",
    "authors": ["V Murthy", "Mitesh Khapra", "Pushpak Bhattacharyya"],
    "venue": "arXiv preprint arXiv:1607.00198",
    "year": 2016
  }, {
    "title": "A survey of named entity recognition and classification",
    "authors": ["David Nadeau", "Satoshi Sekine."],
    "venue": "Lingvisticae Investigationes, 30(1):3–26.",
    "year": 2007
  }, {
    "title": "A survey on transfer learning",
    "authors": ["Sinno Jialin Pan", "Qiang Yang."],
    "venue": "IEEE Trans. on Knowl. and Data Eng., 22(10):1345–1359.",
    "year": 2010
  }, {
    "title": "Named entity recognition for chinese social media with jointly trained embeddings",
    "authors": ["Nanyun Peng", "Mark Dredze."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal.",
    "year": 2015
  }, {
    "title": "Improving named entity recognition for chinese social media with word segmentation representation learning",
    "authors": ["Nanyun Peng", "Mark Dredze."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
    "year": 2016
  }, {
    "title": "Multi-task domain adaptation for sequence tagging",
    "authors": ["Nanyun Peng", "Mark Dredze."],
    "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 91–100, Vancouver, Canada. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Machine learning for targeted display advertising: Transfer learning in action",
    "authors": ["Claudia Perlich", "Brian Dalessandro", "Troy Raeder", "Ori Stitelman", "Foster Provost."],
    "venue": "Mach. Learn., 95(1):103–127.",
    "year": 2014
  }, {
    "title": "Named entity recognition in tweets: An experimental study",
    "authors": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524–1534, Edinburgh, Scotland,",
    "year": 2011
  }, {
    "title": "Beyond sharing weights for deep domain adaptation",
    "authors": ["Artem Rozantsev", "Mathieu Salzmann", "Pascal Fua."],
    "venue": "arXiv preprint arXiv:1603.06432.",
    "year": 2016
  }, {
    "title": "Wasserstein distance guided representation learning for domain adaptation",
    "authors": ["Jian Shen", "Yanru Qu", "Weinan Zhang", "Yong Yu."],
    "venue": "arXiv preprint arXiv:1707.01217.",
    "year": 2017
  }, {
    "title": "Discriminative transfer learning with tree-based priors",
    "authors": ["Nitish Srivastava", "Ruslan R Salakhutdinov."],
    "venue": "Advances in Neural Information Processing Systems 26, pages 2094–2102. Curran Associates, Inc.",
    "year": 2013
  }, {
    "title": "An introduction to conditional random fields. Foundations and Trends R",
    "authors": ["Charles Sutton", "Andrew McCallum"],
    "year": 2012
  }, {
    "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder."],
    "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
    "year": 2003
  }, {
    "title": "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text",
    "authors": ["Özlem Uzuner", "Brett R South", "Shuying Shen", "Scott L DuVall."],
    "venue": "Journal of the American Medical Informatics Association, 18(5):552–556.",
    "year": 2011
  }, {
    "title": "Named entity recognition in chinese clinical text using deep neural network",
    "authors": ["Yonghui Wu", "Min Jiang", "Jianbo Lei", "Hua Xu."],
    "venue": "Studies in health technology and informatics, 216:624.",
    "year": 2015
  }, {
    "title": "Transfer learning for sequence tagging with hierarchical recurrent networks",
    "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W Cohen."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "How transferable are features in deep neural networks",
    "authors": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"],
    "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2,",
    "year": 2014
  }, {
    "title": "Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts",
    "authors": ["Shaodian Zhang", "Noémie Elhadad."],
    "venue": "Journal of biomedical informatics, 46(6):1088–1098.",
    "year": 2013
  }, {
    "title": "Collective noise contrastive estimation for policy transfer learning",
    "authors": ["Weinan Zhang", "Ulrich Paquet", "Katja Hofmann."],
    "venue": "AAAI, pages 1408–1414.",
    "year": 2016
  }, {
    "title": "Supervised representation learning: Transfer learning with deep autoencoders",
    "authors": ["Fuzhen Zhuang", "Xiaohu Cheng", "Ping Luo", "Sinno Jialin Pan", "Qing He."],
    "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI’15, pages 4119–",
    "year": 2015
  }, {
    "title": "2017) and we report the best performance in the target domain test",
    "authors": ["Yang"],
    "year": 2017
  }],
  "id": "SP:8b93193cc0beb3b2e0a653fa1c959aee06aef044",
  "authors": [{
    "name": "Zhenghui Wang",
    "affiliations": []
  }, {
    "name": "Yanru Qu",
    "affiliations": []
  }, {
    "name": "Liheng Chen",
    "affiliations": []
  }, {
    "name": "Jian Shen",
    "affiliations": []
  }, {
    "name": "Weinan Zhang",
    "affiliations": []
  }, {
    "name": "Shaodian Zhang",
    "affiliations": []
  }, {
    "name": "Yimei Gao",
    "affiliations": []
  }, {
    "name": "Gen Gu",
    "affiliations": []
  }, {
    "name": "Ken Chen",
    "affiliations": []
  }, {
    "name": "Yong Yu",
    "affiliations": []
  }],
  "abstractText": "We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a labelaware double transfer learning framework (LaDTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that LaDTL is potential to be seamlessly adapted to a wide range of NER tasks.",
  "title": "Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition"
}