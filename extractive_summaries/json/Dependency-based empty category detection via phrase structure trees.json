{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2013, pages 1051–1060, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "In modern theoretical linguistics, empty categories (ECs) are an important piece of machinery in representing the syntactic structure of a sentence and they are used to represent phonologically null elements such as dropped pronouns and traces of dislocated elements. They have also found their way into largescale treebanks which have played an important role in advancing the state of the art in syntactic parsing. In phrase-structure treebanks, ECs have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Together with labeled brackets and function tags, they make up the full syntactic representation of a sentence.\nThe use of ECs captures some cross-linguistic commonalities and differences. For example, while both the Penn English TreeBank (PTB) (Marcus et al., 1993) and the Chinese TreeBank (CTB) (Xue\net al., 2005) use traces to represent the extraction site of a dislocated element, dropped pronouns (represented as *pro*s) are much more widespread in the CTB. This is because Chinese is a pro-drop language (Huang, 1984) that allows the subject to be dropped in more contexts than English does. While detecting and resolving traces is important to the interpretation of the syntactic structure of a sentence in both English and Chinese, the prevalence of dropped nouns in Chinese text gives EC detection added significance and urgency. They are not only an important component of the syntactic parse of a sentence, but are also essential to a wide range of NLP applications. For example, any meaningful tracking of entities and events in natural language text would have to include those represented by dropped pronouns. If Chinese is translated into a different language, it is also necessary to render these dropped pronouns explicit if the target language does not allow pro-drop. In fact, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic EC detection on statistical machine translation.\nSome ECs can be resolved to an overt element in the same text while others only have a generic reference that cannot be linked to any specific entity. Still others have a plausible antecedent in the text, but are not annotated due to annotation limitations. A common practice is to resolve ECs in two separate stages (Johnson, 2002; Dienes and Dubey, 2003b; Dienes and Dubey, 2003a; Campbell, 2004; Gabbard et al., 2006; Schmid, 2006; Cai et al., 2011). The first stage is EC detection, where empty categories are first located and typed. The second stage\n1051\nis EC resolution, where empty categories are linked to an overt element if possible.\nIn this paper we describe a novel approach to detecting empty categories in Chinese, using the CTB as training and test data. More concretely, EC detection involves (i) identifying the position of the EC, relative to some overt word tokens in the same sentence, and (ii) determining the type of EC, e.g., whether it is a dropped pronoun or a trace. We focus on EC detection here because most of the ECs in the Chinese Treebank are either not resolved to an overt element or linked to another EC. For example, dropped pronouns (*pro*) are not resolved, and traces (*T*) in relative clauses are linked to an empty relative pronoun (*OP*).\nIn previous work, ECs are either represented linearly, where ECs are indexed to the following word (Yang and Xue, 2010) or attached to nodes in a phrase structure tree (Johnson, 2002; Dienes and Dubey, 2003b; Gabbard et al., 2006). In a linear representation where ECs are indexed to the following word, it is difficult to represent consecutive ECs because that will mean more than one EC will be indexed to the same word (making the classification task more complicated). While in English consecutive ECs are relatively rare, in Chinese this is very common. For example, it is often the case that an empty relative pronoun (*OP*) is followed immediately by a trace (*T*). Another issue with the linear representation of ECs is that it leaves unspecified where the EC should be attached, and crucial dependencies between ECs and other elements in the syntactic structure are not represented, thus limiting the utility of this task.\nIn a phrase structure representation, ECs are attached to a hierarchical structure and the problem of multiple ECs indexed to the same word token can be avoided because linearly consecutive ECs may be attached to different non-terminal nodes in a phrase structure tree. In a phrase structure framework, ECs are evaluated based on their linear position as well as on their contribution to the overall accuracy of the syntactic parse (Cai et al., 2011).\nIn the present work, we propose to look at EC detection in a dependency structure representation, where we define EC detection as (i) determining its linear position relative to the following word token, (ii) determining its head it is a dependent of, and (iii)\ndetermining the type of EC. Framing EC detection this way also requires a new evaluation metric. An EC is considered to be correctly detected if its linear position, its head, and its type are all correctly determined. We report experimental results that show even using this more stringent measure, our EC detection system achieved performance that improved significantly over the state-of-the-art results.\nThe rest of the paper is organized as follows. In Section 2, we will describe how to represent ECs in a dependency structure in detail and present our approach to EC detection. In Section 3, we describe how linguistic information is encoded as features. In Section 4, we discuss our experimental setup and present our results. In Section 5, we describe related work. Section 6 concludes the paper."
  }, {
    "heading": "2 Approach",
    "text": "In order to detect ECs anchored in a dependency tree, we first convert the phrase structure trees in the CTB into dependency trees. After the conversion, each word token in a dependency tree, including the ECs, will have one and only one head (or parent). We then train a classifier to predict the position and type of ECs in the dependency tree. Let W be a sequence of word tokens in a sentence, and T is syntactic parse tree for W , our task is to predict whether there is a tuple (h, t, e), such that h and t are word tokens in W , e is an EC, h is the head of e, and t immediately follows e. When EC detection is formulated as a classification task, each classification instance is thus a tuple (h, t). The input to our classifier is T , which can either be a phrase structure tree or a dependency tree. We choose to use a phrase structure tree because phrase structure parsers trained on the Chinese Treebank are readily available, and we also hypothesize that phrase structure trees have a richer hierarchical structure that can be exploited as features for EC detection."
  }, {
    "heading": "2.1 Empty categories in the Chinese Treebank",
    "text": "According to the CTB bracketing guidelines (Xue and Xia, 2000), there are seven different types of ECs in the CTB. Below is a brief description of the empty categories:\n1. *pro*: small pro, used to represent dropped pronouns.\n2. *PRO*: big PRO, used to represent shared elements in control structures or elements that have generic references. 3. *OP*: null operator, used to represent empty relative pronouns. 4. *T*: trace left by movement such as topicalization and relativization. 5. *RNR*: right node raising. 6. *: trace left by passivization and raising. 7. *?*: missing elements of unknown category.\nAn example parse tree with ECs is shown in Figure 1. In the example, there are two ECs, an empty relative pronoun (*OP*) and a trace (*T*), a common syntactic pattern for relative clauses in the CTB."
  }, {
    "heading": "2.2 Converting phrase structure to dependency structure",
    "text": "We convert the phrase structure parses in the CTB to dependency trees using the conversion tool that generated the Chinese data sets for the CoNLL 2009 Shared Task on multilingual dependency parsing and semantic role labeling (Hajič et al., 2009)1. While the Chinese data of CoNLL 2009 Shared Task does not include ECs, the tool has an option of preserving the ECs in the conversion process. As an example, the dependency tree in Figure 2 is converted from the phrase structure tree in Figure 1, with the ECs preserved.\n1The tool can be downloaded at http://www.cs.brandeis.edu/ clp/ctb/ctb.html.\nIn previous work EC detection has been formulated as a classification problem with the target of the classification being word tokens (Yang and Xue, 2010; Chung and Gildea, 2010), or constituents in a parse tree (Gabbard et al., 2006). When word tokens are used as the target of classification, the task is to determine whether there is an EC before each word token, and what type EC it is. One shortcoming with that representation is that more than one EC can precede the same word token, as is the case in the example in Figure 1, where both *OP* and *T* precede 涉及 (“involve”). In fact, (Yang and Xue, 2010) takes the last EC when there is a sequence of ECs and as a result, some ECs will never get the chance to be detected. Notice that this problem can be avoided in a dependency structure representation if we make the target of classification a tuple that consists of the following word token and the head of the EC. From Figure 2, it should be clear that while *OP* and *T* both precede the same word token涉 及 (“involve”), they have different heads, which are 的 (DE) and涉及 respectively.\nDependency-based EC detection also has other nice properties. For ECs that are arguments of their verbal head, when they are resolved to some overt element, the dependency between the referent of the EC and its head will be naturally established. This can be viewed as an alternative to the approach adopted by Levy and Manning (2004), where phrase structure parses are augmented to recover non-local dependencies. Dependency structures are also easily decomposable into head/dependency pairs and this makes the evaluation more straightforward. Each classification instance can be evaluated independently of other parts of the dependency structure."
  }, {
    "heading": "2.3 One pass vs two passes",
    "text": "With pairs of tokens (h, t) as the classification target, all possible pairs in a sentence will have to be considered and there will be a large number of (h, t) tuples that are not associated with an EC, leading to a highly imbalanced data set. One can conceive a two-pass scenario where we first make a binary decision of whether there is an empty category associated with the head in the first pass and then determine whether there is an EC associated with the tuple as well as the EC type in the second pass. The alternative is to have a one-pass model in which we\nadd a NONE category indicating there is no EC associated with the tuple. With the seven EC types presented earlier in this section, this will be an eightway classification problem. There are reasons for either model: the one-pass model is simpler but in the two-pass model we can bring different sources of information to bear on each sub-problem. Ultimately which model leads to better accuracy is an empirical question. We experimented with both models and it turned out that they led to very similar results. In this paper, we report results from the simpler onepass model."
  }, {
    "heading": "3 Features",
    "text": "We explored a wide range of features, all derived from the phrase structure parse tree (T ). With each classification instance being a tuple (h, t), the “pivots” for these features are h the head, t the word token following the EC, and p, the word token preceding the EC. The features we tried fall into six broad groups that are all empirically confirmed to have made a positive contribution to our classification task. These are (i) horizontal features, (ii) vertical features, (iii) targeted grammatical constructions, (iv) head information, (v) transitivity features, and (vi) semantic role features. We obviously have looked at features used in previous work on Chinese EC detection, most notably (Yang and Xue, 2010), which has also adopted a classification-based approach, but because we frame our classification task very differently, we have to use very different features. However, there is a subset of features we used here that has at least a partial overlap with their features, and such features are clearly indicated with ∗."
  }, {
    "heading": "3.1 Horizontal features",
    "text": "The first group of features we use can be described as horizontal features that exploit lexical context of the head (h), the word token following the EC (t),\nand the word token before the EC (p) . These include different combinations of h, t and p, as well as their parts-of-speech. They also include various linear distance features between h and t. Below is the full list of lexical features:\n1. ∗The token string representation of h, t and p, as well as their part-of-speech tag (POS). 2. ∗The POS combination of h and t, the POS combination of t and p.\n3. The normalized word distance between h and t, with the values of this feature being same, immediately before, immediately after, near before, and near after, and other.\n4. The verb distance between h and t, defined as the number of verbs that occur between h and t.\n5. The comma distance between h and t, defined as the number of commas that occur between h and t."
  }, {
    "heading": "3.2 Vertical features",
    "text": "Vertical features are designed to exploit the hierarchical structure of the syntactic tree. Our hierarchical features are based on the following observations. An empty category is always located between its left frontier and right frontier, anchored by t and p. Given the lowest common ancestor A of p and t, the right frontier is the path from t to A and the left frontier is the path from the p to A. We also define a path feature from h to t, which constrains the distance between the EC and its head, just as it constrains the distance between a predicate and its argument in the semantic role labeling task (Gildea and Jurafsky, 2002). Given the lowest common ancestor A′ of h and t, the path from h to t is the path from h to A′ and from A′ to t.\nIn Figure 3, assuming that t is 迅速 (“rapidly”) and h is 崛起 (“take off”), the vertical features ex-\ntracted include:\n1. The string representation of the right frontier, AD↑ADVP↑VP↑IP↑VP\n2. The path from the head t to h, AD↑ADVP↑VP↓VP↓VV\n3. The path from the head h to A, VV↑VP↑VP↑IP↑VP. Notice there is not always a path from h to A.\nThe vertical features are really a condensed representation of a certain syntactic configuration that helps to predict the presence or absence of an empty category as well as the empty category type. For example, the right frontier of *PRO* in Figure 3 AD↑ADVP↑VP↑IP↑VP represents a subjectless IP. Had there been an overt subject in the place of the *PRO*, the right frontier would have been AD↑ADVP↑VP↑IP. Therefore, the vertical features are discriminative features that can help detect the presence or absence of an empty category."
  }, {
    "heading": "3.3 Targeted grammatical constructions",
    "text": "The third group of features target specific, linguistically motivated grammatical constructions. The majority of features in this group hinge on the immediate IP (roughly corresponds to S in the PTB) ancestor of t headed by h. These features are only invoked when t starts (or is on the left edge of) the immediate IP ancestor, and they are designed to capture the context in which the IP ancestor is located. This context can provide discriminative clues that may help identify the types of empty category. For example, both *pro*s and *PRO*s tend to occur in the subject position of an IP, but the larger context of the\nIP often determines the exact empty category type. In Figure 3, the IP that has a *PRO* subject is the complement of a verb in a canonical object-control construction. An IP can also be a sentential subject, the complement of a preposition or a localizer (also called postposition in the literature), or the complement in a CP (roughly SBAR in the PTB), etc. These different contexts tend to be associated with different types of empty categories. The full list of features that exploit these contexts include:\n1. ∗Whether t starts an IP 2. ∗Whether t starts a subjectless IP 3. The left sisters of the immediate IP parent that\nt starts 4. The right sisters of the immediate IP parent that\nt starts 5. The string representation of the governing verb\nof the immediate IP parent that t starts 6. Whether the IP started by t is the complement\nof a localizer phrase 7. Whether the immediate IP parent that t starts is\na sentential subject"
  }, {
    "heading": "3.4 Head information",
    "text": "Most ECs have a verb as its head, but when there is a coordination VP structure where more than one VP share an EC subject, only one such verb can be the head of this EC. The phrase structure to dependency structure conversion tool designates the first verb as the head of the coordinated VP and thus the head of the EC subject in the dependency structure. Other verbs have no chance of being the head. We use a VP head feature to capture this information. It is a binary feature indicating whether a verb can be a head."
  }, {
    "heading": "3.5 Transitivity features",
    "text": "A transitivity lexicon has been extracted from the Chinese Treebank and it is used to determine the transitivity value of a word. A word can be transitive, intransitive, or unknown if it is not a verb. Ditransitive verbs are small in number and are folded into transitive verbs. Transitivity features are defined on h and constrained by word distance: it is only used when h immediately precedes t. This feature category is intended to capture transitive verbs that are missing an object."
  }, {
    "heading": "3.6 Semantic role features",
    "text": "There are apparent connections between semantic role labeling and EC detection. The task of semantic role labeling is typically defined as one of detecting and classifying arguments for verbal or nominal predicates, with more work done so far on verbal than nominal predicates. Although empty categories are annotated as arguments to verbal predicates in linguistic resources such as the English (Palmer et al., 2005) and Chinese (Xue and Palmer, 2009) Propbanks, they are often left out in semantic role labeling systems trained on these resources. This is because the best performing semantic role labeling systems rely on syntactic features extracted from automatic parses (Gildea and Palmer, 2002; Punyakanok et al., 2005) and the parsers that produce them do not generally reproduce empty categories. As a result, current semantic role labeling systems can only recover explicit arguments. However, assuming that all the explicit arguments to a predicate are detected and classified, one can infer the empty arguments of a predicate from its explicit arguments, given a list of expected arguments for the predicate. The list of expected arguments can be found in the “frame files” that are used to guide probank annotation. We defined a semantic role feature category on h when it is a verb and the value of this feature is the semantic role labels for the EC arguments. Like transitivity features, this feature category is also constrained by word distance. It is only used when h immediately precedes t.\nTo extract semantic role features, we retrained a Chinese semantic role labeling system on the Chinese Propbank. We divided the Chinese Propbank data into 10 different subsets, and automatically assigned semantic roles to each subset with a system trained on the other nine subsets. Using the frame files for the Chinese Propbank, we are able to infer the semantic roles for the missing arguments and use them as features."
  }, {
    "heading": "4 Experimental Results",
    "text": ""
  }, {
    "heading": "4.1 Experimental setup",
    "text": "Our EC detection models are trained and evaluated on a subset of the Chinese TreeBank 6.0. The training/development/test data split in our experiments is recommended in the CTB documentation. The\nCTB file IDs for training, development and testing are listed in Table 1. The development data is used for feature selection and tuning, and results are reported on the test set.\nAs discussed in Section 2, the gold standard dependency structure parses are converted from the CTB parse trees, with the ECs preserved. From these gold standard parse trees, we extract triples of (e, h, t) where e is the EC type, h is (the position of) the head of the EC, and t is (the position of) the word token following the EC. During the training phrase, features are extracted from automatic phrase structure parses and paired with these triples. The automatic phrase structure parses are produced by the the Berkeley parser2 with a 10-fold cross-validation, which each fold parsed using a model trained on the other nine folds. Measured by the ParsEval metric (Black et al., 1991), the parsing accuracy on the CTB test set stands at 83.63% (F-score), with a precision of 85.66% and a recall of 81.69%. We chose to train a Maximum Entropy classifier using the Mallet toolkit3 (McCallum, 2002) to detect ECs."
  }, {
    "heading": "4.2 Evaluation metric",
    "text": "We use standard metrics of precision, recall and Fmeasure in our evaluation. In a dependency structure representation, evaluation is very straightforward because individual arcs from the dependency tree can be easily decomposed. An EC is considered to be correctly detected if it is attached to the correct head h, correctly positioned relative to t, and correctly typed. This is a more stringent measure than metrics proposed in previous work, which evaluates EC detection based on its position and type without considering the head it is a dependent of."
  }, {
    "heading": "4.3 Results",
    "text": "There are 1,838 total EC instances in the test set, and if we follow (Yang and Xue, 2010) and collapse all\n2http://code.google.com/p/berkeleyparser 3http://mallet.cs.umass.edu\nconsecutive ECs before the same word token to one, we will end up with a total EC count of 1,352, and this is also the EC count used by (Cai et al., 2011) in their evaluation. On the dependency-based representation adopted here, after collapsing all consecutive ECs before the same word token AND attached to the same head to one, we end up with a total EC count of 1,765. The distribution of the ECs in the test set are presented in Table 2, with the EC count per type from (Yang and Xue, 2010) in parenthesis if it is different. The number of *OP*s, in particular, has increased dramatically from 134 to 527, and this is because a null relative pronoun (*OP*) immediately followed by a trace (*T*) in the subject position of a relative clause is a very common pattern in the Chinese Treebank, as illustrated in Figure 2. In (Yang and Xue, 2010), the *OP*-*T* sequences are collapsed into one, and only the *T*s are counted. That leads to the much smaller count of *OP*s.\nOur results are shown in Table 3. These results are achieved by using the full feature set presented in Section 3. The overall accuracy by F1-measure is 0.574 if we assume there can only be one EC associated with a given (h, t) tuple and hence the total EC count in the gold standard is 1,765, or 0.561 if we factor in all the EC instances and use the higher total count of 1,838, which lowers the recall. If instead we use the total EC count of 1,352 that was used in previous work (Yang and Xue, 2010; Cai et al., 2011), then the F1-measure is 0.660 because the lower total count greatly improves the recall. This is a significant improvement over the best previous result reported by Cai et al (2011), which is an F1 measure of 0.586 on the same test set but based on a less stringent metric of just comparing the EC position and type, without considering whether the EC is attached to the correct head.\nThere are several observations worth noting from these results. One is that our method performs particularly well on null relative pronouns (*OP*) and\ntraces (*T*), indicating that our features are effective in capturing information from relative clause constructions. This accounts for most of the gain compared with previous approaches. The *OP* category, in particular, benefits most from the dependency representation because it is collapsed to the immediately following *T* in previous approaches and does not even get a chance to be detected. On the other hand, our model did poorly on dropped pronouns (*pro*). One possible explanation is that *pro*s generally occupy subject positions in a sentence and is attached as an immediate child of an IP, which is the top-level structure of a sentence that an automatic parser tends to get wrong. Unlike *PRO*, it is not constrained to well-defined grammatical constructions such as subject- and objectcontrol structures.\nTo evaluate the effectiveness of our features, we also did an ablation study on the contribution of different feature groups. The most effective features are the ones when taken out lead to the most drop in accuracy. As should be clear from Table 4, the most effective features are the horizontal features, followed by vertical structures. Features extracted from targeted grammatical constructions and features representing whether h is the head of a coordinated VP lead to modest improvement. Transitivity and semantic role features make virtually no difference at all. We believe it is premature to conclude that they are not useful. Possible explanations for their lack of effectiveness is that they are used in very limited context and the accuracy of the semantic role label-\ning system is not sufficient to make a difference."
  }, {
    "heading": "5 Related Work",
    "text": "The work reported here follows a fruitful line of research on EC detection and resolution, mostly in English. Empty categories have initially been left behind in research on syntactic parsing (Collins, 1999; Charniak, 2001) for efficiency reasons, but more recent work has shown that EC detection can be effectively integrated into the parsing process (Schmid, 2006; Cai et al., 2011). In the meantime, both pre-processing and post-processing approaches have been explored in previous work as alternatives. Johnson (2002) has showed that empty categories can be added to the skeletal parses with reasonable accuracy with a simple pattern-matching algorithm in a postprocessing step. Dienes and Dubey (2003b; 2003a) achieved generally superior accuracy using a machine learning framework without having to refer to the syntactic structure in the skeletal parses. They described their approach as a pre-processing step for parsing because they only use as features morphosyntactic clues (passives, gerunds and to-infinitives) that can be found in certain function words and partof-speech tags. Even better results, however, were obtained by Campbell (2004) in a postprocessing step that makes use of rules inspired by work in theoretical linguistics. Gabbard et al (2006) reported further improvement largely by recasting the Campbell rules as features to seven different machine learning classifiers.\nWe adopted a machine-learning based postprocessing approach based on insights gained from prior work in English and on Chinese-specific considerations. All things being equal, we believe that a machine learning approach that can exploit partial\ninformation is more likely to succeed than deterministic rules that have to make reference to morphosyntactic clues such as to-infinitives and gerunds that are largely non-existent in Chinese. Without these clues, we believe a preprocessing approach that does not take advantage of skeletal parses is unlikely to succeed either. The work we report here also builds on emerging work in Chinese EC detection. Yang and Xue (2010) reported work on detecting just the presence and absence of empty categories without further classifying them. Chung and Gildea (2010) reported work on just detecting just a small subset of the empty categories posited in the Chinese TreeBank. Kong and Zhou (2010) worked on Chinese zero anaphora resolution, where empty category detection is a subtask. More recently, Cai et al (2011) has successfully integrated EC detection into phrasestructure based syntactic parsing and reported stateof-the-art results in both English and Chinese."
  }, {
    "heading": "6 Conclusions and Future Work",
    "text": "We described a novel approach to detecting empty categories (EC) represented in dependency trees and a new metric for measuring EC detection accuracy. The new metric takes into account not only the position and type of an EC, but also the head it is a dependent of in a dependency structure. We also proposed new features that are more suited for this new approach. Tested on a subset of the Chinese Treebank, we show that our system improved significantly over the best previously reported results despite using a more stringent evaluation metric, with most of the gain coming from an improved representation. In the future, we intend to work toward resolving ECs to their antecedents when EC detection can be done with adequate accuracy. We also plan to test our approach on the Penn (English) Treebank, with the first step being converting the Penn Treebank to a dependency representation with the ECs preserved."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported by the National Science Foundation via Grant No. 0910532 entitled“Richer Representations for Machine Translation”. All views expressed in this paper are those of the authors and do not necessarily represent the\nview of the National Science Foundation."
  }],
  "year": 2013,
  "references": [{
    "title": "A procedure for quantitively comparing the syntactic coverage",
    "authors": ["E. Black", "S. Abney", "D. Flickinger", "C. Gdaniec", "R. Grishman", "P. Harrison", "D. Hindle", "R. Ingria", "F. Jelinek", "J. Klavans", "M. Liberman", "M. Marcus", "S. Roukos", "B. Santorini", "T. Strzalkowski"],
    "year": 1991
  }, {
    "title": "Language-independent parsing with empty elements",
    "authors": ["Shu Cai", "David Chiang", "Yoav Goldberg."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 212–216, Portland, Ore-",
    "year": 2011
  }, {
    "title": "Using linguistic principles to recover empty categories",
    "authors": ["Richard Campbell."],
    "venue": "Proceedings of the 42nd Annual Meeting on Association For Computational Linguistics.",
    "year": 2004
  }, {
    "title": "Immediate-head Parsing for Language Models",
    "authors": ["E. Charniak."],
    "venue": "ACL-01.",
    "year": 2001
  }, {
    "title": "Effects of empty categories on machine translation",
    "authors": ["Tagyoung Chung", "Daniel Gildea."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645, Cambridge, MA.",
    "year": 2010
  }, {
    "title": "Head-driven Statistical Models for Natural Language Parsing",
    "authors": ["Michael Collins."],
    "venue": "Ph.D. thesis, University of Pennsylvania.",
    "year": 1999
  }, {
    "title": "Antecendant Recovery: Experiments with a Trace Tagger",
    "authors": ["Péter Dienes", "Amit Dubey."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing , Sapporo, Japan.",
    "year": 2003
  }, {
    "title": "Deep syntactic processing by combining shallow methods",
    "authors": ["Péter Dienes", "Amit Dubey."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan.",
    "year": 2003
  }, {
    "title": "Fully parsing the penn treebank",
    "authors": ["Ryan Gabbard", "Seth Kulick", "Mitchell Marcus."],
    "venue": "Proceedings of HLT-NAACL 2006, pages 184–191, New York City.",
    "year": 2006
  }, {
    "title": "Automatic labeling for semantic roles",
    "authors": ["D. Gildea", "D. Jurafsky."],
    "venue": "Computational Linguistics, 28(3):245–288.",
    "year": 2002
  }, {
    "title": "The Necessity of Parsing for Predicate Argument Recognition",
    "authors": ["Dan Gildea", "Martha Palmer."],
    "venue": "Proceedings of the 40th Meeting of the Association for Computational Linguistics, Philadelphia, PA.",
    "year": 2002
  }, {
    "title": "The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages",
    "authors": ["Màrquez", "Adam Meyers", "Joakim Nivre", "Sebastian Padó", "Jan Štěpánek", "Pavel Straňák", "Mihai Surdeanu", "Nianwen Xue", "Yi Zhang."],
    "venue": "Proceedings of",
    "year": 2009
  }, {
    "title": "On the distribution and reference of empty pronouns",
    "authors": ["James C.T. Huang."],
    "venue": "Linguistics Inquiry, 15:531– 574.",
    "year": 1984
  }, {
    "title": "A simple pattern-matching algorithm for recovering empty nodes and their antecedents",
    "authors": ["Mark Johnson."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.",
    "year": 2002
  }, {
    "title": "A Tree Kernelbased unified framework for Chinese zero anaphora resolution",
    "authors": ["Fang Kong", "Guodong Zhou."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, MIT, Massachusetts.",
    "year": 2010
  }, {
    "title": "Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation",
    "authors": ["Roger Levy", "Christopher Manning."],
    "venue": "Proceedings of the ACL.",
    "year": 2004
  }, {
    "title": "Building a Large Annotated Corpus of English: the Penn Treebank",
    "authors": ["M. Marcus", "B. Santorini", "M.A. Marcinkiewicz."],
    "venue": "Computational Linguistics.",
    "year": 1993
  }, {
    "title": "Mallet: A machine learning for language toolkit",
    "authors": ["Andrew Kachites McCallum."],
    "venue": "http://mallet.cs.umass.edu.",
    "year": 2002
  }, {
    "title": "The Proposition Bank: An annotated corpus of semantic roles",
    "authors": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."],
    "venue": "Computational Linguistics, 31(1):71– 106.",
    "year": 2005
  }, {
    "title": "The Necessity of Syntactic Parsing for Semantic Role Labeling",
    "authors": ["Vasin Punyakanok", "Dan Roth", "W. Yih."],
    "venue": "Proceedings of IJCAI-2005, pages 1124– 1129, Edinburgh, UK.",
    "year": 2005
  }, {
    "title": "Trace prediction and recovery with unlexicalized PCFGs and slash features",
    "authors": ["Helmut Schmid."],
    "venue": "Proc of ACL.",
    "year": 2006
  }, {
    "title": "Adding semantic roles to the Chinese Treebank",
    "authors": ["Nianwen Xue", "Martha Palmer."],
    "venue": "Natural Language Engineering, 15(1):143–172.",
    "year": 2009
  }, {
    "title": "The Bracketing Guidelines for Penn Chinese Treebank Project",
    "authors": ["Nianwen Xue", "Fei Xia."],
    "venue": "Technical Report IRCS 00-08, University of Pennsylvania.",
    "year": 2000
  }, {
    "title": "The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus",
    "authors": ["Nianwen Xue", "Fei Xia", "Fu dong Chiou", "Martha Palmer."],
    "venue": "Natural Language Engineering, 11(2):207–238.",
    "year": 2005
  }, {
    "title": "Chasing the Ghost: Recovering Empty Categories in the Chinese",
    "authors": ["Yaqin Yang", "Nianwen Xue"],
    "year": 2010
  }],
  "id": "SP:4101e922b788c4d60560b46d3cd9903f9559a81d",
  "authors": [{
    "name": "Nianwen Xue",
    "affiliations": []
  }, {
    "name": "Yaqin Yang",
    "affiliations": []
  }],
  "abstractText": "We describe a novel approach to detecting empty categories (EC) as represented in dependency trees as well as a new metric for measuring EC detection accuracy. The new metric takes into account not only the position and type of an EC, but also the head it is a dependent of in a dependency tree. We also introduce a variety of new features that are more suited for this approach. Tested on a subset of the Chinese Treebank, our system improved significantly over the best previously reported results even when evaluated with this more stringent metric.",
  "title": "Dependency-based empty category detection via phrase structure trees"
}