{
  "sections": [{
    "text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2755–2768, Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics\n2755"
  }, {
    "heading": "1 Introduction",
    "text": "For more than a decade, research on data-driven dependency parsing has been dominated by two approaches: transition-based parsing and graphbased parsing (McDonald and Nivre, 2007, 2011). Transition-based parsing reduces the parsing task to scoring single parse actions and is often combined with local optimization and greedy search algorithms. Graph-based parsing decomposes parse trees into subgraphs and relies on global optimization and exhaustive (or at least non-greedy)\n∗We gratefully acknowledge the inspiration for our subtitle in the seminal paper by Zhang and Clark (2008).\nsearch to find the best tree. These radically different approaches often lead to comparable parsing accuracy, but with distinct error profiles indicative of their respective strengths and weaknesses, as shown by McDonald and Nivre (2007, 2011).\nIn recent years, dependency parsing, like most of NLP, has shifted from linear models and discrete features to neural networks and continuous representations. This has led to substantial accuracy improvements for both transition-based and graph-based parsers and raises the question whether their complementary strengths and weaknesses are still relevant. In this paper, we replicate the analysis of McDonald and Nivre (2007, 2011) for neural parsers. In addition, we investigate the impact of deep contextualized word representations (Peters et al., 2018; Devlin et al., 2019) for both types of parsers.\nBased on what we know about the strengths and weaknesses of the two approaches, we hypothesize that deep contextualized word representations will benefit transition-based parsing more than graph-based parsing. The reason is that these representations make information about global sentence structure available locally, thereby helping to prevent search errors in greedy transition-based parsing. The hypothesis is corroborated in experiments on 13 languages, and the error analysis supports our suggested explanation. We also find that deep contextualized word representations improve parsing accuracy for longer sentences, both for transition-based and graph-based parsers."
  }, {
    "heading": "2 Two Models of Dependency Parsing",
    "text": "After playing a marginal role in NLP for many years, dependency-based approaches to syntactic parsing have become mainstream during the last fifteen years. This is especially true if we consider languages other than English, ever since the influ-\n2756\nential CoNLL shared tasks on dependency parsing in 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007) with data from 19 languages.\nThe transition-based approach to dependency parsing was pioneered by Yamada and Matsumoto (2003) and Nivre (2003), with inspiration from history-based parsing (Black et al., 1992) and data-driven shift-reduce parsing (Veenstra and Daelemans, 2000). The idea is to reduce the complex parsing task to the simpler task of predicting the next parsing action and to implement parsing as greedy search for the optimal sequence of actions, guided by a simple classifier trained on local parser configurations. This produces parsers that are very efficient, often with linear time complexity, and which can benefit from rich non-local features defined over parser configurations but which may suffer from compounding search errors.\nThe graph-based approach to dependency parsing was developed by McDonald et al. (2005a,b), building on earlier work by Eisner (1996). The idea is to score dependency trees by a linear combination of scores of local subgraphs, often single arcs, and to implement parsing as exact search for the highest scoring tree under a globally optimized model. These parsers do not suffer from search errors but parsing algorithms are more complex and restrict the scope of features to local subgraphs.\nThe terms transition-based and graph-based were coined by McDonald and Nivre (2007, 2011), who performed a contrastive error analysis of the two top-performing systems in the CoNLL 2006 shared task on multilingual dependency parsing: MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2006), which represented the state of the art in transition-based and graph-based parsing, respectively, at the time. Their analysis shows that, despite having almost exactly the same parsing accuracy when averaged over 13 languages, the two parsers have very distinctive error profiles. MaltParser is more accurate on short sentences, on short dependencies, on dependencies near the leaves of the tree, on nouns and prounouns, and on subject and object relations. MSTParser is more accurate on long sentences, on long dependencies, on dependencies near the root of the tree, on verbs, and on coordination relations and sentence roots.\nMcDonald and Nivre (2007, 2011) argue that these patterns can be explained by the complementary strengths and weaknesses of the systems. The\n0 2 4 6 8 10 12 14 Dependency length\n0.5\n0.6\n0.7\n0.8\n0.9\nD ep\nen de\nnc y\npr ec\nis io\nn\nMSTParser MaltParser ZPar\n2 4 6 8 10 12 14 Dependency length\n0.5\n0.6\n0.7\n0.8\n0.9\nD ep\nen de\nnc y\nre ca\nll\nMSTParser MaltParser ZPar\nFigure 3: Dependency arc precision/recall relative to predicted/gold dependency length.\n1 2 3 4 5 6 7 Distance to root\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\nD ep\nen de\nnc y\npr ec\nis io\nn\nMSTParser MaltParser ZPar\n1 2 3 4 5 6 7 Distance to root\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nD ep\nen de\nnc y\nre ca\nll\nMSTParser MaltParser ZPar\nFigure 4: Dependency arc precision/recall relative to predicted/gold distance to root.\nZPar performs better than MaltParser and MSTParser, particularly on short sentences ( 30), due to the richest feature representation. For longer sentences (20 to 50), the performance of ZPar drops as quickly as that of MaltParser. One possible reason is that the effect of a fixedsize beam on the reduction of error propagation becomes less obvious when the number of possible parse trees grows exponentially with sentence size. The performance of MSTParser decreases less quickly as the size of the sentence increases, demonstrating the advantage of exact inference. Sentences with 50+ words are relatively rare in the test set.\nThe three parsers show larger variance in performance when evaluated against specific properties of the dependency tree. Figure 3 shows the precision and recall for each parser relative to the arc lengths in the predicted and gold-standard dependency trees. Here the length of an arc is defined as the absolute difference between the indices of the head and modifier. Precision represents the percentage of predicted arcs with a particular length that are correct, and recall represents the percentage of gold arcs of a particular length that are correctly predicted.\nMaltParser gives higher precision than MSTParser for short dependency arcs ( 4), but its precision drops rapidly for arcs with increased lengths. These arcs take more shift-reduce actions to build, and are hence more prone to error propagation. The precision of ZPar drops much slower compared to MaltParser, demonstrating the effect of beam-search for the reduction of error propagation. Another important factor is the use of rich non-local features by ZPar, which is a likely reason for its precision to drop slower even than that of MSTParser when the arc size increases from 1 to 8. Interestingly, the precision of ZPar is almost indistinguishable from that of MaltParser for size 1 arcs (arcs between neighbouring words), showing that the wider range of features in ZPar is the most helpful in arcs that take more than one, but not too many shiftreduce actions to build. The recall curves of the three parsers are similar, with ZPar having\nFigure 1: Labeled precision by dependency length fo MST (global–exhaustive–graph), Malt (local–greedy– transition) and ZPar (global–beam–transition). From Zhang and Nivre (2012).\ntransition-based MaltParser prioritizes rich structural features, which enable accurate disambiguation in local contexts, but is limited by a locally optimized model and greedy algorithm, resulting in search errors for structures that require longer transition sequences. The graph-based MSTParser benefits from a globally optimized model and ex-\nact inference, which gives a better analysis of\nglobal sentence structure, but is more restricted in the features it can use, which limits its capacity to score local structures accurately.\nMany of the developments in dependency parsing during the last decade can be understood in this light as attempts to mitigate the weaknesses of traditional transition-based and graph-based parsers without sacrificing their strengths. This may mean evolving the model structure through new transition systems (Nivre, 2008, 2009; Kuhlmann et al., 2011) or higher-order models for graphbased parsing (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010); it may mean exploring alternative learning strategies, in particular for transition-based parsing, where improvements have been achieved thanks to global structure learning (Zhang and Clark, 2008; Zhang and Nivre, 2011; Andor et al., 2016) and dynamic oracles (Goldberg and Nivre, 2012, 2013); it may mean using alternative search strategies, such as transition-based parsing with beam search (Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008) or exact search (Huang and Sagae, 2010; Kuhlmann et al., 2011) or graph-based parsing with heuristic search to cope with the complexity of higher-order models, especially for non-projective parsing (McDonald and Pereira, 2006; Koo et al., 2010; Zhang and McDonald, 2012); or it may mean hybrid or en-\nsemble systems (Sagae and Lavie, 2006; Nivre and McDonald, 2008; Zhang and Clark, 2008; Bohnet and Kuhn, 2012). A nice illustration of the impact of new techniques can be found in Zhang and Nivre (2012), where an error analysis along the lines of McDonald and Nivre (2007, 2011) shows that a transition-based parser using global learning and beam search (instead of local learning and greedy search) performs on par with graph-based parsers for long dependencies, while retaining the advantage of the original transition-based parsers on short dependencies (see Figure 1).\nNeural networks for dependency parsing, first explored by Titov and Henderson (2007) and Attardi et al. (2009), have come to dominate the field during the last five years. While this has dramatically changed learning architectures and feature representations, most parsing models are still either transition-based (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016) or graph-based (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). However, more accurate feature learning using continuous representations and nonlinear models has allowed parsing architectures to be simplified. Thus, most recent transition-based parsers have moved back to local learning and greedy inference, seemingly without losing accurracy (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). Similarly, graph-based parsers again rely on first-order models and obtain no improvements from using higher-order models (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).\nThe increasing use of neural networks has also led to a convergence in feature representations and learning algorithms for transition-based and graph-based parsers. In particular, most recent systems rely on an encoder, typically in the form of a BiLSTM, that provides contextualized representations of the input words as input to the scoring of transitions – in transition-based parsers – or of dependency arcs – in graph-based parsers. By making information about the global sentence context available in local word representations, this encoder can be assumed to mitigate error propagation for transition-based parsers and to widen the feature scope beyond individual word pairs for graph-based parsers. For both types of parsers, this also obviates the need for complex structural feature templates, as recently shown by\nFalenska and Kuhn (2019). We should therefore expect neural transition-based and graph-based parsers to be not only more accurate than their non-neural counterparts but also more similar to each other in their error profiles."
  }, {
    "heading": "3 Deep Contextualized Word Representations",
    "text": "Neural parsers rely on vector representations of words as their primary input, often in the form of pretrained word embeddings such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), or fastText (Bojanowski et al., 2016), which are sometimes extended with characterbased representations produced by recurrent neural networks (Ballesteros et al., 2015). These techniques assign a single static representation to each word type and therefore cannot capture contextdependent variation in meaning and syntactic behavior.\nBy contrast, deep contextualized word representations encode words with respect to the sentential context in which they appear. Like word embeddings, such models are typically trained with a language-modeling objective, but yield sentence-level tensors as representations, instead of single vectors. These representations are typically produced by transferring a model’s entire feature encoder – be it a BiLSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017) – to a target task, where the dimensionality of the tensor S is typically S ∈ RN×L×D for a sentence of length N , an encoder with L layers, and word-level vectors of dimensionality D. The advantage of such models, compared to the parser-internal encoders discussed in the previous section, is that they not only produce contextualized representations but do so over several layers of abstraction, as captured by the model’s different layers, and are pre-trained on corpora much larger than typical treebanks.\nDeep contextualized embedding models have proven to be adept at a wide array of NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2019). Though many such models have been proposed, we adopt the two arguably most popular ones for our experiments: ELMo and BERT. Both models have previously been used for dependency parsing (Che et al., 2018; Jawahar et al., 2018; Lim et al., 2018;\nKondratyuk, 2019; Schuster et al., 2019), but there has been no systematic analysis of their impact on transition-based and graph-based parsers."
  }, {
    "heading": "3.1 ELMo",
    "text": "ELMo is a deep contextualized embedding model proposed by Peters et al. (2018), which produces sentence-level representations yielded by a multi-layer BiLSTM language model. ELMo is trained with a standard language-modeling objective, in which a BiLSTM reads a sequence of N learned context-independent embeddings w1, . . . ,wN (obtained via a character-level CNN) and produces a context-dependent representation hj,k = BiLSTM(w1:N , k), where j (1≤ j≤L) is the BiLSTM layer and k is the index of the word in the sequence. The output of the last layer hL,k is then employed in conjunction with a softmax layer to predict the next token at k + 1.\nThe simplest way of transferring ELMo to a downstream task is to encode the input sentence S = w1, . . . , wN by extracting the representations from the BiLSTM at layer L for each token wk ∈ S: hL,1, . . . ,hL,N ,. However, Peters et al. (2018) posit that the best way to take advantage of ELMo’s representational power is to compute a linear combination of BiLSTM layers:\nELMok = γ L∑\nj=0\nsjhj,k (1)\nwhere sj is a softmax-normalized task-specific parameter and γ is a task-specific scalar. Peters et al. (2018) demonstrate that this scales the layers of linguistic abstraction encoded by the BiLSTM for the task at hand."
  }, {
    "heading": "3.2 BERT",
    "text": "BERT (Devlin et al., 2019) is similar to ELMo in that it employs a language-modeling objective over unannotated text in order to produce deep contextualized embeddings. However, BERT differs from ELMo in that, in place of a BiLSTM, it employs a bidirectional Transformer (Vaswani et al., 2017), which, among other factors, carries the benefit of learning potential dependencies between words directly. This lies in contrast to recurrent models, which may struggle to learn correspondences between constituent signals when the time-lag between them is long (Hochreiter et al., 2001). For a token wk in sentence S, BERT’s input representation is composed by summing a\nword embedding xk, a position embedding ik, and a WordPiece embedding sk (Wu et al., 2016): wk = xk + ik + sk.\nEach wk ∈ S is passed to an L-layered BiTransformer, which is trained with a masked language modeling objective (i.e., randomly masking a percentage of input tokens and only predicting said tokens). For use in downstream tasks, Devlin et al. (2019) propose to extract the Transformer’s encoding of each token wk ∈ S at layer L, which effectively produces BERTk."
  }, {
    "heading": "4 Hypotheses",
    "text": "Based on our discussion in Section 2, we assume that transition-based and graph-based parsers still have distinctive error profiles due to the basic trade-off between rich structural features, which allow transition-based parsers to make accurate local decisions, and global learning and exact search, which give graph-based parsers an advantage with respect to global sentence structure. At the same time, we expect the differences to be less pronounced than they were ten years ago because of the convergence in neural architectures and feature representations. But how will the addition of deep contextualized word representations affect the behavior of the two parsers?\nGiven recent recent work showing that deep contextualized word representations incorporate rich information about syntactic structure (Goldberg, 2019; Liu et al., 2019; Tenney et al., 2019; Hewitt and Manning, 2019), we hypothesize that transition-based parsers have most to gain from these representations because it will improve their capacity to make decisions informed by global sentence structure and therefore reduce the number of search errors. Our main hypothesis can be stated as follows:\nDeep contextualized word representations are more effective at reducing errors in transitionbased parsing than in graph-based parsing.\nIf this holds true, then the analysis of McDonald and Nivre (2007, 2011) suggests that the differential error reduction should be especially visible on phenomena such as:\n1. longer dependencies, 2. dependencies closer to the root, 3. certain parts of speech, 4. certain dependency relations, 5. longer sentences.\nThe error analysis will consider all these factors as well as non-projective dependencies."
  }, {
    "heading": "5 Experimental Setup",
    "text": ""
  }, {
    "heading": "5.1 Parsing Architecture",
    "text": "To be able to compare transition-based and graphbased parsers under equivalent conditions, we use and extend UUParser1 (de Lhoneux et al., 2017a; Smith et al., 2018a), an evolution of bistparser (Kiperwasser and Goldberg, 2016), which supports transition-based and graph-based parsing with a common infrastructure but different scoring models and parsing algorithms.\nFor an input sentence S = w1, . . . , wN , the parser creates a sequence of vectors w1:N , where the vector wk = xk ◦ BILSTM(c1:M ) representing input word wk is the concatenation of a pretrained word embedding xk and a character-based embedding BILSTM(c1:M ) obtained by running a BiLSTM over the character sequence c1:M of wk. Finally, each input element is represented by a BiLSTM vector, hk = BILSTM(w1:N , k).\nIn transition-based parsing, the BiLSTM vectors are input to a multi-layer perceptron (MLP) for scoring transitions, using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a SWAP transition to allow the construction of non-projective dependency trees (Nivre, 2009; de Lhoneux et al., 2017b). The scoring is based on the top three words on the stack and the first word of the buffer, and the input to the MLP includes the BiLSTM vectors for these words as well as their leftmost and rightmost dependents (up to 12 words in total).\nIn graph-based parsing, the BiLSTM vectors are input to an MLP for scoring all possible dependency relations under an arc-factored model, meaning that only the vectors corresponding to the head and dependent are part of the input (2 words in total). The parser then extracts a maximum spanning tree over the score matrix using the ChuLiu-Edmonds (CLE) algorithm2 (Edmonds, 1967) which allows us to construct non-projective trees.\nIt is important to note that, while we acknowledge the existence of graph-based parsers that outperform the implementation of Kiperwasser and Goldberg (2016), such models do not meet our criteria for systematic comparison. The parser\n1https://github.com/UppsalaNLP/ uuparser\n2We use the implementation from Qi et al. (2018).\nby Dozat et al. (2017) is very similar, but employs the MLP as a further step in the featurization process prior to scoring via a biaffine classifier. To keep the comparison as exact as possible, we forego comparing our transition-based systems to the Dozat et al. (2017) parser (and its numerous modifications). In addition, preliminary experiments showed that our chosen graph-based parser outperforms its transition-based counterpart, which was itself competitive in the CoNLL 2018 shared task (Zeman et al., 2018)."
  }, {
    "heading": "5.2 Input Representations",
    "text": "In our experiments, we evaluate three pairs of systems – differing only in their input representations. The first is a baseline that represents tokens by wk = xk ◦ BILSTM(c1:M ), as described in Section 5.1. The word embeddings xk are initialized via pretrained fastText vectors (xk ∈ R300) (Grave et al., 2018), which are updated for the parsing task. We term these transition-based and graphbased baselines TR and GR.\nFor the ELMo experiments, we make use of pretrained models provided by Che et al. (2018), who train ELMo on 20 million words randomly sampled from raw WikiDump and Common Crawl datasets for 44 languages. We encode each goldsegmented sentence in our treebank via the ELMo model for that language, which yields a tensor SELMo = RN×L×D, where N is the number of words in the sentence, L = 3 is the number of ELMo layers, and D = 1024 is the ELMo vector dimensionality. Following Peters et al. (2018) (see Eq. 1), we learn a linear combination and a task-specific γ of each token’s ELMo representation, which yields a vector ELMok ∈ R1024. We then concatenate this vector with wk and pass it to the BiLSTM. We call the transition-based and graph-based systems enhanced with ELMo TR+E and GR+E.\nFor the BERT experiments, we employ the pretrained multilingual cased model provided by Google,3 4 which is trained on the concatenation of WikiDumps for the top 104 languages with the largest Wikipedias.5 The model’s parameters feature a 12-layer transformer trained with 768 hid-\n3https://github.com/google-research/ bert\n4Except for Chinese, for which we make use of a separate, pretrained model.\n5See sorted list here: https://meta.wikimedia. org/wiki/List_of_Wikipedias\nden units and 12 self-attention heads. In order to obtain a word-level vector for each token in a sentence, we experimented with a variety of representations: namely, concatenating each transformer layer’s word representation into a single vector wconcat ∈ R768∗12, employing the last layer’s representation, or learning a linear combination over a range of layers, as we do with ELMo (e.g., via Eq. 1). In a preliminary set of experiments, we found that the latter approach over layers 4–8 consistently yielded the best results, and thus chose to adopt this method going forward. Regarding tokenization, we select the vector for the first subword token, as produced by the native BERT tokenizer. Surprisingly, this gave us better results than averaging subword token vectors in a preliminary round of experiments. Like with the ELMo representations, we concatenate each BERT vector BERTk ∈ R768 with wk and pass it to the respective TR+B and GR+B parsers.\nIt is important to note that while the ELMo models we work with are monolingual, the BERT model is multilingual. In other words, while the standalone ELMo models were trained on the tokenized WikiDump and CommonCrawl for each language respectively, the BERT model was trained only on the former, albeit simultaneously for 104 languages. This means that the models are not strictly comparable, and it is an interesting question whether either of the models has an advantage in terms of training regime. However, since our purpose is not to compare the two models but to study their impact on parsing, we leave this question for future work."
  }, {
    "heading": "5.3 Language and Treebank Selection",
    "text": "For treebank selection, we rely on the criteria proposed by de Lhoneux et al. (2017c) and adapted by Smith et al. (2018b) to have languages from different language families, with different morphological complexity, different scripts and character set sizes, different training sizes and domains, and with good annotation quality. This gives us 13 treebanks from UD v2.3 (Nivre et al., 2018), information about which is shown in Table 1."
  }, {
    "heading": "5.4 Parser Training and Evaluation",
    "text": "In all experiments, we train parsers with default settings6 for 30 epochs and select the model with\n6All hyperparameters are specified in the supplementary material (Part A).\nthe best labeled attachment score on the dev set. For each combination of model and training set, we repeat this procedure three times with different random seeds, apply the three selected models to the test set, and report the average result."
  }, {
    "heading": "5.5 Error Analysis",
    "text": "In order to conduct an error analysis along the lines of McDonald and Nivre (2007, 2011), we extract all sentences from the smallest development set in our treebank sample (Hebrew HTB, 484 sentences) and sample the same number of sentences from each of the other development sets (6,292 sentences in total). For each system, we then extract parses of these sentences for the three training runs with different random seeds (18,876 predictions in total). Although it could be interesting to look at each language separately, we follow McDonald and Nivre (2007, 2011) and base our main analysis on all languages together to prevent data sparsity for longer dependencies, longer sentences, etc.7"
  }, {
    "heading": "6 Results and Discussion",
    "text": "Table 2 shows labeled attachment scores for the six parsers on all languages, averaged over three training runs with random seeds. The results clearly corroborate our main hypothesis. While ELMo and BERT provide significant improvements for both transition-based and graph-based\n7The supplementary material contains tables for the error analysis (Part B) and graphs for each language (Part C).\nparsers, the magnitude of the improvement is greater in the transition-based case: 3.99 vs. 2.85 for ELMo and 4.47 vs. 3.13 for BERT. In terms of error reduction, this corresponds to 21.1% vs. 16.5% for ELMo and 22.5% vs. 17.4% for BERT. The differences in error reduction are statistically significant at α = 0.01 (Wilcoxon).\nAlthough both parsing accuracy and absolute improvements vary across languages, the overall trend is remarkably consistent and the transitionbased parser improves more with both ELMo and BERT for every single language. Furthermore, a linear mixed effect model analysis reveals that, when accounting for language as a random effect, there are no significant interactions between the improvement of each model (over its respective baseline) and factors such as language family (IE vs. non-IE), dominant word order, or number of training sentences. In other words, the improvements for both parsers seem to be largely independent of treebank-specific factors. Let us now see to what extent they can be explained by the error analysis."
  }, {
    "heading": "6.1 Dependency Length",
    "text": "Figure 2 shows labeled F-score for dependencies of different lengths, where the length of a dependency between words wi and wj is equal to |i− j| (and with root tokens in a special bin on the far left). For the baseline parsers, we see that the curves diverge with increasing length, clearly indicating that the transition-based parser still suffers\nfrom search errors on long dependencies, which require longer transition sequences for their construction. However, the differences are much smaller than in McDonald and Nivre (2007, 2011) and the transition-based parser no longer has an advantage for short dependencies, which is consistent with the BiLSTM architecture providing the parsers with more similar features that help the graph-based parser overcome the limited scope of the first-order model.\nAdding deep contextualized word representations clearly helps the transition-based parser to perform better on longer dependencies. For ELMo there is still a discernible difference for dependencies longer than 5, but for BERT the two curves\nare almost indistinguishable throughout the whole range. This could be related to the aforementioned intuition that a Transformer captures long dependencies more effectively than a BiLSTM (see Tran et al. (2018) for contrary observations, albeit for different tasks). The overall trends for both baseline and enhanced models are quite consistent across languages, although with large variations in accuracy levels."
  }, {
    "heading": "6.2 Distance to Root",
    "text": "Figure 3 reports labeled F-score for dependencies at different distances from the root of the tree, where distance is measured by the number of arcs in the path from the root. There is a fairly strong (inverse) correlation between dependency length and distance to the root, so it is not surprising that the plots in Figure 3 largely show the mirror image of the plots in Figure 2. For the baseline parsers, the graph-based parser has a clear advantage for dependencies near the root (including the root itself), but the transition-based parser closes the gap with increasing distance.8 For ELMo and BERT, the curves are much more similar, with only a slight advantage for the graph-based parser near the root and with the transition-based BERT parser being superior from distance 5 upwards. The main trends are again similar across all languages."
  }, {
    "heading": "6.3 Non-Projective Dependencies",
    "text": "Figure 4 shows precision and recall specifically for non-projective dependencies. We see that there is a clear tendency for the transition-based parser to have better precision and the graph-based parser better recall.9 In other words, non-projective dependencies are more likely to be correct when they are predicted by the transition-based parser using the swap transition, but real non-projective dependencies are more likely to be found by the graphbased parser using a spanning tree algorithm. Interestingly, adding deep contextualized word representations has almost no effect on the graphbased parser,10 while especially the ELMo em-\n8At the very end, the curves appear to diverge again, but the data is very sparse in this part of the plot.\n9Incidentally, the same pattern is reported by McDonald and Nivre (2007, 2011), even though the techniques for processing non-projective dependencies are different in that study: pseudo-projective parsing (Nivre and Nilsson, 2005) for the transition-based parser and approximate second-order non-projective parsing (McDonald and Pereira, 2006) for the graph-based parser.\n10The breakdown per language shows marginal improvements for the enhanced graph-based models on a few lan-\nbeddings improve both precision and recall for the transition-based parser."
  }, {
    "heading": "6.4 Parts of Speech and Dependency Types",
    "text": "Thanks to the cross-linguistically consistent UD annotations, we can relate errors to linguistic categories more systematically than in the old study. The main impression, however, is that there are very few clear differences, which is again indicative of the convergence between the two parsing approaches. We highlight the most notable differences and refer to the supplementary material (Part B) for the full results.\nLooking first at parts of speech, the baseline graph-based parser is slightly more accurate on verbs and nouns than its transition-based counterpart, which is consistent with the old study for verbs but not for nouns. After adding the deep contextualized word representations, both differences are essentially eliminated.\nWith regard to dependency relations, the baseline graph-based parser has better precision and recall than the baseline transition-based parser for the relations of coordination (conj), which is consistent with the old study, as well as clausal subjects (csubj) and clausal complements (ccomp), which are relations that involve verbs in clausal structures. Again, the differences are greatly reduced in the enhanced parsing models, especially for clausal complements, where the transitionbased parser with ELMo representations is even slightly more accurate than the graph-based parser."
  }, {
    "heading": "6.5 Sentence Length",
    "text": "Figure 5 plots labeled attachment score for sentences of different lengths, measured by number of words in bins of 1–10, 11–20, etc. Here we\nguages, canceled out by equally marginal degradations on others.\nfind the most unexpected results of the study. First of all, although the baseline parsers exhibit the familiar pattern of accuracy decreasing with sentence length, it is not the transition-based but the graph-based parser that is more accurate on short sentences and degrades faster. In other words, although the transition-based parser still seems to suffer from search errors, as shown by the results on dependency length and distance to the root, it no longer seems to suffer from error propagation in the sense that earlier errors make later errors more probable. The most likely explanation for this is the improved training for transition-based parsers using dynamic oracles and aggressive exploration to learn how to behave optimally also in non-optimal configurations (Goldberg and Nivre, 2012, 2013; Kiperwasser and Goldberg, 2016).\nTurning to the models with deep contextualized word representations, we find that transitionbased and graph-based parsers behave more similarly, which is in line with our hypotheses. However, the most noteworthy result is that accuracy improves with increasing sentence length. For ELMo this holds only from 1–10 to 11–20, but for BERT it holds up to 21–30, and even sentences of length 31–40 are parsed with higher accuracy than sentences of length 1–10. A closer look at the breakdown per language reveals that this picture is slightly distorted by different sentence length distributions in different languages. More precisely, high-accuracy languages seem to have a higher proportion of sentences of mid-range length, causing a slight boost in the accuracy scores of these bins, and no single language exhibits exactly the patterns shown in Figure 5. Nevertheless, several languages exhibit an increase in accuracy from the first to the second bin or from the second to the third bin for one or more of the enhanced models (especially the BERT models). And almost all languages show a less steep degradation for the enhanced models, clearly indicating that deep contextualized word representations improve the capacity to parse longer sentences."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we have essentially replicated the study of McDonald and Nivre (2007, 2011) for neural parsers. In the baseline setting, where parsers use pre-trained word embeddings and character representations fed through a BiLSTM, we can still discern the basic trade-off identified\nin the old study, with the transition-based parser suffering from search errors leading to lower accuracy on long dependencies and dependencies near the root of the tree. However, important details of the picture have changed. The graph-based parser is now as accurate as the transition-based parser on shorter dependencies and dependencies near the leaves of the tree, thanks to improved representation learning that overcomes the limited feature scope of the first order model. And with respect to sentence length, the pattern has actually been reversed, with the graph-based parser being more accurate on short sentences and the transitionbased parser gradually catching up thanks to new training methods that prevent error propagation.\nWhen adding deep contextualized word representations, the behavior of the two parsers converge even more, and the transition-based parser in particular improves with respect to longer dependencies and dependencies near the root, as a result of fewer search errors thanks to enhanced information about the global sentence structure. One of the most striking results, however, is that both parsers improve their accuracy on longer sentences, with some models for some languages in fact being more accurate on medium-length sentences than on shorter sentences. This is a milestone in parsing research, and more research is needed to explain it.\nIn a broader perspective, we hope that future studies on dependency parsing will take the results obtained here into account and extend them by investigating other parsing approaches and neural network architectures. Indeed, given the rapid development of new representations and architectures, future work should include analyses of how all components in neural parsing architectures (embeddings, encoders, decoders) contribute to distinct error profiles (or lack thereof)."
  }, {
    "heading": "Acknowledgments",
    "text": "We want to thank Ali Basirat, Christian Hardmeier, Jamie Henderson, Ryan McDonald, Paola Merlo, Gongbo Tang, and the EMNLP reviewers and area chairs for valuable feedback on preliminary versions of this paper. We acknowledge the computational resources provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu)."
  }],
  "year": 2019,
  "references": [{
    "title": "Globally normalized transition-based neural networks",
    "authors": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."],
    "venue": "Proceedings of the 54th Annual Meeting of the Asso-",
    "year": 2016
  }, {
    "title": "Accurate dependency parsing with a stacked multilayer perceptron",
    "authors": ["Giuseppe Attardi", "Felice Dell’Orletta", "Maria Simi", "Joseph Turian"],
    "venue": "In Proceedings of EVALITA",
    "year": 2009
  }, {
    "title": "Improved transition-based parsing by modeling characters instead of words with LSTMs",
    "authors": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),",
    "year": 2015
  }, {
    "title": "Towards history-based grammars: Using richer models for probabilistic parsing",
    "authors": ["Ezra Black", "Frederick Jelinek", "John D. Lafferty", "David M. Magerman", "Robert L. Mercer", "Salim Roukos."],
    "venue": "Proceedings of the 5th DARPA Speech and Natural",
    "year": 1992
  }, {
    "title": "The best of both worlds – a graph-based completion model for transition-based parsers",
    "authors": ["Bernd Bohnet", "Jonas Kuhn."],
    "venue": "Proceedings of the 13th Conference of the European Chpater of the Association for Computational Linguistics (EACL), pages",
    "year": 2012
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.04606.",
    "year": 2016
  }, {
    "title": "CoNLL-X shared task on multilingual dependency parsing",
    "authors": ["Sabine Buchholz", "Erwin Marsi."],
    "venue": "Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149–164.",
    "year": 2006
  }, {
    "title": "Experiments with a higherorder projective dependency parser",
    "authors": ["Xavier Carreras."],
    "venue": "Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 957–961.",
    "year": 2007
  }, {
    "title": "Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation",
    "authors": ["Wanxiang Che", "Yijia Liu", "Yuxuan Wang", "Bo Zheng", "Ting Liu."],
    "venue": "Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing",
    "year": 2018
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.",
    "year": 2014
  }, {
    "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
    "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova."],
    "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
    "year": 2019
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Stanford’s graph-based neural dependency parser at the conll 2017 shared task",
    "authors": ["Timothy Dozat", "Peng Qi", "Christopher D. Manning."],
    "venue": "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,",
    "year": 2017
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Optimum branchings",
    "authors": ["Jack Edmonds."],
    "venue": "Journal of Research of the National Bureau of Standards, 71B:233–240.",
    "year": 1967
  }, {
    "title": "Three new probabilistic models for dependency parsing: An exploration",
    "authors": ["Jason M. Eisner."],
    "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING), pages 340– 345.",
    "year": 1996
  }, {
    "title": "The (non)utility of structural features in BiLSTM-based dependency parsers",
    "authors": ["Agnieszka Falenska", "Jonas Kuhn."],
    "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 117–128.",
    "year": 2019
  }, {
    "title": "Assessing BERT’s syntactic abilities",
    "authors": ["Yoav Goldberg."],
    "venue": "CoRR, abs/1901.05287.",
    "year": 2019
  }, {
    "title": "A dynamic oracle for arc-eager dependency parsing",
    "authors": ["Yoav Goldberg", "Joakim Nivre."],
    "venue": "Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 959–976.",
    "year": 2012
  }, {
    "title": "Training deterministic parsers with non-deterministic oracles",
    "authors": ["Yoav Goldberg", "Joakim Nivre."],
    "venue": "Transactions of the Association for Computational Linguistics, 1:403–414.",
    "year": 2013
  }, {
    "title": "Learning word vectors for 157 languages",
    "authors": ["Edouard Grave", "Piotr Bojanowski", "Prakhar Gupta", "Armanpd Joulin", "Tomas Mikolov."],
    "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).",
    "year": 2018
  }, {
    "title": "Thw World Atlas of Language Structures",
    "authors": ["Martin Haspelmath", "Matthew S. Dryer", "David Gil", "Bernard Comrie."],
    "venue": "Oxford University Press.",
    "year": 2005
  }, {
    "title": "A structural probe for finding syntax in word representations",
    "authors": ["John Hewitt", "Christopher D. Manning."],
    "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2019
  }, {
    "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
    "authors": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jürgen Schmidhuber"],
    "year": 2001
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Dynamic programming for linear-time incremental parsing",
    "authors": ["Liang Huang", "Kenji Sagae."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077–1086.",
    "year": 2010
  }, {
    "title": "ELMoLex: Connecting ELMo and lexicon features for dependency parsing",
    "authors": ["Ganesh Jawahar", "Benjamin Muller", "Amal Fethi", "Louis Martin", "Eric Villemonte de la Clergerie", "Benoı̂t Sagot", "Djamé Seddah"],
    "venue": "In Proceedings of the CoNLL",
    "year": 2018
  }, {
    "title": "Incremental dependency parsing using online learning",
    "authors": ["Richard Johansson", "Pierre Nugues."],
    "venue": "Proceedings of the CoNLL Shared Task of EMNLPCoNLL 2007, pages 1134–1138.",
    "year": 2007
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional lstm feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "Transactions of the Association for Computational Linguistics, 4:313–327.",
    "year": 2016
  }, {
    "title": "75 languages, 1 model: Parsing universal dependencies universally",
    "authors": ["Daniel Kondratyuk."],
    "venue": "CoRR, abs/1904.02099.",
    "year": 2019
  }, {
    "title": "Efficient thirdorder dependency parsers",
    "authors": ["Terry Koo", "Michael Collins."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–11.",
    "year": 2010
  }, {
    "title": "Dual decomposition for parsing with non-projective head automata",
    "authors": ["Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-",
    "year": 2010
  }, {
    "title": "Dynamic programming algorithms for transition-based dependency parsers",
    "authors": ["Marco Kuhlmann", "Carlos Gómez-Rodrı́guez", "Giorgio Satta"],
    "venue": "In Proceedings of the 49th Annual Meeting of the Association",
    "year": 2011
  }, {
    "title": "2017a. From raw text to Universal Dependencies – Look, no tags",
    "authors": ["Miryam de Lhoneux", "Yan Shao", "Ali Basirat", "Eliyahu Kiperwasser", "Sara Stymne", "Yoav Goldberg", "Joakim Nivre"],
    "venue": "In Proceedings of the CoNLL",
    "year": 2017
  }, {
    "title": "Arc-hybrid non-projective dependency parsing with a static-dynamic oracle",
    "authors": ["Miryam de Lhoneux", "Sara Stymne", "Joakim Nivre."],
    "venue": "Proceedings of the 15th International Conference on Parsing Technologies, pages 99–104.",
    "year": 2017
  }, {
    "title": "Old school vs",
    "authors": ["Miryam de Lhoneux", "Sara Stymne", "Joakim Nivre."],
    "venue": "new school: Comparing transition-based parsers with and without neural network enhancement. In Proceedings of the 15th Treebanks and Linguistic Theories Workshop (TLT).",
    "year": 2017
  }, {
    "title": "SEx BiST: A multi-source trainable parser with deep contextualized lexical representations",
    "authors": ["KyungTae Lim", "Cheoneum Park", "Changki Lee", "Thierry Poibeau."],
    "venue": "Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to",
    "year": 2018
  }, {
    "title": "Linguistic knowledge and transferability of contextual representations",
    "authors": ["Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith."],
    "venue": "CoRR, abs/1903.08855.",
    "year": 2019
  }, {
    "title": "Online large-margin training of dependency parsers",
    "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 91–98.",
    "year": 2005
  }, {
    "title": "Multilingual dependency analysis with a twostage discriminative parser",
    "authors": ["Ryan McDonald", "Kevin Lerman", "Fernando Pereira."],
    "venue": "Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 216–220.",
    "year": 2006
  }, {
    "title": "Characterizing the errors of data-driven dependency parsing models",
    "authors": ["Ryan McDonald", "Joakim Nivre."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
    "year": 2007
  }, {
    "title": "Analyzing and integrating dependency parsers",
    "authors": ["Ryan McDonald", "Joakim Nivre."],
    "venue": "Computational Linguistics, pages 197–230.",
    "year": 2011
  }, {
    "title": "Online learning of approximate dependency parsing algorithms",
    "authors": ["Ryan McDonald", "Fernando Pereira."],
    "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88.",
    "year": 2006
  }, {
    "title": "Non-projective dependency parsing using spanning tree algorithms",
    "authors": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Hajič."],
    "venue": "Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in",
    "year": 2005
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "An efficient algorithm for projective dependency parsing",
    "authors": ["Joakim Nivre."],
    "venue": "Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.",
    "year": 2003
  }, {
    "title": "Algorithms for deterministic incremental dependency parsing",
    "authors": ["Joakim Nivre."],
    "venue": "Computational Linguistics, 34:513–553.",
    "year": 2008
  }, {
    "title": "Non-projective dependency parsing in expected linear time",
    "authors": ["Joakim Nivre."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-",
    "year": 2009
  }, {
    "title": "Universal dependencies 2.3. LINDAT/CLARIN digital library at the Institute",
    "authors": ["Zeldes", "Daniel Zeman", "Manying Zhang", "Hanzhi Zhu"],
    "year": 2018
  }, {
    "title": "The CoNLL 2007 shared task on dependency parsing",
    "authors": ["Joakim Nivre", "Johan Hall", "Sandra Kübler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."],
    "venue": "Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915–",
    "year": 2007
  }, {
    "title": "Labeled pseudo-projective dependency parsing with support vector machines",
    "authors": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Gülsen Eryiğit", "Svetoslav Marinov."],
    "venue": "Proceedings of the 10th Conference on Computational Natural Language Learning",
    "year": 2006
  }, {
    "title": "Integrating graph-based and transition-based dependency parsers",
    "authors": ["Joakim Nivre", "Ryan McDonald."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 950–958.",
    "year": 2008
  }, {
    "title": "Pseudoprojective dependency parsing",
    "authors": ["Joakim Nivre", "Jens Nilsson."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 99–106.",
    "year": 2005
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher clark", "Kenton Lee", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
    "year": 2018
  }, {
    "title": "Universal dependency parsing from scratch",
    "authors": ["Peng Qi", "Timothy Dozat", "Yuhao Zhang", "Christopher D Manning."],
    "venue": "Proceedings of the 2018 CoNLL Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, page 160.",
    "year": 2018
  }, {
    "title": "Parser combination by reparsing",
    "authors": ["Kenji Sagae", "Alon Lavie."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 129–132.",
    "year": 2006
  }, {
    "title": "Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing",
    "authors": ["Tal Schuster", "Ori Ram", "Regina Barzilay", "Amir Globerson."],
    "venue": "Proceedings of the 2019 Conference of the North American Chapter of",
    "year": 2019
  }, {
    "title": "An investigation of the interactions between pre-trained word embeddings, character models and pos tags in dependency parsing",
    "authors": ["Aaron Smith", "Miryam de Lhoneux", "Sara Stymne", "Joakim Nivre."],
    "venue": "Proceedings of the 2018 Conference on Empirical",
    "year": 2018
  }, {
    "title": "What do you learn from context? probing for sentence structure",
    "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R. Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"],
    "year": 2019
  }, {
    "title": "A latent variable model for generative dependency parsing",
    "authors": ["Ivan Titov", "James Henderson."],
    "venue": "Proceedings of the 10th International Conference on Parsing Technologies (IWPT), pages 144–155.",
    "year": 2007
  }, {
    "title": "The importance of being recurrent for modeling hierarchical structure",
    "authors": ["Ke Tran", "Arianna Bisazza", "Christof Monz."],
    "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4731–4736.",
    "year": 2018
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems, pages 5998–6008.",
    "year": 2017
  }, {
    "title": "A memory-based alternative for connectionist shiftreduce parsing",
    "authors": ["Jorn Veenstra", "Walter Daelemans."],
    "venue": "Technical Report ILK-0012, Tilburg University.",
    "year": 2000
  }, {
    "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
    "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman."],
    "venue": "Proceedings of the 7th International Conference on",
    "year": 2019
  }, {
    "title": "Structured training for neural network transition-based parsing",
    "authors": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 323–333.",
    "year": 2015
  }, {
    "title": "Statistical dependency analysis with support vector machines",
    "authors": ["Hiroyasu Yamada", "Yuji Matsumoto."],
    "venue": "Proceedings of the 8th International",
    "year": 2003
  }, {
    "title": "CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
    "authors": ["Daniel Zeman", "Jan Hajič", "Martin Popel", "Martin Potthtyersast", "Milan Straka", "Filip Ginter", "Joakim Nivre", "Slav Petrov."],
    "venue": "Proceedings of the CoNLL 2018",
    "year": 2018
  }, {
    "title": "Generalized higher-order dependency parsing with cube pruning",
    "authors": ["Hao Zhang", "Ryan McDonald."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    "year": 2012
  }, {
    "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2008
  }, {
    "title": "Transition-based parsing with rich non-local features",
    "authors": ["Yue Zhang", "Joakim Nivre."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 188–193.",
    "year": 2011
  }, {
    "title": "Analyzing the effect of global learning and beam-search on transition-based dependency parsing",
    "authors": ["Yue Zhang", "Joakim Nivre."],
    "venue": "Proceedings of COLING 2012: Posters, pages 1391–1400.",
    "year": 2012
  }],
  "id": "SP:bcc51c76850379bf41a6cc6e9d0f138ffaf6dcb0",
  "authors": [{
    "name": "Artur Kulmizev",
    "affiliations": []
  }, {
    "name": "Miryam de Lhoneux",
    "affiliations": []
  }, {
    "name": "Johannes Gontrum",
    "affiliations": []
  }, {
    "name": "Elena Fano",
    "affiliations": []
  }, {
    "name": "Joakim Nivre",
    "affiliations": []
  }],
  "abstractText": "Transition-based and graph-based dependency parsers have previously been shown to have complementary strengths and weaknesses: transition-based parsers exploit rich structural features but suffer from error propagation, while graph-based parsers benefit from global optimization but have restricted feature scope. In this paper, we show that, even though some details of the picture have changed after the switch to neural networks and continuous representations, the basic trade-off between rich features and global optimization remains essentially the same. Moreover, we show that deep contextualized word embeddings, which allow parsers to pack information about global sentence structure into local feature representations, benefit transition-based parsers more than graph-based parsers, making the two approaches virtually equivalent in terms of both accuracy and error profile. We argue that the reason is that these representations help prevent search errors and thereby allow transitionbased parsers to better exploit their inherent strength of making accurate local decisions. We support this explanation by an error analysis of parsing experiments on 13 languages.",
  "title": "Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited"
}