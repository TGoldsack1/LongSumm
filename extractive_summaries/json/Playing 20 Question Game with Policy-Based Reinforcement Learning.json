{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3233–3242 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3233"
  }, {
    "heading": "1 Introduction",
    "text": "The 20 Question Game (Q20 Game) is a classic game that requires deductive reasoning and creativity. At the beginning of the game, the answerer thinks of a target object and keeps it concealed. Then the questioner tries to figure out the target object by asking questions about it, and the answerer answers each question with a simple “Yes”, “No” or “Unknown”, honestly. The questioner wins the game if the target object is found within 20 questions. In a Q20 game system, the\n∗The work was done when the first author was an intern in Microsoft XiaoIce team.\nuser is considered as the answerer while the system itself acts as the questioner which requires a good question selection strategy to win the game.\nAs a game with the hype read your mind, Q20 has been played since the 19th century, and was brought to screen in the 1950s by the TV show Twenty Questions. Burgener’s program (Burgener, 2006) further popularized Q20 as an electronic game in 1988, and modern virtual assistants like Microsoft XiaoIce and Amazon Alexa also incorporate this game into their system to demonstrate their intelligence.\nHowever, it is not easy to design the algorithm to construct a Q20 game system. Although the decision tree based method seems like a natural fit to the Q20 game, it typically require a well defined Knowledge Base (KB) that contains enough information about each object, which is usually not available in practice. Burgener (2006) instead uses a object-question relevance table as the pivot for question and object selection, which does not depend on an existing KB. Wu et al. (2018) further improve the relevance table with a lot of engineering tricks. Since these table-based methods greedily select questions and the model parameters are only updated by rules, their models are very sensitive to noisy answers from users, which is common in the real-world Q20 games. Zhao and Maxine (2016) utilizes a value-based Reinforcement Learning (RL) model to improve the generalization ability but still relies on the existing KB.\nIn this paper, we formulate the process of question selction in the game as a Markov Decision Process (MDP), and further propose a novel policy-based RL framework to learn the optimal policy of question selection in the Q20 game. Our questioner agent maintains a probability distribution over all objects to model the confidence of the target object, and updates the confidence based on answers from the user. At each time-step. the agent uses a policy network πθ(a|s) to take in\nthe confidence vector and output a question distribution for selecting the next question. To solve the problem that there is no immediate reward for each selected question, we also propose to employ a RewardNet to estimate the appropriate immediate reward at each time-step, which is further used to calculate the long-term return to train our RL model. Our RL framework makes the agent robust to noisy answers since the model parameters are fully learnable and the question distribution from πθ(a|s) provides us with a principled way to sample questions, which enables the agent to jump out of the local optimum caused by incorrect answers and also introduces more randomness during training to improve the model generalization ability. Furthermore, the ability to sample questions, compared to greedy selection, also improves the diversity of the questions asked by our agent, which is crucial for user experience.\nOur contributions can be summarized as follows: (1) We propose a novel RL framework to learn the optimal policy of question selection in the Q20 game without any dependencies on the existing KBs of target objects. Our trained agent is robust to noisy answers and has a good diversity in its selected questions. (2) To make the reward more meaningful, we also propose a novel neural network on reward function approximation to deliver the appropriate immediate rewards at each time-step. (3) Extensive experiments show that our RL method clearly outperforms a highly engineered baseline in the real-world Q20 games where noisy answers are common. Besides, our RL method is also competitive to that baseline on a noise-free simulation environment."
  }, {
    "heading": "2 Method",
    "text": "In this section, we first describe our RL framework for playing the Q20 game, which is shown in the\nFig. 1. The user in our system is the answerer who thinks of a target object otgt in the object set O at the beginning of the game. Our policy-based agent acts as the questioner that can ask 20 questions to figure out what exactly otgt is. Specifically, an internal state vector s is maintained by our agent, which describes the confidence about otgt. At each time-step t, the agent picks up the promising action (select a question) according to the policy πθ(a|st), and transits from the state st to the next state st+1 after receiving the answer (“Yes”/“No”/“Unknown”) from the user. The historical trajectories 〈st, at, rt+1, st+1〉 are stored in a replay memory which enables the agent to be trained on previously observed data by sampling from it. Note that only when a guess is made about otgt at the end of game can the agent receive a reward signal, which makes it unable to distinguish the importance of each selected question. Therefore, we design a RewardNet to learn the more informative reward at each time-step and thus lead the agent to achieve the better performance.\nIn the rest of this section, we first describe how to formulate the Q20 game into a RL framework, and then introduce the RewardNet. Finally, we will demonstrate our training procedure in detail."
  }, {
    "heading": "2.1 Modeling of the Q20 Game",
    "text": "In the Q20 game, the goal of our agent is to figure out the object otgt that the user thinks of at the beginning of game by asking 20 questions. We formulate the process of question selection as a finite Markov Decision Process (MDP) which can be solved with RL. A tuple 〈S,A, P ,R, γ〉 is defined to represent the MDP, where S is the continuous state space, A = {a1, a2, · · · , am} is the set of all available actions, P(St+1 = s′|St = s,At = a) is the transition probability matrix,R(s, a) is the reward function and γ ∈ [0, 1] is the discount factor used to calculate the long-time return. In the RL framework, at each time-step t, the agent takes an action at under the state st according to the policy πθ(a|st). After interacting with the environment, the agent receives a reward scalar rt+1 and transits to the next state st+1, then another time-step begins. All these trajectories 〈st, at, rt+1, st+1〉 in a game constitute an episode which is an instance of the finite MDP. The long-time return Gt of the time-step t is calculated as follows:\nGt = T∑ k=0 γkrt+k+1 (1)\nIn the following parts, we describe each component of RL corresponding to the Q20 game.\nEnvironment. The major component of our environment is the user in the Q20 game who decides the target object otgt and answers questions from the agent. Besides, the environment also needs to deliver the reward based on the outcome of the game and store historical data into the replay memory (see Fig. 1).\nAction. Since the agent interacts with the user by asking questions, the action at ∈ A taken by our agent refers to selecting the question qat at timestep t, andA is the set of the indices to all available questions in the Q20 game.\nState. In our method, we use the state st to keep track of the current confidence of target object otgt. Specifically st ∈ R|O| and ∑n i=1 st,i = 1, where O = {o1, o2, · · · , on} represents the set of all the objects that can be chosen by the user. Therefore, the state st is a probability distribution over all the objects and st,i is the confidence that the object oi is the target object otgt at time-step t.\nThe initial state s0 can either be a uniform distribution or initialized by the prior knowledge. We observe that users typically prefer to choose popular objects which are more concerned by the public. For example, the founder of Tesla Inc. and the designer of SpaceX, “Elon Musk”, is more likely to be chosen compared to a CEO of a new startup. Motivated by this, we could use the yearly retrieval frequency C(oi) of object oi on a commercial search engine to calculate the initial state s0, where s0,i = C(oi) / ∑n j=1C(oj).\nTransition Dynamics. In our method, the transition dynamics is deterministic. Given the object set O and the question set A, we collect the normalized probabilities of the answer over “Yes”, “No” and “Unknown” for each object-question pair. And the rule of state transition is define as:\nst+1 = st α (2)\nwhere α depends on the answer xt to the question qat which is selected by the agent at the step t:\nα =  [R(1, at), . . . , R(|O|, at)], xt = Y es\n[W (1, at), . . . ,W (|O|, at)], xt = No [U(1, at), . . . , U(|O|, at)], xt = Unk\n(3) where O is the object set and for each objectquestion pair (oi, qj), R(i, j) and W (i, j) are cal-\nculated as follows:\nR(i, j) = Cyes(i, j) + δ\nCyes(i, j) + Cno(i, j) + Cunk(i, j) + λ\nW (i, j) = Cno(i, j) + δ\nCyes(i, j) + Cno(i, j) + Cunk(i, j) + λ\n(4)\nR(i, j) and W (i, j) are probabilities of answering “Yes” and “No” to question qj with respect to the object oi respectively. Cyes(i, j), Cno(i, j) and Cunk(i, j) are frequencies of answering “Yes”, “No” and “Unknown” to question qj with respect to the object oi. δ and λ are smoothing parameters. Then the probability of answering “Unknown” to question qj with respect to the object oi is:\nU(i, j) = 1−R(i, j)−W (i, j) (5)\nIn this way, the confidence st,i that the object oi is the target object otgt is updated following the user’s answer xt to the selected question qat at the time-step t.\nPolicy Network. We directly parameterize the policy πθ(a|st) with a neural network which maps the state st to a probability distribution over all available actions: πθ(a|st) = P[a|st; θ]. The parameters θ are updated to maximize the expected return which is received from the environment. Instead of learning a greedy policy in value-based methods like DQN, the policy network is able to learn a stochastic policy which can increase the diversity of questions asked by our agent and potentially make the agent more robust to noisy answers in the real-world Q20 game. The policy πθ(a|s) is modeled by a Multi-Layer Perceptron (MLP) and the output layer is normalized by using a masked softmax function to avoid selecting the question that has been asked before. Because asking the same question twice does not provide extra information about otgt in a game."
  }, {
    "heading": "2.2 Problem of Direct Reward",
    "text": "For most reinforcement learning applications, it is always a critical part to design reward functions, especially when the agent needs to precisely take actions in a complex task. A good reward function can improve the learning efficiency and help the agent achieve better performances.\nIn the Q20 game, however, the immediate reward rt of selecting question qat is unknown at the time-step t (t < T ) because each selected question is just answered with a simple “Yes”, “No” or\n“Unknown” and there is no extra information provided by user. Only when the game ends (t = T ) can the agent receive a reward signal of win or loss. So we intuitively consider the direct reward: rT = 30 and −30 for the win and loss respectively while rt = 0 for all t < T . Unfortunately, the direct reward is not discriminative because the agent receives the same immediate reward rt = 0 (t < T ) for selecting both good and bad questions. For example, if the otgt is “Donald Trump”, then selecting question (a) “Is your role the American president?” should receive more immediate reward rt than selecting question (b) “Has your role been married?”. The reason is that as for the otgt, question (a) is more relevant and can narrow down the searching space to a greater extent.\nTherefore, it is necessary to design a better reward function to estimate a non-zero immediate reward rt, and make the long-time return Gt =∑T\nk=0 γ krt+k+1 more informative."
  }, {
    "heading": "2.3 Reward Function Approximation by Neural Network",
    "text": "To solve the problem of the direct reward, we propose a reward function which employs a neural network to estimate a non-zero immediate reward rt at each time-step. So that Gt can be more informative, which thus leads to a better trained questioner agent.\nThe reward function takes the state-action pair (st, at) as input and outputs the corresponding immediate reward rt+1. In our method, we use a MLP with sigmoid output to learn the appropriate immediate reward during training, and this network is referred as RewardNet. In each episode, the long-term return Gt is used as a surrogate indicator of rt+1 to train our RewardNet with the following loss function:\nL1(σ) = (R(st, at;σ)− sigmoid(Gt))2 (6)\nwhere σ is the network parameters. Here we apply the sigmoid function on Gt so as to prevent Gt from growing too large. Besides, we also use the replay memory to store both old and recent experiences, and then train the network by sampling mini-batches from it. The training process based on the experience replay technique can decorrelate the sample data and thus make the training of the RewardNet more efficient.\nFurthermore, since the target object otgt can be obtained at the end of each episode, we can\nuse the extra information provided by otgt to estimate a better immediate reward rt. To capture the relevance between the selected questions and otgt in an episode, we further propose a objectaware RewardNet which takes the 〈st, at, otgt〉 tuple as input and produces corresponding rt+1 as output. The detailed training algorithm is shown in Algo. 1.\nAlgorithm 1: Training Object-Aware RewardNet 1 Initialize replay memory D1 to capacity N1 2 Initialize RewardNet with random weights σ 3 for episode i← 1 to Z do 4 User chooses object oi from O 5 Initialize temporary set S1 and S2 6 Play with policy πθ(at|st), and store (st, at) in S1, where t ∈ [0, T ] 7 rT ← 30 or −30 for a win or loss 8 for (st, at) in S1 do 9 Get rt+1 from RewardNet\n10 Store (st, at, rt+1) tuple in S2 11 for (st, at, rt+1) in S2 do 12 Gt ← ∑T k=0 γ\nkrt+k+1 13 r′t+1 ← sigmoid(Gt) 14 Store (st, at, oi, r′t+1) in D1 15 if len(D1) > K1 then 16 Sample mini-batch from D1 17 Update σ with loss L1(σ) in Eq. 6"
  }, {
    "heading": "2.4 Training the Policy-Based Agent",
    "text": "We train the policy network using REINFORCE (Williams, 1992) algorithm and the corresponding loss function is defined as follows:\nL2(θ) = −Eπθ [log πθ(at|st)(Gt − bt)] (7)\nwhere the baseline bt is a estimated value of the expected future reward at the state st, which is produced by a value network Vη(st). Similarly, the value network Vη(st) is modeled as a MLP which takes the state st as input and outputs a real value as the expected return. By introducing the baseline bt for the policy gradient, we can reduce the variance of gradients and thus make the training process of policy network more stable. The network parameters η are updated by minimizing the loss function below:\nL3(η) = (Vη(st)−Gt)2 (8)\nNote that, in our method, both the RewardNet and the value network Vη(st) approximate the reward during training. But the difference lies in that the RewardNet is designed to estimate a appropriate non-zero reward rt and further derive the more informative return Gt while Vη(st) aims to learn a baseline bt to reduce the variance of policy gradients. We combine both of two networks to improve the gradients for our policy network and thus lead to a better agent. The training procedure is described in Algo. 2.\nAlgorithm 2: Training the Agent 1 Initialize replay memory D2 to capacity N2 2 Initialize policy net π with random weights θ 3 Initialize value net V with random weights η 4 Initialize RewardNet with random weights σ 5 for episode i← 1 to Z do 6 Rollout, collect rewards, and save the history in S2 (4-10 in Algo. 1) 7 for (st, at, rt+1) in S2 do 8 Gt ← ∑T k=0 γ\nkrt+k+1 9 Update RewardNet (13-17 in\nAlgo. 1) 10 Store (st, at, Gt) in D2 11 if len(D2) > K2 then 12 Sample mini-batch from D2 13 Update η with loss L3 in Eq. 8 14 Update θ with loss L2 in Eq. 7"
  }, {
    "heading": "3 Experimental Setup",
    "text": "We use a user simulator to train our questioner agent and test the agent with the simulated answerer and real users. Specifically, our experiments answer three questions: (1) Is our method more robust in real-world Q20 games, compared to the methods based on relevance table? (Section. 4.2) And how does it perform in the simulation environment? (Section. 4.1) (2) Does our RewardNet help in the training process? (Section. 4.3) (3) How the winning rate grows with the number of questions, and whether it is possible to stop earlier? (Section. 4.4)"
  }, {
    "heading": "3.1 User Simulator",
    "text": "Training the RL agent is challenging because the agent needs to continuously interact with the environment. To speed up the training process of the proposed RL model, we construct a user simulator\nwhich has enough prior knowledge to choose objects and answer questions selected by the agent.\nWe collect 1,000 famous people and 500 questions for them. Besides, for every person-question pair in our dataset, a prior frequency distribution over “Yes”, “No” and “Unknown” is also collected from thousands of real users. For example, as for “Donald Trump”, question (a) “Is your role the American president?” is answered with “Yes” for 9,500 times, “No” for 50 times and “Unknown” for 450 times. We use Eq.4 and 5 to construct three matrices R,W,U ∈ R|O|∗|A| (|O| = 1000, |A| = 500) which are used for state transition in the Section. 2.1. Then given the object oi and question qj , the user simulator answers “Yes”, “No” and “Unknown” whenR(i, j),W (i, j), andU(i, j) has the max value among them respectively.\nConstructed by the prior knowledge, the simulator can give noise-free answer in most cases. Because the prior frequency distribution for each person-question pair is collected from thousands of users with the assumption that most of them do not lie when answering questions in the Q20 game.\nIn an episode, the simulator randomly samples a person following the object distribution s0, which is generated from the object popularity (see the state part of Section. 2.1), as the target object. Then the agent gives a guess when the number of selected questions reaches 20. After that, the simulator check the agent’s answer and return a reward signal of win or loss. There is only one chance for the agent to guess in an episode. The win and loss reward are 30 and -30 respectively."
  }, {
    "heading": "3.2 Implementation Details",
    "text": "While the architectures of the policy network, RewardNet and value network can vary in different scenarios, in this paper, we simply use the MLP with one hidden layer of size 1,000 for all of them, but with different parameters. These networks take in the state vector directly, which is a probability distribution over all objects. The RewardNet further takes in the one-hot vector of action at. Based on the input of RewardNet, the objectaware RewardNet takes one more target object otgt as the feature which is also a one-hot vector.\nWe use the ADAM optimizer (Kingma and Ba, 2014) with the learning rate 1e-3 for policy network and 1e-2 for both RewardNet and value network. The discounted factor γ for calculating the long-term return is 0.99. The model was trained up\nto 2,000,000 steps (2,00,000 games) and the policy network was evaluated every 5,000 steps. Each evaluation records the agent’s performance with a greedy policy for 2,000 independent episodes. The 2,000 target objects for these 2,000 episodes are randomly selected following the distribution s0, which is generated from the object popularity and kept the same for all the training settings."
  }, {
    "heading": "3.3 Competitor",
    "text": "We compare our RL method with the entropybased model proposed by Wu et al. (2018), which utilizes the real-world answers to each objectquestion pair to calculate an object-question relevance matrix with the entropy-based method. The relevance matrix is then used for question ranking and object ranking via carefully designed formulas and engineering tricks. Since this method is shown to be effective in their production environment, we consider it to be a strong baseline to our proposed RL model."
  }, {
    "heading": "4 Experimental Results",
    "text": ""
  }, {
    "heading": "4.1 Simulated Evaluation",
    "text": "We first evaluate our agent and the entropy-based baseline (referred to as EntropyModel, see Section. 3.3) by using the simulated user (Section. 3.1). To investigate which initialization strategy of the state s0 is better (see the state part of Section. 2.1), we further evaluate two variants of our model: the agent with uniform distribution s0 (RL uniform) and the agent with the distribution s0 initialized by the prior knowledge on the object popularity (RL popularity).\nFig. 2 shows the curves on the win rate of these methods evaluated on 2,000 independent episodes\nwith respect to the number of training steps. Note that, the EntropyModel only needs to update its statistics during training and has already accumulated a significant number of data since it has been run for over a year in their production environment. Therefore, only a small fraction of its statistics can be changed, which leads to a small rise at the beginning of training, and its win rate remains at around 95% afterwards.\nOn the other hand, both our RL models continuously improve the win rate with the growing number of interactions with the user simulator, and they achieve 50% win rate after around 20,000 steps. As we can see, although the s0 initialized with the prior knowledge of object popularity keeps consistent with the object selection strategy of the simulator, the agent with uniform distribution s0 (RL uniform) still performs clearly better than the agent with s0 based on the prior knowledge (RL popularity). The reason is that the former can explore the Q20 game environment more fully. The prior knowledge based s0 helps the agent narrow down the candidate space more quickly when the target object is a popular object. However, it also becomes misleading when the target object is not popular and makes the agent even harder to correct the confidence of the target object. On the contrary, the uniform distribution s0 makes the agent keep track of the target object only based on the user’s answers. And the superior performance of the RL uniform indicates that our question selection policy is highly effective, which means it is not necessary to use the RL popularity to increase the win rate of hot objects in the game.\nAs shown in Fig. 2, RL uniform achieves win rate 94% which is very close to EntropyModel. Compared to our RL method, EntropyModel needs more user data to calculate their entropybased relevance matrix and involves many engineering tricks. The fact that RL uniform is competitive to EntropyModel in the noise-free simulation environment indicates that our RL method is very cost-effective: it makes use of user data more efficiently and is easier to implement."
  }, {
    "heading": "4.2 Human Evaluation",
    "text": "To further investigate the performance of our RL method in the real-world Q20 game where noisy answers are common, we also conduct an human evaluation experiment. Specifically, we let real\nusers to play the game with EntropyModel and RL uniform for 1,000 times respectively. In the real-world Q20 game, users sometimes make mistakes when they answer the questions during the game. For example, as for the target object “Donald Trump”, question (a) “Is your role the American president?” is sometimes answered with “No” or “Unknown” by real users. On the contrary, the simulator hardly makes such mistakes since we have provided it with enough prior knowledge. As shown in Table. 1, RL uniform outperforms EntropyModel by about 4.5% on win rate in the real-world Q20 games. It shows that our RL method is more robust to noisy answers than EntropyModel. Specifically, the robustness of our RL method to the noise is shown in the following two aspects. First, compared to the rulebased statistics update in EntropyModel, our RL model can be trained by modern neural network optimizers in a principled way, which results in the better generalization ability of our model. Secondly, different from the EntropyModel selecting the top-ranked question at each time-step, RL uniform samples a question following its question probability distribution πθ(a|s), which enables our agent to jump out of the local optimum caused by incorrect answers from users. And since more randomness is introduced by sampling from the question probability distribution during training, it also improves the tolerance of our model towards the unexpected question sequences.\nBesides, we also find some interesting cases during human evaluation. Sometimes, the RL agent selects a few strange questions which seems to be not that much relevant to the chosen object, but it can still find the correct answer at the end of game. This situation is caused by the fact that our method samples questions based on the output of policy net, rather than greedy selection during training. We find that this phenomenon increases the user experience since it makes the agent more unpredictable to the users."
  }, {
    "heading": "4.3 The Effectiveness of RewardNet",
    "text": "To investigate the effectiveness of our RewardNet (Section. 2.3), we further evaluate three variants of our model in the simulation environment: the model trained with with direct reward, RewardNet, and object-aware RewardNet, which are referred to as DirectReward, RewardNet, and ObjectRewardNet respectively. They are all trained with the uniform distribution s0.\nAs shown in Fig. 3, DirectReward converges in the early steps and has a relatively poor performance with the win rate 89%. Both RewardNet and ObjectRewardNet achieve the better performance with a win rate of 94% after convergence. This clear improvement shows that the more informative long-term return, calculated with the immediate reward delivered by our RewardNet method, significantly helps the training of the agent.\nFurthermore, as shown in Fig. 3, we can also see that ObjectRewardNet learns faster than RewardNet in the early steps. This indicates that ObjectRewardNet can estimate the immediate reward more quickly with the extra information provided by the target object, which leads to the faster convergence of the agent."
  }, {
    "heading": "4.4 Win Rate Regarding Question Numbers",
    "text": "In this section, we investigate how the win rate grows with the number of asked questions and whether a early-stop strategy can be adopted in the game. We use the user simulator to play the game with the RL uniform agent and two settings are taken into account: the simulator samples the target object following the uniform object distribution (UnifSimulator), and samples following\nthe prior object distribution based on the object popularity (PopSimulator). We perform 1,000 simulations for each number of questions, and the win rate curve is shown in Fig. 4.\nAs we can see that UnifSimulator achieves the win rate of 80% with only 14 questions in both settings. And the flat curves in the region after 18 questions indicate that the game can be early stopped with the almost same win rate at step 18. Since a lower win rate is acceptable sometimes, other early-stop strategies can also be derived for the better user experience with the trade-off between the win rate and game steps.\nBesides, the fact that RL uniform performs similarly under both settings actually shows that our RL method is robust to different objects. It also performs well on infrequent objects where we may have the limited user data for constructing a well-tuned state transition dynamics."
  }, {
    "heading": "4.5 Case Study",
    "text": "When our agent is playing the game with real users, we select two cases from records. In the first case, the person that the user chooses is Cristiano Ronaldo, the famous football player. As we can see in Tab. 2, our agent can still figure out the target person while No.17 and No.19 questions are answered wrong by the user, which indicates our agent is robust to noisy answers. In the second case, the chosen person is Napoleon Bonaparte who was the French Emperor. Although there are some other candidates satisfied the constraints, the target person can be figured out because of the people popularity, which is shown in Tab. 3."
  }, {
    "heading": "5 Related Work",
    "text": "Q20. The Q20 game is popularized as an electronic game by the program of Robin Burgener in 1988 (Burgener, 2006), which uses a objectquestion relevance table to rank questions and target objects. Wu et al. (Wu et al., 2018) improves the relevance table with entropy-based metrics, and uses complicated engineering tricks to make it perform quite well in their production environment. These table-based methods use rules to update parameters, which makes them easily affected by noisy answers. Besides, Zhao and Maxine (2016) also explores Q20 in their dialogue state tracking research. However, they only use a small toy Q20 setting where the designed questions are about 6 person attributes in the Knowledge Base (KB). Since their method relies on the KB for narrowing down the scope of target object, it is not applicable to real-world Q20 games where a welldefined object KB is often unavailable. Compared to previous approaches, our RL method is robust to the answer noise and does not rely on the KB.\nDeep Reinforcement Learning. DRL has witnessed great success in playing complex games like Atari games (Mnih et al., 2015) , Go (Silver et al., 2016), and etc. In the natural language processing (NLP), DRL is also used to play text-based games (Narasimhan et al., 2015), and used to handle fundamental NLP tasks like machine translation (He et al., 2016) and machine comprehension (Hu et al., 2017) as well. Our Q20 game lies in the intersection of the field of game and NLP. In this work, we propose a policy-based RL model that acts as the questioner in the Q20 game, and it exhibits the superior performance in our human evaluation.\nNatural Language Games. In the literature, there are some works focusing on solving and generating English riddles (De Palma and Weiner, 1992; Binsted, 1996) and Chinese character riddles (Tan et al., 2016). Compared to riddles, the Q20 game is a sequential decision process which requires careful modeling of this property."
  }, {
    "heading": "6 Conclusions",
    "text": "In this paper, we propose a policy-based RL method to solve the question selection problem in the Q20 Game. Instead of using the direct reward, we further propose an object-aware RewardNet to estimate the appropriate non-zero reward and\nthus make the long-time return more informative. Compared to previous approaches, our RL method is more robust to the answer noise which is common in the real-world Q20 game. Besides, our RL agent can also ask various questions and does not require the existing KB and complicated engineering tricks. The experiments on a noisy-free simulation environment show that our RL method is competitive to an entropy-based engineering system, and clearly outperforms it on the human evaluation where noisy answers are common.\nAs for the future work, we plan to explore methods to use machine reading to automatically construct the state transition dynamics from corpora like Wikipedia. In this way, we can further build an end-to-end framework for the large-scale Q20 games in the real world."
  }, {
    "heading": "Acknowledgement",
    "text": "We gratefully thank the anonymous reviewers for their insightful comments and suggestions on the earlier version of this paper. The first author also thanks the Microsoft for providing resources for the research."
  }],
  "year": 2018,
  "references": [{
    "title": "Machine humour: An implemented model of puns",
    "authors": ["Kim Binsted"],
    "year": 1996
  }, {
    "title": "Artificial neural network guessing method and game",
    "authors": ["Robin Burgener"],
    "year": 2006
  }, {
    "title": "Riddles: accessibility and knowledge representation",
    "authors": ["Paul De Palma", "E Judith Weiner."],
    "venue": "Proceedings of the 14th conference on Computational linguistics-Volume 4, pages 1121–1125. Association for Computational Linguistics.",
    "year": 1992
  }, {
    "title": "Dual learning for machine translation",
    "authors": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tieyan Liu", "Wei-Ying Ma."],
    "venue": "Advances in Neural Information Processing Systems, pages 820–828.",
    "year": 2016
  }, {
    "title": "Reinforced mnemonic reader for machine comprehension",
    "authors": ["Minghao Hu", "Yuxing Peng", "Xipeng Qiu."],
    "venue": "CoRR, abs/1705.02798.",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Humanlevel control through deep reinforcement learning",
    "authors": ["V Mnih", "K Kavukcuoglu", "D Silver", "A.A. Rusu", "J Veness", "M.G. Bellemare", "A Graves", "M Riedmiller", "A.K. Fidjeland", "G Ostrovski."],
    "venue": "Nature, 518(7540):529.",
    "year": 2015
  }, {
    "title": "Language understanding for textbased games using deep reinforcement learning",
    "authors": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–",
    "year": 2015
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"],
    "year": 2016
  }, {
    "title": "Solving and generating chinese character riddles",
    "authors": ["Chuanqi Tan", "Furu Wei", "Li Dong", "Weifeng Lv", "Ming Zhou."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 846–855.",
    "year": 2016
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Reinforcement Learning, pages 5–32. Springer.",
    "year": 1992
  }, {
    "title": "Q20: Rinna riddles your mind by asking 20 questions",
    "authors": ["Xianchao Wu", "Huang Hu", "Momo Klyen", "Kyohei Tomita", "Zhan Chen."],
    "venue": "Japan NLP.",
    "year": 2018
  }, {
    "title": "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning",
    "authors": ["Tiancheng Zhao", "Maxine Eskenazi."],
    "venue": "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 1.",
    "year": 2016
  }],
  "id": "SP:f25553d4b0154a158cb83f9965e0fa7dce426705",
  "authors": [{
    "name": "Huang Hu",
    "affiliations": []
  }, {
    "name": "Xianchao Wu",
    "affiliations": []
  }, {
    "name": "Bingfeng Luo",
    "affiliations": []
  }, {
    "name": "Chongyang Tao",
    "affiliations": []
  }, {
    "name": "Can Xu",
    "affiliations": []
  }, {
    "name": "Wei Wu",
    "affiliations": []
  }, {
    "name": "Zhan Chen",
    "affiliations": []
  }],
  "abstractText": "The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisyfree simulation environment.",
  "title": "Playing 20 Question Game with Policy-Based Reinforcement Learning"
}