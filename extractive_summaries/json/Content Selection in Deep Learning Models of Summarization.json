{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Content selection is a central component in many natural language generation tasks, where, given a generation goal, the system must determine which information should be expressed in the output text (Gatt and Krahmer, 2018). In summarization, content selection is usually accomplished through sentence (and, occasionally, phrase) extraction. Despite being a key component of both extractive and abstractive summarization systems, it is is not well understood how deep learning models perform content selection with only word and sentence embedding based features as input. Nonneural network approaches often use frequency and information theoretic measures as proxies for content salience (Hong and Nenkova, 2014), but these are not explicitly used in most neural network summarization systems.\nIn this paper, we seek to better understand how deep learning models of summarization perform content selection across multiple domains (§ 4):\nnews, personal stories, meetings, and medical articles (for which we collect a new corpus).1 We analyze several recent sentence extractive neural network architectures, specifically considering the design choices for sentence encoders (§ 3.1) and sentence extractors (§ 3.2). We compare Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based sentence representations to the simpler approach of word embedding averaging to understand the gains derived from more sophisticated architectures. We also question the necessity of auto-regressive sentence extraction (i.e. using previous predictions to inform future predictions), which previous approaches have used (§ 2), and propose two alternative models that extract sentences independently.\nOur main results (§ 5) reveal: 1. Sentence position bias dominates the learn-\ning signal for news summarization, though not for other domains.2 Summary quality for news is only slightly degraded when content words are omitted from sentence embeddings. 2. Word embedding averaging is as good or better than either RNNs or CNNs for sentence embedding across all domains. 3. Pre-trained word embeddings are as good, or better than, learned embeddings in five of six datasets. 4. Non auto-regressive sentence extraction performs as good or better than auto-regressive extraction in all domains.\nTaken together, these and other results in the paper suggest that we are over-estimating the abil-\n1Data preprocessing and implementation code can be found here: https://github.com/kedz/nnsum/ tree/emnlp18-release\n2This is a known bias in news summarization (Nenkova, 2005).\nity of deep learning models to learn robust and meaningful content features for summarization. In one sense, this might lessen the burden of applying neural network models of content to other domains; one really just needs in-domain word embeddings. However, if we want to learn something other than where the start of the article is, we will need to design other means of sentence representation, and possibly external knowledge representations, better suited to the summarization task."
  }, {
    "heading": "2 Related Work",
    "text": "The introduction of the CNN-DailyMail corpus by Hermann et al. (2015) allowed for the application of large-scale training of deep learning models for summarization. Cheng and Lapata (2016) developed a sentence extractive model that uses a word level CNN to encode sentences and a sentence level sequence-to-sequence model to predict which sentences to include in the summary. Subsequently, Nallapati et al. (2017) proposed a different model using word-level bidirectional RNNs along with a sentence level bidirectional RNN for predicting which sentences should be extracted. Their sentence extractor creates representations of the whole document and computes separate scores for salience, novelty, and location. These works represent the state-of-the-art for deep learningbased extractive summarization and we analyze them further in this paper.\nOther recent neural network approaches include, Yasunaga et al. (2017), who learn a graphconvolutional network (GCN) for multi-document summarization. They do not closely examine the choice of sentence encoder, which is one of the focuses of the present paper; rather, they study the best choice of graph structure for the GCN, which is orthogonal to this work.\nNon-neural network learning-based approaches have also been applied to summarization. Typically they involve learning n-gram feature weights in linear models along with other non-lexical word or structural features (Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Durrett et al., 2016). In this paper, we study representation learning in neural networks that can capture more complex word level feature interactions and whose dense representations are more compatible with current practices in NLP.\nThe previously mentioned works have focused on news summarization. To further understand the\ncontent selection process, we also explore other domains of summarization. In particular, we explore personal narrative summarization based on stories shared on Reddit (Ouyang et al., 2017), workplace meeting summarization (Carletta et al., 2005), and medical journal article summarization (Mishra et al., 2014).\nWhile most work on these summarization tasks often exploit domain-specific features (e.g. speaker identification in meeting summarization (Galley, 2006; Gillick et al., 2009)), we purposefully avoid such features in this work in order to understand the extent to which deep learning models can perform content selection using only surface lexical features. Summarization of academic literature (including medical journals), has long been a research topic in NLP (Kupiec et al., 1995; Elhadad et al., 2005), but most approaches have explored facet-based summarization (Jaidka et al., 2017), which is not the focus of our work."
  }, {
    "heading": "3 Methods",
    "text": "The goal of extractive text summarization is to select a subset of a document’s text to use as a summary, i.e. a short gist or excerpt of the central content. Typically, we impose a budget on the length of the summary in either words or bytes. In this work, we focus on sentence extractive summarization, where the basic unit of extraction is a sentence and impose a word limit as the budget.\nWe model the sentence extraction task as a sequence tagging problem, following (Conroy and O’Leary, 2001). Specifically, given a document containing n sentences s1, . . . , sn we generate a summary by predicting a corresponding label sequence y1, . . . , yn ∈ {0, 1}n, where yi = 1 indicates the i-th sentence is to be included in the summary. Each sentence is itself a sequence of word embeddings si = w (i) 1 , . . . , w (i) |si| where |si| is the length of the sentence in words. The word budget c ∈ N enforces a constraint that the total summary word length ∑n i=1 yi · |si| ≤ c.\nFor a typical deep learning model of extractive summarization there are two main design decisions: a) the choice of sentence encoder which maps each sentence si to an embedding hi, and b) the choice of sentence extractor which maps a sequence of sentence embeddings h = h1, . . . , hn to a sequence of extraction decisions y = y1, . . . , yn."
  }, {
    "heading": "3.1 Sentence Encoders",
    "text": "We experiment with three architectures for mapping sequences of word embeddings to a fixed length vector: averaging, RNNs, and CNNs. Hyperparameter settings and implementation details can be found in Appendix A.\nAveraging Encoder Under the averaging encoder, a sentence embedding h is simply the average of its word embeddings, i.e. h = 1|s| ∑|s| i=1wi.\nRNN Encoder When using the RNN sentence encoder, a sentence embedding is the concatenation of the final output states of a forward and backward RNN over the sentence’s word embeddings. We use a Gated Recurrent Unit (GRU) for the RNN cell (Chung et al., 2014).\nCNN Encoder The CNN sentence encoder uses a series of convolutional feature maps to encode each sentence. This encoder is similar to the convolutional architecture of Kim (2014) used for text classification tasks and performs a series of “one-dimensional” convolutions over word embeddings. The final sentence embedding h is a concatenation of all the convolutional filter outputs after max pooling over time."
  }, {
    "heading": "3.2 Sentence Extractors",
    "text": "Sentence extractors take sentence embeddings h1:n and produce an extract y1:n. The sentence extractor is essentially a discriminative classifier p(y1:n|h1:n). Previous neural network approaches to sentence extraction have assumed\nan auto-regressive model, leading to a semiMarkovian factorization of the extractor probabilities p(y1:n|h) = ∏n i=1 p(yi|y<i, h), where each prediction yi is dependent on all previous yj for all j < i. We compare two such models proposed by Cheng and Lapata (2016) and Nallapati et al. (2017). A simpler approach that does not allow interaction among the y1:n is to model p(y1:n|h) = ∏n i=1 p(yi|h), which we explore in two proposed extractor models that we refer to as the RNN and Seq2Seq extractors. Implementation details for all extractors are in Appendix B.\nPreviously Proposed Sentence Extractors We consider two recent state-of-the-art extractors.\nThe first, proposed by Cheng and Lapata (2016), is built around a sequence-to-sequence model. First, each sentence embedding3 is fed into an encoder side RNN, with the final encoder state passed to the first step of the decoder RNN. On the decoder side, the same sentence embeddings are fed as input to the decoder and decoder outputs are used to predict each yi. The decoder input is weighted by the previous extraction probability, inducing the dependence of yi on y<i. See Figure 1.c for a graphical layout of the extractor.\nNallapati et al. (2017) proposed a sentence extractor, which we refer to as the SummaRunner Extractor, that factorizes the extraction probability into contributions from different sources. First, a bidirectional RNN is run over the sentence em-\n3Cheng and Lapata (2016) used an CNN sentence encoder with this extractor architecture; in this work we pair the Cheng & Lapata extractor with several different encoders.\nbeddings4 and the output is concatenated. A representation of the whole document is made by averaging the RNN output. A summary representation is also constructed by taking the sum of the previous RNN outputs weighted by their extraction probabilities. Extraction predictions are made using the RNN output at the i-th step, the document representation, and i-th version of the summary representation, along with factors for sentence location in the document. The use of the iteratively constructed summary representation creates a dependence of yi on all y<i. See Figure 1.d for a graphical layout.\nProposed Sentence Extractors We propose two sentence extractor models that make a stronger conditional independence assumption p(y|h) = ∏ni=1 p(yi|h), essentially making independent predictions conditioned on h.\nRNN Extractor Our first proposed model is a very simple bidirectional RNN based tagging model. As in the RNN sentence encoder we use a GRU cell. The forward and backward outputs of each sentence are passed through a multi-layer perceptron with a logsitic sigmoid output to predict the probability of extracting each sentence. See Figure 1.a for a graphical layout.\nSeq2Seq Extractor One shortcoming of the RNN extractor is that long range information from one end of the document may not easily be able to affect extraction probabilities of sentences at the other end. Our second proposed model, the Seq2Seq extractor mitigates this problem with an attention mechanism commonly used for neural machine translation (Bahdanau et al., 2014) and abstractive summarization (See et al., 2017). The sentence embeddings are first encoded by a bidirectional GRU. A separate decoder GRU transforms each sentence into a query vector which attends to the encoder output. The attention weighted encoder output and the decoder GRU output are concatenated and fed into a multi-layer perceptron to compute the extraction probability. See Figure 1.b for a graphical layout."
  }, {
    "heading": "4 Datasets",
    "text": "We perform our experiments across six corpora from varying domains to understand how differ-\n4Nallapati et al. (2017) use an RNN sentence encoder with this extractor architecture; in this work we pair the SummaRunner extractor with different encoders.\nent biases within each domain can affect content selection. The corpora come from the news domain (CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit), workplace meetings (AMI), and medical journal articles (PubMed). See Table 1 for dataset statistics.\nCNN-DailyMail We use the preprocessing and training, validation, and test splits of See et al. (2017). This corpus is a mix of news on different topics including politics, sports, and entertainment.\nNew York Times The New York Times (NYT) corpus (Sandhaus, 2008) contains two types of abstracts for a subset of its articles. The first summary is an archival abstract and the second is a shorter online teaser meant to entice a viewer of the webpage to click to read more. From this collection, we take all articles that have a concatenated summary length of at least 100 words. We create training, validation, and test splits by partitioning on dates; we use the year 2005 as the validation data, with training and test partitions including documents before and after 2005 respectively.\nDUC We use the single document summarization data from the 2001 and 2002 Document Understanding Conferences (DUC) (Over and Liggett, 2002). We split the 2001 data into training and validation splits and reserve the 2002 data for testing.\nAMI The AMI corpus (Carletta et al., 2005) is a collection of real and staged office meetings annotated with text transcriptions, along with abstractive summaries. We use the prescribed splits.\nReddit Ouyang et al. (2017) collected a corpus of personal stories shared on Reddit5 along with multiple extractive and abstractive summaries. We randomly split this data using roughly three and five percent of the data validation and test respectively.\nPubMed We created a corpus of 25,000 randomly sampled medical journal articles from the PubMed Open Access Subset6. We only included articles if they were at least 1000 words long and had an abstract of at least 50 words in length. We used the article abstracts as the ground truth human summaries."
  }, {
    "heading": "4.1 Ground Truth Extract Summaries",
    "text": "Since we do not typically have ground truth extract summaries from which to create the labels yi, we construct gold label sequences by greedily optimizing ROUGE-1, using the algorithm in Appendix C. We choose to optimize for ROUGE-1 rather than ROUGE-2 similarly to other optimization based approaches to summarization (Sipos et al., 2012; Durrett et al., 2016) which found this to be the easier target to learn.\n5www.reddit.com 6https://www.ncbi.nlm.nih.gov/pmc/\ntools/openftlist/"
  }, {
    "heading": "5 Experiments",
    "text": "We evaluate summary quality using ROUGE-2 recall (Lin, 2004); ROUGE-1 and ROUGE-LCS trend similarity in our experiments. We use target word lengths of 100 words for news, and 75, 290, and 200 for Reddit, AMI, and PubMed respectively. We also evaluate using METEOR (Denkowski and Lavie, 2014).7 Summaries are generated by extracting the top ranked sentences by model probability p(yi = 1|y<i, h), stopping when the word budget is met or exceeded. We estimate statistical significance by averaging each document level score over the five random initializations. We then test the difference between the best system on each dataset and all other systems using the approximate randomization test (Riezler and Maxwell, 2005) with the Bonferroni correction for multiple comparisons, testing for significance at the 0.05 level."
  }, {
    "heading": "5.1 Training",
    "text": "We train all models to minimize the weighted negative log-likelihood\nL = − ∑ s,y∈D h=enc(s) n∑ i=1 ω(yi) log p (yi|y<i, h)\n7We use the default settings for METEOR and use remove stopwords and no stemming options for ROUGE, keeping defaults for all other parameters.\nover the training data D using stochastic gradient descent with the ADAM optimizer (Kingma and Ba, 2014). ω(0) = 1 and ω(1) = N0/N1 where Ny is the number of training examples with label y. We trained for a maximum of 50 epochs and the best model was selected with early stopping on the validation set according to ROUGE-2. Each epoch constitutes a full pass through the dataset. The average stopping epoch was: CNN-DailyMail, 16.2; NYT, 21.36; DUC, 37.11; Reddit, 36.59; AMI, 19.58; PubMed, 19.84. All experiments were repeated with five random initializations. Unless specified, word embeddings were initialized using pretrained GloVe embeddings (Pennington et al., 2014) and we did not update them during training. Unknown words were mapped to a zero embedding. See Appendix D for more optimization and training details."
  }, {
    "heading": "5.2 Baselines",
    "text": "Lead As a baseline we include the lead summary, i.e. taking the first x words of the document as summary, where x is the target summary length for each dataset (see the first paragraph of § 5). While incredibly simple, this method is still a competitive baseline for single document summa-\nrization, especially on newswire.\nOracle To measure the performance ceiling, we show the ROUGE/METEOR scores using the extractive summary which results from greedily optimizing ROUGE-1. I.e., if we had clairvoyant knowledge of the human reference summary, the oracle system achieves the (approximate) maximum possible ROUGE scores. See Appendix C for a detailed description of the oracle algorithm."
  }, {
    "heading": "5.3 Results",
    "text": "The results of our main experiment comparing the different extractors/encoders are shown in Table 2. Overall, we find no major advantage when using the CNN and RNN sentence encoders over the averaging encoder. The best performing encoder/extractor pair either uses the averaging encoder (five out of six datasets) or the differences are not statistically significant.\nWhen looking at extractors, the Seq2Seq extractor is either part of the best performing system (three out of six datasets) or is not statistically distinguishable from the best extractor.\nOverall, on the news and medical journal domains, the differences are quite small with the dif-\nferences between worst and best systems on the CNN/DM dataset spanning only .56 of a ROUGE point. While there is more performance variability in the Reddit and AMI data, there is less distinction among systems: no differences are significant on Reddit and every extractor has at least one configuration that is indistinguishable from the best system on the AMI corpus. This is probably due to the small test size of these datasets.\nWord Embedding Learning Given that learning a sentence encoder (averaging has no learned parameters) does not yield significant improvement, it is natural to consider whether learning word embeddings is also necessary. In Table 3 we compare the performance of different extractors using the averaging encoder, when the word embeddings are held fixed or learned during training. In both cases, word embeddings are initialized with GloVe embeddings trained on a combination of Gigaword and Wikipedia. When learning embeddings, words occurring fewer than three times in the training data are mapped to an unknown token (with learned embedding).\nIn all but one case, fixed embeddings are as good or better than the learned embeddings. This is a somewhat surprising finding on the CNN/DM data since it is reasonably large, and learning embeddings should give the models more flexibility to identify important word features.8 This sug-\n8The AMI corpus is an exception here where learning\ngests that we cannot extract much generalizable learning signal from the content other than what is already present from initialization. Even on PubMed, where the language is quite different from the news/Wikipedia articles the GloVe embeddings were trained on, learning leads to significantly worse results.\nPOS Tag Ablation It is also not well explored what word features are being used by the encoders. To understand which classes of words were most important we ran an ablation study, selectively removing nouns, verbs (including participles and auxiliaries), adjectives & adverbs, and function words (adpositions, determiners, conjunctions). All datasets were automatically tagged using the spaCy part-of-speech (POS) tagger9. The embeddings of removed words were replaced with a zero vector, preserving the order and position of the non-ablated words in the sentence. Ablations were performed on training, validation, and test partitions, using the RNN extractor with averaging encoder. Table 4 shows the results of the POS tag ablation experiments. While removing any word class from the representation generally hurts performance (with statistical significance), on the news domains, the absolute values of the\ndoes lead to small performance boosts, however, only in the Seq2Seq extractor is this diference significant; it is quite possible that this is an artifact of the very small test set size.\n9https://github.com/explosion/spaCy\ndifferences are quite small (.18 on CNN/DM, .41 on NYT, .3 on DUC) suggesting that the model’s predictions are not overly dependent on any particular word types. On the non-news datasets, the ablations have a larger effect (max differences are 1.89 on Reddit, 2.56 on AMI, and 1.3 on PubMed). Removing nouns leads to the largest drop on AMI and PubMed. Removing adjectives and adverbs leads to the largest drop on Reddit, suggesting the intensifiers and descriptive words are useful for identifying important content in personal narratives. Curiously, removing the function word POS class yields a significant improvement on DUC 2002 and AMI.\nDocument Shuffling Sentence position is a well known and powerful feature for news summarization (Hong and Nenkova, 2014), owing to the intentional lead bias in the news article writing10; it also explains the difficulty in beating the lead baseline for single-document summarization (Nenkova, 2005; Brandow et al., 1999). In examining the generated summaries, we found most of the selected sentences in the news domain came from the lead paragraph of the document. This is despite the fact that there is a long tail of sentence extractions from later in the document in the ground truth extract summaries (31%, 28.3%, and 11.4% of DUC, CNN/DM, and NYT training extract labels come from the second half of the document). Because this lead bias is so strong, it is questionable whether the models are learning to identify important content or just find the start of the document. We conduct a sentence order experiment where each document’s sentences are randomly shuffled during training. We then evaluate each model performance on the unshuffled test data, comparing to the model trained on unshuffled data; if the models trained on shuffled data drop in performance, then this indicates the lead bias is the relevant factor.\nTable 5 shows the results of the shuffling experiments. The news domains and PubMed suffer a significant drop in performance when the document order is shuffled. By comparison, there is no significant difference between the shuffled and inorder models on the Reddit domain, and shuffling actually improves performance on AMI. This suggest that position is being learned by the models in the news/journal article domain even when the\n10https://en.wikipedia.org/wiki/ Inverted_pyramid_(journalism)\nmodel has no explicit position features, and that this feature is more important than either content or function words."
  }, {
    "heading": "6 Discussion",
    "text": "Learning content selection for summarization in the news domain is severely inhibited by the lead bias. The summaries generated by all systems described here–the prior work and our proposed simplified models–are highly similar to each other and to the lead baseline. The Cheng & Lapata and Seq2Seq extractors (using the averaging encoder) share 87.8% of output sentences on average on the CNN/DM data, with similar numbers for the other news domains (see Table 6 for a typical example). Also on CNN/DM, 58% of the Seq2Seq selected sentences also occur in the lead summary, with similar numbers for DUC, NYT, and Reddit. Shuffling reduces lead overlap to 35.2% but the overall system performance drops significantly; the models are not able to identify important information without position.\nThe relative robustness of the news domain to part of speech ablation also suggests that models are mostly learning to recognize the stylistic features unique to the beginning of the article, and not the content. Additionally, the drop in performance when learning word embeddings on the news domain suggests that word embeddings alone do not provide very generalizable content features compared to recognizing the lead.\nThe picture is rosier for non-news summarization where part of speech ablation leads to larger performance differences and shuffling either does not inhibit content selection significantly or leads to modest gains. Learning better word-level representations on these domains will likely require much larger corpora, something which might remain unlikely for personal stories and meetings.\nThe lack of distinction among sentence encoders is interesting because it echoes findings in the generic sentence embedding literature where word embedding averaging is frustratingly difficult to outperform (Iyyer et al., 2015; Wieting et al., 2015; Arora et al., 2016; Wieting and Gimpel, 2017). The inability to learn useful sentence representations is also borne out in the SummaRunner model, where there are explicit similarity computations between document or summary representations and sentence embeddings; these computations do not seem to add much to the per-\nformance as the Cheng & Lapata and Seq2Seq models which lack these features generally perform as well or better. Furthermore, the Cheng & Lapata and SummaRunner extractors both construct a history of previous selection decisions to inform future choices but this does not seem to significantly improve performance over the Seq2Seq extractor (which does not). This suggests that we need to rethink or find novel forms of sentence representation for the summarization task.\nA manual examination of the outputs revealed some interesting failure modes, although in general it was hard to discern clear patterns of behaviour other than lead bias. On the news domain, the models consistently learned to ignore quoted material in the lead, as often the quotes provide color to the story but are unlikely to be included in the summary (e.g. “It was like somebody slugging a punching bag.”). This behavior was most likely triggered by the presence of quotes, as the quote attributions, which were often tokenized as separate sentences, would subsequently be included in the summary despite also not containing much information (e.g. Gil Clark of the National Hurricane Center said Thursday)."
  }, {
    "heading": "7 Conclusion",
    "text": "We have presented an empirical study of deep learning based content selection algorithms for summarization. Our findings suggest such models face stark limitations on their ability to learn robust features for this task and that more work is needed on sentence representation for summarization."
  }, {
    "heading": "8 Acknowledgements",
    "text": "The authors would like to thank the anonymous reviewers for their valuable feedback. Thanks goes out as well to Chris Hidey for his helpful comments. We would also like to thank Wen Xiao for identifying an error in the oracle results for the AMI corpus, which as since been corrected.\nThis research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA865017-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
  }, {
    "heading": "A Details on Sentence Encoders",
    "text": "We use 200 dimenional word embeddingswi in all models. Dropout is applied to the embeddings during training. Wherever dropout is applied, the drop probability is .25.\nA.1 Details on RNN Encoder Under the RNN encoder, a sentence embedding is defined as h = [ −→ h |s|; ←− h 1] where\n−→ h 0 = 0; −→ h i = −−−→ GRU(wi, −→ h i−1) (1)\n←− h |s|+1 = 0; ←− h i = ←−−− GRU(wi, ←− h i+1), (2)\nand −−−→ GRU amd ←−−− GRU indicate the forward and backward GRUs respectively, each with separate parameters. We use 300 dimensional hidden layers for each GRU. Dropout is applied to GRU during training.\nA.2 Details on CNN Encoder\nThe CNN encoder has hyperparameters associated with the window sizes K ⊂ N of the convolutional filters (i.e. the number of words associated with each convolution) and the number of feature maps Mk ∈ N associated with each filter (i.e. the output dimension of each convolution). The CNN sentence embedding h is computed as follows:\na (m,k) i = b (m,k) + k∑ j=1 W (m,k) j · wi+j−1 (3)\nh(m,k) = max i∈1,...,|s|−k+1\nReLU ( a (m,k) i ) (4)\nh = [ h(m,k)|m ∈ {1, . . . ,Mk}, k ∈ K ] (5)\nwhere b(m,k) ∈ R and W (m,k) ∈ Rk×n′ are learned bias and filter weight parameters respectively, and ReLU(x) = max(0, x) is the rectified linear unit activation. We use window sizes K = {1, 2, 3, 4, 5, 6} with corresponding feature maps sizes M1 = 25,M2 = 25,M3 = 50,M4 = 50,M5 = 50,M6 = 50, giving h a dimensionality of 250. Dropout is applied to the CNN output during training."
  }, {
    "heading": "B Details on Sentence Extractors",
    "text": "B.1 Details on RNN Extractor\n−→z 0 = 0; −→z i = −−−→ GRU(hi, −→z i−1) (6) ←−z n+1 = 0; ←−z i = ←−−− GRU(hi,\n←−z i+1) (7) ai = ReLU (U · [−→z i;←−z i] + u) (8)\np(yi = 1|h) = σ (V · ai + v) (9)\nwhere −−−→ GRU and ←−−− GRU indicate the forward and backward GRUs respectively, and each have separate learned parameters; U, V and u, v are learned weight and bias parameters. The hidden layer size of the GRU is 300 for each direction and the MLP hidden layer size is 100. Dropout is applied to the GRUs and to ai.\nB.2 Details on Seq2Seq Extrator\n−→z 0 = 0; −→z i = −−−→ GRUenc(hi, −→z i−1) (10) ←−z n+1 = 0; ←−z i = ←−−− GRUenc(hi,\n←−z i+1) (11) −→q i = −−−→ GRUdec(hi,\n−→q i−1) (12) ←−q i = ←−−− GRUdec(hi, ←−q i+1) (13)\nqi = [ −→q i;←−q i], zi = [−→z i;←−z i] (14)\nαi,j = exp (qi · zj)∑n j=1 exp (qi · zj) , z̄i = n∑ j=1 αi,jzj (15)\nai = ReLU (U · [z̄i; qi] + u) (16) p(yi = 1|h) = σ (V · ai + v) . (17)\nThe final outputs of each encoder direction are passed to the first decoder steps; additionally, the first step of the decoder GRUs are learned “begin decoding” vectors −→q 0 and←−q 0 (see Figure 1.b). Each GRU has separate learned parameters; U, V and u, v are learned weight and bias parameters. The hidden layer size of the GRU is 300 for each direction and MLP hidden layer size is 100. Dropout with drop probability .25 is applied to the GRU outputs and to ai.\nB.3 Details on Cheng & Lapata Extractor. The basic architecture is a unidirectional sequence-to-sequence model defined as follows:\nz0 = 0; zi = GRUenc(hi, zi−1) (18) q1 = GRUdec(h∗, zn) (19)\nqi = GRUdec(pi−1 · hi−1, qi−1) (20) ai = ReLU (U · [zi; qi] + u) (21)\npi = p(yi = 1|y<i, h) = σ (V · ai + v) (22)\nwhere h∗ is a learned “begin decoding” sentence embedding (see Figure 1.c). Each GRU has separate learned parameters; U, V and u, v are learned weight and bias parameters. Note in Equation 20 that the decoder side GRU input is the sentence embedding from the previous time step weighted by its probabilitiy of extraction (pi−1) from the previous step, inducing dependence of each output yi on all previous outputs y<i. The hidden layer size of the GRU is 300 and the MLP hidden layer size is 100. Dropout with drop probability .25 is applied to the GRU outputs and to ai.\nNote that in the original paper, the Cheng & Lapata extractor was paired with a CNN sentence encoder, but in this work we experiment with a variety of sentence encoders.\nB.4 Details on SummaRunner Extractor. Like the RNN extractor it starts with a bidrectional GRU over the sentence embeddings\n−→z 0 = 0; −→z i = −−−→ GRU(hi, −→z i−1) (23) ←−z n+1 = 0; ←−z i = ←−−− GRU(hi, ←−z i+1), (24)\nIt then creates a representation of the whole document q by passing the averaged GRU output states through a fully connected layer:\nq = tanh ( bq +Wq 1\nn n∑ i=1\n[−→z i;←−z i] )\n(25)\nA concatentation of the GRU outputs at each step are passed through a separate fully connected layer to create a sentence representation zi, where\nzi = ReLU (bz +Wz[ −→z i;←−z i]) . (26)\nThe extraction probability is then determined by contributions from five sources:\ncontent a(con)i = W (con)zi, (27)\nsalience a(sal)i = z T i W (sal)q, (28)\nnovelty a(nov)i = −zTi W (nov) tanh(gi), (29) position a(pos)i = W (pos)li, (30) quartile a(qrt)i = W (qrt)ri, (31)\nwhere li and ri are embeddings associated with the i-th sentence position and the quarter of the document containing sentence i respectively. In Equation 29, gi is an iterative summary representation computed as the sum of the previous z<i weighted by their extraction probabilities,\ngi = i−1∑ j=1 p(yj = 1|y<j , h) · zj . (32)\nNote that the presence of this term induces dependence of each yi to all y<i similarly to the Cheng & Lapata extractor.\nThe final extraction probability is the logistic sigmoid of the sum of these terms plus a bias,\np(yi = 1|y<i, h) = σ ( a (con) i + a (sal) i + a (nov) i\n+a (pos) i + a (qrt) i + b\n) . (33)\nThe weight matrices Wq, Wz , W (con), W (sal), W (nov), W (pos), W (qrt) and bias terms bq, bz , and b are learned parameters; The GRUs have separate learned parameters. The hidden layer size of the GRU is 300 for each direction zi, q, and gi have 100 dimensions. The position and quartile embeddings are 16 dimensional each. Dropout with drop probability .25 is applied to the GRU outputs and to zi.\nNote that in the original paper, the SummaRunner extractor was paired with an RNN sentence encoder, but in this work we experiment with a variety of sentence encoders."
  }, {
    "heading": "C Ground Truth Extract Summary Algorithm",
    "text": "Algorithm 1: ORACLEEXTRACTSUMMARYLABELS Data: input document sentences s1, s2, . . . , sn,\nhuman reference summary R, summary word budget c.\n1 yi := 0 ∀i ∈ 1, . . . , n // Initialize extract labels to be 0. 2 S := [ ] // Initialize summary as empty list. 3 while ∑ s∈S WORDCOUNT(s) ≤ c do // While summary word count ≤ word budget. 4\n/* Add the next best sentence to the summary if it will improve the ROUGE\nscore otherwise no improvement can be made so break. */\n5\n6 î = arg maxi∈{1,...,n}, yi 6=1 ROUGE(S + [si], R) 7 8 if ROUGE(S + [sî], R) > ROUGE(S,R) then 9 S := S + [sî] // Add sî to the summary sentence list. 10 yî := 1 // Set the î-th extract label to indicate extraction. 11 else 12 break\nResult: extract summary labels y1, . . . , yn"
  }, {
    "heading": "D Optimizer and initialization settings.",
    "text": "We use a learning rate of .0001 and a dropout rate of .25 for all dropout layers. We also employ gradient clipping (−5 < ∇θ < 5). Weight matrix parameters are initialized using Xavier initialization with the normal distribution (Glorot and Bengio, 2010) and bias terms are set to 0. We use a batch size of 32 for all datasets except AMI and PubMed, which are often longer and consume more memory, for which we use sizes two and four respectively. For the Cheng & Lapata model, we train for half of the maximum epochs with teacher forcing, i.e. we set pi = 1 if yi = 1 in the gold data and 0 otherwise when computing the decoder input pi · hi; we revert to the predicted model probability during the second half training."
  }],
  "year": 2019,
  "references": [{
    "title": "A simple but tough-to-beat baseline for sentence embeddings",
    "authors": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"],
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Jointly learning to extract and compress",
    "authors": ["Taylor Berg-Kirkpatrick", "Dan Gillick", "Dan Klein."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 481–490. As-",
    "year": 2011
  }, {
    "title": "Automatic condensation of electronic publications by sentence selection",
    "authors": ["Ronald Brandow", "Karl Mitze", "Lisa Rau."],
    "venue": "Jan Fagerberg, David C. Mowery, and Richard R. Nelson, editors, Advances in Automatic Text Summarization, chapter 19, pages",
    "year": 1999
  }, {
    "title": "The ami meeting corpus: A pre-announcement",
    "authors": ["Jean Carletta", "Simone Ashby", "Sebastien Bourban", "Mike Flynn", "Mael Guillemot", "Thomas Hain", "Jaroslav Kadlec", "Vasilis Karaiskos", "Wessel Kraaij", "Melissa Kronenthal"],
    "year": 2005
  }, {
    "title": "Neural summarization by extracting sentences and words",
    "authors": ["Jianpeng Cheng", "Mirella Lapata."],
    "venue": "arXiv preprint arXiv:1603.07252.",
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1412.3555.",
    "year": 2014
  }, {
    "title": "Text summarization via hidden markov models",
    "authors": ["John M Conroy", "Dianne P O’Leary"],
    "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,",
    "year": 2001
  }, {
    "title": "Meteor universal: Language specific translation evaluation for any target language",
    "authors": ["Michael Denkowski", "Alon Lavie."],
    "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.",
    "year": 2014
  }, {
    "title": "Learning-based single-document summarization with compression and anaphoricity constraints",
    "authors": ["Greg Durrett", "Taylor Berg-Kirkpatrick", "Dan Klein."],
    "venue": "arXiv preprint arXiv:1603.08887.",
    "year": 2016
  }, {
    "title": "Customization in a unified",
    "authors": ["Noemie Elhadad", "M-Y Kan", "Judith L Klavans", "KR McKeown"],
    "year": 2005
  }, {
    "title": "A skip-chain conditional random field for ranking meeting utterances by importance",
    "authors": ["Michel Galley."],
    "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 364–372. Association for Computational",
    "year": 2006
  }, {
    "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
    "authors": ["Albert Gatt", "Emiel Krahmer."],
    "venue": "Journal of Artificial Intelligence Research, 61:65–170.",
    "year": 2018
  }, {
    "title": "A global optimization framework for meeting summarization",
    "authors": ["Dan Gillick", "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur."],
    "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, pages",
    "year": 2009
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256.",
    "year": 2010
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Advances in Neural Information Processing Systems (NIPS).",
    "year": 2015
  }, {
    "title": "Improving the estimation of word importance for news multidocument summarization",
    "authors": ["Kai Hong", "Ani Nenkova."],
    "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–",
    "year": 2014
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and",
    "year": 2015
  }, {
    "title": "Insights from cl-scisumm 2016: the faceted scientific document summarization shared task",
    "authors": ["Kokil Jaidka", "Muthu Kumar Chandrasekaran", "Sajal Rustagi", "Min-Yen Kan."],
    "venue": "International Journal on Digital Libraries, pages 1–9.",
    "year": 2017
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "A trainable document summarizer",
    "authors": ["Julian Kupiec", "Jan Pedersen", "Francine Chen."],
    "venue": "Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 68–73. ACM.",
    "year": 1995
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out.",
    "year": 2004
  }, {
    "title": "Text summarization in the biomedical domain: a systematic review of recent research",
    "authors": ["Rashmi Mishra", "Jiantao Bian", "Marcelo Fiszman", "Charlene R Weir", "Siddhartha Jonnalagadda", "Javed Mostafa", "Guilherme Del Fiol."],
    "venue": "Journal of biomedical in-",
    "year": 2014
  }, {
    "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
    "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."],
    "venue": "AAAI, pages 3075–3081.",
    "year": 2017
  }, {
    "title": "Automatic text summarization of newswire: Lessons learned from the document understanding conference",
    "authors": ["Ani Nenkova."],
    "venue": "AAAI, volume 5, pages 1436–1441.",
    "year": 2005
  }, {
    "title": "Crowd-sourced iterative annotation for narrative summarization corpora",
    "authors": ["Jessica Ouyang", "Serina Chang", "Kathy McKeown."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2,",
    "year": 2017
  }, {
    "title": "Introduction to duc: An intrinsic evaluation of generic news text summarization systems",
    "authors": ["Paul Over", "Walter Liggett."],
    "venue": "Proc. DUC. http://wwwnlpir. nist. gov/projects/duc/guidelines/2002. html.",
    "year": 2002
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "On some pitfalls in automatic evaluation and significance testing for mt",
    "authors": ["Stefan Riezler", "John T Maxwell."],
    "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 57–",
    "year": 2005
  }, {
    "title": "The new york times annotated corpus",
    "authors": ["Evan Sandhaus."],
    "venue": "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
    "year": 2008
  }, {
    "title": "Get to the point: Summarization with pointer-generator networks",
    "authors": ["Abigail See", "Peter J Liu", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1704.04368.",
    "year": 2017
  }, {
    "title": "Large-margin learning of submodular summarization models",
    "authors": ["Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims."],
    "venue": "Proceedings of the",
    "year": 2012
  }, {
    "title": "Towards universal paraphrastic sentence embeddings",
    "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "arXiv preprint arXiv:1511.08198.",
    "year": 2015
  }, {
    "title": "Revisiting recurrent networks for paraphrastic sentence embeddings",
    "authors": ["John Wieting", "Kevin Gimpel."],
    "venue": "arXiv preprint arXiv:1705.00364.",
    "year": 2017
  }, {
    "title": "Graph-based neural multi-document summarization",
    "authors": ["Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev."],
    "venue": "arXiv preprint arXiv:1706.06681.",
    "year": 2017
  }, {
    "title": "Weight matrix parameters are initialized using Xavier initialization with the normal distribution (Glorot and Bengio, 2010) and bias terms are set to 0. We use a batch size of 32 for all datasets except AMI and PubMed, which are often longer and consume more memory, for which we use sizes two and four respectively. For the Cheng & Lapata model, we train for half of the maximum epochs with teacher forcing, i.e. we set pi = 1 if yi = 1 in the gold data and 0 otherwise when computing",
    "authors": ["∇θ"],
    "year": 2010
  }],
  "id": "SP:95b3cfb7726bfae3eeb0b9dfa28b6021a0408478",
  "authors": [{
    "name": "Chris Kedzie",
    "affiliations": []
  }, {
    "name": "Kathleen McKeown",
    "affiliations": []
  }],
  "abstractText": "We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.",
  "title": "Content Selection in Deep Learning Models of Summarization"
}