{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In supervised classification, we need a vast amount of labeled data in the training phase. However, in many realworld problems, it is time-consuming and laborious to label a huge amount of unlabeled data. To deal with this problem, weakly-supervised classification (Zhou, 2018) has been explored in various setups, including semi-supervised classification (Chapelle & Zien, 2005; Belkin et al., 2006; Chapelle et al., 2010; Miyato et al., 2016; Laine & Aila, 2017; Sakai et al., 2017; Tarvainen & Valpola, 2017; Luo et al., 2018), multiple instance classification (Li & Vasconcelos, 2015; Miech et al., 2017; Bao et al., 2018), and positive-unlabeled (PU) classification (Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Niu et al., 2016; Kiryo et al., 2017).\nAnother line of research from the clustering viewpoint is semi-supervised clustering, where pairwise similarity and dissimilarity data (a.k.a. must-link and cannot-link constraints) are utilized to guide unsupervised clustering to a desired solution. The common approaches are (i) constrained clustering (Wagstaff et al., 2001; Basu et al., 2002;\n1The University of Tokyo, Japan 2RIKEN, Japan. Correspondence to: Han Bao <tsutsumi@ms.k.u-tokyo.ac.jp>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\naDiscriminative clustering methods are designed for out-ofsample inference, such as maximum margin clustering (Xu et al., 2005) and information maximization clustering (Krause et al., 2010; Sugiyama et al., 2011).\n2004; Li & Liu, 2009), which utilize pairwise links as constraints on clustering. (ii) metric learning (Xing et al., 2002; Bilenko et al., 2004; Weinberger et al., 2005; Davis et al., 2007; Li et al., 2008; Niu et al., 2012), which perform (k-means) clustering on learned metrics (iii) matrix completion (Yi et al., 2013; Chiang et al., 2015), which recover unknown entries in a similarity matrix.\nSemi-supervised clustering and weakly-supervised classification are similar in that they do not use fully-supervised data. However, they are different from the learning theoretic viewpoint—weakly-supervised classification methods are justified as supervised learning methods, while semi-supervised clustering methods are still evaluated as unsupervised learning (see Table 1). Indeed, weaklysupervised learning methods based on empirical risk minimization (du Plessis et al., 2014; 2015; Niu et al., 2016; Sakai et al., 2017) were shown that their estimation errors achieve the optimal parametric convergence rate, while such generalization guarantee is not available for semi-supervised\nclustering methods.\nThe goal of this paper is to propose a novel weaklysupervised learning method called SU classification, where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are employed, in order to bridge these two different paradigms. In SU classification, the information available for training a classifier is similar to semi-supervised clustering. However, our proposed method gives an inductive model, which learns decision functions from training data and can be applied for out-of-sample prediction (i.e., prediction of unseen test data). Furthermore, the proposed method can not only separate two classes but also identify which class is positive (class identification) under certain conditions.\nSU classification is particularly useful to predict people’s sensitive matters such as religion, politics, and opinions on racial issues—people often hesitate to give explicit answers to these matters, instead indirect questions might be easier to answer: “Which person do you have the same belief as?”1\nFor this SU classification problem, our contributions in this paper are three-fold:\n1. We propose an empirical risk minimization method for SU classification (Section 2). This enables us to obtain an inductive classifier. Under certain loss conditions together with the linear-in-parameter model, its objective function becomes even convex in the parameters.\n2. We theoretically establish an estimation error bound for our SU classification method (Section 4), showing that the proposed method achieves the optimal parametric convergence rate.\n3. We experimentally demonstrate the practical usefulness of the proposed SU classification method (Section 5).\nRelated problem settings are summarized in Figure 1."
  }, {
    "heading": "2. Classification from Pairwise Similarity and Unlabeled Data",
    "text": "In this section, we propose a learning method to train a classifier from pairwise similarity and unlabeled data."
  }, {
    "heading": "2.1. Preliminaries",
    "text": "We formulate the standard binary classification problem briefly. Let X ⇢ Rd be a d-dimensional example space and Y = {+1, 1} be a binary label space. We assume that labeled data (x, y) 2 X ⇥ Y is drawn from the joint\n1 This questioning can be regarded as one type of randomized response (indirect questioning) techniques (Warner, 1965; Fisher, 1993), which is a survey method to avoid social desirability bias.\nprobability distribution with density p(x, y). The goal of binary classification is to obtain a classifier f : X ! R which minimizes the classification risk defined as\nR(f) , E (X,Y )⇠p [`(f(X), Y )] , (1)\nwhere E(X,Y )⇠p[·] denotes the expectation over the joint distribution p(X,Y ) and ` : R⇥Y ! R+ is a loss function. The loss function `(z, t) measures how well the true class label t 2 Y is estimated by an output of a classifier z 2 R, generally yielding a small/large value if t is well/poorly estimated by z.\nIn standard supervised classification scenarios, we are given positive and negative training data independently following p(x, y). Then, based on these training data, the classification risk (1) is empirically approximated and the empirical risk minimizer is obtained. However, in many real-world problems, collecting labeled training data is costly. The goal of this paper is to train a binary classifier only from pairwise similarity and unlabeled data, which are cheaper to collect than fully labeled data."
  }, {
    "heading": "2.2. Pairwise Similarity and Unlabeled Data",
    "text": "First, we discuss underlying distributions of similar data pairs and unlabeled data points, in order to perform the empirical risk minimization.\nPairwise Similarity: If x and x0 belong to the same class, they are said to be pairwise similar (S). We assume that similar data pairs are drawn following\npS(x,x 0 ) = p(x,x0|y = y0 = +1 _ y = y0 = 1)\n=\n⇡2+p+(x)p+(x 0 ) + ⇡2 p (x)p (x0)\n⇡2+ + ⇡ 2\n, (2)\nwhere ⇡+ , p(y = +1) and ⇡ , p(y = 1) are the class-prior probabilities satisfying ⇡+ + ⇡ = 1, and p+(x) , p(x|y = +1) and p (x) , p(x|y = 1) are the class-conditional densities. Eq. (2) means that we draw two labeled data independently following p(x, y), and we accept/reject them if they belong to the same class/different classes.\nUnlabeled Data: We assume that unlabeled (U) data points are drawn following the marginal density p(x), which can be decomposed into the sum of the class-conditional densities as\np(x) = ⇡+p+(x) + ⇡ p (x). (3)\nOur goal is to train a classifier only from SU data, which we call SU classification. We assume that we have similar pairs DS and an unlabeled dataset DU as\nDS , {(xS,i,x0S,i)}nSi=1 i.i.d.⇠ pS(x,x0),\nDU , {xU,i}nUi=1 i.i.d.⇠ p(x).\nWe also use a notation eDS , {exS,i}2nSi=1 to denote pointwise similar data obtained by ignoring pairwise relations in DS. Lemma 1. eDS = {exS,i}2nSi=1 are independently drawn following\nepS(x) = ⇡2+p+(x) + ⇡ 2 p (x) ⇡S , (4)\nwhere ⇡S , ⇡2+ + ⇡2 .\nA proof is given in Appendix A.\nLemma 1 states that a similar data pair (xS,x0S) is essentially symmetric, and xS,x0S can be regarded as being independently drawn following epS, if we assume the pair (xS,x0S) is drawn following pS. This perspective is important when we analyze the variance of the risk estimator (Section 2.4), and estimate the class-prior (Section 3.2)."
  }, {
    "heading": "2.3. Risk Expression with SU Data",
    "text": "Below, we attempt to express the classification risk (1) only in terms of SU data. Assume ⇡+ 6= 12 , and let è(z), LS,`(z) and LU,`(z) be\nè (z) , `(z,+1) `(z, 1),\nLS,`(z) , 1 2⇡+ 1 è (z), LU,`(z) , ⇡\n2⇡+ 1 `(z,+1) + ⇡+ 2⇡+ 1 `(z, 1).\nThen we have the following theorem. Theorem 1. The classification risk (1) can be equivalently expressed as\nRSU,`(f) = ⇡S E (X,X0)⇠pS\n LS,`(f(X)) + LS,`(f(X 0))\n2\n+ E X⇠p [LU,`(f(X))] .\nA proof is given in Appendix B.\nAccording to Theorem 1, the following is a natural candidate for an unbiased estimator of the classification risk (1):\nbRSU,`(f)\n= ⇡S nS\nnSX\ni=1\nLS,`(f(xS,i)) + LS,`(f(x0S,i)) 2\n+\n1\nnU\nnUX\ni=1\nLU,`(f(xU,i))\n= ⇡S 2nS\n2nSX\ni=1\nLS,`(f(exS,i)) + 1\nnU\nnUX\ni=1\nLU,`(f(xU,i)),\n(5)\nwhere in the last line we use the decomposed version of similar pairs eDS instead of DS, since the loss form is symmetric.\nLS,` and LU,` are illustrated in Figure 2."
  }, {
    "heading": "2.4. Minimum-Variance Risk Estimator",
    "text": "Eq. (5) is one of the candidates of an unbiased SU risk estimator. Indeed, due to the symmetry of (x,x0) ⇠ pS(x,x0), we have the following lemma. Lemma 2. The first term of RSU,`(f), i.e.,\n⇡S E (X,X0)⇠pS\n LS,`(f(X)) + LS,`(f(X 0))\n2\n, (6)\ncan be equivalently expressed as\n⇡S E (X,X0)⇠pS\n[↵LS,`(f(X)) + (1 ↵)LS,`(f(X 0))] ,\nwhere ↵ 2 [0, 1] is an arbitrary weight.\nA proof is given in Appendix C.1. By Lemma 2,\n⇡S nS\nnSX\ni=1\n↵LS,`(f(xS,i)) + (1 ↵)LS,`(f(x0S,i))\n(7)\nis also an unbiased estimator of Eq. (6). Then, a natural question arises: is the risk estimator (5) best among all ↵? We answer this question by the following theorem.\nTheorem 2. The estimator\n⇡S nS\nnSX\ni=1\nLS,`(f(xS,i)) + LS,`(f(x0S,i)) 2\n(8)\nhas the minimum variance among estimators in the form Eq. (7) with respect to ↵ 2 [0, 1].\nA proof is given in Appendix C.2.\nThus, the variance minimality (with respect to ↵ in Eq. (7)) of the risk estimator (5) is guaranteed by Theorem 2. We use this risk estimator in the following sections."
  }, {
    "heading": "2.5. Practical Implementation",
    "text": "Here, we investigate the objective function when the linearin-parameter model f(x) = w> (x)+w0 is employed as a classifier, where w 2 Rd and w0 2 R are parameters and : Rd ! Rb is basis functions. In general, the bias parameter w0 can be ignored 2. We formulate SU classification as the following empirical risk minimization problem using Eq. (5) together with the `2 regularization:\nbw = min w\nbJ`(w), (9)\nwhere\nbJ`(w) , ⇡S 2nS\n2nSX\ni=1\nLS,`(w> (exS,i))\n+\n1\nnU\nnUX\ni=1\nLU,`(w> (xU,i)) +\n2\nkwk2,\n(10)\n2 Let e (x) , [ (x)> 1]> and ew , [w> w0]> then w > (x) + w0 = ew> e (x).\nand > 0 is the regularization parameter. We need the class-prior ⇡+ (included in ⇡S) to solve this optimization problem. We discuss how to estimate ⇡+ in Section 3.2.\nNext, we will investigate appropriate choices of the loss function `. From now on, we focus on margin loss functions (Mohri et al., 2012): ` is said to be a margin loss function if there exists : R! R+ such that `(z, t) = (tz). In general, our objective function (10) is non-convex even if a convex loss function is used for ` 3. However, the next theorem, inspired by Natarajan et al. (2013) and du Plessis et al. (2015), states that a certain loss function will result in a convex objective function. Theorem 3. If the loss function `(z, t) is a convex margin loss, twice differentiable in z almost everywhere (for every fixed t 2 {±1}), and satisfies the condition\n`(z,+1) `(z, 1) = z,\nthen bJ`(w) is convex. A proof of Theorem 3 is given in Appendix D.\nExamples of margin loss functions satisfying the conditions in Theorem 3 are shown in Table 2 (also illustrated in Figure 3). Below, as special cases, we show the objective functions for the squared and the double-hinge losses. The detailed derivations are given in Appendix E.\nSquared Loss: The squared loss is `SQ(z, t) = 14 (tz 1)2. Substituting `SQ into Eq. (10), the objective function is bJSQ(w) = w> ✓ 1\n4nU X>UXU + 2 I\n◆ w\n+\n1\n2⇡+ 1 ✓ ⇡S 2nS 1>XS + 1 2nU 1>XU ◆ w,\n3 In general, LU,` is non-convex because either ⇡ 2⇡+ 1 `(·,+1) or ⇡+ 2⇡+ 1\n`(·, 1) is convex and the other is concave. LS,` is not always convex even if ` is convex, either.\nwhere 1 is the vector whose elements are all ones, I is the identity matrix, XS , [ (exS,1) · · · (exS,2nS)]>, and XU , [ (xU,1) · · · (xU,nU)]>. The minimizer of this objective function can be obtained analytically as\nw = nU\n2⇡+ 1\n· X>UXU + 2 nUI 1 ✓ ⇡S nS X>S 1 1 nU X>U1 ◆ .\nThus the optimization problem can be easily implemented and solved highly efficiently if the number of basis functions is not so large.\nDouble-Hinge Loss: Since the hinge loss `H(z, t) = max(0, 1 tz) does not satisfy the conditions in Theorem 3, the double-hinge loss `DH(z, t) = max( tz,max(0, 12 1 2 tz)) is proposed by du Plessis et al. (2015) as an alternative. Substituting `DH into Eq. (10), we can reformulate the optimization problem as follows:\nmin w,⇠,⌘ ⇡S 2nS(2⇡+ 1) 1>XSw ⇡ nS(2⇡+ 1) 1>⇠\n+ ⇡+ nU(2⇡+ 1) 1>⌘ + 2 w>w\ns.t. ⇠ 0, ⇠ 1 2 1+ 1 2 XUw, ⇠ XUw,\n⌘ 0, ⌘ 1 2 1 1 2 XUw, ⌘ XUw,\nwhere for vectors denotes the element-wise inequality. This optimization problem is a quadratic program (QP). The transformation into the standard QP form is given in Appendix E."
  }, {
    "heading": "3. Relation between Class-Prior and SU Classification",
    "text": "In Section 2, we assume that the class-prior ⇡+ is given in advance. In this section, we first clarify the relation between behaviors of the proposed method and ⇡+, then we propose an algorithm to estimate ⇡+ in case we do not have ⇡+ in advance."
  }, {
    "heading": "3.1. Class-Prior-Dependent Behaviors of Proposed Method",
    "text": "We discuss the following three different cases on prior knowledge of ⇡+ (summarized in Table 3).\n(Case 1) The class-prior is given: In this case, we can directly solve the optimization problem (9). The solution does not only separate data but also identifies classes, i.e., determine which class is positive.\n(Case 2) No prior knowledge on the class-prior is given: In this case, we need to estimate ⇡+ before solving (9). If we assume ⇡+ > ⇡ , the estimation method in Section 3.2 gives an estimator of ⇡+. Thus, we can regard the larger cluster as positive class and solve the optimization problem (9). This time the solution just separates data because we have no prior information for class identifiability.\n(Case 3) Magnitude relation of the class-prior is given: Finally, consider the case where we know which class has a larger class-prior. In this case, we also need to estimate ⇡+, but surprisingly, we can identify classes. For example, if the negative class has a larger class-prior, first we estimate the class-prior (let b⇡ be an estimated value). Since Algorithm 1 given in Sec. 3.2 always gives an estimate of the classprior of the larger class, the positive class-prior is given as ⇡+ = 1 b⇡. After that, it reduces to Case 1. Remark: In all of the three cases above, our proposed method gives an inductive model, which is applicable to out-of-sample prediction without any modification. On the other hand, most of the unsupervised/semi-supervised clustering methods are designed for in-sample prediction, which can only give predictions for data at hand given in advance."
  }, {
    "heading": "3.2. Class-Prior Estimation from Pairwise Similarity and Unlabeled Data",
    "text": "We propose a class-prior estimation algorithm only from SU data. First, let us begin with connecting the pairwise marginal distribution p(x,x0) and pS(x,x0) when two examples x and x0 are drawn independently:\np(x,x0) = p(x)p(x0)\n= ⇡2+p+(x)p+(x 0 ) + ⇡2 p (x)p (x 0 )\n⇡+⇡ p+(x)p (x0) + ⇡+⇡ p (x)p+(x0) = ⇡SpS(x,x 0 ) + ⇡DpD(x,x 0 ), (11)\nAlgorithm 1 Prior estimation from SU data. CPE is a classprior estimation algorithm.\nInput: DU = {xU,i}nUi=1 (samples from p), eDS = {exS,i}2nSi=1 (samples from epS) Output: class-prior ⇡+ ⇡S CPE(DU, eDS) ⇡+ p 2⇡S 1+1\n2\nwhere Eq. (2) was used to derive the last line, ⇡D , 2⇡+⇡ , and\npD(x,x 0 )\n= p(x,x0|(y = +1 ^ y0 = 1) _ (y = 1 ^ y0 = +1))\n= ⇡+⇡ p+(x)p (x0) + ⇡+⇡ p (x)p+(x0) 2⇡+⇡ . (12)\nMarginalizing out x0 in Eq. (11) as Lemma 1, we obtain\np(x) = ⇡SepS(x) + ⇡DepD(x),\nwhere epS is defined in Eq. (4) and epD(x) , (p+(x) + p (x))/2. Since we have samples DU and eDS drawn from p and epS respectively (see Eqs. (3) and (4)), we can estimate ⇡S by mixture proportion estimation4 methods (Scott, 2015; Ramaswamy et al., 2016; du Plessis et al., 2017).\nAfter estimating ⇡S, we can calculate ⇡+. By the discussion in Section 3.1, we assume ⇡+ > ⇡ . Then, following 2⇡S 1 = ⇡S ⇡D = (⇡+ ⇡ )2 = (2⇡+ 1)2 0, we obtain ⇡+ = p 2⇡S 1+1\n2 . We summarize a wrapper of mixture proportion estimation in Algorithm 1."
  }, {
    "heading": "4. Estimation Error Bound",
    "text": "In this section, we establish an estimation error bound for the proposed method. Hereafter, let F ⇢ RX be a function class of a specified model.\nDefinition 1. Let n be a positive integer, Z1, . . . , Zn be i.i.d. random variables drawn from a probability distribution with density µ, H = {h : Z ! R} be a class of measurable functions, and = ( 1, . . . , n) be Rademacher variables, i.e., random variables taking +1 and 1 with even probabilities. Then (expected) Rademacher complexity of H is defined as\nR(H;n, µ) , E Z1,...,Zn⇠µ E\n\" sup\nh2H 1 n\nnX\ni=1\nih(Zi)\n# .\n4 Given a distribution F which is a convex combination of distributions G and H such that F = (1 )G+H , the mixture proportion estimation problem is to estimate  2 [0, 1] only with samples from F and H . In our case, F , H , and  correspond to p(x), epS(x), and ⇡S, respectively. See, e.g., Scott (2015).\nIn this section, we assume for any probability density µ, our model class F satisfies\nR(F ;n, µ)  CFp n\n(13)\nfor some constant CF > 0. This assumption is reasonable because many model classes such as the linear-inparameter model class F = {f(x) = w> (x) | kwk  C\nw , k k1  C } (Cw and C are positive constants) satisfy it (Mohri et al., 2012).\nSubsequently, let f⇤ , arg min f2F R(f) be the true risk minimizer, and bf , arg min f2F bRSU,`(f) be the empirical risk minimizer.\nTheorem 4. Assume the loss function ` is ⇢-Lipschitz with respect to the first argument (0 < ⇢ < 1), and all functions in the model class F are bounded, i.e., there exists a constant Cb such that kfk1  Cb for any f 2 F . Let C` , supt2{±1} `(Cb, t). For any > 0, with probability at least 1 ,\nR( bf) R(f⇤)  CF,`, ✓\n2⇡Sp 2nS + 1p nU\n◆ , (14)\nwhere\nCF,`, = 4⇢CF +\nq 2C2` log 4\n|2⇡+ 1| .\nA proof is given in Appendix F.\nTheorem 4 shows that if we have ⇡+ in advance, our proposed method is consistent, i.e., R( bf) ! R(f⇤) as nS ! 1 and nU ! 1. The convergence rate is Op(1/ p nS + 1/ p nU), where Op denotes the order in probability. This order is the optimal parametric rate for the empirical risk minimization without additional assumptions (Mendelson, 2008)."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we empirically investigate the performance of class-prior estimation and the proposed method for SU classification.\nDatasets: Datasets are obtained from the UCI Machine Learning Repository (Lichman, 2013), the LIBSVM (Chang & Lin, 2011), and the ELENA project 5. We randomly subsample the original datasets, to maintain that similar pairs consist of positive and negative pairs with the ratio of ⇡2+ to ⇡2 (see Eq. (2)), while the ratios of unlabeled and test data are ⇡+ to ⇡ (see Eq. (3)).\n5 https://www.elen.ucl.ac.be/neural-nets/\nResearch/Projects/ELENA/elena.htm"
  }, {
    "heading": "5.1. Class-Prior Estimation",
    "text": "First, we study empirical performance of class-prior estimation. We conduct experiments on benchmark datasets. Different dataset sizes {200, 400, 800, 1600} are tested, where half of the data are S pairs and the other half are U data.\nIn Figure 4, KM1 and KM2 are plotted, which are proposed by Ramaswamy et al. (2016). We used them as CPE in Algorithm 1 6. Since ⇡S = ⇡2++⇡2 = 2(⇡+ 12 )2+ 1 2 1 2 , we use additional heuristic to set left = 2 in Algorithm 1 of Ramaswamy et al. (2016)."
  }, {
    "heading": "5.2. Classification Complexity",
    "text": "We empirically investigate our proposed method in terms of the relationship between classification performance and the number of training data. We conduct experiments on benchmark datasets with the fixed number of S pairs (fixed to 200), and the different numbers of U data {200, 400, 800, 1600}. The experimental results are shown in Figure 5. It indicates that the classification error decreases as nU grows, which well agree with our theoretical analysis in Theorem 4. Furthermore, we observe a tendency that classification error becomes smaller as the class-prior becomes farther from 12 . This is because CF,`, in Eq. (14) has the term |2⇡+ 1| in the denominator, which will make the upper bound looser when ⇡+ is close to 12 .\nThe detailed setting about the proposed method is described below. Our implementation is available at https:// github.com/levelfour/SU_Classification.\nProposed Method (SU): We use the linear-in-input model f(x) = w>x + b. In Section 5.2, the squared loss is used, and ⇡+ is given (Case 1 in Table 3). In Section 5.3, the squared loss and the double-hinge loss are used, and the class-prior is estimated by Algorithm 1 with KM2 (Ramaswamy et al., 2016) (Case 2 in Table 3). The regulariza-\n6We used the author’s implementations published in http://web.eecs.umich.edu/\n˜\ncscott/code/\nkernel_CPE.zip.\ntion parameter is chosen from {10 1, 10 4, 10 7}. To choose hyperparameters, 5-fold cross-validation is used. Since we do not have any labeled data in the training phase, the validation error cannot be computed directly. Instead, Eq. (5) equipped with the zero-one loss `01(·) = 12 (1 sign(·)) is used as a proxy to estimate the validation error. In each experimental trial, the model with minimum validation error is chosen."
  }, {
    "heading": "5.3. Benchmark Comparison with Baseline Methods",
    "text": "We compare our proposed method with baseline methods on benchmark datasets. We conduct experiments on each dataset with 500 similar data pairs, 500 unlabeled data, and 100 test data. As can be seen from Table 4, our proposed method outperforms baselines for many datasets. The details about the baseline methods are described below.\nBaseline 1 (KM): As a simple baseline, we consider kmeans clustering (MacQueen, 1967). We ignore pair information of S data and apply k-means clustering with k = 2 to U data.\nBaseline 2 (ITML): Information-theoretic metric learning (Davis et al., 2007) is a metric learning method by regularizing the covariance matrix based on prior knowledge, with pairwise constraints. We use the identity matrix as prior knowledge, and the slack variable parameter is fixed to = 1, since we cannot employ the cross-validation without any class label information. Using the obtained metric, k-means clustering is applied on test data.\nBaseline 3 (SERAPH): Semi-supervised metric learning paradigm with hyper sparsity (Niu et al., 2012) is another metric learning method based on entropy regularization. Hyperparameter choice follows SERAPHhyper. Using the obtained metric, k-means clustering is applied on test data.\nBaseline 4 (3SMIC): Semi-supervised SMI-based clustering (Calandriello et al., 2014) models class-posteriors and maximizes mutual information between unlabeled data at hand and their cluster labels. The penalty parameter and the kernel parameter t are chosen from {10 2, 100, 102} and {4, 7, 10}, respectively, via 5-fold cross-validation. Baseline 5 (DIMC): DirtyIMC (Chiang et al., 2015) is a noisy version of inductive matrix completion, where the similarity matrix is recovered from a low-rank feature matrix. Similarity matrix S is assumed to be expressed as UU>, where U is low-rank feature representations of input data. After obtaining U , k-means clustering is conducted on U . Two hyperparameters M , N in Eq. (2) in (Chiang et al., 2015) are set to M = N = 10 2.\nBaseline 6 (IMSAT): Information maximizing selfaugmented training (Hu et al., 2017) is an unsupervised learning method to make a probabilistic classifier that maps\nsimilar data to similar representations, combining information maximization clustering with self-augmented training, which make the predictions of perturbed data close to the predictions of the original ones. Instead of data perturbation, self-augmented training can be applied on S data to make each pair of data similar. Here the logistic regressor p ✓\n(y|x) = (1 + exp( ✓>x)) 1 is used as a classification model, where ✓ is parameters to learn. Trade-off parameter is set to 1.\nRemark: KM, ITML, and SERAPH rely on k-means, which is trained by using only training data. Test prediction is based on the metric between test data and learned cluster centers. Among the baselines, DIMC can only handle insample prediction, so it is trained by using both training and test data at the same time."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we proposed a novel weakly-supervised learning problem named SU classification, where only similar pairs and unlabeled data are needed. SU classification even becomes class-identifiable under a certain condition on the class-prior (see Table 3). Its optimization problem with the linear-in-parameter model becomes convex if we choose certain loss functions such as the squared loss and the doublehinge loss. We established an estimation error bound for the proposed method, and confirmed that the estimation error decreases with the parametric optimal order, as the number of similar data and unlabeled data becomes larger. We also investigated the empirical performance and confirmed that our proposed method performs better than baseline methods."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported by JST CREST JPMJCR1403 including the AIP challenge program, Japan. We thank Ryuichi Kiryo for fruitful discussions on this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Convex formulation of multiple instance learning from positive and unlabeled bags",
    "authors": ["H. Bao", "T. Sakai", "I. Sato", "M. Sugiyama"],
    "venue": "Neural Networks,",
    "year": 2018
  }, {
    "title": "Semi-supervised clustering by seeding",
    "authors": ["S. Basu", "A. Banerjee", "R.J. Mooney"],
    "venue": "In ICML, pp",
    "year": 2002
  }, {
    "title": "A probabilistic framework for semi-supervised clustering",
    "authors": ["S. Basu", "M. Bilenko", "R.J. Mooney"],
    "venue": "In SIGKDD, pp",
    "year": 2004
  }, {
    "title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
    "authors": ["M. Belkin", "P. Niyogi", "V. Sindhwani"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2006
  }, {
    "title": "Integrating constraints and metric learning in semi-supervised clustering",
    "authors": ["M. Bilenko", "S. Basu", "R.J. Mooney"],
    "venue": "In ICML, pp",
    "year": 2004
  }, {
    "title": "Semisupervised information-maximization clustering",
    "authors": ["D. Calandriello", "G. Niu", "M. Sugiyama"],
    "venue": "Neural Networks,",
    "year": 2014
  }, {
    "title": "LIBSVM: A library for support vector machines",
    "authors": ["Chang", "C.-C", "Lin", "C.-J"],
    "venue": "ACM Transactions on Intelligent Systems and Technology,",
    "year": 2011
  }, {
    "title": "Semi-supervised classification by low density separation",
    "authors": ["O. Chapelle", "A. Zien"],
    "venue": "AISTATS",
    "year": 2005
  }, {
    "title": "Matrix completion with noisy side information",
    "authors": ["Chiang", "K.-Y", "Hsieh", "C.-J", "I.S. Dhillon"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Information-theoretic metric learning",
    "authors": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"],
    "venue": "In ICML,",
    "year": 2007
  }, {
    "title": "Analysis of learning from positive and unlabeled data",
    "authors": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Convex formulation for learning from positive and unlabeled data",
    "authors": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Class-prior estimation for learning from positive and unlabeled data",
    "authors": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"],
    "venue": "Machine Learning,",
    "year": 2017
  }, {
    "title": "Learning classifiers from only positive and unlabeled data",
    "authors": ["C. Elkan", "K. Noto"],
    "venue": "In SIGKDD, pp",
    "year": 2008
  }, {
    "title": "Social desirability bias and the validity of indirect questioning",
    "authors": ["R. Fisher"],
    "venue": "Journal of Consumer Research,",
    "year": 1993
  }, {
    "title": "Learning discrete representations via information maximizing self-augmented training",
    "authors": ["W. Hu", "T. Miyato", "S. Tokui", "E. Matsumoto", "M. Sugiyama"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Positive-unlabeled learning with non-negative risk estimator",
    "authors": ["R. Kiryo", "G. Niu", "M.C. du Plessis", "M. Sugiyama"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Discriminative clustering by regularized information maximization",
    "authors": ["A. Krause", "P. Perona", "R. Gomes"],
    "venue": "In NIPS, pp",
    "year": 2010
  }, {
    "title": "Temporal ensembling for semisupervised learning",
    "authors": ["S. Laine", "T. Aila"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Multiple instance learning for soft bags via top instances",
    "authors": ["W. Li", "N. Vasconcelos"],
    "venue": "In CVPR, pp",
    "year": 2015
  }, {
    "title": "Constrained clustering by spectral kernel learning",
    "authors": ["Z. Li", "J. Liu"],
    "venue": "In ICCV, pp",
    "year": 2009
  }, {
    "title": "Pairwise constraint propagation by semidefinite programming for semi-supervised classification",
    "authors": ["Z. Li", "J. Liu", "X. Tang"],
    "venue": "In ICML, pp",
    "year": 2008
  }, {
    "title": "Smooth neighbors on teacher graphs for semi-supervised learning",
    "authors": ["Y. Luo", "J. Zhu", "M. Li", "Y. Ren", "B. Zhang"],
    "year": 2018
  }, {
    "title": "Lower bounds for the empirical minimization algorithm",
    "authors": ["S. Mendelson"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2008
  }, {
    "title": "Learning from video and text via large-scale discriminative clustering",
    "authors": ["A. Miech", "J. Alayrac", "P. Bojanowski", "I. Laptev", "J. Sivic"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Distributional smoothing with virtual adversarial training",
    "authors": ["T. Miyato", "S. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Foundations of Machine Learning",
    "authors": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"],
    "year": 2012
  }, {
    "title": "Learning with noisy labels",
    "authors": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"],
    "venue": "In NIPS, pp",
    "year": 2013
  }, {
    "title": "Information-theoretic semi-supervised metric learning via entropy regularization",
    "authors": ["G. Niu", "B. Dai", "M. Yamada", "M. Sugiyama"],
    "venue": "In ICML, pp",
    "year": 2012
  }, {
    "title": "Theoretical comparisons of positive-unlabeled learning against positive-negative learning",
    "authors": ["G. Niu", "M.C. du Plessis", "T. Sakai", "Y. Ma", "M. Sugiyama"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Mixture proportion estimation via kernel embedding of distributions",
    "authors": ["H.G. Ramaswamy", "C. Scott", "A. Tewari"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Semi-supervised classification based on classification from positive and unlabeled data",
    "authors": ["T. Sakai", "M.C. du Plessis", "G. Niu", "M. Sugiyama"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "A rate of convergence for mixture proportion estimation, with application to learning from noisy labels",
    "authors": ["C. Scott"],
    "venue": "In AISTATS, pp",
    "year": 2015
  }, {
    "title": "On information-maximization clustering: Tuning parameter selection and analytic solution",
    "authors": ["M. Sugiyama", "M. Yamada", "M. Kimura", "H. Hachiya"],
    "venue": "In ICML, pp",
    "year": 2011
  }, {
    "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
    "authors": ["A. Tarvainen", "H. Valpola"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Constrained k-means clustering with background knowledge",
    "authors": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schrödl"],
    "venue": "In ICML, pp",
    "year": 2001
  }, {
    "title": "Randomized response: A survey technique for eliminating evasive answer bias",
    "authors": ["S. Warner"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1965
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"],
    "venue": "In NIPS,",
    "year": 2005
  }, {
    "title": "Distance metric learning, with application to clustering with sideinformation",
    "authors": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"],
    "venue": "In NIPS, pp",
    "year": 2002
  }, {
    "title": "Maximum margin clustering",
    "authors": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"],
    "venue": "In NIPS, pp",
    "year": 2005
  }, {
    "title": "Semisupervised clustering by input pattern assisted pairwise similarity matrix completion",
    "authors": ["J. Yi", "L. Zhang", "R. Jin", "Q. Qian", "A. Jain"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "A brief introduction to weakly supervised learning",
    "authors": ["Zhou", "Z.-H"],
    "venue": "National Science Review,",
    "year": 2018
  }],
  "id": "SP:3dff63212603481f8a0efa419e6ba2972ebde2c9",
  "authors": [{
    "name": "Han Bao",
    "affiliations": []
  }, {
    "name": "Gang Niu",
    "affiliations": []
  }, {
    "name": "Masashi Sugiyama",
    "affiliations": []
  }],
  "abstractText": "Supervised learning needs a huge amount of labeled data, which can be a big bottleneck under the situation where there is a privacy concern or labeling cost is high. To overcome this problem, we propose a new weakly-supervised learning setting where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are needed instead of fully labeled data, which is called SU classification. We show that an unbiased estimator of the classification risk can be obtained only from SU data, and the estimation error of its empirical risk minimizer achieves the optimal parametric convergence rate. Finally, we demonstrate the effectiveness of the proposed method through experiments.",
  "title": "Classification from Pairwise Similarity and Unlabeled Data"
}