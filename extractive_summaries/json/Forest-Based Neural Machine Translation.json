{
  "sections": [{
    "heading": "1 Introduction",
    "text": "NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017).\n∗ Contribution during internship at National Institute of Information and Communications Technology.\n†Corresponding author\nBecause of these advantages, tree-based methods become the focus of many researches of NMT nowadays.\nBased on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper.\nIn spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1.\nBecause of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent.\nInspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en-\n1Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a forest-structured neural network recently, but it is computationally inefficient (see Section 5).\ncode the syntactic information of a packed forest on the basis of a novel weighted linearization method for a packed forest (Section 3.1), and can decode the linearized packed forest under the simple sequence-to-sequence framework (Section 3.2). Experiments demonstrate the effectiveness of our method (Section 4)."
  }, {
    "heading": "2 Preliminaries",
    "text": "We first review the general sequence-to-sequence model (Section 2.1), then describe tree-based NMT systems based on linearization (Section 2.2), and finally introduce the packed forest, through which exponentially many trees can be represented in a compact manner (Section 2.3)."
  }, {
    "heading": "2.1 Sequence-to-sequence model",
    "text": "Current NMT systems usually resort to a simple framework, i.e., the sequence-to-sequence model (Cho et al., 2014; Sutskever et al., 2014). Given a source sequence (x0, . . . , xT ), in order to find a target sequence (y0, . . . , yT ′) that maximizes the conditional probability p(y0, . . . , yT ′ | x0, . . . , xT ), the sequence-to-sequence model uses one RNN to encode the source sequence into a fixed-length context vector c and a second RNN to decode this vector and generate the target sequence. Formally, the probability of the target sequence can be calculated as follows:\np(y0, . . . ,yT ′ | x0, . . . , xT )\n= T ′∏ t=0 p(yt | c, y0, . . . , yt−1), (1)\nwhere\np(yt | c, y0, . . . , yt−1) = g(yt−1, st, c), (2) st = f(st−1, yt−1, c), (3)\nc = q(h0, . . . , hT ), (4)\nht = f(et, ht−1). (5)\nHere, g, f , and q are nonlinear functions; ht and st are the hidden states of the source-side RNN and target-side RNN, respectively, c is the context vector, and et is the embedding of xt.\nBahdanau et al. (2014) introduced an attention mechanism to deal with the issues related to long sequences (Cho et al., 2014). Instead of encoding the source sequence into a fixed vector c, the attention model uses different ci-s when calculating\nthe target-side output yi at time step i:\nci = T∑\nj=0\nαijhj , (6)\nαij = exp(a(si−1, hj))∑T k=0 exp(a(si−1, hk)) . (7)\nThe function a(si−1, hj) can be regarded as representing the soft alignment between the target-side RNN hidden state si−1 and the source-side RNN hidden state hj .\nBy changing the format of the source/target sequences, this framework can be regarded as a string-to-string NMT system (Sutskever et al., 2014), a tree-to-string NMT system (Li et al., 2017), or a string-to-tree NMT system (Aharoni and Goldberg, 2017)."
  }, {
    "heading": "2.2 Linear-structured tree-based NMT systems",
    "text": "Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely.\nRegarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993). Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT.\nIt can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation."
  }, {
    "heading": "2.3 Packed forest",
    "text": "The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list\n(Huang, 2008). Figure 1a shows a packed forest, which can be unpacked into two constituent trees (Figure 1b and Figure 1c).\nFormally, a packed forest is a pair 〈V,E〉, where V is the set of nodes and E is the set of hyperedges. Each v ∈ V can be represented as Xi,j , where X is a constituent label and i, j ∈ [0, n] are indices of words, showing that the node spans the words ranging from i (inclusive) to j (exclusive). Here, n is the length of the input sentence. Each e ∈ E is a three-tuple 〈head(e), tails(e), score(e)〉, where head(e) ∈ V is similar to the head node in a constituent tree, and tails(e) ∈ V ∗ is similar to the set of child nodes in a constituent tree. score(e) ∈ R is the logarithm of the probability that tails(e) represents the tails of head(e) calculated by the parser. Based on score(e), the score of a constituent tree T can be calculated as follows:\nscore(T ) = −λn+ ∑\ne∈E(T )\nscore(e), (8)\nwhere E(T ) is the set of hyperedges appearing in tree T , and λ is a regularization coefficient for the sentence length2.\n2Following the configuration of Charniak and Johnson"
  }, {
    "heading": "3 Forest-based NMT",
    "text": "We first propose a linearization method for the packed forest (Section 3.1), then describe how to encode the linearized forest (Section 3.2), which can then be translated by the conventional decoder (see Section 2.1)."
  }, {
    "heading": "3.1 Forest linearization",
    "text": "Recently, several studies have focused on the linearization methods of a syntax tree, both in the area of tree-based NMT (Section 2.2) and in the area of parsing (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Basically, these methods follow a fixed traversal order (e.g., depthfirst), which does not exist for the packed forest (a directed acyclic graph (DAG)). Furthermore, the weights are attached to edges of a packed forest instead of the nodes, which further increase the difficulty.\nTopological ordering algorithms for DAG (Kahn, 1962; Tarjan, 1976) are not good solutions, because the outputted ordering is not always optimal for machine translation. In particular, a topo-\n(2005), for all the experiments in this paper, we fixed λ to log2 600.\nAlgorithm 1 Linearization of a packed forest 1: function LINEARIZEFOREST(〈V,E〉,w) 2: v ← FINDROOT(V ) 3: r← [] 4: EXPANDSEQ(v, r, 〈V,E〉,w) 5: return r 6: function FINDROOT(V ) 7: for v ∈ V do 8: if v has no parent then 9: return v 10: procedure EXPANDSEQ(v, r, 〈V,E〉,w) 11: for e ∈ E do 12: if head(e) = v then 13: if tails(e) 6= ∅ then 14: for t ∈ SORT(tails(e)) do . Sort\ntails(e) by word indices. 15: EXPANDSEQ(t, r, 〈V,E〉,w) 16: l← LINEARIZEEDGE(head(e),w) 17: r.append(〈l, σ(0.0)〉) . σ is the sigmoid\nfunction, i.e., σ(x) = 1 1+e−x , x ∈ R.\n18: l ← c©LINEARIZEEDGES(tails(e),w) . c© is a unary operator. 19: r.append(〈l, σ(score(e))〉) 20: else 21: l← LINEARIZEEDGE(head(e),w) 22: r.append(〈l, σ(0.0)〉) 23: function LINEARIZEEDGE(Xi,j ,w) 24: return X ⊗ ( j−1k=iwk) 25: function LINEARIZEEDGES(v,w) 26: return ⊕v∈vLINEARIZEEDGE(v,w)\nlogical ordering could ignore “word sequential information” and “parent-child information” in the sentences.\nFor example, for the packed forest in Figure 1a, although “[10]→[1]→[2]→ · · · →[9]→[11]” is a valid topological ordering, the word sequential information of the words (e.g., “John” should be located ahead of the period), which is fairly crucial for translation of languages with fixed pragmatic word order such as Chinese or English, is lost.\nAs another example, for the packed forest in Figure 1a, nodes [2], [9], and [10] are all the children of node [11]. However, in the topological order “[1]→[2]→ · · · →[9]→[10]→[11],” node [2] is quite far from node [11], while nodes [9] and [10] are both close to node [11]. The parent-child information cannot be reflected in this topological order, which is not what we would expect.\nTo address the above two problems, we propose a novel linearization algorithm for a packed forest (Algorithm 1). The algorithm linearizes the packed forest from the root node (Line 2) to leaf nodes by calling the EXPANDSEQ procedure (Line 15) recursively, while preserving the word order in the sentence (Line 14). In this way, word sequential information is preserved. Within the\nEXPANDSEQ procedure, once a hyperedge is linearized (Line 16), the tails are also linearized immediately (Line 18). In this way, parent-child information is preserved. Intuitively, different parts of constituent trees should be combined in different ways, therefore we define different operators ( c©, ⊗, ⊕, or ) to represent the relationships between different parts, so that the representations of these parts can be combined in different ways (see Section 3.2 for details). Words are concatenated by the operator “ ” with each other, a word and a constituent label is concatenated by the operator “⊗”, the linearization results of child nodes are concatenated by the operator “⊕” with each other, while the unary operator “ c©” is used to indicate that the node is the child node of the previous part. Furthermore, each token in the linearized sequence is related to a score, representing the confidence of the parser.\nThe linearization result of the packed forest in Figure 1a is shown in Figure 2. Tokens in the linearized sequence are separated by slashes. Each token in the sequence is composed of different types of symbols and combined by different operators. We can see that word sequential information is preserved. For example, “NNP⊗John” (linearization result of node [1]) is in front of “VBZ⊗has” (linearization result of node [3]), which is in front of “DT⊗a” (linearization result of node [4]). Moreover, parent-child information is also preserved. For example, “NP⊗John” (linearization result of node [2]) is followed by “ c©NNP⊗John” (linearization result of node [1], the child of node [2]).\nNote that our linearization method cannot fully recover packed forest. What we want to do is not to propose a fully recoverable linearization method. What we actually want to do is to encode syntax information as much as possible, so that we can improve the performance of NMT. As will be shown in Section 4, this goal is achieved.\nAlso note that there is one more advantage of our linearization method: the linearized sequence\nis a weighted sequence, while all the previous studies ignored the weights during linearization. As will be shown in Section 4, the weights are actually important not only for the linearization of a packed forest, but also for the linearization of a single tree.\nBy preserving only the nodes and hyperedges in the 1-best tree and removing all others, our linearization method can be regarded as a treelinearization method. Compared with other treelinearization methods, our method combines several different kinds of information within one symbol, retaining the parent-child information, and incorporating the confidence of the parser in the sequence. We examine whether the weights can be useful not only for linear structured tree-based NMT but also for our forest-based NMT.\nFurthermore, although our method is nonreversible for packed forests, it is reversible for constituent trees, in that the linearization is processed exactly in the depth-first traversal order and all necessary information in the tree nodes has been encoded. As far as we know, there is no previous work on linearization of packed forests."
  }, {
    "heading": "3.2 Encoding the linearized forest",
    "text": "The linearized packed forest forms the input of the encoder, which has two major differences from the input of a sequence-to-sequence NMT system. First, the input sequence of the encoder consists of two parts: the symbol sequence and the score sequence. Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators ( c©, ⊗, ⊕, or ). Based on these observa-\ntions, we propose two new frameworks, which are illustrated in Figure 3.\nFormally, the input layer receives the sequence (〈l0, ξ0〉, . . . , 〈lT , ξT 〉), where li denotes the i-th symbol and ξi its score. Then, the sequence is fed into the score layer and the symbol layer. The score and symbol layers receive the sequence and output the score sequence ξ = (ξ0, . . . , ξT ) and symbol sequence l = (l0, . . . , lT ), respectively, from the input. Any item l ∈ l in the symbol layer has the form\nl = o0x1o1 . . . xm−1om−1xm, (9)\nwhere each xk (k = 1, . . . ,m) is a word or a constituent label, m is the total number of words and constituent labels in a symbol, o0 is “ c©” or empty, and each ok (k = 1, . . . ,m − 1) is either “⊗”, “⊕”, or “ ”. Then, in the node/operator layer, the x-s and o-s are separated and rearranged as x = (x1, . . . , xm, o0, . . . , om−1), which is fed to the pre-embedding layer. The pre-embedding layer generates a sequence p = (p1, . . . , pm, . . . , p2m), which is calculated as follows:\np =Wemb[I(x)]. (10)\nHere, the function I(x) returns a list of the indices in the dictionary for all the elements in x, which consist of words, constituent labels, or operators. In addition, Wemb is the embedding matrix of size (|wword| + |wlabel| + 4) × dword, where |wword| and |wlabel| are the total number of words and constituent labels, respectively, dword is the dimension of the word embedding, and there are four possible operators: “ c©,” “⊗,” “⊕,” and “ .” Note\nthat p is a list of 2m vectors, and the dimension of each vector is dword.\nBecause the length of the sequence of the input layer is T + 1, there are T + 1 different ps in the pre-embedding layer, which we denote by P = (p0, . . . ,pT ). Depending on where the score layer is incorporated, we propose two frameworks: Score-on-Embedding (SoE) and Score-onAttention (SoA). In SoE, the k-th element of the embedding layer is calculated as follows:\nek = ξk ∑ p∈pk p, (11)\nwhile in SoA, the k-th element of the embedding layer is calculated as\nek = ∑ p∈pk p, (12)\nwhere k = 0, . . . , T . Note that ek ∈ Rdword . In this manner, the proposed forest-to-string NMT framework is connected with the conventional sequence-to-sequence NMT framework.\nAfter calculating the embedding vectors in the embedding layer, the hidden vectors are calculated using Equation 5. When calculating the context vector ci-s, SoE and SoA differ from each other. For SoE, the ci-s are calculated using Equation 6 and 7, while for SoA, the αij-s used to calculate the ci-s are determined as follows:\nαij = exp(ξja(si−1, hj))∑T k=0 exp(ξka(si−1, hk)) . (13)\nThen, using the decoder of the sequence-tosequence framework, the sentence of the target language can be generated."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Setup",
    "text": "We evaluate the effectiveness of our forest-based NMT systems on English-to-Chinese and Englishto-Japanese translation tasks3. The statistics of the corpora used in our experiments are summarized in Table 1.\nThe packed forests of English sentences are obtained by the constituent parser proposed by Huang (2008)4. We filtered out the sentences for\n3English is commonly chosen as the target language. We chose English as the source language because a highperformance forest parser is not available for other languages.\n4http://web.engr.oregonstate.edu/ ˜huanlian/software/forest-reranker/ forest-charniak-v0.8.tar.bz2\nwhich the parser cannot generate the packed forest successfully and the sentences longer than 80 words. For NIST datasets, we simply choose the first reference among the four English references of NIST corpora, because all of them are independent with each other, according to the documents of NIST datasets. For Chinese sentences, we used Stanford segmenter5 for segmentation. For Japanese sentences, we followed the preprocessing steps recommended in WAT 20176.\nWe implemented our framework based on nematus8 (Sennrich et al., 2017). For optimization, we used the Adadelta algorithm (Zeiler, 2012). In order to avoid overfitting, we used dropout (Srivastava et al., 2014) on the embedding layer and hidden layer, with the dropout probability set to 0.2. We used the gated recurrent unit (Cho et al., 2014) as the recurrent unit of RNNs, which are bi-directional, with one hidden layer.\nBased on the tuning result, we set the maximum length of the input sequence to 300, the hidden layer size as 512, the dimension of word embedding as 620, and the batch size for training as 40. We pruned the packed forest using the algorithm of Huang (2008), with a threshold of 5. If the linearization of the pruned forest is still longer than 300, then we linearize the 1-best parsing tree instead of the forest. During decoding, we used beam search, and fixed the beam size to 12. For the case of Forest (SoA), with 1 core of Tesla K80 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed is about 10 sentences per second.\n5https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip\n6http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html\n7LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06\n8https://github.com/EdinburghNLP/ nematus"
  }, {
    "heading": "4.2 Experimental results",
    "text": "Table 2 and 3 summarize the experimental results. To avoid the affect of segmentation errors, the performance were evaluated by character-level BLEU (Papineni et al., 2002). We compare our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to-string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree-based models that use the 1-best parsing tree (1-best (No score, SoE, SoA)). For the 1-best models, we preserve the nodes and hyperedges that are used in the 1-best constituent tree in the packed forest, and remove all other nodes and hyperedges, yielding a pruned forest that contains only the 1-best constituent tree. For the “No score” configurations, we force the input score sequence to be a sequence of 1.0 with the same length as the input symbol sequence, so that neither the embedding layer nor the attention layer are affected by the score sequence.\nIn addition, we also perform a comparison with some state-of-the-art tree-based systems that are\npublicly available, including an SMT system (Mi et al., 2008) and the NMT systems (Eriguchi et al. (2016)9, Chen et al. (2017)10, and Li et al. (2017)). For Mi et al. (2008), we use the implementation of cicada11. For Li et al. (2017), we reimplemented the “Mixed RNN Encoder” model, because of its outstanding performance on the NIST MT corpus.\nWe can see that for both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configurations yield better results. This indicates syntactic information contained in the constituent trees or forests is indeed useful for machine translation. Specifically, we observe the following facts.\nFirst, among the three different frameworks SoE, SoA, and No-score, the SoA framework performs the best, while the No-score framework per-\n9https://github.com/tempra28/tree2seq 10https://github.com/howardchenhd/\nSyntax-awared-NMT 11https://github.com/tarowatanabe/ cicada\nforms the worst. This indicates that the scores of the edges in constituent trees or packed forests, which reflect the confidence of the correctness of the edges, are indeed useful. In fact, for the 1-best constituent parsing tree, the score of the edge reflects the confidence of the parser. By using this information, the NMT system succeed to learn a better attention, paying much attention to the confident structure and not paying attention to the unconfident structure, which improved the translation performance. This fact is ignored by previous studies on tree-based NMT. Furthermore, it is better to use the scores to modify the values of attention instead of rescaling the word embeddings, because modifying word embeddings carelessly may change the semantic meanings of words.\nSecond, compared with the cases that only using the 1-best constituent trees, using packed forests yields statistical significantly better results for the SoE and SoA frameworks. This shows the effectiveness of using more syntactic information. Compared with one constituent tree, the packed forest, which contains multiple different trees, describes the syntactic structure of the sentence in different aspects, which together increase the accuracy of machine translation. However, without using the scores, the 1-best constituent tree is preferred. This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.\nCompared with other types of state-of-the-art systems, our systems using only the 1-best tree (1-best(SoE, SoA)) are better than the other treebased systems. Moreover, our NMT systems using the packed forests achieve the best performance. These results also support the usefulness of the scores of the edges and packed forests in NMT.\nAs for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline. The training time of the tree-based system was about 1.5 times of the baseline. For the\ncase of Forest (SoA), with 1 core of Tesla P100 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed was about 10 sentences per second. The reason for the relatively low efficiency is that the linearized sequences of packed forests were much longer than word sequences, enlarging the scale of the inputs. Despite this, the training process ended within reasonable time."
  }, {
    "heading": "4.3 Qualitative analysis",
    "text": "Figure 4 illustrates the translation results of an English sentence using several different configurations: the s2s baseline, using only the 1-best tree (SoE), and using the packed forest (SoE). This is a sentence from NIST MT 03, and the training corpus is the LDC corpus.\nFor the s2s case, no syntactic information is utilized, and therefore the output of the system is not a grammatical Chinese sentence. The attributive phrase of “Czech border region” is a complete sentence. However, the attributive is not allowed to be a complete sentence in Chinese.\nFor the case of using 1-best constituent tree, the output is a grammatical Chinese sentence. However, the phrase “adjacent to neighboring Slovakia” is completely ignored in the translation result. After analyzing the constituent tree, we found that this phrase was incorrectly parsed as an “adverb phrase”, so that the NMT system paid little attention to it, because of the low confidence given by the parser.\nIn contrast, for the case of the packed forest, we can see this phrase was not ignored and was translated correctly. Actually, besides “adverb phrase”, this phrase was also correctly parsed as an “adjective phrase”, and covered by multiple different nodes in the forest, making it difficult for the encoder to ignore the phrase.\nWe also noticed that our method performed better on learning attention. For the example in Figure 4, we observed that for s2s model, the decoder paid attention to the word “Czech” twice, which\ncauses the output sentence contains the Chinese translation of Czech twice. On the other hand, for our forest model, by using the syntax information, the decoder paid attention to the phrase “In the Czech Republic” only once, making the decoder generates the correct output."
  }, {
    "heading": "5 Related work",
    "text": "Incorporating syntactic information into NMT systems is attracting widespread attention nowadays. Compared with conventional string-to-string NMT systems, tree-based systems demonstrate a better performance with the help of constituent trees or dependency trees.\nThe first noteworthy study is Eriguchi et al. (2016), which used Tree-structured LSTM (Tai et al., 2015) to encode the HPSG syntax tree of the sentence in the source-side in a bottom-up manner. Then, Chen et al. (2017) enhanced the encoder with a top-down tree encoder.\nAs a simple extension of Eriguchi et al. (2016), very recently, Zaremoodi and Haffari (2017) proposed a forest-based NMT method by representing the packed forest with a forest-structured neural network. However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences). In contrast, our proposed method is effective in a largescale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT.\nAlthough these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, therefore restricts the scope of application.\nOther attempts at encoding syntactic trees have also been proposed. Eriguchi et al. (2017) combined the Recurrent Neural Network Grammar (Dyer et al., 2016) with NMT systems, while Li et al. (2017) linearized the constituent tree and encoded it using RNNs. The training of these methods is fast, because of the linear structures of RNNs. However, all these syntax-based NMT systems used only the 1-best parsing tree, making the systems sensitive to parsing errors.\nInstead of using trees to represent syntactic information, some studies use other data structures to represent the latent syntax of the input sentence. For example, Hashimoto and Tsuruoka (2017) proposed translating using a latent graph. However, such systems do not enjoy the benefit of\nhandcrafted syntactic knowledge, because they do not use a parser trained from a large treebank with human annotations.\nCompared with these related studies, our framework utilizes a linearized packed forest, meaning the encoder can encode exponentially many trees in an efficient manner. The experimental results demonstrated these advantages."
  }, {
    "heading": "6 Conclusion and future work",
    "text": "We proposed a new NMT framework, which encodes a packed forest for the source sentence using linear-structured neural networks, such as RNN. Compared with conventional string-tostring NMT systems and tree-to-string NMT systems, our framework can utilize exponentially many linearized parsing trees during encoding, without significantly decreasing the efficiency. This represents the first attempt at using a forest under the string-to-string NMT framework. The experimental results demonstrate the effectiveness of our framework.\nAs future work, we plan to design some more elaborate structures to incorporate the score layer in the encoder. Further improvement in the translation performance is expected to be achieved for the forest-based NMT system. We will also apply the proposed linearization method to other tasks."
  }, {
    "heading": "Acknowledgements",
    "text": "We are grateful to the anonymous reviewers for their insightful comments and suggestions. We thank Lemao Liu from Tencent AI Lab for his suggestions about the experiments. We thank Atsushi Fujita whose suggestions greatly improve the readability and the logical soundness of this paper. This work was done during the internship of Chunpeng Ma at NICT. Akihiro Tamura is supported by JSPS KAKENHI Grant Number JP18K18110. Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and State High-Tech Development Plan of China (863 program) via grant 2015AA015405."
  }],
  "year": 2019,
  "references": [{
    "title": "Towards string-to-tree neural machine translation",
    "authors": ["Roee Aharoni", "Yoav Goldberg."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 132–140.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Coarseto-fine n-best parsing and maxent discriminative reranking",
    "authors": ["Eugene Charniak", "Mark Johnson."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180.",
    "year": 2005
  }, {
    "title": "Improved neural machine translation with a syntax-aware encoder and decoder",
    "authors": ["Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2017
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "arXiv preprint",
    "year": 2014
  }, {
    "title": "Recurrent neural network grammars",
    "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1602.07776.",
    "year": 2016
  }, {
    "title": "Tree-to-sequence attentional neural machine translation",
    "authors": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2016
  }, {
    "title": "Learning to parse and translate improves neural machine translation",
    "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
    "year": 2017
  }, {
    "title": "Neural machine translation with source-side latent graph parsing",
    "authors": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka."],
    "venue": "arXiv preprint arXiv:1702.02265.",
    "year": 2017
  }, {
    "title": "Forest reranking: Discriminative parsing with non-local features",
    "authors": ["Liang Huang."],
    "venue": "Proceedings of ACL-08: HLT, pages 586–594.",
    "year": 2008
  }, {
    "title": "Topological sorting of large networks",
    "authors": ["Arthur B Kahn."],
    "venue": "Communications of the ACM, 5(11):558– 562.",
    "year": 1962
  }, {
    "title": "Statistical significance tests for machine translation evaluation",
    "authors": ["Philipp Koehn."],
    "venue": "Proceedings of EMNLP 2004, pages 388–395.",
    "year": 2004
  }, {
    "title": "Modeling source syntax for neural machine translation",
    "authors": ["Junhui Li", "Deyi Xiong", "Zhaopeng Tu", "Muhua Zhu", "Min Zhang", "Guodong Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
    "year": 2017
  }, {
    "title": "Deterministic attention for sequence-to-sequence constituent parsing",
    "authors": ["Chunpeng Ma", "Lemao Liu", "Akihiro Tamura", "Tiejun Zhao", "Sumita Eiichiro."],
    "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 3237–3243.",
    "year": 2017
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Computational linguistics, 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Forest-based translation rule extraction",
    "authors": ["Haitao Mi", "Liang Huang."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214.",
    "year": 2008
  }, {
    "title": "Forestbased translation",
    "authors": ["Haitao Mi", "Liang Huang", "Qun Liu."],
    "venue": "Proceedings of ACL-08: HLT, pages 192–199.",
    "year": 2008
  }, {
    "title": "Syntax-aware neural machine translation using ccg",
    "authors": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Brich."],
    "venue": "arXiv preprint arXiv:1702.01147.",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
    "year": 2002
  }, {
    "title": "The impact of parse quality on syntactically-informed statistical machine translation",
    "authors": ["Chris Quirk", "Simon Corston-Oliver."],
    "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 62–69.",
    "year": 2006
  }, {
    "title": "Nematus: a toolkit for neural machine",
    "authors": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"],
    "year": 2017
  }, {
    "title": "Linguistic input features improve neural machine translation",
    "authors": ["Rico Sennrich", "Barry Haddow."],
    "venue": "arXiv preprint arXiv:1606.02892.",
    "year": 2016
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of machine learning research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Improved semantic representations",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"],
    "year": 2015
  }, {
    "title": "Edge-disjoint spanning trees and depth-first search",
    "authors": ["Robert Endre Tarjan."],
    "venue": "Acta Informatica, 6(2):171–185.",
    "year": 1976
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "Advances in Neural Information Processing Systems, pages 2773–2781.",
    "year": 2015
  }, {
    "title": "Sequence-to-dependency neural machine translation",
    "authors": ["Shuangzhi Wu", "Dongdong Zhang", "Nan Yang", "Mu Li", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2017
  }, {
    "title": "Incorporating syntactic uncertainty in neural machine translation with forest-to-sequence model",
    "authors": ["Poorya Zaremoodi", "Gholamreza Haffari."],
    "venue": "arXiv preprint arXiv:1711.07019.",
    "year": 2017
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701.",
    "year": 2012
  }, {
    "title": "Prior knowledge integration for neural machine translation using posterior regularization",
    "authors": ["Jiacheng Zhang", "Yang Liu", "Huanbo Luan", "Jingfang Xu", "Maosong Sun."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
    "year": 2017
  }],
  "id": "SP:97f6c6092d839614f539f4bbff596aeb66826292",
  "authors": [{
    "name": "Chunpeng Ma",
    "affiliations": []
  }, {
    "name": "3∗Akihiro",
    "affiliations": []
  }, {
    "name": "Masao Utiyama",
    "affiliations": []
  }, {
    "name": "Tiejun Zhao1†Eiichiro",
    "affiliations": []
  }],
  "abstractText": "Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forestbased methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems.",
  "title": "Forest-Based Neural Machine Translation"
}