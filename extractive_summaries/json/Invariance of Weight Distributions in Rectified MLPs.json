{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Neural networks have recently been applied to a number of diverse problems with impressive results (van den Oord et al., 2016; Silver et al., 2017; Berthelot et al., 2017). These breakthroughs largely appear to be driven by ap-\n1School of ITEE, University of Queensland, Brisbane, Queensland, Australia 2School of Mathematics and Physics, University of Queensland, Brisbane, Queensland, Australia 3International Computer Science Institute, Berkeley, California, USA. Correspondence to: Russell Tsuchida <s.tsuchida@uq.edu.au>, Farbod RoostaKhorasani <fred.roosta@uq.edu.au>, Marcus Gallagher <marcusg@uq.edu.au>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nplication rather than an understanding of the capabilities and training of neural networks. Recently, significant work has been done to increase understanding of neural networks (Choromanska et al., 2015; Haeffele & Vidal, 2015; Poole et al., 2016; Schoenholz et al., 2017; Zhang et al., 2016; Martin & Mahoney, 2017; Shwartz-Ziv & Tishby, 2017; Balduzzi et al., 2017; Raghu et al., 2017). However, there is still work to be done to bring theoretical understanding in line with the results seen in practice.\nThe connection between neural networks and kernel machines has long been studied (Neal, 1994). Much past work has been done to investigate the equivalent kernel of certain neural networks, either experimentally (Burgess, 1997), through sampling (Sinha & Duchi, 2016; Livni et al., 2017; Lee et al., 2017), or analytically by assuming some random distribution over the weight parameters in the network (Williams, 1997; Cho & Saul, 2009; Pandey & Dukkipati, 2014a;b; Daniely et al., 2016; Bach, 2017a). Surprisingly, in the latter approach, rarely have distributions other than the Gaussian distribution been analyzed. This is perhaps due to early influential work on Bayesian Networks (MacKay, 1992), which laid a strong mathematical foundation for a Bayesian approach to training networks. Another reason may be that some researchers may hold the intuitive (but not necessarily principled) view that the Central Limit Theorem (CLT) should somehow apply.\nIn this work, we investigate the equivalent kernels for networks with Rectified Linear Unit (ReLU), Leaky ReLU (LReLU) or other activation functions, one-hidden layer, and more general weight distributions. Our analysis carries over to deep networks. We investigate the consequences that weight initialization has on the equivalent kernel at the beginning of training. While initialization schemes that mitigate exploding/vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994; Hochreiter et al., 2001) for other activation functions and weight distribution combinations have been explored in earlier works (Glorot & Bengio, 2010; He et al., 2015), we discuss an initialization scheme for Muli-Layer Perceptrons (MLPs) with LReLUs and weights coming from distributions with 0 mean and finite absolute third moment. The derived kernels also allow us to analyze the loss of information as an input is propagated through the network, offering a complementary view to the shattered gradient problem (Balduzzi et al., 2017)."
  }, {
    "heading": "2. Preliminaries",
    "text": "Consider a fully connected (FC) feedforward neural network with m inputs and a hidden layer with n neurons. Let σ : R → R be the activation function of all the neurons in the hidden layer. Further assume that the biases are 0, as is common when initializing neural network parameters. For any two inputs x,y ∈ Rm propagated through the network, the dot product in the hidden layer is\n1 n h(x) · h(y) = 1 n n∑ i=1 σ(wi · x)σ(wi · y), (1)\nwhere h(·) denotes the n dimensional vector in the hidden layer and wi ∈ Rm is the weight vector into the ith neuron. Assuming an infinite number of hidden neurons, the sum in (1) has an interpretation as an inner product in feature space, which corresponds to the kernel of a Hilbert space. We have\nk(x,y) = ∫ Rm σ(w · x)σ(w · y)f(w)dw, (2)\nwhere f(w) is the probability density function (PDF) for the identically distributed weight vector W = (W1, ...,Wm)\nT in the network. The connection of (2) to the kernels in kernel machines is well-known (Neal, 1994; Williams, 1997; Cho & Saul, 2009).\nProbabilistic bounds for the error between (1) and (2) have been derived in special cases (Rahimi & Recht, 2008) when the kernel is shift-invariant. Two specific random feature mappings are considered: (1) Random Fourier features are taken for the σ in (1). Calculating the approximation error in this way requires being able to sample from the PDF defined by the Fourier transform of the target kernel. More explicitly, the weight distribution f is the Fourier transform of the target kernel and the n samples σ(wi ·x) are replaced by some appropriate scale of cos(wi · x). (2) A random bit string σ(xi) is associated to each input according to a grid with random pitch δ sampled from f imposed on the input space. This method requires having access to the second derivative of the target kernel to sample from the distribution f .\nOther work (Bach, 2017b) has focused on the smallest error between a target function g in the reproducing kernel Hilbert space (RKHS) defined by (2) and an approximate function ĝ expressible by the RKHS with the kernel (1). More explicitly, let g(x) = ∫ Rm G(w)σ(w,x)f(w) dw be\nthe representation of g in the RKHS. The quantity ∥∥ĝ −\ng ∥∥ = ∥∥∑ni=1 αiσ(wi, ·) − ∫Rm G(w)σ(w, ·)f(w) dw∥∥ (with some suitable norm) is studied for the best set of αi and random wi with an optimized distribution.\nYet another measure of kernel approximation error is investigated by Rudi & Rosasco (2017). Let ĝ and g be the\noptimal solutions to the ridge regression problem of minimizing a regularized cost function C using the kernel (1) and the kernel (2) respectively. The number of datapoints n required to probabilistically bound C(ĝ)−C(g) is found to be O( √ n log n) under a suitable set of assumptions. This work notes the connection between kernel machines and one-layer Neural Networks with ReLU activations and Gaussian weights by citing Cho & Saul (2009). We extend this connection by considering other weight distributions and activation functions.\nIn this work our focus is on deriving expressions for the target kernel, not the approximation error. Additionally, we consider random mappings that have not been considered elsewhere. Our work is related to work by Poole et al. (2016) and Schoenholz et al. (2017). However, our results apply to the unbounded (L)ReLU activation function and more general weight distributions, and their work considers random biases as well as weights."
  }, {
    "heading": "3. Equivalent Kernels for Infinite Width Hidden Layers",
    "text": "The kernel (2) has previously been evaluated for a number of choices of f and σ (Williams, 1997; Roux & Bengio, 2007; Cho & Saul, 2009; Pandey & Dukkipati, 2014a;b). In particular, the equivalent kernel for a one-hidden layer network with spherical Gaussian weights of variance E[W 2i ] and mean 0 is the Arc-Cosine Kernel (Cho & Saul, 2009)\nk(x,y) = E[W 2i ]‖x‖‖y‖\n2π\n( sin θ0 + (π− θ0) cos θ0 ) , (3)\nwhere θ0 = cos−1 ( x·y ‖x‖‖y‖ ) is the angle between the inputs x and y and ‖·‖ denotes the `2 norm. Noticing that the ArcCosine Kernel k(x,y) depends on x and y only through their norms, with an abuse of notation we will henceforth set k(x,y) ≡ k(θ0).Define the normalized kernel to be the cosine similarity between the signals in the hidden layer. The normalized Arc-Cosine Kernel is given by\ncos θ1 = k(x,y)√ k(x,x) √ k(y,y) = 1 π\n( sin θ0 + (π − θ0) cos θ0 ) ,\nwhere θ1 is the angle between the signals in the first layer. Figure 1 shows a plot of the normalized Arc-Cosine Kernel. One might ask how the equivalent kernel changes for a different choice of weight distribution. We investigate the equivalent kernel for networks with (L)ReLU activations and general weight distributions in Section 3.1 and 3.2. The equivalent kernel can be composed and applied to deep networks. The kernel can also be used to choose good weights for initialization. These, as well as other implications for practical neural networks, are investigated in Section 5."
  }, {
    "heading": "3.1. Kernels for Rotationally-Invariant Weights",
    "text": "In this section we show that (3) holds more generally than for the case where f is Gaussian. Specifically, (3) holds when f is any rotationally invariant distribution. We do this by casting (2) as the solution to an ODE, and then solving the ODE. We then extend this result using the same technique to the case where σ is LReLU.\nA rotationally-invariant PDF one with the property f(w) = f(Rw) = f(‖w‖) for all w and orthogonal matrices R. Recall that the class of rotationally-invariant distributions (Bryc, 1995), as a subclass of elliptically contoured distributions (Johnson, 2013), includes the Gaussian distribution, the multivariate t-distribution, the symmetric multivariate Laplace distribution, and symmetric multivariate stable distributions.\nProposition 1. Suppose we have a one-hidden layer feedforward network with ReLU σ and random weights W with uncorrelated and identically distributed rows with rotationally-invariant PDF f : Rm → R and E[W 2i ] <∞. The equivalent kernel of the network is (3).\nProof. First, we require the following.\nProposition 2. With the conditions in Proposition 1 and inputs x,y ∈ Rm the equivalent kernel of the network is the solution to the Initial Value Problem (IVP)\nk′′(θ0) + k(θ0) = F (θ0), k ′(π) = 0, k(π) = 0, (4)\nwhere θ0 ∈ (0, π) is the angle between the inputs x and y. The derivatives are meant in the distributional sense; they are functionals applying to all test functions in C∞c (0, π). F (θ0) is given by the m− 1 dimensional integral\nF (θ0) = ∫ Rm−1 f ( (s sin θ0,−s cos θ0, w3, ..., wm)T )\nΘ(s)s3 ds dw3 dw4... dwm‖x‖‖y‖ sin θ0, (5)\nwhere Θ is the Heaviside step function.\nThe proof is given in Appendix A. The main idea is to rotate w (following Cho & Saul (2009)) so that\nk(x,y) = ∫ Rm Θ(w1)Θ(w1 cos θ0 + w2 sin θ0)w1\n(w1 cos θ0 + w2 sin θ0)f(w) dw‖x‖‖y‖.\nNow differentiating twice with respect to θ0 yields the second order ODE (4). The usefulness of the ODE in its current form is limited, since the forcing term F (θ0) as in (5) is difficult to interpret. However, regardless of the underlying distribution on weights w, as long as the PDF f in (5) corresponds to any rotationally-invariant distribution, the integral enjoys a much simpler representation.\nProposition 3. With the conditions in Proposition 1, the forcing term F (θ0) in the kernel ODE is given by F (θ0) = K sin θ0, where\nK = ∫ Rm−1 Θ(s)s3f ( (s, 0, w3, ..., wm) T )\nds dw3, ... dwm‖x‖‖y‖ <∞,\nand the solution to the distributional ODE (4) is the solution to the corresponding classical ODE.\nThe proof is given in Appendix B.\nNote that in the representation F (θ0) = K sin θ0 of the forcing term, the underlying distribution appears only as a constant K. For all rotationally-invariant distributions, the forcing term in (4) results in an equivalent kernel with the same form. We can combine Propositions 2 and 3 to find the equivalent kernel assuming rotationally-invariant weight distributions. Due to the rotational invariance of f , k(0) =∫ Rm Θ(w1)w 2 1f(Rw) dw‖x‖‖y‖ = ‖x‖‖y‖E[W 2i ] 2 . The solution to the ODE in Proposition 2 using the forcing term from Proposition 3 is k(θ0) = c1 cos θ0 + c2 sin θ0 − 1 2Kθ0 cos θ0. Using the conditions from the IVP and k(0), the values of c1, c2 and K give the required result.\nOne can apply the same technique to the case of LReLU activations σ(z) = ( a+ (1− a)Θ(z) ) z, where a specifies the gradient of the activation for z < 0. Proposition 4. Consider the same situation as in Proposition 1 with the exception that the activations are LReLU. The integral (2) is then given by\nk(x,y) = [ (1− a)2\n2π\n( sin θ0 + (π − θ0) cos θ0 ) + a cos θ0 ] E[W 2i ]‖x‖‖y‖, (6)\nwhere a ∈ [0, 1) is the LReLU gradient parameter.\nThis is just a slightly more involved calculation than the ReLU case; we defer our proof to the supplementary material."
  }, {
    "heading": "3.2. Asymptotic Kernels",
    "text": "In this section we approximate k for large m and more general weight PDFs. We invoke the CLT as m → ∞, which requires a condition that we discuss briefly before presenting it formally. The dot product w·x can be seen as a linear combination of the weights, with the coefficients corresponding to the coordinates of x. Roughly, such a linear combination will obey the CLT if many coefficients are non-zero. To let m → ∞, we construct a sequence of inputs {x(m)}∞m=2. This may appear unusual in the context of neural networks, since m is fixed and finite in practice. The sequence is used only for asymptotic analysis.\nAs an example if the dataset were CelebA (Liu et al., 2015) with 116412 inputs, one would have x(116412). To generate an artificial sequence, one could down-sample the image to be of size 116411, 116410, and so on. At each point in the sequence, one could normalize the point so that its `2 norm is ‖x(116412)‖. One could similarly up-sample the image.\nIntuitively, if the up-sampled image does not just insert zeros, as m increases the we expect the ratio |x (m) i |\n‖x(m)‖ to decrease because the denominator stays fixed and the numerator gets smaller. In our proof the application of CLT requires maxmi=1 |x(m)i | ‖x(m)‖ to decrease faster than m\n1/4. Hypothesis 5 states this condition precisely.\nHypothesis 5. For x(m),y(m) ∈ Rm, define sequences of inputs {x(m)}∞m=2 and {y(m)}∞m=2 with fixed ‖x(m)‖=‖x‖, ‖y(m)‖=‖y‖, and θ0= cos−1 x (m)·y(m) ‖x‖‖y‖ for all m.\nLetting x(m)i be the i th coordinate of x(m),\nassume that lim m→∞\nm(1/4) maxmi=1 |x(m)i | ‖x‖ and\nlim m→∞\nm(1/4) maxmi=1 |y(m)i | ‖y‖ are both 0.\nFigures 2 and 5 empirically investigate Hypothesis 5 for two datasets, suggesting it makes reasonable assumptions on high dimensional data such as images and audio. Theorem 6. Consider an infinitely wide FC layer with almost everywhere continuous activation functions σ. Suppose the random weights W come from an IID distribution with PDF fm such that E[Wi] = 0 and E|W 3i | <∞. Suppose that the conditions in Hypothesis 5 are satisfied. Then\nσ(W(m) · x(m))σ(W(m) · y(m)) D−→ σ(Z1)σ(Z2),\nwhere D−→ denotes convergence in distribution and Z = (Z1, Z2) T is a Gaussian random vector with covari-\nance matrix E[W 2i ] [\n‖x‖2 ‖x‖‖y‖ cos θ0 ‖x‖‖y‖ cos θ0 ‖y‖2\n] and\n0 mean. Every Z(m) = (W(m) · x(m),W(m) · y(m))T has the same mean and covariance matrix as Z.\nConvergence in distribution is a weak form of convergence, so we cannot expect in general that all kernels should converge asymptotically. For some special cases however, this is indeed possible to show. We first present the ReLU case.\nCorollary 7. Let m, W, fm, E[Wi] and E|W 3i | be as defined in Theorem 6. Define the corresponding kernel to be k\n(m) f\n( x(m),y(m) ) . Consider a second infinitely wide FC layer with m inputs. Suppose the random weights come from a spherical Gaussian with E[Wi] = 0 and finite variance E[W 2i ] with PDF gm. Define the corresponding kernel to be k(m)g ( x(m),y(m) ) . Suppose that the conditions in Hypothesis 5 are satisfied and the activation functions are σ(z) = Θ(z)z. Then for all s ≥ 2,\nlim m→∞\nk (m) f\n( x(m),y(m) ) = k(s)g ( x(s),y(s) ) = E [ σ(Z1)σ(Z2) ] ,\nwhere Z is as in Theorem 6. Explicitly, k(m)f converges to (3).\nThe proof is given in Appendix D. This implies that the Arc-Cosine Kernel is well approximated by ReLU layers with weights from a wide class of distributions. Similar results hold for other σ including the LReLU and ELU (Clevert et al., 2016), as shown in the supplementary material."
  }, {
    "heading": "4. Empirical Verification of Results",
    "text": "We empirically verify our results using two families of weight distributions. First, consider the m-dimensional tdistribution\nf(w) = Γ[(ν +m)/2] Γ(ν/2)νm/2πm/2 √ |det(Σ)|[\n1 + 1\nν (wTΣ−1w)\n]−(ν+m)/2 ,\nwith degrees of freedom ν and identity shape matrix Σ = I . The multivariate t-distribution approaches the multivariate Gaussian as ν → ∞. Random variables drawn from the multivariate t-distribution are uncorrelated but not independent. This distribution is rotationally-invariant and satisfies the conditions in Propositions (1) and (4).\nSecond, consider the multivariate distribution\nf(w) = m∏ i=1\nβ\n2αΓ(1/β) e−|wi/α|\nβ\n, (7)\nwhich is not rotationally-invariant (except when β = 2, which coincides with a Gaussian distribution) but whose random variables are IID and satisfy the conditions in Theorem 6. As β → ∞ this distribution converges pointwise to the uniform distribution on [−α, α].\nIn Figure 3, we empirically verify Propositions 1 and 4. In the one hidden layer case, the samples follow the blue curve j = 1, regardless of the specific multivariate t weight distribution which varies with ν. We also observe that the universality of the equivalent kernel appears to hold for the distribution (7) regardless of the value of β, as predicted by theory. We discuss the relevance of the curves j 6= 1 in Section 5."
  }, {
    "heading": "5. Implications for Practical Networks",
    "text": ""
  }, {
    "heading": "5.1. Composed Kernels in Deep Networks",
    "text": "A recent advancement in understanding the difficulty in training deep neural networks is the identification of the shattered gradients problem (Balduzzi et al., 2017). Without skip connections, the gradients of deep networks approach white noise as they are backpropagated through the network, making them difficult to train.\nA simple observation that complements this view is obtained through repeated composition of the normalized kernel. As m → ∞, the angle between two inputs in the jth layer of a LReLU network random weights with E[W ] = 0 and E|W 3| < ∞ approaches cos θj =\n1 1+a2 ( (1−a)2 π ( sin θj−1+(π−θj−1) cos θj−1 ) +2a cos θ0 ) .\nA result similar to the following is hinted at by Lee et al. (2017), citing Schoenholz et al. (2017). Their analysis, which considers biases in addition to weights (Poole et al., 2016), yields insights on the trainability of random neural networks that our analysis cannot. However, their argument does not appear to provide a complete formal proof for the case when the activation functions are unbounded, e.g., ReLU. The degeneracy of the composed kernel with more general activation functions is also proved by Daniely (2016), with the assumption that the weights are Gaussian distributed. Corollary 8. The normalized kernel corresponding to LReLU activations converges to a fixed point at θ∗ = 0.\nProof. Let z = cos θj−1 and define\nT (z)= 1\n1 + a2 ( (1− a)2 π (√ 1− z2+(π−cos−1 z)z ) +2az ) .\nThe magnitude of the derivative of T is ∣∣∣1−( 1−a1+a)2 cos−1 zπ ∣∣∣\nwhich is bounded above by 1 on [−1, 1]. Therefore, T is a contraction mapping. By Banach’s fixed point theorem there exists a unique fixed point z∗ = cos θ∗. Set θ∗ = 0 to verify that θ∗ = 0 is a solution, and θ∗ is unique.\nCorollary 8 implies that for this deep network, the angle between any two signals at a deep layer approaches 0. No matter what the input is, the kernel “sees” the same thing after accounting for the scaling induced by the norm of the input. Hence, it becomes increasingly difficult to train deeper networks, as much of the information is lost and the outputs will depend merely on the norm of the inputs; the signals decorrelate as they propagate through the layers.\nAt first this may seem counter-intuitive. An appeal to intuition can be made by considering the corresponding linear network with deterministic and equal weight matrices in each layer, which amounts to the celebrated power iteration method. In this case, the repeated application of a matrix transformation A to a vector v converges to the dominant eigenvector (i.e. the eigenvector corresponding to the largest eigenvalue) of A.\nFigure 3 shows that the theoretical normalized kernel for networks of increasing depth closely follows empirical samples from randomly initialized neural networks.\nIn addition to convergence of direction, by also requiring that ‖x‖ = ‖y‖ it can be shown that after accounting for scaling, the magnitude of the signals converge as the signals propagate through the network. This is analogous to having the dominant eigenvalue equal to 1 in the power iteration method comparison.\nCorollary 9. The quantity E [( σ(j)(x) −\nσ(j)(y) )2]\n/E[σ(j)(x)2] in a j-layer random (L)ReLU network of infinite width with random uncorrelated and identically distributed rotationally-invariant weights with ‖x‖=‖y‖ approaches 0 as j →∞.\nProof. Denote the output of one neuron in the jth layer of a network σ(W (1) ·σ(...σ(W (j)x)) by σ(j)(x) and let kj be the kernel for the j-layer network. Then\nE [( σ(j)(x)− σ(j)(y) )2] /E[σ(j)(x)2]\n= ( kj(x,x)− 2kj(x,y) + kj(y,y) ) /kj(x,x),\n= 2− 2 cos θj\nwhich approaches 0 as j →∞.\nContrary to the shattered gradients analysis, which applies to gradient based optimizers, our analysis relates to any optimizers that initialize weights from some distribution satisfying conditions in Proposition 4 or Corollary 7. Since information is lost during signal propagation, the network’s output shares little information with the input. An\noptimizer that tries to relate inputs, outputs and weights through a suitable cost function will be “blind” to relationships between inputs and outputs.\nOur results can be used to argue against the utility of controversial Extreme Learning Machines (ELM) (Huang et al., 2004), which randomly initialize hidden layers from symmetric distributions and only learn the weights in the final layer. A single layer ELM can be replaced by kernel ridge regression using the equivalent kernel. Furthermore, a Multi-Layer ELM (Tang et al., 2016) with (L)ReLU activations utilizes a pathological kernel as shown in Figure 3. It should be noted that ELM bears resemblance to early works (Schmidt et al., 1992; Pao et al., 1994)."
  }, {
    "heading": "5.2. Initialization",
    "text": "Suppose we wish to approximately preserve the `2 norm from the input to hidden layer. By comparing (1) and (2), we approximately have ‖h(x)‖ ≈ √ k(x,x)n. Letting\nθ0 = 0 in (6), we have ‖h(x)‖ = ‖x‖ √ nE[W 2i ](1+a2) 2 . Setting ‖h(x)‖ = ‖x‖,\n√ E[W 2i ] = √√√√ 2( 1 + a2 ) n . (8)\nThis applies whenever the conditions in Proposition 4 or Corollary 12 are satisfied. This agrees with the well-known case when the elements of W are IID (He et al., 2015) and a = 0. For small values of a, (8) is well approximated by the known result (He et al., 2015). For larger values of a, this approximation breaks down, as shown in Figure 4.\nAn alternative approach to weight initialization is the datadriven approach (Mishkin & Matas, 2016), which can be applied to more complicated network structures such as\nconvolutional and max-pooling layers commonly used in practice. As parameter distributions change during training, batch normalization inserts layers with learnable scaling and centering parameters at the cost of increased computation and complexity (Ioffe & Szegedy, 2015)."
  }, {
    "heading": "6. Conclusion",
    "text": "We have considered universal properties of MLPs with weights coming from a large class of distributions. We have theoretically and empirically shown that the equivalent kernel for networks with an infinite number of hidden ReLU neurons and all rotationally-invariant weight distributions is the Arc-Cosine Kernel. The CLT can be applied to approximate the kernel for high dimensional input data. When the activations are LReLUs, the equivalent kernel has a similar form. The kernel converges to a fixed point, showing that information is lost as signals propagate through the network.\nOne avenue for future work is to study the equivalent kernel for different activation functions, noting that some activations such as the ELU may not be expressible in a closed form (we do show in the supplementary material however, that the ELU does have an asymptotically universal kernel).\nSince wide networks with centered weight distributions have approximately the same equivalent kernel, powerful trained deep and wide MLPs with (L)ReLU activations should have asymmetric, non-zero mean, non-IID parameter distributions. Future work may consider analyzing the equivalent kernels of trained networks and more complicated architectures. We should not expect that k(x,y) may be expressed neatly as k(θ0) in these cases. This work is a crucial first step in identifying invariant properties in neural networks and sets a foundation from which we hope to expand in future."
  }, {
    "heading": "A. Proof of Proposition 2",
    "text": "Proof. The kernel with weight PDF f(ω) and ReLU σ is\nk(x,y) = ∫ Rm Θ(ω · x)Θ(ω · y)(ω · x)(ω · y)f(ω) dω. Let θ0 be the angle between x and y. Define u = (‖x‖, 0, ..., 0)T and v = (‖y‖ cos θ0, ‖y‖ sin θ0, 0, ..., 0)T with u,v ∈ Rm. Following Cho & Saul (2009), there exists some m×m rotation matrix R such that x = Ru and y = Rv. We have\nk(x,y) = ∫ Rm Θ(ω ·Ru)Θ(ω ·Rv)(ω ·Ru)(ω ·Rv)\nf(ω) dω.\nLet ω = Rw and note that the dot product is invariant under rotations and the determinant of the Jacobian of the\ntransformation is 1 since R is orthogonal. We have\nk(x,y) = ∫ Rm Θ(w · u)Θ(w · v)(w · u)(w · v)\nf(Rw) dw,\n= ∫ Rm Θ(‖x‖w1)Θ(‖y‖(w1 cos θ0 + w2 sin θ0))\nw1(w1 cos θ0 + w2 sin θ0)f(w) dw‖x‖‖y‖. (9)\nOne may view the integrand as a functional acting on test functions of θ0. Denote the set of infinitely differentiable test functions on (0, π) by C∞c (0, π). The linear functional acting over C∞c (0, π) is a Generalized Function and we may take distributional derivatives under the integral by Theorem 7.40 of Jones (1982). Differentiating twice,\nk′′ + k\n= ∫ Rm Θ(w1)w1(−w1 sin θ0 + w2 cos θ0)2\nδ ( w1 cos θ0 + w2 sin θ0 ) f(w) dw‖x‖‖y‖,\n= ∫ Rm−1 f ( (s sin θ0,−s cos θ0, w3, ..., wm)T )\nΘ(s)s3 ds dw3 dw4... dwm‖x‖‖y‖ sin θ0. The initial condition k(π) = 0 is obtained by putting θ0 = π in (9) and noting that the resulting integrand contains a factor of Θ(w1)Θ(−w1)w1 which is 0 everywhere. Similarly, the integrand of k′(π) contains a factor of Θ(w2)Θ(−w2)w2. The ODE is meant in a distributional sense, that∫ π 0 ψ(θ0) ( k′′(θ0) + k(θ0)− F (θ0) ) dθ0 = 0 ∀ψ ∈ Cc∞(0, π), where k is a distribution with a distributional second derivative k′′."
  }, {
    "heading": "B. Proof of Proposition 3",
    "text": "Proof. Denote the marginal PDF of the first two coordinates of W by f12. Due to the rotational invariance of f , f(Ox) = f(‖x‖) = f(x) for any orthogonal matrix O. So\nF (θ0) = ∫ Rm−1 f ( (s sin θ0,−s cos θ0, w3, ..., wm)T ) sin θ0Θ(s)s 3 ds dw3, ... dwm‖x‖‖y‖,\n= sin θ0 ∫ R Θ(s)s3f12 ( (s, 0, )T ) ds‖x‖‖y‖,\n= K sin θ0, K ∈ (0,∞]. It remains to check that K <∞. F is integrable since∫\nR2 ∫ π 0 Θ(w1)w1(−w1 sin θ0 + w2 cos θ0)2\nδ(w1 cos θ0 + w2 sin θ0)f12(w1, w2)dθ0dw1dw2\n= ∫ R2 Θ(w1)w1 ∣∣(w21 + w22)1/2∣∣f12(w1, w2)dw1dw2,\n≤ √ E [ Θ2(W1)W 21 ]√ E [ W 21 +W 2 2 ] <∞.\nTherefore, F is finite almost everywhere. This is only true if K < ∞. k′′ = F − k must be a function, so the distributional and classical derivatives coincide."
  }, {
    "heading": "C. Proof of Theorem 6",
    "text": "Proof. There exist some orthonormal R1,R2∈Rm such that y(m)=‖y(m)‖(R1 cos θ0 + R2 sin θ0) and x(m) = ‖x(m)‖R1. We would like to examine the asymptotic distribution of σ ( ‖y(m)‖W(m)· ( R1 cos θ0+R2 sin θ0\n)) σ ( ‖x(m)‖W(m)·R1 ) .\nLet U (m)1 =W ·R1 cos θ0 + W ·R2 sin θ0 and U\n(m) 2 = −W·R1 sin θ0+W·R2 cos θ0. Note that\nE[U (m)21 ]=E[U (m)2 2 ]=E[W 2i ] and E[U (m) 1 ]=E[U (m) 2 ]=0. Also note that U (m)1 and U (m) 2 are uncorrelated since E[U (m)1 U (m) 2 ] = E [ (W·R1)(W·R2)(cos2 θ0+sin2 θ0)−\ncos θ0 sin θ0 ( (W ·R1)2 − (W ·R2)2 )] = 0.\nLetMk = E ∣∣W ki ∣∣, U(m) = (U1, U2)T , I be the 2×2 iden-\ntity matrix and Q ∼ N ( 0,M2I ) . Then for any convex set S ∈ R2 and some C ∈ R, by the Berry-Esseen Theorem,∣∣P[U ∈ S]− P[Q ∈ S]∣∣2 ≤ Cγ2 where γ2 is given by( m∑ j=1 E ∥∥∥M −122 Wi I ( R1j cos θ0 +R2j sin θ0−R1j sin θ0 +R2j cos θ0\n)∥∥∥3)2, = ( M −3 2\n2 M3 m∑ j=1 E ∥∥∥( R1j cos θ0 +R2j sin θ0−R1j sin θ0 +R2j cos θ0 )∥∥∥3)2, = ( M −3 2\n2 M3 m∑ j=1 ∣∣∣R21j +R22j∣∣∣(3/2))2, ≤M−32 M23m\nm∑ j=1 ∣∣∣R21j +R22j∣∣∣3, = M−32 M 2 3m\nm∑ j=1 ∣∣∣R61j + 3R41jR22j + 3R21jR42j +R62j∣∣∣, ≤M−32 M23m ( 4\nm max k=1 R41k + 4 m max k=1 R42k\n) .\nThe last line is due to the fact that m∑ j=1 ∣∣∣R61j + 3R41jR22j∣∣∣ ≤ mmax k=1 R41k ( m∑ j=1 R21j + 3R 2 2j ) .\nNow R1k = xk‖x‖ and R2k = 1\nsin θ0\n( yk ‖y‖ − xk ‖x‖ cos θ0 ) ,\nso if θ0 6= 0, π by Hypothesis 5 U(m) converges in distribution to the bivariate spherical Gaussian with variance E[W 2i ]. Then the random vector Z(m) = (Z (m) 1 , Z (m) 2 ) T =\n( ‖x‖W ·R1, ‖y‖(W ·R1 cos θ0 + W ·R2 sin θ0) )T =(\n‖x‖(U1 cos θ0−U2 sin θ0), ‖y‖U1 )T\nconverges in distribution to the bivariate Gaussian random variable with covariance matrix E[W 2i ] [\n‖x‖2 ‖x‖‖y‖ cos θ0 ‖x‖‖y‖ cos θ0 ‖y‖2\n] .\nSince σ is continuous almost everywhere, by the Continuous Mapping Theorem,\nσ(W(m) · x(m))σ(W(m) · y(m)) D−→ σ(Z1)σ(Z2). If θ0 = 0 or θ0 = π, we may treat R2 as 0 and the above still holds."
  }, {
    "heading": "D. Proof of Corollary 7",
    "text": "Proof. We have limm→∞ k (m) f\n( x(m),y(m) ) =\nlimm→∞ E [ σ(Z (m) 1 )σ(Z (m) 2 ) ] and would like to bring the limit inside the expected value. By Theorem 6 and Theorem 25.12 of Billingsley (1995), it suffices to show that σ(Z(m)1 )σ(Z (m) 2 ) is uniformly integrable. Define h to be the joint PDF of Z(m). We have\nlim α→∞ ∫ |σ(z1)σ(z2)|>α |σ(z1)σ(z2)|h(z1, z2) dz1dz2\n= lim α→∞ ∫ |Θ(z1)Θ(z2)z1z2|>α |Θ(z1)Θ(z2)z1z2|h(z1, z2)\ndz1dz2,\nbut the integrand is 0 whenever z1 ≤ 0 or z2 ≤ 0. So∫ |σ(z1)σ(z2)|>α |σ(z1)σ(z2)|h(z1, z2) dz1dz2\n= ∫ R2 z1z2Θ(z1z2 − α)Θ(z1)Θ(z2)h(z1, z2) dz1dz2.\nWe may raise the Heaviside functions to any power without changing the value of the integral. Squaring the Heaviside functions and applying Hölder’s inequality, we have(∫\nR2 z1z2Θ\n2(z1z2 − α)Θ2(z1)Θ2(z2)h(z1, z2)dz1dz2 )2\n≤ E[z21Θ(z1z2 − α)Θ(z1)Θ(z2)] E[z22Θ(z1z2 − α)Θ(z1)Θ(z2)].\nExamining the first of these factors,∫ ∞ 0 ∫ ∞ α/z1 z21h(z1, z2) dz2dz1,\n= ∫ ∞ 0 z21 ∫ ∞ α/z1 h(z1, z2) dz2dz1.\nNow let gα(z1) = ∫∞ α/z1\nh(z1, z2) dz2. gα(z1)z21 is monotonically pointwise non-increasing to 0 in α for all z1 > 0 and ∫ z21g0(z1)dz1 ≤ E[Z21 ] <∞ . By the Monotone Convergence Theorem limα→∞ E[z21Θ(z1z2 − α)Θ(z1)] = 0. The second factor has the same limit, so the limit of the right hand side of Hölder’s inequality is 0."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the anonymous reviewers for directing us toward relevant work and providing helpful recommendations regarding the presentation of the paper. Farbod Roosta-Khorasani gratefully acknowledges the support from the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923). Russell Tsuchida’s attendance at the conference was made possible by an ICML travel award."
  }],
  "year": 2018,
  "references": [{
    "title": "Breaking the curse of dimensionality with convex neural networks",
    "authors": ["F. Bach"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "On the equivalence between kernel quadrature rules and random feature expansions",
    "authors": ["F. Bach"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "The shattered gradients problem: If resnets are the answer, then what is the question",
    "authors": ["D. Balduzzi", "M. Frean", "L. Leary", "J.P. Lewis", "K.W. Ma", "B. McWilliams"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "The third chime speech separation and recognition challenge: Analysis and outcomes",
    "authors": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"],
    "venue": "Computer Speech and Language,",
    "year": 2017
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Y. Bengio", "P. Simard", "P. Frasconi"],
    "venue": "IEEE transactions on neural networks,",
    "year": 1994
  }, {
    "title": "Boundary equilibrium generative adversarial networks",
    "authors": ["D. Berthelot", "T. Schumm", "Metz", "L. Began"],
    "venue": "arXiv preprint arXiv:1703.10717,",
    "year": 2017
  }, {
    "title": "Probability and Measure",
    "authors": ["P. Billingsley"],
    "venue": "WileyInterscience, 3rd edition,",
    "year": 1995
  }, {
    "title": "Rotation invariant distributions",
    "authors": ["W. Bryc"],
    "venue": "In The Normal Distribution,",
    "year": 1995
  }, {
    "title": "Estimating equivalent kernels for neural networks: A data perturbation approach",
    "authors": ["A.N. Burgess"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1997
  }, {
    "title": "Kernel methods for deep learning",
    "authors": ["Y. Cho", "L.K. Saul"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2009
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2015
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
    "authors": ["A. Daniely", "R. Frostig", "Y. Singer"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["X. Glorot", "Y. Bengio"],
    "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Global optimality in tensor factorization, deep learning, and beyond",
    "authors": ["B.D. Haeffele", "R. Vidal"],
    "venue": "arXiv preprint arXiv:1506.07540,",
    "year": 2015
  }, {
    "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE international conference on computer vision,",
    "year": 2015
  }, {
    "title": "Untersuchungen zu dynamischen neuronalen netzen",
    "authors": ["S. Hochreiter"],
    "venue": "Diploma, Technische Universität München,",
    "year": 1991
  }, {
    "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
    "authors": ["S. Hochreiter", "Y. Bengio", "P. Frasconi"],
    "venue": "Field Guide to Dynamical Recurrent Networks. IEEE Press,",
    "year": 2001
  }, {
    "title": "Extreme learning machine: a new learning scheme of feedforward neural networks",
    "authors": ["G. Huang", "Q. Zhu", "C. Siew"],
    "venue": "In Neural Networks,",
    "year": 2004
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Multivariate statistical simulation: A guide to selecting and generating continuous multivariate distributions",
    "authors": ["M.E. Johnson"],
    "year": 2013
  }, {
    "title": "The Theory of Generalised Functions, chapter 7, pp. 263",
    "authors": ["D.S. Jones"],
    "year": 1982
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"],
    "year": 2009
  }, {
    "title": "Deep neural networks as gaussian processes",
    "authors": ["J. Lee", "Y. Bahri", "R. Novak", "S.S. Schoenholz", "J. Pennington", "J. Sohl-Dickstein"],
    "venue": "arXiv preprint arXiv:1611.01232,",
    "year": 2017
  }, {
    "title": "Deep learning face attributes in the wild",
    "authors": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"],
    "venue": "In Proceedings of International Conference on Computer Vision (ICCV),",
    "year": 2015
  }, {
    "title": "Learning infinite layer networks without the kernel trick",
    "authors": ["R. Livni", "D. Carmon", "A. Globerson"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "A practical Bayesian framework for backpropagation networks",
    "authors": ["D.J.C. MacKay"],
    "venue": "Neural Computation,",
    "year": 1992
  }, {
    "title": "Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior",
    "authors": ["C.H. Martin", "M.W. Mahoney"],
    "venue": "arXiv preprint arXiv:1710.09553,",
    "year": 2017
  }, {
    "title": "All you need is a good init",
    "authors": ["D. Mishkin", "J. Matas"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "Bayesian Learning for Neural Networks",
    "authors": ["R.M. Neal"],
    "venue": "PhD thesis, University of Toronto,",
    "year": 1994
  }, {
    "title": "To go deep or wide in learning",
    "authors": ["G. Pandey", "A. Dukkipati"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2014
  }, {
    "title": "Learning by stretching deep networks",
    "authors": ["G. Pandey", "A. Dukkipati"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Learning and generalization characteristics of the random vector functionallink",
    "authors": ["Y. Pao", "G. Park", "D.J. Sobajic"],
    "venue": "net. Neurocomputing,",
    "year": 1994
  }, {
    "title": "Exponential expressivity in deep neural networks through transient chaos",
    "authors": ["B. Poole", "S. Lahiri", "M. Raghu", "J. Sohl-Dickstein", "S. Ganguli"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Random features for large-scale kernel machines",
    "authors": ["A. Rahimi", "B. Recht"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2008
  }, {
    "title": "Continuous neural networks",
    "authors": ["Roux", "N. Le", "Y. Bengio"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2007
  }, {
    "title": "Generalization properties of learning with random features",
    "authors": ["Rudi", "Alessandro", "Rosasco", "Lorenzo"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Feedforward neural networks with random weights",
    "authors": ["W.F. Schmidt", "M.A. Kraaijveld", "R.P.W. Duin"],
    "venue": "In Pattern Recognition,",
    "year": 1992
  }, {
    "title": "Deep information propagation",
    "authors": ["S.S. Schoenholz", "J. Gilmer", "S. Ganguli", "J. SohlDickstein"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Opening the Black Box of Deep Neural Networks via Information",
    "authors": ["R. Shwartz-Ziv", "N. Tishby"],
    "venue": "arXiv preprint arXiv:1703.00810,",
    "year": 2017
  }, {
    "title": "Learning kernels with random features",
    "authors": ["A. Sinha", "J.C. Duchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Extreme learning machine for multilayer perceptron",
    "authors": ["J. Tang", "C. Deng", "G. Huang"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
    "year": 2016
  }, {
    "title": "Wavenet: A generative model for raw audio",
    "authors": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1609.03499,",
    "year": 2016
  }, {
    "title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition",
    "authors": ["E. Vincent", "S. Watanabe", "A. Nugraha", "J. Barker", "R. Marxer"],
    "venue": "Computer Speech and Language,",
    "year": 2017
  }, {
    "title": "Computing with infinite networks",
    "authors": ["C.K.I. Williams"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1997
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "venue": "arXiv preprint arXiv:1611.03530,",
    "year": 2016
  }],
  "id": "SP:6960db4729ce09ffdfdf7353b7b293f774e442ca",
  "authors": [{
    "name": "Russell Tsuchida",
    "affiliations": []
  }, {
    "name": "Farbod Roosta-Khorasani",
    "affiliations": []
  }, {
    "name": "Marcus Gallagher",
    "affiliations": []
  }],
  "abstractText": "An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having 0 mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.",
  "title": "Invariance of Weight Distributions in Rectified MLPs"
}