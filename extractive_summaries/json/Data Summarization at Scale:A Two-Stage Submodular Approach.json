{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In the context of machine learning, it is not uncommon to have to repeatedly optimize a set of functions that are fundamentally related to each other. In this paper, we focus on a class of functions called submodular functions. These functions exhibit a mathematical diminishing returns property that allows us to find nearly-optimal solutions in linear time. However, modern datasets are growing so large that even linear time solutions can be computationally expensive. Ideally, we want to find a sublinear summary of the given dataset so that optimizing these related functions over this reduced subset is nearly as effective, but not nearly as expensive, as optimizing them over the full dataset.\nAs a concrete example, suppose Uber is trying to give their\n*Equal contribution 1Department of Computer Science, Yale University, New Haven, Connecticut, USA 2Google Research, Zurich, Switzerland. Correspondence to: Ehsan Kazemi <ehsan.kazemi@yale.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndrivers suggested waiting locations across New York City based on historical rider pick-ups. Even if they discretize the potential waiting locations to just include points at which pick-ups have occurred in the past, there are still hundreds of thousands, if not millions, of locations to consider. If they wish to update these ideal waiting locations every day (or at any routine interval), it would be invaluable to be able to drastically reduce the number of locations that need to be evaluated, and still achieve nearly optimal results.\nIn this scenario, each day would have a different function that quantifies the value of a set of locations for that particular day. For example, in the winter months, spots near ice skating rinks would be highly valuable, while in the summer months, waterfront venues might be more prominent. On the other hand, major tourist destinations like Times Square will probably be busy year-round.\nIn other words, although the most popular pick-up locations undoubtedly vary over time, there is also some underlying distribution of the user behavior that remains relatively constant and ties the various days together. This means that even though the functions for future days are technically unknown, if we can select a good reduced subset of candidate locations based on the functions derived from historical data, then this same reduced subset should perform well on future functions that we cannot explicitly see yet.\nIn more mathematical terms, consider some unknown distribution of functions D and a ground set Ω of n elements to pick from. We want to select a subset S of ` elements (with ` n) such that optimizing functions (drawn from this distribution D) over the reduced subset S is comparable to optimizing them over the entire ground set Ω.\nThis problem was first introduced by Balkanski et al. (2016) as two-stage submodular maximization. This name comes from the idea that the overall framework can be viewed as two separate stages. First, we want to use the given functions to select a representative subset S, that is ideally sublinear in size of the entire ground set Ω. In the second stage, for any functions drawn from this same distribution, we can optimize over S, which will be much faster than optimizing over Ω.\nOur Contributions. In today’s era of massive data, an algorithm is rarely practical if it is not scalable. In this\npaper, we build on existing work to provide solutions for two-stage submodular maximization in both the streaming and distributed settings. Table 1 summarizes the theoretical results of this paper and compares them with the previous state of the art. The proofs of all the theoretical results are deferred to the Supplementary Material."
  }, {
    "heading": "2. Related Work",
    "text": "Data summarization is one of the most natural applications that falls under the umbrella of submodularity. As such, there are many existing works applying submodular theory to a variety of important summarization settings. For example, Mirzasoleiman et al. (2013) used an exemplar-based clustering approach to select representative images from the Tiny Images dataset (Torralba et al., 2008). Kirchhoff & Bilmes (2014) and Feldman et al. (2018) also worked on submodular image summarization, while Lin & Bilmes (2011) and Wei et al. (2013) focused on document summarization.\nIn addition to data summarization, submodularity appears in a wide variety of other machine learning applications including variable selection (Krause & Guestrin, 2005), recommender systems (Gabillon et al., 2013), crowd teaching (Singla et al., 2014), neural network interpretability (Elenberg et al., 2017), robust optimization (Kazemi et al., 2017), network monitoring (Gomez Rodriguez et al., 2010), and influence maximization in social networks (Kempe et al., 2003).\nThere have also been many successful efforts in scalable submodular optimization. For our distributed implementation we will primarily build on the framework developed by Barbosa et al. (2015). Other similar algorithms include works by Mirzasoleiman et al. (2013) and Mirrokni & Zadimoghaddam (2015), as well as Kumar et al. (2015). In terms of the streaming setting, there are two existing works we will focus on: Badanidiyuru et al. (2014) and Buchbinder et al. (2015). The key difference between the two is that Badanidiyuru et al. (2014) relies on thresholding and will terminate as soon as k elements are selected from the stream, while Buchbinder et al. (2015) will continue through the end of the stream, swapping elements in and out when required.\nRepeated optimization of related submodular functions has\nbeen a well-studied problem with works on structured prediction (Lin & Bilmes, 2012), submodular bandits (Yue & Guestrin, 2011; Chen et al., 2017), and online submodular optimization (Jegelka & Bilmes, 2011). However, unlike our work, these approaches are not concerned with data summarization as a key pre-processing step.\nThe problem of two-stage submodular maximization was first introduced by Balkanski et al. (2016). They present two algorithms with strong approximation guarantees, but both runtimes are prohibitively expensive. Recently, Stan et al. (2017) presented a new algorithm known as REPLACEMENT-GREEDY that improved the approximation guarantee from 12 (1 − 1e ) to 12 (1 − 1e2 ) and the run time from O(km`n2 log(n)) to O(km`n). They also show that, under mild conditions over the functions, maximizing over the sublinear summary can be arbitrarily close to maximizing over the entire ground set. In a nutshell, their method indirectly constructs the summary S by greedily building up solutions Ti for each given function fi simultaneously over ` rounds.\nAlthough Balkanski et al. (2016) and Stan et al. (2017) presented centralized algorithms with constant factor approximation guarantees, there is a dire need for scalable solutions in order for the algorithm to be practically useful. In particular, the primary purpose of two-stage submodular maximization is to tackle problems where the dataset is too large to be repeatedly optimized by simple greedy-based approaches. As a result, in many cases, the datasets can be so large that existing algorithms cannot even be run once. The greedy approach requires that the entire data must fit into main memory, which may not be possible, thus requiring a streaming-based solution. Furthermore, even if we have enough memory, the problem may simply be so large that it requires a distributed approach in order to run in any reasonable amount of time."
  }, {
    "heading": "3. Problem Definition",
    "text": "In general, if we want to optimally choose ` out of n items, we need to consider every single one of the exponentially many possibilities. This makes the problem intractable for any reasonable number of elements, let alone the billions of elements that are common in modern datasets. Fortunately,\nmany data summarization formulations satisfy an intuitive diminishing returns property known as submodularity.\nMore formally, a set function f : 2V → R is submodular (Fujishige, 2005; Krause & Golovin, 2012) if, for all sets A ⊆ B ⊆ V and every element v ∈ V \\ B, we have f(A + v) − f(A) ≥ f(B + v) − f(B).1 That is, the marginal contribution of any element v to the value of f(A) diminishes as the set A grows.\nMoreover, a submodular function f is said to be monotone if f(A) ≤ f(B) for all sets A ⊆ B ⊆ V . That is, adding elements to a set cannot decrease its value. Thanks to a celebrated result by Nemhauser et al. (1978), we know that if our function f is monotone submodular, then the classical greedy algorithm will obtain a (1 − 1/e)-approximation to the optimal value. Therefore, we can nearly-optimize monotone submodular functions in linear time.\nNow we formally re-state the problem we are going to solve.\nProblem Statement. Consider some unknown distribution D of monotone submodular functions and a ground set Ω of n elements to choose from. We want to select a set S of at most ` items that maximizes the following function:\nG(S) = Ef∼D[ max T⊆S,|T |≤k f(T )]. (1)\nThat is, the set S we choose should be optimal in expectation over all functions in this distribution D. However, in general, the distribution D is unknown and we only have access to a small set of functions F = (f1, . . . , fm) drawn from D. Therefore, the best approximation we have is to optimize the following related function:\nGm(S) = 1\nm m∑ i=1 max T∗i ⊆S,|T∗i |≤k fi(T ∗ i ). (2)\nStan et al. (2017, Theorem 1) shows that with enough sample functions, Gm(S) becomes an arbitrarily good approximation to G(S).\nTo be clear, each T ∗i ⊂ S is the corresponding size k optimal solution for fi. However, in general we cannot find the true optimal T ∗i , so throughout the paper we will use Ti to denote the approximately-optimal size k solution we select for each fi. Table 2 (Appendix A) summarizes the important terminology and can be used as a reference, if needed.\nIt is very important to note that although each function fi is monotone submodular, G(S) is not submodular (Balkanski et al., 2016), and thus using the regular greedy algorithm to directly build up S will give no theoretical guarantees. We also note that although G(S) is an instance of an XOS function (Feige, 2009), existing methods that use the XOS property would require an exponential number of evaluations in this scenario (Stan et al., 2017).\n1For notational convenience, we use A+ v = A ∪ {v}."
  }, {
    "heading": "4. Streaming Algorithm",
    "text": "In many applications, the data naturally arrives in a streaming fashion. This may be because the data is too large to fit in memory, or simply because the data is arriving faster than we can store it. Therefore, in the streaming setting we are shown one element at a time and we must immediately decide whether or not to keep this element. There is a limited number of elements we can hold at any one time and once an element is rejected it cannot be brought back.\nThere are two general approaches for submodular maximization (under the cardinality constraint k) in the streaming setting: (i) Badanidiyuru et al. (2014) introduced a thresholding-based framework where each element from the stream is added only if its marginal value is at least 12k of the optimum value. The optimum is usually not known a priori, but they showed that with only a logarithmic increase in memory requirement, it is possible to efficiently guess the optimum value. (ii) Buchbinder et al. (2015) introduced streaming submodular maximization with preemption. At each step, they keep a solution A of size k with value f(A). Each incoming element is added if and only if it can be exchanged with a current element of A for a net gain of at least f(A)k . In this paper, we combine these two approaches in a novel and non trivial way in order to design a streaming algorithm (called REPLACEMENT-STREAMING) for the two-stage submodular maximization problem.\nThe goal of REPLACEMENT-STREAMING is to pick a set S of at most ` elements from the data stream, where we keep sets Ti ⊆ S, 1 ≤ i ≤ m as the solutions for functions fi. We continue to process elements until one of the two following conditions holds: (i) ` elements are chosen, or (ii) the data stream ends. This algorithm starts from empty sets S and {Ti}. For every incoming element ut, we use the subroutine EXCHANGE to determine whether we should keep that element or not. To formally describe EXCHANGE, we first need to define a few notations.\nWe define the marginal gain of adding an element x to a set A as follows: fi(x|A) = fi(x + A) − fi(A). For an element x and set A, REPi(x,A) is an element of A such that removing it from A and replacing it with x results in the largest gain for function fi, i.e.,\nREPi(x,A) = arg max y∈A fi(A+ x− y)− fi(A). (3)\nThe value of this largest gain is represented by ∆i(x,A) = fi(A+ x− REPi(x,A))− fi(A). (4)\nWe define the gain of an element x with respect to a set A as follows:\n∇i(x,A) = {\n1{fi(x|A)≥(α/k)·fi(A)}fi(x|A) if |A| < k, 1{∆i(x,A)≥(α/k)·fi(A)}∆i(x,A) o.w.,\nwhere 1 is the indicator function. That is, ∇i(x,A) tells us how much we can increase the value of fi(A) by either\nAlgorithm 1 EXCHANGE 1: Input: u, S, {Ti}, τ and α {∇i terms use α} 2: if |S| < ` then 3: if 1m ∑m i=1∇i(u, Ti) ≥ τ then\n4: S ← S + u 5: for 1 ≤ i ≤ m do 6: if∇i(u, Ti) > 0 then 7: if |Ti| < k then 8: Ti ← Ti + u 9: else\n10: Ti ← Ti + u− REP(u, Ti)\nadding x to A (if |A| < k) or optimally swapping it in (if |A| = k). However, if this potential increase is less than α k · fi(A), then ∇i(x,A) = 0. In other words, if the gain of an element does not pass a threshold of αk · fi(A), we consider its contribution to be 0.\nAn incoming element is picked if the average of the ∇i terms is larger than or equal to a threshold τ . Indeed, for ut, the EXCHANGE routine computes the average gain 1 m ∑m i=1∇i(ut, Ti). If this average gain is at least τ , then ut is added to S; ut is also added to all sets Ti with ∇i(ut, Ti) > 0. Algorithm 1 explains EXCHANGE in detail. Now we define the optimum solution to Eq. (2) by\nSm,` = arg max S⊆Ω,|S|≤`\n1\nm m∑ i=1 max |T |≤k,T⊆S fi(T ),\nwhere the optimum solution to each function is defined by\nSm,`i = arg max S⊆Sm,`,|S|≤k fi(S).\nWe define OPT = 1m ∑m i=1 fi(S m,` i ).\nIn Section 4.1, we assume that the value of OPT is known a priori. This allows us to design REPLACEMENTSTREAMING-KNOW-OPT, which has a constant factor approximation guarantee. Furthermore, in Section 4.2, we show how we can efficiently guess the value of OPT by a moderate increase in the memory requirement. This enables us to finally explain REPLACEMENT-STREAMING."
  }, {
    "heading": "4.1. Knowing OPT",
    "text": "If OPT is somehow known a priori, we can use REPLACEMENT-STREAMING-KNOW-OPT. As shown in Algorithm 2, we begin with empty sets S and {Ti}. For each incoming element ut, it uses EXCHANGE to update sets S and {Ti}. The threshold parameter τ in EXCHANGE is set to OPTβ` for a constant value of β. This threshold guarantees that if an element is added to S, then the average of functions fi over Ti’s is increased by a value of at least OPTβ` . Therefore, if we end up with ` elements in S, we guarantee that 1m ∑m i=1 fi(Ti) ≥ OPTβ . On the other hand, if |S| < `, we are still able to prove that our algorithm has picked good\nAlgorithm 2 REPLACEMENT-STREAMING-KNOW-OPT 1: Input: OPT, α and β 2: Output: Sets S and {Ti}1≤i≤m, where Ti ⊂ S 3: S ← ∅ and 4: Ti ← ∅ for all 1 ≤ i ≤ m 5: for every arriving element ut do 6: EXCHANGE(ut, S, {Ti}, OPTβ` , α) 7: Return: S and {Ti}1≤i≤m\nenough elements such that 1m ∑m i=1 fi(Ti) ≥ α·(β−1)·OPT β·((α+1)2+α) . The pseudocode of REPLACEMENT-STREAMING-KNOWOPT is provided in Algorithm 2. Theorem 1. The approximation factor of REPLACEMENT-STREAMING-KNOW-OPT is at least min{ α(β−1)β·((α+1)2+α) , 1β }. Hence, for α = 1 and β = 6 the competitive ratio is at least 1/6."
  }, {
    "heading": "4.2. Guessing OPT in the Streaming Setting",
    "text": "In this section, we discuss ideas on how to efficiently guess the value of OPT, which is generally not known a priori. First consider Lemma 1, which provides bounds on OPT. Lemma 1. Assume δ = 1m maxu∈Ω ∑m i=1 fi(u). Then we have δ ≤ OPT ≤ ` · δ.\nNow consider the following set:\nΓ = {(1 + )l | l ∈ Z, δ 1 + ≤ (1 + )l ≤ ` · δ}\nWe define τl = (1 + )l. From Lemma 1, we know that one of the τl ∈ Γ is a good estimate of OPT. More formally, there exists a τl ∈ Γ such that OPT1+ ≤ τl ≤ OPT. For this reason, we should run parallel instances of Algorithm 2, one for each τl ∈ Γ. The number of such thresholds is O( log ` ). The final answer is the best solution obtained among all the instances.\nNote that we do not know the value of δ in advance. So we would need to make one pass over the data to learn δ, which is not possible in the streaming setting. The question is, can we get a good enough estimate of δ within a single pass over the data? Let’s define δt = 1m maxut′ ,t′≤t ∑m i=1 fi(u\nt′) as our current guess for the maximum value of δ. Unfortunately, getting δt as an estimate of δ does not resolve the problem. This is due to the fact that a newly instantiated threshold τ could potentially have already seen elements with additive value of τ/(β`). For this reason, we instantiate thresholds for an increased range of δt/(1 + ) ≤ τl ≤ ` · β · δt. To show that this new range would solve the problem, first consider the next lemma. Lemma 2. For the maximum gain of an incoming element ut, we have 1m ∑m i=1∇i(ut, T t−1i ) ≤ δt.\nWe need to show that for a newly instantiated threshold τ at time t+ 1, the gain of all elements which arrived before\nAlgorithm 3 REPLACEMENT-STREAMING\n1: Γ0 = {(1 + )l|l ∈ Z} 2: For each τ ∈ Γ0 set Sτ ← ∅ and Tτ,i ← ∅ for all\n1 ≤ i ≤ m {Maintain the sets lazily} 3: δ0 ← 0 4: for every arriving element ut do 5: δt = max{δt−1, 1m ∑m i=1 fi(u\nt)} 6: Γt = {(1 + )l | l ∈ Z, δt(1+ )·β·` ≤ (1 + )l ≤ δt} 7: Delete all Sτ and Tτ,i such that τ /∈ Γt 8: for all τ ∈ Γt do 9: EXCHANGE(ut, Sτ , {Tτ,i}1≤i≤m, τ, α)\n10: Return: arg maxτ∈Γn{ 1m ∑m i=1 fi(Tτ,i)}\ntime t+ 1 is less than τ ; therefore this new instance of the algorithm would not have picked them if it was instantiated from the beginning. To prove this, note that since τ is a new threshold at time t + 1, we have τ > `·β·δ t\nβ·` = δ t.\nFrom Lemma 2 we conclude that the marginal gain of all the ut ′ , t′ ≤ t is less than τ and EXCHANGE would not have picked them. The REPLACEMENT-STREAMING algorithm is shown pictorially in Figure 1 and the pseudocode is given in Algorithm 3.\nTheorem 2. Algorithm 3 satisfies the following properties:\n• It outputs sets S and {Ti} ⊂ S for 1 ≤ i ≤ m, such that |S| ≤ `, |Ti| ≤ k and 1m ∑m i=1 fi(Ti) ≥\nmin{ α(β−1)β((α+1)2+α) , 1β(1+ )} · OPT.\n• For α = 1 and β = 6+ 1+ the approximation factor is at least 16+ . For = 1.0 the approximation factor is 1/7.\n• It makes one pass over the dataset and stores at most O( ` log ` ) elements. The update time per each element is O(km log ` ).\nAlgorithm 4 REPLACEMENT-DISTRIBUTED 1: for e ∈ Ω do 2: Assign e to a machine chosen uniformly at random 3: Run REPLACEMENT-GREEDY on each machine l to\nobtain Sl and {T li } for 1 ≤ i ≤ m 4: S, {Ti} ← arg maxSl,{T li } 1 m ∑m i=1 fi(T l i )\n5: S′, {T ′i} ← REPLACEMENT-GREEDY( ⋃ l S\nl) 6: Return: arg max{ 1m ∑m i=1 fi(Ti), 1 m ∑m i=1 fi(T ′ i )}"
  }, {
    "heading": "5. Distributed Algorithm",
    "text": "In recent years, there have been several successful approaches to the problem of distributed submodular maximization (Kumar et al., 2015; Mirzasoleiman et al., 2013; Mirrokni & Zadimoghaddam, 2015; Barbosa et al., 2015). Specifically, Barbosa et al. (2015) proved that the following simple procedure results in a distributed algorithm with a constant factor approximation guarantee: (i) randomly split the data amongst M machines, (ii) run the classical greedy on each machine and pass outputs to a central machine, (iii) run another instance of the greedy algorithm over the union of all the collected outputs from all M machines, and (iv) output the maximizing set amongst all the collected solutions. Although our objective function G(S) is not submodular, we use a similar framework and still manage to prove that our algorithms achieve constant factor approximations to the optimal solution.\nIn REPLACEMENT-DISTRIBUTED (Algorithm 4), a central machine first randomly partitions data among M machines. Next, each machine runs REPLACEMENT-GREEDY (Stan et al., 2017) on its assigned data. The outputs Sl, {T li } of all the machines are sent to the central machine, which runs another instance of REPLACEMENT-GREEDY over the union of all the received answers. Finally, the highest value set amongst all collected solutions is returned as the final answer. See Appendix F for a detailed explanation of REPLACEMENT-GREEDY.\nTheorem 3. The REPLACEMENT-DISTRIBUTED algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that\nE[ 1\nm m∑ i=1 fi(T ∗ i )] ≥ α 2 · OPT,\nwhere α = 12 (1− 1e2 ). The time complexity of algorithm is O(km`n/M + Mkm`2).\nUnfortunately, for very large datasets, the time complexity of REPLACEMENT-GREEDY could be still prohibitive. For this reason, we can use a modified version of REPLACEMENT-STREAMING (called REPLACEMENTPSEUDO-STREAMING) to design an even more scalable distributed algorithm. This algorithm receives all elements in a centralized way, but it uses a predefined order to generate a (pseudo) stream before processing the data. This\nconsistent ordering is used to ensure that the output of REPLACEMENT-PSEUDO-STREAMING is independent of the random ordering of the elements. The only other difference between REPLACEMENT-PSEUDO-STREAMING and REPLACEMENT-STREAMING is that it outputs all sets Sτ , {Tτ,i} for all τ ∈ Γn (instead of just the maximum). We use this modified algorithm as one of the main building blocks for DISTRIBUTED-FAST (outlined in Appendix E).\nTheorem 4. The DISTRIBUTED-FAST algorithm outputs sets S∗, {T ∗i } ⊂ S, with |S∗| ≤ `, |T ∗i | ≤ k, such that\nE[ 1\nm m∑ i=1 fi(T ∗ i )] ≥ α · γ α+ γ · OPT,\nwhere α = 12 (1 − 1e2 ) and γ = 16+ . The time complexity of algorithm is O(kmn log /̀M + Mkm`2 log `).\nFrom Theorems 3 and 4, we conclude that the optimum number of machines M for REPLACEMENT-DISTRIBUTED and DISTRIBUTED-FAST isO( √ n/`) andO( √ n/`), respectively. Therefore, DISTRIBUTED-FAST is a factor of O( √ n/log `) and O( √ /̀log `) faster than REPLACEMENT-GREEDY and REPLACEMENT-DISTRIBUTED, respectively."
  }, {
    "heading": "6. Applications",
    "text": "In this section, we evaluate the performance of our algorithms in both the streaming and distributed settings. We compare our work against several different baselines."
  }, {
    "heading": "6.1. Streaming Image Summarization",
    "text": "In this experiment, we will use a subset of the VOC2012 dataset (Everingham et al.). This dataset has images containing objects from 20 different classes, ranging from birds to boats. For the purposes of this application, we will use n = 756 different images and we will consider all m = 20 classes that are available. Our goal is to choose a small subset S of images that provides a good summary of the entire ground set Ω. In general, it can be difficult to even define what a good summary of a set of images should look like. Fortunately, each image in this dataset comes with a human-labelled annotation that lists the number of objects from each class that appear in that image.\nUsing the exemplar-based clustering approach (Mirzasoleiman et al., 2013), for each image we generate an mdimensional vector x such that xi represents the number of objects from class i that appear in the image (an example is given in Appendix G). We define Ωi to be the set of all images that contain objects from class i, and correspondingly Si = Ωi ∩ S (i.e. the images we have selected that contain objects from class i).\nWe want to optimize the following monotone submodular functions: fi(S) = Li({e0}) − Li(S ∪ {e0}), where Li(S) =\n1 |Ωi| ∑ x∈Ωi miny∈Si d(x, y). We use d(x, y) to\ndenote the “distance” between two images x and y. More accurately, we measure the distance between two images as the `2 norm between their characteristic vectors. We also use e0 to denote some auxiliary element, which in our case is the all-zero vector.\nSince image data is generally quite storage-intensive, streaming algorithms can be particularly desirable. With this in mind, we will compare our streaming algorithm REPLACEMENT-STREAMING against the non-streaming baseline of REPLACEMENT-GREEDY. We also compare against a heuristic streaming baseline that we call STREAMSUM. This baseline first greedily optimizes the submodular function F (S) = ∑m i=1 fi(S) using the streaming algorithm developed by Buchbinder et al. (2015). Having selected ` elements from the stream, it then constructs each Ti by greedily selecting k of these elements for each fi.\nTo evaluate the various algorithms, we consider two primary metrics: the objective value, which we define as∑m i=1 fi(Ti), and the wall-clock running time. We note that the trials were run using Python 2.7 on a quad-core Linux machine with 3.3 GHz Intel Core i5 processors and 8 GB of RAM. Figure 2 shows our results.\nThe graphs are organized so that each column shows the effects of varying a particular parameter, with the objective value being shown in the top row and the running time in the bottom row. The primary observation across all the graphs is that our streaming algorithm REPLACEMENT-STREAMING not only achieves an objective value that is similar to that of the non-streaming baseline REPLACEMENT-GREEDY, but it also speeds up the running time by a full order of magnitude. We also see that REPLACEMENT-STREAMING outperforms the streaming baseline STREAM-SUM in both objective value and running time.\nAnother noteworthy observation from Figure 2(c) is that can be increased all the way up to = 0.5 before we start to see loss in the objective value. Recall that is the parameter that trades off the accuracy of REPLACEMENT-STREAMING with the running time by changing the granularity of our guesses for OPT. As seen Figure 2(f), increasing up to 0.5 also covers the majority of running time speed-up, with diminishing returns kicking in as we get close to = 1.\nAlso in the context of running time, we see in Figure 2(e) that REPLACEMENT-STREAMING actually speeds up as k increases. This seems counter-intuitive at first glance, but one possible reason is that the majority of the time cost for these replacement-based algorithms comes from the swapping that must be done when the Ti’s fill up. Therefore, the longer each Ti is not completely full, the faster the overall algorithm will run.\nFigure 3 shows some sample images selected by REPLACEMENT-GREEDY (top) and REPLACEMENT-\nSTREAMING (bottom). Although the two summaries contain only one image that is exactly the same, we see that the different images still have a similar theme. For example, both images in the second column contain bikes and people; while in the third column, both images contain sheep."
  }, {
    "heading": "6.2. Distributed Ride-Share Optimization",
    "text": "In this application we want to use past Uber data to select optimal waiting locations for idle drivers. Towards this end, we analyze a dataset of 100,000 Uber pick-ups in Manhattan from September 2014 (UberDataset), where each entry in the dataset is given as a (latitude, longitude) coordinate pair. We model this problem as a classical facility location problem, which is known to be monotone submodular.\nGiven a set of potential waiting locations for drivers, we want to pick a subset of these locations so that the distance from each customer to his closest driver is minimized. In\nparticular, given a customer location a = (xa, ya), and a waiting driver location b = (xb, yb), we define a “convenience score” c(a, b) as follows: c(a, b) = 2− 2\n1+e−200d(a,b) ,\nwhere d(a, b) = |xa − xb|+ |ya − yb| is the Manhattan distance between the two points.\nNext, we need to introduce some functions we want to maximize. For this experiment, we can think about different functions corresponding to different (possibly overlapping) regions around Manhattan. The overlap means that there will still be some inherent connection between the functions, but they are still relatively distinct from each other. More specifically, we construct regions R1, . . . , Rm by randomly picking m points across Manhattan. Then, for each point pi, we want to define the corresponding region Ri by all the pick-ups that have occurred within one kilometer of pi. However, to keep the problem computationally tractable, we instead randomly select only ten pick-up locations within that same radius. Figure 4(a) shows the center points of the m = 20 randomly selected regions, overlaid on top of a heat map of all the customer pick-up locations.\nGiven any set of driver waiting locations Ti, we define fi(Ti) as follows: fi(Ti) = ∑ a∈Ri maxb∈Ti c(a, b). For this application, we will use every customer pick-up location as a potential waiting location for a driver, meaning we have 100,000 elements in our ground set Ω. This large number of elements, combined with the fact that each single function evaluation is computationally intensive, means running the regular REPLACEMENT-GREEDY will be prohibitively ex-\npensive. Hence, we will use this setup to evaluate the two distributed algorithms we presented in Section 5. We will also compare our algorithms against a heuristic baseline that we call DISTRIBUTED-GREEDY. This baseline will first select ` elements using the greedy distributed framework introduced by Mirzasoleiman et al. (2013), and then greedily optimize each fi over these ` elements.\nEach algorithm produces two outputs: a small subset S of potential waiting locations (with size ` = 30), as well as a solution Ti (of size k = 3) for each function fi. In other words, each algorithm will reduce the number of potential waiting locations from 100,000 to 30, and then choose 3 different waiting locations for drivers in each region.\nIn Figure 4(b), we graph the average distance from each customer to his closest driver, which we will refer to as the cost. One interesting observation is that while the cost of DISTRIBUTED-FAST decreases with the number of machines, the costs of the other two algorithms stay relatively constant, with REPLACEMENT-DISTRIBUTED marginally outperforming DISTRIBUTED-GREEDY. In Figure 4(c), we graph the run time of each algorithm. We see that the algorithms achieve their optimal speeds at different values of M , verifying the theory at the end of Section 5. Overall, we see that while all three algorithms have very comparable costs, DISTRIBUTED-FAST is significantly faster than the others.\nWhile in the previous application we only looked at the\nobjective value for the given functions f1, . . . , fm, in this experiment we also evaluate the utility of our summary on new functions drawn from the same distribution. That is, using the regions shown in Figure 4(a), each algorithm will select a subset S of potential waiting locations. Using only these reduced subsets, we then greedily select k waiting locations for each of the twenty new regions shown in 4(d).\nIn Figure 4(e), we see that the summaries from all three algorithms achieve a similar cost, which is significantly better than RANDOM. In this scenario, RANDOM is defined as the cost achieved when optimizing over a random size ` subset and OPTIMAL is defined as the cost that is achieved when optimizing the functions over the entire ground set rather than a reduced subset. In Figure 4(f), we confirm that DISTRIBUTED-FAST is indeed the fastest algorithm for constructing each summary. Note that 4(f) is demonstrating how long each algorithm takes to construct a size ` summary, not how long it is taking to optimize over this summary."
  }, {
    "heading": "7. Conclusion",
    "text": "To satisfy the need for scalable data summarization algorithms, this paper focused on the two-stage submodular maximization framework and provided the first streaming and distributed solutions to this problem. In addition to constant factor theoretical guarantees, we demonstrated the effectiveness of our algorithms on real world applications in image summarization and ride-share optimization."
  }, {
    "heading": "Acknowledgements",
    "text": "Amin Karbasi was supported by a DARPA Young Faculty Award (D16AP00046) and a AFOSR Young Investigator Award (FA9550-18-1-0160). Ehsan Kazemi was supported by the Swiss National Science Foundation (Early Postdoc.Mobility) under grant number 168574."
  }],
  "year": 2018,
  "references": [{
    "title": "Streaming submodular maximization: Massive data summarization on the fly",
    "authors": ["A. Badanidiyuru", "B. Mirzasoleiman", "A. Karbasi", "A. Krause"],
    "venue": "In ACM KDD,",
    "year": 2014
  }, {
    "title": "Learning sparse combinatorial representations via twostage submodular maximization",
    "authors": ["E. Balkanski", "B. Mirzasoleiman", "A. Krause", "Y. Singer"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "The power of randomization: Distributed submodular maximization on massive datasets",
    "authors": ["R. Barbosa", "A. Ene", "H. Nguyen", "J. Ward"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Online submodular maximization with preemption",
    "authors": ["N. Buchbinder", "M. Feldman", "R. Schwartz"],
    "venue": "In SODA. Society for Industrial and Applied Mathematics,",
    "year": 2015
  }, {
    "title": "Interactive submodular bandit",
    "authors": ["L. Chen", "A. Krause", "A. Karbasi"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Streaming weak submodularity: Interpreting neural networks on the fly",
    "authors": ["E. Elenberg", "A.G. Dimakis", "M. Feldman", "A. Karbasi"],
    "year": 2017
  }, {
    "title": "On maximizing welfare when utility functions are subadditive",
    "authors": ["U. Feige"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Do Less, Get More: Streaming Submodular Maximization with Subsampling",
    "authors": ["M. Feldman", "A. Karbasi", "E. Kazemi"],
    "venue": "CoRR, abs/1802.07098,",
    "year": 2018
  }, {
    "title": "Submodular functions and optimization",
    "authors": ["S. Fujishige"],
    "venue": "Elsevier Science,",
    "year": 2005
  }, {
    "title": "Adaptive submodular maximization in bandit settings",
    "authors": ["V. Gabillon", "B. Kveton", "Z. Wen", "B. Eriksson", "S. Muthukrishnan"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Inferring networks of diffusion and influence",
    "authors": ["M. Gomez Rodriguez", "J. Leskovec", "A. Krause"],
    "venue": "In KDD,",
    "year": 2010
  }, {
    "title": "Online submodular minimization for combinatorial structures",
    "authors": ["S. Jegelka", "J.A. Bilmes"],
    "venue": "In ICML, Bellevue, Washington,",
    "year": 2011
  }, {
    "title": "DeletionRobust Submodular Maximization at Scale",
    "authors": ["E. Kazemi", "M. Zadimoghaddam", "A. Karbasi"],
    "venue": "CoRR, abs/1711.07112,",
    "year": 2017
  }, {
    "title": "Maximizing the spread of influence through a social network",
    "authors": ["D. Kempe", "J. Kleinberg", "E. Tardos"],
    "venue": "In KDD,",
    "year": 2003
  }, {
    "title": "Submodularity for data selection in statistical machine translation",
    "authors": ["K. Kirchhoff", "J. Bilmes"],
    "venue": "In EMNLP,",
    "year": 2014
  }, {
    "title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems",
    "authors": ["A. Krause", "D. Golovin"],
    "year": 2012
  }, {
    "title": "Near-optimal nonmyopic value of information in graphical models",
    "authors": ["A. Krause", "C. Guestrin"],
    "venue": "In UAI,",
    "year": 2005
  }, {
    "title": "Fast greedy algorithms in mapreduce and streaming",
    "authors": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"],
    "venue": "ACM Transactions on Parallel Computing,",
    "year": 2015
  }, {
    "title": "A class of submodular functions for document summarization",
    "authors": ["H. Lin", "J. Bilmes"],
    "venue": "In ACL,",
    "year": 2011
  }, {
    "title": "Learning mixtures of submodular shells with application to document summarization",
    "authors": ["H. Lin", "J. Bilmes"],
    "venue": "In UAI,",
    "year": 2012
  }, {
    "title": "Randomized composable core-sets for distributed submodular maximization",
    "authors": ["V. Mirrokni", "M. Zadimoghaddam"],
    "venue": "In STOC. ACM,",
    "year": 2015
  }, {
    "title": "Distributed submodular maximization: Identifying representative elements in massive data",
    "authors": ["B. Mirzasoleiman", "A. Karbasi", "R. Sarkar", "A. Krause"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions - I",
    "authors": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "Near-optimally teaching the crowd to classify",
    "authors": ["A. Singla", "I. Bogunovic", "G. Bartók", "A. Karbasi", "A. Krause"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Probabilistic submodular maximization in sub-linear time",
    "authors": ["S. Stan", "M. Zadimoghaddam", "A. Krause", "A. Karbasi"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "80 million tiny images: A large data set for nonparametric object and scene recognition",
    "authors": ["A. Torralba", "R. Fergus", "W.T. Freeman"],
    "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
    "year": 2008
  }, {
    "title": "Using document summarization techniques for speech data subset selection",
    "authors": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"],
    "venue": "In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2013
  }, {
    "title": "Linear submodular bandits and their application to diversified retrieval",
    "authors": ["Y. Yue", "C. Guestrin"],
    "venue": "In NIPS,",
    "year": 2011
  }],
  "id": "SP:30e28beb92239447aff0718119195c0539aa58d8",
  "authors": [{
    "name": "Marko Mitrovic",
    "affiliations": []
  }, {
    "name": "Ehsan Kazemi",
    "affiliations": []
  }, {
    "name": "Morteza Zadimoghaddam",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set. In this paper, we develop the first streaming and distributed solutions to this problem. In addition to providing strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share optimization.",
  "title": "Data Summarization at Scale:A Two-Stage Submodular Approach"
}