{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Machine teaching (Zhu, 2015; 2013; Zhu et al., 2018) is the problem of constructing a minimal dataset for a target concept such that a student model (i.e., leaner) can learn the target concept based on this minimal dataset. Recently, machine teaching has been shown very useful in applications ranging from human computer interaction (Suh et al., 2016), crowd sourcing (Singla et al., 2014; 2013) to cyber security (Alfeld et al., 2016; 2017). Besides various applications, machine teaching also has nice connections with curriculum learning (Bengio et al., 2009; Hinton et al., 2015). In traditional machine learning, a teacher usually constructs a batch set of training samples, and provides them to a student in one shot without further interactions. Then the student keeps learning from this batch dataset and tries to learn the target concept. Previous machine teaching paradigm (Zhu, 2013; 2015; Liu et al., 2016) usually focuses on constructing the smallest such dataset, and characterizing the size of such dataset, called the teaching dimension of the student model.\n*Equal contribution 1Georgia Tech 2University of Minnesota 3Ant Financial. Correspondence to: W. L. <wyliu@gatech.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nFor machine teaching to work effectively in practical scenarios, (Liu et al., 2017a) propose an iterative teaching framework which takes into consideration that the learner usually uses iterative algorithms (e.g. gradient descent) to update the models. Different from the traditional machine teaching framework where the teacher only interacts with the student in one-shot, the iterative machine teaching allows the teacher to interact with the student in every single iteration. It hence shifts the teaching focus from models to algorithms: the objective of teaching is no longer constructing a minimal dataset in one shot but searching for samples so that the student learns the target concept in a minimal number of iterations (i.e., fastest convergence for the student algorithm). Such a minimal number of iterations is called the iterative teaching dimension for the student algorithm. (Liu et al., 2017a) mostly consider the simplest iterative case where the teacher can fully observe the student. This case is interesting in theory but too restrictive in practice.\nHuman teaching is arguably the most realistic teaching scenario in which the learner is completely a black-box to the teacher. Analogously, the ultimate problem for machine teaching is how to teach a black-box learner. We call such problem black-box machine teaching. Inspired by the fact that the teacher and the student typically represent the same concept but in different ways, we present a step towards the black-box machine teaching – cross-space machine teaching, where the teacher i) does not share the same feature representation with the student, and ii) can not observe the\nstudent model. This setting is interesting in the sense that it can both relax the assumptions for iterative machine teaching and improve our understanding on human learning.\nInspired by a real-life fact, that a teacher will regularly examine the student to learn how well the student has mastered the concept, we propose an active teacher model to address the cross-space teaching problem. The active teacher is allowed to actively query the student with a few (limited) samples every certain number of iterations, and the student can only return the corresponding prediction results to the teacher. For example, if the student uses a linear regression model, it will return to the teacher its prediction 〈wt, x̃〉 where wt is the student parameter at the t-th iteration and x̃ is the representation of the query example in student’s feature space. Under suitable conditions, we show that the active teacher can always achieve faster rate of improvement than a random teacher that feeds samples randomly. In other words, the student model guided by the active teacher can provably achieve faster convergence than the stochastic gradient descent (SGD). Additionally, we discuss the extension of the active teacher to deal with the learner with forgetting behavior, and the learner guided by multiple teachers.\nTo validate our theoretical findings, we conduct extensive experiments on both synthetic data and real image data. The results show the effectiveness of the active teacher."
  }, {
    "heading": "2. Related Work",
    "text": "Machine teaching defines a task where we need to find an optimal training set given a learner and a target concept. (Zhu, 2015) describes a general teaching framework which has nice connections to curriculum learning (Bengio et al., 2009) and knowledge distillation (Hinton et al., 2015). (Zhu, 2013) considers Bayesian learners in exponential family and formulates the machine teaching as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. (Liu et al., 2016) give the teaching dimension of linear learners. Machine teaching has been found useful in cyber security (Mei & Zhu, 2015), human computer interaction (Meek et al., 2016), and human education (Khan et al., 2011). (Johns et al., 2015) extend machine teaching to human-in-the-loop settings. (Doliwa et al., 2014; Gao et al., 2015; Zilles et al., 2008; Samei et al., 2014; Chen et al., 2018) study the machine teaching problem from a theoretical perspective.\nPrevious machine teaching works usually ignore the fact that a student model is typically optimized by an iterative algorithm (e.g., SGD), and in practice we focus more on how fast a student can learn from the teacher. (Liu et al., 2017a) propose the iterative teaching paradigm and an omniscient teaching model where the teacher knows almost everything about the learner and provides training examples based on the learner’s status. Our cross-space teaching serves as a stepping stone towards the black-box iterative teaching."
  }, {
    "heading": "3. Cross-Space Iterative Machine Teaching",
    "text": "The cross-space iterative teaching paradigm is different from the standard iterative machine teaching in terms of two major aspects: i) the teacher does not share the feature representation with the student; ii) the teacher cannot observe the student’s current model parameter in each iteration. Specifically, we consider the following teaching settings:\nTeacher. The teacher model observes a sample A (e.g. image, text, etc.) and represents it as a feature vector xA∈ Rd and a label y∈R. The teacher knows the model (e.g., loss function) and the optimization algorithm (including the learning rate1) of the learner, and the teacher preserves an optimal parameter v∗ of this model in its own feature space. We denote the prediction of the teacher as ŷv∗=〈v∗, x〉2. Learner. The learner observes the same sample A and represents it as a vectorized feature x̃A∈Rs and a label ỹ∈R. The learner uses a linear model 〈w, x̃〉 where w is its model parameter and updates it with SGD (if guided by a passive teacher). We denote the prediction of the student model as ŷtw=〈wt, x̃〉 in t-th iteration. Representation. Although the teacher and learner do not share the feature representation, we still assume their representations have an intrinsic relationship. For simplicity, we assume there exists a unknown one-to-one mapping G from the teacher’s feature space to the student’s feature space such that x̃=G(x). However, the conclusions in this paper are also applicable to injective mappings. Unless specified, we assume that y = ỹ by default.\nInteraction. In each iteration, the teacher will provide a training example to the learner and the learner will update its model using this example. The teacher cannot directly observe the model parameter w of the student. In this paper, the active teacher is allowed to query the learner with a few examples every certain number of iterations. The learner can only return to the teacher its prediction 〈wt, x̃〉 in the regression scenario, its predicted label sign(〈wt, x̃〉) or confidence score S(〈wt, x̃〉) in the classification scenario, where wt is the student’s model parameter at t-th iteration and S(·) is some nonlinear function. Note that the teacher and student preserve the same loss function `(·, ·). Similar to (Liu et al., 2017a), we consider three ways for the teacher to provide examples to the learner:\nSynthesis-based teaching. In this scenario, the space of provided examples is\nX = {x ∈ Rd, ‖x‖ ≤ R} Y = R (Regression) or {−1, 1} (Classification).\nCombination-based teaching. In this scenario, the space 1For simplicity, the teacher is assumed to know the learning rate of the learner, but this prior is not necessary, as discussed later. 2For simplicity, we omit the bias term throughout the paper. It is straightforward to add them back.\nof provided examples is (αi ∈ R) X = { x| ‖x‖ ≤ R, x = Σki=1αixi, xi ∈ D } ,D = {x1, . . . , xk}\nY = R (Regression) or {−1, 1} (Classification)\nRescalable pool-based teaching. This scenario further restrict the knowledge pool for samples. The teacher can pick examples from X ×Y: X = {x| ‖x‖ ≤ R, x = γxi, xi ∈ D, γ ∈ R},D = {x1, . . .} Y = R (Regression) or {−1, 1} (Classification)\nWe also note that the pool-based teaching (without rescalability) is the most restricted teaching scenario and it is very close to the practical settings."
  }, {
    "heading": "4. The Active Teaching Algorithm",
    "text": "To address the cross-space iterative machine teaching, we propose the active teaching algorithm, which actively queries its student for its prediction output. We first describe the general version of the active teaching algorithm. Then without loss of generality, we will discuss three specific examples: least square regression (LSR) learner for regression, logistic regression (LR) and support vector machine (SVM) learner for classification (Friedman et al., 2001)."
  }, {
    "heading": "4.1. General Algorithm",
    "text": "Inspired by human teaching, we expand the teacher’s capabilities by enabling the teacher to actively query the student. The student will return its predictions to the teacher. Based on the student’s feedback, The teacher will estimate the student’s status and determine which example to provide next time. The student’s feedback enables the active teacher to teach without directly observing the student’s model.\nThe active teacher can choose to query the learner with a few samples in each iteration, and the learner will usually report the prediction F (〈w, x̃〉) where F (·) denotes some function of the inner product prediction. For example, we usually have F (z) = z for regression and F (z)=sign(z) or F (z)= 11+exp(−z) for classification. Based on our assumption that there is an unknown mapping from teacher’s feature to student’s feature, there also exists a mapping from the model parameters of the teacher to those of the student. These active queries enables the teacher to estimate the student’s corresponding model parameter “in the teacher’s space” and maintain a virtual learner, the teacher’s estimation of the real learner, in its own space. The teacher will\ndecide which example to provide based on its current virtual learner model. The ideal virtual learner v will have the same prediction output as the real learner, i.e. 〈v, x〉=〈w, x̃〉 where x̃=G(x). Equivalently, v=G>(w) always holds for the ideal virtual learner, where G> is the conjugate mapping of G. Note that for the purpose of analysis, we assume that G is a generic linear operator, though our analysis can easily extends to general cases. In fact, one of the most important challenges in active teaching is to recover a virtual student that approximates the real leaner as accurately as possible. The estimation error of the teacher may affect the quality of training examples that the teacher provides for the real learner. Intuitively, if we can recover the virtual learner with an appropriate accuracy, then we can still achieve faster teaching speed than that of passive learning. Fig. 2 shows the pipeline of the cross-space teaching.\nWith full access to the obtained virtual learner in the teacher’s space, the teacher can perform omniscient teaching as in (Liu et al., 2017a). Specifically, the active teacher will optimize the following objective:\nargmin x∈X ,y∈Y\nη 2 t ∥∥∥∥∥∂`( 〈 vt, x 〉 , y) ∂vt ∥∥∥∥∥ 2\n2\n− 2ηt 〈 v t − v∗, ∂`( 〈 vt, x 〉 , y)\n∂vt\n〉 (1)\nwhere ` is a loss function and vt is the teacher’s estimation of G>(wt) after the teacher performs an active query in t-th iteration (i.e., the current model parameter of the virtual learner). ηt is the learning rate of the virtual learner. The learning rate of the student model is not necessarily needed. The general teaching algorithm is given in Algorithm 1.\nParticularly, different types of feedback (i.e., the form of F (·)) from learners contain different amount of information, resulting in different levels of difficulties in recovering the parameters of the learner’s model. We will discuss two general ways to recover the virtual learner for two types of frequently used feedbacks in practice. Exact recovery of the virtual learner. We know that the learner returns a prediction in the form of F (〈w, x̃〉). In general, if F (·) is an one-to-one mapping, we can exactly recover the ideal virtual learner (i.e. G>(w)) in the teacher’s space using the system of linear equations. In other words, the recovery of virtual learner could be exact as long as there is no information loss from 〈w, x̃〉 to F (〈w, x̃〉). Specifically, we have 〈v, qj〉=〈w, q̃j〉 where qj is the j-th query for the learner. Because 〈w, q̃j〉 is given by the real learner, we only need to construct d queries (d is the dimension of the teacher space) and require {q1, q2, · · · , qd} to be linearly independent to estimate v. Without no numerical error, we can exactly recover v. Since the recovery is exact, we have G>(w)=v. Note that there are cases that we can achieve exact recovery without F (·) being an one-to-one mapping. For example, F (z) = max(0, z) (hinge function) is not an one-to-one mapping but we can still achieve exact recovery. Approximate recovery of the virtual learner. If F (·) is not an one-to-one mapping (e.g., sign(·) which provides\nAlgorithm 1 The active teacher 1: Randomly initialize the student parameter w0; 2: Set t = 1, exam = True (i.e., whether we make the student\ntakes exams) and maximal iteration number T ; 3: while vt has not converged or t < T do 4: if G>G 6= I and exam = True then 5: Obtain an estimation Ĝ>(wt) of the student model in\nthe teacher’s space using the virtual learner construction Algorithm 2;\n6: vt = Ĝ>(wt); 7: else if G>G = I and exam = True then 8: Perform the one-time “background” exam using Algorithm 2 and set exam to False; 9: end if 10: Solve the optimization for the virtual learner (e.g. poolbased teaching):\n(xt, yt) = argmin x∈X ,y∈Y η2t ∥∥∥∥∥∂` (〈 vt−1, x 〉 , y ) ∂vt−1 ∥∥∥∥∥ 2\n− 2ηt 〈 vt−1 − v∗, ∂` (〈 vt−1, x 〉 , y )\n∂vt−1 〉 11: if exam = False then 12: Use the selected example (xt, yt) to perform the update\nof the virtual learner in the teacher’s space: vt = vt−1 − ηt ∂` (〈 vt−1, xt 〉 , yt )\n∂vt−1 .\n13: end if 14: Use the selected example (x̃t, ỹt) where x̃=G(x), ỹ=y to\nperform the update of the real learner in the student’s space: wt = wt−1 − ηt ∂` (〈 wt−1, x̃t 〉 , ỹt )\n∂wt−1 .\n15: t← t+ 1; 16: end while\n1-bit feedback), then generally we may not be able to exactly recover the student’s parameters. Therefore, we have to develop a more intelligent technique (i.e. less sample complexity) to estimate G>(w). In this paper, we use active learning (Settles, 2010) to help the teacher better estimate G>(w) for the virtual learner. One of the difficulties is that the active learning algorithm obtains the parameters of a model based on the predicted labels on which the norm of the weights has no effect. It becomes ambiguous which set of weights the teacher should choose. Therefore, the active teacher also needs to have access to the norm of the student’s weights for recovering the virtual learner. In the following sections, we will develop and analyze our estimation algorithm for the virtual learner based on the existing active learning algorithms with guarantees on sample complexity (Balcan et al., 2009; Ailon, 2012; Hanneke, 2007; Schein & Ungar, 2007; Settles, 2010)."
  }, {
    "heading": "4.2. Least Square Regression Learner",
    "text": "For the LSR learner, we use the following model:\nmin w∈Rs,b∈R\n1\nn n∑ i=1 1 2 (〈w, x̃i〉 − ỹi)2. (2)\nAlgorithm 2 The virtual learner construction 1: if The feedback function F (z) is an one-to-one mapping or a\nhinge function then 2: Perform one-time exam by actively query multiple examples; 3: Solve a system of linear equations to obtain the exact recovery of the ideal virtual learner; 4: else 5: Apply acitve learning algorithms to perform an approximate\nrecovery of the ideal virtual learner (in this case, the teacher will need to know the norm of the student model);\n6: end if\nBecause F (〈w, x̃〉)=〈w, x̃〉, the LSR learner belongs to the case where the active teacher can exactly recover the ideal virtual learner. When G>G = I , the teacher only need to perform active exam once. It can be viewed as a “background exam” for the teacher to figure out how well the student has mastered the knowledge at the beginning, and the teacher can track the dynamics of students exactly later. Otherwise, for a general one-to-one mapping G, the teacher needs to query the student in each iteration. Still, the teacher can reuse the same set of queries in all iterations."
  }, {
    "heading": "4.3. Logistic Regression Learner",
    "text": "For the LR learner, we use the following model (without loss of generality, we consider the binary classification):\nmin w∈Rs,b∈R\n1\nn n∑ i=1 log ( 1 + exp{−ỹi(〈w, x̃i〉)} ) (3)\nWe discuss two cases separately: (1) the learner returns the probability of each class (i.e. F (z) = S(z) where S(·) denotes a sigmoid function); (2) the learner only returns the predicted label (i.e. F (z) = sign(z)).\nIn the first case where F (·) is a sigmoid function, we can exactly recover the ideal virtual learner. This case is essentially similar to the LSR learner where we need only one “background exam” if G>G=I and we can reuse the queries in each iteration for a general one-to-one mapping G (G>G 6=I). In the second case where F (·) is a sign function, we can only approximate the ideal virtual learner with some error. In this case, we use active learning to do the recovery."
  }, {
    "heading": "4.4. Support Vector Machine Learner",
    "text": "For the SVM learner, we use the following model for the binary classification:\nmin w∈Rs,b∈R\n1\nn n∑ i=1 max(1− yi(wT x̃i + b), 0) (4)\nSimilarly, we have two cases: (1) the learner returns the hinge value of each class (i.e. F (z)=max(0, z); (2) the learner only returns the label (i.e. F (z) = sign(z)).\nIn the first case where F (·) is a hinge function, we can still recover the ideal virtual learner. Although the hinge function is not a bijective mapping (only half of it is one-to-one),\nwe prove that it can still achieve exact recovery with slightly more query samples. For G>G = I , we need only one “background exam” as in the case of the LR learner. Otherwise, we still need to query the student in each iteration. In the second case where F (·) is a sign function, we can only approximate the ideal virtual learner with some error."
  }, {
    "heading": "5. Theoretical Results",
    "text": "We define an important notion of being “exponentially teachable” to characterize the teacher’s performance.\nDefinition 1 Given > 0, the loss function ` and feature mapping G, (`,G) is exponentially teachable (ET) if the number of total samples (teaching samples and query samples) is t = O(poly(log 1 )) for a learner to achieve - approximation, i.e.,\n∥∥G>(wt)− v∗∥∥ ≤ . Note that the potential dependence of t on the problem dimension is omitted here, which will be discussed in detail in the following. We summarize our theoretical results in Table 1. Given a learner that is exponentially teachable by the omniscient teacher, we find that the learner is not exponentially teachable by the active teacher only when F (·) is not an one-to-one mapping and the teacher uses rescalable pool-based teaching."
  }, {
    "heading": "5.1. Synthesis-Based Active Teaching",
    "text": "We denote σmax = maxx>x=1 G>(x)G(x) and σmin = minx>x=1 G>(x)G(x) > 0 (G is invertible). We first discuss the teaching algorithm when the teacher is able to exactly recover the student’s parameters. A generic theory for synthesis-based ET is provided as follows.\nTheorem 2 Suppose that the teacher can recover G>(wt) exactly using m samples at each iteration. If for any v∈Rd, there exists γ 6=0 and ŷ such that x̂=γ (v−v∗) and\n0 < γ∇〈vt,x̂〉` (〈 vt, x̂ 〉 , ŷ ) <\n2σmin ησ2max ,\nthen (`,G) is ET with O ( (m+ 1) log 1 ) samples.\nExistence of the exponentially teachable (`,G) via exact recovery. Different from (Liu et al., 2017a) where the condition for synthesis-based exponentially teaching is only related to the loss function `, the condition for the cross-space teaching setting is related to both loss function ` and feature mapping G. The spectral property of G is involved due to the differences of feature spaces, leading to the mismatch of parameters of the teacher and student. It is easy to see that ∃ G such that the commonly used loss functions, e.g., absolute loss, square loss, hinge loss, and logistic loss, are ET with exact recovery, i.e., G>(wt) = vt. This can be shown\nby construction. For example, if the σminσ2max = 1 2 , the ET condition will be the same for both omniscient teacher (Liu et al., 2017a) and active teacher.\nNext we present generic results of the sample complexity m required to recover G>, which is a constant to (i.e., (`,G) is ET), shown as follows.\nLemma 3 If F (·) is bijective, then we can exactly recover G>(w)∈Rd with d samples.\nLemma 4 If F (·) = max (0, ·), then we can exactly recover G>(w) ∈ Rd with 2d samples.\nLemma 3 and 4 cover F (·)=I(·), F (·)=S(·), or F (·)= max (0, ·), where I denotes the identity mapping and S denotes some sigmoid function, e.g., logistic function, hyperbolic tangent, error function, etc. If the student’s answers to the queries via these student feedbacks F (·) in the exam phase, then we can exactly recover v=G>(w)∈Rd with arbitrary d independent data, omitting the numerical error. Also note that the query samples in Lemma 3 and 4 can be reused in each iteration, thus the query sample complexity is m = O(d), which is formalized as follows.\nCorollary 5 Suppose that the student answers questions in query phase via F (·) = I(·), F (·) = S(·), or F (·) = max (0, ·), then (`,G) is ET with O ( log 1 ) teaching samples and O(d) query samples via exact recovery.\nHere we emphasize that the number of query samples (i.e. active queries) does not depend on specific tasks. For both regression and classification, as long as the student feedbacks F (·) are bijective functions, then Corollary 5 holds. The loss function only affects the synthesis or selection of the teaching samples.\nIn both regression and classification, if F (·) = sign(·) which only provides 1-bit feedback, F−1 no longer exists and the exact recovery of G>(w) may not be obtained. In such case, the teacher may only approximate the student’s parameter using active learning. We first present the generic result for ET via approximate recovery as follows.\nTheorem 6 Suppose that the loss function ` is L-Lipschitz smooth in a compact domain Ωv⊂Rd containing v∗ and sample candidates (x, y) are from bounded set X × Y , where X = { x∈Rd, ‖x‖≤R } . Further suppose at t-th iteration, the teacher estimates the student est :=∥∥G>(wt)−vt∥∥=O ( ) with probability at least 1− δ using m ( est, δ) samples. If for any v ∈ Ωv , there exists γ 6=0 and ŷ such that for x̂=γ (v−v∗), we have\n0 < γ∇〈vt,x̂〉` (〈 vt, x̂ 〉 , ŷ ) <\n2 (1− λ)σmin ησ2max , with 0 < λ < min (κ (G>G) √\n2 , 1 ) ,\nthen the student can achieve -approximation of v∗ with O ( log 1 ( 1 +m ( λ , δ\nlog 1\n))) samples with probability\nat least 1− δ. If m ( est, δ) = O(log 1 ), then (`,G) is ET.\nExistence of exponentially teachable (`,G) via approximate recovery. m ( est, δ) is the number of samples needed for approximately recovering G>(wt) in each iteration. Different from the exact recovery setting wherem only depends on the feature dimension, m ( est, δ) here also depends on how accurately the teacher wants to recover G>(wt) in each iteration ( est denotes the estimation error of G>(wt)). The condition for exponentially teachable with approximate recovery is related to both (`,G) and the approximation level of the student parameters, i.e., the effect of λ. For example, if the σminσ2max = 1 and λ = 1 2 , the exponentially teachable condition will be the same for both the omniscient teaching (Liu et al., 2017a) and active teaching with exact recovery.\nFor F (·)=sign(·), if the student provides sign (〈w,G(x)〉) for the query x, it is unlikely to recover G>(w) unless we know ∥∥G>(w)∥∥. This leads to the following assumption. Assumption 1 The feedback is 1-bit, i.e. F (·)=sign(·), and the norm of G>(w) is known to teacher. Assumption 1 is necessary because sign(·) is scale invariant. We cannot distinguish between G>(w) and k · G>(w) for any k ∈ R+ only with their signs. The following theorem provides the query sample complexity in this scenario.\nTheorem 7 Suppose that Assumption 1 holds. Then with probability at least 1−δ, then we can recover G>(w) ∈ Rd with Õ (( d2 + d log 1δ ) log 1 ) query samples. Combining Theorem 6 with Theorem 7, we have the results for the 1-bit feedback case.\nCorollary 8 Suppose Assumption 1 holds. Then then (`,G) is ET with O ( log 1 ) teaching samples and\nÕ (\nlog 1 log 1 λ\n( d2 + d log\nlog 1 δ\n)) query samples.\nTrade-off between teaching samples and query samples. There is a delicate trade-off between query sample complexity (in the exam phase) and teaching sample complexity. Specifically, with est =O ( 1 t2 ) and m ( O ( 1 t2\n)) query samples, we can already achieve the conclusion that∥∥G>(wt+1)−v∗∥∥2 converges in rate O ( 1t ), which makes the number of teaching samples to be O ( 1 2 ) . We emphasize that this rate is the same with the convergence of SGD minimizing strongly convex functions. Note that the teaching algorithm can achieve at least this rate for general convex loss. Compared to the number of teaching samples in Corollary 8, although the query samples is less, this setting requires much more effort in teaching. Such phenomenon is reasonable in practice in the sense that if the examination is not accurate, the teacher provides the student less effective samples and hence has to teach for more iterations when the teacher cannot accurately evaluate student’s performance.\nWe remark that if G is a unitary operator, i.e., G>G = I , we can show that the teacher need only one exam. The key insight is that after the first “background exam”, the teacher can replace the following exams by updating the virtual\nlearner via the same dynamic of the real learner. This is formalized as follows. Lemma 9 Suppose that G is a unitary operator. If∥∥G>(w0)− v0∥∥ ≤ , then ∥∥G>(wt+1)− vt+1∥∥ ≤ . Therefore, with a unitary feature mapping, we only need one exam in the whole teaching procedure. It follows that the query sample complexity in theorem 6 will be reduced to Õ (\nlog 1λ\n( d2 + d log\nlog 1 δ\n)) via approximate recovery."
  }, {
    "heading": "5.2. Combination-Based Active Teaching",
    "text": "We discuss how the results for synthesis-based active teaching can be extended to the combination-based active teaching. In this scenario, we assume both training and query samples are constructed by linear combination of k samples in D={xi}ki=1. We have the following corollaries for both exact recovery and approximate recovery in the sense of\n〈v1, v2〉D := √ v>1 D (D>D)\n+D>v2, and ‖v‖D := 〈v, v〉D .\nNote that with the introduced metric, for v ∈ Rd, we only consider its component in span (D) and the components in the null space will be ignored. Therefore, ∀ v1, v2 ∈ span(D) such that ‖v1‖D = ‖v2‖D, we have v>1 x=v>2 x= 〈v1, x〉D for all x ∈ Rd. Then we have the result via exact recovery as follows. Corollary 10 Suppose the learner gives feedbacks in query phase by F (·)=I(·) or F (·)=S(·), and G>(w0), v∗∈ span (D). Then (`,G) is ET with O ( log 1 ) teaching samples and rank(D) query samples for exact recovery. The result via approximate recovery holds analogously to synthesis-based active teaching, given as follows. Corollary 11 Suppose Assumption 1 holds, the student answers questions in query phase via F (·)= I(·) or F (·)=S(·) and G>(w0), v∗∈span (D). Then (`,G) is ET with O ( log 1 ) teaching samples and\nÕ (\nlog 1 log 1 λ\n( d2 + d log\nlog 1 δ\n)) query samples via ap-\nproximate recovery."
  }, {
    "heading": "5.3. Rescaled Pool-Based Active Teaching",
    "text": "In this scenario, the teacher can only pick examples from a fixed sample candidate pool, D={xi}ki=1, for teaching and active query. We still evaluate with the metric ‖·‖D defined in (5.2). We first define pool volume to characterize the richness of the pool (Liu et al., 2017a). Definition 12 (Pool Volume) Given the training example pool X ∈ Rd, the volume of X is defined as\nV(X ) := min w∈span(D) max x∈X 〈w, x〉D ‖w‖2D .\nThen the result via exact recovery is given as follows. Theorem 13 Suppose that the student answers questions in the exam phase via F (·)=I(·) or F (·)=S(·)\nand G>(w0), v∗∈span (D). If ∀ G>(w) ∈ span(D), there exist (x, y) ∈ D × Y and γ such that for x̂= γ‖G>(w)−v∗‖D\n‖x‖D x, ŷ=y, we have 0 ≤ γ∇〈vt,x̂〉` (〈 vt, x̂ 〉 , ŷ ) ≤ 2V (X )σmin\nησ2max , then (`,G) is ET with O ( log 1 ) teaching samples and rank(D) query samples. For the approximate recovery case, the active learning is no longer able to achieve the desired accuracy for estimating the student’s parameter in the restricted pool scenario. Thus the active teacher may not achieve exponential teaching."
  }, {
    "heading": "6. Discussions and Extensions",
    "text": "The active teacher need not know the learning rate. To estimate the learning rate, the active teacher should first estimate the student’s initial parameters w1 ∈ Rd, and then feed the student with one random sample (xr, yr). Once the updated student’s parameter w2 is estimated by the teacher, the learning rate η can be computed by η= 1d ∑( (w1−w2)./∇w`(wT1 x, y) ) where ./ denotes the element-wise division and the sum is over all the dimensions in w1. The number of samples for estimating η will be 2m+1, wherem denotes the samples used in estimating student’s parameter. Even if the learning rate is unknown, the teacher only needs 2m+1 more samples to estimate it. Most importantly, it will not affect the exponential teachability.\nTeaching with forgetting. We consider the scenario where the learner may forget some knowledge that the teacher has taught, which is very common in human teaching. We model the forgetting behavior of the learner by adding a deviation to the learned parameter. Specifically in one iteration, the learner updates its model with wt+1 =wt+ ∇w`(〈wt, x〉, y), but due to the forgetting, its truly learned parameter ŵt+1 is wt+1 + t where t is a random deviation vector. Based on Theorem 6, we can show that such forgetting learner is not ET with a teacher that only knows the learner’s initial parameter and can not observe the learner along iteration. However, the active teacher can make the forgetting learner ET via the active query strategy. More details and experiments are provided in Appendix D.\nTeaching by multiple teachers. Suppose multiple teachers sequentially teach a learner, a teacher can not guide the learner without knowing its current parameter. It is natural for the teacher to actively estimate the learner. Our active teaching can be easily extended to multiple teacher scenario."
  }, {
    "heading": "7. Experiments",
    "text": "General settings. Detailed settings are given in Appendix B. We mainly evaluate the practical pool-based teaching (without rescaling) in the experiments. Still, in the exam stage, our active teacher is able to synthesize novel query examples as needed. The active teacher works in a different feature space from the learner’s space, while the omniscient\nteacher (Liu et al., 2017a) can fully observe the learner and works in the same feature space as the learner. The omniscient teacher serves as a baseline (possibly an upper bound) in our experiments. For active learning, we use the algorithm in (Balcan et al., 2009; Schein & Ungar, 2007).\nEvaluation. For synthetic data, we use two metrics to evaluate the convergence performance: the objective value and∥∥G>(wt)−v∗∥∥\n2 w.r.t. the training set. For real images, we\nfurther use accuracy on the testing set for evaluation. We put the experiments of forgetting learner in Appendix D."
  }, {
    "heading": "7.1. Teaching with Synthetic Data",
    "text": "We use Gaussian distributed data to evaluate our active teacher model on linear regression and binary linear classification tasks. We study the LRS learner with F (〈w, x̃〉)= 〈w, x̃〉, LR learner with F (〈w, x̃〉) being the sigmoid function, LR learner with F (〈w, x̃〉)=sign(〈w, x̃〉). For the first two cases, the active teacher can perform an one-time exam (“background exam”) to exactly recover the ideal virtual learner. After recovering the ideal virtual learner, the active teaching could achieve the performance of the omniscient teaching. The experimental results in Fig. 3(a) and Fig. 3(b) meet our expectations. In the initial iterations (on the order of feature dimensions), we can see that the learner does not update itself. In this stage, the active teacher provides query samples to the learner and recover a virtual learner based on the feedbacks of these query samples. After the exact recovery of the virtual learner, one can observe that the active teacher achieves faster convergence compared with the random teacher (SGD). In fact, the active teacher and the omniscient teacher should achieve the same convergence speed if omitting numerical errors.\nFor the LR learner with F (〈w, x̃〉)=sign(〈w, x̃〉), the teacher could only approximate the learner with the active learning algorithm. Besides, the active teacher needs to know the norm of the student model. We use the algorithm in (Schein & Ungar, 2007) and recover the virtual learner in each iteration such that ‖Ĝ>(w)−G>(w)‖2 becomes small enough. From the results in Fig. 3(c), we can see that due to the approximation error between the recovered virtual learner and the ideal virtual learner, the active teacher can not achieve the same performance as the omniscient teacher. However, the convergence of the active teacher is very close to the omniscient teacher, and is still much faster than SGD. Note that, we remove the iterations used for exams to better compare the convergence of different approaches."
  }, {
    "heading": "7.2. Teaching with Real Image Data",
    "text": "We apply the active teacher to teach the LR learner on the MNIST dataset (LeCun et al., 1998) to further evaluate the performance. In this experiment, we perform binary classification on the digits 7 and 9. We use two random projections to obtain two sets of 24-dim features for each image: one is for the teacher’s feature space and the other is for the\nTowards Black-box Iterative Machine Teaching\n0 100 200 300 400 500 600\n0\n10\n20\n30\n40\n50\n60\n70\n0 100 200 300 400 500 600\n0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omniscient Teacher Active Learner\n0 200 400 600 800\n0\n1\n2\n3\n4\n5\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 800\n0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nBackground Exam\nBackground Test\nBackground Test Background Test\n0 200 400 600 800 1000 1200\n0\n1\n2\n3\n4\n5\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 800 1000 1200\n0\n0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nIteration Number\nSGD Omniscient Teacher Active Teacher\nIteration NumberIteration Number\nIteration Number Iteration NumberIteration Number\nD if\nfe re\nn c e b\ne tw\ne e n\nw t a n\nd w\n*\nD if\nfe re\nn c e b\ne tw\ne e n\nw t a n\nd w\n*\nD if\nfe re\nn c e b\ne tw\ne e n\nw t a n\nd w\n*\nO b\nje c ti\nv e V\na lu\ne\nO b\nje c ti\nv e V\na lu\ne\nO b\nje c ti\nv e V\na lu\ne\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmoid) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regression (F(z) is sign)\n0 100 200 300 400 500 600 0\n10\n20\n30\n40 50 60 70\n0 100 2 0 300 400 5 0 600 0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 800 0\n1\n2\n3 4 5\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 8 0 0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nBackground Test\nBackground Test\nBackground Exam Background Test\n0 200 400 600 800 1000 1200 0\n1\n2\n3 4 5 SGD Omniscient Teacher Active Teacher\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n2 2.5 SGD Omniscient Teacher Active Teacher\nIteration Number\nSGD Omniscient Teacher Active Learner\nIteration NumberIteration Number\nItera ion Number Iteration NumberIteration Number\nD iff\ner en\nce b\net w\nee n vt a nd v *\nD iff\ner en\nce b\net w\nee n vt a nd v *\nD iff\ner en\nce b\net w\nee n vt a nd v *\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmoid) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regression (F(z) is sign)\nLSR LSR\n0 100 200 300 400 500 600 0\n10\n20\n30\n40\n50\n60\n70\n0 100 200 300 400 500 600 0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omniscient Teacher Active Learner\n0 200 400 600 800 0\n1\n2\n3\n4\n5\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 800 0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nBackground Test\nBackground Exam\nBackground Test Background Test\n0 200 400 600 800 1000 1200 0\n1\n2\n3\n4 5 SGD Omniscient Teacher Active Teacher\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n2 2.5 SGD Omniscient Teacher Active Teacher\nIteration Number\nSGD Omniscient Teacher Active Learner\nIteration NumberIteration Number\nIteration Number Iteration NumberIteration Number\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmoid) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regression (F(z) is sign)\n0 100 200 300 400 500 600 0\n10\n20\n30\n40\n50\n60\n70\n0 100 200 300 400 500 600 0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omniscient Teacher Active Learner\n0 200 400 600 800 0\n1\n2\n3\n4\n5\nSGD Omnisc ent Teacher Active Teacher\n0 2 0 400 600 80 0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nBackground Test\nBackground Test\nBackground Test Background Exam\n0 200 400 600 800 1000 1200 0\n1\n2\n3\n4 5 SGD Omniscient eac r Active Teacher\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n2 2.5 SGD Omniscient eac er Active Teacher\nIteration Number\nSGD Omniscient Teacher Active Learner\nIteration NumberIteration Number\nIteration Number Iteration NumberIteration Number\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmoid) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regression (F(z) is sign)\nLR (F (z) is sigmoid) LR (F (z) is sigmoid)\n0 100 200 300 400 500 600 0\n10\n20\n30\n40\n50\n60\n70\n0 100 200 300 400 500 600 0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omniscient Teacher Active Learner\n0 200 400 600 800 0\n1\n2\n3\n4\n5\nSGD Omniscient Teacher Active Teacher\n0 200 400 600 800 0.5\n1\n1.5\n2\n2.5\nSGD Omniscient Teacher Active Teacher\nBackground Test\nBackground Test\nBackground Test Background Test\n0 200 400 600 800 1000 1200 0\n1\n2\n3\n4\n5\nO niscient Teacher Active Teacher\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n2\n2.5\nSGD\nOmniscient Teacher\nActive Teacher\nIteration Number\nSGD Omniscient Teacher Active Learner\nIteration NumberIteration Number\nIteration Number Iteration NumberIteration Number\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nD iff\ner en\nce b\net w\nee n\nw t a\nnd w\n*\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\nO\nbj\nec tiv\ne Va\nlu e\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmoid) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regression (F(z) is sign)\n0 100 200 300 400 500 600 0\n10\n20\n30\n40\n50\n60\n70\n0 100 2 0 300 400 5 0 600 0\n1\n2\n3\n4\n5\n6\n7\n8\nSGD Omnisc ent Teacher Active Learner\n0 200 400 600 800 0\n1\n2\n3\n4\n5\nSGD Omniscient Teacher Active Teacher\n0 2 0 400 600 80 0.5\n1\n1.5\n2\n2.5\nSGD Omnisc ent Teacher Active Teacher\nBackground Test\nBackground Test\nBackground Test Background Test\n0 200 400 600 800 1000 1200 0\n1\n2\n3\n4 5 SGD Omniscient Teacher Active Teacher\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n2 2.5 SGD Omniscient Teacher Active Teacher\nIteration Number\nSGD Omniscient Teacher Active Learner\nIteration NumberIteration Number\nIteration Number Iteration NumberItera ion Number\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\nO bj\nec tiv\ne Va\nlu e\n(d) Least Square Regression (e) Logistic Regression (F(z) is sigmo d) (f) Logistic Regression (F(z) is sign)\n(a) Least Square Regression (b) Logistic Regression (F(z) is sigmoid) (c) Logistic Regre sion (F(z) is sign)\nLR (F (z) is sign) LR (F (z) is sign)\nFigure 3: The convergence performance of random teacher (SGD), omniscient teacher and ctive teacher. As we need to perform the active query in each iteration for logistic regression (F (z) is sign), we remove the iteration for fair comparison. We only show the teaching complexity for fair comparison.\nstudent’s feature space. The omniscient teacher uses the student’s space as its own space (i.e., shared feature space), while the active teacher uses different feature space with the student. For the LR learner with sign function (i.e. 1-bit feedbacks), one can observe that the active teacher has comparable performance to the omniscient teacher, even doing better at the beginning. Because we evaluate the teaching performance on real image data, the omniscient teacher will not necessarily be an upper bound of all the teacher. Still, as the algorithms iterate, the active teacher becomes worse than the omniscient teacher due to its approximation error.\nIn the right side of Fig.4, we visualize the images selected by the active teacher, omniscient teacher and random teacher. The active teacher preserves the pattern of images selected by the omniscient teacher: starting from easy examples first and gradually shifting to difficult ones, while the images selected by the random teacher have no patterns."
  }, {
    "heading": "8. Conclusions and Open Problems",
    "text": "As a step towards the ultimate black-box machine teaching, cross-space teaching greatly relaxes the assumptions of previous teaching scenarios and bridges the gap between the iterative machine teaching and the practical world. The ac-\nIteration Number Iteration NumberIteration Number\nO bj\nec tiv\ne Va\nlu e\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nA cc\nur ac\ny\n(a) Logistic Regression (F(z) is sign) (b) Logistic Regression (F(z) is sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSGD Omniscient Teacher Active Teacher\n500 1000 1500 2000 0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nSGD Omniscient Teacher Active Teacher\n500 10 1500 2000 0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nSGD Omniscient Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher (e) Omniscient Teacher (f) Random Teacher (SGD)\nIteration Number Iteration NumberIteration Number\nO bj\nec tiv\ne Va lu e\nD iff\ner en\nce b\net w ee n vt a nd v *\nA cc\nur ac y\n(a) Logistic Regression (F(z) is sign) (b) Logistic Regression (F(z) is sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65 0.7 0.75 0.8 0.85\nSGD Omniscient Teacher Active Teacher\n0 500 1000 1500 2000 0.35\n0.4\n0.45\n0.5\n0.55 0.6 0.65 0.7 0.75\nSGD Omniscient Teacher Active Teacher\n0 500 1000 1500 2000 0\n0.05\n0.1\n0.15 0.2 0.25 0.3\nSGD Omniscient Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher (e) Omniscient Teacher (f) Random Teacher (SGD)LR (F (z) is sign) Active Teacher\nIteration Number Iteration NumberIteration Number\nO bj\nec tiv\ne Va\nlu e\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nA cc\nur ac\ny\n(a) Logistic Regression (F(z) is sign) (b) Logistic Regression (F(z) is sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSGD Omniscient Teacher Active Teacher\n0 500 10 1500 2000 0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nSGD Omniscient Teacher Active Teacher\n0 500 1000 1500 2000 0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nSGD Omniscient Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher (e) Omniscient Teacher (f) Random Teacher (SGD)\nIt r ti r Iteration NumberIteration Number\nO bj\nec tiv\ne Va\nlu e\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nA cc\nur ac\ny\n(a) Logistic Regre sion (F(z) is sign) (b) Logistic Regression (F(z) i sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSGD Omniscient Teacher Active Teacher\n5 0 10 1500 2000 0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.7\nSGD Omniscient Teacher Active Teach r\n0 500 1 00 1500 200 0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nSGD Omnisci nt Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher e Omniscient T acher (f) Random Teacher (SGD)LR (F (z) is sign) Omniscient Teacher\nIteration Number Iteration NumberIteration Number\nO bj\nec tiv\ne Va\nlu e\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nA cc\nur ac\ny\n(a) Logistic Regression (F(z) is sign) (b) Logistic Regression (F(z) is sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSGD Omniscient Teacher Active Teacher\n0 500 1000 1500 2000 0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nSGD Omniscient Teacher Active Teacher\n0 500 1000 1500 2000 0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nSGD Omniscient Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher (e) Omni cient T acher (f) Random Teacher (SGD)\nIteration u ber Iteration NumberIteration u ber\nO bj\nec tiv\ne Va\nlu e\nD iff\ner en\nce b\net w\nee n\nvt a\nnd v\n*\nA cc\nur ac\ny\n(a) Logistic Regres ion (F(z) is sign) (b) Logistic Regression (F(z) is sign) (c) Logistic Regression (F(z) is sign)\n0 1000 2000 3000 4000 0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSGD Omniscient Teacher Active Teacher\n0 5 100 1500 2000 .35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nSGD Omniscient Teacher Active Teacher\n0 500 1 0 1500 2000 0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nSGD Omniscient Teacher Active Teacher\nIteration 1-40 Iteration 601-640 Iteration 1201-1240Iteration 1-40 Iteration 601-640 Iteration 1201-1240 Iteration 1-40 Iteration 601-640 Iteration 1201-1240\n(d) Active Teacher (e) Omnisc ent Teacher (f) Random Teacher (SGD)LR (F (z) is sign) Random Teacher (SGD)\nFigure 4: The convergence performance of random teacher (SGD), omniscient teacher and active teacher in MNIST 7/9 classification. Similar to the previous, we only show the teaching complexity for fair comparison. More experiments on the logistic regression with F (z)=S(z) is in Appendix C.\ntive teaching strategy is inspired by realistic human teaching. For machine teaching to be applicable in practice, we need to gradually remove all the unrealistic assumptions to obtain more realistic teaching scenario. The benefits of more realistic machine teaching are in two folds. First, it enables us make better use of the existing off-the-shelf pretrained models to teach a new model on some new tasks. It is also related to transfer learning (Pan & Yang, 2010). Second, it can improve our understanding on human education and provide more effective teaching strategies for humans.\nRescalable pool-based active teaching with 1-bit feedback. The proposed algorithm may not work the in poolbased teaching setting when the student return 1-bit feedback. We leave the possibility of achieving exponential teachability in this setting as an open problem.\nRelaxation for the conditions on G. Current constraints on the operator G are still too strong to match more practical scenarios. How to relax the conditions on G is important. A better alternative to approximate recovery? Is there some other tool other than active learning for our teacher to recover the virtual learner? For example, 1-bit compressive sensing (Boufounos & Baraniuk, 2008) may help."
  }, {
    "heading": "Acknowledgements",
    "text": "The project was supported in part by NSF IIS-1218749, NSF Award BCS-1524565, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA, and Amazon AWS."
  }],
  "year": 2018,
  "references": [{
    "title": "An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity",
    "authors": ["N. Ailon"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Data poisoning attacks against autoregressive models",
    "authors": ["S. Alfeld", "X. Zhu", "P. Barford"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Explicit defense actions against test-set attacks",
    "authors": ["S. Alfeld", "X. Zhu", "P. Barford"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "Agnostic active learning",
    "authors": ["Balcan", "M.-F", "A. Beygelzimer", "J. Langford"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 2009
  }, {
    "title": "1-bit compressive sensing",
    "authors": ["P.T. Boufounos", "R.G. Baraniuk"],
    "venue": "In CISS,",
    "year": 2008
  }, {
    "title": "Nearoptimal machine teaching via explanatory teaching",
    "authors": ["Y. Chen", "O.M. Aodha", "S. Su", "P. Perona", "Y. Yue"],
    "year": 2018
  }, {
    "title": "Recursive teaching dimension, vc-dimension and sample compression",
    "authors": ["T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "The elements of statistical learning, volume 1. Springer series in statistics",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "venue": "New York,",
    "year": 2001
  }, {
    "title": "On the teaching complexity of linear sets",
    "authors": ["Z. Gao", "H.U. Simon", "S. Zilles"],
    "venue": "In International Conference on Algorithmic Learning Theory,",
    "year": 2015
  }, {
    "title": "A bound on the label complexity of agnostic active learning",
    "authors": ["S. Hanneke"],
    "venue": "In Proceedings of the 24th international conference on Machine learning,",
    "year": 2007
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["G. Hinton", "O. Vinyals", "J. Dean"],
    "venue": "arXiv preprint arXiv:1503.02531,",
    "year": 2015
  }, {
    "title": "Becoming the expertinteractive multi-class machine teaching",
    "authors": ["E. Johns", "O. Mac Aodha", "G.J. Brostow"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "How do humans teach: On curriculum learning and teaching dimension",
    "authors": ["F. Khan", "B. Mutlu", "X. Zhu"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "The teaching dimension of linear learners",
    "authors": ["J. Liu", "X. Zhu", "H.G. Ohannessian"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Iterative machine teaching",
    "authors": ["W. Liu", "B. Dai", "A. Humayun", "C. Tay", "C. Yu", "L.B. Smith", "J.M. Rehg", "L. Song"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Deep hyperspherical learning",
    "authors": ["W. Liu", "Zhang", "Y.-M", "X. Li", "Z. Yu", "B. Dai", "T. Zhao", "L. Song"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Analysis of a design pattern for teaching with features and labels",
    "authors": ["C. Meek", "P. Simard", "X. Zhu"],
    "venue": "arXiv preprint arXiv:1611.05950,",
    "year": 2016
  }, {
    "title": "Using machine teaching to identify optimal training-set attacks on machine learners",
    "authors": ["S. Mei", "X. Zhu"],
    "venue": "In AAAI,",
    "year": 2015
  }, {
    "title": "Robust stochastic approximation approach to stochastic programming",
    "authors": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"],
    "venue": "SIAM Journal on optimization,",
    "year": 2009
  }, {
    "title": "A survey on transfer learning",
    "authors": ["S.J. Pan", "Q. Yang"],
    "venue": "IEEE Transactions on knowledge and data engineering,",
    "year": 2010
  }, {
    "title": "Algebraic methods proving sauer’s bound for teaching complexity",
    "authors": ["R. Samei", "P. Semukhin", "B. Yang", "S. Zilles"],
    "venue": "Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Active learning for logistic regression: an evaluation",
    "authors": ["A.I. Schein", "L.H. Ungar"],
    "venue": "Machine Learning,",
    "year": 2007
  }, {
    "title": "Active learning literature survey",
    "authors": ["B. Settles"],
    "venue": "University of Wisconsin, Madison,",
    "year": 2010
  }, {
    "title": "On actively teaching the crowd to classify",
    "authors": ["A. Singla", "I. Bogunovic", "G. Bartók", "A. Karbasi", "A. Krause"],
    "venue": "In NIPS Workshop on Data Driven Education, number EPFL-POSTER-221572,",
    "year": 2013
  }, {
    "title": "Near-optimally teaching the crowd to classify",
    "authors": ["A. Singla", "I. Bogunovic", "G. Bartok", "A. Karbasi", "A. Krause"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "The label complexity of mixedinitiative classifier training",
    "authors": ["J. Suh", "X. Zhu", "S. Amershi"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Machine teaching for bayesian learners in the exponential family",
    "authors": ["X. Zhu"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Machine teaching: An inverse problem to machine learning and an approach toward optimal education",
    "authors": ["X. Zhu"],
    "venue": "In AAAI,",
    "year": 2015
  }, {
    "title": "An overview of machine teaching",
    "authors": ["X. Zhu", "A. Singla", "S. Zilles", "A.N. Rafferty"],
    "venue": "arXiv preprint arXiv:1801.05927,",
    "year": 2018
  }, {
    "title": "Teaching dimensions based on cooperative learning",
    "authors": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"],
    "venue": "In COLT,",
    "year": 2008
  }],
  "id": "SP:9e3a968ba627a2ed0f0c5c16c2ad36b7513dcd7b",
  "authors": [{
    "name": "Weiyang Liu",
    "affiliations": []
  }, {
    "name": "Bo Dai",
    "affiliations": []
  }, {
    "name": "Xingguo Li",
    "affiliations": []
  }, {
    "name": "Zhen Liu",
    "affiliations": []
  }, {
    "name": "James M. Rehg",
    "affiliations": []
  }, {
    "name": "Le Song",
    "affiliations": []
  }],
  "abstractText": "In this paper, we make an important step towards the black-box machine teaching by considering the cross-space machine teaching, where the teacher and the learner use different feature representations and the teacher can not fully observe the learner’s model. In such scenario, we study how the teacher is still able to teach the learner to achieve faster convergence rate than the traditional passive learning. We propose an active teacher model that can actively query the learner (i.e., make the learner take exams) for estimating the learner’s status and provably guide the learner to achieve faster convergence. The sample complexities for both teaching and query are provided. In the experiments, we compare the proposed active teacher with the omniscient teacher and verify the effectiveness of the active teacher model.",
  "title": "Towards Black-box Iterative Machine Teaching"
}