{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Targeted sentiment analysis has drawn growing research interests over the past few years. Compared with traditional sentiment analysis tasks, which extract the overall sentiment of a document, a sentence or a tweet, targeted sentiment analysis extracts the sentiment over given targeted entities from a text, and therefore is practically more informative. An example is shown in Figure 1. There are at least two practical scenarios:\n(1) Certain entities of concern are specified, and the requirement is to extract the sentiment towards their mentions in a text. For example, one can be interested in the sentiment towards Google Inc., Microsoft and Facebook in financial news texts, or the sentiment towards Manchester United, Liverpool and Chelsea in tweets.\nSo excited to meet my [baby Farah]+ !!! [Baseball Warehouse]+ : easy to understand information. The [#Afghan #Parlaiment Speaker]− should Resign . Saw [Erykah Badu]− last night , vile venue unfortunately . [AW service]0 will be back at work .\nThere are two sub tasks in targeted sentiment analysis, namely entity recognition and sentiment classification for each entity mention which apply to both scenarios above. In scenario (1), entity recognition is relatively trivial, and can typically be achieved by pattern matching. Partly due to this reason, most previous work has addressed targeted sentiment analysis as a pure classification task, assuming that target mentions have been given (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014; Vo and Zhang, 2015). For scenario (2), a named entity recognition (NER) system can be used to extract targets, before the same targeted sentiment classification algorithms are applied. There has also been work that concentrates on extracting opinion targets (Jin et al., 2009; Jakob and Gurevych, 2010). In both cases, the data in Figure 1 can be used for training sentiment classifiers.\nMitchell et al. (2013) took a different approach, extracting named entities and their sentiment classes jointly. They model the joint task\n612\nas an extension to the NER task, where an extra sentiment label is assigned to each named entity, in addition to the entity label. As a result, the task can be solved using sequence labeling methods. As claimed by Mitchell et al. (2013), the joint task is particularly suitable when no extra resources are available for training separate syntactic analyzers or name entity recognizers. Such situations can include tweets and low-resource languages/domains. Interestingly, because of containing entity information, the annotation in Figure 1 suffices for training joint entity and sentiment labels even if it is the only resource available.\nThe annotations in Figure 1 can be transformed into label sequences, as shown in Figure 2. Figure 2 consists of two types of labels, where the B/I/O labels indicate span boundaries, and the +/- /0 labels indicate sentiment classes. The two types of labels can be assigned in a span→sentiment pipeline, or jointly as a multi-label task. Alternatively, as shown in Figure 2(b), the two types of labels can be collapsed into a joint label, such as B+ and I-, indicating the beginning of a positive entity and the middle of a negative entity, respectively. The collapsed labels allow joint entity recognition and sentiment classification to be achieved using a standard sequence labeler.\nMitchell et al. (2013) compare a pipeline model, a joint model and a collapsed model under the same conditional random field (CRF) framework, finding that the pipeline method outperforms the joint model on a tweet dataset. Intuitively, the interaction between entity boundaries and sentiment classes might not be as strong as that between more closely-coupled sources of information, such as word boundaries and POS (Zhang and Clark, 2008), or named entities and constituents (Finkel and Manning, 2009), for which joint models significantly outperform pipeline models. On the\nother hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X.\nRecently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper, we exploit structured neural models for open targeted sentiment.\nWe take the CRF model of Mitchell et al. (2013) as the baseline, and explore two research questions. First, we make an empirical comparison between discrete and neural CRF models, and further combine the strengths of each model via feature integration. Second, we compare the effects of the pipeline, joint and collapsed models for open targeted sentiment analysis under the neural model settings. Our experiments show that the neural model gives competitive results compared with the discrete baseline, with relatively higher recalls. In addition, the integrated model significantly improves over both the discrete and the neural models."
  }, {
    "heading": "2 Related Work",
    "text": "Targeted sentiment analysis is closely related prior work on aspect-oriented (Hu and Liu, 2004), feature-oriented (Popescu and Etzioni, 2007) and topic-oriented (Yi et al., 2003) sentiment analysis. These related tasks are typically concentrated on product review settings. In contrast, targeted sentiment analysis has a more general setting.\nRecently, Wang et al. (2011) proposed a topicoriented model, which extracts sentiments towards certain topics from tweets. Topics in their model resemble targets in our work, although topics are represented by hashtags, which exists in 14.6% tweets and 27.5% subjective tweets (Wang et al.,\n2011). In contrast, targeted sentiment analysis can identify all the mentions to target entities in tweets, thereby having a larger coverage. The drawback is that the identification of mentions is subject to errors, and thus suffers a lower precision compared to hashtag matching.\nSequence labeling models have been used for extracting opinions and target entities as a joint task. Jin et al. (2009) use HMM to extract opinionbaring expressions and opinion targets. Li et al. (2010) improve the results by using CRF to identify the opinion expressions and targets jointly. The task is sometimes referred to as fine-grained sentiment analysis (Wiebe et al., 2005). It is different from our setting in that the predicate-argument relation between opinion-baring expressions and target entities are not explicitly modeled.\nRecently, Yang and Cardie (2013) use CRF to extract opinion-baring expressions, opinion holders and opinion targets simultaneously. Their method is also centralized on opinion-baring expressions and therefore in line with Jin et al. (2009) and Li et al. (2010). In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without explicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation management, but offer less fine-grained operational insight. It requires less fine-grained manual annotation.\nAs discussed in the introduction, targeted sentiment analysis falls into two main settings. The first is targeted sentiment classification, assuming that entity mentions are given. Most previous work fall under this category (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014). The second is open domain targeted sentiment, which has been discussed by Mitchell et al. (2013). The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction (Li and Ji, 2014) in that both are information extraction tasks with multi-label outputs.\nOur work is related to the line of work on using neural networks for sentiment analysis. Socher et al. (2011) use recursive auto-encoders for sentiment analysis on the sentence level. They further extend the method to a syntactic treebank annotated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure\nof a sentence automatically, before classifying its sentiment. Zhou et al. (2014) apply deep belief networks for semi-supervised sentiment classification. dos Santos and Gatti (2014) use deep convolution neural networks with rich features to classify sentiments over tweets and movie reviews. These methods use different models to represent sentence structures, performing sentiment analysis on the sentence level, without modeling targets.\nDong et al. (2014) perform targeted sentiment classification by using a recursive neural network to model the transmission of sentiment signal from opinion baring expressions to a target. They assume that the target mention is given, and perform three-way sentiment classification. In contrast, we apply a structural neural model for open domain targeted sentiment analysis, identifying and classifying all targets in a sentence simultaneously."
  }, {
    "heading": "3 Discrete CRF Baselines",
    "text": "As shown in Figure 2, the input ~x to our tasks is a word sequence. Assuming no external resources, there is no POS given to each input word xi. For\nthe pipeline and collapsed tasks, there is a single output label sequence ~y. For the joint task, there are two label sequences ~y and ~z, for entity and sentiment labels, respectively. We take the models of Mitchell et al. (2013) as our baseline, which are standard CRFs with discrete manual features. To facilitate comparison between the discrete baseline and our neural models, we give a unified formulation to all the models in this paper, introducing the neural and integrated models as extensions to the discrete models.\nThe baseline CRF structures for pipeline, joint and collapsed targeted sentiment analysis are shown in Figure 3(a), 3(b) and 3(c), respectively. In the figures, the input features are represented as black and white circles, indicating that they take 0/1 binary values. The labels O, B and I indicate a non-target, the beginning of a target, and part of a target, respectively. The labels +, −, 0 and Φ indicate positive, negative, neutral and NULL sentiments, respectively. The NULL sentiment is assigned to O entities automatically, and modeled as a hidden variable in the pipeline and joint CRFs.1 The collapsed labels take combined meanings from their components.\nThe links between labels and inputs represent output clique potentials:\nΨ(~x, yi) = exp { ~θ · ~f(~x, yi) } ,\nwhere ~f(~x, yi), is a discrete manual feature vector, and ~θ is the model parameter vector.\nThe links between labels represent edge clique potentials:\nΦ(~x, yi, yi−1) = exp { τ(yi, yi−1) } ,\nwhere τ(yi, yi−1) is the transition weight, which is also a model parameter.\nFor both the pipeline and collapsed models, the conditional probability of a label sequence given an input sequence is:\nP (~y|~x) =\n|x|∏ i=1 Ψ(~x, yi) |x|∏ j=1 Φ(~x, yi, yi−1)\nZ(~x) ,\n1Note the difference between neural and NULL sentiments. The former indicates that a target does not bare any sentiment, and the latter simply means that the term is not a part of a target.\nwhere Z(~x) is the partition function:\nZ(~x) = ∑ ~y′ ( |x|∏ i=1 Ψ(~x, y′i) |x|∏ j=1 Φ(~x, y′i, y ′ i−1) ) ,\nFor the joint model, we apply a multi-label CRF structure, where there are two separate sets of output clique potentials Ψ1(~x, yi) and Ψ2(~x, zi) and two separate sets of edge clique potentials Φ1(~x, yi, yi−1) and Φ2(~x, zi, zi−1) for the label sets {B, I,O} and {+,−, 0}, respectively. In the Figure 3(b), there are also links between the span label yi and the sentiment label zi for each word xi. These links indicate label dependencies, which are constraints for decoding. For example, if yi = O, then zi must be φ.\nWe apply Viterbi decoding for all tasks, and training is performed using a max-margin objective, which is discussed in Section 6. Our training algorithm is different from that of Mitchell et al. (2013), but gives similar discrete CRF accuracies in our experiments. Wang and Mori (2009) also applied a max-margin trainig strategy to train CRF models. The set of features is taken from Mitchell et al. (2013) without changes, as shown in Table 1. Here the cluster features refer to Brown word clusters (Brown et al., 1992)."
  }, {
    "heading": "4 Neural Models",
    "text": "We extend the discrete baseline system with two salient changes, which are illustrated in Figure 4. First, the input discrete features are replaced with continuous word embeddings. Each node in the input takes a real value between 0 and 1, as represented by grey nodes in Figure 4. Second, a hidden\nneural layer ~h is added between the input nodes ~x and the label nodes yi.\nFormally, the links between the input nodes ~x and the hidden nodes ~hi for the node yi in Figure 4 represent a feature combination function:\n~hi =tanh (\nW.(e(~xi−2)⊕ e(~xi−1)⊕ e(~xi) ⊕ e(~xi+1)⊕ e(~xi+2)) +~b ) where e is the embedding lookup function, ⊕ is the vector concatenation function, the matrix W and vector ~b are model parameters and tanh is the activation function.\nThe output clique potential of yi becomes: Ψ(~x, yi) = exp { ~σ · ~hi } where ~σ is a model parameter, and the edge clique potentials remain the same as the baseline. By\nusing a hidden layer for automatic feature combinations, the neural model is free of manual features, and can benefit from unsupervised embeddings. Decoding and training are performed using the same algorithms as the baseline.\nThe major neural architectures in Figure 4 have been explored as conditional neural fields by Peng et al. (2009) and neural conditional random fields by Do et al. (2010), and is connected to the sentence-level likelihood neural networks of Collobert et al. (2011), as pointed out by Wang and Manning (2013b). The main differences between our model and the prior work are in the multi-label settings and training details."
  }, {
    "heading": "5 Integrated Models",
    "text": "Gleaning different sources of information, neural features and discrete linear features comple-\nments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2\nThe architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input.\nA separate hidden layer is given to each type of input nodes, with the hidden layer for the embeddings being the same as the neural baseline:\n~hi =tanh (\nW · (e(~xi−2)⊕ e(~xi−1)⊕ e(~xi) ⊕ e(~xi+1)⊕ e(~xi+2)) +~b ) The hidden nodes ~gi between the discrete features and the node yi are:\n~gi = tanh ( ~θ · ~f(~x, yi) ) Finally, the output clique potential of yi becomes:\n~Ψ(~x, yi) = exp { ~σ · (~hi ⊕ ~gi) } The edge clique potentials remain the same as the baseline models; the same training and decoding algorithms are used."
  }, {
    "heading": "6 Training",
    "text": "We use a max-margin objective to train our model parameters Θ, which consist of ~θ, τ , W, ~b and ~σ for each model. The objective function is defined as:\nL(Θ) = 1 N N∑ n=1 l(~xn, ~yn,Θ) + λ 2 ‖ Θ ‖2,\n2Wang and Manning (2013a) also investigated the integration of discrete and neural features in CRF models. They compared the effect of integration without hidden layers (i.e. Turian et al. (2010)) and with hidden layers (i.e. our methods) for NER and chunking, finding that the formal outperforms the latter. Our results are different from theirs, and a hidden layer gives significant improvements to the targeted sentiment analysis task.\nwhere (~xn, ~yn)|Nn=1 are the set of training examples, λ is a regularization parameter, and l(~xn, ~yn,Θ) is the loss function towards one example (~xn, ~yn).\nThe loss function is defined as:\nl(~xn, ~yn,Θ) = max ~y (s(~xn, ~y,Θ) + δ(~y, ~yn))\n− s(~xn, ~yn,Θ),\nwhere s(~x, ~y,Θ) = logP (~y|~x) is the log probability of ~y, and δ(~y, ~yn) is the Hamming distance between ~y and ~yn.\nWe use online learning to train model parameters, updating the parameters using the AdaGrad algorithm (Duchi et al., 2011). One thing to note is that, our objective function is not differentiable because of the loss function l(~xn, ~yn,Θ). Thus we use sub-gradients for l(~xn, ~yn,Θ) instead, which can be computed by the formula:\n∂l(~xn, ~yn,Θ) ∂Θ = ∂s(~xn, ~̂y,Θ) ∂Θ − ∂s(~xn, ~yn,Θ) ∂Θ ,\nwhere ~̂y is the predicted label sequence which corresponds to l(~xn, ~yn,Θ).\nMaximum-likelihood training is a commonly used alternative to max-margin training for neural networks. It has been applied to the models of Do et al. (2010) and Collobert et al. (2011), for example. However, our experiments show that maximum-likelihood training cannot be applied to open-domain targeted sentiment tasks. Although giving comparable overall accuracies in both entity and sentiment labels, it suffers from unbalanced sentiment labels, assigning the neutral sentiment to most entities. This problem can be addressed by imposing a polarity-sensitive cost to the training, such as the sentence-level averaged F1-score between positive, negative and neutral labels. We skip these results due to space limitations. In contrast, max-margin training does not suffer from the label skew issue, thanks to the use of Hamming loss in the objective function."
  }, {
    "heading": "7 Experiments",
    "text": ""
  }, {
    "heading": "7.1 Experimental Settings",
    "text": "Data: We use the data of Mitchell et al. (2013)3 to conduct all the experiments, which consist of entity and sentiment annotations on both English and Spanish tweets. Simple normalizations are\n3http://www.m-mitchell.com/code/index.html\nconducted to replace all usernames and URLs into the special tokens 〈username〉 and 〈url〉, respectively. Following Mitchell et al. (2013), we report ten-fold cross-validation results. During training, we split 10% of the training corpus as the development corpus to tune hyper-parameters. Table 2 shows the corpus statistics.\nParameters: For all the neural models, we set the hidden layer size |~h| for neural features to 200, the hidden layer size |~g| for discrete features to 30, the initial learning rate for adagrad to 0.01 and the regularization parameter λ to 10−8. English and Spanish word embeddings are trained using the word2vec tool4, with respective corpora of 20 minion random tweets crawled by tweet API5. The size of word embeddings is 100. For English, there are 8,061 unique words, for which 25% are out of word embedding vocabulary (OOE) words, while for Spanish, there are 14,648 unique words, for which 15% are OOE words.\nMetrics: We take full-span metrics for evaluation, which is different from Mitchell et al. (2013), who evaluate mainly the beginning of spans. We measure the precision, recall and F-score of entity recognition (Entity), targeted sentiment analysis (SA) (both entity and sentiment), and targeted subjectivity detection (Subjectivity) (both entity and subjectivity, namely merging the + and - labels as “1” label, and performing two-way 0/1 subjectivity classification on entities). For SA, an entity is taken as correct only when the span and the sentiment are both correctly recognized. Similarly, for Subjectivity, an entity is taken as correct only when both the span and the subjectivity are correctly recognized.\nCode: We make the C++ implementations of the discrete, neural and combined models available and GPL, at https://github.com/ SUTDNLP/OpenTargetedSentiment."
  }, {
    "heading": "7.2 Comparing Neural and Discrete Models",
    "text": "The main results on both the English and Spanish dataset are shown in Table 3, which are mea-\n4https://code.google.com/p/word2vec/ 5https://dev.twitter.com/\nsured on the pipeline, the joint and the collapsed tasks, respectively. As can be seen from the table, the neural models give higher F-scores than the discrete CRF models on the English dataset, while comparable overall F-scores on the Spanish dataset. The gains on English are mostly attributed to improved recalls, while the precision of the neural CRF models are relatively lower. A likely reason for this observation is that the neural model takes embedding inputs, which allow semantically similar words to be represented with similar vectors. As a result, the neural model can better capture patterns that do not occur in the training data. In contrast, the discrete model is based on manually defined binary features, which do not fire if not contained in the training data. Because discrete feature instantiation is based on exact matching, the discrete model gives a relatively higher precision.\nTo further contrast the discrete and neural models, we draw the per-word accuracies of sentiment labels according to both models in Figure 6. In the figure, each dot represents the accuracy of a sentence, measured in the pipeline task. The dots for both English and Spanish are scattered from the reverse diagonal, showing that the two models make very different errors, which suggests that model integration can lead to better accuracies."
  }, {
    "heading": "7.3 The Integrated Model",
    "text": "As shown in Table 3, the integrated model combines the relative advantages of both pure models, improving the recall over the discrete model and the precision over the neural model. In most cases, it gives the best results in terms of both precision and recall. For the English pipeline model, the integrated model improves the entity recognition F-score from 43.84% to 55.67% (significant with p < 10−5 by pair-wise t-test) as compared to the discrete baseline, namely Mitchell et al. (2013).\nThe overall SA score is improved from 31.73% to 40.06% (p < 10−5). Similar improvements are achieved to the other test datasets."
  }, {
    "heading": "7.4 Fine-tuning Word Embeddings",
    "text": "In the experiments above, word embeddings are fine-tuned for the neural models, but not for the integrated models. By fine-tuning, embeddings of in-vocabulary words are treated as model parameters, and updated with other parameters in supervised training. This can improve the accuracy of the model by significantly enlarging the parameter space. However, it can make the embeddings of OOV words less useful to the model, because the hidden layers are tuned with adjusted embeddings.\nFigure 7 shows the effectiveness of fine-tuning on the neural and integrated models using the Spanish data. Similar findings apply to the English data. The neural model heavily relies on fine-tuning of embeddings, and a likely reason is that manual discrete features offer sufficient parameters for capturing in-vocabulary patterns. On\nthe other hand, thanks to the rich discrete features in parameter space, the integrated model does not rely on fine-tuning of word embeddings, which even caused slight overfitting and reduced the performances. This makes the non-fine-tuned integrated model potentially advantageous in handling test data with many OOV words."
  }, {
    "heading": "7.5 Comparing pipeline, joint and collapsed models",
    "text": "Mitchell et al. (2013) find that for discrete CRF, the pipeline task gives competitive overall performances compared with the joint task. This suggests a relatively weak connection between entity boundary information and sentiment classes. We re-examine the comparisons under the neural network setting, where automatic feature combinations can be useful in capturing more subtle correlations between two sources of information.\nAs shown in Table 3, the overall results are similar to those of Mitchell et al. (2013), with both the neural and the integrated models demonstrating the same trends as the discrete baselines. A more detail analysis, however, shows some relative strengths of the joint task. Table 4 give the precision, recall and F-scores of subjectivity, and those of SA excluding neutral sentiment labels on the Spanish data. Findings on the English dataset are consistent.\nThe latter metrics highlight sentiment polarities, which can be relatively more useful. The joint task gives better F-scores on both metrics, which suggest that is a considerable choice for open targeted sentiment. When there is external resource for en-\ntity recognition, the pipeline can be a favorable choice. On the other hand, although useful for some joint sequence labeling task (Ng and Low, 2004), the collapsed task does not seem to address the joint sentiment task as effectively. We find this result empirical, but consistent across our datasets."
  }, {
    "heading": "8 Conclusion",
    "text": "We explored open domain targeted sentiment analysis using neural network models, which gave competitive results when evaluated against a strong discrete CRF baseline, with relatively higher recalls. Given complementary error distributions by the discrete and neural CRFs, we proposed a novel combination which significantly outperformed both models. Under the neural setting, we find that it is preferable to solve open targeted sentiment as a pipeline or joint multi-label task, but not as a joint task with collapsed labels."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their constructive comments, which helped to improve the paper. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design."
  }],
  "year": 2015,
  "references": [{
    "title": "Class-based n-gram models of natural language",
    "authors": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."],
    "venue": "Computational linguistics, 18(4):467–479.",
    "year": 1992
  }, {
    "title": "Extracting diverse sentiment expressions with target-dependent polarity from twitter",
    "authors": ["Lu Chen", "Wenbo Wang", "Meenakshi Nagarajan", "Shaojun Wang", "Amit P Sheth."],
    "venue": "ICWSM.",
    "year": 2012
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."],
    "venue": "Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Neural conditional random fields",
    "authors": ["Trinh Do", "Thierry Arti"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Adaptive recursive neural network for target-dependent twitter sentiment classification",
    "authors": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
    "year": 2014
  }, {
    "title": "Deep convolutional neural networks for sentiment analysis of short texts",
    "authors": ["Cicero dos Santos", "Maira Gatti."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 69–78.",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Joint parsing and named entity recognition",
    "authors": ["Jenny Rose Finkel", "Christopher D Manning."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2009
  }, {
    "title": "Revisiting embedding features for simple semi-supervised learning",
    "authors": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120.",
    "year": 2014
  }, {
    "title": "Mining and summarizing customer reviews",
    "authors": ["Minqing Hu", "Bing Liu."],
    "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.",
    "year": 2004
  }, {
    "title": "Extracting opinion targets in a single-and cross-domain setting with conditional random fields",
    "authors": ["Niklas Jakob", "Iryna Gurevych."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045.",
    "year": 2010
  }, {
    "title": "Target-dependent twitter sentiment classification",
    "authors": ["Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
    "year": 2011
  }, {
    "title": "A novel lexicalized hmm-based learning framework for web opinion mining",
    "authors": ["Wei Jin", "Hung Hay Ho", "Rohini K Srihari."],
    "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 465–472.",
    "year": 2009
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2014
  }, {
    "title": "Incremental joint extraction of entity mentions and relations",
    "authors": ["Qi Li", "Heng Ji."],
    "venue": "Proceedings of the Association for Computational Linguistics. 620",
    "year": 2014
  }, {
    "title": "Structure-aware review mining and summarization",
    "authors": ["Fangtao Li", "Chao Han", "Minlie Huang", "Xiaoyan Zhu", "Ying-Ju Xia", "Shu Zhang", "Hao Yu."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 653–661.",
    "year": 2010
  }, {
    "title": "Tagging the web: Building a robust web tagger with neural network",
    "authors": ["Ji Ma", "Yue Zhang", "Jingbo Zhu."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 144–154.",
    "year": 2014
  }, {
    "title": "Open domain targeted sentiment",
    "authors": ["Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654.",
    "year": 2013
  }, {
    "title": "Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based",
    "authors": ["Hwee Tou Ng", "Jin Kiat Low"],
    "venue": "In EMNLP,",
    "year": 2004
  }, {
    "title": "Conditional neural fields",
    "authors": ["Jian Peng", "Liefeng Bo", "Jinbo Xu."],
    "venue": "Advances in neural information processing systems, pages 1419–1427.",
    "year": 2009
  }, {
    "title": "Extracting product features and opinions from reviews",
    "authors": ["Ana-Maria Popescu", "Orena Etzioni."],
    "venue": "Natural language processing and text mining, pages 9–28. Springer.",
    "year": 2007
  }, {
    "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-",
    "year": 2011
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the conference on",
    "year": 2013
  }, {
    "title": "Word representations: A simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.",
    "year": 2010
  }, {
    "title": "Target-dependent twitter sentiment classification with rich automatic features",
    "authors": ["Duy-Tin Vo", "Yue Zhang."],
    "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015), pages 1347–1353.",
    "year": 2015
  }, {
    "title": "Effect of non-linear deep architecture in sequence labeling",
    "authors": ["Mengqiu Wang", "Christopher D. Manning."],
    "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1285–1291.",
    "year": 2013
  }, {
    "title": "Fast dropout training",
    "authors": ["Sida Wang", "Christopher Manning."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML13), pages 118–126.",
    "year": 2013
  }, {
    "title": "Max-margin hidden conditional random fields for human action recognition",
    "authors": ["Yang Wang", "Greg Mori."],
    "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 872– 879.",
    "year": 2009
  }, {
    "title": "Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach",
    "authors": ["Xiaolong Wang", "Furu Wei", "Xiaohua Liu", "Ming Zhou", "Ming Zhang."],
    "venue": "Proceedings of the 20th ACM international conference on Information and knowl-",
    "year": 2011
  }, {
    "title": "Annotating expressions of opinions and emotions in language",
    "authors": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."],
    "venue": "Language resources and evaluation, 39(2-3):165–210.",
    "year": 2005
  }, {
    "title": "Joint inference for fine-grained opinion extraction",
    "authors": ["Bishan Yang", "Claire Cardie."],
    "venue": "ACL (1), pages 1640–1649.",
    "year": 2013
  }, {
    "title": "Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques",
    "authors": ["Jeonghee Yi", "Tetsuya Nasukawa", "Razvan Bunescu", "Wayne Niblack."],
    "venue": "Data Mining, 2003. ICDM 2003. Third IEEE International Conference",
    "year": 2003
  }, {
    "title": "Compound embedding features for semi-supervised learning",
    "authors": ["Mo Yu", "Tiejun Zhao", "Daxiang Dong", "Hao Tian", "Dianhai Yu."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
    "year": 2013
  }, {
    "title": "Joint word segmentation and POS tagging using a single perceptron",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of ACL-08: HLT, pages 888– 896.",
    "year": 2008
  }, {
    "title": "Hybrid deep belief networks for semi-supervised sentiment classification",
    "authors": ["Shusen Zhou", "Qingcai Chen", "Xiaolong Wang", "Xiaoling Li."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Techni-",
    "year": 2014
  }],
  "id": "SP:0d4fef0ef83c6bad2e14fe4a4880fa153f550974",
  "authors": [{
    "name": "Meishan Zhang",
    "affiliations": []
  }, {
    "name": "Yue Zhang",
    "affiliations": []
  }],
  "abstractText": "Open domain targeted sentiment is the joint information extraction task that finds target mentions together with the sentiment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved using state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combinations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by significantly increasing the recall. In addition, we propose a novel integration of neural and discrete features, which combines their relative advantages, leading to significantly higher results compared to both baselines.",
  "title": "Neural Networks for Open Domain Targeted Sentiment"
}