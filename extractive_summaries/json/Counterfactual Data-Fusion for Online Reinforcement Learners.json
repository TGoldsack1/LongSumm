{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Active learning agents are becoming increasingly integrated into complex environments, where a number of heterogeneous sources of information are available for use (4; 6; 10). These agents have the ability to both intervene in their environments (choosing actions, receiving feedback on the quality of their choices, and then modifying future actions accordingly), as well as observe other agents interacting. With the opportunity to learn from data collected from different sources other than personal experimentation come new challenges of “transfer” in learning. In particular, agents should know that actions that are desirable for populations may not be desirable for all individuals, and as such, should be wary of how observed behavior generalizes (i.e., transfers) to them and how these observations should be combined with the agent’s own experience.\n1University of California, Los Angeles, California, USA 2Purdue University, West Lafayette, Indiana, USA. Correspondence to: Andrew Forney <forns@cs.ucla.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nIn this work, we study the conditions under which data collected under heterogeneous conditions (to be defined) can be combined by an online agent to improve performance in a reinforcement learning task. This challenge is not without precedent, as recent works have investigated dataset transportability (when source and target differ structurally), though in offline domains (3; 4). Others have studied scenarios in which agents learn from expert teachers in the inverse reinforcement learning problems (1; 8). Recent work from causal analysis has addressed data-fusion for interventional quantities in reinforcement learning tasks (18). However, this work addresses data-fusion in domains where counterfactual quantities are sought, as for personalized decision-making (2; 17).\nEnvironments for which an agent possesses fully labelled data and a fully specified model (in which all factors relating contexts, actions, and their associated rewards are known) are trivial from a learning transfer perspective; in such scenarios, collected data is homogeneous because all factors that may introduce bias between samples can be controlled. Conversely, in this paper, we focus on the challenges that arise due to unobserved confounders (UCs), namely, unmeasured and unlabelled variables that influence an agent’s natural action choice as well as the outcome of that action. Such factors are particularly subtle when left uncontrolled due to their invisible nature and emergence of what is known as confounding bias (12).\nOur agent’s goal is to quickly learn about its environment by consolidating data collected from observing other agents and data collected through its own experience, so UCs pose a fundamental challenge: the results from seeing another agent performing an action are not always qualitatively the same as doing the action itself. As such, throughout this paper, we will differentiate three classes of data that may be employed by an autonomous agent to inform its decision-making:\n1. Observational data is gathered through passive examination of the actions and rewards of agents other than the actor, but for whom the actor is assumed to be exchangeable.\n2. Experimental data is gathered through randomization (e.g., standard MAB exploration), or from a fixed, non-reactive policy.\n3. Counterfactual data (though traditionally computed from a fully specified model or under specific empirical conditions) represents the rewards associated with actions under a particular (or “personalized”) instantiation of the UCs.\nIn the remainder of this work, we demonstrate how these data types can be fused to facilitate learning in a variant of the Multi-Armed Bandit problem with Unobserved Confounders (MABUC), first discussed in (2). In traditional bandit instances (e.g., (13; 9; 7; 14; 5)), an agent is faced with K ∈ N,K ≥ 2 discrete action choices (often called “arms”), each with its own, independent, and initially unknown reward distribution. The agent’s task is to maximize cumulative rewards over a series of rounds, which requires learning about the underlying reward distributions associated with each arm. In the MABUC (formalized shortly), agents are faced with the same task, except that UCs modify the agent’s arm-choice predilections and payout rates at each round, and the dimensionality and functional form of the UCs are unknown.\nThough the data-fusion problem is an ongoing exploration in the data sciences (4), this paper is the first to study learning techniques in MABUC settings that combine data sampled under heterogeneous data-collection modes. Specifically, our contributions are as follows:\n1. Using counterfactual calculus, we formally show that counterfactual quantities can be estimated by active agents empirically (Sec. 4).\n2. We demonstrate how observational, experimental, and counterfactual datasets can be combined through a heuristic for MABUC agents (Sec. 4).\n3. We then develop a variant of the Thompson Sampling algorithm that implements this new heuristic, and run extensive simulations demonstrating its faster convergence rates compared to the current state-of-the-art (Sec. 5).1"
  }, {
    "heading": "2. The Greedy Casino Revisited",
    "text": "In this section, we consider an expanded version of the Greedy Casino problem introduced in (2). In its new floor’s configuration, the Greedy Casino is considering four new themed slot-machines (instead of the two used in the previous version) and wishes to make them as lucrative as possible. After running a battery of preliminary tests, the casino executives discover that two traits in particular predict which of the four machines that a gambler is likely\n1Supplemental material. For paper appendices and other resources, visit: https://goo.gl/MYJWbY\nto play: whether or not the machines are all blinking (denotedB ∈ {0, 1}), and whether or not the gambler is drunk (denoted D ∈ {0, 1}). After consulting with a team of psychologists and statisticians, the casino learns that any arbitrary gambler’s natural machine choice can be modeled by the structural equation (12): X ← fX(B,D) = B+2 ∗D if the four machines are indexed as X ∈ {0, 1, 2, 3}. The casino also learns that its patrons have an equal chance of being drunk or not (i.e., P (D = 1) = 0.5) and decide to program their new machines to blink half of the time (i.e., P (B = 1) = 0.5).\nTo prevent casinos from exploiting their patrons, a new gambling law stipulates that all slot machines in the state must maintain a minimum 30% win rate. Wishing to leverage their new discovery about gamblers’ machine choice predilections while conscious of this law, the casino implements a reactive payout strategy for their machines, which are equipped with sensors to determine if their gambler is drunk or not (assume that the sensors are perfect at making this determination). As such, the machines are programmed with the payout distribution illustrated in Table 1a.\nAfter the launch of the new slot machines, some observant gamblers note that players appear to be winning only 20% of the time, and report their suspicions to the state gambling commission. An investigator is then sent to the casino to determine the merit of these complaints, and begins recruiting random gamblers from the casino floor to play at randomly selected machines, despite the players’ natural predilections. Surprisingly, he finds that players in this experiment win 40% of the time, and declares that the casino has committed no crime. Meanwhile, the casino continues to exploit players’ gambling predilections, paying them 10% less than the law-mandated minimum. Plainly, gamblers are unaware of being manipulated by the UCs B,D, and of the predatory payout policy that the casino has constructed around them. The collected data is summarized in Table 1b; the second column (E[y1|X]) represents the observations drawn from the casino’s floor while the third"
  }, {
    "heading": "X = 0 0.20 0.40",
    "text": ""
  }, {
    "heading": "X = 1 0.20 0.40",
    "text": ""
  }, {
    "heading": "X = 2 0.20 0.40",
    "text": ""
  }, {
    "heading": "X = 3 0.20 0.40",
    "text": "(E[y1|do(X)]) represents the randomized experiment performed by the state investigator (both with large sample sizes).\nIn an attempt to find a better gambling strategy, an observant habitué decides to run a battery of experiments using standard MAB algorithms (e.g., -greedy, UCB, Thomson Sampling) as well as an algorithm following an approach presented in (2) known as the Regret Decision Criterion (RDC) (reviewed in the next section). Importantly, the RDC agent lacks the capability to identify and observe the UCs. The results of her experiments are depicted in Fig. 1. She notes, somewhat surprised, that all algorithms which ignore the influence of the UCs (i.e., all but RDC) perform equivalently to the randomized experiment conducted by the investigator. Noting the differences in the payout rates between the observational and experimental data, she ponders how this can be the case and how she might use these datasets to improve her gambling strategy and winnings."
  }, {
    "heading": "3. Background",
    "text": "In this section, we formalize the MABUC problem in the language of Structural Causal Models (SCMs), which will allow us to articulate the notions of observational, experimental, and counterfactual distributions as well as formalize the problem of confounding due to the influence of UCs.\nEach SCM M is associated with a causal diagram G and encodes a set of endogenous (or observed) variables V and exogenous (or unobserved) variables U ; edges in G correspond to functional relationships relative to each endogenous variable Vi ∈ V , namely, Vi ← fi(PAi, Ui), where PAi ⊆ V \\ Vi and Ui ⊆ U ; and a probability distribution over the exogenous variables P (U = u).\nEach M induces: (1) observational distributions P (V = v), which represent the “natural” world, without external interventions; (2) a set of experimental (a.k.a. interventional) distributions P (Y = y|do(X = x)) for X,Y ⊆ V , which represent the world in whichX is forced to the value x despite any causal influences that would otherwise functionally determine its value in the natural setting; and (3) a set of counterfactual distributions, defined next (12).2\nDefinition 3.1. (Counterfactual) (12) LetX and Y be two subsets of endogenous variables in V . The counterfactual sentence “Y would be y (in situation U = u), had X been x” is interpreted as the equality Yx(u) = y, where Yx(u) encodes the solution for Y in a structural system where for every Vi ∈ X , the equation fi is replaced with the constant x.\nNote that the counterfactual expression E[Yx = y|X = x′] is well-defined, even when x 6= x′, and is read “The expectation that Y = y had X been x given that X was observed to be x′”. Despite being logically valid statements in SCMs, counterfactual quantities must be estimated from either a fully specified model, or, in the absence of such, from data. In offline settings, however, counterfactual quantities are not empirically estimable (namely, when the antecedent of the counterfactual contradicts the observed value), except in some special cases ((12), Chs. 7, 9). The reason is that if we submitted the subject to condition X = x′, we cannot go back in time before exposure and submit the same subject to a new condition X = x. As is well understood in the causal inference literature, this procedure is not the same as first exposing a random unit to condition X = x′ since the ones who initially were inclined to act as X = x are somehow different than the randomly selected subject. That said, we will show (in Sec. 4) that online learning agents possess the means to estimate counterfactuals directly.\nIn practice, the observational and experimental distributions can be estimated through procedures known as random sampling and random experimentation, respectively. Confounding bias emerges when UCs are present and can be seen through the difference between these two distributions, P (Y |do(X = x)) − P (Y |X = x). The absence of UCs implies that P (Y |do(X = x)) = P (Y |X = x), which allows random sampling (instead of a randomized experiment) to estimate the experimental distribution.\nThe contrast between observational and experimental data is mirrored in the distinction between actions (which represent reactive “choices” resulting from an agents’ environments, beliefs, and other causes) and acts (which represent deliberate choices resulting from rational decision-making or interventions that sever the causal influences of the sys-\n2For a comprehensive review of SCMs, we refer readers to (12).\ntem (12)). To tie these concepts to the MABUC problem, one important tool introduced in (2) is known as the agent’s intent.\nDefinition 3.2. (Intent) Consider a SCM M and an endogenous variable X ∈ V that is amenable to external interventions and is (naturally) determined by the structural equation fx(PAx, Ux), where PAx ⊆ V represents the observed parents of X , and Ux ⊆ U are the UCs of X . After realization PAx = pax and Ux = ux (without any external intervention), we say that the output of the structural function given the current configuration of all UCs is the agent’s intent, I = fx(pax, ux).\nThus, intent can be seen as an agent’s chosen action before its execution, which, in fact, is a proxy for any influencing UCs.3 To ground these notions, consider again the Greedy Casino example in which the gamblers’ intents are enacted on the unperturbed casino floor, but are then averaged over during the investigator’s randomized study.\nWe can now put these observations together and explicitly define the MABUC problem:\nDefinition 3.3. (K-Armed Bandits with Unobserved Confounders) A K-Armed bandit problem (K ∈ N,K ≥ 2) with unobserved confounders (MABUC, for short) is defined as a model M with a reward distribution over P (u) where, for each round 0 < t < T, t ∈ N:\n1. Unobserved confounders: Ut represents the unobserved variable encoding the payout rate and unobserved influences to the propensity to choose arm xt at round t.\n2. Intent: It ∈ {i1, ..., ik} represents the agent’s intended arm choice at round t (prior to its final choice, Xt) such that It = fi(paxt , ut).\n3. Policy: πt ∈ {x1, ..., xk} denotes the agent’s decision algorithm as a function of its history (discussed shortly) and current intent, fπ(ht, it).4\n4. Choice: Xt ∈ {x1, ..., xk} denotes the agent’s final arm choice that is “pulled” at round t, xt = fx(πt).\n5. Reward: Yt ∈ {0, 1} represents the Bernoulli reward (0 for losing, 1 for winning) from choosing arm xt under UC state ut as decided by yt = fy(xt, ut).\n3Def. 3.2 does not require that all its influencing factors be measured or acknowledged by the agent. This definition accomodates the fact that an agent’s decisions can be influenced by unknown factors, an observation that is not new to the cognitive sciences (16; 11).\n4Using this representation, the distinction between obs. and exp. settings is made transparent – πt copies it in the former, but ignores it and listens instead to a random device (e.g., a coin toss) in the latter.\nThe graphical model in Fig. 2 represents a prototypical MABUC (Def. 3.3). We also add a graphical representation of the agent’s history Ht, a data structure containing the agent’s observations, experiments, and counterfactual experiences up to time step t. The means by which these different data-collections can be used in the agent’s policy are explored at length in the next section. In summary, at every round t of MABUC, the unobserved state ut is drawn from P (u), which then decides it, which is then considered by the strategy πt in concert with the game’s history ht; the strategy makes a final arm choice, which is then pulled, as represented by xt, and the reward yt is revealed.\nBased on this definition, the regret decision criterion can be stated (2):\nDefinition 3.4. (Regret Decision Criterion (RDC)) (2) In a MABUC instance with arm choice X , intent I = i, and reward Y , agents should choose the action a that maximizes their intent-specific reward, or formally:\nargmax a\nE[YX=a|X = i] (1)\nIn brief, RDC prescribes that the arm X = a that maximizes the expected value of reward Y having conditioned on the intended arm X = i should be selected, even when a 6= i."
  }, {
    "heading": "4. Fusing Datasets",
    "text": "Suppose our agent assumes the role of a gambler in the Greedy Casino (Sec. 2) and possesses (1) observations of arm choices and payouts from other players in the casino, (2) the randomized experimental results from the state investigator, and (3) the knowledge to use intent in its decision-making for choosing arms by the Regret Decision Criterion (RDC). In other words, the agent begins\nthe MABUC problem with large samples of observations (E[Y |X]) and experimental results (E[Y |do(X)]), and will maximize the counterfactual RDC (E[YX=a = 1|X = i]) because it recognizes the presence of UCs (viz. E[Y |X] 6= E[Y |do(X)]; see Table 1b). We note that the observational and experimental data available to our agent contains information about its environment, but cannot simply be incorporated into the counterfactual maximization criteria (viz. E[Y |X] 6= E[Y |do(X)] 6= E[Yx|x′]; see Table 1). So, the agent can choose to either discard its observations and experiments, and simply gamble by the tenets of RDC, or combine them in an informative way. This section explores the latter option.\nRelating the Datasets\nNote that the experimental quantity E[Y |do(X = x)] can be written in counterfactual notation E[YX=x] = E[Yx], which reads as “The expected value of Y had X been x.” Note also that E[Yx] can be written as a weighted average of the reward associated with arm x across all intent conditions (by the law of total probabilities), namely:\nE[Yx] = E[Yx|x1]P (x1) + ...+ E[Yx|xK ]P (xK) (2)\nExamining Eq. 2, we see that the equation is composed of expressions from our agent’s three datasets (observational, experimental, and counterfactual). By definition, the LHS of the equation (E[Yx]) is drawn from the experimental dataset. On the RHS, we have two types of quantities. Expressions of the form E[Yx|x′] for which x = x′ are observational by the consistency axiom (12), because when the hypothesized and observed actions are the same, the value of y is the same (i.e., E[Yx|x] = E[Y |x]). On the other hand, expressions of the form E[Yx|x′] for which x 6= x′ are non-trivial counterfactuals, mixing observations and antecedents occurring in different worlds.\nIn general, evaluating counterfactuals empirically is not possible, except for some special cases, such as when the action X is binary (12). However, RDC asserts that if one preempts the agent’s decision process when the intent I = i is about to become a decision (X), i still encodes information about the UCs Ui (because i = fi(PAx, Ux)). This implies that randomizing within intent conditions can lead to the computation of the counterfactual given by RDC, which is a special counterfactual also called the effect of treatment on the treated (ETT) (12).\nIn order for us to exploit the properties of this equivalence to improve the performance of RDC agents in the MABUC setting, we first demonstrate that RDC indeed measures the counterfactual quantity of the ETT.\nTheorem 4.1. The counterfactual ETT is empirically estimable for arbitrary action-choice dimension (i.e., |X| = k for k ≥ 2) when agents condition on their intent I = i\nand estimate the response Y to their final action choice X = a.\nFor a proof of Theorem 4.1, see supplementary material, Appendix A. Because RDC is equivalently an interventional quantity, we have shown that the ETT, a counterfactual expression, can be estimated empirically through counterfactual-based randomization.5 The main advantages of this, now proven, equivalence are threefold: (1) the empirical estimation of previously unidentifiable counterfactual quantities presents opportunities for further exploration in causal analysis, (2) the ETT’s prescription for integrating experimental and observational data (see Eq. 2) permits a now interventional data-fusion strategy when such data is available, and (3) data points sampled by the agent using intent-specific decision-making are counterfactual in nature, and should therefore be added to the agent’s counterfactual history. Procedures implementing RDC-type randomization should thus record intent-specific arm rewards in a table similar to Fig. 3. A second consequence of recording arm-intent-specific payouts in this fashion is that observational data may be substituted directly into cells for which the final arm choice and intent agree (see reference to consistency axiom below Eq. 2).\nStrategies for Online Agents\nNow that we have illustrated how the different datasets relate to our agent in the MABUC setting, we consider that the counterfactual expressions in Eq. 2 must be learned by our agent and are not known at the start of the game. Because of this finite-sample concern, we propose different learning strategies that exploit the datasets’ relationship while managing the uncertainty implicit in a MAB learning scenario.\nStrategy 1: Cross-Intent Learning. Consider Eq. 2 once 5It is understood that the ETT can be computed for binary decisions or when the backdoor criterion holds (12), but it was not believed to be estimable for arbitrary dimensions prior to RDCrandomization.\nmore. This holds for every arm X = x, which induces a system of equations as shown in Fig. 3. Consider a single cell in this system, say E[Yxr |xw], which we can solve and rewrite as:\nEXInt[Yxr |xw] = [E[Yxr ]− K∑ i 6=w E[Yxr |xi]P (xi)]/P (xw)\n(3)\nThis form provides a systematic way of learning about arm payouts across intent conditions, which is desirable because an arm pulled under one intent condition provides knowledge about the payouts of that arm under other intent conditions. This can be depicted graphically, as shown by row B in Fig. 3 – information about Yxr flows from intent conditions xi 6= xw to intent xw (a form of information leakage, (15)).\nStrategy 2: Cross-Arm Learning. Consider any three arms, xr, xs, xw such that r /∈ {s, w} and assume we are interested in estimating the value of E[Yxr |xw] (our query, for short). Considering again the equations induced by Eq. (2), we have,\nE[Yxr ] = K∑ i E[Yxr |xi]P (xi) (4)\nE[Yxs ] = K∑ i E[Yxs |xi]P (xi) (5)\nNote that each of Eqs. (4, 5) share the same intent priors on our query intent P (xw), so we can solve for P (xw) in both equations using simple algebra, which yields,\nP (xw) = E[Yxr ]− ∑K i 6=w E[Yxr |xi]P (xi) E[Yxr |xw]\n= E[Yxs ]− ∑K i6=w E[Yxs |xi]P (xi) E[Yxs |xw]\n(6)\nUsing Eq. (6) and solving for the query in terms of our paired arm xs, ∀ r 6= s we have\nE[Yxr |xw] = [ E[Yxr ]− ∑K i 6=w E[Yxr |xi]P (xi) ] E[Yxs |xw]\nE[Yxs ]− ∑K i 6=w E[Yxs |xi]P (xi)\n(7)\nEq. (7) illustrates that any non-diagonal cell from the table in Fig. 3 can be estimated through pairwise arm comparisons with the same intent. Put differently, Eq. (7) allows our agent to estimate E[Yxr |xw] from samples in which any arm xs 6= xr was pulled under the same intent xw.\nIn practice, the online nature of the problem can make some of these pairwise computations noisy due to sampling variability when xr is an infrequently explored arm. To obtain\na more robust estimate of the target quantity, this pairwise comparison can be repeated between the query arm and all other arms with the same intent, and then pooled together. This can be seen as information about Yxr |xw flowing from arm xs 6= xr to xr (under intent xw) – column C in Fig. 2(b).\nOne such pooling strategy is to take the inverse-varianceweighted average.6 Formally, we can consider a function E[Yxr |xw] = hXArm(xr, xw, xs) such that hXArm performs the empirical evaluation of the RHS of Eq. (7). Additionally, let σ2x,i = V arsamp[Yx|i] indicate the empirical payout variance for each arm-intent condition (as from the reward successes and failures captured by the agent in Table 3). To estimate our query from all other arms in the same intent through inverse-variance weighting, we have our now complete, second heuristic:\nEXArm[Yxr |xw] = ∑K i6=r hXArm(xr, xw, xi)/σ 2 xi,xw∑K\ni 6=r 1/σ 2 xi,xw\n(8)\nStrategy 3: The Combined Approach. The payout estimates for a MABUC algorithm using RDC (Fig. 3) can be estimated from three different sources: (1) Esamp[Yxr |xw], the sample estimates collected by the agent during the execution of the algorithm. (2) EXInt[Yxr |xw], the computed estimate using cross-intent learning. (3) EXArm[Yxr |xw], the computed estimate using cross-arm learning. Naturally, these three quantities can be combined to obtain a more robust and stable estimate to the target query.\nWe employ an inverse-variance weighting scheme so as to leverage these three estimators, and so we must formulate a metric for the payout variance associated with each strategy’s computed estimate. To do so, we define an average variance for each strategy, which is the average over each sample estimate’s variance (i.e., σ2x,i) used in the computation. Specifically, for the cross-arm approach (Eq. 8), we have two summations over sample payout estimates E[Yxr |xi], E[Yxs |xi] ∀i 6= w which involve 2(K − 1) terms, plus the numerator’s E[Yxs |xw], giving us a total of 2(K − 1) + 1 = 2K − 1 variances to average. The same is true for the cross-intent apprach (Eq. 3), which involvesK−1 sample variances to average. When estimating\n6This strategy follows from the fact that we have Bernoulli rewards for each arm-intent condition, and as the number of samples increases for these distributions, the variance diminishes, meaning that arm-intent conditions with smaller variances are more reliable than those with larger ones.\nE[Yxr |xw], we can write the corresponding variances:\nσ2XArm = 1\n2K − 1 [[ K∑ i 6=w σ2xr,xi ] + [ K∑ i 6=w σ2xs,xi ] + σ2xs,xw ]\nσ2XInt = 1\nK − 1 K∑ i 6=w σ2xr,xi\nFinally, to estimate E[Yxr |xw] using our combined approach, we have:\nα = Esamp[Yxr |xw]/σ2xr,xw + EXInt[Yxr |xw]/σ 2 XInt\n+ EXArm[Yxr |xw]/σ2XArm β = 1/σ2xr,xw + 1/σ 2 XInt + 1/σ 2 XArm\nEcombo[Yxr |xw] = α\nβ (9)\nTo visualize the data-fusion process discussed here, consider the diagram in Figure 4.\n1. In this scenario, we consider that our agent has collected large samples of experimental and observational data from its environment (e.g., in the Greedy Casino, the agent might observe other gamblers to comprise its observational data and incorporate experimental findings from the state investigator’s report).\n2. Unobserved confounders are realized in the environment, though their labels and values are unknown to the agent.\n3. From these UCs and any other observed features in the environment, the agent’s heuristics suggest an action to take, i.e., its intent. With its intent known, the agent combines the data in its history (in this work, by the prescription of Strategy 3 above) to better inform its decision-making.\n4. Based on its intent and combined history, the agent commits to a final action choice.\n5. The action’s response in the environment (i.e., its reward) is observed, and the collected data point is added to the agent’s counterfactual dataset (as a consequence of Theorem 4.1)."
  }, {
    "heading": "5. Simulations & Results",
    "text": "In this section, we validate the efficacy of the strategies discussed previously through simulations. To make a fair comparison to previous MABUC bandit players, we will follow the first implementation of RDC that used Thompson Sampling (TS) as its basis, embedding the strategies described in the previous section within a TS player called (TSRDC∗). We note that after moving from traditional to counterfactual-based decision-making we moved from optimal arm-choice nonconvergence to convergence in the MABUC setting (e.g., Fig. 1); now, the goal is to accelerate convergence.\nIn brief, TSRDC∗ agents perform the following at each round: (1) Observe the intent it from the current round’s realization of UCs, ut. (2) Sample Êsamp[Yxr |it] from each arm’s (xr) corresponding intent-specific beta distribution β(sxr,it , fxr,it)\n7 in which sxr,it is the number of successes (wins) and fxr,it is the number of failures (losses). (3) Compute each arm’s it-specific score using the combined datasets via Strategy 3 (Eq. 9). (4) Choose the arm, xa, with the highest score computed in previous step. (5) Observe result (win / loss) and update Êsamp[Yxa |it].\nProcedure. Simulations were performed on the 4-arm MABUC problem, with results averaged across N = 1000 Monte Carlo repetitions, each T = 3000 rounds in duration. To illustrate the robustness of each proposed strategy, we performed simulations spanning across a wide range of payout parameterizations (see Appendix B for a complete report of experimental results).\nCompared Algorithms. Each simulation compares the performance of four variants of Thompson Sampling, described below and with the data-sets employed by each indicated in Table 2:\n7The parameters for these distributions are decided by the agent’s history (see Figure 2(b)), including contributions from observational data for cells in which action and intent agree.\n1. TS is the traditional Thompson Sampling bandit algorithm that attempts to maximize the interventional quantity E[y|do(x)], and does not condition on intent.\n2. TSRDC is TS player that uses RDC (Def. 3.4), but employs no additional observational or experimental data in its play.\n3. TSRDC+ is the approach produced by (2), which uses RDC and employs observational data, but does not incorporate experimental data nor exploit the relationship between data types detailed in the previous section.\n4. TSRDC∗ follows the algorithm described above and uses the data-fusion strategy described in the previous section.\nEvaluation. Each algorithm’s performance is evaluated using two standard metrics: (1) the probability of optimal arm choice and (2) cumulative regret, both as a function of t averaged across all N Monte Carlo simulations. However, unlike in the traditional MAB literature, we compare each algorithm’s choice to the optimal choice of an omniscient oracle that knows the value of any UCs in any given round of any MC repetition (indicated as x∗n,t). Formally, for all 0 < t < T we evaluate (1) as 1N ∑ n 1(x ∗ n,t = xn,t) and\n(2) as 1N ∑ n ∑t i E[Yx∗n,i |un,i]− yn,i.\nExperiment 1: “Greedy Casino.” The Greedy Casino parameterization, as described in Table 1, exemplifies the scenario where all arms are both observationally equivalent (E[Y |x] = E[Y |x′],∀x, x′) and experimentally equivalent (E[Y |do(x)] = E[Y |do(x′)],∀x, x′), but distinguishable within intent conditions (E[Yx|x′]). In this reward parameterization, TSRDC∗ experienced significantly less regret (M = 42.23) than its chief competitor, TSRDC+, (M = 65.04), t(1998) = 13.25, p < .001.\nExperiment 2: “Paradoxical Switching.” The Paradoxical Switching parameterization (see Appendix B for parameters) exemplifies a curious scenario whereinE[Yx1 ] = 0.5 > E[Yx′ ],∀x′ 6= x1, but for which x1 is the optimal arm choice in only one intent condition (I = x1). Agents unempowered by RDC will face a paradox in that the arm with the highest experimental payout is not always optimal. Again, TSRDC∗ experienced significantly less re-\ngret (M = 36.91) than its chief competitor, TSRDC+, (M = 64.70), t(1998) = 22.43, p < .001.\nThe accelerated learning enjoyed by RDC+ is not localized to these parameter choices alone. In Appendix B, we show that TSRDC∗ consistently experiences significantly less regret than its competitors across a wide range of reward parametrizations."
  }, {
    "heading": "6. Conclusion",
    "text": "The present work addresses the challenges faced by online learning agents that gain access to increasingly diverse, and qualitatively different sources of information, and how these sources can be meaningfully synthesized to accelerate learning. This data-fusion problem is complicated by the presence of unobserved confounders (UCs), whose identities and influences are unknown to the modeler. In response, we present a novel method by which online agents may combine their observations, experiments, and counterfactual (i.e., personalized) experiences to more quickly learn about their environments, even in the presence of UCs. We then illustrate the efficacy of this approach in the Multi-Armed Bandit problem with Unobserved Confounders (MABUC), and demonstrate how a traditional Thompson Sampling player may be improved by its application. Simulations demonstrate that our data-fusion approach generalizes across reward parameterizations and results in significantly less regret (in some cases, as much as half) than other competitive MABUC algorithms."
  }],
  "year": 2017,
  "references": [{
    "title": "Apprenticeship learning via inverse reinforcement learning",
    "authors": ["P. Abbeel", "A.Y. Ng"],
    "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
    "year": 2004
  }, {
    "title": "Bandits with unobserved confounders: A causal approach",
    "authors": ["E. Bareinboim", "A. Forney", "J. Pearl"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Causal inference by surrogate experiments: z-identifiability",
    "authors": ["E. Bareinboim", "J. Pearl"],
    "venue": "Proceedings of the Twenty- Eighth Conference on Uncertainty in Artificial Intelligence (UAI",
    "year": 2012
  }, {
    "title": "Causal inference and the data-fusion problem",
    "authors": ["E. Bareinboim", "J. Pearl"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Frontiers in massive data analysis",
    "authors": ["N.R. Council"],
    "year": 2013
  }, {
    "title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems",
    "authors": ["E. Even-Dar", "S. Mannor", "Y. Mansour"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2006
  }, {
    "title": "Showing versus doing: Teaching by demonstration",
    "authors": ["M.K. Ho", "M. Littman", "J. MacGlashan", "F. Cushman", "J. Austerweil", "J.L. Austerweil"],
    "venue": "In Advances In Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Asymptotically efficient adaptive allocation rules",
    "authors": ["T.L. Lai", "H. Robbins"],
    "venue": "Advances in Applied Mathematics,",
    "year": 1985
  }, {
    "title": "Optimal Learning from Multiple Information Sources",
    "authors": ["A. Liang", "X. Mu", "V. Syrgkanis"],
    "year": 2017
  }, {
    "title": "Basic study on prevention of human error-how cognitive biases distort decision making and lead to crucial accidents",
    "authors": ["A. Murata", "T. Nakamura"],
    "venue": "Advances in Cross-Cultural Decision Making,",
    "year": 2014
  }, {
    "title": "Causality: Models, Reasoning, and Inference",
    "authors": ["J. Pearl"],
    "venue": "Second ed.,",
    "year": 2000
  }, {
    "title": "Some aspects of the sequential design of experiments",
    "authors": ["H. Robbins"],
    "venue": "Bull. Amer. Math. Soc., 58(5):527–535,",
    "year": 1952
  }, {
    "title": "A modern bayesian look at the multiarmed bandit",
    "authors": ["S.L. Scott"],
    "venue": "Applied Stochastic Models in Business and Industry,",
    "year": 2010
  }, {
    "title": "Identifying Best Interventions through Online Importance",
    "authors": ["R. Sen", "K. Shanmugam", "A. Dimakis", "S. Shakkottai"],
    "venue": "Sampling . International Conference on Machine Learning, page to appear,",
    "year": 2017
  }, {
    "title": "Judgment under uncertainty: Heuristics and biases. In Utility, probability, and human decision making",
    "authors": ["A. Tversky", "D. Kahneman"],
    "year": 1975
  }, {
    "title": "Markov decision processes with unobserved confounders: A causal approach",
    "authors": ["J. Zhang", "E. Bareinboim"],
    "venue": "Technical Report",
    "year": 2016
  }, {
    "title": "Transfer Learning in Multi-Armed Bandits: A Causal Approach",
    "authors": ["J. Zhang", "E. Bareinboim"],
    "venue": "International Joint Conference on Artificial Intelligence,",
    "year": 2017
  }],
  "id": "SP:a142b4a4673af19cbbf98925a58552660f494384",
  "authors": [{
    "name": "Andrew Forney",
    "affiliations": []
  }, {
    "name": "Judea Pearl",
    "affiliations": []
  }, {
    "name": "Elias Bareinboim",
    "affiliations": []
  }],
  "abstractText": "The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent’s decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our findings’ efficacy with extensive simulations.",
  "title": "Counterfactual Data-Fusion for Online Reinforcement Learners"
}