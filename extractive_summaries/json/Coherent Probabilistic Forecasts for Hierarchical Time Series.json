{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Producing forecasts that support decision-making in a hierarchical structure is a central problem for many organizations. For example, retail sales forecasts typically form a hierarchy, with the inventory control system of a retail outlet relying on forecasts for store-level demand, while forecasts of regionally aggregated demand are needed for managing inventory at a distribution centre (Kremer et al., 2016). Another context where a hierarchy naturally arises is electricity demand, where the bottom level might consist of time series of the electricity consumption of individual customers, while the top level could be the total load on the grid. Forecasts of electricity consumption are needed at\n1Monash University, Melbourne, Australia 2University of Oxford, Oxford, UK. Correspondence to: Souhaib Ben Taieb <souhaib.bentaieb@monash.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nvarious levels of aggregation in order to operate the power grid efficiently and securely (van Erven & Cugliari, 2015).\nProducing accurate forecasts for these hierarchical structures is particularly challenging. First, the many time series involved can interact in varying and complex ways. In particular, time series at different levels of the hierarchy can contain very different patterns (see, for example, Figure 3); time series at the bottom level are typically very noisy sometimes exhibiting intermittency, while aggregated series at higher levels are much smoother. As a result, a naive bottom-up approach whereby forecasts of aggregates are generated by summing the forecasts of the corresponding series in the lower levels is unlikely to deliver accurate results when the aggregation involves a large number of series (Hyndman et al., 2011). Second, in order to ensure coherent decision-making at the different levels of a hierarchy, it is essential that the forecast of each aggregated series should equal the sum of the forecasts of the corresponding disaggregated series. Unfortunately, independently forecasting each time series within each level is very unlikely to deliver coherent forecasts. Finally, the bottom level can consist of several thousand or even millions of time series, which can induce a massive computational load.\nRecent work in this area (Wickramasuriya et al., 2015; van Erven & Cugliari, 2015) has focused on a two-stage approach in which base forecasts are first produced independently for each series in the hierarchy; these are then combined to generate coherent revised forecasts (see Section 2). The rationale behind this approach is both to improve forecast accuracy due to the synthesis of information from different forecasts, as well as to produce coherent forecasts. A fundamental limitation of actual research is that it has looked only at the problem of forecasting the mean of each time series. This contrasts with the shift in the forecasting literature over the past two decades towards probabilistic forecasting (Gneiting & Katzfuss, 2014). This form of prediction quantifies the uncertainty, which enables improved decision making and risk management (see, for example, Berrocal et al. (2010)).\nWe address the key problem of generating probabilistic forecasts for large-scale hierarchical time series. This is particularly challenging since it is requires the estimation of the entire distribution of future observations, not only the mean (Kneib, 2013; Hothorn et al., 2014). Furthermore,\nbecause of the hierarchical structure, this problem also involves computing the distribution of hierarchical sums of random variables in high dimensions. Finally, another challenge is the possible variety of distributions in the hierarchy. In fact, although the distributions become more normally distributed with the aggregation level as a consequence of the central limit theorem, the series at lower levels often exhibit non-normality including multi-modality and high levels of skewness.\nWe propose an algorithm that computes predictive distributions under the form of random samples for each series in the hierarchy. First, probabilistic forecasts are independently computed for all series in the hierarchy, and samples are computed from the associated predictive distributions. Then, a sequence of permutations extracted from estimated copulas are applied to the multivariate samples in a hierarchical manner to restore the dependencies between the variables before computing the sums (see Section 3). Finally, the algorithm computes sparse forecast combinations for all series in the hierarchy, where the combination weights are estimated by solving a possibly highdimensional LASSO problem (see Section 3.2). The result is a set of coherent probabilistic forecasts for each series in the hierarchy.\nOur algorithm has multiple advantages compared to the state-of-the art hierarchical forecasting methods: (1) it quantifies the uncertainty in the predictions for the entire hierarchy while satisfying the aggregation constraints; (2) it is scalable to high-dimensional hierarchies since the problem is decomposed into multiple lower-dimensional subproblems; and (3) it synthesizes information from different levels in the hierarchy to estimate the marginal distributions and the dependence structures through the mean forecast combination and the hierarchical aggregation, respectively.\nWe evaluate our algorithm using both simulated data sets (see Section 4.2) and a large scale electricity smart meter data set (see Section 4.3)."
  }, {
    "heading": "2. Mean Hierarchical Forecasting",
    "text": "A hierarchical time series is a multivariate time series with a hierarchical structure. Figure 1 gives an example with five bottom series and three aggregate series. The different observations in the hierarchy satisfy the following aggregation constraints: yt = yA,t + yB,t, yA,t = yAA,t + yAB,t+yAC,t and yB,t = yBA,t+yBB,t for all time periods t = 1, . . . , T .\nLet at be an r-vector containing the observations at the different levels of aggregation at time t, bt be an m-vector with the observations at the bottom level only, and yt = (at bt)\n′ be an n-vector that contains the observations of all series in the hierarchy with n = r + m. For the ex-\nample in Figure 1, we have at = (yt, yA,t, yB,t)′, bt = (yAA,t, yAB,t, . . . , yBB,t)\n′, r = 3, and m = 5. We can then write yt = Sbt, where S = [ S′a Im ]′ ∈ {0, 1}n×m is the summing matrix, Sa ∈ {0, 1}r×m and Im is an identity matrix of order m.\nSuppose we have access to T historical observations, y1, . . . ,yT , of a hierarchical time series. Under mean squared error (MSE) loss, the optimal h-period-ahead forecasts are given by the conditional mean (Gneiting, 2011), i.e.\nE[yT+h|y1, . . . ,yT ] = S E[bT+h|y1, . . . ,yT ], (1)\nwhere h = 1, 2, . . . ,H .\nIt is possible to compute forecasts for all series at all levels independently, which we call base forecasts. For example, we can estimate E[yi,T+h|yi,1, . . . , yi,T ] for i = 1, . . . , n, i.e. for all series in the hierarchy. This approach is very flexible since we can use different forecasting methods for each series and aggregation level. However, the aggregation constraints will not necessarily be satisfied.\nDefinition 1. The coherency errors of the h-period-ahead base forecasts ŷT+h = (âT+h b̂T+h)′ are given by r̂T+h = âT+h − Sab̂T+h.\nIn other words, r̂T+h is a vector containing the magnitude of constraint violations for each aggregate series.\nDefinition 2. The h-period-ahead base forecasts ŷT+h = (âT+h b̂T+h)\n′ are (mean) coherent if r̂T+h = 0, i.e. if there are no coherency errors.\nSince the optimal mean forecasts in (1) are coherent by definition, it seems sensible to impose the aggregation constraints when generating hierarchical mean forecasts. Also, from a decision-making perspective, coherent forecasts will guarantee coherent decisions over the entire hierarchy."
  }, {
    "heading": "2.1. Best Linear Unbiased Mean Revised Forecasts",
    "text": "Hyndman et al. (2011) proposed coherent hierarchical mean forecasts of the following form:\nỹT+h = SP ŷT+h, (2)\nfor some appropriately chosen matrix P ∈ Rm×n, and where ŷT+h are some base forecasts.\nThis approach has multiple advantages: (1) the forecasts are coherent by construction; (2) the forecasts are generated by combining forecasts from all levels; and (3) multiple hierarchical forecasting methods can be represented as particular cases, including bottom-up forecasts with P = [ 0m×r|1m×m ] , and top-down forecasts with P =[\npm×1|0m×(n−1) ]\nwhere p is a vector of proportions that sum to one. Theorem 1. (Adapted from Wickramasuriya et al., 2015) LetWh be the positive definite covariance matrix of the hperiod-ahead base forecast errors, êT+h = yT+h− ŷT+h, i.e.Wh = E[êT+hê′T+h].\nThen, assuming unbiased base forecasts, the best (i.e. having minimum sum of variances) linear unbiased revised forecasts are given by (2) with P = (S′W−1h S) −1S′W−1h . We will denote this method MinT.\nIn practice, the error covariance matrixWh needs to be estimated using historical observations of the base forecast errors. Wickramasuriya et al. (2015) estimated W1, and assumed that Wh ∝ W1, since the estimation of Wh is challenging for h > 1. To trade off bias and estimation variance, structural assumptions on the entries of the sample covariance matrix have also been considered in Hyndman et al. (2016)."
  }, {
    "heading": "2.2. Optimal Mean Combination and Reconciliation",
    "text": "The approach presented in the previous section applies both combination and reconciliation of the forecasts at the same time. van Erven & Cugliari (2015) proposed splitting the problem into two independent steps: “first one comes up with the best possible forecasts for the time series without worrying about aggregate consistency; and then a reconciliation procedure is used to make the forecasts aggregate consistent”.\nGiven some possibly incoherent base forecasts ŷT+h, and a weight matrixA ∈ Rn×n, they proposed a method called GTOP, which solves the following quadratic optimization problem:\nminimize xa∈Rr,xb∈Rm ∥∥∥∥AŷT+h −A(xaxb )∥∥∥∥2 (3)\nsubject to (xa xb)′ ∈ A ∩ B,\nwhere A = {(xa xb)′ : xa = Saxb} is the set of coherent vectors, and B is an additional set that allows the specification of additional constraints.\nThe solution of the previous problem is also equivalent to an optimal strategy in a minimax problem where the goal is to minimize the maximum error between the loss of the\nreconciled and the base forecasts. WhenA = I and B = ∅, the problem reduces to finding the closest reconciled forecasts to the base forecasts in terms of sum of squared errors (SSE).\nA distinctive advantage of the GTOP approach compared to MinT is the guarantee of producing revised forecasts ỹT+h = (x ∗ a x ∗ b) ′ with the same or smaller SSE than the base forecasts ŷT+h. Furthermore, compared to MinT, the base forecasts are not required to be unbiased. Also, by separating forecast combination and reconciliation, the GTOP approach allows the inclusion of regularization in the forecast combination step. One comparative weakness of GTOP is that it does not have a closed-form solution in the general case."
  }, {
    "heading": "3. Probabilistic Hierarchical Forecasting",
    "text": "There has been a shift in the forecasting literature, over the past two decades, towards probabilistic forecasting (Gneiting & Katzfuss, 2014). This form of prediction quantifies the uncertainty, which enables improved decision making and risk management. GTOP does not provide any quantification of the uncertainty in the predictions, and, although MinT allows the calculation of the forecast variances, this might not be enough to fully describe the uncertainty in the predictions.\nWe propose an algorithm to compute, for all series in the hierarchy, the conditional predictive cumulative distribution function:\nFi,T+h(y|y1, . . . ,yT ) = P(yi,T+h ≤ y|y1, . . . ,yT ),\nrather than just the conditional mean E[yi,T+h|y1, . . . ,yT ] and conditional variance V[yi,T+h|y1, . . . ,yT ], with i = 1, . . . , n.\nAs with mean forecasts, it is possible to independently compute probabilistic forecasts for each series in the hierarchy, but, again, these forecasts will not necessarily be coherent. In fact, hierarchical probabilistic forecasts are coherent if the predictive distribution of each aggregate series is equal to the distribution of the sum of the children series. Naturally, probabilistic coherency implies mean coherency as given in Definition 2."
  }, {
    "heading": "3.1. Bottom-Up Probabilistic Forecasting",
    "text": "With mean forecasts, it was possible to compute coherent bottom-up forecasts for the ith aggregated series by simply summing the associated lowest level mean forecasts, i.e. ỹit = sib̂t where si is the ith row of the S matrix, and i = 1, . . . , r. Now, given some base probabilistic forecasts for all the bottom series, how do we compute the bottom-up coherent probabilistic forecasts for all aggregated series?\nSince each aggregate series is the sum of a subset of bottom series, bottom-up probabilistic forecasting is harder to compute than mean forecasting because we need to compute the joint distribution of the component random variables. The marginal predictive distributions are not enough.\nDefinition 3. Let X1, . . . , Xd be a set of continuous random variables with joint distribution function F . Then, the distribution of Z = ∑d i=1Xi is given by\nFX1+···+Xd(z) = ∫ Rd\n1{x1+· · ·+xd ≤ z} dF (x1, . . . , xd). (4)\nTo model the joint distribution, we can use the copula framework (Nelsen, 2007). Copulas originate from Sklar’s theorem (Sklar, 1959), which states that for any continuous distribution function F with marginals F1, . . . , Fd, there exists a unique function C : [0, 1]d → [0, 1] such that F can be written as F (x1, . . . , xn) = C(F1(x1), . . . , Fd(xd)). In other words, starting from marginal predictive distributions for each series, and using a copula for the dependence structure, we can first compute the joint distribution, and then compute the distribution of the sum using (4).\nAlthough it is convenient to decompose the estimation of the joint distribution into the estimation of multiple marginal predictive distributions and one copula, the number of bottom series can be large in practice, which implies a high-dimensional copula. Furthermore, in highly disaggregated time series data, the bottom series are often very noisy, and as a result, the estimation of the dependence structure between all bottom series will be very challenging.\nSince we are only interested in specific aggregations, we can avoid explicitly modelling the (often) high-dimensional copula that describes the dependence between all bottom series. Building on the approach proposed by Arbenz et al. (2012), we propose to decompose the possibly highdimensional copula into multiple lower-dimensional copulas for all child series of each aggregate series.\nExample 3.1. Let us consider the hierarchy given in Figure 1. A classical bottom-up approach would require modelling the joint distribution of (yAA,t, yAB,t, yAC,t, yBA,t, yBB,t). Then, the distribution of all aggregate series yA,t, yB,t and yt can be computed using (4).\nHowever, since the marginals and the copula completely specify the joint distribution, the following procedure allows us to compute the marginal predictive distributions of all aggregates using three lower-dimensional copulas in a hierarchal manner:\n1. Compute FAA,t, FAB,t, FAC,t, FBA,t, and FBB,t.\n2. Compute FA,t using C1(FAA,t, FAB,t, FAC,t). 3. Compute FB,t using C2(FBA,t, FBB,t). 4. Compute Ft using C3(FA,t, FB,t).\nExcept in some special cases where the distribution of the sum can be computed analytically, we would typically resort to Monte Carlo simulations.\nBy Sklar’s theorem, we can write F (x1, . . . , xd) = P(X1 ≤ x1, . . . , Xd ≤ xd) = C(F1(x1), . . . , Fd(xd)). Suppose we have samples xik ∼ Fi, and uk = (u1k, . . . , u d k) ∼ C, k = 1, . . . ,K, then we can compute\nF̂ (x1, . . . , xd) = Ĉ(F̂1(x1), . . . , F̂d(xd)),\nwhere F̂i are the empirical margins and Ĉ is the empirical copula (see Rüschendorf, 2009, and the references therein), given respectively by\nF̂i(x) = 1\nK K∑ k=1 1{xik ≤ x}, x ∈ R,\nand\nĈ(u) = 1\nK K∑ k=1 1 { rk(u1k) K ≤ u1, . . . , rk(udk) K ≤ ud } ,\nfor u = (u1, . . . , ud) ∈ [0, 1]d, where rk(uik) is the rank of uik within the set {ui1, . . . , uiK}.\nThe procedure of applying empirical copulas to empirical margins can be efficiently represented in terms of sample reordering. In fact, the order statistics ui(1), . . . , u i (K) of the samples ui1, . . . , u i K induce a permutation pi of the integers {1, . . . ,K}, defined by pi(k) = rk(uik) for k = 1, . . . ,K. If we then apply the permutations to each independent marginal sample {xi1, . . . , xiK}, the reordered samples inherit the multivariate rank dependence structure from the copula Ĉ. We can then compute the samples for the sum {x1, . . . , xK} where xk = ∑d i=1 x i k.\nIntroducing a dependence structure into originally independent marginal samples goes back to Iman & Conover (1982) who considered the special case of normal copulas. A similar idea has been considered more recently in Schefzik et al. (2013) to specify multivariate dependence structure with applications to weather forecasting.\nSince we are interested in multivariate forecasting, we will need another version of Sklar’s theorem for conditional joint distributions proposed by Patton (2006):\nIf yt|Ft−1 ∼ F (·|Ft−1), with yit|Ft−1 ∼ Fi(·|Ft−1), i = 1, . . . , n, then\nF (y|Ft−1) = C(F1(y1|Ft−1), . . . , Fn(yn|Ft−1)|Ft−1).\nAs in Patton (2012), we will assume the following structure for our series:\nyit = µi(yt−1,yt−2, . . . ) + σi(yt−1,yt−2, . . . )εit, (5)\nwhere εit|yt−1,yt−2, · · · ∼ Fi(0, 1). In other words, each series can have a potentially time-varying conditional mean and variance, but the standardized residual, εit, has a constant conditional distribution for simplicity. See Fan & Patton (2014) for a review on copulas in econometrics.\nThe following algorithm describes how to compute the bottom-up samples using the reordering procedure for a complete hierarchy: Algorithm 1. (Bottom-up Probabilistic Forecasting)\n1. For all series in the hierarchy, as defined in (5), model the conditional marginal distributions; i.e. compute µ̂i and σ̂i for i = 1, . . . , n. 2. Then, compute the standardized residuals ε̂it = (yi,t− µ̂i,t)/σ̂i,t, and define the permutations pi(t) = rk(ε̂it), where i = 1, . . . , n and t = 1, . . . , T . 3. For all bottom series i = r + 1, . . . , n: (a) Compute h-period ahead conditional marginal\npredictive distributions F̂i,T+h. (b) Extract a discrete sample of size K = T , say\nxi1, . . . , x i K , where x i k = F̂ −1 i,T+h(k/K + 1), and\nk = 1, . . . ,K. 4. For all aggregate series i = 1, . . . , r:\n(a) Let i(1), . . . , i(nc) be the nc children series of the aggregate series i. (b) Recursively compute\nxik = x i(1) (pi(1)(k)) + · · ·+ xi(nc)(pi(nc)(k)),\nwhere xi(k) denotes the kth order statistics of {xi1, . . . , xiK}, i.e. xi(1) ≤ x i (2) ≤ · · · ≤ x i (K).\nSimilarly to the classical bottom-up algorithm, Algorithm 1 produces coherent samples by construction. Furthermore, the samples of each aggregate are computed using only the predictive distributions of the bottom series. However, Algorithm 1 has two main advantages compared to a classical bottom-up algorithm: (1) instead of estimating a highdimensional copula for the dependence between all the bottom series, we only need to specify the joint dependence between the child series of each aggregate series, and (2) since each copula is estimated at different aggregate levels, we can benefit from better estimation since the series are smoother, and easier to model and forecast."
  }, {
    "heading": "3.2. Mean Forecast Combination and Reconciliation",
    "text": "Algorithm 1 computes bottom-up probabilistic forecasts by estimating the copula dependence functions using data from different levels of the hierarchy. However, the resulting mean forecasts are equal to classical bottom-up forecasts, i.e. no data from other levels is used. In order to\nfurther improve the accuracy of our probabilistic forecasts, we add a mean forecast combination step, which allows to exploit possibly better mean forecasts from higher levels. Forecast combination is known to improve forecasts in many cases (Timmermann, 2006; Genre et al., 2013). We could adjust the means of our predictive distributions using the MinT revised forecasts. However, as van Erven & Cugliari (2015), we propose to first combine the mean forecasts, and then apply a reconciliation step.\nLet ŷT+h be the means of our predictive distributions. We compute the following forecast combination:\ny̆t = Qŷt, (6)\nwhereQ = [ q1, . . . , qn ]′ ∈ Rn×n is a weight matrix. Since the combined mean forecasts y̆t are not necessarily coherent, we also apply a reconciliation step using the GTOP approach described in Section 2.2. More precisely, we solve the quadratic optimization problem in (3), and obtain reconciled forecasts ỹt.\nSince the total number of series in the hierarchy, n, can be very large compared to the number of observations T , it is necessary to use some regularization for the weights. Therefore, we will estimate the weights by solving the following L1 optimization problem:\nminimize Q\n1\nT T∑ t=1 ‖yt −Qŷt‖2 + n∑ i=1 λi ‖qi‖1 ,\nwhere λi ≥ 0 is a regularization parameter for the ith weight vector qi. The previous problem can be rewritten as\nminimize q1,...,qn n∑ i=1 1 T T∑ t=1 (yit − ŷ′tqi)2 + n∑ i=1 λi ‖qi‖1 ,\nwhich is decomposable in the vectors qi. As a result, we can solve the n problems independently. Our implementation of the LASSO is based on a cyclical coordinate descent algorithm (Friedman et al., 2007), and the regularization parameters are selected by minimizing time series cross-validated errors (Hyndman & Athanasopoulos, 2014, Section 2.5).\nThe forecast combination that we are considering in (6) has multiple advantages compared to the MinT forecast combination in (2). First, since Q ∈ Rn×n, all series in the hierarchy can benefit directly from the forecast combination, not only the bottom series as in MinT with P ∈ Rm×n. Second, we do not assume the base forecast are unbiased, and we do not seek to compute unbiased revised forecasts as in MinT. We rather seek to optimize the weights in order to obtain combined forecasts with low forecast errors; i.e.\nwith the right trade-off between bias and estimation variance. Finally, even if we start with coherent base forecasts, we can still apply a forecast combination, and eventually reconcile them later. In contrast with MinT, no forecast combination will be applied in that case. Of course, MinT has the advantage of having a closed-form solution, which does not require the solution of n possibly highdimensional regression problems. Finally, our reconciled forecasts are guaranteed to have smaller or equal SSE than the combined forecasts, which is guaranteed by the GTOP method as discussed in Section 2.2. Our final algorithm can be summarized as follows:\nAlgorithm 2. (Mean Combined and Reconciled Probabilistic Forecasting)\n1. Run Algorithm 1 to obtain bottom-up samples for all series in the hierarchy, say xi1, . . . , x i K with i =\n1, . . . , n. 2. Extract mean forecasts ŷT+h from all base predictive\ndistributions F̂i,T+h, and compute combined forecasts y̆T+h given in (6). 3. Given a weight matrix A, and using the combined forecasts y̆T+h as base forecasts, solve the optimization problem in (3) to obtain reconciled forecasts ỹT+h. 4. Compute revised samples x̃i1, . . . , x̃ i K where x̃ i k =\nxik+θi and θi = (y̌i,t−ŷi,t)+(ỹi,t−y̆i,t) = y̆i,t−ŷi,t is an adjustment term, with i = 1, . . . , n.\nAlgorithm 2 computes coherent forecasts since both the bottom-up samples (computed using Algorithm 1) and the reconciled means are coherent."
  }, {
    "heading": "4. Experiments",
    "text": "We compare the following forecasting methods: (1) BASE: the base predictive distributions; (2) NAIVEBU: the naive bottom-up forecasts computed by summing independent samples from the bottom predictive distributions (without forecast combination); (3) PERMBU: the bottom-up forecasts computed using Algorithm 1 (without forecast combination); (4) PERMBU-MINT: similar to PERMBU with mean forecasts computed using MinT; (5) PERMBU-GTOP1: the forecasts are computed using Algorithm 2 with A = I; and (6) PERMBU-GTOP2: similar to PERMBU-GTOP1 but with A = diag(0, . . . , 0︸ ︷︷ ︸\nr , 1, . . . , 1︸ ︷︷ ︸ m ); i.e. bottom-up instead\nof reconciled combined mean forecasts."
  }, {
    "heading": "4.1. Probabilistic Forecast Evaluation",
    "text": "We evaluate our predictive distributions using the continuous ranked probability score (CRPS), which is a proper scoring rule, i.e. the score is maximized when the true dis-\ntribution is reported (Gneiting & Raftery, 2007). Given an h-period-ahead cumulative predictive distribution function F̂t+h and an observation yt+h, the CRPS can be defined as (Gneiting & Ranjan, 2011):\nCRPS(F̂t+h, yt+h) = ∫ 1 0 QSτ ( F̂−1t+h(τ), yt+h ) dτ,\nwhere QSτ is the quantile score, defined as\nQSτ ( F̂−1t+h(τ), yt+h ) = 2 ( 1{yt+h ≤ F̂−1t+h(τ)} − τ )( F̂−1t+h(τ)− yt+h ) ,\nwhich is also known as the pinball or check loss (Koenker & Bassett, 1978).\nIn order to quantify the gain/loss of the different methods with respect to the base forecasts, we compute the Skill Score defined as (SCOREBASE − SCORE)/SCOREBASE where SCORE is the considered evaluation score. Low values of the score are desirable, and so high positive values are preferable for the skill score. In the following experiments, SCORE will be computed by averaging the CRPS or QS over all observations in the test set. Finally, as proposed by Laio & Tamea (2007), we will plot the QSτ (skill score) versus τ as a diagnostic tool in the comparison of the different methods."
  }, {
    "heading": "4.2. Simulated Data",
    "text": "We begin with simulated time series, implemented using the same processes as Wickramasuriya et al. (2015) to evaluate different hierarchical forecasting methods. However, we focus on distributional forecasts rather than mean forecasts. We used a hierarchy with four bottom series, where the two pairs of bottom series are aggregated in two aggregate series, which are then aggregated in a top series. Hence, the hierarchy is composed of n = 7 series, m = 4 bottom series and r = 3 aggregate series.\nEach series in the bottom level is generated from an ARIMA(p, d, q) process, with p and q taking values of 0, 1 and 2 with equal probability and d taking values of 0 and 1 with equal probability. The parameters are chosen randomly from a uniform distribution from a specific parameter space for each each component of the ARIMA process (see Table 3.2 in Wickramasuriya et al. (2015)). The error terms of the bottom-level ARIMA processes have a multivariate Gaussian distribution with a covariance structure that allows a strongly positive correlation among series with the same parents, but a moderately positive correlation among series with different parents.\nFor each series, we generate T = 100, 300 or 500 observations, with an additional H = 10 observations as a test set.\nWe fit an ARIMA model by minimizing the AIC, and compute 10-period ahead Gaussian predictive distributions as base forecasts. The whole process is repeated 2, 000 times.\nFigure 2 shows the results for T = 100. The first panel gives the CRPS skill score for each horizon; the second and third panels show the QS skill score averaged over horizons h = 1–6 and h = 7–10, respectively; the last panel gives the CRPS skill score for the bottom level.\nIn the first panel, we can see that PERMBU has a better skill score than NAIVEBU until horizon 6, and vice versa for the subsequent horizons. The second panel shows that PERMBU outperforms NAIVEBU especially in the lower and upper tails. In other word, the independence assumption of NAIVEBU is not valid, and modelling the dependence structure between the children series of each aggregated series provides better tail forecasts for the aggregate series. The third panel shows that NAIVEBU has consistently better QS skill score compared to PERMBU for horizons 7–10. This suggests that using one-period ahead dependence structure for 7 to 10-period ahead forecasts (i.e. using a misspecified dependence structure) is worse than assuming independence.\nThe first panel also shows that the methods using forecast combinations have significantly increased the CRPS skill score compared to PERMBU. This suggests that the mean forecast combination step is particularly useful in further improving the distributional forecasts. Furthermore, we can see that PERMBU-GTOP2 has better skill score than PERMBU-MINT until horizon 6. This shows the benefit of our forecast combination, which learns the best combination weights, without making an unbiasedness assumption. The better skill score of PERMBU-GTOP2 compared to PERMBU-GTOP1 suggests an advantage in splitting the forecast combination and reconciliation steps. The same observations can be made in the last panel for the bottom level.\nFinally, with a larger training set size (T = 300 and T = 500), the forecast combination methods have similar skill scores, as can be seen in Figures A1 and A2 in the appendix. With more observations, the fitted ARIMA model becomes more accurate, and therefore, forecast combination is less likely to improve the base forecasts. However, even with a large training set, modeling the dependence structure is still important, as shown by the better skill score of PERMBU compared to NAIVEBU."
  }, {
    "heading": "4.3. Electricity Smart Meter Data",
    "text": "We used smart meter electricity consumption data collected by four energy supply companies in Great Britain (AECOM, 2011). Consumption was recorded at half-hourly intervals for more than 14,000 households, along with ge-\nographic and demographic information. In our study, we were interested only in relatively long time series without missing values, and this led us to use data recorded at 1,578 meters for the period 20 April 2009 to 31 July 2010, inclusive. Each series, therefore, consisted of T = 22, 464 halfhourly observations. We constructed a hierarchy based on geographical information comprising four levels of aggregation with m = 1, 578 series in the bottom level of the hierarchy, and r = 55 aggregated series in the other three levels of the hierarchy . Figure 3 presents observations for a one-week period for just one series taken from each of the four levels of the hierarchy. The values shown on the right hand side of the figure correspond to the number of bottom level series that have been summed to give each of the aggregated series in the figure.\nWe considered the problem of one-day-ahead (i.e. the next H = 48 half-hours) probabilistic demand forecasting, with a forecast origin at 23:30 for each day. We split each time series into training, validation and test sets; the first 12 months for training, the next month for validation and the remaining, approximately, three months for testing. Each model is re-estimated before forecasting each day in the test set using a rolling window of the historical observations.\nWe used different forecasting methods for the aggregate and bottom series. For the aggregate series, we capture the yearly cycle, the within-day and within-week seasonalities using seasonal Fourier terms with coefficients estimated by\nTime\n1578\n450\n179\n68\n10\n1\nFigure 3. One week of electricity demand with different number of aggregated series.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\n− 10\n0 5\n10 15\n20 25\nLog10(number of aggregated meters)\nC R\nP S\ns ki\nll (%\n)\n●\n●\n●●\n●●\n●\n●\n● ●●●\n●\n●\n● NAIVEBU PERMBU PERMBU−MINT PERMBU−GTOP1 PERMBU−GTOP2\n0.0 0.2 0.4 0.6 0.8 1.0\n0. 0\n0. 5\n1. 0\n1. 5\n2. 0\nAggregate levels\nProbability level\nQ S\nFigure 4. CRPS skill for different aggregations and average QS of all aggregate series. A positive/negative CRPS skill gives the percentage of decrease/increase in CRPS with respect to the base forecasts. Higher skill score and lower QS are better.\nLASSO. After extracting the trend and seasonalities, we fitted an ARIMA model and computed Gaussian predictive distributions. This is justified by the fact that aggregate series are often smoother and easier to forecast, and by the central limit theorem. For the base forecasts, we implemented the kernel density estimation approach that performed the best in the work of Arora & Taylor (2016).\nIn Figure 4, the first panel gives the skill score for different aggregations computed using the average half-hourly CRPS over the test set, while the second panel shows the average QS of all aggregated series. In the first panel, we can see that PERMBU has better skill score than NAIVEBU especially for large aggregations. The second panel shows that PERMBU, by modelling the dependence structure, has contributed to significantly decrease the QS in the lower tail. By analyzing the forecasts (not shown here), we noticed that NAIVEBU is penalized both for not being able to capture the trend at the top (i.e. a bad mean forecasts), and for having too sharp predictive distributions (i.e. computed using a bad dependence structure). The fact that NAIVEBU seems competitive at moderately large quantiles can be explained by the unnecessarily wide prediction intervals for\nthe other methods, which are penalized by the QS.\nOverall, the first panel shows that the mean forecast combination methods have better skill score than the base forecasts. We found that 75% of the series have less than 100 non-zero weights (see appendix); i.e. many forecast combinations were very sparse — an advantage of our approach compared to MinT, which produces dense combination weights. Furthermore, we can see that PERMBU-GTOP1 is dominating the other methods for almost all aggregations. This suggests that computing bottom-up mean combined forecasts is better than reconciling the aggregate and bottom combined mean forecasts. This can be explained by the fact that PERMBU already produces competitive forecasts with the base forecasts, and so reconciling the bottom combined forecasts with the aggregate combined forecasts is unlikely to improve the final forecasts.\nFinally, the first panel shows that all the mean forecast combination methods have lower skill score than the base forecasts for the bottom series (i.e. for the first aggregation). This suggests that improving the forecast accuracy at the bottom level using forecast combination is particularly challenging especially with very noisy time series. However, the forecast improvement at the aggregate levels are magnitudes larger than the decrease in accuracy at the bottom level."
  }, {
    "heading": "5. Conclusion",
    "text": "We have proposed an algorithm to compute coherent probabilistic forecasts for hierarchical time series. The algorithm provides samples from coherent predictive distributions for each series in the hierarchy. To do so, we first generate independent samples from all series in the hierarchy. Then a sequence of permutations are applied to the samples in order to restore the dependencies between the children series of all aggregate series. Finally, a sparse forecast combination is applied using the base mean forecasts of all series in the hierarchy. Our algorithm has the advantage of synthesizing information from multiple levels in the hierarchy. Using simulated data, and a large scale electricity demand data set, we showed that restoring the dependencies of the children series consistently improves the forecast accuracy, especially in the tails, while the mean forecast combining weights provide an additional improvement by enabling a synthesis of information from the different forecasts. Our algorithm can be used to produce coherent probabilistic forecasts for hierarchical time series in many applications."
  }],
  "year": 2017,
  "references": [{
    "title": "Copula based hierarchical risk aggregation through sample reordering",
    "authors": ["Arbenz", "Philipp", "Hummel", "Christoph", "Mainik", "Georg"],
    "venue": "Insurance, Mathematics & Economics,",
    "year": 2012
  }, {
    "title": "Forecasting electricity smart meter data using conditional kernel density estimation",
    "authors": ["Arora", "Siddharth", "Taylor", "James W"],
    "venue": "Omega, 59,",
    "year": 2016
  }, {
    "title": "Probabilistic weather forecasting for winter road maintenance",
    "authors": ["Berrocal", "Veronica J", "Raftery", "Adrian E", "Gneiting", "Tilmann", "Steed", "Richard C"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2010
  }, {
    "title": "Copulas in econometrics",
    "authors": ["Fan", "Yanqin", "Patton", "Andrew J"],
    "venue": "Annual Review of Economics,",
    "year": 2014
  }, {
    "title": "Pathwise coordinate optimization",
    "authors": ["Friedman", "Jerome", "Hastie", "Trevor", "Höfling", "Holger", "Tibshirani", "Robert"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2007
  }, {
    "title": "Combining expert forecasts: Can anything beat the simple average",
    "authors": ["Genre", "Véronique", "Kenny", "Geoff", "Meyler", "Aidan", "Timmermann", "Allan"],
    "venue": "International Journal of Forecasting,",
    "year": 2013
  }, {
    "title": "Making and evaluating point forecasts",
    "authors": ["Gneiting", "Tilmann"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2011
  }, {
    "title": "Probabilistic forecasting",
    "authors": ["Gneiting", "Tilmann", "Katzfuss", "Matthias"],
    "venue": "Annual Review of Statistics and Its Application,",
    "year": 2014
  }, {
    "title": "Strictly proper scoring rules, prediction, and estimation",
    "authors": ["Gneiting", "Tilmann", "Raftery", "Adrian E"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2007
  }, {
    "title": "Comparing density forecasts using threshold- and Quantile-Weighted scoring rules",
    "authors": ["Gneiting", "Tilmann", "Ranjan", "Roopesh"],
    "venue": "Journal of Business & Economic Statistics,",
    "year": 2011
  }, {
    "title": "Conditional transformation models",
    "authors": ["Hothorn", "Torsten", "Kneib", "Thomas", "Bühlmann", "Peter"],
    "venue": "Journal of the Royal Statistical Society. Series B, Statistical methodology,",
    "year": 2014
  }, {
    "title": "Forecasting: principles and practice",
    "authors": ["Hyndman", "Rob J", "Athanasopoulos", "George"],
    "venue": "OTexts,",
    "year": 2014
  }, {
    "title": "Optimal combination forecasts for hierarchical time series",
    "authors": ["Hyndman", "Rob J", "Ahmed", "Roman A", "Athanasopoulos", "George", "Shang", "Han Lin"],
    "venue": "Computational Statistics & Data Analysis,",
    "year": 2011
  }, {
    "title": "Fast computation of reconciled forecasts for hierarchical and grouped time series",
    "authors": ["Hyndman", "Rob J", "Lee", "Alan J", "Wang", "Earo"],
    "venue": "Computational Statistics & Data Analysis,",
    "year": 2016
  }, {
    "title": "A distribution-free approach to inducing rank correlation among input variables. Communications in Statistics ",
    "authors": ["Iman", "Ronald L", "Conover", "W J"],
    "venue": "Simulation and Computation,",
    "year": 1982
  }, {
    "title": "Beyond mean regression",
    "authors": ["Kneib", "Thomas"],
    "venue": "Statistical Modelling,",
    "year": 2013
  }, {
    "title": "Regression quantiles",
    "authors": ["Koenker", "Roger", "Bassett", "Gilbert"],
    "venue": "Econometrica: journal of the Econometric Society,",
    "year": 1978
  }, {
    "title": "The sum and its parts: Judgmental hierarchical forecasting",
    "authors": ["Kremer", "Mirko", "Siemsen", "Enno", "Thomas", "Douglas J"],
    "venue": "Management Science,",
    "year": 2016
  }, {
    "title": "Verification tools for probabilistic forecasts of continuous hydrological variables",
    "authors": ["F Laio", "S. Tamea"],
    "venue": "Hydrology and Earth System Sciences,",
    "year": 2007
  }, {
    "title": "An introduction to copulas",
    "authors": ["Nelsen", "Roger B"],
    "venue": "Springer Science & Business Media,",
    "year": 2007
  }, {
    "title": "Copula methods for forecasting multivariate time series",
    "authors": ["Patton", "A J"],
    "venue": "Handbook of economic forecasting,",
    "year": 2012
  }, {
    "title": "Modelling asymmetric exchange rate dependence",
    "authors": ["Patton", "Andrew J"],
    "venue": "International Economic Review,",
    "year": 2006
  }, {
    "title": "On the distributional transform, sklar’s theorem, and the empirical copula process",
    "authors": ["Rüschendorf", "Ludger"],
    "venue": "Journal of Statistical Planning and Inference,",
    "year": 2009
  }, {
    "title": "Uncertainty quantification in complex simulation models using ensemble copula coupling",
    "authors": ["Schefzik", "Roman", "Thorarinsdottir", "Thordis L", "Gneiting", "Tilmann"],
    "venue": "Statistical Science: a review journal of the Institute of Mathematical Statistics,",
    "year": 2013
  }, {
    "title": "Fonctions de répartition à n dimensions et leurs marges",
    "authors": ["M. Sklar"],
    "venue": "Université Paris",
    "year": 1959
  }, {
    "title": "Forecast combinations",
    "authors": ["A. Timmermann"],
    "venue": "In Handbook of Economic Forecasting,",
    "year": 2006
  }, {
    "title": "Forecasting hierarchical and grouped time series through trace minimization",
    "authors": ["Wickramasuriya", "Shanika L", "Athanasopoulos", "George", "Hyndman", "Rob J"],
    "venue": "Technical Report 15/15,",
    "year": 2015
  }],
  "id": "SP:7a22088a08648e628d278fe8c89a3e3ded1eb66d",
  "authors": [{
    "name": "Souhaib Ben Taieb",
    "affiliations": []
  }, {
    "name": "James W. Taylor",
    "affiliations": []
  }, {
    "name": "Rob J. Hyndman",
    "affiliations": []
  }],
  "abstractText": "Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels — the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.",
  "title": "Coherent Probabilistic Forecasts for Hierarchical Time Series"
}