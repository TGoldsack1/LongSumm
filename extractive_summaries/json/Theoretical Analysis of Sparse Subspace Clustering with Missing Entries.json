{
  "sections": [{
    "heading": "1. INTRODUCTION",
    "text": "Clustering data lying close to an unknown union of lowdimensional linear subspaces is a fundamental problem in unsupervised machine learning, known as Subspace Clustering or Generalized Principal Component Analysis (Vidal et al., 2016). Indeed, this problem is intimately related to the extension of the classical Principal Component Analysis (PCA) to multiple subspaces, and in recent years has\n1School of Information Science and Technology, ShanghaiTech University, Shanghai, China. 2Mathematical Institute for Data Science and Department of Biomedical Engineering, Johns Hopkins University, Baltimore, USA. Correspondence to: Manolis C. Tsakiris <mtsakiris@shanghaitech.edu.cn>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfound numerous applications in machine learning, computer vision, pattern recognition, bioinformatics and systems theory. Moreover, recent work is beginning to explore connections between subspace clustering and deep learning, with the goal of learning unions of low-dimensional non-linear manifolds (Peng et al., 2016).\nAmong a variety of subspace clustering methods (Vidal et al., 2016) including algebraic (Vidal et al., 2005; Tsakiris & Vidal, 2017b; 2018a), iterative (Bradley & Mangasarian, 2000), recursive (Fischler & Bolles, 1981; Tsakiris & Vidal, 2017a), and spectral (Aldroubi et al., 2017; Heckel & Bölcskei, 2015; Lu et al., 2012; Chen & Lerman, 2009) techniques, Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013) is one of the most popular methods. The reason is that it exhibits a very competitive performance in real-world datasets, it admits efficient algorithmic implementations, and is supported by a rich body of theory (Elhamifar & Vidal, 2013; Soltanolkotabi & Candès, 2012; Wang & Xu, 2016; Soltanolkotabi et al., 2014). In addition, SSC is able to cluster data from incomplete observations reasonably well (Yang et al., 2015), which is an important problem (Ongie et al., 2017; Pimentel-Alarcon & Nowak, 2016; Elhamifar, 2016; Yang et al., 2015; Heckel & Bölcskei, 2015; Pimentel-Alarcon et al., 2015; Eriksson et al., 2012; Recht, 2011; Balzano et al., 2010), since in many applications not all features are available for every data point: Users of recommendation systems only rate a few items, medical patients undergo a few tests and treatments, images are corrupted by occlusions, dynamic processes are observed across short time intervals and so on.\nEven though the theoretical foundations of SSC are by now mature, there are many lingering open questions. For example, it is still unclear whether better conditions exist for the performance of SSC even for uncorrupted data; contrast this to the recent study of You & Vidal (2015), who establish a hierarchy of such conditions for sparse subspace recovery. More importantly, even though a satisfactory theory for SSC with general noise does exist (Wang & Xu, 2016), the theoretical properties of SSC for data with missing entries remain elusive. The works of Wang et al. (2016) and Charles et al. (2018) are important recent efforts towards understanding SSC with missing entries. However, the conditions of Wang et al. (2016) are hard to interpret and they refer to the formulation of SSC with exact self-\nexpressiveness equality constraint, which is not an optimal choice for corrupted data. On the other hand, following Wang & Xu (2016), Charles et al. (2018) provide bounds similar to a subset of the results in the present paper.1\nIn this paper we provide a novel theoretical analysis of SSC for incomplete data. More precisely, we provide theoretical performance guarantees for SSC applied to i) ZeroFilled data (ZF-SSC), in which case all unobserved entries are filled with zeros, and ii) Projected-Zero-Filled data (PZF-SSC), in which case all unobserved entries are filled with zeros and in addition all data points are projected onto the observation pattern of the point being expressed each time.2. A direct comparison of the tolerable bounds of missing entries for ZF-SSC (Theorem 7) and PZF-SSC (Theorem 5) serves as a theoretical indication for the latter being a better method than the former. This is in agreement with experimental evaluation given here and also previously reported by Yang et al. (2015). Since PZF data have in principle many more missing entries than ZF data, this is a remarkable phenomenon, of potentially wider significance to the entire class of self-expressive-based methods, e.g., (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016).\nThe rest of the paper is organized as follows. In §1.1 we introduce the notation and the main mathematical objects of this paper. In §2 we review SSC for uncorrupted data, and discuss the two known elementary formulations of SSC for incomplete data, i.e., ZF-SSC and PZF-SSC. In §3 we present the main contributions of this paper, which consist of deterministic and probabilistic characterizations of the tolerable percentage of missing entries for ZF-SSC and PZF-SSC, as well as a theoretical and experimental comparison between the two methods (all proofs can be found in our pre-print (Tsakiris & Vidal, 2018b)). We conclude in §4, where we discuss the main insights of this paper as well as existing challenges."
  }, {
    "heading": "1.1. Notation and Main Objects",
    "text": "The nature of the problem studied in this paper calls for a rather heavy notation, which we have strived to simplify and unify as much as possible. To avoid introducing complicated notation amidst other technical developments, we have found it convenient to gather all relevant objects in Definition 1,3 which the reader is encouraged to refer to\n1In the terminology of the present paper Charles et al. (2018) independently study ZF-SSC.\n2This is called EWZF-SSC by Yang et al. (2015); here we have taken the liberty to rename the method according to the more suggestive name PZF-SSC.\n3For simplicity and clarity, and without loss of generality, we have chosen to present our theoretical results in the context of expressing a single point in terms of the remaining points in the dataset (the precise problem formulation is deferred to §2).\nwhen necessary. Other than that, for ` a positive integer, we define [`] := {1, . . . , `}. For a vector w ∈ RD we define ŵ := w/‖w‖2, if w 6= 0 and ŵ := 0, otherwise. For any linear subspace V of RD, we denote by P V the square matrix that represents the orthogonal projection of RD onto V . Given a binary relation, RHS stands for Right-Hand-Side, and similarly for LHS. Finally, 〈·, ·〉 is the standard inner product of RD. Definition 1. We define the following objects:\n1. The linear subspaces: For i ∈ [n], we let Si be a linear subspace of RD, where dimSi = di < D.\n2. The complete data: With an abuse of notation we let\nX = [X(1), . . . ,X(n)]Γ ∈ RD×N (1)\ndenote a data matrix as well as a set (formed by the columns of this matrix) of unit `2-norm points in the union of the linear subspaces Si, i ∈ [n], where X(i) = [x\n(i) 1 , . . . ,x (i) Ni ] ⊂ Si, Span(X(i)) = Si, and Γ is an unknown permutation, indicating that the clustering of the points with respect to the subspaces is unknown. We define X(1)−1 := X\n(1) \\{x(1)1 }, X−1 := X \\{x(1)1 }, and X\n(−1) := X \\X(1), where \\ denotes set-theoretic difference.\n3. The pattern of missing entries: For every point x (i) j ∈ RD we consider an observation pattern ω (i) j ∈\n{0, 1}D, where a value of 1 indicates an observed entry, while a value of 0 indicates an unobserved entry. We assume each ω(i)j has precisely m zeros. We let ω̃ (i) j := 1− ω (i) j , where 1 is the vector of all ones.\n4. The observed/unobserved coordinate subspaces: We let Ē(i)j := Span{ek : e>k ω (i) j 6= 0}, with ek the\ncanonical vector of RD with zeros everywhere and a 1 at position k. The orthogonal projection onto Ē(i)j is given by P̄ (i)j := diag(ω (i) j ), the matrix with ω (i) j on its diagonal and zeros everywhere else. Ẽ(i)j is the orthogonal complement of Ē(i)j and P̃ (i) j = diag(ω̃ (i) j ) is the orthogonal projection onto Ẽ(i)j .\n5. The zero-filled data (ZF-data): We let X̄ ∈ RD×N be the data X with zeros appearing in the unobserved entries, i.e., the column of X̄ associated to point x(i)j is x̄(i)j := P̄ (i) j x (i) j , ∀i, j.\n6. The projected data: We let Ẋ := P̄ (1)1 X be the projection of the data X onto the observed coordinate subspace Ē(1)1 associated to point x (1) 1 . The column of\nẊ associated to x(i)j is ẋ (i) j := P̄ (1) 1 x (i) j , ∀i, j.\n7. The projected and zero-filled data (PZF-data): We let ˙̄X be the projection of the zero-filled data onto Ē(1)1 , i.e., ˙̄X := P̄ (1) 1 X̄ . The column of\n˙̄X associated to point x(i)j is ˙̄x (i) j := P̄ (1) 1 x̄ (i) j , ∀i, j.\n8. The unobserved data: We define X̃ to be the unobserved components of the data, i.e., X̃ := X − X̄ , and x̃(i)j := P̃ (i) j x (i) j , ∀i, j. Similarly, for PZF data\nwe define ˙̃X := Ẋ − ˙̄X , and ˙̃x(i)j := P̄ (1) 1 x̃ (i) j , ∀i, j.\n9. The projected subspaces: For i ∈ [n], we let Ṡi ⊂ RD be the orthogonal projection of Si onto the subspace Ē(1)1 . In other words, if b (i) 1 , . . . , b (i) di\nis a basis for Si, then Ṡi is the subspace of RD spanned by the vectors P̄ (1)1 b (i) k ,∀k ∈ [di].\n10. The inradius: We let r be the relative inradius of the symmetrized convex hull Q of all points X(1)−1 lying in subspace S1, except point x(1)1 , i.e., r is the radius of the largest Euclidean ball of S1 contained in Q.\n11. The dual directions: For W = X, X̄, ˙̄X corresponding to complete data X , ZF-data X̄ and PZFdata ˙̄X , consider the reduced Lasso-SSC problem\nmin c,e ‖c‖1 +\nλ 2 ‖e‖22 s.t. w (1) 1 = W (1) −1c + e, (2)\ncorresponding to either complete data X , ZF-data X̄ or PZF-data ˙̄X . Consider the dual problem\nmax v 〈v,w(1)1 〉 −\n1\n2λ ‖v‖22 s.t. ‖v >W (1) −1‖∞ ≤ 1. (3)\nLet v∗λ, v̄ ∗ λ, ˙̄v ∗ λ be the optimal solution to problem (3) corresponding to W = X, X̄, ˙̄X respectively; these solutions are unique because (3) is strongly convex. Then we define the corresponding dual directions v̂1,λ, ˆ̄v1,λ, ˆ̄̇v1,λ to be the normalized projections of v∗λ, v̄ ∗ λ, ˙̄v ∗ λ onto S1,S1, Ṡ1 respectively (if any of these projections is equal to zero, then we define the corresponding dual direction to be the zero vector).\n12. The inter-subspace coherences: We define the intersubspace coherences for complete data, ZF-data, and PZF-data respectively as\nµλ := max i>1, k∈[Ni]\n|〈x(i)k , v̂1,λ〉| (4)\nµ̄λ := max i>1, k∈[Ni]\n|〈x̄(i)k , ˆ̄v1,λ〉| (5)\n˙̄µλ := max i>1, k∈[Ni]\n|〈 ˙̄x(i)k , ˆ̄̇v1,λ〉|. (6)\n13. The intra-subspace coherences:\nζ := ‖(X(1)−1)>x (1) 1 ‖∞, (7) ζ̄ := ‖(X̄(1)−1)>x̄ (1) 1 ‖∞, (8) ˙̄ζ := ‖( ˙̄X(1)−1)> ˙̄x (1) 1 ‖∞, (ζ̄ = ˙̄ζ) (9)\n14. Other quantities:\nη̄ := ‖x̄(1)1 ‖2, (10) ˙̄η := ‖ ˙̄x(1)1 ‖2, (η̄ = ˙̄η) (11)\nγ̄ := max i>1,k∈[Ni], j∈[N1]\n|〈x̄(i)k ,P S⊥1 x̃ (1) j 〉| (12)\n˙̄γ := max i>1,k∈[Ni], j∈[N1] |〈 ˙̄x(i)k ,P Ṡ⊥1 ˙̃x (1) j 〉|. (13)"
  }, {
    "heading": "2. Review of Sparse Subspace Clustering",
    "text": "We begin by reviewing Sparse Subspace Clustering (SSC) for data with no corruptions (§2.1), as well as the two elementary approaches to SSC for incomplete data (§2.2), which this paper is devoted to analyzing."
  }, {
    "heading": "2.1. SSC With Uncorrupted Data",
    "text": "In the absence of data corruptions (noise, missing entries, outliers, etc.) we consider a data matrix X ∈ RD×N as in Definition 1, whose columns are unit-`2 points4 that lie in an unknown union of low-dimensional linear subspaces ⋃n i=1 Si ⊂ RD, with di := dim(Si). Thus X = [X(1) · · ·X(n)]Γ, where each X(i) := [x(i)1 · · ·x (i) Ni\n] ∈ RD×Ni consists of Ni points spanning subspace Si, and Γ is an unknown permutation, indicating that the clustering of the points is unknown.\nAmong a variety of methods (Vidal et al., 2016) for retrieving the clusters {X(i)}, one may apply Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013), whose main principle is to express each point in X as a sparse linear combination of other points in X . Specifically, we seek an expression, say, of point x(1)1 as a sparse linear combination of all other points X−1 := X \\ {x(1)1 } by means of the basis pursuit problem (Chen et al., 1998)\nmin c∈RN−1\n‖c‖1 s.t. x (1) 1 = X−1c, (14)\nand then form an affinity graph in which we connect x(1)1 to those points of X−1 that correspond to the support (nonzero coefficients) of the computed optimal solution of (14). Clearly, we want these points to lie in the same subspace as x (1) 1 , i.e., to be points of X (1) −1 := X\n(1) \\ {x(1)1 }, in which 4This assumption simplifies the theoretical analysis.\ncase we say that the solution is subspace preserving. When this is true for the expression of each and every point in X , then the corresponding affinity graph contains no connections between points in different subspaces, i.e., it is a subspace preserving graph. Assuming that points within each subspace are sufficiently well connected, the affinity graph will have precisely n connected components, and spectral clustering will be guaranteed to furnish the correct clusters.\nOften, it is more practical to search for approximate sparse linear combinations rather exact ones as in (14). Thus one may approximately express point x(1)1 by solving the Lasso problem (Tibshirani, 2013)\nmin c,e\n‖c‖1 + λ\n2 ‖e‖22 s.t. x (1) 1 = X−1c + e, (15)\nwhere e represents the self-representation error. We have the following known guarantee: Theorem 1 (SSC with uncorrupted data, deterministic (Wang & Xu, 2016)). Recall the notation of Definition 1, and suppose that\nµλ < r and 1/ζ < λ. (16)\nThen every optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving.\nTheorem 1 can be interpreted as follows: If all data points from S1 other than x(1)1 are well distributed (large r), the data points from other subspaces are sufficiently far from S1 as measured by their inner product with the dual direction v̂1,λ (small µλ), and the reconstruction error is penalized sufficiently enough (large λ), then the Lasso problem (15) is guaranteed to furnish non-zero and subspace preserving solutions.\nTheorem 2 is an even more interpretable statement and is derived by bounding in probability the terms in Theorem 1 under the following simplified fully random model. Definition 2 (Random model). For each i ∈ [n], let the ith subspace be chosen uniformly at random from the Grassmannian manifold of d-dimensional subspaces of RD. Moreover, let N/n =: ρd + 1 points5 be chosen uniformly at random from the intersection of each subspace and the unit sphere SD−1. Finally, define the quantities\nα :=\n√ log(ρ)\n16d , β :=\n√ 6 log(N)\nD . (17)\nTheorem 2 (SSC with uncorrupted data, probabilistic (Soltanolkotabi & Candès, 2012; Wang & Xu, 2016)). Consider the random model of Definition 2. If ρ is larger than a universal constant, λ > 1/α, and\nα > β, (18)\n5For simplicity, we assume that n divides N .\nthen any optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving, with probability at least 1− 2/N2 − exp(−√ρd).\nCondition (18) agrees with intuition, since it effectively says that the subspace preserving property is easier to achieve for small relative subspace dimensions d/D, fewer subspaces, and more points per subspace. In §3 we will give analogues of Theorems 1 and 2 for two elementary variants of SSC for incomplete data, described next."
  }, {
    "heading": "2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",
    "text": "When the data are incomplete but otherwise uncorrupted, one may consider using a low-rank matrix completion algorithm to first complete the data and then apply SSC to the completed data. However, this procedure is guaranteed to succeed only when the underlying complete matrix X is of low rank and sufficiently incoherent (Candès & Recht, 2009; Recht, 2011), an assumption which might become invalid in the presence of data from many distinct subspaces. As a simple alternative, one may fill with zeros the unobserved entries to obtain a zero-filled data matrix X̄ exactly as in Definition 1, and subsequently solve the problem\nmin c,e\n‖c‖1 + λ\n2 ‖e‖22 s.t. x̄ (1) 1 = X̄−1c + e, (19)\na procedure called Zero-Filled SSC (ZF-SSC) (Yang et al., 2015). In spite of its simplicity (after all we are just filling in the missing entries with zeros), as per Figs. 2(a) and 2(c) in Yang et al. (2015), ZF-SSC performs only slightly worse than low-rank matrix completion followed by SSC.\nEven so, ZF-SSC has an evident shortcoming: it penalizes the reconstruction error of the zero vector along the unobserved part of the point being expressed, which is clearly an undesirable feature of the method. More precisely, letting Ē(1)1 and Ẽ (1) 1 be, respectively, the observed and unobserved subspaces associated to point x(1)1 , and P̄ (1) 1 , P̃ (1)\n1 the orthogonal projections onto them (see Definition 1), and recalling that (Ē(1)1 )⊥ = Ẽ (1) 1 , we have that\nx̄ (1) 1 = P̄ (1) 1 x̄ (1) 1 , and (20)\nX̄−1 = P̄ (1) 1 X̄−1 + P̃\n(1) 1 X̄−1, (21)\nand so we can rewrite the objective function of ZF-SSC as\n‖c‖1 + λ\n2 ‖x̄(1)1 − X̄−1c‖ 2 2 = ‖c‖1+ (22)\nλ 2 ‖x̄(1)1 − P̄ (1) 1 X̄−1c‖ 2 2 + λ 2 ‖P̃ (1)1 X̄−1c‖ 2 2. (23)\nWe then see that ZF-SSC penalizes the reconstruction error ‖x̄(1)1 − P̄ (1) 1 X̄−1c‖2 of the observed part of x (1) 1 , which\nis desirable, as well as the norm of the vector P̃ (1)\n1 X̄−1c. The latter is an artifact of the zero-filling process, and could bias the coefficients c away from a subspace preserving pattern. Thus, it is reasonable to remove this term and obtain self-expressive coefficients for x̄(1)1 by solving instead\nmin c,e ‖c‖1 +\nλ 2 ‖e‖22, s.t. e = x̄ (1) 1 − ˙̄X−1c, (24)\nwhere ˙̄X := P̄ (1)1 X̄ is the projected and zero-filled data, as in Definition 1. Yang et al. (2015) called this approach EWZF-SSC; here we take the liberty to rename it ProjectedZero-Filled Sparse-Subspace-Clustering (PZF-SSC).\nPZF-SSC is known to provide accurate clustering while tolerating a higher percentage of missing entries than ZF-SSC or even low-rank matrix completion followed by SSC (e.g., see Fig. 2 in Yang et al. (2015)). This is rather fascinating, since, after all, PZF-SSC works with the projected and zero-filled data ˙̄X , which have more missing entries than the zero-filled data X̄ . Because of this reason, direct application of any generic noise bound, such as that of Theorem 6 in Wang & Xu (2016), would naively suggest that ZF-SSC tolerates more missing entries than PZF-SSC, contradicting experimental evidence. This apparent mystery is resolved in §3, where we adopt a more sophisticated view of PZF-SSC, which unveils its advantage over ZF-SSC."
  }, {
    "heading": "3. SSC Theory for Incomplete Data",
    "text": "This section contains the main contributions of this paper. In §3.1-3.2 we give deterministic and probabilistic theorems of correctness for PZF-SSC and ZF-SSC, respectively, in analogy with Theorems 1-2 for SSC with uncorrupted data, while in §3.3 we discuss how the conditions for the two methods compare."
  }, {
    "heading": "3.1. PZF-SSC Theory",
    "text": "As already remarked so far, PZF-SSC is experimentally known to be a superior method to ZF-SSC, i.e., it can provide an accurate clustering for a higher percentage of missing entries. This is remarkable, because the projected and zero-filled data ˙̄X (see Definition 1 for notation) that PZF-SSC operates on contain more missing entries than the zero-filled data X̄ that ZF-SSC operates on. On the other hand, we already saw in §2.2 that the additional zeros in ˙̄X are inflicted in such a way, that the objective function minimized by PZF-SSC is, at least on an intuitive level, more accurate than the one minimized by ZF-SSC.\nIn this paper we give a theoretical justification for the superiority of PZF-SSC over ZF-SSC. Our main insight is the following observation: expressing point x̄(1)1 = ˙̄x (1) 1 as a sparse linear combination of ˙̄X−1, can be seen as ex-\npressing the complete point x̄(1)1 from partial observations ˙̄X−1 of the complete points Ẋ−1, where now the underly-\ning complete data Ẋ lie in the union of subspaces ⋃n i=1 Ṡi, i.e., the original subspaces projected onto the coordinate subspace defined by the observation pattern of the point being expressed (see Definition 1). With this in mind, inspired by the seminal work of Wang & Xu (2016), and by\n1. making more frequent use of strong duality than in the proof of Theorem 6 in Wang & Xu (2016),\n2. using a novel bound for the norm of the dual vector,\n3. and not decoupling the noise from the data,6\nwe arrive at the following key result:\nTheorem 3 (PZF-SSC, deterministic). With the notation of Definition 1, further define the positive quantity\n˙̄λ∗ := 1\n2\n{ 1\n2 ˙̄ζ − ˙̄µλ ˙̄γ ˙̄η +\n√ 9\n4 ˙̄ζ2 +\n˙̄µλ\n˙̄γ ˙̄η ˙̄ζ + 2 ˙̄γ ˙̄η2 + ˙̄µ2λ ˙̄γ2 ˙̄η2\n} . (25)\nThen the interval ˙̄Λ := (1/ ˙̄ζ, ˙̄λ∗) is non-empty, if\n˙̄µλ ˙̄η < ˙̄ζ. (26)\nIf in addition7 λ ∈ ˙̄Λ, then every optimal solution to the Lasso SSC problem (24) with projected and zero-filled data is non-zero and subspace preserving.\nWhat is notable about Theorem 3 is the simplicity of the condition ˙̄µλ ˙̄η < ˙̄ζ, as well as its resemblance to the condition µλ < r of Theorem 1. In fact, the quantity ˙̄µλ is a direct analogue of the inter-subspace coherence µλ, adjusted for the case of PZF data. Indeed, as seen from its definition in (6), ˙̄µλ is the maximum inner product between the dual direction associated to the PZF data of subspace S1 and the PZF data from the remaining subspaces. The quantity ˙̄η ≤ 1 is the Euclidean norm of the point being expressed, which in the absence of missing entries is equal to 1.\nFinally, to understand the quantity ˙̄ζ, we first look at its noiseless counterpart ζ defined in (7). This measures how well distributed are the points X(1)−1 with respect to point x (1) 1 , or in other words, how coherent they are with that\n6By that we mean that we allow our conditions to be stated in terms of the corrupted data as opposed to quantities that depend only on clean data and only on noise. This latter approach, e.g. followed by Wang & Xu (2016), usually leads to less tight conditions due to the heavy use of the triangle inequality. Instead, we do this decoupling in the probability analysis.\n7Since the interval ˙̄Λ is a function of λ, it is misleading to write “for any λ ∈ ˙̄Λ”, as Wang & Xu (2016) do in their Theorem 6: ˙̄Λ being non-empty does not alone guarantee that also λ ∈ ˙̄Λ.\npoint. Notice here that ζ is a more relevant quantity than the inradius r, since the latter does not involve any information about the point being expressed. In addition, ζ is directly computable from the data, while the inradius is in principle hard to compute. Furthermore, it is almost always true that r < ζ, so that if we were to replace condition µλ < r with condition µλ < ζ, we would obtain a better result. This is precisely the condition that Theorem 3 reduces to for complete data, which is a novel result itself:\nTheorem 4 (SSC with uncorrupted data, deterministic). Consider expressing point x(1)1 in terms of the rest of the points in X via the Lasso SSC formulation (15). If µλ < ζ then the open interval Λλ := ( ζ−1, 0.5ζ−1 + 0.5µ−1λ ) is non-empty, and if λ ∈ Λλ, then any optimal solution is non-zero and subspace preserving.\nReturning to the discussion of Theorem 3, we see that the quantity ˙̄ζ captures how well distributed the PZF data ˙̄X(1)−1 are with respect to the point x̄(1)1 that is being expressed, which certainly depends on both how well-distributed the original data X(1)−1 are, as well as on how uniform the observation pattern is. We can now interpret condition (26): the PZF data ˙̄X(1)−1 associated to the same subspace S1 as the point ˙̄x(1)1 being expressed must be well distributed with respect to that point normalized (large ˙̄ζ/ ˙̄η), while the PZF points ˙̄X(−1) in the remaining subspaces must be sufficiently far away from the projected subspace Ṡ1, as measured by their inner product with the corresponding dual direction ˆ̄̇v1,λ ∈ Ṡ1 (small ˙̄µλ). Note here that as the numberm of missing entries increases, the quantity ˙̄η decreases but so does ˙̄ζ; moreover the projection is onto a subspace of even lower dimensionD−m, which makes ˙̄µλ increase, thus overall making it harder for (26) to be satisfied.\nNext, we derive a probabilistic statement from Theorem 3. This is done by constructing high-probability upper and lower bounds for the LHS and RHS of (26), where we exploit the fact that data corruptions due to missing entries are induced by orthogonal projections, i.e., for every x(i)j ,\nx̄ (i) j = P̄ (i) j x (i) j = x (i) j + (−P̃\n(i) j x (i) j ). (27)\nTheorem 5 (PZF-SSC, probabilistic). Consider the random model of Definition 2. Suppose that for each point we do not observe exactly m < D−d entries, with the pattern of missing entries being arbitrary, but otherwise fixed apriori. Suppose that the point density ρ is larger than a universal constant, and let > 0 be a parameter that controls the probability of success. Then there exists a universal constant c, such that if ω := m/D satisfies\nα > √ 2ω + β √ 1− ω + (1 + β) √ + β2/3, (28)\nthen there exists a non-empty interval Λ ⊂ R such that for any λ ∈ Λ, any optimal solution to the PZF-SSC problem (24) is non-zero and subspace preserving, with probability at least 1− 2/N2 − exp(−√ρd)− (2/n) exp(−cD ).\nTo get an insight into how the maximal tolerable level of missing entries scales with the subspace dimension d, we note that for high-ambient dimensions D the quantity β is negligible with respect to the quantity α. Similarly, ignoring the small parameter , (28) becomes approximately α ≥ √ 2ω, which by the definition of α and ω gives\nPZF-SSC : m\nD <\n1\n2\nlog(ρ)\n16d = O\n( 1\nd\n) . (29)\nInformally, (29) says that the maximal tolerable percentage of missing entries of PZF-SSC as predicted by Theorem 5, scales inversely proportionally to the subspace dimension."
  }, {
    "heading": "3.2. ZF-SSC Theory",
    "text": "Similar techniques that led to Theorems 3 and 5 can be employed to yield deterministic and probabilistic statements about ZF-SSC. In particular, we have:\nTheorem 6 (ZF-SSC, deterministic). With the notation of Definition 1, further define the positive quantity\nλ̄∗ := 1\n2\n{ 1\n2ζ̄ − µ̄λ γ̄η̄ − 1 2η̄2 +\n( 9\n4ζ̄2 + µ̄λ γ̄η̄ζ̄ +\n2\nγ̄η̄2 + µ̄2λ γ̄2η̄2 + 1 4η̄4 + 1 η̄2 ( µ̄λ γ̄η̄ − 1 2ζ̄ ))1/2} . (30)\nThen the interval Λ̄ := (1/ζ̄, λ̄∗) is non-empty, if\nµ̄λ η̄ + γ̄ < ζ̄. (31)\nIf in addition λ ∈ Λ̄, then every optimal solution to the Lasso SSC problem (19) with zero-filled data is non-zero and subspace preserving.\nThe quantities µ̄λ, η̄, ζ̄ are in direct analogy with the quantities ˙̄µλ, ˙̄η, ˙̄ζ that appeared in Theorem 3, except that now they are defined in terms of ZF data instead of PZF data. In fact, as seen from their definitions in (10) and (7), η̄ = ˙̄η and ζ̄ = ˙̄ζ, while in principle the inter-subspace coherences µ̄λ, ˙̄µλ need not coincide. Instead, the main difference between (31) and (26) is the appearance of the quantity γ̄, whose PZF counterpart ˙̄γ appears in Theorem 3 only in the definition of the allowable interval for λ.\nThe quantity γ̄ admits an interesting interpretation: As seen from its definition in (12), γ̄ captures the coherence between the ZF data X̄(−1) associated to subspaces Si, i > 1, and a projected version of the unobserved components\nX̃ (1)\n−1 of the data from S1. A large such coherence intuitively means that significant information about S1, potentially crucial for the reconstruction of x̄(1)1 as a linear combination of points in X̄−1, is leaked away into X̃ (1) −1, with which X̄(−1) highly correlates (assuming large γ̄). In turn, this may lead the optimization problem to favor points of X̄\n(−1) in expressing x̄(1)1 , thus leading to the loss of the subspace-preserving property by the solutions to (19).\nInterestingly, comparison of the proofs of Theorems 3 and 6 reveals that ˙̄γ did not appear in (26) because x̄(1)1 is complete when the underlying subspace arrangement is taken to be ⋃n i=1 Ṡi, which is the natural view that we adopted for our analysis of PZF-SSC. On the contrary, such a feature is not available in the analysis of ZF-SSC, as x̄(1)1 is in principle incomplete with respect to ⋃n i=1 Si.\nAs we did for PZF-SSC, we use the deterministic Theorem 6 to derive a probabilistic statement: Theorem 7 (ZF-SSC, probabilistic). Consider the exact setting of Theorem 5. If ω := m/D satisfies α >( √ 2 + √ + β2/3) √ ω + (β + √ + β2/3)\n√ 1− ω+√\nω(1− ω) + (1 + β + √ + β2/3) √ + β2/3,(32)\nthen there exists a non-empty interval Λ ⊂ R such that for any λ ∈ Λ, any optimal solution to the ZF-SSC problem (19) is non-zero and subspace preserving, with probability at least 1−2/N2−exp(−√ρd)−2(1+1/n) exp ( −cD ) .\nRepeating the informal arguments that led to (29), i.e., for high ambient dimension D ignoring β and , (32) becomes α > √ 2ω + √ ω(1− ω). Since √ ω ≥ √ ω(1− ω), we then have that this latter simplified condition is satisfied if the stronger condition α > (1 + √ 2) √ ω is true. This gives\nZF-SSC : m\nD <\n1\n(1 + √ 2)2 log(ρ) 16d = O\n( 1\nd\n) , (33)\ni.e., ZF-SSC can tolerate 1/d fraction of missing entries.8"
  }, {
    "heading": "3.3. A Comparison between PZF-SSC and ZF-SSC",
    "text": "As per (29) and (33), both PZF-SSC and ZF-SSC give subspace preserving solutions as long as the ratio of missing entries scales as 1/d. On the other hand, the multiplying constant associated to PZF-SSC is about 3 times larger, suggesting a superiority of PZF-SSC. Alternatively, with\nfPZF(ω) :=α− √ 2ω−β √ 1−ω−(1+β) √ +β2/3, (34)\nthe PZF Theorem 5 asks that\nfPZF(ω) > 0, (35)\n8This result is in agreement with the result of Charles et al. (2018), who studied only ZF-SSC.\nwhile the ZF Theorem 7 asks that fZF(ω) :=− √ + β2/3( √ ω + √ 1− ω + √ + β2/3)\n− √ ω(1− ω) + fPZF(ω) > 0, (36)\na significantly harder condition to satisfy than (35), due to the dominating negative term − √ ω(1− ω). Once again, this suggests that PZF-SSC has an advantage over ZF-SSC.\nThe actual algorithmic behavior is depicted in Fig. 1. In Figs. 1(a)-1(b) we plot the subspace preserving accuracies for PZF-SSC and ZF-SSC, defined as the ratios of the `1- norm of the N ×N self-representation matrices CPZF and CZF restricted to intra-subspace connections over the total `1 norm of CPZF and CZF respectively. This quantity measures the degree to which points within a subspace use only points from the same subspace for their representation. In Figs. 1(c)-1(d) we show the clustering accuracy that corresponds to spectral clustering applied on the affinity graphs defined by CPZF and CZF.\nThere are at least four notable observations. First, as seen in Figs. 1(a)-1(b), the phase transition between subspace preserving solutions and non-subspace preserving ones is\nindeed of hyperbolic nature for both methods, as theoretically predicted by (29) and (33). Second, PZF-SSC has indeed higher subspace preserving accuracy than ZF-SCC, as suggested by our two theoretical arguments in the beginning of this section: For example, for 5-dimensional subspaces (d = 5) in R100 PZF-SSC can tolerate up to 34 missing entries per point, while ZF-SSC can tolerate up to 19. Third, both methods start breaking down rather quickly as the number of missing entries increases: for d = 10 PZFSSC and ZF-SSC can tolerate, respectively, at most 23 and 10 missing entries per point before their solutions become non-subspace preserving, while for d = 20 the maximal tolerable number of missing entries becomes 10 and 4, respectively. Notice how close the values for ZF-SSC are to D/d in each of the above cases. Finally, even though the quality of the connections degrades quickly as d and ω increase, the clustering accuracy remains very high (close to 1) for both methods, a phenomenon that we attribute to the robustness of spectral clustering (Figs. 1(c)-1(d))."
  }, {
    "heading": "4. Discussion",
    "text": "Bounding dual vectors and inradius. A feature of our theory is that the subspace separation conditions for complete, ZF and PZF data have the same geometric form, i.e.,\nµλ < ζ, µ̄λη̄ + γ̄ < ζ̄, and ˙̄µλ ˙̄η < ˙̄ζ (37)\nrespectively. This nice structure comes from a novel bound on the norm of the so-called dual vector v that takes into consideration both the objective function as well as the constraint of the reduced dual problem (2). Instead, Soltanolkotabi & Candès (2012) bound v exclusively from the constraint of (2). The two techniques lead to a trade-off between tightness of subspace separation conditions and upper bounds for the Lasso parameter λ9 and it is an open problem to optimally bound v, which is then expected to\n9This is more easily seen by comparing the conditions of Theorems 1 and 4 for complete data.\nlead to jointly better conditions. At any case, the probabilistic lower bound on r < ζ that we also have used in our analysis is the quantity α = √ log(ρ)/16d (AlonsoGutierrez, 2008), which even though of fundamental theoretical importance, is too pessimistic: for ρ = 5 and d = 5 eq. (26) predicts at most 1 tolerable missing entry for PZFSSC in R100, while as per Fig. 1(a) the method handles 34 missing entries per point. Can we do better than that?\nPZF vs. ZF. As argued theoretically by comparing Theorems 5 and 7, and corroborated experimentally by Fig. 1 (§3.3), projecting the incomplete dataset onto the observation pattern of the point being expressed increases the robustness of the self-representation of the dataset to missing entries with respect to the subspace preserving property, at least for low-dimensional subspaces. Our study was solely in the context of SSC, yet we believe that working with PZF data instead of ZF data is advantageous regardless of the choice of self-expressive method (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016); a conjecture to be established.\nBeyond PZF-SSC. Even though the clustering accuracy for PZF-SSC seems rather satisfactory as depicted for higher subspace dimensions and higher missing rates in Fig. 2(a), its rather poor subspace preserving accuracy shown in 2(b), suggests that PZF-SSC is still too simple a method to handle the subspace clustering problem for incomplete data, and that its performance relies to a significant extent on the robustness of spectral clustering. E.g., as per Figs. 2(a)-2(b), for three 60-dimensional subspaces inside R100 and 15 missing entries per point, about 40% of the points a point connects to live in different subspaces; yet the clustering accuracy is 99%. On the other hand, the more sophisticated approach of Elhamifar (2016) builds on the SSC formulation and allows for both clustering and completion in a unified framework. Nevertheless, that approach comes with no theoretical guarantees and appears to be computationally burdensome, leaving as an open challenge the proposal of a theoretically sound, efficient and accurate algorithm for clustering incomplete data associated to a union of low-dimensional subspaces."
  }, {
    "heading": "Acknowledgements",
    "text": "Work funded by ShanghaiTech University and NSF grants 1447822 and 1618637. The first author thanks Yunzhen Yao for proof-reading the longer version of this manuscript and catching some mistakes, as well as for producing the experiments. He thanks Dr. Chun-Guang Li for insightful comments on an earlier version of this manuscript, Dr. Gregory Ongie for comments on the current manuscript, and Ron Boger for interesting conversations on missing entries and for sharing his code. The authors thank all four anonymous reviewers for their constructive comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Similarity matrix framework for data from union of subspaces",
    "authors": ["A. Aldroubi", "A. Sekmen", "A.B. Koku", "A.F. Cakmak"],
    "venue": "Applied and Computational Harmonic Analysis,",
    "year": 2017
  }, {
    "title": "On the isotropy constant of random convex sets",
    "authors": ["D. Alonso-Gutierrez"],
    "venue": "Proceedings of the American Mathematical Society,",
    "year": 2008
  }, {
    "title": "High-dimensional matched subspace detection when data are missing",
    "authors": ["L. Balzano", "B. Recht", "R. Nowak"],
    "venue": "In Information Theory Proceedings (ISIT),",
    "year": 2010
  }, {
    "title": "Exact matrix completion via convex optimization",
    "authors": ["E. Candès", "B. Recht"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2009
  }, {
    "title": "Subspace clustering with missing and corrupted data",
    "authors": ["Z. Charles", "A. Jalali", "R. Willet"],
    "year": 2018
  }, {
    "title": "Spectral curvature clustering (SCC)",
    "authors": ["G. Chen", "G. Lerman"],
    "venue": "International Journal of Computer Vision,",
    "year": 2009
  }, {
    "title": "Atomic decomposition by basis pursuit",
    "authors": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"],
    "venue": "SIAM J. Sci. Comput.,",
    "year": 1998
  }, {
    "title": "High-rank matrix completion and clustering under self-expressive models",
    "authors": ["E. Elhamifar"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Sparse subspace clustering",
    "authors": ["E. Elhamifar", "R. Vidal"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2009
  }, {
    "title": "Sparse subspace clustering: Algorithm, theory, and applications",
    "authors": ["E. Elhamifar", "R. Vidal"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2013
  }, {
    "title": "High-rank matrix completion",
    "authors": ["B. Eriksson", "L. Balzano", "R. Nowak"],
    "venue": "Journal of Machine Learning Research, Proceedings Track 22:373–381,",
    "year": 2012
  }, {
    "title": "RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
    "authors": ["M.A. Fischler", "R.C. Bolles"],
    "venue": "Communications of the ACM,",
    "year": 1981
  }, {
    "title": "Robust subspace clustering via thresholding",
    "authors": ["R. Heckel", "H. Bölcskei"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Robust recovery of subspace structures by low-rank representation",
    "authors": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Ma"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2013
  }, {
    "title": "Robust and efficient subspace segmentation via least squares regression",
    "authors": ["Lu", "C-Y", "H. Min", "Zhao", "Z-Q", "L. Zhu", "Huang", "D-S", "S. Yan"],
    "venue": "In European Conference on Computer Vision, pp",
    "year": 2012
  }, {
    "title": "Algebraic variety models for high-rank matrix completion",
    "authors": ["G. Ongie", "R. Willett", "R.D. Nowak", "L. Balzano"],
    "venue": "In 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Deep subspace clustering with sparsity prior",
    "authors": ["X. Peng", "S. Xiao", "J. Feng", "Yau", "W.-Y", "Z. Yi"],
    "venue": "In 25th International Joint Conference on Artificial Intelligence (IJCAI),",
    "year": 2016
  }, {
    "title": "Deterministic conditions for subspace identifiability from incomplete sampling",
    "authors": ["D. Pimentel-Alarcon", "N. Boston", "R.D. Nowak"],
    "venue": "Information Theory (ISIT),",
    "year": 2015
  }, {
    "title": "The information-theoretic requirements of subspace clustering with missing data",
    "authors": ["D.L. Pimentel-Alarcon", "R.D. Nowak"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "A simpler approach to matrix completion",
    "authors": ["Recht", "Benjamin"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "A geometric analysis of subspace clustering with outliers",
    "authors": ["M. Soltanolkotabi", "E.J. Candès"],
    "venue": "Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Robust subspace clustering",
    "authors": ["M. Soltanolkotabi", "E. Elhamifar", "E.J. Candès"],
    "venue": "Annals of Statistics,",
    "year": 2014
  }, {
    "title": "The lasso problem and uniqueness",
    "authors": ["R. Tibshirani"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2013
  }, {
    "title": "Hyperplane clustering via dual principal component pursuit",
    "authors": ["M.C. Tsakiris", "R. Vidal"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Filtrated algebraic subspace clustering",
    "authors": ["M.C. Tsakiris", "R. Vidal"],
    "venue": "SIAM Journal on Imaging Sciences,",
    "year": 2017
  }, {
    "title": "Algebraic clustering of affine subspaces",
    "authors": ["M.C. Tsakiris", "R. Vidal"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2018
  }, {
    "title": "Theoretical analysis of sparse subspace clustering with missing entries. arXiv:1801.00393v3 [cs.LG], 2018b",
    "authors": ["M.C. Tsakiris", "R. Vidal"],
    "year": 2018
  }, {
    "title": "Generalized Principal Component Analysis (GPCA)",
    "authors": ["R. Vidal", "Y. Ma", "S. Sastry"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2005
  }, {
    "title": "Generalized Principal Component Analysis",
    "authors": ["R. Vidal", "Y. Ma", "S. Sastry"],
    "year": 2016
  }, {
    "title": "On deterministic conditions for subspace clustering under missing data",
    "authors": ["W. Wang", "S. Aeron", "V. Aggarwal"],
    "venue": "In International Symposium on Information Theory,",
    "year": 2016
  }, {
    "title": "Noisy sparse subspace clustering",
    "authors": ["Wang", "Y.-X", "H. Xu"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Provable subspace clustering: When LRR meets SSC",
    "authors": ["Wang", "Y-X", "H. Xu", "C. Leng"],
    "venue": "In Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Sparse subspace clustering with missing entries",
    "authors": ["C. Yang", "D. Robinson", "R. Vidal"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Geometric conditions for subspacesparse recovery",
    "authors": ["C. You", "R. Vidal"],
    "venue": "In International Conference on Machine learning,",
    "year": 2015
  }, {
    "title": "Oracle based active set algorithm for scalable elastic net subspace clustering",
    "authors": ["C. You", "Li", "C.-G", "D. Robinson", "R. Vidal"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }],
  "id": "SP:e796935db682a7eec15750473f5c6324826bd433",
  "authors": [{
    "name": "Manolis C. Tsakiris",
    "affiliations": []
  }, {
    "name": "René Vidal",
    "affiliations": []
  }],
  "abstractText": "Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subspaces; a problem with numerous applications in pattern recognition and computer vision. Even though the behavior of SSC for complete data is by now wellunderstood, little is known about its theoretical properties when applied to data with missing entries. In this paper we give theoretical guarantees for SSC with incomplete data, and provide theoretical evidence that projecting the zero-filled data onto the observation pattern of the point being expressed can lead to substantial improvement in performance; a phenomenon already known experimentally. The main insight of our analysis is that even though this projection induces additional missing entries, this is counterbalanced by the fact that the projected and zerofilled data are in effect incomplete points associated with the union of the corresponding projected subspaces, with respect to which the point being expressed is complete. The significance of this phenomenon potentially extends to the entire class of self-expressive methods.",
  "title": "Theoretical Analysis of Sparse Subspace Clustering with Missing Entries"
}