{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Most of the world’s languages are dying out and have little recorded data or linguistic documentation (Austin and Sallabank, 2011). It is important to adequately document languages while they are alive so that they may be investigated in the future. Language documentation traditionally involves one-onone elicitation of speech from native speakers in order to produce lexicons and grammars that describe the language. However, this does not scale: linguists must first transcribe the speech phonemically as most of these languages have no standardized orthography. This is a critical bottleneck since it takes a trained linguist about 1 hour to transcribe the phonemes of 1 minute of speech (Do et al., 2014).\nSmartphone apps for rapid collection of bilingual data have been increasingly investigated (De Vries et al., 2011; De Vries et al., 2014; Reiman, 2010; Bird et al., 2014; Blachon et al., 2016). It is common for these apps to collect speech segments paired with spoken translations in another language, making spoken translations quicker to obtain than phonemic transcriptions.\nWe present a method to improve automatic phoneme transcription by harnessing such bilingual data to learn a lexicon and translation model directly from source phoneme lattices and their written target translations, assuming that the target side is a major language that can be efficiently transcribed.1 A Bayesian non-parametric model expressed with a weighted finite-state transducer (WFST) framework represents the joint distribution of source acoustic features, phonemes and latent source words given the target words. Sampling of alignments is used to learn source words and their target translations, which are then used to improve transcription of the source audio they were learnt from. Importantly, the model assumes no prior lexicon or translation model.\nThis method builds on work on phoneme translation modeling (Besacier et al., 2006; Stüker et al., 2009; Stahlberg et al., 2012; Stahlberg et al., 2014; Adams et al., 2015; Duong et al., 2016), speech translation (Casacuberta et al., 2004; Matusov et al., 2005), computer-aided translation, (Brown et al., 1994; Vidal et al., 2006; Khadivi and Ney, 2008; Reddy and Rose, 2010; Pelemans et al., 2015), translation modeling from automatically transcribed\n1Code is available at https://github.com/oadams/latticetm.\n2377\nspeech (Paulik and Waibel, 2013), word segmentation and translation modeling (Chang et al., 2008; Dyer, 2009; Nguyen et al., 2010; Chen and Xu, 2015), Bayesian word alignment (Mermer et al., 2013; Zezhong et al., 2013) and language model learning from lattices (Neubig et al., 2012). While we previously explored learning a translation model from word lattices (Adams et al., 2016), in this paper we extend the model to perform unsupervised word segmentation over phoneme lattices in order to improve phoneme recognition.\nExperiments demonstrate that our method significantly reduces the phoneme error rate (PER) of transcriptions compared with a baseline recogniser and a similar model that harnesses only monolingual information, by up to 17% and 5% respectively. We also find that the model learns meaningful bilingual lexical items."
  }, {
    "heading": "2 Model description",
    "text": "Our model extends the standard automatic speech recognition (ASR) problem by seeking the best phoneme transcription φ̂ of an utterance in a joint probability distribution that incorporates acoustic features x, phonemes φ, latent source words f and observed target transcriptions e:\nφ̂ = argmax φ,f\nP (x|φ)P (φ|f)P (f |e) , (1)\nassuming a Markov chain of conditional independence relationships (bold symbols denote utterances as opposed to tokens). Deviating from standard ASR, we replace language model probabilities with those of a translation model, and search for phonemes instead of words. Also, no lexicon or translation model are given in training."
  }, {
    "heading": "2.1 Expression of the distribution using finite-state transducers",
    "text": "We use a WFST framework to express the factors of (1) since it offers computational tractability and simple inference in a clear, modular framework. Figure 1 uses a toy German–English error resolution example to illustrate the components of the framework: a phoneme lattice representing phoneme uncertainty according to P (x|φ); a lexicon that transduces phoneme substrings φs of φ to source tokens f according to P (φs|f); and a lexical translation\nmodel representing P (f |e) for each e in the written translation. The composition of these components is also shown at the bottom of Figure 1, illustrating how would-be transcription errors can be resolved. This framework is reminiscent of the WFST framework used by Neubig et al. (2012) for lexicon and language model learning from monolingual data."
  }, {
    "heading": "2.2 Learning the lexicon and translation model",
    "text": "Because we do not have knowledge of the source language, we must learn the lexicon and translation model from the phoneme lattices and their written translation. We model lexical translation probabilities using a Dirichlet process. Let A be both the transcription of each source utterance f and its word alignments to the translation e that generated them. The conditional posterior can be expressed as:\nP (f |e;A) = cA(f, e) + αP0(f) cA(e) + α , (2)\nwhere cA(f, e) is a count of how many times f has aligned to e in A and cA(e) is a count of how many times e has been aligned to; P0 is a base distribution that influences how phonemes are clustered; and α determines the emphasis on the base distribution.\nIn order to express the Dirichlet process using the WFST components, we take the union of the lexicon with a spelling model base distribution that consumes phonemes φi . . . φj and produces a special 〈unk〉 token with probability P0(φi . . . φj). This 〈unk〉 token is consumed by a designated arc in the translation model WFST with probability αcA(e)+α , yielding a composed probability of αP0(f)cA(e)+α . Other arcs in the translation model express the probability cA(f,e) cA(e)+α\nof entries already in the lexicon. The sum of these two probabilities equates to (2).\nAs for the spelling model P0, we consider three distributions and implement WFSTs to represent them: a geometric distribution, Geometric(γ), a Poisson distribution, Poisson(λ),2 and a ‘shifted’ geometric distribution, Shifted(α, γ). The shifted geometric distribution mitigates a shortcoming of the geometric distribution whereby words of length 1 have the highest probability. It does so by having\n2While the geometric distribution can be expressed recursively, we cap the number of states in the Poisson WFST to 100.\nanother parameter α that specifies the probability of a word of length 1, with the remaining probability mass distributed geometrically. All phonemes types are treated the same in these distributions, with uniform probability."
  }, {
    "heading": "2.3 Inference",
    "text": "In order to determine the translation model parameters as described above, we require the alignments A. We sample these proportionally to their probability given the data and our prior, in effect integrating over all parameter configurations T :\nP (A|X ;α, P0) = ∫\nT P (A|X , T )P (T ;α, P0)dT ,\n(3) where X is our dataset of source phoneme lattices paired with target sentences.\nThis is achieved using blocked Gibbs sampling, with each utterance constituting one block. To sample from WFSTs, we use forwardfiltering/backward-sampling (Scott, 2002; Neubig et al., 2012), creating forward probabilities using the forward algorithm for hidden Markov models before backward-sampling edges proportionally to the product of the forward probability and the edge weight.3\n3No Metropolis-Hastings rejection step was used."
  }, {
    "heading": "3 Experimental evaluation",
    "text": "We evaluate the lexicon and translation model by their ability to improve phoneme recognition, measuring phoneme error rate (PER)."
  }, {
    "heading": "3.1 Experimental setup",
    "text": "We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011).\nWe run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1- best path uninformed by the model (ASR); a monolingual version of our model that is identical except without conditioning on the target side (Mono); and the model applied using the source language sentence as the target (Oracle).\nWe tuned on the first 1,000 utterences (about 1 hour) of speech and trained on up to 9 hours of the\nremaining data.4 Only the oracle setup was used for tuning, with Geometric(0.01) (taking the form of a vague prior), Shifted(10−5, 0.25) and Poisson(7) performing best."
  }, {
    "heading": "3.2 Results and Discussion",
    "text": "Table 1 shows en–ja and ja–en results for all methods with the full training data. Figure 2 shows improvements of ja–en over both the ASR baseline and the Mono method as the training data increases, with translation modeling gaining an increasing advantage with more training data.\nNotably, English recognition gains less from using Japanese as the target side (en–ja) than the other way around, while the ‘oracle’ approach for Japanese recognition, which also uses Japanese as the target, underperforms ja–en. These observations suggest that using the Japanese target is less helpful, likely explained by the fine-grained morphological segmentation we used, making it harder for the model to relate source phonemes to target tokens.\nThe vague geometric prior significantly underperforms the other priors. In the en–ja/vague case, the\n4A 1 hour subset was used for PER evaluation.\nmodel actually underperforms its monolingual counterpart. The vague prior biases slightly towards finegrained English source segmentation, with words of length 1 most common. In this case, fine-grained Japanese is also used as the target which results in most lexical entries arising from uninformative alignments between single English phonemes and Japanese syllables, such as [t]⇔す. For similar reasons, the shifted geometric prior gains an advantage over Poisson, likely because of its ability to even further penalize single-phoneme lexical items, which regularly end up in all lexicons anyway due to their combinatorical advantage when sampling.\nWhile many bilingual lexical entries are correct, such as [w2n]⇔一 (‘one’), most are not. Some have segmentation errors [li:z]⇔くださ (‘please’); some are correctly segmented but misaligned to commonly co-occurring words [w2t]⇔時 (‘what’ aligned to ‘time’); others do not constitute individual words, but morphemes aligned to common Japanese syllables [i:N]⇔く (‘-ing’); others still align multi-word units correctly [haUm2tS]⇔いく ら (‘how much’). Note though that entries such as those listed above capture information that may nevertheless help to reduce phoneme transcription errors."
  }, {
    "heading": "4 Conclusion and Future Work",
    "text": "We have demonstrated that a translation model and lexicon can be learnt directly from phoneme lattices in order to improve phoneme transcription of those very lattices.\nOne of the appealing aspects of this modular framework is that there is much room for extension and improvement. For example, by using adaptor grammars to encourage syllable segmentation (Johnson, 2008), or incorporating language model probabilities in addition to our translation model probabilities (Neubig et al., 2012).\nWe assume a good acoustic model with phoneme error rates between 20 and 25%. In a language documentation scenario, acoustic models for the lowresource source language won’t exist. Future work should use a universal phoneme recognizer or acoustic model of a similar language, thus making a step towards true generalizability."
  }, {
    "heading": "Acknowledgments",
    "text": "We gratefully acknowledge support from the DARPA LORELEI program."
  }],
  "year": 2016,
  "references": [{
    "title": "Inducing bilingual lexicons from small quantities of sentence-aligned phonemic transcriptions",
    "authors": ["Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird."],
    "venue": "Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2015), Da",
    "year": 2015
  }, {
    "title": "Learning a translation model from word lattices",
    "authors": ["Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird."],
    "venue": "17th Annual Conference of the International Speech Communication Association (INTERSPEECH 2016), San Francisco, California, USA.",
    "year": 2016
  }, {
    "title": "The Cambridge Handbook of Endangered Languages",
    "authors": ["Peter Austin", "Julia Sallabank."],
    "venue": "Cambridge Handbooks in Language and Linguistics. Cambridge University Press.",
    "year": 2011
  }, {
    "title": "Towards speech translation of non written languages",
    "authors": ["Laurent Besacier", "Bowen Zhou", "Yuqing Gao."],
    "venue": "2006 IEEE Spoken Language Technology Workshop (SLT 2006), pages 222–225, Palm Beach, Aruba.",
    "year": 2006
  }, {
    "title": "Aikuma: A mobile app for collaborative language documentation",
    "authors": ["Steven Bird", "Florian R Hanke", "Oliver Adams", "Haejoong Lee."],
    "venue": "Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 1–5,",
    "year": 2014
  }, {
    "title": "Parallel speech collection for underresourced language studies using the lig-aikuma mobile device app",
    "authors": ["David Blachon", "Elodie Gauthier", "Laurent Besacier", "GuyNoël Kouarata", "Martine Adda-Decker", "Annie Rialland."],
    "venue": "Procedia Computer Science, 81:61–",
    "year": 2016
  }, {
    "title": "Automatic speech recognition in machine-aided translation",
    "authors": ["Peter F Brown", "Stanley F Chen", "Stephen A Della Pietra", "Vincent J Della Pietra", "Andrew S Kehler", "Robert L Mercer."],
    "venue": "Computer Speech & Language, 8(3):177–187.",
    "year": 1994
  }, {
    "title": "Optimizing Chinese word segmentation for machine translation performance",
    "authors": ["Pi-Chuan Chang", "Michel Galley", "Christopher D Manning."],
    "venue": "Proceedings",
    "year": 2008
  }, {
    "title": "Semi-supervised Chinese word segmentation based on bilingual information",
    "authors": ["Wei Chen", "Bo Xu."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), pages 1207–1216, Lisbon, Portugal.",
    "year": 2015
  }, {
    "title": "Woefzela - an open-source platform for ASR data collection in the developing world",
    "authors": ["Nic J De Vries", "Jaco Badenhorst", "Marelie H Davel", "Etienne Barnard", "Alta De Waal."],
    "venue": "12th Annual Conference of the International Speech Communication Association",
    "year": 2011
  }, {
    "title": "A smartphone-based ASR data collection tool for under-resourced languages",
    "authors": ["Nic J De Vries", "Marelie H Davel", "Jaco Badenhorst", "Willem D Basson", "Febe De Wet", "Etienne Barnard", "Alta De Waal."],
    "venue": "Speech Communication, 56:119–131.",
    "year": 2014
  }, {
    "title": "Towards the automatic processing of Yongning Na (Sino-Tibetan): developing a ‘light’ acoustic model of the target language and testing ‘heavyweight’ models from five national languages",
    "authors": ["Thi-Ngoc-Diep Do", "Alexis Michaud", "Eric Castelli."],
    "venue": "4th Inter-",
    "year": 2014
  }, {
    "title": "An attentional model for speech translation without transcription",
    "authors": ["Long Duong", "Antonios Anastasopoulos", "David Chiang", "Steven Bird", "Trevor Cohn."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-",
    "year": 2016
  }, {
    "title": "Using a maximum entropy model to build segmentation lattices for MT",
    "authors": ["Chris Dyer."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT",
    "year": 2009
  }, {
    "title": "The NAIST ASR system for IWSLT 2015",
    "authors": ["M Heck", "Q T Do", "S Sakti", "G Neubig", "T Toda", "S Nakamura."],
    "venue": "Proceedings of the International Workshop on Spoken Language Translation (IWSLT 2015), Da Nang, Vietnam.",
    "year": 2015
  }, {
    "title": "Unsupervised word segmentation for Sesotho using adaptor grammars",
    "authors": ["Mark Johnson."],
    "venue": "Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology (SIGMORPHON 2008), pages 20–27, Columbus,",
    "year": 2008
  }, {
    "title": "Integration of speech recognition and machine translation in computer-assisted translation",
    "authors": ["Shahram Khadivi", "Hermann Ney."],
    "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 16(8):1551–1564.",
    "year": 2008
  }, {
    "title": "On the integration of speech recognition and statistical machine translation",
    "authors": ["Evgeny Matusov", "Stephan Kanthak", "Hermann Ney."],
    "venue": "6th Interspeech 2005 and 9th European Conference on Speech Communication and Technology (INTERSPEECH 2005),",
    "year": 2005
  }, {
    "title": "Improving statistical machine translation using Bayesian word alignment and Gibbs sampling",
    "authors": ["Coskun Mermer", "Murat Saraçlar", "Ruhi Sarikaya."],
    "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 21(5):1090–1101.",
    "year": 2013
  }, {
    "title": "Pointwise prediction for robust, adaptable Japanese morphological analysis",
    "authors": ["Graham Neubig", "Yosuke Nakata", "Shinsuke Mori."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-",
    "year": 2011
  }, {
    "title": "Bayesian learning of a language model from continuous speech",
    "authors": ["Graham Neubig", "Masato Mimura", "Tatsuya Kawahara."],
    "venue": "IEICE TRANSACTIONS on Information and Systems, 95(2):614–625.",
    "year": 2012
  }, {
    "title": "Nonparametric word segmentation for machine translation",
    "authors": ["ThuyLinh Nguyen", "Stephan Vogel", "Noah A Smith."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 815–823, Beijing, China.",
    "year": 2010
  }, {
    "title": "Training speech translation from audio recordings of interpretermediated communication",
    "authors": ["Matthias Paulik", "Alex Waibel."],
    "venue": "Computer Speech & Language, 27(2):455–474.",
    "year": 2013
  }, {
    "title": "Efficient language model adaptation for automatic speech recognition of spoken translations",
    "authors": ["Joris Pelemans", "Tom Vanallemeersch", "Kris Demuynck", "Patrick Wambacq", "Others."],
    "venue": "16th Annual Conference of the International Speech Communication As-",
    "year": 2015
  }, {
    "title": "Integration of statistical models for dictation of document translations in a machine-aided human translation task",
    "authors": ["Aarthi Reddy", "Richard C Rose."],
    "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 18(8):2015–2027.",
    "year": 2010
  }, {
    "title": "Basic oral language documentation",
    "authors": ["D Will Reiman."],
    "venue": "Language Documentation & Conservation, pages 254–268.",
    "year": 2010
  }, {
    "title": "Bayesian methods for hidden Markov models",
    "authors": ["Steven L Scott."],
    "venue": "Journal of the American Statistical Association, pages 337–351.",
    "year": 2002
  }, {
    "title": "Word segmentation through crosslingual word-to-phoneme alignment",
    "authors": ["Felix Stahlberg", "Tim Schlippe", "Sue Vogel", "Tanja Schultz."],
    "venue": "2012 IEEE Workshop on Spoken Language Technology (SLT 2012), pages 85–90, Miami, Florida, USA.",
    "year": 2012
  }, {
    "title": "Word segmentation and pronunciation extraction from phoneme sequences through cross-lingual word-to-phoneme alignment",
    "authors": ["Felix Stahlberg", "Tim Schlippe", "Stephan Vogel", "Tanja Schultz."],
    "venue": "Computer Speech & Language, pages 234–261.",
    "year": 2014
  }, {
    "title": "Human translations guided language discovery for ASR systems",
    "authors": ["Sebastian Stüker", "Laurent Besacier", "Alex Waibel."],
    "venue": "10th Annual Conference of the International Speech Communication Association (INTERSPEECH 2009), pages 3023–3026, Brighton,",
    "year": 2009
  }, {
    "title": "Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world",
    "authors": ["Toshiyuki Takezawa", "Eiichiro Sumita", "Fumiaki Sugaya", "Hirofumi Yamamoto", "Seiichi Yamamoto."],
    "venue": "Third International Conference on Language Re-",
    "year": 2002
  }, {
    "title": "Computer-assisted translation using speech recognition",
    "authors": ["Enrique Vidal", "Francisco Casacuberta", "Luis Rodriguez", "Jorge Civera", "Carlos D Martı́nez Hinarejos"],
    "year": 2006
  }, {
    "title": "Bayesian word alignment and phrase table training for statistical machine translation",
    "authors": ["L I Zezhong", "Hideto Ikeda", "Junichi Fukumoto."],
    "venue": "IEICE TRANSACTIONS on Information and Systems, 96(7):1536–1543. 2382",
    "year": 2013
  }],
  "id": "SP:a5370ccc6df6c29c45d991c6d56287fb5b799003",
  "authors": [{
    "name": "Oliver Adams",
    "affiliations": []
  }, {
    "name": "Graham Neubig",
    "affiliations": []
  }, {
    "name": "Trevor Cohn",
    "affiliations": []
  }, {
    "name": "Steven Bird",
    "affiliations": []
  }, {
    "name": "Quoc Truong Do",
    "affiliations": []
  }, {
    "name": "Satoshi Nakamura",
    "affiliations": []
  }],
  "abstractText": "Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phonemic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model’s ability to learn useful bilingual lexical entries.",
  "title": "Learning a Lexicon and Translation Model from Phoneme Lattices"
}