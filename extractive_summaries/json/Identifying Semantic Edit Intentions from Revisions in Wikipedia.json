{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2000–2010 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Many online text production communities, including Wikipedia, maintain a history of revisions made by millions of participants. As Wikipedia statistics as of January 2017 show, English Wikipedia has 5.3 million articles with an average of 162.89 revisions per article, with revisions growing at a rate of about 2 revisions per second. This provides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Ya-\nmangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010).\nTo avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Yang et al., 2016). For instance, Daxenberger and Gurevych (2012) categorized edits based on whether edits affect the text meaning, resulting in syntactic edit categories such as file deletion, reference modification, etc. However, simply understanding the syntactic revision operation types does not provide the information we seek: why do editors do what they do? how effective are their actions? For example, syntactic edit type taxonomies cannot tell the difference between simplifying a paragraph and maliciously damaging that paragraph, since both involve deleting a sentence.\nIn this work, we focus explicitly on revision intention. We introduce a fine-grained taxonomy of the reasons why an author in Wikipedia made an edit. Example edit intentions include copy editing, elaboration, verification, and simplification. Compared to taxonomies that either focus on low-level syntactic operations (Faigley and Witte, 1981) or that mix syntactic and semantic classes (Daxenberger and Gurevych, 2013), a clean higher-level semantic categorization enables us to easily identify textual meaning changes, and to connect revisions to “what happens in the mind of the revising author during the revision”\n2000\n(Fitzgerald, 1987; Daxenberger, 2016). In order to capture the meaning behind edits, we worked with 13 Wikipedians to build a taxonomy that captured the meaning of an revision, which we term edit intention, and hand-labeled a corpus of 7,177 revisions with their edit intentions. We then developed an automated method to identify these edit intentions from differences between revisions of Wikipedia articles. To explore the utility of this taxonomy, we applied this model to better understand two important issues for Wikipedia: new editor retention and article quality. Specifically, we examined whether edit intentions in newcomers’ first editing sessions predict their retention, and examined how edits with different intentions lead to changes in article quality. These analyses showed that specific types of editing work were positively correlated with newcomer survival and articles in different stages of development benefited differently from different types of edits."
  }, {
    "heading": "2 Related Work",
    "text": "Wikipedia revision histories have been used for a wide range of NLP tasks (Yamangil and Nelken, 2008; Aji et al., 2010; Zanzotto and Pennacchiotti, 2010; Ganter and Strube, 2009; Nelken and Yamangil, 2008). For instance, Yatskar et al. (2010) used Wikipedia comments associated with revisions to collect relevant edits for sentence simplification. Max and Wisniewski (2010) constructed a corpus of rewritings that can be used for spelling errors and paraphrases (Zesch, 2012). Similarly, Zanzotto and Pennacchiotti (2010) used edits as training data for textual entailment recognition, and Recasens et al. (2013) analyzed real instances of human edits designed to remove bias from Wikipedia articles. Most of these work employed manually defined rules or filters to collect relevant edits to the NLP task at hand.\nTowards analyzing revisions and developing unified revision taxonomies (Bronner and Monz, 2012; Liu and Ram, 2011), Fong and Biuk-Aghai (2010) built machine learning models to distinguish between factual and fluency edits in revision histories. Faigley and Witte (1981) made a distinction between changes that affect meaning, called text-base changes and changes which do not affect meaning, called surface changes. The two categories are further divided into formal changes, meaning-preserving changes, microstructure changes and macro-structure changes.\nThis taxonomy was later extended by Jones (2008) to take into account edit categories such as significant deletion, style, image insertion, revert, etc. Pfeil et al. (2006) proposed a 13-category taxonomy based on the data and performed manual annotation to compare cultural differences in the writing process in different versions of Wikipedia. Daxenberger and Gurevych (2013) introduced a finer-grained edit taxonomy, and performed multi-label classification to extract edit categories based on unparsed source text (Daxenberger and Gurevych, 2012). However, most taxonomies of edit categories contain only syntactic actions or a mixture of syntactic and semantic actions, failing to capturing the intention of revisions.\nIn terms of revision intentions, Zhang and Litman (2016) incorporated both argumentative writing features and surface changes from Faigley and Witte (1981) and constructed eight categories of revision purposes, such as claims/ideas, warrant/reasoning/backing, rebuttal/reservation, organization, clarify, etc. Tan and Lee (2014) used revisions to understand statement strength in academic writings. There are multiple works on the detection of specific subsets of revision intentions in Wikipedia, such as vandalism detection where the goal is to classify revisions as vandalized or non-vandalized (Harpalani et al., 2011; Adler et al., 2011) and language bias/neutral point of view detection (Recasens et al., 2013). Instead of recognizing a specific type of revision intention each time, our work aims at designing a systematic and comprehensive edit intention taxonomy to capture intentions behind textual changes.\nPrior work also used edit types and intentions to better understand the process of collaborative writing, such as article quality improvement (Kittur and Kraut, 2008). For example, Liu and Ram (2011) found that Wikipedia article quality correlates with different types of contributors; similarly Yang et al. (2016) pointed out articles in different quality stages need different types of editors. However, there are few studies examining the specific types of edits that are predictive of article quality. Recent research shows that the number of active contributors in Wikipedia has been steadily declining since 2007, and Halfaker et al. (2012) suggested that the semi-automated rejection of new editors’ contributions is a key cause, but they did not explore whether or not specific types of newcomers’ work got rejected at different rates\nand how that affects retention. In this paper, we take advantage of this new taxonomy to explore correlations between edit intentions, newcomers’ retention, and article quality."
  }, {
    "heading": "3 Semantic Taxonomy of Edit Intentions",
    "text": "A revision is created whenever an editor saves changes to a Wikipedia page. As one revision could contain multiple local changes, each revision can be labeled with one or more edit intentions, representing the purposes of why an editor made that change. Different from prior research (Daxenberger, 2016; Yang et al., 2016), we do not distinguish between revisions and edits. Although an edit is a coherent local change and might belong to any edit categories, it cannot be used to represent the intentions of editors during the revision. For example, it might be difficult\nto recognize Refactoring if only one single edit is present. Since relocation or reorganization might involve several changes in the article, looking at one might lose the whole picture and lead to information loss. Moreover, edit types simply extracted from an edit is inadequate in outlining the correct intentions, for instance, adding a sentence could be Clarification, Elaboration, or Vandalism."
  }, {
    "heading": "3.1 Taxonomy of Edit Intentions",
    "text": "Our semantic taxonomy of edit intentions builds on prior literature on collaborative writing (Faigley and Witte, 1981; Fitzgerald, 1987), research on document revision analyses (Bronner and Monz, 2012), studies on edit categories (Daxenberger and Gurevych, 2012; Fong and BiukAghai, 2010), and work on purpose/intention classification (Zhang and Litman, 2016). In order to\nensure that our taxonomy captured the intentions that Wikipedians would find meaningful, we set up discussions with a group of 12 interested editors on a Wikipedia project talk page, and iteratively refined our taxonomy based on their feedback. Our discussion with Wikipedia editors is in this page1. We also analyzed which intentions get more confused with which and used that to guide the refinement.\nWe define a top level layer for the revision intention taxonomy: intentions that are common in general revisions: General Revision Intentions, and intentions that are specific in Wikipedia: Wikipedia Specific Intentions. This categorization leads to 13 distinct semantic intentions, and Table 1 provides detailed descriptions. Specifically, general revision intentions include: Clarification, Copy Editing, Elaboration, Fact Update, Point of View, Refactoring, Simplification and verification, and can be applicable to other contexts. Counter Vandalism, Disambiguation, Process, Vandalism, and Wikification are edit intentions related to Wikipeida. We also propose an Other category, intended for edits that cannot be labeled using the above taxonomy.\nAs the first work to model intentions of revisions, our taxonomy distills and extends existing edit type taxonomies. For instance, our intentions of “elaboration” and “verification” are extensions of “evidence” type proposed by (Zhang and Litman, 2016), and a syntactic category of “information deletion” in (Daxenberger and Gurevych, 2013) could be an instance of our “vandalism” or “simplification” depending on the context."
  }, {
    "heading": "3.2 Corpus Construction",
    "text": "To construct a reliable, hand-coded dataset to serve as ground truth for automatic recognition of edit intentions, we employed four undergraduate students who had basic Wikipedia editing experience to label edits using our intention taxonomy, based on written annotation guidelines2 vetted by Wikipedia editors and provided examples3. Moreover, to expose annotators to more working knowledge of Wikipedia, we provided three one-hour training sessions where annotators were asked to\n1https://en.wikipedia.org/wiki/ Wikipedia_talk:Labels/Edit_types/ Taxonomy\n2http://www.cs.cmu.edu/˜diyiy/data/ edit_intention_annotation_doc.pdf\n3https://en.wikipedia.org/wiki/ Wikipedia:Labels/Edit_types/Examples\nlabel a small set of revisions (around 50 each time) and to discuss their disagreements until consensus.\nWe randomly sampled 5,000 revisions from Jan, 2016 to June 2016 from the recent changes table4 in the Wikipedia database. For each revision, we displayed the content difference5 before and after the change to annotators, via a labeling interface that we developed. Because an editor could make several different types of edits within a single revision, we asked four RAs to label each revision with one or more of the possible semantic intentions. We collected four valid annotations for 4,977 revisions. We used Cronbach’s α , a measure of internal consistency, to evaluate agreement among the annotators. The overall agreement α score was 0.782, indicating substantial agreement between different annotators; The rule of thumb 1993 suggests that Cronbachs alpha scores larger than 0.7 are considered as acceptable. The interannotator agreement per semantic intention is described in column α in Table 1."
  }, {
    "heading": "3.3 Corpus Expansion",
    "text": "As shown in column Before in Table 1, some types of edit intentions, such as disambiguation and clarification, were very rare in the random-sample corpus. To address this under-representation problem, we used the text of editors’ comments to expand the corpus by retrieving 200 more revisions for each edit intention except Vandalism and Counter-Vandalism, resulting in 2,200 revisions6. More precisely, as a common practice (Zanzotto and Pennacchiotti, 2010; Recasens et al., 2013), we utilized regular expressions to match the text from the comments, which editors often wrote when saving their revisions, to the edit intentions. For example, editors might be signalling that they were intending to fix problems of Point of View when their comments contained keywords such as “npov” or “neutral”. Even though the comments sometimes signal the editors’ intents, they are not infallible, editors may fail to complete the comment field, may only label one of the multiple edit intentions for a single revision, or write comments that are inaccurate, irrelevant, or incomplete. Thus the first author annotated the 2,200 revisions from the expanded corpus and\n4https://www.mediawiki.org/wiki/ Manual:Recentchanges_table\n5en.wikipedia.org/wiki/?diff=712140761 6We used a practical and economic way to expand the corpus, and this made the intention distribution skewed away. We acknowledge this expansion as a limitation.\nmerged it with the randomly sampled corpus. The frequency of the edit intentions before and after the expansion is in Table 1. We used the majority voting to resolve the disagreement. That is, if at least 3 out of 4 annotators picked an intention for a revision, it will be selected as the ground-truth. The final corpus contains 5,777 revisions, and can be downloaded from here7."
  }, {
    "heading": "4 Identification of Edit Intentions",
    "text": "We frame automated identification of edit intentions as a multi-label classification task. We designed four sets of features for identifying edit intentions from revisions. Set I comprised two features associated with the Editor: user registration indicating whether the editor of a particular revision was registered or anonymous and tenure, which refers to the elapsed months between the current revision and editors’ registration date. Set II comprised 16 features associated with the Comment written by the editor to describe the revision, including comment length and a set of regular expressions to match intentions such as *pov*, *clarify*, *simplif*, *add link*, etc. Set III comprised 198 features associated with the Revision Diff, based on content differences between current revision and the previous one. They are similar to textual features defined in Daxenberger and Gurevych (2013), but we considered a wider range of objects being modified. In particular, we computed the difference in the number of characters, uppercase words, numeric chars, white-spaces, markups, Chinese/Japanese/Korean characters, HTML entity characters, URLs, punctuations, break characters, etc. We also considered languages features, such as the use of stop words, obscene words and informal words. Set IV comprises two features associated with Vandalism and Revert. We utilized the Wikipedia API to extract whether a revision was likely to be vandalism8 or reverting revisions9."
  }, {
    "heading": "4.1 Identification Result",
    "text": "We extracted the input features with the help of Revision Scoring package10 and framed this task\n7http://www.cs.cmu.edu/˜diyiy/data/ edit_intention_dataset.csv\n8https://ores.wmflabs.org/v2/scores/ enwiki/goodfaith/71076450\n9http://pythonhosted.org/mwreverts/ api.html\n10http://pythonhosted.org/revscoring/\na multi-label classification problem. For multilabel classification, we considered solving them by using single-label classification algorithms and by transforming it into one or more single-label classification tasks. We used the multi-label classifiers implemented in Mulan (Tsoumakas et al., 2011), with 10-fold cross validation. We utilized Binary Relevance (BR) to convert our multi-label classification into 13 binary single-label problems. Similar to Daxenberger and Gurevych (2013); Yang et al. (2016), we used Random k-labelsets RAKEL method that randomly chooses l small subset with k categories from the overall set of categories. We set l as 26, twice the size of the categories, and set k as 3. MLKNN method that classifies edit intentions based on K (K=10) nearest neighbor method. We used C4.5 decision tree classifiers in BR and RAKEL, as recommended by prior work (Daxenberger and Gurevych, 2013; Potthast et al., 2013). Prior research shows that sophisticated neural network models for text-classification largely rely on factors such as dataset size (Zhang et al., 2015; Joulin et al., 2016). Due to the size of our corpus and the complexity of this task, we did not use them.\nTo evaluate the relative accuracy of the multilabel classifier, we compared it to several baselines. The random baseline, denoted as Random in Table 2, assigns labels randomly. The majority category baseline, denoted as Majority, assigns all edits the most frequent intention, elaboration. Since revision comments may be especially as informative in reflecting edit intentions, the comment baseline, denoted as CMT, is a Binary Relevance classifier that includes only the comments features from Set II. We also created a Binary Relevance classifier, denoted as BR-, which excludes comment features and only used features from Sets I, III and IV.\nTable 2 shows the evaluation metrics for the baselines and our multi-label classifiers. The metrics include the Exact Match subset accuracy, which evaluates whether the predicted labels are the same as the actual labels. These classifiers are available upon request. Table 2 also shows example-based measures of Accuracy, Precision, Recall and F1 Score, weighting each edit equally. It also shows label-based measures of accuracy – the micro- and macro-averaged F1 scores– which weight each edit intention category equally. As a ranking based measure, we measured One Error,\nwhich evaluates how many times the top ranked predicted intention is not in the set of true labels of the instance.\nResults show that the Binary Relevance (BR) and MLKNN classifiers, which used all our constructed features, outperformed Random and Majority baselines. Moreover, the BR and MLKNN methods show relatively similar best performances. Although multiple studies have utilized revisions’ comments as “groundtruth” to collect desired edits, the CMT method, which includes only comment features, is less accurate than either the BR or MLKNN models. Note that predicting 14-category semantic intentions is more challenging compared to classifying lowlevel syntactic actions, such as inserting an image (Daxenberger and Gurevych, 2013)."
  }, {
    "heading": "5 Intentions, Survival and Quality",
    "text": "The automated measurement of edit intentions provides a general framework to analyze revisions and can facilitate a wide range of applications, such as collecting specific types of revisions (Yatskar et al., 2010; Recasens et al., 2013; Zanzotto and Pennacchiotti, 2010) and outlining the\nevolution of author roles (Arazy et al., 2015; Yang et al., 2016). In this section, we demonstrate two examples of how this intention taxonomy can be applied to better understand the success of online collaboration communities (Kraut et al., 2010), specifically the process of these sites to retain new contributors and create innovative products. To this end, we first investigate what newcomers are intended for in their first sessions and whether their edit intentions can account for their survival in Wikipedia. We then examine how edits carrying on different intentions at distinct times in an article’s history influence changes in its quality."
  }, {
    "heading": "5.1 How Edit Intentions Affect Survival",
    "text": "To explore newcomers’ intentions during their first experience editing articles, we focus on users’ first edit sessions in Wikipedia. Here, Edit Session is defined as a sequence of edits performed by a registered user with less than one hour’s time gap between two adjacent edits (Halfaker et al., 2012). We then compare edit intentions of newcomers who survive - Survivors, and newcomers who do not - Non-survivors. Here, newcomers are defined as surviving if they performed an edit at\nleast two months after their first edit session."
  }, {
    "heading": "5.1.1 Intention Comparison",
    "text": "Among 100,000 randomly sampled Wikipedia users, 21,096 made revisions in the Main/Article namespace during their first editing session. Among these 4,407 were survivors (i.e., made an edit two months after registering) and 16,689 were non-survivors. We applied our edit intention model to 53,248 revisions in users’ first sessions, and compared the percentages of different types of edit intentions between survivors and nonsurvivors, as shown in Intention Dist column in Table 3. We also performed 1-way ANOVA to test whether survivors and non-survivors have the same mean for each edit intention. We observed that, survivors tend to do more copy-editing (∆+=2.3%) and more wikification (∆+=6.5%), while non-survivors seem to perform more simplification and vandalism, which might provide signals for detecting vandals."
  }, {
    "heading": "5.1.2 Revert Analysis",
    "text": "To explore the relationship between rejection of contributions and newcomer retention, we also visualized the revert ratios of different types of edit intentions for survivors and non-survivors in their\nfirst session. Here, Revert refers to whether an edit from the author was reverted or completely removed by another user, and we detect reverts using MediaWiki Reverts library11. We then measured the revert ratio for each edit intention by calculating the percentage of revisions belonging to a specific edit intention, among all reverted revisions in users’ first sessions. As shown in the Revert Ratio column in Table 3, in general, nonsurvivors get reverted more compared to survivors, across all edit intentions. Interestingly, nonsurvivors compared to survivors get reverted more when performing Wikification, verification and Refactoring, suggesting that sophisticated types of work might not be suitable for beginners."
  }, {
    "heading": "5.1.3 Newcomer Survival",
    "text": "As a further exploration of the relationship between edit intentions and newcomer survival, we performed a logistic regression using edits in survivors’ and non-survivors’ first sessions. To handle this imbalanced data (i.e., many more negative examples than positive examples in training), we performed majority-class under-sampling to make this dataset balanced. Similar to Halfaker et al. (2012), we controlled the number of revisions completed during the first session (a proxy for an editor’s initial investment), and the number of revisions reverted in their first sessions. We\n11http://pythonhosted.org/mwreverts/\ndescribed the regression coefficients of statistically significant edit intentions in the Survival column of Table 4. This logistic model achieves an Accuracy of 60.98%, Recall of 58.30%, Precision of 78.08% and F1-score of 66.76%. Editing articles for the purposes of Process, Verification and Wikification significantly predict the survival of newcomers, while performing vandalism is a strong negative predictor for survival."
  }, {
    "heading": "5.2 How Intentions Affect Article Quality",
    "text": "Although there are over 5.5 million articles in the English Wikipedia, fewer than 0.2% have been evaluated by Wikipedians as good articles and around 92% have been evaluated as start or stub class articles, Wikipedia’s two lowest quality categories. In this section, we examine how edits with different intentions at distinct times in an article’s history influence changes in its quality.\nThis task is framed as a prediction task, i.e. using edits’ intentions and a set of control variables to predict changes in article quality. We borrowed a Article Quality Prediction Dataset released in Yang et al. (2016), which consists of the quality ratings collected in January and June, 2015 of 151,452 articles. We collected 1,623,446 revisions made to these articles between January and June 2015, by randomly sampling 10% revisions that were made to these articles during that time periods. Specifically, the outcome article quality change is calculated by subtracting the previous quality score from the end quality score. The control variables include the previous article quality score, the total number of edits, the total number of editors, the changed bytes to an article, and the total number of edits to the article talk page during the six months. To construct edit-intention predictors, we summed the number of edits for each edit intention during the six months divided by the total number of revisions in this article.\nResults of the linear regression model, shown in Quality Changes column of Table 4, show that our constructed regression model is significantly predictive of article quality changes (R2 = 0.225). The results show that, keeping all control variables fixed, more Copy Editing, Elaboration, Refactoring and Verification are positively associated with improvements in article quality; in contrast, Vandalism, Counter Vandalism, Disambiguation, Process and Simplification predict declines in article quality. The first four of these edits types often\noccur with reducing the article content, removing or redirecting pages. Improper use of them might be detrimental to article quality.\nTo determine if the effect of edit intentions on quality changes depends upon the initial quality of the article, we added the interaction terms between the previous quality score and edit percentages of different intentions (e.g., clarification x previous quality), and visualized interaction effects in Figure 2. When examining the interaction terms in more detail: the negative slope of copy editing (when prev=2) suggests that, as articles increase in quality, copy editing is needed less. We found similar trends for interactions between previous quality and elaboration and verification, which are essential for articles in the starting stages. In contrast, the positive slopes for simplification, wikification and process suggest that, as articles increase in quality, simplifying articles’ content, adding proper links or reorganizing their structure becomes more important. Overall, these results reveal that different types of edit intentions are needed at different quality stages of articles."
  }, {
    "heading": "6 Discussion and Conclusion",
    "text": "In this work, we proposed 13 semantic intentions that motivate editors’ revisions in English Wikipedia. Example edit intentions include copy editing, elaboration, simplification, etc. Based\nin a labeled corpus of revisions, we developed machine-learning models to automatically identify these edit intentions. We then examine the relations between edit intentions, newcomers survival, and article quality improvement. We found that (1) survivors tend to do more copy editing and wikification; non-survivors seem to perform more vandalism and other sophisticated types of work, and the latter often gets reverted more; (2) Different types of contributions are needed by articles in different quality stages, with elaboration and verification are needed more for articles in the starting stages, and simplification and process become more important as article quality increases.\nOur proposed edit intention taxonomy and the constructed corpus can facilitate a set of downstream NLP applications. First, classifiers based on this intention taxonomy can help retrieve large scale and high quality revisions around simplification, neutral point of view or copy editing, which provides amazing corpora for studying lexical simplification, language bias detection and paraphrases. Second, as we showed in Section 5.2, determining how different edit types influence changes in articles is of great use to better the causes of quality variance in collaborative writing, such as detecting quality flaws (Anderka et al., 2012) and providing insights on which specific aspects of an article needs improvement and what type of work should be performed. The ability to identify the need for editing, and specifically the types of editing work required, can greatly assist not only collaborative writing but also individual improvement of text. Moreover, even though our edit taxonomy is for English Wikipedia, it can be applied to other language versions of Wikipedia. We are now deploying the same edit intention taxonomy for Italian Wikipedia, and plan to apply it to other low resourced languages in Wikipedia. Finally, beyond the context of Wikipedia, similar taxonomies can be designed for analyzing the collaboration and interaction happened in other online contexts such as academic writing (e.g., Google Docs or ShareLatex, etc)."
  }, {
    "heading": "Acknowledgement",
    "text": "This research was supported in part by a grant from Google to Robert Kraut. The first author was supported by Carnegie Mellon Presidential Fellowship. The authors would like to thank our reviewers and Wikipedian for helpful feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Wikipedia vandalism detection: Combining natural language, metadata, and reputation features",
    "authors": ["B Thomas Adler", "Luca De Alfaro", "Santiago M MolaVelasco", "Paolo Rosso", "Andrew G West."],
    "venue": "International Conference on Intelligent Text Pro-",
    "year": 2011
  }, {
    "title": "Using the past to score the present: Extending term weighting models through revision history analysis",
    "authors": ["Ablimit Aji", "Yu Wang", "Eugene Agichtein", "Evgeniy Gabrilovich."],
    "venue": "Proceedings of the 19th ACM international conference on Information and",
    "year": 2010
  }, {
    "title": "Predicting quality flaws in user-generated content: the case of wikipedia",
    "authors": ["Maik Anderka", "Benno Stein", "Nedim Lipka."],
    "venue": "Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages",
    "year": 2012
  }, {
    "title": "Functional roles and career paths in wikipedia",
    "authors": ["Ofer Arazy", "Felipe Ortega", "Oded Nov", "Lisa Yeo", "Adam Balila."],
    "venue": "Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW ’15, pages 1092–",
    "year": 2015
  }, {
    "title": "User edits classification using document revision histories",
    "authors": ["Amit Bronner", "Christof Monz."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL’12, pages 356–366.",
    "year": 2012
  }, {
    "title": "What is coefficient alpha? an examination of theory and applications",
    "authors": ["Jose M Cortina."],
    "venue": "Journal of applied psychology, 78(1):98.",
    "year": 1993
  }, {
    "title": "The Writing Process in Online Mass Collaboration: NLP-Supported Approaches to Analyzing Collaborative Revision and User Interaction",
    "authors": ["Johannes Daxenberger."],
    "venue": "Ph.D. thesis, Technische Universität.",
    "year": 2016
  }, {
    "title": "A corpus-based study of edit categories in featured and non-featured Wikipedia articles",
    "authors": ["Johannes Daxenberger", "Iryna Gurevych."],
    "venue": "Proceedings of COLING 2012, pages 711–726, Mumbai, India. The COLING 2012 Organizing Committee.",
    "year": 2012
  }, {
    "title": "Automatically classifying edit categories in Wikipedia revisions",
    "authors": ["Johannes Daxenberger", "Iryna Gurevych."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 578–589, Seattle, Washington, USA.",
    "year": 2013
  }, {
    "title": "Analyzing revision",
    "authors": ["Lester Faigley", "Stephen Witte."],
    "venue": "College composition and communication, 32(4):400–414.",
    "year": 1981
  }, {
    "title": "A survey of nlp methods and",
    "authors": ["Oliver Ferschke", "Johannes Daxenberger", "Iryna Gurevych"],
    "year": 2013
  }, {
    "title": "Research on revision in writing",
    "authors": ["Jill Fitzgerald."],
    "venue": "Review of educational research, 57(4):481–506.",
    "year": 1987
  }, {
    "title": "What did they do? deriving high-level edit histories in wikis",
    "authors": ["Peter Kin-Fong Fong", "Robert P. Biuk-Aghai."],
    "venue": "Proceedings of the 6th International Symposium on Wikis and Open Collaboration, WikiSym ’10, pages 2:1–2:10, New York, NY, USA.",
    "year": 2010
  }, {
    "title": "Finding hedges by chasing weasels: Hedge detection using wikipedia tags and shallow linguistic features",
    "authors": ["Viola Ganter", "Michael Strube."],
    "venue": "Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACL Short ’09, pages 173–176.",
    "year": 2009
  }, {
    "title": "The rise and decline of an open collaboration system: How wikipedia’s reaction to popularity is causing its decline",
    "authors": ["Aaron Halfaker", "R Stuart Geiger", "Jonathan T Morgan", "John Riedl."],
    "venue": "American Behavioral Scientist, page 0002764212469365.",
    "year": 2012
  }, {
    "title": "Language of vandalism: Improving wikipedia vandalism detection via stylometric analysis",
    "authors": ["Manoj Harpalani", "Michael Hart", "Sandesh Singh", "Rob Johnson", "Yejin Choi."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computa-",
    "year": 2011
  }, {
    "title": "Patterns of revision in online writing a study of wikipedia’s featured articles",
    "authors": ["John Jones."],
    "venue": "Written Communication, 25(2):262–289.",
    "year": 2008
  }, {
    "title": "Bag of tricks for efficient text classification",
    "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."],
    "venue": "arXiv preprint arXiv:1607.01759.",
    "year": 2016
  }, {
    "title": "Harnessing the wisdom of crowds in wikipedia: Quality through coordination",
    "authors": ["Aniket Kittur", "Robert E. Kraut."],
    "venue": "Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work, CSCW ’08, pages 37–46.",
    "year": 2008
  }, {
    "title": "Dealing with newcomers",
    "authors": ["Robert Kraut", "Moira Burke", "John Riedl", "P Resnick."],
    "venue": "Evidencebased Social Design Mining the Social Sciences to Build Online Communities, 1:42.",
    "year": 2010
  }, {
    "title": "Who does what: Collaboration patterns in the wikipedia and their impact on article quality",
    "authors": ["Jun Liu", "Sudha Ram."],
    "venue": "ACM Trans. Manage. Inf. Syst., 2(2):11:1–11:23.",
    "year": 2011
  }, {
    "title": "Mining naturally-occurring corrections and paraphrases from wikipedias revision history",
    "authors": ["Aurlien Max", "Guillaume Wisniewski."],
    "venue": "Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Val-",
    "year": 2010
  }, {
    "title": "Mining wikipedia’s article revision history for training computational linguistics algorithms",
    "authors": ["Rani Nelken", "Elif Yamangil."],
    "venue": "Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy, pages 31–36.",
    "year": 2008
  }, {
    "title": "Cultural differences in collaborative authoring of wikipedia",
    "authors": ["Ulrike Pfeil", "Panayiotis Zaphiris", "Chee Siang Ang."],
    "venue": "Journal of Computer-Mediated Communication, 12(1):88–113.",
    "year": 2006
  }, {
    "title": "Overview of the 5th international competition on plagiarism detection",
    "authors": ["Martin Potthast", "Matthias Hagen", "Tim Gollub", "Martin Tippmann", "Johannes Kiesel", "Paolo Rosso", "Efstathios Stamatatos", "Benno Stein."],
    "venue": "CLEF Conference on Multilingual and",
    "year": 2013
  }, {
    "title": "Linguistic models for analyzing and detecting biased language",
    "authors": ["Marta Recasens", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2013
  }, {
    "title": "A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication",
    "authors": ["Chenhao Tan", "Lillian Lee."],
    "venue": "Proceedings of ACL (short paper).",
    "year": 2014
  }, {
    "title": "Mulan: A java library for multi-label learning",
    "authors": ["Grigorios Tsoumakas", "Eleftherios SpyromitrosXioufis", "Jozef Vilcek", "Ioannis Vlahavas."],
    "venue": "Journal of Machine Learning Research, 12:2411–2414.",
    "year": 2011
  }, {
    "title": "Mining wikipedia revision histories for improving sentence compression",
    "authors": ["Elif Yamangil", "Rani Nelken."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short",
    "year": 2008
  }, {
    "title": "Who did what: Editor role identification in wikipedia",
    "authors": ["Diyi Yang", "Aaron Halfaker", "Robert Kraut", "Eduard Hovy."],
    "venue": "Tenth International AAAI Conference on Web and Social Media.",
    "year": 2016
  }, {
    "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia",
    "authors": ["Mark Yatskar", "Bo Pang", "Cristian Danescu-NiculescuMizil", "Lillian Lee."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the",
    "year": 2010
  }, {
    "title": "Expanding textual entailment corpora from wikipedia using co-training",
    "authors": ["Fabio Massimo Zanzotto", "Marco Pennacchiotti."],
    "venue": "Proceedings of the COLING-Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Re-",
    "year": 2010
  }, {
    "title": "Measuring contextual fitness using error contexts extracted from the wikipedia revision history",
    "authors": ["Torsten Zesch."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538,",
    "year": 2012
  }, {
    "title": "Using context to predict the purpose of argumentative writing revisions",
    "authors": ["Fan Zhang", "Diane Litman."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2016
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "Advances in neural information processing systems, pages 649–657. 2010",
    "year": 2015
  }],
  "id": "SP:f8245f9f15f286cfd689ac29d5a3c4ff97ac9bd7",
  "authors": [{
    "name": "Diyi Yang",
    "affiliations": []
  }, {
    "name": "Aaron Halfaker",
    "affiliations": []
  }, {
    "name": "Robert Kraut",
    "affiliations": []
  }, {
    "name": "Eduard Hovy",
    "affiliations": []
  }],
  "abstractText": "Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness: how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of edits that users make in their first session predict their subsequent survival as Wikipedia editors, and articles in different stages need different types of edits.",
  "title": "Identifying Semantic Edit Intentions from Revisions in Wikipedia"
}