{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 910–921 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n910\nWe present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.1"
  }, {
    "heading": "1 Introduction",
    "text": "Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa-\n1Our code and data are available at: https://github. com/rotmanguy/DPCCA.\ntions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015).\nThese findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016).\nWhile the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016).\nIn this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup. The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language.\nWe propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao, 1969). To the best of our knowledge, PCCA has not been used in multilingual settings before. In short, PCCA is a variant of CCA which learns maximally correlated linear projections of two views (e.g., two language-specific “text-based views”) conditioned on a shared third view (e.g.,\nthe “visual view”). We discuss the PCCA and DPCCA methods in §3 and show how they can be applied without having access to the shared images at test time inference.\nPCCA inherits one disadvantageous property from CCA: both methods compute estimates for covariance matrices based on all training data. This would prevent feasible training of their deep nonlinear variants, since deep neural nets (DNNs) are predominantly optimized via stochastic optimization algorithms. To resolve this major hindrance, we propose an effective optimization algorithm for DPCCA, inspired by the work of Wang et al. (2015b) on Deep CCA (DCCA) optimization.\nWe evaluate our DPCCA architecture on two semantic tasks: 1) multilingual word similarity and 2) cross-lingual image description retrieval. For the former, we construct and provide to the community a new Word-Image-Word (WIW) dataset containing bilingual lexicons for three languages with shared images for 5K+ concepts. WIW is used as training data for word similarity experiments, while evaluation is conducted on the standard multilingual SimLex-999 dataset (Hill et al., 2015; Leviant and Reichart, 2015).\nThe results reveal stable improvements over a large space of non-deep and deep CCA-style baselines in both tasks. Most importantly, 1) PCCA is overall better than other methods which do not use the additional perceptual view; 2) DPCCA outperforms PCCA, indicating the importance of nonlinear transformations modeled through DNNs; 3) DPCCA outscores DCCA, again verifying the importance of conditioning multilingual text embedding induction on the shared visual view; and 4) DPCCA outperforms two recent multi-modal bilingual models which also leverage visual information (Gella et al., 2017; Rajendran et al., 2016)."
  }, {
    "heading": "2 Related Work",
    "text": "This work is related to two research threads: 1) multi-modal models that combine vision and language, with a focus on multilingual settings; 2) correlational multi-view models based on CCA which learn a shared vector space for multiple views.\nMulti-Modal Modeling in Multilingual Settings Research in cognitive science suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011). Visual context serves as a useful cross-\nlingual grounding signal (Bruni et al., 2014; Glavaš et al., 2017) due to its language invariance, even enabling the induction of word-level bilingual semantic spaces solely through tagged images obtained from the Web (Bergsma and Van Durme, 2011; Kiela et al., 2015). Vulić et al. (2016) combine text embeddings with visual features via simple techniques of concatenation and averaging to obtain bilingual multi-modal representations, with noted improvements over text-only embeddings on word similarity and bilingual lexicon extraction. However, similar to the monolingual model of Kiela and Bottou (2014), their models lack the training phase, and require the visual signal at test time.\nRecent work from Gella et al. (2017) exploits visual content as a bridge between multiple languages by optimizing a contrastive loss function. Furthermore, Rajendran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7).\nFinally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia).\nCorrelational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view.\nCCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini-\nbatches on very large training sets. To address this limitation, various optimization methods for Deep CCA were proposed. Andrew et al. (2013) use L-BFGS (Byrd et al., 1995) over all training samples, while Arora and Livescu (2013) and Yan and Mikolajczyk (2015) train with large batches. However, these methods suffer from high memory complexity with unstable numerical computations.\nWang et al. (2015b) have recently proposed a stochastic approach for CCA and DCCA which copes well with small and large batch sizes while preserving high model performance. They use orthogonal iterations to estimate a moving average of the covariance matrices, which improves memory consumption. Therefore, we base our novel optimization algorithm for DPCCA on this approach."
  }, {
    "heading": "3 Methodology: Deep Partial CCA",
    "text": "Given two image descriptions x and y in two languages and an image z that they refer to, the task is to learn a shared bilingual space such that similar descriptions obtain similar representations in the induced space. The image z serves as a shared third view on the textual data during training. The representation model is then utilized in cross-lingual and monolingual tasks. In this paper we focus on the more realistic scenario where no relevant visual content is available at test time. For this goal we propose a novel Deep Partial CCA (DPCCA) framework.\nIn what follows, we first review the CCA model and its deep variant: DCCA. We then introduce our DPCCA architecture, and describe our new stochastic optimization algorithm for DPCCA."
  }, {
    "heading": "3.1 CCA and Deep CCA",
    "text": "DCCA (Andrew et al., 2013) extends CCA by learning non-linear (instead of linear) transformations of features contained in the input matrices X ∈ RDx×N and Y ∈ RDy×N , where Dx and Dy are input vector dimensionalities, and N is the number of input items. Since CCA is a special case of the non-linear DCCA (see below), we here briefly outline the more general DCCA model.\nThe DCCA architecture is illustrated in Figure 1a. Non-linear transformations are achieved through two DNNs f : RDx×N → RD′x×N and g : RDy×N → RD′y×N for X and Y . D′x and D′y are the output dimensionalities. A final linear layer is added to resemble the linear CCA projection.\nThe goal is to project the features of X and\nY into a shared L-dimensional (1 ≤ L ≤ min(D′x, D ′ y)) space such that the canonical correlation of the final outputs F (X) = W Tf(X) and G(Y ) = V T g(Y ) is maximized. W ∈ RD′x×L and V ∈ RD′y×L are projection matrices: they project the final outputs of the DNNs to the shared space. Wf and Vg (the parameters of f and g) and the projection matrices are the model parameters: WF = {Wf ,W }; VG = {Vg,V }.2 Formally, the DCCA objective can be written as:\nmax WF ,VG Tr(Σ̂FG)\nso that Σ̂FF = Σ̂GG = I. (1)\nΣ̂FG ≡ 1N−1F (X)G(Y ) T is the estimation of the cross-covariance matrix of the outputs, and Σ̂FF ≡ 1N−1F (X)F (X)\nT , Σ̂GG ≡ 1 N−1G(Y )G(Y ) T are the estimations of the autocovariance matrices of the outputs.3 Further, following Wang et al. (2015b), the optimal solution of Eq. (1) is equivalent to the optimal solution of the following:\nmin WF ,VG\n1\nN − 1‖F (X)−G(Y )‖ 2 F\ns.t. Σ̂FF = Σ̂GG = I.\n(2)\nThe main disadvantage of DCCA is its inability to support more than two views, and to learn conditioned on an additional shared view, which is why we introduce Deep Partial CCA."
  }, {
    "heading": "3.2 New Model: Deep Partial CCA",
    "text": "Figure 1b illustrates the architecture of DPCCA. The training data now consists of triplets (xi,yi, zi) N 1=1 from three views, forming the columns of X , Y and Z, where xi ∈ RDx ,yi ∈ RDy , zi ∈ RDz for i = 1, . . . , N . The objective is to maximize the canonical correlation of the first two views X and Y conditioned on the shared third variable Z. Following Rao (1969)’s work on Partial CCA, we first consider two multivariate linear multiple regression models:\nF (X) = AZ + F (X|Z), (3) G(Y ) = BZ +G(Y |Z). (4)\n2For notational simplicity, we assume f(X) and g(Y ) to have zero-means, otherwise it is possible to centralize them at the final layer of each network to the same effect.\n3The CCA model can be seen as a special (linear) case of the more general DCCA model. The basic CCA objective can be recovered from the DCCA objective by simply setting D′x = Dx, D′y = Dy and f(X) = idX , g(Y ) = idY ; id is the identity mapping.\nA,B ∈ RL×Dz are matrices of coefficients, and F (X|Z),G(Y |Z) ∈ RL×N are normal random error matrices: residuals. We then minimize the mean-squared error regression criterion:\nmin A\n1\nN − 1‖F (X)−AZ‖ 2 F , (5)\nmin B\n1\nN − 1‖G(Y )−BZ‖ 2 F . (6)\nAfter obtaining the optimal solutions for the coefficients, Â and B̂, the residuals are as follows:\nF (X|Z) = F (X)− ÂZ\n= F (X)− Σ̂FZΣ̂−1ZZZ. (7)\nG(Y |Z) is computed in the analogous manner, now relying on G(Y ) and B̂Z. Σ̂S′Z ≡\n1 N−1SZ T refers to the covariance matrix estimator of S′ and Z, where (S′, S) ∈ {(F ,F (X)), (G,G(Y )), (Z,Z)}.4\nThe canonical correlation between the residual matrices F (X|Z) and G(Y |Z) is referred to as the partial canonical correlation. The Deep PCCA objective can be obtained by replacing F (X) and G(Y ) with their residuals in Eq. (2):\nmin WF ,VG\n1\nN − 1‖F (X|Z)−G(Y |Z)‖ 2 F\ns.t. Σ̂FF |Z = Σ̂GG|Z = I.\n(8)\nThe computation of the conditional covariance matrix Σ̂FF |Z can be formulated as follows:\nΣ̂FF |Z ≡ 1\nN − 1F (X|Z)F (X|Z) T = Σ̂FF − Σ̂FZΣ̂−1ZZΣ̂ T FZ . (9)\n4A small value > 0 is added to the main diagonal of the covariance estimators for numerical stability.\nThe other conditional covariance matrix Σ̂GG|Z is again computed in the analogous manner, replacing F with G and X with Y .5\nWhile the (D)PCCA objective is computed over the residuals, after the network is trained (using multilingual texts and corresponding images) we can compute the representations of F (X) and G(Y ) at test time without having access to images (see the network structure in Figure 1b). This heuristic enables the use of DPCCA in a real-life scenario in which images are unavailable at test time, and its encouraging results are demonstrated in §7.\nModel Variants We consider two DPCCA variants : 1) in DPCCA Variant A, the shared view Z is kept fixed; 2) DPCCA Variant B also optimizes over Z, as illustrated in Figure 1b. Variant A may be seen as a special case of Variant B.6\nVariant B learns a non-linear function of the shared variable, H(Z) = UTh(Z), during training, where h : RDz×N → RDz′×N is a DNN having the same architecture as f and g. U ∈ RDz′×L is the final linear layer of H , such that overall, the additional parameters of the model are UH = {Uh,U}. Instead of assuming a linear connection between F (X) and G(Y ) to Z, as in Variant A, we now assume that the linear connection takes place with H(Z). This assumption\n5The original PCCA objective can be recovered by setting D′x = Dx, D′y = Dy and f(X) = idX , g(Y ) = idY .\n6For Variant A, in order for Z to be on the same range of values as in F and G, we pass it through the activation function of the network, Z = σ(Z). Due to space constraints we discuss DPCCA Variant A in the supplementary material only.\nchanges Eq. (3) and Eq. (4) to:7\nF (X) = A′ ·H(Z) + F (X|H(Z)), (10) G(Y ) = B′ ·H(Z) +G(Y |H(Z)). (11)"
  }, {
    "heading": "4 DPCCA: Optimization Algorithm",
    "text": "Training deep variants of CCA-style multi-view models is non-trivial due to estimation on the entire training set related to whitening constraints (i.e., the orthogonality of covariance matrices). To overcome this issue, Wang et al. (2015b) proposed a stochastic optimization algorithm for DCCA via non-linear orthogonal iterations (DCCA NOI). Relying on the solution for DCCA (§4.1), we develop a new optimization algorithm for DPCCA in §4.2."
  }, {
    "heading": "4.1 Optimization of DCCA",
    "text": "The DCCA optimization from Wang et al. (2015b), fully provided in Algorithm 1, relies on three key steps. First, the estimation of the covariance matrices in the form of Σ̂FF t at time t is calculated by a moving average over the minibatches:\nΣ̂FF t ←ρΣ̂FF t−1\n+ (1− ρ) ( |bt| N − 1 )−1 F (Xbt)F (Xbt) T . (12)\nbt is the minibatch at time t, Xbt is the current input matrix at time t, and ρ ∈ [0, 1] controls the ratio between the overall covariance estimation and the covariance estimation of the current minibatch.8 This step eliminates the need of estimating the covariances over all training data, as well as the inherent bias when the estimate relies only on the current minibatch.\nSecond, the DCCA NOI algorithm forces the whitening constraints to hold by performing an explicit matrix transformation in the form of:\n˜F (Xbt) = Σ̂ − 1 2 FFt F (Xbt). (13)\nAccording to Horn et al. (1988), if ρ = 0:( |bt| N − 1 )−1 ˜F (Xbt) ˜F (Xbt)T = I. (14) Finally, in order to optimize the DCCA objective (see Eq. (2)), the weights of the two DNNs are decoupled: i.e., the objective is disassembled into two separate mean-squared error objectives. Instead of\n7Note that the matrices of coefficients A′ , B′ ∈ RL×L. 8Setting ρ to a high value indicates slow updates of the estimator; setting it low mostly erases the overall estimation and relies more on the current minibatch estimation.\nAlgorithm 1 The non-linear orthogonal iterations (NOI) algorithm for DCCA (DCCA NOI) Input: Data matrices X ∈ RDx×N , Y ∈ RDy×N , time constant ρ, learning rate η.\ninitialization: Initialize weights (WF , VG). Randomly choose a minibatch (Xb0 , Yb0 ). Initialize covariances: Σ̂FF ← N−1|b0| F (Xb0)F (Xb0) T Σ̂GG ← N−1|b0| G(Yb0)G(Yb0) T\nfor t = 1, 2, . . . , n do Randomly choose a minibatch (Xbt , Ybt ).\nUpdate covariances: Σ̂FF ← ρΣ̂FF + (1− ρ)N−1|bt| F (Xbt)F (Xbt) T Σ̂GG ← ρΣ̂GG + (1− ρ)N−1|bt| G(Ybt)G(Ybt) T Fix G̃(Ybt) = Σ̂ − 1 2 GGG(Ybt), and compute ∇WF with respect to:\nmin WF\n1 |bt|‖F (Xbt)− G̃(Ybt)‖ 2 F\nUpdate parameters: WF ←WF − η∇WF Fix ˜F (Xbt) = Σ̂ − 1 2\nFF F (Xbt), and compute ∇VG with respect to:\nmin VG\n1 |bt|‖G(Ybt)− ˜F (Xbt)‖2F\nUpdate parameters: VG ← VG − η∇VG end for\nOutput: (WF ,VG)\ntrying to bring F (Xbt) and G(Ybt) closer in one gradient descent step, two steps are performed: one of the views is fixed, and a gradient step over the other is performed, and so on, iteratively. The final objective functions at each time step are:\nmin WF\n1\n|bt| ‖F (Xbt)− G̃(Ybt)‖ 2 F , (15)\nmin VG\n1\n|bt| ‖G(Ybt)− ˜F (Xbt)‖ 2 F . (16)\nWang et al. (2015b) show that the projection matrices W and V converge to the exact solutions of CCA as t→∞ when considering linear CCA."
  }, {
    "heading": "4.2 Optimization of DPCCA",
    "text": "Our DPCCA optimization is based on the DCCA NOI algorithm with several adjustments. Besides the requirement to obtain the sample covariances Σ̂FF and Σ̂GG, when calculating the conditional variables F (X|Z), G(Y |Z), Σ̂FF |Z and Σ̂GG|Z , we additionally have to obtain the stochastic estimators Σ̂FZ , Σ̂GZ and Σ̂ZZ . To this end, we use the moving average estimation from Eq. (12). Next, we define the whitening transformation on the residuals:\n˜F (Xbt |Zbt) = Σ̂ − 1 2 FFt|ZF (Xbt |Zbt), (17)\n˜G(Ybt |Zbt) = Σ − 1 2 GGt|ZG(Ybt |Zbt). (18)\nAs before, the whitening constraints hold when ρ = 0. From here, we derive our two final objective functions over the residuals at time t:\nmin WF\n1\n|bt| ‖F (Xbt |Zbt)− ˜G(Ybt |Zbt)‖ 2 F , (19)\nmin VG\n1\n|bt| ‖G(Ybt |Zbt)− ˜F (Xbt |Zbt)‖ 2 F . (20)\nEquivalently to Eq. (15)-(16) that replace Eq. (2), Eq. (19)-(20) replace Eq. (8) by performing stochastic, decoupled and unconstrained steps. As our algorithm performs CCA over the residuals, we gain the same guarantees as Wang et al. (2015b), now for the projection matrices of the residuals.\nAlgorithm 2 shows the full optimization procedure for the more complex DPCCA Variant B. The full algorithm for Variant A is provided in the supplementary material. The main difference is that with Variant B we replace Z with H(Z) in all equations where it appears, and we optimize over UH along with WF and VG in Eq. (19) and Eq. (20), respectively."
  }, {
    "heading": "5 Tasks and Data",
    "text": "Cross-lingual Image Description Retrieval The cross-lingual image description retrieval task is formulated as follows: taking an image description as a query in the source language, the system has to retrieve a set of relevant descriptions in the target language which describe the same image. Our evaluation assumes a single-best scenario, where only a single target description is relevant for each query. In addition, in our setup, images are not available during inference: retrieval is performed based solely on text queries. This enables a fair comparison between our model and many baseline models that cannot represent images and text in a shared space. Moreover, it allows us to test our model in the realistic setup where images are not available at test time. To avoid the use of images at retrieval time with DPCCA, we perform the retrieval on F (X) and G(Y ), rather than on F (X|Z) and G(Y |Z) (see §3.2).\nWe use the Multi30K dataset (Elliott et al., 2016), originated from Flickr30K (Young et al., 2014) that is comprised of Flicker images described with 1-5 English descriptions per image. Multi30K adds\nAlgorithm 2 The non-linear orthogonal iterations (NOI) algorithm for DPCCA Variant B Input: Data matrices X ∈ RDx×N , Y ∈ RDy×N , Z ∈ RDz×N , time constant ρ, learning rate η.\ninitialization: Initialize weights (WF ,VG,UH ). Randomly choose a minibatch (Xb0 ,Yb0 ,Zb0 ). Initialize covariances: Σ̂FF ← N−1|b0| F (Xb0)F (Xb0) T Σ̂GG ← N−1|b0| G(Yb0)G(Yb0) T Σ̂HH ← N−1|b0| H(Zb0)H(Zb0) T Σ̂FH ← N−1|b0| F (Xb0)H(Zb0) T Σ̂GH ← N−1|b0| G(Yb0)H(Zb0) T\nfor t = 1, 2, . . . , n do Randomly choose a minibatch (Xbt ,Ybt ,Zbt ). Update covariances: Σ̂FF ← ρΣ̂FF + (1− ρ)N−1|bt| F (Xbt)F (Xbt) T Σ̂GG ← ρΣ̂GG + (1− ρ)N−1|bt| G(Ybt)G(Ybt) T Σ̂HH ← ρΣ̂HH + (1− ρ)N−1|bt| H(Zbt)H(Zbt) T Σ̂FH ← ρΣ̂FH + (1− ρ)N−1|bt| F (Xbt)H(Zbt) T Σ̂GH ← ρΣ̂GH + (1− ρ)N−1|bt| G(Ybt)H(Zbt) T\nUpdate conditional variables: F |H ← F (Xbt)− Σ̂FHΣ̂ −1 HHH(Zbt) G|H ← G(Ybt)− Σ̂GHΣ̂ −1 HHH(Zbt) Σ̂FF |H ← Σ̂FF − Σ̂FHΣ̂−1HHΣ̂ T FH Σ̂GG|H ← Σ̂GG − Σ̂GHΣ̂−1HHΣ̂ T GH\nFix G̃|H = Σ̂− 1 2\nGG|HG|H , and compute ∇WF , ∇UH with respect to: min WF ,UH 1 |bt|‖F |H − G̃|H‖ 2 F\nUpdate parameters: WF ←WF − η∇WF ,UH ← UH − η∇UH Fix F̃ |H = Σ̂− 1 2\nFF |HF |H , and compute ∇VG, ∇UH with respect to: min VG,UH 1 |bt|‖G|H − F̃ |H‖ 2 F\nUpdate parameters: VG ← VG − η∇VG,UH ← UH − η∇UH end for\nOutput: (WF ,VG,UH )\nGerman descriptions to a total of 30,014 images: most were written independently of the English descriptions, while some are direct translations. Each image is associated with one English and one German description. We rely on the original Multi30K splits with 29,000, 1,014, and 1,000 triplets for training, validation, and test, respectively.\nMultilingual Word Similarity The word similarity task tests the correlation between automatic and human generated word similarity scores. We evaluate with the Multilingual SimLex-999 dataset (Leviant and Reichart, 2015): the 999 English (EN)\nword pairs from SimLex-999 (Hill et al., 2015) were translated to German (DE), Italian (IT), and Russian (RU), and similarity scores were crowdsourced from native speakers.\nWe introduce a new dataset termed Word-ImageWord (WIW), which we use to train word-level models for the multilingual word similarity task. WIW contains three bilingual lexicons (EN-DE, EN-IT, EN-RU) with images shared between words in a lexicon entry. Each WIW entry is a triplet: an English word, its translation in DE/IT/RU, and a set of images relevant to the pair.\nEnglish words were taken from the January 2017 Wikipedia dump. After removing stop words and punctuation, we extract the 6,000 most frequent words from the cleaned corpus not present in SimLex. DE/IT/RU words were obtained semiautomatically from the EN words using Google Translate. The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only. Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10\nTable 1 provides a summary of the WIW dataset. The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2)."
  }, {
    "heading": "6 Experimental Setup",
    "text": "Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower-\n9https://github.com/douwekiela/mmfeat. 10Offensive words and images are manually cleaned. 11POS tag information is taken from the NLTK toolkit for\nthe English words.\ncased and tokenized. Each sentence is represented with one vector: the average of its word embeddings. For English, we rely on 500-dimensional English skip-gram word embeddings (Mikolov et al., 2013) trained on the January 2017 Wikipedia dump with bag-of-words contexts (window size of 5). For German we use the deWaC 1.7B corpus (Baroni et al., 2009) to obtain 500-dimensional German embeddings using the same word embedding model. For word similarity, to be directly comparable to previous work, we rely on 300-dim word vectors in EN, DE, IT, and RU from Mrkšić et al. (2017).\nVisual features are extracted from the penultimate layer (FC7) of the VGG-19 network (Simonyan and Zisserman, 2015), and compressed to the dimensionality of the textual inputs by a Principal Component Analysis (PCA) step. For the word similarity task, we average the visual vectors across all images of each word pair as done in, e.g., (Vulić et al., 2016), before the PCA step.\nBaseline Models We consider a wide variety of multi-view CCA-based baselines. First, we compare against the original (linear) CCA model (Hotelling, 1936), and its deep non-linear extension DCCA (Andrew et al., 2013). For DCCA: 1) we rely on its improved optimization algorithm from Wang et al. (2015a) which uses a stochastic approach with large minibatches; 2) we compare against the DCCA NOI variant (Wang et al., 2015b) described by Algorithm 1, and another recent DCCA variant with the optimization algorithm based on a stochastic decorrelational loss (Chang et al., 2017) (DCCA SDL); and 3) we also test the DCCA Autoencoder model (DCCAE) (Wang et al., 2015a), which offers a trade-off between maximizing the canonical correlation of two sets of variables and finding informative features for their reconstruction.\nAnother baseline is Generalized CCA (GCCA) (Funaki and Nakayama, 2015; Horst, 1961; Rastogi et al., 2015): a linear model which extends CCA to\nthree or more views. Unlike PCCA, GCCA does not condition two variables on the third shared one, but rather seeks to maximize the canonical correlations of all pairs of views. We also compare to Nonparametric CCA (NCCA) (Michaeli et al., 2016), and to a probabilistic variant of PCCA (PPCCA, Mukuta and Harada (2014)).\nFinally, we compare with the two recent models which operate in the setup most similar to ours: 1) Bridge Correlational Networks (BCN) (Rajendran et al., 2016); and 2) Image Pivoting (IMG PIVOT) from Gella et al. (2017). For both models, we report results only with the strongest variant based on the findings from the original papers, also verified by additional experimentation in our work.12\nHyperparameter Tuning The hyperparameters of the different models are tuned with a grid search over the following values: {2,3,4,5} for number of layers, {tanh, sigmoid, ReLU} as the activation functions (we use the same activation function in all the layers of the same network), {64,128,256} for minibatch size, {0.001,0.0001} for learning rate, and {128,256} for L (the size of the output vectors). The dimensions of all mid-layers are set to the input size. We use the Adam optimizer (Kingma and Ba, 2015), with the number of epochs set to 300.\nFor all participating models, we report test performance of the best hyperparameter on the validation set. For word similarity, following a standard practice (Levy et al., 2015; Vulić et al., 2017) we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa. The reported score is the average of the two halves. Similarity scores for all tasks were computed using the cosine similarity measure."
  }, {
    "heading": "7 Results and Discussion",
    "text": "Cross-lingual Image Description Retrieval We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retrieval task with all models are summarized in Table 2.\n12 More details about preprocessing and baselines (including all links to their code), are in the the supplementary material. We use original readily available implementations of all baselines whenever this is possible, and our in-house implementations for baselines for which no code is provided by the original authors.\nThe results clearly demonstrate the superiority of DPCCA (with a slight advantage to the more complex Variant B) and of the concatenation of their representation with that of the DCCA NOI (strongest) baseline. Furthermore, the non-deep, linear PCCA achieves strong results: it outscores all non-deep models, as well as all deep models except from DCCA NOI, IMG PIVOT in one case, and its deep version: DPCCA. This emphasizes our contribution in proposing PCCA for multilingual processing with images as a cross-lingual bridge.\nThe results suggest that: 1) the inclusion of visual information in the training process helps the retrieval task even without such information during inference. DPCCA outscores all DCCA variants (either alone or through a concatenation with the DCCA NOI representation), and PCCA outscores the original two-view CCA model; and 2) deep, non-linear architectures are useful: our DPCCA outperforms the linear PCCA model.\nWe also note clear improvements over the two recent models which also rely on visual information: IMG PIVOT and BCN. The gain over IMG PIVOT is observed despite the fact that IMG PIVOT is a more complex multi-modal model which relies on RNNs, and is tailored to sentence-level tasks. Finally, the scores from Table 2 suggest that improved performance can be achieved by an ensemble model, that is, a simple concatenation of DPCCA (B) and DCCA NOI.\nMultilingual Word Similarity The results, presented as standard Spearman’s rank correlation scores, are summarized in Table 3: we present fine-grained results over different POS classes for EN and DE, and compare them to the results from\na selection of strongest baselines. Further, Table 4 presents results on all SimLex word pairs. The POS class result patterns for EN-IT and EN-RU are very similar to the patterns in Table 3 and are provided in the supplementary material. First, the results over the initial monolingual embeddings before training (INIT EMB) clearly indicate that multilingual information is beneficial for the word similarity task. We observe improvements with all models (the only exception being extremely lowscoring PPCCA and NCCA, not shown). Moreover, by additionally grounding concepts from two languages in the visual modality it is possible to further boost word similarity scores. This result is in line with prior work in monolingual settings (Chrupała et al., 2015; Kiela and Bottou, 2014; Lazaridou et al., 2015), which have shown to profit from multi-modal features.\nThe results on the POS classes represented in SimLex-999 (nouns, verbs, adjectives, Table 3) form our main finding: conditioning the multilingual representations on a shared image leads to improvements in verb and adjective representations. While for nouns one of the DPCCA variants is the best performing model for both languages, the gaps from the best performing baselines are much smaller. This is interesting since, e.g., verbs are\nmore abstract than nouns (Hartmann and Søgaard, 2017; Hill et al., 2014). Considering the fact that SimLex-999 consists of 666 noun pairs, 222 verb pairs and 111 adjective pairs, this is the reason that the gains of DPCCA over the strongest baselines across the entire evaluation set are more modest (Table 4). We note again that the same patterns presented in Table 3 for EN-DE – more prominent verb and adjective gains and a smaller gain on nouns – also hold for EN-IT and EN-RU (see the supplementary material)."
  }, {
    "heading": "8 Conclusion and Future Work",
    "text": "We addressed the problem of utilizing images as a bridge between languages to learn improved bilingual text representations. Our main contribution is two-fold. First, we proposed to use the Partial CCA (PCCA) method. In addition, we proposed a stochastic optimization algorithm for the deep version of PCCA that overcomes the challenges posed by the covariance estimation required by the method. Our experiments reveal the effectiveness of these methods for both sentence-level and wordlevel tasks. Crucially, our proposed solution does not require access to images at inference/test time, in line with the realistic scenario where images that describe sentential queries are not readily available.\nIn future work we plan to improve our methods by exploiting the internal structure of images and sentences as well as by effectively integrating signals from more than two languages."
  }, {
    "heading": "Acknowledgments",
    "text": "IV is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). GR and RR are supported by the Infomedia Magnet Grant and by an AOL grant on ”connected experience technologies”."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep canonical correlation analysis",
    "authors": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu."],
    "venue": "Proceedings of ICML, pages 1247–1255.",
    "year": 2013
  }, {
    "title": "VQA: Visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "Lawrence C. Zitnick", "Devi Parikh."],
    "venue": "Proceedings of ICCV, pages 2425– 2433.",
    "year": 2015
  }, {
    "title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains",
    "authors": ["Raman Arora", "Karen Livescu."],
    "venue": "Proceedings of ICASSP, pages 7135–7139.",
    "year": 2013
  }, {
    "title": "The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora",
    "authors": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta."],
    "venue": "Language Resources and Evaluation, 43(3):209–226.",
    "year": 2009
  }, {
    "title": "Situating abstract concepts",
    "authors": ["Lawrence W. Barsalou", "Katja Wiemer-Hastings."],
    "venue": "D. Pecher and R. Zwaan, editors, Grounding cognition: The role of perception and action in memory, language, and thought, pages 129–163.",
    "year": 2005
  }, {
    "title": "Learning bilingual lexicons using the visual similarity of labeled web images",
    "authors": ["Shane Bergsma", "Benjamin Van Durme."],
    "venue": "Proceedings of IJCAI, pages 1764–1769.",
    "year": 2011
  }, {
    "title": "Multimodal distributional semantics",
    "authors": ["Elia Bruni", "Nam Khanh Tram", "Marco Baroni."],
    "venue": "Journal of Artificial Intelligence Research, 49:1–47.",
    "year": 2014
  }, {
    "title": "A limited memory algorithm for bound constrained optimization",
    "authors": ["Richard H Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu."],
    "venue": "SIAM Journal on Scientific Computing, 16(5):1190–1208.",
    "year": 1995
  }, {
    "title": "Incorporating global visual features into attention-based neural machine translation",
    "authors": ["Iacer Calixto", "Qun Liu."],
    "venue": "Proceedings of EMNLP, pages 992– 1003.",
    "year": 2017
  }, {
    "title": "Multilingual multi-modal embeddings for natural language processing",
    "authors": ["Iacer Calixto", "Qun Liu", "Nick Campbell."],
    "venue": "arXiv preprint arXiv:1702.01101.",
    "year": 2017
  }, {
    "title": "Correlational neural networks",
    "authors": ["Sarath Chandar", "Mitesh M Khapra", "Hugo Larochelle", "Balaraman Ravindran."],
    "venue": "Neural Computation, 28:257–285.",
    "year": 2016
  }, {
    "title": "Deep multi-view learning with stochastic decorrelation loss",
    "authors": ["Xiaobin Chang", "Tao Xiang", "Timothy M. Hospedales."],
    "venue": "CoRR, abs/1707.09669.",
    "year": 2017
  }, {
    "title": "Learning language through pictures",
    "authors": ["Grzegorz Chrupała", "Ákos Kádár", "Afra Alishahi."],
    "venue": "Proceedings of ACL, pages 112–118.",
    "year": 2015
  }, {
    "title": "Multilingual image description with neural sequence models",
    "authors": ["Desmond Elliott", "Stella Frank", "Eva Hasler."],
    "venue": "arXiv preprint arXiv:1510.04709.",
    "year": 2015
  }, {
    "title": "Multi30K: Multilingual EnglishGerman image descriptions",
    "authors": ["Desmond Elliott", "Stella Frank", "Khalil Sima’an", "Lucia Specia"],
    "venue": "In Proceedings of the 5th Workshop on Vision and Language,",
    "year": 2016
  }, {
    "title": "Every picture tells a story: Generating sentences from images",
    "authors": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."],
    "venue": "Proceedings of ECCV, pages 15–29.",
    "year": 2010
  }, {
    "title": "Improving vector space word representations using multilingual correlation",
    "authors": ["Manaal Faruqui", "Chris Dyer."],
    "venue": "Proceedings of EACL, pages 462– 471.",
    "year": 2014
  }, {
    "title": "Visual information in semantic representation",
    "authors": ["Yansong Feng", "Mirella Lapata."],
    "venue": "Proceedings of NAACL-HLT, pages 91–99.",
    "year": 2010
  }, {
    "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
    "authors": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."],
    "venue": "Proceedings of EMNLP, pages 457–468.",
    "year": 2016
  }, {
    "title": "Imagemediated learning for zero-shot cross-lingual document retrieval",
    "authors": ["Ruka Funaki", "Hideki Nakayama."],
    "venue": "Proceedings of EMNLP, pages 585–590.",
    "year": 2015
  }, {
    "title": "Unsupervised visual sense disambiguation for verbs using multimodal embeddings",
    "authors": ["Spandana Gella", "Mirella Lapata", "Frank Keller."],
    "venue": "Proceedings of NAACL-HLT, pages 182–192.",
    "year": 2016
  }, {
    "title": "Image pivoting for learning multilingual multimodal representations",
    "authors": ["Spandana Gella", "Rico Sennrich", "Frank Keller", "Mirella Lapata."],
    "venue": "Proceedings of EMNLP, pages 2839–2845.",
    "year": 2017
  }, {
    "title": "If sentences could see: Investigating visual information for semantic textual similarity",
    "authors": ["Goran Glavaš", "Ivan Vulić", "Simone Paolo Ponzetto."],
    "venue": "Proceedings of IWCS.",
    "year": 2017
  }, {
    "title": "The symbol grounding problem",
    "authors": ["Stevan Harnad."],
    "venue": "Physica D: Nonlinear Phenomena, 42(1–3).",
    "year": 1990
  }, {
    "title": "Limitations of cross-lingual learning from image search",
    "authors": ["Mareike Hartmann", "Anders Søgaard."],
    "venue": "CoRR, abs/1709.05914.",
    "year": 2017
  }, {
    "title": "Multi-modal models for concrete and abstract concept meaning",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "Transactions of the ACL, 2:285–296.",
    "year": 2014
  }, {
    "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "Computational Linguistics, 41(4):665–695.",
    "year": 2015
  }, {
    "title": "Multimodal pivots for image caption translation",
    "authors": ["Julian Hitschler", "Shigehiko Schamoni", "Stefan Riezler."],
    "venue": "Proceedings of ACL, pages 2399– 2409.",
    "year": 2016
  }, {
    "title": "Closed-form solution of absolute orientation using orthonormal matrices",
    "authors": ["Berthold K.P. Horn", "Hugh M. Hilden", "Shahriar Negahdaripour."],
    "venue": "Journal of Optical Society of America, 5(7):1127–1135.",
    "year": 1988
  }, {
    "title": "Generalized canonical correlations and their applications to experimental data",
    "authors": ["Paul Horst."],
    "venue": "Journal of Clinical Psychology, 17(4):331–347.",
    "year": 1961
  }, {
    "title": "Relations between two sets of variates",
    "authors": ["Harold Hotelling."],
    "venue": "Biometrika, 28(3/4):321–377.",
    "year": 1936
  }, {
    "title": "Attention-based multimodal neural machine translation",
    "authors": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer."],
    "venue": "Proceedings of WMT, pages 639–645.",
    "year": 2016
  }, {
    "title": "Optimal whitening and decorrelation",
    "authors": ["Agnan Kessy", "Alex Lewin", "Korbinian Strimmer."],
    "venue": "The American Statistician.",
    "year": 2017
  }, {
    "title": "MMFeat: A toolkit for extracting multi-modal features",
    "authors": ["Douwe Kiela."],
    "venue": "Proceedings of ACL System Demonstrations, pages 55–60.",
    "year": 2016
  }, {
    "title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics",
    "authors": ["Douwe Kiela", "Léon Bottou."],
    "venue": "Proceedings of EMNLP, pages 36–45.",
    "year": 2014
  }, {
    "title": "Improving multi-modal representations using image dispersion: Why less is sometimes more",
    "authors": ["Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark."],
    "venue": "Proceedings of ACL, pages 835–841.",
    "year": 2014
  }, {
    "title": "Comparing data sources and architectures for deep visual representation learning in semantics",
    "authors": ["Douwe Kiela", "Anita Lilla Verő", "Stephen Clark."],
    "venue": "Proceedings of EMNLP, pages 447–456.",
    "year": 2016
  }, {
    "title": "Visual bilingual lexicon induction with transferred ConvNet features",
    "authors": ["Douwe Kiela", "Ivan Vulić", "Stephen Clark."],
    "venue": "Proceedings of EMNLP, pages 148–158.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "Proceedings of ICLR (Conference Track).",
    "year": 2015
  }, {
    "title": "Multimodal neural language models",
    "authors": ["Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel."],
    "venue": "Proceedings of ICML, pages 595–603.",
    "year": 2014
  }, {
    "title": "Philosophy in the flesh: The embodied mind and its challenge to Western thought",
    "authors": ["George Lakoff", "Mark Johnson"],
    "year": 1999
  }, {
    "title": "Combining language and vision with a multimodal skip-gram model",
    "authors": ["Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni."],
    "venue": "Proceedings of NAACL-HLT, pages 153–163.",
    "year": 2015
  }, {
    "title": "Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics",
    "authors": ["Ira Leviant", "Roi Reichart."],
    "venue": "CoRR, abs/1508.00106.",
    "year": 2015
  }, {
    "title": "Improving distributional similarity with lessons learned from word embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."],
    "venue": "Transactions of the ACL, 3:211–225.",
    "year": 2015
  }, {
    "title": "ORANGE: A method for evaluating automatic evaluation metrics for machine translation",
    "authors": ["Chin-Yew Lin", "Franz Josef Och."],
    "venue": "Proceedings of COLING, pages 501–507.",
    "year": 2004
  }, {
    "title": "Symbol interdependency in symbolic and embodied cognition",
    "authors": ["Max M. Louwerse."],
    "venue": "Topics in Cognitive Science, 59(1):617–645.",
    "year": 2011
  }, {
    "title": "Deep multilingual correlation for improved word embeddings",
    "authors": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Proceedings of NAACL-HLT, pages 250–256.",
    "year": 2015
  }, {
    "title": "Deep captioning with multimodal recurrent neural networks (mRNN)",
    "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."],
    "venue": "Proceedings of ICLR (Conference Track).",
    "year": 2015
  }, {
    "title": "Explain images with multimodal recurrent neural networks",
    "authors": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille."],
    "venue": "arXiv preprint arXiv:1410.1090.",
    "year": 2014
  }, {
    "title": "Nonparametric canonical correlation analysis",
    "authors": ["Tomer Michaeli", "Weiran Wang", "Karen Livescu."],
    "venue": "Proceedings of ICML, pages 1967–1976.",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of ICLR (Conference Track).",
    "year": 2013
  }, {
    "title": "Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints",
    "authors": ["Nikola Mrkšić", "Ivan Vulić", "Diarmuid Ó Séaghdha", "Ira Leviant", "Roi Reichart", "Milica Gašić", "Anna Korhonen", "Steve Young."],
    "venue": "Transactions",
    "year": 2017
  }, {
    "title": "Probabilistic partial canonical correlation analysis",
    "authors": ["Yusuke Mukuta", "Harada."],
    "venue": "Proceedings of ICML, pages 1449–1457.",
    "year": 2014
  }, {
    "title": "Zeroresource machine translation by multimodal encoder–decoder network with multimedia pivot",
    "authors": ["Hideki Nakayama", "Noriki Nishida."],
    "venue": "Machine Translation, 31(1-2):49–64.",
    "year": 2017
  }, {
    "title": "Bridge correlational neural networks for multilingual multimodal representation learning",
    "authors": ["Janarthanan Rajendran", "Mitesh M. Khapra", "Sarath Chandar", "Balaraman Ravindran."],
    "venue": "Proceedings of NAACL-HLT, pages 171–181.",
    "year": 2016
  }, {
    "title": "Partial canonical correlations",
    "authors": ["B. Raja Rao."],
    "venue": "Trabajos de estadistica y de investigación operativa, 20(2-3):211–219.",
    "year": 1969
  }, {
    "title": "Multiview LSA: Representation learning via generalized CCA",
    "authors": ["Pushpendre Rastogi", "Benjamin Van Durme", "Raman Arora."],
    "venue": "Proceedings of NAACLHLT, pages 556–566.",
    "year": 2015
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["Karen Simonyan", "Andrew Zisserman."],
    "venue": "Proceedings of ICLR (Workshop Track).",
    "year": 2015
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "Proceedings of CVPR, pages 3156–3164.",
    "year": 2015
  }, {
    "title": "Multi-modal representations for improved bilingual lexicon learning",
    "authors": ["Ivan Vulić", "Douwe Kiela", "Stephen Clark", "MarieFrancine Moens."],
    "venue": "Proceedings of ACL, pages 188–194. ACL.",
    "year": 2016
  }, {
    "title": "Automatic selection of context configurations for improved class-specific word representations",
    "authors": ["Ivan Vulić", "Roy Schwartz", "Ari Rappoport", "Roi Reichart", "Anna Korhonen."],
    "venue": "Proceedings of CoNLL, pages 112–122.",
    "year": 2017
  }, {
    "title": "On deep multi-view representation learning",
    "authors": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes."],
    "venue": "Proceedings of ICML, pages 1083– 1092.",
    "year": 2015
  }, {
    "title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations",
    "authors": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Nathan Srebro."],
    "venue": "Proceedings of Communication, Control, and Computing, pages 688–695.",
    "year": 2015
  }, {
    "title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
    "authors": ["Huijuan Xu", "Kate Saenko."],
    "venue": "Proceedings of ECCV, pages 451–466.",
    "year": 2016
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "Proceedings of ICML, pages 2048–2057.",
    "year": 2015
  }, {
    "title": "Deep correlation for matching images and text",
    "authors": ["Fei Yan", "Krystian Mikolajczyk."],
    "venue": "Proceedings of CVPR, pages 3441–3450.",
    "year": 2015
  }, {
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the ACL, 2:67–78.",
    "year": 2014
  }],
  "id": "SP:eccf4d7109d2f3354967b95b58962ce38cb6f0b8",
  "authors": [{
    "name": "Guy Rotman",
    "affiliations": []
  }, {
    "name": "Ivan Vulić",
    "affiliations": []
  }, {
    "name": "Roi Reichart",
    "affiliations": []
  }],
  "abstractText": "We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.1",
  "title": "Bridging Languages through Images with Deep Partial Canonical Correlation Analysis"
}