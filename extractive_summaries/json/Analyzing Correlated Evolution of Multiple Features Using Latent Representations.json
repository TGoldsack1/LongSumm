{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4371‚Äì4382 Brussels, Belgium, October 31 - November 4, 2018. c¬©2018 Association for Computational Linguistics\n4371"
  }, {
    "heading": "1 Introduction",
    "text": "Research on structural properties (typological features) of language, such as the order of subject, object and verb (examples are SOV and SVO) and the presence or absence of tone, is largely synchronic in nature. Since languages of the world exhibit an astonishing diversity, the sample of languages used in a typical typological study is selected from a diverse set of language families and from various geographical regions. Not surprisingly, most of them lack historical documentation that allows us to directly trace their evolutionary history.\nAt the same time, however, typologists have long struggled to dynamicize synchronic typology, or to infer diachronic universals of change from current cross-linguistic variation (Greenberg, 1978; Nichols, 1992; Maslova, 2000; Bickel, 2013). They have also tried to uncover deep historical relations between languages (Nichols, 1992).\nOne of the main developments in diachronic typology in the last decade has been the application of powerful statistical tools borrowed from the\nfield of evolutionary biology (Dediu, 2010; Greenhill et al., 2010; Dunn et al., 2011; Maurits and Griffiths, 2014; Greenhill et al., 2017). As illustrated in Figure 1, the key idea is that if a phylogenetic tree is given, we can infer the ancestral states with varying degrees of confidence, and by extension, can induce diachronic universals of change. To perform statistical inference, we assume that each feature evolves along the branches of the tree according to a continuous-time Markov chain (CTMC) model, which is controlled by a transition rate matrix (TRM). Once TRMs are estimated, we can gain insights from them, for example, by simulating language evolution (Maurits and Griffiths, 2014).\nOne problem in previous studies is that they do not adequately model a characteristic of typological features that has been central to linguistic typology, that is, the fact that these features are not independent but depend on each other (Greenberg, 1963; DaumeÃÅ III and Campbell, 2007). For example, if a language takes a verb before an object (VO), then it takes postnominal relative clauses\n1 1 0 0‚Ä¶ 2 1 ‚Ä¶ 3 features xùëôùëô,‚àóparameters zùëôùëô,‚àó\nStep 1. Map each language into the latent representation\nStep 2. Infer a set of transition rate matrices using phylo. trees (also infer the states and dates of the internal nodes)\ninfer\n(NRel) (VO ‚Üí NRel, in shorthand), and a related universal, RelN ‚Üí OV, also holds (Dryer, 2011). Despite the long-standing interest in interfeature dependencies, most statistical models assume independence between features (DaumeÃÅ III, 2009; Dediu, 2010; Greenhill et al., 2010, 2017; Murawaki, 2016; Murawaki and Yamauchi, 2018). A rare exception is Dunn et al. (2011), who extended Greenberg‚Äôs idea by applying a phylogenetic model of correlated evolution (Pagel and Meade, 2006). However, the model adopted by Dunn et al. (2011) can only handle the dependency between a pair of binary features. Typological features have two or more possible values in general, and more importantly, the dependencies between features are not limited to a pair (Itoh and Ueda, 2004). For example, the order of relative clauses has connections to the order of adjective and noun (AdjN or NAdj), in addition to the order of object and verb, as two universals, RelN ‚Üí AdjN and NAdj ‚Üí NRel, are known to hold well (Dryer, 2011).\nIn this paper, we propose latent representationbased analysis of diachronic typology. Figure 2 shows an overview of our framework. Follow-\ning Murawaki (2017), we assume that a sequence of discrete surface features that represents a language is generated from a sequence of binary latent variables called parameters (Step 1). Parameters are, by assumption, independent of each other and switching one parameter entails multiple changes of surface features in general. Thus, by performing phylogenetic inference on the latent space, we can handle the dependencies of all available features in an implicit manner (Step 2). The latent parameter representation can be projected back to the surface feature representation when needed for analysis. Like Maurits and Griffiths (2014), we run simulation experiments to interpret the estimated model parameters (Step 3).\nWhat we propose is a general framework with which we can analyze any discrete feature, but as a proof-of-concept demonstration, we follow Maurits and Griffiths (2014) in focusing on the order of subject, object and verb (hereafter simply referred to as basic word order or BWO).1 In the dataset we use, the BWO feature has 7 possible values, 6 logically possible orders plus the special value No dominant order (Dryer, 2013b), meaning that it cannot be analyzed directly with Dunn et al.‚Äôs model. We show that languages sharing the same word order are not a coherent group but exhibit varying degrees of diachronic stability depending on other features."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Statistical Diachronic Typology",
    "text": "The building block of statistical phylogenetic models2 is a time-tree, which places nodes on an axis of time. In their standard applications to language (Gray and Atkinson, 2003; Bouckaert et al., 2012), time-trees are inferred from cognate\n1 We chose the BWO feature because it is appealing to a wider audience. We are aware that Matthew S. Dryer, who provided language data for the BWO feature, favors binary classifications (OV vs. VO and SV vs. VS) over the six-way classification (Dryer, 1997, 2013a). He argues that the binary classifications are more fundamental than the six-way classification, but our latent representation-based analysis does not require feature values to be primitive in nature because it reorganizes feature values into various latent parameters.\n2 Statistical phylogenetic models can be either distancebased and character-based. Character-based models are classified into parsimony-based and likelihood-based. In this paper, we focus on likelihood-based Bayesian models for their ability to date internal nodes. However, it is worth noting that attempts to overcome the limitations of the tree model mostly rely on non-likelihood-based models (Nakhleh et al., 2005; Nelson-Sathi et al., 2010).\ndata (Dyen et al., 1992; Greenhill et al., 2008).3 However, if a tree is given a priori, phylogenetic models can also be used to estimate the parameters of a TRM, which controls how languages change their feature values over time. This is how typological features are analyzed in previous studies.\nDediu (2010) aggregated TRMs taken from various families to measure the stability of features. Greenhill et al. (2010) compared typological data with cognate data in terms of stability. Maurits and Griffiths (2014) focused on the BWO feature and analyzed how it had changed in the past and was likely to change in the future. Dunn et al. (2011) estimated TRMs for pairs of binary features and found that perceived correlated evolution was mostly lineage-specific rather than universal.\nTaking a closer look at these studies, we can see that they vary as to how to prepare trees, as summarized in Table 1. Leaf nodes are assumed to be at the present date t = 0, but how can we assign backward dates t to internal nodes? A popular approach (Greenhill et al., 2010; Dunn et al., 2011) is to construct a time-tree with absolute (calendar) dates, using binary-coded lexical cognate data, and then to fit each trait of interest independently on the time-tree.4\nHowever, cognate data are available only for a handful of language families such as IndoEuropean, Austronesian and Niger-Congo (or its mammoth Bantu branch). Moreover, phylogenetic\n3 See Pereltsvaig and Lewis (2015) for a criticism of computational approaches to historical linguistics and Chang et al. (2015) for an elegant solution to a set of problems commonly found in inferred time-trees.\n4To be precise, a set of tree samples given by MCMC sampling is usually employed to account for uncertainty.\ninference was performed separately one after the other. This marks a sharp contrast with the long tradition of testing against a worldwide sample. In fact, it is suggested that sample diversity and aggregate time depth are not large enough to draw meaningful conclusions (Croft et al., 2011; Levy and DaumeÃÅ III, 2011).\nFor this reason, we take another approach, which was employed by Dediu (2010). He used language families established by historical linguists. Because such tree topologies are not associated with dates, he inferred the dates of internal nodes together with the states of internal nodes and TRMs. This was possible because he jointly fitted a sequence of traits, instead of fitting each trait independently. If multiple traits are combined, they provide considerable information on a branch length, or the time elapsing from a parent to a child, because the elapsed time is roughly inversely proportional to the similarity between the two nodes.5\nOur approach differs from Dediu‚Äôs mainly in two points. First, whereas Dediu (2010) performed posterior inference separately for each language family, we tie a single set of TRMs to all available language families. Second, Dediu (2010) only inferred relative dates because he did not perform calibration (Drummond and Bouckaert, 2015). In order to assign calendar dates to nodes, we use multiple calibration points (the clock in Figure 2 indicates a calibration point). As is com-\n5 Although some previous studies adopted relaxed clock models, in which different branches have different rates of evolution (Drummond and Bouckaert, 2015), we use the simple strict clock model because our calibration points are not large enough in number to harness the very flexible models.\nmonly done in the cognate-based reconstruction of a time-tree (Bouckaert et al., 2012), we set the Gaussian, Gaussian mixture, log-normal and uniform distributions as priors on the dates of the corresponding internal nodes."
  }, {
    "heading": "2.2 Latent Representations of Languages",
    "text": "While previous studies analyzed the evolution of a single categorical feature (Dediu, 2010; Greenhill et al., 2010; Maurits and Griffiths, 2014) or a pair of binary features (Dunn et al., 2011), we capture the dependencies of all available features by mapping each language to a sequence of independent latent variables. To our knowledge, Murawaki (2015) was the first to introduce latent representations to typological features. Pointing out several critical problems, however, Murawaki (2017) superseded the earlier model. The present study is built on top of a slightly modified version of the Bayesian model presented by Murawaki (2017).\nLike the present study, Murawaki (2015) performed phylogenetic inference on the latent space. However, since this model lacks the notion of time, it does not have descriptive power beyond clustering. Borrowing statistical models from the field of evolutionary biology, we perform timeaware inference."
  }, {
    "heading": "3 Proposed Method",
    "text": ""
  }, {
    "heading": "3.1 Latent Representations of Languages",
    "text": "Central to our framework of diachronic analysis are the latent representations of languages (Murawaki, 2017). Each language l is represented as a sequence of N discrete features xl,‚àó = (xl,1, ¬∑ ¬∑ ¬∑ , xl,N ) ‚àà NN0 . xl,n can take a binary value (xl,n ‚àà {0, 1}) or categorical value (xl,n ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , Fn}, where Fn is the number of distinct values). We assume that xl,‚àó is stochastically generated from its latent representation, zl,‚àó = (zl,1, ¬∑ ¬∑ ¬∑ , zl,K) ‚àà {0, 1}K , where K is the number of binary parameters, which is given a priori.\nDependencies between surface features are captured by weight matrix W ‚àà RK√óM . M will be described below. In the generative story, we first calculate feature score vector Œ∏ÃÉl,‚àó = (zTl,‚àóW )\nT ‚àà RM . We then obtain model parameter vector Œ∏l,‚àó ‚àà (0, 1)M by normalizing Œ∏ÃÉl,‚àó for each feature type n. We use the sigmoid function for binary features,\nŒ∏l,f(n,1) = 1\n1 + exp(‚àíŒ∏ÃÉl,f(n,1)) , (1)\nand the softmax function for categorical features,\nŒ∏l,f(n,i) = exp(Œ∏ÃÉl,f(n,i))‚àëFn i‚Ä≤=1 exp(Œ∏ÃÉl,f(n,i‚Ä≤)) . (2)\nNote that while a binary feature corresponds to one model parameter, categorical feature n is tied to Fn model parameters. We use function f(n, i) ‚àà {1, ¬∑ ¬∑ ¬∑ ,m, ¬∑ ¬∑ ¬∑ ,M} to map feature n to the corresponding model parameter index. Finally, we draw a binary feature from Bernoulli(Œ∏l,f(n,1)), and a categorical feature from Categorical(Œ∏l,f(n,1), ¬∑ ¬∑ ¬∑ , Œ∏l,f(n,Fn)).\nTo gain an insight into how W captures interfeature dependencies, suppose that for parameter k, a certain group of languages take zl,k = 1. If two categorical feature values (n1, i1) and (n2, i2) have large positive weights (i.e., wk,f(n1,i1) > 0 and wk,f(n2,i2) > 0), then the pair must often cooccur in these languages because W raises both Œ∏l,f(n1,i1) and Œ∏l,f(n2,i2). Likewise, the fact that two feature values do not co-occur can be encoded as a positive weight for one value and a negative weight for the other.\nThe remaining question is how zl,k is generated. We draw z‚àó,k = (z1,k, ¬∑ ¬∑ ¬∑ , zL,k) from an autologistic model (Besag, 1974) that incorporates the observation that phylogenetically or areally close languages tend to take the same value.\nTo complete the generative story, letX andZ be the matrices of languages in the surface and latent representations, respectively, and let A be a set of latent variables controllingK autologistic models. The joint distribution is defined as\nP (A,Z,W,X)=P (A)P (Z|A)P (W )P (X|Z,W ),\nwhere hyperparameters are omitted for brevity. For prior probabilities P (A) and P (W ), please refer to Murawaki (2017).\nEven if less than 30% of the items of X are present, this model has been demonstrated to recover missing values reasonably well. Also, when plotted on a world map, some parameters appear to retain phylogenetic and areal signals observed for surface features, indicating that they are not mere statistical artifacts (Murawaki, 2017)."
  }, {
    "heading": "3.2 Transition Rate Matrices (TRMs)",
    "text": "We assume that each parameter k independently evolves along the branches of trees according to a continuous-time Markov chain (CTMC)\nmodel (Drummond and Bouckaert, 2015). The CTMC is a continuous extension to the more familiar discrete-time Markov chain. It is is controlled by a TRMQk. If the number of states (possible values) is 2, then Qk is a 2√ó 2 matrix:\nQk = ( ‚àíŒ±k Œ±k Œ≤k ‚àíŒ≤k ) .\nWe set Gamma priors on Œ±k, Œ≤k > 0. Qk can be used to calculate the transition probability, or the probability of language l taking value b for parameter k conditioned on l‚Äôs parent œÄ(l) and t, the time span between the two:\nP (zl,k = b|zœÄ(l),k = a, t) = exp(tQk)a,b. (3)\nThe matrix exponential exp(tQk) can be solved analytically if Qk is a 2√ó 2 matrix:\nexp(tQk)=\n( Œ≤k+Œ±ke ‚àí(Œ±k+Œ≤k)t\nŒ±k+Œ≤k Œ±k‚àíŒ±ke‚àí(Œ±k+Œ≤k)t\nŒ±k+Œ≤k Œ≤k‚àíŒ≤ke‚àí(Œ±k+Œ≤k)t\nŒ±k+Œ≤k\nŒ±k+Œ≤ke ‚àí(Œ±k+Œ≤k)t\nŒ±k+Œ≤k\n) .\nAs t approaches to infinity, we obtain the stationary probability ( Œ≤kŒ±k+Œ≤k , Œ±k Œ±k+Œ≤k\n)T. We can see that Œ±k and Œ≤k control both the speed of change (the larger the higher) and the stationary distribution.\nA root node has no parent by definition. We draw the state of a root node from the stationary distribution. Thus, language isolates do have impact on posterior inference of TRMs."
  }, {
    "heading": "3.3 Posterior Inference of Time-trees",
    "text": "To estimate TRMs, we need to specify the generative model of time-trees and an inference algorithm. In the generative story, each tree topology is drawn from some uniform distribution. The dates of its nodes are determined next. If the node in question is not a calibration point, its date is drawn from some uniform distribution, subject to the ancestral ordering constraint: a node must be older than its descendants. If the node is a calibration point, its date is drawn from the corresponding prior distribution.6 TRM parameters, Œ±k and Œ≤k, are generated from Gamma priors. For the root node, the value of parameter k is drawn from the corresponding stationary distribution. The states of the non-root nodes are generated using Eq. (3).\nGiven tree topologies, the states of the leaf nodes and calibration points, we need to infer\n6This model is slightly leaky because some priors (e.g., Gaussian) assign non-zero probabilities to illogical time-trees that violate the ancestral ordering constraint.\n(1) the dates of the internal nodes, (2) the states of the internal nodes, and (3) TRM parameters, Œ±k and Œ≤k, for each latent parameter k. Gibbs sampling updates of these variables are as follows:\nUpdate dates We update the dates of the internal nodes one by one. The time span in which the target node can move is bound by its parent (if there is) and its eldest child. We use slice sampling (Neal, 2003) to update the date. In addition, we use a Metropolis-Hastings operator that multiplies the dates of all the internal nodes of a tree by a rate drawn from a log-normal distribution.\nUpdate states For each parameter k, we blocksample a whole given tree. Specifically, we implement a Bayesian version of Felsenstein‚Äôs tree-pruning algorithm, which is akin to the forward filtering-backward sampling algorithm for Bayesian hidden Markov models.\nUpdate Œ±k and Œ≤k We jointly sample Œ±k and Œ≤k for each k. Since both the transition and stationary probabilities can be obtained analytically for binary traits, we use Hamiltonian Monte Carlo to exploit gradient information (Neal, 2011)."
  }, {
    "heading": "3.4 Three-Step Analysis",
    "text": "Now we are ready to elaborate on the proposed framework of diachronic analysis (Figure 2).\nStep 1 We map each language, represented as a sequence of N discrete surface features, to a sequence of K binary latent parameters. Let feature matrix X be decomposed into observed and missing portions, Xobs and Xmis, respectively. Given Xobs, we use Gibbs sampling to infer A, parameter matrix Z, weight matrix W , and Xmis (Murawaki, 2017).7\nIn the present study, we set K = 100. We run 5 independent MCMC chains. For each chain, we start with 1,000 burn-in iterations. We then obtain 10 samples with an interval of 10 iterations. Note that after burn-in iterations, we fix W and only sample A, Z, and Xmis to avoid the identifiability problems (e.g., label-switching). For each item of Z and Xmis, we output the most frequent value among the 10 samples. We do this to reduce uncertainty.\n7 We employ a slightly modified Metropolis-Hastings operator to improve the mobility of Z.\nStep 2 We fit a set of K TRMs on family trees around the world. Formally, what are observed are tree topologies, the states of the leaf nodes (i.e., sequences of latent parameters), and multiple calibration points. Given these, we infer TRM parameters, Œ±k and Œ≤k, for each latent parameter k, as well as the dates and states of the internal nodes. We, again, use Gibbs sampling as explained in Section 3.3.\nWe collect 10 samples with an interval of 10 iterations after 1,000 burn-in iterations. We do this for each of the 5 samples obtained in Step 1. As a result, we obtain 50 samples in total.\nStep 3 We analyze the TRMs by simulating language evolution. Given the latent representation of language l, we stochastically generate its descendant l‚Ä≤ after some time span t. Specifically, we draw zl‚Ä≤,k according to the transition probability of Eq. (3) for each parameter k. Using weight matrix W , we then project the latent representation zl‚Ä≤,‚àó back to the surface representation xl‚Ä≤,‚àó. To be precise, we use model parameter vector Œ∏l‚Ä≤,‚àó, instead of xl‚Ä≤,‚àó, for further analysis. For each of the 50 samples obtained in Step 2, we simulate the evolution of a given language 100 times (5,000 samples in total)."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Data and Preprocessing",
    "text": "The database of typological features we used is the online edition8 of the World Atlas of Language Structures (WALS) (Haspelmath et al., 2005). We preprocessed the database as was done in Murawaki (2017), with different thresholds. As a result, we obtained a language‚Äìfeature matrix consisting of L = 2,607 languages and N = 152 features. Only 19.98% of items in the matrix were present. We manually classified features into binary and categorical ones. The number of model parameters, M , was 760.\nWe used Glottolog 3.2 (HammarstroÃàm et al., 2018) as the source of family trees. Glottolog has three advantages over Ethnologue (Lewis et al., 2014), another commonly-used catalog of the world‚Äôs languages. (1) Glottolog makes explicit that it adopts a genealogical classification, rather than hierarchical clusterings of modern languages. (2) It reflects more recent research. (3) Mapping between Glottolog and WALS is easy be-\n8http://wals.info/\ncause WALS provides Glottolog‚Äôs language codes (glottocodes) when available.\nAfter Step 1 of Section 3.4, we dropped languages from WALS that could not be mapped to Glottolog. As a result, 2,557 languages remained. We subdivided a Glottolog node if multiple languages from WALS shared the same glottocode. We removed leaf nodes that were not present in WALS and repeatedly dropped internal nodes that had only one child. We obtained 309 language families among which 154 had only one node (i.e., language isolates).\nWe collected 50 calibration points from secondary literature (Holman et al., 2011; Bouckaert et al., 2012; Gray et al., 2009; Maurits and Griffiths, 2014; Grollemund et al., 2015). For example, we set a Gaussian prior with mean 2,500 BP (before present) and standard deviation 500 on the date of (Proto-)Hmong-Mien. See Table S.1 of the supplementary materials for details. Our calibration points are by no means definitive or exhaustive but should be seen as a first step toward worldscale dating."
  }, {
    "heading": "4.2 Case Study: Basic Word Order (BWO)",
    "text": "As a proof-of-concept demonstration of the proposed framework, we investigate the BWO feature, or WALS‚Äôs Feature 81A (Dryer, 2013b). The cross-linguistic variation of BWO attracts attention not only from typologists but from psycholinguists (see Maurits and Griffiths (2014) for a brief review). Some claim that the fact that SOV is the most frequent order indicates its optimality, presumably in terms of functionality. Some others point to an apparent historical trend of SOV changing to SVO, but not vice versa (Gell-Mann and Ruhlen, 2011), which might imply (1) the functional superiority of SVO over SOV and (2) an even higher prevalence of SOV in the past. SOV preferences in emerging sign languages (Sandler et al., 2005) and in elicited pantomime (Goldin-Meadow et al., 2008) are also reported.\nMaurits and Griffiths (2014) fitted a 6√ó6 TRM9 on large language families. However, we suspect that singling out BWO is oversimplification. Given its profound effect on the whole grammatical system, a BWO change can hardly occur independently of other features. In fact, Mithun (1995) lists a variety of morphological factors that have\n9The special value No dominant order was removed in their experiments.\ndiachronically reduced the rigidity of SOV order in Native American languages. Her analysis suggests that languages sharing the same word order might not be a monolithic group.\nHere, we use latent representation-based analysis to answer questions: how variable language sharing the same BWO are with respect to diachronic stability, and what kind of features are correlated with BWO stability?"
  }, {
    "heading": "4.3 Variability of Diachronic Stability",
    "text": "Among the 2,557 modern languages, we chose 1,357 languages for which the BWO feature was present. We simulated evolution with t = 2,000, as described in Section 3.4. Let n be the index of the BWO feature. For the 5,000 samples of each simulated language l‚Ä≤, we averaged the BWO probability vectors, Œ∏l‚Ä≤,f(n,1), ¬∑ ¬∑ ¬∑ , Œ∏l‚Ä≤,f(n,Fn).\nBefore going into inter-language variability, let us take a look at the overall trend. We took the average of the BWO probability vectors for each word order. The result is shown in Figure 3. Our findings largely agree with those of Maurits and Griffiths (2014): (1) SOV is the most diachronically stable word order, which is followed by SVO, (2) SOV prefers changing to SVO over VSO (although hardly visually recognizable), and (3) VSO is more likely to change to SVO than to SOV, just to name a few.\nNext, the variability is visualized in Figure 4. We can see that languages sharing the same word order differ considerably in terms of diachronic\nstability. For comparison, we fitted the 7√ó7 TRM of the BWO feature on the samples of time-trees obtained in Step 2 of Section 3.4. For SOV and SVO, the probabilities based on the surface feature pointed to the modal probabilities based on the latent representations. This is somewhat surprising because we anticipated that the combination of the stochastic surface-to-latent and latentto-surface mappings would amplify uncertainty of estimation.\nThe two least common word orders, OVS (0.8%) and OSV (0.3%) exhibited huge gaps between the two types of probabilities. The probabilities based on the surface feature were consistently larger (i.e., more stable). Surface featurebased estimation had no other way to explain the presence of these uncommon word orders than slowing down the convergence to the stationary distribution (otherwise they go extinct). Maurits and Griffiths (2014) also reported some counterintuitive results regarding OVS and OSV. By contrast, latent parameter-based estimation appears to have explained the low frequencies partly with the stochasticity of observation associated with the latent-to-surface mapping.\nNow we attempt to explain the variability of diachronic stability. Although we have all model parameters in hand, it is not easy to manually ana-\nlyze their complex dependencies. The approach we adopt in the present study is to let a simpler model explain the model‚Äôs complex behavior. Specifically, we used linear regression with L1 regularization (i.e., lasso). The hyperparameter was tuned using 3-fold cross-validation. For each word order i, the target variable was the average of Œ∏l‚Ä≤,f(n,i) while explanatory variables were the current surface features, xl,1, ¬∑ ¬∑ ¬∑ , xl,N . For better interpretability, we excluded from explanatory variables surface features that trivially depended on the BWO feature (Takamura et al., 2016). Note that missing values were imputed in Step 1 of Section 3.4.\nTables 2 and 3 show the results of regression analysis for SOV and SVO languages, respectively. As expected, feature values typically associated with the specified word order had positive weights while negative weights indicate inconsistency. A stable SOV language may use prenominal relative clauses, postpositions and/or case suffixes. The trend was less clear for SVO languages, but those characterized by heavy use of prefixes were stable too. Interestingly, Feature 85A (Order of Adposition and Noun Phrase) had two positively weighted values: Prepositions and No adpositions. We speculate that SVO order is suitable for analytic languages that rely heavily on word ordering to encode syntactic structure (e.g., English and languages of Mainland Southeast Asia) but is not necessarily so for languages with rich morphological devices for marking syn-\ntactic structure so that word ordering can relatively freely convey information structure."
  }, {
    "heading": "4.4 Language-Specific Analysis",
    "text": "In Section 4.3, we suggested that stable SVO languages do not form a coherent group but can be grouped into at least two clusters. This can be confirmed in Table 4, where most of the most stable SVO languages exhibit either (1) little affixation or (2) strong prefixation. To analyze these languages in detail, we performed language-specific simulations. We chose Tetum and South-Central Kikongo as the examples of analytic and strongly prefixing languages, respectively.\nFigure 5 shows the word order probabilities of\nTetum as a function of time. For each time t, we performed simulation 500 times for each of the 50 samples and took the average of the BWO probability vectors, Œ∏l‚Ä≤,f(n,1), ¬∑ ¬∑ ¬∑ , Œ∏l‚Ä≤,f(n,Fn). According to our analysis, Tetum will remain SVO with a probability of 81.1% at t = 2,000. SVO was followed by SOV (8.4%) and No dominant order (5.3%).\nWhat will the Austronesian language of East Timor look like in the future, if it switches to SOV? To answer this question, we performed regression analysis again with t = 2,000. For each word order i, the target variable was Œ∏l‚Ä≤,f(n,i) of each sample of simulated language l‚Ä≤ whereas explanatory variables were the items of the probability vector Œ∏l‚Ä≤,‚àó. In other words, we aimed at finding out features that were characteristic of the specified word order. As before, we removed surface features with trivial dependencies on the BWO feature (Takamura et al., 2016) as well as the BWO feature itself.\nTable 5 shows the result of regression analysis. If the relatively analytic language switches to SOV, Tetum will be characterized by a holistic reconfiguration. It is likely to develop suffixes and to replace prepositions with postposi-\ntions. South-Central Kikongo is analyzed in the same manner, as shown in Figure 6 and Table 6. The Bantu language of Africa is markedly different from Tetum as it is characterized by a higher tendency to switch to No dominant order."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we presented a new framework of latent representation-based analysis of diachronic typology, which enables us to investigate correlated evolution of multiple surface features in an exploratory manner. We focused on the order of subject, object and verb as a proof-of-concept demonstration, but investigating other features would be fruitful too. We analyzed the estimated model parameters with simulation experiments. In the future, we would like to investigate the inferred trees in detail.10 The source code is publicly available at https://github.com/murawaki/ lattyp."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was partly supported by JSPS KAKENHI Grant Number 18K18104.\n10 A preliminary analysis is presented in Section S.2 of the supplementary materials."
  }],
  "year": 2018,
  "references": [{
    "title": "Spatial interaction and the statistical analysis of lattice systems",
    "authors": ["Julian Besag."],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 192‚Äì236.",
    "year": 1974
  }, {
    "title": "Distributional biases in language families",
    "authors": ["Balthasar Bickel."],
    "venue": "David A. Peterson and Alan Timberlake, editors, Language Typology and Historical Contingency, pages 415‚Äì444. John Benjamins.",
    "year": 2013
  }, {
    "title": "Mapping the origins and expansion of the Indo-European lan",
    "authors": ["Remco Bouckaert", "Philippe Lemey", "Michael Dunn", "Simon J. Greenhill", "Alexander V. Alekseyenko", "Alexei J. Drummond", "Russell D. Gray", "Marc A. Suchard", "Quentin D. Atkinson"],
    "year": 2012
  }, {
    "title": "Ancestry-constrained phylogenetic analysis supports the Indo-European steppe hypothesis",
    "authors": ["Will Chang", "Chundra Cathcart", "David Hall", "Andrew Garrett."],
    "venue": "Language, 91(1):194‚Äì244.",
    "year": 2015
  }, {
    "title": "Greenbergian universals, diachrony, and statistical analyses",
    "authors": ["William Croft", "Tanmoy Bhattacharya", "Dave Kleinschmidt", "D. Eric Smith", "T. Florian Jaeger."],
    "venue": "Linguistic Typology, 15(2):433‚Äì453.",
    "year": 2011
  }, {
    "title": "Non-parametric Bayesian areal linguistics",
    "authors": ["Hal Daum√© III."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 593‚Äì601.",
    "year": 2009
  }, {
    "title": "A Bayesian model for discovering typological implications",
    "authors": ["Hal Daum√© III", "Lyle Campbell."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 65‚Äì72.",
    "year": 2007
  }, {
    "title": "A Bayesian phylogenetic approach to estimating the stability of linguistic features and the genetic biasing of tone",
    "authors": ["Dan Dediu."],
    "venue": "Proceedings of the Royal Society of London B: Biological Sciences, 278(1704):474‚Äì479.",
    "year": 2010
  }, {
    "title": "Bayesian Evolutionary Analysis with BEAST",
    "authors": ["Alexei J. Drummond", "Remco R. Bouckaert."],
    "venue": "Cambridge University Press.",
    "year": 2015
  }, {
    "title": "On the six-way word order typology",
    "authors": ["Matthew S. Dryer."],
    "venue": "Studies in Language. International Journal sponsored by the Foundation ‚ÄúFoundations of Language‚Äù, 21(1):69‚Äì103.",
    "year": 1997
  }, {
    "title": "The evidence for word order correlations: a response to Dunn, Greenhill, Levinson and Gray‚Äôs paper in Nature",
    "authors": ["Matthew S. Dryer."],
    "venue": "Linguistic Typology, 15:335‚Äì380.",
    "year": 2011
  }, {
    "title": "On the six-way word order typology, again",
    "authors": ["Matthew S. Dryer."],
    "venue": "Studies in Language. International Journal sponsored by the Foundation ‚ÄúFoundations of Language‚Äù, 37(2):267‚Äì301.",
    "year": 2013
  }, {
    "title": "Order of subject, object and verb",
    "authors": ["Matthew S. Dryer."],
    "venue": "Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online. Max Planck Institute for Evolutionary Anthropology.",
    "year": 2013
  }, {
    "title": "Prefixing vs",
    "authors": ["Matthew S. Dryer."],
    "venue": "suffixing in inflectional morphology. In Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online. Max Planck Institute for Evolutionary Anthropology.",
    "year": 2013
  }, {
    "title": "Evolved structure of language shows lineage-specific trends in wordorder universals",
    "authors": ["Michael Dunn", "Simon J. Greenhill", "Stephen C. Levinson", "Russell D. Gray."],
    "venue": "Nature, 473(7345):79‚Äì82.",
    "year": 2011
  }, {
    "title": "An Indoeuropean classification: A lexicostatistical experiment",
    "authors": ["Isidore Dyen", "Joseph B. Kruskal", "Paul Black."],
    "venue": "Transactions of the American Philosophical Society, 82(5):1‚Äì132.",
    "year": 1992
  }, {
    "title": "Evolutionary trees from DNA sequences: A maximum likelihood approach",
    "authors": ["Joseph Felsenstein."],
    "venue": "Journal of Molecular Evolution, 17(6):368‚Äì376.",
    "year": 1981
  }, {
    "title": "The origin and evolution of word order",
    "authors": ["Murray Gell-Mann", "Merritt Ruhlen."],
    "venue": "Proceedings of the National Academy of Sciences, 108(42):17290‚Äì 17295.",
    "year": 2011
  }, {
    "title": "The natural order of events: How speakers of different languages represent events nonverbally",
    "authors": ["Susan Goldin-Meadow", "Wing Chee So", "Aslƒ± √ñzy√ºrek", "Carolyn Mylander."],
    "venue": "Proceedings of the National Academy of Sciences, 105(27):9163‚Äì9168.",
    "year": 2008
  }, {
    "title": "Language-tree divergence times support the Anatolian theory of Indo-European origin",
    "authors": ["Russell D. Gray", "Quentin D. Atkinson."],
    "venue": "Nature, 426(6965):435‚Äì439.",
    "year": 2003
  }, {
    "title": "Language phylogenies reveal expansion pulses and pauses in Pacific settlement",
    "authors": ["Russell D. Gray", "Alexei J. Drummond", "Simon J. Greenhill."],
    "venue": "science, 323(5913):479‚Äì483.",
    "year": 2009
  }, {
    "title": "Some universals of grammar with particular reference to the order of meaningful elements",
    "authors": ["Joseph H. Greenberg."],
    "venue": "Joseph H. Greenberg, editor, Universals of Language, pages 73‚Äì113. MIT Press.",
    "year": 1963
  }, {
    "title": "Diachrony, synchrony and language universals",
    "authors": ["Joseph H. Greenberg."],
    "venue": "Joseph H. Greenberg, Charles A. Ferguson, and Edith A. Moravesik, editors, Universals of Human Language, volume 1. Stanford University Press.",
    "year": 1978
  }, {
    "title": "The shape and tempo of language evolution",
    "authors": ["Simon J. Greenhill", "Quentin D. Atkinson", "Andrew Meade", "Russel D. Gray."],
    "venue": "Proceedings of the Royal Society B: Biological Sciences, 277(1693):2443‚Äì2450.",
    "year": 2010
  }, {
    "title": "The Austronesian basic vocabulary database: From bioinformatics to lexomics",
    "authors": ["Simon J. Greenhill", "Robert Blust", "Russell D. Gray."],
    "venue": "Evolutionary Bioinformatics, 4:271‚Äì283.",
    "year": 2008
  }, {
    "title": "Evolutionary dynamics of language systems",
    "authors": ["Simon J. Greenhill", "Chieh-Hsi Wu", "Xia Hua", "Michael Dunn", "Stephen C. Levinson", "Russell D. Gray."],
    "venue": "Proceedings of the National Academy of Sciences, 114(42):E8822‚ÄìE8829.",
    "year": 2017
  }, {
    "title": "Bantu expansion shows that habitat alters the route and pace of human dispersals",
    "authors": ["Rebecca Grollemund", "Simon Branford", "Koen Bostoen", "Andrew Meade", "Chris Venditti", "Mark Pagel."],
    "venue": "Proceedings of the National Academy of Sciences, 112(43):13296‚Äì",
    "year": 2015
  }, {
    "title": "Automated dating of the world‚Äôs language families based on lexical similarity",
    "authors": ["Dmitry Egorov."],
    "venue": "Current Anthropology, 52(6):841‚Äì875.",
    "year": 2011
  }, {
    "title": "The Ising model for changes in word ordering rules in natural languages",
    "authors": ["Yoshiaki Itoh", "Sumie Ueda."],
    "venue": "Physica D: Nonlinear Phenomena, 198(3):333‚Äì339.",
    "year": 2004
  }, {
    "title": "Computational methods are invaluable for typology, but the models must match the questions",
    "authors": ["Roger Levy", "Hal Daum√© III."],
    "venue": "Linguistic Typology, 15:393‚Äì399.",
    "year": 2011
  }, {
    "title": "A dynamic approach to the verification of distributional universals",
    "authors": ["Elena Maslova."],
    "venue": "Linguistic Typology, 4(3):307‚Äì333.",
    "year": 2000
  }, {
    "title": "Tracing the roots of syntax with Bayesian phylogenetics",
    "authors": ["Luke Maurits", "Thomas L. Griffiths."],
    "venue": "Proceedings of the National Academy of Sciences, 111(37):13576‚Äì13581.",
    "year": 2014
  }, {
    "title": "Morphological and prosodic forces shaping word order",
    "authors": ["Marianne Mithun."],
    "venue": "Pamela A. Downing and Michael Noonan, editors, Word Order in Discourse, pages 387‚Äì423. John Benjamins Publishing.",
    "year": 1995
  }, {
    "title": "Continuous space representations of linguistic typology and their application to phylogenetic inference",
    "authors": ["Yugo Murawaki."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
    "year": 2015
  }, {
    "title": "Statistical modeling of creole genesis",
    "authors": ["Yugo Murawaki."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1329‚Äì1339.",
    "year": 2016
  }, {
    "title": "Diachrony-aware induction of binary latent representations from typological features",
    "authors": ["Yugo Murawaki."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 451‚Äì461. Asian",
    "year": 2017
  }, {
    "title": "A statistical model for the joint inference of vertical stability and horizontal diffusibility of typological features",
    "authors": ["Yugo Murawaki", "Kenji Yamauchi."],
    "venue": "Journal of Language Evolution, 3(1):13‚Äì25.",
    "year": 2018
  }, {
    "title": "Perfect phylogenetic networks: A new methodology for reconstructing the evolutionary history of natural languages",
    "authors": ["Luay Nakhleh", "Don Ringe", "Tandy Warnow."],
    "venue": "Language, pages 382‚Äì420.",
    "year": 2005
  }, {
    "title": "Slice sampling",
    "authors": ["Radford M. Neal."],
    "venue": "Annals of Statistics, 31(3):705‚Äì767.",
    "year": 2003
  }, {
    "title": "MCMC using Hamiltonian dynamics",
    "authors": ["Radford M. Neal."],
    "venue": "Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, editors, Handbook of Markov Chain Monte Carlo, pages 113‚Äì162. CRC Press.",
    "year": 2011
  }, {
    "title": "Networks uncover hidden lexical borrowing in Indo-European language evolution",
    "authors": ["Shijulal Nelson-Sathi", "Johann-Mattis List", "Hans Geisler", "Heiner Fangerau", "Russell D. Gray", "William Martin", "Tal Dagan."],
    "venue": "Proceedings of the Royal Society",
    "year": 2010
  }, {
    "title": "Linguistic Diversity in Space and Time",
    "authors": ["Johanna Nichols."],
    "venue": "University of Chicago Press.",
    "year": 1992
  }, {
    "title": "Bayesian analysis of correlated evolution of discrete characters by reversible-jump Markov chain Monte Carlo",
    "authors": ["Mark Pagel", "Andrew Meade."],
    "venue": "The American Naturalist, 167(6):808‚Äì825.",
    "year": 2006
  }, {
    "title": "The Indo-European Controversy: Facts and Fallacies in Historical Linguistics",
    "authors": ["Asya Pereltsvaig", "Martin W. Lewis."],
    "venue": "Cambridge University Press.",
    "year": 2015
  }, {
    "title": "The emergence of grammar: Systematic structure in a new language",
    "authors": ["Wendy Sandler", "Irit Meir", "Carol Padden", "Mark Aronoff."],
    "venue": "Proceedings of the National Academy of Sciences, 102(7):2661‚Äì 2665.",
    "year": 2005
  }, {
    "title": "Discriminative analysis of linguistic features for typological study",
    "authors": ["Hiroya Takamura", "Ryo Nagata", "Yoshifumi Kawasaki."],
    "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pages 69‚Äì76.",
    "year": 2016
  }],
  "id": "SP:4e954d2fddf03fd642cdf45056d4431942095220",
  "authors": [{
    "name": "Yugo Murawaki",
    "affiliations": []
  }],
  "abstractText": "Statistical phylogenetic models have allowed the quantitative analysis of the evolution of a single categorical feature and a pair of binary features, but correlated evolution involving multiple discrete features is yet to be explored. Here we propose latent representation-based analysis in which (1) a sequence of discrete surface features is projected to a sequence of independent binary variables and (2) phylogenetic inference is performed on the latent space. In the experiments, we analyze the features of linguistic typology, with a special focus on the order of subject, object and verb. Our analysis suggests that languages sharing the same word order are not necessarily a coherent group but exhibit varying degrees of diachronic stability depending on other features.",
  "title": "Analyzing Correlated Evolution of Multiple Features Using Latent Representations"
}