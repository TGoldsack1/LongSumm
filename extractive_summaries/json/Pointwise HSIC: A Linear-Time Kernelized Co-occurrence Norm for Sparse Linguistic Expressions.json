{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1763–1775 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1763"
  }, {
    "heading": "1 Introduction",
    "text": "Computing the co-occurrence strength between two linguistic expressions is a fundamental task in natural language processing (NLP). For example, in collocation extraction (Manning and Schütze, 1999), word bigrams are collected from corpora and then strongly co-occurring bigrams (e.g., “New York”) are found. In dialogue response selection (Lowe et al., 2015), pairs comprising a context and its response sentence are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence. In either case, a set of linguistic expression pairs D = {(xi, yi)}ni=1 is first collected and then the co-occurrence strength of a (new) pair (x, y) is computed.\nPointwise mutual information (PMI) (Church and Hanks, 1989) is frequently used to model the co-occurrence strength of linguistic expression pairs. There are two typical types of PMI estimation (computation) method. One is a countingbased estimator using maximum likelihood estimation, sometimes with smoothing techniques, for example,\nP̂MIMLE(x, y;D)= log n · c(x, y)∑ y′c(x, y ′) ∑ x′c(x ′, y) ,\n(1)\nwhere c(x, y) denotes the frequency of the pair (x, y) in given dataD. This is easy to compute and is commonly used to measure co-occurrence between words, such as in collocation extraction1; however, when data D is sparse, i.e., when x or y is a phrase or sentence, this approach is unrealistic. The second method uses recurrent neural networks (RNNs). Li et al. (2016) proposed to em1 In collocation extraction, simple counting c(x, y) ∝ P̂(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., “of the”) higher (Manning and Schütze, 1999).\nploy PMI to suppress dull responses for utterance generation in dialogue systems2. They estimated P(y) and P(y|x) using RNN language models and estimated PMI as follows:\nP̂MIRNN(x, y;D) = log P̂RNN(y|x) P̂RNN(y) . (2)\nThis way of estimating PMI is applicable to sparse language expressions; however, learning RNN language models is computationally costly.\nTo eliminate this trade-off between robustness to data sparsity and learning time, in this study we propose a new kernel-based co-occurrence measure, which we call the pointwise Hilbert–Schmidt independence criterion (PHSIC) (see Table 1). Our contributions are as follows: • We formalize PHSIC, which is derived from\nHSIC (Gretton et al., 2005), a kernel-based dependence measure, in the same way that PMI is derived from mutual information (Section 3). • We give an intuitive explanation why PHSIC is robust to data sparsity. PHSIC is a “smoothed variant of PMI”, which allows various similarity metrics to be plugged in as kernels (Section 4). • We propose fast estimators of PHSIC, which are reduced to a simple and fast matrix calculation regardless of whether we use linear or nonlinear kernels (Section 5). • We empirically confirmed the effectiveness of PHSIC, i.e., its robustness to data sparsity and learning time, in two different types of experiment, a dialogue response selection task and a data selection task for machine translation (Section 6)."
  }, {
    "heading": "2 Problem Setting",
    "text": "Let X and Y denote random variables on X and Y , respectively. In this paper, we deal with the tasks of taking a set of linguistic expression pairs\nD = {(xi, yi)}ni=1 ∼i.i.d. PXY , (3)\nwhich is regarded as a set of i.i.d. samples drawn from a joint distribution PXY , and then measuring the “co-occurrence strength” for each given pair (x, y) ∈ X × Y . Such tasks include collocation extraction and dialogue response selection (Section 1).\n2 In dialogue response selection or generation, a simple conditional probability P̂(y|x), rather than PMI, ranks dull responses (e.g., “I don’t know.”) higher (Li et al., 2016)."
  }, {
    "heading": "3 Pointwise HSIC",
    "text": "In this section, we give the formal definition of PHSIC, a new kernel-based co-occurrence measure. We show a summary of this section in Table 2. Intuitively, PHSIC is a “kernelized variant of PMI.”"
  }, {
    "heading": "3.1 Dependence Measure",
    "text": "As a preliminary step, we introduce the simple concept of dependence (see Dependence Measure in Table 2). Recall that random variables X and Y are independent if and only if the joint probability density PXY and the product of the marginals PXPY are equivalent. Therefore, we can measure the dependence between random variables X and Y via the difference between PXY and PXPY .\nBoth the mutual information and the Hilbert– Schmidt independence criterion, to be described below, are such dependence measures."
  }, {
    "heading": "3.2 MI and PMI",
    "text": "We briefly review the well-known mutual information and PMI (see MI & PMI in Table 2).\nThe mutual information (MI)3 between two random variables X and Y is defined by\nMI(X,Y ) := KL[PXY ‖PXPY ] (4)\n(Cover and Thomas, 2006), where KL[·‖·] denotes the Kullback–Leibler (KL) divergence. Thus, MI(X,Y ) is the degree of dependence between X and Y measured by the KL divergence between PXY and PXPY .\nHere, by definition of the KL divergence, MI can be represented in the form of the expectation over PXY , i.e., the summation over all possible pairs (x, y) ∈ X×Y:\nMI(X,Y ) = E (x,y)\n[ log PXY (x, y)\nPX(x)PY (y)\n] . (5)\nThe shaded part in Equation (5) is actually the pointwise mutual information (PMI) (Church and Hanks, 1989):\nPMI(x, y;X,Y ) := log PXY (x, y)\nPX(x)PY (y) . (6)\nTherefore, PMI(x, y) can be thought of as the contribution of (x, y) to MI(X,Y ).\n3 Conventionally, mutual information is denoted by I(X;Y ); in this paper, however, for notational consistency, mutual information is denoted by MI(X,Y )."
  }, {
    "heading": "3.3 HSIC and PHSIC",
    "text": "As seen in the previous section, PMI can be derived from MI. Here, we consider replacing MI with the Hilbert–Schmidt independence criterion (HSIC). Then, in analogy with the relationship between PMI and MI, we derive PHSIC from HSIC (see HSIC & PHSIC in Table 2).\nLet k : X × X → R and ` : Y × Y → R denote positive definite kernels on X and Y , respectively (intuitively, they are similarity functions between linguistic expressions). The Hilbert– Schmidt independence criterion (HSIC) (Gretton et al., 2005), a kernel-based dependence measure, is defined by\nHSIC(X,Y; k, `) :=MMD2k,`[PXY ,PXPY ], (7)\nwhere MMD[·, ·] denotes the maximum mean discrepancy (MMD) (Gretton et al., 2012), which measures the difference between random variables on a kernel-induced feature space. Thus, HSIC(X,Y ; k, `) is the degree of dependence between X and Y measured by the MMD between PXY and PXPY , while MI is measured by the KL divergence (Equation (4)).\nAnalogous to MI in Equation (5), HSIC can be represented in the form of the expectation on PXY by a simple deformation:\nHSIC(X,Y ; k, `)\n= E (x,y)\n[ (φ(x)−mX)>CXY (ψ(y)−mY ) ] (8)\n= E (x,y)\n[ E (x′,y′) [k̃(x, x′)˜̀(y, y′)] ], (9)\nwhere\nφ(x) := k(x, ·), ψ(y) := `(y, ·), (10)\nmX := Ex[φ(x)], mY := Ey[ψ(y)], (11)\nCXY := E (x,y)\n[ (φ(x)−mX)(ψ(y)−mY )> ] , (12)\nk̃(x, x′) := k(x, x′)−Ex′ [k(x, x′)] −Ex[k(x, x′)] +Ex,x′ [k(x, x′)]. (13)\nAt first glance, these equations are somewhat complicated; however, the estimators of PHSIC we actually use are reduced to a simple matrix calculation in Section 5. Unlike MI in Equation (5), HSIC has two representations: Equation (8) is the representation in feature space and Equation (9) is the representation in data space.\nSimilar to the relationship between MI and PMI (Section 3.2), we define the pointwise Hilbert– Schmidt independence criterion (PHSIC) by the shaded parts in Equations (8) and (9):\nPHSIC(x, y;X,Y, k, `)\n:= (φ(x)−mX)>CXY (ψ(y)−mY ) (14)\n= E (x′,y′) [k̃(x, x′)˜̀(y, y′)] . (15) Namely, PHSIC(x, y) is defined as the contribution of (x, y) to HSIC(X,Y ).\nIn summary, we define PHSIC such that “MI:PMI = HSIC:PHSIC” holds (see Table 2)."
  }, {
    "heading": "4 PHSIC as Smoothed PMI",
    "text": "This section gives an intuitive explanation for the first feature of PHSIC, i.e., the robustness to data sparsity, using Table 3. In short, we show that PHSIC is a “smoothed variant of PMI.”\nFirst, the maximum likelihood estimator of PMI\nin Equation (1) can be rewritten as P̂MI(x, y;D)= log n · ∑\niI[x=xi ∧ y=yi]∑ iI[x=xi] ∑ iI[y=yi] , (16)\nwhere I[condition] = 1 if the condition is true and I[condition] = 0 otherwise. According to Equation (16), P̂MI(x, y) is the amount computed by repeating the following operation (see the first row in Table 3):\ncollate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if (x, y) and (xi, yi) match exactly or deduct the scores if either the x side or the y side (but nor both) matches.\nMoreover, an estimator of PHSIC in data space (Equation (15)) is\nP̂HSIC(x, y;D, k, `)= 1n ∑ i ̂̃ k(x, xi)̂˜̀(y, yi) ,\n(17)\nwhere ̂̃k(·, ·) and ̂̀̃(·, ·) are similarity functions centered on the data4. According to Equation (17), P̂HSIC(x, y) is the amount computed by repeating the following operation (see the second row in Table 3):\ncollate the given (x, y) and the observed (xi, yi) in D in order, and add the scores if the similarities on the x and y sides are both\nhigher (both ̂̃k(x, xi) > 0 and ̂̀̃(y, yi) > 0 hold)5 or deduct the scores if the similarities on either the x or y sides are similar but those on the other side are not similar.\n4 To be exact, ̂̃k(x, x′) := k(x, x′) − 1 n ∑n j=1 k(x, xj) − 1 n ∑n i=1 k(xi, x ′) + 1 n2 ∑n i=1 ∑n j=1 k(xi, xj), which is an estimator of the centered kernel k̃(x, x′) in Equation (13). 5 In addition, the scores are added if the similarity on the x\nside and that on the y side are both lower, that is, if ̂̃k(x, xi) < 0 and ̂̀̃(y, yi) < 0 hold.\nAs described above, when comparing the estimators of PMI and PHSIC from the viewpoint of “methods of matching the given (x, y) and the observed (xi, yi),” it is understood that PMI matches them in an exact manner, while PHSIC smooths the matching using kernels (similarity functions).\nWith this mechanism, even for completely unknown pairs, it is possible to estimate the cooccurrence strength by referring to observed pairs through the kernels. Therefore, PHSIC is expected to be robust to data sparsity and can be applied to phrases and sentences.\nAvailable Kernels for PHSIC In NLP, a variety of similarity functions (i.e., positive definite kernels) are available. We can freely utilize such resources, such as cosine similarity between sentence embeddings. For a more detailed discussion, see Appendix A."
  }, {
    "heading": "5 Empirical Estimators of PHSIC",
    "text": "Recall that we have two types of empirical estimator of PMI, the maximum likelihood estimator (Equation (1)) and the RNN-based estimator (Equation (2)). In this section, we describe how to rapidly estimate PHSIC from data. When using the linear kernel or cosine similarity (e.g., cosine similarity between sentence embeddings), PHSIC can be efficiently estimated in feature space (Section 5.1). When using a nonlinear kernel such as the Gaussian kernel, PHSIC can also be estimated efficiently in data space via a simple matrix decomposition (Section 5.2)."
  }, {
    "heading": "5.1 Estimation Using Linear Kernel or Cosine",
    "text": "When using the linear kernel or cosine similarity, the estimator of PHSIC in feature space (14) is as follows:\nP̂HSICfeature(x, y;D, k, `)\n= (φ(x)−φ(x))>ĈXY (ψ(y)−ψ(y)) , (18)\nwhere\nφ(x) = { x (k(x, x′) = x>x′) x/‖x‖ (k(x, x′) = cos(x, x′)) , (19)\nφ(x) := 1\nn n∑ i=1 φ(xi), ψ(y) := 1 n n∑ i=1 ψ(yi), (20)\nĈXY := 1\nn n∑ i=1 φ(xi)ψ(yi) >− φ(x)ψ(y)>. (21)\nGenerally in kernel methods, a feature map φ(·) induced by a kernel k(·, ·) is unknown or highdimensional and it is difficult to compute estimated values in feature space6. However, when we use the linear kernel or cosine similarity, feature maps can be explicitly determined (Equation (19)).\nComputational Cost When learning Equation (18) with feature maps φ : X → Rd and ψ : Y → Rd, computing the vectors φ(x), ψ(y) ∈ Rd and the matrix ĈXY ∈ Rd×d takes O(nd2) time and O(nd) space (linear in the size of the input, n). When estimating PHSIC(x, y), computing φ(x), ψ(y) ∈ Rd and Equation (18) takes O(d2) time (constant; does not depend on the size of the input, n)."
  }, {
    "heading": "5.2 Estimation Using Nonlinear Kernels",
    "text": "When using a nonlinear kernel such as the Gaussian kernel, it is necessary to estimate PHSIC in data space. Using a simple matrix decomposition, this can be achieved with the same computational cost as the estimation in feature space. See Appendix B for a detailed derivation."
  }, {
    "heading": "6 Experiments",
    "text": "In this section, we provide empirical evidence for the greater effectiveness of PHSIC than PMI, i.e., a very short learning time and robustness to data sparsity. Among the many potential applications of PHSIC, we choose two fundamental scenarios, (re-)ranking/classification and data selection. • In the ranking/classification scenario (measuring\nthe co-occurrence strength of new data pairs with reference to observed pairs), PHSIC is applied\n6 One of the characteristics of kernel methods is that an intractable estimation in feature space is replaced with an efficient estimation in data space.\nas a criterion for the dialogue response selection task (Section 6.2). • In the data selection/filtering scenario (ordering the entire set of observed data pairs according to the co-occurrence strength), PHSIC is also applied as a criterion for data selection in the context of machine translation (Section 6.3)."
  }, {
    "heading": "6.1 PHSIC Settings",
    "text": "To take advantage of recent developments in representation learning, we used several pre-trained models for encoding sentences into vectors and several kernels between these vectors for PHSIC.\nEncoders As sentence encorders, we used two pre-trained models without fine-tuning. First, the sum of the word vectors effectively represents a sentence (Mikolov et al., 2013a):\nx= ∑ w∈xvec(w), y= ∑ w∈yvec(w). (22)\nFor vec(·), we used the pre-trained fastText model7, which is a high-accuracy and popular word embedding model (Bojanowski et al., 2017); models in 157 languages are publicly distributed (Grave et al., 2018). Second, we also used a DNN-based sentence encoder, called the universal sentence encoder (Cer et al., 2018), which utilizes the deep averaging network (DAN) (Iyyer et al., 2015). The pre-trained model for English sentences we used is publicly available8.\nKernels As kernels between these vectors, we used cosine similarity (cos)\nk(x,x′) = cos(x,x′) (23)\nand the Gaussian kernel (also known as the radial basis function kernel; RBF kernel)\nk(x,x′) = exp ( −‖x− x\n′‖22 2σ2\n) , (24)\nand similarly for `(y,y′). The experiments are ran with hyperparameter σ = 1.0 for the RBF kernel, and d = 100 for incomplete Cholesky decomposition (for more detail, see Section B)."
  }, {
    "heading": "6.2 Ranking: Dialogue Response Selection",
    "text": "In the first experiment, we applied PHSIC as a ranking criterion of the task of dialogue response 7 https://fasttext.cc/docs/en/english-vectors. html, https://fasttext.cc/docs/en/crawl-vectors. html 8 https://www.tensorflow.org/hub/modules/google/ universal-sentence-encoder/1\nselection (Lowe et al., 2015); in the task, pairs comprising a context (previous utterance sequence) and its response are collected from dialogue corpora and the goal is to rank the candidate responses for each given context sentence.\nThe task entails sentence sequences (very sparse linguistic expressions); moreover, Li et al. (2016) pointed out that (RNN-based) PMI has a positive impact on suppressing dull responses (e.g., “I don’t know.”) in dialogue systems. Therefore, PHSIC, another co-occurrence measure, is also expected to be effective for this. With this setting, where the validity of PMI is confirmed, we investigate whether PHSIC can replace RNN-based PMI in terms of both learning time and robustness to data sparsity.\nExperimental Settings\nDataset For the training data, we gathered approximately 5× 105 reply chains from Twitter, following Sordoni et al. (2015)9. In addition, we randomly selected {103, 104, 105} reply chains from that dataset. Using these small subsets, we confirmed the effect of the difference in the size of the training set (data sparseness) on the learning time and predictive performance.\nFor validation and test data, we used a small (approximately 2000 pairs each) but highly reliable dataset created by Sordoni et al. (2015)10, which consists only of conversations given high scores by human annotators. Therefore, this set was not expected to include dull responses.\nFor each dataset, we converted each contextmessage-response triple into a context-response pair by concatenating the context and message following Li et al. (2016). In addition, to convert the test set (positive examples) to ten-choice multiplechoice questions, we shuffled the combinations of context and response to generate pseudo-negative examples.\nEvaluation Metrics We adopted the following evaluation metrics for the task: (i) ROC-AUC (the area under the receiver operating characteristic curve), (ii) MRR (the mean reciprocal rank), and (iii) Recall@{1,2}.\n9 We collected tweets after 2017 for our training set to avoid duplication with the test set, which contains tweets from the year 2012. 10 https://www.microsoft.com/en-us/download/ details.aspx?id=52375\nExperimental Procedure We used the following procedure: (i) train the model with a set of context-response pairs D = {(xi, yi)}ni=1; (ii) for each context sentence x in the test data, rank the candidate responses {yj}10j=1 by the model; and (iii) report three evaluation metrics.\nBaseline Measures As baseline measures, both (1) an RNN language model P̂RNN(y) (Mikolov et al., 2010) and (2) a conditional RNN language model P̂RNN(y|x) (Sutskever et al., 2014) were trained, and (3) PMI based on these language models, RNN-PMI, was also used for experiments (see Equation (2)). We trained these models with all combinations of the following settings: (a) the number of dimensions of the hidden layers being 300 or 1200 and (b) the initialization of the embedding layer being random (uniform on [−0.1, 0.1]) or fastText. For more detailed settings, see Appendix C.\nExperimental Results Learning Time Table 4 shows the experimental results of the learning time11. Regardless of the size of the training set n, the learning time for\n11 The computing environment was as follows: (i) CPU: Xeon E5-1650-v3 (3.5 GHz, 6 Cores); (ii) GPU: GTX 1080 (8 GB).\nPHSIC is much shorter than that of the RNN-based method. For example, even when the size of the training set n is 5× 105, PHSIC is approximately 1400–4000 times faster than RNN-based PMI. This is because the estimators of PHSIC are reduced to a deterministic and efficient matrix calculation (Section 5), whereas neural network-based models involve the sequential optimization of parameters via gradient descent methods.\nRobustness to Data Sparsity Table 5 shows the experimental results of the predictive performance. When the size of the training data is small (n=103, 104), that is, when the data is extremely sparse, the predictive performance of PHSIC hardly deteriorates while that of PMI rapidly decays as the number of data decreases. This indicates that PHSIC is more robust to data sparsity than RNN-based PMI owing to the effect of kernels. Moreover, PHSIC with the simple cosine kernel outperforms the RNN-based model regardless of the number of data, while the learning time of PHSIC is thousands of times shorter than those of the baseline methods (Section 6.2).\nAdditionally we report Spearman’s rank correlation coefficient between models to verify whether PHSIC shows similar behavior to PMI. See Appendix D for more detail."
  }, {
    "heading": "6.3 Data Selection for Machine Translation",
    "text": "The aim of our second experiment was to demonstrate that PHSIC is also beneficial as a criterion of data selection. To achieve this, we attempted to apply PHSIC to a parallel corpus filtering task that has been intensively discussed in recent (neural) machine translation (MT, NMT) studies. This task was first adopted as a shared task in the third conference on machine translation (WMT 2018)12.\nSeveral existing parallel corpora, especially those automatically gathered from large-scale text data, such as the Web, contain unacceptable amounts of noisy (low-quality) sentence pairs that greatly affect the translation quality. Therefore, the development of an effective method for parallel corpus filtering would potentially have a large influence on the MT community; discarding such noisy pairs may improve the translation quality and shorten the training time.\nWe expect PHSIC to give low scores to exceptional sentence pairs (misalignments or missing 12 http://www.statmt.org/wmt18/ parallel-corpus-filtering.html\ntranslations) during the selection process because PHSIC assigns low scores to pairs that are highly inconsistent with other pairs (see Section 4). Note that applying RNN-based PMI to a parallel corpus selection task is unprofitable since obtaining RNNbased PMI also has an identical computational cost for training a sequence-to-sequence model for MT, and thus, we cannot expect a reduction of the total training time.\nExperimental Settings\nDataset We used the ASPEC-JE corpus13, which is an official dataset used for the MT-evaluation shared task held in the fourth workshop on Asian translation (WAT 2017)14 (Nakazawa et al., 2017). ASPEC-JE consists of approximately three million (3M) Japanese–English parallel sentences from scientific paper abstracts. As discussed by Kocmi et al. (2017), ASPEC-JE contains many low-quality parallel sentences that have the potential to significantly degrade the MT quality. In fact, they empirically revealed that using only the reliable part of the training parallel corpus significantly improved the translation quality. Therefore, ASPEC-JE is a suitable dataset for evaluating the data selection ability.\nModel For our data selection evaluation, we selected the Transformer architecture (Vaswani et al., 2017) as our baseline NMT model, which is widelyused in the NMT community and known as one of the current state-of-the-art architectures. We utilized fairseq15, a publicly available tool for neural sequence-to-sequence models, for building our models.\nExperimental Procedure We used the following procedure for this evaluation: (1) rank all parallel sentences in a given parallel corpus according to each criterion, (2) extract the top K ranked parallel sentences, (3) train the NMT model using the extracted parallel sentences, and (4) evaluate the translation quality of the test data using a typical MT automatic evaluation measure, i.e., BLEU (Papineni et al., 2002)16. In our experiments we evaluated PHSIC with K = 0.5M and 1M."
  }, {
    "heading": "13 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/",
    "text": ""
  }, {
    "heading": "14 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/",
    "text": ""
  }, {
    "heading": "15 https://github.com/pytorch/fairseq",
    "text": "16 We used multi-bleu.perl in the Moses tool (https:// github.com/moses-smt/mosesdecoder).\nBaseline Measure As a baseline measure, we utilize a publicly available script17 of fast align (Dyer et al., 2013), which is one of the state-of-theart word aligner. We firstly used the fast align for the training set D = {(xi, yi)}i to obtain the word alignment between each sentence pair (xi, yi), i.e., a set of aligned word pairs with its probabilities. We then computed the co-occurrence score of (xi, yi) with sentence-length normalization, i.e., the average log probability of aligned word pairs.\nExperimental Results Table 6 shows the results of our data selection evaluation. It is common knowledge in NMT that more data gives better performance in general. However, we observed that PHSIC successfully extracted beneficial parallel sentences from the noisy parallel 17 https://github.com/clab/fast align\ncorpus; the result using 1M data extracted from the 3M corpus by PHSIC was almost the same as that using 3M data (the decrease in the BLEU score was only 0.07), whereas that by random extraction reduced the BLEU score by 1.20.\nThis was actually a surprising result because PHSIC utilizes only monolingual similarity measures (kernels) without any other language resources. This indicates that PHSIC can be applied to a language pair poor in parallel resources. In addition, the surface form and grammatical characteristics between English and Japanese are extremely different18; therefore, we expect that PHSIC will work well regardless of the similarity of the language pair."
  }, {
    "heading": "7 Related Work",
    "text": "Dependence Measures Measuring independence or dependence (correlation) between two random variables, i.e., estimating dependence from a set of paired data, is a fundamental task in statistics and a very wide area of data science. To measure the complex nonlinear dependence that real data has, we have several choices.\nFirst, information-theoretic MI (Cover and Thomas, 2006) and its variants (Suzuki et al., 2009; Reshef et al., 2011) are the most commonly used dependence measures. However, to the best of our knowledge, there is no practical method of computing MIs for large-multi class high-dimensional 18 For example, word order; English is an SVO (subject-verbobject) language and Japanese is an SOV (subject-object-verb) language.\n(having a complex generative model) discrete data, such as sparse linguistic data.\nSecond, several kernel-based dependence measures have been proposed for measuring nonlinear dependence (Akaho, 2001; Bach and Jordan, 2002; Gretton et al., 2005). The reason why kernelbased dependence measures work well for real data is that they do not explicitly estimate densities, which is difficult for high-dimensional data. Among them, HSIC (Gretton et al., 2005) is popular because it has a simple estimation method, which is used for various tasks such as feature selection (Song et al., 2012), dimensionality reduction (Fukumizu et al., 2009), and unsupervised object matching (Quadrianto et al., 2009; Jagarlamudi et al., 2010). We follow this line.\nCo-occurrence Measures First, In NLP, PMI (Church and Hanks, 1989) and its variants (Bouma, 2009) are the de facto co-occurrence measures between dense linguistic expressions, such as words (Bouma, 2009) and simple narrative-event expressions (Chambers and Jurafsky, 2008). In recent years, positive PMI (PPMI) has played an important role as a component of word vectors (Levy and Goldberg, 2014).\nSecond, there are several studies in which the pairwise ranking problem has been solved by using deep neural networks (DNNs) in NLP. Li et al. (2016) proposed a PMI estimation using RNN language models; this was used as a baseline model in our experiments (see Section 6.2). Several studies have used DNN-based binary classifiers modeling P(C = positive | (x, y)) to solve the given ranking problem directly (Hu et al., 2014; Yin et al., 2016; Mueller and Thyagarajan, 2016) (these networks are sometimes called Siamese neural networks). Our study focuses on comparing co-occurrence measures. It is unknown whether Siamese NNs capture the co-occurrence strength; therefore we did not deal with Siamese NNs in this paper.\nFinally, to the best of our knowledge, Yokoi et al. (2017)’s paper is the first study that suggested converting HSIC to a pointwise measure. The present study was inspired by their suggestion; here, we have (i) provided a formal definition (population) of PHSIC; (ii) analyzed the relationship between PHSIC and PMI; (iii) proposed linear-time estimation methods; and (iv) experimentally verified the computation speed and robustness to data sparsity of PHSIC for practical applications."
  }, {
    "heading": "8 Conclusion",
    "text": "The NLP community has commonly employed PMI to estimate the co-occurrence strength between linguistic expressions; however, existing PMI estimators have a high computational cost when applied to sparse linguistic expressions (Section 1). We proposed a new kernel-based co-occurrence measure, the pointwise Hilbert– Schmidt independent criterion (PHSIC). As well as defining PMI as the contribution to mutual information, PHSIC is defined as the contribution to HSIC; PHSIC is intuitively a “kernelized variant of PMI” (Section 3). PHSIC can be applied to sparse linguistic expressions owing to the mechanism of smoothing by kernels. Comparing the estimators of PMI and PHSIC, PHSIC can be interpreted as a smoothed variant of PMI, which allows various similarity metrics to be plugged in as kernels (Section 4). In addition, PHSIC can be estimated in linear time owing to the efficient matrix calculation, regardless of whether we use linear or nonlinear kernels (Section 5). We conducted a ranking task for dialogue systems and a data selection task for machine translation (Section 6). The experimental results show that (i) the learning of PHSIC was completed thousands of times faster than that of the RNN-based PMI while outperforming it in ranking accuracy (Section 6.2); and (ii) even when using a nonlinear kernel, PHSIC can be applied to a large dataset. Moreover, PHSIC reduces the amount of training data to one third without sacrificing the output translation quality (Section 6.3).\nFuture Work Using the PHSIC estimator in feature space (Equation (18)), we can generate the most appropriate ψ(y) for a given φ(x) (uniquely, up to scale). That is, if a DNN-based sentence decoder is used, y (a sentence) can be restored from ψ(y) (a feature vector) so that generative models of strong co-occurring sentences can be realized."
  }, {
    "heading": "Acknowledgments",
    "text": "We are grateful to anonymous reviewers for their helpful comments. We also thank Weihua Hu for useful discussions, Kenshi Yamaguchi for collecting data, and Paul Reisert for proofreading. This work was supported in part by JSPS KAKENHI Grant Number JP15H01702 and JST CREST Grant Number JPMJCR1513, Japan."
  }, {
    "heading": "A Available Kernels for PHSIC",
    "text": "Similarity between Sentence Vectors A variety of vector representations of phrases and sentences based on the distributional hypothesis have recently been proposed, including sentence encoders (Kiros et al., 2015; Dai and Le, 2015; Iyyer et al., 2015; Hill et al., 2016; Cer et al., 2018) and the sum of word embeddings; it is known as additive compositionality (Mitchell and Lapata, 2010; Mikolov et al., 2013a; Wieting et al., 2015) that we can express the meaning of phrases and sentences well with the sum of word vectors (e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017)). Note that various pre-trained models of sentence encoders and word embeddings have also been made available.\nThe cosine of these vectors, which is a positive definite kernel, can be used as a convenient and highly accurate similarity function between phrases or sentences. Other major kernels can also be used, such as the RBF kernel, the Laplacian kernel, and polynomial kernels.\nStructured Kernels Various structured kernels for NLP, such as tree kernels, which capture fine structure of sentences such as syntax, were devised in the support vector machine era (Collins and Duffy, 2002; Bunescu and Mooney, 2006; Moschitti, 2006).\nCombinations We can freely combine the previously mentioned kernels because the sum and the product of positive definite kernels are also positive definite kernels (Shawe-Taylor and Cristianini, 2004, Proposition 3.22)."
  }, {
    "heading": "B Derivation of Fast PHSIC Estimation in Data Space",
    "text": "Although estimators of HSIC and PHSIC depend on kernels k, ` and data D, hereinafter, we use the following notation for the sake of simplicity:\nĤSIC(X,Y ) := ĤSIC(X,Y ;D, k, `), (25)\nP̂HSIC(x, y) := P̂HSIC(x, y;D, k, `). (26)\nNaı̈ve Estimation Fist, an estimator of PHSIC in the data space (15) is\nP̂HSICkernel(x, y)=(k − k)>( 1nH)(`− `), (27)\nwhere k := (k(x, x1), . . . , k(x, xn))> ∈ Rn, so as `; and vector k := 1nK1 denotes empirical mean of {ki}ni=1, so as `. This estimation has a\nlarge computational cost. When learning, computing the vectors k, ` takes O(n2) time and O(n) space. When estimating PHSIC, computing k, ` and multiplying the matrix 1nH takes O(n) time.\nFast Estimation via Incomplete Cholesky Decomposition Equation (27) has a large computational cost because it is necessary to construct the Gram matrices K and L ∈ Rn×n. In kernel methods, several methods have been proposed for approximating Gram matrices at low cost without constructing them explicitly, such as incomplete Cholesky decomposition (Fine and Scheinberg, 2001).\nBy incomplete Cholesky decomposition, from data points {x1, . . . , xn} ⊆ X and a positive definite kernel k : X × X → R, a matrix A = (a1, . . . ,an)\n> ∈ Rn×d (d n) can be obtained with O(nd2) time complexity. This makes it possible to approximate the Gram matrix K by vectors ai ∈ Rd without configuring the entire of K:\na>i aj ≈ k(xi, xj) (28) AA> ≈ K. (29)\nAlso, for HSIC, an efficient approximation method utilizing incomplete Cholesky decomposition has been proposed (Gretton et al., 2005, Lemma 2):\nĤSICICD(X,Y ) = 1 n2 ‖(HA)>B‖2F, (30)\nwhere A = (a1, . . . ,an)> ∈ Rn×d is a matrix satisfying AA> ≈ K computed via incomplete Cholesky decomposition, so as B (BB> ≈ L). Equation (30) can be represented in the form of the expectation on data points:\nĤSICICD(X,Y )= 1\nn n∑ i=1 [ (ai−a)>ĈICD(bi−b) ] (31)\nĈICD := 1 n (HA)>B ∈ Rd×d, (32)\nwhere vector a := 1nA >1 ∈ Rd denotes empirical mean of {ai}ni=1, so as b := 1nB >1.\nRecall that PHSIC(x, y) is the contribution of (x, y) to HSIC(X,Y ) (see Section 3.3); PHSIC then can be efficiently estimated by the shaded part of Equation (31):\nP̂HSICICD(x, y)= (a−a)>ĈICD(b−b) . (33)\nHere, the vector a ∈ Rd corresponding to the new x can be calculated by “performing from halfway”\non the incomplete Cholesky decomposition algorithm. Let x(1), . . . , x(d) denote the dominant xis adopted during decomposition algorithm. The jth element of a can be computed as follows:\na[j]= [ k(x, x(j))− j−1∑ m=1 a[m]Ajm ] /Ajj , (34)\nso as b ∈ Rd corresponding to the new y. The estimation via incomplete Cholesky decomposition (33) is extremely efficient compared to the naive estimation (27); Equation (33)’s computational complexity is equivalent to the estimation in the feature space (18)."
  }, {
    "heading": "C Detailed Settings for Learning RNNs",
    "text": "Detailed settings for learning RNNs used in this research are as follows. • Hidden layers: single layer LSTMs (Hochreiter\nand Schmidhuber, 1997) • Vocabulary: words with a frequency: 10 or more\n(n = 5× 105), 2 or more (otherwise) • Dropout rate: 0.1 (300-dim), 0.3 (1200-dim) • Batch size: 64 • Max epoch number: 5 (n = 5× 105), 30 (other-\nwise) • Deep learning framework: Chainer (Tokui et al.,\n2015)"
  }, {
    "heading": "D Correlation Between Models in Dialogue Response Selection Task",
    "text": "Table D shows Spearman’s rank correlation coefficient (Spearman’s ρ) between the co-occurrence scores on the test set computed by the models in the dialogue response selection task (Section 6.2). This shows that the behavior of RNN-based PMI and\nPHSIC are considerably different. Furthermore, interestingly, the behavior of PHSICs using different kernels is also different. Possible reasons for these observations are as follows: (1) the difference in the dependence measures (MI or HSIC) on which each model is based; (2) the validity or numerical stability of estimating PMI with RNN language models; and (3) differences in the behavior of PHSIC originating from differences in the plugged in kernels. A more detailed analysis of the compatibility between tasks and measures (or kernels) is attractive future work."
  }],
  "year": 2018,
  "references": [{
    "title": "A kernel method for canonical correlation analysis",
    "authors": ["Shotaro Akaho."],
    "venue": "IMPS, pages 1–7.",
    "year": 2001
  }, {
    "title": "Kernel Independent Component Analysis",
    "authors": ["Francis R. Bach", "Michael I. Jordan."],
    "venue": "JMLR, 3(Jul):1–",
    "year": 2002
  }, {
    "title": "Enriching Word Vectors with Subword Information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "TACL, 5:135–146.",
    "year": 2017
  }, {
    "title": "Normalized (Pointwise) Mutual Information in Collocation Extraction",
    "authors": ["Gerlof Bouma."],
    "venue": "GSCL, pages 31–40.",
    "year": 2009
  }, {
    "title": "Subsequence Kernels for Relation Extraction",
    "authors": ["Razvan C Bunescu", "Raymond J Mooney."],
    "venue": "NIPS, pages 171–178.",
    "year": 2006
  }, {
    "title": "Universal Sentence Encoder",
    "authors": ["Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil."],
    "venue": "CoRR,",
    "year": 2018
  }, {
    "title": "Unsupervised Learning of Narrative Event Chains",
    "authors": ["Nathanael Chambers", "Dan Jurafsky."],
    "venue": "ACL, pages 789–797.",
    "year": 2008
  }, {
    "title": "Word Association Norms, Mutual Information, and Lexicography",
    "authors": ["Kenneth Ward Church", "Patrick Hanks."],
    "venue": "ACL, pages 76–83.",
    "year": 1989
  }, {
    "title": "Convolution Kernels for Natural Language",
    "authors": ["Michael Collins", "Nigel Duffy."],
    "venue": "NIPS, pages 625– 632.",
    "year": 2002
  }, {
    "title": "Elements of Information Theory",
    "authors": ["Thomas M. Cover", "Joy A. Thomas."],
    "venue": "John Wiley & Sons.",
    "year": 2006
  }, {
    "title": "Semi-supervised Sequence Learning",
    "authors": ["Andrew M Dai", "Quoc V Le."],
    "venue": "NIPS, pages 3079–3087.",
    "year": 2015
  }, {
    "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2",
    "authors": ["Chris Dyer", "Victor Chahuneau", "Noah A Smith."],
    "venue": "NAACL-HLT, pages 644– 648.",
    "year": 2013
  }, {
    "title": "Efficient SVM Training Using Low-Rank Kernel Representations",
    "authors": ["Shai Fine", "Katya Scheinberg."],
    "venue": "JMLR, 2(Dec):243–264.",
    "year": 2001
  }, {
    "title": "Kernel dimension reduction in regression",
    "authors": ["Kenji Fukumizu", "Francis R. Bach", "Michael I. Jordan."],
    "venue": "Annals of Statistics, 37(4):1871–1905.",
    "year": 2009
  }, {
    "title": "Learning Word Vectors for 157 Languages",
    "authors": ["Edouard Grave", "Piotr Bojanowski", "Prakhar Gupta", "Armand Joulin", "Tomas Mikolov."],
    "venue": "LREC, pages 3483–3487.",
    "year": 2018
  }, {
    "title": "A Kernel Two-Sample Test",
    "authors": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola."],
    "venue": "JMLR, 13(Mar):723–773.",
    "year": 2012
  }, {
    "title": "Measuring Statistical Dependence with Hilbert-Schmidt Norms",
    "authors": ["Arthur Gretton", "Olivier Bousquet", "Alex Smola", "Bernhard Schölkopf."],
    "venue": "ALT, pages 63–77.",
    "year": 2005
  }, {
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."],
    "venue": "NAACL-HLT, pages 1367– 1377.",
    "year": 2016
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jrgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "NIPS, pages 2042–2050.",
    "year": 2014
  }, {
    "title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III."],
    "venue": "ACL/IJCNLP, pages 1681–1691.",
    "year": 2015
  }, {
    "title": "Kernelized Sorting for Natural Language Processing",
    "authors": ["Jagadeesh Jagarlamudi", "Seth Juarez", "Hal Daumé III."],
    "venue": "AAAI, pages 1020–1025.",
    "year": 2010
  }, {
    "title": "Skip-Thought Vectors",
    "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan R. Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."],
    "venue": "NIPS, pages 3294–3302.",
    "year": 2015
  }, {
    "title": "Cuni nmt system for wat 2017 translation tasks",
    "authors": ["Tom Kocmi", "Dušan Variš", "Ondřej Bojar."],
    "venue": "WAT, pages 154–159.",
    "year": 2017
  }, {
    "title": "Neural Word Embedding as Implicit Matrix Factorization",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "NIPS, pages 2177–2185.",
    "year": 2014
  }, {
    "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "NAACL-HLT, pages 110–119.",
    "year": 2016
  }, {
    "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured MultiTurn Dialogue Systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."],
    "venue": "SIGDIAL, pages 285– 294.",
    "year": 2015
  }, {
    "title": "Foundations of Statistical Natural Language Processing",
    "authors": ["Christopher D. Manning", "Hinrich Schütze."],
    "venue": "MIT press.",
    "year": 1999
  }, {
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "authors": ["Tomas Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean."],
    "venue": "Proc. of the Workshop on ICLR, pages 1–12.",
    "year": 2013
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."],
    "venue": "Interspeech, pages 1045–1048.",
    "year": 2010
  }, {
    "title": "Composition in Distributional Models of Semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive Science, 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "Making Tree Kernels practical for Natural Language Learning",
    "authors": ["Alessandro Moschitti."],
    "venue": "EACL, volume 6, pages 113–120.",
    "year": 2006
  }, {
    "title": "Siamese Recurrent Architectures for Learning Sentence Similarity",
    "authors": ["Jonas Mueller", "Aditya Thyagarajan."],
    "venue": "AAAI, 2012, pages 2786–2792.",
    "year": 2016
  }, {
    "title": "Overview of the 4th workshop on asian translation",
    "authors": ["Toshiaki Nakazawa", "Shohei Higashiyama", "Chenchen Ding", "Hideya Mino", "Isao Goto", "Hideto Kazawa", "Yusuke Oda", "Graham Neubig", "Sadao Kurohashi."],
    "venue": "WAT, pages 1–54.",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "ACL, pages 311– 318.",
    "year": 2002
  }, {
    "title": "GloVe: Global Vectors for Word Representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Kernelized sorting",
    "authors": ["Novi Quadrianto", "Le Song", "Alex J. Smola."],
    "venue": "NIPS, pages 1289–1296.",
    "year": 2009
  }, {
    "title": "Detecting Novel Associations in Large Data Sets",
    "authors": ["David N. Reshef", "Yakir A. Reshef", "Hilary K. Finucane", "Sharon R. Grossman", "Gilean McVean", "Peter J. Turnbaugh", "Eric S. Lander", "Michael Mitzenmacher", "Pardis C. Sabeti."],
    "venue": "Science, 334(6062):1518–",
    "year": 2011
  }, {
    "title": "Kernel Methods for Pattern Analysis",
    "authors": ["J. Shawe-Taylor", "N. Cristianini."],
    "venue": "Cambridge University Press.",
    "year": 2004
  }, {
    "title": "Feature Selection via Dependence Maximization",
    "authors": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt."],
    "venue": "JMLR, 13:1393–1434.",
    "year": 2012
  }, {
    "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Sequence to Sequence Learning with Neural Networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "NIPS, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Mutual information estimation reveals global associations between stimuli and biological processes",
    "authors": ["Taiji Suzuki", "Masashi Sugiyama", "Takafumi Kanamori", "Jun Sese."],
    "venue": "BMC Bioinformatics, 10(Suppl 1):S52.",
    "year": 2009
  }, {
    "title": "Chainer: a Next-Generation Open Source Framework for Deep Learning",
    "authors": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton."],
    "venue": "LearningSys.",
    "year": 2015
  }, {
    "title": "Attention is All you Need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "ukasz Kaiser", "Illia Polosukhin"],
    "year": 2017
  }, {
    "title": "Towards Universal Paraphrastic Sentence Embeddings",
    "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "ICLR, pages 1–19.",
    "year": 2015
  }, {
    "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs",
    "authors": ["Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou."],
    "venue": "TACL, 4(1):259–272.",
    "year": 2016
  }, {
    "title": "Learning CoSubstructures by Kernel Dependence Maximization",
    "authors": ["Sho Yokoi", "Daichi Mochihashi", "Ryo Takahashi", "Naoaki Okazaki", "Kentaro Inui."],
    "venue": "IJCAI, pages 3329–3335.",
    "year": 2017
  }, {
    "title": "2016; Cer et al., 2018) and the sum of word embeddings; it is known as additive compositionality",
    "authors": ["Dai", "Le", "Iyyer et al", "Hill"],
    "venue": "(Mitchell and Lapata,",
    "year": 2010
  }, {
    "title": "Combinations We can freely combine the previously mentioned kernels because the sum and the product of positive definite kernels",
    "authors": ["Duffy", "Bunescu", "Mooney"],
    "venue": "Moschitti,",
    "year": 2006
  }, {
    "title": "Correlation Between Models in Dialogue Response Selection Task Table D shows Spearman’s rank correlation coefficient (Spearman’s ρ) between the co-occurrence",
    "authors": [],
    "year": 2015
  }],
  "id": "SP:399d82e5d4af3dc5970849f16fc836c764a1cb8a",
  "authors": [{
    "name": "Sho Yokoi",
    "affiliations": []
  }, {
    "name": "Sosuke Kobayashi",
    "affiliations": []
  }, {
    "name": "Kenji Fukumizu",
    "affiliations": []
  }, {
    "name": "Jun Suzuki",
    "affiliations": []
  }, {
    "name": "Kentaro Inui",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information, we derive this new measure from the Hilbert–Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNNbased PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.",
  "title": "Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions"
}