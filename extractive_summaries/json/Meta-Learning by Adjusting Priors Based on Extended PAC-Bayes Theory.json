{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Learning from examples is the process of inferring a general rule from a finite set of examples. It is well known in statistics (e.g., Devroye et al. (1996)) that learning cannot take place without prior assumptions. Recent work in deep neural networks has achieved significant success in using prior knowledge in the implementation of structural\n1The Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel. Correspondence to: Ron Amit <ronamit@campus.technion.ac.il>, Ron Meir <rmeir@ee.technion.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nconstraints, e.g., convolutions and weight sharing (LeCun et al., 2015). However, often the relevant prior information for a given task is not clear, and there is a need for building it through learning from previous interactions with the world. Learning from previous experience can take several forms: Continual Learning (Kirkpatrick et al., 2017) - a learning agent is trained on a sequence of tasks, aiming to solve the current task while maintaining good performance on previous tasks. Multi-Task Learning (Caruana, 1997) - a learning agent learns how to solve several observed tasks, while exploiting their shared structure. Domain Adaptation (Ben-David et al., 2010) - a learning agent solves a ‘target’ learning task using ‘source’ tasks (both are observed, but usually the target is predominantly unlabeled). We work within the framework of Meta-Learning / Learning-toLearn / Inductive Transfer (Thrun & Pratt, 1997; Vilalta & Drissi, 2002) 1 in which a ‘meta-learner’ extracts knowledge from several observed tasks to facilitate the learning of new tasks by a ‘base-learner’ (see Figure 1). In this setup the meta-learner must generalize from a finite set of observed tasks. The performance is evaluated when learning related new tasks (which are unavailable to the meta-learner) .\nAs a motivational example, consider the case in which a meta-learner observes many image classification tasks of natural images, and uses a CNN to learn each task. The meta-learner might learn a prior which fixes the lower layers of the network to extract generic image features, but allows variation in the higher layers to adapt to new classes. Thus, new tasks can be learned using fewer examples than learning from scratch (e.g., Yosinski et al. (2014)). Generally, other scenarios might instead benefit from sharing other parts of the network (e.g., Yin & Pan (2017)). In our framework the prior is automatically inferred from the observed tasks, rather than being manually inserted by the algorithm designer.\nThe notion of ‘task-environment’ was formulated by Baxter (2000). In analogy to the standard single-task learning where data is sampled from an unknown distribution, Baxter\n1In our setting all observed tasks are available simultaneously to the meta-learner. The setting in which task are observed sequentially is often termed as Lifelong Learning (Thrun, 1996; Alquier et al., 2017)\nsuggested a setting where tasks are sampled from an unknown task distribution (environment), so that knowledge acquired from previous tasks can be used in order to improve performance on a novel task. Baxter’s work not only provided an interesting and mathematically precise perspective for meta-learning, but also provided generalization bounds demonstrating the potential improvement in performance due to prior knowledge.\nIn this paper we work within the framework formulated by Baxter (2000), and, following the setup in Pentina & Lampert (2014), provide generalization error bounds within the PAC-Bayes framework. These bounds are then used to develop a practical learning algorithm that is applied to neural networks, demonstrating the utility of our approach. The main contributions of this work are the following. (i) An improved and tighter bound in the theoretical framework of Pentina & Lampert (2014) derived using a technique which can extend different single-task PAC-Bayes bounds to the meta-learning setup. (ii) A principled meta-learning method and its implementation using probabilistic feedforward neural networks. (iii) Empirical demonstration of the performance enhancement compared to naive approaches as well as recent methods in this field.\nRelated Work While there have been many recent developments in meta-learning (e.g., Edwards & Storkey (2016); Andrychowicz et al. (2016); Finn et al. (2017)), most of them were not based on generalization error bounds, which is the focus of the present work. An elegant extension of generalization error bounds to meta-learning was provided by Pentina & Lampert (2014), mentioned above (extended in Pentina & Lampert (2015)). Their work, however, did not provide a practical algorithm applicable to deep neural networks. More recently, Dziugaite & Roy (2017) developed a single-task algorithm based on PAC-Bayes bounds that was demonstrated to yield good performance in simple classification tasks with deep networks. Other recent theoretical approaches to meta or multitask learning (e.g. Maurer (2005; 2009); Ruvolo & Eaton (2013); Maurer et al. (2016); Alquier et al. (2017)) provide increasingly general bounds but have not led to practical algorithms for neural networks."
  }, {
    "heading": "2. Preliminaries: PAC-Bayes Learning",
    "text": "In the common setting for learning, a set of independent samples, S = {zi}mi=1, from a space of examples Z , is given, each sample drawn from an unknown probability distribution D, namely zi ∼ D. We will use the notation S ∼ Dm to denote the distribution over the full sample. In supervised learning, the samples are input/output pairs zi = (xi, yi). The usual learning goal is, based on S, to find a hypothesis h ∈ H, where H is the so-called hypothesis space, that minimizes the expected loss function E`(h, z), where `(h, z) is a loss function bounded in [0, 1] . As the distribution D is unknown, learning consists of selecting an appropriate h based on the sample S. In classification H is a space of classifiers mapping the input space to a finite set of classes. As noted in the Introduction, an inductive bias is required for effective learning. While in the standard approach to learning, described in the previous paragraph, one usually selects a single classifier (e.g., the one minimizing the empirical error), the PAC-Bayes framework, first formulated by McAllester (1999), considers the construction of a complete probability distribution overH, and the selection of a single hypothesis h ∈ H based on this distribution. Since this distribution depends on the data it is referred to as a posterior distribution and will be denoted by Q. We note that while the term ‘posterior’ has a Bayesian connotation, the framework is not necessarily Bayesian, and the posterior does not need to be related to the prior through the likelihood function as in standard Bayesian analysis.\nThe PAC-Bayes framework has been widely studied in recent years, and has given rise to significant flexibility in learning, and, more importantly, to some of the best generalization bounds available Seeger (2002); Catoni (2007); Audibert (2010); Lever et al. (2013). Recent works analyzed transfer-learning in neural networks with PAC-Bayes tools (Galanti et al., 2016; McNamara & Balcan, 2017). The framework has been recently extended to the meta-learning setting by Pentina & Lampert (2014), and will be extended and applied to neural networks in the present contribution."
  }, {
    "heading": "2.1. Single-task Problem Formulation",
    "text": "Following the notation introduced above we define the expected error er (h,D) , E\nz∼D `(h, z) and the empirical error êr (h, S) , (1/m) ∑m j=1 ` (h, zi) for a single hypothesis h ∈ H. Since the distribution D is unknown, er (h,D) cannot be directly computed. In the PAC-Bayes setting the learner outputs a distribution over the entire hypothesis space H, i.e, the goal is to provide a posterior distribution Q ∈ M, where M denotes the set of distributions over H. The expected error and empirical error are then given in this setting by averaging over the posterior distribution Q ∈ M, namely er (Q,D) , E\nh∼Q er (h,D) and\nêr (Q,S) , E h∼Q êr (h, S), respectively."
  }, {
    "heading": "2.2. PAC-Bayes Generalization Bound",
    "text": "In this section we introduce a PAC-Bayes bound for the single-task setting. The bound will also serve us for the meta-learning setting in the next sections. PAC-Bayes bounds are based on specifying some ‘prior’ reference distribution P ∈M, that must not depend on the observed data S. The distribution over hypotheses Q which is provided as an output from the learning process is called the posterior (since it is allowed to depend on S) 2. The classical PACBayes theorem for single-task learning was formulated by McAllester (1999). Theorem 1 (McAllester’s single-task bound). Let P ∈M be some prior distribution overH. Then for any δ ∈ (0, 1], the following inequality holds uniformly for all posteriors distributions Q ∈M with probability at least 1− δ,\ner (Q,D) ≤ êr (Q,S) +\n√ D(Q||P ) + log mδ\n2(m− 1)\nwhere D(Q||P ) is the Kullback-Leibler (KL) divergence, D(Q||P ) , E\nh∼Q log Q(h)P (h) .\nTheorem 1 can be interpreted as stating that with high probability the expected error er (Q,D) is upper bounded by the empirical error plus a complexity term. Since, with high probability, the bound holds uniformly for all Q ∈M, it holds also for data dependent Q. By choosing Q that minimizes the bound we obtain a learning algorithm with generalization guarantees. Note that PAC-Bayes bounds express a trade-off between fitting the data (empirical error) and a complexity/regularization term (distance from prior) which encourages selecting a ‘simple’ hypothesis, namely one similar to the prior. The specific choice of P affects the bound’s tightness and so should express prior knowledge about the problem. Generally, we want the prior to be close to posteriors which can achieve low training error."
  }, {
    "heading": "3. PAC-Bayes Meta-Learning",
    "text": "In this section we introduce the meta-learning setting. In this setting a meta-learning agent observes several ‘training’ tasks from the same task environment. The meta-learner must extract some common knowledge (‘learned prior’) from these tasks, which will be used for learning new tasks from the same environment. In the literature this setting\n2As noted above, the terms ‘prior’ and ‘posterior’ might be misleading, since, this is not a Bayesian inference setting (the prior and posterior are not connected through the Bayes rule). However, PAC-Bayes and Bayesian analysis have interesting and practical connections, as we will see in the next sections (see also Germain et al. (2016)).\nis often called learning-to-learn, lifelong-learning, metalearning or bias learning (Baxter, 2000). We will formulate the problem and provide a generalization bound which will later lead to a practical algorithm. Our work extends Pentina & Lampert (2014) and establishes a potentially tighter bound. Furthermore, we will demonstrate how to apply this result practically to deep neural networks using stochastic learning."
  }, {
    "heading": "3.1. Meta-Learning Problem Formulation",
    "text": "The meta-learning problem formulation follows Pentina & Lampert (2014). We assume all tasks share the sample space Z , hypothesis spaceH and loss function ` : H×Z → [0, 1]. The learning tasks differ in the unknown sample distribution Dt associated with each task t. The meta-learning agent observes the training sets S1, ..., Sn corresponding to n different tasks. The number of samples in task i is denoted by mi. Each observed dataset Si is assumed to be generated from an unknown sample distribution Si ∼ Dmii . As in Baxter (2000), we assume that the sample distributions Di are generated i.i.d. from an unknown tasks distribution τ . The goal of the meta-learner is to extract some knowledge from the observed tasks that will be used as prior knowledge for learning new (yet unobserved) tasks from τ . The prior knowledge comes in the form of a distribution over hypotheses, P ∈ M. When learning a new task, the base learner uses the observed task’s data S and the prior P to output a posterior distribution Q(S, P ) overH. We assume that all tasks are learned via the same learning process. Namely, for a given S and P there is a specific output Q(S, P ). Hence the base learner Q is a mapping: Q : Zm ×M→M. 3\nThe quality of a prior P is measured by the expected loss when using it to learn new tasks, as defined by,\ner (P, τ) , E (D,m)∼τ E S∼Dm E h∼Q(S,P ) E z∼D `(h, z). (1)\nThe expectation is taken w.r.t. (i) tasks drawn from the task environment, (ii) training samples, (iii) hypotheses drawn from the posterior which is learned based on the training samples and prior (iv) a ‘test’ sample.\nAs described in section 2, in the single-task PAC-Bayes framework, the learner assumes a prior over hypotheses P (h), then observes the training samples and outputs a posterior distribution over hypotheses Q(h). In an analogous way, in the meta-learning PAC-Bayes framework, the meta-learner assumes a prior distribution over priors, a ‘hyper-prior’ P(P ), observes the training tasks, and then outputs a distribution over priors, a ‘hyper-posterior’ Q(P ).\nWe emphasize that while the hyper-posterior is learned us-\n3In the next section we will use stochastic optimization methods as learning algorithms, but we can assume convergence to a same solution for any execution with a given S and P .\ning the observed tasks, the goal is to use it for learning new, independent task from the environment. When encountering a new task, the learner samples a prior from the hyper posterior Q(P ), and then use it for learning. Ideally, the performance of the hyper-posterior Q is measured by the expected loss of learning new tasks using priors drawn from Q. This quantity is denoted as the transfer error\ner (Q, τ) , E P∼Q er (P, τ) . (2)\nWhile er (Q, τ) is not computable, we can however evaluate the average empirical risk when learning the observed tasks using priors drawn from Q, which is denoted as the empirical multi-task error\nêr (Q, S1, ..., Sn) , E P∼Q\n1\nn n∑ i=1 êr (Q(Si, P ), Si) , (3)\nIn the single-task PAC-Bayes setting one selects a prior P ∈M before seeing the data, and updates it to a posterior Q ∈ M after observing the training data. In the present meta-learning setup, following Pentina & Lampert (2014), one selects an initial hyper-prior distribution P , essentially a distribution over prior distributions P , and, following the observation of the data from all tasks, updates it to a hyperposterior distribution Q. As a simple example, assume the initial prior P is a Gaussian distribution over neural network weights, characterized by a mean and covariance. A hyper distribution would correspond in this case to a distribution over the mean and covariance of P ."
  }, {
    "heading": "3.2. Meta-Learning PAC-Bayes Bound",
    "text": "In this section we present a novel bound on the transfer error in the meta-learning setup. The theorem is proved in section A.1 of the supplementary material.\nTheorem 2 (Meta-learning PAC-Bayes bound). Let Q : Zm ×M→M be a base learner, and let P be some predefined hyper-prior distribution. Then for any δ ∈ (0, 1] the following inequality holds uniformly for all hyper-posterior distributions Q with probability at least 1− δ, 4\ner (Q, τ) ≤ 1 n n∑ i=1 E P∼Q êri (Qi, Si) (4)\n+ 1\nn n∑ i=1 √√√√D(Q||P) + EP∼Q D(Qi||P ) + log 2nmiδ 2(mi − 1)\n+\n√ D(Q||P) + log 2nδ\n2(n− 1) ,\nwhere Qi , Q(Si, P ).\n4The probability is taken over sampling of (Di,mi) ∼ τ and Si ∼ Dmii , i = 1, ..., n.\nNotice that the transfer error (2) is bounded by the empirical multi-task error (3) plus two complexity terms. The first is the average of the task-complexity terms of the observed tasks. This term converges to zero in the limit of a large number of samples in each task (mi →∞). The second is an environment-complexity term. This term converges to zero if infinite number of tasks is observed from the task environment (n→∞). As in Pentina & Lampert (2014), our proof is based on two main steps. The second step, similarly to Pentina & Lampert (2014), bounds the transfer-risk at the task-environment level (i.e, the error caused by observing only a finite number of tasks) by the average expected error in the observed tasks plus the environment-complexity term. The first step differs from Pentina & Lampert (2014). Instead of using a single joint bound on the average expected error, we use a single-task PAC-Bayes theorem to bound the expected error in each task separately (when learned using priors from the hyper-posterior), and then use a union bound argument. By doing so our bound takes into account the specific number of samples in each observed task (instead of their harmonic mean). Therefore our bound is better adjusted the observed data set.\nOur proof technique can utilize different single-task bounds in each of the two steps. In section A.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina & Lampert (2014). Therefore, the complexity terms\nare in the form of √\n1 mD(Q||P ) instead of 1√ m D(Q||P )\nas in Pentina & Lampert (2014). This means the bound is tighter 5. In section A.2 we demonstrate how our technique can use other, possibly tighter, single-task bounds. In Section 5 we will empirically evaluate the different bounds as meta-learning objectives and show that the improved tightness is critical for performance."
  }, {
    "heading": "4. Meta-Learning Algorithm",
    "text": "As in the single-task case, the bound of Theorem 2 can be evaluated from the training data and so can serve as a minimization objective for a principled meta-learning algorithm. Since the bound holds uniformly for all Q, it is ensured to hold also for the minimizer of the objective Q∗. Provided that the bound is tight enough, the algorithm will approximately minimize the transfer-risk itself, avoiding overfitting to the observed tasks. In this section we will derive a practical learning procedure that can applied to a large family of differentiable models, including deep neural networks."
  }, {
    "heading": "4.1. Hyper-Posterior Model",
    "text": "In this section we choose a specific form of hyper-posterior distribution Q which enables practical implementation.\n5E.g., Seldin et al. (2012) Theorems 5 and 6\nGiven a parametric family of priors { Pθ̃ : θ̃ ∈ RNP } , NP ∈ N, the space of hyper-posteriors consists of all distributions over RNP . We will limit our search to a family of isotropic Gaussian distributions defined by Qθ , N ( θ, κ2QINP×NP ) , where κQ > 0 is a predefined constant. Notice that Q appears in the bound (4) in two forms (i) divergence from the hyper-prior D(Q||P) and (ii) expectations over P ∼ Q.\nBy setting the hyper-prior as zero-mean isotropic Gaussian, P = N ( 0, κ2PINP×NP ) , where κP > 0 is another constant, we get a simple form for the KL-divergence term, D(Qθ||P) = 12κ2P ‖θ‖ 2 2 . Note that the hyper-prior acts as a regularization term which prefers solutions with small L2 norm.\nThe expectations can be approximated by averaging several Monte-Carlo samples of P . Notice that sampling from Qθ means adding Gaussian noise to the parameters θ during training, θ̃ = θ + εP , εP ∼ N ( 0, κ2QINP×NP ) . This means the learned parameters must be robust to perturbations, which encourages selecting solutions which are less prone to over-fitting."
  }, {
    "heading": "4.2. Joint Optimization",
    "text": "The term appearing on the RHS of the meta-learning bound in (4) can be compactly written as\nJ(θ) , 1\nn n∑ i=1 Ji(θ) + Υ(θ), (5)\nwhere we defined,\nJi(θ) , E θ̃∼Qθ\nêri ( Qi(Si, Pθ̃), Si ) (6)\n+ √√√√D(Qθ||P) + Eθ̃∼Qθ D(Q(Si, Pθ̃)||Pθ̃)+ log 2nmiδ 2(mi − 1) ,\nΥ(θ) ,\n√ D(Qθ||P) + log 2nδ\n2(n− 1) . (7)\nTheorem 2 allows us to choose any procedure Q(Si, P ) : Zmi ×M→M as a base learner. We will use a procedure which minimizes Ji(θ) due to the following advantages: (i) It minimizes a bound on the expected error of the observed task 6. (ii) It uses the prior knowledge gained from the prior P to get a tighter bound and a better learning objective. (iii) As will be shown next, formulating the single task learning as an optimization problem enables joint learning of the shared prior and the task posteriors.\nTo formulate the single-task learning as an optimization problem, we choose a parametric form for the posterior of\n6See section A.1 in the supplementary material.\neach task Qφi , φi ∈ RNQ (see section 4.3 for an explicit example). The base-learning algorithm can be formulated as φ∗i = argminφi Ji(θ, φi), where we abuse notation by denoting the term Ji(θ) evaluated with posterior parameters φi as Ji(θ, φi). The meta-learning problem of minimizing J(θ) over θ can now be written more explicitly,\nmin θ,φ1,...,φn\n{ 1\nn n∑ i=1 Ji(θ, φi) + Υ(θ)\n} . (8)\nThe optimization process is illustrated in Figure 2."
  }, {
    "heading": "4.3. Distributions Model",
    "text": "In this section we make the meta-learning optimization problem (8) more explicit by defining a model for the posterior and prior distributions. First, we define the hypothesis class H as a family of functions parameterized by a weight vector{ hw : w ∈ Rd } . Given this parameterization, the posterior and prior are distributions over Rd.\nWe will present an algorithm for any differentiable model 7, but our aim is to use neural network (NN) architectures. In fact, we will use Stochastic NNs (Graves, 2011; Blundell et al., 2015) since in our setting the weights are random and we are optimizing their posterior distribution. The techniques presented next will be mostly based on Blundell et al. (2015). Next we define the posteriors Qφi , i = 1, ..., n, and the prior Pθ as factorized Gaussian distributions8,\nPθ(w) = d∏ k=1 N ( wk;µP,k, σ 2 P,k ) (9)\nQφi(w) = d∏ k=1 N ( wk;µi,k, σ 2 i,k ) (10)\n7The only assumption on { hw : w ∈ Rd } is that the loss function `(hw, z) is differentiable w.r.t w. 8This choice makes optimization easier, but in principle we can use other distributions as long as the density function is differentiable w.r.t. the parameters.\nwhere for each task, the posterior parameters vector φi = (µi, ρi) ∈ R2d is composed of the means and log-variances of each weight , µi,k and ρi,k = log σ2P,k, k = 1, ..., d. 9 The shared prior vector θ = (µP , ρP ) ∈ R2d has a similar structure. Since we aim to use deep models where d could be in the order of millions, distributions with more parameters might be impractical.\nSince Qφi and Pθ are factorized Gaussian distributions the KL-divergence, D(Qφi ||Pθ), takes a simple analytic form,\n1\n2 d∑ k=1\n{ log\nσ2P,k σ2i,k + σ2i,k + (µi,k − µP,k)\n2\nσ2P,k − 1\n} . (11)"
  }, {
    "heading": "4.4. Optimization Technique",
    "text": "As an underlying optimization method, we will use stochastic gradient descent (SGD). In each iteration, the algorithm takes a parameter step in a direction of an estimated negative gradient. As is well known, lower variance facilitates convergence and its speed. Recall that each single-task bound is composed of an empirical error term and a complexity term (6). The complexity term is a simple function of D(Qφi ||Pθ) (11), which can easily be differentiated analytically. However, evaluating the gradient of the empirical error term is more challenging.\nRecall the definition of the empirical error, êr (Qφi , Si) = Ew∼Qφi (1/mi) ∑mi j=1 ` (hw, zi,j). This term poses two major challenges. (i) The data set Si could be very large making it expensive to cycle over all the mi samples. (ii) The term ` (hw, zj) might be highly non-linear in w, rendering the expectation intractable. Still, we can get an unbiased and low variance estimate of the gradient.\nFirst, instead of using all of the data for each gradient estimation we will use a randomly sampled mini-batch S′i ⊂ Si. Next, we require an estimate of a gradient of the form ∇φ E\nw∼Qφ f(w) which is a common problem in\nmachine learning. We will use the ‘re-parametrization trick’ (Kingma & Welling, 2013; Rezende et al., 2014) which is an efficient and low variance method 10 . The re-parametrization trick is easily applicable in our setup since we are using Gaussian distributions. The trick is based on describing the Gaussian distribution w ∼ Qφi (9) as first drawing ε ∼ N (0̄, Id×d) and then applying the deterministic function w(φi, ε) = µi + σi ε (where\n9Note that we use ρ = log σ2 as a parameter in order to keep the parameters unconstrained (while σ2 = exp(ρ) is guaranteed to be strictly positive).\n10In fact, we will use the ‘local re-parameterization trick’ (Kingma et al., 2015) in which we sample a different ε for each data point in the batch, which reduces the variance of the estimate. To make the computation more efficient with neural-networks, the random number generation is performed w.r.t the activations instead of the weights (see Kingma et al. (2015) for more details.).\nis an element-wise multiplication). Therefore, we get ∇φ E\nw∼Qφ f(w) = ∇φ E ε∼N (0̄,Id×d) f(w(φi, ε)). The expectation can be approximated by averaging a small number of Monte-Carlo samples with reasonable accuracy. For a fixed sampled ε, the gradient∇φf(w(φi, ε)) is easily computable with backpropagation.\nIn summary, the Meta-Learning by Adjusting Priors (MLAP) algorithm is composed of two phases In the first phase (Algorithm 1, termed “meta-training”) several observed “training tasks” are used to learn a prior. In the second phase (Algorithm 2, termed “meta-testing”) the previously learned prior is used for the learning of a new task (which was unobserved in the first phase). Note that the first phase can be used independently as a multi-task learning method. Both algorithms are described in pseudo-code in the supplementary material (section A.4) 11 12."
  }, {
    "heading": "5. Experimental Demonstration",
    "text": "In this section we demonstrate the performance of our transfer method with image classification tasks solved by deep neural networks. In image classification, the data samples, z , (x, y), consist of a an image, x, and a label, y. The hypothesis class { hw : w ∈ Rd } is the set of neural networks with a given architecture (which will be specified later). As a loss function `(hw, z) we will use the cross-entropy loss. While the theoretical framework is defined with a bounded loss, in our experiments we use an unbounded loss function in the learning objective. Still, we can have theoretical guarantees on a variation of the loss which is clipped to [0, 1]. Furthermore, in practice the loss function is almost always smaller than one.\nWe conduct two experiments with two different task environments, based on augmentations of the MNIST dataset (LeCun, 1998). In the first environment, termed permuted labels, each task is created by a random permutation of the labels. In the second environment, termed permuted pixels, each task is created by a permutation of the image pixels. The pixel permutations are created by a limited number of location swaps to ensure that the tasks stay reasonably related.\nIn both experiments, the meta-training set is composed of tasks from the environment with 60, 000 training examples. Following the meta-training phase, the learned prior is used to learn a new meta-test task with fewer training samples (2, 000). The network architecture used for the permutedlabels experiment is a small CNN with 2 convolutionallayers, a linear hidden layer and a linear output layer. In\n11Code is available at: https://github.com/ ron-amit/meta-learning-adjusting-priors.\n12For a visual illustration of the algorithm using a toy example see section A.6 in the supplementary material.\nthe permuted-pixels experiment we used a fully-connected network with 3 hidden layers and a linear output layer. See section A.5 for more implementation details.\nWe compare the average test error of learning a new task from each environment when using the following methods. As a baseline, we measure the performance of learning from scratch, i.e., with no transfer from the meta-training tasks. Scratch-D: deterministic (standard) learning from scratch. Scratch-S: stochastic learning from scratch (using a stochastic network with no prior/complexity term).\nOther methods transfer knowledge from only one of the meta-training tasks. Warm-start: Standard learning with initial weights taken from the standard learning of a single task from the meta-training set. Oracle: Same as the previous method, but some of the layers are frozen (unchanged from their initial value) depending on the experiment. In the permuted labels experiment all layers besides the output are frozen. In the permuted pixels we freeze all layers except the input layer. We refer to this method as ‘oracle’ since the transfer technique is tailored to each task-environment, while the other methods are applied identically in any environment (and so must learn to adjust to the environment automatically).\nFinally, we compare methods which transfer knowledge from all of the training tasks: MLAP-M: The objective is based on Theorem 2 - the meta-learning bound obtained using Theorem 1 (McAllester’s single-task bound). MLAP-S: The objective is based on the meta-learning bound derived from Seeger’s single-task bound (see section A.2 in the supplementary material, eq.(18)). MLAP-PL: In this method we use the main theorem of Pentina & Lampert (2014) as an objective for the algorithm, instead of Theorem 2. MLAPVB: In this method the learning objective is derived from a Hierarchal Bayesian framework using variational Bayes tools 13. Averaged: Each of the training tasks is learned in a standard way to obtain a weights vector, wi. The learned prior is set as an isotropic Gaussian with unit variances and a mean vector which is the average of wi, i = 1, .., n. This prior is used for meta-testing as in MLAP-S. MAML: The Model-Agnostic-Meta-Learning (MAML) algorithm by Finn et al. (2017). In MAML the base learner takes few gradient steps from an initial point, θ, to adapt to a task. The meta-learner optimizes θ based on the sum of losses on the observed tasks after base-learning. We tested several hyper-parameters and report the best results (see details in the supplementary material A.5).\nTable 1 summarizes the results for the permuted labels experiment with 5 training-tasks and the permuted pixels experiment with 200 pixel swaps and 10 training-tasks. In\n13See section A.3 in the supplementary material for details. The explicit learning objective is in equation (23).\nthe permuted labels experiment the best results are obtained with the “oracle” method. Recall that the oracle method has the “unfair” advantage of a “hand-engineered” transfer technique which is based on knowledge about the problem. In contrast, the other methods must automatically learn the task environment by observing several tasks.\nThe MLAP-M and MLAP-S variants of the MLAP algorithm improves considerably over learning from scratch and over the naive warm-start transfer. They even improve over the “oracle” method in the permuted pixels experiment. The result of the MLAP-VB are close to the MLAP-M and MLAP-S variants. However the MLAP-PL variant performed much worse since the complexity terms are disproportionately large compared to the empirical error terms. This demonstrates the importance of using the tight generalization bound developed in our work as a learning objective. The results for the “averaged-prior” method are about the same as learning from scratch. Due to the high non-linearity of the problem, averaging weights was not expected to perform well.\nThe results of the MLAP algorithm are slightly better than MAML. Note that in MAML the meta-learning only infers an initial point for base-learning. Thus there is a trade-off in choosing the number of adaptation steps. Taking many gradient steps exploits a larger number of samples but the effect of the initial weights diminishes. Also, taking a large number of steps is computationally infeasible in meta-training. Therefore MAML is especially suited for few-shot learning, which is not the case in our experiment. In our method we infer a prior that serves both as an initial point and as a regularizer which can fix some of the weights, while allowing variation in others, depending on the amount of data. Recent work by Grant et al. (2018) showed that MAML can be interpreted as an approximate empirical Bayes procedure. This interesting perspective, differs from the present\ncontribution that is based on generalization bounds within a non-Bayesian setting.\nNext we investigate whether using more training tasks improves the quality of the learned prior. In Figure 3 we plot the average test error of learning a new task based on the number of training-tasks in the different environments, namely the permuted labels environment, and the permuted pixels environment with 100, 200, 300 pixel swaps. We used the MLAP-S variant of the algorithm. The results clearly show that the more tasks are used to learn the prior, the better the performance on the new task. For example, in the permuted labels case, a prior that is learned based on one or two tasks leads to negative transfer, i.e, worse results than standard learning from scratch (with no transfer), which achieves 2.27% error. However after observing 3 or more tasks, the transfered prior facilitates learning with lower expected error. In the permuted pixels experiment, standard learning from scratch achieves 7.9% test error. The number of training tasks needed for positive transfer depends on the number of pixels swapped. A higher number of swaps means larger variation in the task environment and more training-tasks are needed to learn a beneficial prior.\nAnalysis of learned prior Qualitative examination of the learned prior affirms that it has indeed adjusted to each task environment. In Figure 4 we inspect the average logvariance parameter the learned prior assigns to the weights of each layer in the network. Higher values of this parameter indicate that the weight is more flexible to change. i.e, it is more weakly penalized for deviating form the nominal prior value. In the permuted-labels experiment the learned prior assigns low variance to the lower layers (fixed representation) and high variance to the output layer (which enable easy adjustment to different label permutations). As expected, in the permuted-pixels experiment the opposite phenomenon occurs. The mapping from the final hidden layer to the output becomes fixed, and the mapping from the input to the final hidden layer (representation) has more flexibility to change in light of the task data."
  }, {
    "heading": "6. Discussion and Future Work",
    "text": "We have presented a framework for meta-learning, motivated by extended PAC-Bayes generalization bounds, and implemented through the adjustment of a learned prior, based on tasks encountered so far. The framework bears conceptual similarity to the empirical Bayes method while not being Bayesian, and is implemented at the level of tasks rather than samples (see Section A.3 in the supplementary material for details about a Bayesian perspective). Combining the flexibility of the approach, with the rich representational structure of deep neural networks, and learning through gradient based methods leads to an efficient procedure for meta-learning, as motivated theoretically and demonstrated empirically. While our experimental results are preliminary, we believe that our work attests to the utility of using rigorous performance bounds to derive learning algorithms, and demonstrates that tighter bounds indeed lead to improved performance.\nThere are several open issues to consider. First, the current version learns to solve all available tasks in parallel, while a more useful procedure should be sequential in nature. This can be easily incorporated into our framework by updating the prior following each novel task. Second, our method requires training stochastic models which is challenging due to the the high-variance gradients. We would like to develop new methods within our framework which have more stable convergence and are easier to apply in larger scale problems. Third, there is much current effort in applying meta-learning ideas to reinforcement learning, for example, Teh et al. (2017) presents a heuristically motivated framework that is conceptually similar to ours. An interesting challenge would be to extend our techniques to derive meta-learning algorithms for reinforcement learning based on performance bounds."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "We thank Asaf Cassel, Guy Tennenholtz, Baruch Epstein, Daniel Soudry, Elad Hoffer and Tom Zahavy for helpful discussions of this work, and the anonymous reviewers for their helpful comment. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. The work was partially supported by the Ollendorff Center of the Viterbi Faculty of Electrical Engineering at the Technion."
  }],
  "year": 2018,
  "references": [{
    "title": "Regret Bounds for Lifelong Learning",
    "authors": ["P. Alquier", "T.T. Mai", "M. Pontil"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2017
  }, {
    "title": "Learning to learn by gradient descent by gradient descent",
    "authors": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "N. de Freitas"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "PAC-Bayesian aggregation and multi-armed bandits",
    "authors": ["Audibert", "J.-Y"],
    "venue": "PhD thesis, Université Paris-Est,",
    "year": 2010
  }, {
    "title": "A model of inductive bias learning",
    "authors": ["J. Baxter"],
    "venue": "J. Artif. Intell. Res.(JAIR),",
    "year": 2000
  }, {
    "title": "A theory of learning from different domains",
    "authors": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"],
    "venue": "Machine learning,",
    "year": 2010
  }, {
    "title": "Weight uncertainty in neural network",
    "authors": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Multitask learning",
    "authors": ["R. Caruana"],
    "venue": "Machine Learning,",
    "year": 1997
  }, {
    "title": "PAC-Bayesian supervised classification",
    "authors": ["O. Catoni"],
    "venue": "Lecture Notes-Monograph Series. IMS,",
    "year": 2007
  }, {
    "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
    "authors": ["G.K. Dziugaite", "D.M. Roy"],
    "venue": "In Conference on Uncertainty in Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "Towards a neural statistician",
    "authors": ["H. Edwards", "A. Storkey"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2016
  }, {
    "title": "Model-agnostic metalearning for fast adaptation of deep networks",
    "authors": ["C. Finn", "P. Abbeel", "S. Levine"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "A theoretical framework for deep transfer learning",
    "authors": ["T. Galanti", "L. Wolf", "T. Hazan"],
    "venue": "Information and Inference: A Journal of the IMA,",
    "year": 2016
  }, {
    "title": "PAC-Bayesian theory meets Bayesian inference",
    "authors": ["P. Germain", "F. Bach", "A. Lacoste", "S. Lacoste-Julien"],
    "venue": "In Advances In Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Recasting gradient-based meta-learning as hierarchical Bayes",
    "authors": ["E. Grant", "C. Finn", "S. Levine", "T. Darrell", "T. Griffiths"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2018
  }, {
    "title": "Practical variational inference for neural networks",
    "authors": ["A. Graves"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2011
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2013
  }, {
    "title": "Variational dropout and the local reparameterization trick",
    "authors": ["D.P. Kingma", "T. Salimans", "M. Welling"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Overcoming catastrophic forgetting in neural networks",
    "authors": ["J. Kirkpatrick", "R. Pascanu", "N. Rabinowitz", "J. Veness", "G. Desjardins", "A.A. Rusu", "K. Milan", "J. Quan", "T. Ramalho", "A Grabska-Barwinska"],
    "venue": "Proceedings of the National Academy of Sciences (PNAS),",
    "year": 2016
  }, {
    "title": "The mnist database of handwritten digits. http://yann",
    "authors": ["Y. LeCun"],
    "venue": "lecun. com/exdb/mnist/,",
    "year": 1998
  }, {
    "title": "Tighter PAC-Bayes bounds through distribution-dependent priors",
    "authors": ["G. Lever", "F. Laviolette", "J. Shawe-Taylor"],
    "venue": "Theoretical Computer Science,",
    "year": 2013
  }, {
    "title": "Algorithmic stability and meta-learning",
    "authors": ["A. Maurer"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2005
  }, {
    "title": "Transfer bounds for linear feature learning",
    "authors": ["A. Maurer"],
    "venue": "Machine learning,",
    "year": 2009
  }, {
    "title": "The benefit of multitask representation learning",
    "authors": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2016
  }, {
    "title": "PAC-Bayesian model averaging",
    "authors": ["D.A. McAllester"],
    "venue": "In Conference on Computational Learning Theory (COLT),",
    "year": 1999
  }, {
    "title": "Risk bounds for transferring representations with and without fine-tuning",
    "authors": ["D. McNamara", "Balcan", "M.-F"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "A PAC-Bayesian bound for lifelong learning",
    "authors": ["A. Pentina", "C.H. Lampert"],
    "venue": "In International Conference on Machine (ICML),",
    "year": 2014
  }, {
    "title": "Lifelong learning with noniid tasks",
    "authors": ["A. Pentina", "C.H. Lampert"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2014
  }, {
    "title": "ELLA: An efficient lifelong learning algorithm",
    "authors": ["P. Ruvolo", "E. Eaton"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2013
  }, {
    "title": "PAC-Bayesian generalisation error bounds for gaussian process classification",
    "authors": ["M. Seeger"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2002
  }, {
    "title": "PAC-Bayesian inequalities for martingales",
    "authors": ["Y. Seldin", "F. Laviolette", "N. Cesa-Bianchi", "J. Shawe-Taylor", "P. Auer"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2012
  }, {
    "title": "Distral: Robust multitask reinforcement learning",
    "authors": ["Y. Teh", "V. Bapst", "W.M. Czarnecki", "J. Quan", "J. Kirkpatrick", "R. Hadsell", "N. Heess", "R. Pascanu"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2017
  }, {
    "title": "Is learning the n-th thing any easier than learning the first? In Advances in neural information processing systems (NIPS)",
    "authors": ["S. Thrun"],
    "year": 1996
  }, {
    "title": "A perspective view and survey of meta-learning",
    "authors": ["R. Vilalta", "Y. Drissi"],
    "venue": "Artificial Intelligence Review,",
    "year": 2002
  }, {
    "title": "Knowledge transfer for deep reinforcement learning with hierarchical experience replay",
    "authors": ["H. Yin", "S.J. Pan"],
    "venue": "In AAAI Conference on Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "How transferable are features in deep neural networks? In Advances in neural information processing systems",
    "authors": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"],
    "year": 2014
  }],
  "id": "SP:6748cab9c2a36d5edb0d787baf2d99ab6e067692",
  "authors": [{
    "name": "Ron Amit",
    "affiliations": []
  }, {
    "name": "Ron Meir",
    "affiliations": []
  }],
  "abstractText": "In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumption that future tasks are ‘related’ to previous tasks, the accumulated knowledge should be learned in a way which captures the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of new tasks. We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to metalearning. Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks. We develop a gradient-based algorithm which minimizes an objective function derived from the bounds and demonstrate its effectiveness numerically with deep neural networks. In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.",
  "title": "Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory"
}