{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 193–203 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n193"
  }, {
    "heading": "1 Introduction",
    "text": "Coreference resolution is the task of recognizing different expressions that refer to the same entity. The referring expressions are called mentions. For instance, the sentence “[Susan]1 sent [her]1 daughter to a boarding school” contains two coreferring mentions. “her” is an anaphor which refers to the antecedent “Susan”.\nThe availability of coreference information benefits various Natural Language Processing (NLP) tasks including automatic summarization, question answering, machine translation and information extraction. Current coreference developments are almost only targeted at improving scores on\n∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universität Darmstadt, https://www.ukp.tu-darmstadt.de.\nthe CoNLL official test set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1.\nIn Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference resolution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and Barzilay (2005),Yu and Ji (2016), and Elsner and Charniak (2008).\nIn this paper, we explore the role of linguistic features for improving generalization. The incorporation of linguistic features is considered as a potential solution for building more generalizable NLP systems2. While linguistic features3\n1The single model of Lee et al. (2017) is used here. 2E.g. there is a dedicated workshop for this topic https: //sites.google.com/view/relsnnlp. 3We refer to features that are based on linguistic intu-\nwere shown to be important for coreference resolution, e.g. Uryupina (2007) and Bengtson and Roth (2008), state-of-the-art systems no longer use them and mainly rely on word embeddings and deep neural networks. Since all recent systems are using neural networks, we focus on the effect of linguistic features on a neural coreference resolver.\nThe contributions of this paper are as follows:\n– We show that linguistic features are more beneficial for a neural coreference resolver if we incorporate features and subsets of their values that are informative for discriminating coreference relations. Otherwise, employing linguistic features with all their values only slightly affects the performance and generalization.\n– We propose an efficient discriminative pattern mining algorithm, called EPM, for determining (feature, value) pairs that are informative for the given task. We show that while the informativeness of EPM mined patterns is onpar with those of its counterparts, it scales best to large datasets.4\n– By improving generalization, we achieve state-of-the-art performance on all examined out-of-domain evaluations. Our out-ofdomain performance on WikiCoref is on-par with that of Ghaddar and Langlais (2016b)’s coreference resolver, which is a system specifically designed for WikiCoref and uses its domain knowledge."
  }, {
    "heading": "2 Importance of Features in Coreference",
    "text": "Uryupina (2007)’s thesis is one of the most thorough analyses of linguistically motivated features for coreference resolution. She examines a large set of linguistic features, i.e. string match, syntactic knowledge, semantic compatibility, discourse structure and salience, and investigates their interaction with coreference relations. She shows that even imperfect linguistic features, which are extracted using error-prone preprocessing modules, boost the performance and argues that coreference resolvers could and should benefit from linguistic theories. Her claims are based on analyses on the MUC dataset. Ng and Cardie (2002), Yang et al. (2004), Ponzetto and Strube (2006), Bengtson and\nitions, e.g. string match, or are acquired from linguistic preprocessing modules, e.g. POS tags, as linguistic features.\n4The EPM code is available at https://github. com/ns-moosavi/epm\nRoth (2008), and Recasens and Hovy (2009) also study the importance of features in coreference resolution.\nApart from the mentioned studies, which are mainly about the importance of individual features, studies like Björkelund and Farkas (2012), Fernandes et al. (2012), and Uryupina and Moschitti (2015) generate new features by combining basic features. Björkelund and Farkas (2012) do not use a systematic approach for combining features. Fernandes et al. (2012) use the Entropy guided Feature Induction (EFI) approach (Fernandes and Milidiú, 2012) to automatically generate discriminative feature combinations. The first step is to train a decision tree on a dataset in which each sample consists of features describing a mention pair. The EFI approach traverses the tree from the root in a depth-first order and recursively builds feature combinations. Each pattern that is generated by EFI starts from the root node. As a result, EFI tends to generate long patterns. A decision tree does not represent all patterns of data. Therefore, it is not possible to explore all feature combinations from a decision tree.\nUryupina and Moschitti (2015) propose an alternative approach to EFI. They formulate the problem of generating feature combinations as a pattern mining approach. They use the Jaccard Item Mining (JIM) algorithm5 (Segond and Borgelt, 2011). They show that the classifier that uses the JIM features significantly outperforms the one that employs the EFI features."
  }, {
    "heading": "3 Baseline Coreference Resolver",
    "text": "deep-coref (Clark and Manning, 2016a) and e2ecoref (Lee et al., 2017) are among the best performing coreference resolvers from which e2ecoref performs better on the CoNLL test set. deepcoref is a pipelined system, i.e. a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deepcoref has three variations: (1) “ranking” uses the slack-rescaled max-margin training objective of Wiseman et al. (2015), (2) “reinforce” is a variation of the “ranking” model in which the hyperparameters are set in a reinforcement learning framework (Sutton and Barto, 1998), and (3) “top-\n5http://www.borgelt.net/jim.html\npairs” is a simple variation of the “ranking” model that uses a probabilistic objective function and is used for pretraining the “ranking” model.\ne2e-coref is an end-to-end system that jointly models mention detection and coreference resolution. It considers all possible (start, end) word spans of each sentence as candidate mentions. Apart from a single model, e2e-coref includes an ensemble of five models.\nWe use deep-coref as the baseline in our experiments. The reason is that some of the examined features require the head of each mention to be known, e.g. head match, while e2e-coref mentions do not have specific heads and heads are automatically determined using an attention mechanism. We also observe that if we limit e2e-coref candidate spans to those that correspond to deep-coref’s detected mentions, the performance of e2e-coref drops to a level on-par with deep-coref6."
  }, {
    "heading": "4 Examined Features",
    "text": "The examined linguistic features include string match, syntactic, shallow semantic and discourse features. Mention-based features include: – Mention type: proper, nominal or pronominal\n– Fine mention type: proper, definite or indefinite nominal, or the citation form of pronouns\n– Gender: female, male, neutral, unknown\n– Number: singular, plural, unknown\n– Animacy: animate, inanimate, unknown\n– Named entity type: person, location, organization, date, time, number, etc.\n– Dependency relation: enhanced dependency relation (Schuster and Manning, 2016) of the head word to its parent\n– POS tags of the first, last, head, two words preceding and following of each mention\nPairwise features include: – Head match: both mentions have the same\nhead, e.g. “red hat” and “the hat”\n– String of one mention is contained in the other, e.g. “Mary’s hat” and “Mary”\n– Head of one mention is contained in the other, e.g. “Mary’s hat” and “hat”\n– Acronym, e.g. “Heidelberg Institute for Theoretical Studies” and “HITS”\n6 The CoNLL score of the e2e-coref single model on the CoNLL development set drops from 67.36 to 65.81, while that of the deep-coref “ranking” model is 66.09.\n– Compatible pre-modifiers: the set of premodifiers of one mention is contained in that of the other, e.g. “the red hat that she is wearing” and “the red hat”\n– Compatible7 gender, e.g. “Mary” and “women”\n– Compatible number, e.g. “Mary” and “John”\n– Compatible animacy, e.g. “those hats” and “it”\n– Compatible attributes: compatible gender, number and animacy, e.g. “Mary” and “she”\n– Closest antecedent that has the same head and compatible premodifiers, e.g. “this new book” and “This book” in “Take a look at this new book. This book is one of the best sellers.”\n– Closest antecedent that has compatible attributes, e.g. the antecedent “Mary” and the anaphor “she” in the sentence “John saw Mary, and she was in a hurry”\n– Closest antecedent that has compatible attributes and is a subject, e.g. the antecedent “Mary” and the anaphor “she” in the sentence “Mary saw John, but she was in a hurry”\n– Closest antecedent that has compatible attributes and is an object, e.g. “Mary” and “she” in “John saw Mary, and she was in a hurry”\nThe last three features are similar to the discourselevel features discussed by Uryupina (2007), which are created by combining proximity, agreement and salience properties. She shows that such features are useful for resolving pronouns. we estimate proximity by considering the distance of two mentions. The salience is also incorporated by discriminating subject or object antecedents. We do not use any gold information. All features are extracted using Stanford CoreNLP (Manning et al., 2014)."
  }, {
    "heading": "5 Impact of Linguistic Features",
    "text": "In this section, we examine the effect of employing all linguistic features described in Section 4 in a neural coreference resolver, i.e. deep-coref. We use MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), LEA (Moosavi and Strube, 2016), and the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC, B3, and CEAFe, for evaluations.\nThe results of employing those features in deepcoref’s “ranking” and “top-pairs” models on the\n7One value is unknown, or both values are identical.\nCoNLL development set are reported in Table 1.\nThe rows “ranking” and “top-pairs” show the base results of deep-coref’s “ranking” and “toppairs” models, respectively. “+linguistic” rows represents the results for each of the mentionranking models in which the feature set of Section 4 is employed. The gender, number, animacy and mention type features, which have less than five values, are converted to binary features. Named entity and POS tags, and dependency relations are represented as learned embeddings.\nWe observe that incorporating all the linguistic features bridges the gap between the performance of “top-pairs” and “ranking”. However, it does not improve significantly over “ranking”. Henceforth, we use the “top-pairs” model of deep-coref as the baseline model to incorporate linguistic features.\nTo assess the impact on generalization, we evaluate “top-pairs” and “+linguistic”8 models that are trained on CoNLL, on WikiCoref (see Table 2). We observe that the impact on generalization is also not notable, i.e. the CoNLL score improves only by 0.5pp over “ranking”.\nBased on an ablation study, while our feature set contains numerous features, the resulting improvements of “linguistic” over “top-pairs” mainly comes from the last four pairwise features in Section 4, which are carefully designed features."
  }, {
    "heading": "6 Better Exploiting Linguistic Features",
    "text": "As discussed by Moosavi and Strube (2017a), there is a large lexical overlap between the coreferring mentions of the CoNLL training and evaluation sets. As a result, lexical features provide a\n8i.e. “top-pairs+linguistic”\nvery strong signal for resolving coreference relations.\nFor linguistic features to be more effective in current coreference resolvers, which rely heavily on lexical features, they should also provide a strong signal for coreference resolution.\nAdditional linguistic features are not necessarily all informative for coreference resolution, especially if they are extracted automatically and are noisy. Besides, for features with multiple values, e.g. mention-based features, only a small subset of values may be informative.\nTo better exploit linguistic features, we only employ (feature, value) pairs9 that are informative for coreference resolution. Coreference resolution is a complex task in which features have complex interactions (Recasens and Hovy, 2009). As a result, we cannot determine the informativeness of feature-values in isolation.\nWe use a discriminative pattern mining approach (Cheng et al., 2007, 2008; Batal and Hauskrecht, 2010) that examines all combinations of feature-values, up to a certain length, and determines which feature-values are informative when they are considered in combination.\nDue to the large data size (all mention-pairs of the CoNLL training data) and the high dimensionality of feature-values, compared to common evaluation sets of pattern mining methods, the existing discriminative pattern mining approaches were not applicable to our data. In this section, we propose an efficient discriminative pattern mining approach, called Efficient Pattern Miner (EPM), that is scalable to large NLP datasets. The most important properties of EPM are (1) it examines all frequent feature-values combinations, up to the desired length, (2) it is scalable to large datasets, and (3) it is only data dependent and independent of the coreference resolver."
  }, {
    "heading": "6.1 Notation",
    "text": "We use the following notations and definitions throughout this section:\n– D = {Xi, c(Xi)}ni=1: set of n training samples. Xi is the set of feature-values that describes the ith sample. c(Xi) ∈ C is the label of Xi, e.g. coreferent and non-coreferent. – A = {a1, . . . , al}: set of all feature-values present in D. Each ai ∈ A is called an item, e.g. ai =“anaphor type=proper”.\n9Henceforth, we refer to them as feature-values.\n– p: pattern p = {ai1 , . . . , aik} is a set of one or more items, e.g. p ={“anaphor type=proper”, “antecedent type=proper”}.\n– support(p, ci): the number of samples that contain pattern p and are labeled with ci."
  }, {
    "heading": "6.2 Data Structure",
    "text": "For representing the input samples, we use the Frequent Pattern Tree (FP-Tree) structure that is the data structure of the FP-Growth algorithm (Han et al., 2004), i.e. one of the most common algorithms for frequent pattern mining. FP-Tree provides a structure for representing all existing patterns of data in a compressed form. Using the FP-Tree structure allows an efficient enumeration of all frequent patterns. In the FP-Tree structure, items are arranged in descending order of frequency. Frequency of an item corresponds to∑\nci∈C support(ai, ci). Except for the root, which is a null node, each node n contains an item ai ∈ A. It also contains the support values of ai in the subpath of the tree that starts from the root and ends with n, i.e. supportn(ai, cj).\nThe FP-Tree construction method (Han et al., 2004) is as follows: (a) scan D to collect the set of all items, i.e. A. Compute support(ai, cj) for each item ai ∈ A and label cj ∈ C. Sort A’s members in descending order according to their frequencies, i.e. ∑ ci∈C support(ai, ci). (b) create a null-labeled node as the root, and (c) scan D again. For each (Xi, c(Xi)) ∈ D:\n1. Order all items aj ∈ Xi according to the order in A.\n2. Set the current node (T ) to the root.\n3. Consider Xi = [ak|X̄i], where ak is the first (ordered) item of xi , and X̄i = Xi − ak. If T has a child n that contains ak then increment supportn(ak, c(Xi)) by one. Otherwise, create a new node n that contains ak with supportn(ak, c(Xi)) = 1. Add n to the tree as a child of T .\n4. If X̄i is non-empty, set T to n. Assign Xi = X̄i and go to step 3.\nAs an example, assume D contains the following two samples:\nX1={ana-type=NAM, ant-type=NAM, headmatch=F}, C(X1) = 0\nX2={ana-type=NAM, ant-type=NAM, headmatch=T}, C(X2) = 1\nBased on these samples A={ana-type=NAM, ant-type=NAM, head-match=F, headmatch=T}, support(ai, 0)ai∈A= {1,1,1,0}, and support(ai, 1)ai∈A={1,1,0,1}. If we sort A based on ai’s frequencies (support(ai, 0) + support(ai, 1)), the ordering of A’s items will remain the same.\nThe FP-Tree construction steps for the above samples are demonstrated in Figure 1. ana-type, ant-type, and head-match features are abbreviated as ana, ant, and head, respectively.\nFrom an initial FP-Tree (T ) that represents all existing patterns, one can easily obtain a new FPTree in which all patterns include a given pattern p. This can be done by only including sub-paths of T that contain pattern p. The new tree is called conditional FP-Tree of p, Tp. An example of conditional FP-Tree is included in the supplementary materials."
  }, {
    "heading": "6.3 Informativeness Measures",
    "text": "We use a discriminative power and an information novelty measure for determining informativeness. We also use a frequency measure which determines the required minimum frequency of a pattern in training samples. It helps to avoid overfitting to the properties of the training data. Discriminative power: We use the G2 likelihood ratio test (Agresti, 2007) in order to choose patterns whose association with the class variable is statistically significant.10 The G2 test is successfully used for text analysis (Dunning, 1993). Information Novelty: A large number of redundant patterns can be generated by adding irrelevant items to a base pattern that is discriminative itself.\n10A pattern is considered discriminative if the corresponding p-value is less than a fixed threshold (0.01).\nWe consider the pattern p as novel if (1) p predicts the target class label c significantly better than all of its containing items, and (2) p predicts c significantly better than all of its sub-patterns that satisfy the frequency, discriminative power, and the first information novelty conditions. Similar to Batal and Hauskrecht (2010), we employ a binomial distribution to determine information novelty."
  }, {
    "heading": "6.4 Mining Algorithm",
    "text": "The EPM algorithm is summarized in Algorithm 1. It takes FP-Tree T , pattern p on which T is conditioned, and set of items (Aj ⊂ A) whose combinations with p will be examined. Initially, p is empty and the FP-Tree is constructed based on all frequent items of data and Aj = A. Resulting patterns are collected in P .\nFor each ai ∈ Aj , the algorithm builds new pattern q by combining ai with p. frequent(q) checks whether q meets the frequency condition. If q is frequent, the algorithm continues the search process. Otherwise, it is not possible to build any frequent pattern out of a non-frequent one. Discriminative power and the first condition of information novelty are then checked for pattern q.\nAlgorithm EPM(T , p, Aj) foreach ai ∈ Aj do\nq = p ∪ {ai} if Frequent(q) then\nif Discriminative(q) then if Novel(q) then\nP = P ∪ q end\nend if |q| >= Θl then\ncontinue end construct Tq = q’s conditional tree EPM(Tq, q, ancestors(ai))\nend end Algorithm 1: The EPM algorithm.\nWe use a threshold (Θl) for the maximum length of mined patterns. Θl can be set to large values if more complex and specific patterns are desirable.\nIf |q| is smaller than Θl, the conditional FP-Tree Tq is built that represents patterns of T that include the pattern q. The mining algorithm then continues to recursively search for more specific\npatterns by combining q with the items included in ancestors(ai), which keeps the list of all ancestors of ai in the original FP-Tree. EPM examines all frequent patterns of up to length Θl.\nIf we use a statistical test multiple times, the risk of making false discoveries increases (Webb, 2006). To tackle this, we apply the Bonferroni correction for multiple tests in a post-pruning function after the mining process. This function also applies the second information novelty condition on the resulting patterns."
  }, {
    "heading": "7 Why Use EPM?",
    "text": "In this section, we explain why EPM is a better alternative compared to its counterparts for large NLP datasets. We compare EPM with two efficient discriminative pattern mining algorithms, i.e. Minimal Predictive Patterns (MPP) (Batal and Hauskrecht, 2010) and Direct Discriminative Pattern Mining (DDPMine) (Cheng et al., 2008), on standard machine learning datasets.\nMPP selects patterns that are significantly more predictive than all their sub-patterns. It measures significance by the binomial distribution. For each pattern of length l, MPP checks 2l−1 sub-patterns. DDPMine is an iterative approach that selects the most discriminative pattern at each iteration and reduces the search space of the next iteration by removing all samples that include the selected pattern. DDPMine uses the FP-Tree structure.\nWe show that EPM scales best and compares favorably based on the informativeness of resulting patterns. Due to its efficiency, EPM can handle large datasets similar to ones that are commonly used in various NLP tasks."
  }, {
    "heading": "7.1 Experimental Setup",
    "text": "We use the same FP-Tree implementation for DDPMine and EPM. In all algorithms, we consider a pattern as frequent if it occurs in 10% of the samples of one of the classes. We use Θl = 3 for both MPP and EPM.\nWe perform 5-times repeated 5-fold cross validation and the results are averaged. In each validation, all experiments are performed on the same split. We use a linear SVM, i.e. LIBLINEAR 2.11 (Fan et al., 2008), as the baseline classifier.\nWe use several datasets from the UCI machine learning repository (Lichman, 2013) whose characteristics are presented in the first three columns of Table 3, i.e. the number of (1)\n(real/integer/nominal) features (#Features), (2) frequent items (#FI), and (3) samples (n). We use one[the minority class]-vs-all technique for datasets with more than two classes."
  }, {
    "heading": "7.2 How Informative are EPM Patterns?",
    "text": "To evaluate the informativeness of mined patterns, the common practice is to add them as new features to the feature set of the baseline classifier; the more informative the patterns, the greater impact they would have on the overall performance. All patterns are added as binary features, i.e. the feature is true for samples that contain all items of the corresponding pattern.\nThe effect of the patterns of DDPMine, MPP and EPM on the overall accuracy is presented in Table 3. The columns #Patterns show the number of patterns mined by each of the algorithms. The Orig columns show the results of the SVM using the original feature sets. The DDP, MPP, and EPM columns show the results of the SVM on the datasets for which the feature set is extended by the features mined by DDPMine, MPP, and EPM, respectively. The results of the 5-repeated 5-fold cross validation are reported if each single validation takes less than 10 hours.\nBased on the results of Table 3 (1) EPM efficiently scales to larger datasets, (2) MPP and EPM patterns considerably improves the performance, and (3) EPM has on-par results with MPP while it mines considerably fewer patterns."
  }, {
    "heading": "7.3 How Does it Scale?",
    "text": "Figure 2 compares EPM mining time (in seconds) with those of DDPMine and MPP. The parameter in the parentheses is the pattern size threshold, e.g. Θl = 4 for EPM(4). The experiments that take more than two days are terminated and are not included. EPM is notably faster in comparison to the other two approaches. It is notable that the examined datasets are considerably smaller than\nthe coreference data, which includes more than 33 million samples and 200 frequent feature-values."
  }, {
    "heading": "8 Impact of Informative Feature-values",
    "text": ""
  }, {
    "heading": "8.1 Experimental Setup",
    "text": "For determining informative feature-values, we extract all features for all mention-pairs11 of the CoNLL training data and then apply EPM on this data. In order to prevent learning annotation errors and specific properties of the training data, we consider a pattern as frequent if it occurs in coreference relations of at least m different coreferring anaphors (m = 20). Since the majority of mention-pairs are non-coreferent and we are not interested in patterns for non-coreferring relations, we also consider the coreference probability of each pattern p, i.e. |{Xi|p∈Xi∧c(Xi)=coreferent}||{Xi|p∈Xi}| , in the post-pruning function. The coreference probability should be higher than a threshold (60% in our experiments), so we only mine patterns that are informative for coreferring mentions.\nFor the coreference resolution experiments, instead of incorporating informative patterns, we incorporate feature-values that are included in the\n11Each mention is paired with all the preceding mentions.\ninformative patterns mined by EPM. The reason is that deep-coref, or any other recent coreference resolver, uses a deep neural network, which has a fully automated feature generation process. We add these feature-values as binary features.\nBy setting Θl to five,12 EPM results in 13 pairwise feature-values, 112 POS tags, i.e. 53 POS for anaphors and 59 for antecedents, 25 dependency relations, 26 mention types (mention types or fine mention types), and finally, 14 named entity tags.13\nBased on the observation in Section 5, we use the top-pairs model of deep-coref as the baseline to employ additional features, i.e. “+EPM” is the top-pairs model in which EPM feature-values are incorporated."
  }, {
    "heading": "8.2 Impact on In-domain Performance",
    "text": "The performance of the “+EPM” model compared to recent state-of-the-art coreference models on the CoNLL test set is presented in Table 4. The “single” and “ensemble” rows represent the results of the single and ensemble models of e2e-coref.\nWe also compare EPM with the pattern mining approach used by Uryupina and Moschitti (2015), i.e. Jaccard Item Mining (JIM). For a fair comparison, while Uryupina and Moschitti (2015) used mined patterns for extracting feature templates, we use them for selecting feature-values. We run the JIM algorithm on the same data and with the same setup as that of EPM.14 This results in nine pair-\n12We observe that using larger Θl values will result in many over-specified patterns.\n13Following the previous studies that show different features are of different importance for various types of mentions, e.g. Denis and Baldridge (2008) and Moosavi and Strube (2017b), we mine a separate set of patterns for each type of anaphor. These resulting feature-values are the union of informative feature-values for all types of anaphora.\n14 We set the minimum frequency, maximum pattern length and score+ threshold parameters of JIM to 20, 5 and\nwise features, 260 POS tags, 38 dependency relations, 32 mention types, and 18 named entity tags. The “+JIM” row shows the results of deep-coref top-pairs model in which these feature-values are incorporated. As we see, EPM feature-values result in significantly better performance than those of JIM while the number of EPM feature-values is considerably less than JIM.\nFeature Ablation Table 5 shows the effect of each group of EPM feature-values, i.e. pairwise features, mention types, dependency relations, named entity tags and POS tags, on the performance of “+EPM”. The performance of “+EPM” from which each of the above feature groups is removed, one feature group at a time, is represented as “-pairwise”, “-types”, “-dep”, “-NER”, and “-POS”, respectively. The POS and named entity tags have the least and the pairwise features have the most significant effect. Since pairwise features have the most significant effect, we also perform an experiment in which only pairwise features are incorporated in the “top-pairs” model, i.e. “+pairwise”. The results of “-pairwise” compared to “+pairwise” show that pairwise feature-values have a significant impact, but only when they are considered in combination with other EPM\n0.6.\nfeature-values."
  }, {
    "heading": "8.3 Impact on Generalization",
    "text": "We use the same setup as that of Moosavi and Strube (2017a) for evaluating generalization including (1) training on the CoNLL data and testing on WikiCoref15 and (2) excluding a genre of the CoNLL data from training and development sets and testing on the excluded genre. Similar to Moosavi and Strube (2017a), we use the pt and wb genres for the latter evaluation setup.\nThe results of the first evaluation setup are shown in Table 6. The best performance on WikiCoref is achieved by Ghaddar and Langlais (2016a) (“G&L” in Table 6) who introduced WikiCoref and design a domain-specific coreference resolver that makes use of the Wikipedia markups of a document as well as links to Freebase, which are annotated in WikiCoref.\nIncorporating EPM feature-values improves the performance by about three points. While “+EPM” does not use the WikiCoref data during training, and unlike “G&L”, it does not employ any domain-specific features, it achieves onpar performance with that of “G&L”. This indeed\n15WikiCoref only contains 30 documents, which is not enough for training neural coreference resolvers.\nshows the effectiveness of informative featurevalues in improving generalization.\nThe second set of generalization experiments is reported in Table 7. “in-domain” columns show the results when the evaluation genres were included in training and development sets while the “out-of-domain” columns show the results when the evaluation genres were excluded. As we can see, “+EPM” generalizes best, and in out-ofdomain evaluations, it considerably outperforms the ensemble model of e2e-coref, which has the best performance on the CoNLL test set."
  }, {
    "heading": "9 Conclusions",
    "text": "In this paper, we show that employing linguistic features in a neural coreference resolver significantly improves generalization. However, the incorporated features should be informative enough to be taken into account in the presence of lexical features, which are very strong features in the CoNLL dataset. We propose an efficient algorithm to determine informative feature-values in large datasets. As a result of a better generalization, we achieve state-of-the-art results in all examined outof-domain evaluations."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank Mark-Christoph Müller, Benjamin Heinzerling, Alex Judea, Steffen Eger and the anonymous reviewers for their helpful comments and feedbacks. This work has been supported by the Klaus Tschira Foundation, Heidelberg, Germany and the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1."
  }],
  "year": 2018,
  "references": [{
    "title": "An Introduction to Categorical Data Analysis",
    "authors": ["Alan Agresti."],
    "venue": "John Wiley & Sons.",
    "year": 2007
  }, {
    "title": "Algorithms for scoring coreference chains",
    "authors": ["Amit Bagga", "Breck Baldwin."],
    "venue": "Proceedings of the 1st International Conference on Language Resources and Evaluation, Granada, Spain, 28–30 May 1998, pages 563–566.",
    "year": 1998
  }, {
    "title": "Constructing classification features using minimal predictive patterns",
    "authors": ["Iyad Batal", "Milos Hauskrecht."],
    "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Management, pages 869–878.",
    "year": 2010
  }, {
    "title": "Understanding the value of features for coreference resolution",
    "authors": ["Eric Bengtson", "Dan Roth."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25–27 October 2008, pages 294–303.",
    "year": 2008
  }, {
    "title": "Datadriven multilingual coreference resolution using resolver stacking",
    "authors": ["Anders Björkelund", "Richárd Farkas."],
    "venue": "Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July",
    "year": 2012
  }, {
    "title": "Discriminative frequent pattern analysis for effective classification",
    "authors": ["Hong Cheng", "Xifeng Yan", "Jiawei Han", "Chih-Wei Hsu."],
    "venue": "Proceedings of the IEEE 23rd International Conference on Data Engineering (ICDE 2007), pages 716–725.",
    "year": 2007
  }, {
    "title": "Direct discriminative pattern mining for effective classification",
    "authors": ["Hong Cheng", "Xifeng Yan", "Jiawei Han", "Philip S Yu."],
    "venue": "Proceedings of the IEEE 24th International Conference on Data Engineering (ICDE 2008), pages 169–178.",
    "year": 2008
  }, {
    "title": "Improving coreference resolution by learning entitylevel distributed representations",
    "authors": ["Kevin Clark", "Christopher D. Manning."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
    "year": 2016
  }, {
    "title": "Deep reinforcement learning for mention-ranking coreference models",
    "authors": ["Kevin Clark", "Christopher D. Manning."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Tex., 1–5 November",
    "year": 2016
  }, {
    "title": "Specialized models and ranking for coreference resolution",
    "authors": ["Pascal Denis", "Jason Baldridge."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25–27 October 2008, pages 660–",
    "year": 2008
  }, {
    "title": "Accurate methods for the statistics of surprise and coincidence",
    "authors": ["Ted Dunning."],
    "venue": "Computational Linguistics, 19(1):61–74.",
    "year": 1993
  }, {
    "title": "Coreference-inspired coherence modeling",
    "authors": ["Micha Elsner", "Eugene Charniak."],
    "venue": "Proceedings ACL-HLT 2008 Conference Short Papers, Columbus, Ohio, 15–20 June 2008, pages 41–44.",
    "year": 2008
  }, {
    "title": "LIBLINEAR: A library for large linear classification",
    "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."],
    "venue": "The Journal of Machine Learning Research, 9:1871–1874.",
    "year": 2008
  }, {
    "title": "Entropy-guided feature generation for structured learning of Portuguese dependency parsing",
    "authors": ["Eraldo R Fernandes", "Ruy L Milidiú."],
    "venue": "Proceedings of the International Conference on Computational Processing of the Portuguese Language,",
    "year": 2012
  }, {
    "title": "Latent structure perceptron with feature induction for unrestricted coreference resolution",
    "authors": ["Eraldo Rezende Fernandes", "Cı́cero Nogueira dos Santos", "Ruy Luiz Milidiú"],
    "venue": "In Proceedings of the Shared Task of the 16th Conference on Computa-",
    "year": 2012
  }, {
    "title": "Coreference in Wikipedia: Main concept resolution",
    "authors": ["Abbas Ghaddar", "Philippe Langlais."],
    "venue": "Proceedings of the 20th Conference on Computational Natural Language Learning, Berlin, Germany, 7–11 August 2016, pages 229–238.",
    "year": 2016
  }, {
    "title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles",
    "authors": ["Abbas Ghaddar", "Philippe Langlais."],
    "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation, Portorož, Slovenia, 23–28 May 2016.",
    "year": 2016
  }, {
    "title": "Mining frequent patterns without candidate generation: A frequent-pattern tree approach",
    "authors": ["Jiawei Han", "Jian Pei", "Yiwen Yin", "Runying Mao."],
    "venue": "Data Mining and Knowledge Discovery, 8(1-5):53–87.",
    "year": 2004
  }, {
    "title": "Automatic evaluation of text coherence: Models and representations",
    "authors": ["Mirella Lapata", "Regina Barzilay."],
    "venue": "Proceedings of the 19th International Joint Conference on Artificial Intelligence, Edinburgh, Scotland, 30 July – 5 August 2005, pages",
    "year": 2005
  }, {
    "title": "End-to-end neural coreference resolution",
    "authors": ["Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197, Copenhagen, Denmark.",
    "year": 2017
  }, {
    "title": "On coreference resolution performance metrics",
    "authors": ["Xiaoqiang Luo."],
    "venue": "Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8",
    "year": 2005
  }, {
    "title": "A deeper",
    "authors": ["Marta Recasens", "Eduard Hovy"],
    "year": 2009
  }, {
    "title": "Item set mining based on cover similarity",
    "authors": ["Marc Segond", "Christian Borgelt."],
    "venue": "Advances in Knowledge Discovery and Data Mining, pages 493– 505.",
    "year": 2011
  }, {
    "title": "Reinforcement learning: An introduction, volume 1",
    "authors": ["Richard S. Sutton", "Andrew G. Barto."],
    "venue": "MIT press Cambridge.",
    "year": 1998
  }, {
    "title": "Knowledge acquisition for coreference resolution",
    "authors": ["Olga Uryupina."],
    "venue": "Ph.D. thesis, Saarland University.",
    "year": 2007
  }, {
    "title": "A state-of-the-art mention-pair model for coreference resolution",
    "authors": ["Olga Uryupina", "Alessandro Moschitti."],
    "venue": "Proceedings of STARSEM 2015: The Fourth Joint Conference on Lexical and Computational Semantics, Denver, Col., 4–5 June 2015,",
    "year": 2015
  }, {
    "title": "A modeltheoretic coreference scoring scheme",
    "authors": ["Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman."],
    "venue": "Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, Cal. Morgan",
    "year": 1995
  }, {
    "title": "Discovering significant patterns",
    "authors": ["Geoffrey I. Webb."],
    "venue": "Machine Learning, 68(1):1–39.",
    "year": 2006
  }, {
    "title": "Learning anaphoricity and antecedent ranking features for coreference resolution",
    "authors": ["Sam Wiseman", "Alexander M. Rush", "Stuart Shieber", "Jason Weston."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (Volume",
    "year": 2015
  }, {
    "title": "Improving pronoun resolution by incorporating coreferential information of candidates",
    "authors": ["Xiaofeng Yang", "Jian Su", "Guodung Zhou", "Chew Lim Tan."],
    "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguis-",
    "year": 2004
  }, {
    "title": "Unsupervised person slot filling based on graph mining",
    "authors": ["Dian Yu", "Heng Ji."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany, 7–12 August 2016, pages",
    "year": 2016
  }],
  "id": "SP:66814fe66801d797a515eaf042bfa90ee44896b4",
  "authors": [{
    "name": "Nafise Sadat Moosavi",
    "affiliations": []
  }, {
    "name": "Michael Strube",
    "affiliations": []
  }],
  "abstractText": "Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.",
  "title": "Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers"
}