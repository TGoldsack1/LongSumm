{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 167–171 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2026"
  }, {
    "heading": "1 Introduction",
    "text": "Continuous word representations, derived from unlabeled text, have proven useful in many NLP tasks. Such word representations (or embeddings) associate a low-dimensional, real-valued vector with each word, typically induced via neural language models or matrix factorization.\nSubstantial benefit arises when embeddings can be efficiently trained on large volumes of data. Hence the recent considerable interest in the continuous bag-of-words (CBOW) and skip-gram with negative sampling (SGNS) models, described in (Mikolov et al., 2013), as implemented in the opensource toolkit word2vec. These models are based on a relatively simple log-linear method and avoid hidden layers typical to neural networks. Consequently, they can be trained to produce high-quality word embeddings on large corpora like the entirety of English Wikipedia in several hours, compared to days or even weeks in the case of other continuous models. Recent studies obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity extraction (Passos et al., 2014)\nand dependency parsing (Bansal et al., 2014). In recent years, there were several attempts to mathematically interpret word embedding models (Arora et al., 2016; Pennington et al., 2014; Stratos et al., 2015). Our study pursues this established line of work, attempting to explain the objective function of the SGNS word embedding algorithm.\nIn the SGNS model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context. The objective function is a binary logistic regression classifier that treats a word and its observed context as a positive example, and a word and a randomly sampled context as a negative example. Levy and Goldberg (2014) offered a motivation for this function by showing that it obtains its global maximum value at the word-context pointwise mutual information (PMI) matrix. In this study, we take their analysis one step further and provide an informationtheoretical interpretation of the SGNS objective function. In Section 2, we define a new measure of mutual information between random variables based the Jensen-Shennon divergence (Lin, 1991) instead of the KL divergence. In Section 3, we show that the value of the SGNS objective computed at the PMI matrix is this information measure. We then derive an explicit expression for the information loss caused by the low-dimensional embedding learned by the SGNS algorithm. Finally, in Section 4, we illustrate this by computing the information loss caused by actual SGNS embeddings learned on a standard text corpus."
  }, {
    "heading": "2 A Dependency Measure based on Jensen-Shannon",
    "text": "In this section, we define a dependency measure between two random variables, which is based on the Jensen-Shannon divergence. Later, in Section 3, we show how it relates to the SGNS objective function.\n167\nThere are several standard methods of measuring the distance between two discrete probability distributions, defined on a given finite set A. The Kullback-Leibler (KL) divergence of a distribution p from a distribution q is defined as follows: KL(p||q) =∑i∈A pi log piqi . The mutual information between two jointly distributed random variables X and Y is defined as the KL divergence of the joint distribution p(x, y) from the product p(x)p(y) of the marginal distributions of X and Y, i.e. I(X;Y ) = KL(p(x, y)||p(x)p(y)).\nThe Jensen-Shannon (JS) divergence (Lin, 1991) between distributions p and q is:\nJSα(p, q) =αKL(p||r) + (1−α)KL(q||r) (1)\n= H(r)− αH(p)− (1−α)H(q) such that 0 < α < 1, r = αp+ (1− α)q and H is the entropy function (i.e. H(p) = −∑i pi log pi). Unlike KL divergence, JS divergence is bounded from above and 0 ≤ JSα(p, q) ≤ 1.\nWe next propose a new measure for mutualinformation using the JS-divergence between p(x, y) and p(x)p(y) instead of the KL-divergence. We define the Jensen-Shannon Mutual information (JSMI) as follows:\nJSMIα(X,Y ) = JSα(p(x, y), p(x)p(y)). (2)\nIt can be easily verified that X and Y are independent if and only if JSMIα(X,Y ) = 0.\nWe next derive an alternative definition of the JSMI dependency measure. Assume we choose between the two distributions, p(x, y) and the product of marginal distributions p(x)p(y), according to a binary random variableZ, such that p(Z = 1) = α. We first sample a binary value for Z and next, we sample a r.v. W as follows:\np(W =(x, y)|Z)= { p(x)p(y) if Z=0 p(x, y) if Z=1.\n(3) The divergence measure JSMIα(X,Y ) can be alternatively defined in terms of mutual information between W and Z. The mutual-information between W and Z is:\nI(W;Z) = H(W )− ∑\ni=0,1\np(Z= i)H(W |Z= i)\n= H(αp(x, y) + (1−α)p(x)p(y))\n−αH(p(x, y))− (1−α)H(p(x)p(y)).\nEq. (1) thus implies that:\nJSMIα(X,Y ) = I(W ;Z). (4)\nApplying Bayes rule we obtain:\np(Z=1|W =(x, y)) (5)\n= αp(x, y)\nαp(x, y) + (1−α)p(x)p(y)\n= 1\n1 + exp(− log( αp(x,y)(1−α)p(x)p(y))) = σ(pmix,y)\nsuch that σ(u) = 11+exp(−u) is the sigmoid function and\npmix,y = log p(x, y)\np(x)p(y) + log\nα\n1−α (6)\nis a shifted version of the PMI function. Equations (4) and (5) imply that:\nJSMIα(X,Y ) = H(Z)−H(Z|W ) (7)\n= h(α)+α ∑\nx,y\np(x, y) log σ(pmix,y)\n+(1−α) ∑\nx,y\np(x)p(y) log σ(−pmix,y)\nsuch that h(α) = −α log(α) − (1−α) log(1−α) is the binary entropy function."
  }, {
    "heading": "3 The Skip-Gram Embedding Algorithm",
    "text": "The SGNS embedding algorithm (Mikolov et al., 2013) represents each word x and each context y as d-dimensional vectors ~x and ~y, with the purpose that words that are “similar” to each other will have similar vector representations. We can represent a given d-dimensional embedding by a matrix m, such that m(x, y) = ~x · ~y. The rank of the embedding matrix m is (at most) d.\nLet p(x, y) be the normalized number of cooccurrences of word x and context-word y in a given corpus and let p(x) and p(y) be the corresponding unigram distributions. Consider a binary classifier that treats a word and its observed context as a positive example, and a word and a randomly sampled context as a negative example. The classification is made based on the embedding in such a way that the probability that (x, y) is a positive example is σ(~x · ~y). The objective function ideally maximized by the SGNS word embedding\nalgorithm is the expectation of the log-likelihood function of the embedding:\nS(m) = h( 1\nk+1 ) +\n1\nk+1\n∑\nx,y\np(x, y) log σ(~x · ~y)\n+ k\nk+1\n∑\nx,y\np(x)p(y) log σ(−~x · ~y).\n(8) Note that the term h( 1k+1), which does not appear in the original SGNS objective function (Mikolov et al., 2013), is a constant number that was added here to simplify the following presentation.\nThe sparsity of p(x, y) (which is obtained as normalized counts from a given learning corpus) makes it feasible to compute the second term of (8). The number of summed-over elements in the third term of (8), however, is quadratic in the size of the vocabulary, making it hard to compute. Therefore, in practice, we can approximate the expectation by sampling of ‘negative’ examples. The actual SGNS score, then, is:\nS(m) ≈ h( 1 k+1 ) + 1 k+1 · 1 n\nn∑\nt=1\n(log σ(~xt · ~yt)\n+\nk∑\ni=1\nlog σ(−~xt · ~yti)).\n(9) such that t goes over all the word-context pairs in a given corpus. The negative examples yti are created for each pair (xt, yt) by drawing k random contexts from the context-word distribution p(y).\nAs pointed out in (Levy et al., 2015), k has two distinct functions in the SGNS objective function. First, it is used to better estimate the distribution of negative examples. Second, it is used as a weight on the probability of observing a positive example versus a negative example; a higher k means that negative examples are more probable.\nWe can compute the SGNS score function S(m) for every real-valued matrix m = (mx,y). Levy and Goldberg (2014) showed that the function achieves its global maximal value when for each word-pair (x, y) the inner product of the embedding vectors ~x · ~y is equal to pmi(x, y). In other words they showed that S(m) ≤ S(pmi) for every matrix m. We next show that the value of the function S(m) at its maximum point, the PMI matrix, has a concrete interpretation, namely it is exactly the Jensen-Shannon Mutual Information (JSMI) between words and their contexts.\nTheorem 1: The value of the SGNS score with k negative samples (8) at the PMI matrix satisfies:\nS(pmi) = JSMIα(X,Y )\nsuch that α = 1k+1 . Proof: It can be easily verified that by substituting α = 1k+1 in the definition of JSMI (Eq. (7)), we exactly obtain the SGNS score (8) at the PMI matrix. 2\nLevy and Goldberg (2014) showed that SGNS’s objective achieves its maximal value at the PMI matrix. However, this result reveals nothing about the more interesting lower dimensional case, where the PMI matrix factorization is forced to compress the joint distribution and thereby learn a meaningful embedding. We next derive an explicit description of the approximation criterion that quantifies the gap between S(m) and S(pmi).\nGiven the word co-occurrences joint distribution p(x, y), we obtained in Eq. (5) a conditional distribution on the alphabet of (Z,W ) as follows:\np(Z=1|W =(x, y)) = σ(pmix,y).\nIn a similar way, given any matrixm, we can define a conditional distribution pm on the alphabet of (Z,W ) as follows:\npm(Z=1|W =(x, y)) = σ(mx,y).\nNote that in the special case where m is the PMI matrix, ppmi(z|w) coincides with the original p(z|w) that was defined in Eq. (5). Theorem 2: The difference between the SGNS score at the PMI matrix and the SGNS score at a given matrix m can be written as:\nS(pmi)− S(m) = KL(ppmi(Z|W )||pm(Z|W )) (10) Proof:\nS(pmi)−S(m) = ∑\nx,y\n(αp(x, y) log σ(pmix,y) σ(mx,y)\n+(1−α)p(x)p(y) log σ(−pmix,y) σ(−mx,y) )\n= ∑\nx,y\n(αp(x, y) log ppmi(Z=1|x, y) pm(Z=1|x, y)\n+(1−α)p(x)p(y) log ppmi(Z=0|x, y) pm(Z=0|x, y) )\n= ∑\nw,z\np(W =w,Z=z) log ppmi(Z=z|W =w) pm(Z=z|W =w)\n= KL(ppmi(Z|W )||pm(Z|W )).2 The KL divergence between two distributions is always non-negative and is zero only if the two distributions are the same. Therefore, we rederive the results of (Levy and Goldberg, 2014) that S(pmi) = maxm S(m). Theorem 2 can be viewed as an instance of the well-known connection between maximizing log-likelihood and minimizing KL divergence between the estimated and the true data-generating distribution. In this case, the true distribution is the pmi-based classifier ppmi(Z|W ).\nCombining theorems 1 and 2 we obtain that S(m) ≤ JSMIα(X,Y ) for every low-dimensional embedding matrix. The difference JSMIα(X,Y )− S(m) is the information loss caused by the lowdimensional embedding. We can view it as a Jensen-Shannon variant of the information bottleneck principle (Tishby et al., 1999; Globerson et al., 2007) that is defined in terms of the KL divergence. The optimal d-dimensional embedding, is the best d-dimensional approximation of the JSMI dependency measure in the sense that it minimizes the information loss. The JSMI is the upper bound that any embedding can obtain. To illustrate that, in the next section we compute the JSMI between words and their contexts based on a standard text corpus and show the information gap between the JSMI and the actual SGNS score as a function of the embedding dimension d.\nFrom Theorem 2 we can also derive an explicit information-theoretic interpretation of the score function S(m) (7) as the difference between two KL-divergence terms:\nS(m) = S(pmi)− (S(pmi)− S(m)) =\nI(Z;W )− (S(pmi)− S(m)) = KL(p(Z|W )||p(Z))− KL(p(Z|W )||pm(Z|W ))\nThe word embedding problem can be also viewed as a factorization of the PMI matrix. Previous works suggested other criteria for matrix factorization such as least-squares (Eckart and Young, 1936) and KL-divergence between the original matrix and the low-rank matrix approximation (Lee and Seung, 2000). We have shown that the SGNS algorithm factorizes the PMI matrix based on the JSMI-based criterion stated in Eq. (10)."
  }, {
    "heading": "4 Experiments",
    "text": "In this section we use word2vec to train real skipgram with negative sampling (SGNS) embedding models. By measuring the value of their objective function and comparing it against the optimal one using exact PMI values, we demonstrate how a well-trained model minimizes the difference in Eq. (10). We note that this is an intrinsic measure that does not necessarily reflect the usefulness of the learned embeddings for other tasks.\nWe used the Penn Tree Bank (PTB), a popular small-scale corpus, for our experiments. A version of this dataset is available from Tomas Mikolov.1 It consists of 929K training words with a 10K word vocabulary, which we used to train our models. To learn the SGNS word embeddings, we used word2vec’s default parameter values: windowsize = 5, min-count = 5, and number of negative samples k = 5. We varied the dimensionality of the embeddings and the number of training iterations performed. Once the models were trained, we measured their score (9) on the training corpus.\nBased on the same learning corpus, we computed S(pmi) = JSMIα(X,Y ) for α = 1k+1 = 1/6. Note that p(x, y) = 0 implies that pmix,y = −∞ and therefore log σ(−pmix,y) = 0. Hence, as in the second term, to compute the third term of S(m) (8) for the case of m = pmi, we can sum only\n1http://www.fit.vutbr.cz/~imikolov/ rnnlm/simple-examples.tgz\nover the positive pairs (x, y) that actually appear in the corpus.2 In other words, for the special case m = pmi, it is feasible to compute the exact score (8) and not just its approximation (9) that is based on negative sampling. Figure 1 illustrates the optimal PMI-based score, compared with the scores obtained by different models with varied embedding dimensionality and number of training iterations. As can be seen, the embeddings score gets close to the optimal value using higher dimensionality and more training iterations, but doesn’t surpass it."
  }, {
    "heading": "5 Conclusion",
    "text": "In this study, we developed a new correlation measure between random variables, denoted JSMI. This measure is based on the JS divergence and differs from the standard mutual information measure that is based on the KL divergence. We showed that the optimization of skip-gram embeddings with negative sampling finds the best low-dimensional approximation of the JSMI measure. Thus, we provided an information theory framework that hopefully contributes to a better understanding of this embedding algorithm. Furthermore, although we focused here on the case of word-context joint distributions, the connection we haven shown between the PMI matrix and the JSMI function is valid for every joint distribution of two random variables."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
  }],
  "year": 2017,
  "references": [{
    "title": "A latent variable model approach to pmi-based word embeddings",
    "authors": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski."],
    "venue": "Transactions of the Association for Computational Linguistics 4:385–399.",
    "year": 2016
  }, {
    "title": "Tailoring continuous word representations for dependency parsing",
    "authors": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2014
  }, {
    "title": "The approximation of one matrix by another of lower rank",
    "authors": ["Carl Eckart", "Gale Young."],
    "venue": "Psychometrika 1:211–218. 2We used the exact same positive co-occurrence pairs sampled by word2vec during the training of the SGNS embeddings",
    "year": 1936
  }, {
    "title": "Euclidean embedding of cooccurrence data",
    "authors": ["Amir Globerson", "Gal Chechik", "Fernando Pereira", "Naftaly Tishby."],
    "venue": "Journal of Machine Learning Research 8:2265–2295.",
    "year": 2007
  }, {
    "title": "Algorithms for nonnegative matrix factorization",
    "authors": ["Daniel D. Lee", "H. Sebastian Seung."],
    "venue": "Advances in Neural Information Processing Systems.",
    "year": 2000
  }, {
    "title": "Neural word embedding as implicit matrix factorization",
    "authors": ["Omer Levy", "Yoav Goldberg."],
    "venue": "Advances in Neural Information Processing Systems.",
    "year": 2014
  }, {
    "title": "Improving distributional similarity with lessons learned from word embeddings",
    "authors": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."],
    "venue": "Trans. of the Association for Computational Linguistics 3:211–225.",
    "year": 2015
  }, {
    "title": "Divergence measures based on the shannon entropy",
    "authors": ["Jianhua Lin."],
    "venue": "IEEE Transactions on Information Theory 37(1):145–151.",
    "year": 1991
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Advances in Neural Information Processing Systems.",
    "year": 2013
  }, {
    "title": "Lexicon infused phrase embeddings for named entity resolution",
    "authors": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."],
    "venue": "Conference on Natural Language Learning (CoNLL).",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "EMNLP. volume 14, pages 1532– 1543.",
    "year": 2014
  }, {
    "title": "Model-based word embeddings from decompositions of count matrices",
    "authors": ["Karl Stratos", "Michael Collins", "Daniel J Hsu."],
    "venue": "ACL (1). pages 1282– 1291.",
    "year": 2015
  }, {
    "title": "The information bottleneck method",
    "authors": ["Naftaly Tishby", "Fernando C. Pereira", "William Bialek."],
    "venue": "Allerton Conf. on Communication, Control, and Computing.",
    "year": 1999
  }],
  "id": "SP:e03a0927c2e47b5d1e80f5165b13bb0580e23774",
  "authors": [{
    "name": "Oren Melamud",
    "affiliations": []
  }, {
    "name": "Jacob Goldberger",
    "affiliations": []
  }],
  "abstractText": "In this paper, we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution and the product of their marginal distributions. Then, we show that word2vec’s skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts. The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard text corpus.",
  "title": "Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function"
}