{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1111"
  }, {
    "heading": "1 Introduction",
    "text": "Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015). When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate. However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations. In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined. Hence, the pipeline of word segmentation, POS tagging and dependency parsing always suffers from word seg-\nmentation errors. Once words have been wronglysegmented, word embeddings and traditional onehot word features, used in dependency parsers, will mistake the precise meanings of the original sentences. As a result, pipeline models achieve dependency scores of around 80% for Chinese.\nA traditional solution to this error propagation problem is to use joint models. Many Chinese words play multiple grammatical roles with only one grammatical form. Therefore, determining the word boundaries and the subsequent tagging and dependency parsing are closely correlated. Transition-based joint models for Chinese word segmentation, POS tagging and dependency parsing are proposed by Hatori et al. (2012) and Zhang et al. (2014). Hatori et al. (2012) state that dependency information improves the performances of word segmentation and POS tagging, and develop the first transition-based joint word segmentation, POS tagging and dependency parsing model. Zhang et al. (2014) expand this and find that both the inter-word dependencies and intraword dependencies are helpful in word segmentation and POS tagging.\nAlthough the models of Hatori et al. (2012) and Zhang et al. (2014) perform better than pipeline models, they rely on the one-hot representation of characters and words, and do not assume the similarities among characters and words. In addition, not only words and characters but also many incomplete tokens appear in the transitionbased joint parsing process. Such incomplete or unknown words (UNK) could become important cues for parsing, but they are not listed in dictionaries or pre-trained word embeddings. Some recent studies show that character-based embeddings are effective in neural parsing (Ballesteros et al., 2015; Zheng et al., 2015), but their models could not be directly applied to joint models because they use given word segmentations. To solve\n1204\nthese problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing. We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens.\nAnother problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering. Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016). In their models, the bi-LSTM is used to represent the tokens including their context. Indeed, such neural networks can observe whole sentence through the bi-LSTM. This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014). As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models. We also develop joint models with ngram character string bi-LSTM.\nIn the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better dependency scores than the previous joint models. To the best of our knowledge, this is the first model to use embeddings and neural networks for Chinese full joint parsing.\nOur contributions are summarized as follows: (1) we propose the first embedding-based fully joint parsing model, (2) we use character string embeddings for UNK and incomplete tokens. (3) we also explore bi-LSTM models to avoid the detailed feature engineering in previous approaches. (4) in experiments using Chinese corpus, we achieve state-of-the-art scores in word segmentation, POS tagging and dependency parsing."
  }, {
    "heading": "2 Model",
    "text": "All full joint parsing models we present in this paper use the transition-based algorithm in Section 2.1 and the embeddings of character strings in Section 2.2. We present two neural networks: the feed-forward neural network models in Section 2.3 and the bi-LSTM models in Section 2.4."
  }, {
    "heading": "2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",
    "text": "Based on Hatori et al. (2012), we use a modified arc-standard algorithm for character transi-\n技术有了新的进展。\nTechnology have made new progress.\ntions (Figure 1). The model consists of one buffer and one stack. The buffer contains characters in the input sentence, and the stack contains words shifted from the buffer. The stack words may have their child nodes. The words in the stack are formed by the following transition operations.\n• SH(t) (shift): Shift the first character of the buffer to the top of the stack as a new word.\n• AP (append): Append the first character of the buffer to the end of the top word of the stack.\n• RR (reduce-right): Reduce the right word of the top two words of the stack, and make the right child node of the left word.\n• RL (reduce-left): Reduce the left word of the top two words of the stack, and make the left child node of the right word.\nThe RR and RL operations are the same as those of the arc-standard algorithm (Nivre, 2004a). SH makes a new word whereas AP makes the current word longer by adding one character. The POS tags are attached with the SH(t) transition.\nIn this paper, we explore both greedy models and beam decoding models. This parsing algorithm works in both types. We also develop a joint model of word segmentation and POS tagging, along with a dependency parsing model. The joint model of word segmentation and POS tagging does not have RR and RL transitions."
  }, {
    "heading": "2.2 Embeddings of Character Strings",
    "text": "First, we explain the embeddings used in the neural networks. Later, we explain details of the neural networks in Section 2.3 and 2.4.\nBoth meaningful words and incomplete tokens appear during transition-based joint parsing. Although embeddings of incomplete tokens are not used in previous work, they could become useful features in several cases. For example, “南京 东路” (Nanjing East Road, the famous shopping street of Shanghai) is treated as a single Chinese word in the Penn Chinese Treebank (CTB) corpus. There are other named entities of this form in CTB, e.g, “北京西路” (Beijing West Road) and “湘西路” (Hunan West Road). In these cases, “南京” (Nanjing) and “北京” (Beijing) are location words, while “东路” (East Road) and “西 路” (West Road) are sub-words. “东路” and “西 路” are similar in terms of their character composition and usage, which is not sufficiently considered in the previous work. Moreover, representations of incomplete tokens are helpful for compensating the segmentation ambiguity. Suppose that the parser makes over-segmentation errors and segments “南京东路” to “南京” and “东 路”. In this case, “东路” becomes UNK. However, the models could infer that “东路” is also a location, from its character composition and neighboring words. This could give models robustness of segmentation errors. In our models, we prepare the word and character embeddings in the pretraining. We also use the embeddings of character strings for sub-words and UNK which are not in the pre-trained embeddings.\nThe characters and words are embedded in the same vector space during pre-training. We prepare the same training corpus with the segmented word files and the segmented character files. Both files are concatenated and learned by word2vec (Mikolov et al., 2013). We use the embeddings of 1M frequent words and characters. Words and characters that are in the training set and do not have pre-trained embeddings are given randomly initialized embeddings. The development set and the test set have out-of-vocabulary (OOV) tokens for these embeddings.\nThe embeddings of the unknown character strings are generated in the neural computation graph when they are required. Consider a character string c1c2 · · · cn consisting of characters ci. When this character string is not in the pretrained embeddings, the model obtains the embeddings v(c1c2 · · · cn) by the mean of each character embeddings ∑n i=1 v(ci). Embeddings of words, characters and character strings have the same di-\nmension and are chosen in the neural computation graph. We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens. However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon."
  }, {
    "heading": "2.3 Feed-forward Neural Network",
    "text": ""
  }, {
    "heading": "2.3.1 Neural Network",
    "text": "We present a feed-forward neural network model in Figure 2. The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015). We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2. This neural network has two hidden layers with 8,000 dimensions. This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions). We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy\nneural network. There are three randomly initialized weight matrices between the embedding layers and the softmax function. The loss function L(θ) for the greedy training is\nL(θ) = − ∑\ns,t\nlog pgreedys,t + λ\n2 ||θ||2,\npgreedys,t (β) ∝ exp\n ∑\nj\nwtjβj + bt\n  ,\nwhere t denotes one transition among the transition set T ( t ∈ T ). s denotes one element of the single mini-batch. β denotes the output of the previous layer. w and b denote the weight matrix and the bias term. θ contains all parameters. We use the L2 penalty term and the Dropout. The backprop is performed including the word and character embeddings. We use Adagrad (Duchi et al., 2010) to optimize learning rate. We also consider Adam (Kingma and Ba, 2015) and SGD, but find that Adagrad performs better in this model. The other learning parameters are summarized in Table 1.\nIn our model implementation, we divide all sentences into training batches. Sentences in the same training batches are simultaneously processed by the neural mini-batches. By doing so, the model can parse all sentences of the training batch in the number of transitions required to parse the longest sentence in the batch. This allows the model to parse more sentences at once, as long as the neural mini-batch can be allocated to the GPU memory. This can be applied to beam decoding."
  }, {
    "heading": "2.3.2 Features",
    "text": "The features of this neural network are listed in Table 2. We use three kinds of features: (1) features obtained from Hatori et al. (2012) by removing combinations of features, (2) features obtained from Chen and Manning (2014), (3) original features related to character strings. In particular,\nType Features\nStack word and tags s0w, s1w, s2w s0p, s1p, s2p Stack 1 children and tags s0l0w, s0r0w, s0l1w, s0r1w s0l0p, s0r0p, s0l1p, s0r1p Stack 2 children s1l0w, s1r0w, s1l1w, s1r1w Children of children s0l0lw, s0r0rw, s1l0lw, s1r0rw Buffer characters b0c, b1c, b2c, b3c Previously shifted words q0w, q1w Previously shifted tags q0p, q1p Character of q0 q0e Parts of q0 word q0f1, q0f2, q0f3 Strings across q0 and buf. q0b1, q0b2, q0b3 Strings of buffer characters b0-2, b0-3, b0-4\nthe original features include sub-words, character strings across the buffer and the stack, and character strings in the buffer. Character strings across the buffer and stack could capture the currentlysegmented word. To avoid using character strings that are too long, we restrict the length of character string to a maximum of four characters. Unlike Hatori et al. (2012), we use sequential characters of sentences for features, and avoid handengineered combinations among one-hot features, because such combinations could be automatically generated in the neural hidden layers as distributed representations (Hinton et al., 1986).\nIn the later section, we evaluate a joint model for word segmentation and POS tagging. This model does not use the children and children-ofchildren of stack words as features."
  }, {
    "heading": "2.3.3 Beam Search",
    "text": "Structured learning plays an important role in previous joint parsing models for Chinese.1 In this paper, we use the structured learning model proposed by Weiss et al. (2015) and Andor et al. (2016).\nIn Figure 2, the output layer for the beam decoding is at the top of the network. There are a perceptron layer which has inputs from the two hidden layers and the greedy output layer: [h1,h2,pgreedy(y)]. This layer is learned by the following cost function (Andor et al., 2016):\nL(d∗1:j ; θ) = − j∑\ni=1\nρ(d∗1:i−1, d ∗ i ; θ)\n+ ln ∑\nd′1:j∈B1:j exp\nj∑\ni=1\nρ(d′1:i−1, d ′ i; θ),\nwhere d1:j denotes the transition path and d∗1:j denotes the gold transition path. B1:j is the set of transition paths from 1 to j step in beam. ρ is the value of the top layer in Figure 2. This training can be applied throughout the network. However, we separately train the last beam layer and the previous greedy network in practice, as in Andor et al. (2016). First, we train the last perceptron layer using the beam cost function freezing the previous greedy-trained layers. After the last layer has been well trained, backprop is performed including the previous layers. We notice that training the embedding layer at this stage could make the results worse, and thus we exclude it. Note that this whole network backprop requires considerable GPU memory. Hence, we exclude particularly large batches from the training, because they cannot be on GPU memory. We use multiple beam sizes for training because models can be trained faster with small beam sizes. After the small beam size training, we use larger beam sizes. The test of this fully joint model takes place with a beam size of 16.\nHatori et al. (2012) use special alignment steps in beam decoding. The AP transition has size-2 steps, whereas the other transitions have a size-1 step. Using this alignment, the total number of steps for an N -character sentence is guaranteed to be 2N − 1 (excluding the root arc) for any transition path. This can be interpreted as the AP transition doing two things: appending characters and\n1Hatori et al. (2012) report that structured learning with a beam size of 64 is optimal.\nresolving intra-word dependencies. This alignment stepping assumes that the intra-word dependencies of characters to the right of the characters exist in each Chinese word."
  }, {
    "heading": "2.4 Bi-LSTM Model",
    "text": "In Section 2.3, we describe a neural network model with feature extraction. Unfortunately, although this model is fast and very accurate, it has two problems: (1) the neural network cannot see the whole sentence information. (2) it relies on feature engineering. To solve these problems, Kiperwasser and Goldberg (2016) propose a bi-LSTM neural network parsing model. Surprisingly, their model uses very few features, and bi-LSTM is applied to represent the context of the features. Their neural network consists of three parts: bi-LSTM, a feature extraction function and a multilayer perceptron (MLP). First, all tokens in the sentences are converted to embeddings. Second, the bi-LSTM reads all embeddings of the sentence. Third, the feature function extracts the feature representations of tokens from the bi-LSTM layer. Finally, an MLP with one hidden layer outputs the transition scores of the transition-based parser.\nIn this paper, we propose a Chinese joint parsing model with simple and global features using n-gram bi-LSTM and a simple feature extraction function. The model is described in Figure 3. We consider that Chinese sentences consist of tokens, including words, UNKs and incomplete tokens, which can have some meanings and are useful for parsing. Such tokens appear in many parts of the sentence and have arbitrary lengths. To capture them, we propose the n-gram bi-LSTM. The n-gram bi-LSTM read through characters ci · · · ci+n−1 of the sentence (ci is the i-th character). For example, the 1-gram bi-LSTM reads each character, and the 2-gram bi-LSTM reads two consecutive characters cici+1. After the n-gram forward LSTM reads character string ci · · · ci+n−1, it next reads ci+1 · · · ci+n. The backward LSTM reads from ci+1 · · · ci+n toward ci · · · ci+n−1. This allows models to capture any n-gram character strings in the input sentence.2 All n-gram inputs to bi-LSTM are given by the embeddings of words and characters or the dynamically generated embeddings of character strings, as described in\n2At the end of the sentence of length N , character strings ci · · · cN (N < i+n−1), which are shorter than n characters, are used.\nSection 2.2. Although these arbitrary n-gram tokens produce UNKs, character string embeddings can capture similarities among them. Following the bi-LSTM layer, the feature function extracts the corresponding outputs of the bi-LSTM layer. We summarize the features in Table 3. Finally, MLP and the softmax function outputs the transition probability. We use an MLP with three hidden layers as for the model in Section 2.3. We train this neural network with the loss function for the greedy training."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Experimental Settings",
    "text": "We use the Penn Chinese Treebank 5.1 (CTB5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al. (2008) for CTB-5 and Wang et al. (2011) for CTB-7. The statistics of datasets are presented in Table 4. We use the Chinese Gigaword Corpus for embedding pre-training. Our model is developed for unlabeled dependencies. The development set is used for parameter tuning. Following Hatori et al. (2012) and Zhang et al. (2014), we use the standard word-level evaluation with F1-measure. The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented.\nWe trained three models: SegTag, SegTagDep and Dep. SegTag is the joint word segmentation and POS tagging model. SegTagDep is the full joint segmentation, tagging and dependency parsing model. Dep is the dependency parsing model which is similar to Weiss et al. (2015) and Andor et al. (2016), but uses the embeddings of character strings. Dep compensates for UNKs and segmentation errors caused by previous word segmentation using embeddings of character strings. We will examine this effect later.\nMost experiments are conducted on GPUs, but some of beam decoding processes are performed on CPUs because of the large mini-batch size. The neural network is implemented with Theano."
  }, {
    "heading": "3.2 Results",
    "text": ""
  }, {
    "heading": "3.2.1 Joint Segmentation and POS Tagging",
    "text": "First, we evaluate the joint segmentation and POS tagging model (SegTag). Table 5 compares the performance of segmentation and POS tagging using the CTB-5 dataset. We train two modles: a greedy-trained model and a model trained with beams of size 4. We compare our model to three previous approaches: Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015). Our SegTag joint model is superior to these previous models, including Hatori et al. (2012)’s model with rich dictionary information, in terms of both segmentation and POS tagging accuracy."
  }, {
    "heading": "3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",
    "text": "Table 6 presents the results of our full joint model. We employ the greedy trained full joint model SegTagDep(g) and the beam decoding model SegTagDep. All scores for the existing models in this table are taken from Zhang et al. (2014). Though our model surpasses the previous best end-to-end joint models in terms of segmentation and POS tagging, the dependency score is slightly lower than the previous models. The greedy model SegTagDep(g) achieves slightly lower scores than beam models, although this model works considerably fast because it does not use beam decoding."
  }, {
    "heading": "3.2.3 Pipeline of Our Joint SegTag and Dep Model",
    "text": "We use our joint SegTag model for the pipeline input of the Dep model (SegTag+Dep). Both SegTag and Dep models are trained and tested by the beam cost function with beams of size 4. Table 7 presents the results. Our SegTag+Dep model performs best in terms of the dependency and word segmentation. The SegTag+Dep model is better than the full joint model. This is because most segmentation errors of these models occur around named entities. Hatori et al. (2012)’s alignment step assumes the intra-word dependencies in words, while named entities do not always have them. For example, SegTag+Dep model treats named entity “海赛克”, a company name, as one word, while the SegTagDep model divides this to “海” (sea) and “赛克”, where “赛克” could be used for foreigner’s name. For such words, SegTagDep prefers SH because AP has size-2 step of the character appending and intra-word dependency resolution, which does not exist for named entities. This problem could be solved by adding a special transition AP_named_entity which is similar to AP but with size-1 step and used\nonly for named entities. Additionally, Zhang et al. (2014)’s STD (arc-standard) model works slightly better than Hatori et al. (2012)’s fully joint model in terms of the dependency score. Zhang et al. (2014)’s STD model is similar to our SegTag+Dep because they combine a word segmentator and a dependency parser using “deque” of words."
  }, {
    "heading": "3.2.4 Effect of Character String Embeddings",
    "text": "Finally, we compare the two pipeline models of SegTag+Dep to show the effectiveness of using character string representations instead of “UNK” embeddings. We use two dependency models with greedy training: Dep(g) for dependency model and Dep(g)-cs for dependency model without the character string embeddings . In the Dep(g)-cs model, we use the “UNK” embedding when the embeddings of the input features are unavailable, whereas we use the character string embeddings in model Dep(g). The results are presented in Table 8. When the models encounter unknown tokens, using the embeddings of character strings is better than using the “UNK” embedding."
  }, {
    "heading": "3.2.5 Effect of Features across the Buffer and Stack",
    "text": "We test the effect of special features: q0bX in Table 2. The q0bX features capture the tokens across the buffer and stack. Joint transition-based parsing models by Hatori et al. (2012) and Chen and Manning (2014) decide POS tags of words before corresponding word segmentations are determined. In our model, the q0bX features capture words even if their segmentations are not determined. We examine the effectiveness of these features by training greedy full joint models with and without them. The results are shown in Table 9. The q0bX features boost not only POS tagging scores but also word segmentation scores."
  }, {
    "heading": "3.2.6 CTB-7 Experiments",
    "text": "We also test the SegTagDep and SegTag+Dep models on CTB-7. In these experiments, we no-\ntice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5. We speculate that this is caused by the difference in the training set size. We present the final results with four hidden layers in Table 10."
  }, {
    "heading": "3.2.7 Bi-LSTM Model",
    "text": "We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3. We summarize the result in Table 11. The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering."
  }, {
    "heading": "4 Related Work",
    "text": "Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron. Zhang and Clark (2010) improve this model by using both character and word-based decoding. Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model. Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing. Wang et al. (2013) also propose a lattice-based joint model for constituency parsing. Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system. This system requires\nbase parsers. In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences. They extend their models for joint segmentation and POS tagging. Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network."
  }, {
    "heading": "5 Conclusion",
    "text": "We propose the joint parsing models by the feedforward and bi-LSTM neural networks. Both of them use the character string embeddings. The character string embeddings help to capture the similarities of incomplete tokens. We also explore the neural network with few features using n-gram bi-LSTMs. Our SegTagDep joint model achieves better scores of Chinese word segmentation and POS tagging than previous joint models, and our SegTag and Dep pipeline model achieves state-of-the-art score of dependency parsing. The bi-LSTM models reduce the cost of feature engineering."
  }],
  "year": 2017,
  "references": [{
    "title": "Improved transition-based parsing and tagging with neural networks",
    "authors": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association",
    "year": 2015
  }, {
    "title": "Globally normalized transition-based neural networks",
    "authors": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."],
    "venue": "Proceedings of the 54th Annual Meeting",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Bengio."],
    "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.",
    "year": 2014
  }, {
    "title": "Improved transition-based parsing by modeling characters instead of words with lstms",
    "authors": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2015
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association",
    "year": 2014
  }, {
    "title": "Incremental parsing with minimal features using bi-directional lstm",
    "authors": ["James Cross", "Liang Huang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computa-",
    "year": 2016
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "UCB/EECS-2010-24.",
    "year": 2010
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Com-",
    "year": 2015
  }, {
    "title": "Incremental joint pos tagging and dependency parsing in chinese",
    "authors": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun’ichi Tsujii"],
    "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation",
    "year": 2011
  }, {
    "title": "Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese",
    "authors": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun’ichi Tsujii"],
    "venue": "In Proceedings of the 50th Annual Meeting of the Association",
    "year": 2012
  }, {
    "title": "Learning distributed representations of concepts",
    "authors": ["Geoffrey E. Hinton", "J.L. McClelland", "D.E. Rumelhart."],
    "venue": "Proceedings of the eighth annual conference of the cognitive science society. pages Vol.1, p.12.",
    "year": 1986
  }, {
    "title": "A cascaded linear model for joint chinese word segmentation and part-of-speech tagging",
    "authors": ["Wenbin Jiang", "Liang Huang", "Qun Liu", "Yajuan Lü."],
    "venue": "Proceedings of ACL-08: HLT . Association for Computational Linguistics, pages 897–904.",
    "year": 2008
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Vol-",
    "year": 2015
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional lstm feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "Transactions of the Association for Computational Linguistics 4:313–327.",
    "year": 2016
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "volume abs/1301.3781. http://arxiv.org/abs/1301.3781.",
    "year": 2013
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["Vinod Nair", "Geoffrey E. Hinton."],
    "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel. pages 807–814.",
    "year": 2010
  }, {
    "title": "Incrementality in deterministic dependency parsing",
    "authors": ["Joakim Nivre."],
    "venue": "Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition To-",
    "year": 2004
  }, {
    "title": "Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data",
    "authors": ["Yiou Wang", "Jun’ichi Kazama", "Yoshimasa Tsuruoka", "Wenliang Chen", "Yujie Zhang", "Kentaro Torisawa"],
    "year": 2011
  }, {
    "title": "A lattice-based framework for joint chinese word segmentation, pos tagging and parsing",
    "authors": ["Zhiguo Wang", "Chengqing Zong", "Nianwen Xue."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2013
  }, {
    "title": "Structured training for neural network transition-based parsing",
    "authors": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
    "year": 2015
  }, {
    "title": "Chinese parsing exploiting characters",
    "authors": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
    "year": 2013
  }, {
    "title": "Character-level chinese dependency parsing",
    "authors": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association",
    "year": 2014
  }, {
    "title": "Randomized greedy inference for joint segmentation, pos tagging and dependency parsing",
    "authors": ["Yuan Zhang", "Chengtao Li", "Regina Barzilay", "Kareem Darwish."],
    "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligenc.",
    "year": 2015
  }, {
    "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association",
    "year": 2008
  }, {
    "title": "A fast decoder for joint word segmentation and POS-tagging using a single discriminative model",
    "authors": ["Yue Zhang", "Stephen Clark."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2010
  }, {
    "title": "Deep learning for Chinese word segmentation and POS tagging",
    "authors": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2013
  }, {
    "title": "Characterbased parsing with convolutional neural network",
    "authors": ["Xiaoqing Zheng", "Haoyuan Peng", "Yi Chen", "Pengjing Zhang", "Zhang Wenqiang."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
    "year": 2015
  }, {
    "title": "A neural probabilistic structuredprediction model for transition-based dependency parsing",
    "authors": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "A re-ranking model for dependency parser with recursive convolutional neural network",
    "authors": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }],
  "id": "SP:0d3950fa9967d74825d7685be052ed55b231347c",
  "authors": [{
    "name": "Shuhei Kurita",
    "affiliations": []
  }, {
    "name": "Daisuke Kawahara",
    "affiliations": []
  }, {
    "name": "Sadao Kurohashi",
    "affiliations": []
  }],
  "abstractText": "We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features.",
  "title": "Neural Joint Model for Transition-based Chinese Syntactic Analysis"
}