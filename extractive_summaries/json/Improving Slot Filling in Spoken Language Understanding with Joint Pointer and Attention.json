{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 426–431 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n426"
  }, {
    "heading": "1 Introduction",
    "text": "Slot filling is a key component in spoken language understanding (SLU), which is usually treated as a sequence labeling problem and solved using methods such as conditional random fields (CRFs) (Raymond and Riccardi, 2007) or recurrent neural networks (RNNs) (Yao et al., 2013; Mesnil et al., 2015).\nAlthough these models have achieved good results, they are learned on the datasets with wordlevel annotations, e.g., with the BIO tagging schema as in ATIS (Hemphill et al., 1990). Manual annotations at word level require big effort and some corpora has only sentence-level annotations available, e.g., the utterance “... moderately priced restaurant” has a slot-value pair annotation of “pricerange=moderate”. As such datasets lack explicit alignment between the annotations and the\ninput words, some systems rely on handcrafted rules to find the alignments in order to automatically create word-level labels to learn the sequence model (Zhou and He, 2011; Henderson, 2015), but finding such alignments is non-trivial. For example, it was shown in (Henderson, 2015) that when applying the manually created word aliases to the speech recognition hypotheses, only around 73% of alignments can be found due to the noise, and a CRF model trained on such noisy data performs particularly worse than some other methods. In addition it is time-consuming to adapt the manual rules or aliases to new domains.\nSome other work avoids this issue by regarding slot filling as a classification task (Henderson et al., 2012; Williams, 2014; Barahona et al., 2016), where an utterance is classified into one or more slot-value pairs. This, however, brings other challenges. One is that some types of slots may have a large or even unlimited number of possible values, so the classifiers may suffer from the data sparsity problem when the training data is limited. Another is the OOV problem caused by unknown slot values (e.g., restaurant name, street name), which is impossible to predefine and is very common in real-world spoken dialogue applications.\nTo address these challenges, we present a neural generative model for slot filling on unaligned dialog data, specifically for slot value prediction as it has more challenges caused by OOV. The model uses Seq2Seq learning to predict a sequence of slot values from an utterance. Inspired by the ability of pointer network (Ptr-Net) (Vinyals et al., 2015) at addressing OOV problems, we incorporate Ptr-Net into a standard Seq2Seq attentional model to handle OOV slots. It can predict slot values by either generating one from a fixed vocabulary or selecting a word from the utterance. The final model is a weighted combination of the two operations.\nTo summarize, our main contributions are:\n• We use a neural generative model for slot filling on the data without word-level annotations which has received less attention.\n• We adopt the pointer network to handle the OOV problem in slot value prediction, which achieves good performance without any manually-designed rules or features."
  }, {
    "heading": "2 Background of Pointer Network",
    "text": "Ptr-Net is a variation of the standard Seq2Seq model with attention. At each decoding step, it selects a position from the input sequence based on the attention distribution instead of generating a token from the target vocabulary. Given the input X = {x1, ..., xT }, the output yt at time step t is predicted by:\nPptr(yt = w|yt−11 , X) = ∑\ni:xi=w\nati (1)\nwhere ati is the attention weight of position i at step t. The advantage of Ptr-Net is that it can make better predictions on unknown or rare words. It has been successfully applied to tasks such as abstractive summarization (See et al., 2017), question answering (He et al., 2017), reading comprehension (Wang and Jiang, 2016), and chunking (Zhai et al., 2017)."
  }, {
    "heading": "3 Model for Slot Value Prediction",
    "text": "Our model for slot value prediction is a hybrid of a Seq2Seq attentional model and a Ptr-Net, similar as the one in See et al. (2017). The input is a sequence of words in an utterance, and the output is a sequence of slot values whose tokens may or may not appear in the input.\nThe hybrid model, illustrated in Figure 1, allows us to both generate a slot value from a fixed vocabulary and pick a value from the input via pointing. The two components (Seq2Seq and Ptr-Net) share the same encoder-decoder architecture and attention scores. We adopt a single-layer bidirectional GRU (Cho et al., 2014) for the encoder, and a single-layer unidirectional GRU for the decoder. The attention is calculated as in Bahdanau et al. (2014).\nThe slot vocabulary is set to contain only the values of enumerable slots, but not those of non-enumerable slots (e.g., values of “restaurant name”) as we assume these are not known in advance in the real scenarios.\nWe use the term “extended vocabulary” to denote the union of the slot vocabulary and all words from the input utterances. The probability distribution over the extended vocabulary is calculated as:\nP (w) = ptPgen + (1− pt)Pptr (2)\nThat is, the model makes the final predictions using a weighted combination of the predictions from two individual components. At the decoding step t, the Seq2Seq component produces the probability distribution Pgen for the next slot value within the vocabulary, while Ptr-Net produces the probability distribution Pptr over the input positions. pt ∈ [0, 1] is a parameter to balance the two components. It is learned at each time step based on the decoder input dt, decoder state st and the context vector ct as follows:\npt = σ(wcct + wsst + wddt + b) (3)\nwhere σ is a sigmoid function. wc, ws and wd are all trainable weights."
  }, {
    "heading": "4 Experiments",
    "text": "In this section, we present our experimental results on DSTC2 (Dialog State Tracking Challenge) (Henderson et al., 2014), including the results of slot value prediction solely and a complete SLU system. Our models are implemented using Keras1 with TensorFlow as backend. In all the experiments, the dimension of hidden states is 128, dimension of word embeddings is 100, dropout rate is 0.5, and batch size is 32. Word embeddings are not pre-trained but learned from scratch during training. Teacher forcing is used during training, with Adam optimizer (Kingma and Ba, 2014). All training consists of 10 epochs with early stopping on the development set."
  }, {
    "heading": "4.1 Data",
    "text": "DSTC2 consists of multi-turn dialogues between users and a dialog system, in the restaurant search domain. Each utterance is annotated with semantics including dialog-acts and slot-value pairs. For an utterance, both its transcription and 10-best hypotheses are provided. We use the top hypothesis as input throughout our experiments. The corpus has been separated into training, development and testing, containing 11,677, 3,934 and 9,890 utterances respectively."
  }, {
    "heading": "4.2 A Complete SLU System",
    "text": "For better evaluation and comparison, we integrated our model of slot value prediction into a complete SLU system, which uses a CNN classifier to obtain dialog-acts and slot types respectively after slot value prediction. For dialog act prediction, the input to the CNN model is the utterance and the output is one or more dialog acts (some utterances can have more than one dialog acts). For slot type prediction, the input is each predicted slot value together with the utterance, and the output is one of the predefined slot types. Given the limited numbers of various dialog-acts and slot types for classification, a standard CNN model is expected to achieve good performance.\n1https://keras.io\nNote that we can adopt other SLU frameworks as well (e.g., some joint frameworks), but given our focus in this work is to explore the hybrid Seq2Seq solutions for slot filling, we do not explore much on the SLU architecture, nor do we use any extra information (e.g., dialogue context). Despite the simplicity of our SLU system, it outperforms the prior state-of-the-art. In the whole process, neither manually designed features nor domain-specific rules are employed."
  }, {
    "heading": "4.3 Baselines",
    "text": "We compare the overall SLU performance of our system against two existing baselines on DSTC2. One baseline (Williams, 2014) uses binary SVM classifiers to predict the existence of each slotvalue pair and dialog act. The other (Barahona et al., 2016) uses CNN and LSTM jointly for classification.\nFor slot value prediction, since it is a sub-task of SLU and not reported in the prior work, we implemented another two models for it. One adopts CNN to classify an utterance into one or more slot values. The other uses the basic Seq2Seq attentional model (without Ptr-Net). Note that when learning this basic model, the target vocabulary is set to contain all the slot values in the training set."
  }, {
    "heading": "4.4 Results of Slot Value Prediction",
    "text": "We first report the results on slot value prediction only. We compare the results of our proposed model and our own implemented baselines in Table 1, using precision, recall and F1.\nWe can see that the proposed hybrid model achieves the best F1 score. Although CNN has a high precision, it suffers from the low recall. By looking into the results for each slot type, it is ob-\nserved that CNN performs much poorer on nonenumerable types of slots such as “food” due to its high cardinality. While both our model and the basic Seq2Seq model have higher recall.\nSince our assumption is that the proposed model can better handle the OOV problem, we analyze the OOV rate in the corpus to obtain more insight. By checking the percentage of slot values in the testing set that do not exist in the training, we find that the OOV problem in DSTC2 is not that severe, with a OOV ratio less than 0.1%. This could be a reason why our model does not obtain larger gain on the complete dataset. We therefore design more experiments in the next section to assess the model when the OOV problem is more severe."
  }, {
    "heading": "4.5 OOV Slot Prediction",
    "text": "We create specific datasets by re-sampling from the original corpus. In particular, let group A denote all the training utterances that contain nonenumerable slots, and group B denote the rest of the training utterances. We randomly select 5%, 10%, 15%, and 20% of group A, plus the whole set of group B. In this way, we can create training data with less non-enumerable slot values thus resulting in a higher OOV ratio. The testing set is same as before. We compare the proposed model with the baselines on these four specific datasets with different OOV ratios (Table 2).\nAs shown in each column, on all the specific datasets, our model achieves the best performance. The CNN model performs much poorer than before in terms of the recall. We can see that by reducing the training size, the OOV ratio (indicated in the first row in the brackets) goes up, and the performance of all models decreases in general. While CNN and the basic Seq2Seq model decline 10.3% and 9.2% in F1 respectively using the smallest training set compared to using the complete one, our model is the most stable one with the least performance drop of 5.4%. The gain of our model over the other two becomes more significant with the larger OOV rate. This shows the capability of the Ptr-Net to correctly predict the OOV slots.\nOverall, the results in Section 4.4 and 4.5 demonstrate the effectiveness of the proposed hybrid model for slot value prediction, especially when the training set is small and the OOV ratio is large."
  }, {
    "heading": "4.6 SLU Results",
    "text": "Table 3 compares the results of the overall SLU task by our systems (incorporated with different slot value prediction models) and prior arts. All our systems outperform the prior work, and among them the one with the proposed hybrid model achieves the best F1 score.\nWe also conduct the similar OOV experiments as in Section 4.5 for SLU (Table 4). Similar trend is observed as discussed before. The performance of the proposed model with 20% training data already reaches that of the best system reported in the literature with 100% training data."
  }, {
    "heading": "4.7 Case Study and Error Analysis",
    "text": "Table 5 gives some examples of slot values predicted by the proposed model and baselines. We\ncan see that for the less frequent slots, our model can still predict the values correctly, while without the Ptr-Net, the basic Seq2Seq model tends to generate words not appearing in the input, and CNN outputs nothing in many cases, which aligns with our assumption. We analyze the cases where PtrNet does not perform well and find several major types of errors: 1) partial prediction (e.g., detect only “oriental” for “asian oriental food”; 2) the prediction contains repetition of correct values; 3) speech recognition error although the prediction is proper if we look at the hypothesis itself (the third example). There are also cases where all the models fail to give a completely correct prediction, yet with different behaviors (the last example)."
  }, {
    "heading": "5 Conclusion",
    "text": "We adopt an attentional Seq2Seq model with Ptr-Net to predict slot values on dialogue data when only sentence-level semantic annotations are available. By switching between copying and generating words, this solution can bypass the need of word-level annotations and overcome the OOV issue which is very common in real-world spoken dialogue applications. It does not require any domain specific rules or dictionaries, and therefore can be easily adapted to new domains. Our model has achieved the state-of-the-art performance for both slot value prediction and SLU on the benchmark even with less training data."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Yifan He for helpful discussions and proofreading, and the anonymous reviewers for their valuable feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Exploiting sentence and context representations in deep neural models for spoken language understanding",
    "authors": ["Lina M Rojas Barahona", "Milica Gasic", "Nikola Mrkšić", "Pei-Hao Su", "Stefan Ultes", "Tsung-Hsien Wen", "Steve Young."],
    "venue": "Proceedings of COL-",
    "year": 2016
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"],
    "year": 2014
  }, {
    "title": "Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning",
    "authors": ["Shizhu He", "Cao Liu", "Kang Liu", "Jun Zhao."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
    "year": 2017
  }, {
    "title": "The atis spoken language systems pilot corpus",
    "authors": ["Charles T Hemphill", "John J Godfrey", "George R Doddington"],
    "venue": "In Proceedings of the DARPA speech and natural language workshop,",
    "year": 1990
  }, {
    "title": "Discriminative spoken language understanding using word confusion networks",
    "authors": ["Matthew Henderson", "Milica Gašić", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young."],
    "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE,",
    "year": 2012
  }, {
    "title": "The second dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Jason D Williams."],
    "venue": "SIGDIAL Conference, pages 263–272.",
    "year": 2014
  }, {
    "title": "Discriminative methods for statistical spoken dialogue systems",
    "authors": ["Matthew S Henderson."],
    "venue": "Ph.D. thesis, University of Cambridge.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Using recurrent neural networks for slot filling in spoken language understanding",
    "authors": ["Grégoire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"],
    "year": 2015
  }, {
    "title": "Generative and discriminative algorithms for spoken language understanding",
    "authors": ["Christian Raymond", "Giuseppe Riccardi."],
    "venue": "Eighth Annual Conference of the International Speech Communication Association.",
    "year": 2007
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J Liu", "Christopher D Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
    "year": 2017
  }, {
    "title": "Pointer networks",
    "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."],
    "venue": "Advances in Neural Information Processing Systems, pages 2692–2700.",
    "year": 2015
  }, {
    "title": "Machine comprehension using match-LSTM and answer pointer",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "arXiv preprint arXiv:1608.07905.",
    "year": 2016
  }, {
    "title": "Web-style ranking and slu combination for dialog state tracking",
    "authors": ["Jason D Williams."],
    "venue": "SIGDIAL Conference, pages 282–291.",
    "year": 2014
  }, {
    "title": "Recurrent neural networks for language understanding",
    "authors": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."],
    "venue": "Interspeech, pages 2524–2528.",
    "year": 2013
  }, {
    "title": "Neural models for sequence chunking",
    "authors": ["Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou."],
    "venue": "AAAI, pages 3365–3371.",
    "year": 2017
  }, {
    "title": "Learning conditional random fields from unaligned data for natural language understanding",
    "authors": ["Deyu Zhou", "Yulan He."],
    "venue": "Advances in Information Retrieval, pages 283–288.",
    "year": 2011
  }],
  "id": "SP:448d3978ba5bd07870fa117cecd99050dea4d0dc",
  "authors": [{
    "name": "Lin Zhao",
    "affiliations": []
  }, {
    "name": "Zhe Feng",
    "affiliations": []
  }],
  "abstractText": "We present a generative neural network model for slot filling based on a sequenceto-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data.",
  "title": "Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention"
}