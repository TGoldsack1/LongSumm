{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 401–406 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n401"
  }, {
    "heading": "1 Introduction",
    "text": "Neural NER trains a deep neural network for the NER task and has become quite popular as they minimize the need for hand-crafted features and, learn feature representations from the training data itself. Recently, multilingual learning has been shown to benefit Neural NER in a resource-rich language setting (Gillick et al., 2016; Yang et al., 2017). Multilingual learning aims to improve the NER performance on the language under consideration (primary language) by adding training data from one or more assisting languages. The neural network is trained on the combined data of the primary (DP ) and the assisting languages (DA). The neural network has a combination of languagedependent and language-independent layers, and, the network learns better cross-lingual features via these language-independent layers.\n∗This work began when the second author was a research scholar at IIT Bombay\nExisting approaches add all training sentences from the assisting language to the primary language and train the neural network on the combined data. However, data from assisting languages can introduce a drift in the tag distribution for named entities, since the common named entities from the two languages may have vastly divergent tag distributions. For example, the entity China appears in training split of Spanish (primary) and English (assisting) (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) with the corresponding tag frequencies, Spanish = { Loc : 20, Org : 49, Misc : 1 } and English = { Loc : 91, Org : 7 }. By adding English data to Spanish, the tag distribution of China is skewed towards Location entity in Spanish. This leads to a drop in named entity recognition performance. In this work, we address this problem of drift in tag distribution owing to adding training data from a supporting language.\nThe problem is similar to the problem of data selection for domain adaptation of various NLP tasks, except that additional complexity is introduced due to the multilingual nature of the learning task. For domain adaptation in various NLP tasks, several approaches have been proposed to address drift in data distribution (Moore and Lewis, 2010; Axelrod et al., 2011; Ruder and Plank, 2017). For instance, in machine translation, sentences from out-of-domain data are selected based on a suitably defined metric (Moore and Lewis, 2010; Axelrod et al., 2011). The metric attempts to capture similarity of the out-of-domain sentences with the in-domain data. Out-of-domain sentences most similar to the in-domain data are added.\nLike the domain adaptation techniques summarized above, we propose to judiciously add sentences from the assisting language to the primary language data based on the divergence between the tag distributions of named entities in the train-\ning instances. Adding assisting language sentences with lower divergence reduces the possibility of entity drift enabling the multilingual model to learn better cross-lingual features.\nFollowing are the contributions of the paper: (a) We present a simple approach to select assisting language sentences based on symmetric KLDivergence of overlapping entities (b) We demonstrate the benefits of multilingual Neural NER on low-resource languages. We compare the proposed data selection approach with monolingual Neural NER system, and the multilingual Neural NER system trained using all assisting language sentences. To the best of our knowledge, ours is the first work for judiciously selecting a subset of sentences from an assisting language for multilingual Neural NER."
  }, {
    "heading": "2 Judicious Selection of Assisting Language Sentences",
    "text": "For every assisting language sentence, we calculate the sentence score based on the average symmetric KL-Divergence score of overlapping entities present in that sentence. By overlapping entities, we mean entities whose surface form appears in both the languages’ training data. The symmetric KL-Divergence SKL(x), of a named entity x, is defined as follows,\nSKL(x) = [ KL( Pp(x) || Pa(x) ) +KL( Pa(x) || Pp(x) ) ] /2 (1)\nwhere Pp(x) and Pa(x) are the probability distributions for entity x in the primary (p) and the assisting (a) languages respectively. KL refers to the standard KL-Divergence score between the two probability distributions.\nKL-Divergence calculates the distance between the two probability distributions. Lower the KLDivergence score, higher is the tag agreement for an entity in both the languages thereby, reducing the possibility of entity drift in multilingual learning. Assisting language sentences with the sentence score below a threshold value are added to the primary language data for multilingual learning. If an assisting language sentence contains no overlapping entities, the corresponding sentence score is zero resulting in its selection.\nNetwork Architecture\nSeveral deep learning models (Collobert et al., 2011; Ma and Hovy, 2016; Murthy and Bhattacharyya, 2016; Lample et al., 2016; Yang et al., 2017) have been proposed for monolingual NER in the literature. Apart from the model by Collobert et al. (2011), remaining approaches extract sub-word features using either Convolution Neural Networks (CNNs) or Bi-LSTMs. The proposed data selection strategy for multilingual Neural NER can be used with any of the existing models. We choose the model by Murthy and Bhattacharyya (2016)1 in our experiments.\nMultilingual Learning\nWe consider two parameter sharing configurations for multilingual learning (i) sub-word feature extractors shared across languages (Yang et al., 2017) (Sub-word) (ii) the entire network trained in a language independent way (All). As Murthy and Bhattacharyya (2016) use CNNs to extract sub-word features, only the character-level CNNs are shared for the Sub-word configuration.\n1The code is available here: https://github.com/ murthyrudra/NeuralNER"
  }, {
    "heading": "3 Experimental Setup",
    "text": "In this section we list the datasets used and the network configurations used in our experiments."
  }, {
    "heading": "3.1 Datasets",
    "text": "The Table 1 lists the datasets used in our experiments along with pre-trained word embeddings used and other dataset statistics. For German NER, we use ep-96-04-16.conll to create train and development splits, and use ep-96-04-15.conll as test split. As Italian has a different tag set compared to English, Spanish and Dutch, we do not share output layer for All configuration in multilingual experiments involving Italian. Even though the languages considered are resource-rich languages, we consider German and Italian as primary languages due to their relatively lower number of train tokens. The German NER data followed IO notation and for all experiments involving German, we converted other language data to IO notation. Similarly, the Italian NER data followed IOBES notation and for all experiments involving Italian, we converted other language data to IOBES notation.\nFor low-resource language setup, we consider the following Indian languages: Hindi, Marathi2, Bengali, Tamil and Malayalam. Except for Hindi all are low-resource languages. We consider only Person, Location and Organization tags. Though the scripts of these languages are different, they share the same set of phonemes making script mapping across languages easier. We convert Tamil, Bengali and Malayalam data to the Devanagari script using the Indic NLP li-\n2Data is available here: http://www.cfilt.iitb. ac.in/ner/annotated_corpus/\nbrary3 (Kunchukuttan et al., 2015) thereby, allowing sharing of sub-word features across the Indian languages. For Indian languages, the annotated data followed the IOB format."
  }, {
    "heading": "3.2 Network Hyper-parameters",
    "text": "With the exception of English, Spanish and Dutch, remaining language datasets did not have official train and development splits provided. We randomly select 70% of the train split for training the model and remaining as development split. The threshold for sentence score SKL, is selected based on cross-validation for every language pair. The dimensions of the Bi-LSTM hidden layer are 200 and 400 for the monolingual and multilingual experiments respectively. We extract 20 features per convolution filter, with width varying from 1 to 9. The initial learning rate is 0.4 and multiplied by 0.7 when validation error increases. The training is stopped when the learning rate drops below 0.002. We assign a weight of 0.1 to assisting language sentences and oversample primary language sentences to match the assisting language sentence count in all multilingual experiments.\nFor European languages, we have performed hyper-parameter tuning for both the monolingual and multilingual learning (with all assisting language sentences) configurations. The best hyperparameter values for the language pair involved were observed to be within similar range. Hence, we chose the same set of hyper-parameter values for all languages.\n3https://github.com/anoopkunchukuttan/ indic_nlp_library"
  }, {
    "heading": "4 Results",
    "text": "We now present the results on both resource-rich and resource-poor languages."
  }, {
    "heading": "4.1 Resource-Rich Languages",
    "text": "Table 2 presents the results for German and Italian NER. We consistently observe improvements for German and Italian NER using our data selection strategy, irrespective of whether only subword features are shared (Sub-word) or the entire network (All) is shared across languages.\nAdding all Spanish/Dutch sentences to Italian data leads to drop in Italian NER performance when all layers are shared. Label drift from overlapping entities is one of the reasons for the poor results. This can be observed by comparing the histograms of English and Spanish sentences ranked by the SKL scores for Italian multilingual learning (Figure 1). Most English sentences have lower SKL scores indicating higher tag agreement for overlapping entities and lower drift in tag distribution. Hence, adding all English sentences improves Italian NER accuracy. In contrast, most Spanish sentences have larger SKL\nscores and adding these sentences adversely impacts Italian NER performance. By judiciously selecting assisting language sentences, we eliminate sentences which are responsible for drift occurring during multilingual learning.\nTo understand how overlapping entities impact the NER performance, we study the statistics of overlapping named entities between ItalianEnglish and Italian-Spanish pairs. 911 and 916 unique entities out of 4061 unique Italian entities appear in the English and Spanish data respectively. We had hypothesized that entities with divergent tag distribution are responsible for hindering the performance in multilingual learning. If we sort the common entities based on their SKL divergence value. We observe that 484 out of 911 common entities in English and 535 out of 916 common entities in Spanish have an SKL score greater than 1.0. 162 out of 484 common entities in English-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the English corpus. Similarly, 123 out of 535 common entities in Spanish-Italian data having SKL divergence value greater than 1.0 also appear more than 10 times in the Spanish corpus. However, these common 162 entities have a combined frequency of 12893 in English, meanwhile the 123 common entities have a combined frequency of 34945 in Spanish. To summarize, although the number of overlapping entities is comparable in English and Spanish sentences, entities with larger SKL divergence score appears more frequently in Spanish sentences compared to English sentences. As a consequence, adding all Spanish sentences leads to significant drop in Italian NER performance which is not the case when all English sentences are added."
  }, {
    "heading": "4.2 Resource-Poor Languages",
    "text": "As Indian languages exhibit high lexical overlap (Kunchukuttan and Bhattacharyya, 2016) and syntactic relatedness (V Subbãrão, 2012), we share all layers of the network across languages. Table 3 presents the results. Bengali, Malayalam, and Tamil (low-resource languages) benefits from our data selection strategy. Hindi and Marathi NER performance improves when the other is used as assisting language.\nBengali, Malayalam, and Tamil have weaker baselines compared to Hindi and Marathi, and are benefited from our approach irrespective of the assisting language chosen. However, Hindi and Marathi are not benefited from multilingual learning with Bengali, Malayalam and Tamil. Malayalam and Tamil being morphologically rich have low entity overlap (surface level) with Hindi and Marathi. As a result, only 2-3% of Malayalam and Tamil sentences are eliminated from our approach, leading to no gains from multilingual learning. Hindi and Marathi are negatively impacted by noisy Bengali data. Bengali has less training sentences compared to other languages and, choosing a low SKL threshold results in selecting very few Bengali sentences for multilingual learning."
  }, {
    "heading": "4.3 Influence of SKL Threshold",
    "text": "Here, we study the influence of SKL score threshold on the NER performance. We run experiments for Italian NER by adding Spanish training sentences and sharing all layers except for output layer across languages. We vary the threshold value from 1.0 to 9.0 in steps of 1, and select sentences with score less than the threshold. A threshold of 0.0 indicates monolingual training and threshold greater than 9.0 indicates all assist-\ning language sentences considered. The plot of Italian test F-Score against SKL score is shown in the Figure 2. Italian test F-Score increases initially as we add more and more Spanish sentences and then drops due to influence of drift becoming significant. Finding the right SKL threshold is important, hence we use a validation set to tune the SKL threshold."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we address the problem of divergence in tag distribution between primary and assisting languages for multilingual Neural NER. We show that filtering out the assisting language sentences exhibiting significant divergence in the tag distribution can improve NER accuracy. We propose to use the symmetric KL-Divergence metric to measure the tag distribution divergence. We observe consistent improvements in multilingual Neural NER performance using our data selection strategy. The strategy shows benefits for extremely low resource primary languages too.\nThis problem of drift in data distribution may not be unique to multilingual NER, and we plan to study the influence of data selection for multilingual learning on other NLP tasks like sentiment analysis, question answering, neural machine translation, etc. We also plan to explore more metrics for multilingual learning, specifically for morphologically rich languages."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Gajanan Rane and Geetanjali Rane for annotating the Marathi data, which was created as part of the CLIA project."
  }],
  "year": 2018,
  "references": [{
    "title": "Domain adaptation via pseudo in-domain data selection",
    "authors": ["Amittai Axelrod", "Xiaodong He", "Jianfeng Gao."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, United Kingdom.",
    "year": 2011
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2011
  }, {
    "title": "Eigenwords: Spectral word embeddings",
    "authors": ["Paramveer S. Dhillon", "Dean P. Foster", "Lyle H. Ungar."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2015
  }, {
    "title": "Training and evaluating a German Named Entity Recognizer with semantic generalization",
    "authors": ["Manaal Faruqui", "Sebastian Padó."],
    "venue": "Proceedings of KONVENS.",
    "year": 2010
  }, {
    "title": "Multilingual language processing from bytes",
    "authors": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics, San Diego, US.",
    "year": 2016
  }, {
    "title": "Orthographic syllable as basic unit for SMT between related languages",
    "authors": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Texas, USA.",
    "year": 2016
  }, {
    "title": "Brahmi-net: A transliteration and script conversion system for languages of the Indian subcontinent",
    "authors": ["Anoop Kunchukuttan", "Ratish Puduppully", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Asso-",
    "year": 2015
  }, {
    "title": "Indian language NER annotated FIRE 2014 corpus (FIRE 2014 NER Corpus)",
    "authors": ["Shobha Lalitha Devi", "Pattabhi RK Rao", "Malarkodi C.S", "R Vijay Sundar Ram."],
    "venue": "Named-Entity Recognition Indian Languages FIRE 2014 Evaluation Track.",
    "year": 2014
  }, {
    "title": "Neural architectures for Named Entity Recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for",
    "year": 2016
  }, {
    "title": "End-to-end sequence labeling via Bi-directional LSTM-CNNsCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Intelligent selection of language model training data",
    "authors": ["Robert C. Moore", "William Lewis."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.",
    "year": 2010
  }, {
    "title": "A deep learning solution to Named Entity Recognition",
    "authors": ["Rudra V. Murthy", "Pushpak Bhattacharyya."],
    "venue": "CICLing, Konya, Turkey.",
    "year": 2016
  }, {
    "title": "Learning to select data for transfer learning with Bayesian Optimization",
    "authors": ["Sebastian Ruder", "Barbara Plank."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark.",
    "year": 2017
  }, {
    "title": "The Named Entity Recognition task at EVALITA 2009",
    "authors": ["Manuela Speranza."],
    "venue": "Proceedings of the Workshop Evalita.",
    "year": 2009
  }, {
    "title": "Introduction to the conll-2002 shared task: Language-independent Named Entity Recognition",
    "authors": ["Erik F. Tjong Kim Sang."],
    "venue": "Proceedings of the 6th Conference on Natural Language Learning at COLING-02, Taipei, Taiwan.",
    "year": 2002
  }, {
    "title": "Introduction to the conll-2003 shared task: Language-independent Named Entity Recognition",
    "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder."],
    "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, Ed-",
    "year": 2003
  }, {
    "title": "South Asian Languages: A Syntactic Typology",
    "authors": ["Kãrumũri V Subbãrão."],
    "venue": "Cambridge University Press.",
    "year": 2012
  }, {
    "title": "Multi-task cross-lingual sequence tagging from scratch",
    "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen."],
    "venue": "International Conference on Learning Representations, Toulon, France.",
    "year": 2017
  }],
  "id": "SP:38f1d25773f7869bc043fe863fb8897becab425f",
  "authors": [{
    "name": "Rudra Murthy V",
    "affiliations": []
  }, {
    "name": "Anoop Kunchukuttan",
    "affiliations": []
  }, {
    "name": "Pushpak Bhattacharyya",
    "affiliations": []
  }],
  "abstractText": "Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data.",
  "title": "Judicious Selection of Training Data in Assisting Language for Multilingual Neural NER"
}