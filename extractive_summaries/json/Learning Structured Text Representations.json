{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013).\nRecent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Struc-\nture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.\nLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling. Moreover, despite recent advances in discourse processing, the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance.\nIt is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is\n63\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 63–75, 2018. Action Editor: Bo Pang. Submission batch: 5/2017; Published 1/2018.\nc©2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nto obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of the document.\nOur work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees. Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful task-specific dependency structures, achieving competitive results in several sentence-level tasks. However, for document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.1\nIn this paper, we propose a new model for representing documents while automatically learning richer structural dependencies. Using a variant of Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984), our model implicitly considers non-projective depen-\n1In our experiments, adding the inside-outside pass increases training time by a factor of 10.\ndency tree structures. We keep each step of the learning process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser. The inside-outside model of Kim et al. (2017) and our model both have a O(n3) worst case complexity. However, major operations in our approach can be parallelized efficiently on GPU computing hardware. Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences. Advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007).\nOur contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful."
  }, {
    "heading": "2 Background",
    "text": "In this section, we describe how previous work uses the attention mechanism for representing individual sentences. The key idea is to capture the interaction between tokens within a sentence, generating a context representation for each word with weak structural information. This type of intra-sentence attention encodes relationships between words within\neach sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences.\nFigure 2 provides a schematic view of the intrasentential attention mechanism. Given a sentence represented as a sequence of n word vectors [u1,u2, · · · ,un], for each word pair 〈ui,uj〉, the attention score aij is estimated as:\nfij = F (ui,uj) (1)\naij = exp(fij)∑n k=1 exp(fik)\n(2)\nwhere F () is a function for computing the unnormalized score fij which is then normalized by calculating a probability distribution aij . Individual words collect information from their context based on aij and obtain a context representation:\nri = n∑\nj=1\naijuj (3)\nwhere attention score aij indicates the (dependency) relation between the i-th and the j-th-words and how information from uj should be fed into ui.\nDespite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies. Since attention is computed as a simple probability distribution, it cannot capture more elaborate\nstructural dependencies such as trees (or graphs). Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism. Specifically, they normalize fij with a projective dependency tree using the inside-outside algorithm (Baker, 1979):\nfij = F (ui,uj) (4)\na = inside-outside(f) (5)\nri = n∑\nj=1\naijuj (6)\nThis process is differentiable, so the model can be trained end-to-end and learn structural information without relying on a parser. However, efficiency is a major issue, since the inside-outside algorithm has time complexity O(n3) (where n represents the number of tokens) and does not lend itself to easy parallelization. The high order complexity renders the approach impractical for real-world applications."
  }, {
    "heading": "3 Encoding Text Representations",
    "text": "In this section we present our document representation model. We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation. Structural information is taken into account while learning representations for both sentences and documents and an attention mechanism is applied on both words within a sentence and sentences within a document. The general idea is to force pair-wise attention between text units to form a non-projective dependency tree, and automatically induce this tree for different natural language processing tasks in a differentiable way. In the following, we first describe how the attention mechanism is applied to sentences, and then move on to present our document-level model."
  }, {
    "heading": "3.1 Sentence Model",
    "text": "Let T = [u1,u2, · · · ,un] denote a sentence containing a sequence of words, each represented by a vector u, which can be pre-trained on a large corpus. Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have\nbeen successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition (Graves et al., 2013), and image caption generation (Xu et al., 2015). In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an “infinite” window around it. Specifically, we run a bidirectional LSTM over sentence T , and take the output vectors [h1,h2, · · · ,hn] as the representations of words in T , where ht ∈ Rk is the output vector for word ut based on its context.\nWe then exploit the structure of T which we induce based on an attention mechanism detailed below to obtain more precise representations. Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSTM output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:\n[et,dt] = ht (7)\nwhere et ∈ Rkt , the semantic vector, encodes semantic information for specific tasks, and dt ∈ Rks , the structure vector, is used to calculate structured attention.\nWe use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the struc-\ntural bias of non-projective dependency trees into the attention weights. We constrain the probability distributions aij (see Equation (2)) to be the posterior marginals of a dependency tree structure. We then use the normalized structured attention, to build a context vector for updating the semantic vector of each word, obtaining new representations [r1, r2, · · · , rn]. An overview of the model is presented in Figure 3. We describe the attention mechanism in detail in the following section."
  }, {
    "heading": "3.2 Structured Attention Mechanism",
    "text": "Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic relations through directed graphs. Much work in descriptive linguistics (Melc̆uk, 1988; Tesniére, 1959) has advocated their suitability for representing syntactic structure across languages. A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions arising from long distance dependencies or free word order through nonprojective dependency edges.\nMore formally, building a dependency tree amounts to finding latent variables zij for all i 6= j, where word i is the parent node of word j, under some global constraints, amongst which the single-head constraint is the most important, since it forces the structure to be a rooted tree. We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each dependency edge P (zij = 1) of a non-projective dependency tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i.\nWe first calculate unnormalized attention scores fij with structure vector d (see Equation (7)) via a bilinear function:\ntp = tanh(Wpdi) (8)\ntc = tanh(Wcdj) (9)\nfij = t T pWatc (10)\nwhere Wp ∈ Rks∗ks and Wc ∈ Rks∗ks are the weights for building the representation of parent and child nodes. Wa ∈ Rks∗ks is the weight for the bilinear transformation. f ∈ Rn∗n can be viewed as\na weighted adjacency matrix for a graph G with n nodes where each node corresponds to a word in a sentence. We also calculate the root score f ri , indicating the unnormalized possibility of a node being the root:\nf ri = Wrdi (11)\nwhere Wr ∈ R1∗ks . We calculate P (zij = 1), the marginal probability of the dependency edge, following Koo et al. (2007):\nAij = { 0 if i = j exp(fij) otherwise\n(12)\nLij =\n{∑n i′=1Ai′j if i = j\n−Aij otherwise (13)\nL̄ij =\n{ exp(f ri ) i = 1\nLij i > 1 (14)\nP (zij = 1) = (1− δ1,j)Aij [L̄−1]jj − (1− δi,1)Aij [L̄−1]ji (15) P (root(i)) = exp(f ir)[L̄ −1]i1\nwhere 1 ≤ i ≤ n, 1 ≤ j ≤ n. L ∈ Rn∗n is the Laplacian matrix for graph G and L̄ ∈ Rn∗n is a variant of L that takes the root node into consideration, and δ is the Kronecker delta. The key for the calculation to hold is for Lii, the minor of the Laplacian matrix L with respect to row i and column i, to be equal to the sum of the weights of all directed spanning trees of G which are rooted at i. P (zij = 1) is the marginal probability of the dependency edge between the i-th and j-th words. P (root(i) = 1) is the marginal probability of the ith word headed by the root of the tree. Details of the proof can be found in Koo et al. (2007).\nWe denote the marginal probabilities P (zij = 1) as aij and P (root(i)) as ari . This can be interpreted as attention scores which are constrained to converge to a structured object, a non-projective dependency tree, in our case. We update the semantic\nvector ei of each word with structured attention:\npi = n∑\nk=1\nakiek + a r ieroot (16)\nci = n∑\nk=1\naikei (17)\nri = tanh(Wr[ei,pi, ci]) (18)\nwhere pi ∈ Rke is the context vector gathered from possible parents of ui and ci ∈ Rke the context vector gathered from possible children, and eroot is a special embedding for the root node. The context vectors are concatenated with ei and transformed with weights Wr ∈ Rke∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3)."
  }, {
    "heading": "3.3 Document Model",
    "text": "We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013).\nAs illustrated in Figure 4, given a document with n sentences [s1, s2, · · · , sn], for each sentence si, the input is a sequence of word embeddings [ui1,ui2, · · · ,uim], where m is the number of tokens in si. By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1, ri2, · · · , rim]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sentence vectors [v1,v2, · · · ,vn] whose embeddings are fed to a document-level bi-LSTM. Application of the structured attention mechanism creates new semantic vectors [q1, q2, · · · , qn] and another pooling operation yields the final document representation y."
  }, {
    "heading": "3.4 End-to-End Training",
    "text": "Our model can be trained in an end-to-end fashion since all operations required for computing structured attention and using it to update the semantic\nvectors are differentiable. In contrast to in Kim et al. (2017), training can be done efficiently. The major complexity of our model lies in the computation of the gradients of the the inverse matrix. Let A denote a matrix depending on a real parameter x; assuming all component functions in A are differentiable, and A is invertible for all possible values, the gradient of A with respect respect to x is:\ndA−1\ndx = −A−1dA dx A−1 (19)\nMultiplication of the three matrices and matrix inversion can be computed efficiently on modern parallel hardware architectures such as GPUs. In our experiments, computation of structured attention takes only 1/10 of training time."
  }, {
    "heading": "4 Experiments",
    "text": "In this section we present our experiments for evaluating the performance of our model. Since sentence representations constitute the basic building blocks of our document model, we first evaluate the performance of structured attention on a sentence-level task, namely natural language inference. We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language. Our\ncode is available at https://github.com/ nlpyang/structured."
  }, {
    "heading": "4.1 Natural Language Inference",
    "text": "The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing.\nSentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows. Let [xp1, · · · ,xpn] and [xh1 , · · · ,xhm] be the input vectors for the premise and hypothesis, respectively. Application of structured attention yields new vector representations [rp1, · · · , rpn] and [rh1 , · · · , rhm]. Then we combine these two vectors with inter-sentential attention, and apply an average pooling operation:\noij = MLP (r p i ) TMLP (rhj ) (20)\nr̄pi = [r p i ,\nm∑\nj=1\nexp(oij)∑m k=1 exp(oik) ] (21)\nr̄hi = [r h i ,\nm∑\ni=1\nexp(oij)∑m k=1 exp(okj) ] (22)\nrp = n∑\ni=1\ng(r̄pi ), r h =\nm∑\ni=1\ng(r̄hi ) (23)\nwhere MLP () is a two-layer perceptron with a ReLU activation function. The new representations rp, rh are then concatenated and fed into another two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels.\nThe hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the\nlearning rate was set to 0.05. The hidden size of the two-layer perceptron was set to 200 and dropout was used with ratio 0.2. The mini-batch size was 32.\nWe compared our model (and variants thereof) against several related systems. Results (in terms of 3-class accuracy) are shown in Table 1. Most previous systems employ LSTMs and do not incorporate a structured attention component. Exceptions include Cheng et al. (2016) and Parikh et al. (2016) whose models include intra-attention encoding relationships between words within each sentence (see Equation (2)). It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2017). The second block of Table 1 presents a version of our model without an intra-sentential attention mechanism as well as three variants with attention, assuming the structure of word-to-word re-\nlations and dependency trees. In the latter case we compare our matrix inversion based model against Kim et al.’s (2017) inside-outside attention model. Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention. Structured attention further enhances performance. Our own model with tree matrix inversion slightly outperforms the inside-outside model of Kim et al. (2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2017; Cheng et al., 2016; Parikh et al., 2016).\nTable 2 compares the running speed of the models shown in the second block of Table 1. As can be seen matrix inversion does not increase running speed over the simpler attention mechanism and is considerably faster compared to inside-outside. The latter is 10–20 times slower than our model on the same platform."
  }, {
    "heading": "4.2 Document Classification",
    "text": "In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset.\nYelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in Tang et al. (2015a) and report experiments on their training, development, and testing partitions (80/10/10).\nIMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies. Each review is associated with user ratings ranging from 1 to 10.\nCzech reviews were obtained from Brychcın and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative. We include Czech in our experiments since it has more flexible word order compared to English, with non-projective dependency structures being more frequent. Experiments on this dataset perform 10-fold cross-validation following previous work (Brychcın and Habernal, 2013).\n2http://www.csfd.cz/\nCongressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (“yea” or “nay”) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3\nFollowing previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token. Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset. In our experiments, we set the word embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel LSTMs to 100 (the dimensions of the semantic and structure vectors were set to 75 and 25, respectively). We used a mini-batch size of 32 during training and documents of similar length were grouped in one batch. Parameters were optimized with Adagrad (Duchi et al., 2011), the learning rate was set to 0.05. We used L2 regularization for all parameters except word embeddings with regularization constant set to 1e−4. Dropout was applied on the input and output\n3http://www.cs.cornell.edu/˜ainur/data. html\nlayers with dropout rate 0.3. Our results are summarized in Table 4. We compared our model against several related models covering a wide spectrum of representations including word-based ones (e.g., paragraph vector and CNN models) as well as hierarchically composed ones (e.g., a CNN or LSTM provides a sentence vector and then a recurrent neural network combines the sentence vectors to form a document level representation for classification). Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence. On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser. Table 4 presents three variants4 of our model, one with structured attention on the sentence level, another one with structured attention on the document level and a third model which employs attention on both levels. As can be seen, the combination is beneficial achieving best results on three out of four datasets. Furthermore, structured attention is superior to the simpler word-to-word attention mechanism, and both types of attention bring improvements over no attention. The structured attention approach is also very efficient, taking only 20 minutes for one training epoch on the largest dataset."
  }, {
    "heading": "4.3 Analysis of Induced Structures",
    "text": "To gain further insight on structured attention, we inspected the dependency trees it produces. Specifically, we used the Chu-Liu-Edmonds algo-\n4We do not report comparisons with the inside-outside approach on document classification tasks due to its prohibitive computation cost leading to 5 hours of training for one epoch.\nrithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores. We report various statistics on the characteristics of the induced trees across different tasks and datasets. We also provide examples of tree output, in an attempt to explain how our model uses dependency structures to model text.\nSentence Trees We compared the dependency trees obtained from our model with those produced by a state-of-the-art dependency parser trained on the English Penn Treebank. Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014). As can be seen, the induced dependency structures are simpler compared to those obtained from the Stanford parser. The trees are generally less deep (their height is 5.78 compared to 8.99 for the Stanford parser), with the majority being of depth 2–4. Almost half of the induced trees have a projective structure, although there is nothing in the model to enforce this constraint. We also calculated the percentage of headdependency edges that are identical between the two\nsets of trees. Although our model is not exposed to annotated trees during training, a large number of edges agree with the output of the Stanford parser.\nFigure 5 shows examples of dependency trees induced on the SNLI dataset. Although the model is trained without ever being exposed to a parse tree, it is able to learn plausible dependency structures via the attention mechanism. Overall we observe that the induced trees differ from linguistically motivated ones in the types of dependencies they create which tend to be of shorter length. The dependencies obtained from structured attention are more direct as shown in the first premise sentence in Figure 5 where words at and bar are directly connected to the verb drink. This is perhaps to be expected since the attention mechanism uses the dependency structures to collect information from other words, and the direct links will be more effective.\nDocument Trees We also used the Chu-LiuEdmonds algorithms to obtain document-level dependency trees. Table 6 summarizes various characteristics of these trees. For most datasets, documentlevel trees are not very deep, they mostly contain up to nodes of depth 3. This is not surprising as the documents are relatively short (see Table 3) with the exception of debates which are longer and the induced trees more complex. The fact that most documents exhibit simple discourse structures is further corroborated by the large number (over 70%) of projective trees induced on Yelp, IMBD, and CZ Movies datasets. Unfortunately, our trees cannot be directly compared with the output of a discourse parser which typically involves a segmentation process splitting sentences into smaller units. Our trees are constructed over entire sentences, and there is no mechanism currently in the model to split sentences\ninto discourse units.\nFigure 6 shows examples of document-level trees taken from Yelp and the Czech Movie dataset. In the first tree, most edges are examples of the “elaboration” discourse relation, i.e., the child presents\nadditional information about the parent. The second tree is non-projective, the edges connecting sentences 1 and 4 and 3 and 5 cross. The third review, perhaps due to its colloquial nature, is not entirely coherent. However, the model manages to link sentences 1 and 3 to sentence 2, i.e., the movie being discussed; it also relates sentence 6 to 4, both of which express highly positive sentiment."
  }, {
    "heading": "5 Conclusions",
    "text": "In this paper we proposed a new model for representing documents while automatically learning rich structural dependencies. Our model normalizes intra-attention scores with the marginal probabilities of a non-projective dependency tree based on a matrix inversion process. Each operation in this process is differentiable and the model can be trained efficiently end-to-end, while inducing structural information. We applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure. Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems. Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser.\nDirections for future work are many and varied. Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism. We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content.\nAcknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760). We also thank the anonymous TACL reviewers and the action editor whose feedback helped improve the present paper, members of EdinburghNLP for helpful discussions and suggestions, and Barbora Skarabela for translating the Czech document for us."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the ICLR Conference.",
    "year": 2015
  }, {
    "title": "Trainable grammars for speech recognition",
    "authors": ["James K. Baker."],
    "venue": "The Journal of the Acoustical Society of America 65(S1):S132–S132.",
    "year": 1979
  }, {
    "title": "Modeling local coherence: An entity-based approach",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Computational Linguistics 34(1):1–34.",
    "year": 2008
  }, {
    "title": "Better document-level sentiment analysis from RST discourse parsing",
    "authors": ["Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein."],
    "venue": "Proceedings of the EMNLP Conference. pages 2212– 2218.",
    "year": 2015
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."],
    "venue": "Proceedings of the EMNLP Conference. pages 632–642.",
    "year": 2015
  }, {
    "title": "A fast unified model for parsing and sentence understanding",
    "authors": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."],
    "venue": "Proceedings of the ACL Conference. pages 1466–1477.",
    "year": 2016
  }, {
    "title": "Unsupervised improving of sentiment analysis using global target context",
    "authors": ["Tomáš Brychcın", "Ivan Habernal."],
    "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing. pages 122–128.",
    "year": 2013
  }, {
    "title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory",
    "authors": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."],
    "venue": "Proceedings of the Second SIGdial Workshop on Discourse and Dialogue.",
    "year": 2001
  }, {
    "title": "Enhanced LSTM for natural language inference",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."],
    "venue": "Proceedings of the ACL Conference. pages 1657– 1668.",
    "year": 2017
  }, {
    "title": "Distraction-based neural networks for modeling documents",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."],
    "venue": "Proceedings of the IJCAI Conference. pages 2754–2760.",
    "year": 2016
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the EMNLP Conference. pages 551–561.",
    "year": 2016
  }, {
    "title": "On shortest arborescence of a directed graph",
    "authors": ["Yoeng-Jin Chu", "Tseng-Hong Liu."],
    "venue": "Scientia Sinica 14(10):1396.",
    "year": 1965
  }, {
    "title": "Frustratingly short attention spans in neural language modeling",
    "authors": ["Michał Daniluk", "Tim Rocktäschel", "Johannes Welbl", "Sebastian Riedel."],
    "venue": "Proceedings of the ICLR Conference .",
    "year": 2017
  }, {
    "title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)",
    "authors": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."],
    "venue": "Proceedings of the ACM SIGKDD Conference.",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "Journal of Machine Learning Research 12(Jul):2121–2159.",
    "year": 2011
  }, {
    "title": "Optimum branchings",
    "authors": ["Jack Edmonds."],
    "venue": "Journal of Research of the national Bureau of Standards B 71(4):233–240.",
    "year": 1967
  }, {
    "title": "Textlevel discourse parsing with rich linguistic features",
    "authors": ["Vanessa Wei Feng", "Graeme Hirst."],
    "venue": "Proceedings of the ACL Conference. pages 60–68.",
    "year": 2012
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Alex Graves", "Abdel-Rahman Mohamed", "Geoffrey Hinton."],
    "venue": "Proceedings of the IEEE ICASSP Conference. pages 6645–6649.",
    "year": 2013
  }, {
    "title": "Empirical comparison of dependency conversions for RST discourse trees",
    "authors": ["Katsuhiko Hayashi", "Tsutomu Hirao", "Masaaki Nagata."],
    "venue": "Pro-ceedings of the Annual Meeting of SIGDIAL. page 128.",
    "year": 2016
  }, {
    "title": "Single-document summarization as a tree knapsack problem",
    "authors": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."],
    "venue": "Proceedings of the EMNLP Conference. pages 1515–1520.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Neural discourse structure for text categorization",
    "authors": ["Yangfeng Ji", "Noah Smith."],
    "venue": "Proceedings of the ACL Conference.",
    "year": 2017
  }, {
    "title": "Structured attention networks",
    "authors": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."],
    "venue": "Proceedings of the ICLR Conference.",
    "year": 2017
  }, {
    "title": "Structured prediction models via the matrix-tree theorem",
    "authors": ["Terry Koo", "Amir Globerson", "Xavier Carreras Pérez", "Michael Collins."],
    "venue": "Proceedings of the EMNLP Conference. pages 141–150.",
    "year": 2007
  }, {
    "title": "Complexity of dependencies in discourse: Are dependencies in discourse more complex than in syntax",
    "authors": ["Alan Lee", "Rashmi Prasad", "Aravind Joshi", "Nikhil Dinesh", "Bonnie Webber."],
    "venue": "Proceedings of the International Workshop on Tree-",
    "year": 2006
  }, {
    "title": "Text-level discourse dependency parsing",
    "authors": ["Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li."],
    "venue": "Proceedings of the ACL Conference. pages 25–",
    "year": 2014
  }, {
    "title": "Automatically evaluating text coherence using discourse relations",
    "authors": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."],
    "venue": "Proceedings of the ACL Conference. pages 997–1006.",
    "year": 2011
  }, {
    "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
    "authors": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."],
    "venue": "Transactions of the Association for Computational Linguistics 4:521–535.",
    "year": 2016
  }, {
    "title": "Learning contextually informed representations for linear-time discourse parsing",
    "authors": ["Yang Liu", "Mirella Lapata."],
    "venue": "Proceedings of the EMNLP Conference. pages 1300–1309.",
    "year": 2017
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C. Mann", "Sandra A. Thompson."],
    "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243–281.",
    "year": 1988
  }, {
    "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky."],
    "venue": "Proceedings of the ACL Conference (System Demon-",
    "year": 2014
  }, {
    "title": "On the complexity of non-projective data-driven",
    "authors": ["Ryan McDonald", "Giorgio Satta"],
    "year": 2007
  }, {
    "title": "Dependency Syntax: Theory and Practice",
    "authors": ["Igor A. Melc̆uk"],
    "year": 1988
  }, {
    "title": "Graphbased coherence modeling for assessing readability",
    "authors": ["Mohsen Mesgar", "Michael Strube."],
    "venue": "Proceedings of the 4th Joint Conference on Lexical and Computational Semantics. pages 309–318.",
    "year": 2015
  }, {
    "title": "Implicitation of discourse connectives in (machine) translation",
    "authors": ["Thomas Meyer", "Bonnie Webber."],
    "venue": "Proceedings of the Workshop on Discourse in Machine Translation. pages 19–26.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."],
    "venue": "Proceedings of the NIPS Conference. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Key-value memory networks for directly reading documents",
    "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."],
    "venue": "Proceedings of the EMNLP Conference. pages 1400–1409.",
    "year": 2016
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "Proceedings of the EMNLP Conference. pages 2249– 2255.",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proceedings of the EMNLP Conference. pages 1532–1543.",
    "year": 2014
  }, {
    "title": "The Penn discourse TreeBank",
    "authors": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K. Joshi", "Bonnie L. Webber"],
    "year": 2008
  }, {
    "title": "Reasoning about entailment with neural attention",
    "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiskỳ", "Phil Blunsom."],
    "venue": "Proceedings of the ICLR Conference.",
    "year": 2016
  }, {
    "title": "Document modeling with gated recurrent neural network for sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proceedings of the EMNLP Conference. pages 1422–1432.",
    "year": 2015
  }, {
    "title": "Learning semantic representations of users and products for document level sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proceedings of the ACL Conference. pages 1014–1023.",
    "year": 2015
  }, {
    "title": "Éléments de Syntaxe Structurale",
    "authors": ["Louis Tesniére."],
    "venue": "Editions Klincksieck.",
    "year": 1959
  }, {
    "title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts",
    "authors": ["Matt Thomas", "Bo Pang", "Lillian Lee."],
    "venue": "Proceedings of the EMNLP Conference. pages 327–335.",
    "year": 2006
  }, {
    "title": "Discourse-based answering of why-questions",
    "authors": ["Suzan Verberne", "Lou Boves", "Nelleke Oostdijk", "Peter-Arno Coppen."],
    "venue": "Traitement Automatique des Language, Discours et Document: Traitements Automatics 47(2):21–41.",
    "year": 2007
  }, {
    "title": "Learning natural language inference with LSTM",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "Proceedings of NAACL Conference. pages 1442–1451.",
    "year": 2016
  }, {
    "title": "Coherence in Natural Language: Data Structures and Applications",
    "authors": ["Florian Wolf", "Edward Gibson."],
    "venue": "The MIT Press.",
    "year": 2006
  }, {
    "title": "Integrating document clustering and topic modeling",
    "authors": ["Pengtao Xie", "Eric P. Xing."],
    "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence. pages 694–703.",
    "year": 2013
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "International Conference on",
    "year": 2015
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "Proceedings of the NAACL Conference. pages 1480–1489.",
    "year": 2016
  }, {
    "title": "Linguistic structured sparsity in text categorization",
    "authors": ["Dani Yogatama", "Noah A. Smith."],
    "venue": "Proceedings of the ACL Conference. pages 786– 796.",
    "year": 2014
  }],
  "id": "SP:29219d826ead654f2b863de6eceb69811850b7d4",
  "authors": [{
    "name": "Yang Liu",
    "affiliations": []
  }],
  "abstractText": "In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.",
  "title": "Learning Structured Text Representations"
}