{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2017 Conference, pages 103–114, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Goal oriented dialogue systems help users with accomplishing tasks, like making restaurant reservations or booking flights, by interacting with them in natural language. The capability to understand user utterances and break them down into task specific semantics is a key requirement for these systems. This is accomplished in the spoken language understanding module, which typically parses user utterances into semantic frames, composed of domains, intents and slots (Tur and De Mori, 2011), that can then be processed by downstream dia-\nlogue system components. An example semantic frame is shown for a restaurant reservation related query in Figure 1. As the complexity of the task supported by a dialogue system increases, there is a need for an increased back and forth interaction between the user and the agent. For example, a restaurant reservation task might require the user to specify a restaurant name, date, time and number of people required for the reservation. Additionally, based on reservation availability, the user might need to negotiate on date, time, or any other attribute with the agent. This puts the burden of parsing in-dialogue contextual user utterances on the language understanding module. The complexity increases further when the system supports more than one task and the user is allowed to have goals spanning multiple domains within the same dialogue. Natural language utterances are often ambiguous, and the context from previous user and system turns could help resolve the errors arising from these ambiguities.\nIn this paper, we explore approaches to improve dialogue context modeling within a Recurrent Neural Network (RNN) based spoken language understanding system. We propose a novel model architecture to improve dialogue context modeling for spoken language understanding on a\n103\nmulti-domain dialogue dataset. The proposed architecture is an extension of Hierarchical Recurrent Encoder Decoders (HRED) (Sordoni et al., 2015), where we combine the query level encodings with a representation of the current utterance, before feeding it into the session level encoder. We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014). Furthermore, we describe a dialogue recombination technique to enhance the complexity of the training dataset by injecting synthetic domain switches, to create a better match with the mixed domain dialogues in the test dataset. This is, in principle, a multi-turn extension of (Jia and Liang, 2016). Instead of inducing and composing grammars to synthetically enhance single turn text, we combine single domain dialogue sessions into multi-domain dialogues to provide richer context during training."
  }, {
    "heading": "2 Related Work",
    "text": "The task of understanding a user utterance is typically broken down into 3 tasks: domain classification, intent classification and slot-filling (Tur and De Mori, 2011). Most modern approaches to Spoken language understanding involve training machine learning models on labeled training data (Young, 2002; Hahn et al., 2011; Wang et al., 2005, among others). More recently, recurrent neural network (RNN) based approaches have been shown to perform exceedingly well on spoken language understanding tasks (Mesnil et al., 2015; Hakkani-Tür et al., 2016; Kurata et al., 2016, among others). RNN based approaches have also been applied successfully to other tasks for di-\nalogue systems, like dialogue state tracking (Henderson, 2015; Henderson et al., 2014; Perez and Liu, 2016, among others), policy learning (Su et al., 2015) and system response generation (Wen et al., 2015, 2016, among others). In parallel, joint modeling of tasks and addition of contextual signals has been shown to result in performance gains for several applications. Modeling domain, intent and slots in a joint RNN model was shown to result in reduction of overall frame error rates (Hakkani-Tür et al., 2016). Joint modeling of intent classification and language modeling showed promising improvements in intent recognition, especially in the presence of noisy speech recognition (Liu and Lane, 2016). Similarly, models incorporating more context from dialogue history (Chen et al., 2016) or semantic context from the frame (Dauphin et al., 2014; Bapna et al., 2017) tend to outperform models without context and have shown potential for greater generalization on spoken language understanding and related tasks. (Dhingra et al., 2016) show improved performance on an informational dialogue agent by incorporating knowledge base context into their dialogue system. Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al., 2015). In the next few sections, we describe the proposed model architecture, the dataset and our dialogue recombination approach. This is followed by experimental results and analysis."
  }, {
    "heading": "3 Model Architecture",
    "text": "We compare the performance of 3 model architectures for encoding dialogue context on a multidomain dialogue dataset. Let the dialogue be a sequence of system and user utterances Dt =\n{u1, u2...ut} and at time step t we are trying to output the parse of a user utterance ut, given Dt. Let any utterance uk be a sequence of tokens given by {xk1, xk2...xknk}. We divide the model into 2 components, the context encoder that acts on Dt to produce a vector representation of the dialogue context denoted by ht = H(Dt), and the tagger, which takes the dialogue context encoding ht, and the current utterance ut as input and produces the domain, intent and slot annotations as output."
  }, {
    "heading": "3.1 Context Encoder Architectures",
    "text": "In this section we describe the architectures of the context encoders used for our experiments. We compare the performance of 3 different architectures that encode varying levels of dialogue context."
  }, {
    "heading": "3.1.1 Previous Utterance Encoder",
    "text": "This is the baseline context encoder architecture. We feed the embeddings corresponding to tokens in the previous system utterance, ut−1 = {xt−11 , xt−12 ...xt−1nt−1}, into a single Bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU) (Chung et al., 2014) cells and 128 dimensions (64 in each direction). The embeddings are shared with the tagger. The final state of the context encoder GRU is used as the dialogue context.\nht = BiGRUc(ut−1) (1)"
  }, {
    "heading": "3.1.2 Memory Network",
    "text": "This architecture is identical to the approach described in (Chen et al., 2016). We encode all dialogue context utterances, {u1, u2...ut−1}, into memory vectors denoted by {m1, m2, ...mt−1} using a Bidirectional GRU (BiGRU) encoder with 128 dimensions (64 in each direction). To add temporal context to the dialogue history utter-\nances, we append special positional tokens to each utterance.\nmk = BiGRUm(uk) for 0 ≤ k ≤ t−1 (2)\nWe also encode the current utterance with another BiGRU encoder with 128 dimensions (64 in each direction), into a context vector denoted by c, as in equation 3. This is conceptually depicted in Figure 2\nc = BiGRUc(ut) (3)\nLet M be a matrix with the ith row given by mi. We obtain the cosine similarity between each memory vector, mi, and the context vector c. The softmax of this similarity is used as an attention distribution over the memory M , and an attention weighted sum of M is used to produce the dialogue context vector ht (Equation 4). This is conceptually depicted in Figure 3.\na = softmax(Mc)\nht = aT M (4)"
  }, {
    "heading": "3.1.3 Sequential Dialogue Encoder Network",
    "text": "We enhance the memory network architecture described above by adding a session encoder (Sordoni et al., 2015) that temporally combines a joint representation of the current utterance encoding, c, (Eq. 3) and the memory vectors, {m1, m2...mt−1}, (Eq. 2). We combine the context vector c with each memory vector mk, for 1 ≤ k ≤ nk, by concatenating and passing them through a feed forward layer (FF) to produce 128 dimensional context encodings, denoted by {g1, g2...gt−1} (Eq. 5).\ngk = sigmoid(FF (mk, c)) for 0 ≤ k ≤ t−1 (5) These context encodings are fed as token level inputs into the session encoder, which is a 128 di-\nFigure 4: Architecture of the Sequential Dialogue Encoder Network. The feed-forward networks share weights across all memories.\nmensional BiGRU layer. The final state of the session encoder represents the dialogue context encoding ht (Eq. 6).\nht = BiGRUs({g1, g2, ...gt−1}) (6) The architecture is depicted in Figure 4."
  }, {
    "heading": "3.2 Tagger Architecture",
    "text": "For all our experiments we use a stacked BiRNN tagger to jointly model domain classification, intent classification and slot-filling, similar to the approach described in (Hakkani-Tür et al., 2016). We feed learned 256 dimensional embeddings corresponding to the current utterance tokens into the tagger. The first RNN layer uses GRU cells with 256 dimensions (128 in each direction) as in equation 7. The token embeddings are fed into the token level inputs of the first RNN layer to produce the token level outputs o1 = {o11, o12...o1nt}.\no1 = BiGRU1(ut) (7)\nThe second layer uses Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) cells with 256 dimensions (128 in both dimensions). We use a LSTM based second layer since that improved slot-filling performance on the validation set for all architectures. We apply dropout to the outputs of both layers. The initial states of both forward and backward LSTMs of the second tagger layer are initialized with the dialogue encoding ht as in equation 8. The token level outputs of the first RNN layer, o1, are fed as input\ninto the second RNN layer to produce token level outputs o2 = {o21, o22...o2nt} and the final state s2.\no2, s2 = BiLSTM2(o1, ht) (8)\nThe final state of the second layer, s2, is used as input to classification layers for domain and intent classification.\npdomain = softmax(Us2)\npintent = sigmoid(V s2) (9)\nThe token level outputs of the second layer, o2, are used as input to a softmax layer that outputs the IOB slot labels. This results in a softmax layer with 2N+1 dimensions for a domain with N slots.\npsloti = softmax(So 2 i ) for 0 ≤ i ≤ nt\n(10) The architecture is depicted in Figure 5."
  }, {
    "heading": "4 Dataset",
    "text": "We crowd sourced multi-turn dialogue sessions for 3 tasks: buying movie tickets, searching for a restaurant and reserving tables at a restaurant. Our data collection process comprises of two steps: (i) Generating user-agent interactions comprising of dialog acts and slots based on the interplay of a simulated user and a rule based dialogue policy. (ii) Using a crowd sourcing platform to elicit natural language utterances that align with the semantics of the generated interactions. The goal of the spoken language understanding module of our dialogue system is to map each user utterance into frame based semantics that can be processed by the downstream components. Tables describing the intents and slots present in the dataset can be found in the appendix. We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy. The user goal is specified in terms of a tuple of slots, which denote the user constraints. Some constraints might be unspecified, in which case the user is indifferent to the value of those slots. At any given turn, the simulator samples a user dialogue act from a set of acceptable actions based on (i) the user goal and agenda that includes slots that still need to be specified, (ii) a randomly chosen user profile (co-operative/aggressive, verbose/succinct etc.) and (iii) the previous user and\nsystem actions. Based on the chosen user dialogue act, the rule based policy might make a backend call to inquire for restaurant or movie availability. Based on the user act and the backend response the system responds back with a dialogue act or a combination of dialogue acts, based on a hand designed rule based policy. These generated interactions were then translated to their natural language counterparts and sent out to crowdworkers for paraphrasing into natural language human-machine dialogues. The simulator and policy were also extended to handle multiple goals spanning different domains. In this set-up, the user goal for the simulator would include multiple tasks and slot values could be conditioned on the previous task, for example, the simulator would ask for booking a table ”after the movie”, or search for a restaurant ”near the theater”. The set of slots supported by the simulator is enumerated in Table 1. We collected 1319 dialogues for restaurant reservation, 976 dialogues for finding restaurants and 1048 dialogues for buying movie tickets. All single domain datasets were\nused for training. The multi-domain simulator was used to collect 467 dialogues for training, 50 for validation and 273 for the test set. Since the natural language dialogues were paraphrased versions of known dialogue- act and slot combinations, they were automatically labeled. These labels were verified by an expert annotator, and turns with missing annotations were manually annotated by the expert."
  }, {
    "heading": "5 Dialogue Recombination",
    "text": "As described in the previous section, we train our models on a large set of single domain dialogue datasets and a small set of multi-domain dialogues. These models are then evaluated on a test set composed of multi-domain dialogues, where the user attempts to fulfill multiple goals spanning several domains. This results in a distribution drift that might result in performance degradation. To counter this drift in the training-test data distributions we device a dialogue recombination scheme to generate multi-domain dialogues from single domain training datasets.\nThe key idea behind the recombination approach is the conditional independence of sub-dialogues aimed at performing distinct tasks (Grosz and Sidner, 1986). We exploit the presence of task intents, or intents that denote a switch in the primary task the user is trying to perform, since they are a strong indicator of a switch in the focus of the dialogue. We exploit the independence of the sub-dialogue following these intents from the previous dialogue context, to generate synthetic dialogues with multi-domain context. The recombination process is described as follows: Let a dialogue d be defined as a sequence of turns and corresponding semantic labels (domain, intent and slot annotations) {(td1, fd1), (td2, fd2), ...(tdnd , fdnd}. To obtain a re-combined dataset composed of dialogues from dataset dataset1 and dataset2, we repeat the following steps 10000 times, for each combination of (dataset1, dataset2) from the three single domain datasets.\n• Sample dialogues x and y from dataset1 and dataset2 respectively.\n• Find the first user utterance labeled with a task intent in y. Let this be turn l.\n• Randomly sample an insertion point in dialogue x. Let this be turn k.\n• The new recombined dialogue is {(tx1, fx1), ...(txk, fxk), (tyl, fyl), ...(tyny , fyny)}.\nA sample dialogue generated using the above procedure is described in table 2. We drop the utterances from dialogue x following the insertion point (turn k) in the recombined dialogue since these turns become ambiguous or confusing in the absence of preceding context. In a sense our approach is one of partial dialogue recombination."
  }, {
    "heading": "6 Experiments",
    "text": "We compare the domain classification, intent classification and slot-filling performances, and the overall frame error rates of the encoder-decoder, memory network and sequential dialogue encoder network on the dataset described above. The frame error rate of a SLU system is the percentage of utterances where it makes a wrong prediction i.e. any of domain, intent or slot is predicted incorrectly. We trained all 3 models with RMSProp for 100000 training steps with a batch size of 100. We started with a learning rate of 0.0003 which was decayed by a factor of 0.95 every 3000 steps. Gradient norms were clipped if they exceed a magnitude of 2.5. All model and optimization hyper-parameters were chosen based on a grid search, to minimize validation set frame error rates.\nWe restrict the model vocabularies to contain only tokens occurring more than 10 times in the training set, to prevent over-fitting to training set entities. Digits were replaced with a special ”#” token to allow better generalization to unseen numbers. The dialogue history was padded to 40 utterances for batch processing. We report results with and without the recombined dataset in Table 3."
  }, {
    "heading": "7 Results",
    "text": "The encoder decoder model trained on just the previous turn context performs worst on almost all metrics, irrespective of the presence of recombined data. This can be explained by worse performance on in-dialogue utterances, where just the previous turn context isn’t sufficient to accurately identify the domain, and in several cases, the intents and slots of the utterance. The memory network is the best performing model in the absence of recombined data, indicating that\nthe model is able to encode additional context effectively to improve performance on all tasks, even when only a small amount of multi-domain data is available. The Sequential dialogue encoder network performs slightly worse than the memory network in the absence of recombined data. This could be explained by the model over-fitting to the single domain context seen during training and failure to utilize context effectively in a multi-domain setting. In the presence of recombined dialogues it outperforms all other implementations. Apart from increasing the noise in the dialogue context, adding recombined dialogues to the training set increases the average turn length of the training data, bringing it closer to that of the test dialogues. Our augmentation approach is, in spirit, an extension of the data recombination described in (Jia and Liang, 2016) to conversations. We hypothesize that the presence of synthetic con-\ntext has a regularization-like effect on the models. Similar effects were observed by (Jia and Liang, 2016), where training with longer, syntheticallyaugmented utterances resulted in improved semantic parsing performance on a simpler test set. This is also supported by the observation that performance improvements obtained by addition of recombined data increase as the complexity of the model increases."
  }, {
    "heading": "8 Discussion and Conclusions",
    "text": "Table 4 demonstrates an example dialogue from the test set, along with the gold and model annotations from all 3 models. We observe that Encoder Decoder (ED) and Sequential Dialogue Encoder Network (SDEN) are able to successfully identify the domain, intent and slots, while the Memory Network (MN) fails to identify the movie name.\nLooking at the attention distributions, we notice that the MN attention is very diffused, whereas SDEN is focusing on the most recent last 2 utterances, which directly identify the domain and the presence of the movie slot in the final user utterance. ED is also able to identify the presence of a movie in the final user utterance from the previous utterance context. Table 5 displays another example where the SDEN model outperforms both MN and ED. Constrained to just the previous utterance ED is unable to correctly identify the domain of the user utterance. The MN model correctly identifies the domain, using its strong focus on the task-intent bearing utterance, but it is unable to identify the presence of a restaurant in the user utterance. This highlights its failure to combine context from multiple history utterances. On the other hand, as indicated by its attention distribution on the final\ntwo utterances, SDEN is able to successfully combine context from the dialogue to correctly identify the domain and the restaurant name from the user utterance, despite the presence of several outof-vocabulary tokens. The above two examples hint that SDEN performs better in scenarios where multiple history utterances encode complementary information that could be useful to interpret user utterances. This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979). On the other hand, we also observed that SDEN performs significantly worse in the absence of recombined data. Due to its complex architecture and a much larger set of parameters SDEN is prone to over-fitting in low data scenarios. In this paper, we collect a multi-domain dataset of goal oriented human-machine conversations and analyze and compare the SLU performance of multiple neural network based model architectures that can encode varying amounts of context. Our experiments suggest that encoding more context from the dialogue, and enabling the model to combine contextual information in a sequential order results in a reduction in overall frame error rate. We also introduce a data augmentation scheme to generate longer dialogues with richer context, and empirically demonstrate that it results in performance improvement for multiple model architectures."
  }, {
    "heading": "9 Acknowledgements",
    "text": "We would like to thank Pararth Shah, Abhinav Rastogi, Anna Khasin and Georgi Nikolov for their help with the user-machine conversation data collection and labeling. We would also like to thank the anonymous reviewers for their insightful comments."
  }],
  "year": 2017,
  "references": [{
    "title": "Towards zero-shot frame semantic parsing for domain scaling",
    "authors": ["Ankur Bapna", "Gokhan Tür", "Dilek Hakkani-Tür", "Larry Heck."],
    "venue": "In Proceedings of the Interspeech. Stockholm, Sweden.",
    "year": 2017
  }, {
    "title": "Learning end-to-end goal-oriented dialog",
    "authors": ["Antoine Bordes", "Jason Weston."],
    "venue": "arXiv preprint arXiv:1605.07683 .",
    "year": 2016
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Y.-N. Chen", "D. Hakkani-Tür", "G. Tur", "J. Gao", "L. Deng"],
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1412.3555 .",
    "year": 2014
  }, {
    "title": "Zero-shot learning and clustering for semantic utterance classification",
    "authors": ["Y. Dauphin", "G. Tur", "D. Hakkani-Tür", "L. Heck."],
    "venue": "Proceedings of the ICLR.",
    "year": 2014
  }, {
    "title": "End-to-end reinforcement learning of dialogue agents for information access",
    "authors": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng."],
    "venue": "arXiv preprint arXiv:1609.00777 .",
    "year": 2016
  }, {
    "title": "Focusing and description in natural language dialogues",
    "authors": ["Barbara J Grosz."],
    "venue": "Technical report, DTIC Document.",
    "year": 1979
  }, {
    "title": "Attention, intentions, and the structure of discourse",
    "authors": ["Barbara J Grosz", "Candace L Sidner."],
    "venue": "Computational linguistics 12(3):175–204.",
    "year": 1986
  }, {
    "title": "Comparing stochastic approaches to spoken language understanding in multiple languages",
    "authors": ["S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lefevre", "P. Lehnen", "R. De Mori", "A. Moschitti", "H. Ney", "G. Riccardi."],
    "venue": "IEEE Transactions on Audio, Speech,",
    "year": 2011
  }, {
    "title": "Multidomain joint semantic frame parsing using bidirectional RNN-LSTM",
    "authors": ["D. Hakkani-Tür", "G. Tur", "A. Celikyilmaz", "Y.-N. Chen", "J. Gao", "L. Deng", "Y.-Y. Wang."],
    "venue": "Proceedings of the Interspeech. San Francisco, CA.",
    "year": 2016
  }, {
    "title": "Machine learning for dialog state tracking: A review",
    "authors": ["Matthew Henderson."],
    "venue": "Proceedings of The First International Workshop on Machine Learning in Spoken Language Processing.",
    "year": 2015
  }, {
    "title": "The second dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Jason D Williams"],
    "year": 2014
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "arXiv preprint arXiv:1606.03622 .",
    "year": 2016
  }, {
    "title": "Leveraging sentence-level information with encoder LSTM for semantic slot filling",
    "authors": ["G. Kurata", "B. Xiang", "B. Zhou", "M. Yu."],
    "venue": "Proceedings of the EMNLP. Austin, TX.",
    "year": 2016
  }, {
    "title": "Joint online spoken language understanding and language modeling with recurrent neural networks",
    "authors": ["Bing Liu", "Ian Lane."],
    "venue": "CoRR abs/1609.01462. http://arxiv.org/abs/1609.01462.",
    "year": 2016
  }, {
    "title": "Using recurrent neural networks for slot filling in spoken language understanding",
    "authors": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tür", "X. He", "L. Heck", "G. Tur", "D. Yu."],
    "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
    "year": 2015
  }, {
    "title": "Dialog state tracking, a machine reading approach using memory network",
    "authors": ["Julien Perez", "Fei Liu."],
    "venue": "arXiv preprint arXiv:1606.04052 .",
    "year": 2016
  }, {
    "title": "Agenda-based user simulation for bootstrapping a pomdp dialogue system",
    "authors": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the",
    "year": 2007
  }, {
    "title": "Hierarchical neural network generative models for movie dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."],
    "venue": "CoRR abs/1507.04808. http://arxiv.org/abs/1507.04808.",
    "year": 2015
  }, {
    "title": "Interactive reinforcement learning for taskoriented dialogue management",
    "authors": ["Pararth Shah", "Dilek Hakkani-Tür", "Larry Heck"],
    "year": 2016
  }, {
    "title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
    "authors": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jian-Yun Nie."],
    "venue": "Proceedings of the 24th",
    "year": 2015
  }, {
    "title": "Reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems",
    "authors": ["Pei-Hao Su", "David Vandyke", "Milica Gasic", "Nikola Mrksic", "Tsung-Hsien Wen", "Steve Young."],
    "venue": "arXiv preprint arXiv:1508.03391 .",
    "year": 2015
  }, {
    "title": "Spoken language understanding: Systems for extracting semantic information from speech",
    "authors": ["Gokhan Tur", "Renato De Mori."],
    "venue": "John Wiley & Sons.",
    "year": 2011
  }, {
    "title": "Spoken language understanding - an introduction to the statistical framework",
    "authors": ["Y.-Y. Wang", "L. Deng", "A. Acero."],
    "venue": "IEEE Signal Processing Magazine 22(5):16–31.",
    "year": 2005
  }, {
    "title": "Multi-domain neural network language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young."],
    "venue": "arXiv preprint arXiv:1603.01232 .",
    "year": 2016
  }, {
    "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "PeiHao Su", "David Vandyke", "Steve Young."],
    "venue": "arXiv preprint arXiv:1508.01745 .",
    "year": 2015
  }, {
    "title": "Memory networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "arXiv preprint arXiv:1410.3916 .",
    "year": 2014
  }, {
    "title": "Talking to machines (statistically speaking)",
    "authors": ["S. Young."],
    "venue": "Proceedings of the ICSLP. Denver, CO. 112",
    "year": 2002
  }],
  "id": "SP:84327b6267b867ce888ca8a99823b1f574090ba5",
  "authors": [{
    "name": "Ankur Bapna",
    "affiliations": []
  }, {
    "name": "Gokhan Tür",
    "affiliations": []
  }, {
    "name": "Dilek Hakkani-Tür",
    "affiliations": []
  }, {
    "name": "Larry Heck",
    "affiliations": []
  }],
  "abstractText": "Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.",
  "title": "Sequential Dialogue Context Modeling for Spoken Language Understanding"
}