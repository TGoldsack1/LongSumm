{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2018 Conference, pages 410–419, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics\n410"
  }, {
    "heading": "1 Introduction",
    "text": "An important part of dialog management in dialog systems is to detect the type of update to be performed for a slot after every turn in order to keep track of the dialog state. (The dialog state reflects the user goals specified as slot-value pairs.) User dialog acts (Young, 2007) express the user’s intents towards slots mentioned in the conversation. They are extracted in the spoken language understanding (SLU) module and are utilized by the downstream state tracking systems to update belief estimates (Williams et al., 2013; Lee and Stent, 2016; Henderson et al., 2014c). However,\n∗The work was done when the author was at Yahoo Research, Oath Inc.\ncurrently used dialog acts do not capture the update intended by the user in the following cases: 1. Implicit denials: User denials for slot-values are expressed using the “deny” and “negate” dialog acts. However, these acts only address explicit negations/denials such as “no”, “I do not want 〈slot-value〉’. But a user may express denial for a value implicitly. Consider utterances 8 and 9 in Table 1 where a user adds and removes people from a slot, PNAMES, which contains names of people going to an event. Current SLU systems would detect the “inform” dialog act in both utterances and, hence, would miss the (implicit denial) “remove” update. 2. Updates to numeric slots: Numeric slots are the slots whose values can be increased and decreased in addition to getting set/replaced. Since dialog acts do not capture the “increase” and “decrease” intents towards a numeric value, such updates cannot be handled using dialog acts alone. For example, consider utterances 4, 5 and 6 in Table 1 where the value of a numeric slot, NGUEST (number of guests in an invite), is set, increased and decreased respectively. The dialog act expressed in these utterances is “inform” which does not convey the update type. 3. Preference for slot values: The “inform” dialog act specifies values for slots but does not take into account the preferences for any particular slot value(s). Consider utterances 1, 2 and 3 in Table 1 where the location slot (LOC) is referred. In utterance 2, the user is equally interested in the three locations (“Ross”, “Napa” and “San Jose”). However, in utterance 3 the user prefers “Gilroy” over other values and intends to replace the old values with “Gilroy”. Clearly, the SLU output does not capture this change in the user intent.\nWe posit that identifying the above intents in user utterances as a part of SLU would improve estimation of user goals in task based dialogs. To ad-\ndress the above issues, we propose five generic update intents (UI’s) which are directly related to the type of update expressed by the user: Append, Remove, Replace, IncreaseBy and DecreaseBy, and build a model to identify them in a user utterance. Table 2 defines the five UI’s. We model the problem of identifying UI’s as a multi-class classification. For a user utterance, we classify UI’s for all the slot-values present in the utterance into one of the five classes. We treat an utterance as a sequence of tokens and slot-values, and perform sequence labeling using LSTM’s for the classification. It should be noted that the focus of this work is on identifying the UI’s in user utterance and not on investigating the mechanisms of using them for belief tracking, which is part of our larger goal.\nUI’s are generic in nature and independent of the dialog domain. Given a slot type (such as numeric), they can be applied to any slot of that type. This enables transfer learning across similar slots in different domains. To demonstrate this, we experiment with two domains (shopping and restaurants) and define three types of slots: 1. Numeric slots, 2. Conjunctive multi-value (CMV) slots, and 3. Disjunctive multi-value (DMV) slots (explained in Section 3.1.1). We then delexicalize slot-values in user utterances with the corresponding slot type (not slot name) and conduct cross-domain training and testing experiments. Experimental results demonstrate strong classification performance in individual domains as well as across domains.\nContributions: 1) We propose a new semantic class of slot-specific user intents (UI’s) which are\ndirectly related to the update a user intends to perform for a slot. 2) The proposed UI’s enable effective updates to slots. 3) Our models predict UI’s with high accuracy. 4) We present a novel delexicalization approach which enables transfer learning of UI’s across domains."
  }, {
    "heading": "2 Related Work",
    "text": "Although we are not aware of any prior work on identifying update intents, our current work is related to dialog act identification and dialog state tracking. Here, we review works in these two areas and contrast them against ours. Dialog act identification: Dialog acts (DA) in an utterance express the intention of their speaker/writer. Identifying DA types has been found to be useful in many natural language processing tasks such as question answering, summarization, and spoken language understanding (SLU) in dialog systems. A variety of DA’s have been proposed for specific application tasks and domains, such as email conversations (Cohen et al., 2004), online forum discussions (Bhatia et al., 2012; Kim et al., 2010), and dialog systems (Young, 2007). The latter is relevant to this work. In dialog systems, DA’s are used to infer a user’s intention towards either the slots or the conversation in general. Some of the DA’s used in dialog systems are inform, confirm, deny, and negate. Previous works on DA identification in dialog systems have used a range of approaches like n-grams based utterance level SVM classifier (Mairesse et al., 2009), SVM classifier built\non weighted n-grams using word confusion networks incorporating ASR uncertainties and dialog context (Henderson et al., 2012), log linear models (Chen et al., 2013), and recurrent neural networks (Hori et al., 2015, 2016; Ushio et al., 2016). This work is similar to DA identification in the sense that both the UI’s and the DA’s express certain semantics in the utterance and are independent of the dialog domain. However, there are important differences: 1) DA’s mainly reflect the intent towards the conversation; however, UI’s convey the type of update a user wants to a particular slot. 2) DA’s can be slot-independent (such as hello, negate, etc.) whereas UI’s are always defined with respect to a slot. Dialog State Tracking: Dialog state tracking (DST) entails updating the conversation state (also known as belief state) after every dialog turn. A conversation state is a probability distribution over competing user goals which are expressed in the form of slot-value pairs. For a user utterance, DST relies on SLU to get a list of k-best hypotheses of DA’s and slot-value pairs expressed in the utterance. To update the belief state, DST approaches utilize DA’s by using their SLU confidence scores as features (Ren et al., 2013; Kim and Banchs, 2014), encoding the DA’s using n-gram vectors weighted by the SLU confidence scores (Henderson et al., 2014c; Mrkšić et al., 2015), and using rule-based updates (Lee and Stent, 2016). Recently, efforts have been made to bypass the SLU output and learn update mechanisms directly from user utterance (Mrksic et al., 2017). Though DA’s are important for updating belief state, as explained in Section 1, certain updates like implicit denials, numeric updates, and slot preferences are not handled by the DA’s used in the dialog systems literature. UI’s, on the other hand, are proposed to address this problem. The work by Hakkani-Tür et al. (Hakkani-Tür et al., 2012) on identifying action updates in a multi-domain dialog system is closely related to the current work. Some of their action updates are similar to UI’s. However, unlike the current work, they did not deal with numeric updates and did not distinguish between types of multi-value slots (explained in Section 3.1.1)."
  }, {
    "heading": "3 Approach",
    "text": "In task-based dialogs, users complete a task by giving sequences of utterances in which they specify slot-values with corresponding intents. Dialog\nsystems extract this information using dialog act detection and slot-filling as part of SLU. The most common and helpful intents for completing a task are setting a value for a slot and denying a particular value for a slot. Traditionally, these two intents are determined by the inform and deny dialog acts. However, as explained in Section 1, a user may not always set and deny a value explicitly. While denials can be implicit, relative preferences can also be provided for slot-value(s). In case of numeric slots, user can set a value by incrementing or decrementing the previous values of slots. All these common scenarios are not handled by the inform and deny dialog acts.\nIn this work, we propose a new set of slotspecific intents which are directly related to the type of update expressed towards the slot. We call these intents update intents or UI’s. The UI’s express five common types of updates: 1. Append: A user specifies a value or multiple values for a multi-value slot. This is equivalent to “appending” the specified value(s) to a multi-value slot. (Refer to Section 3.1.1 for the definition of multi-value and numeric slots). 2. Remove: A user denies a value or multiple values for a multi-value slot implicitly or explicitly. This is equivalent to “removing” the specified value(s) from a multi-value slot. 3. Replace: A user specifies a preference for a slot value in case of multi-value slots. In case of numeric (single value) slots, this intent expresses setting and re-setting of a slot value (Utterances 4 and 7 in Table 1). This UI is defined with respect to the slot-value for which the preference is expressed. For example, in the utterance “I would prefer San Jose over Gilroy” the UI for San Jose is replace, whereas for Gilroy it is remove. Note that in case of multi-value slots, replace cannot be decomposed into a combination of an “append” and a “remove” update when the “remove” intent is\nnot specified. For example, in “I would prefer San Jose” there is no “remove” intent and, hence, simply using the “append” intent for San Jose would not extract the preference for San Jose.\n4. IncreaseBy: A user specifies a value by which a particular numeric slot’s value is to be increased.\n5. DecreaseBy: A user specifies a value by which a particular numeric slot’s value is to be decreased.\nTable 1 shows examples of the above five UI’s. The third column shows the expected SLU output with UI’s."
  }, {
    "heading": "3.1 Modeling",
    "text": "Given a user utterance, the goal is to determine UI’s for all the slot-values present in it. We formulate this task as a classification problem. Given a user utterance and the mentioned slot-values, classify the update intents for all the slot-values in one of the five classes: Append, Remove, Replace, IncreaseBy and DecreaseBy.\nWe model the above problem as a sequence labeling task. We treat a user utterance as a sequence of words and slot-values. The labels for slot-values are the corresponding UI’s whereas for words (which are not slot-values), we define a generic label “TOKEN”. For model optimization and error computation, we do not consider the “TOKEN” labels. Figure 1 describes our model architecture. We used Bidirectional LSTMs (Graves and Schmidhuber, 2005) for sequence labeling. For input representation, we used GloVe word embeddings (Pennington et al., 2014). For regularization, we used dropout and early stopping. We give more details about model parameters in Section 5."
  }, {
    "heading": "3.1.1 Learning Across Domains",
    "text": "In many cases, it is not possible to list all the values of a slot in the ontology. Even if the values are listed, it may not be practical to generate a training data containing all the values, if there are too many values for the slot. In such cases, it is beneficial to unlink the learning from particular slot-values and link it, instead, to the slot itself. This is because the word patterns used to refer to values of the same slot are similar and hence can be shared across the values. For example, a user would use similar word patterns to refer to values of slot “LOCATION”. One way to do this is by replacing slotvalues in utterances with the name of the slot. This is also called delexicalization and has been used successfully in many previous works (Henderson et al., 2014c; Mrkšić et al., 2015). In our model, we also delexicalize slot-values with the name of the slot as shown in Figure 1.\nDelexicalization with slot names is helpful in generalizing to slot-values not seen in the training data in one domain. However, it cannot be used for cross-domain generalization as different domains may not share the same slots. To address this problem, we define three generic slot types depending upon the values (numeric/non-numeric) a slot can take and whether a slot can take multiple values simultaneously (list-based) or not: 1. Numeric slot: Slots whose values can be increased and decreased. NGUEST in Table 1 is a numeric slot. A numeric slot is a single value slot, i.e., “appending” and “removing” (multiple) values are not allowed for numeric slots. This slot supports IncreaseBy, DecreaseBy and Replace UI’s. 2. Disjunctive multi-value (DMV) slot: Slots which can take multiple values only in disjunction, i.e., when user specifies those values as options. In a restaurant search domain, examples of DMV slots are location and cuisine. LOC slot in Table 1 is a DMV slot. 3. Conjunctive multi-value (CMV) slots: These are list type slots which can take multiple values in conjunction. Examples are slots containing names of people going to an event, items in a shopping list, etc. Slot PNAMES in Table 1 is a CMV slot. Both CMV and DMV slots support Append, Remove and Replace UI’s.\nDifferent domains may not share same slots but they often share slots with same types. For example, list of groceries in the shopping domain and\nlist of people in a dinner invite in the restaurant domain are of type CMV. Similarly, the number of guests in a dinner reservation and the number of items of a particular grocery are of type numeric. If we delexicalize slot-values with slot types, we can transfer learning for a slot type in one domain to the same slot type in another domain.\nThere can be cases where there are different ways (word patterns) to specify updates to two slots even if they are of the same type, because of differences in the corresponding domains or some other reason. For example, lets say slots S1 and S2 are in different domains but share the same slottype S and we have training data for slot S1. S1 and S2 have similarities owing to their common slot-type but have certain differences in the ways users can express update intents for them. In such a case, to generate training data for S2, we would need data capturing the differences between the two slots because the examples with common features are already contained in S1’s training data. Generating this additional data is easier than generating the full data for S2. The amount of additional data required will depend upon the degree by which the slots (S1 and S2) differ. When applied to a large number of slots and domains, this strategy would significantly reduce the time and effort that goes into data generation. To demonstrate this, we conduct training and testing experiments on two domains, restaurants and online shopping, and report results in Section 5.2."
  }, {
    "heading": "4 Data Preparation",
    "text": "To evaluate our approach, we needed dialogs containing numeric, CMV, and DMV slots in the domain ontology along with the proposed update intents expressed in user utterances. Existing datasets with annotated dialog acts such as WOZ 2.0 (Wen et al., 2017), ATIS (Dahl et al., 1994), Switchboard DA corpus 1, Dialog State Tracking Challenge (DSTC) datasets (Henderson et al., 2014a; Williams et al., 2013; Henderson et al., 2014b) and ICSI meeting recorder DA corpus (Shriberg et al., 2004) did not satisfy these requirements. DSTC 2 and DSTC 3 datasets contained DMV slots but not the CMV (list-based slots) and numeric slots 2. DSTC 4 (Kim et al., 2015), DSTC 5 (Kim et al., 2016) and DSTC\n1http://compprag.christopherpotts.net/ swda.html\n2The pricerange slot in DSTC2 and 3 is a categorical (and not a numeric) slot with a fixed set of values\n6 (Boureau et al., 2017) introduced a new set of speech acts which contains “HOW MUCH” act for the numeric price range and time slots. However, the act only supports the Replace UI and not the IncreaseBy and DecreaseBy UI’s. Moreover, the three datasets are not public. Therefore, we generated our own datasets.\nWe generated user utterances in two domains: restaurants and online shopping. In each domain, eight human editors generated user utterances independent of each other. The sets of editors were different across the two domains. Table 3 explains the slots used in the two domains. For each domain, we defined certain tasks which are listed in Table 4. Editors wrote conversations to complete those tasks. Since this was not a real dialog system, editors were asked to assume appropriate bot responses based on their requests such as “Okay”, “Added”, “Removed”, “Done” during the conversation. Also, since the focus was on identifying update intents and not on the overall SLU, (dialog act detection, slot-filling, etc.), we did not build our own custom slot-tagger and, instead, asked the editors to annotate the slot-values with the corresponding slot name in addition to the update intents. Here is a sample annotation for the task “restaurant reservation”.\nDrop NGUEST︷ ︸︸ ︷ one︸︷︷︸\nDecreaseBy\nperson, PNAMES︷ ︸︸ ︷ Joe︸︷︷︸\nRemove\ncan′t make it.\nFor the shopping domain, 305 conversations with 1308 user utterances were generated. For the restaurant domain, 280 conversations with 1323 user utterances were generated. The distribution of utterances among editors is 96, 110, 212, 79, 176, 258, 211 and 166 for the shopping domain. For the restaurant domain, the editorial distribution is 322, 181, 116, 106, 143, 107, 109 and 239. The distribution of Append, Remove, Replace, IncreaseBy and DecreaseBy UI’s for restaurant domain is 1022, 301, 601, 92, 112 respectively. The corresponding distribution for the shopping domain is 1249, 241, 521, 297, 90. Note that, an utterance may have more than one UI."
  }, {
    "heading": "5 Experiments and Results",
    "text": ""
  }, {
    "heading": "5.1 Experimental Setting",
    "text": "We implement the proposed architecture in Section 3 using Keras (Chollet et al., 2015), a high-level neural networks API, with the Tensorflow (Abadi et al., 2015) backend. Training is\ndone by mini-batch RMSProp (Hinton et al., 2012) with a fixed learning rate. In all our experiments, mini-batch size is fixed to 64. Training and inference are done on a per-utterance level. The embedding layer in the model is initialized with 300- dimensional Glove word vectors obtained from common crawl (Pennington et al., 2014). Embeddings for missing words are initialized randomly with values between −0.5 and 0.5. Evaluation: Using a random split of train and test sets would have examples from the same editor in both train and test sets which would bias the estimation. Therefore, we split our data into eight folds corresponding to the eight editors, i.e., each fold contains examples from only one of the editors. To evaluate our models, we train and validate on the data from seven folds and test the performance on the held-out (eighth) fold. We run this experiment for each editor, i.e., eight times, and average results across the eight folds. For validation, we use 15% of the training data. We use precision, recall and F-1 score to report the performance of our classifiers. Overall classification performance metrics are computed by taking the weighted average of the metrics for individual classes. A class’s weight is the ratio of the number of instances in it to the total number of instances. Parameter tuning: In each experiment, 15% of the current training set is utilized as a development set for hyper-parameter tuning and the model with best setting is applied to the test set to report the results. We tune learning rate, dropout via grid search on the development set. In addition, we uti-\nlize early stopping to avoid over-fitting. The optimal hyper-parameter settings for our classification experiments (reported in Table 5) is dropout = 0.3, learningrate = 0.001 for the restaurants domain and dropout = 0.25, learningrate = 0.001 for the shopping domain. Baseline: We used n-grams based multinomial logistic regression as a baseline. N-grams based models have been extensively used in text classification (Biyani et al., 2016, 2013, 2012). Such models have also been found to be effective as semantic tuple classifiers for dialog act detection and slot filling tasks (Chen et al., 2013; Henderson et al., 2012). Since there can be multiple slot-values and, hence, multiple UI’s expressed in a user utterance, the entire utterance cannot be used to extract n-grams for all the expressed UI’s. Therefore, we segment user utterances into relevant contexts for the slot-values and classify the contexts into one of the five UI classes. A context for a value is an ordered list of words which are indicative of the update to be performed for the value. We use two approaches for segmentation based on the k words window approach: a) hard segmentation, b) soft segmentation. In the first approach, we assign the words around the value to its context based on the following constraints: 1. If an utterance contains only one value, the entire utterance is taken as the context for the value. 2. If there are n words (s.t. n < 2k) between two slot values then the preference is given to the right value. That is, k words are assigned to the context of the right value and n− k words are assigned to the context of the left value. 3. All the words to the left of the first value (in the utterance) are added to the value’s context. Similarly, all the words to the right of the last value are added to its context.\nIn soft segmentation, we do not perform a hard assignment of the words, between the two values to the context of one of the values. Instead, we encode the words into one of these categories based on its position with respect to the value and if it is in between two values (and let the model learn weights for words in each category): 1) towards left of a value and between two values, 2) towards right of a value and between two values, 3) towards left of a value, 4) towards right of a value.\nWe extracted unigrams and bigrams from the context of slot-values. We experimented with different window sizes and k=2 gave the best results."
  }, {
    "heading": "5.2 Results",
    "text": "In this section, we present the results of our classification and domain-independence experiments."
  }, {
    "heading": "5.2.1 Classification Results",
    "text": "Table 5 shows the classification results on the two domains. For both the domains, our model achieves more than 90% overall F-1 scores. Perclass results are also strong. The Append, Replace, and IncreaseBy classes achieve more than 91% F-1 scores for the restaurant domain. For the shopping domain, IncreaseBy is the best performing class (97% F-1) followed by Append and DecreaseBy. Despite having significantly fewer examples compared to the other classes, IncreaseBy\nand DecreaseBy perform very well. One of the reasons for this behavior could be that after delexicalization, for these two classes, there is only one slot (QTY in shopping and NGUEST in restaurants) for which the model learns the patterns. Other than these two classes, this slot is shared by the Replace class. Hence, given a delexicalized numeric slot-value, the model needs to discriminate between these three classes whose relative distribution is much smoother than the overall distribution of the five classes. For the other delexicalized slot-values, the model discriminates between Append, Remove and Replace, where the majority class has a much higher number of examples than the minority Remove class. Hence, we see that the Append class performs significantly\nbetter than the Remove class. We also compare our model with the two baselines explained in Section 5.1. Table 6 presents these results. We see that the proposed model significantly outperforms the two baselines. This shows that for UI classification, contextual information around a slot-value is captured much more effectively using sequence models than static classifiers. We also experimented with our model without delexicalization to verify the gains it brings. As can be seen, delexicalization does improve the performance in both domains."
  }, {
    "heading": "5.2.2 Domain Independence Results",
    "text": "We conducted experiments to explore if learning of UI’s in a domain can be used to predict UI’s in a different domain. We use one of the domains as the “in-domain” (where learning is transferred to) and the other as the “out-domain” (where learning is transferred from). For this experiment, we set aside 20% of the in-domain data as the test set. At each step, we use 15% of the training data as the validation set. We explored two settings: 1. Combined-training: In this setting, we start by training our model on the entire out-domain data and then, incrementally, add a fraction (10%) of the in-domain data (left after taking out the test data) to the current training data, retrain the model (from scratch) on the combined data. 2. Pre-training: Here, we train a model on the out-domain data and fine-tune it with the indomain data. At each step, we add a fraction (10%) of the in-domain data to the current training data and refit the pre-trained out-domain model on it by initializing the model weights to the weights of the model trained on the out-domain data.\nFigures 2 and 3 report the results of these two settings. For Figure 2, the model trained only on the out-domain (restaurant) data achieves F-1 score of more than 80% on the in-domain test set. As we add more in-domain data, the F-1 score increases. With only 30% of the in-domain data, we get 89% F-1 score. Also, we see that pre-training and combined-training have similar performances.\nFor Figure 3, the out-domain model achieves a much lower F-1 score on the in-domain data. This shows that the transfer is not symmetric. This could be due to the PNAME slot, which has no similar slots in the shopping domain. There is also a difference between the performance curve of pre-training and combined-training. This indicates that fine-tuning a pre-trained model is harder than combined training when patterns are not covered by the out-domain data."
  }, {
    "heading": "6 Conclusions and Future Work",
    "text": "We proposed a new type of slot-specific user intents, update intents (UI’s), that are directly related to the type of update a user intends for a slot. The UI’s address user intents containing implicit denials, numeric updates and preferences for slot-values, which are not handled by the currently used dialog acts. We presented a sequence labeling model for classifying UI’s. We also proposed a method to transfer learning of UI’s across domains by delexicalizing slot-values with their slot types. For that, we defined three generic slot types. Experimental results showed strong performance for UI classification and promising results for the domain independence experiments. In the future, we plan to explore mechanisms to utilize the UI’s in belief tracking."
  }],
  "year": 2018,
  "references": [{
    "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
    "authors": ["Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."],
    "venue": "Software available from tensor-",
    "year": 2015
  }, {
    "title": "Classifying user messages for managing web forum data",
    "authors": ["Sumit Bhatia", "Prakhar Biyani", "Prasenjit Mitra."],
    "venue": "Proceedings of the 15th International Workshop on the Web and Databases. pages 13–18.",
    "year": 2012
  }, {
    "title": "Predicting subjectivity orientation of online forum threads",
    "authors": ["Prakhar Biyani", "Cornelia Caragea", "Prasenjit Mitra."],
    "venue": "Computational Linguistics and Intelligent Text Processing, Springer, pages 109–120.",
    "year": 2013
  }, {
    "title": "I want what i need!: analyzing subjectivity of online forum threads",
    "authors": ["Prakhar Biyani", "Cornelia Caragea", "Amit Singh", "Prasenjit Mitra."],
    "venue": "Proceedings of the 21st ACM international conference on Information and knowledge management. ACM,",
    "year": 2012
  }, {
    "title": " 8 amazing secrets for getting more clicks”: Detecting clickbaits in news streams using article informality",
    "authors": ["Prakhar Biyani", "Kostas Tsioutsiouliklis", "John Blackmer."],
    "venue": "AAAI. pages 94–100.",
    "year": 2016
  }, {
    "title": "Dialog state tracking challenge 6 end-to-end goal-oriented dialog track",
    "authors": ["Y-Lan Boureau", "Antoine Bordes", "Julien Perez."],
    "venue": "Technical report, Tech. Rep.",
    "year": 2017
  }, {
    "title": "An empirical investigation of sparse log-linear models for improved dialogue act classification",
    "authors": ["Yun-Nung Chen", "William Yang Wang", "Alexander I Rudnicky."],
    "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Confer-",
    "year": 2013
  }, {
    "title": "Keras",
    "authors": ["François Chollet"],
    "venue": "https:// github.com/fchollet/keras.",
    "year": 2015
  }, {
    "title": "Learning to classify email into“speech acts",
    "authors": ["William W Cohen", "Vitor R Carvalho", "Tom M Mitchell."],
    "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2004
  }, {
    "title": "Expanding the scope of the atis",
    "authors": ["Deborah A Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate Hunicke-Smith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg"],
    "year": 1994
  }, {
    "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks 18(5-6):602–610.",
    "year": 2005
  }, {
    "title": "A discriminative classification-based approach to information state updates for a multi-domain dialog system",
    "authors": ["Dilek Hakkani-Tür", "Gokhan Tur", "Larry Heck", "Ashley Fidler", "Asli Celikyilmaz."],
    "venue": "Thirteenth Annual Conference of the International",
    "year": 2012
  }, {
    "title": "Discriminative spoken language understanding using word confusion networks",
    "authors": ["Matthew Henderson", "Milica Gašić", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young."],
    "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE,",
    "year": 2012
  }, {
    "title": "The second dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Jason D Williams."],
    "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). pages 263–272.",
    "year": 2014
  }, {
    "title": "The third dialog state tracking challenge",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Jason D Williams."],
    "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, pages 324–329.",
    "year": 2014
  }, {
    "title": "Word-based dialog state tracking with recurrent neural networks",
    "authors": ["Matthew Henderson", "Blaise Thomson", "Steve J Young."],
    "venue": "SIGDIAL Conference. pages 292–299.",
    "year": 2014
  }, {
    "title": "Rmsprop: Divide the gradient by a running average of its recent magnitude",
    "authors": ["G Hinton", "Nitish Srivastava", "Kevin Swersky."],
    "venue": "Neural networks for machine learning, Coursera lecture 6e .",
    "year": 2012
  }, {
    "title": "Context sensitive spoken language understanding using role dependent lstm layers",
    "authors": ["Chiori Hori", "Takaaki Hori", "Shinji Watanabe", "John R Hershey."],
    "venue": "Machine Learning for SLU Interaction NIPS 2015 Workshop.",
    "year": 2015
  }, {
    "title": "Context-sensitive and role-dependent spoken language understanding using bidirectional and attention lstms",
    "authors": ["Chiori Hori", "Takaaki Hori", "Shinji Watanabe", "John R Hershey."],
    "venue": "INTERSPEECH. pages 3236–3240.",
    "year": 2016
  }, {
    "title": "Sequential labeling for tracking dynamic dialog states",
    "authors": ["Seokhwan Kim", "Rafael E. Banchs."],
    "venue": "Proceedings of the SIGDIAL 2014 Conference, The 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 18-20 June",
    "year": 2014
  }, {
    "title": "The fifth dialog state",
    "authors": ["Seokhwan Kim", "Luis Fernando D’Haro", "Rafael E Banchs", "Jason D Williams", "Matthew Henderson", "Koichiro Yoshino"],
    "year": 2016
  }, {
    "title": "Dialog state tracking challenge",
    "authors": ["Seokhwan Kim", "Luis Fernando DHaro", "Rafael E Banchs", "Jason Williams", "Matthew Henderson"],
    "year": 2015
  }, {
    "title": "Tagging and linking web forum posts",
    "authors": ["Su Nam Kim", "Li Wang", "Timothy Baldwin."],
    "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, pages 192–202.",
    "year": 2010
  }, {
    "title": "Task lineages: Dialog state tracking for flexible interaction",
    "authors": ["Sungjin Lee", "Amanda Stent."],
    "venue": "SIGDIAL Conference. pages 11–21.",
    "year": 2016
  }, {
    "title": "Spoken language understanding from unaligned data using discriminative classification models",
    "authors": ["François Mairesse", "Milica Gasic", "Filip Jurcı́cek", "Simon Keizer", "Blaise Thomson", "Kai Yu", "Steve Young"],
    "venue": "In Acoustics, Speech and Signal Processing,",
    "year": 2009
  }, {
    "title": "Multidomain dialog state tracking using recurrent neural networks",
    "authors": ["Nikola Mrkšić", "Diarmuid O Séaghdha", "Blaise Thomson", "Milica Gašić", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."],
    "venue": "arXiv preprint arXiv:1506.07190 .",
    "year": 2015
  }, {
    "title": "Neural belief tracker: Data-driven dialogue state tracking",
    "authors": ["Nikola Mrksic", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve J. Young."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.",
    "year": 2014
  }, {
    "title": "Dialog state tracking using conditional random fields",
    "authors": ["Hang Ren", "Weiqun Xu", "Yan Zhang", "Yonghong Yan."],
    "venue": "Proceedings of the SIGDIAL 2013 Conference. pages 457–461.",
    "year": 2013
  }, {
    "title": "The icsi meeting recorder dialog act (mrda) corpus",
    "authors": ["Elizabeth Shriberg", "Raj Dhillon", "Sonali Bhagat", "Jeremy Ang", "Hannah Carvey."],
    "venue": "Technical report, INTERNATIONAL COMPUTER SCIENCE INST BERKELEY CA.",
    "year": 2004
  }, {
    "title": "Recurrent convolutional neural networks for structured speech act tagging",
    "authors": ["Takashi Ushio", "Hongjie Shi", "Mitsuru Endo", "Katsuyoshi Yamagami", "Noriaki Horii."],
    "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, pages 518–524.",
    "year": 2016
  }, {
    "title": "A networkbased end-to-end trainable task-oriented dialogue system",
    "authors": ["Tsung-Hsien Wen", "David Vandyke", "Nikola Mrkšić", "Milica Gasic", "Lina M Rojas Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young."],
    "venue": "Proceedings of the 15th Conference of",
    "year": 2017
  }, {
    "title": "The dialog state tracking challenge",
    "authors": ["Jason D Williams", "Antoine Raux", "Deepak Ramachandran", "Alan W Black."],
    "venue": "SIGDIAL Conference. pages 404–413.",
    "year": 2013
  }, {
    "title": "Cued standard dialogue acts",
    "authors": ["Steve Young."],
    "venue": "Report, Cambridge University Engineering Department, 14th October .",
    "year": 2007
  }],
  "id": "SP:559aab8e6949cd608eb329bac0e82c628aa72a2d",
  "authors": [{
    "name": "Prakhar Biyani",
    "affiliations": []
  }, {
    "name": "Cem Akkaya",
    "affiliations": []
  }, {
    "name": "Kostas Tsioutsiouliklis",
    "affiliations": []
  }],
  "abstractText": "One important problem in task-based conversations is that of effectively updating the belief estimates of user-mentioned slot-value pairs. Given a user utterance, the intent of a slot-value pair is captured using dialog acts (DA) expressed in that utterance. However, in certain cases, DA’s fail to capture the actual update intent of the user. In this paper, we describe such cases and propose a new type of semantic class for user intents. This new type, Update Intents (UI), is directly related to the type of update a user intends to perform for a slot-value pair. We define five types of UI’s, which are independent of the domain of the conversation. We build a multi-class classification model using LSTM’s to identify the type of UI in user utterances in the Restaurant and Shopping domains. Experimental results show that our models achieve strong classification performance in terms of F-1 score.",
  "title": "Identifying Domain Independent Update Intents in Task Based Dialogs"
}