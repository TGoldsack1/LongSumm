{
  "sections": [{
    "text": "∑n i=1G(|xi|/α) ≤ 1} .\nWe consider the cases where the function G(·) grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given matrix A ∈ Rn×d with Orlicz norm into a lower dimensional space with `2 norm. Specifically, we show how to efficiently find an embedding matrix S ∈ Rm×n,m < n such that ∀x ∈ Rd,Ω(1/(d log n)) · ‖Ax‖G ≤ ‖SAx‖2 ≤ O(d2 log n) · ‖Ax‖G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem minx∈Rd ‖Ax−b‖G, up to aO(d log2 n) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the `p low rank matrix approximation problem for 1 ≤ p < 2."
  }, {
    "heading": "1. Introduction",
    "text": "Numerical linear algebra problems play a significant role in machine learning, data mining, and statistics. One of the most important such problems is the regression problem, see some recent advancements in, e.g., (Zhong et al., 2016; Bhatia et al., 2015; Jain & Tewari, 2015; Liu et al., 2014; Dhillon et al., 2013). In a linear regression problem, given a data matrix A ∈ Rn×d with n data points A1, A2, · · · , An in Rd and the response vector b ∈ Rn, the goal is to find a set of coefficients x∗ ∈ Rd such that\nx∗ = arg minx∈Rd l(Ax− b), (1) *Equal contribution 1Computer Science Department, Columbia University, New York City, NY 10027, U.S.A.. Correspondence to: Peilin Zhong <pz2225@columbia.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhere l : Rn → R+ is the loss function. When l(y) = ‖y‖22 = ∑n i=1 y 2 i , then the problem is the classic least square regression problem (`2 regression). While there has been extensive research on efficient algorithms for solving `2 regression, it is not always a suitable loss function to use.\nIn many settings, alternative choices for a loss function lead to qualitatively better solutions x∗. For example, a popular such alternative is the least absolute deviation (`1) regression — with l(y) = ‖y‖1 = ∑n i=1 |yi| — which leads to solutions that are more robust than those of `2 regression (see (Wikipedia; Gorard, 2005). In a nutshell, the `2 regression is suitable when the data contains Gaussian noise, whereas `1 — when the noise is Laplacian or sparse.\nA further popular class of loss functions l(·) arises from M-estimators, defined as l(y) = ∑n i=1M(yi) where M(·) is an M-estimator function (see (Zhang, 1997) for a list of M-estimators). The benefit of (some) M-estimators is that they enjoy advantages of both `1 and `2 regression. For example, when M(·) is the Huber function (Huber et al., 1964), then the regression looks like `2 regression when yi is small, and looks like `1 regression otherwise. However, these loss functions come with a downside: they depend on the scale, and rescaling the data may give a completely different solution! Our contributions. We introduce a generic algorithmic technique for solving regression for an entire class of loss functions that includes the aforementioned examples, and in particular, a “scale-invariant” version of M-estimators. Specifically, our class consists of loss functions l(y) that are Orlicz norms, defined as follows: given a non-negative convex function G : R+ → R+ with G(0) = 0, for x ∈ Rn,we can define ‖x‖G to be an Orlicz norm with respect to G(·): ‖x‖G , inf {α > 0 | ∑n i=1G(|xi|/α) ≤ 1} . Note that `p norm, for p ∈ [1,∞), is a special case of Orlicz norm with G(x) = xp. Another important example is the following “scale-free” version of M-estimator. Taking f(·) to be a Huber function, i.e.\nf(x) = { x2/2 |x| ≤ δ δ(|x| − δ/2) otherwise\nfor some constant δ, we take G(x) = f(f−1(1)x). Then the norm ‖x‖G looks like `2 norm when x is flat, and looks like `1 norm when x is sparse. Figure 1 shows the unit norm ball of this kind of Orlicz norm.\nOur main result is a generic algorithm for solving any regression problem Eqn. (1) with any loss function that is a “nice” Orlicz norm; see Section 2 for a formal definition of “nice”, and think of it as subquadratic for now.\nOur main result employs the concept of subspace embeddings, which is a powerful tool for solving numerical linear algebra problems, and as such has many applications beyond regression. We say that a subspace embedding matrix S ∈ Rm×n embeds the column space of A ∈ Rn×d (n > m) with u-norm into a subspace with v-norm, if ∀x ∈ Rd, we have ‖Ax‖u/α ≤ ‖SAx‖v ≤ β‖Ax‖u where αβ is called distortion (approximation). A long line of work studied `2 regression problem based on `2 subspace embedding techniques; see, e.g., (Clarkson & Woodruff, 2009; 2013; Nelson & Nguyên, 2013). Furthermore, there are works on `p regression problem based on `p subspace embedding techniques (see, e.g. (Sohler & Woodruff, 2011; Meng & Mahoney, 2013; Clarkson et al., 2013; Woodruff & Zhang, 2013)), and similarly for M-estimators (Clarkson & Woodruff, 2015).\nOur overall results are composed of four parts:\n1. We develop the first subspace embedding method for all “nice” Orlicz norms. The embedding obtains a distortion factor polynomial in d, which was recently shown necessary (Wang & Woodruff, 2018). 2. Using the above subspace embedding, we obtain the first approximation algorithm for solving the linear regression problem with any “nice” Orlicz norm loss. 3. As a further illustration of the power of the subspace embedding method, we employ it towards improving on the best known result for another problem: `p low rank approximation for 1 ≤ p < 2 from (Song et al., 2017), which is the “`p-version of PCA”. 4. Finally, we complement our theoretical results with experimental evaluation of our algorithms. Our experiments reveal that that the solution of regression under the Orlicz norm induced by Huber loss is much better than the solution given by regression under `1 or `2 norms, under natural noise distributions in practice. We also perform experiments for Orlicz regression with different Orlicz functions G and show their behavior under different noise settings, thus exhibiting the flexibility of our framework.\nTo the best of our knowledge, our algorithms are the first low distortion embedding and regression algorithms for general Orlicz norm. For the problem of low rank approximation under `p norm, p ∈ [1, 2), our algorithms achieve simultaneously the best approximation and the best running time. In contrast, all the previous algorithms achieve either the best approximation, or the best running time, but not both at the same time.\nOur algorithms for subspace embedding and regression are simple, and in particular are not iterative. In particular, for the subspace embedding, the embedding matrix S is generated independently of the data. In the regression problem, we multiply the input with the embedding matrix, and thus reduce to the `2 regression problem, for which we can use any of the known algorithm.\nTechnical discussion. Next we highlight some of our techniques used to obtain the theoretical results.\nSubspace embedding. Our starting point is a technique introduced in (Andoni et al., 2017) for the Orlicz norms, which can be seen as an embedding that has guarantees for a fixed vector only. In contrast, our main challenge here is to obtain an embedding for all vectors x ∈ Rn in a certain d-dimensional subspace. Consider a random diagonal matrix D ∈ Rn×n with each diagonal entry is a “generalized exponential” random variable, i.e., drawn from a distribution with cumulative distribution function 1− e−G(x). Then, for a fixed x ∈ Rd, (Andoni et al., 2017) show that ‖D−1Ax‖∞ is not too small with high probability. We can combine this statement together with a net argument and the dilation bound on ‖D−1Ax‖G, to argue that ∀x ∈ Rd, ‖D−1Ax‖∞ is not too small.\nThe other direction is more challenging — to show that for a given matrix A ∈ Rn×d, and any fixed x ∈ Rd, ‖D−1Ax‖G cannot be too large. Once we show this “dilation bound”, we combine it with the well-conditioned basis argument (similar to (Dasgupta et al., 2009)), and prove that ∀x ∈ Rd, ‖D−1Ax‖G cannot be too large. Overall, we have that ∀x ∈ Rd, ‖D−1Ax‖G ≤ O(d2 log n) · ‖Ax‖G, and ‖D−1Ax‖∞ ≥ Ω(1/(d log n)) · ‖Ax‖G. Since `2 norm is sandwiched by ‖ · ‖G and `∞ norm, we have that ∀x ∈ Rd,Ω(1/(d log n)) · ‖Ax‖G ≤ ‖D−1Ax‖2 ≤ O(d2 log n) · ‖Ax‖G. Then, the remaining part is to use standard techniques (Woodruff & Zhang, 2013; Woodruff, 2014) to perform the `2 subspace embedding for the column space of D−1A. See Theorem 16 for details.\nThe actual proof of the dilation bound is the most technically intricate result. Traditionally, since the pth power of the `p norm is the sum of the pth power of all the entries, it is easy to bound the expectation by using linearity of the expectation. However it is impossible to apply this analysis to Orlicz norm directly since Orlicz norm is not an ”entrywise” norm. Instead, we exploit a key observation that the\nOrlicz norm of vectors which are on the unit ball can be seen as the sum of contribution of each coordinate. Thus, we propose a novel analysis for any fixed vector by analyzing the behavior of the normalized vector which is on the unit Orlicz norm ball. To extend the dilation bound for a fixed vector to all the vectors in a subspace, we generalize the definition of `p norm well-conditioned basis to the Orlicz norm case, and then show that the Auerbach basis provides a good basis for Orlicz norm. To the best of our knowledge, this is the first time Auerbach basis are used to analyze the dilation bound of an embedding method for a norm distinct from an `p norm. See Section 3 for details.\nRegression with Orlicz norm. Here, given a matrix A ∈ Rn×d, a vector b ∈ Rn, the goal is to solve Equation 1 with Orlicz norm loss function. We can now solve this problem directly using the subspace embedding from above, in particular by applyingit to the column space of [A b]. We obtain an O(d3 log2 n) approximation ratio, which we can further improve by observing that it actually suffices to have the dilation bound on ‖D−1Ax∗‖G only for the optimal solution x∗ (as opposed to for an arbitrary x). Using this observation, we improve the approximation ratio to O(d log2 n). See Theorem 18 for details. We evaluate the algorithm’s performance and show that it performs well (even when n is not much larger than d). See Section 5.\n`p low rank matrix approximation. The `p norm is a special case of the Orlicz norm ‖ · ‖G, where G(x) = xp. This connection allows us to consider the following problem: given A ∈ Rn×d, n ≥ d ≥ k ≥ 1, find a rank-k matrix B ∈ Rn×d such that ‖A − B‖p is minimized. Here we consider the case of 1 ≤ p < 2 and k = ω(log n). The best known algorithm for this problem is from (Song et al., 2017), which uses the dense p-stable transform to achieves k2 · poly(log n) approximation ratio. It has the downside that its runtime does not compare favorably to the golden standard of runtime linear in the sparsity of the input. To improve the runtime, one can apply the sparse p-stable transform and achieve input sparsity runtime, but that comes at the cost of an Ω(k6) factor loss in the approximation ratio.\nUsing the above techniques, we develop an algorithm with best of both worlds: k2 · poly(log n) approximation ratio and the input sparsity running time at the same time. In particular, the main inefficiency of the algorithm (Song et al., 2017) is the relaxation from `p norm to `2 norm, which incurs a further poly(k) approximation factor. In contrast, the embedding based on exponential random variables embeds `p norm to `2 norm directly, without further approximation loss. Our embedding also comes with its own pitfalls — as we now need to deal with mixed norms — thus requiring a new analysis of the overall algorithm. See Theorem 23 for details."
  }, {
    "heading": "2. Notations and preliminaries",
    "text": "In this paper, we denote R+ to be the set of nonnegative reals. Define [n] = {1, 2, · · · , n}. Given a matrixA ∈ Rn×d, ∀i ∈ [n], j ∈ [d], Ai and Aj denotes the ith row and the jth column of A respectively. nnz(A) denotes the number of nonzero entries of A. The column space of A ∈ Rd is {y | ∃x ∈ Rd, y = Ax}. ∀p 6= 2, ‖A‖p , ( ∑ |Ai,j |p)1/p, i.e. entrywise p-norm. ‖A‖F defines the Frobenius norm of A, i.e. ( ∑ A2i,j)\n1/2. A† denotes the Moore-Penrose pseudoinverse of A. Given an invertible function f(·), let f−1(·) be the inverse function of f(·). If f(·) is not invertible in (−∞,+∞) but it is invertible in [0,+∞), then we denote f−1(·) to be the inverse function of f(·) in domain [0,+∞). inf and sup denote the infimum and supremum respectively. f ′(x), f ′+(x), f ′ −(x) denote the derivative, right derivative and left derivative of f(x), respectively. Similarly, define f ′′(x) for the second derivatives, and we define f ′′+(x) = limh→0+(f\n′(x+h)−f ′+(x))/h. In the following, we give the definition of Orlicz norm. Definition 1 (Orlicz norm) For any nonzero monotone nondecreasing convex function G : R+ → R+ with G(0) = 0. Define Orlicz norm ‖ · ‖G as: ∀n ∈ Z, n ≥ 1, x ∈ Rn, ‖x‖G = inf {α > 0 | ∑n i=1G(|xi|/α) ≤ 1} . For any function G1(·) which is valid to define an Orlicz norm, we can always “simplify/normalize” the function to get another function G2 such that computing ‖ · ‖G1 is equivalent to computing ‖ · ‖G2 . Fact 2 Given a function G1 : R+ → R+ which can induce an Orlicz norm ‖ · ‖G1 (Definition 1), define function G2 : R+ → R+ as the following:\nG2(x) =\n{ G1(G −1 1 (1)x) 0 ≤ x ≤ 1\nsx− (s− 1) x > 1 where s =\nsup {(G2(y)−G2(x)) /(y − x) | 0 ≤ x ≤ y ≤ 1} . Then ‖ · ‖G2 is a valid Orlicz norm. Furthermore, ∀n ∈ Z, n ≥ 1, x ∈ Rn, we have ‖x‖G1 = ‖x‖G2/G−11 (1). Thus, without loss of generality, in this paper we consider the Orlicz norm induced by function G which satisfies G(1) = 1, and G(x) is a linear function for x > 1. In addition, we also require that G(x) grows no faster than quadratically in x. Thus, we define the property P of a function G : R → R+ as the following: 1) G is a nonzero monotone nondecreasing convex function in [0,∞); 2) G(0) = 0, G(1) = 1,∀x ∈ R, G(x) = G(−x); 3) G(x) is a linear function for x > 1, i.e. ∃s > 0,∀x > 1, G(x) = sx + (1 − s); 4) ∃δG > 0 such that G is twice differentiable on interval (0, δG). Furthermore, G′+(0) and G′′+(0) exist, and either G ′ +(0) > 0 or G ′′ +(0) > 0; 5) ∃CG > 0,∀0 < x < y,G(y)/G(x) ≤ CG(y/x)2.\nThe condition 1 is required to define an Orlicz norm. The conditions 2,3 are required because we can always do the simplification/normalization (see Fact 2). The condition 4 is required for the smoothness of G. The condition 5 is due to the subquadratic growth condition. Subquadratic\nTable 1. Some of M-estimators.\nHUBER\n{\nx2/2 |x| ≤ c c(|x| − c/2) |x| > c\n`1 − `2 2( √\n1 + x2/2− 1) “FAIR” c2 (|x|/c− log(1 + |x|/c))\ngrowth condition is necessary for sketching ∑n i=1G(xi) with sketch size sub-polynomial in the dimension n, as shown by (Braverman & Ostrovsky, 2010). For example, if G(x) = xp for some p > 2, then ‖ · ‖G is the same as ‖ · ‖p. It is necessary to take Ω(n1−2/p) space to sketch `p norm in n-dimensional space. Condition 5 is also necessary for 2-concave property, (Kwapien & Schuett, 1985; Kwapie & Schtt, 1989) shows that ‖ · ‖G can be embedded into `1 space if and only if G is 2-concave. Although (Schtt, 1995) gives an explicit embedding to `1, it cannot be computed efficiently.\nThere are many potential choices of G(·) which satisfies property P , the following are some examples: 1) G(x) = |x|p for some 1 ≤ p ≤ 2. In this case ‖ · ‖G is exactly the `p norm ‖ · ‖p; 2) G(x) can be a normalized M-estimator function (see (Zhang, 1997)), i.e. define f(x) to be one of the functions in Table 1. and let\nG(x) = { f(f−1(1)x) |x| ≤ 1 G′−(1)|x| − (G′−(1)− 1) |x| > 1 .\nThe following presents some useful properties of function G with property P. See Appendix for details of proofs of the following Lemmas. Lemma 3 Given a function G(·) with property P , then ∀0 ≤ x ≤ 1, x2/CG ≤ G(x) ≤ x. Lemma 4 Given a function G(·) with property P , then ∀x ∈ Rn, ‖x‖2/ √ CG ≤ ‖x‖G ≤ ‖x‖1. Lemma 5 Given a function G(·) with property P , then ∀0 < x < y, we have y/x ≤ G(y)/G(x). Lemma 6 Given a function G(·) with property P , there exist a constant αG > 0 which may depend on G, such that ∀0 ≤ a, b, if ab ≤ 1, then G(a)G(b) ≤ αGG(ab)."
  }, {
    "heading": "3. Subspace embedding for Orlicz norm using exponential random variables",
    "text": "In this section, we develop the subspace embedding under the Orlicz norms which are induced by functions G with the property P . We first show how to embed the subspace with ‖ · ‖G norm into a subspace with `2 norm, and then we use dimensionality reduction techniques for the `2 norm. Overall, we will prove Theorem 16 stated at the end of this section. Before discussing the details, we give formal definitions of subspace embedding. Definition 7 (Subspace embedding for Orlicz norm) Given a matrix A ∈ Rn×d, if S ∈ Rm×n satisfies ∀x ∈ Rd, ‖Ax‖G/α ≤ ‖SAx‖v ≤ β‖Ax‖G where α, β ≥ 1, ‖ · ‖v is a norm (can still be ‖ · ‖G), then we say S embeds the column space of A with Orlicz norm into the column space of SA with v-norm. The distortion is αβ.\nIf the distortion and the v-norm are clear from the context, we just say S is a subspace embedding matrix for A.\nDefinition 8 (Subspace embedding for `2 norm) Given a matrix A ∈ Rn×d, if S ∈ Rm×n satisfies ∀x ∈ Rd, (1− ε)‖Ax‖22 ≤ ‖SAx‖22 ≤ (1 + ε)‖Ax‖22, then we say S is a subspace embedding of column space of A.\nThere are many choices of `2 subspace embedding matrix A satisfying the above definition. Examples are: random sign JL matrix (Achlioptas, 2003; Clarkson & Woodruff, 2009), fast JL matrix (Ailon & Chazelle, 2009), and sparse embedding matrices (Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013).\nThe main technical thrust is to embed ‖·‖G into `2 norm. As the embedding matrix, we use S = ΠD−1 where Π is one of the above `2 embedding matrices andD is a diagonal matrix of which diagonal entries are i.i.d. random variables draw from the distribution with CDF 1 − e−G(t). Equivalently, each entry on the diagonal of D is G−1(u), where u is an i.i.d. sample from the standard exponential distribution, i.e. CDF is 1 − e−t. In Section 3.1, we will prove that ∀x ∈ Rd, ‖D−1Ax‖Gwill not be too large. In Section 3.2, we will show that ∀x ∈ Rd, ‖D−1Ax‖∞ cannot be too small. Then due to Lemma 4, we know that ‖D−1Ax‖2 is a good estimator to ‖Ax‖G. In Section 3.3, we show how to put all the ingredients together."
  }, {
    "heading": "3.1. Dilation bound",
    "text": "We construct a randomized linear map f : Rn → Rn: (x1, x2, ..., xn)\nf7−→ (x1/u1, x2/u2, ..., xn/un) where each ui is drawn from a distribution with CDF 1− e−G(t). Notice that for proving the dilation bound, we do not need to assume ui are independent. Theorem 9 Given x ∈ Rn, let ‖ · ‖G be an Orlicz norm induced by function G(·) which has property P , and let f(x) = (x1/u1, x2/u2, ..., xn/un), where each ui is drawn from a distribution with CDF 1 − e−G(t). Then with probability at least 1 − δ − O(1/n19), ‖f(x)‖G ≤ O(αGδ\n−1 log(n))‖x‖G, where αG is a constant may depend on function G(·). Proof sketch: By taking union bound, we have ∀i ∈ [n], ui ≥ G−1(1/n20) with high probability. Let α = ‖x‖G. For γ ≥ 1, we want to upper bound the probability that ‖f(x)‖G ≥ γα. This is equivalent to upper bound the probability that ‖f(x)/(γα)‖G ≥ 1. Notice that Pr(‖f(x)/(γα)‖G ≥ 1) = Pr( ∑ G(xi/α·1/(γui)) ≥ 1). By Markov inequality, it suffices to bound the expectation of ∑ G(xi/α ·1/(γui)) conditioned on ui are not too small.\nBy lemma 6, ∑ G(xi/α · 1/(γui)) ≤ αG/γ · ∑ G(xi/α) · 1/G(ui). Because ui is not too small, the conditional expectation of 1/G(ui) is roughly O(log n). So the probability that ‖f(x)‖G ≥ γα is bounded by O(αG log n/γ), set γ = O(log n)αG/δ, we can complete the proof. See appendix for the details of the whole proof.\nThe final step is to use a well-conditioned basis; see details in appendix.We then obtain the following theorem.\nTheorem 10 Let G(·) be a function which has property P. Given a matrix A ∈ Rn×m with rank d ≤ n, let D ∈ Rn×n be a diagonal matrix of which each entry on the diagonal is drawn from a distribution with CDF 1 − e−G(t). Then, with probability at least 0.99, ∀x ∈ Rm, ‖D−1Ax‖G ≤ O(αGd\n2 log n)‖Ax‖G, where αG ≥ 1 is a constant which only depends on G(·)."
  }, {
    "heading": "3.2. Contraction bound",
    "text": "As in Section 3.1, we construct a randomized linear map f : Rn → Rn: (x1, x2, ..., xn) f7−→ (x1/u1, x2/u2, ..., xn/un) where each ui is an i.i.d. random variable drawn from a distribution with CDF 1−e−G(t). Notice that the difference from proving the dilation bound is that we need ui to be independent here. We use the following theorem:\nTheorem 11 (Lemma 3.1 of (Andoni et al., 2017)) Given x ∈ Rn, let ‖ · ‖G be an Orlicz norm induced by function G(·) which has property P , and let f(x) = (x1/u1, x2/u2, ..., xn/un) , where each ui is an i.i.d random variable drawn from a distribution with CDF 1 − e−G(t). Then for α ≥ 1, with probability at least 1− e−α, ‖f(x)‖∞ ≥ ‖x‖G/α. By combining the result with the net argument (see appendix), and Theorems 11, 10, we get the following:\nTheorem 12 G(·) is a function with property P. Given a matrix A ∈ Rn×m with rank d ≤ n, let D ∈ Rn×n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1− e−G(t). Then, with probability at least 0.98, ∀x ∈ Rm,Ω(1/(α′Gd log n))‖Ax‖G ≤ ‖D−1Ax‖∞, where α′G ≥ 1 is a constant which only depends on G(·). Proof sketch: Set ε = 1/poly(nd), we can build an ε-net (see Appendix) N for the column space of A. By taking the union bound over all the net points, we have ∀x ∈ N, ‖D−1x‖∞ is not too small. Due to Theorem 10, we have ∀x in the column space of A, ‖D−1x‖G is not too large. Now, for any unit vector y in the column space of A, we can find the closest point x ∈ N, and ‖x− y‖2 ≤ ε. Since ‖D−1y‖∞ ≥ ‖D−1x‖∞−‖D−1(y−x)‖∞, ‖D−1x‖∞ is not too small, and ‖D−1(y − x)‖∞ is not too large, we can get a lower bound for ‖D−1y‖∞. See appendix for details."
  }, {
    "heading": "3.3. Putting it all together",
    "text": "We now combine Theorem 12, Theorem 10, and Lemma 4, to get the following theorem.\nTheorem 13 Let G(·) be a function which has property P. Given a matrix A ∈ Rn×m with rank d ≤ n, let D ∈ Rn×n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1 − e−G(t). Then, with probability at least 0.98, ∀x ∈ Rm,Ω(1/(α′Gd log n))‖Ax‖G ≤\n‖D−1Ax‖2 ≤ O(α′′Gd2 log n)‖Ax‖G, where α′′G, α′G ≥ 1 are two constants which only depend on G(·).\nThe above theorem successfully embeds ‖ · ‖G into `2 space. We now use `2 subspace embedding to reduce the dimension. The following two theorems provide efficient `2 subspace embeddings. Theorem 14 ( (Clarkson & Woodruff, 2013)) Given matrix A ∈ Rn×m with rank d. Let t = Θ(d2/ε2), S = ΦY ∈ Rt×n, where Y ∈ Rn×n is a diagonal matrix with each diagonal entry independently uniformly chosen to be ±1, Φ ∈ Rt×n is a binary matrix with Φh(i),i = 1,∀i ∈ [n], and remaining entries 0. Here h : [n] → [t] is a random hashing function such that for each i ∈ [n], h(i) is uniformly distributed in [t]. Then with probability at least 0.99, ∀x ∈ Rm, (1 − ε)‖Ax‖22 ≤ ‖SAx‖22 ≤ (1 + ε)‖Ax‖22. Furthermore, SA can be computed in nnz(A) time.\nTheorem 15 (See e.g. (Woodruff, 2014)) Given matrix A ∈ Rn×m with rank d. Let t = Θ(d/ε2), S ∈ Rt×n be a random matrix of i.i.d. standard Gaussian variables scaled by 1/ √ t. Then with probability at least 0.99, ∀x ∈ Rm, (1− ε)‖Ax‖22 ≤ ‖SAx‖22 ≤ (1 + ε)‖Ax‖22. We conclude the full theorem for our subspace embedding: Theorem 16 Let G(·) be a function which has property P. Given a matrix A ∈ Rn×d, d ≤ n, let D ∈ Rn×n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1 − e−G(t). Let Π1 ∈ Rt1×n be a sparse embedding matrix (see Theorem 14) and let Π2 ∈ Rt2×t1 be a random Gaussian matrix (see Theorem 15) where t1 = Ω(d2), t2 = Ω(d). Then, with probability at least 0.9, ∀x ∈ Rd,Ω(1/(α′Gd log n))‖Ax‖G ≤ ‖Π2Π1D−1Ax‖2 ≤ O(α′′Gd\n2 log n)‖Ax‖G, where α′′G, α′G ≥ 1 are two constants which only depend on G(·). Furthermore, Π2Π1D −1A can be computed in nnz(A) + poly(d) time."
  }, {
    "heading": "4. Applications",
    "text": "In this section, we discuss regression problem with Orlicz norm error measure, and low rank approximation problem with `p norm, which is a special case of the Orlicz norms."
  }, {
    "heading": "4.1. Linear regression under Orlicz norm",
    "text": "We first give the definition of regression problem with Orlicz norm. Definition 17 Function G(·) has property P . Given A ∈ Rn×d, b ∈ Rn, the goal is to solve the following minimization problem minx∈Rd ‖Ax− b‖G.\nTheorem 18 Let G(·) have property P . Given A ∈ Rn×d, b ∈ Rn, Algorithm 1 can output a solution x̂ ∈ Rd such that with probability at least 0.8, ‖Ax̂ − b‖G ≤ O(βGd log\n2 n) minx∈Rd ‖Ax − b‖G, where βG is a constant which may depend on G(·). In addition, the running time of Algorithm 1 is nnz(A) + poly(d).\nAlgorithm 1 Linear regression with Orlicz norm ‖ · ‖G 1: Input: A ∈ Rn×d, b ∈ Rn. 2: Output: x̂. 3: Let t1 = Θ(d2), t2 = Θ(d). 4: Let Π1 ∈ Rt1×n be a random sparse embedding matrix,\nΠ2 ∈ Rt2×t1 be a random gaussian matrix, and D ∈ Rn×n be a random diagonal matrix with each diagonal entry independently drawn from distribution whose CDF is 1− e−G(t). (See Theorem 16.)\n5: Compute x̂ = (Π2Π1D−1A)†Π2Π1D−1b.\nProof sketch: Let S = Π2Π1D−1 be the subspace embedding for column space of [A b]. Let x∗ = arg minx∈Rd ‖Ax − b‖G. Due to Theorem 16, ‖Ax̂ − b‖G is bounded by O(d log n)‖S(Ax̂ − b)‖2 ≤ O(d log n)‖S(Ax∗−b)‖2 ≤ O(d log n)‖D−1(Ax∗−b)‖2. Due to Theorem 9, ‖D−1(Ax∗−b)‖2 ≤ O(1)‖D−1(Ax∗− b)‖G ≤ O(log n)‖Ax∗ − b‖G."
  }, {
    "heading": "4.2. Regression with combined loss function",
    "text": "In this section, we want to point out that our technique can be used on solving regression problem with more general cost function. Recall that the goal is to solve the minimization problem minx∈Rd ‖Ax− b‖G. Now, we consider there are multiple goals, and we want to minimize a linear combination of the costs. Now we give the definition of regression problem with combined cost function.\nDefinition 19 Suppose functionG1(·), G2(·), ..., Gk(·) satisfies property P . Given A1 ∈ Rn1×d, A2 ∈ Rn2×d, ..., Ak ∈ Rnk×d, b1 ∈ Rn1 , b2 ∈ Rn2 , ..., bk ∈ Rnk , the goal is to solve the following minimization problem minx∈Rd ∑k i=1 ‖Aix− bi‖Gi .\nThe idea of solving this problem is that we can embed every term into l1 space, and then merge them into one term. By the standard technique, there is a way to embed l2 space to l1 space. We show the embedding as below. For the completeness, we put the proof of this lemma to the appendix.\nLemma 20 Let Q ∈ Rt×n be a random matrix with each entry drawn uniformly from i.i.d. N (0, 1) Gaussian distribution. Let B = ( √ π/2/t) ·Q. If t = Ω( −2n log(n −1)), then with probability at least 0.98, ∀x ∈ Rn, ‖Bx‖1 ∈ ((1− )‖x‖2, (1 + )‖x‖2).\nTheorem 21 Let k > 0 be a constant, and G1(·), G2(·), ..., Gk(·) satisfy property P . Given A1 ∈ Rn1×d, A2 ∈ Rn2×d, ..., Ak ∈ Rnk×d, b1 ∈ Rn1 , b2 ∈ Rn2 , ..., bk ∈ Rnk , Algorithm 2 can output a solution x̂ ∈ Rd such that with probability at least 0.7, ∑k i=1 ‖Aix̂ − bi‖Gi ≤ O(β′Gd log 2 n) minx∈Rd ∑k i=1 ‖Aix− bi‖Gi , where β′G is a constant which may depend on G1(·), G2(·), ..., Gk(·). In addition, the running time of Algorithm 2 is∑k\ni=1 nnz(Ai) + poly(d).\nAlgorithm 2 Linear regression with combined loss functions 1: Input: A1 ∈ Rn1×d, A2 ∈ Rn2×d, ..., Ak ∈ Rnk×d, b1 ∈\nRn1 , b2 ∈ Rn2 , ..., bk ∈ Rnk 2: Output: x̂. 3: Let t1 = Θ(d2), t2 = Θ(d), t3 = Θ(t2 log(t2)). 4: Let Π(1)1 ∈ Rt1×n1 , · · ·Π (k) 1 ∈ Rt1×nk be k random sparse\nembedding matrices, Π(1)2 , · · · ,Π (k) 2 ∈ Rt2×t1 be k random Gaussian matrices, and D(1) ∈ Rn1×n1 , · · · , D(k) ∈ Rnk×nk be k random diagonal matrices where each diagonal entry of D(i) is independently drawn from distribution whose CDF is 1 − e−Gi(t). (See Theorem 16.) Let Q(1), · · · , Q(k) ∈ Rt3×t2 be random matrices with each entry drawn uniformly from i.i.d. N (0, 1) Gaussian distribution. ∀i ∈ [k], let B(i) = ( √ π/2/t3) · Q(i) (see Lemma 20.) Let B ∈ Rkt3×kt2 ,Π2 ∈ Rkt2×kt1 ,Π1 ∈ Rkt1× ∑k j=1 nj , D ∈ R ∑k j=1 nj× ∑k j=1 nj be four block diagonal matrices such that ∀i ∈ [k], the ith block of B,Π2,Π1, D is B(i),Π(i)2 ,Π (i) 1 , D\n(i) respectively. 5: Let A = [A>1 , A>2 , ..., A>k ] >, b = [b>1 , b > 2 , ..., b > k ] > and S =\nBΠ2Π1D −1.\n6: Use classical method of solving l1 regression to get x̂ = arg minx∈Rd ‖S(Ax− b)‖1.\nProof Sketch: Let A = [A>1 , A>2 , ..., A>k ]>, b = [b>1 , b > 2 , ..., b > k ] >, and S = BΠ2Π1D−1 be the subspace embedding for column space of [A b]. Let Si = B (i)Π (i) 2 Π (i) 1 (D\n(i))−1. Notice that ∀x, ‖S(Ax − b)‖1 = ∑k i=1 ‖Si(Aix − bi)‖1. Let x∗ =\narg minx∈Rd ∑k i=1 ‖Aix − bi‖Gi . Due to Theorem 16\nand Lemma 20, ∑k i=1 ‖Aix̂ − bi‖Gi is bounded by\nO(d log n) ∑k i=1 ‖Si(Aix̂ − bi)‖1 = O(d log n)‖S(Ax̂ −\nb)‖1 ≤ O(d log n)‖S(Ax∗ − b)‖1 = ∑k i=1 ‖Si(Aix∗ −\nbi)‖1. Due to Theorem 16, ∑k i=1 ‖Si(Aix∗ − bi)‖1 ≤\nO(log n) ∑k i=1 ‖Aix∗ − bi‖Gi .\nOne application of the above Theorem is to solve the LASSO (Least Absolute Shrinkage Sector Operator) regression. In LASSO regression problem, the goal is to minimize ‖Ax − b‖22 + λ‖x‖1, where λ is a parameter of regularizer. It is easy to show that it is equivalent to minimize ‖Ax − b‖2 + λ′‖x‖1 for some other parameter λ′. When we look at ‖Ax− b‖2 + λ′‖x‖1, we can set A1 = A, b1 = b, A2 = λ\n′I, b2 = 0, G1(·) ≡ x2, G2(·) ≡ x, then we are able to apply Theorem 21 to give a good approximation. The merit of our algorithm is that it is very simple, and can be computed very fast.\n4.3. `p norm low rank approximation using exponential random variables We discuss a special case of Orlicz norm ‖ · ‖G, `p norm, i.e. G(x) ≡ xp for p ∈ [1, 2]. When rank parameter k is ω(log n+log d), by using exponential random variables, we can significantly improve the approximation ratio of input sparsity time algorithms shown by (Song et al., 2017). The\nhigh level ideas combine the results of (Woodruff & Zhang, 2013; Song et al., 2017) and the dilation bound in Section 3. We define the problem in the following. See Appendix for the proof of Theorem 23. Definition 22 Let p ∈ [1, 2]. Given A ∈ Rn×d, n ≥ d, k ∈ Z, 1 ≤ k ≤ min(n, d), the goal is to solve the following minimization problem: minU∈Rn×k,V ∈Rk×d ‖UV −A‖pp.\nAlgorithm 3 `p norm low rank approximation using exponential random variables. 1: Input: A ∈ Rn×d, k ∈ Z,min(n, d) ≥ k ≥ 1. 2: Output: Û ∈ Rn×k, V̂ ∈ Rk×d. 3: Let t1 = Θ(k2), t2 = Θ(k), t3 = Θ(k log k). 4: Let Π1, S1 ∈ Rt1×n be two random sparse embedding matri-\nces, Π2, S2 ∈ Rt2×t1 be two random gaussian matrices, and D1, D2 ∈ Rn×n be two random diagonal matrices with each diagonal entry independently drawn from distribution whose CDF is 1− e−t p\n. (See Theorem 16.) 5: Let T2, R ∈ Rd×t3 be two random matrix, with i.i.d. entries\ndrawn from standard p-stable distribution. 6: Let S = S2S1D−11 , T1 = Π2Π1D −1 2 . 7: Solve X̂, Ŷ = arg minX∈Rt2×k,Y ∈Rk×t3 ‖T1ARXY SAT2− T1AT2‖2F . 8: Û = ARX̂, V̂ = Ŷ SA.\nTheorem 23 Let 1 ≤ p ≤ 2. Given A ∈ Rn×d, n ≥ d, k ∈ Z, 1 ≤ k ≤ min(n, d), with probability at least 2/3, Û , V̂ outputted by Algorithm 3 satisfies: ‖Û V̂ − A‖pp ≤ αminU∈Rn×k,V ∈Rk×d ‖UV − A‖pp, where α = O(min((k log k)4−p log2p+2 n, (k log k)4−2p log4+p n)). In addition, the running time of Algorithm 3 is nnz(A) + (n+ d)poly(k)."
  }, {
    "heading": "5. Experiments",
    "text": "Implementation setups can be seen in appendix."
  }, {
    "heading": "5.1. Orlicz Norm Linear Regression",
    "text": "In this section, we show that our algorithm i) has reasonable and predictable performance under different scenarios and ii) is flexible, general and easy to use. We perform 3 sets of experiments. The first is to compare its performance with the standard `1 and `2 regression under different noise assumptions and dimensions of the regression problem; the second is to compare the performance of Orlicz regression with different G under different noise assumptions; the third is to experiment with Orlicz function G that is different from standard `p and Huber function. We evaluate the performance of our Orlicz norm linear regression algorithm on simulated data. Comparison with `1 and `2 regression We would like to see whether Orlicz norm linear regression leads to expected performance relative to `1 and `2 regression. We choose our Orlicz norm ‖ · ‖G to be induced by the normalized Huber function where the Huber function is defined as\nf(x) = { x2/2 |x| ≤ δ δ · (|x| − δ/2) o.w. . We chose the parameter δ to be 0.75. Intuitively, it is between `1 and `2\nnorm (see Figure 1). In all the simulations, we generate matrix A ∈ Rn×d, ground truth x∗ ∈ Rd, and b to be Ax∗ plus some particular noise. We evaluate the performance of each algorithm by the `2 distances between the output x and the ground truth x∗. In terms of algorithm details, since n, d are not too large in our simulation, we did not apply the `2 subspace embedding to reduce the dimension; we only use reciprocal exponential random diagonal embedding matrix to embed ‖ · ‖G to `2 norm (see Theorem 13)1.\nWe experiment with two n, d combinations, i) n = 200, d = 10 ii) n = 100, d = 75, and 3 noise setting with i) Gaussian noise ii) sparse noise and iii) mixed noise (addition of i) and ii)), altogether 2 × 3 = 6 setting. The detail of data simulation can be seen in appendix. For each experiment we repeat 50 times and compute the mean. The results are shown in Table 2. Orlicz norm regression has better performance than `1 and `2 when the noise is mixed. When the noise is Gaussian or sparse, Orlicz norm regression works better than `1 and `2 respectively. We did not experiment with Huber loss regression, since if we rescale the data and make it small/large in absolute values, the Huber regression will degenerate into respectively `2/`1 regression (see Introduction). See appendix for results on approximation ratio.\nChoice of δ for G as a normalized Huber function We compare the performance of Orlicz norm regression induced by G as normalized Huber loss function with different δ under different noise assumptions. We fix n = 500, d = 30 and generate A and x as in the first set of experiments (see appendix). The noise is a mixture of N(0, 5) Gaussian noise and sparse noise on 1% entries with different scale of uniform noise from [−s‖Ax∗‖2, s‖Ax∗‖2], where scale s is chosen from [0, 0.5, 1, 2]. Under each noise assumptions with different scale s, we compare the performance of Orlicz norm regression induced by G with δ from [0.05, 0.1, 0.2, 0.4, 1, 2]. We repeat each experiment 50 times and report the mean of the `2 distance between output x and the ground truth x∗. The result is shown in Figure 2. When the scale is 0/2, the noise is almost Gaussian/sparse and we expect `2/`1 norm and thus large/small δ to perform the best; anything scale lying in between these extremes will have an optimal δ in between. We observe the expected trend: as s increases, the performance is optimal with smaller δ.\n1We use MATLAB’s linprog to solve `1 regression.\nBeyond Huber function - A General Framework We explore a variant Orlicz function G and evaluate it under a particular setting; the evaluation criteria is the same as the first set of the experiments. The G is of the same form aforementioned, except that it now grows at the order of x1.5 when x is small. We denote it by G`1.5 , which is the normalization of function f , and f is defined as: f(x) ={ x1.5/1.5 x ≤ δ δ0.5 · (|x| − δ/3) o.w. . We generate a 500×30 matrix A and the ground truth vector x∗ in the same way as before, and then addN(0, 5) Gaussian noises and 1 sparse outlier with scale s = 100. We find that the modified G`1.5 under this settings outperforms `1, `2, `1.5, Gδ=0.25, Gδ=0.75 regression by a significant amount where Gδ=0.25, Gδ=0.75 are Orlicz norm induced by regular normalized Huber function with δ = 0.25, 0.75 respectively. The results are shown in Table 3. This experiment demonstrates that our algorithm is i) flexible enough to combine the advantage of norm functions, ii) general for any function that satisfies the nice property, and iii) easy to experiment with different settings, as long as we can compute G and G−1."
  }, {
    "heading": "5.2. `1 low rank matrix approximation",
    "text": "In this section, we evaluate the performance of the `1 low rank matrix approximation algorithm. We mainly compare the `1 norm error of our algorithm with the error of (Song et al., 2017) and standard PCA. Inputs are a matrix A ∈ Rn×d and a rank parameter k; the goal is to output a rank k matrix B such that ‖A − B‖1 is as small as possible. The details of implementations are in the appendix. For each input, we run the algorithm 50 times and pick the best solution.\nDatasets. We first run experiment on synthetic data: we randomly choose two matrices U ∈ R2000×5, V ∈ R5×2000 with each entry drawn uniformly from (0, 1) Then we randomly choose 100 entries of UV , and add random outliers uniformly drawn from (−100, 100) on those entries, thus\nwe can get a matrix A ∈ R2000×2000. In our experiment, ‖A‖1 is about 5.0× 106. Then, we run experiments on real datasets diabetes and glass in UCI repository(Bache & Lichman, 2013). The data matrix of diabetes has size 768× 8, and the data matrix of glass has size 214× 9. For each data matrix, we randomly add outliers on 1% number of entries.\nFor each dataset, we evaluate the ‖A−B‖1. The result for the experiment on synthetic data is shown in Table 4, and the results for diabetes and glass are shown in Figure 3. The running time of algorithm in (Song et al., 2017) on diabetes and on glass are 5.69 and 11.97 seconds respectively, with ours being 3.18 and 3.74 seconds respectively. We also find that our algorithm consistently outperforms the other two alternatives (note that the y-coordinates are at log scale with base 10)."
  }, {
    "heading": "6. Conclusion and Future Work",
    "text": "In this paper we presented an efficient subspace embedding algorithm for orlicz norm and demonstrated its usefulness in regression/low rank approximation problem on synthetic and real datasets. Nevertheless, O(d log2 n) is still a large theoretical approximation factor, and hence it is worth i) investigating whether the theoretical approximation ratio can be smaller if input are under some statistical distribution ii) calculating the actual approximation ratio with ground truth obtained by some slower but more accurate optimization algorithm. It is also worth examining whether our exponential embedding sketching method preserves the statistical properties of the regression error, since we assumed a different noise distribution from Gaussian/double-exponential as a starting point (Raskutti & Mahoney, 2014; Lopes et al., 2018)."
  }, {
    "heading": "Acknowledgements",
    "text": "Research supported in part by Simons Foundation (#491119 to Alexandr Andoni), NSF (CCF-1617955, CCF- 1740833), and Google Research Award."
  }],
  "year": 2018,
  "references": [{
    "title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
    "authors": ["D. Achlioptas"],
    "venue": "Journal of computer and System Sciences,",
    "year": 2003
  }, {
    "title": "The fast johnson–lindenstrauss transform and approximate nearest neighbors",
    "authors": ["N. Ailon", "B. Chazelle"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Approximate near neighbors for general symmetric norms",
    "authors": ["A. Andoni", "A. Nikolov", "I. Razenshteyn", "E. Waingarten"],
    "venue": "In Proceedings of the Forty-ninth annual ACM symposium on Theory of computing,",
    "year": 2017
  }, {
    "title": "Uci machine learning repository",
    "authors": ["K. Bache", "M. Lichman"],
    "venue": "URL http://archive. ics. uci. edu/ml,",
    "year": 2013
  }, {
    "title": "Robust regression via hard thresholding",
    "authors": ["K. Bhatia", "P. Jain", "P. Kar"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "Zero-one frequency laws",
    "authors": ["V. Braverman", "R. Ostrovsky"],
    "venue": "In Proceedings of the forty-second ACM symposium on Theory of computing,",
    "year": 2010
  }, {
    "title": "Numerical linear algebra in the streaming model",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
    "year": 2009
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
    "year": 2013
  }, {
    "title": "Sketching for mestimators: A unified approach to robust regression",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2015
  }, {
    "title": "Sampling algorithms and coresets for `p regression",
    "authors": ["A. Dasgupta", "P. Drineas", "B. Harb", "R. Kumar", "M.W. Mahoney"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Revisiting a 90-year-old debate: the advantages of the mean deviation",
    "authors": ["S. Gorard"],
    "venue": "British Journal of Educational Studies,",
    "year": 2005
  }, {
    "title": "Robust estimation of a location parameter",
    "authors": ["Huber", "P. J"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1964
  }, {
    "title": "Some combinatorial and probabilistic inequalities and their application to banach space theory",
    "authors": ["S. Kwapien", "C. Schuett"],
    "venue": "Studia Mathematica,",
    "year": 1985
  }, {
    "title": "Some combinatorial and probabilistic inequalities and their application to banach space theory ii",
    "authors": ["S. Kwapie", "C. Schtt"],
    "venue": "Studia Mathematica,",
    "year": 1989
  }, {
    "title": "Multivariate regression with calibration",
    "authors": ["H. Liu", "L. Wang", "T. Zhao"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2014
  }, {
    "title": "Error estimation for randomized least-squares algorithms via the bootstrap",
    "authors": ["M.E. Lopes", "S. Wang", "M.W. Mahoney"],
    "venue": "arXiv preprint arXiv:1803.08021,",
    "year": 2018
  }, {
    "title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression",
    "authors": ["X. Meng", "M.W. Mahoney"],
    "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
    "year": 2013
  }, {
    "title": "Faster numerical linear algebra algorithms via sparser subspace embeddings",
    "authors": ["J. Nelson", "Nguyên", "H.L. Osnap"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2013
  }, {
    "title": "A statistical perspective on randomized sketching for ordinary least-squares",
    "authors": ["G. Raskutti", "M. Mahoney"],
    "venue": "arXiv preprint arXiv:1406.5986,",
    "year": 2014
  }, {
    "title": "On the embedding of 2-concave orlicz spaces into l1",
    "authors": ["C. Schtt"],
    "venue": "Studia Mathematica,",
    "year": 1995
  }, {
    "title": "Subspace embeddings for the `1-norm with applications",
    "authors": ["C. Sohler", "D.P. Woodruff"],
    "venue": "In Proceedings of the fortythird annual ACM symposium on Theory of computing,",
    "year": 2011
  }, {
    "title": "Low rank approximation with entrywise `1-norm error",
    "authors": ["Z. Song", "D.P. Woodruff", "P. Zhong"],
    "venue": "In Proceedings of the 49th Annual Symposium on the Theory of Computing. ACM, arXiv preprint arXiv:1611.00898,",
    "year": 2017
  }, {
    "title": "Tight bounds for `p oblivious subspace embeddings",
    "authors": ["R. Wang", "D.P. Woodruff"],
    "venue": "arXiv preprint arXiv:1801.04414,",
    "year": 2018
  }, {
    "title": "Subspace embeddings and `p regression using exponential random variables",
    "authors": ["D. Woodruff", "Q. Zhang"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2013
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["D.P. Woodruff"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Parameter estimation techniques: A tutorial with application to conic fitting",
    "authors": ["Z. Zhang"],
    "venue": "Image and vision Computing,",
    "year": 1997
  }, {
    "title": "Mixed linear regression with multiple components",
    "authors": ["K. Zhong", "P. Jain", "I.S. Dhillon"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2016
  }],
  "id": "SP:79df70814de320d27cc4ad0572da0578533eadca",
  "authors": [{
    "name": "Alexandr Andoni",
    "affiliations": []
  }, {
    "name": "Chengyu Lin",
    "affiliations": []
  }, {
    "name": "Ying Sheng",
    "affiliations": []
  }, {
    "name": "Peilin Zhong",
    "affiliations": []
  }, {
    "name": "Ruiqi Zhong",
    "affiliations": []
  }],
  "abstractText": "We consider a generalization of the classic linear regression problem to the case when the loss is an Orlicz norm. An Orlicz norm is parameterized by a non-negative convex function G : R+ → R+ with G(0) = 0: the Orlicz norm of a vector x ∈ R is defined as ‖x‖G = inf {α > 0 | ∑n i=1G(|xi|/α) ≤ 1} . We consider the cases where the function G(·) grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given matrix A ∈ Rn×d with Orlicz norm into a lower dimensional space with `2 norm. Specifically, we show how to efficiently find an embedding matrix S ∈ Rm×n,m < n such that ∀x ∈ R,Ω(1/(d log n)) · ‖Ax‖G ≤ ‖SAx‖2 ≤ O(d log n) · ‖Ax‖G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem minx∈Rd ‖Ax−b‖G, up to aO(d log n) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the `p low rank matrix approximation problem for 1 ≤ p < 2.",
  "title": "Subspace Embedding and Linear Regression with Orlicz Norm"
}