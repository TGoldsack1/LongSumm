{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep neural networks have achieved great success in classification tasks; in particular, residual network (ResNet) (He et al., 2016) and its variants such as wide-ResNet (Zagoruyko & Komodakis, 2016), ResNeXt (Xie et al., 2017), and DenseNet (Huang et al., 2017b) have become the most prominent architectures in computer vision. Thus, to reveal a factor in their success, several studies have explored the behavior of ResNets and some promising perceptions have been advocated. Concerning the behavior of ResNets, there are mainly two types of thoughts. One is the ensemble views, which were pointed out in Veit et al. (2016); Littwin\n1Graduate School of Information Science and Technology, The University of Tokyo 2Center for Advanced Intelligence Project, RIKEN. Correspondence to: Atsushi Nitanda <atsushi nitanda@mist.i.u-tokyo.ac.jp>, Taiji Suzuki <taiji@mist.i.u-tokyo.ac.jp>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n& Wolf (2016). They presented that ResNets are ensemble of shallower models using an unraveled view of ResNets. Moreover, Veit et al. (2016) enhanced their claim by showing that dropping or shuffling residual blocks does not affect the performance of ResNets experimentally. The other is the optimization or ordinary differential equation views. In Jastrzebski et al. (2017), it was observed experimentally that ResNet layers iteratively move data representations along the negative gradient of the loss function with respect to hidden representations. Moreover, several studies (Weinan, 2017; Haber et al., 2017; Chang et al., 2017a;b; Lu et al., 2017) have pointed out that ResNet layers can be regarded as discretization steps of ordinary differential equations. Since optimization methods are constructed based on the discretization of gradient flows, these studies are closely related to each other.\nOn the other hand, gradient boosting (Mason et al., 1999; Friedman, 2001) is known to be a state-of-the-art method in data analysis; in particular, XGBoost (Chen & Guestrin, 2016) and LightGBM (Ke et al., 2017) are notable because of their superior performance. Although ResNets and gradient boosting are prominent methods in different domains, we notice an interesting similarity by recalling that gradient boosting is an ensemble method based on iterative refinement by functional gradients for optimizing predictors. However, there is a key difference between ResNets and gradient boosting methods. While gradient boosting directly updates the predictor, ResNets iteratively optimize the feature extraction by stacking ResNet layers rather than the predictor, according to the existing work.\nIn this paper, leveraging this observation, we propose a new gradient boosting method called ResFGB for classification tasks based on ResNet perception, that is, the feature extraction gradually grows by functional gradient methods in the space of feature extractions and the resulting predictor naturally forms a ResNet-type architecture. The expected benefit of the proposed method over usual gradient boosting methods is that functional gradients with respect to feature extraction can learn a deep model rather than a shallow model like usual gradient boosting. As a result, more efficient optimization is expected.\nIn the theoretical analysis of the proposed method, we first formalize the gradient boosting perspective of ResNet math-\nematically using the notion of functional gradients in the space of feature extractions. That is, we explain that optimization in that space is achieved by stacking ResNet layers. We next show a good consistency property of the functional gradient, which motivates us to find feature extraction with small functional gradient norms for estimating the correct label of data. This fact is very helpful from the optimization perspective because minimizing the gradient norm is much easier than minimizing the objective function without strong convexity. Moreover, we show the margin maximization property of the proposed method and derive the margin bound by utilizing this formalization and the standard complexity analysis techniques developed in Koltchinskii & Panchenko (2002); Bartlett & Mendelson (2002), which guarantee the generalization ability of the method. As for another generalization guarantee, we also provide convergence analysis of the sample-splitting variant of the method for the expected risk minimization. We finally show superior performance, empirically, of the proposed method over state-of-the-art methods including LightGBM.\nRelated work. Several studies have attempted to grow neural networks sequentially based on the boosting theory. Bengio et al. (2006) introduced convex neural networks consisting of a single hidden layer, and proposed a gradient boosting-based method in which linear classifiers are incrementally added with their weights. However, the expressive power of the convex neural network is somewhat limited because the method cannot learn deep architectures. Moghimi et al. (2016) proposed boosted convolutional neural networks and showed superior empirical performance on fine-grained classification tasks, where convolutional neural networks are iteratively added, while our method constructs a deeper network by iteratively adding layers. Cortes et al. (2017) proposed AdaNet to adaptively learn both the structure of the network and its weight, and provided data-dependent generalization guarantees for an adaptively learned network; however, the learning strategy quite differs from our method and the convergence rate is unclear. The most related work is BoostResNet (Huang et al., 2017a), which constructs ResNet iteratively like our method; however, this method is based on an different theory rather than functional gradient boosting with a constant learning rate. This distinction makes the different optimizationgeneralization tradeoff. Indeed, our method exhibits a tradeoff with respect to the learning rate, which recalls perception of usual functional gradient boosting methods, namely a smaller learning rate leads to a good generalization performance."
  }, {
    "heading": "2. Preliminary",
    "text": "In this section, we provide several notations and describe a problem setting of the classification. An important notion in\nthis paper is the functional gradient, which is also introduced in this section.\nProblem setting. Let X = Rd and Y be a feature space and a finite label set of cardinal c, respectively. We denote by ν a true Borel probability measure on X × Y and by νn an empirical probability measure of samples (xi, yi)\nn i=1 independently drawn from ν, i.e., dνn(X,Y ) =∑n\ni=1 δ(xi,yi)(X,Y )dXdY/n, where δ denotes the Dirac delta function. We denote by νX the marginal distribution on X and by ν(·|X) the conditional distribution on Y . We also denote empirical variants of these distributions by νn,X and νn(·|X). In general, for a probability measure µ, we denote by Eµ the expectation with respect to a random variable according to µ, by L2(µ) the space of square-integrable functions with respect to µ, and by Lq2(µ) the product space of L2(µ) equipped with 〈·, ·〉Lq2(µ)-inner product: for ∀ξ,∀ζ ∈ Lq2(µ),\n〈ξ, ζ〉Lq2(µ) def = Eµ[ξ(X)>ζ(X)] = Eµ  q∑ j=1 ξj(X)ζj(X)  . We also use the following norm: for ∀p ∈ (0, 2] and ∀ξ ∈ Lq2(µ), ‖ξ‖ p Lqp(µ) def = Eµ[‖ξ(X)‖p2] =\nEµ [ ( ∑q j=1 ξ 2 j (X)) p/2 ] .\nThe ultimate goal in classification problems is to find a predictor f ∈ Lc2(νX) such that arg maxy∈Y fy(x) correctly classifies its label. The quality of the predictor is measured by a loss function l(ζ, y) ≥ 0. A typical choice of l in multiclass classification problems is l(ζ, y) = − log(exp(ζy)/ ∑ y∈Y exp(ζy)), which is used for the multiclass logistic regression. The goal of classification is achieved by solving the expected risk minimization problem:\nmin f∈Lc2(νX)\n{ L(f) def= Eν [l(f(X), Y )] } . (1)\nHowever, the true probability measure ν is unknown, so we approximate L using the observed data probability measure νn and solve the empirical risk minimization problems:\nmin f∈Lc2(νX)\n{ Ln(f) def = Eνn [l(f(X), Y )] } . (2)\nIn general, some regularization is needed for the problem (2) to guarantee generalization. In this paper, we rely on early stopping (Zhang et al., 2005) and some restriction on optimization methods for solving the problem.\nSimilar to neural networks, we split the predictor f into the feature extraction and linear predictor, that is, f(x) = w>φ(x), where w ∈ Rd×c is a weight for the last layer and φ ∈ Ld2(νX) is a feature extraction from X to X . For\nsimplicity, we also denote l(z, y, w) = l(w>z, y). Usually, φ is parameterized by a neural network and optimized using the stochastic gradient method. In this paper, we propose a way to optimize φ in Ld2(νX) via the following problem:\nmin w∈Rd×c φ∈Ld2(νX)\n{ R(φ,w) def= Eν [l(φ(X), Y, w)] + λ\n2 ‖w‖22\n} (3)\nwhere λ > 0 is a regularization parameter to stabilize the optimization procedure and ‖ · ‖2 for w is a Euclidean norm. When we focus on the problem with respect to φ, we use the notation R(φ) def= minw∈Rd×c R(φ,w). We also denote byRn(φ,w) andRn(φ) empirical variants ofR(φ,w) and R(φ), respectively, which are defined by replacing Eν by Eνn . In this paper, we denote by ∂ the partial derivative and its subscript indicates the direction.\nFunctional gradient. The key notion used for solving the problem is the functional gradient in function spaces. Since they are taken in some function spaces, we first introduce Fréchet differential in general Hilbert spaces.\nDefinition 1. LetH be a Hilbert space and h be a function onH. For ξ ∈ H, we call that h is Fréchet differentiable at ξ inH when there exists an element ∇ξh(ξ) ∈ H such that\nh(ζ) = h(ξ) + 〈∇ξh(ξ), ζ − ξ〉H + o(‖ξ − ζ‖H).\nMoreover, for simplicity, we call∇ξh(ξ) Fréchet differential or functional gradient.\nWe here make an assumption to guarantee Fréchet differentiability of R,Rn, which is valid for multiclass logistic loss: l(z, y, w) = − log(exp(w>y z)/ ∑ y∈Y exp(w > y z)). Assumption 1. The loss function l(z, y, w) : X × Y × Rd×c → R is a non-negative C2-function with respect to z and w, convex with respect to w, and satisfies the following smoothness: There exists a real number Ar > 0 depending on r > 0 such that\n‖∂2z l(z, y, w)‖ ≤ Ar for z ∈ X , y ∈ Y, w ∈ Br(0),\nwhere ‖ · ‖ used for a matrix ∂2z l is the spectral norm and Br(0) ⊂ Rd×c is a closed ball of center 0 and radius r.\nFor φ ∈ Ld2(νX), we set wφ def = arg minw∈Rd×c R(φ,w) and wn,φ def = arg minw∈Rd×c Rn(φ,w). Moreover, we define the following notations:\n∇φR(φ)(x) def = Eν(Y |x)[∂zl(φ(x), Y, wφ)],\n∇φRn(φ)(x) def =\n{ ∂zl(φ(xi), yi, wn,φ) (x = xi),\n0 (otherwise).\nWe also similarly define functional gradients ∂φR(φ,w) and ∂φRn(φ,w) for fixed w by replacing wφ, wn,φ by w. It follows that\n∇φR(φ) = ∂φR(φ,wφ),∇φRn(φ) = ∂φRn(φ,wn,φ).\nThe next proposition means that the above maps are functional gradients in Ld2(νX) and L d 2(νn,X). We set l0 = maxy∈Y l(0, y). Proposition 1. Let Assumption 1 hold. Then, for ∀φ, ψ ∈ Ld2(νX), it follows that\nR(ψ) = R(φ) + 〈∇φR(φ), ψ − φ〉Ld2(νX) +Hφ(ψ), (4)\nwhere Hφ(ψ) ≤ Acλ 2 ‖φ− ψ‖ 2 Ld2(νX)\n(cλ = √\n2l0/λ). Furthermore, the corresponding statements hold for R(·, w) (∀w ∈ Rd) by replacingR(·) byR(·, w) and for empirical variants by replacing νX by νn,X .\nWe can also show differentiability of L(f) and Ln(f). Their functional gradients have the form ∇fL(f)(x) = Eν(Y |x)[∂ζ l(f(x), Y )] and∇fLn(f)(xi) = ∂ζ l(f(xi), yi). In this paper, we derive functional gradient methods using ∇φRn(φ) rather than ∇fLn(f) like usual gradient boosting (Mason et al., 1999; Friedman, 2001), and provide convergence analyses for problems (1) and (2). However, we cannot apply ∇φRn(φ) or ∂φRn(φ,w) directly to the expected risk minimization problem because these functional gradients are zero outside the training data. Thus, we need a smoothing technique to propagate these to unseen data. The expected benefit of functional gradient methods using ∇φRn(φ) over usual gradient boosting is that the former can learn a deep model that is known to have high representational power. Before providing a concrete algorithm description, we first explain the basic property of functional gradients and functional gradient methods."
  }, {
    "heading": "3. Basic Property of Functional Gradient",
    "text": "In this section, we explain the motivation for using functional gradients for solving classification problems. We first show the consistency of functional gradient norms, namely predicted probabilities by predictors with small norms converge to empirical/expected conditional probabilities. We next explain the superior performance of functional gradient methods intuitively, which motivate us to use it for finding predictors with small norms. Moreover, we explain that the optimization procedure of functional gradient methods can be realized by stacking ResNet layers iteratively on the top of feature extractions.\nConsistency of functional gradient norm. We here provide upper bounds on the gaps between true empirical/expected conditional probabilities and predicted probabilities.\nProposition 2. Let l(ζ, y) be the loss function for the multiclass logistic regression. Then,\n‖∇fL(f)‖Lc1(νX) ≥ 1√ c ∑ y∈Y ‖ν(y|·)− pf (y|·)‖L1(νX),\n‖∇fLn(f)‖Lc1(νn,X) ≥ 1√ c ∑ y∈Y ‖νn(y|·)− pf (y|·)‖L1(νn,X),\nwhere we denote by pf (y|x) the softmax function defined by the predictor f , i.e., exp(fy(·))/ ∑ y∈Y exp(fy(·)).\nMany studies (Zhang, 2004; Steinwart, 2005; Bartlett et al., 2006) have exploited the consistency of convex loss functions for classification problems in terms of the classification error or conditional probability. Basically, these studies used the excess empirical/expected risk to estimate the excess classification error or the gap between the true conditional probability and the predicted probability. On the other hand, Proposition 2 argues that functional gradient norms give sufficient bounds on such gaps. This fact is very helpful from the optimization perspective for non-strongly convex smooth problems since the excess risk always bounds the functional gradient norm by the reasonable order, but the inverse relationship does not always hold. This means that finding a predictor with a small functional gradient is much easier than finding a small excess risk.\nNote that the latter inequality in Proposition 2 provides the lower bound on empirical classification accuracy, which is confirmed by Markov inequality as follows.\nPνn [1− pf (Y |X) ≥ 1/2] ≤ 2Eνn [1− pf (Y |X)] ≤ 2 √ c‖∇fLn(f)‖Lc1(νn,X).\nGenerally, we can derive a bound on the empirical margin distribution (Koltchinskii & Panchenko, 2002) by using the functional gradient norm in a similar way, and can obtain a generalization bound using it, as shown later.\nPowerful optimization ability and connection to residual networks. In the above discussion, we have seen that the classification problem can be reinterpreted as finding a predictor with small functional gradient norms, which may lead to reasonable convergence compared to minimizing the excess risk. However, finding such a good predictor is still difficult because a function space is quite comprehensive, and thus, a superior optimization method is required to achieve this goal. We explain that functional gradient methods exhibit an excellent performance by using the simplified problem. Namely, we apply the functional gradient method to the following problem:\nmin φ∈Ld2(νX)\n{ R′(φ) def= EνX [r(φ(X))] } , (5)\nwhere r is a sufficiently smooth function. Note that the main problem (3) is not interpreted as this simplified problem correctly, but is useful in explaining a property and an advantage of the method and leads to a deeper understanding.\nIf R′ is Fréchet differentiable, the functional gradient is represented as∇φR′(φ)(·) = ∇zr(φ(·)), where z indicates the input to r. Therefore, the negative functional gradient indicates the direction of decreasing the objective r at each point φ(x). An iteration of the functional gradient method with a learning rate η is described as\nφt+1 ← φt − η∇zr ◦ φt = (id− η∇zr) ◦ φt.\nWe can immediately notice that this iterate makes φt one level deeper by stacking a residual network-type layer id − η∇zr (He et al., 2016), and data representation is refined through this layer. Starting from a simple feature extraction φ0 and running the functional gradient method for T -iterations, we finally obtain a residual network:\nφT = (id− η∇zr) ◦ · · · ◦ (id− η∇zr) ◦ φ0.\nTherefore, feature extraction φ gradually grows by using the functional gradient method to optimizeR′. Indeed, we can show the convergence of φT to a stationary point of R′ in Ld2(νX) under smoothness and boundedness assumptions by analogy with a finite-dimensional optimization method. This is a huge advantage of the functional gradient method because stationary points in Ld2(νX) are potentially significant better than those of finite-dimensional spaces. Note that this formulation explains the optimization view (Jastrzebski et al., 2017) of ResNet mathematically.\nWe now briefly explain how powerful the functional gradient method is compared to the gradient method in a finitedimensional space, for optimizingR′. Let us consider any parameterization of φt ∈ Ld2(νX). That is, we assume that φt is contained in a family of parameterized feature extractionsM = {gθ : X → X | θ ∈ Θ ⊂ Rm} ⊂ Ld2(νX), i.e., ∃θ′ ∈ Θ s.t. φt = gθ′ . Typically, the familyM is given by neural networks. IfR′(gθ) is differentiable at θ′, we get ∇θR′(gθ)|θ=θ′ = 〈∇φR′(φt),∇θg|θ=θ′〉Ld2(νX) according to the chain rule of derivatives. Note that ∇φR′(φt) dominates the norm of gradients. Namely, if φt is a stationary point inLd2(νX), φt is also a stationary point inM and there is no room for improvement using gradient-based methods. This result holds for any familyM, but the inverse relation does not always hold. This means that gradient-based methods may fail to optimizeR′ in the function space, while the functional gradient method exceeds such a limit by making a feature extraction φt deeper. For detailed descriptions and related work in this line, we refer to Ambrosio et al. (2008); Daneri & Savaré (2010); Sonoda & Murata (2017); Nitanda & Suzuki (2017; 2018)."
  }, {
    "heading": "4. Algorithm Description",
    "text": "In this section, we provide concrete description of the proposed method. Let φt ∈ Ld2(νX) and wt denote t-th iterates of φ and w. As mentioned above, since functional gradients ∂φRn(φt, wt+1) for the empirical risk vanish outside the training data, we need a smoothing technique to propagate these to unseen data. Hence, we use the convolution Tkt,n∂φRn(φt, wt+1) of the functional gradient by using an adaptively chosen kernel function kt on X . The convolution is applied element-wise as follows.\nTkt,n∂φRn(φt, wt+1) def = Eνn,X [∂φRn(φt, wt+1)(X)kt(X, ·)]\n= 1\nn n∑ i=1 ∂zl(φt(xi), yi, wt+1)kt(xi, ·).\nNamely, this quantity is a weighted sum of ∂φRn(φt, wt+1)(xi) by kt(xi, ·), which we also call a functional gradient. In particular, we restrict the form of a kernel kt to the inner-product of a non-linear feature embedding to a finite-dimensional space by ιt : Rd → RD, that is, kt(x, x′) = ιt(φt(x))>ιt(φt(x)). The requirements on the choice of ιt to guarantee the convergence are the uniform boundedness and sufficiently preserving the magnitude of the functional gradient ∂φRn(φt, wt+1). Let F be a given restricted class of bounded embeddings. We pick up ιt from this class F by approximately solving the following problem to acquire magnitude preservation:\nmax ιt∈F\n‖Tkt,n∂φRn(φt, wt+1)‖2kt . (6)\nwhere we define ‖Tkt,nξ‖2kt = 〈ξ, Tkt,nξ〉Ld2(νn,X) for a vector function ξ. Detailed conditions on ιt and an alternative problem to guarantee the convergence will be discussed later. Note that due to the restriction on the form of kt, the computation of the functional gradient is compressed to the matrix-vector product. Namely,\nAt def =\n1\nn n∑ i=1 ∂φRn(φt, wt+1)(xi)ιt(φt(xi))>,\nTkt,n∂φRn(φt, wt+1) = Atιt(φt(·)).\nTherefore, the functional gradient method φt+1 ← φt − ηTkt,n∂φRn(φt, wt+1) can be recognized as the procedure of successively stacking layers id − ηAtιt(φt(·)) (t ∈ {0, . . . , T − 1}) and obtaining a residual network. The entire algorithm is described in Algorithm 1. Note that because a loss function l is chosen typically to be convex with respect to w, a procedure in Algorithm 1 to obtain wn,φt is easily achieved by running an efficient method for convex minimization problems. The notation T0 is the stopping time of iterates with respect to w. That is, functional gradients ∂φRn(φt, wt+1) are computed at wt+1 = wn,φt\nand correspond to ∇φRn(φt) when t < T0 and computed at an older point of w when t ≥ T0, rather than∇φRn(φt).\nAlgorithm 1 ResFGB Input: S = (xi, yi)ni=1, initial points φ0, w0, the number of iterations T of φ, the number of iterations T0 of w, embedding class F , and learning rate η for t = 0 to T − 1 do\nif t < T0 then wt+1 ← wn,φt = arg minw∈Rd×c Rn(φt, w) else wt+1 ← wt end if Get ιt by approximately solving (6) on S At ← 1n ∑n i=1 ∂zl(φt(xi), yi, wt+1)ιt(φt(xi)) >\nφt+1 ← φt − ηAtιt(φt(·)) end for Return φT−1 and wT\nChoice of embedding. We here provide policies for the choice of ιt. A sufficient condition for ιt to achieve good convergence is to maintain the functional gradient norm, which is summarized below. Assumption 2. For positive values γ, , p ≤ 2, q, and K, a function kt(x, x′) = ιt(φt(x))>ιt(φt(x)) satisfies ‖ιt(x)‖2 ≤ √ K onX , and γ‖∂φRn(φt, wt+1)‖qLdp(νn,X)− γ ≤ ‖Tkt,n∂φRn(φt, wt+1)‖2kt .\nThis assumption is a counterpart of that imposed in Mason et al. (1999). The existence of ιt, not necessarily included in F , satisfying this assumption is confirmed as follows. We here assume that φt is a bijection that is a realistic assumption when learning rates are sufficiently small because of the inverse mapping theorem. Then, since νn(·|X) = νn(·|φt(X)), functional gradients ∂φRn(φt, wt+1)(x) become the map of φt(x), so we can choose ιt such that\nιt(φt(·)) = ∂φRn(φt, wt+1)(·)/‖∂φRn(φt, wt+1)(·)‖2.\nBy simple computation, we find that kt(x, x′) ≤ 1 and ‖Tkt,n∂φRn(φt, wt+1)‖2kt are lower-bounded by 1 d‖∂φRn(φt, wt+1)‖ 2 Ld1(νn,X)\n. A detailed derivation is provided in Appendix. Thus, Assumption 2 may be satisfied if an embedding class F is sufficiently large, but we note that too large F leads to overfitting. Therefore, one way of choosing ιt is to approximate ∂φRn(φt, wt+1)(·)/‖∂φRn(φt, wt+1)(·)‖2 rather than maximizing (6) directly, and indeed, this procedure has been adopted in experiments."
  }, {
    "heading": "5. Convergence Analysis",
    "text": "In this section, we provide a convergence analysis for the proposed method. All proofs are included in Appendix. For\nthe empirical risk minimization problem, we first show the global convergence rate, which also provides the generalization bound by combining the standard complexity analyses. Next, for the expected risk minimization problem, we describe how the size of F and the learning rate control the tradeoff between optimization speed and generalization by using the sample-splitting variant of Algorithm 1, whose detailed description will be provided later.\nEmpirical risk minimization. Using Proposition 1, Assumption 2, and an additional assumption on wt, we can show the global convergence of Algorithm 1. The following inequality shows how functional gradients decrease the objective function, which is a direct consequence of Proposition 1. When η ≤ 1AcλK , we have\nRn(φt+1, wt+2) ≤ Rn(φt, wt+1)\n− η 2 ‖Tkt,n∂φRn(φt, wt+1)‖2kt .\nTherefore, Algorithm 1 provides a certain decrease in the objective function; moreover, we can conclude a stronger result. Theorem 1. Let Assumptions 1 and 2 hold. Consider running Algorithm 1 with a learning rate η ≤ 1AcλK . If p ≥ 1 and the minimum eigenvalues of (wt>wt)T0t=0 have a uniform lower bound σ2 > 0, then we get\nmin t∈[T ]\n‖∇fLn(ft)‖Lc1(νn,X) ≤ ( 2Rn(φ0, w1) ηγσqT + σq ) 1 q\n(7) where we denote ft = w>t+1φt and [T ] = {0, . . . , T − 1}.\nRemark. (i) This theorem states the convergence of the minimum functional gradient norm obtained by running Algorithm 1, but returning the last iterate or the best iterate on the validation set is practically better. (ii) Although a larger value of T0 may affect the bound in Theorem 1 because of dependency on the minimum eigenvalue of (w>t wt) T0 t=0, optimizing w at each iteration facilitates the convergence speed empirically.\nGeneralization bound. Here, we derive a generalization bound using the margin bound developed by Koltchinskii & Panchenko (2002), which is composed of the sum of the empirical margin distribution and Rademacher complexity of predictors. The margin and the empirical margin distribution for multiclass classification are defined as mf (x, y) def = fy(x) − maxy′ 6=y fy′(x) and Pνn [mf (x, y) ≤ δ] (δ > 0), respectively. When l is the multiclass logistic loss, using Markov inequality and Proposition 2, we can obtain an upper bound on the margin distribution:\nPνn [mf (x, y) ≤ δ] ≤ ( 1 + 1\nexp(−δ)\n)√ c‖∇fLn(f)‖Lc1(νn,X).\nSince the convergence of functional gradient norms has been shown in Theorem 1, the resulting problem to derive a generalization bound is to estimate Rademacher complexity, which can be achieved using standard techniques developed by Bartlett & Mendelson (2002); Koltchinskii & Panchenko (2002). Thus, we specify here the architecture of predictors. In the theoretical analysis, we suppose F is the set of shallow neural networks Bσ(Cx) for simplicity, where B,C are weight matrices and σ is an element-wise activation function. Then, the t-th layer is represented as\nφt+1(x) = φt(x)− ηDtσ(Ctφt(x)),\nwhere Dt = AtBt, and a predictor is fT−1(x) = w>T φT−1(x). Bounding norms of these weights by controlling the size of F and λ, we can restrict the Rademacher complexity of a set of predictors and obtain a generalization bound. We denote by GT−1 the set of predictors under constraints on weight matrices where L1-norms of each row of w>T , Ct, and Dt are bounded by Λw,Λ, and Λ ′. Theorem 2. Let l be the multiclass logistic regression loss. Fix δ > 0. Suppose σ is Lσ-Lipschitz continuous and ‖x‖2 ≤ Λ∞ on X . Then, for ∀ρ > 0, with probability at least 1−ρ over the random choice of S from νn, we have ∀f ∈ GT−1,\nPν [mf (X,Y ) ≤ 0] ≤ 2c3Λ∞Λw δ √ n (1 + ηΛΛ′Lσ) T−1\n+\n√ log(1/ρ)\n2n +\n( 1 +\n1\nexp(−δ)\n)√ c‖∇fLn(f)‖Lc1(νn,X).\nCombining Theorems 1 and 2, we observe that the learning rate η, the number of iterations T , and the size of F have an impact on the optimization-generalization tradeoff, that is, larger values of these quantities facilitate the convergence on training data while the generalization bound becomes gradually loose. Especially, this bound has an exponential dependence on depth T , which is known to be unavoidable (Neyshabur et al., 2015) in the worst case for some networks with L1 or the group norm constraints, but this bound is useful when an initial objective is small and required T is also small sufficiently.\nWe note another type of bound can be derived by utilizing VC-dimension or pseudo-dimension (Vapnik & Chervonenkis, 1971). When the activation function is piece-wise linear, such as Relu function σ(x) = max{0, x}, reasonable bounds on these quantities are given by Bartlett et al. (1998; 2017). Thus, for that case, we can obtain better bounds with respect to T by combining our analysis and the VC bound, but we omit the precise description for simplicity. We next show the other generalization guarantee from the optimization perspective by using the modified algorithm, which may slow down the optimization speed but alleviates the exponential dependence on T in the generalization bound.\nSample-splitting technique. To remedy the exponential dependence on T of the generalization bound, we introduce the sample-splitting technique which has been used recently to provide statistical guarantee of expectation-maximization algorithms (Balakrishnan et al., 2017; Wang et al., 2015). That is, instead of Algorithm 1, we analyze its samplesplitting variant. Although Algorithm 1 exhibits good empirical performance, the sample-splitting variant is useful for analyzing the behavior of the expected risk. In this variant, the entire dataset is split into T pieces, where T is the number of iterations, and each iteration uses a fresh batch of samples. The key benefit of the sample-splitting method is that it allows us to use concentration inequalities independently at each iterate φt rather than using the complexity measure of the entire model. As a result, sample-splitting alleviates the exponential dependence on T presented in Theorem 2. We now present the details in Algorithm 2. For simplicity, we assume T0 = 0, namely the weight vector wt is fixed to the initial weight w0.\nAlgorithm 2 Sample-splitting ResFGB Input: S = (xi, yi)ni=1, initial points φ0, w0, the number of iterations T , embedding class F , and learning rates η Split S into T disjoint subsets S1, . . . , ST of size bn/T c for t = 0 to T − 1 do\nDefineRbn/Tc(φt, w) using St Get ιt by approximately solving (6) on St At ← ⌊ T n ⌋∑bn/Tc i=1 ∂zl(φt(xi), yi, w0)ιt(φt(xi)) >\nφt+1 ← φt − ηAtιt(φt(·)) end for Return φT−1 and w0\nOur proof mainly relies on bounding a statistical error of the functional gradient at each iteration in Algorithm 2. Because the population version of Algorithm 1 strictly decreases the value of R due to its smoothness, we can show that Algorithm 2 also decreases it with high probability when the norm of a functional gradient is larger than a statistical error bound. Thus, we make here an additional assumption on the loss function to bound the statistical error, which is satisfied for a multiclass logistic loss function.\nAssumption 3. For the differentiable loss function l(z, y, w) with respect to z, w, there exists a positive real number βr depending on r > 0 such that ‖∂zl(z, y, w)‖2 ≤ βr for z ∈ X , y ∈ Y, w ∈ Br(0).\nWe here introduce the notation required to describe the statement. We let F j be a collection of j-th elements of functions in F . For a positive value M , we set\n(m, ρ) def = β‖w0‖2\n√ KdD\nm\n( 2M + √ 2K log 2dD\nρ\n) .\nThe following proposition is a key result to bound a statistical error as mentioned above.\nProposition 3. Let Assumption 3 hold and each F j be the VC-class (for the definition see van der Vaart & Wellner (1996)). For ι ∈ F , we assume ‖ι(x)‖2 ≤ √ K on X . We set µ to be νX or νm,X and k(x, x′) to be ι(φ(x))>ι(φ(x′)). Then, there exists a positive value M depending on F and it follows that with probability at least 1−ρ over the choice of the sample of size m, (m, ρ) upper-bounds the following.\nsup ι∈F ‖Tk∂φR(φ,w0)− Tk,m∂φRm(φ,w0)‖Ld2(µ) .\nSince each iterate in Algorithm 2 is computed on a fresh batch not depending on previous batches, Proposition 3 can be applied to all iterates with m ← bn/T c and ρ ← δ/T for δ ∈ (0, 1). Thus, when bn/T c is large and η is small sufficiently, functional gradients used in Algorithm 2 become good approximation to the population variant, and we find that the expected risk function is likely to decrease from Proposition 1. Moreover, we note that statistical errors are accumulated additively rather than the exponential growth. Concretely, we obtain the following generalization guarantee.\nTheorem 3. Let Assumptions 1, 2, and 3 and the same assumption in Proposition 3 hold. Consider running Algorithm 2. If p ≥ 1, ‖∂ζ l(ζ, y)‖2 ≤ B, and the minimum eigenvalue of w0>w0 is lower-bounded by σ2 > 0, then we get with probability at least 1− ρ, ‖∇fL(w>0 φt∗)‖Lc1(νX) ≤ B ( 2T\nn log\nT\nρ\n) 1 4\n+\n√ B\nγ 1 q σ\n· { R0 ηT + β‖w0‖2 ( n T , ρ T ) + η 2 A‖w0‖2K 2β2‖w0‖2 + γ } 1 2q\nwhereR0 = R(w0, φ0) and t∗ is the index giving the minimum value of ‖∇fLbn/Tc(w>0 φt)‖Lcp(νbnT c,X)."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we present experimental results of the binary and multiclass classification tasks. We run Algorithm 1 and compare it with support vector machine, random forest, multilayer perceptron, and gradient boosting methods. We here introduce settings used for Algorithm 1. As for the loss function, we test both multiclass logistic loss and smooth hinge loss, and as for the embedding class F , we use two or three hidden-layer neural networks. The number of hidden units in each layer is set to 100 or 1000. Linear classifiers and embeddings are trained by Nesterov’s momentum method. The learning rate is chosen from {10−3, 10−2, 10−1, 1}. These parameters and the number of iterations T are tuned based on the performance on the validation set.\n0 5 10 15 20 0.8\n0.9\n1.0\nac cu\nra cy\nletter\n0 1 0.90\n0.95\n1.00 usps\n0 5 10 15 20 25 0.90\n0.95\n1.00 ijcnn1\n0 1 2 3 4 5 6 7 8\niterations\n0.90\n0.95\n1.00\nac cu\nra cy\nmnist\n0 10 20 30 40 50\niterations\n0.8\n0.9\n1.0 covtype\n0 1\niterations\n0.78\n0.80\n0.82 susy\ntrain test\nFigure 1. Learning curves for Algorithm 1 with multiclass logistic loss on libsvm datasets showing classification accuracy on training and test sets versus the number of iterations.\nWe use the following benchmark datasets: letter, usps, ijcnn1, mnist, covtype, and susy. We now explain the experimental procedure. For datasets not providing a fixed test set, we first divide each dataset randomly into two parts: 80% for training and the rest for test. We next divide each training set randomly and use 80% for training and the rest for validation. We perform each method on the training dataset with several hyperparameter settings and choose the best setting on the validation dataset. Finally, we train each model on the entire training dataset using this setting and evaluate it on the test dataset. This procedure is run 5 times.\nThe mean classification accuracy and the standard deviation are listed in Table 1. The support vector machine is performed using a random Fourier feature (Rahimi & Recht, 2007) with an embedding dimension of 103 or 104. For multilayer perceptron, we use three, four, or five hidden layers and rectified linear unit as the activation function. The number of hidden units in each layer is set to 100 or\n1000. As for random forest, the number of trees is set to 100, 500, or 1000 and the maximum depth is set to 10, 20, or 30. Gradient boosting in Table 1 indicates LightGBM (Ke et al., 2017) with the hyperparameter settings: the maximum number of estimators is 1000, the learning rate is chosen from {10−3, 10−2, 10−1, 1}, and number of leaves in one tree is chosen from {16, 32, . . . , 1024}.\nAs seen in Table 1, our method shows superior performance over the competitors except for covtype. However, the method that achieves higher accuracy than our method is only LightGBM on covtype. We plot learning curves for one run of Algorithm 1 with logistic loss, which depicts classification accuracies on training and test sets. Note that the number of iterations are determined by classification results on validation sets. This figure shows the efficiency of the proposed method."
  }, {
    "heading": "7. Conclusion",
    "text": "We have formalized the gradient boosting perspective of ResNet and have proposed new gradient boosting method by leveraging this viewpoint. We have shown two types of generalization bounds: one is by the margin bound and the other is by the sample-splitting technique. These bounds clarify the optimization-generalization tradeoff of the proposed method. Impressive empirical performance of the method has been confirmed on several benchmark datasets. We note that our method can take in convolutional neural networks as feature extractions, but additional efforts will be required to achieve high performance on image datasets. This is one of important topics left for future work."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was partially supported by MEXT KAKENHI (25730013, 25120012, 26280009, and 15H05707), JSTPRESTO (JPMJPR14E4), and JST-CREST (JPMJCR14D7, JPMJCR1304)."
  }],
  "year": 2018,
  "references": [{
    "title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures",
    "authors": ["L. Ambrosio", "N. Gigli", "G. Savaré"],
    "venue": "Lectures in Mathematics. ETH Zürich. Birkhäuser Basel,",
    "year": 2008
  }, {
    "title": "Statistical guarantees for the em algorithm: From population to sample-based analysis",
    "authors": ["S. Balakrishnan", "M.J. Wainwright", "B Yu"],
    "venue": "The Annals of Statistics,",
    "year": 2017
  }, {
    "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
    "authors": ["P.L. Bartlett", "S. Mendelson"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2002
  }, {
    "title": "Almost linear VC dimension bounds for piecewise polynomial networks",
    "authors": ["P.L. Bartlett", "V. Maiorov", "R. Meir"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 1998
  }, {
    "title": "Convexity, classification, and risk bounds",
    "authors": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2006
  }, {
    "title": "Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks",
    "authors": ["P.L. Bartlett", "N. Harvey", "C. Liaw", "A. Mehrabian"],
    "venue": "arXiv preprint arXiv:1703.02930,",
    "year": 2017
  }, {
    "title": "Reversible architectures for arbitrarily deep residual neural networks",
    "authors": ["B. Chang", "L. Meng", "E. Haber", "L. Ruthotto", "D. Begert", "E. Holtham"],
    "venue": "arXiv preprint arXiv:1709.03698,",
    "year": 2017
  }, {
    "title": "Multi-level residual networks from dynamical systems view",
    "authors": ["B. Chang", "L. Meng", "E. Haber", "F. Tung", "D. Begert"],
    "venue": "arXiv preprint arXiv:1710.10348,",
    "year": 2017
  }, {
    "title": "Xgboost: A scalable tree boosting system",
    "authors": ["T. Chen", "C. Guestrin"],
    "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
    "year": 2016
  }, {
    "title": "Adanet: Adaptive structural learning of artificial neural networks",
    "authors": ["C. Cortes", "X. Gonzalvo", "V. Kuznetsov", "M. Mohri", "S. Yang"],
    "venue": "In International Conference on Machine Learning",
    "year": 2017
  }, {
    "title": "Lecture notes on gradient flows and optimal transport",
    "authors": ["S. Daneri", "G. Savaré"],
    "venue": "arXiv preprint arXiv:1009.3737,",
    "year": 2010
  }, {
    "title": "Greedy function approximation: a gradient boosting machine",
    "authors": ["J.H. Friedman"],
    "venue": "The Annals of Statistics,",
    "year": 2001
  }, {
    "title": "Learning across scales-a multiscale method for convolution neural networks",
    "authors": ["E. Haber", "L. Ruthotto", "E. Holtham"],
    "venue": "arXiv preprint arXiv:1703.02009,",
    "year": 2017
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Learning deep resnet blocks sequentially using boosting theory",
    "authors": ["F. Huang", "J. Ash", "J. Langford", "R. Schapire"],
    "venue": "arXiv preprint arXiv:1706.04964,",
    "year": 2017
  }, {
    "title": "Densely connected convolutional networks",
    "authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger", "L. van der Maaten"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Residual connections encourage iterative inference",
    "authors": ["S. Jastrzebski", "D. Arpit", "N. Ballas", "V. Verma", "T. Che", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1710.04773,",
    "year": 2017
  }, {
    "title": "Lightgbm: A highly efficient gradient boosting decision tree",
    "authors": ["G. Ke", "Q. Meng", "T. Finley", "T. Wang", "W. Chen", "W. Ma", "Q. Ye", "Liu", "T.-Y"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Empirical margin distributions and bounding the generalization error of combined classifiers",
    "authors": ["V. Koltchinskii", "D. Panchenko"],
    "venue": "The Annals of Statistics,",
    "year": 2002
  }, {
    "title": "The loss surface of residual networks: Ensembles and the role of batch normalization",
    "authors": ["E. Littwin", "L. Wolf"],
    "venue": "arXiv preprint arXiv:1611.02525,",
    "year": 2016
  }, {
    "title": "Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations",
    "authors": ["Y. Lu", "A. Zhong", "Q. Li", "B. Dong"],
    "venue": "arXiv preprint arXiv:1710.10121,",
    "year": 2017
  }, {
    "title": "Boosting algorithms as gradient descent",
    "authors": ["L. Mason", "J. Baxter", "P.L. Bartlett", "M.R. Frean"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 1999
  }, {
    "title": "Boosted convolutional neural networks",
    "authors": ["M. Moghimi", "S.J. Belongie", "M.J. Saberian", "J. Yang", "N. Vasconcelos", "Li", "L.-J"],
    "venue": "In Proceedings of the British Machine Vision Conference,",
    "year": 2016
  }, {
    "title": "Norm-based capacity control in neural networks",
    "authors": ["B. Neyshabur", "R. Tomioka", "N. Srebro"],
    "venue": "In Proceedings of Conference on Learning Theory",
    "year": 2015
  }, {
    "title": "Stochastic particle gradient descent for infinite ensembles",
    "authors": ["A. Nitanda", "T. Suzuki"],
    "venue": "arXiv preprint arXiv:1712.05438,",
    "year": 2017
  }, {
    "title": "Gradient layer: Enhancing the convergence of adversarial training for generative models",
    "authors": ["A. Nitanda", "T. Suzuki"],
    "venue": "arXiv preprint arXiv:1801.02227,",
    "year": 2018
  }, {
    "title": "Random features for large-scale kernel machines",
    "authors": ["A. Rahimi", "B. Recht"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2007
  }, {
    "title": "Double continuum limit of deep neural networks",
    "authors": ["S. Sonoda", "N. Murata"],
    "venue": "In ICML Workshop Principled Approaches to Deep Learning,",
    "year": 2017
  }, {
    "title": "Consistency of support vector machines and other regularized kernel classifiers",
    "authors": ["I. Steinwart"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2005
  }, {
    "title": "Weak Convergence and Empirical Processes: With Applications to Statistics",
    "authors": ["A. van der Vaart", "J. Wellner"],
    "year": 1996
  }, {
    "title": "On the uniform convergence of relative frequencies of events to their probabilities",
    "authors": ["V. Vapnik", "A.Y. Chervonenkis"],
    "venue": "Theory of Probability and its Applications,",
    "year": 1971
  }, {
    "title": "Residual networks behave like ensembles of relatively shallow networks",
    "authors": ["A. Veit", "M.J. Wilber", "S. Belongie"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "High dimensional em algorithm: Statistical optimization and asymptotic normality",
    "authors": ["Z. Wang", "Q. Gu", "Y. Ning", "H. Liu"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "A proposal on machine learning via dynamical systems",
    "authors": ["E. Weinan"],
    "venue": "Communications in Mathematics and Statistics,",
    "year": 2017
  }, {
    "title": "Aggregated residual transformations for deep neural networks",
    "authors": ["S. Xie", "R. Girshick", "P. Dollár", "Z. Tu", "K. He"],
    "venue": "In Proceedings of the IEEE Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Wide residual networks",
    "authors": ["S. Zagoruyko", "N. Komodakis"],
    "venue": "In Proceedings of the British Machine Vision Conference,",
    "year": 2016
  }, {
    "title": "Statistical behavior and consistency of classification methods based on convex risk minimization",
    "authors": ["T. Zhang"],
    "venue": "The Annals of Statistics,",
    "year": 2004
  }, {
    "title": "Boosting with early stopping: Convergence and consistency",
    "authors": ["T. Zhang", "B Yu"],
    "venue": "The Annals of Statistics,",
    "year": 2005
  }],
  "id": "SP:3f55dd6f9c02ff01811259836d3ce494d9570ff3",
  "authors": [{
    "name": "Atsushi Nitanda",
    "affiliations": []
  }, {
    "name": "Taiji Suzuki",
    "affiliations": []
  }],
  "abstractText": "Residual Networks (ResNets) have become stateof-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by combining an ensemble of effective features. In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis. To do so, we formalize the gradient boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception. Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique. Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.",
  "title": "Functional Gradient Boosting based on Residual Network Perception"
}