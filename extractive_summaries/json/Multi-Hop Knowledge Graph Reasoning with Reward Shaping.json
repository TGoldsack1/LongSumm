{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3243–3253 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3243\nMulti-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained onehop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models."
  }, {
    "heading": "1 Introduction",
    "text": "Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017). Whether curated automatically or manually, practical KGs often fail to include many relevant facts. A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018).\nEmbedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017).\nMore recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths.\nWe refer to the RL formulation adopted by MINERVA as “learning to walk towards the answer” or “walk-based query-answering (QA)”. Walk-based QA eliminates the need to precompute path features, yet this setup poses several challenges for training. First, because practical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative targets, Figure 2). Second, since no ground truth path is available for training, the agent may traverse spurious paths that lead to a correct answer only incidentally (false positive paths). Because REINFORCE (Williams, 1992) is an on-policy RL algorithm (Sutton and Barto, 1998) which encourages past actions with high reward, it can bias the policy toward spurious paths found early in training (Guu et al., 2017).\nWe propose two modeling advances for RL approaches in the walk-based QA framework to address the aforementioned problems. First, instead of using a binary reward based on whether the agent has reached a correct answer or not, we adopt pre-trained state-of-the-art embeddingbased models (Dettmers et al., 2018; Trouillon et al., 2016) to estimate a soft reward for target entities whose correctness cannot be determined. As embedding-based models capture link semantics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained model. Second, we perform action dropout which randomly blocks some outgoing edges of the agent at each training step so as to enforce effective exploration of a diverse set of paths and dilute the negative impact of the spurious ones. Empirically, our overall model significantly improves over state-of-the-\nart multi-hop reasoning approaches on four out of five benchmark KG datasets (UMLS, Kinship, FB15k-237, WN18RR). It is also the first pathbased model that achieves consistently comparable or better performance than embedding-based models. We perform a thorough ablation study and result analysis, demonstrating the effect of each modeling innovation."
  }, {
    "heading": "2 Approach",
    "text": "In this section, we first review the walk-based QA framework (§2.2) and the on-policy reinforcement learning approach proposed by Das et al. (2018) (§2.3,§2.4). Then we describe our proposed solutions to the false negative reward and spurious path problems: knowledge-based reward shaping (§2.5) and action dropout (§2.6)."
  }, {
    "heading": "2.1 Formal Problem Definition",
    "text": "We formally represent a knowledge graph as G = (E ,R), where E is the set of entities and R is the set of relations. Each directed link in the knowledge graph l = (es, r, eo) ∈ G represents a fact (also called a triple).\nGiven a query (es, rq, ?), where es is the source entity and rq is the relation of interest, the goal is to perform an efficient search over G and collect the set of possible answers Eo = {eo} where (es, rq, eo) /∈ G due to incompleteness."
  }, {
    "heading": "2.2 Reinforcement Learning Formulation",
    "text": "The search can be formulated as a Markov Decision Process (MDP) (Sutton and Barto, 1998): starting from es, the agent sequentially selects an outgoing edge l and traverses to a new entity until it arrives at a target. Specifically, the MDP consists of the following components (Das et al., 2018).\nStates Each state st = (et, (es, rq)) ∈ S is a tuple where et is the entity visited at step t and (es, rq) are the source entity and query relation. et can be viewed as the state-dependent information while (es, rq) are the global context shared by all states.\nActions The set of possible actions At ∈ A at step t consists of the outgoing edges of et in G, i.e., At = {(r′, e′)|(et, r′, e′) ∈ G}. To give the agent the option to terminat a search, a self-loop edge is added to every At. When search is unrolled for a fixed number of steps T , the self-loop acts similarly to a “stop” action.\nTransition A transition function δ : S×A→ S is defined by δ(st, At) = δ(et, (es, rq), At). In walk-based QA, the transition is determined by G.\nRewards In the default formulation, the agent receives a terminal reward of 1 if it arrives at a correct target entity when search ends and 0 otherwise.\nRb(sT ) = {(es, rq, eT ) ∈ G}. (1)"
  }, {
    "heading": "2.3 Policy Network",
    "text": "The search policy is parameterized using state information and global context, plus the search history (Das et al., 2018).\nSpecifically, every entity and relation in G is assigned a dense vector embedding e ∈ d and r ∈ d. A particular action at = (rt+1, et+1) ∈ At is represented as the concatenation of the relation embedding and the end node embedding at = [r; e′t].\nThe search history ht = (es, r1, e1, . . . , rt, et) ∈ H consists of the sequence of actions taken up to step t, and can be encoded using an LSTM:\nh0 = LSTM(0, [r0; es]) (2) ht = LSTM(ht−1,at−1), t > 0, (3)\nwhere r0 is a special start relation introduced to form a start action with es.\nThe action space At is encoded by stacking the embeddings of all actions in it: At ∈ |At|×2d. And the policy network π is defined as:\nπθ(at|st) = σ(At ×W2 ReLU(W1[et;ht; rq])), (4)\nwhere σ is the softmax operator."
  }, {
    "heading": "2.4 Optimization",
    "text": "The policy network is trained by maximizing the expected reward over all queries in G:\nJ(θ) = (es,r,eo)∈G [ a1,...,aT∼πθ [R(sT |es, r)]]. (5) The optimization is done using the REINFORCE (Williams, 1992) algorithm, which iterates through all (es, r, eo) triples in G1 and updates\n1This training strategy treats a query with n > 1 answers as n single-answer queries. In particular, given a query (es, rq, ?) with multiple answers {et1 , . . . etn}, when training w.r.t. the example (es, rq, eti), MINERVA removes all {etj |j ̸= i} observed in the training data from the possible set of target entities in the last search step so as to force the agent to walk towards eti . We adopt the same technique in our training.\nθ with the following stochastic gradient:\n∇θJ(θ) ≈ ∇θ T∑\nt=1\nR(sT |es, r) log πθ(at|st).\n(6)"
  }, {
    "heading": "2.5 Knowledge-Based Reward Shaping",
    "text": "According to Equation 1, the agent receives a binary reward based solely on the observed answers in G. However, G is intrinsically incomplete and this approach penalizes the false negative search attempts identically to true negatives. To alleviate this problem, we adopt existing KG embedding models designed for the purpose of KG completion (Trouillon et al., 2016; Dettmers et al., 2018) to estimate a soft reward for target entities whose correctness is unknown.\nFormally, the embedding models map E and R to a vector space, and estimate the likelihood of each fact l = (es, r, et) ∈ G using f(es, r, et), a composition function of the entity and relation embeddings. f is trained by maximizing the likelihood of all facts in G. We propose the following reward shaping strategy (Ng et al., 1999):\nR(sT ) = Rb(sT ) + (1−Rb(sT ))f(es, rq, eT ). (7) Namely, if the destination eT is a correct answer according to G, the agent receives reward 1. Otherwise the agent receives a fact score estimated by f(es, rq, eT ), which is pre-trained. Here we keep f in its general form and it can be replaced by any state-of-the-art model (Trouillon et al., 2016; Dettmers et al., 2018) or ensemble thereof."
  }, {
    "heading": "2.6 Action Dropout",
    "text": "The REINFORCE training algorithm performs onpolicy sampling according to πθ(at|st), and updates θ stochastically using Equation 6. Because the agent does not have access to any oracle path, it is possible for it to arrive at a correct answer eo via a path irrelevant to the query relation. As shown in Figure 1, the path Obama −endorsedBy→ McCain −liveIn→ U.S. ←locatedIn− Hawaii does not infer the fact bornIn(Obama,Hawaii).\nDiscriminating paths of different qualities is non-trivial, and existing RL approaches for walkbased KGQA largely rely on the terminal reward to bias the search. Since there are usually more spurious paths than correct ones, spurious paths are often found first, and following exploration can be increasingly biased towards them (Equation 6).\nEntities with larger fan-in (in-degree) and fan-out (out-degree) often exacerbate this problem.\nGuu et al. (2017) identified a similar issue in RL-based semantic parsing with weak supervision, where programs that do not semantically match the user utterance frequently pass the tests. To solve this problem, Guu et al. (2017) proposed randomized beam search combined with a meritocratic update rule to ensure all trajectories that obtain rewards are up-weighted roughly equally.\nHere we propose the action dropout technique which achieves similar effect as randomized search and is simpler to implement over graphs. Action dropout randomly masks some outgoing edges for the agent in the sampling step of REINFORCE. The agent then performs sampling2 according to the adjusted action distribution\nπ̃θ(at|st) ∝ (πθ(at|st) ·m+ ϵ) (8) mi ∼ Bernoulli(1− α), i = 1, . . . |At|, (9)\nwhere each entry of m ∈ {0, 1}|At| is a binary variable sampled from the Bernoulli distribution with parameter 1 − α. A small value ϵ is used to smooth the distribution in case m = 0, where π̃θ(at|st) becomes uniform.\nOur overall approach is illustrated in Figure 3."
  }, {
    "heading": "3 Related Work",
    "text": "In this section, we summarize the related work and discuss their connections to our approach.\n2We only modify the sampling distribution and still use πθ(at|st) to compute the gradient update in equation 6."
  }, {
    "heading": "3.1 Knowledge Graph Embeddings",
    "text": "KG embeddings (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) are one-hop KG modeling approaches which learn a scoring function f(es, r, eo) to define a fuzzy truth value of a triple in the embedding space. These models can be adapted for query answering by simply return the eo’s with the highest f(es, r, eo) scores. Despite their simplicity, embedding-based models achieved state-of-the-art performance on KGQA (Das et al., 2018). However, such models ignore the symbolic compositionality of KG relations, which limits their usage in more complex reasoning tasks. The reward shaping (RS) strategy we proposed is a step to combine their capability in modeling triple semantics with the symbolic reasoning capability of the path-based approach."
  }, {
    "heading": "3.2 Multi-Hop Reasoning",
    "text": "Multi-hop reasoning focus on learning symbolic inference rules from relational paths in the KG and has been formulated as sequential decision problems in recent works (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, DeepPath (Xiong et al., 2017) first adopted REINFORCE to search for generic representative paths between pairs of entities. DIVA (Chen et al., 2018) also performs generic path search between entities using RL and its variational objective can be interpreted as model-based reward assignment. MINERVA (Das et al., 2018) first introduced RL\nto search for answer entities of a particular KG query end-to-end. MINERVA uses entropy regularization to softly encourage the policy to sample diverse paths, and we show that hard action dropout is more effective in this setup. ReinforceWalk (Shen et al., 2018) further proposed to solve the reward sparsity problem in walk-based QA using off-policy learning. ReinforceWalk scores the search targets with a value function which is updated based on the search history cached through epochs. In comparison, we leveraged existing embedding-based models for reward shaping, which is much more efficient during training."
  }, {
    "heading": "3.3 Reinforcement Learning",
    "text": "Recently, RL has seen a variety of applications in NLP including machine translation (Ranzato et al., 2015), summarization (Paulus et al., 2017), and semantic parsing (Guu et al., 2017). Compared to the domain of gaming (Mnih et al., 2013) where RL is mostly applied for, RL formulations in NLP often have a large discrete action space. For example, in machine translation, the space of possible actions is the entire vocabulary of a language. Walk-based QA also suffers from this problem, as some entities may have thousands of neighbors (e.g. U.S.). Since often there is no golden path available for a KG reasoning problem, we cannot leverage supervised pre-training to initialize the path search following the common practice in RL-based natural language generation (Ranzato et al., 2015). On the other hand, the inference paths being studied in a KG are often much shorter (usually containing 2-5 steps) compared to the target sentences in the NL generation problems (often containing 20-30 words), which simplifies the training to some extent."
  }, {
    "heading": "4 Experiment Setup",
    "text": "We evaluate our modeling contributions on five KGs from different domains and exhibiting different graph properties (§ 4.1). We compare with two classes of state-of-the-art KG models: multi-hop neural symbolic approaches and KG embeddings (§4.2). In this section, we describe the datasets and our experiment setup in detail."
  }, {
    "heading": "4.1 Dataset",
    "text": "We adopt five benchmark KG datasets for query answering: (1) Alyawarra Kinship, (2) Unified Medical Language Systems (Kok and Domingos,\n2007), (3) FB15k-237 (Toutanova et al., 2015), (4) WN18RR (Dettmers et al., 2018), and (5) NELL995 (Xiong et al., 2017). The statistics of the datasets are shown in Table 1."
  }, {
    "heading": "4.2 Baselines and Model Variations",
    "text": "We compare with three embedding based models: DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018). We also compare with three multi-hop neural symbolic models: (a) NTP-λ, an improved version of Neural Theorem Prover (Rocktäschel and Riedel, 2017), (b) Neural Logical Programming (NeuralLP) (Yang et al., 2017) and (c) MINERVA. For our own approach, we include two model variations that use ComplEx and ConvE as the reward shaping modules respectively, denoted as Ours(ComplEx) and Ours(ConvE). We quote the results of NeuralLP, NTP-λ and MINERVA reported in Das et al. (2018), and replicated the embedding based systems.3"
  }, {
    "heading": "4.3 Implementation Details",
    "text": "Beam Search Decoding We perform beam search decoding to obtain a list of unique entity predictions. Because multiple paths may lead to the same target entity, we compute the list of unique entities reached in the final search step and assign each of them the maximum score of all paths that led to it. We then output the top-ranked unique entities. We find this approach to improve over directly taking the entities ranked at the beam top, as many of them are repetitions.\nKG Setup Following previous work, we treat every KG link as bidirectional and augment the graph with the reversed (eo, r−1, es) links. We use the same train, dev, and test set splits as Das et al. (2018). We exclude any link from the dev and\n3 Das et al. (2018) reported MINERVA results with the entity embedding usage as an extra hyperparameter – the quoted performance of MINERVA in Table 2 on UMLS and Kinship were obtained with entity embeddings setting to zero. In contrast, our system always uses trained entity embeddings.\ntest set (and its reversed link) from the train set. Following Das et al. (2018), we cut the maximum number of outgoing edges of an entity by threshold η to prevent GPU memory overflow: for each entity we keep its top-η neighbors with the highest PageRank scores (Page et al., 1999) in the graph.\nHyperparameters We set the entity and relation embedding size to 200 for all models. We use Xavier initialization (Glorot and Bengio, 2010) for the embeddings and the NN layers. For ConvE, we use the same convolution layer and label smoothing hyperparameters as Dettmers et al. (2018). For path-based models, we use a three-layer LSTM as the path encoder and set its hidden dimension to 200. We perform grid search on the reasoning path length (2, 3), the node fan-out threshold η (256- 512) and the action dropout rate α (0.1-0.9). Following Das et al. (2018), we add an entropy regularization term in the objective and tune the weight parameter β within 0-0.1. We use Adam optimization (Kingma and Ba, 2014) and search the learning rate (0.001-0.003) and mini-batch size (128- 512).4 For all models we apply dropout to the entity and relation embeddings and all feed-forward layers, and search the dropout rates within 0-0.5. We use a decoding beam size of 512 for NELL995 and 128 for the other datasets.\nEvaluation Protocol We convert each triple (es, r, eo) in the test set into a query and compute ranking-based evaluation metrics. The models take es, r as the input and output a list of candidate answers Eo = [e1, . . . , eL] ranked in decreasing order of confidence score. We compute\n4On some datasets, we found larger batch size to continue improving the performance but had to stop at 512 due to memory constraints.\nreo , the rank of eo among Eo, after removing the other correct answers from Eo and use it to compute two types of metrics: (1) Hits@k which is the percentage of examples where reo ≤ k and (2) mean reciprocal rank (MRR) which is the mean of 1/reo for all examples in the test set. We use the entire test set for evaluation, with the exception of NELL-995, where test triples with unseen entities are removed following Das et al. (2018).\nOur Pytorch implementation of all experiments is released at https://github.com/ salesforce/MultiHopKG."
  }, {
    "heading": "5 Results",
    "text": ""
  }, {
    "heading": "5.1 Model Comparison",
    "text": "Table 2 shows the evaluation results of our proposed approach and the baselines. The top part presents embedding based approaches and the bottom part presents multi-hop reasoning approaches.5\nWe find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on UMLS, Kinship, FB15K237 and NELL-995 despite their simplicity. While previous path based approaches achieve comparable performance on some of the datasets (WN18RR, NELL-995, and UMLS), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on Kinship and FB15k-237 respectively). A possible reason for this is that embedding based methods map every link in the KG into the same embedding space, which implicitly encodes the connectivity of the whole graph. In contrast, path based models use the discrete represen-\n5We report the model robustness measurements in § A.1.\ntation of a KG as input, and therefore have to leave out a significant proportion of the combinatorial path space by selection. For some path based approaches, computation cost is a bottleneck. In particular, NeuralLP and NTP-λ failed to scale to the larger datasets and their results are omitted from the table, as Das et al. (2018) reported.\nOurs is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets. The best single model, Ours(ConvE), improves the SOTA performance of path-based models on three datasets (UMLS, Kinship, and FB15k-237) by 4%, 9%, and 39% respectively. On NELL-995, our approach did not significantly improve over existing SOTA. The NELL-995 dataset consists of only 12 relations in the test set and, as we further detail in the analysis (§ 5.3.3), our approach is less effective for those relation types.\nThe model variations using different reward shaping modules perform similarly. While a better reward shaping module typically results in a better overall model, an exception is WN18RR, where ComplEx performs slightly worse on its own but is more helpful for reward shaping. We left the study of the relationship between the reward shaping module accuracy and the overall model performance as future work."
  }, {
    "heading": "5.2 Ablation Study",
    "text": "We perform an ablation study where we remove reward shaping (−RS) and action dropout (−AD) from Ours(ConvE) and compare their MRRs to the whole model on the dev sets.6 As shown in Table 3, on most datasets, removing each component results in a significant performance drop. The exception is WN18RR, where removing the ConvE reward shaping module improves the performance.7 Removing reward shaping on NELL-\n6According to Table 3 and Table 2, the dev and test set evaluation metrics differ significantly on several datasets. We discuss the cause of this in § A.2.\n7A possible explanation for this is that as path-based models tend to outperform the embedding based approaches on WN18RR, ConvE may be supplying more noise than useful\n995 does not change the results significantly. In general, removing action dropout has a greater impact, suggesting that thorough exploration of the path space is important across datasets."
  }, {
    "heading": "5.3 Analysis",
    "text": ""
  }, {
    "heading": "5.3.1 Convergence Rate",
    "text": "We are interested in studying the impact of each proposed enhancement on the training convergence rate. In particular, we expect reward shaping to accelerate the convergence of RL (to a better performance level) as it propagates prior knowledge about the underlying KG to the agent. On the other hand, a fair concern for action dropout is that it can be slower to train, as the agent is forced to explore a more diverse set of paths. Figure 4 eliminates this concern.\nThe first row of Figure 4 shows the changes in dev set MRR of Ours(ConvE) (green ∗) and the two ablated models w.r.t. # epochs. In general, the proposed approach is able to converge to a higher accuracy level much faster than either of the ablated models and the performance gap often persists until the end of training (on UMLS, Kinship, and FB15k-237). Particularly, on FB15k-237, our approach still shows improvement even after the two ablated models start to overfit, with −AD beginning to overfit sooner. On WN18RR, introducing reward shaping hurt dev set performance from the beginning, as discussed in § 5.2. On NELL995, Ours(ConvE) performs significantly better in the beginning, but −RS gradually reaches a comparable performance level.\nIt is especially interesting that introducing action dropout immediately improves the model performance on all datasets. A possible explanation for this is that by exploring a more diverse set of paths the agent learns search policies that generalize better."
  }, {
    "heading": "5.3.2 Path Diversity",
    "text": "We also compute the total number of unique paths the agent explores during training and visualize its change w.r.t. # training epochs in the second row of Figure 4. When counting a unique path, we include both the edge label and intermediate entity.\ninformation about the KG. Yet counter-intuitively, we found that adding the ComplEx reward shaping module helps, despite the fact that ComplEx performs slightly worse than ConvE on this dataset. This indicates that dev set accuracy is not the only factor which determines the effectiveness of reward shaping.\nFirst we observe that, on all datasets, the agent explores a large number of paths before reaching a good performance level. The speed of path discovery slowly decreases as training progresses. On smaller KGs (UMLS and Kinship), the rate of encountering new paths is significantly lower after a certain number of epochs, and the dev set accuracy plateaus correspondingly. On much larger KGs (FB15k-237, WN18RR, and NELL-995), we did not observe a significant slowdown before severe overfitting occurs and the dev set performance starts to drop. A possible reason for this is that the larger KGs are more sparsely connected compared to the smaller KGs (Table 1), therefore it is less efficient to gain generalizable knowledge from the KG by exploring a limited proportion of the path space through sampling.\nSecond, while removing action dropout significantly lowers the effectiveness of path exploration (orange ! vs. green ∗), we observe that removing reward shaping (blue △) slightly increases the # paths visited if the action dropout rate is kept the same. This indicates that the correlation between\n# paths explored and dev set performance is not strictly positive. The best performing model is not always the model that explored the largest # paths. It also demonstrates the role of reward shaping as a regularizer which guides the agent to avoid noisy paths with its prior knowledge."
  }, {
    "heading": "5.3.3 Performance w.r.t. Relation Types",
    "text": "We investigate the behaviors of our proposed approach w.r.t different relation types. For each KG, we classify its set of relations into two categories based on the answer set cardinality. Specifically, we define the metric ξr as the average answer set cardinality of all queries with topic relation r. We count r as a “to-many” relation if ξr > 1.5, which indicates that most queries in relation r has more than 1 correct answer; we count r as a “to-one” relation otherwise, meaning most queries of this relation have only 1 correct answer.\nTable 4 shows the percentage of examples of tomany and to-one relations on each dev dataset and the MRR evaluation metrics of previously studied models computed on the examples of each relation\ntype. Since UMLS and Kinship are densely connected, they almost exclusively contain to-many relations. FB15k-237 mostly contains to-many relations. In Figure 4, we observe the biggest relative gains from the ablated models on these three datasets. WN18RR is more balanced and consists of slightly more to-many relations than toone relations. The NELL-995 dev set is a unique one which almost exclusively consists of to-one relations. There is no common performance pattern over the two relation types across datasets: on some datasets all models perform better on tomany relations (UMLS, WN18RR) while others show the opposite trend (FB15k-237, NELL-995). We leave the study of these discrepancies to future work.\nWe show the relative performance change of the ablated models−RS and−AD w.r.t. Ours(ConvE) in parentheses. We observe that in general our proposed enhancements are effective in improving query-answering over both relation types (more effective for to-many relations). However, adding the ConvE reward shaping module on WN18RR hurts the performance over both to-many and toone relations (more for to-one relations). On NELL-995, both techniques hurt the performance over to-many relations."
  }, {
    "heading": "5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries",
    "text": "Since most benchmark datasets randomly split the KG triples into train, dev and test sets, the queries that have multiple answers may fall into multiple splits. As a result, some of the test queries (es, rq, ?) are seen in the training set (with a different set of answers) while the others are not. We investigate the behaviors of our proposed approach w.r.t. seen and unseen queries.\nTable 5 shows the percentage of examples associated with seen and unseen queries on each dev dataset and the corresponding MRR evaluation metrics of previously studied models. On\nmost datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations (Table 4) as a result of random data split, with the exception of WN18RR. On some datasets, all models perform better on seen queries (UMLS, Kinship, WN18RR) while others reveal the opposite trend. We leave the study of these model behaviors to future work. On NELL-995 both of our proposed enhancements are not effective over the seen queries. In most cases, our proposed enhancements improve the performance over unseen queries, with AD being more effective."
  }, {
    "heading": "6 Conclusions",
    "text": "We propose two modeling advances for end-toend RL-based knowledge graph query answering: (1) reward shaping via graph completion and (2) action dropout. Our approach improves over state-of-the-art multi-hop reasoning models consistently on several benchmark KGs. A detailed analysis indicates that the access to a more accurate environment representation (reward shaping) and a more thorough exploration of the search space (action dropout) are important to the performance boost.\nOn the other hand, the performance gap between RL-based approaches and the embeddingbased approaches for KGQA remains. In future work, we would like to investigate learnable reward shaping and action dropout schemes and apply model-based RL to this domain."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Mark O. Riedl, Yingbo Zhou, James Bradbury and Vena Jia Li for their feedback on early draft of the paper, and Mark O. Riedl for helpful conversations on reward shaping. We thank the anonymous reviewers and the Salesforce research team members for their thoughtful comments and discussions. We thank Fréderic Godin for pointing out an error in Equation 8 in an early version of the paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers. Association for Computational Linguistics",
    "authors": ["Regina Barzilay", "Min-Yen Kan", "editors"],
    "year": 2017
  }, {
    "title": "Semantic parsing on freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "(Yarowsky et al., 2013), pages 1533–1544.",
    "year": 2013
  }, {
    "title": "Translating embeddings for modeling multirelational data",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDurán", "Jason Weston", "Oksana Yakhnenko."],
    "venue": "Proceedings of the 26th International Conference on Neural Information Process-",
    "year": 2013
  }, {
    "title": "Variational knowledge graph reasoning",
    "authors": ["Wenhu Chen", "Wenhan Xiong", "Xifeng Yan", "William Yang Wang."],
    "venue": "CoRR, abs/1803.06581.",
    "year": 2018
  }, {
    "title": "2018. Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement",
    "authors": ["Rajarshi Das", "Shehzaad Dhuliawala", "Manzil Zaheer", "Luke Vilnis", "Ishan Durugkar", "Akshay Krishnamurthy", "Alex Smola", "Andrew McCallum"],
    "year": 2018
  }, {
    "title": "Convolutional 2d knowledge graph embeddings",
    "authors": ["Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel."],
    "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February",
    "year": 2018
  }, {
    "title": "Improving learning and inference in a large knowledge-base using latent syntactic cues",
    "authors": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell."],
    "venue": "(Yarowsky et al., 2013), pages 833–838.",
    "year": 2013
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Xavier Glorot", "Yoshua Bengio."],
    "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sar-",
    "year": 2010
  }, {
    "title": "Traversing knowledge graphs in vector space",
    "authors": ["Kelvin Guu", "John Miller", "Percy Liang."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015,",
    "year": 2015
  }, {
    "title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood",
    "authors": ["Kelvin Guu", "Panupong Pasupat", "Evan Zheran Liu", "Percy Liang."],
    "venue": "(Barzilay and Kan, 2017), pages 1051–1062.",
    "year": 2017
  }, {
    "title": "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings",
    "authors": ["He He", "Anusha Balakrishnan", "Mihail Eric", "Percy Liang."],
    "venue": "(Barzilay and Kan, 2017), pages 1766– 1776.",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Statistical predicate invention",
    "authors": ["Stanley Kok", "Pedro M. Domingos."],
    "venue": "ICML, volume 227 of ACM International Conference Proceeding Series, pages 433–440. ACM.",
    "year": 2007
  }, {
    "title": "Random walk inference and learning in a large scale knowledge base",
    "authors": ["Ni Lao", "Tom Mitchell", "William W. Cohen."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 529–539, Stroudsburg,",
    "year": 2011
  }, {
    "title": "Reading the web with learned syntactic-semantic inference rules",
    "authors": ["Ni Lao", "Amarnag Subramanya", "Fernando C.N. Pereira", "William W. Cohen."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
    "year": 2012
  }, {
    "title": "Chains of reasoning over entities, relations, and text using recurrent neural networks",
    "authors": ["Andrew McCallum", "Arvind Neelakantan", "Rajarshi Das", "David Belanger."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association",
    "year": 2017
  }, {
    "title": "Playing atari with deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller."],
    "venue": "CoRR, abs/1312.5602.",
    "year": 2013
  }, {
    "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
    "authors": ["Andrew Y. Ng", "Daishi Harada", "Stuart J. Russell."],
    "venue": "Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999), Bled,",
    "year": 1999
  }, {
    "title": "The pagerank citation",
    "authors": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"],
    "year": 1999
  }, {
    "title": "A deep reinforced model for abstractive summarization",
    "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher."],
    "venue": "CoRR, abs/1705.04304.",
    "year": 2017
  }, {
    "title": "Sequence level training with recurrent neural networks. CoRR, abs/1511.06732",
    "authors": ["Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"],
    "year": 2015
  }, {
    "title": "End-toend differentiable proving",
    "authors": ["Tim Rocktäschel", "Sebastian Riedel."],
    "venue": "(Guyon et al., 2017), pages 3791–3803.",
    "year": 2017
  }, {
    "title": "Reinforcewalk: Learning to walk in graph with monte carlo tree search",
    "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao."],
    "venue": "CoRR, abs/1802.04394.",
    "year": 2018
  }, {
    "title": "Reasoning with neural tensor networks for knowledge base completion",
    "authors": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."],
    "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1,",
    "year": 2013
  }, {
    "title": "Reinforcement learning - an introduction",
    "authors": ["Richard S. Sutton", "Andrew G. Barto."],
    "venue": "Adaptive computation and machine learning. MIT Press.",
    "year": 1998
  }, {
    "title": "Representing text for joint embedding of text and knowledge bases",
    "authors": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."],
    "venue": "EMNLP, pages 1499– 1509. The Association for Computational Linguis-",
    "year": 2015
  }, {
    "title": "Compositional learning of embeddings for relation paths in knowledge base and text",
    "authors": ["Kristina Toutanova", "Xi Victoria Lin", "Wen-tau Yih", "Hoifung Poon", "Chris Quirk."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational",
    "year": 2016
  }, {
    "title": "Complex embeddings for simple link prediction",
    "authors": ["Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard."],
    "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY,",
    "year": 2016
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J. Williams."],
    "venue": "Machine Learning, 8:229–256.",
    "year": 1992
  }, {
    "title": "Deeppath: A reinforcement learning",
    "authors": ["Wenhan Xiong", "Thien Hoang", "William Yang Wang"],
    "year": 2017
  }, {
    "title": "Embedding entities and relations for learning and inference in knowledge bases",
    "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."],
    "venue": "CoRR, abs/1412.6575.",
    "year": 2014
  }, {
    "title": "Differentiable learning of logical rules for knowledge base reasoning",
    "authors": ["Fan Yang", "Zhilin Yang", "William W. Cohen."],
    "venue": "(Guyon et al., 2017), pages 2316–2325.",
    "year": 2017
  }],
  "id": "SP:7a941148d8c5865749801b2f9f67f9ad1fba1d25",
  "authors": [{
    "name": "Xi Victoria Lin",
    "affiliations": []
  }, {
    "name": "Richard Socher",
    "affiliations": []
  }, {
    "name": "Caiming Xiong",
    "affiliations": []
  }],
  "abstractText": "Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained onehop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",
  "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping"
}