{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider the task of labeling unsegmented sequence data and predicting future labels. This is a ubiquitous problem with important applications in many perceptual tasks, e.g., recognition and anticipation of human activities or speech. A generic modeling approach is key for intelligent agents to perform future-aware tasks (e.g., assistive activities).\nSuch tasks often exhibit non-Markovian and compositional properties, which should be captured by a top-down prediction algorithm. Consider the video sequence in Figure 1, human observers recognize the first two actions and predict the most likely future action based on the entire history. Context-free grammars are natural choices to model such reasoning processes, and it is one step closer to Turing machines than Markov models (e.g., Hidden Markov Models) in the Chomsky hierarchy.\nHowever, it is not clear how to directly use symbolic gram-\n1University of California, Los Angeles, USA 2Peking University, Beijing, China. Correspondence to: Siyuan Qi <syqi@cs.ucla.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n...\n... open microwave take food put into microwave\nParse Tree Future prediction\n...Sequenceinput data\nClassi er raw output\nFinal output\nGrammar\nGeneralized Earley Parser microwave food\nFigure 1. The input of the generalized Earley parser is a matrix of probabilities of each label for each frame, given by an arbitrary classifier. The parser segments and labels the sequence data into a label sentence in the language of a given grammar. Future predictions are then made based on the grammar.\nmars to parse and label sequence data. Traditional grammar parsers take symbolic sentences as inputs instead of noisy sequence data like videos or audios. The data has to be i) segmented and ii) labeled to apply existing grammar parsers to. One naive solution is to first segment and label the data using a classifier and thus generating a label sentence. Then grammar parsers can be applied on top of it for prediction. But this is apparently non-optimal, since the grammar rules are not considered in the classification process. It may not even be possible to parse this label sentence, because they are very often grammatically incorrect.\nIn this paper, we design a grammar-based parsing algorithm that directly operates on sequence input data, which goes beyond the scope of symbolic string inputs. Specifically, we propose a generalized Earley parser based on the Earley parser (Earley, 1970). The algorithm finds the optimal segmentation and label sentence according to both a symbolic grammar and a classifier output of probabilities of labels for each frame as shown in Figure 1. Optimality here means maximizing the probability of the label sentence according to the classifier output while being grammatically correct.\nThe difficulty of achieving this optimality lies in the joint optimization of both the grammar structure and the parsing likelihood of the output label sentence. For example, an expectation-maximization-type algorithm will not work well\nsince i) there is no guarantee for optimality, and ii) any grammatically incorrect sentence has a grammar prior of probability 0. The algorithm can easily get stuck in local minimums and fail to find a grammatically correct solution.\nThe core idea of our algorithm is to directly and efficiently search for the optimal label sentence in the language defined by the grammar. The constraint of the search space ensures that the sentence is grammatically correct. Specifically, a heuristic search is performed on the prefix tree expanded according to the grammar, where the path from the root to a node represents a partial sentence (prefix). By carefully defining the heuristic as a prefix probability computed based on the classifier output, we can efficiently search through the tree to find the optimal label sentence.\nThe generalized Earley parser has four major advantages. i) The inference process highly integrates a high-level grammar with an underlying classifier; the grammar gives guidance for segmenting and labeling the sequence data. ii) It can be applied to any classifier outputs. iii) It generates a grammar parse tree for data sequence that is highly explainable. iv) It is principled and generic, as it applies to most sequence data parsing and prediction problems.\nWe evaluate the proposed approach on two datasets of human activities in the computer vision domain. The first dataset CAD-120 (Koppula et al., 2013) consists of daily activities and most activity prediction methods are based on this dataset. Comparisons show that our method significantly outperforms state-of-the-art methods on future activity prediction. The second dataset Watch-n-Patch (Wu et al., 2015) is designed for “action patching”, which includes daily activities that have action forgotten by people. Experiments on the second dataset show the robustness of our method on noisy data. Both experiments show that the generalized Earley parser performs particularly well on prediction tasks, primarily due to its non-Markovian property.\nThis paper makes three major contributions. •We design a parsing algorithm for symbolic context-free grammars. It directly operates on sequence data to obtain the optimal segmentation and labels. •We propose a prediction algorithm that naturally integrates with this parsing algorithm. •We formulate an objective for future prediction for both grammar induction and classifier training. The generalized Earley parser serves as a concrete example for combining symbolic reasoning methods and connectionist approaches."
  }, {
    "heading": "2. Background: Context-Free Grammars",
    "text": "In formal language theory, a context-free grammar is a type of formal grammar, which contains a set of production rules that describe all possible sentences in a given formal language. A context-free grammar G in Chomsky Normal\nForm is defined by a 4-tuple G = (V,Σ, R, γ) where 1. V is a finite set of non-terminal symbols that can be replaced by/expanded to a sequence of symbols. 2. Σ is a finite set of terminal symbols that represent actual words in a language, which cannot be further expanded. 3. R is a finite set of production rules describing the replacement of symbols, typically of the form A→ BC or A→ α for A,B,C ∈ V and α ∈ Σ. 4. γ ∈ V is the start symbol (root of the grammar).\nGiven a formal grammar, parsing is the process of analyzing a string of symbols, conforming to the production rules and generating a parse tree. A parse tree represents the syntactic structure of a string according to some contextfree grammar. The root node of the tree is the grammar root. Other non-leaf nodes correspond to non-terminals in the grammar, expanded according to grammar production rules. The leaf nodes are terminal symbols. All the leaf nodes together form a sentence.\nThe above definition can be augmented by assigning a probability to each production rule, thus becoming a probabilistic context-free grammar. The probability of a parse tree is the product of the production rules that derive the parse tree."
  }, {
    "heading": "3. Earley Parser",
    "text": "In this section, we review the original Earley parser (Earley, 1970) and introduce the basic concepts. Earley parser is an algorithm for parsing sentences of a given context-free language. In the following descriptions, α, β, and γ represent any string of terminals/nonterminals (including the empty string ), A and B represent single nonterminals, and a represents a terminal symbol. We adopt Earley’s dot notation: for production of form A → αβ, the notation A → α · β means α has been parsed and β is expected.\nInput position n is defined as the position after accepting the nth token, and input position 0 is the position prior to input. At each input position m, the parser generates a state set S(m). Each state is a tuple (A→ α ·β, i), consisting of\n• The production currently being matched (A→ αβ). • The dot: the current position in that production. • The position i in the input at which the matching of this\nproduction began: the position of origin.\nSeeded with S(0) containing only the top-level rule, the parser then repeatedly executes three operations: prediction, scanning and completion:\n• Prediction: for every state in S(m) of the form (A→ α · Bβ, i), where i is the origin position as above, add (B → ·γ,m) to S(m) for every production in the grammar with B on the left-hand side (i.e., B → γ). • Scanning: if a is the next symbol in the input stream, for every state in S(m) of the form (A → α · aβ, i), add (A→ αa · β, i) to S(m+ 1). • Completion: for every state in S(m) of the form (A → γ·, j), find states in S(j) of the form (B → α · Aβ, i) and add (B → αA · β, i) to S(m).\nIn this process, duplicate states are not added to the state set. These three operations are repeated until no new states can be added to the set. The Earley parser executes in O(n2) for unambiguous grammars regarding the string length n, and O(n) for almost all LR(k) grammars. A simple example is demonstrated in Figure 2."
  }, {
    "heading": "4. Generalized Earley Parser",
    "text": "In this section, we introduce the proposed algorithm. Instead of taking symbolic sentences as input, we aim to design an algorithm that can parse raw sequence data x of length T (e.g., videos or audios) into a sentence l of labels (e.g., actions or words) of length |l| ≤ T , where each label k ∈ {0, 1, · · · ,K} corresponds to a segment of a sequence. To achieve that, a classifier (e.g., a neural network) is first applied to each sequence x to get a T × K probability matrix y (e.g., softmax activations of the neural network), with ykt representing the probability of frame t being labeled as k. The proposed generalized Earley parser takes y as input and outputs the sentence l∗ that best explains the data according to a grammar G of Chomsky normal form.\nThe core idea is to use the original Earley parser to help construct a prefix tree according to the grammar as illustrated in Figure 3. A prefix tree is composed of terminal symbols and terminations that represent ends of sentences. The root node of the tree is the “empty” symbol. The path from the root to any node in the tree represents a partial sentence (prefix). For each prefix, we can compute the probability that the best label sentence starts with this prefix. This probability\nis used as a heuristic to search for the best label sentence in the prefix tree: the prefix probabilities prioritize the nodes to be expanded in the prefix tree. The parser finds the best solution when it expands a termination node in the tree. It then returns the current prefix string as the best solution.\nThis heuristic search is realized by generalizing the Earley parser. Specifically, the scan operation in the Earley parser essentially expands a new node in the grammar prefix tree. For each prefix l, we can compute p(l|x0:T ) and p(l···|x0:T ) based on y, where p(l|x0:T ) is the probability of l being the best label, and p(l···|x0:T ) is the probability of l being the prefix of the best label of x0:T . The formulations for p(l|x0:T ) and p(l···|x0:T ) are derived in Section 4.1.\nWe now describe the details. Each scan operation will create a new set S(m,n) ∈ S(m), where m is the length of the scanned string, n is the total number of the terminals that have been scanned at position m. This can be thought of as creating a new leaf node in the prefix tree, and S(m) is the set of all created nodes at level m. A priority queue q is kept for state sets for prefix search. Scan operations will push the newly created set into the queue with priority p(l···), where l is the parsed string of the state being scanned.\nEach state is a tuple (A→ α · β, i, j, l, p(l···)) augmented from the original Earley parser by adding j, l, p(l···). Here l is the parsed string of the state, and i, j are the indices of the set that this rule originated. The parser then repeatedly executes three operations: prediction, scanning, and completion modified from Earley parser:\n• Prediction: for every state in S(m,n) of the form (A→ α · Bβ, i, j, l, p(l···)), add (B → ·γ,m, n, l, p(l···)) to S(m,n) for every production in the grammar with B on the left-hand side. • Scanning: for every state in S(m,n) of the form (A → α · aβ, i, j, l, p(l···)), append the new terminal a to l and compute the probability p((l + a)···). Create a new set\nS(m + 1, n′) where n′ is the current size of S(m + 1). Add (A→ αa·β, i, j, l+a, p((l+a)···)) to S(m+1, n′). Push S(m+ 1, n′) into q with priority p((l + a)···). • Completion: for every state in S(m,n) of the form (A → γ·, i, j, l, p(l···)), find states in S(i, j) of the form (B → α · Aβ, i′, j′, l′, p(l′···)) and add (B → αA · β, i′, j′, l, p(l···)) to S(m,n).\nThis parsing process is efficient since we do not need to search through the entire tree. As shown in Figure 3 and Algorithm 1, the best label sentence l is returned when the probability of termination is larger than any other prefix probabilities. As long as the prefix probability is computed correctly, it is guaranteed to return the best solution.\nThe original Earley parser is a special case of the generalized Earley parser. Intuitively, for any input sentence to Earley parser, we can always convert it to one-hot vectors and apply the proposed algorithm. On the other hand, the original Earley parser cannot be applied to segmented one-hot vectors since the labels are often grammatically incorrect. Hence we have the following proposition.\nProposition 1. Earley parser is a special case of the generalized Earley parser. Proof. Let L(G) denote the language of grammar G, h(·) denote a one-to-one mapping from a label to a one-hot vector. L(G) is the input space for Earley parser. ∀ l ∈ L(G), the generalized Earley parser accepts h(l) as input. Therefore the proposition follows.\nHere we emphasize two important distinctions of our method to traditional probabilistic parsers with prefix probabilities. i) In traditional parsers, the prefix probability is the probability of a string being a prefix of some strings generated by a grammar (top-down grammar prior). In our case, the parser computes the bottom-up data likelihood. It is straightforward to extend this to a posterior. ii) Traditional parsers only maintain a parse tree, while our algorithm maintains both a parse tree and a prefix tree. The introduction of the prefix tree into the parser enables us to efficiently search in the grammar according to a desired heuristic."
  }, {
    "heading": "4.1. Parsing Probability Formulation",
    "text": "The parsing probability p(l|x0:T ) is computed in a dynamic programming fashion. Let k be the last label in l. For t = 0, the probability is initialized by:\np(l|x0) = { yk0 l contains only k 0 otherwise\n(1)\nLet l− be the label sentence obtained by removing the last label k from the label sentence l. For t > 0, the last frame t must be classified as k. The previous frames can be labeled as either l or l−. Then we have:\np(l|x0:t) = ykt (p(l|x0:t−1) + p(l−|x0:t−1)) (2)\nAlgorithm 1 Generalized Earley Parser Initialize: S(0, 0) = {(γ → ·R, 0, 0, , 1.0)}. q = priorityQueue() q.push(1.0, (0, 0, , S(0, 0))) {Push to queue with prob 1.0 and index (0, 0)} while (m,n, l−, currentSet) = q.pop() do\nfor s = (r, i, j, l, p(l···)) ∈ currentSet do if p(l) > p(l∗): l∗ = l if r is (A→ α ·Bβ) then {prediction}\nfor each (B → γ) in grammar g do r′ = (B → ·γ) s′ = (r′,m, n, l, p(l···)) S(m,n).add(s′)\nend for else if r is (A→ α · aβ) then {scanning} r′ = (A→ αa · β) m′ = m+ 1, n′ = |S(m+ 1)| − 1 s′ = (r′, i, j, l + a, p((l + a)···)) S(m′, n′).add(s′) q.push(p((l + a)···), (m\n′, n′, S(m′, n′))) else if r is (B → γ·) then {completion}\nfor each ((A→ α ·Bβ), i′, j′) in S(i, j) do r′ = (A→ αB · β) s′ = (r′, i′, j′, l, p(l···)) S(m,n).add(s′)\nend for end if\nend for if p(l−) > p(l) for all un-expanded l: return l−\nend while return l∗\nIt is worth mentioning that when ykt is wrongly given as 0, the dynamic programming process will have trouble correcting the mistake. Even if p(l−|x0:t−1) is high, the probability p(l|x0:t) will be 0. Fortunately, since the softmax function is usually adopted to compute y, ykt will not be 0 and the solution will be kept for consideration.\nThen we compute the prefix probability p(l···|x0:T ) based on p(l−|x0:t). For l to be the prefix, the transition from l− to l can happen at any frame t ∈ {0, · · · , T}. Once the label k is observed (the transition happens), l becomes the prefix and the rest frames can be labeled arbitrarily. Hence the probability of l being the prefix is:\np(l···|x0:T ) = p(l|x0) + T∑ t=1 ykt p(l −|x0:t−1) (3)\nIn practice, the probability p(l|x0:t) decreases exponentially as t increases and will soon lead to numeric underflow. To avoid this, the probabilities need to be computed in log space. The time complexity of computing the probabilities is O(T ) for each sentence l because p(l−|x0:t) are cached. The worst case complexity of the entire parsing is O(T |G|)."
  }, {
    "heading": "4.2. Segmentation and Labeling",
    "text": "The generalized Earley parser gives us the best grammatically correct label sentence l to explain the sequence data, which takes all possible segmentations into consideration. Therefore the probability p(l|x0:T ) is the summation of probabilities of all possible segmentations. Let p(l|y0:e) be the probability of the best segmentation based on the classifier output y for sentence l. We perform a maximization over different segmentations by dynamic programming to find the best segmentation:\np(l|y0:e) = max b<e p(l−|y0:b) e∏ t=b ykt (4)\nwhere e is the time frame that l ends and b is the time frame that l− ends. The best segmentation can be obtained by backtracing the above probability. Similar to the previous probabilities, this probability needs to be computed in log space as well. The time complexity of the segmentation and labeling is O(T 2)."
  }, {
    "heading": "4.3. Future Label Prediction",
    "text": "Given the parsing result l, we make grammar-based predictions for the next label z to be observed. The predictions are naturally obtained by the predict operation in the generalized Earley parser.\nTo predict the next possible symbols at current position (m,n), we search through the states S(m,n) of the form (X → α · zβ, i, j, l, p(l···)), where the first symbol z after the current position is a terminal node. The predictions Σ are then given by the set of all possible z:\nΣ = {z : ∃s ∈ S(m,n), s = (X → α · zβ, i, j, l, p(l···))} (5)\nThe probability of each prediction is then given by the parsing likelihood of the sentence constructed by appending the predicted label z to the current sentence l. Assuming that the best prediction corresponds to the best parsing result, the goal is to find the best prediction z∗ that maximizes the following conditional probability as parsing likelihood:\nz∗ = argmax z∈Σ p(z, l|G) (6)\nFor a grammatically complete sentence u, the parsing likelihood is simply the Viterbi likelihood (Viterbi, 1967) given by the probabilistic context-free grammar. For an incomplete sentence l of length |l|, the parsing likelihood is given by the sum of all the grammatically possible sentences:\np(l|G) = ∑\nu1:|l|=l\np(u|G) (7)\nwhere u1:|l| denotes the first |l|words of a complete sentence u, and p(u|G) is the Viterbi likelihood of u."
  }, {
    "heading": "4.4. Maximum Likelihood Estimation for Prediction",
    "text": "We are interested in finding the best grammar and classifier that give us the most accurate predictions based on the generalized Earley parser. Let G be the grammar, f be the classifier, and D be the set of training examples. The training set consists of pairs of complete or partial data sequence x and the corresponding label sequence y for all the frames in x. By merging consecutive labels in y that are the same, we can obtain partial label sentences l and predicted labels z. Hence we have D = {(x,y, l, z)}. The best grammar G∗ and the best classifier f∗ together minimizes the prediction loss:\nG∗, f∗ = argmin G,f Lpred(G, f) (8)\nwhere the prediction loss is given by the negative log likelihood of the predictions over the entire training set:\nLpred(G, f) = − ∑\n(x,z)∈D\nlog(p(z|x))\n= − ∑\n(x,l,z)∈D (log(p(z|l, G))︸ ︷︷ ︸ grammar + log(p(l|x))︸ ︷︷ ︸ classifier )\n(9) Given the intermediate variable l, the loss is decomposed into two parts that correspond to the induced grammar and the trained classifier, respectively. Let u ∈ {l} be the complete label sentences in the training set (i.e., the label sentence for a complete sequence x). The best grammar maximizes the following probability:∏\n(z,l)∈D\np(z|l, G) = ∏\n(z,l)∈D\np(z, l|G) p(l|G) = ∏ u∈D p(u|G)\n(10) where denominators p(l|G) are canceled by the previous numerator p(z, l−|G), and only the likelihood of the complete sentences remain. Therefore inducing the best grammar that gives us the most accurate future prediction is equivalent to the maximum likelihood estimation (MLE) of the grammar for complete sentences in the dataset. This finding lets us to turn the problem (induce the grammar that gives the best future prediction) into a standard grammar induction problem, which can be solved by existing algorithms, e.g., (Solan et al., 2005) and (Tu et al., 2013).\nThe best classifier minimizes the second term of Eq. 9:\nf∗ = argmin f − ∑ (x,l,z)∈D log(p(l|x)\n≈ argmin f − ∑ (x,y)∈D ∑ k yk log(ŷk) (11)\nwhere p(l|x) can be maximized by the CTC loss (Graves et al., 2006). In practice, it can be substituted by the commonly adopted cross entropy loss for efficiency. Therefore we can directly apply generalized Earley parser to outputs of general detectors/classifiers for parsing and prediction."
  }, {
    "heading": "5. Related Work",
    "text": "Future activity prediction. This is a relatively new but important domain in computer vision. (Ziebart et al., 2009; Gupta et al., 2009; Yuen & Torralba, 2010; Ryoo, 2011; Kitani et al., 2012; Kuderer et al., 2012; Wang et al., 2012; Pei et al., 2013; Walker et al., 2014; Vu et al., 2014; Li & Fu, 2014; Wei et al., 2016; Holtzen et al., 2016; Alahi et al., 2016; Xie et al., 2018; Rhinehart & Kitani, 2017; Ma et al., 2017) predict human trajectories/actions in various settings including complex indoor/outdoor scenes. Koppula, Gupta and Saxena (KGS) (Koppula et al., 2013) proposed a model incorporating object affordances that detects and predicts human activities. Koppula et al. (Koppula & Saxena, 2016) later proposed an anticipatory temporal conditional random field to model the spatial-temporal relations. Qi et al. (Qi et al., 2017) proposed a spatial-temporal And-Or graph (STAOG) for activity prediction.\nHierarchical/grammar models. Pei et al. (Pei et al., 2013) unsupervisedly learned a temporal grammar for video parsing. Pirsiavash et al. (Pirsiavash & Ramanan, 2014) proposed a segmental grammar to parse videos. Holtzen et al. (Holtzen et al., 2016) inferred human intents by a hierarchical model. The ST-AOG (Qi et al., 2017) is also a type of grammar model. Grammar-based methods show effectiveness on tasks that have compositional structures.\nHowever, previous grammar-based algorithms take symbolic inputs like the traditional language parsers. This seriously limits the applicability of these algorithms. Additionally, the parser does not provide guidance for either training the classifiers or segmenting the sequences. They also lack a good approach to handle grammatically incorrect label sentences. For example, (Qi et al., 2017) finds in the training corpus the closest sentence to the recognized sentence and applies the language parser afterward. In our case, the proposed parsing algorithm takes sequence data of raw signals as input and generates the label sentence as well as the parse tree. All parsed label sentences are grammatically correct, and a learning objective is formulated for the classifier."
  }, {
    "heading": "6. Human Activity Detection and Prediction",
    "text": "We evaluate our method on the task of human activity detection and prediction. We present and discuss our experiment results on two datasets, CAD-120 (Koppula et al., 2013) and Watch-n-Patch (Wu et al., 2015), for comparisons with state-of-the-art methods and evaluation of the robustness of our approach. CAD-120 is the dataset that most existing prediction algorithms are evaluated on. It contains videos of daily activities that are long sequences of sub-activities. Watch-n-Patch is a daily activity dataset that features forgotten actions. Results show that our method significantly outperforms the other methods for activity prediction."
  }, {
    "heading": "6.1. Grammar Induction",
    "text": "In both experiments, we used a modified version of the ADIOS (automatic distillation of structure) (Solan et al., 2005) grammar induction algorithm to learn the event grammar. The algorithm learns the production rules by generating significant patterns and equivalent classes. The significant patterns are selected according to a context-sensitive criterion defined regarding local flow quantities in the graph: two probabilities are defined over a search path. One is the right-moving ratio of fan-through (through-going flux of path) to fan-in (incoming flux of paths). The other one, similarly, is the left-going ratio of fan-through to fan-in. The criterion is described in detail in (Solan et al., 2005).\nThe algorithm starts by loading the corpus of activity onto a graph whose vertices are sub-activities, augmented by two special symbols, begin and end. Each event sample is represented by a separate path over the graph. Then it generates candidate patterns by traversing a different search path. At each iteration, it tests the statistical significance of each subpath to find significant patterns. The algorithm then finds the equivalent classes that are interchangeable. At the end of the iteration, the significant pattern is added to the graph as a new node, replacing the subpaths it subsumes. We favor shorter patterns in our implementation."
  }, {
    "heading": "6.2. Experiment on CAD-120 Dataset",
    "text": "Dataset The CAD-120 dataset is a standard dataset for human activity prediction. It contains 120 RGB-D videos of four different subjects performing 10 high-level activities, where each high-level activity was performed three times with different objects. It includes a total of 61,585 total video frames. Each video is a sequence of sub-activities involving 10 different sub-activity labels. The videos vary from subject to subject regarding the lengths and orders of the sub-activities as well as the way they executed the task.\nEvaluation metrics We use the following metrics to evaluate and compare the algorithms. 1) Frame-wise detection accuracy of sub-activity labels for all frames. 2) Future 3s online prediction accuracy. We compute the frame-wise accuracy of prediction of the sub-activity labels of the future 3s (using the frame rate of 14Hz as reported in (Koppula et al., 2013)). The predictions are made online at each frame t, i.e., the algorithms only sees frame 0 to t and predicts the labels of frame t + 1 to t + δt. 3) Future segment online prediction accuracy. At each frame t, the algorithm predicts the sub-activity label of the next video segment.\nWe consider the overall micro accuracy (P/R), macro precision, macro recall and macro F1 score for all evaluation metrics. Micro accuracy is the percentage of correctly classified labels. Macro precision and recall are the average of precision and recall respectively for all classes.\nComparative methods We compare our method with four state-of-the-art methods for activity prediction: 1. KGS (Koppula et al., 2013): a Markov random field model where the nodes represent objects and sub-activities, and the edges represent the spatial-temporal relationships. Future frames are predicted based on the transition probabilities given the inferred label of the last frame. 2. Anticipatory temporal CRF (ATCRF) (Koppula & Saxena, 2016): an anticipatory temporal conditional random field that models the spatial-temporal relations through object affordances. Future frames are predicted by sampling a spatial-temporal graph. 3. ST-AOG + Earley (Qi et al., 2017): a spatial-temporal And-Or graph (ST-AOG) that uses a symbolic context-free grammar to model activities. This sets up a comparison between our proposed method and methods that use traditional probabilistic parsers. Since traditional parsers operate on symbolic data, extra efforts need to be done first to extract symbols from sequence data. In this comparative method, the videos are first segmented and labeled by classifiers; the predictions are then made by the original Earley parser. 4. Bidirectional LSTM (Bi-LSTM): a two-layer Bi-LSTM with a hidden size of 256. For the detection task, the output for each frame input is the sub-activity label. For the future 3s prediction, the LSTM is trained to output the label for frame t+3s for an input frame at time t. For future segment prediction, it outputs the label of the next segment for an input frame. All three tasks use the same training schemes. 5. Bi-LSTM + generalized Earley parser (our method): the proposed generalized Earley parser applied to the classifier output of the above detection Bi-LSTM. The predictions for the next segments are made according to Section 4.3. The lengths of unobserved segments are sampled from a log-normal distribution for the future 3s prediction.\nFeature extraction All methods in the experiment use the same publicly available features from KGS (Koppula et al., 2013). These features include the human skeleton features and human-object interaction features for each frame. The human skeleton features are location and distance features (relative to the subjects head location) for all upper-skeleton joints of a subject. The human-object features are spatialtemporal, containing the distances between object centroids and skeleton joint locations as well as the temporal changes.\nExperiment results We follow the convention in KGS (Koppula et al., 2013) to train on three subjects and test on a new subject with a 4-fold validation. The results for the three evaluation metrics are summarized in Table 1, Table 2 and Table 3, respectively. Our method outperforms the comparative methods on all three tasks. Specifically, the generalized Earley parser on top of a Bi-LSTM performs better than ST-AOG, while ST-AOG outperforms the Bi-LSTM. More discussions are highlighted in Section 6.4."
  }, {
    "heading": "6.3. Experiment on Watch-n-Patch Dataset",
    "text": "Dataset Watch-n-Patch is an RGB-D dataset that features forgotten actions. For example, a subject might fetch milk from a fridge, pour milk, and leave. The typical action “putting the milk back into the fridge” is forgotten. The dataset contains 458 videos with a total length of about 230 minutes, in which people forgot actions in 222 videos. Each video in the dataset contains 2-7 actions interacted with different objects. 7 subjects are asked to perform daily activities in 8 offices and 5 kitchens with complex backgrounds. It consists of 21 types of fully annotated actions (10 in the office, 11 in the kitchen) interacted with 23 types of objects.\nFeature extraction We extract the same features as described in (Wu et al., 2015) for all methods. Similar to the previous experiment, the features are composed of skeleton features and human-object interaction features extracted from RGB-D images. The skeleton features include angles between connected parts, the change of joint positions and angles from previous frames. Each frame is segmented into super-pixels, and foreground masks are detected. We extract features from the image segments with more than 50% in the foreground mask and within a distance to the human hand joints in both 3D points and 2D pixels. Six kernel descriptors (Wu et al., 2014) are extracted from these image segments: gradient, color, local binary pattern, depth gradient, spin, surface normals, and KPCA/self-similarity.\nExperiment results We use the same evaluation metrics as the previous experiment and compare our method to STAOG (Qi et al., 2017) and Bi-LSTM. We use the train/test split in (Wu et al., 2015). The results for the three evaluation metrics are summarized in Table 4, Table 5 and Table 6, respectively. Our method slightly improves the detection re-\nsults over the Bi-LSTM outputs, and outperforms the other methods on both prediction tasks. In general, the algorithms make better predictions on CAD-120, since Watch-n-Patch features forgotten actions and the behaviors are more unpredictable. More details are discussed in Section 6.4."
  }, {
    "heading": "6.4. Discussion",
    "text": "How different are the classifier outputs and the final outputs for detection? Figure 4 shows some qualitative examples of the ground truth segmentations and results given by different methods. The segmentation results show that the refined outputs are similar with the classifier outputs since the confidence given by the classifiers are often very high.\nHow does the generalized Earley parser refine the classifier detection outputs? When the classifier outputs violate the grammar, two types of refinements occur: i) correction and deletion of wrong labels as shown in 4a; ii) insertion of new labels as shown in 4b. The inserted segments are usually very short to accommodate both the grammar and the classifier outputs. Most boundaries of the refined results are well aligned with the classifier outputs.\nWhy do we use two metrics for future prediction? The future 3s prediction is a standard evaluation criterion set up by KGS and ATCRF. However, this criterion does not tell how well the algorithm predicts the next segment label. i) At any time frame, part of the future 3s involves the current sub-activity for most of the times. ii) If the predicted length of the current sub-activity is inaccurate, the framewise inaccuracy drops proportionally, even when the future segment label prediction is accurate. Therefore we also compare against the future segment label prediction because it is invariant to variations in activity lengths.\nHow well does the generalized Earley parser perform for activity detection and prediction? From the results we can see that it slightly improves over the classifier outputs\nfor detection, but significantly outperforms the classifier for predictions. The modifications on classifier outputs (corrections and insertions in Figure 4) are minor but important to make the sentences grammatically correct, thus high-quality predictions can be made.\nHow useful is the grammar for activity modeling? From Table 2, Table 3, Table 5 and Table 6 we can see that both STAOG and generalized Earley parser outperforms Bi-LSTM for prediction. Prediction algorithms need to give different outputs for similar inputs based on the observation history. Hence the non-Markovian property of grammars is useful for activity modeling, especially for future prediction.\nHow robust is the generalized Earley parser? Comparing Table 3 and Table 6 we can see that there is a performance drop when the action sequences are more unpredictable (in the Watch-n-Patch dataset). But it is capable of improving over the noisy classifier inputs and significantly outperforms the other methods. It is also robust in the sense that it can always find the best sentence in a given language that best explains the classifier outputs."
  }, {
    "heading": "7. Conclusions",
    "text": "We proposed a generalized Earley parser for parsing sequence data according to symbolic grammars. Detections and predictions are made by the parser given the probabilistic outputs from any classifier. We are optimistic about and interested in further applications of the generalized Earley parser. In general, we believe this is a step towards the goal of integrating the connectionist and symbolic approaches."
  }, {
    "heading": "Acknowledgment",
    "text": "The authors thank Professor Ying Nian Wu from UCLA Statistics Department for helpful comments on this work. The work reported herein is supported by DARPA XAI N66001-17-2-4029, ONR MURI N00014-16-1-2007, and N66001-17-2-3602."
  }],
  "year": 2018,
  "references": [{
    "title": "Social lstm: Human trajectory prediction in crowded spaces",
    "authors": ["A. Alahi", "K. Goel", "V. Ramanathan", "A. Robicquet", "L. FeiFei", "S. Savarese"],
    "year": 2016
  }, {
    "title": "An efficient context-free parsing algorithm",
    "authors": ["J. Earley"],
    "venue": "Communications of the ACM,",
    "year": 1970
  }, {
    "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
    "authors": ["A. Graves", "S. Fernández", "F. Gomez", "J. Schmidhuber"],
    "venue": "In ICML,",
    "year": 2006
  }, {
    "title": "Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos",
    "authors": ["A. Gupta", "P. Srinivasan", "J. Shi", "L.S. Davis"],
    "venue": "In CVPR,",
    "year": 2009
  }, {
    "title": "Inferring human intent from video by sampling hierarchical plans",
    "authors": ["S. Holtzen", "Y. Zhao", "T. Gao", "J.B. Tenenbaum", "Zhu", "S.-C"],
    "year": 2016
  }, {
    "title": "Anticipating human activities using object affordances for reactive robotic response",
    "authors": ["H.S. Koppula", "A. Saxena"],
    "year": 2016
  }, {
    "title": "Learning human activities and object affordances from rgb-d videos",
    "authors": ["H.S. Koppula", "R. Gupta", "A. Saxena"],
    "venue": "IJRR,",
    "year": 2013
  }, {
    "title": "Feature-based prediction of trajectories for socially compliant navigation",
    "authors": ["M. Kuderer", "H. Kretzschmar", "C. Sprunk", "W. Burgard"],
    "venue": "In RSS,",
    "year": 2012
  }, {
    "title": "Prediction of human activity by discovering temporal sequence patterns",
    "authors": ["K. Li", "Y. Fu"],
    "year": 2014
  }, {
    "title": "Forecasting interactive dynamics of pedestrians with fictitious play",
    "authors": ["Ma", "W.-C", "Huang", "D.-A", "N. Lee", "K.M. Kitani"],
    "year": 2017
  }, {
    "title": "Video event parsing and learning with goal and intent prediction",
    "authors": ["M. Pei", "Z. Si", "B. Yao", "Zhu", "S.-C"],
    "year": 2013
  }, {
    "title": "Parsing videos of actions with segmental grammars",
    "authors": ["H. Pirsiavash", "D. Ramanan"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Predicting human activities using stochastic grammar",
    "authors": ["S. Qi", "S. Huang", "P. Wei", "Zhu", "S.-C"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "First-person activity forecasting with online inverse reinforcement learning",
    "authors": ["N. Rhinehart", "K.M. Kitani"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos",
    "authors": ["M.S. Ryoo"],
    "venue": "In ICCV,",
    "year": 2011
  }, {
    "title": "Unsupervised structure learning of stochastic and-or grammars",
    "authors": ["K. Tu", "M. Pavlovskaia", "Zhu", "S.-C"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
    "authors": ["A. Viterbi"],
    "venue": "IEEE transactions on Information Theory,",
    "year": 1967
  }, {
    "title": "Predicting actions from static scenes",
    "authors": ["Vu", "T.-H", "C. Olsson", "I. Laptev", "A. Oliva", "J. Sivic"],
    "venue": "In ECCV,",
    "year": 2014
  }, {
    "title": "Patch to the future: Unsupervised visual prediction",
    "authors": ["J. Walker", "A. Gupta", "M. Hebert"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Probabilistic modeling of human movements for intention",
    "authors": ["Z. Wang", "M.P. Deisenroth", "H.B. Amor", "D. Vogt", "B. Schölkopf", "J. Peters"],
    "year": 2012
  }, {
    "title": "Modeling 4d human-object interactions for joint event segmentation, recognition, and object localization",
    "authors": ["P. Wei", "Y. Zhao", "N. Zheng", "Zhu", "S.-C"],
    "year": 2016
  }, {
    "title": "Hierarchical semantic labeling for task-relevant rgb-d perception",
    "authors": ["C. Wu", "I. Lenz", "A. Saxena"],
    "venue": "In RSS,",
    "year": 2014
  }, {
    "title": "Watch-npatch: Unsupervised understanding of actions and relations",
    "authors": ["C. Wu", "J. Zhang", "S. Savarese", "A. Saxena"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "Modeling and inferring human intents and latent functional objects for trajectory prediction",
    "authors": ["D. Xie", "T. Shu", "S. Todorovic", "Zhu", "S.-C"],
    "year": 2018
  }, {
    "title": "A data-driven approach for event prediction",
    "authors": ["J. Yuen", "A. Torralba"],
    "venue": "In ECCV,",
    "year": 2010
  }, {
    "title": "Planning-based prediction for pedestrians",
    "authors": ["B.D. Ziebart", "N. Ratliff", "G. Gallagher", "C. Mertz", "K. Peterson", "J.A. Bagnell", "M. Hebert", "A.K. Dey", "S. Srinivasa"],
    "venue": "In IROS,",
    "year": 2009
  }],
  "id": "SP:417890cc6d43a3082a6ab2ac64527f8db5b0125b",
  "authors": [{
    "name": "Siyuan Qi",
    "affiliations": []
  }, {
    "name": "Baoxiong Jia",
    "affiliations": []
  }, {
    "name": "Song-Chun Zhu",
    "affiliations": []
  }],
  "abstractText": "Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture nonMarkovian and compositional properties of highlevel semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled. This generalized Earley parser integrates a grammar parser with a classifier to find the optimal segmentation and labels, and makes top-down future predictions. Experiments show that our method significantly outperforms other approaches for future human activity prediction.",
  "title": "Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction"
}