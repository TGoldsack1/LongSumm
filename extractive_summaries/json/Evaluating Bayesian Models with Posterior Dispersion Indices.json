{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Probabilistic modeling is a flexible approach to analyzing structured data. Three steps define the approach. First, we specify a model; this captures our structural assumptions about the data. Then, we infer the hidden structure; this means computing (or approximating) the posterior. Last, we evaluate the model; this helps build better models down the road (Blei, 2014).\nHow do we evaluate models? Decades of reflection have led to deep and varied forays into model checking, comparison, and criticism (Gelman et al., 2013). But a common theme permeates all approaches to model evaluation: the desire to generalize well.\nIn machine learning, we traditionally use two complementary tools: predictive accuracy and cross-validation. Predictive accuracy is the target evaluation metric. Cross-\n1Columbia University, New York City, USA. Correspondence to: Alp Kucukelbir <alp@cs.columbia.edu>.\nProceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nvalidation captures a notion of generalization and justifies holding out data. This simple combination has fueled the development of myriad probabilistic models (Bishop, 2006; Murphy, 2012).\nDoes predictive accuracy tell the whole story? The predictive accuracy of an observation is its per-datapoint likelihood averaged over the posterior. In this sense, predictive accuracy reports a mean value for each datapoint; it ignores how each per-datapoint likelihood varies with respect to the posterior.\nMain idea. We propose to evaluate probabilistic models through the idea of posterior dispersion, analyzing how each datapoint fares in relation to posterior uncertainty around the hidden structure. To capture this, we propose a family of posterior dispersion indices (PDI). These are per-datapoint quantities, each a variance to mean ratio of the datapoint’s likelihood with respect to the posterior. A PDI highlights observations whose likelihoods exhibit the most uncertainty under the posterior.\nConsider a model p.x; / and the likelihood of a datapoint p.xn j /. It depends on some hidden structure that we seek to infer. Since is random, we can view the likelihood of each datapoint as a random variable. Predictive accuracy reports the average likelihood of each xn with respect to the posterior p. jx/. But this ignores how the likelihood changes under the posterior. How can we capture this uncertainty and compare datapoints to each other?\nTo answer this, we appeal to various forms of dispersion, such as the variance of the likelihood under the posterior. We propose a family of dispersion indices in Section 2.2; they have the following form:\nPDI D variance of likelihood under posterior\nmean of likelihood under posterior\nD V jxŒp.xn j /\nE jxŒp.xn j / :\nPDIs compliment predictive accuracy. Here is a mental picture. Consider a nuclear power plant where we monitor the temperature of a pool of water. We train a probabilistic model; the posterior represents our uncertainty around some safe temperature, say 80 degrees. Suppose we receive a high measurement thigh (Figure 1). Its likelihood varies rapidly\nacross plausible posterior values for t . This datapoint is reasonably well modeled, but is sensitive to the posterior. Now imagine the thermostat breaks and we receive a zero measurement tzero. This zero datapoint is poorly modeled.\nBoth datapoints may have similar predictive accuracy values under the model. (See Section 2.3 for how this can occur.) But the high measurement is different than the zero measurement. A PDI differentiates these measurements by considering not only their predictive accuracy scores, but also how their per-datapoint likelihoods vary with respect to the posterior.\nSection 3 presents an empirical study of model mismatch in three real-world examples: voting preferences, supermarket shopping, and population genetics. In each case, a PDI provides insight beyond predictive accuracy and highlights potential directions for improvement.\nRelated work. This paper relates to a constellation of ideas from statistical model criticism.\nThe first connection is to analysis of variance: PDI bears similarity to ANOVA, which is a frequentist approach to evaluating explanatory variables in linear regression (Davison, 2003). Gelman et al. (1996) cemented the idea of studying predictive accuracy of probabilistic models at the data level; Vehtari et al. (2012) and Betancourt (2015) give up-to-date overviews of these ideas. PDIs add to this body of work by considering the variance of each datapoint in context of its predictive accuracy.\nThe second connection is to model comparison. Recent research, such as Gelman et al. (2014), Vehtari et al. (2014) and Piironen & Vehtari (2015), explore the relationship between cross validation and information criteria, such as the widely applicable information criterion (WAIC) (Watanabe, 2010; Vehtari et al., 2016). WAIC offers an intuitive connection to cross validation (Vehtari & Lampinen, 2002; Watanabe, 2015); we draw inspiration from it in this paper too. However our focus is on evaluation at the datapoint level, not at the dataset level. In this sense, PDIs and information criteria are complimentary tools.\nThe third connection is to a body of work from the ma-\nchine learning community. Gretton et al. (2007) and Chwialkowski et al. (2016) developed effective kernel-based methods for independence and goodness-of-fit tests. Recently, Lloyd & Ghahramani (2015) visualized smooth regions of data space that the model fails to explain. In contrast, we focus directly on the datapoints, which can live in high-dimensional spaces that may be difficult to visualize.\nA final connection is to scoring rules. While the literature on scoring rules originally focused on probability forecasting (Winkler, 1996; Dawid, 2006), recent advances draw new connections to decision theory, divergence measures, and information theory (Dawid & Musio, 2014). We discuss how PDIs fit into this picture in the conclusion."
  }, {
    "heading": "2. Posterior Dispersion Indices",
    "text": "A posterior dispersion index (PDI) highlights datapoints that exhibit the most uncertainty with respect to the hidden structure of a model. Here is the road map for this section. A small case study illustrates how a PDI gives more insight beyond predictive accuracy. Definitions, theory, and another small analysis give further insight; a straightforward algorithm leads into the empirical study."
  }, {
    "heading": "2.1. 44% outliers?",
    "text": "Hayden (2005) considers the number of days each U.S. president stayed in office; he submits that 44% of presidents may be outliers. Figure 2 plots the data. One-term presidents stay in office for around 1460 days; two-term presidents approximately double that. Yet many presidents deviate from this “two bump” trend.\nA reasonable model for such data is a mixture of negative binomial distributions.1 Consider the parameterization of the negative binomial with mean and variance C 2= . Posit gamma priors on the (non-negative) latent variables. Set the prior on to match the mean and variance of the data (Robbins, 1964). Choose an uninformative prior on . Three mixtures make sense: two for the typical trends and one for the rest.\n1 A Poisson likelihood is too underdispersed.\nThe complete model is\np. / D Dirichlet. I ˛ D .1; 1; 1//\np. / D 3Y kD1 Gam. k I mean and variance\nmatched to that of data/\np. / D 3Y kD1 Gam. k I a D 1; ˇ D 0:01/\np.xn j ; ; / D 3X kD1 kNB.xn I k ; k/:\nFitting this model gives posterior mean estimates b D .1461; 2896; 1578/ with correspondingb D .470; 509; 1.3/. The first two clusters describe the two typical term durations, while the third (highlighted in bold red) is a dispersed negative binomial that attempts to describe the rest of the data.\nWe compute a PDI (defined in Section 2.2) and the posterior predictive density for each president p.xn jx/. Figure 3 compares both metrics and sorts the presidents according to the PDI.\nSome presidents are clear outliers: Harrison [31: natural death], Roosevelt [4452: four terms], and Garfield [199: assassinated]. However, there are three presidents with worse predictive accuracy than Harrison: Coolidge, Nixon, and Johnson. A PDI differentiates Harrison from these three because his likelihood is varying rapidly with respect to the dispersed negative binomial cluster.\nThis PDI also calls attention to McKinley [1655: assassinated] and Arthur [1260: succeeded Garfield], because they are close to the sharp negative binomial cluster at 1460 but not close enough to have good predictive accuracy. They are datapoints whose likelihoods are rapidly changing with respect to a peaked posterior, like\nthe high measurement in the nuclear plant example in the introduction.\nThis case study suggests that predictive probability does not tell the entire story. Datapoints can exhibit low predictive accuracy in different ways. We now turn to a formal definition of PDIs."
  }, {
    "heading": "2.2. Definitions",
    "text": "Let x D fxngN1 be a dataset with N observations. A probabilistic model has two parts. The first is the likelihood, p.xn j /. It relates an observation xn to hidden patterns described by a set latent random variables . If the observations are independent and identically distributed, the likelihood of the dataset factorizes as p.x j / D Q n p.xn j /.\nThe second is the prior density, p. /. It captures the structure we expect from the hidden patterns. Combining the likelihood and the prior gives the joint density p.x; / D p.x j /p. /. Conditioning on observed data gives the posterior density, p. jx/.\nTreat the likelihood of each datapoint as a function of . To evaluate the model, we analyze how each datapoint fares in relation to the posterior density. Consider these expectations and variances with respect to the posterior,\n.n/ D E jxŒp.xn j /\nlog.n/ D E jxŒlogp.xn j /\n2.n/ D V jxŒp.xn j /\n2log.n/ D V jxŒlogp.xn j / :\n(1)\nEach includes the likelihood in a slightly different fashion. The first expectation is a familiar object: .n/ is the posterior predictive density.\nA PDI is a ratio of these variances to expectations. Taking the ratio calibrates this quantity for each datapoint. Recall the mental picture from the introduction. The variance of\nthe likelihood under the posterior highlights potential model mismatch; dividing by the mean calibrates this spread to its predictive accuracy.\nCalibration puts all datapoints on a common scale. Imagine a binary classification problem where each datapoint yn lives in f0; 1g. The variances of the zero measurements may be numerically quite different than the one measurements; considering the mean renders these values comparable.\nRelated ratios also appear in classical statistics under a variety of forms, such as indices of dispersion (Hoel, 1943), coefficients of variation (Koopmans et al., 1964), or the Fano factor (Fano, 1947). They all quantify dispersion of samples from a random process. PDIs extend these ideas by connecting to the posterior density of a probabilistic model.\nIn this paper, we study a particular PDI, called the widely applicable posterior dispersion index (WAPDI),\nWAPDI.n/ D 2log.n/\nlog .n/ :\nIts form and name comes from the widely applicable information criterion WAIC D 1\nN\nP n log .n/ C 2 log.n/:\nWAIC measures generalization error; it asymptotically equates to leave-one-one cross validation (Watanabe, 2010;\n2015). WAPDI has two advantages; both are practically motivated. First, we hope the reader is computing an estimate of generalization error. Gelman et al. (2014) suggests WAIC because it is easy to compute and designed for common machine learning models (Watanabe, 2010). Computing WAIC gives WAPDI for free. Second, the variance is a second-order moment calculation; using the log likelihood gives numerical stability to the computation. (More on computation in Section 2.4.)\nWAPDI compares the variance of the log likelihood to the log posterior predictive. This gives insight into how the likelihood of a datapoint fares under the posterior distribution of the hidden patterns. We now study this in more detail."
  }, {
    "heading": "2.3. Intuition: not all predictive probabilities are created equal",
    "text": "The posterior predictive density is an expectation, E jxŒp.xnew j / D R p.xnew j /p. jx/ d : Expectations are integrals: areas under a curve. Different likelihood and posterior combinations can lead to similar integrals.\nA toy model illustrates this. Consider a gamma likelihood with fixed shape, and place a gamma prior on the rate. The\nmodel is\np.ˇ/ D Gam.ˇ I a0 D 1; b0 D 1/;\np.x jˇ/ D NY nD1 Gam.xn I a D 5; ˇ/;\nwhich gives the posterior p.ˇ jx/ D Gam.ˇ I a D a0 C 5N; b D b0 C P n xn/.\nNow simulate a dataset of size N D 10 with ˇ D 1; the data have mean a=ˇ D 5. Now consider an outlier at 15. We can find another x value with essentially the same predictive accuracy\nlogp.x1 D 0:727 jx/ D 5:633433; logp.x2 D 15 jx/ D 5:633428:\nYet their WAPDI values differ by an order of magnitude\nWAPDI.x1 D 0:727/ D 0:067;\nWAPDI.x2 D 15/ D 0:229:\nIn this case, WAPDI highlights x2 D 15 as a more severe outlier than x1 D 0:727, even though they have the same predictive accuracy. What does that mean? Figure 4 depicts the difference.\nThe following lemma explains how WAPDI measures this effect. (Proof in Appendix B.)\nLemma 1 If logp.xn j / is at least twice differentiable and the posterior p. jx/ has finite first and second moments, then a second-order Taylor approximation gives\nWAPDI.n/\nlogp0.xn jE jxŒ / 2 V jxŒ\nlog E jxŒp.xn j / : (2)\nCorollary 1 WAPDI highlights datapoints whose likelihood is rapidly changing at the posterior mean estimate of the latent variables. (V jxŒ is constant across n.)\nLooking back at Figure 4, the likelihood p.x2 D 15 jˇ/ indeed changes rapidly under the posterior. WAPDI reports the ratio of this rate-of-change to the area under the curve. In this specific example, only the numerator matters, since the denominator is effectively the same for both datapoints.\nCorollary 2 Equation (2) is zero if and only if the posterior mean coincides with the maximum likelihood estimate of for datapoint xn. (V jxŒ is positive for finite N .)\nFor most interesting models, we do not expect such a coincidence. However, in practice, we find WAPDI to be close to zero for datapoints that match the model well. With that, we now turn to computation."
  }, {
    "heading": "2.4. Computation",
    "text": "Calculating WAPDI is straightforward. The only requirement are samples from the posterior. This is precisely the output of an Markov chain Monte Carlo (MCMC) sampling algorithm. (We used the no-U-turn sampler (Hoffman & Gelman, 2014) for the analyses above.) Other inference procedures, such as variational inference, give an analytic approximation to the posterior (Jordan et al., 1999; Blei et al., 2016). Drawing samples from an approximate posterior also works. (We use this approach for the empirical study in Section 3.)\nEquipped with S samples from the posterior, Monte Carlo integration (Robert & Casella, 1999) gives unbiased estimates of the quantities in Equation (1). The variance of these estimates decreases as O.1=S/; we assume S is sufficiently large to cover the posterior (Gelman et al., 2014). We default to S D 1000 in our experiments. Algorithm 1 summarizes these steps.\nAlgorithm 1: Calculating WAPDI.\nInput: Data x D fxngN1 , model p.x; /.\nOutput: WAPDI for each datapoint xn.\nDraw S samples f gS1 from posterior (approximation) p. jx/.\nfor n in 1, 2, . . . , N do\nEstimate log .n/; 2log.n/ from samples f g S 1 .\nStore and return\nWAPDI.n/ D 2log.n/\nlog .n/ :\nend"
  }, {
    "heading": "3. Experimental Study",
    "text": "We now explore three real data examples using modern machine learning models: voting preferences, supermarket shopping, and population genetics."
  }, {
    "heading": "3.1. Voting preferences: a hierarchical logistic regression model",
    "text": "In 1988, CBS conducted a U.S. nation-wide survey of voting preferences. Citizens indicated their preference towards the Democratic or Republican presidential candidate. Each individual also declared their gender, age, race, education level, and the state they live in; 11 566 individuals participated.\nGelman & Hill (2006) study this data through a hierarchical logistic regression model. They begin by modeling gender, race, and state; the state variable has a hierarchical prior. This model is easy to fit using automatic differentiation variational inference (ADVI) within Stan (Kucukelbir et al., 2015; Carpenter et al., 2015). (Model and inference details in Appendix C.)\nVOTE R R R R R R R R R R SEX F F F F F F F F F F\nRACE B B B B B B B B B B STATE WA WA NY WI NY NY NY NY MA MA\nRACE W W W W W W W W W W STATE WY WY WY WY WY WY WY DC DC NV\nTable 2. Worst WAPDI values.\nTables 1 and 2 show the individuals with the lowest predictive accuracy and WAPDI. The nation-wide trend predicts that females (F) who identify as black (B) have a strong preference to vote democratic (D); predictive accuracy identifies the few individuals who defy this trend. However, there is not much to do with this information; the model identifies a nation-wide trend that correctly describes most female black voters. In contrast, WAPDI points to parts of the dataset that the model fails to describe; these are datapoints that we might try to explain better with a revised model.\nMost of the individuals with poor WAPDI live in Wyoming, the District of Columbia, and Nevada. We focus on Wyoming and Nevada. The average WAPDI for Wyoming and Nevada are 0:057 and 0:041; these are baselines that we seek to improve. (The closer to zero, the better.)\nConsider expanding the model by modeling age. Introducing age into the model with a hierarchical prior reveals that older voters tend to vote Republican. This helps explain Wyoming voters; their average WAPDI improves from 0:057 to 0:04; however Nevada’s average WAPDI remains unchanged. This means that Nevada’s voters may not follow the national age-dependent trend. Now consider removing age and introducing education in a similar way. Education helps explain voters from both states; the average WAPDI for Wyoming and Nevada improve to 0:041 and 0:029.\nWAPDI thus captures interesting datapoints beyond what predictive accuracy reports. As expected, predictive accuracy still highlights the same female black voters in both expanded models; WAPDI illustrates a deeper way to evaluate this model."
  }, {
    "heading": "3.2. Supermarket shopping: a hierarchical Poisson factorization model",
    "text": "Market research firm IRi hosts an anonymized dataset of customer shopping behavior at U.S. supermarkets (Bronnenberg et al., 2008). The dataset tracks 136 584 “checkout” sessions; each session contains a basket of purchased items. An inventory of 7 903 items range across categories such as carbonated beverages, toiletries, and yogurt.\nWhat items do customers tend to purchase together? To study this, consider a hierarchical Poisson factorization (HPF) model (Gopalan et al., 2015). HPF models the quantities of items purchased in each session with a Poisson likelihood; its rate is an inner product between a session’s preferences s and the item attributes ˇ. Hierarchical priors on and ˇ simultaneously promote sparsity, while accounting for variation in session size and item popularity. Some sessions contain only a few items; others are large purchases. (Model and inference details in Appendix D.)\nA 20-dimensional HPF model discovers intuitive trends. A\nfew stand out. Snack-craving customers like to buy Doritos tortilla chips along with Lay’s potato chips. Morning birds typically pair Cheerios cereal with 2% skim milk. Yoplait fans tend to purchase many different flavors at the same time. Tables 3 and 4 show the top five items in two of these twenty trends.\nSessions where a customer purchases many items from different categories have low predictive accuracy. This makes sense as these customers do not exhibit a trend; mathematically, there is no combination of item attributes ˇ that explain buying items from disparate categories. For example, the session with the lowest predictive accuracy contains 117 items ranging from coffee to hot dogs.\nWAPDI highlights a different aspect of the HPF model. Sessions with poor WAPDI contain similar items but exhibit many purchases of a single item. Table 5 shows an example of a session where a customer purchased 14 blackberry Yoplait yogurts, but only a few of the other flavors.\nThis indicates that the Poisson likelihood assumption may not be flexible enough to model customer purchasing behavior. Perhaps a negative binomial likelihood could model this kind of spiked activity better. Another option might be to keep the Poisson likelihood but increase the hierarchy of the probabilistic model; this approach may identify item attributes that explain such purchases. In either case,\nWAPDI identifies a valuable aspect of the data that the HPF struggles to capture: sessions with spiked activity. This is a concrete direction for model revision."
  }, {
    "heading": "3.3. Population genetics: a mixed membership model",
    "text": "Do all people who live nearby have similar genomes? Not necessarily. Population genetics considers how individuals exhibit ancestral patterns of mutations. Begin with N individuals and L locations on the genome. For each location, report whether each individual reveals a mutation. This gives an (N L) dataset x where xnl 2 f0; 1; 2; 3g. (We assume two specific forms of mutation; 3 encodes a missing observation.)\nMixed membership models offer a way to study this (Pritchard et al., 2000). Assume K ancestral populations ; these are the mutation probabilities of each location. Each individual mixes these populations with weights ; these are the mixing proportions. Place a beta prior on the mutation probabilities and a Dirichlet prior on the mixing proportions.\nWe study a dataset of N D 324 individuals from four geographic locations and focus on L D 13 928 locations on the genome. Figure 5 shows how these individuals mix K D 3 ancestral populations. (Data, model, and inference details in Appendix E.)\nWAPDI reveals three interesting patterns of mismatch here. First, individuals with poor WAPDI values have many missing observations; the worst 10% of WAPDI have 1 344 missing values, in contrast to 563 for the lowest 10% of predictive scores. We may consider directly modeling these missing observations.\nSecond, ASW has two individuals with poor WAPDI; their mutation patterns are outliers within the group. While the average individual reveals 272 mutations away from the median genome, these individuals show 410 and 383 mutations. This points to potential mishaps while gathering or pre-processing the data.\nLast, MEX exhibits good predictive accuracy, yet poor WAPDI values compared to other groups. Based on predictive accuracy, we may happily accept these patterns. Yet WAPDI highlights a serious issue with the inferred populations. The blue and red populations are almost twice as correlated across genes (0:58) as the other possible combinations (0:24 and 0:2). In other words, the blue and red populations represent similar patterns of mutations at the same locations. These populations, as they stand, are not necessarily interpretable. Revising the model to penalize correlation may be a direction worth pursuing."
  }, {
    "heading": "4. Discussion",
    "text": "A posterior dispersion index (PDI) identifies informative forms of model mismatch that compliments predictive accuracy. By highlighting which datapoints exhibit the most uncertainty under the posterior, a PDI offers a new perspective into evaluating probabilistic models. Here, we show how one particular PDI, the widely applicable posterior dispersion index (WAPDI), reveals promising directions for model improvement across a range of models and applications.\nThe choice of WAPDI is practically motivated; it comes for free as part of the calculation of WAIC. This highlights how PDIs are complimentary to tools such as predictive accuracy, cross validation, and information criteria. While PDIs and predictive accuracy assess model mismatch at the datapoint level, cross validation and information criteria indicate model mismatch at the dataset level.\nPDIs provide a relative comparison of datapoints with respect to a model. Can PDIs be thresholded to identify “problematic” datapoints? One approach in this direction draws inspiration from posterior predictive checks (PPCs) (Rubin, 1984; Gelman et al., 1996). A PPC works by hallucinating data from the posterior predictive and comparing properties of the hallucinated data to the observed dataset. Comparing PDI values in this way could lead to a meaningful way of thresholding PDIs.\nThere are several research directions. One is to extend the\nnotion of a PDI to non-exchangeable data. Another is to leverage the bootstrap to extend this idea beyond probabilistic models. Computationally, ideas from importance sampling could reduce the variance of PDI computations for very high dimensional models.\nA promising direction is to study PDIs under the viewpoint of scoring rules (Dawid & Musio, 2014). Understanding the decision theoretic properties of a PDI as a loss function could lead to alternative objectives for inference.\nFinally, we end on a reminder that PDIs are simply another tool in the statistician’s toolbox. The design and criticism of probabilistic models is still a careful, manual craft. While good tools can help, an overarching obstacle remains to pursue their adoption by practitioners. To this end, making these tools easier to use and more automatic can only help."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Dustin Tran, Maja Rudolph, David Mimno, Aki Vehtari, Josh Vogelstein, and Rajesh Ranganath for their insightful comments. This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C4032, and the Alfred P. Sloan Foundation."
  }],
  "year": 2017,
  "references": [{
    "title": "A unified treatment of predictive model comparison",
    "authors": ["Betancourt", "Michael"],
    "venue": "arXiv preprint arXiv:1506.02273,",
    "year": 2015
  }, {
    "title": "Pattern Recognition and Machine Learning",
    "authors": ["Bishop", "Christopher M"],
    "year": 2006
  }, {
    "title": "Build, compute, critique, repeat: Data analysis with latent variable models",
    "authors": ["Blei", "David M"],
    "venue": "Annual Review of Statistics and Its Application,",
    "year": 2014
  }, {
    "title": "Variational inference: a review for statisticians",
    "authors": ["Blei", "David M", "Kucukelbir", "Alp", "McAuliffe", "Jon D"],
    "venue": "arXiv preprint arXiv:1601.00670,",
    "year": 2016
  }, {
    "title": "Stan: a probabilistic programming language",
    "authors": ["Carpenter", "Bob", "Gelman", "Andrew", "Hoffman", "Matt", "Lee", "Daniel", "Goodrich", "Ben", "Betancourt", "Michael", "Brubaker", "Marcus A", "Guo", "Jiqiang", "Li", "Peter", "Riddell", "Allen"],
    "venue": "Journal of Statistical Software,",
    "year": 2015
  }, {
    "title": "A kernel test of goodness of fit",
    "authors": ["Chwialkowski", "Kacper", "Strathmann", "Heiko", "Gretton", "Arthur"],
    "venue": "arXiv preprint arXiv:1602.02964,",
    "year": 2016
  }, {
    "title": "Statistical models",
    "authors": ["Davison", "Anthony Christopher"],
    "year": 2003
  }, {
    "title": "Probability Forecasting",
    "authors": ["A.P. Dawid"],
    "year": 2006
  }, {
    "title": "Theory and applications of proper scoring",
    "authors": ["Dawid", "Alexander Philip", "Musio", "Monica"],
    "venue": "rules. Metron,",
    "year": 2014
  }, {
    "title": "Ionization yield of radiations II",
    "authors": ["Fano", "Ugo"],
    "venue": "Physical Review,",
    "year": 1947
  }, {
    "title": "Data analysis using regression and multilevel/hierarchical models",
    "authors": ["Gelman", "Andrew", "Hill", "Jennifer"],
    "year": 2006
  }, {
    "title": "Posterior predictive assessment of model fitness via realized discrepancies",
    "authors": ["Gelman", "Andrew", "Meng", "Xiao-Li", "Stern", "Hal"],
    "venue": "Statistica Sinica,",
    "year": 1996
  }, {
    "title": "Understanding predictive information criteria for Bayesian models",
    "authors": ["Gelman", "Andrew", "Hwang", "Jessica", "Vehtari", "Aki"],
    "venue": "Statistics and Computing,",
    "year": 2014
  }, {
    "title": "Scalable recommendation with hierarchical Poisson factorization",
    "authors": ["Gopalan", "Prem", "Hofman", "Jake M", "Blei", "David M"],
    "year": 2015
  }, {
    "title": "A kernel statistical test of independence",
    "authors": ["Gretton", "Arthur", "Fukumizu", "Kenji", "Teo", "Choon H", "Song", "Le", "Schölkopf", "Bernhard", "Smola", "Alex J"],
    "year": 2007
  }, {
    "title": "A dataset that is 44% outliers",
    "authors": ["Hayden", "Robert W"],
    "venue": "J Stat Educ,",
    "year": 2005
  }, {
    "title": "On indices of dispersion",
    "authors": ["Hoel", "Paul G"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1943
  }, {
    "title": "The No-U-Turn sampler",
    "authors": ["Hoffman", "Matthew D", "Gelman", "Andrew"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "An introduction to variational methods for graphical models",
    "authors": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"],
    "venue": "Machine Learning,",
    "year": 1999
  }, {
    "title": "Confidence intervals for the coefficient of variation for the normal and log normal distributions",
    "authors": ["Koopmans", "Lambert H", "Owen", "Donald B", "Rosenblatt", "JI"],
    "year": 1964
  }, {
    "title": "Automatic variational inference in Stan",
    "authors": ["Kucukelbir", "Alp", "Ranganath", "Rajesh", "Gelman", "Andrew", "Blei", "David M"],
    "year": 2015
  }, {
    "title": "Statistical model criticism using kernel two sample tests",
    "authors": ["Lloyd", "James R", "Ghahramani", "Zoubin"],
    "year": 2015
  }, {
    "title": "Machine Learning: a Probabilistic Perspective",
    "authors": ["Murphy", "Kevin P"],
    "year": 2012
  }, {
    "title": "Comparison of Bayesian predictive methods for model selection",
    "authors": ["Piironen", "Juho", "Vehtari", "Aki"],
    "venue": "Statistics and Computing,",
    "year": 2015
  }, {
    "title": "Inference of population structure using multilocus genotype",
    "authors": ["Pritchard", "Jonathan K", "Stephens", "Matthew", "Donnelly", "Peter"],
    "venue": "data. Genetics,",
    "year": 2000
  }, {
    "title": "The empirical Bayes approach to statistical decision problems",
    "authors": ["Robbins", "Herbert"],
    "venue": "Annals of Mathematical Statistics,",
    "year": 1964
  }, {
    "title": "Monte Carlo statistical methods",
    "authors": ["Robert", "Christian P", "Casella", "George"],
    "year": 1999
  }, {
    "title": "Bayesianly justifiable and relevant frequency calculations for the applied statistician",
    "authors": ["Rubin", "Donald B"],
    "venue": "The Annals of Statistics,",
    "year": 1984
  }, {
    "title": "Bayesian model assessment and comparison using cross-validation predictive densities",
    "authors": ["Vehtari", "Aki", "Lampinen", "Jouko"],
    "venue": "Neural Computation,",
    "year": 2002
  }, {
    "title": "A survey of Bayesian predictive methods for model assessment, selection and comparison",
    "authors": ["Vehtari", "Aki", "Ojanen", "Janne"],
    "venue": "Statistics Surveys,",
    "year": 2012
  }, {
    "title": "Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models",
    "authors": ["Vehtari", "Aki", "Tolvanen", "Ville", "Mononen", "Tommi", "Winther", "Ole"],
    "venue": "arXiv preprint arXiv:1412.7461,",
    "year": 2014
  }, {
    "title": "Practical Bayesian model evaluation using leave-one-out crossvalidation and WAIC",
    "authors": ["Vehtari", "Aki", "Gelman", "Andrew", "Gabry", "Jonah"],
    "venue": "arXiv preprint arXiv:1507.04544,",
    "year": 2016
  }, {
    "title": "Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory",
    "authors": ["Watanabe", "Sumio"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Bayesian cross validation and WAIC for predictive prior design in regular asymptotic theory",
    "authors": ["Watanabe", "Sumio"],
    "venue": "arXiv preprint arXiv:1503.07970,",
    "year": 2015
  }, {
    "title": "Scoring rules and the evaluation",
    "authors": ["Winkler", "Robert L"],
    "venue": "of probabilities. Test,",
    "year": 1996
  }],
  "id": "SP:015ebee24d2846bc14443ead9849e9d35cac574b",
  "authors": [{
    "name": "Alp Kucukelbir",
    "affiliations": []
  }, {
    "name": "Yixin Wang",
    "affiliations": []
  }, {
    "name": "David M. Blei",
    "affiliations": []
  }],
  "abstractText": "Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. This highlights datapoints the model struggles to explain and provides complimentary insight to datapoints with low predictive accuracy. We present a family of posterior dispersion indices (PDI) that capture this idea. We show how a PDI identifies patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.",
  "title": "Evaluating Bayesian Models with Posterior Dispersion Indices"
}