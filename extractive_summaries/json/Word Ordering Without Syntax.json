{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2319–2324, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "We address the task of recovering the original word order of a shuffled sentence, referred to as bag generation (Brown et al., 1990), shake-and-bake generation (Brew, 1992), or more recently, linearization, as standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation models (Zhang and Clark, 2011; Liu et al., 2015; Liu and Zhang, 2015; Zhang and Clark, 2015). The predominant argument of the more recent works is that jointly recovering explicit syntactic structure is crucial for determining the correct word order of the original sentence. As such, these methods either generate or rely on given parse structure to reproduce the order.\nIndependently, Elman (1990) explored linearization in his seminal work on recurrent neural networks. Elman judged the capacity of early recurrent neural networks via, in part, the network’s ability to predict word order in simple sentences. He notes,\nThe order of words in sentences reflects a number of constraints. . . Syntactic structure, selective restrictions, subcategorization, and discourse considerations are among the many factors which join together to fix the order in which words occur. . . [T]here is an abstract structure which underlies the surface strings and it is this structure which provides a more insightful basis for understanding the constraints on word order. . . . It is, therefore, an interesting question to ask whether a network can learn any aspects of that underlying abstract structure (Elman, 1990).\nRecently, recurrent neural networks have reemerged as a powerful tool for learning the latent structure of language. In particular, work on long short-term memory (LSTM) networks for language modeling has provided improvements in perplexity.\nWe revisit Elman’s question by applying LSTMs to the word-ordering task, without any explicit syntactic modeling. We find that language models are in general effective for linearization relative to existing syntactic approaches, with LSTMs in particular outperforming the state-of-the-art by 11.5 BLEU points, with further gains observed when training with additional text and decoding with larger beams."
  }, {
    "heading": "2 Background: Linearization",
    "text": "The task of linearization is to recover the original order of a shuffled sentence. We assume a vocabulary V and are given a sequence of out-of-order phrases x1, . . . , xN , with xn ∈ V+ for 1 ≤ n ≤ N . Define M as the total number of tokens (i.e., the sum of the lengths of the phrases). We consider two varieties of the task: (1) WORDS, where each xn consists of a single word and M = N , and (2) WORDS+BNPS,\n2319\nwhere base noun phrases (noun phrases not containing inner noun phrases) are also provided and M ≥ N . The second has become a standard formulation in recent literature.\nGiven input x, we define the output set Y to be all possible permutations over the N elements of x, where ŷ ∈ Y is the permutation generating the true order. We aim to find ŷ, or a permutation close to it. We produce a linearization by (approximately) optimizing a learned scoring function f over the set of permutations, y∗ = arg maxy∈Y f(x, y)."
  }, {
    "heading": "3 Related Work: Syntactic Linearization",
    "text": "Recent approaches to linearization have been based on reconstructing the syntactic structure to produce the word order. Let Z represent all projective dependency parse trees over M words. The objective is to find y∗, z∗ = arg maxy∈Y,z∈Z f(x, y, z) where f is now over both the syntactic structure and the linearization. The current state of the art on the Penn Treebank (PTB) (Marcus et al., 1993), without external data, of Liu et al. (2015) uses a transitionbased parser with beam search to construct a sentence and a parse tree. The scoring function is a linear model f(x, y) = θ>Φ(x, y, z) and is trained with an early update structured perceptron to match both a given order and syntactic tree. The feature function Φ includes features on the syntactic tree. This work improves upon past work which used best-first search over a similar objective (Zhang and Clark, 2011).\nIn follow-up work, Liu and Zhang (2015) argue that syntactic models yield improvements over pure surface n-gram models for the WORDS+BNPS case. This result holds particularly on longer sentences and even when the syntactic trees used in training are of low quality. The n-gram decoder of this work utilizes a single beam, discarding the probabilities of internal, non-boundary words in the BNPs when comparing hypotheses. We revisit this comparison between syntactic models and surface-level models, utilizing a surface-level decoder with heuristic future costs and an alternative approach for scoring partial hypotheses for the WORDS+BNPS case.\nAdditional previous work has also explored ngram models for the word ordering task. The work of de Gispert et al. (2014) demonstrates improve-\nments over the earlier syntactic model of Zhang et al. (2012) by applying an n-gram language model over the space of word permutations restricted to concatenations of phrases seen in a large corpus. Horvat and Byrne (2014) models the search for the highest probability permutation of words under an n-gram model as a Travelling Salesman Problem; however, direct comparisons to existing works are not provided."
  }, {
    "heading": "4 LM-Based Linearization",
    "text": "In contrast to the recent syntax-based approaches, we use an LM directly for word ordering. We consider two types of language models: an ngram model and a long short-term memory network (Hochreiter and Schmidhuber, 1997). For the purpose of this work, we define a common abstraction for both models. Let h ∈ H be the current state of the model, with h0 as the initial state. Upon seeing a word wi ∈ V , the LM advances to a new state hi = δ(wi,hi−1). At any time, the LM can be queried to produce an estimate of the probability of the next word q(wi,hi−1) ≈ p(wi | w1, . . . , wi−1). For n-gram language models, H, δ, and q can naturally be defined respectively as the state space, transition model, and edge costs of a finite-state machine.\nLSTMs are a type of recurrent neural network (RNN) that are conducive to learning long-distance dependencies through the use of an internal memory cell. Existing work with LSTMs has generated stateof-the-art results in language modeling (Zaremba et al., 2014), along with a variety of other NLP tasks.\nIn our notation we define H as the hidden states and cell states of a multi-layer LSTM, δ as the LSTM update function, and q as a final affine transformation and softmax given as q(∗,hi−1; θq) = softmax(Wh\n(L) i−1 + b) where h (L) i−1 is the top hid-\nden layer and θq = (W , b) are parameters. We direct readers to the work of Graves (2013) for a full description of the LSTM update.\nFor both models, we simply define the scoring function as\nf(x, y) =\nN∑\nn=1\nlog p(xy(n) | xy(1), . . . , xy(n−1))\nwhere the phrase probabilities are calculated wordby-word by our language model.\nAlgorithm 1 LM beam-search word ordering 1: procedure ORDER(x1 . . . xN , K, g) 2: B0 ← 〈(〈〉, {1, . . . , N}, 0,h0)〉 3: for m = 0, . . . ,M − 1 do 4: for k = 1, . . . , |Bm| do 5: (y,R, s,h)← B(k)m 6: for i ∈ R do 7: (s′,h′)← (s,h) 8: for word w in phrase xi do 9: s′ ← s′ + log q(w,h′) 10: h′ ← δ(w,h′) 11: j ← m+ |xi| 12: Bj ← Bj + (y + xi,R− i, s′,h′) 13: keep top-K of Bj by f(x, y) + g(R) 14: return BM\nSearching over all permutations Y is intractable, so we instead follow past work on linearization (Liu et al., 2015) and LSTM generation (Sutskever et al., 2014) in adapting beam search for our generation step. Our work differs from the beam search approach for the WORDS+BNPS case of previous work in that we maintain multiple beams, as in stack decoding for phrase-based machine translation (Koehn, 2010), allowing us to incorporate the probabilities of internal, non-boundary words in the BNPs. Additionally, for both WORDS and WORDS+BNPS, we also include an estimate of future cost in order to improve search accuracy.\nBeam search maintains M + 1 beams, B0, . . . , BM , each containing at most the topK partial hypotheses of that length. A partial hypothesis is a 4-tuple (y,R, s,h), where y is a partial ordering,R is the set of remaining indices to be ordered, s is the score of the partial linearization f(x, y), and h is the current LM state. Each step consists of expanding all next possible phrases and adding the next hypothesis to a later beam. The full beam search is given in Algorithm 1.\nAs part of the beam search scoring function we also include a future cost g, an estimate of the score contribution of the remaining elements in R. Together, f(x, y) + g(R) gives a noisy estimate of the total score, which is used to determine the K best elements in the beam. In our experiments we use a very simple unigram future cost estimate, g(R) =∑\ni∈R ∑ w∈xi log p(w)."
  }, {
    "heading": "5 Experiments",
    "text": "Setup Experiments are on PTB with sections 2- 21 as training, 22 as validation, and 23 as test1. We utilize two UNK types, one for initial uppercase tokens and one for all other low-frequency tokens; end sentence tokens; and start/end tokens, which are treated as words, to mark BNPs for the WORDS+BNPS task. We also use a special symbol to replace tokens that contain at least one numeric character. We otherwise train with punctuation and the original case of each token, resulting in a vocabulary containing around 16K types from around 1M training tokens.\nFor experiments marked GW we augment the PTB with a subset of the Annotated Gigaword corpus (Napoles et al., 2012). We follow Liu and Zhang (2015) and train on a sample of 900k Agence France-Presse sentences combined with the full PTB training set. The GW models benefit from both additional data and a larger vocabulary of around 25K types, which reduces unknowns in the validation and test sets.\nWe compare the models of Liu et al. (2015)\n1In practice, the results in Liu et al. (2015) and Liu and Zhang (2015) use section 0 instead of 22 for validation (author correspondence).\n(known as ZGEN), a 5-gram LM using Kneser-Ney smoothing (NGRAM)2, and an LSTM. We experiment on the WORDS and WORDS+BNPS tasks, and we also experiment with including future costs (g), the Gigaword data (GW), and varying beam size. We retrain ZGEN using publicly available code3 to replicate published results.\nThe LSTM model is similar in size and architecture to the medium LSTM setup of Zaremba et al. (2014)4. Our implementation uses the Torch5 framework and is publicly available6.\nWe compare the performance of the models using the BLEU metric (Papineni et al., 2002). In generation if there are multiple tokens of identical UNK type, we randomly replace each with possible unused tokens in the original source before calculating BLEU. For comparison purposes, we use the BLEU script distributed with the publicly available ZGEN code.\nResults Our main results are shown in Table 1. On the WORDS+BNPS task the NGRAM-64 model scores nearly 5 points higher than the syntax-based model ZGEN-64. The LSTM-64 then surpasses\n2We use the KenLM Language Model Toolkit (https:// kheafield.com/code/kenlm/).\n3https://github.com/SUTDNLP/ZGen 4We hypothesize that additional gains are possible via a\nlarger model and model averaging, ceteris paribus. 5http://torch.ch 6https://github.com/allenschmaltz/word_ ordering\nNGRAM-64 by more than 5 BLEU points. Differences on the WORDS task are smaller, but show a similar pattern. Incorporating Gigaword further increases the result another 2 points. Notably, the NGRAM model outperforms the combined result of ZGEN-64+LM+GW+POS from Liu and Zhang (2015), which uses a 4-gram model trained on Gigaword. We believe this is because the combined ZGEN model incorporates the n-gram scores as discretized indicator features instead of using the probability directly.7 A beam of 512 yields a further improvement at the cost of search time.\nTo further explore the impact of search accuracy, Table 2 shows the results of various models with beam widths ranging from 1 (greedy search) to 512, and also with and without future costs g. We see that for the better models there is a steady increase in accuracy even with large beams, indicating that search errors are made even with relatively large beams.\n7In work of Liu and Zhang (2015), with the given decoder, N-grams only yielded a small further improvement over the syntactic models when discretized versions of the LM probabilities were incorporated as indicator features in the syntactic models.\nOne proposed advantage of syntax in linearization models is that it can better capture long-distance relationships. Figure 1 shows results by sentence length and distortion, which is defined as the absolute difference between a token’s index position in y∗ and ŷ, normalized by M . The LSTM model exhibits consistently better performance than existing syntax models across sentence lengths and generates fewer long-range distortions than the ZGEN model.\nFinally, Table 3 compares the syntactic fluency of the output. As a lightweight test, we parse the output with the Yara Parser (Rasooli and Tetreault, 2015) and compare the unlabeled attachment scores (UAS) to the trees produced by the syntactic system. We first align the gold head to each output token. (In cases where the alignment is not one-to-one, we randomly sample among the possibilities.) The models with no knowledge of syntax are able to recover a higher proportion of gold arcs."
  }, {
    "heading": "6 Conclusion",
    "text": "Strong surface-level language models recover word order more accurately than the models trained with explicit syntactic annotations appearing in a recent series of papers. This has implications for the utility of costly syntactic annotations in generation models, for both high- and low- resource languages and domains."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Yue Zhang and Jiangming Liu for assistance in using ZGen, as well as verification of the\ntask setup for a valid comparison. Jiangming Liu also assisted in pointing out a discrepancy in the implementation of an earlier version of our NGRAM decoder, the resolution of which improved BLEU performance."
  }],
  "year": 2016,
  "references": [{
    "title": "Letting the cat out of the bag: Generation for shake-and-bake MT",
    "authors": ["Chris Brew"],
    "venue": "In Proceedings of the 14th Conference on Computational Linguistics - Volume 2,",
    "year": 1992
  }, {
    "title": "A statistical approach to machine translation",
    "authors": ["Brown et al.1990] Peter F Brown", "John Cocke", "Stephen A Della Pietra", "Vincent J Della Pietra", "Fredrick Jelinek", "John D Lafferty", "Robert L Mercer", "Paul S Roossin"],
    "year": 1990
  }, {
    "title": "Word ordering with phrasebased grammars",
    "authors": ["Marcus Tomalin", "Bill Byrne"],
    "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,",
    "year": 2014
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L. Elman"],
    "venue": "Cognitive Science,",
    "year": 1990
  }, {
    "title": "Generating sequences with recurrent neural networks. CoRR, abs/1308.0850",
    "authors": ["Alex Graves"],
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber"],
    "venue": "Neural Comput.,",
    "year": 1997
  }, {
    "title": "A graph-based approach to string regeneration",
    "authors": ["Horvat", "Byrne2014] Matic Horvat", "William Byrne"],
    "venue": "In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Lin-",
    "year": 2014
  }, {
    "title": "Statistical Machine Translation",
    "authors": ["Philipp Koehn"],
    "year": 2010
  }, {
    "title": "An empirical comparison between n-gram and syntactic language models for word ordering",
    "authors": ["Liu", "Zhang2015] Jiangming Liu", "Yue Zhang"],
    "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2015
  }, {
    "title": "Transition-based syntactic linearization",
    "authors": ["Liu et al.2015] Yijia Liu", "Yue Zhang", "Wanxiang Che", "Bing Qin"],
    "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
    "year": 2015
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"],
    "year": 1993
  }, {
    "title": "Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX",
    "authors": ["Matthew Gormley", "Benjamin Van Durme"],
    "year": 2012
  }, {
    "title": "Bleu: A method for automatic evaluation of machine translation",
    "authors": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"],
    "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,",
    "year": 2002
  }, {
    "title": "Yara parser: A fast and accurate dependency parser. CoRR, abs/1503.06733",
    "authors": ["Rasooli", "Joel R. Tetreault"],
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Oriol Vinyals", "Quoc VV Le"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Recurrent neural network regularization. CoRR, abs/1409.2329",
    "authors": ["Ilya Sutskever", "Oriol Vinyals"],
    "year": 2014
  }, {
    "title": "Syntax-based grammaticality improvement using ccg and guided search",
    "authors": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"],
    "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
    "year": 2011
  }, {
    "title": "Discriminative syntax-based word ordering for text generation",
    "authors": ["Zhang", "Clark2015] Yue Zhang", "Stephen Clark"],
    "venue": "Comput. Linguist.,",
    "year": 2015
  }, {
    "title": "Syntax-based word ordering incorporating a large-scale language model",
    "authors": ["Zhang et al.2012] Yue Zhang", "Graeme Blackwood", "Stephen Clark"],
    "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,",
    "year": 2012
  }],
  "id": "SP:4542dd2b47e3a74c22d214c7f295ca220f01b901",
  "authors": [{
    "name": "Allen Schmaltz",
    "affiliations": []
  }, {
    "name": "Alexander M. Rush",
    "affiliations": []
  }, {
    "name": "Stuart M. Shieber",
    "affiliations": []
  }],
  "abstractText": "Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is even more effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time.",
  "title": "Word Ordering Without Syntax"
}