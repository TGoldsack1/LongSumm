{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 112–122 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1011"
  }, {
    "heading": "1 Introduction",
    "text": "Discourse modes, also known as rhetorical modes, describe the purpose and conventions of the main kinds of language based communication.Most common discourse modes include narration, description, exposition and argument. A typical text would make use of all the modes, although in a given one there will often be a main mode. Despite their importance in writing composition and assessment (Braddock et al., 1963), there is relatively little work on analyzing discourse modes based on computational models. We aim to contribute for automatic discourse mode identification and its application on writing assessment.\nThe use of discourse modes is important in writing composition, because they relate to several aspects that would influence the quality of a text.\nFirst, discourse modes reflect the organization of a text. Natural language texts consist of sen-\ntences which form a unified whole and make up the discourse (Clark et al., 2013). Recognizing the structure of text organization is a key part for discourse analysis. Meurer (2002) points that discourse modes stand for unity as they constitute general patterns of language organization strategically used by the writer. Smith (2003) also proposes to study discourse passages from a linguistic view of point through discourse modes. The organization of a text can be realized by segmenting text into passages according to the set of discourse modes that are used to indicate the functional relationship between the several parts of the text. For example, the writer can present major events through narration, provide details with description and establish ideas with argument. The combination and interaction of various discourse modes make an organized unified text.\nSecond, discourse modes have rhetorical significance. Discourse modes are closely related to rhetoric (Connors, 1981; Brooks and Warren, 1958), which offers a principle for learning how to express material in the best way. Discourse modes have different preferences on expressive styles. Narration mainly controls story progression by introducing and connecting events; exposition is to instruct or explain so that the language should be precise and informative; argument is used to convince or persuade through logical and inspiring statements; description attempts to bring detailed observations of people and scenery, which is related to the writing of figurative language; the way to express emotions may relate to the use of rhetorical devices and poetic language. Discourse modes reflect the variety of expressive styles. The flexible use of various discourse modes should be important evidence of language proficiency.\nAccording to the above thought, we propose the discourse mode identification task. In particular, we make the following contributions:\n112\n• We build a corpus of narrative essays written by Chinese students in native language. Sentence level discourse modes are annotated with acceptable inter-annotator agreement. Corpus analysis reveals the characteristics of discourse modes in several aspects, including discourse mode distribution, co-occurrence and transition patterns.\n• We describe a multi-label neural sequence labeling approach for discourse mode identification so that the co-occurrence and transition preferences can be captured. Experimental results show that discourse modes can be identified with an average F1-score of 0.7, indicating that automatic discourse mode identification is feasible.\n• We demonstrate the effectiveness of taking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Discourse Analysis",
    "text": "Discourse analysis is an important subfield of natural language processing (Webber et al., 2011). Discourse is expected to be both cohesive and coherent. Many principles are proposed for discourse analysis, such as coherence relations (Hobbs, 1979; Mann and Thompson, 1988), the centering theory for local coherence (Grosz et al., 1995) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015).\nThis paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language,\nsince Chinese doesn’t have grammatical tense (Xue and Zhang, 2014) and sentence components are often omitted. This increases the difficulties for situation entity type based discourse mode identification. In this paper, we investigate an end-to-end approach to directly model discourse modes without the necessity of identifying situation entity types first."
  }, {
    "heading": "2.2 Automatic Writing Assessment",
    "text": "Automatic writing assessment is an important application of natural language processing. The task aims to let computers have the ability to appreciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation.\nAES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016).\nWriting assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account."
  }, {
    "heading": "2.3 Neural Sequence Modeling",
    "text": "A main challenge of discourse analysis is hard to collect large scale data due to its complexity, which may lead to data sparseness problem. Recently, neural networks become popular for natural language processing (Bengio et al., 2003; Collobert et al., 2011). One of the advantages is the ability of automatic representation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem\nand enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work describes a hierarchical neural architecture with multiple label outputs for modeling the discourse mode sequence of sentences."
  }, {
    "heading": "3 Discourse Mode Annotation",
    "text": "We are interested in the use of discourse modes in writing composition. This section describes the discourse modes we are going to study, an annotated corpus of student essays and what we learn from corpus analysis."
  }, {
    "heading": "3.1 Discourse Modes",
    "text": "Discourse modes have several taxonomies in the literature. Four basic discourse modes are narration, description, exposition and argument in English composition and rhetoric (Bain, 1890). Smith (2003) proposes five modes for studying discourse passages: narrative, description, report, information and argument. In Chinese composition, discourse modes are categorized into narration, description, exposition, argument and emotion expressing (Zhu, 1983).\nThese taxonomies are similar. Their elements can mostly find corresponding ones in other taxonomies literally or conceptually, e.g., exposition mode has similar functions to information mode. Emotion expressing that is to express the writer’s emotions is relatively special. It can be realized by expressing directly or through lyrical writing with beautiful and poetic language. It is also related to appeal to emotion, which is a method for argumentation by the manipulation of the recipient’s emotions in classical rhetoric (Aristotle and Kennedy, 2006). Proper emotion expressing can touch the hearts of the readers and improve the expressiveness of writing. Therefore, considering it as an independent mode is also reasonable.\nWe cope with essays written in Chinese in this work so that we follow the Chinese convention with five discourse modes. Emotion expressing\nis added on the basis of four recognized discourse modes and Smith’s report mode is viewed as a subtype of description mode: dialogue description.\nIn summary, we study the following discourse modes:\n• Narration introduces an event or series of events into the universe of discourse. The events are temporally related according to narrative time. E.g., Last year, we drove to San Francisco along the State Route 1 (SR 1).\n• Exposition has a function to explain or instruct. It provides background information in narrative context. The information presented should be general and (expected to be) well accepted truth. E.g., SR 1 is a major north-south state highway that runs along most of the Pacific coastline of the U.S.\n• Description re-creates, invents, or vividly show what things are like according to the five senses so that the reader can picture that which is being described. E.g., Along SR 1 are stunning rugged coastline, coastal forests and cliffs, beautiful little towns and some of the West coast’s most amazing nature.\n• Argument makes a point of view and proves its validity towards a topic in order to convince or persuade the reader. E.g., Point Arena Lighthouse is a must see along SR 1, in my opinion.\n• Emotion expressing1 presents the writer’s emotions, usually in a subjective, personal and lyrical way, to involve the reader to experience the same situations and to be touched. E.g., I really love the ocean, the coastline and all the amazing scenery along the route. When could I come back again?\nThe distinction between discourse modes is expected to be clarified conceptually by considering their different communication purposes. However, there would still be specific ambiguous and vague cases. We will describe the data annotation and corpus analysis in the following parts.\n1In some cases, we use emotion for short."
  }, {
    "heading": "3.2 Data Annotation",
    "text": "Discourse modes are almost never found in a pure form but are embedded one within another to help the writer achieve the purpose, but the emphasis varies in different types of writing. We focus on narrative essays. A good narrative composition must properly manipulate multiple discourse modes to make it vivid and impressive.\nThe corpus has 415 narrative essays written by high school students in their native Chinese language.The average number of sentences is 32 and the average length is 670 words.\nWe invited two high school teachers to annotate discourse modes at sentence level, expecting their background help for annotation. A detail manual was discussed before annotation.\nWe notice that discourse modes can mix in the same sentence. Therefore, the annotation standard allows that one sentence can have multiple modes. But we require that every sentence should have a dominant mode. The annotators should try to think in the writer’s perspective and guess the writer’s main purpose of writing the sentence in order to decide the dominant mode.\nAmong the discourse modes, description can be applied in various situations. We focus on the following description types: portrait, appearance, action, dialogue, psychological, environment and detail description. If a sentence has any type of description, it would be assigned a description label."
  }, {
    "heading": "3.3 Corpus Analysis",
    "text": "We conducted corpus analysis on the annotated data to gain observations on several aspects. Inter-Annotator Agreement: 50 essays were independently annotated by two annotators. We evaluate the inter-annotator agreement on the dom-\ninant mode. The two annotators’ annotations are used as the golden answer and prediction respectively. We compute the precision, recall and F1score for each discourse mode separately to measure the inter-annotator agreement. Precision and recall are symmetric for the two annotators.\nThe result of the first round annotation is shown in the INITIAL columns of Table 1. The agreement on argument mode is low, while the agreement on other modes is acceptable. The average F1-score is 0.69. The Cohen’s Kappa (Cohen et al., 1960) is 0.55 over all judgements on the dominant mode.\nThe main disagreement on argument lies in the confusion with emotion expressing. Consider the following sentence:\nFather’s love is the fire that lights the lamp of hope.\nOne annotator thought that it is expressed in an emotional and lyrical way so that the discourse mode should be emotion expressing. The other one thought that it (implicitly) gives a point and should be an argument. Many disagreements happened in cases like this.\nBased on the observations of the first round annotation, we discussed and updated the manual and let the annotators rechecked their annotations. The final result is shown in the FINAL columns of Table 1. The agreement on description decreases. Annotators seem to be more conservative on labeling description as the dominant mode. The overall average F1-score increases to 0.78 and the Cohen’s Kappa is 0.72. This indicates that humans can reach an acceptable agreement on the dominant discourse mode of sentences after training. Discourse mode distribution: After the training phase, the annotators labeled the whole corpus. Figure 1 shows the distribution of dominant\ndiscourse modes. The distribution is imbalanced. Narration, description and emotion expressing are the main discourse modes in narrative essays, while exposition and argument are rare. Co-occurrence: Statistics show that 78% of sentences have only one discourse mode, and 19% have two discourse modes, and 3% have more than two discourse modes.\nTable 2 shows the co-occurrence of discourse modes. The numbers that are in the diagonal represent the distribution of discourse modes of sentences with only one mode. The numbers that are not in the diagonal indicate the co-occurrence of modes in the same sentences. We can see that description tends to co-occur with narration and emotion expressing. Description can provide states that happen together with events and emotion-evoking scenes are often described to elicit a strong emotional response, for example:\nThe bright moon hanging on the distant sky reminds me of my hometown miles away.\nEmotion expressing and argument also co-occur in some cases. It is reasonable, since a successful emotional appeal can enhance the effectiveness of an argument.\nGenerally, these observations are consistent with intuition. Properly combining multiple modes could produce impressive sentences.\nTransition: Table 3 shows the transition matrix between the dominant modes of consecutive sentences within the same paragraphs. All modes tend to transit to themselves except exposition, which is rare and usually brief. This means that discourse modes of adjacent sentences have high correlation. We also see that narration and emotion are more often at the beginning and the end of essays. The above observations indicate that discourse modes have local preferred patterns.\nTo summarize, the implications of corpus analysis include: (1) Manual identification of discourse modes is feasible with an acceptable inter-annotator agreement; (2) The distribution of discourse modes in narrative essays is imbalanced; (3) About 22% sentences have multiple discourse modes; (4) Discourse modes have local transition patterns that consecutive discourse modes have high correlation."
  }, {
    "heading": "4 Discourse Mode Identification based on Neural Sequence Labeling",
    "text": "This section describes the proposed method for discourse mode identification. According to the corpus analysis, sentences often have multiple discourse modes and prefer local transition patterns. Therefore, we view this task as a multi-label sequence labeling problem."
  }, {
    "heading": "4.1 Model",
    "text": "We propose a hierarchical neural sequence labeling model to capture multiple level information. Figure 2(a) shows the basic architecture. We introduce it from the bottom up. Word level embedding layer: We transform words into continuous vectors, word embeddings. Vector representation of words is useful for capturing semantic relatedness. This should be effective in our case, since large amount of training data is not available. It is unrealistic to learn the embedding parameters on limited data so that we just look up embeddings of words from a pre-trained word embedding table. The pre-trained word embeddings were learned with the Word2Vec toolkit (Mikolov et al., 2013) on a domain corpus which consists of about 490,000 student essays. The embeddings are kept unchanged during learning and prediction. Sentence level GRU layer: Each sentence is a sequence of words. We feed the word embeddings into a forward recurrent neural networks. Here,\nwe use the GRU (Cho et al., 2014) as the recurrent unit. The GRU is to make each recurrent unit to adaptively capture dependencies of different time scales. The output of the last time-step is used as the representation of a sentence. Discourse level bidirectional-GRU layer: An essay consists of a sequence of sentences. Accessing information of past and future sentences provides more contextual information for current prediction. Therefore, we use a bidirectional RNN to connect sentences. We use the GRU as the recurrent unit, which is also shown effective on semantic composition of documents for sentiment classification (Tang et al., 2015). The BiGRU represents the concatenation of the hidden states of the forward GRU and the backward GRU units. Multi-Label layer: Since one sentence can have more than one discourse mode, our model allows multiple label outputs. Figure 2(b) details the Mul-Label layer in Figure 2(a). The representation of each sentence after the bidirectional-GRU layer is first fully connected to a hidden layer. The hidden layer output is then fully connected to a five-way output layer, corresponding to five discourse modes. The sigmoid activation function is applied to each way to get the probability that whether corresponding discourse mode should be assigned to the sentence.\nIn the training phase, the probability of any labeled discourse modes is set to 1 and the others are set to 0. In the prediction phase, if the predicted probability of a discourse mode is larger than 0.5, the discourse mode would be assigned."
  }, {
    "heading": "4.1.1 Considering Paragraph Boundaries",
    "text": "Different from NER that processes a single sentence each time, our task processes sequences of sentences in discourse, which are usually\ngrouped by paragraphs to split the whole discourse into several relatively independent segments. Sentences from different paragraphs should have less effect to each other, even though they are adjacent.\nTo capture paragraph boundary information, we insert an empty sentence at the end of every paragraph to indicate a paragraph boundary. The empty sentence is represented by a zero vector and its outputs are set to zeros as well. We expect this modification can better capture position related information."
  }, {
    "heading": "4.2 Implementation Details",
    "text": "We implement the model using the Keras library.2 The models are trained with the binary cross-entropy objective. The optimizer is Adam (Kingma and Ba, 2014). The word embedding dimension is 50. The dimension of the hidden layer in Mul-Label layer is 100. The length of sentences is fixed as 40. All other parameters are set by default parameter values. We adopt early stopping strategy (Caruana et al., 2000) to decide when the training process stops."
  }, {
    "heading": "4.3 Evaluation",
    "text": ""
  }, {
    "heading": "4.3.1 Data",
    "text": "We use 100 essays as the test data. The remaining ones are used as the training data. 10% of the shuffled training data is used for validation."
  }, {
    "heading": "4.3.2 Comparisons",
    "text": "We compare the following systems:\n• SVM: We use bag of ngram (unigram and bigram) features to train a support vector classifier for sentence classification.\n2https://github.com/fchollet/keras/\n• CNN: We implement a convolutional neural network (CNN) based method (Kim, 2014), as it is the state-of-the-art for sentence classification.\n• GRU: We use the sentence level representation in Figure 2(a) for sentence classification.\n• GRU-GRU(GG): This method is introduced in this paper in §4.1, but it doesn’t consider paragraph information.\n• GRU-GRU-SEG (GG-SEG): The model considers paragraph information on the top of GG as introduced in §4.1.1.\nThe first three classification based methods classify sentences independently. To deal with multiple labels, the classifiers are trained for each discourse mode separately. At prediction time, if the classifier for any discourse mode predicts a sentence as positive, the corresponding discourse mode would be assigned."
  }, {
    "heading": "4.3.3 Evaluation Results",
    "text": "Table 4 shows the experimental results. We evaluate the systems for each discourse mode with F1score, which is the harmonic mean of precision and recall. The best performance is in bold.\nThe SVM performs worst among all systems. The reason is due to the data sparseness and termmismatch problem, since the size of the annotated dataset is not big enough. In contrast, systems based on neural networks with pre-trained word embeddings achieve much better performance.\nThe CNN and GRU have comparable performance. The GRU is slightly better. The two methods don’t consider the semantic representations of adjacent sentences.\nThe GG and GG-SEG explore the semantic information of sentences in a sequence by the bidirectional GRU layer. The results demonstrate that considering such information improve the performance on all discourse modes. This proves the advantage of sequential identification compared with isolated sentence classification.\nWe can see that the GG-SEG further improves the performance on three minority discourse modes compared with GG. This means that the minority modes may have stronger preference to special locations. Exposition benefits most, since many exposition sentences in our dataset are isolated.\nThe performance on argument is not so good. As we discussed in corpus analysis, argument and emotion expressing mode interact frequently. Because the amount of emotion expressing sentences is much more, distinguishing argument from them is hard. Actually, their functions in narrative essays seem to be similar that both are to deepen the author’s response or evoke the reader’s response to the story.\nThe overall average F1-score can reach to 0.7 and the performance on identifying three most common discourse modes are consistent, with an average F1-score above 0.76 using the proposed neural sequence labeling models. Automatic discourse mode identification should be feasible."
  }, {
    "heading": "5 Essay Scoring with Discourse Modes",
    "text": "Discourse mode identification can potentially provide features for downstream NLP applications. This section describes our attempt to explore discourse modes for automatic essay scoring (AES)."
  }, {
    "heading": "5.1 Essay Scoring Framework",
    "text": "We adopt the standard regression framework for essay scoring. We use support vector regression (SVR) and Bayesian linear ridge regression (BLRR), which are used in recent work (Phandi et al., 2015). The key is to design effective features."
  }, {
    "heading": "5.2 Features",
    "text": "The basic feature sets are based on (Phandi et al., 2015).The original feature sets include:\n• Length features\n• Part-Of-Speech (POS) features\n• Prompt features\n• Bag of words features\nWe re-implement the feature extractors exactly according to the description in (Phandi et al., 2015) except for the POS features, since we don’t\nhave correct POS ngrams for Chinese. We complement two additional features: (1) The number of words occur in Chinese Proficiency Test 6 vocabulary; (2) The number of Chinese idioms used.\nWe further design discourse mode related features for each essay:\n• Mode ratio: For each discourse mode, we compute its mode ratio according to ratio = #sentences with the discourse mode#sentences in the essay . Such features indicate the distribution of discourse modes.\n• Bag of ngrams of discourse modes: We use the number of unigrams and bigrams of the dominant discourse modes of the sequence of sentences in the essay as features."
  }, {
    "heading": "5.3 Experimental Settings",
    "text": "The experiments were conducted on narrative essays written by Chinese middle school students in native language during regional tests. There are three prompts and students are required to write an essay related to the given prompt with no less than 600 Chinese characters. All these essays were evaluated by professional teachers.\nWe randomly sampled essays from each prompt for experiments. Table 5 shows the details of the datasets. We ran experiments on each prompt dataset respectively by 5-fold cross-validation.\nThe GG-SEG model was used to identify discourse modes of sentences. Notice that a sentence can have multiple discourse modes. The mode ratio features are computed for each mode separately. When extracting the bag of ngrams of discourse modes features, the discourse mode with highest prediction probability was chosen as the dominant discourse mode.\nWe use the Quadratic Weighted Kappa (QWK) as the evaluation metric."
  }, {
    "heading": "5.4 Evaluation Results",
    "text": "Table 6 shows the evaluation results of AES on three datasets. We can see that the BLRR algorithm performs better than the SVR algorithm. No\nmatter which algorithm is adopted, adding discourse mode features make positive contributions for AES compared with using basic feature sets. The trends are consistent over all three datasets. Impact of discourse mode ratio on scores: We are interested in which discourse mode correlates to essay scores best. Table 7 shows the Pearson correlation coefficient between the mode ratio and essay score. LEN represents the correlation of essay length and is listed as a reference. We can see that the ratio of narration has a negative correlation, which means just narrating stories without auxiliary discourse modes would lead to poor scores. The description mode ratio has the strongest positive correlation to essay scores. This may indicate that using vivid language to provide detail information is essential in writing narrative essays. Emotion expressing also has a positive correlation. It is reasonable since emotional writing can involve readers into the stories. The ratio of argument shows a negative correlation. The reason may be that: first, the identification of argument is not good enough; second, the existence of an argument doesn’t mean the quality of argumentation is good. Exposition has little effect on essay scores.\nGenerally, the distribution of discourse modes shows correlations to the quality of essays. This may relate to the difficulties of manipulating different discourse modes. It is easy for students to use narration, but it is more difficult to manipulate description and emotion expressing well. As a result, the ability of descriptive and emotional writ-"
  }, {
    "heading": "100 200 400 600",
    "text": "ing should be an indicator of language proficiency and can better distinguish the quality of writing.\nImpact on scoring essays with various length: It is easy to understand that length is a strong indicator for essay scoring. It is interesting to study that when the effect of length becomes weaker, e.g., the lengths of essays are close, how does the performance of the AES system change?\nWe conducted experiments on essays with various lengths. Only essays that the length is no less than a given threshold are selected for evaluation. The threshold is set to 100, 200, 400 and 600 Chinese characters respectively. We ran 5-fold crossvalidation with BLRR on the datasets after essay selection.\nFigure 3 shows the results on three datasets. We can see the following trends: (1) The QWK scores decrease along with shorter essays are removed gradually; (2) Adding discourse mode features always improves the performance; (3) As the threshold becomes larger, the improvements by adding discourse mode features become larger.\nThe results indicate that the current AES system can achieve a high correlation score when the lengths of essays differ obviously. Even the simple features like length can judge that short essays tend to have low scores. However, when the lengths of essays are close, AES would face greater challenges, because it is required to deeper understand the properties of well written essays. In such situations, features that can model more advanced aspects of writing, such as discourse modes, should play a more important role. It should be also essential for evaluating essays written in the native language of the writer, when spelling and grammar are not big issues any more."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper has introduced a fundamental but less studied task in NLP—discourse mode identification, which is designed in this work to automatically identify five discourse modes in essays.\nA corpus of narrative student essays was manually annotated with discourse modes at sentence level, with acceptable inter-annotator agreement. The corpus analysis revealed several aspects of characteristics of discourse modes including the distribution, co-occurrence and transition patterns.\nConsidering these characteristics, we proposed a neural sequence labeling approach for identifying discourse modes. The experimental results demonstrate that automatic discourse mode identification is feasible.\nWe evaluated discourse mode features for automatic essay scoring and draw preliminary observations. Discourse mode features can make positive contributions, especially in challenging situations when simple surface features don’t work well. The ratio of description and emotion expressing is shown to be positively correlated to essay scores.\nIn future, we plan to exploit discourse mode identification for providing novel features for more downstream NLP applications."
  }, {
    "heading": "Acknowledgements",
    "text": "The research work is partially funded by the National High Technology Research and Development Program (863 Program) of China (No.2015AA015409), National Natural Science Foundation of China (No.61402304), Ministry of Education (No.14YJAZH046), Beijing Municipal Education Commission (KM201610028015, Connotation Development) and Beijing Advanced Innovation Center for Imaging Technology."
  }],
  "year": 2017,
  "references": [{
    "title": "Automatic text scoring using neural networks",
    "authors": ["Dimitrios Alikaniotis", "Helen Yannakoudakis", "Marek Rei."],
    "venue": "Proceedings of ACL 2016. pages 715–725.",
    "year": 2016
  }, {
    "title": "On rhetoric: A theory of civic discourse",
    "authors": ["Omer Aristotle", "George A Kennedy."],
    "venue": "Oxford University Press.",
    "year": 2006
  }, {
    "title": "Automated essay scoring with e-rater R",
    "authors": ["Yigal Attali", "Jill Burstein"],
    "year": 2006
  }, {
    "title": "English composition and rhetoric",
    "authors": ["Alexander Bain."],
    "venue": "Longmans, Green & Company.",
    "year": 1890
  }, {
    "title": "Modeling local coherence: An entity-based approach",
    "authors": ["Regina Barzilay", "Mirella Lapata."],
    "venue": "Computational Linguistics 34(1):1–34.",
    "year": 2008
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Research in written composition",
    "authors": ["Richard Reed Braddock", "Richard Lloyd-Jones", "Lowell Schoer."],
    "venue": "JSTOR.",
    "year": 1963
  }, {
    "title": "An improved error model for noisy channel spelling correction",
    "authors": ["Eric Brill", "Robert C Moore."],
    "venue": "Proceedings of ACL 2000. pages 286–293.",
    "year": 2000
  }, {
    "title": "Modern rhetoric",
    "authors": ["Cleanth Brooks", "Robert Penn Warren."],
    "venue": "Harcourt, Brace.",
    "year": 1958
  }, {
    "title": "Finding the write stuff: Automatic identification of discourse structure in student essays",
    "authors": ["Jill Burstein", "Daniel Marcu", "Kevin Knight."],
    "venue": "Intelligent Systems, IEEE 18(1):32–39.",
    "year": 2003
  }, {
    "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
    "authors": ["Rich Caruana", "Steve Lawrence", "Lee Giles."],
    "venue": "Proceedings of NIPS 2000. pages 402–408.",
    "year": 2000
  }, {
    "title": "Automated essay scoring by maximizing human-machine agreement",
    "authors": ["Hongbo Chen", "Ben He."],
    "venue": "Proceedings of EMNLP 2013. pages 1741–1752.",
    "year": 2013
  }, {
    "title": "Named entity recognition with bidirectional lstm-cnns",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "Transactions of the Association for Computational Linguistics 4:357–370.",
    "year": 2016
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merriënboer Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings",
    "year": 2014
  }, {
    "title": "The handbook of computational linguistics and natural language processing",
    "authors": ["Alexander Clark", "Chris Fox", "Shalom Lappin."],
    "venue": "John Wiley & Sons.",
    "year": 2013
  }, {
    "title": "Toward a more valid account of functional text quality: The case of the patient information leaflet",
    "authors": ["Rosemary Clerehan", "Rachelle Buchbinder."],
    "venue": "Text & Talk-An Interdisciplinary Journal of Language, Discourse Communication Studies 26(1):39–68.",
    "year": 2006
  }, {
    "title": "A coefficient of agreement for nominal scales",
    "authors": ["Jacob Cohen"],
    "venue": "Educational and psychological measurement 20(1):37–46.",
    "year": 1960
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "The rise and fall of the modes of discourse",
    "authors": ["Robert J Connors."],
    "venue": "College Composition and Communication 32(4):444–455.",
    "year": 1981
  }, {
    "title": "Constrained multi-task learning for automated essay scoring",
    "authors": ["Ronan Cummins", "Meng Zhang", "Ted Briscoe."],
    "venue": "Proceedings of ACL 2016. pages 789– 799.",
    "year": 2016
  }, {
    "title": "Automatic features for essay scoring – an empirical study",
    "authors": ["Fei Dong", "Yue Zhang."],
    "venue": "Proceedings of EMNLP 2016. pages 1072–1077.",
    "year": 2016
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."],
    "venue": "Proceedings of ACL 2015. pages 334–343.",
    "year": 2015
  }, {
    "title": "Situation entity types: automatic classification of clause-level aspect",
    "authors": ["Annemarie Friedrich", "Alexis Palmer", "Manfred Pinkal."],
    "venue": "Proceedings of ACL 2016. pages 1757–1768.",
    "year": 2016
  }, {
    "title": "Supervised sequence labelling",
    "authors": ["Alex Graves."],
    "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, Springer Berlin Heidelberg, pages 5–",
    "year": 2012
  }, {
    "title": "Centering: A framework for modeling the local coherence of discourse",
    "authors": ["Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi."],
    "venue": "Computational linguistics 21(2):203–225.",
    "year": 1995
  }, {
    "title": "Texttiling: Segmenting text into multi-paragraph subtopic passages",
    "authors": ["Marti A Hearst."],
    "venue": "Computational linguistics 23(1):33–64.",
    "year": 1997
  }, {
    "title": "Coherence and coreference",
    "authors": ["Jerry R Hobbs."],
    "venue": "Cognitive science 3(1):67–90.",
    "year": 1979
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "On the structure of scientific texts",
    "authors": ["John Hutchins."],
    "venue": "UEA Papers in Linguistics 5(3):18–39.",
    "year": 1977
  }, {
    "title": "Representation learning for text-level discourse parsing",
    "authors": ["Yangfeng Ji", "Jacob Eisenstein."],
    "venue": "Proceedings of ACL 2014. pages 13–24.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of EMNLP 2014. pages 1746–1751.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Automatic essay grading using text categorization techniques",
    "authors": ["Leah S Larkey."],
    "venue": "Proceedings of SIGIR 1998. pages 90–95.",
    "year": 1998
  }, {
    "title": "What makes writing great? first experiments on article quality prediction in the science journalism domain",
    "authors": ["Annie Louis", "Ani Nenkova."],
    "venue": "Transactions of the Association for Computational Linguistics 1:341–352.",
    "year": 2013
  }, {
    "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "Proceedings of ACL 2016. pages 1064–1074.",
    "year": 2016
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C Mann", "Sandra A Thompson."],
    "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243–281.",
    "year": 1988
  }, {
    "title": "Linking discourse modes and situation entity types in a cross-linguistic corpus study",
    "authors": ["Kleio-Isidora Mavridou", "Annemarie Friedrich", "Melissa Peate Sørensen", "Alexis Palmer", "Manfred Pinkal."],
    "venue": "Workshop on Linking Models of Lexical, Sen-",
    "year": 2015
  }, {
    "title": "Genre as diversity, and rhetorical mode as unity in language use",
    "authors": ["José Luiz Meurer."],
    "venue": "Ilha do Desterro A Journal of English Language, Literatures in English and Cultural Studies (43):061–082.",
    "year": 2002
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Proceedings of NIPS 2013. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "A sequencing model for situation entity classification",
    "authors": ["Alexis Palmer", "Elias Ponvert", "Jason Baldridge", "Carlota Smith."],
    "venue": "Proceedings of ACL 2007. pages 896–903.",
    "year": 2007
  }, {
    "title": "Modeling organization in student essays",
    "authors": ["Isaac Persing", "Alan Davis", "Vincent Ng."],
    "venue": "Proceedings of EMNLP 2010. pages 229–239.",
    "year": 2010
  }, {
    "title": "Flexible domain adaptation for automated essay scoring using correlated linear regression",
    "authors": ["Peter Phandi", "Kian Ming A. Chai", "Hwee Tou Ng."],
    "venue": "Proceedings of EMNLP 2015. pages 431–439.",
    "year": 2015
  }, {
    "title": "Generating confusion sets for context-sensitive error correction",
    "authors": ["Alla Rozovskaya", "Dan Roth."],
    "venue": "Proceedings of EMNLP 2010. pages 961–970.",
    "year": 2010
  }, {
    "title": "Modes of discourse: The local structure of texts, volume 103",
    "authors": ["Carlota S Smith."],
    "venue": "Cambridge University Press.",
    "year": 2003
  }, {
    "title": "Discourse element identification in student essays based on global and local cohesion",
    "authors": ["Wei Song", "Ruiji Fu", "Lizhen Liu", "Ting Liu."],
    "venue": "Proceedings of EMNLP 2015. pages 2255–2261.",
    "year": 2015
  }, {
    "title": "A neural approach to automated essay scoring",
    "authors": ["Kaveh Taghipour", "Hwee Tou Ng."],
    "venue": "Proceedings of EMNLP 2016. pages 1882–1891.",
    "year": 2016
  }, {
    "title": "Document modeling with gated recurrent neural network for sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proceedings of EMNLP 2015. pages 1422–1432.",
    "year": 2015
  }, {
    "title": "Summarizing scientific articles: experiments with relevance and rhetorical status",
    "authors": ["Simone Teufel", "Marc Moens."],
    "venue": "Computational linguistics 28(4):409–445.",
    "year": 2002
  }, {
    "title": "Discourse structure and language technology",
    "authors": ["Bonnie Webber", "Markus Egg", "Valia Kordoni."],
    "venue": "Natural Language Engineering 18(4):437–490.",
    "year": 2011
  }, {
    "title": "Buy one get one free: Distant annotation of chinese tense, event type and modality",
    "authors": ["Nianwen Xue", "Yuchen Zhang."],
    "venue": "Proceedings of LREC 2014. pages 1412–1416.",
    "year": 2014
  }],
  "id": "SP:bfc0462e1121647ca003abdba86a23912d161999",
  "authors": [{
    "name": "Wei Song",
    "affiliations": []
  }, {
    "name": "Dong Wang",
    "affiliations": []
  }, {
    "name": "Ruiji Fu",
    "affiliations": []
  }, {
    "name": "Lizhen Liu",
    "affiliations": []
  }, {
    "name": "Ting Liu",
    "affiliations": []
  }, {
    "name": "Guoping Hu",
    "affiliations": []
  }],
  "abstractText": "Discourse modes play an important role in writing composition and evaluation. This paper presents a study on the manual and automatic identification of narration, exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a corpus to study the characteristics of discourse modes and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as features that improve automatic essay scoring (AES). The impacts of discourse modes for AES are also discussed.",
  "title": "Discourse Mode Identification in Essays"
}