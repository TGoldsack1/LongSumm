{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 2238–2249 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The meaning of conversational utterances depends strongly on the history of the interaction. Consider a user querying a flight database using natural language (Figure 1). Given a user utterance, the system must generate a query, execute it, and display results to the user, who then provides the next request. Key to correctly mapping utterances to executable queries is resolving references. For example, the second utterance implicitly depends on the first, and the reference ones in the third utterance explicitly refers to the response to the second utterance. Within an interactive system, this information needs to be composed with mentions of database entries (e.g., Seattle, next Monday) to generate a formal executable representation. In this paper, we propose encoder-decoder models that directly map user utterances to executable queries, while considering the history of the interaction, including both previous utterances and their generated queries.\nReasoning about how the meaning of an utterance depends on the history of the interaction is critical to correctly respond to user requests. As interactions progress, users may omit previouslymentioned constraints and entities, and an increas-\ning portion of the utterance meaning must be derived from the interaction history. Figure 2 shows SQL queries for the utterances in Figure 1. As the interaction progresses, the majority of the generated query is derived from the interaction history (underlined), rather than from the current utterance. A key challenge is resolving what past information is incorporated and how. For example, in the figure, the second utterance depends on the set of flights defined by the first, while adding a new constraint. The third utterance further refines this set by adding a constraint to the constraints from both previous utterances. In contrast, the fourth utterance refers only to the first one, and skips the two utterances in between.1 Correctly generating the fourth query requires understanding that the time constraint (at 7pm) can be ignored as it follows an airline constraint that has been replaced.\nWe study complementary methods to enable this type of reasoning. The first set of methods implicitly reason about references by modifying the encoder-decoder architecture to encode information from previous utterances for generation decisions. We experiment with attending over previous utterances and using an interaction-level recurrent encoder. We also study explicitly maintaining a set of referents using segments from pre-\n1An alternative explanation is that utterance four refers to utterance three, and deletes the time and airline constraints.\n2238\nvious queries. At each step, the decoder chooses whether to output a token or select a segment from the set, which is appended to the output in a single decoding step. In addition to enabling references to previously mentioned entities, sets, and constraints, this method also reduces the number of generation steps required, illustrated by the underlined segments in Figure 2. For example, the query ȳ2 will require 17 steps instead of 94.\nWe evaluate our approach using the ATIS (Hemphill et al., 1990; Dahl et al., 1994) task, where a user interacts with a SQL flight database using natural language requests, and almost all queries require joins across multiple tables. In addition to reasoning about contextual phenomena, we design our system to effectively resolve database values, including resolution of time expressions (e.g., next monday in Figure 1) using an existing semantic parser. Our evaluation shows that reasoning about the history of the interaction is necessary, relatively increasing performance by 28.6% over a baseline with no access to this information, and that combining the implicit and explicit methods provides the best performance. Furthermore, our analysis shows that our full approach maintains its performance as interaction length increases, while the performance of systems without explicit modeling deteriorates. Our code is available at https://github.com/clic-lab/atis."
  }, {
    "heading": "2 Technical Overview",
    "text": "Our goal is to map utterances in interactions to formal executable queries. We evaluate our approach with the ATIS corpus (Hemphill et al., 1990; Dahl et al., 1994), where users query a realistic flight planning system using natural language. The system responds by displaying tables and database entries. User utterances are mapped to SQL to query a complex database with 27 tables and 162K entries. 96.6% of the queries require joins of different tables. Section 7 describes ATIS.\nTask Notation Let I be the set of all interactions, X the set of all utterances, and Y the set of all formal queries. A user utterance x̄ ∈ X of length |x̄| is a sequence 〈x1, . . . , x|x̄|〉, where each xi is a natural language token. A formal query ȳ ∈ Y of length |ȳ| is a sequence 〈y1, . . . , y|ȳ|〉, where each yi is a formal query token. An interaction Ī ∈ I is a sequence of n utterance-query pairs 〈(x̄1, ȳ1), . . . , (x̄n, ȳn)〉 representing an interaction with n turns. To refer to indexed interactions and their content, we mark Ī(l) as an interaction with index l, the i-th utterance and query in Ī(l) as x̄(l)i and ȳ (l) i , and the j-th tokens in x̄ (l) i and ȳ(l)i as x (l) i,j and y (l) i,j . At turn i, we denote the interaction history of length i − 1 as Ī[: i − 1] = 〈(x̄1, ȳ1), . . . , (x̄i−1, ȳi−1)〉. Given Ī[: i− 1] and utterance x̄i our goal is to generate ȳi, while considering both x̄i and Ī[: i − 1]. Following the ex-\necution of ȳi, the interaction history at turn i + 1 becomes Ī[: i] = 〈(x̄1, ȳ1), . . . , (x̄i, ȳi)〉. Model Our model is based on the recurrent neural network (RNN; Elman, 1990) encoderdecoder framework with attention (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We modify the model in three ways to reason about context from the interaction history by attending over previous utterances (Section 4.2), adding a turn-level recurrent encoder that updates after each turn (Section 4.3), and adding a mechanism to copy segments of queries from previous utterances (Section 4.4). We also design a scoring function to score values that are abstracted during pre-processing, including entities and times (Section 6). The full model selects between generating query tokens and copying complete segments from previous queries. Learning We assume access to a training set that contains N interactions {Ī(l)}Nl=1. We train using a token-level cross-entropy objective (Section 5). For models that use the turn-level encoder, we construct computational graphs for the entire interaction and back-propagate the loss for all queries together. Without the turn-level encoder, each utterance is processed separately. Evaluation We evaluate using a test set {Ī(l)}Ml=1 of M interactions. We measure the accuracy of each utterance for each test interaction against the annotated query and its execution result. For models that copy segments from previous queries, we evaluate using both predicted and gold previous queries."
  }, {
    "heading": "3 Related Work",
    "text": "Mapping sentences to formal representations, commonly known as semantic parsing, has been studied extensively with linguistically-motivated compositional representations, including variablefree logic (e.g., Zelle and Mooney, 1996; Clarke et al., 2010), lambda calculus (e.g., Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011; Kushman and Barzilay, 2013), and dependencybased compositional semantics (e.g., Liang et al., 2011; Berant et al., 2013). Recovering lambdacalculus representations was also studied with ATIS with focus on context-independent meaning using grammar-based approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Wang et al., 2014) and neural networks (Dong and Lapata, 2016; Jia and Liang, 2016).\nRecovering context-independent executable representations has been receiving increasing attention. Mapping sentence in isolation to SQL queries has been studied with ATIS using statistical parsing (Popescu et al., 2004; Poon, 2013) and sequence-to-sequence models (Iyer et al., 2017). Generating executable programs was studied with other domains and formal languages (Giordani and Moschitti, 2012; Ling et al., 2016; Zhong et al., 2017; Xu et al., 2017). Recently, various approaches were proposed to use the formal language syntax to constrain the search space (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Cheng et al., 2017) making all outputs valid programs. These contributions are orthogonal to ours, and can be directly integrated into our decoder.\nGenerating context-dependent formal representations has received less attention. Miller et al. (1996) used ATIS and mapped utterances to semantic frames, which were then mapped to SQL queries. For learning, they required full supervision, including annotated parse trees and contextual dependencies.2 Zettlemoyer and Collins (2009) addressed the problem with lambda calculus, using a semantic parser trained separately with context-independent data. In contrast, we generate executable formal queries and require only interaction query annotations for training.\nRecovering context-dependent meaning was also studied with the SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017) corpora. We compare ATIS to these corpora in Section 7. Resolving explicit references, a part of our problem, has been studied as co-reference resolution (Ng, 2010). Context-dependent language understanding was also studied for dialogue systems, including with ATIS, as surveyed by Tür et al. (2010). More recently, encoder-decoder methods were applied to dialogue systems (Peng et al., 2017; Li et al., 2017), including using hierarchical RNNs (Serban et al., 2016, 2017), an architecture related to our turn-level encoder. These approaches use slot-filling frames with limited expressivity, while we focus on the original representation of unconstrained SQL queries.\n2Miller et al. (1996) provide limited details about their evaluation. Later work notes that they evaluate SQL query correctness (Zettlemoyer and Collins, 2009) with an accuracy of 78.4%, higher than our results. However, the lack of details (e.g., if the metric is strict or relaxed) makes comparison difficult. In addition, we use significantly less supervision, and re-split the data to avoid scenario bias (Section 7)."
  }, {
    "heading": "4 Context-dependent Model",
    "text": "We base our model on an encoder-decoder architecture with attention (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). At each interaction turn i, given the current utterance x̄i and the interaction history Ī[: i − 1], the model generates the formal query ȳi. Figure 3 illustrates our architecture. We describe the base architecture, and gradually add components."
  }, {
    "heading": "4.1 Base Encoder-Decoder Architecture",
    "text": "Our base architecture uses an encoder to process the user utterance x̄i = 〈xi,1, . . . , xi,|x̄i|〉 and a decoder to generate the output query ȳi token-bytoken. This architecture does not observe the interaction history Ī[: i− 1].\nThe encoder computes a hidden state hEj =\n[h −→ E j ;h ←− E j ] for each token xi,j using a bi-directional RNN. The forward RNN is defined by: 3\nh −→ E j = LSTM −→ E ( φx(xi,j);h −→ E j−1 ) , (1)\nwhere LSTM −→ E is a long short-term memory recurrence (LSTM; Hochreiter and Schmidhuber, 1997) and φx is a learned embedding function for input tokens. The backward RNN recurs in the opposite direction with separate parameters.\nWe generate the query with an RNN decoder. The decoder state at step k is:\nhDk = LSTM D ( [φy(yi,k−1); ck−1] ;h D k−1 ) ,\nwhere LSTMD is a two-layer LSTM recurrence, φy is a learned embedding function for query tokens, and ck is an attention vector computed from the encoder states. yi,0 is a special start token, and c0 is a zero-vector. The initial hidden state and cell memory of each layer are initialized as hE|x̄i| and cE|x̄i|. The attention vector ck is a weighted sum of the encoder hidden states:\nsk(j) = h E j W AhDk (2) αk = softmax(sk) (3)\nck =\n|x̄i|∑\nj=1\nhEj αk(j) , (4)\nwhere WA is a learned matrix. The probabilities of output query tokens are computed as:\nmk = tanh ( [hDk ; ck]W m ) (5)\nP (yi,k = w | x̄i, ȳi,1:k−1) ∝ exp(mkWow + bow) (6)\nwhere Wm, Wo, and bo are learned.\n3We omit the memory cell (often denoted as cj) from all LSTM descriptions. We use only the LSTM hidden state hj in other parts of the architecture unless explicitly noted."
  }, {
    "heading": "4.2 Incorporating Recent History",
    "text": "We provide the model with the most recent interaction history by concatenating the previous h utterances 〈x̄i−h, ..., x̄i−1〉 with the current utterance in order, adding a special delimiter token between each utterance. The concatenated input provides the model access to previous utterances, but not to previously generated queries, or utterances that are more than h turns in the past. The architecture remains the same, except that the encoder and attention are computed over the concatenated sequence of tokens. The probability of an output query token is computed the same, but is now conditioned on the interaction history:\nP (yi,k = w | x̄i, ȳi,1:k−1, Ī[: i− 1]) ∝ (7) exp(mkW o w + b o w) ."
  }, {
    "heading": "4.3 Turn-level Encoder",
    "text": "Concatenating recent utterances to provide access to recent history has computational drawbacks. The encoding of the utterance depends on its location in the concatenated string. This requires encoding all recent history for each new utterance, and does not allow re-use of computation between utterances during encoding. It also introduces a tradeoff between computation cost and expressivity: attending over the h previous utterances allows the decoder access to the information in these utterances when generating a query, but is computationally more expensive as h increases. We address this by encoding each utterance once. To account for the influence of the interaction history on utterance encoding, we maintain a discourse state encoding hIi computed with a turn-level recurrence, and use it during utterance encoding. The state is maintained and updated over the entire interaction. At turn i, this model has access to the complete prefix of the interaction Ī[: i−1] and the current request x̄i. In contrast, the concatenationbased encoder (Section 4.2) has access only to information from the previous h utterances. We also use positional encoding in the attention computation to account for the position of each utterance relative to the current utterance.\nFormally, we modify Equation 1 to encode x̄i: h −→ E i,j = LSTM −→ E ([ φx(xi,j);h I i−1 ] ;h −→ E i,j−1 ) ,\nwhere hIi−1 is the discourse state following utterance x̄i−1. LSTM ←− E is modified analogously. In contrast to the concatenation-based model, the recurrence processes a single utterance. The dis-\ncourse state hIi is computed as hIi = LSTM I ( hEi,|x̄i|;h I i−1 ) .\nSimilar to the concatenation-based model, we attend over the current utterance and the h previous utterances. We add relative position embeddings φI to each hidden state. These embeddings are learned for each possible distance 0, . . . , h− 1 from the current utterance. We modify Equation 2 to index over both utterances and tokens:\nsk(t, j) = [ hEt,j ;φ I(i− t) ] WAhDk . (8)\nIn contrast to the concatenation model, without position embeddings, the attention computation has no indication of the utterance position, as our ablation shows in Section 8. The attention distribution is computed as in Equation 3, and normalized across all utterances. The position embedding is also used to compute the context vector ck:\nck =\ni∑\nt=i−h\n|x̄t|∑\nj=1\n[ hEt,j ;φ I(i− t) ] αk(t, j) ."
  }, {
    "heading": "4.4 Copying Query Segments",
    "text": "The discourse state and attention over previous utterances allow the model to consider the interaction history when generating queries. However, we observe that context-dependent reasoning often requires generating sequences that were generated in previous turns. Figure 2 shows how segments (underlined) extracted from previous utterances are predominant in later queries. To take advantage of what was previously generated, we add copying of complete segments from previous queries by expanding the set of outputs at each generation step. This mechanism explicitly models references, reduces the number of steps re-\nquired to generate queries, and provides an interpretable view of what parts of a query originate in context. Figure 3 illustrates this architecture. Extracting Segments Given the interaction history Ī[: i − 1], we construct the set of segments Si−1 by deterministically extracting subtrees from previously generated queries.4 In our data, we extract 13 ± 5.9 (µ ± σ) segments for each annotated query. Each segment s̄ ∈ Si−1 is a tuple 〈a, b, l, r〉, where a and b are the indices of the first and most recent queries, ȳa and ȳb, in the interaction that contain the segment. l and r are the start and end indices of the segment in ȳb. Encoding Segments We represent a segment s̄ = 〈a, b, l, r〉 using the hidden states of an RNN encoding of the query ȳb. The hidden states 〈hQ1, ...,hQ|ȳb|〉 are computed using a bi-directional LSTM RNN similar to the utterance encoder (Equation 1), except using separate LSTM parameters and φy to embed the query tokens. The embedded representation of a segment is a concatenation of the hidden states at the segment endpoints and an embedding of the relative position of the utterance where it appears first:\nhS = [ hQl ;h Q r ;φ g(min(g, i− a)) ] ,\nwhere φg is a learned embedding function of the position of the initial query ȳa relative to the current turn index i. We learn an embedding for each relative position that is smaller than g, and use the same embedding for all other positions. Generation with Segments At each generation step, the decoder selects between a single query token or a segment. When a segment is selected, it\n4The process of extracting sub-trees is described in the supplementary material.\nis appended to the generated query, an embedded segment representation for the next step is computed, and generation continues. The probability of a segment s̄ = 〈a, b, l, r〉 at decoding step k is:\nP (yi,k = s̄ | x̄i, ȳi,1:k−1, Ī[: i− 1]) ∝ (9) exp ( mkW S hS ) ,\nwhere mk is computed in Equation 5 and W S\nis a learned matrix. To simplify the notation, we assign the segment to a single output token. The output probabilities (Equations 7 and 9) are normalized together to a single probability distribution. When a segment is selected, the embedding used as input for the next generation step is a bagof-words encoding of the segment. We extend the output token function φy to take segments:\nφy(s̄ = 〈a, b, l, r〉) = 1 r − l\nr∑\nk=l\nφy (yb,k) .\nThe recursion in φy is limited to depth one because segments do not contain other segments."
  }, {
    "heading": "4.5 Inference with Full Model",
    "text": "Given an utterance x̄i and the history of interaction Ī[: i − 1], we generate the query ȳi. An interaction starts with the user providing the first utterance x̄1. The utterance is encoded using the initial discourse state hI0, the discourse state h I 1 is computed, the query ȳ1 is generated, and the set of segments S1 is created. The initial discourse state hI0 is learned, and the set of segments S0 used when generating ȳ1 is the empty set. The attention is computed only over the first utterance because no previous utterances exist. The user then provides the next utterance or concludes the interaction. At turn i, the utterance x̄i is encoded using the discourse state hIi−1, the discourse state h I i is computed, and the query ȳi is generated using the set of segments Si−1. The model has no access to future utterances. We use greedy inference for generation. Figure 3 illustrates a single decoding step."
  }, {
    "heading": "5 Learning",
    "text": "We assume access to a training set of N interactions {Ī(l)}Nl=1. Given an interaction Ī(l), each utterance x̄(l)i where 1 ≤ i ≤ |Ī(l)|, is paired with an annotated query ȳ(l)i . The set of segments from previous utterances is deterministically extracted from the annotated queries during learning. However, the data does not indicate what parts of each query originate in segments copied from previous utterances. We adopt a simple approach and heuristically identify context-dependent segments\nbased on entities that appear in the utterance and the query.5 Once we identify a segment in the annotated query, we replace it with a unique placeholder token, and it appears to the learning algorithm as a single generation decision. Treating this decision as latent is an important direction for future work. Given the segment copy decisions, we minimize the token cross-entropy loss: L(y(l)i,k) = − logP ( y (l) i,k | x̄ (l) i , ȳ (l) i,1:k−1, Ī (l)[: i− 1] ) ,\nwhere k is the index of the output token. The base and recent-history encoders (Sections 4.1 and 4.2) can be trained by processing each utterance separately. For these models, given a mini-batch B of utterances, each identified by an interactionutterance index pair, the loss is the mean token loss\nL = 1∑ (i,j)∈B |ȳ (j) i |\n∑\n(i,j)∈B\n|ȳ(j)i |∑\nk=1\nL(y(j)i,k ) .\nThe turn-level encoder (Section 4.3) requires building a computation graph for the entire interaction. We update the model parameters for each interaction. The interaction loss is\nL = n B 1 ∑n\ni=1 |ȳ (j) i |\nn∑\ni=1\n|ȳ(j)i |∑\nk=1\nL(y(j)i,k ) ,\nwhereB is the batch size, and nB re-normalizes the loss so the gradient magnitude is not dependent on the number of utterances in the interaction. Our ablations (−batch re-weight in Table 2) shows the importance of this term. For both cases, we use teacher forcing (Williams and Zipser, 1989)."
  }, {
    "heading": "6 Reasoning with Anonymized Tokens",
    "text": "An important practical consideration for generation in ATIS and other database domains is reasoning about database values, such as entities, times, and dates. For example, the first utterance in Figure 2 includes two entities and a date reference. With limited data, learning to both reason about a large number of entities and to resolve dates are challenging for neural network models. Following previous work (Dong and Lapata, 2016; Iyer et al., 2017), we address this with anonymization, where the data is pre- and post-processed to abstract over tokens that can be heuristically resolved to tokens in the query language. In contrast to previous work, we design a special scoring function to anonymized tokens to reflect how they are used in the input utterances. Figure 4 illustrates preprocessing in ATIS. For example, we use a temporal semantic parser to resolve dates (e.g., next\n5The alignment is detailed in the supplementary material.\nMonday) and replace them with day, month, and year placeholders. To anonymize database entries, we use a dictionary compiled from the database (e.g., to map Seattle to SEATTLE). The full details of the anonymization procedure are provided in the supplementary material. Following preprocessing, the model reasons about encoding and generation of anonymized tokens (e.g., CITY#1) in addition to regular output tokens and query segments from the interaction history. Anonymized tokens are typed (e.g., CITY), map to a token in the query language (e.g., ’BOSTON’), and appear both in input utterances and generated queries.\nWe modify our encoder and decoder embedding functions (φx and φy) to map anonymized tokens to the embeddings of their types (e.g., CITY). The type embeddings in φx and φy are separate. Using the types only, while ignoring the indices, avoids learning biases that arise from the arbitrary ordering of the tokens in the training data. However, it does not allow distinguishing between entries with the same type for generation decisions; for example, the common case where multiple cities are mentioned in an interaction. We address this by scoring anonymized token based on the magnitude of attention assigned to them at generation step k. The attention magnitude is computed from the encoder hidden states. This computation considers both the decoder state and the location of the anonymized tokens in the input utterances to account for how they are used in the interaction. The probability of an anonymized token w at generation step k is P (yi,k = w | x̄i, ȳi,1:k−1, Ī[: i− 1]) ∝\ni∑\nt=i−h\n|x̄t|∑\nj=1\n(exp (sk (t, j)))\nwhere sk (t, j) is the attention score computed in Equation 8. This probability is normalized to-\ngether with the probabilities in Equations 7 and 9 to form the complete output probability."
  }, {
    "heading": "7 Experimental Setup",
    "text": "Hyperparameters, architecture details, and other experimental choices are detailed in the supplementary material.\nData We use ATIS (Hemphill et al., 1990; Dahl et al., 1994) to evaluate our approach. The data was originally collected using wizard-of-oz experiments, and annotated with SQL queries. Each interaction was based on a scenario given to a user. We observed that the original data split shares scenarios between the train, development, and test splits. This introduces biases, where travel patterns that appeared during training repeat in testing. For example, a model trained on the original data split often correctly resolves the exact referenced by on Saturday with no pre-processing or access to the document date. We evaluate this overfitting empirically in the supplementary material. We re-split the data to avoid this bias. We evenly distribute scenarios across splits so that each split contains both scenarios with many and few representative interactions. The new split follows the original split sizes with 1148/380/130 train/dev/test interactions. Table 1 shows data statistics. The system uses a SQL database of 27 tables and 162K entries. 96.6% of the queries require at least one join, and 93% at least two joins. The most related work on ATIS to ours is Miller et al. (1996), which we discuss in Section 3.\nThe most related corpora to ATIS are SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017). SCONE (Long et al., 2016) contains micro-domains consisting of stack- or list-like elements. The formal representation is linguistically-motivated and the majority of queries include a single binary predicate. All interactions include five turns. SequentialQA (Iyyer et al., 2017) contains sequences of questions on a single Wikipedia table. Interactions are on average 2.9 turns long, and were created by re-phrasing a question from a context-independent corpus (Pasupat and Liang, 2015). In contrast, ATIS uses a significantly larger\ndatabase, requires generating complex queries with multiple joins, includes longer interactions, and was collected through interaction with users. The supplementary material contains analysis of the contextual phenomena observed in ATIS. Pre-processing We pre-process the data to identify and anonymize entities (e.g., cities), numbers, times, and dates. We use string matching heuristics to identify entities and numbers, and identify and resolve times and dates using UWTime (Lee et al., 2014). When resolving dates we use the original interaction date as the document time. The supplementary material details this process. Metrics We evaluate using query accuracy, strict denotation accuracy, and relaxed denotation accuracy. Query accuracy is the percentage of predicted queries that match the reference query. Strict denotation accuracy is the percentage of predicted queries that execute to exactly the same table as the reference query. In contrast to strict, relaxed gives credit to a prediction query that fails to execute if the reference table is empty. In cases when the utterance is ambiguous and there are multiple gold queries, we consider the query or table correct if they match any of the gold labels. Systems We evaluate four systems: (a) SEQ2SEQ-0: the baseline encoder-decoder model (Section 4.1); (b) SEQ2SEQ-H: encoderdecoder with attention on current and previous utterances (Section 4.2); (c) S2S+ANON: encoderdecoder with attention on previous utterances and anonymization scoring (Section 6); and (d) FULL: the complete approach including segment copying (Section 4.4). For FULL, we evaluate with predicted and gold (FULL-GOLD) previous queries, and without attention on previous utterances (FULL-0). All models except SEQ2SEQ-0 and FULL-0 use h = 3 previous utterances. We limit segment copying to segments that appear in the most recent query only.6 Unless specifically ablated, all experiments use pre-processing."
  }, {
    "heading": "8 Results",
    "text": "Table 2 shows development and test results. We run each experiment five times and report mean and standard deviation. The main metric we focus on is strict denotation accuracy. The relatively low performance of SEQ2SEQ-0 demon-\n6While we only use segments from the most recent query, they often appear for the first time much earlier in the interaction, which influences their absolute position value a.\nstrates the need for context in this task. Attending on recent history significantly increases performance. Both SEQ2SEQ models score anonymized tokens as regular vocabulary tokens. Adding anonymized token scoring further increases performance (S2S+ANON). FULL-0 and FULL add segment copying and the turn-level encoder. The relatively high performance of FULL-0 shows that substituting segment copying with attention maintains and even improves the system effectiveness. However, the best performance is provided with FULL, which combines both. This shows the benefit of redundancy in accessing contextual information. Unlike the other systems, both FULL and FULL-0 suffer from cascading errors due to selecting query segments from previously incorrect predictions. The higher FULL-GOLD performance illustrates the influence of error propagation. While part of this error can be mitigated by having both attention and segment copying, this behavior is unlikely to be learned from supervised learning, where errors are never observed.\nAblations show that all components contribute to the system performance. Performance drops when using a concatenation-based encoder instead of the turn-level encoder (−turn-level enc.; Section 4.3). Using batch-reweighting (−batchreweight; Section 5) and input position embeddings (−input pos. embs.; Section 4.3) are critical to the performance of the turn-level encoder. Removing copying of query segments\nfrom the interaction history lowers performance (−query segments; Section 4.4). Treating indexed anonymized tokens as regular tokens, rather than using attention-based scoring and type embeddings, lowers performance (−anon. scoring; Section 6). Finally, pre-processing, which includes anonymization, is critical (−pre-processing).\nFigure 5(a) shows the performance as interactions progress. All systems show a drop in performance after the first utterance, which is always context-independent. As expected, SEQ2SEQ-0 shows the biggest drop. The FULL approach is the most stable as the interaction progresses.\nFigure 5(b) shows the performance as we decrease the number of previous utterances used for attention h. Without the turn-level encoder and segment copying (SEQ2SEQ-H and S2S+ANON), performance decreases significantly as h decreases. In contrast, the FULL model shows a smaller decrease (1.5%). The supplementary material includes attention analysis demonstrating the importance of previous-utterance attention. However, attending on fewer utterances improves inference speed: FULL-0 is 30% faster than FULL.\nFinally, while we re-split the data due to scenario sharing between train and test early in development and used this split only for development, we also evaluate on the original split (Table 3). We report mean and standard deviation over three trials. The high performance of S2S+ANON potentially indicates it benefits more from the differences between the splitting procedures."
  }, {
    "heading": "9 Analysis",
    "text": "We analyze errors made by the full model on thirty development interactions. When analyzing the output of FULL, we focus on error propagation and analyze predictions that resulted in an incorrect table when using FULL, but a correct table when using FULL-GOLD. 56.7% are due to selection of a segment that contained an incorrect constraint. 43.4% of the errors are caused by a necessary segment missing during generation. 93.0% of all predictions are valid SQL and follow the database schema. We also analyze the errors of FULL-GOLD. We observe that 30.0% of errors are due to generating constraints that were not mentioned by the user. Other common errors include generating relevant constraints with incorrect values (23.3%) and missing constraints (23.3%).\nWe also evaluate our model’s ability to recover long-distance references while constraints are added, changed, or removed, and when target attributes change. The supplementary material includes the analysis details. In general, the model resolves references well. However, it fails to recover constraints mentioned in the past following a user focus state change (Grosz and Sidner, 1986)."
  }, {
    "heading": "10 Discussion",
    "text": "We study models that recover context-dependent executable representations from user utterances by reasoning about interaction history. We observe that our segment-copying models suffer from error propagation when extracting segments from previously-generated queries. This could be mitigated by training a model to ignore erroneous segments, and recover by relying on attention for generation. However, because supervised learning does not expose the model to erroneous states, a different learning approach is required. Our analysis demonstrates that our model is relatively insensitive to interaction length, and is able to recover both explicit and implicit references to previouslymentioned entities and constraints. Further study of user focus change is required, an important phenomenon that is relatively rare in ATIS."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported by the NSF (CRII1656998), Schmidt Sciences, a gift from Google, and cloud computing credits from Amazon. We thank Valts Blukis, Luke Zettlemoyer, and the anonymous reviewers for their helpful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Bootstrapping semantic parsers from conversations",
    "authors": ["Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. http://www. aclweb.org/anthology/D11-1039.",
    "year": 2011
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. http://www.aclweb.",
    "year": 2013
  }, {
    "title": "Learning structured natural language representations for semantic parsing",
    "authors": ["Jianpeng Cheng", "Siva Reddy", "Vijay Saraswat", "Mirella Lapata."],
    "venue": "Proceedings Association for Computational Linguistics. https://doi.org/10.18653/v1/",
    "year": 2017
  }, {
    "title": "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings",
    "year": 2014
  }, {
    "title": "Driving semantic parsing from the world’s response",
    "authors": ["James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth."],
    "venue": "Proceedings of the Conference on Computational Natural Language Learning. http://www.aclweb.org/",
    "year": 2010
  }, {
    "title": "Expanding the scope of the ATIS task: The ATIS-3 corpus",
    "authors": ["Deborah A. Dahl", "Madeleine Bates", "Michael Brown", "William Fisher", "Kate Hunicke-Smith", "David Pallett", "Christine Pao", "Alexander Rudnicky", "Elizabeth Shriberg."],
    "venue": "In",
    "year": 1994
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the Association for Computational Linguistics. https: //doi.org/10.18653/v1/P16-1004.",
    "year": 2016
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L. Elman."],
    "venue": "Cognitive Science 14:179–211.",
    "year": 1990
  }, {
    "title": "Translating questions to SQL queries with generative parsers discriminatively reranked",
    "authors": ["Alessandra Giordani", "Alessandro Moschitti."],
    "venue": "Con-",
    "year": 2012
  }, {
    "title": "Context-dependent semantic parsing for time expressions",
    "authors": ["Kenton Lee", "Yoav Artzi", "Jesse Dodge", "Luke Zettlemoyer."],
    "venue": "Proceedings of the Association for Computational Linguistics. https: //doi.org/10.3115/v1/P14-1135.",
    "year": 2014
  }, {
    "title": "End-to-end task-completion neural dialogue systems",
    "authors": ["Xiujun Li", "Yun-Nung Chen", "Lihong Li", "Jianfeng Gao."],
    "venue": "CoRR abs/1703.01008.",
    "year": 2017
  }, {
    "title": "Learning dependency-based compositional semantics",
    "authors": ["Percy Liang", "Michael Jordan", "Dan Klein."],
    "venue": "Proceedings of the Association for Computational Linguistics: Human Language Technologies. http://www.aclweb.",
    "year": 2011
  }, {
    "title": "Latent predictor networks for code generation",
    "authors": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Fumin Wang", "Andrew Senior."],
    "venue": "CoRR abs/1603.06744.",
    "year": 2016
  }, {
    "title": "Simpler context-dependent logical forms via model projections",
    "authors": ["Reginald Long", "Panupong Pasupat", "Percy Liang."],
    "venue": "Proceedings of the Association for Computational Linguistics. https:// doi.org/10.18653/v1/P16-1138.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.",
    "year": 2015
  }, {
    "title": "A fully statistical approach to natural language interfaces",
    "authors": ["Scott Miller", "David Stallard", "Robert Bobrow", "Richard Schwartz."],
    "venue": "Proceedings of the Association for Computational Linguistics. http://www.aclweb.org/",
    "year": 1996
  }, {
    "title": "Supervised noun phrase coreference research: The first fifteen years",
    "authors": ["Vincent Ng."],
    "venue": "Proceedings of the Association for Computational Linguistics. http://www.aclweb.org/ anthology/P10-1142.",
    "year": 2010
  }, {
    "title": "Compositional semantic parsing on semi-structured tables",
    "authors": ["Panupong Pasupat", "Percy Liang."],
    "venue": "Proceedings of the Association for Computational Linguistics. https://doi.org/10. 3115/v1/P15-1142.",
    "year": 2015
  }, {
    "title": "Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning",
    "authors": ["Baolin Peng", "Xiujun Li", "Lihong Li", "Jianfeng Gao", "Asli Celikyilmaz", "Sungjin Lee", "Kam-Fai Wong."],
    "venue": "Proceedings of the Conference on",
    "year": 2017
  }, {
    "title": "Grounded unsupervised semantic parsing",
    "authors": ["Hoifung Poon."],
    "venue": "Proceedings of the Association for Computational Linguistics. http://www.aclweb. org/anthology/P13-1092.",
    "year": 2013
  }, {
    "title": "Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability",
    "authors": ["Ana-Maria Popescu", "Alex Armanasu", "Oren Etzioni", "David Ko", "Alexander Yates."],
    "venue": "International Conference on Computa-",
    "year": 2004
  }, {
    "title": "Abstract syntax networks for code generation and semantic parsing",
    "authors": ["Maxim Rabinovich", "Mitchell Stern", "Dan Klein."],
    "venue": "Proceedings of the Association for Computational Linguistics. https: //doi.org/10.18653/v1/P17-1105.",
    "year": 2017
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."],
    "venue": "Association for the Advancement of Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."],
    "venue": "Association for the Advancement of Artificial Intel-",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Neural Information Processing Systems.",
    "year": 2014
  }, {
    "title": "What is left to be understood in ATIS",
    "authors": ["Gökhan Tür", "Dilek Z. Hakkani-Tür", "Larry P. Heck"],
    "venue": "In Proceedings of the Spoken Language Technology Workshop",
    "year": 2010
  }, {
    "title": "Morpho-syntactic lexical generalization for CCG semantic parsing",
    "authors": ["Adrienne Wang", "Tom Kwiatkowski", "Luke Zettlemoyer."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.",
    "year": 2014
  }, {
    "title": "A learning algorithm for continually running fully recurrent neural networks",
    "authors": ["Ronald J. Williams", "David Zipser."],
    "venue": "Neural Computation 1:270–280.",
    "year": 1989
  }, {
    "title": "SQLNet: Generating structured queries from natural language without reinforcement learning",
    "authors": ["Xiaojun Xu", "Chang Liu", "Dawn Xiaodong Song."],
    "venue": "CoRR abs/1711.04436.",
    "year": 2017
  }, {
    "title": "A syntactic neural model for general-purpose code generation",
    "authors": ["Pengcheng Yin", "Graham Neubig."],
    "venue": "Proceedings of the Association for Computational Linguistics. https://doi.org/10. 18653/v1/P17-1041.",
    "year": 2017
  }, {
    "title": "Comparative results on using inductive logic programming for corpus-based parser construction",
    "authors": ["John M. Zelle", "Raymond J. Mooney."],
    "venue": "Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing.",
    "year": 1996
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence.",
    "year": 2005
  }, {
    "title": "Online learning of relaxed CCG grammars for parsing to logical form",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
    "year": 2007
  }, {
    "title": "Learning context-dependent mappings from sentences to logical form",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the Joint Conference of the Association for Computational Linguistics and the International Joint",
    "year": 2009
  }, {
    "title": "Seq2SQL: Generating structured queries from natural language using reinforcement learning",
    "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher."],
    "venue": "CoRR abs/1709.00103. 2249",
    "year": 2017
  }],
  "id": "SP:3171f531bdd917ebe9a9849339ee67f0e9ce887b",
  "authors": [{
    "name": "Alane Suhr",
    "affiliations": []
  }, {
    "name": "Srinivasan Iyer",
    "affiliations": []
  }, {
    "name": "Yoav Artzi",
    "affiliations": []
  }],
  "abstractText": "We propose a context-dependent model to map utterances within an interaction to executable formal queries. To incorporate interaction history, the model maintains an interaction-level encoder that updates after each turn, and can copy sub-sequences of previously predicted queries during generation. Our approach combines implicit and explicit modeling of references between utterances. We evaluate our model on the ATIS flight planning interactions, and demonstrate the benefits of modeling context and explicit references.",
  "title": "Learning to Map Context-Dependent Sentences to Executable Formal Queries"
}