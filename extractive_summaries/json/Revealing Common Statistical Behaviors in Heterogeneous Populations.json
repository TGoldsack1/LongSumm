{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Revealing common statistical behaviors among a group of subjects is fundamental to neuroscience and bio-medical data analysis. For example, in functional magnetic resonance imaging (fMRI) research (Bullmore et al., 1996; Smith et al., 2011; Varoquaux & Craddock, 2013), group level analyses are used for detecting brain networks from resting-state recordings (Fox et al., 2005), for detecting activities of specific regions in response to various stimuli (Haxby et al., 2001), for studying the connectivity of a specific brain region to other regions through seed based\n1Electrical Engineering Dept., Technion, Israel. Correspondence to: Andrey Zhitnikov <andreyz@campus.technion.ac.il>, Rotem Mulayoff <smulayof@campus.technion.ac.il>, Tomer Michaeli <tomer.m@ee.technion.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nanalysis (Hagler et al., 2006), etc. Group analyses often rely on the assumption that all subjects in the group behave according to the same statistical model. For example, to estimate the covariance (or partial covariance) matrix of several variables, a popular approach is to average the covariance matrices estimated for each of the individual subjects in the group (Power et al., 2011). This is done using either the Euclidean mean (arithmetic average) or the intrinsic (Riemannian) mean (Förstner & Moonen, 2003), (Fletcher & Joshi, 2007), which respects the geometry of the manifold of positive definite matrices (Varoquaux et al., 2010a).\nReal data, however, rarely conform to this assumption. Often times, each subject in a group deviates from the common model in a different way. For example, it has been shown that estimates of connectivity patterns from fMRI scans, tend to vary significantly between subjects (Moussa et al., 2012). Subject-specific deviations may even be more dominant than the common model itself. Therefore, if ignored, these deviations may severely degrade the quality of the estimate of the common model. This phenomenon is illustrated in Fig. 1 in the context of nonparametric density estimation of two variables (brain regions). In this example, the deviations from the common model are additive and have a different distribution for each subject. Thus, as can be seen on the right, kernel density estimation (KDE) applied to the entire group, fails to reveal the common behavior.\nApproaches for accounting for subject-specific deviations often make limiting assumptions. For example, in the context of covariance estimation, (Varoquaux et al., 2010b) assumed that the precision matrices of all subjects in the group have the same sparsity pattern, and proposed a modified graph Lasso technique (Friedman et al., 2008) for simultaneously estimating those matrices. In (Marrelec et al., 2006), the authors assumed that each subject’s samples follow a Gaussian distribution with a covariance matrix that follows an inverse Wishart distribution around the group covariance. In the context of regression, a popular strategy is to use a linear mixed-effects model (Friston et al., 2005; Chen et al., 2013), which relies on a Gaussian distribution assumption for the subject specific factors. Similar lines of work include grouplevel independent component analysis (ICA) (Calhoun et al., 2001; Beckmann & Smith, 2005; Varoquaux et al., 2010c), dictionary learning (Varoquaux et al., 2011; Mensch et al., 2016), and causal structure estimation (Ramsey et al., 2010).\nIn this paper we present non-parametric methods for estimating a common model in the presence of subject-specific noise factors. Specifically, we present a common-covariance estimation algorithm and a common probability density function (pdf) estimation method, both of which do not assume any particular form for the underlying distributions. Our only assumption is that the subject-specific noise factors are additive and have diverse distributions (otherwise they could be considered part of the common model). In this setting, the Euclidean and Riemannian mean estimates do not approach the true covariance matrix as the number of subjects grows. In contrast, we prove that our estimate does converge to the true covariance under very mild assumptions. We verify the advantages of our approach through extensive experiments on simulated and on real data."
  }, {
    "heading": "2. Problem formulation",
    "text": "Let u ∈ Rd be a random vector, which represents the common source of variability across a group of subjects. For example, in Fig. 1, u ∈ R2 is distributed according to the ‘ground truth’ density function (top right). Let xj ∈ Rd be a random vector, which represents the jth subject in the group (in Fig. 1, the jth scatter plot shows realizations of xj). We assume the additive model\nxj = u+ vj , (1)\nwhere {vj} are random vectors that are independent of u and represent subject-specific factors. Generally, each vj has a different distribution (had they been distributed identically, they would have been part of the common model).\nGiven realizations of xj , for j = 1 . . .m, our goal is to estimate statistical properties of the common component u.\nIn particular, we are interested in estimating either the covariance matrix Σu or the full pdf pu of u.\nObviously, the performance in those estimation tasks will generally depend on both the number of subjects m and the number of samples per subject. However, here, we are interested in the common situation in which the number of samples per subject suffices to obtain reasonably accurate estimates for Σxj or pxj (e.g., when the dimension d is relatively small). Our assumption is thus that the covariances (or pdfs) of the subjects xj are known and our focus is on the problem of recovering the common covariance (or pdf) from them. To apply our algorithms in practice, one has to plug in estimates of the covariances (or pdfs) of the subjects (obtained using, e.g., KDE)."
  }, {
    "heading": "3. Common covariance estimation",
    "text": "Since u and vj are independent, we have from (1) that\nΣxj = Σu + Σvj (2)\nfor every j = 1, . . . ,m. We would like to estimate the covariance matrix Σu of the common component, given the covariance matrices {Σxj} of the subjects. To avoid ambiguity, we define the common component Σu to be the largest one satisfying such a decomposition. In particular, this means that the smallest eigenvalue of (at least some of) the subject-specific factors {Σvj} must be arbitrarily small. Indeed, otherwise there would exist some α > 0 such that Σvj αI for every j so that αI would be common to all {Σvj} and not subject-specific. In other words, the common component in this case is in fact Σu + αI and the noise covariances are Σvj − αI .\nLet us first informally describe the key idea underlying\nour method, and then provide a formal “group consistency” result. Denote the eigenvalues of Σu by λ1 ≤ λ2 ≤ . . . , λd and the corresponding eigenvectors by q1, q2, . . . , qd. We will begin by estimating the smallest eigenvalue, λ1, and its associated eigenvector, q1. By definition,\nλ1 = min ‖q‖=1\nqTΣuq. (3)\nNow, observe that\nqTΣuq ≤ qTΣxjq, ∀j,∀q (4)\nsince qTΣxjq = q TΣuq + q TΣvjq and q TΣvjq ≥ 0. Our assumption, which we formalize mathematically below, is that the subject-specific noise covariances Σvj are diverse in the sense that their bottom eigenvectors tend to point in different directions. This, together with the fact their smallest eigenvalue can be arbitrarily small, implies that as the number of subjects grows, it becomes increasingly likely that for every direction q, at least one of the values {qTΣvjq}mj=1 be small. This motivates us to estimate λ1 and q1 as\nq̂1 = arg min ‖q‖=1 min j∈{1,...,m}\nqTΣxjq, (5)\nλ̂1 = min ‖q‖=1 min j∈{1,...,m}\nqTΣxjq. (6)\nThat is, we minimize over the pointwise minimum of the quadratic functions of the individual subjects. Figure 2 visualizes this objective for the case of 2× 2 matrices. Here, the thick blue curve corresponds to the desired objective function (3), which we cannot directly minimize (as it involves the unknown Σu). The thin curves correspond to\nAlgorithm 1 Common covariance estimation Input: Covariance matrices Σx1 , . . . ,Σxm in Rd×d. Output: Common covariance estimate Σ̂u. for k = 1 . . . d do\nUsing (14), compute q̂k and λ̂k as\nq̂k = arg min q∈Sk min j∈{1,...,m}\nqTΣxjq, (11)\nλ̂k = min q∈Sk min j∈{1,...,m}\nqTΣxjq, (12)\nwhere\nSk = { q : ‖q‖ = 1, q ⊥ span{q̂1, . . . , q̂k−1} } .\n(13)\nend for Construct Σ̂u from {q̂k}mk=1 and {λ̂k}mk=1 as in (10).\nthe quadratic functions of the subjects (involving the known matrices {Σxj}). As can be seen, the pointwise minimum of the thin curves (dotted curve) is close to the thick curve when the number of subjects is large.\nNext, we turn to estimate λ2 and q2. Note that\nλ2 = min {q:‖q‖=1,q⊥q1}\nqTΣuq\n≤ min {q:‖q‖=1,q⊥q1} qTΣxjq, ∀j. (7)\nTherefore, following the logic above, and replacing q1 by its estimate q̂1, we propose to calculate q̂2 and λ̂2 as\nq̂2 = arg min {q:‖q‖=1,q⊥q̂1} min j∈{1,...,m}\nqTΣxjq, (8)\nλ̂2 = min {q:‖q‖=1,q⊥q̂1} min j∈{1,...,m}\nqTΣxjq. (9)\nThis process can be repeated, where at the kth step, we constrain the search to the subspace orthogonal to span{q̂1, . . . , q̂k−1}. The last eigenvector, q̂d, is completely determined by q̂1, . . . , q̂d−1 and thus does not involve an optimization problem. The associated eigenvalue is estimated as λ̂d = minj∈{1,...,m} q̂ T d Σxj q̂d.\nHaving estimated all the eigenvalues and eigenvectors, we construct our estimate of Σu as\nΣ̂u = d∑ k=1 λ̂kq̂kq̂ T k . (10)\nThis is summarized in Alg. 1."
  }, {
    "heading": "3.1. Practical implementation",
    "text": "The objective in Problems (11),(12) is the pointwise minimum of a finite set of continuous (quadratic) functions over\na compact set. Therefore, the minimum is attained at the minimum of one of those functions, each of which has a closed form. Specifically, when k = 1, we only have the constraint ‖q‖ = 1, and the minimum of the jth problem is the smallest eigenvalue of Σxj (attained by the corresponding eigenvector). When k > 1, we have an additional set of linear constraints, which can be written asQkq = 0, where Qk = ∑k−1 i=1 q̂iq̂ T i . In this case, the minimizer is given by the top eigenvector of (I −Qk)(cI −Σxj )(I −Qk), which we denote by vkj , where c is any constant such that cI −Σxj 0 (Blau & Michaeli, 2017). Thus, in summary,\nq̂k = v k j∗ , λ̂k = (v k j∗) TΣxjv k j∗ , (14)\nwhere j∗ = arg minj∈{1,...,m}(v k j ) TΣxjv k j .\nIn the Supplementary Material, we discuss ways to speed up the estimation on parallel platforms."
  }, {
    "heading": "3.2. Group consistency",
    "text": "To analyze the behavior of Alg. 1 as the number of subjects m increases, one must assume something regarding the variability of the subject-specific noise covariances Σvj . A rather general assumption is that they are independent draws from some distribution over PSD matrices, namely\nΣvj ∼ pΣv . (15)\nThe next theorem shows that under very mild conditions on pΣv , our estimate Σ̂u converges to Σu almost surely (a.s.). We refer to this as group consistency.\nTheorem 1 (Group consistency). Assume that\nP (λmax(Σv) ≤ α) = 1 (16)\nfor some α > 0 and that P ( qTΣvq ≤ ) > 0 (17)\nfor every > 0 and every unit-norm q. Let Σ̂ m\nu denote the estimate produced by Alg. 1 using m subjects. Then\nP (\nlim m→∞ ∥∥∥Σ̂mu −Σu∥∥∥ = 0) = 1. (18) Assumption (16) merely states that the noise factors are not arbitrarily large. Assumption (17) is a condition on the distribution of the smallest eigenvalue of Σv and its associated eigenvector. Roughly speaking, it requires that there be a positive probability for the smallest eigenvalue to be arbitrarily small and, simultaneously, for the corresponding eigenvector to point in any direction (i.e., this eigenvector can have any distribution on the unit sphere as long as it does not vanish on a set of nonzero Lebesgue measure). Recall that the condition on the smallest eigenvalue is actually\npart of the definition of the common covariance estimation problem, and therefore not a limiting assumption.\nTo prove the theorem, let us denote ψ(q) , qTΣuq, gj(q) , qTΣvjq, and hm(q) , minj∈{1,...,m} gj(q). Note that ψ(q) is a deterministic function (as Σu is deterministic) whereas {gj(q)} and {hm(q)} are sequences of random functions (as {Σvj} are random). We will need the following lemmas (see proofs in the Supplementary).\nLemma 1. For every q, the sequence of random variables {hm(q)} converges to zero almost surely. Furthermore, for any sequence of vectors {qm}∞m=1 that converges to some vector q∗, the sequence of random variables {hm(qm)} converges to zero almost surely.\nLemma 2. Let φ(q) be a continuous bounded function on a compact set C, which achieves a strict global minimum at q∗ ∈ C. Let {fn(q)}∞n=1 be a sequence of continuous bounded nonnegative functions on C satisfying fn(q∗)→ 0, and let wn(q) = φ(q) + fn(q). Then any sequence of the form qn ∈ arg minq∈C wn(q) converges to q∗, and the sequence wn(qn) converges to φ(q ∗).\nproof of Theorem 1. For simplicity, we prove the theorem for d = 2. The extension to higher dimensions is similar.\nSince problem (11) is symmetric, we can divide the unit circle into two disjoint half circles A and B such that A is closed, and restrict the search for the minimum to A. Let us first assume that λ1 6= λ2. In this case, the minimum of ψ(q) over the unit circle is achieved at the points q1 and −q1. Without loss of generality, we assume that q1 ∈ A and −q1 ∈ B. The objective in (11) can be written as ψ(q) + hm(q). Since hm(q) is continuous for every m and hm(q1)\na.s.→ 0 (Lemma 1), the conditions of Lemma 2 hold a.s. Therefore, our estimate of the bottom eigenvector, q̂m1 , converges a.s. to the true eigenvector q1, namely\nq̂m1 a.s.→ q1. (19)\nOur estimate (12) of the bottom eigenvalue, λ̂m1 , can be written as ψ(q̂m1 ) + hm(q̂ m 1 ). Since q̂ m 1\na.s.→ q1, we have from Lemma 1 that hm(q̂ m 1 ) a.s.→ 0, and therefore λ̂m1 a.s.→ ψ(q1) = λ1 as well.\nThe top eigenvector is given by q2 = Rq1, where R is a 90◦ rotation matrix, and our estimate of this eigenvector is simply q̂2 = Rq̂1. Therefore, (19) implies that also\nq̂m2 a.s.→ q2. (20)\nThe convergence of λ̂m2 to λ2 follows similarly by Lemma 1.\nLet us now treat the case where λ1 = λ2. In this setting, the vectors q̂m1 , q̂ m 2 do not necessarily converge. However, for the matrix Σ̂ m\nu to converge to Σu, it suffices that only\nthe eigenvalue estimates λ̂m1 , λ̂ m 2 converge to λ1, λ2 (in that case, the vectors q̂m1 , q̂ m 2 have no effect in (10)). To see that the eigenvalues converge, note that the solution of (12) is bounded from below by minq∈S1 ψ(q) = λ1, because hm(q) ≥ 0. Additionally, we have that\nλ̂m1 = min q∈S1 min j∈{1,...,m} qTΣxjq\n= λ1 + min q∈S1 hm(q) ≤ λ1 + hm(q̄) a.s.→ λ1, (21)\nwhere q̄ is an arbitrary point in S1, and the convergence is due to Lemma 1. Therefore λ̂m1 converges to λ1. Similar arguments can be invoked to show that λ̂m2 converges to λ2.\nSince the eigenvectors and eigenvalues converge, Σ̂ m\nu converges to Σu, and the proof is complete."
  }, {
    "heading": "4. Common density function estimation",
    "text": "Next, we address the problem of estimating the pdf pu of the common component, given the pdfs {pxj} of the subjects in the group.\nSince u and xj are statistically independent, we have that pxj (α) = ( pu ∗ pvj ) (α), (22)\nwhere ‘∗’ denotes convolution. Furthermore,\nϕxj (t) = ϕu(t)ϕvj (t), (23)\nwhere ϕz(t) = E[ejt T z] denotes the characteristic function of a random vector z. We will focus on estimating ϕu(t), from which pu can be retrieved by a Fourier transform.\nA well known property of characteristic functions is that |ϕz(t)| ≤ 1 for every t. Therefore, we have from (23) that |ϕu(t)| ≥ |ϕxj (t)| for every j and for all t. In particular,\n|ϕu(t)| ≥ max j∈{1,...,m} ∣∣ϕxj (t)∣∣ , ∀t. (24) Based on this observation, we propose to take the maximum among the values {|ϕxj (t)|}mj=1 as our estimate of |ϕu(t)|, for every t. The idea is that if the noise characteristic functions {ϕvj (t)} are diverse, then for every t, it is likely that at least one of them attain a value close to 1 (in absolute value). Namely, at least one of the values {|ϕxj (t)|} is close to |ϕu(t)|, which justifies our estimator. To estimate the phase of ϕu(t), we take the phase of the characteristic function ϕxj (t) that attains the maximum. That is, we construct our estimate as\nk(t) = arg max j∈{1,...,m} |ϕxj (t)|,\nϕ̂u(t) = ϕxk(t)(t). (25)\nAlgorithm 2 Common density estimation Input: Density functions px1 , . . . , pxm . Output: Common density estimate p̂u. for j = 1 . . .m do\nSet ϕxj ← IDFT{pxj}. for all t do\nSet k as the index of the largest value in {ϕxj (t)}. Set ϕ̂u(t)← ϕxk(t).\nend for end for Set p̂u ← DFT{ϕ̂u}. Truncate the negative values of p̂u and normalize it to have unit area.\nNote that our phase estimate is accurate when the pdfs {pvj} are symmetric (e.g., when {vj} are zero-mean Gaussian random vectors). Indeed, in that case the phase of ϕvj is zero, so that the phase of ϕu equals the phase of ϕxj . Our common pdf estimation algorithm is summarized in Alg. 2.\nIt is interesting to note that Alg. 2 has been proposed in the Image Processing community, as a way of removing blur from several blurry images the of same scene (Delbracio & Sapiro, 2015). The analogy to our setting is quite natural. The functions pxj in our context can be thought of as “blurry” versions of the function pu, where the “blur kernels” are the functions pvj (see (22))."
  }, {
    "heading": "5. Experiments",
    "text": "In this section we verify the effectiveness of our methods, first on simulated data and then on real data."
  }, {
    "heading": "5.1. Estimation of Pearson correlation coefficient",
    "text": "In our first experiment, we study the behavior of our common covariance estimator as a function of the number of subjects and the signal to noise ratio (SNR). We take the common component u to be a two-dimensional random vector with covariance matrix\nΣu =\n( 1 0.5\n0.5 1\n) . (26)\nOur goal is to estimate the Pearson correlation coefficient between u(1) and u(2) (which is ρ = 0.5 in this case) from the perturbed versions Σxj = Σu + Σvj . This can be done by first estimating Σu and then normalizing the offdiagonal entry by the square-roots of the diagonal entries. We compare our estimator (Alg. 1) with naive averaging of {Σxj} using either Euclidean or Riemannian mean.\nWe generate the matrices {Σvj} as\nΣvj = M jΛjM T j , (27)\nwhereM j are random rotation matrices whose angles are distributed uniformly in [0, 2π], and Λj are random diagonal matrices Λj = diag{βj1, β j 2} with β j 1 ∼ U [0, b] and βj2 ∼ U [b, 2b] for some b > 0. We draw {M j}, {β j 1}, {β j 2} independently. The SNR, which we define as SNR = Tr{Σu}/E{Tr{Σv}}, is 1/b in this case.\nFigures 3 and 4 visualize the mean and variance of our estimator as well as of the naive Euclidean and Riemanian mean estimators (using 200 trials per setting) as functions of the number of subjects and the SNR. As can be seen, while the variance of our estimator is slightly larger than the variances of the naive estimators, its bias is significantly smaller. Therefore, overall, it attains a substantially lower mean square error. Figure 3 also indicates that our estimator is asymptotically (in the number of subjects) unbiased. The naive estimators, on the other hand, have severe biases, which do not decrease with the number of subjects. Figure 4 further illustrates that the performance of the naive\nestimators degrades rapidly as the SNR decreases, while our estimator remains relatively accurate even at low SNRs.\nIn this example, the poor performance of the naive estimators is mainly rooted in their over-estimation of the diagonal entries of Σu. This happens because the contributions of the noise matrices {Σvj} are only positive on the diagonal, so that averaging does not cancel them out."
  }, {
    "heading": "5.2. Clustered subject-specific noise covariances",
    "text": "In most practical cases, the advantage of our approach is not confined to the diagonal elements of Σu. Specifically, although our algorithm relies on the diversity of the noise covariances, it does not require their eigenvectors to be uniformly distributed on the unit sphere. Therefore, our technique can even handle cases in which the noise covariances tend to cluster around a certain matrix. As long as there exists a nonzero probability to encounter matrices away from the cluster, our algorithm is guaranteed to produce an accurate estimate as the number of subjects grows. This is in contrast to naive averaging, which typically produces estimates with severe bias in all matrix entries.\nTo illustrate this, we next perform a 3× 3 common covariance estimation experiment. We generate Σvj as in (27), where now we construct the unitary matrixM j assin(θ1) cos(θ2) sin(θ1) sin(θ2) cos(θ1)cos(θ1) cos(θ2) cos(θ1) sin(θ2) − sin(θ1)\n− sin(θ2) cos(θ2) 0  , (28) with θ1 and θ2 being two independent random variables with a normal distribution N (1, 1) truncated to [0, π] and\n[0, 2π], respectively (Chopin, 2011).\nFigure 5 depicts the estimation results obtained with Alg. 1 and with naive averaging, using 1000 subjects. We show results for three different common covariance matrices. These include a zero matrix (first row), an identity matrix (second row), and a random PSD matrix (third row). As can be seen, the Euclidean and Riemannian means produce inaccurate estimates in all entries of the matrix while our estimator produces accurate results. This is despite the preference of the noise covariances towards specific patterns."
  }, {
    "heading": "5.3. FMRI data",
    "text": "Next, we applied our covariance estimation algorithm on the ADHD200-preprocessed dataset (Bellec et al., 2017). We used the Athena pipeline. In particular, we used preprocessed resting state fMRI data, written into MNI space at\n4mm×4mm×4mm voxel resolution. We removed nuisance variance (Lund, 2001; Fox et al., 2005), applied a temporal bandpass filter (0.009 Hz < f < 0.08 Hz) (Fox et al., 2005; Biswal et al., 1995; Cordes et al., 2001) and a spatial Gaussian filter (6mm FWHM), and removed linear trend from the extracted time-courses. We took the 458 control subjects from the published training set (for results on 141 subjects with ADHD, please see the Supplementary). From each subject, we extracted time-courses of 39 regions of interest (ROI) of the MSDL atlas (Varoquaux et al., 2011) and estimated their covariance using the Ledoit-Wolf estimator (Ledoit & Wolf, 2004). This gave us a 39× 39 covariance matrix per subject. We estimated the common covariance matrix using Alg. 1, using Geometric (Riemannian) mean (Varoquaux et al., 2010a), and using Euclidean mean. From the estimated covariances, we calculated correlation matrices. We used the nilearn and scikit-learn python packages\n(Abraham et al., 2014; Pedregosa et al., 2011; Buitinck et al., 2013). The running time of Alg. 1 was about 10s on an 8 core Intel i7-6700 with 16GB of RAM working at 3.40GHz. The results are depicted in Fig. 6.\nIt has been shown that estimates of connectivity patterns often vary significantly between subjects (Moussa et al., 2012). As can be seen in Fig. 6, our estimator detects activity within known networks despite the large variability between subjects. In particular, our estimator detects stronger correlations than the Euclidean and Riemannian mean estimators within the Default Mode Network, the Right Ventral Attention network, the Left Ventral Attention network, and the Cingulate Insula (connectivity between cingulate cortex and insula) (Moussa et al., 2012). Zoomed versions of those networks are shown in Fig. 7. Note that the Euclidean mean estimator shows very low correlations within some of those regions."
  }, {
    "heading": "5.4. Common density of PPG and ABP",
    "text": "In our last experiment, we used Alg. 2 to estimate the joint density function of arterial blood pressure (ABP) and photoplethysmogram (PPG) recordings. We used measurements from 25 subjects in critical care taken from the MIMIC 2 dataset (Kachuee et al., 2015). As a preprocessing step, we normalized the signals to have zero mean and unit variance.\nFor each subject, we then estimated the 2D pdf of ABP and PPG using Gaussian KDE with bandwidth 0.08. From the resulting 25 pdfs, we estimated the common pdf using Alg. 2. As can be seen in Fig. 8, our algorithm manages to reveal delicate structures in the common pdf, which are not seen when applying KDE on all the data from all the subjects. In the Supplementary, we show that these structures are not detected with naive KDE with any bandwidth. This illustrates again the ability of our approach to suppress subject-specific noise factors that have different distributions."
  }, {
    "heading": "6. Conclusion",
    "text": "We presented algorithms for estimating the covariance and the pdf of the common component of a group of subjects, when noise has a different distribution for each subject. Our algorithms take advantage of the diversity of the subjectspecific noise distributions in order to efficiently suppress them. In contrast to previous approaches, we did not assume any parametric model for the underlying distributions. We proved that under rather mild assumptions, our common covariance estimate tends to the covariance of the common component as the number of subjects grows. We presented experiments on simulated and on real data, which confirmed the advantages of our methods over alternative approaches."
  }],
  "year": 2018,
  "references": [{
    "title": "Machine learning for neuroimaging with scikit-learn",
    "authors": ["A. Abraham", "F. Pedregosa", "M. Eickenberg", "P. Gervais", "A. Mueller", "J. Kossaifi", "A. Gramfort", "B. Thirion", "G. Varoquaux"],
    "venue": "Frontiers in neuroinformatics,",
    "year": 2014
  }, {
    "title": "Tensorial extensions of independent component analysis for multisubject fmri analysis",
    "authors": ["C.F. Beckmann", "S.M. Smith"],
    "year": 2005
  }, {
    "title": "The neuro bureau adhd-200 preprocessed repository",
    "authors": ["P. Bellec", "C. Chu", "F. Chouinard-Decorte", "Y. Benhajali", "D.S. Margulies", "R.C. Craddock"],
    "year": 2017
  }, {
    "title": "Functional connectivity in the motor cortex of resting human brain using echo-planar mri",
    "authors": ["B. Biswal", "F. Zerrin Yetkin", "V.M. Haughton", "J.S. Hyde"],
    "venue": "Magnetic resonance in medicine,",
    "year": 1995
  }, {
    "title": "Non-redundant spectral dimensionality reduction",
    "authors": ["Y. Blau", "T. Michaeli"],
    "venue": "In ECML/PKDD,",
    "year": 2017
  }, {
    "title": "Statistical methods of estimation and inference for functional mr image analysis",
    "authors": ["E. Bullmore", "M. Brammer", "S.C. Williams", "S. Rabe-Hesketh", "N. Janot", "A. David", "J. Mellers", "R. Howard", "P. Sham"],
    "venue": "Magnetic Resonance in Medicine,",
    "year": 1996
  }, {
    "title": "A method for making group inferences from functional mri data using independent component analysis",
    "authors": ["V.D. Calhoun", "T. Adali", "G.D. Pearlson", "J. Pekar"],
    "venue": "Human brain mapping,",
    "year": 2001
  }, {
    "title": "Linear mixed-effects modeling approach to fmri group analysis",
    "authors": ["G. Chen", "Z.S. Saad", "J.C. Britton", "D.S. Pine", "R.W. Cox"],
    "year": 2013
  }, {
    "title": "Fast simulation of truncated gaussian distributions",
    "authors": ["N. Chopin"],
    "venue": "Statistics and Computing,",
    "year": 2011
  }, {
    "title": "Removing camera shake via weighted fourier burst accumulation",
    "authors": ["M. Delbracio", "G. Sapiro"],
    "venue": "IEEE Transactions on Image Processing,",
    "year": 2015
  }, {
    "title": "Riemannian geometry for the statistical analysis of diffusion tensor data",
    "authors": ["P.T. Fletcher", "S. Joshi"],
    "venue": "Signal Processing,",
    "year": 2007
  }, {
    "title": "A metric for covariance matrices",
    "authors": ["W. Förstner", "B. Moonen"],
    "venue": "In Geodesy-The Challenge of the 3rd Millennium,",
    "year": 2003
  }, {
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "year": 2008
  }, {
    "title": "Mixed-effects and fmri studies",
    "authors": ["K.J. Friston", "K.E. Stephan", "T.E. Lund", "A. Morcom", "S. Kiebel"],
    "year": 2005
  }, {
    "title": "Smoothing and cluster thresholding for cortical surface-based group analysis of fmri data",
    "authors": ["D.J. Hagler", "A.P. Saygin", "M.I. Sereno"],
    "year": 2006
  }, {
    "title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex",
    "authors": ["J.V. Haxby", "M.I. Gobbini", "M.L. Furey", "A. Ishai", "J.L. Schouten", "P. Pietrini"],
    "year": 2001
  }, {
    "title": "Cuff-less high-accuracy calibration-free blood pressure estimation using pulse transit time",
    "authors": ["M. Kachuee", "M.M. Kiani", "H. Mohammadzade", "M. Shabany"],
    "venue": "In Circuits and Systems (ISCAS),",
    "year": 2015
  }, {
    "title": "A well-conditioned estimator for large-dimensional covariance matrices",
    "authors": ["O. Ledoit", "M. Wolf"],
    "venue": "Journal of multivariate analysis,",
    "year": 2004
  }, {
    "title": "fcmrimapping functional connectivity or correlating cardiac-induced noise? Magnetic resonance in medicine",
    "authors": ["T.E. Lund"],
    "year": 2001
  }, {
    "title": "Partial correlation for functional brain interactivity investigation in functional mri",
    "authors": ["G. Marrelec", "A. Krainik", "H. Duffau", "M. Pélégrini-Issac", "S. Lehéricy", "J. Doyon", "H. Benali"],
    "year": 2006
  }, {
    "title": "Compressed online dictionary learning for fast resting-state fmri decomposition",
    "authors": ["A. Mensch", "G. Varoquaux", "B. Thirion"],
    "venue": "In Biomedical Imaging (ISBI),",
    "year": 2016
  }, {
    "title": "Consistency of network modules in resting-state fmri connectome data",
    "authors": ["M.N. Moussa", "M.R. Steen", "P.J. Laurienti", "S. Hayasaka"],
    "venue": "PloS one,",
    "year": 2012
  }, {
    "title": "Functional network organization of the human brain",
    "authors": ["J.D. Power", "A.L. Cohen", "S.M. Nelson", "G.S. Wig", "K.A. Barnes", "J.A. Church", "A.C. Vogel", "T.O. Laumann", "F.M. Miezin", "Schlaggar", "B. L"],
    "year": 2011
  }, {
    "title": "Six problems for causal inference from fmri",
    "authors": ["J.D. Ramsey", "S.J. Hanson", "C. Hanson", "Y.O. Halchenko", "R.A. Poldrack", "C. Glymour"],
    "venue": "neuroimage,",
    "year": 2010
  }, {
    "title": "Network modelling methods for fmri",
    "authors": ["S.M. Smith", "K.L. Miller", "G. Salimi-Khorshidi", "M. Webster", "C.F. Beckmann", "T.E. Nichols", "J.D. Ramsey", "M.W. Woolrich"],
    "year": 2011
  }, {
    "title": "Learning and comparing functional connectomes across subjects",
    "authors": ["G. Varoquaux", "R.C. Craddock"],
    "year": 2013
  }, {
    "title": "Brain covariance selection: better individual functional connectivity models using population prior",
    "authors": ["G. Varoquaux", "A. Gramfort", "Poline", "J.-B", "B. Thirion"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2010
  }, {
    "title": "A group model for stable multi-subject",
    "authors": ["G. Varoquaux", "S. Sadaghiani", "P. Pinel", "A. Kleinschmidt", "Poline", "J.-B", "B. Thirion"],
    "venue": "ica on fmri datasets. Neuroimage,",
    "year": 2010
  }, {
    "title": "Multi-subject dictionary learning to segment an atlas of brain spontaneous activity",
    "authors": ["G. Varoquaux", "A. Gramfort", "F. Pedregosa", "V. Michel", "B. Thirion"],
    "venue": "In Biennial International Conference on Information Processing in Medical Imaging,",
    "year": 2011
  }],
  "id": "SP:8ded569980c17ceca79175fca2490121613f2071",
  "authors": [{
    "name": "Andrey Zhitnikov",
    "affiliations": []
  }, {
    "name": "Rotem Mulayoff",
    "affiliations": []
  }, {
    "name": "Tomer Michaeli",
    "affiliations": []
  }],
  "abstractText": "In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects. Such analyses play important roles e.g., in detecting functional brain networks from fMRI scans and in identifying brain regions which show increased activity in response to certain stimuli. Group level techniques usually assume that all subjects in the group behave according to a single statistical model, or that deviations from the common model have simple parametric forms. Therefore, complex subject-specific deviations from the common model severely impair the performance of such methods. In this paper, we propose nonparametric algorithms for estimating the common covariance matrix and the common density function of several variables in a heterogeneous group of subjects. Our estimates converge to the true model as the number of subjects tends to infinity, under very mild conditions. We illustrate the effectiveness of our methods through extensive simulations as well as on real-data from fMRI scans and from arterial blood pressure and photoplethysmogram measurements.",
  "title": "Revealing Common Statistical Behaviors in Heterogeneous Populations"
}