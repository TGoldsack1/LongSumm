{
  "sections": [{
    "heading": "1. Introduction",
    "text": "KDE (Rosenblatt, 1956; Parzen, 1962) is a foundational aspect of nonparametric statistics. It is a powerful method to estimate the probability density function of a random variable. Moreover, it is simple to compute and has played a significant role in a very wide range of practical applications. Its convergence properties have been studied for a long time with most of the work dedicated to its asymptotic behavior or mean-squared risk (Tsybakov, 2008). However, there is still a surprising amount not yet fully understood about its convergence behavior. In this paper, we focus on the uniform finite-sample facet of KDE convergence theory. We handle the multivariate KDE setting in Rd which allows a d × d bandwidth matrix H. This generalizes the scalar bandwidth h > 0 i.e. H = h2I. Such a generalization is significant to multivariate statistics e.g. Silverman (1986); Simonoff (1996).\nOur work begins by using VC-based Bernstein-type uniform convergence bounds to attain finite-sample rates for a fixed unknown density f over Rd (Theorem 1). These bounds hold with high-probability under general assumptions on f and the kernel i.e. we only require f to be\n1Google. Correspondence to: Heinrich Jiang <heinrich.jiang@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nbounded as well as decay assumptions on the kernel functions. Moreover, these bounds hold uniformly over Rd and bandwidth matrices H.\nWe then show the versatility of our results by applying it to the related areas of KDE rates under `∞, mode estimation, density level-set estimation, and class probability estimation. We then extend our analysis to the manifold setting. Finally, we provide uniform finite-sample results for local intrinsic dimension estimation. Each of these contributions are significant on their own."
  }, {
    "heading": "2. Contributions and Related Works",
    "text": "`∞ bounds for KDE: It must first be noted that bounding |f̂h − f |∞ where f̂h is the KDE of f for scalar h > 0 is a more difficult problem than for example bounding the mean-squared error Ef [(f̂h − f)2]. Gine & Guillon (2002) and Einmahl & Mason (2005) give asymptotic convergence results on KDE for |f̂h − Ef f̂h|∞. In their work about density clustering, Rinaldo & Wasserman (2010) extends the results of the former to obtain high-probability finitesample bounds. This is to our knowledge the strongest and most general uniform finite-sample result about KDE thus far. We show a general bound of form |f̂h(x) − f(x)| . x +√ log n/nhd where x is a function of the kernel and the smoothness of f at x which holds with probability 1− 1/n uniformly over x ∈ Rd and h (Theorem 1). An almost direct consequence is that if we take f to be α-Hölder continuous then under the optimal choice for h ≈ n−1/(2α+d), we have |f̂h−f |∞ . n−α/(2α+d) with probability 1−1/n (Theorem 2). This matches the known lower bound (Tsybakov, 2008).\nWhen comparing our finite-sample results to that of Rinaldo & Wasserman (2010), there are a few notable differences. Our results hold uniformly across bandwidths and the probability that the bounds hold are independent of the bandwidth (in fact, holds with probability 1 − 1/n). Our results also extends to general bandwidth matrix H.\nThis can be significant to analyze KDE-based procedures with adaptive bandwidths– i.e. when the bandwidths change depending on the region. Then the need for bounds which hold simultaneously over bandwidth choices be-\ncomes clear. Such an example includes adaptive or variable KDE (Terrell & Scott, 1992; Botev et al., 2010) which extends KDE to bandwidths that vary over the data space.\nThus our result for uniform finite-sample KDE bounds can be seen as a refinement to existing results.\nMode estimation Estimating the modes of a distribution has a long history e.g. Parzen (1962); Chernoff (1964); Eddy (1980); Silverman (1981); Cheng (1995); Abraham et al. (2004); Li et al. (2007); Dasgupta & Kpotufe (2014); Genovese et al. (2015); Jiang & Kpotufe (2017). The modes can be viewed as the central tendancies of a distribution and this line of work has played a significant role in areas such as clustering, image segmentation, and anomaly detection.\nMuch of the early work focused on the estimator argmaxx∈Rd f̂h(x). While many useful insights have come from studying this, it is difficult to algorithmically compute. Abraham et al. (2004) turned to the simple estimator argmaxx∈X f̂h(x) and showed that it behaves asymptotically as argmaxx∈Rd f̂h(x) where X is the data. In this paper, we show that this estimator is actually a rate-optimal estimator of the mode under finite samples with appropriate bandwidth choice. This would not have been possible without the appropriate bounds on KDE. This approach is similar to that of Dasgupta & Kpotufe (2014), who apply their k-NN density estimation bounds to show that the kNN analogue of the estimator is rate-optimal.\nAnother approach to mode estimation that must be noted is mean-shift (Fukunaga & Hostetler, 1975; Cheng, 1995; Comaniciu & Meer, 2002; Arias-Castro et al., 2016), which is a popular clustering algorithm amongst practitioners based on performing a gradient-ascent of the KDE. Its theoretical analysis however is still far from complete; the difficulty comes from analyzing KDE’s ability to estimate gradients. Here we are focused on density estimation rather than density derivative estimation so our results do not appear immediately applicable to mean-shift.\nDensity level-set estimation The problem of density-level set estimation has been extensively studied e.g. Carmichael et al. (1968); Hartigan (1975); Cuevas & Fraiman (1997); Tsybakov (1997); Cadre (2006); Rigollet & Vert (2009); Singh et al. (2009); Rinaldo & Wasserman (2010); Steinwart (2011); Jiang (2017). It involves estimating {x : f(x) ≥ λ} for some λ > 0 and density f based on samples drawn from f . This turns out to be one of the earliest and still currently most popular means of modeling clusters in the context of density-based clustering. The level-sets also influenced much of the work on hierarchical clustering (Chaudhuri & Dasgupta, 2010).\nNaturally, we must use some density estimator to get a handle on λ. It turns out that in order to obtain the most gen-\neral uniform recovery bounds (e.g. finite-sample Hausdorff rates (Singh et al., 2009)), one also needs similar uniform density estimation bounds. The strongest known results thus far use density estimators that are often impractical (e.g. histogram density estimator) in order to obtain these theoretical rates over a practical one such as KDE. Much of the work, especially ones using more practical density estimators have focused on bounding metrics such as symmetric set difference, which are computed as an expectation over f . This is considerably weaker than the Hausdorff metric, which imposes a uniform guarantee over each estimated point and each point in the level-set.\nWe show that a simple KDE-based estimator is consistent under the Hausdorff metric; moreoever when the bandwidth is appropriately chosen, it attains the minimax optimal rate established by Tsybakov (1997).\nClass probability estimation Class probability estimation involves estimating the probability distribution over a set of classes for a given input. In other words, it is an approach to classification which involves first estimating the marginal density f(Y |X) (where X is the observation and Y is its category) and then choosing the category with highest probability. This density-based approach to classification has been studied in many places under nonparametric assumptions. e.g. Rigollet (2007); Chaudhuri et al. (2009). However, there are still aspects about its convergence properties that haven’t been fully understood. In the current work, we give uniform rates on the approximation of f(Y |X). Much of the related work assume the binary classification case and derive a hard classifier based on the marginal density and compare the risk between that and the Bayes-optimal classifier. Our work differs in that we give uniform bounds on the recovery of the marginal density, which is a considerably stronger notion of consistency. This is important in situations where a worst-case bound on classifier performance is required.\nDensity Estimation on Manifolds Density estimation on manifolds has received much less attention than the fulldimensional counterpart. However, understanding density estimation in situations where the intrinsic dimension can be much lower than the ambient dimension is becoming ever more important: modern systems are able to capture data at an increasing resolution while the number of degrees of freedom stays relatively constant. One of the limiting aspects of density-based approaches is their performance in high dimensions. It takes an exponential in dimension number of samples to estimate the density – this is the so-called curse of dimensionality. Here we give results whose rates of convergence depend on the dimension of the manifold dM compared to a much higher ambient dimension d; thus the convergence properties become much more attractive under the manifold hypothesis.\nLocal Intrinsic Dimension Estimation Many learning algorithms require the intrinsic dimension as an input in order to take advantage of the lower dimensional structure that arises. There has been much work on estimating the intrinsic dimension of the data given finite samples e.g. (Kégl, 2003). However, the more interesting problem of estimating the local intrinsic dimension has received much less attention. The bulk of the work in this area e.g. (Costa et al., 2005; Houle, 2013; Amsaleg et al., 2015) provide interesting estimators, but are unable to establish strong finitesample guarantees under nonparametric assumptions. In this paper, we consider a simple notion of local intrinsic dimension based on the doubling dimension and utilize a simple estimator. We then give a uniform finite-sample convergence result for the estimator under nonparametric assumptions. To the best of our knowledge, this is perhaps the strongest finite-sample result obtained this far for this problem."
  }, {
    "heading": "3. Background and Setup",
    "text": "Definition 1. Let f be a probability density over Rd with corresponding distribution F . Let X = {X1, ..., Xn} be n i.i.d. samples drawn from it and letFn denote the empirical distribution w.r.t. X . i.e. Fn(A) = 1n ∑n i=1 1{Xi ∈ A}.\nWe only require that f is bounded.\nAssumption 1. ||f ||∞ <∞. Definition 2. Define kernel function K : Rd → R≥0 where R≥0 denotes the non-negative real numbers such that∫\nRd K(u)du = 1.\nWe make the following mild regularity assumptions on K.\nAssumption 2. (Spherically Symmetric and nonincreasing) There exists non-increasing function k : R≥0 → R≥0 such that K(u) = k(|u|) for u ∈ Rd. Assumption 3. (Exponential Decays) There exists ρ, Cρ, t0 > 0 such that for t > t0,\nk(t) ≤ Cρ · exp(−tρ).\nRemark 1. These assumptions allow the popular kernels such as Gaussian, Exponential, Silverman, uniform, triangular, tricube, Cosine, and Epanechnikov.\nAssumption 3 implies the next result which will be useful later on. The proof is elementary and is omitted.\nLemma 1. For all m > 0, we have∫ Rd K(u)|u|mdu <∞.\nDefinition 3 (Bandwidth matrix). H is a valid bandwidth matrix if it is a positive definite and symmetric d×dmatrix. H0 is a unit bandwidth matrix if it is a valid bandwidth matrix and |H0| = 1.\nLet σ1(H0) ≥ · · · ≥ σd(H0) > 0 be the eigenvalues of H0.\nRemark 2. In the scalar bandwidth case, H0 = I. Remark 3. It will be useful later on that if H = h2H0 where H0 is a unit bandwidth, then for u ∈ Rd,√\nσd(H0) · h · |u| ≤ |H1/2u| ≤ √ σ1(H0) · h · |u|.\nDefinition 4 (Kernel Density Estimation). Given a kernel K and h > 0 and H0, the KDE for H := h2H0 is given by\nf̂H(x) := 1\nn · |H|−d/2 n∑ i=1 K ( H−1/2(x−Xi) ) = 1\nn · hd n∑ i=1 K\n( H0 −1/2(x−Xi)\nh\n) ."
  }, {
    "heading": "4. Uniform Convergence Bounds",
    "text": "The following is a paraphrase of Bousquet et al. (2004), which was given in Chaudhuri & Dasgupta (2010). Lemma 2. Let G be a class of functions from X to {0, 1} with VC dimension d < ∞, and F a probability distribution on X . Let E denote expectation with respect to F . Suppose n points are drawn independently at random from F; let En denote expectation with respect to this sample. Then with probability at least 1− 1/n, the following holds for all g ∈ G:\n−min{βn √ Eng, β 2 n + βn √ Eg}\n≤ Eg − Eng ≤ min{β2n + βn √ Eng, βn √ Eg},\nwhere βn ≥ √ 4(d+ 3) log 2n/n.\nChaudhuri & Dasgupta (2010) takes G to be the indicator functions over balls. Dasgupta & Kpotufe (2014) uses this to provide similar bounds for the k-NN density estimator as in this paper. Here, we extend this idea to ellipsoids by taking G = B (the indicator functions over ellipsoids), which has VC dimension (d2 + 3d)/2 as determined by Akama & Irie (2011). Lemma 3. Define ellipsoid BH0(x, r) := {x′ ∈ Rd : |H0−1/2(x − x′)| ≤ r}, and B := {BH0(x, r) : x ∈ Rd, r > 0,H0 is a unit bandwidth}. With probability at least 1 − 1/n, the following holds uniformly for every B ∈ B and γ ≥ 0:\nF(B) ≥ γ ⇒ Fn(B) ≥ γ − βn √ γ − β2n, F(B) ≤ γ ⇒ Fn(B) ≤ γ + βn √ γ + β2n,\nwhere βn = 8d √ log n/n.\nRemark 4. We could have alternatively used a fixed confidence δ so that our results would hold with probability at least 1 − δ. This would only require a modification of βn (e.g. by taking βn = 4d √ 2(log n+ log(1/δ))/n). In this paper, we have simply taken δ = 1/n."
  }, {
    "heading": "5. KDE Bound",
    "text": "Define the following which characterizes how much the density can respectively decrease and increase from x in B(x, r). Definition 5.\nǔx(r) := f(x)− inf x′∈B(x,r)\nf(x′).\nûx(r) := sup x′∈B(x,r)\nf(x′)− f(x).\nThe first are general upper and lower bounds for f̂H.\nTheorem 1. [Uniform Upper and Lower Bounds for f̂H] Let vd be the volume of the unit ball in Rd. Then the following holds uniformly in x ∈ Rd, > 0, unit bandwidths H0, and h > (log n/n)1/d with probability at least 1 − 1/n. Let H := h2H0.\nf̂H(x) > f(x)− − C √ log n\nn · hd ,\nif ∫ Rd K(u) · ǔx(h|u|/ √ σd(H0))du < , and\nf̂H(x) < f(x) + + C\n√ log n\nn · hd ,\nif ∫ Rd K(u) · ûx(h|u|/ √ σd(H0))du < , where C =\n8d √ ·vd · ||f ||∞ (∫∞ 0 k(t) · td/2dt+ 1 ) + 64d2 · k(0). Remark 5. The conditions on ǔx(h|u|/ √ σd) and ûx(h|u|/ √ σd) can be interpreted as a bound on\ntheir expectations over the probability measure K (i.e.∫ Rd K(u)du = 1). These conditions can be satisfied by taking h sufficiently small.\nRemark 6. The parameter allows us the amount of slack in the estimation errors. This is useful in a few aspects. Oftentimes, we don’t require tight bounds, especially when reasoning about low density regions thus having a large allows us to satisfy the conditions more easily. In the case that we want tight bounds, the additive error controlled by the pointwise smoothness of the density can be encoded in , so to not require global smoothness assumptions.\nRemark 7. Besides the ||f ||∞ factor, the value of C at the end of the theorem statement is a quantity which can be known without any a priori knowledge of f . We can bound ||f ||∞ in terms of known quantities given smoothness assumptions near argmaxxf(x). This is used in later results where knowing a value of C is important.\nIn order to prove Theorem 1, we first define the following two functions which serve to approximateK as a step-wise linear combination of uniform kernels. Definition 6. Let ∆ > 0.\nK∆(u) := ∞∑ j=0 (k(j∆)− k((j + 1)∆)) · 1 {|u| < j∆} ,\nK∆(u) := ∞∑ j=0 (k(j∆)− k((j + 1)∆)) · 1 {|u| < (j + 1)∆} .\nThen it is clear that the following holds for all ∆ > 0.\nK∆(u) ≤ K(u) ≤ K∆(u).\nThe next Lemma is useful in computing the expectations of functions over the kernel measure.\nLemma 4. Suppose g is an integrable function over R≥0 and let vd denote the volume of a unit ball in Rd. Then∫\nRd K(u)g(|u|)du = vd · ∫ ∞ 0 k(t) · tdg(|u|)du.\nProof of Lemma 4. Let Sd = 2πd/2/Γ(d/2) denote the surface area of the unit ball in Rd.∫\nRd K(u)g(|u|)du = Sd ∫ ∞ 0 k(t) · td−1 · g(|t|)dt\n= Sd d ∫ ∞ 0 (k(t) · g(|t|))tddt = vd ∫ ∞ 0 (k(t) · g(|t|))tddt,\nwhere the second last equality follows from integration by parts and the last follows from the fact that vd = Sd/d.\nThe following follows immediately from Lemma 4.\nCorollary 1.∫ Rd K(u)ǔx(h|u|)du = vd · ∫ ∞ 0\nk(t) · td · ǔx(ht)dt,∫ Rd K(u)ûx(h|u|)du = vd · ∫ ∞ 0 k(t) · td · ûx(ht)dt.\nProof of Theorem 1. Assume that the event that Lemma 3 holds, which occurs with probability at least 1 − 1/n. We first show the lower bound for f̂H(x). Define\nf̂∆,H(x) := 1 n · hd n∑ i=1 K∆\n( H0 −1/2(x−Xi)\nh\n) .\nIt is clear that f̂H(x) ≥ f̂∆,H(x) for all x ∈ Rd. Let us use the following shorthand ∆k,j := k(j∆). We have\nf̂∆,H(x) = 1\nhd ∞∑ j=0 (∆k,j −∆k,j+1) · Fn (BH0(x, jh∆)) .\nWe next get a handle on each Fn (BH0(x, jh∆)). We have\nF (BH0(x, jh∆)) ≥ vd · (jh∆) d · Fj ,\nwhere Fj := max{0, f(x)− ǔx(jh∆/ √ σd(H0))}. Thus, by Lemma 3, we have\nFn (BH0(x, jh∆)) ≥ vd · (jh∆)d · Fj − βn √ vd · (jh∆)d/2 · √ Fj − β2n\n≥ vd · (jh∆)d · Fj − βn √ vd · ||f ||∞ · (jh∆)d/2 − β2n.\nTherefore,\nf̂∆,h(x) ≥ vd ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d · f(x)\n− vd ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d · ǔx ( jh∆√ σd(H0) )\n− βn √ vd · ||f ||∞ hd/2 · ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d/2\n− β2n k(0)\nhd .\nWe handle each term separately. For the first term, we have\nlim ∆→0 vd ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d\n= vd ∫ ∞ 0 k(t)tddt = 1.\nwhere the last equality follows from Lemma 4. Next, we have\nlim ∆→0 vd ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d · ǔx ( jh∆√ σd(H0) )\n= vd ∫ ∞ 0 k(t) · td · ǔx(th/ √ σd(H0))dt < .\nFinally, we have\nlim ∆→0 ∞∑ j=0 (∆k,j −∆k,j+1)(j∆)d/2\n= ∫ ∞ 0 k(t) · td/2dt <∞.\nThus, taking ∆→ 0 we get f̂H(x) ≥ f(x)− − βn √ vd · ||f ||∞ hd/2 · ∫ ∞\n0\nk(t) · td/2dt\n− β2n k(0)\nhd .\nThis gives us the lower bound. Next we derive an upper bound. Let us redefine\nf̂∆,H(x) := 1 n · hd n∑ i=1 Km ( x−Xi h ) .\nIt is clear that f̂H(x) ≤ f̂∆,H(x) for all x ∈ Rd and\nf̂∆,H(x)\n= 1\nhd ∞∑ j=0 (∆k,j −∆k,j+1) · Fn (BH0(x, (j + 1)h∆)) .\nWe next get a handle on each Fn (BH0(x, jh∆)). We have\nF(BH0(x, jh∆)) ≤ vd · (jh∆)d · Fj where Fj = min{||f ||∞, f(x) + û(jh∆/ √ σd(H0))}. Thus by Lemma 3 we have\nFn (BH0(x, jh/m)) ≤ vd(jh∆)dFj + βn(jh∆)d/2 √ vd · Fj + β2n.\nUsing this, we now have\nf̂∆,H(x) ≤ vd ∞∑ j=0 (∆k,j −∆k,j+1)((j + 1) )d · f(x)\n+ vd ∞∑ j=0 (∆k,j −∆k,j+1)((j + 1)∆)d · ûx ( (j + 1)h∆√ σd(H0) )\n+ βn √ vd · ||f ||∞ hd/2 · ∞∑ j=0 (∆k,j −∆k,j+1)((j + 1)∆)d/2\n+ β2n k(0)\nhd .\nWe proceed the same way as the other direction. Thus taking ∆→ 0 we get\nf̂∆,H(x) ≤ f(x) + + βn √ vd · ||f ||∞ hd/2 · ∫ ∞\n0\nk(t) · td/2dt\n+ β2n k(0)\nhd .\nThe result follows."
  }, {
    "heading": "6. Sup-norm Bounds for KDE",
    "text": "Theorem 2. [`∞ bound for α-Hölder continuous functions] If f is Hölder-continuous (i.e. |f(x) − f(x′)| ≤ Cα|x − x|α for x, x′ ∈ Rd and 0 < α ≤ 1), then there exists positive constant C ′ ≡ C ′(C,Cα, α,K) such that the following holds with probability at least 1 − 1/n uniformly in h > (log n/n)1/d and unit bandwidths H0. Let H := h2H0.\nsup x∈Rd\n|f̂H(x)− f(x)| < C ′ ·\n( hα\nσd(H0)α/2 +\n√ log n\nn · hd\n) .\nRemark 8. Taking h = n−1/(2α+d) in the above r.h.s. optimizes the rates to n−α/(2α+d) (ignoring log factors). Remark 9. We can attain similar results (although not uniform in bandwidth) by a straightforward application of Theorem 3.1 of Sriperumbudur & Steinwart (2012) or Proposition 9 of Rinaldo & Wasserman (2010)."
  }, {
    "heading": "7. Mode Estimation Results",
    "text": "The goal of this section is to utilize the KDE to estimate the mode of a uni-modal distribution from its samples. We borrow the estimator from Abraham et al. (2004)\nx̂ := argmaxx∈X f̂H(x),\nwhere H := h2I.\nWe adopt the mode estimation framework assumptions from Dasgupta & Kpotufe (2014) which are summarized below. Definition 7. x0 is a mode of f if f(x) < f(x0) for all x ∈ B(x0, r)\\{x0} for some r > 0. Assumption 4. • f has a single mode x0.\n• f is twice differentiable in a neighborhood around x0.\n• f has a negative-definite Hessian at x0.\nThese assumptions lead to the following. Lemma 5 ((Dasgupta & Kpotufe, 2014)). Let f satisfy Assumption 4. Then there exists Ĉ, Č, r0, λ > 0 such that the following holds.\nČ · |x0 − x|2 ≤ f(x0)− f(x) ≤ Ĉ · |x0 − x|2\nfor all x ∈ Ax where A0 is a connected component of {x : f(x) ≥ λ} and A0 contains B(x0, r0).\nWe obtain the following result for the estimation error of x̂. Theorem 3. Suppose that Assumptions 1, 2, 3, 4 hold. Choose h such that (log n)2/ρ ·h→ 0 and log n/(nhd)→ 0 as n → ∞. Then, for n sufficiently large depending on d, ||f ||∞,K, Ĉ, Č, r0 the following holds with probability least 1− 1/n.\n|x̂− x0|2 ≤ max\n{ 32Ĉ\nČ (log n)4/ρ · h2, 17 · C\n√ log n\nn · hd\n} .\nRemark 10. Taking h = n−1/(4+d) optimizes the above expression so that |x̂− x0| . n−1/(4+d) (ignoring log factors) which matches the lower bound rate for mode estimation as established in Tsybakov (1990). Remark 11. This result can be extended to multi-modal distributions as done by Dasgupta & Kpotufe (2014) by using the connected components of nearest neighbor graphs at appropriate empirical density levels to isolate the modes away from each other."
  }, {
    "heading": "8. Density Level Set Estimation Results",
    "text": "In this section, we estimate the density level set Lf (λ) := {x : f(x) ≥ λ} where λ > 0 is given. We make the following standard regularity assumptions e.g. (Singh et al., 2009). To simplify the analysis, let us take H = h2I. It is clear that the results that follow can be extended to arbitrary H0. Assumption 5 (β-regularity). Let 0 < β < ∞. There exists 0 < λ0 < λ and Čβ , Ĉβ , r̄ > 0 such that the following holds for x ∈ Lf (λ0)\\Lf (λ).\nČβ · d(x, Lf (λ))β ≤ λ− f(x) ≤ Ĉβ · d(x, Lf (λ))β ,\nwhere d(x,A) := infx′∈A{|x − x′|}. and B(Lf (λ), r̄) ⊆ Lf (λ0) where B(A, r) := {x : d(x,A) ≤ r}.\nThen we consider following estimator.\nL̂f := { x ∈ X : f̂H(x) > λ− C̃ √ log n\nn · hd\n} .\nwhere C̃ is obtained by taking C and replacing the ||f ||∞ factor by 1+5 maxx∈Xn0 f̂H(x) whereXn0 is a fixed sample of size n0. Then, C̃ can be viewed as a constant w.r.t. n and can be known without any a priori knowledge of f while ensuring that C̃ ≥ max{1, 2C}.\nWe use the following Hausdorff metric. Definition 8 (Hausdorff Distance).\ndH(A,B) := max{sup x∈A d(x,B), sup x∈B d(x,A)}.\nTheorem 4. Suppose that Assumptions 1, 2, 3, 5 hold and that f is α-Hölder continuous for some 0 < α ≤ 1. Choose h such that (log n)2/ρ · h → 0 and log n/(nhd) → 0 as n → ∞. Then, for n sufficiently large depending on d,C, C̃,K, Ĉβ , Čβ , β, r̄ the following holds with probability least 1− 1/n.\ndH(L̂f , Lf (λ)) ≤ C ′′ ( (log n)2/ρ · h+ ( log n\nn · hd\n)1/(2β)) ,\nwhere C ′′ ≡ C ′′(C, C̃, Ĉβ , Čβ , C̃, β). Remark 12. Choosing h = n−β/(2β+d) gives us a densitylevel set estimation rate of O(n−1/(2β+d)). This matches the lower bound (ignoring log factors) determined by Tsybakov (1997). Remark 13. This result can be extended so that we can recover each component separately (i.e. identify which points correspond to which connected components of Lf (λ)). Similar to the mode estimation result, this can be done using nearest neighbor graphs at the appropriate level to isolate the connected components of Lf (λ) away from each other. This has been done extensively in the related area of cluster tree estimation e.g. (Chaudhuri & Dasgupta, 2010).\nRemark 14. The global α-Hölder continuous assumption is not required and is only here for simplicity. Smoothness in a neighborhood around a maximizer of f is sufficient so that for n0 large enough, C̃ ≥ 2C."
  }, {
    "heading": "9. Class Probability Estimation Results",
    "text": "We consider the setting where we have observations from compact subset X ⊂ Rd and labels y ∈ {1, ..., L}. Given a label y, an instance x ∈ Rd has density fy(x) where fy is w.r.t. the uniform measure on Rd. Instance-label pairs (X,Y ) are thus drawn according to a mixture distribution where Y is chosen from {1, ..., L} with corresponding probabilities π1, ..., πL (i.e. ∑L j=1 πj = 1) and then X is chosen according to fY .\nThen given x ∈ X , we can define the marginal distribution as follows.\ng(x) := (g1(x), ..., gL(x)),\ngy(x) := f(Y = y|X = x) = πyfy(x)∑ j πjfj(x) .\nThe goal of class probability estimation is to learn g based on samples (x1, y1), ..., (xn, yn). We define our estimator naturally as follows. Let f̂h,y be the KDE of fy w.r.t. to bandwidth matrix H = h2I.\nĝh(x) := (ĝh,1(x), ..., ĝh,L(x)),\nĝh,y(x) := π̂y f̂h,y(x)∑ j π̂j f̂h,j(x) and π̂y := 1 n n∑ j=1 1[y = yi].\nWe make the following regularity assumption on fy .\nAssumption 6. (α-Hölder densities) For each y ∈ {1, ..., L} and x ∈ Rd we have\n|fy(x)− fy(x′)| ≤ Cα|x− x′|α,\nwhere 0 < α ≤ 1.\nWe state the result below:\nTheorem 5. Suppose that Assumptions 1, 2, 3, 6 hold. Then for n sufficiently large depending on miny πy , there exists positive constants C ′′ ≡ C ′′(L,C,Cα, α,K) and C̃ ≡ C̃(miny πy, L) such that the following holds with probability at least 1−C̃/n uniformly in h > (log n/n)1/d.\nsup x∈Rd\n||ĝh(x)− g(x)||∞ ≤ C ′′ · ( hα + √ log n\nn · hd\n) .\nRemark 15. This corresponds to an optimized rate of Õ(n−α/(2α+d)). This matches the lower bounds up to log factors for misclassification as established in related\nworks e.g. Audibert et al. (2007); Chaudhuri & Dasgupta (2014). Note that misclassification rate for a hard classifier is a slightly different but very related to what is done here, which is directly estimating the marginal density."
  }, {
    "heading": "10. Extension to Manifolds",
    "text": "We make the following regularity assumptions which are standard among works in manifold learning e.g. (Baraniuk & Wakin, 2009; Genovese et al., 2012; Balakrishnan et al., 2013).\nAssumption 7. F is supported on M where:\n• M is a dM -dimensional smooth compact Riemannian manifold without boundary embedded in compact subset X ⊆ RD.\n• The volume of M is bounded above by a constant.\n• M has condition number 1/τ , which controls the curvature and prevents self-intersection.\nLet f be the density of F with respect to the uniform measure on M .\nIn this section, we assume that our density estimator is w.r.t. to dM instead of the ambient dimension d.\nf̂H(x) := 1\nn · hdM n∑ i=1 K\n( H0 −1/2(x−Xi)\nh\n) .\nRemark 16. It is then the case that we must know the intrinsic dimension dM . There are numerous known techniques for doing so e.g. (Kegl, 2002; Levina & Bickel, 2004; Hein & Audibert, 2005; Farahmand et al., 2007).\nNext, we need the following guarantee on the volume of the intersection of a Euclidean ball and M ; this is required to get a handle on the true mass of the ball under F in later arguments. The upper and lower bounds follow from Chazal (2013) and Lemma 5.3 of Niyogi et al. (2008). The proof can be found in (Jiang, 2017).\nLemma 6 (Ball Volume). If 0 < r < min{τ/4dM , 1/τ}, and x ∈M then\n1− τ2r2 ≤ voldM (B(x, r) ∩M) vdM r dM ≤ 1 + 4dMr/τ,\nwhere voldM is the volume w.r.t. the uniform measure on M .\nWe then give analogues to Theorem 1 and Theorem 2.\nTheorem 6. [Manifold Case Uniform Upper and Lower Bounds for f̂H] There exists CM ≡ CM (dM , d,K, ||f ||∞, τ) such that the following holds\nuniformly in x ∈ M , > 0, unit bandwidths H0, and h > (log n/n)1/dM with probability at least 1 − 1/n. Let H := h2H0.\nf̂H(x) > f(x)− − CM ( h2 + √ log n\nn · hdM\n) ,\nif ∫ Rd K(u) · ǔx(h|u|/ √ σd(H0))du < , and\nf̂H(x) < f(x) + + CM\n( h+ √ log n\nn · hdM\n) ,\nif ∫ Rd K(u) · ûx(h|u|/ √ σd(H0))du < . Remark 17. The extra h2 and h term in the lower and upper bounds respectively come from the approximation of the volume of the full-dimensional balls w.r.t. the uniform measure on M in Lemma 6.\nProof Sketch of Theorem 6. The proof mirrors that of the full dimensional case so we only highlight the differences. For the lower bound, instead of\nF (BH0(x, jhδ)) ≥ vd · (jhδ) d · Fj ,\nwe have\nF (BH0(x, jhδ)) ≥ vdM (jhδ) dM Fj(1− τ2(jhδ)2)\n= vdM (jhδ) dM Fj − hdM+2vdM τ2||f ||∞ (jδ) dM+2 .\nThe first term can be treated in the same way as before, while the second term contributes in an extra term with an h2 factor after taking the total summation.\nFor the upper bound, instead of\nF(BH0(x, jhδ)) ≤ vd · (jhδ)d · Fj ,\nwe have\nF(BH0(x, jhδ)) ≤ vdM · (jhδ)dM · Fj(1 + 4dM (jhδ)/τ).\nSimilary, this contributes an extra term with an h factor after taking the total summation.\nTheorem 7. [Manifold Case `∞ bound for α-Hölder continuous functions] If f is Hölder-continuous (i.e. |f(x) − f(x′)| ≤ Cα|x − x|α for x, x′ ∈ Rd with 0 < α ≤ 1), then there exists positive constant C ′M ≡ C ′M (||f ||∞, Cα, α,K, τ, dM , d, σd(H0)) such that the following holds with probability at least 1− 1/n uniformly in h satisfying (log n/n)1/dM < h < 1.\nsup x∈M |f̂H(x)− f(x)| < C ′M ·\n( hα + √ log n\nn · hdM\n) ."
  }, {
    "heading": "11. Local Intrinsic Dimension Estimation",
    "text": "In this section, we only assume a distribution F on Rd whose support is defined asX := {x ∈ Rd : F(B(x, h)) > 0 ∀h > 0} and X is assumed to be compact. We use the following notion of intrinsic dimension, which is based on the doubling dimension and adapted from previous works such as Houle (2013).\nDefinition 9. For x ∈ X , define the following local intrinsic dimension wherever the quantity exists\nID(x) := lim h→0 log2 ( F(B(x, 2h)) F(B(x, h)) ) .\nWe can then define our estimator of local intrinsic dimension at x ∈ X as follows:\nÎDn,h(x) := log2 ( Fn(B(x, 2h)) Fn(B(x, h)) ) .\nThe following is a uniform convergence result for ÎDn,h(x).\nTheorem 8. Define the following\nIDh(x) := log2 ( F(B(x, 2h)) F(B(x, h)) ) .\nSuppose that h > 0 and n satisfy βn < 1 10 infx′∈X √ F(B(x′, h)). Then the following holds with probability at least 1− 1/n uniformly in x ∈ X .\n|ÎDn,h(x)− IDh(x)| ≤ 6βn infx′∈X √ F(x′, 2h) .\nRemark 18. The r.h.s. goes to 0 as n → ∞. Moreover, if IDh(x) converges to ID(x) uniformly in x ∈ X , then simultaneously taking h → 0 and n → ∞ such that βn· ( infx′∈X √ F(x′, 2h) )−1 → 0 gives us a finite-sample\nuniform convergence rate for local intrinsic dimension estimation.\nRemark 19. If we assume a global intrinsic dimension d0 and a density, the condition βn < 1 10 infx′∈X √ F(B(x′, h)) can be interpreted as logn nhd0 → 0\nand the r.h.s. of the bound is on the order of √\nlogn nhd0 .\nIn fact, this result is similar to the uniform convergence results for the KDE for estimating the smoothed density. e.g. |f̂h − fh|∞ = O (√ logn nhd ) when (ignoring some log\nfactors) nhd → ∞ where fh is the density convolved with the uniform kernel with bandwidth h. It is interesting that an analogous result comes up when estimating the intrinsic dimension with our notion of smoothed ID."
  }],
  "year": 2017,
  "references": [{
    "title": "On the asymptotic properties of a simple estimate of the mode",
    "authors": ["C. Abraham", "G. Biau", "B. Cadre"],
    "venue": "ESAIM: Probability and Statistics,",
    "year": 2004
  }, {
    "title": "On the estimation of the gradient lines of a density and the consistency of the mean-shift algorithm",
    "authors": ["Arias-Castro", "Ery", "Mason", "David", "Pelletier", "Bruno"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Fast learning rates for plug-in classifiers",
    "authors": ["Audibert", "Jean-Yves", "Tsybakov", "Alexandre B"],
    "venue": "The Annals of statistics,",
    "year": 2007
  }, {
    "title": "Cluster trees on manifolds",
    "authors": ["S. Balakrishnan", "S. Narayanan", "A. Rinaldo", "A. Singh", "L. Wasserman"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Random projections of smooth manifolds",
    "authors": ["Baraniuk", "Richard G", "Wakin", "Michael B"],
    "venue": "Foundations of computational mathematics,",
    "year": 2009
  }, {
    "title": "Kernel density estimation via diffusion",
    "authors": ["Botev", "Zdravko I", "Grotowski", "Joseph F", "Kroese", "Dirk P"],
    "venue": "The Annals of Statistics,",
    "year": 2010
  }, {
    "title": "Introduction to statistical learning theory",
    "authors": ["O. Bousquet", "S. Boucheron", "G. Lugosi"],
    "venue": "Lecture Notes in Artificial Intelligence,",
    "year": 2004
  }, {
    "title": "Kernel estimation of density level sets",
    "authors": ["Cadre", "Benoit"],
    "venue": "Journal of Multivariate Analysis,",
    "year": 2006
  }, {
    "title": "Rates for convergence for the cluster tree",
    "authors": ["K. Chaudhuri", "S. Dasgupta"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Rates of convergence for nearest neighbor classification",
    "authors": ["Chaudhuri", "Kamalika", "Dasgupta", "Sanjoy"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Classification based on hybridization of parametric and nonparametric classifiers",
    "authors": ["Chaudhuri", "Probal", "Ghosh", "Anil K", "Oja", "Hannu"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2009
  }, {
    "title": "An upper bound for the volume of geodesic balls in submanifolds of euclidean spaces",
    "authors": ["F. Chazal"],
    "year": 2013
  }, {
    "title": "Mean shift, mode seeking, and clustering",
    "authors": ["Y. Cheng"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 1995
  }, {
    "title": "Estimation of the mode",
    "authors": ["Chernoff", "Herman"],
    "venue": "Annals of the Institute of Statistical Mathematics,",
    "year": 1964
  }, {
    "title": "Mean shift: A robust approach toward feature space analysis",
    "authors": ["D Comaniciu", "P. Meer"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2002
  }, {
    "title": "Estimating local intrinsic dimension with k-nearest neighbor graphs",
    "authors": ["Costa", "Jose A", "Girotra", "Abhishek", "Hero", "AO"],
    "venue": "In Statistical Signal Processing,",
    "year": 2005
  }, {
    "title": "A plug-in approach to support estimation",
    "authors": ["A. Cuevas", "R. Fraiman"],
    "venue": "Annals of Statistics,",
    "year": 1997
  }, {
    "title": "Optimal rates for k-nn density and mode estimation",
    "authors": ["S. Dasgupta", "S. Kpotufe"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Optimum kernel estimators of the mode",
    "authors": ["Eddy", "William F"],
    "venue": "Annals of Statistics,",
    "year": 1980
  }, {
    "title": "Uniform in bandwidth consistency of kernel-type function estimators",
    "authors": ["Einmahl", "Uwe", "Mason", "David M"],
    "venue": "Annals of Statistics,",
    "year": 2005
  }, {
    "title": "The estimation of the gradient of a density function, with applications in pattern recognition",
    "authors": ["Fukunaga", "Hostetler"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1975
  }, {
    "title": "Minimax manifold estimation",
    "authors": ["Genovese", "Christopher", "Perone-Pacifico", "Marco", "Verdinelli", "Isabella", "Wasserman", "Larry"],
    "venue": "Journal of machine learning research,",
    "year": 2012
  }, {
    "title": "Nonparametric inference for density modes",
    "authors": ["Genovese", "Christopher R", "Verdinelli", "Marco PeronePacificoand Isabella", "Wasserman", "Larry"],
    "venue": "Series B Statistical Methodology,",
    "year": 2015
  }, {
    "title": "Rates of strong uniform consistency for multivariate kernel density estimators",
    "authors": ["Gine", "Guillon"],
    "venue": "Ann. Inst. H. Poincare Probab. Statist.,",
    "year": 2002
  }, {
    "title": "Intrinsic dimensionality estimation of submanifolds in rd",
    "authors": ["Hein", "Matthias", "Audibert", "Jean-Yves"],
    "year": 2005
  }, {
    "title": "Dimensionality, discriminability, density and distance distributions",
    "authors": ["Houle", "Michael E"],
    "venue": "In Data Mining Workshops (ICDMW),",
    "year": 2013
  }, {
    "title": "Density level set estimation on manifolds with dbscan",
    "authors": ["Jiang", "Heinrich"],
    "venue": "International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Modal-set estimation with an application to clustering",
    "authors": ["Jiang", "Heinrich", "Kpotufe", "Samory"],
    "year": 2017
  }, {
    "title": "Intrinsic dimension estimation using packing numbers",
    "authors": ["Kegl", "Balazs"],
    "year": 2002
  }, {
    "title": "Intrinsic dimension estimation using packing numbers",
    "authors": ["Kégl", "Balázs"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2003
  }, {
    "title": "Maximum likelihood estimation of intrinsic dimension",
    "authors": ["Levina", "Elizaveta", "Bickel", "Peter J"],
    "year": 2004
  }, {
    "title": "A nonparametric statistical approach to clustering via mode identification",
    "authors": ["J. Li", "S. Ray", "B. Lindsay"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2007
  }, {
    "title": "Finding the homology of submanifolds with high confidence from random samples",
    "authors": ["P. Niyogi", "S. Smale", "S. Weinberger"],
    "venue": "Discrete and Computational Geometry,",
    "year": 2008
  }, {
    "title": "On estimation of a probability density function and mode",
    "authors": ["Parzen", "Emanual"],
    "venue": "Annals of mathematical statistics,",
    "year": 1962
  }, {
    "title": "Fast rates for plug-in estimators of density level",
    "authors": ["P. Rigollet", "R. Vert"],
    "venue": "sets. Bernoulli,",
    "year": 2009
  }, {
    "title": "Generalization error bounds in semisupervised classification under the cluster assumption",
    "authors": ["Rigollet", "Philippe"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2007
  }, {
    "title": "Generalized density clustering",
    "authors": ["A. Rinaldo", "L. Wasserman"],
    "venue": "Annals of Statistics,",
    "year": 2010
  }, {
    "title": "Remarks on some nonparametric estimates of a density function",
    "authors": ["M. Rosenblatt"],
    "venue": "Ann. Math. Statist.,",
    "year": 1956
  }, {
    "title": "Density Estimation for Statistics and Data Analysis",
    "authors": ["B. Silverman"],
    "venue": "CRC Press,",
    "year": 1986
  }, {
    "title": "Using kernel density estimates to investigate multimodality",
    "authors": ["B.W. Silverman"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
    "year": 1981
  }, {
    "title": "Adaptive hausdorff estimation of density level sets",
    "authors": ["Singh", "Aarti", "Scott", "Clayton", "Nowak", "Robert"],
    "venue": "Annals of Statistics,",
    "year": 2009
  }, {
    "title": "Consistency and rates for clustering with dbscan",
    "authors": ["Sriperumbudur", "Bharath K", "Steinwart", "Ingo"],
    "venue": "AISTATS,",
    "year": 2012
  }, {
    "title": "Adaptive density level set clustering",
    "authors": ["I. Steinwart"],
    "venue": "24th Annual Conference on Learning Theory,",
    "year": 2011
  }, {
    "title": "Variable kernel density estimation",
    "authors": ["Terrell", "George R", "Scott", "David W"],
    "venue": "The Annals of Statistics,",
    "year": 1992
  }, {
    "title": "Recursive estimation of the mode of a multivariate distribution",
    "authors": ["A. Tsybakov"],
    "venue": "Problemy Peredachi Informatsii,",
    "year": 1990
  }, {
    "title": "Introduction to nonparametric estimation",
    "authors": ["A. Tsybakov"],
    "year": 2008
  }, {
    "title": "On non-parametric estimation of density level sets",
    "authors": ["A.B. Tsybakov"],
    "venue": "Annals of Statistics,",
    "year": 1997
  }],
  "id": "SP:2368882c556787c0d3feda6acea2d2953b5c52c7",
  "authors": [{
    "name": "Heinrich Jiang",
    "affiliations": []
  }],
  "abstractText": "Kernel density estimation (KDE) is a popular nonparametric density estimation method. We (1) derive finite-sample high-probability density estimation bounds for multivariate KDE under mild density assumptions which hold uniformly in x ∈ R and bandwidth matrices. We apply these results to (2) mode, (3) density level set, and (4) class probability estimation and attain optimal rates up to logarithmic factors. We then (5) provide an extension of our results under the manifold hypothesis. Finally, we (6) give uniform convergence results for local intrinsic dimension estimation.",
  "title": "Uniform Convergence Rates for Kernel Density Estimation"
}