{
  "sections": [{
    "heading": "1. Introduction",
    "text": "One of the first appearances of SRP was in graph theory (Goemans & Williamson, 1995) where the authors looked at the probability of a random hyperplane separating two vectors. This probability was used to come up with an improved algorithm to approximate the maximum cut in a graph. Today, this probability is also used for distance based learning algorithms given high dimensional data.\nConsider a data matrix Xn×p with n observations, and p features. Computing pairwise distances between each observation takes at least O(n2p) of time which is computationally costly when n, p are large.\nIt was a few years later when the fact that the probability of the separation was proportional to the angle θ behind the vectors was used in a locality sensitive hashing (LSH) scheme (Charikar, 2002). High dimensional data in Rp would be mapped to lower dimensional binary vectors in Rk under SRP, and the computed Hamming distance between the binary pairs would give an estimate of their an-\n*Equal contribution 1Singapore University Of Technology And Design. Correspondence to: Keegan Kang <keegan kang@sutd.edu.sg>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ngular similarity with high probability. The time taken under this technique would be reduced to O(npk + n2k).\nBy going one step further and storing the norms of vectors (Li et al., 2006), SRP could also be used to recover the inner products between these vectors. This proved to be competitive with conventional random projections under certain conditions. Binary codes require 1 bit of storage per entry under SRP, but storing real numbers as doubles could take 64 bits per entry under conventional random projections. If the variance of the inner product estimates using SRP is lower than the variance of the inner product estimates using conventional random projections for the same amount of bits, then SRP would be highly preferred.\nThese results gave rise to an efficient Approximate Nearest Neighbor (ANN) algorithm as computing Hamming distances in the lower dimensional space would be faster than computing angles in the higher dimensional one. An example of this could be image classification, where a 1024 × 1024 pixel image would mean working with vectors in Rp, p = 1048576. Transforming these vectors to binary codes of length k, k p would drastically reduce the time taken.\nMoreover, these estimates can be used in conjunction with the “kernel trick”, where (estimates of) inner products are used within algorithms such as support vector machines (SVM) to further reduce the computational cost."
  }, {
    "heading": "1.1. Sign Random Projections",
    "text": "Given a data matrix Xn×p, consider a random matrix Rp×k with entries i.i.d from N(0, 1). We compute V = sgn(XR), where we define\nsgn(x) = { 1 x ≥ 0 0 x < 0\n(1)\nand apply sgn to every element in XR. We set sgn(x) = 0 when x < 0 for convenience of notation in Figures 2 and 8.\nTheorem 1.1. The estimate for the angle θ between vectors xi,xj is given by the Hamming distance between vi,vj multiplied by π/k.\nProof. This proof is given in (Goemans & Williamson, 1995).\nThis estimate of θ is the sum of k Bernoulli observations, which has variance Var[θ̂] = θ(π−θ)k . Figure 1 shows how the theoretical variance of SRP varies with θ."
  }, {
    "heading": "1.2. Super-Bit LSH",
    "text": "Super-Bit LSH (SBLSH) (Ji et al., 2012) was proposed to reduce the variance of the estimates of angular similarity. SBLSH was proven to have lower variance than SRP, when the angle to estimate is within the interval [0, π2 ]. This algorithm only takes place in the pre-processing stage to get a different random matrix R̃. While SRP only involved generating a random matrix R with k columns, SBLSH divided the k columns into L groups of N columns, and orthogonalized the N vectors within each group. These new vectors become the columns of the new matrix R̃. The subsequent process is then the same as ordinary SRP.\nEmpirical results with SBLSH showed that given some K = k, the best division to ensure a variance reduction is to have L = 1 group, with N = k orthogonal vectors.\nWhile SBLSH achieves variance reduction, there are areas where this algorithm could do better. For example, orthogonalizing batches of N vectors can be costly when the dimension of the data p is large, and therefore a longer preprocessing period is needed. Furthermore, geometry theory (Ball, 1997) states that any two normalized vectors sampled in extremely high dimensions would be orthogonal (with high probability) to each other, so SRP would achieve roughly the same performance as SBLSH with large p."
  }, {
    "heading": "2. Our Contributions",
    "text": "We propose an estimator for SRP by using additional information. Our estimator keeps to the same order of time as SRP, and has a less costly pre-processing period than SBLSH. We demonstrate that our estimator can be used in conjunction with any modifications of SRP such as SBLSH to get a more accurate estimate. We prove that the variance\nof our estimator is lower than the ordinary SRP estimate. We explain how we can build better and more accurate estimators from our work. Lastly, we demonstrate our results on the MNIST test dataset and the Gisette dataset.\nOur work was inspired by Deming (Deming & Stephan, 1940) and Li (Li & Church, 2007). Deming examined the computation of maximum likelihood estimates for a multinomial distribution when the marginal probabilities are known. His result was used by Li to estimate two way associations in word data by making use of information known at the margins. We go one step further by adding information to the dataset to construct our own margins."
  }, {
    "heading": "2.1. Our Estimator",
    "text": "We denote θxi,xj to be the angle between the vectors xi,xj . Given Xn×p, we construct a vector e ∈ Rp and compute and store the angles θx1,e, . . . , θxn,e. We now proceed the same way as SRP and generate the random matrix Rp×k. We store V = sgn(XR) as well as ve = sgn(eTR).\nWithout loss of generality, suppose we want to compute θx1,x2 . Consider the vectors v1 = (v11, . . . , v1k), v2 = (v21, . . . , v2k), ve = (ve1, . . . , vek) and the k 3-tuples {(v1s, v2s, ves)}ks=1. Each 3-tuple is a binary string of length 3. Suppose we categorize these tuples in a 2 × 2 contingency table, with the following four sets\nA = {s | sgn(v2s) = sgn(ves)} (2) B = {s | sgn(v2s) 6= sgn(ves)} (3) C = {s | sgn(v1s) 6= sgn(ves)} (4) D = {s | sgn(v1s) = sgn(ves)} (5)\nIn statistics, given a contingency table with samples drawn from a large population, the goal is to find the probability of each cell occurring in the population. The probability of exactly observing n1, n2, n3, n4 counts with n = n1 + n2 + n3 + n4 is given by\np = P [X1 = n1, X2 = n2, X3 = n3, X4 = n4] (6)\n=\n( n\nn1 n2 n3 n4\n) pn11 p n2 2 p n3 3 p n4 4 (7)\nwhere each Xi denotes the respective cell, and pi the probability of seeing an observation from that cell. The log like-\nlihood function is given by\nl(p1, p2, p3, p4) = C + 4∑ j=1 nj log(pj) (8)\nwhere C is some constant. The maximum likelihood estimates are given by p̂i = nin . In SRP and SBLSH, we compute n2+n3n to estimate θxi,xj .\nHowever, we already know the probability p1 + p3 of observing n1 + n3 and the probability p3 + p4 of observing n3 + n4. These probabilities are given by 1 − θx2e π and\n1− θx1eπ respectively. We also know that ∑ i pi = 1.\nThe maximum likelihood estimate (8) can be rewritten in terms of p3\nl(p3) = n1 log ( 1− θx2e\nπ − p3\n) + n4 log ( 1− θx1e\nπ − p3 ) + n3 log(p3) + n2 log ( p3 − 1 +\nθx1e + θx2e π\n) (9)\nwith the following first and second derivatives given by\ndl\ndp3 =\nn1π\nθx2e + π(p3 − 1) +\nn4π\nθx1e + π(p3 − 1)\n+ n2π θx1e + θx2e + π(p3 − 1) + n3 p3\n(10)\nd2l\ndp23 = − n1π\n2\n(θx2e + π(p3 − 1))2 − n4π\n2\n(θx1e + π(p3 − 1))2\n− n2π 2\n(θx1e + θx2e + π(p3 − 1)) 2 − n3 p23\n(11)\nTo maximize l(p3) in (9), we need p̂3 which makes the first derivative zero. The second derivative is always negative which makes our value of p3 a maximum. We mention that the denominators of the fractions in dldp3 are only equal to zero when pi = 0, for i = 1, 2, 3, 4. By using the relationships p1+p3 = 1− θx2e π , p3+p4 = 1− θx1e π , p1+p4 = θ π\nand ∑ i pi = 1, we can express pi’s in terms of θx1e, θx2e and θ:\np1 = θ+θx1e−θx2e 2π p2 = θx1e+θx2e−θ 2π p3 = 2π−θx1e−θx2e−θ 2π p4 = θ+θx2e−θx1e 2π\n(12)\nHence, the denominators of the fractions in dldp3 are equal zero when θ = −θx1e+ θx2e, θ = θx1e+ θx2e, θ+ θx1e+ θx2e = 2π or θ = θx1e − θx2e. These are the cases when all three vectors x1,x2, e lie on the same 2D-plane.\nNewton Raphson can be used to find p̂3 with the starting value n3n , which has quadratic convergence. We should cross multiply the terms in the first derivative and use Newton Raphson on the numerator for numerical stability.\nOnce p̂3 is found, we compute p̂2 by p̂3 − 1 + θx1e+θx2e\nπ , and estimate θx1x2 by θ̂ = π(1 − p̂2 − p̂3). Algorithm 1 describes our algorithm for our estimator.\nAlgorithm 1 Algorithm For Our Estimator Pre-processing stage Initialize e ; Initialize R Compute V = sgn(XR) ; Compute ve = sgn(eTR) for each xi ∈ X do\nCompute and store θxie end Actual stage for each vi,vj ∈ V do Count n1, n2, n3, n4 as in Figure 2 Find p̂3 which maximizes l(p3) in Equation 9 Compute p̂2 = p̂3 − 1 + θx1e+θx2e π\nSet θ̂xixj = π(1− p̂2 − p̂3) end"
  }, {
    "heading": "2.2. Theoretical Variance Of Our Estimator",
    "text": "Theorem 2.1. The variance of our maximum likelihood estimate θ̂ is given by\nVar[θ̂] = 4π2 k (\n1 p1 + 1p2 + 1 p3 + 1p4 ) (13) Proof. Our estimate θ̂ is given by\nθ̂ = π(1− p̂2 − p̂3) = −2πp̂3 + 2π − (θx1e + θx2e) (14)\nand its variance given by Var[θ̂] = 4π2Var[p̂3] where p̂3 is our maximum likelihood estimate. Thus we need to find Var[p̂3]. It can be shown (Shao, 2003) that p̂3 converges in distribution to a normal random variable N(p3, 1I(p3) ). I(p3) is the expected Fisher information of p3. We have that\nI(p3) = E [ − d 2l\ndp23 ] = E[n1]π2\n(θx2e + π(p3 − 1))2 +\nE[n4]π2\n(θx1e + π(p3 − 1))2\n+ E[n2]π2\n(θx1e + θx2e + π(p3 − 1)) 2 + E[n3] p23 (15)\n= kp1π\n2\n(θx2e + π(p3 − 1))2 +\nkp4π 2\n(θx1e + π(p3 − 1))2\n+ kp2π\n2\n(θx1e + θx2e + π(p3 − 1)) 2 + kp3 p23\n(16)\n= k\np1 +\nk p2 + k p3 + k p4 (17)\n= k\n( 1\np1 +\n1\np2 +\n1\np3 +\n1\np4\n) (18)\nand hence\nVar[p̂3] = 1 k (\n1 p1 + 1 p2 + 1 p3 + 1 p4 ) (19) We note that Var[p̂3] tends to 0 when any pi tends to 0.\nThis maximum likelihood estimate can be biased with small k, but the bias vanishes and the estimator enjoys good performance when k is large. This is not surprising since maximum likelihood estimators are consistent and asymptotically efficient estimators (Shao, 2003).\nTheorem 2.2. The variance of our maximum likelihood estimate Var[θ̂] is lower than the original variance Var[θ̂orig], with equality iff i) p1 = p4 and p2 = p3, i.e. θx1e = θx2e = π 2 , ii) p1 = p4 = 0, i.e. θ = 0 or iii) p2 = p3 = 0, i.e. θ = π.\nProof. We have that the variance of θ̂ under SRP is given by θ(π−θ)k , which can be rewritten as π2(p1+p4)(p2+p3) k . It suffices to show that V := Var[θ̂orig]−Var[θ̂] ≥ 0 under the constraints that ∑ i pi = 1. We can show that\nV = π2 [ (p1 + p4)(p2 + p3)( 1 p1 + 1 p2 + 1 p3 + 1 p4 )− 4 ] k( 1\np1 + 1 p2 + 1 p3 + 1 p4 )\n(20) = π2 [ (p1−p4)2 p1p4 (p2 + p3) + (p2−p3)2 p2p3 (p1 + p4) ]\nk( 1 p1 + 1 p2 + 1 p3 + 1 p4 )\n, (21)\nwhich clearly is always positive, and equal to zero when p1 = p4 and p2 = p3, or p1 = p4 = 0 or p2 = p3 = 0.\nWhile Theorem 2.2 guarantees we always achieve a lower variance or do no worse than the original variance, we should also account for the storage of θxi,e, 1 ≤ i ≤ n. If we store these values in 64 bits, then for a fixed k, we should consider Var[θ̂] with k columns versus Var[θ̂orig] with k + 64 columns, as storing 64 extra bits is equivalent to running and storing the results of SRP 64 more times.\nWe notice that\nVar[θ̂orig]k+64 − Var[θ̂]k (22)\n= π2(p1 + p4)(p2 + p3) k + 64 − 4π 2 k (\n1 p1 + 1 p2 + 1 p3 + 1 p4 ) (23) = π2 [ k(p1 + p4)(p2 + p3)( 1 p1 + 1 p2 + 1 p3 + 1 p4 )− 4(k + 64) ] k(k + 64)( 1\np1 + 1 p2 + 1 p3 + 1 p4 )\n(24)\n= kπ2\n[ (p1−p4)2\np1p4 (p2 + p3) + (p2−p3)2 p2p3 (p1 + p4)− 256k ]\nk(k + 64)( 1 p1 + 1 p2 + 1 p3 + 1 p4 )\n,\n(25)\nwhich is only positive for certain configurations of p1, p2, p3, p4, which depend on (θ, θx1e, θx2e). However, assuming that (θ, θx1e, θx2e) is uniformly distributed in S := the set of all valid triplets (θ, θx1e, θx2e), the proportion of points (θ, θx1e, θx2e) with a variance reduction\nPk := volume({(θ, θx1e, θx2e) ∈ S | Var[θ̂orig]k+64 − Var[θ̂]k ≥ 0})\nvolume(S) (26)\nrapidly increases to 0.95 at about k = 1500 as seen in Figure 3. However, the distribution of our angles in practical applications are not uniform, as data must have some structure, hence we can have a better proportion of “good points” with smaller k, as we show in our experiments."
  }, {
    "heading": "2.3. The choice of the extra vector e",
    "text": "While Theorem 2.2 gives us an inequality guaranteeing a variance reduction or at least being no worse off, we also consider how (θ, θx1e, θx2e) affect our variance reduction.\nConsider the 3D set of axes with x ∈ [0, π], y ∈ [0, π], z ∈ [0, π]. We let x denote values of θxi,e, y denote values of θxj ,e, and z denote values of θ. Our goal is to visualize the 3D region that consists of valid values of (θ, θx1e, θx2e).\nBy visualizing any arbitrary three vectors on the unit sphere S2, we can have any two angles θi, θj between these three vectors vary between (0, π), but the third angle θk necessarily needs to fulfill θi + θj + θk ≤ 2π and θi + θj ≥ θk.\nHence the 3D region of feasible θ, θxi,e, and θxj ,e is the intersection of the following regions given by {0 ≤ x + y + z ≤ 2π}, {0 ≤ z ≤ x + y}, {0 ≤ y ≤ x + z}, and {0 ≤ x ≤ y + z}, which is the tetrahedron with (0, 0, 0), (π, π, 0), (π, 0, π) and (0, π, π) as its corners. The four faces of the tetrahedron correspond to the cases when Var[θ̂] = 0 and the center ( π 2 , π 2 , π 2 ) corresponds to Var[θ̂]max = Var[θ̂orig]max = π 2\n4k .\nMoreover, the regions which correspond to negligible or no reduction in variance, i.e. Var[θ̂orig]−Var[θ̂] < occur near the three lines {x + y = π, z = π}, {x = y, z = 0} and\n{x = y = π2 , 0 ≤ z ≤ π}. The first two lines {x + y = π, z = π}, {x = y, z = 0} are the cases where the original variance Var[θ̂orig] is zero, thus no variance reductions can occur. The third line {x = y = π2 , 0 ≤ z ≤ π} contains the cases where the additional vector e doesn’t provide us additional information as it’s orthogonal to x1 and x2.\nFigure 4 shows some plots of Var[θ̂orig] − Var[θ̂] over the valid region of (θx1e, θx2e) for some fixed θ’s. We can see that the variance reduction always occurs at the boundary and no significant variance reductions near the center.\nThe choice of e has to be fixed for our dataset X . Estimating all pairwise similarities with a fixed extra vector e implies that some pairs xi,xj will enjoy good values of θxi,e, θxj ,e to get substantial variance reduction if the point (θ, θxi,e, θxj ,e) stays away from the three lines. However, bad pairs xi′ ,xj′ would not get substantial variance reduction if (θ, θxi′,e , θxj′,e) is near the three lines.\nAs a heuristic, we first normalize the vectors xi, then we set e to be the first singular vector of X to maximize the proportion of good pairs. The intuition behind this is that among all the unit vectors v, v = v1 := first singular vector of X is a vector that will maximize\n‖Xv‖2 = ∑ i (xi · v)2 = ∑ i cos2 θxi,v. (27)\nIntuitively, this means v1 will make a small angle or an angle near π with the majority of the xi’s, which implies that the majority of (θ, θxi,v1 , θxj ,v1) will stay far away from the low variance reduction region {θxi,v1 = θxi,v2 = π 2 , 0 ≤ θ ≤ π}.\nHowever, it may be the case that the best choice of e depends on the data and the type of analysis to be done."
  }, {
    "heading": "2.4. Analysis Of Our Estimator",
    "text": "We look at the computational cost of our estimator in the pre-processing period and the evaluation period.\nThe computational cost could vary widely based on generating the extra vector e. If we want an exact value of the first singular vector, this could be computationally expensive at O(min(np2, n2p)). However, as Theorem 2.2 guarantees that any e would do no worse, we could get an estimation of the first singular vector by probabilistic algorithms (Halko et al., 2011), which can take O(n2s), where s < n is a parameter chosen based on the data.\nFinding all angles between e and xi, 1 ≤ i ≤ n takes O(np) time, which is the same as the cost of normalizing or scaling data. Storing these angles takes O(n) space, which is an extra 64 bits per observation.\nTo estimate the angle between each vector pair, our estimator takes O(k) time which is the same order of time as SRP and SBLSH. Computing the values ni take O(k) time, and using Newton Raphson costs O(1).\nHence, the overall computational complexity of our algorithm isO(n2s+npk+n2k) which isO(min{n2k, n2s}).\nOur estimator only requires the probability of the random hyperplane separating the two vectors xi,xj . Any derivatives of SRP which have a different probability can still utilize this estimator."
  }, {
    "heading": "3. Our Experiments",
    "text": "We run our algorithm on the MNIST test dataset (Lecun et al., 1998) and Gisette dataset (Guyon et al., 2005; Lichman, 2013). The MNIST test dataset has n = 10, 000 observations, and p = 784 parameters. The Gisette dataset has n = 13, 500 observations, and p = 5, 000 parameters. For both datasets, we normalize the vectors to unit length.\nWe demonstrate our estimator on both SRP and SBLSH with the super bit depth being the number of columns of the random projection matrix k to ensure the best accuracy. We set the vector e to be the first singular vector of our datasets for reproducibility.\nWe run our experiments for the number of columns k (equivalently number of bits) ranging from {64, 128, . . . , 3008} of our random matrix over 100 simulations for both datasets. The motivation for the multiple of 64 stems from the fact that we are keeping an extra 64 bits of information per vector for our estimator by storing the extra angles. This can be thought of as computing and storing an additional 64 more bits with SRP, SBLSH without the use of our estimator. This means that our plots for our estimator are “shifted” to the right by 64 units to account for extra storage. Nevertheless, our estimator still performs well even accounting for the extra bits which we will show in our experiments.\nWe denote SRP and SBLSH to be our baselines, and SRP-est, SBLSH-est to be SRP implemented with our estimator, and SBLSH implemented with our estimator respectively.\nOur goal is to show that while our estimator depends on one fixed e and varying θxi,e, θxj ,e for all vector pairs, we can still get competitive variance reduction over all vector pairs in estimating angular similarity.\nLooking at all pairwise estimates also makes a fairer test, as we do not want to only look at “good pairs of vectors” where θs are within our region and far away from the boundary. Hence we compute the average RMSE of all pairwise estimates of the inner product. This ensures that all good and bad pairs of vectors are accounted for and that there is no cherry picking at all.\nWe compute the average RMSE of 49,995,000 pairwise angular similarity estimates for the MNIST test dataset, as well as the average RMSE of 91,118,250 pairwise angular similarity estimates for the Gisette dataset. We display them in Figure 5 and Figure 6 respectively. The left side of these plots show our RMSE averaged over 100 simulations, while the right side of these plots show the standard deviations of these RMSE.\nIn general, we see that SRP-est and SBLSH have lower and similar average RMSE when computing all pairwise\nangular similarities compared with SRP. In this case, SBLSH may potentially outperform SRP-est with lower average RMSE. However, as our estimator can be used with any derivation of SRP, we have SBLSH-est giving an even lower RMSE.\nWhen we look at the deviations of the average RMSE of all pairwise estimates, we see that SRP-est, SBLSH-est tend to have lower standard deviations compared to SRP, SBLSH.\nTo give some intuition to magnitude of the right and left graphs of Figure 5 and Figure 6, we show the exact values of the RMSE estimates and 3 standard deviations at k = 1024 in Figure 7.\nOverall, we see that as k is sufficiently large (k ≈ 256), using our estimator in conjunction with SRP or SBLSH can result in a lower RMSE (and lower deviations). Moreover, if our observations are extremely high dimensional, our\nestimator may be preferred to SBLSH due to lower computational complexity in the pre-processing stage, since SRP-est and SBLSH-est have about the same performance.\nOur experiments also do verify the theory that when k is large, our maximum likelihood estimates are unbiased and outperforms SRP and SBLSH."
  }, {
    "heading": "4. Building Better Estimators When k Is Large",
    "text": "Suppose we repeat the process in building our estimator and generate vectors e1, e2. We can compute and store the angles θxiej for all 1 ≤ i ≤ n, 1 ≤ j ≤ 2, and compute V = sgn(XR),ve1 = sgn(e T 1 R),ve2 = sgn(eT2 R). To compute θx1x2 , then we look at the k 4- tuples {(v1s, v2s, ve1s, ve2s)}ks=1.\nWe then have a 2× 2× 2 contingency table, which we give in Figure 8 in terms of n1 to n8.\nSimilar to Estimator 1, we now compute the probabilities of the six margins consisting of M1 = n5 + n6,M2 = n2 + n3,M3 = n1 + n4,M4 = n1 + n5,M5 = n2 + n8,M6 = n4 + n6. We require the following theorem for this computation.\nTheorem 4.1. Suppose we have three vectors x1,x2,x3, with angles θij being the angle between xi,xj . Given a random hyperplane, the probability that all the vectors are on one side of the hyperplane is given by 1− ∑ ij θij\n2π .\nProof. We can find some subspace spanned by orthogonal vectors b1,b2,b3 such that x1,x2,x3 are represented in terms of these bis. We can therefore think of this sub-\nspace as a “copy” of R3. WLOG, let x1,x2,x3 be unit vectors corresponding to points A,B,C on the unit sphere S2, which define a spherical triangleABC with sides a = θ2,3, b = θ1,3 and c = θ1,2, as the radius of the sphere is 1.\nFor the 2D plane containing the origin, A and B, consider its unique normal vector pointing at the same hemisphere containing C. We let C ′ be the point of intersection between that normal vector and the unit sphere. We define A′ and B′ similarly and thus A′B′C ′ is the polar triangle corresponding to the spherical triangle ABC. For any 2D plane containing the origin, it corresponds to a unique normal vector pointing at the same hemisphere that contains A′B′C ′.\nIt is clear any 2D plane containing the origin that intersects the spherical triangle ABC if and only if it separates one xi from the other two vectors (modulo the measure zero case when the plane contains two corners of the spherical triangle ABC). It is also clear that the corresponding normal vector of such a 2D plane intersects the sphere outside the polar triangle A′B′C ′. Hence, we have that\nP[the 3 vectors are not all on the same side of the hyperplane]\n= Area of hemisphere− Area of polar triangle A′B′C′\nArea of hemisphere (28)\nso\nP[all the 3 vectors are on one side of the hyperplane]\n= Area of polar triangle A′B′C ′\nArea of hemisphere (29)\nSince the polar triangle A′B′C ′ is a spherical triangle as well, its area is given by ∠A′ + ∠B′ + ∠C ′ − π. A standard result in spherical trigonometry shows the polar triangle A′B′C ′ is related to the spherical triangle ABC by\n∠A′ = π − a, ∠B′ = π − b, ∠C ′ = π − c (30)\nPutting everything together, we have\nP[all the 3 vectors are on one side of the hyperplane]\n= Area of polar triangle A′B′C ′\nArea of hemisphere (31)\n= ∠A′ + ∠B′ + ∠C ′ − π\n4π 2\n(32)\n= π − a+ π − b+ π − c− π\n2π (33)\n= 2π − θ2,3 − θ1,3 − θ1,2\n2π (34)\n= 1− θ1,2 + θ2,3 + θ1,3 2π . (35)\nTheorem 4.1 allows us to express pis in Figure 8 in terms of θx1,e1 , θx1,e2 , θx2,e1 , θx2,e2 , θ, and we can estimate θ by optimizing the new log-likelihood function\nl = C + 8∑ i=1 ni log(pi) (36)\nby using stored values of θx1,e1 , θx1,e2 , θx2,e1 , θx2,e2 .\nIn fact, we could keep on generating vectors e1, . . . , es to create more estimators if we know the probabilities of vectors x1, . . . ,xs being on one side of the hyperplane, which can be found via geometry theory. Algorithm 2 shows the general estimation algorithm.\nAlgorithm 2 General Algorithm For Our Estimators Pre-processing stage Initialize e1, . . . , es ; Initialize R Compute V = sgn(XR) Compute all ves = sgn(e T s R) for each xi ∈ X do Compute and store θxies Compute and store θeset end Actual stage for each vi,vj ∈ V do Count n1, n2, n3, . . . , n2s+1 Factorize log-likelihood into disjoint parts Find parameters p̂t which maximizes these log likelihoods Express θ̂ in terms of required p̂t. end\nWe note that these estimators only prove to be competitive when k is large, as the maximum likelihood estimates will be biased with small k. Intuitively, consider what it means if we set an arbitrary k = 64, and we have 8 cells. For some pi = ab , a < b, the observed ni k would not be close to pi unless k is large. For example, our original estimator quickly outperforms SRP and SBLSH when k is around 256 for our datasets. An estimator with vectors e1 and e2 would outperform our original estimator when k is an order of magnitude greater.\nWe conclude that constructing such estimators are only useful if we have extremely large k, which may not be practical in some cases."
  }, {
    "heading": "5. Future Work And Applications Of Our Estimator",
    "text": "From our theoretical analysis, any e generated always gives a reduction in variance or do no worse than the original estimate. In our experiments, we have set e to be the first singular vector as our heuristic. However, ê may be depen-\ndent on the problem to be solved. Finding ê which theoretically yields the most variance reduction based on a particular problem may be a good avenue of future research.\nHowever, this strategy of adding information to a dataset to take advantage of margins gives rise to many possibilities to further reduce the variance of several other estimates of distances, and we briefly describe some of them here.\nConventional random projections - Conventional random projections are used to estimate the inner products, Euclidean distances or lp distances (Li et al., 2012) with even values of p between vectors. The estimates are continuous, so a control variate (Kang, 2017) approach together with adding additional information may yield good results.\nStable random projections - Stable random projections are used to estimate lα distances between vectors. Similar to conventional random projections, we could add extra information and use a control variate technique to further improve these estimates.\nMinwise hashing - b-bit minwise hashing (Li & König, 2010) is used to estimate the resemblance of binary vectors, while weighted minwise hashing algorithms (Shrivastava, 2016; Ioffe, 2010) are used to estimate the Jaccard similarity between vectors. We can add an extra information e that is a binary vector (or weighted vector) to better improve the estimates of the resemblance (or Jaccard similarity).\nConditional random sampling - Conditional random sampling (Church et al., 2006) is a local sampling strategy which is used to estimate Hamming distances and χ2 distances between vectors, amongst other distances. Adding extra information could also improve these estimates."
  }, {
    "heading": "6. Conclusion",
    "text": "We have demonstrated that our estimator works well with SRP and SBLSH, as well as showed how to construct similar estimators of the same form.\nOur estimator can also be easily implemented on existing applications which use SRP or a modification of SRP without running SRP again. As we are only counting the margins, we can set e1 to be an existing vector xs, and compute the respective angles θxs,xi , s 6= i.\nWhile we have demonstrated an estimator with good performance, we feel that the idea behind the construction of this estimator (and subsequent estimators) is more important. We hope this idea can be extended to many other practical applications as well."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank the reviewers who provided us with many helpful comments. We hope we have addressed most of these comments in this version of the paper where possible. This research was supported by the SUTD Faculty Fellow Grant RGFECA17003."
  }],
  "year": 2018,
  "references": [{
    "title": "An elementary introduction to modern convex geometry",
    "authors": ["K. Ball"],
    "venue": "Flavors of geometry,",
    "year": 1997
  }, {
    "title": "Similarity estimation techniques from rounding algorithms",
    "authors": ["M.S. Charikar"],
    "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,",
    "year": 2002
  }, {
    "title": "Conditional random sampling: A sketch-based sampling technique for sparse data",
    "authors": ["K.W. Church", "P. Li", "T.J. Hastie"],
    "venue": "In In NIPS,",
    "year": 2006
  }, {
    "title": "On a least squares adjustment of a sampled frequency table when the expected marginal totals are known",
    "authors": ["W.E. Deming", "F.F. Stephan"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1940
  }, {
    "title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
    "authors": ["M.X. Goemans", "D.P. Williamson"],
    "venue": "Journal of the ACM (JACM),",
    "year": 1995
  }, {
    "title": "Result analysis of the nips 2003 feature selection challenge",
    "authors": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"],
    "venue": "Advances in Neural Information Processing Systems",
    "year": 2005
  }, {
    "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
    "authors": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"],
    "venue": "SIAM Rev.,",
    "year": 2011
  }, {
    "title": "Improved consistent sampling, weighted minhash and l1 sketching",
    "authors": ["S. Ioffe"],
    "venue": "In Data Mining (ICDM),",
    "year": 2010
  }, {
    "title": "Super-bit locality-sensitive hashing",
    "authors": ["J. Ji", "J. Li", "S. Yan", "B. Zhang", "Q. Tian"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "In Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations",
    "authors": ["P. Li", "K.W. Church"],
    "venue": "Comput. Linguist.,",
    "year": 2007
  }, {
    "title": "b-bit minwise hashing",
    "authors": ["P. Li", "C. König"],
    "venue": "In Proceedings of the 19th international conference on World wide web,",
    "year": 2010
  }, {
    "title": "Improving Random Projections Using Marginal Information",
    "authors": ["P. Li", "T. Hastie", "K.W. Church"],
    "venue": "COLT, volume 4005 of Lecture Notes in Computer Science,",
    "year": 2006
  }, {
    "title": "Approximating higher-order distances using random projections",
    "authors": ["P. Li", "M.W. Mahoney", "Y. She"],
    "venue": "CoRR, abs/1203.3492,",
    "year": 2012
  }, {
    "title": "Mathematical Statistics",
    "authors": ["J. Shao"],
    "venue": "URL https://books.google.com.sg/books?id= cyqTPotl7QcC",
    "year": 2003
  }, {
    "title": "Exact weighted minwise hashing in constant time",
    "authors": ["A. Shrivastava"],
    "venue": "arXiv preprint arXiv:1602.08393,",
    "year": 2016
  }],
  "id": "SP:9f431888e180883738a351cde48e0f492976b834",
  "authors": [{
    "name": "Keegan Kang",
    "affiliations": []
  }, {
    "name": "Wei Pin",
    "affiliations": []
  }],
  "abstractText": "Sign random projections (SRP) is a technique which allows the user to quickly estimate the angular similarity and inner products between data. We propose using additional information to improve these estimates which is easy to implement and cost efficient. We prove that the variance of our estimator is lower than the variance of SRP. Our proposed method can also be used together with other modifications of SRP, such as SuperBit LSH (SBLSH). We demonstrate the effectiveness of our method on the MNIST test dataset and the Gisette dataset. We discuss how our proposed method can be extended to random projections or even other hashing algorithms.",
  "title": "Improving Sign Random Projections With Additional Information"
}