{
  "sections": [{
    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013).\nDespite these results, the advantage of multimodal over linguistic-only models has only been\ndemonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a). Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b). Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic modality.\nExisting multi-modal architectures generally extract and process all the information from their specified sources of perceptual input. Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types. The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts. For instance, 72% of word tokens in the British National Corpus (Leech et al., 1994) were rated by contributors to the University of South Florida dataset (USF) (Nelson et al., 2004) as more abstract than the noun war, a concept that many would consider quite abstract.\nIn light of these considerations, we propose a novel algorithm for approximating conceptual concreteness. Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in captur-\n835\ning the semantic similarity of concepts. Further, our algorithm constitutes the first means of quantifying conceptual concreteness that does not rely on labor-intensive experimental studies or annotators. Finally, we demonstrate the application of this unsupervised concreteness metric to the semantic classification of adjective-noun pairs, an existing NLP task to which concreteness data has proved valuable previously."
  }, {
    "heading": "2 Experimental Approach",
    "text": "Our experiments focus on multi-modal models that extract their perceptual input automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets.\nWe use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets."
  }, {
    "heading": "2.1 Image Dispersion-Based Filtering",
    "text": "Following the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts. Our algorithm is motivated by the intuition that the diversity of images returned for a particular concept depends on its concreteness (see Figure 1). Specifically, we anticipate greater congruence or similarity among a set of images for, say, elephant than among images for happiness. By exploiting this connection, the method approximates the concreteness of concepts, and provides a basis to filter the corresponding perceptual information.\nFormally, we propose a measure, image dispersion d of a concept word w, defined as the average pairwise cosine distance between all the image representations { ~w1 . . . ~wn} in the set of images for that concept:\nd(w) = 1 2n(n− 1) ∑ i<j≤n 1− ~wi · ~wj| ~wi|| ~wj | (1)\nWe use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50.\nGenerating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an\nimage by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means.\nPrevious NLP-related work uses SIFT (Feng and Lapata, 2010; Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008).\nThe descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images.\nGenerating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b)."
  }, {
    "heading": "2.2 Evaluation Gold-standards",
    "text": "We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)).\nTo test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see\ne.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators.\nWe create a representative evaluation set of USF pairs as follows. We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness. We denote these sets C, for concrete, and A for abstract respectively. We then extract all pairs (w1, w2) in the USF dataset such that bothw1 andw2 are inA∪C. This yields an evaluation set of 903 pairs, of which 304 are such that w1, w2 ∈ C and 317 are such that w1, w2 ∈ A.\nThe images used in our experiments and the evaluation gold-standards can be downloaded from http://www.cl.cam.ac.uk/ ˜dk427/dispersion.html."
  }, {
    "heading": "3 Improving Multi-Modal Representations",
    "text": "We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included. If not, in accordance with the Dual Coding Theory of human concept processing (Paivio, 1990), only the linguistic representation is used. For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter. We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations). Similarity between concept pairs is calculated using cosine similarity.\nAs Figure 3 shows, dispersion-filtered multimodal representations significantly outperform\nstandard multi-modal representations on both evaluation datasets. We observe a 17% increase in Spearman correlation on WordSim353 and a 22% increase on the USF norms. Based on the correlation comparison method of Steiger (1980), both represent significant improvements (WordSim353, t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1). In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012))."
  }, {
    "heading": "4 Concreteness and Image Dispersion",
    "text": "The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail."
  }, {
    "heading": "4.1 Quantifying Concreteness",
    "text": "To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying con-\ncepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%.\nWhile well-understood intuitively, concreteness is not a formally defined notion. Quantities such as the USF concreteness score depend on the subjective judgement of raters and the particular annotation guidelines. According to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation. As illustrated in Figure 4, our binary classification conforms to this characterization. The importance of the visual modality is significantly greater when evaluating on pairs for which both concepts are classified as concrete than on pairs of two abstract concepts.\nImage dispersion is also an effective predictor of concreteness on samples for which the abstract/concrete distinction is less clear. On a different set of 200 concepts extracted by random sampling from the USF dataset stratified by concreteness rating (including concepts across the concreteness spectrum), we observed a high correlation between abstractness and dispersion (Spearman ρ = 0.61, p < 0.001). On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying\nthe very abstract or very concrete concepts. As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract.\nIt should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, Sánchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of these approaches, the image dispersion approach requires no hand-coded resources. It is therefore more scalable, and instantly applicable to a wide range of languages."
  }, {
    "heading": "4.2 Classifying Adjective-Noun Pairs",
    "text": "Finally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy for concreteness. Turney et al. (2011) showed that concreteness is applicable to the classification of adjective-noun modification as either literal or non-literal. By applying a logistic regression with noun concreteness as the predictor variable, Turney et al. achieved a classification accu-\nracy of 79% on this task. This model relies on significant supervision in the form of over 4,000 human lexical concreteness ratings.1 Applying image dispersion in place of concreteness in an identical classifier on the same dataset, our entirely unsupervised approach achieves an accuracy of 63%. This is a notable improvement on the largest-class baseline of 55%."
  }, {
    "heading": "5 Conclusions",
    "text": "We presented a novel method, image dispersionbased filtering, that improves multi-modal representations by approximating conceptual concreteness from images and filtering model input. The results clearly show that including more perceptual input in multi-modal models is not always better. Motivated by this fact, our approach provides an intuitive and straightforward metric to determine whether or not to include such information.\nIn addition to improving multi-modal representations, we have shown the applicability of the image dispersion metric to several other tasks. To our knowledge, our algorithm constitutes the first unsupervised method for quantifying conceptual concreteness as applied to NLP, although it does, of course, rely on the Google Images retrieval algorithm. Moreover, we presented a method to classify adjective-noun pairs according to modification type that exploits the link between image dispersion and concreteness. It is striking that this apparently linguistic problem can be addressed solely using the raw data encoded in images.\nIn future work, we will investigate the precise quantity of perceptual information to be included for best performance, as well as the optimal filtering threshold. In addition, we will explore whether the application of image data, and the interaction between images and language, can yield improvements on other tasks in semantic processing and representation."
  }, {
    "heading": "Acknowledgments",
    "text": "DK is supported by EPSRC grant EP/I037512/1. FH is supported by St John’s College, Cambridge. AK is supported by The Royal Society. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We thank the anonymous reviewers for their helpful comments.\n1The MRC Psycholinguistics concreteness ratings (Coltheart, 1981) used by Turney et al. (2011) are a subset of those included in the USF dataset."
  }],
  "year": 2014,
  "references": [{
    "title": "A study on similarity and relatedness using distributional and wordnet-based approaches",
    "authors": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Paşca", "Aitor Soroa."],
    "venue": "Proceedings of Human Language Technologies: The 2009",
    "year": 2009
  }, {
    "title": "Integrating experiential and distributional data to learn semantic representations",
    "authors": ["Mark Andrews", "Gabriella Vigliocco", "David Vinson."],
    "venue": "Psychological review, 116(3):463.",
    "year": 2009
  }, {
    "title": "Grounding conceptual knowledge in modality-specific systems",
    "authors": ["Lawrence W Barsalou", "W Kyle Simmons", "Aron K Barbey", "Christine D Wilson."],
    "venue": "Trends in cognitive sciences, 7(2):84–91.",
    "year": 2003
  }, {
    "title": "Using visual information to predict lexical preference",
    "authors": ["Shane Bergsma", "Randy Goebel."],
    "venue": "RANLP, pages 399–405.",
    "year": 2011
  }, {
    "title": "Image classification using random forests and ferns",
    "authors": ["Anna Bosch", "Andrew Zisserman", "Xavier Munoz."],
    "venue": "Proceedings of ICCV.",
    "year": 2007
  }, {
    "title": "Distributional semantics in technicolor",
    "authors": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Asso-",
    "year": 2012
  }, {
    "title": "Multimodal distributional semantics",
    "authors": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni."],
    "venue": "Journal of Artificial Intelligence Research, 49:1–47.",
    "year": 2014
  }, {
    "title": "The MRC psycholinguistic database",
    "authors": ["Max Coltheart."],
    "venue": "The Quarterly Journal of Experimental Psychology, 33(4):497–505.",
    "year": 1981
  }, {
    "title": "Imagenet: A large-scale hierarchical image database",
    "authors": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."],
    "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.",
    "year": 2009
  }, {
    "title": "WordNet",
    "authors": ["Christiane Fellbaum."],
    "venue": "Wiley Online Library.",
    "year": 1999
  }, {
    "title": "Visual information in semantic representation",
    "authors": ["Yansong Feng", "Mirella Lapata."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99. Asso-",
    "year": 2010
  }, {
    "title": "Placing search in context: The concept revisited",
    "authors": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."],
    "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406–",
    "year": 2001
  }, {
    "title": "Concreteness and corpora: A theoretical and practical analysis",
    "authors": ["Felix Hill", "Douwe Kiela", "Anna Korhonen."],
    "venue": "CMCL 2013.",
    "year": 2013
  }, {
    "title": "A quantitative empirical analysis of the abstract/concrete distinction",
    "authors": ["Felix Hill", "Anna Korhonen", "Christian Bentz."],
    "venue": "Cognitive science, 38(1).",
    "year": 2013
  }, {
    "title": "Improving word representations via global context and multiple word prototypes",
    "authors": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-",
    "year": 2012
  }, {
    "title": "A preliminary study on the impact of lexical concreteness on word sense disambiguation",
    "authors": ["Oi Yee Kwong."],
    "venue": "PACLIC, pages 235–244.",
    "year": 2008
  }, {
    "title": "Claws4: the tagging of the british national corpus",
    "authors": ["Geoffrey Leech", "Roger Garside", "Michael Bryant."],
    "venue": "Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622– 628. Association for Computational Linguistics.",
    "year": 1994
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of International Conference of Learning Representations, Scottsdale, Arizona, USA.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "The University of South Florida free association, rhyme, and word fragment norms",
    "authors": ["Douglas L Nelson", "Cathy L McEvoy", "Thomas A Schreiber."],
    "venue": "Behavior Research Methods, Instruments, & Computers, 36(3):402–407.",
    "year": 2004
  }, {
    "title": "Mental representations: A dual coding approach",
    "authors": ["Allan Paivio."],
    "venue": "Oxford University Press.",
    "year": 1990
  }, {
    "title": "A multimodal LDA model integrating textual, cognitive and visual modalities",
    "authors": ["Stephen Roller", "Sabine Schulte im Walde."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157, Seattle,",
    "year": 2013
  }, {
    "title": "Ontology-based information content computation",
    "authors": ["David Sánchez", "Montserrat Batet", "David Isern."],
    "venue": "Knowledge-Based Systems, 24(2):297–303.",
    "year": 2011
  }, {
    "title": "Web-scale k-means clustering",
    "authors": ["D Sculley."],
    "venue": "Proceedings of the 19th international conference on World wide web, pages 1177–1178. ACM.",
    "year": 2010
  }, {
    "title": "Grounded models of semantic representation",
    "authors": ["Carina Silberer", "Mirella Lapata."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods",
    "year": 2012
  }, {
    "title": "Video Google: a text retrieval approach to object matching in videos",
    "authors": ["J. Sivic", "A. Zisserman."],
    "venue": "Proceedings of the Ninth IEEE International Conference on Computer Vision, volume 2, pages 1470– 1477, Oct.",
    "year": 2003
  }, {
    "title": "Tests for comparing elements of a correlation matrix",
    "authors": ["James H Steiger."],
    "venue": "Psychological Bulletin, 87(2):245.",
    "year": 1980
  }, {
    "title": "Literal and metaphorical sense identification through concrete and abstract context",
    "authors": ["Peter D Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen."],
    "venue": "Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing, pages",
    "year": 2011
  }, {
    "title": "VLFeat: An open and portable library of computer vision algorithms",
    "authors": ["A. Vedaldi", "B. Fulkerson."],
    "venue": "http://www.vlfeat.org/.",
    "year": 2008
  }, {
    "title": "Labeling images with a computer game",
    "authors": ["Luis Von Ahn", "Laura Dabbish."],
    "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319–326. ACM.",
    "year": 2004
  }],
  "id": "SP:1c64663865a09b871d7377722f1a0bfcabdf13df",
  "authors": [{
    "name": "Douwe Kiela",
    "affiliations": []
  }, {
    "name": "Felix Hill",
    "affiliations": []
  }, {
    "name": "Anna Korhonen",
    "affiliations": []
  }, {
    "name": "Stephen Clark",
    "affiliations": []
  }],
  "abstractText": "Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition. However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others. We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks.",
  "title": "Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More"
}