{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The success of deep learning owes much to efficient gradient computation using backpropagation (Rumelhart et al., 1986). When the model of interest includes internal stochasticities, the objective function is often written as a stochastic computational graph (Schulman et al., 2015). In this case, the exact gradient computation is intractable in general, and an approximate estimation is required. The variance introduced by the approximation often degrades the optimization performance for deep models, and there-\n1Preferred Networks, Tokyo, Japan 2The University of Tokyo, Tokyo, Japan 3RIKEN, Tokyo, Japan. Correspondence to: Seiya Tokui <tokui@preferred.jp>, Issei Sato <sato@k.utokyo.ac.jp>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nfore variance reduction is crucial for practical learning. However, few things are known about its theoretical aspects, and we often struggle with model-specific heuristics whose degree of optimality is difficult to know.\nThe likelihood-ratio method (Glynn, 1990; Williams, 1992) and the reparameterization trick (Williams, 1992; Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzarogredilla, 2014) are widely used for the gradient estimation. The likelihood-ratio method only requires the computation of density functions and their derivatives, and therefore it is applicable to a wide range of models including those with discrete variables. It requires variance reduction techniques in practice. The most common technique is the use of a baseline value (Paisley et al., 2012; Bengio et al., 2013; Ranganath et al., 2014; Mnih & Gregor, 2014; Gu et al., 2016a) which is subtracted from a sampled objective value. The optimal baseline is difficult to compute in general, and we often use alternatives that are efficiently computed, some of which are based on model-specific heuristics. The reparameterization trick, on the other hand, has a small estimation variance in practice and is only applicable to models with certain continuous variables. Various models with continuous variables have been proposed using it, whereas less progress on the research of deep discrete variable models has been made because of the inapplicability of this method.\nIn this paper, we give a novel framework to formulate gradient estimators. It is derived by the reparameterization and the local marginalization analogous to the local expectation gradient (Titsias & Lázaro-Gredilla, 2015). The likelihood-ratio method and the reparameterization trick can be formalized under this framework, and therefore it bridges these two families of estimators. We can derive the optimal estimator, which gives a lower bound of the variance of all estimators covered by the framework. Since the estimator is derived by applying local marginalization to the reparameterized gradient, we named it the reparameterization and marginalization (RAM) estimator. This estimator is an instance of the likelihood-ratio estimator with the optimal baseline, and therefore it can be used to evaluate the variance of existing baseline techniques.\nWhen the variable of interest follows a Bernoulli distribution, we can derive a tighter connection of a wider range of\nestimators to the framework. For example, the local expectation gradient (Titsias & Lázaro-Gredilla, 2015) becomes covered by our framework, and the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Raiko et al., 2015) approximates the optimal estimator where the finite difference of the objective function is replaced by the infinitesimal first-order approximation. Furthermore, the optimal estimator is reduced to a likelihood-ratio estimator with an input-dependent baseline, which implies that a practical baseline technique might achieve a near-optimal variance.\nThe rest of this paper is organized as follows. We overview the related work in Sec.2 and formulate the gradient estimation problem in Sec.3. We introduce our framework in Sec.4 and derive important estimators with it in Sec.5. We also introduce a wider range of estimators for Bernoulli variables in Sec.6. We then show experimental results in Sec.7 and give a conclusion in Sec.8."
  }, {
    "heading": "2. Related Work",
    "text": "The gradient estimation problem was being studied in the field of simulation around 1990, which is well summarized in L’Ecuyer (1991). The likelihood-ratio method (Glynn, 1989) is a general approach for solving the problem, in which the parametric density qφ(z) is replaced by qφ(z) q0(z) q0(z) where q0 := qφ is fixed against φ on differentiation. The ratio qφ(z)q0(z) is called the likelihood ratio, hence the name of this method. It can be seen as an importance sampling method that uses a proposal q0 (Jie & Abbeel, 2010), with which there is a study on reducing the variance by using a proposal better than q0 (Ruiz et al., 2016a). Another approach is the finite-difference method, in which the use of common random numbers, i.e., using the same random numbers to run two perturbed simulations, is effective in reducing the variance. The common random numbers naturally appear in the formulation of the optimal estimator of our framework.\nThe likelihood-ratio method has been combined with baselines and was introduced to the policy gradient methods for reinforcement learning, which is called the REINFORCE algorithm (Williams, 1992). The baseline technique is used for reducing the variance. A simple estimation of the average reward is commonly used as the baseline, and the optimal constant baseline that minimizes the variance is also derived (Weaver & Tao, 2001). The likelihood-ratio estimator has also been used for black-box variational inference (Ranganath et al., 2014). The likelihood-ratio estimator is used to derive the gradient estimation without depending on the specific form of the distributions. From a statistical point of view, the baseline can be seen as a special form of control variates, for which the optimal one can be derived again. The baseline technique has been\nfurther made sophisticated by involving the variable-wise baselines and those depending on the varaible of interest (Mnih & Gregor, 2014; Gu et al., 2016a). Some of them are also exported to policy-gradient methods (Gu et al., 2016b). Taking the local expectation of the likelihood-ratio estimator (Titsias & Lázaro-Gredilla, 2015) is another approach of variance reduction.\nFor variational inference of models with continuous variables, the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzaro-gredilla, 2014) is widely used. It is easy to implement with modern frameworks of automatic differentiation. It also has low variance in practice, although the superiority to the likelihood-ratio estimator is not guaranteed in general (Gal, 2017). This method is also applied to the continuous relaxation of discrete variables (Jang et al., 2016; Maddison et al., 2016).\nOn the one hand, the connection between the likelihoodratio method and the reparameterization trick is studied in some literature, especially on continuous variables for which a tractable reparameterization is not available (Ruiz et al., 2016b). On the other hand, there are fewer studies for discrete variables. This paper provides a bridge between these estimators for discrete variables."
  }, {
    "heading": "3. Problem Formulation",
    "text": "Our task is to optimize an expectation over a parameterized distribution. The objective function is given as F (φ;x) = Eqφ(z|x)f(x, z), where f is a feasible function, qφ(z|x) = ∏M i=1 qφi(zi|pai) is a directed graphical model of M variables z = (z1, . . . , zM ) conditioned on an input to the system x, pai are the parent nodes of zi, and φ are the model parameters. Each conditional qφi(zi|pai) is continuously differentiable w.r.t. φi and is typically a simple distribution such as a Bernoulli or Gaussian whose parameters are computed by a neural network with weights φi. For simplicity, we will assume that φi and φi′ for i 6= i′ do not share any parameters; however, this assumption can be easily removed. We want to optimize F by stochastic gradient methods, which require an unbiased estimation of its gradient∇φF .\nA motivating example is variational learning of a generative model pθ(x, z) with an approximate posterior qφ(z|x). In this case, the objective function is the expectation of f(x, z) = log pθ(x, z) − log qφ(z|x), which gives a lower bound of the log likelihood log pθ(x). On the one hand, the gradient w.r.t. the generative parameter θ is easily estimated by a Monte Carlo simulation. On the other hand, estimating the gradient w.r.t. φ is not trivial, which falls into the above general setting. Note that we omit the gradient incurred from the dependency of the second term − log qφ(z|x) on φ from our discussions since this gradi-\nent term is easy to estimate with low variance."
  }, {
    "heading": "4. Proposed Framework",
    "text": "Here we give a general formulation of our framework of gradient estimation. The framework is based on the reparameterization of variables, which we also review.\nSuppose that each sample drawn from a conditional is reparameterized as follows.\nzi ∼ qφi(zi|pai)⇔ zi = gφi(pai, i), i ∼ p( i).\nHere i is a noise variable. We will give concrete examples of reparameterization later, and here we only emphasize that gφi might be a non-continuous function. We write the whole reparameterization as z = gφ(x, ).\nUsing this reparameterization, we derive the general form of gradient estimation. Let \\i = { 1, . . . , i−1, i+1, . . . , M}. We partially exchange the differentiation and integration as follows.\n∇φiF (φ;x) = ∇φiE f(x, gφ(x, )) = E \\i∇φiE if(x, gφ(x, )). (1)\nUnlike the reparameterization trick, this equation holds even if the function gφ is not continuous because the local expectation E if(x, gφ(x, )) is differentiable. The technique of separating variables in leave-one-out manner is similar to Eq. (8) of Titsias & Lázaro-Gredilla (2015), whereas it is applied to reparameterized, mutuallyindependent noise variables in our case.\nEquation (1) gives our framework of gradient estimation. Given a way to estimate the local gradient ∇φiE if(x, gφ(x, )), we can estimate ∇φiF (φ;x) by sampling \\i and estimating the local gradient. Many existing estimators are derived by specifying a method of local gradient estimation, which we review in the next section.\nExamples of Reparameterization: We introduce typical ways of reparameterizing popular distributions. When qφi(zi|pai) = N (zi|µi, σ2i ) is a Gaussian distribution, zi can be reparameterized as zi = µi + iσi, i ∼ N ( i; 0, 1) (Kingma & Welling, 2014). In this case, the change-ofvariable formula gφi(pai, i) = µi(pai;φi)+ iσi(pai;φi) is differentiable w.r.t. φi. We can derive a reparameterization for Bernoulli variables as well. Suppose zi ∈ {0, 1} is a binary variable following a Bernoulli distribution qφi(zi|pai) = µ zi i (1− µi)1−zi . It can be reparameterized using a uniform noise i ∼ U(0, 1) as zi = H(µi− i),\nwhere H(x) =\n{ 1 (x > 0)\n0 (x ≤ 0) is the Heaviside step func-\ntion. In this case, the change-of-variable formula zi =\nH(µi(pai;φi)− i) is not continuous in general. For categorical variables, we can use the Gumbel-Max trick (Gumbel, 1954; Jang et al., 2016; Maddison et al., 2016) for the reparameterization in a similar way."
  }, {
    "heading": "5. Derivation of Gradient Estimators",
    "text": "We derive existing estimators on the basis of our general framework (1). We also derive the estimator that is optimal in terms of the estimation variance."
  }, {
    "heading": "5.1. Likelihood-Ratio Estimator",
    "text": "The likelihood-ratio estimator is derived by using the logderivative trick for the local gradient estimation. Let bi(x, ) be a baseline for zi, and \\i ∼ p( \\i). We use z = gφ(x, ) and omit the dependency of z on . The likelihood-ratio estimator with baseline bi is a Monte Carlo estimate of the following expectation.\n∇φiE if(x, gφ(x, )) = E i(f(x, z)− bi(x, ))∇φi log qφi(zi|pai) + Ci(x, \\i).\n(2)\nHere Ci(x, \\i) = E ibi(x, )∇φi log qφi(zi|pai), which has to be analytically computed.\nThere are many baseline techniques for variance reduction. We classify them into four categories as follows.\n• Constant baseline is a constant of all variables {x, }. It is a common choice for the baseline. In this case, it holds that Ci = 0. An exponential moving average of the simulated function f is often used.\n• Independent baseline is a baseline that is constant against i. It can depend on other variables, {x, \\i}. In this case, it again holds that Ci = 0. Two techniques proposed by Mnih & Gregor (2014) can be seen as examples of baselines in this class. One is the input-dependent baseline, which is a neural network that predicts the sampled objective value f(x, z) from x and pai. The other one is the use of local signals, where the terms of f that are not descendants of zi in the stochastic computational graphs are omitted. It can be seen as a baseline that includes all these terms.\n• Linear baseline is a baseline that is a linear function of zi, i.e., bi = z>i ui(x, \\i) + vi(x, \\i)\n1, where ui and vi are arbitrary functions. In this case, we can write Ci = (∇φiµi)>ui(x, \\i), where µi = Eqφi (zi|pai)zi is the mean of zi. The MuProp estimator (Gu et al.,\n1When zi is a binary or continuous scalar, the transposition is not needed. When zi is a categorical variable, we represent it by a one-hot vector, for which z>i ui is the innerproduct of two vectors.\n2016a) is an example of estimators with linear baselines, where the baseline is given as the first-order approximation of the mean-field network of f at µi.\n• Fully-informed baseline is a baseline that depends on all of x and , possibly in a nonlinear way. This is the most general class of baselines.\nIt is easily expected that the fully-informed baseline can achieve the lowest variance. We will show that the optimal estimator under the framework (1) falls into this category."
  }, {
    "heading": "5.2. Reparameterization Trick Estimator",
    "text": "The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzaro-gredilla, 2014) is a common way to estimate the gradient for models with continuous variables. It is derived by exchanging the differentiation and integration of the local gradient as follows.\n∇φiE if(x, gφ(x, )) = E i∇φif(x, gφ(x, )). (3)\nNote that this equation holds only if the function gφ(x, ) is differentiable, and therefore the reparameterization trick is only applicable to continuous variables.\nThe reparameterization trick often gives a better estimation of the gradient compared with the likelihood-ratio estimator, although there is no theoretical guarantee. Indeed, we can construct an example for which the likelihood-ratio estimator gives a better estimation (Gal, 2017)."
  }, {
    "heading": "5.3. Optimal Estimator",
    "text": "The optimal estimator is obtained by analytically computing the local gradient ∇φiE if(x, gφ(x, )). Let z\\i := {z1, . . . , zi−1, zi+1, . . . , zM}. When we fix \\i and modify the value of zi, the descendant variables of zi might be changed because they are functions of zi and noise variables. We denote the resulting values of z\\i by z\\i = hφ\\i(x, zi, \\i). The function hφ\\i(x, zi, \\i) represents the ancestral sampling procedure of z\\i with given \\i and clamped zi. Using the reparameterization again, we obtain E if(x, gφ(x, )) = Eqφi (zi|pai)f(x, z). The local gradient is then computed as follows.\n∇φiE if(x, gφ(x, )) = ∑ zi f(x, z)∇φiqφi(zi|pai) ∣∣∣ z\\i=hφ\\i (x,zi, \\i) . (4)\nIf zi is continuous, the summation is replaced by an integral, which is approximated numerically. The resulting algorithm is shown in Alg. 1, which we name the reparameterization and marginalization (RAM) estimator. It requires M times evaluations of h, and therefore it scales\nAlgorithm 1 Algorithm for RAM estimator (4) for discrete zi’s. If zi is continuous, the loop over all the configurations of zi is replaced by a loop over integration points. Require: a set of parameters φ and an input variable x.\n1: Sample ∼ p( ). 2: for i = 1, . . . ,M do 3: for all configurations of zi do 4: z\\i := hφ\\i(x, zi, \\i). 5: fzi := f(x, z)∇φiqφi(zi|pai). 6: end for 7: ∆i := ∑ zi fzi . 8: end for 9: return (∆1, . . . ,∆M ) as an estimation of∇F (φ;x).\nworse than other estimators 2. However, these evaluations are easily parallelized, and it runs fast enough for models of moderate size.\nThe optimality of this estimator is stated in the following theorem. Let φij be the j-th element of the vector of parameters φi, and let ∂ij = ∂/∂φij for notational simplicity.\nTheorem 1. Suppose an unbiased estimator δij of the local derivative ∂ijE if(x, gφ(x, )), i.e., δij is a random variable whose expectation matches the local derivative. Let Vij be the variance of the estimator δij and V ?ij be the variance of the RAM estimator. Then, it holds that V ?ij ≤ Vij .\nProof. This follows from the standard RaoBlackwellization argument.\nWe briefly review the relationships between the RAM estimator and existing ones.\nRelationship to the Likelihood-Ratio Estimator: The RAM estimator can be seen as an example of the likelihood-ratio estimators with fully-informed baselines. Let bi(x, ) = f(x, gφ(x, )). Then, the log-derivative term of Eq. (2) is canceled, and only the residual Ci(x, \\i) = ∇φiE if(x, gφ(x, )) remains. The analytically-computed residual is equivalent to the RAM estimator. Since this estimator gives the minimum variance, our likelihood-ratio formulation (2) contains the optimal estimator.\nWhile the fully-informed baseline is too general in practice to be efficiently computed, much more restrictive independent baselines can achieve the optimal estimator when zi follows a Bernoulli distribution. Let V LRij (bi) be the variance of the likelihood-ratio estimator with baseline bi.\nTheorem 2. Suppose that qφi(zi|pai) is a Bernoulli distribution. Then, there is one and only one baseline b?i such\n2 The difference in computational cost against the local expectation gradient (Titsias & Lázaro-Gredilla, 2015) comes from the inapplicability of pivot samples.\nthat b?i is constant against i and V ? ij = V LR ij (b ? i ).\nThe proof is given in Sec.6. This result implies that, for Bernoulli variables, the optimal variance might be obtained by a practical class of baseline techniques. Note that the optimal baseline b?i might depend on the noise variables corresponding to the descendants of zi, which are not used by existing baseline techniques.\nRelationship to the Reparameterization Trick: Theorem 1 also states that the RAM estimator gives a variance not larger than that of the reparameterization trick. Indeed, the RAM estimator is based on an analytical computation of the integral (3), and therefore gives the lower or equal variance. In practice, it is infeasible to compute the local gradient analytically, and numerical approximation is required. We can approximate the integral with high precision in practice, because zi is usually a scalar variable and therefore we only need to evaluate the function f(x, gφ(x, )) at a few integration points of zi.\nRelationship to the Local Expectation Gradient: The local expectation gradient (Titsias & Lázaro-Gredilla, 2015) is an application of local marginalization to the likelihood-ratio estimator. Let mbi be the Markov blanket of zi. This estimator is then derived as follows.\n∇φiF (φ;x) = Eqφ(z|x)f(x, z)∇φi log qφi(zi|pai) = Eqφ(z\\i|x)Eqφ(zi|mbi)f(x, z)∇φi log qφi(zi|pai)\n= Eqφ(z\\i|x) ∑ zi qφ(zi|mbi) qφi(zi|pai) f(x, z)∇φiqφi(zi|pai). (5)\nFor the Monte Carlo simulation of z\\i, z is first sampled from qφ(z|x), and then zi is discarded. It corresponds to sampling and computing z\\i by using it in the reparameterized notation. If the latent variables z1, . . . , zM are mutually independent given x, the density ratio factor qφ(zi|mbi)/qφi(zi|pai) equals 1, and therefore this estimator is equivalent to the RAM estimator (4). Otherwise, the density ratio factor remains, and these estimators do not match in general. The inference distribution qφ(zi|mbi) can be computed as\nqφ(zi|mbi) = qφ(zi,mbi \\ pai|pai)∑ zi qφ(zi,mbi \\ pai|pai) .\nTherefore, the density ratio is proportional to qφ(zi,mbi \\ pai|pai). It tends to concentrate on zi used in the sampling of mbi \\ pai, in which case the estimator degenerates to the plain likelihood-ratio estimator. Therefore, it cannot be guaranteed to have a lower variance than the likelihoodratio estimator with baselines in general.\nThe RAM estimator can be seen as an application of the same technique to the reparameterized expectation (3). Thanks to the reparameterization, there is no need to solve the inference problem qφ(zi|mbi), and therefore the problematic density ratio factor does not appear. The evaluation of f(x, z) with fixed z\\i does not reflect the full influence of the choice of zi, whereas the reparameterized counterpart f(x, z)|z\\i=hφ\\i (x,zi, \\i) does reflect it."
  }, {
    "heading": "6. Analyzing Estimators for Binary Variables",
    "text": "A Bernoulli variable is the most fundamental example of a discrete variable, and some estimators are dedicated for it. It is beneficial to study the applications of any estimators to Bernoulli variables because they facilitate understanding and still contain most of the essential characteristics of discrete distributions. In some cases, an estimator has a connection to other estimators only when applied to Bernoulli variables. Here we focus on Bernoulli variables and introduce how each estimator can be formalized and related to others. The derivations are given in the supplementary material 3\nSuppose that qφi(zi|pai) is a Bernoulli distribution of the mean parameter µi = µi(pai, φi). Let fk = f(x, zi = k, z\\i = hφ\\i(x, zi, \\i)) for k ∈ {0, 1}, i.e., fk is the reparameterized objective value for zi = k. All estimators we introduce here can be written as an estimation of the gradient w.r.t. µi multiplied by ∇φiµi, and therefore we only focus on the gradient w.r.t. µi denoted by ∆i."
  }, {
    "heading": "6.1. Likelihood-Ratio Estimator",
    "text": "The likelihood-ratio estimator for a Bernoulli variable with an independent baseline bi is written as follows.\n∆LRi = { (f1 − bi)/µi w.p. µi, −(f0 − bi)/(1− µi) w.p. 1− µi. (6)\nIt can be interpreted as an importance sampling estimation of the sum of f1−bi and−(f0−bi). Indeed, the likelihoodratio estimator for the general class of distributions can be seen as an importance sampling estimation of the expectation. When the distribution has low entropy (i.e., µi is close to 0 or 1), the variance of ∆LRi becomes large. However, it does not always mean that the variance of the gradient w.r.t. φi becomes huge, because in this case the sigmoid activation that outputs µi is in a flat regime so that its small derivative somehow alleviates the large variance. Neither does it mean that the variance is always small enough to optimize complex models.\n3The supplementary material is attached to the arXiv version of this paper."
  }, {
    "heading": "6.2. Optimal Estimator",
    "text": "The RAM estimator of the gradient w.r.t. µi is simply written as the difference of the f value at zi = 1 and zi = 0.\n∆?i = f1 − f0. (7)\nUsing this formulation, we can prove Theorem 2.\nProof of Theorem 2. Let bi = (1−µi)f1 +µif0. Then, the likelihood-ratio estimator (6) with baseline bi is equivalent to the RAM estimator (7). In this case, both cases of (6) are equal to ∆?i . This baseline does not depend on i, and therefore we conclude the proof by letting b?i = bi.\nInterestingly, the optimal baseline is an expectation of fk with the mean of k being 1− µ instead of µ. This is different from the mean objective value f̄ = µif1 + (1− µi)f0, which a constant mean baseline approximates. The difference becomes large when µi is close to 0 or 1.\nThe optimal estimator still has a positive variance since the noise variables \\i are not integrated out. In Eq. (7), f1 and f0 are evaluated with the same configurations of these noise variables. The likelihood-ratio estimator can also be seen as an estimator that separately samples f1 and f0 at different iterations in which they use the separate samples of \\i. When the number of variables is large, the influence of one variable zi on the objective value f(x, z) is small, and we can expect that f1 and f0 have a positive covariance. In general, the estimation variance of the difference of two random variables X,Y is reduced by estimating them with a positive covariance since V[X − Y ] = VX + VY − 2Cov(X,Y ), and therefore our estimator effectively reduces the variance by using the same configuration of the noise variables. This technique is known as common random numbers, which is also used to reduce the variance of the gradient estimations with the finite difference method for stochastic systems (L’Ecuyer, 1991)."
  }, {
    "heading": "6.3. Local Expectation Gradient",
    "text": "The local expectation gradient (5) has a special view as a likelihood-ratio estimator when zi is a Bernoulli variable. Let πi = qφ(zi = 1|mbi). Let f ′k = f(x, zi = k, z\\i = hφ\\i(x, zi = 1 − k, \\i)), i.e., f ′1−k is the objective value with fixed z\\i and flipped zi. Note that f ′k can be different from fk when some variables in z\\i depend on zi. Then, the local expectation gradient is written as follows.\n∆LEGi =\n{ πi µi f1 − 1−πi1−µi f ′ 0 w.p. µi,\nπi µi f ′1 − 1−πi1−µi f0 w.p. 1− µi.\nIt is further rewritten as follows.\n∆LEGi =  f1− 1−πi 1−µi ((1−µi)f1+µif ′0) µi w.p. µi,\n− f0− πi µi ((1−µi)f ′1+µif0) 1−µi w.p. 1− µi.\nThus, it can be seen as a likelihood-ratio estimator with baseline\nbLEGi = 1− qφ(zi|mbi) 1− qφi(zi|pai) b′ik\nwhere b′ik = (1 − qφi(zi|pai))fk + qφi(zi|pai)f ′1−k. The unweighted value b′ik has a similar form as the optimal baseline b?i , where f1−k is replaced by f ′ 1−k. The final baseline bLEGi is given by multiplying b ′ ik by the density ratio. We have seen that it tends to be close to 0, in which case the baseline is also close to 0 so that the estimator degenerates to the plain likelihood-ratio estimator. Even if the weight does not vanish, Theorem 2 shows that the local expectation gradient estimator for Bernoulli variables has higher variance than the optimal one unless all latent variables are mutually independent given x."
  }, {
    "heading": "6.4. Straight-Through Estimator",
    "text": "We give one example of an estimator dedicated for Bernoulli variables, the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Raiko et al., 2015). It is a biased estimator that leverages the gradient of f so that we can obtain the high-dimensional information of the direction towards which the objective would be decreasing. The estimator is written as follows.\n∆STi =  ∂f ∂zi ∣∣∣ zi=1\nw.p. µi, ∂f ∂zi ∣∣∣ zi=0 w.p. 1− µi. (8)\nObserving that Eq. (7) gives the finite difference of f between zi = 1 and zi = 0, we can see that the straightthrough estimator is its infinitesimal counterpart at the sampled zi. Thus, this estimator is equivalent to our estimator (and is therefore unbiased) when f is a linear function of zi. The difference between these estimators becomes large when the nonlinearity of f increases. If we consider a general class of the evaluation function f , we can construct an adversarial function f such that the derivative at zi ∈ {0, 1} has the opposite sign against the finite difference (7). For example, when f(x, z) = ∑ i zi, this estimator is equivalent to the optimal one. However, if we modify it to f̃(x, z) = ∑ i zi−sin (2π ∑ i zi), the straight-through estimator always gives a gradient opposite to the steepest direction of the expectation, which does not change as a result of the modification, i.e., Ef̃(x, z) = Ef(x, z)."
  }, {
    "heading": "7. Experiments",
    "text": "We conduct experiments to empirically verify Theorem 1 and to demonstrate a procedure to analyze the optimal de-\ngree of a given estimator covered by our framework. All the methods are implemented with Chainer (Tokui et al., 2015)."
  }, {
    "heading": "7.1. Experimental Settings",
    "text": "The task is variational learning of sigmoid belief networks (SBN) (Neal, 1992), which is a directed graphical model with layers of Bernoulli variables. Let x ∈ {0, 1}d be a binary vector of input data and Z` = (z`,1, . . . , z`,N`) ∈ {0, 1}N` be a binary vector of the latent variables at the `- th layer. Denote the input layer as Z0 = x for notational simplicity. Let L be the number of latent layers. The generative model is specified by a conditional of each layer pθ(Z`|Z`+1) and the prior of the deepest layer pθ(ZL). In our experiments, the prior of each variable zL,i ∈ ZL is independently parameterized by its logit. The conditional pθ(Z`|Z`+1) is modeled by an affine transformation of Z`+1 that outputs the logit of Z`. The parameters of the affine transformation are optimized through the learning. We use two models with L = 2 and L = 4, respectively. Each layer consists of N` = 200 Bernoulli variables.\nWe model the approximate posterior qφ(z|x) by a reversedirectional SBN. In this case, the prior q(x) is not modeled, and each conditional qφ(Z`+1|Z`) is specified by its logit as an affine transformation of Z`.\nBoth the generative parameter θ and the variational parameter φ are optimized simultaneously to maximize the following variational lower bound.\nlog pθ(x) = logEqφ(z|x) pθ(x, z)\nqφ(z|x)\n≥ Eqφ(z|x) log pθ(x, z)\nqφ(z|x) = L.\nThe second line follows Jensen’s inequality with the concavity of log. The gradient w.r.t. θ is estimated by a Monte Carlo simulation of ∇θL = Eqφ(z|x)∇θ log pθ(x, z). We use gradient estimators for approximating the gradient w.r.t. φ.\nThe plain likelihood-ratio estimator is denoted by LR, whereas the constant baseline using the moving average of f(x, z) = log pθ(x, z) − log qφ(z|x) and the inputdependent baseline of Mnih & Gregor (2014) are expressed by the postfixes +C and +IDB, respectively. We also run experiments for MuProp and the Local Expectation Gradient (LEG). Algorithm 1 is used to obtain the results for the optimal estimator.\nWe use MNIST (Lecun et al., 1998) and Omniglot (Lake et al., 2015) for our experiments. These are sets of 28x28 pixel gray-scale images of hand-written digits and handwritten characters from various languages. We binarize each pixel by sampling from a Bernoulli distribution with\nthe mean equal to the pixel intensity (Salakhutdinov & Murray, 2008). The binarization is done in an online manner, i.e., we sample binarized vectors at each iteration. For the MNIST dataset, we use the standard split of 60,000 training images and 10,000 test images. The training images are further split into 50,000 images and 10,000 images, the latter of which are used for validation. For the Omniglot dataset, we use the standard split of 24,345 training images and 8,070 test images used in the official implementation of Burda et al. (2015) 4. The training images are further split into 20,288 images and 4,057 images, the latter of which are used for validation.\nWe used RMSprop (Tieleman & Hinton, 2012) with a minibatch size of 100 to optimize the variational lower bound. We apply a weight decay of the coefficient 0.001 for all parameters. All the weights are initialized with the method of Glorot & Bengio (2010). The learning rate is chosen from {3 × 10−4, 10−3, 3 × 10−3}. We evaluate the model on the validation set during training, and choose the learning rate with which the best validation performance with earlystopping beats the others. After each evaluation, we also measure the variance of the gradient estimations of variational parameters for the training set with the same minibatch size.\nEach experiment is done on an Intel(R) Xeon(R) CPU E52623 v3 at 3.00 GHz and an NVIDIA GeForce Titan X. Thanks to the parallel computation using the GPU, the computational time of the RAM estimator is only two times larger than the plain likelihood-ratio estimator."
  }, {
    "heading": "7.2. Results",
    "text": "The results for the two-layer SBN and four-layer SBN are shown in Fig. 1 and Fig. 2, respectively. The results for these models have almost the same trends. As is predicted by Theorem 1, the optimal estimator gives the lower bound of the estimation variance. The plots imply that the modern baseline techniques effectively reduce the estimation variance, which is approaching the optimal value. However, the gap between these practical methods and the optimal one is not negligible, and there is still room for improvements. The local expectation gradient actually does not degenerate to the plain likelihood-ratio estimator, whereas the variance reduction effect is limited so that its variance stays at a similar level to that of the likelihood-ratio estimator with a constant baseline. The validation score almost agrees with the variance level, although there are some exceptions caused by the differences in selected learning rates.\nNote that we do not align the computational cost by sampling multiple values in the experiments because the purpose of these experiments is evaluating the optimal degree\n4https://github.com/yburda/iwae\nof each method. We can infer the performance with aligned computational budget by comparing the variance and the computational cost."
  }, {
    "heading": "8. Conclusion",
    "text": "We introduced a novel framework of gradient estimation for stochastic computations using reparameterization. The framework serves as a bridge between the likelihood-ratio method and the reparameterization trick. The optimal estimator is naturally derived under the framework. It provides the minimum variance attainable by the likelihood-ratio estimators with the general class of baselines, and therefore can be used to evaluate the optimal degree of each practical baseline technique. We actually evaluated the common baseline techniques against the optimal estimator for variational learning of sigmoid belief networks and showed that the modern techniques achieve a variance level close to the lower bound.\nComparison between continuous variable models and discrete variable models is needed for the further development of deep probabilistic modeling, which should consider the adequacy of the use of these variables in each task and the efficiency of gradient estimators available for these models. While this study does not provide a way to compare such models in general, it bridges the gradient estimators of them through the optimal case, and therefore provides some insights on their relationships. Observing the experimental results, the modern estimators for Bernoulli variables achieve variance close to the optimal one, and therefore we can expect that the modern estimators for Bernoulli variables are maturing and could be applied to much larger models capturing discrete phenomena."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "We thank members of Preferred Networks, especially Daisuke Okanohara, for the helpful discussions."
  }],
  "year": 2017,
  "references": [{
    "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
    "authors": ["Bengio", "Yoshua", "Léonard", "Nicholas", "Courville", "Aaron C"],
    "year": 2013
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"],
    "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),",
    "year": 2015
  }, {
    "title": "Uncertainty in Deep Learning",
    "authors": ["Gal", "Yarin"],
    "venue": "PhD thesis, Department of Engineering,",
    "year": 2017
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Glorot", "Xavier", "Bengio", "Yoshua"],
    "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10),",
    "year": 2010
  }, {
    "title": "Optimization of stochastic systems via simulation",
    "authors": ["P.W. Glynn"],
    "venue": "In Proceedings of the 21st Conference on Winter Simulation, pp",
    "year": 1989
  }, {
    "title": "Likelihood ratio gradient estimation for stochastic systems",
    "authors": ["Glynn", "Peter W"],
    "venue": "Communication of the ACM,",
    "year": 1990
  }, {
    "title": "Muprop: Unbiased backpropagation for stochastic neural networks",
    "authors": ["Gu", "Shixiang", "Levine", "Sergey", "Sutskever", "Ilya", "Mnih", "Andriy"],
    "venue": "In Proceedings of the 4th International Conference on Learning Representations (ICLR),",
    "year": 2016
  }, {
    "title": "Q-prop: Sample-efficient policy gradient with an off-policy critic",
    "authors": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"],
    "venue": "In NIPS 2016 Deep Reinforcement Learning Workshop,",
    "year": 2016
  }, {
    "title": "Statistical theory of extreme values and some practical applications",
    "authors": ["Gumbel", "Emil Julius"],
    "venue": "U. S. Govt. Print. Office,",
    "year": 1954
  }, {
    "title": "Neural networks for machine learning",
    "authors": ["Hinton", "Geoffrey"],
    "venue": "Coursera, video lectures,",
    "year": 2012
  }, {
    "title": "Categorical Reparameterization with Gumbel-Softmax",
    "authors": ["E. Jang", "S. Gu", "B. Poole"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "On a connection between importance sampling and the likelihood ratio policy gradient",
    "authors": ["Jie", "Tang", "Abbeel", "Pieter"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2010
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["Kingma", "Diederik P", "Welling", "Max"],
    "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
    "year": 2014
  }, {
    "title": "Human-level concept learning through probabilistic program induction",
    "authors": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"],
    "year": 2015
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["Lecun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick"],
    "venue": "In Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "An overview of derivative estimation",
    "authors": ["L’Ecuyer", "Pierre"],
    "venue": "In Proceedings of the 23rd Conference on Winter Simulation, pp",
    "year": 1991
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables. CoRR, abs/1611.00712, 2016",
    "authors": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"],
    "venue": "ICLR",
    "year": 2017
  }, {
    "title": "Neural variational inference and learning in belief networks",
    "authors": ["Mnih", "Andriy", "Gregor", "Karol"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
    "year": 2014
  }, {
    "title": "Connectionist learning of belief networks",
    "authors": ["Neal", "Radford M"],
    "venue": "Artificial Intelligence,",
    "year": 1992
  }, {
    "title": "Variational bayesian inference with stochastic search",
    "authors": ["Paisley", "John", "Blei", "David M", "Jordan", "Michael I"],
    "venue": "In Proceedings of the 29 th International Conference on Machine Learning (ICML),",
    "year": 2012
  }, {
    "title": "Techniques for learning binary stochastic feedforward neural networks",
    "authors": ["Raiko", "Tapani", "Berglund", "Mathias", "Alain", "Guillaume", "Dinh", "Laurent"],
    "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),",
    "year": 2015
  }, {
    "title": "Black box variational inference",
    "authors": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M"],
    "venue": "In Artificial Intelligence and Statistics (AISTATS),",
    "year": 2014
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
    "year": 2014
  }, {
    "title": "Overdispersed black-box variational inference",
    "authors": ["F.J.R. Ruiz", "M.K. Titsias", "D.M. Blei"],
    "venue": "In Uncertainty in Artificial Intelligence (UAI),",
    "year": 2016
  }, {
    "title": "The generalized reparameterization gradient",
    "authors": ["F.J.R. Ruiz", "M.K. Titsias", "D.M. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pp. 696–699",
    "authors": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"],
    "year": 1986
  }, {
    "title": "On the quantitative analysis of Deep Belief Networks",
    "authors": ["Salakhutdinov", "Ruslan", "Murray", "Iain"],
    "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning (ICML),",
    "year": 2008
  }, {
    "title": "Gradient estimation using stochastic computation graphs",
    "authors": ["Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter"],
    "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude",
    "authors": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"],
    "venue": "In CORSERA: Neural Networks for Machine Learning,",
    "year": 2012
  }, {
    "title": "Doubly stochastic variational bayes for non-conjugate inference",
    "authors": ["Titsias", "Michalis", "Lzaro-gredilla", "Miguel"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "The optimal reward baseline for gradient-based reinforcement learning",
    "authors": ["Weaver", "Lex", "Tao", "Nigel"],
    "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,",
    "year": 2001
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["Williams", "Ronald J"],
    "venue": "Machine Learning,",
    "year": 1992
  }],
  "id": "SP:cb9ddcb36a02ad8e70d32f099984bdd04fa21989",
  "authors": [{
    "name": "Seiya Tokui",
    "affiliations": []
  }, {
    "name": "Issei Sato",
    "affiliations": []
  }],
  "abstractText": "The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been proposed, although their degree of optimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases. The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it. It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables. It is derived from the exchange property of the differentiation and integration. To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient. We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.",
  "title": "Evaluating the Variance of Likelihood-Ratio Gradient Estimators"
}