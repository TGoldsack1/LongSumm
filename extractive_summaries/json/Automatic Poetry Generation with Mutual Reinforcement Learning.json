{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3143–3153 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n3143"
  }, {
    "heading": "1 Introduction",
    "text": "Language is one of the most important forms of human intelligence and poetry is a concise and graceful art of human language. Across different countries, nationalities and cultures, poetry is always popular, having far-reaching influence on the development of human society.\nIn this work, we concentrate on automatic poetry generation. Besides the long-term goal of building artificial intelligence, research on this task could become the auxiliary tool to better analyse poetry and understand the internal mechanism of human writing. In addition, these generation\n∗Corresponding author: sms@mail.tsinghua.edu.cn.\nsystems are also helpful for electronic entertainments and literary education.\nIn recent years, neural networks have proven to be powerful on poetry generation. Some neural models are proposed and achieve significant improvement. However, existing models are all based on maximum likelihood estimation (MLE), which brings two substantial problems. First, MLE-based models tend to remember common patterns of the poetry corpus (Zhang et al., 2017), such as high-frequency bigrams and stop words, losing some diversity and innovation for generated poetry. Moreover, based on word-level likelihood, two kinds of loss-evaluation mismatch (Wiseman and Rush, 2016) arise. One is evaluation granularity mismatch. When evaluating, human experts usually focus on sequence level (a poem line) or discourse level (a whole poem), while MLE optimizes word-level loss, which fails to hold a wider view of generated poems. The other is criteria mismatch. Instead of the likelihood, humans usually evaluate poetry in terms of some criteria. In this work we focus on the main four criteria\n(Manurung, 2003; Zhang and Lapata, 2014; Yan, 2016; Yi et al., 2017): fluency (are the lines fluent and well-formed?), coherence (is the poem as a whole coherent in meaning and theme?), meaningfulness (does the poem convey some certain messages?), overall quality (the reader’s general impression on the poem). This mismatch may make the model lean towards optimizing easier criteria, e.g., fluency, and ignore other ones.\nTo tackle these problems, we directly model the four aforementioned human evaluation criteria and use them as explicit rewards to guide gradient update by reinforcement learning. This is a criterion-driven training process, which motivates the model to generate poems with higher scores on these criteria. Besides, in writing theories, writing requires observing other learners (Bandura, 2001). It is also shown that writing is supported as an activity in which writers will learn from more experienced writers, such as other students, teachers, or authors (Prior, 2006). Therefore it is necessary to equip generators with the ability of mutual learning and communication. Inspired by this, we propose a novel mutual reinforcement learning schema (Figure 1), where we simultaneously train two learners (generators). During the training process, one learner will learn not only from the teacher (rewarder) but also from the other. We will show this mutual learning-teaching process leads to better results.\nIn summary, our contributions are as follows:\n• To the best of our knowledge, for the sake of tackling the loss-evaluation mismatch problem in poetry generation, we first utilize reinforcement learning to model and optimize human evaluation criteria.\n• We propose a novel mutual reinforcement learning schema to further improve performance, which is transparent to model architectures. One can apply it to any poetry generation model.\n• We experiment on Chinese quatrains. Both automatic and human evaluation results show that our method outperforms a strong basic method and the state-of-the-art model."
  }, {
    "heading": "2 Related Work",
    "text": "As a desirable entry point of automatic analysing, understanding and generating literary text, the research on poetry generation has lasted for decades.\nIn recent twenty years, the models can be categorized into two main paradigms.\nThe first one is based on statistical machine learning methods. Genetic algorithms (Manurung, 2003; Levy, 2001), Statistical Machine Translation (SMT) approaches (He et al., 2012; Jiang and Zhou, 2008) and Automatic Summarization approaches (Yan et al., 2013) are all adopted to generate poetry.\nMore recently, the second paradigm, neural network, has shown great advantages in this task, compared to statistical models. Recurrent Neural Network (RNN) is first used to generate Chinese quatrains by (Zhang and Lapata, 2014). To improve fluency and coherence, Zhang’s model needs to be interpolated with extra SMT features as shown in their paper. Focusing on coherence, some works (Yi et al., 2017; Wang et al., 2016a) use sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015) to generate poetry. Wang et al. (2016b) design a special Planning schema, which plans some sub-keywords in advance by a language model and then generates each line with the planned sub-keyword to improve coherence. Pursuing better overall quality, Yan (2016) proposes an iterative polishing schema to generate Chinese poetry, which refines the poem generated in one pass for several times. Aiming at enhancing meaningfulness, Ghazvininejad et al. (2016) extend user keywords to incorporate richer semantic information. Zhang et al. (2017) combine a neural memory, which saves hundreds of human-authored poems, with a sequence-to-sequence model to improve innovation of generated poems and achieve style transfer.\nThese neural structures have made some progress and improved different aspects of generated poetry. Nevertheless, as discussed in Section 1, the two essential problems, lack of diversity and loss-evaluation mismatch, are still challenging resulting from MLE. Compared to further adjusting model structures, we believe a better solution is to design more reasonable optimization objectives.\nDeep Reinforcement Learning (DRL) first shows its magic power in automatic game playing, such as Atari electronic games (Mnih et al., 2013) and the game of Go (Silver et al., 2016). Soon, DRL is used to playing text games (Narasimhan et al., 2015; He et al., 2016) and then applied to dialogue generation (Li et al., 2016b).\nFrom the perspective of poetry education, the\nteacher will judge student-created poems in terms of some specific criteria and guide the student to cover the shortage, which naturally accords with DRL process. Therefore we take advantage of DRL. We design four automatic rewarders for the criteria, which act as the teacher. Furthermore, we train two generators and make them learn from each other, which imitates the mutual learning of students, as a step towards multi-agent DRL in literary text generation."
  }, {
    "heading": "3 Methods",
    "text": ""
  }, {
    "heading": "3.1 Basic Generation Model",
    "text": "We apply our method to a basic poetry generation model, which is pre-trained with MLE. Therefore, we first formalize our task and introduce this model.\nThe inputs are user topics specified by K keywords,W = {wk}Kk=1. The output is a poem consisting of n lines, P = L1, L2, · · · , Ln. Since we take the line-by-line generation process, the task can be converted to the generation of an i-th line given previous i-1 lines L1:i−1 andW .\nWe use GRU-based (Cho et al., 2014) sequenceto-sequence model. −→ h t , ←− h t and st represent the forward encoder, backward encoder and decoder hidden states respectively. For each topic word wk = c1, c2, · · · , cTk , we feed characters into the encoder and get the keyword representation vk = [ −→ h Tk ; ←− h Tk ], where [;] means concatenation. Then we get the topic representation by1:\no = f( 1\nK K∑ t=1 vk), (1)\nwhere f defines a non-linear layer. Denote the generated i-th line in decoder, Y = (y1y2 . . . yTi). e(yt) is the word embedding of yt. The probability distribution of each yt to be generated in Li is calculated by:\nst = GRU(st−1, [e(yt−1); o; gi−1]), (2) P (yt|y1:t−1, L1:i−1,W) = softmax(Wst), (3)\nwhere W is the projection parameter. gi−1 is a global history vector, which records what has been generated so far and provides global-level information for the model. Once Li is generated, it is\n1For brevity, we omit biases in all equations.\nupdated by a convolutional layer:\nat = f([st; · · · ; st+d−1]), (4) gi = f(gi−1, ∑ t at), g0 = 0, (5)\nwhere 0 is a vector with all 0-s and d is convolution window size. Then the basic model is pretrained by minimizing standard MLE loss:\nLMLE(θ) = − M∑\nm=1\nlogP (Pm|Wm; θ), (6)\nwhere M is data size and θ is the parameter set to be trained.\nThis basic model is a modified version of (Yan, 2016). The main differences are that we replace vanilla RNN with GRU unit, use convolution to calculate the line representation rather than directly use the last decoder hidden state, and we remove the polishing schema to better obverse the influence of DRL itself. We select this model as our basic framework since it achieves satisfactory performance and the author has done thorough comparisons with other models, such as (Yan et al., 2013) and (Zhang and Lapata, 2014)."
  }, {
    "heading": "3.2 Single-Learner Reinforcement Learning",
    "text": "Before presenting the single-learner version of our method (abbreviated as SRL), we first design corresponding automatic rewarders for the four human evaluation criteria.\nFluency Rewarder. We use a neural language model to measure fluency. Given a poem line Li, higher probability Plm(Li) indicates the line is more likely to exist in the corpus and thus may be more fluent and well-formed. However, it’s inadvisable to directly use Plm(Li) as the reward, since over high probability may damage diversity and innovation. We expect moderate probabilities which fall into a reasonable range, neither too high nor too low. Therefore, we define the fluency reward of a poem P as:\nr(Li) = max(|Plm(Li)− µ| − δ1 ∗ σ, 0), (7)\nR1(P) = 1\nn n∑ i=1 exp(−r(Li)), (8)\nwhere µ and σ are the mean value and standard deviation of Plm calculated over all training sets. δ1 is a hyper-parameter to control the range.\nCoherence Rewarder. For poetry, good coherence means each line Li should be coherent with\nprevious lines in a poem. We use Mutual Information (MI) to measure the coherence of Li and L1:i−1. As shown in (Li et al., 2016a), MI of two sentences, S1 and S2, can be calculated by:\nMI(S1, S2) = logP (S2|S1)− λlogP (S2), (9)\nwhere λ is used to regulate the weight of generic sentences. Based on this, we calculate the coherence reward as:\nMI(L1:i−1, Li) = logPseq2seq(Li|L1:i−1) − λlogPlm(Li), (10)\nR2(P) = 1\nn− 1 n∑ i=2 MI(L1:i−1, Li),\n(11)\nwhere Pseq2seq is a GRU-based sequence-tosequence model, which takes the concatenation of previous i-1 lines as input, and predicts Li. A better choice is to use a dynamic λ instead of a static one. Here we directly set λ = exp(−r(Li)) + 1, which gives smaller weights to lines with extreme language model probabilities.\nMeaningfulness Rewarder. In dialogue generation task, neural models are prone to generate generic sentences such as “I don’t know” (Li et al., 2016a; Serban et al., 2016). We observed similar issues in poetry generation. The basic model tends to generate some common and meaningless words, such as bu zhi (don’t know), he chu (where), and wu ren (no one). It’s quite intractable to quantify the meaningfulness of a whole poem, but we find that TF-IDF values of human-authored poems are significantly higher than values of generated ones (Figure 2). Consequently, we utilize TF-IDF to motivate the model to generate more meaningful words. This is a simple and rough attempt, but it makes generated poems more “meaningful” from the readers perspective.\nDirect use of TF-IDF leads to serious out-ofvocabulary (OOV) problem and high variance, because we need to sample poems during the training process of DRL, which causes many OOV words. Therefore we use another neural network to smooth TF-IDF values. In detail, we have:\nR3(P) = 1\nn n∑ i=1 F (Li), (12)\nwhere F (Li) is a neural network which takes a line as input and predicts its estimated TF-IDF\nvalue. For each line in training sets, we calculate standard TF-IDF values of all words and use the average as the line TF-IDF value. Then we use them to train F (Li) with Huber loss.\nOverall Quality Rewarder. The three kinds of rewards above are all based on line-level. In fact, human experts will also focus on discourselevel to judge the overall quality of a poem, ignoring some minor defects. We train a neural classifier to classify a given poem (in terms of the concatenation of all lines) into three classes: computer-generated poetry (class 1), ordinary human-authored poetry (class 2) and masterpiece (class 3). Then we get the reward by:\nR4(P) = 3∑\nk=1\nPcl(k|P) ∗ k. (13)\nThis classifier should be as reliable as possible. Due to the limited amount of masterpieces, normal classifiers don’t work well. Therefore we use an adversarial training based classifier (Miyato et al., 2017), which achieves F-1 0.96, 0.73, 0.76 for the three classes respectively on the validation set.\nBased on these rewarders, the total reward is:\nR(P) = 4∑\nj=1\nαj ∗ R̃j(P), (14)\nwhere αj is the weight and the symbol ˜ means the four rewards are re-scaled to the same magnitude. As (Gulcehre et al., 2018), we reduce the variance by:\nR ′ (P) = R(P)− bu√\nσ2u + ϵ −B(P), (15)\nwhere bu and σu are running average and standard deviation of R respectively. B(P) is a neural network trained with Huber loss, which takes a poem as input and predicts its estimated reward.\nDRL Process. For brevity, we use Pg(·|W; θ) to represent a basic generator and use REINFORCE algorithm (Williams, 1992) to optimize the model, which minimizes:\nLDRL(θ) = − M∑\nm=1\nEP∼Pg(·|Wm;θ)(R ′ (P)).\n(16)\nTraining with solely Eq.(16) is unstable. Lacking of original MLE supervisory signals, the\nAlgorithm 1 Global Mutual Learning 1: Set history reward lists V1 and V2 empty; 2: for number of iterations do 3: Sample batch (Wm,Pmg ) from training\ndata set; 4: for eachWm do 5: Sample Pm1 ∼ Pg(·|W\nm; θ1); 6: Sample Pm2 ∼ Pg(·|W\nm; θ2); 7: Add R(Pm1 ) to V1, R(Pm2 ) to V2 8: end for 9: Set LM (θ1)=L(θ1), LM (θ2)=L(θ2);\n10: if mean value V2 > V1 ∗ (1 + δ3) then 11: LM (θ1)=L(θ1) +KL(Pg(θ2)||Pg(θ1)); 12: else if V1 > V2 ∗ (1 + δ3) then 13: LM (θ2)=L(θ2) +KL(Pg(θ1)||Pg(θ2)); 14: end if 15: Update θ1 with LM (θ1), θ2 with LM (θ2); 16: end for\nmodel is easy to get lost and totally ignore the corresponding topics specified by W , leading to explosive increase of MLE loss. We use two steps to alleviate this issue. The first one is the Teacher Forcing (Li et al., 2017). For each W , we estimate E(R′(P)) by ns sampled poems, as well as the ground-truth Pg whose reward is set to max(R ′ (Pg), 0). The second step is to combine MLE loss and DRL loss as:\nL(θ) = (1− β) ∗ LMLE(θ) + β ∗ L̃DRL(θ), (17)\nwhere ˜ means the DRL loss is re-scaled to the same magnitude with MLE loss. Ultimately, we use Eq.(17) to fine-tune the basic model."
  }, {
    "heading": "3.3 Mutual Reinforcement Learning",
    "text": "As discussed in Section 1 & 2, to further improve the performance, we mimic the mutual writing learning activity by simultaneously training two generators defined as Pg(θ1) and Pg(θ2). The two learners (generators) learns not only from the teacher (rewarders) but also from each other.\nFrom the perspective of machine learning, one generator may not explore the policy space sufficiently and thus is easy to get stuck in the local minima. Two generators can explore along different directions. Once one generator finds a better path (higher reward), it can communicate with the other and lead it towards this path. This process could also be considered as the ensemble of different generators during the training phase.\nWe implement the Mutual Reinforcement Learning (abbreviated as MRL) by two methods.\nLocal MRL. The first one is a simple instancebased method. For the same input, suppose P1, P2 are generated by Pg(θ1) and Pg(θ2) respectively. If R(P1) > R(P2)∗(1+δ2) and R̃j(P1) > R̃j(P2) for all j, then Pg(θ2) usesP1 instead ofP2 to update itself in Eq.(16) and vice versa. That is, if a learner creates a significantly better poem, then the other learner will learn it. This process gives a generator more high-reward instances and allows it to explore larger space along a more proper direction so as to escape from the local minima.\nGlobal MRL. During the training process, we need to sample poems from the generator, and hence local MRL may cause high variance. Instead of an instance, mutual learning can also be applied on the distribution level. We can pull the distribution of a generator towards that of the other by minimizing KL divergence of them. We detail this method in algorithm 1. The inner thought is that if learner 1 is generally better than learner 2, that is, during the creating history, learner 1 achieves higher average rewards, then learner 2 should directly learn from learner 1, rather than learn the poem itself. This process allows the generator to learn from long-period history and focus on a higher level.\nIn practice, we combine these two methods by simultaneously communicating high-reward samples and using KL loss, which leads to the best testing rewards (Table 1)."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Data and Setups",
    "text": "Our corpus consists of three sets: 117,392 Chinese quatrains (CQ), 10,000 Chinese regulated verses (CRV) and 10,000 Chinese iambics (CI). As men-\ntioned, we experiment on the generation of quatrain which is the most popular genre of Chinese poetry and accounts for the largest part of our corpus. From the three sets, we randomly select 10% for validation. From CQ, we select another 10% for testing. The rest are used for training.\nFor our model and baseline models, we run TextRank (Mihalcea and Tarau, 2004) on all training sets and then extract four keywords from each quatrain. Then we build four < keyword(s), poem > pairs for each quatrain using 1 to 4 keywords respectively, so as to enable the model to cope with different numbers of keywords.\nFor the models and rewarders, the sizes of word embedding and hidden state are 256 and 512 respectively. History vector size is 512 and convolution window size d = 3. The word embedding is initialized with pre-trained word2vec vectors. We use tanh as the activation function. For other more configurations of the basic model, we directly follow (Yan, 2016).\nPlm and Pseq2seq are trained with the three sets. We train F (Li) and B(P) with the CQ, CRV and 120,000 generated poems. There are 9,465 masterpieces in CQ. We use these poems, together with 10,000 generated poems and 10,000 ordinary human-authored poems to train the classifier Pcl. For training rewarders, half of the generated poems are sampled and the other half are generated with beam search (beam size 20). For testing, all models generate poems with beam search.\nWe use Adam (Kingma and Ba, 2015) with shuffled mini-batches. The batch size is 64 for MLE and 32 for DRL. For DRL, we random select batches to fine-tune the basic model. We set δ1 = 0.5, δ2 = 0.1, δ3 = 0.001, α1 = 0.25,\nα2 = 0.31, α3 = 0.14, α4 = 0.30, ns = 4, and β = 0.7.\nA key point for MRL is to give the two pretrained generators some diversity, which can be achieved by using different model structures or parameters. Here we simply initialize the generators differently and train one of them for more epoches."
  }, {
    "heading": "4.2 Models for Comparisons",
    "text": "We compare MRL2 (our model, with both local and global mutual learning), GT (ground-truth, namely human-authored poems), Base (the basic model described in Section 3.1) and Mem (Zhang et al., 2017). The Mem model is the current stateof-the-art model for Chinese quatrain generation, which also achieves the best innovation so far."
  }, {
    "heading": "4.3 Automatic Evaluation",
    "text": "Some previous models (He et al., 2012; Zhang and Lapata, 2014; Yan, 2016) adopt BLEU and perplexity as automatic evaluation metrics. Nevertheless, as discussed in Section 1, word-level likelihood or n-gram matching will greatly diverge from human evaluation manner. Therefore we dispense with them and automatically evaluate generated poems as follows:\nRewarder Scores. The four rewarder scores are objective and model-irrelevant metrics which approximate corresponding human criteria. They\n2Due to length limit, we only display the better of the two simultaneously trained generators. Our source code will be available at https://github.com/XiaoyuanYi/MRLPoetry.\ncan reflect poetry quality to some extent. As shown in Table 1, on each criterion, GT gets much higher rewards than all these models. Compared to Base, MRL gets closer to GT and achieves 31% improvement on the weighted average reward. Mem outperforms Base on the criteria except for meaningfulness (R̃3). This is mainly because Mem generates more distinct words (Table 2), but these words tend to concentrate on the highfrequency area, resulting in unsatisfactory TF-IDF reward. We also test different strategies of MRL. With naive single-learner RL, the improvement is limited, only 14%. With mutual RL, the improvement increases to 27%. Combining local MRL and global MRL leads to another 4% improvement. The results demonstrate our explicit optimization (RL) is more effective than the implicit ones and MRL gets higher scores than SRL.\nDiversity and Innovation. Poetry is a kind of literature text with high requirements on diversity and innovation. Users don’t expect the machine to always generate monotonous poems. We evaluate innovation of generated poems by distinct bigram ratio as (Li et al., 2016b). More novel generated bigrams can somewhat reflect higher innova-\ntion. The diversity is measured by bigram-based average Jaccard similarity of each two generated poems. Intuitively, a basic requirement for innovation is that, with different inputs, the generated poems should be different from each other.\nAs shown in Table 2, Mem gets the highest bigram ratio, close to GT, benefiting from its specially designed structure for innovation. Our MRL achieves 43% improvement over Base, comparable to Mem. We will show later this satisfactory performance may lie in the incorporation of TFIDF (Figure 2). On Jaccard, MRL gets the best result due to the utilization of MI. MI brings richer context-related information which can enhance diversity as shown in (Li et al., 2016a). In fact, human-authored poems often contain strong diversity of personal emotion and experience. Therefore, despite prominent improvement, there is still a large gap between MRL and GT.\nTF-IDF Distribution. As mentioned, the basic model tends to generate common and meaningless words. Consequently, we use TF-IDF as one of the rewards. Figure 2 shows the TF-IDF distributions. As we can see, Base generates poems with lower TF-IDF compared to GT, while MRL pulls the distribution towards that of GT, making the model generate more meaningful words and hence benefiting innovation and diversity.\nTopic Distribution. We run LDA (Blei et al., 2003) with 20 topics on the whole corpus and then inference the topic of each generated poem. Figure 3 gives the topic distributions. Poems generated by Base center in a few topics, which again demonstrates the claim: MLE-based models tend to remember the common patterns. In contrast, humanauthored poems spread on more topics. After finetuning by our MRL method, the topic distribution shows better diversity and balance."
  }, {
    "heading": "4.4 Human Evaluation",
    "text": "From the testing set, we randomly select 80 sets of keywords to generate poems with these mod-\nels. For GT, we select poems containing the given words. Therefore, we obtain 320 quatrains (80*4). We invite 12 experts on Chinese poetry to evaluate these poems in terms of the four criteria: fluency, coherence, meaningfulness and overall quality and each needs to be scored in a 5-point scale ranging from 1 to 5. Since it’s tiring to evaluate all poems for one person, we randomly divide the 12 experts into three groups. Each group evaluates the randomly shuffled 320 poems (80 for each expert). Then for each model, each poem, we get 3 scores on each criterion and we use the average to alleviate individual preference.\nTable 3 gives human evaluation results. MRL achieves better results than the other two models. Since fluency is quite easy to be optimized, our method gets close to human-authored poems on Fluency. The biggest gap between MRL and GT lies on Meaning. It’s a complex criterion involving the use of words, topic, emotion expression and so on. The utilization of TF-IDF does ameliorate the use of words on diversity and innovation, hence improving Meaningfulness to some extent, but there are still lots to do."
  }, {
    "heading": "4.5 Further Analyses and Discussions",
    "text": "In this section we give more discussions. Learning Curve. We show the learning curves of SRL and MRL in Figure 4. As we can see, for SRL, the adequately pre-trained generator 2 al-\nways gets higher rewards than the other one during the DRL training process. With the increase of training steps, the gap between their rewards gets larger. After several hundred steps, rewards of the two generators converge.\nFor MRL, generator 2 gets higher rewards at the beginning, but it is exceeded by generator 1 since generator 1 learns from it and keeps chasing. Finally, the two generators converge to higher rewards compared to SRL.\nCase Study. We show some generated poems in Figure 5. The Base model generates two words, ‘sunset’ and ‘moon’ in poem (1), which appear together and thus cause the conflict of time. The word ‘fishing jetty’ is confusing without any necessary explanation in the context. In contrast, poem (2) describes a clearer scene and expresses some emotion: a lonely man takes a boat from morning till night and then falls asleep solitarily.\nIn poem (3), Mem generates some meaningful words, such as ‘phoenix tree’, ‘wild goose’ and ‘friend’. However, there isn’t any clue to link them together, resulting in poor coherence. On the contrary, things in poem (4) are tightly connected. For example, ‘moonlight’ is related to ‘night’; ‘rain’, ‘frost’ and ‘dew’ are connected with ‘cold’.\nPoem (5) expresses almost nothing. The first two lines seem to talk about the change of time. But the last two lines are almost unrelated to ‘time change’. Poem (6) talks about an old poet, with the description of cheap wine, poem and dream, expressing something about life and time. However, the human-authored poem (7) does much better. It seems to describe a mosquito, but in fact, it’s a metaphor of the author himself."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "In this work, we address two substantial problems in automatic poetry generation: lack of diversity, and loss-evaluation mismatch, which are caused by MLE-based neural models. To this end, we directly model the four widely used human evaluation criteria and design corresponding automatic rewarders. We use these explicit rewards to guide gradient update by reinforcement learning. Furthermore, inspired by writing theories, we propose a novel mutual learning schema to further improve the performance. Mimicking the poetry learning activity, we simultaneously train two generators, which will not only be taught by the rewarders but also learn from each other. Experi-\nmental results show our method achieves significant improvement both on automatic rewards and human evaluation scores, outperforming the current state-of-the-art model3.\nThere are still lots to do. Can we better model the meaningfulness of a whole poem? Can we quantify some other intractable criteria, e.g, poeticness? Besides, we only tried two learners in this work. Would the collaboration of more learners lead to better results? How to design the methods of communication among many generators? We will explore these questions in the future."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Cheng Yang, Jiannan Liang, Zhipeng Guo, Huimin Chen and anonymous reviewers for their insightful comments. This research is funded by the National 973 project (No. 2014CB340501). It is also partially supported by the NExT++ project, the National Research Foundation, Prime Ministers Office, Singapore under its IRC@Singapore Funding Initiative.\n3Our method will be incorporated into Jiuge, the THUNLP online poetry generation system, https:// jiuge.thunlp.cn."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 2015 International Conference on Learning Representations, San Diego, CA.",
    "year": 2015
  }, {
    "title": "Social cognitive theory: An agentic perspective",
    "authors": ["Albert Bandura."],
    "venue": "Annual Review of Psychology, 52(1):1–26.",
    "year": 2001
  }, {
    "title": "Latent dirichlet allocation",
    "authors": ["David Blei", "Andrew Ng", "Michael Jordan."],
    "venue": "Machine Learning Research, (3):993–1022.",
    "year": 2003
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Generating topical poetry",
    "authors": ["Marjan Ghazvininejad", "Xing Shi", "Yejin Choi", "Kevin Knight."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1183–1191, Austin, Texas. Association for Compu-",
    "year": 2016
  }, {
    "title": "Dynamic neural turing machine with continuous and discrete addressing schemes",
    "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Neural Computation, 30(4):857–884.",
    "year": 2018
  }, {
    "title": "Deep reinforcement learning with a natural language action space",
    "authors": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
    "year": 2016
  }, {
    "title": "Generating chinese classical poems with statistical machine translation models",
    "authors": ["Jing He", "Ming Zhou", "Long Jiang."],
    "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence, pages 1650– 1656, Toronto, Canada.",
    "year": 2012
  }, {
    "title": "Generating chinese couplets using a statistical mt approach",
    "authors": ["Long Jiang", "Ming Zhou."],
    "venue": "Proceedings of the 22nd International Conference on Computational Linguistics, pages 377–384, Manchester, UK.",
    "year": 2008
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Lei Ba."],
    "venue": "Proceedings of the 2015 International Conference on Learning Representations, San Diego, CA.",
    "year": 2015
  }, {
    "title": "A computational model of poetic creativity with neural network as measure of adaptive fitness",
    "authors": ["Robert P. Levy."],
    "venue": "Proceedings of the ICCBR-01 Workshop on Creative Systems.",
    "year": 2001
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Deep reinforcement learning for dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192–",
    "year": 2016
  }, {
    "title": "Adversarial learning for neural dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Sébastien Jean", "Alan Ritter", "Dan Jurafsky."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157–2169,",
    "year": 2017
  }, {
    "title": "An Evolutionary Algorithm Approach to Poetry Generation",
    "authors": ["Hisar Maruli Manurung."],
    "venue": "Ph.D. thesis, University of Edinburgh.",
    "year": 2003
  }, {
    "title": "Textrank: Bringing order into texts",
    "authors": ["Rada Mihalcea", "Paul Tarau."],
    "venue": "Proceedings of EMNLP 2004, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
    "year": 2004
  }, {
    "title": "Adversarial training methods for semisupervised text classification",
    "authors": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow."],
    "venue": "Proceedings of the 2017 International Conference on Learning Representations, Toulon, France.",
    "year": 2017
  }, {
    "title": "Playing atari with deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."],
    "venue": "Computer Science.",
    "year": 2013
  }, {
    "title": "Language understanding for textbased games using deep reinforcement learning",
    "authors": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."],
    "venue": "Computer Science, 40(4):1–5.",
    "year": 2015
  }, {
    "title": "Handbook of Writing Research, chapter A Sociocultural Theory of Writing",
    "authors": ["Paul Prior."],
    "venue": "Guilford Press.",
    "year": 2006
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-",
    "year": 2016
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"],
    "year": 2016
  }, {
    "title": "Chinese song iambics generation with neural attention-based model",
    "authors": ["Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing."],
    "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pages 2943–2949, New York,",
    "year": 2016
  }, {
    "title": "Chinese poetry generation with planning based neural network",
    "authors": ["Zhe Wang", "Wei He", "Hua Wu nad Haiyang Wu", "Wei Li", "Haifeng Wang", "Enhong Chen."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational",
    "year": 2016
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Machine Learning, 8(3-4):229–256.",
    "year": 1992
  }, {
    "title": "Sequence-to-sequence learning as beam-search optimization",
    "authors": ["Sam Wiseman", "Alexander M. Rush."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306, Austin, Texas. Association",
    "year": 2016
  }, {
    "title": "I, poet:automatic poetry composition through recurrent neural networks with iterative polishing schema",
    "authors": ["Rui Yan."],
    "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pages 2238–2244, New York, USA.",
    "year": 2016
  }, {
    "title": "I, poet:automatic chinese poetry composition through a generative summarization framework under constrained optimization",
    "authors": ["Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li."],
    "venue": "Proceedings of the 23rd",
    "year": 2013
  }, {
    "title": "Generating chinese classical poems with rnn encoderdecoder",
    "authors": ["Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun."],
    "venue": "Proceedings of the Sixteenth Chinese Computational Linguistics, pages 211–223, Nanjing, China.",
    "year": 2017
  }, {
    "title": "Flexible and creative chinese poetry generation using neural memory",
    "authors": ["Jiyuan Zhang", "Yang Feng", "Dong Wang", "Yang Wang", "Andrew Abel", "Shiyue Zhang", "Andi Zhang."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Compu-",
    "year": 2017
  }, {
    "title": "Chinese poetry generation with recurrent neural networks",
    "authors": ["Xingxing Zhang", "Mirella Lapata."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680, Doha, Qatar. Association",
    "year": 2014
  }],
  "id": "SP:383d0ba13705ead9bcf300e566523c52697e83b7",
  "authors": [{
    "name": "Xiaoyuan Yi",
    "affiliations": []
  }, {
    "name": "Maosong Sun",
    "affiliations": []
  }, {
    "name": "Ruoyu Li",
    "affiliations": []
  }, {
    "name": "Wenhao Li",
    "affiliations": []
  }],
  "abstractText": "Poetry is one of the most beautiful forms of human language art. As a crucial step towards computer creativity, automatic poetry generation has drawn researchers’ attention for decades. In recent years, some neural models have made remarkable progress in this task. However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in lossevaluation mismatch. Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood. To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema. We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance. We experiment on Chinese poetry. Based on a strong basic model, our method achieves better results and outperforms the current state-of-theart method.",
  "title": "Automatic Poetry Generation with Mutual Reinforcement Learning"
}