{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2044–2048, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014).\nArguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to chair, that does not mean we should translate table into French as chaise.\nThis distinction between “genuine” similarity and associative similarity (i.e., relatedness) is well-known in cognitive science (Tversky, 1977). In NLP, however, semantic spaces are generally evaluated on how well they capture both similarity and relatedness, even though, for many word combinations (such as car and petrol), these two objectives are mutually incompatible (Hill et al., 2014b). In part, this oversight stems from the distributional hypothesis itself: car and petrol do not have the same, or even very similar, meanings, but these two words may well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly.\nIn this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for relatedness by learning from both a corpus and a collection of psychological association norms. We also compare the recentlyintroduced technique of graph-based retrofitting (Faruqui et al., 2015) with a skip-gram retrofitting and a skip-gram joint-learning approach. All three methods yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness significantly better than unspecialized spaces, in one case yielding state-of-the-art results for word similarity. More importantly, we show clear improvements in downstream tasks and applications: specialized similarity spaces improve synonym detection, while association spaces work better than both general-purpose and similarityspecialized spaces for document classification."
  }, {
    "heading": "2 Approach",
    "text": "The underlying assumption of our approach is that, during training, word embeddings can be “nudged” in a particular direction by including information from an additional semantic data\n2044\nsource. For directing embeddings towards genuine similarity, we use the MyThes thesaurus developed by the OpenOffice.org project1. It contains synonyms for almost 80,000 words in English. For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004). This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2."
  }, {
    "heading": "2.1 Evaluations (Intrinsic and Extrinsic)",
    "text": "For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles.3"
  }, {
    "heading": "2.2 Joint Learning",
    "text": "The standard skip-gram training objective for a sequence of training wordsw1, w2, ..., wT and a context size c is the log-likelihood criterion:\n1 T T∑ t=1 Jθ(wt) = 1 T T∑ t=1 ∑ −c≤j≤c log p(wt+j |wt)\nwhere p(wt+j |wt) is obtained via the softmax:\np(wt+j |wt) = exp u>wt+j vwt∑\nw′ exp u> w′vwt\nwhere uw and vw are the context and target vector representations for word w, respectively, and w′ ranges over the full vocabulary (Mikolov et al.,\n1https://www.openoffice.org/lingucomponent/thesaurus.html 2The script for obtaining this corpus is available from http://word2vec.googlecode.com/svn/trunk/demo-train-big-model-v1.sh 3We exclude articles with multiple topic labels in order to avoid multi-class document classification. The dataset contains a total of 78 topic labels and 33,226 news articles.\n2013a). For our joint learning approach, we supplement the skip-gram objective with additional contexts (synonyms or free-associates) from an external data source. In the sampling condition, for target word wt, we modify the objective to include an additional context wa sampled uniformly from the set of additional contexts Awt :\n1 T T∑ t=1 ( Jθ(wt) + [wa ∼ UAwt ] log p(wa|wt) ) In the all condition, all additional contexts for a\ntarget word are added at each occurrence:\n1 T T∑ t=1 Jθ(wt) + ∑ wa∈Awt log p(wa|wt) \nThe set of additional contexts Awt contains the relevant contexts for a word wt; e.g., for the word dog, Adog for the thesaurus is the set of all synonyms of dog in the thesaurus."
  }, {
    "heading": "2.3 Retrofitting",
    "text": "Faruqui et al. (2015) introduced retrofitting as a post-hoc graph-based learning objective that improves learned word embeddings. We experiment with their method, calling it graph-based retrofitting. In addition, we introduce a similar approach that instead uses the same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus:\n1 T T∑ t=1 ∑ wa∈Awt log p(wa|wt)\nWe call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014)"
  }, {
    "heading": "3 Results for Intrinsic Evaluation",
    "text": "We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the\ntraining algorithm was run for a single iteration (results from more iterations are presented later).\nAs shown in Table 1, embeddings that were specialized for similarity using a thesaurus perform better on SimLex-999, and those specialized for relatedness using association data perform better on MEN. Fitting, or learning only from the additional semantic resource without access to raw text, does not perform well. Skip-gram retrofitting with the thesaurus performs best on SimLex-999; joint learning with sampling from the USF norms performs best on MEN, although the two retrofitting approaches are close. There is an interesting difference between the two joint learning approaches: while sampling a single free associate as additional context works best for relatedness, presenting all additional contexts (synonyms) works best for similarity. In both cases, skip-gram retrofitting matches or outperforms graph-based retrofitting.\nMore training iterations All the results above were obtained using a single training iteration. When retrofitting, however, it is easy to learn from multiple iterations of the thesaurus or the USF norms. The results are shown in Figure 1, where the dashed lines are the joint learning and standard skip-gram results for comparison with retrofitting scores. As would be expected, too many iterations leads to overfitting on the semantic resource, with performance eventually decreasing after the initial increase. The results show that retrofitting is particularly useful for similarity, as indicated by the large increase in performance on SimLex-999. The highest performance obtained, at 5 iterations, is a Spearman ρs correlation of 0.53, which, as far\nas we know, matches the current state-of-the-art.4\nFor relatedness-specific embeddings, the effect is less clear: joint learning performs comparatively much better. Retrofitting does outperform it, at around 2-10 iterations on the USF norms, but the improvement is marginal. The highest retrofitting score is 0.74; the highest joint learning score is 0.72. Both are highly competitive results on MEN, and outperform e.g. GloVe at 0.71 (Pennington et al., 2014). Joint learning with a thesaurus, however, leads to poor performance on MEN, as expected: the embeddings get dragged away from relatedness and towards similarity."
  }, {
    "heading": "3.1 Curriculum learning?",
    "text": "The fact that joint learning works better when supplementing raw text input with free associates, but skip-gram retrofitting works better with additional thesaurus information, could be due to curriculum learning effects (Bengio et al., 2009). Unlike the USF norms, many of the words from the thesaurus are unusual and have low frequency. This suggests that the thesaurus is more ‘advanced’ (from the perspective of the learning model) than the USF norms as an information source. Its information may be detrimental to model optimization when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus."
  }, {
    "heading": "4 Downstream Tasks and Applications",
    "text": ""
  }, {
    "heading": "4.1 TOEFL Synonym Task",
    "text": "Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option.\n4Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings.\nAs Table 2 shows, similarity-specialized embeddings perform much better than standard embeddings and relatedness-specialized embeddings. Retrofitting outperforms joint learning, and skipgram retrofitting matches or outperforms graphbased retrofitting."
  }, {
    "heading": "4.2 Document Classification",
    "text": "To investigate how well the various semantic spaces perform on document classification, we first construct document-level representations by summing the vector representations for all words in a given document. After setting aside a small development set for tuning the hyperparameters of the supervised algorithm, we train a support vector machine (SVM) classifier with a linear kernel and evaluate document topic classification accuracy using ten-fold cross-validation.\nThe results are reported in the rightmost column of Table 2. Relatedness-specialized embed-\ndings perform better on document topic classification than similarity embeddings, except with graph-based retrofitting, which in fact performs below the standard skip-gram model. The jointlearning model with all relevant free association norms presented as context for each target word is the best performing model. The differences in the table appear small, but the dataset contains more than 10,000 documents, so every percentage point is worth more than 100 documents. Joint learning while presenting all relevant association norms for each target word performs best on this task."
  }, {
    "heading": "5 Conclusion",
    "text": "We have demonstrated the advantage of specializing embeddings for the tasks of genuine similarity and relatedness. In doing so, we compared two retrofitting methods and a joint learning approach. Specialized embeddings outperform standard embeddings by a large margin on instrinsic similarity and relatedness evaluations. We showed that the difference in how embeddings are specialized carries to downstream NLP tasks, demonstrating that similarity embeddings are better at the TOEFL synonym selection task and relatedness embeddings at a document topic classification task. Lastly, we varied the number of iterations that we use for retrofitting, showing that performance could be improved even further by going over several iterations of the semantic resource."
  }, {
    "heading": "Acknowledgments",
    "text": "DK is supported by EPSRC grant EP/I037512/1. FH is supported by St Johns College, Cambridge. SC is supported by ERC Starting Grant DisCoTex\n(306920) and EPSRC grant EP/I037512/1. We thank Yoshua Bengio, Kyunghyun Cho and Ivan Vulić for useful discussions and the anonymous reviewers for their helpful comments."
  }],
  "year": 2015,
  "references": [{
    "title": "A study on similarity and relatedness using distributional and WordNet-based approaches",
    "authors": ["Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."],
    "venue": "NAACL, pages 19–27.",
    "year": 2009
  }, {
    "title": "Don’t count, predict! A systematic comparison of context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "context-predicting semantic vectors. In ACL, pages 238–247.",
    "year": 2014
  }, {
    "title": "Curriculum learning",
    "authors": ["Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM.",
    "year": 2009
  }, {
    "title": "Multimodal distributional semantics",
    "authors": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."],
    "venue": "Journal of Artifical Intelligence Research, 49:1–47.",
    "year": 2014
  }, {
    "title": "Vector Space Models of Lexical Meaning",
    "authors": ["Stephen Clark."],
    "venue": "Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell, Oxford.",
    "year": 2015
  }, {
    "title": "Retrofitting word vectors to semantic lexicons",
    "authors": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."],
    "venue": "Proceedings of NAACL.",
    "year": 2015
  }, {
    "title": "New experiments in distributional representations of synonymy",
    "authors": ["Dayne Freitag", "Matthias Blume", "John Byrnes", "Edmond Chow", "Sadik Kapadia", "Richard Rohwer", "Zhiqiang Wang."],
    "venue": "Proceedings of the Ninth Conference on Computational Natural",
    "year": 2005
  }, {
    "title": "Revisiting embedding features for simple semi-supervised learning",
    "authors": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120.",
    "year": 2014
  }, {
    "title": "Distributional Structure",
    "authors": ["Zelig Harris."],
    "venue": "Word, 10(23):146—162.",
    "year": 1954
  }, {
    "title": "Embedding word similarity with neural machine translation",
    "authors": ["Felix Hill", "Kyunghyun Cho", "Sébastien Jean", "Coline Devin", "Yoshua Bengio."],
    "venue": "CoRR, abs/1412.6448.",
    "year": 2014
  }, {
    "title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation",
    "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."],
    "venue": "CoRR, abs/1408.3456.",
    "year": 2014
  }, {
    "title": "Automatic thesaurus generation through multiple filtering",
    "authors": ["Kyo Kageura", "Keita Tsuji", "Akiko N Aizawa."],
    "venue": "Proceedings of the 18th conference on Computational linguistics-Volume 1, pages 397– 403.",
    "year": 2000
  }, {
    "title": "A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
    "authors": ["Thomas K Landauer", "Susan T Dumais."],
    "venue": "Psychological review, 104(2):211.",
    "year": 1997
  }, {
    "title": "RCV1: A new benchmark collection for text categorization research",
    "authors": ["David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li."],
    "venue": "The Journal of Machine Learning Research, 5:361–397.",
    "year": 2004
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of ICLR, Scottsdale, Arizona, USA.",
    "year": 2013
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "Proceedings of ICLR, Scottsdale, Arizona, USA.",
    "year": 2013
  }, {
    "title": "The University of South Florida free association, rhyme, and word fragment norms",
    "authors": ["Douglas L Nelson", "Cathy L McEvoy", "Thomas A Schreiber."],
    "venue": "Behavior Research Methods, Instruments, & Computers, 36(3):402–407.",
    "year": 2004
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532–1543.",
    "year": 2014
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of EMNLP, Seattle, WA.",
    "year": 2013
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of ACL, pages 384–394.",
    "year": 2010
  }, {
    "title": "From Frequency to Meaning: vector space models of semantics",
    "authors": ["Peter D. Turney", "Patrick Pantel."],
    "venue": "Journal of Artifical Intelligence Research, 37(1):141–188, January.",
    "year": 2010
  }, {
    "title": "Similarity of semantic relations",
    "authors": ["Peter D. Turney."],
    "venue": "Computational Linguistics, 32(3):379–416.",
    "year": 2006
  }, {
    "title": "Features of similarity",
    "authors": ["Amos Tversky."],
    "venue": "Psychological Review, 84(4).",
    "year": 1977
  }],
  "id": "SP:3d5651a4961bcd316e1be4815bfbbb49f1cb0ba4",
  "authors": [{
    "name": "Douwe Kiela",
    "affiliations": []
  }, {
    "name": "Felix Hill",
    "affiliations": []
  }, {
    "name": "Stephen Clark",
    "affiliations": []
  }],
  "abstractText": "We demonstrate the advantage of specializing semantic word embeddings for either similarity or relatedness. We compare two variants of retrofitting and a joint-learning approach, and find that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both.",
  "title": "Specializing Word Embeddings for Similarity or Relatedness"
}