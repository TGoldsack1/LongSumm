{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Consider the following common academic (or similar) hiring scenario: The dean has promised your department 3 faculty slots, in any areas. Your goal is to hire the best candidates possible — but how should you identify them? An immediate problem is that candidates are incomparable across subfields, because, among other things, standards of publication, citation counts, and letter-writing styles can vary considerably across subfields. An attractive way to rank candidates is according to how strong they are relative to others working in the same field, to whom they are directly comparable. If we model each subfield as corresponding to a different distribution over metrics that are monotonically increasing in candidate quality, this is the value we get when we evaluate the CDF function of the distribution on a candidate’s realized value. But because\n1University of Pennsylvania, Philadelphia, USA. Correspondence to: Zhiwei Steven Wu <steven7woo@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe number of candidates each year is small, simply comparing each candidate to their direct competitors this year — i.e. taking their empirical CDF values as truth — would lead to a noisy ranking: it could be that due to chance, the best candidate this year in subfield A would be a mediocre candidate in a typical year, and the top two candidates in subfield B would each be the top candidate in a typical year. We would prefer to evaluate our success by considering the unknown true CDF value of each candidate.1 Similar situations, in which we must select a high quality set of candidates from multiple, mutually incomparable groups, arise frequently. Some affirmative action policies are premised on the assertion that SAT scores and other measures may not be directly comparable across different groups (e.g. due to only advantaged groups having the financial resources for test preparation courses and multiple retakes).\nFor various reasons, in these settings we may also be concerned with the fairness of our choices.2 But what should fairness mean? In this paper, we take inspiration from (Dwork et al., 2012) who propose that fairness should mean that “similar individuals are treated similarly”, where “similarity” is measured with respect to some task specific metric. In our setting, the natural task-specific metric is the true within-group CDF value for each individual. On its own, this is compatible with the goal of selecting the best candidates, but in our work, the main obstacle is that we do not know the true CDF value of each individual, and can only approximate this from data. We study the degree to which fairness and optimality are compatible with one another in this setting."
  }, {
    "heading": "1.1. Our Results",
    "text": "We study a setting in which we wish to select k individuals out of a pool of n for some task. The individuals are drawn from d populations, each represented by a different\n1Letters of recommendation often seek to communicate this information, with statements like “This candidate is among the top 5 students I have seen in my 16 years as a professor.”\n2With respect to men’s and women’s sports, equal opportunity is legislated in Title IX. With respect to faculty hiring, fairness concerns can arise because the proportion of women can vary substantially across subfields. For example, as reported in (Cohoon et al., 2011), the percentage of female authors varies from 10% to 44% across ACM conferences, when averaged over the 10 year period from 1998-2008.\ndistribution over real numbers.3 The number of draws from each distribution may differ. The “quality” of an individual is defined to be their (true) CDF value, as evaluated on the distribution from which they were drawn. An algorithm is evaluated based on the (expected) quality of the k individuals it selects.\nThe meritocratic fairness definition we propose informally asks that lower quality individuals are never (probabilistically) favored over higher quality individuals. When formulating this definition, we have a choice as to how to incorporate randomness. The strongest formulation possible (ex-post fairness) does not involve randomness, and simply requires that every individual actually selected has quality at least that of every individual not selected. The weakest formulation (ex-ante fairness) incorporates the randomness of the selection of the population from the underlying distribution, and informally requires that for any pair of individuals, the higher quality individual is selected with weakly higher probability than the lower quality individual, where the randomness is over the realization of the population from the underlying distributions, as well as any internal randomness of the mechanism. An intermediate formulation (ex-interim fairness) requires informally that higher quality individuals be selected with weakly higher probability than lower quality individuals, where the probability is computed over the randomness of the mechanism, but not over the selection of the population. Roughly speaking, these choices correspond to what an individual may know and still be satisfied by a promise of “fairness”. Individuals should be satisfied with ex-post fairness even after the choices of the mechanism are made, with full knowledge of the applicant pool — that is, they should be satisfied with the actual outcome, regardless of the algorithm used to reach it. In contrast, individuals with full knowledge of the applicant pool should still be satisfied with ex-interim fairness before the mechanism makes its decisions — that is, they should feel satisfied that the algorithm used is fair. An individual should only be satisfied by ex-ante fairness if she has no knowledge of the applicant pool (and so can consider it a random variable) before the choices are made.\nGiven such a spectrum of fairness constraints, we observe that the strongest ex-post fairness is impossible to achieve, whereas the weakest ex-ante fairness is sometimes easy to achieve: when the population sizes are the same, it is satisfied by the mechanism that simply selects the k individuals with highest empirical CDF values.4 Our main results\n3We study the simple setting in which each individual is represented by a 1-dimensional “score” — e.g. a credit score, a time in the 100m dash, etc. — which itself may encapsulate or summarize many features into a single value. Generalizing this work to richer representations is an interesting direction for future work.\n4However, for the cases in which the populations are not the same size, we do not know of better utility guarantees for ex-ante\ntherefore concern the cost (in terms of the expected quality of the selected applicants) of asking for the stronger notion of ex-interim fairness. We show that satisfying an exact variant of this constraint requires the selection algorithm to select uniformly at random amongst all individuals, and hence obtain only trivial utility guarantees, but that subject to an approximate relaxation of this constraint, it is possible to recover asymptotically optimal utility bounds. We show that when we further relax the problem, to allow the algorithm to select approximately k individuals (rather than exactly k), it is possible to recover asymptotically optimal utility bounds while satisfying ex-post fairness guarantees within each sub-population, and approximate ex-interim fairness guarantees across populations. We summarize our results in Table 1. We complement our theoretical results with empirical simulations which emphasize that both the utility and fairness guarantees of our algorithms are better in practice than our theorems promise.\nFinally, we remark on an interesting property of our upper bounds: they are oblivious, in the sense that they do not make use of the raw scores associated with each individual — only their empirical CDF ranking. As such, our upper bounds can be viewed as universal distributions over permutations (of empirical CDF rankings) that satisfy a fairness guarantee, rather than algorithms. Our lower bounds apply not just to oblivious algorithms, but to any algorithm, even those that can make use of raw scores (or indeed, even knowledge of the family of distributions from which populations are drawn)."
  }, {
    "heading": "1.2. Related Work",
    "text": "This paper fits into a rapidly growing line of work studying “fairness” in learning settings that is now too large to summarize fully, and so we discuss only the most closely related work. Our definition of fairness is in the spirit of (Dwork et al., 2012), who propose that individual fairness should mean that “similar individuals are treated similarly” with respect to some underlying task-specific metric. As with the work of (Joseph et al., 2016; Jabbari et al., 2016), we define the metric to be a measure of quality already present in the model (in our case, the CDF values of individuals) but unknown to the algorithm, except through samples. It is this necessity to learn the underlying metric that poses the tension between the fairness constraint and the accuracy goal. Although in this line of work, we adopt a definition that merely requires “better individuals be treated better” according to the true unknown metric, this necessarily requires that “similar individuals be treated similarly” with respect to empirical estimates of the metric.\nTechnically, our work includes adaptations of techniques in\nfairness than those we derive for the stronger notion of ex-interim fairness.\ndifferential privacy (Dwork et al., 2006). Specifically, we adopt variants of the “report noisy max” algorithm (Dwork & Roth, 2014), and Raskhodnikova and Smith’s “exponential mechanism for scores with varying sensitivities” (Raskhodnikova & Smith, 2016), which is itself a variant of the exponential mechanism (McSherry & Talwar, 2007)."
  }, {
    "heading": "2. Model and Preliminaries",
    "text": "There are d different populations, indexed by j. For each population j, there is a pool of candidates with their raw scores (and henceforth observations) drawn i.i.d. from some unknown continuous distribution Fj over R. Let F = F1 × · · · × Fd denote the product distribution. We will slightly abuse notation and write xij to denote both the individual i in the population j and her associated observation, and write X to denote the set of all candidates. Let mj be the size of the candidate pool from population j, n = ∑ jmj be the size of the total population, and m = minjmj be the smallest population size. Each individual xij is associated with the following values.\n• A cumulative distribution function (CDF) value Fj(xij) = PrFj [x < xij ],5 and an empirical CDF value F̂j(xij) = 1mj ∑mj i′=1 1[x < xi′j ].\n• A complementary cumulative distribution function (CCDF) value: pij = 1 − Fj(xij) and an empirical CCDF value p̂ij = 1mj ∑mj i′=1 1[x ≥ xi′j ].\nA selection algorithm A takes all the n observations X drawn from different distributions as input, and (randomly) selects k individuals as outputs. We will write A(X,xij) (or Aij for simplicity) to denote the selection probability over the individual xij . The utility for selecting an individual xij is her true CDF value Fj(xij). Equivalently, the loss for selecting an individual xij is the true CCDF value pij . The loss for an algorithmA on input X is then defined as\nL(A, X) = 1 k ∑ xij∈X A(X,xij)(1−Fj(xij))\n5We adopt a slightly different definition from the standard one: Fj(xij) = PrFj [x ≤ xij ].\nand the expected loss of the algorithm is EX∼F [L(A, X)]."
  }, {
    "heading": "2.1. Fairness Formulation",
    "text": "Our goal is design selection algorithms subject to a meritocratic fairness notion that requires that less qualified candidates (in terms of CDF values) are never preferred over more qualified ones. We will present three different formulations of such notion based on the different forms of randomness we are considering.\nFirst, the weakest formulation is the following ex-ante fairness, which guarantees fairness over the randomness of both the random draws of the candidates and the coin flips of the algorithm.\nDefinition 2.1 (Ex-Ante Fairness). An algorithm A satisfies ex-ante fairness if for any pair of candidates xij , xi′j′ with CDF values Fj(xij) > Fj′(xi′j′), their selection probabilities (when they are in the pool) satisfy\nE [A(X,xij)] ≥ E [A(X,xi′j′)]\nwhere the expectations are taken over the (n − 2) random draws of all the other candidates.\nAn intermediate formulation of fairness is the following exinterim fairness, which guarantees fairness over the randomness of the algorithms (but not the realizations of X) on almost all of inputs drawn from the distribution.\nDefinition 2.2 (Exact Ex-Interim Fairness). Let δ ∈ (0, 1). An algorithm A satisfies δ-exact ex-interim fairness if with probability at least 1− δ over the realized observations X , for any pair of individuals xij , xi′j′ ∈ X ,\nA(X,xij) > A(X,xi′j′) only if Fj(xij) > Fj′(xi′j′)\nWe also consider the following relaxation:\nDefinition 2.3 (Approximate Ex-Interim Fairness). An algorithm A satisfies (ε, δ)-approximate ex-interim fairness if with probability at least 1− δ over the realized observations X , for any pair of individuals xij , xi′j′ ∈ X ,\nA(X,xij) > eεA(X,xi′j′) only if Fj(xij) > Fj′(xi′j′)\nRemark 2.4. We note that this relaxation of ex-interim fairness bears a similarity to the definition of differential\nprivacy (Dwork et al., 2006), and indeed, techniques from the differential privacy literature will prove useful in designing algorithms to satisfy it.\nPerhaps the strongest formulation is the following ex-post fairness condition, which requires that an individual is selected only if a more qualified individual is also selected. Definition 2.5 (Ex-post Fairness). An algorithmA satisfies ex-post fairness if any pair of individuals xij and xi′j′ such that Fj(xij) > Fj′(xi′j′), the individual xi′j′ is admitted only if xij is also selected.\nNote that any algorithm that satisfies ex-post fairness must admit a prefix of individuals from each population, which is also sufficient to guarantee within population ex-post fairness, but that this is not sufficient to satisfy the constraint between populations.\nIt is not hard to see that satisfying ex-post fairness in the generality that we have defined it is impossible, since it requires perfectly selecting the k true best CDF values from only sample data. Thus, the primary focus of our paper is on ex-interim fairness. Unless we specify differently, the term “fair” and “fairness”refer to ex-interim fairness."
  }, {
    "heading": "2.2. Oblivious Algorithms",
    "text": "A special class of selection algorithms is the class of oblivious algorithms, which select candidates with probabilities that only depend on their empirical CDF values, not on their observations. Definition 2.6 (Oblivious Algorithms). An algorithm A is oblivious if for any pair of input observations X and X ′ that induce the same empirical CDF values over the candidates, A(X) = A(X ′).\nAll of our algorithms presented in this paper are oblivious. As a result, we need to make no assumption on the underlying distributions to achieve both fairness and utility guarantees. Moreover, the utility guarantee of an oblivious algorithm can be characterized as follows. Lemma 2.7. The expected loss achieved by any oblivious algorithm A is the expected average empirical CCDF values among the selected candidates.\nA very simple example of an oblivious algorithm is GREEDY which selects the k individuals with the highest empirical CDF values (breaking ties uniformly at random). Lemma 2.8. Suppose that the populations sizes are the same, that is, mj = m for each j. The algorithm GREEDY satisfies ex-ante fairness and has an expected loss at most k2n + 1 m .\nTo simplify our bounds on the expected loss, we will use k/2n as our benchmark and define the regret of an algorithm A to beR(A) = EX∼F [L(A, X)]− k2n ."
  }, {
    "heading": "3. An Approximately Fair Algorithm",
    "text": "In this section, we provide an algorithm that satisfies approximate fairness in the sense of Definition 2.3. We will present our solution in three steps.\n1. First, we provide confidence intervals for the candidates’ CCDF values pij based on their empirical CCDF values p̂ij . As we show, our bound has a tighter dependence on pij , which gives better utility guarantee than using the standard DKW inequality of Dvoretzky et al. (1956).\n2. Next, we give a simple subroutine NOISYTOP that randomly selects k individuals out of n based on their “scores”. We show that individuals with similar scores will have close selection probabilities under this subroutine. This subroutine is similar to the “Report Noisy Max” algorithm (Dwork & Roth, 2014).\n3. Then, we will use the deviation bound in the first step to assign scores to the candidates. We show that running NOISYTOP based on these scores give approximate fairness and low regret guarantees. These scores are computed in a way similar to the generalized exponential mechanism of Raskhodnikova & Smith (2016)."
  }, {
    "heading": "3.1. Confidence Intervals for CCDF Values",
    "text": "We will first give the following concentration inequality specialized for the uniform distribution over (0, 1).\nLemma 3.1. Fix any n ∈ N. Let x1, x2, . . . , xn be i.i.d. draws from the uniform distribution over (0, 1). Then with probability at least 1− δ, for any p ∈ (0, 1),\n|p− p̂| ≤ √ ln(2n/δ)\n(√ 3p\nn +\n2\nn\n)\nwhere p̂ = 1n ∑n i=1 1[xi < p].\nTo translate this result into a deviation bound on the CCDF values, first note that CCDF values for any distribution Fj are drawn from the uniform distribution over (0, 1), so the bound applies immediately to the CCDF values. By a standard calculation, we can also get a bound in terms of the empirical CCDF value p̂ij as shown below.\nLemma 3.2. For each j ∈ [d], draw mj points Xj = {xij} mj i=1 i.i.d. from Fj . For each point xij , let pij be its true CCDF value and p̂ij be its empirical CCDF value in Fj . Then with probability at least 1− δ over the n random draws,\n|pij − p̂ij | ≤ 9 √ p̂ij m ln(2n/δ)\nwhere m = minjmj and n = ∑d j=1mj .\nRemark 3.3. The standard DKW inequality gives a bound of Õ( √ 1/m). Our bound gives a tighter dependence for small empirical CCDF value p̂ij . For example, when p̂ij = 1/m, we obtain a bound of Õ(1/m). 6"
  }, {
    "heading": "3.2. The NoisyTop Subroutine",
    "text": "Given a set of individuals with scores Y = {y1, . . . , yn}, the subroutine NOISYTOP will first perturb each score by adding independent noise drawn from the Laplace distribution,7 and output the k individuals with the minimum noisy scores (ties broken arbitrarily). We will now show that NOISYTOP has the following desirable “Lipschitz” property—individuals with similar scores are chosen with similar probabilities. This is crucial for obtaining approximate fairness.\nAlgorithm 1 NOISYTOP({y1, y2, . . . , yn}, α, k) Input: n numbers {y1, y2, . . . , yn} and parameter α\nFor each i ∈ [n]: let ỹi = yi + Lap(α) Output: the k indices with the smallest ỹi\nLemma 3.4. Let i, j ∈ [n] be such that ∆ = yi − yj ≥ 0. Let Pi and Pj denote the probabilities that the two indices i and j are output by NOISYTOP({y1, y2, . . . , yn}, α) respectively. Then Pi ≤ Pj ≤ Pi exp(2∆/α).\nProof. Let ỹi and ỹj be the noisy scores for i or j. We will introduce a new random variable Q to denote the value of the (k − 1)-st lowest noisy value, not counting ỹi and ỹj . We will slightly abuse notation and write Pr[R = r] as a shorthand for the pdf of any random variable R evaluated at r. The ratio PiPj can then be written as∫ q∈R Pr[Q = q] (∫ t∈R Pr[ỹj = t] Pr[ỹi < min{t, q}]dt ) dq∫\nq∈R Pr[Q = q] (∫ t∈R Pr[ỹi = t] Pr[ỹj < min{t, q}]dt ) dq\n(1)\nFor any fixed value r ∈ R, we also have the following based on the Laplace distribution,\nPr[ỹi = r] Pr[ỹj = r] =\n1 2α exp ( − |r−yi|α ) 1 2α exp ( − |r−yj |α\n) = exp ( |r − yj | α − |r − yi| α\n) By the triangle inequality we know that |r−yj |−|r−yi| ≤ ∆. It follows that for any t and q,\nexp(−∆/α) ≤ Pr[ỹi = t] Pr[ỹj = t] ≤ exp(∆/α) and,\n6As shown in Corollary 3.8, this also gives an improvement over regret when k is small (Õ( √ k/n) versus Õ( √ 1/n)).\n7The Laplace distribution Lap(b) has density function f(x) = exp(−|x|/b).\nPr[ỹi < min{q, t}] Pr[ỹj < min{q, t}] = ∫ r<min{q,t} Pr[ỹi = r] dr∫ r<min{q,t} Pr[ỹj = r] dr\n≤ exp(∆/α)\nPlugging these bounds into Equation (1), we get PiPj ≤ exp(2∆/α). The inequality that Pi/Pj ≤ 1 follows directly from yi ≥ yj ."
  }, {
    "heading": "3.3. Wrapping Up",
    "text": "We will present our algorithm FAIRTOP by combining the methods in the previous two sections. In the light of Lemma 3.2, we will define the following confidence interval width function on the empirical CCDF values\nc(p̂) = 9 ln(2n/δ) √ p̂/m\nand a normalized score function s(p̂) = p̂/c(p̂). We have that any candidate is guaranteed a score not much lower than a less qualified one.\nLemma 3.5. Let x, y ∈ [0, 1] be the (true) CCDF values for two individuals such that x ≤ y. Let x̂, ŷ be the empirical CCDF values respectively. Suppose that |x− x̂| ≤ c(x̂) and |y − ŷ| ≤ c(ŷ), then s(x̂)− s(ŷ) ≤ 1.\nAlgorithm 2 FAIRTOP(X = {xij}, ε, δ, k,m) Input: candidates’ observations X , fairness parameters ε, δ, number of selected individuals k, and smallest population size m\nFor each individual xij ∈ X Compute the empirical CCDF value p̂ij and the associated score s(p̂ij)\nRun NOISYTOP({s(p̂ij}, 2/ε, k)\nOur algorithm FAIRTOP (presented in Algorithm 2) proceeds by first computing the normalized score of every candidates based on their empirical CCDF values, and then calling NOISYTOP to output k individuals. We will first establish the approximate fairness guarantee.\nTheorem 3.6. The algorithm FAIRTOP instantiated with parameters ε and δ satisfies (ε, δ)-approximate fairness.\nProof sketch. By Lemma 3.2, we know that with probability 1 − δ, for every candidate xij , the true and empirical CCDF values satisfy |pij − p̂ij | ≤ c(p̂ij). This means that for any pair of individuals a and a′ with CCDF values pa < pa′ (that is, a is more qualified than a′), we also have s(p̂a) ≤ s(p̂a′) + 1 by Lemma 3.5. Finally, by the result of Lemma 3.4 and the instantiation of NOISYTOP, we guarantee that a′ will not be selected with substantially higher probability: Aa exp(ε) ≥ Aa′ , which recovers the approximate fairness guarantee.\nOur algorithm also has a diminishing regret guarantee:\nTheorem 3.7. Fix any β ∈ (0, 1). Then with probability at least 1 − β, the algorithm FAIRTOP instantiated with fairness parameters ε and δ has regret bounded by(\n1\nε\n√( k\nn +\n1\nm\n) 1\nm +\n1\nmε2\n) · polylog(n, 1/β, 1/δ)\nThus for example, as the smallest sampled population size m grows (fixing k and ε), our regret rapidly approaches 0. To understand the utility guarantee better, we will state the regret bound for the following natural scaling, which is also examined in the simulations of Section 7:\nCorollary 3.8. Consider an instance with two population of sizesm1 andm2 such thatm1 = αm2 for some constant α ≥ 1. Suppose we instantiate FAIRTOP with parameter ε = Θ(1), then the regret is at most Õ (√ k m ) ."
  }, {
    "heading": "4. Within Population Ex-Post Fairness",
    "text": "In this section, we provide a variant of the FAIRTOP algorithm that satisfies approximate ex-interim fairness across different populations, but also ex-post fairness within each population. The key idea here is that since we know the ranking of the candidates true qualities within each population, we can guarantee ex-post fairness within populations as long as we select a prefix of candidates in each population. This will however come at a cost — our algorithm will no longer select exactly k individuals, but only approximately k individuals.\nSimilar to FAIRTOP, the algorithm ABOVETHRE (presented in Algorithm 3) also computes the normalized scores for each candidate. Instead of perturbing the scores, ABOVETHRE computes a noisy threshold Tj for each population by adding Laplace noise to s(k/n). The algorithm then selects all candidates with scores above the noisy threshold. Because the algorithm selects a prefix of the raw scores within each population, within population ex-post fairness is immediate. We also show that ABOVETHRE also achieves approximate ex-interim fairness.\nTheorem 4.1. The algorithm ABOVETHRE instantiated with fairness parameters ε and δ satisfies both (ε, δ)approximate ex-interim fairness and ex-post fairness within each population.\nNote that were the algorithm to take all the individuals with scores above s(k/n), it would select a (k/n) fraction from each population and therefore select k people in total. Due to the noisy thresholds, the algorithm will only select approximately k individuals. We will now establish the utility guarantee of ABOVETHRE and show that the number of selected individuals is roughly k± Õ( √ k) when m = Θ(n).\nAlgorithm 3 ABOVETHRE(X = {xij}, ε, δ, k,m) Input: observations X , fairness parameters ε, δ, target number of selected individuals k, smallest population size m\nFor each individual xij Compute her empirical CCDF value p̂ij and the associated score s(p̂ij) For each population j Compute a noisy threshold Tj = s(k/n)+νj where νj is drawn from Lap(1/ε) Select candidates xij with scores s(p̂ij) above Tj\nTheorem 4.2. Fix any β ∈ (0, 1). With probability at least 1 − β, the algorithm ABOVETHRE instantiated with fairness parameters ε and δ has regret bounded by(\n1\nmε2 +\n√ k\nε √ mn\n) · polylog(n, d, 1/δ, 1/β),\nand selects a total number of k̂ individuals with\n|k − k̂| ≤ d+\n( n\nmε2 +\n√ nk ε √ m\n) · polylog(n, d, 1/δ, 1/β)"
  }, {
    "heading": "5. Lower Bound for Exact Fairness",
    "text": "We will show that it is impossible to achieve exact exinterim fairness with non-trivial regret guarantees.\nTheorem 5.1. Fix any δ < 0.0002 and any δ-fair algorithm A. There exist two distributions F1 and F2 over the two populations such that if algorithm A takes m observations drawn from each distribution as input, and must select at least k = Ω(m1/2+α) individuals for any α > 0, A incurs a regret of Ω(1).\nThe main idea is to show that there exist distributions F1 and F2 such that any fair algorithm will essentially have to select uniformly at random across Ω(m) individuals, which incurs regret Ω(1). We will proceed via Bayesian reasoning. Suppose that the observations from the two populations are drawn from two different unit-variance Gaussian distributions N (µ1, 1) and N (µ2, 1), and both means µ1 and µ2 are themselves drawn from the prior N (0, 1). The following lemma characterizes the posterior distribution on the mean given a collection of observations.\nLemma 5.2. (Murphy, 2007) Suppose that a mean parameter µ is drawn from a prior distribution N (0, 1). Let D = (x1, x2, . . . , xm) be m i.i.d. draws from the distributionN (µ, 1). Then the posterior distribution of µ conditioned on D is the Gaussian distribution N (µ̂, σ2), where µ̂ = ∑ i xi\nm+1 and σ 2 = 1m+1 .\nThe result above shows that conditioned on any m draws from the Gaussian distribution, there is constant probability that the true mean will be bounded away from the posterior maximum likelihood estimate by at least Ω(1/ √ m). With this observation, we will partition the real line into the following intervals: Given any posterior mean µ̂ any integer r ≥ 1, let the two intervals I+r (µ̂) and I−r (µ̂) be\nI+r (µ̂) = [µ̂+ (r − 1)/ √ m, µ̂+ r/ √ m] and I−r (µ̂) = [µ̂− r/ √ m, µ̂− (r − 1)/ √ m]\nThe intervals capture the uncertainty we have regarding the CDF values of the observations xij . Let Xj = (x1j , x2j , . . . , xmj} denote the m draws from each distributionFj , µ̂j = ∑ i xij m+1 be the posterior mean for µj conditioned on the draws. Consider two individuals xi1 and xi′2 such that xi1 ∈ I+r (µ̂1) and xi′2 ∈ I+r+1(µ̂2). Even though (xi′2 − µ̂2) > (xi1 − µ̂1), there is a constant probability that their CDF values satisfy F1(xi1) > F2(xi′2). Any fair algorithm therefore must play these two individuals in these “neighboring” intervals with equal probabilities.\nNext, we show that with high probability over the realizations of the true mean µ and the m draws X , all of the O(m logm) intervals around the posterior mean will be “hit” by points in X . Lemma 5.3. Fix any c < 1 and β ∈ (0, 1). Let mean µ be drawn from N (0, 1), r̂ = √ cm logm + 1 and X = (x1, x2, . . . , xm) be m i.i.d. draws from N (µ, 1). Let µ̂ = ∑ i xi\nm+1 . Then except with probability 2r̂ exp ( −m\n(1/2−c/2) √ 2A\n) + 3β over the joint realizations of µ\nand X , the following holds\n• for all r ≤ r̂ − 2 √\n2 ln(2/β), there exist two draws x+r , x − r ∈ X such that x+r ∈ I+r (µ̂) and x−r ∈ I−r (µ̂); • the number of points that are bigger than µ̂ + (r̂ − 2 √ 2 ln(2/β))/ √ m is no more than\nm1−c/2 +m1/2−c/4 √ 3 ln(1/β)\nWe show that the event that all of the consecutive intervals are occupied for both populations will force a fair algorithm to play all the individuals in these intervals with equal probability. More formally, fix any c and sufficiently small constant β, let r̂ = √ cn log n+ 1 and let Y = {xij | xij ∈\nI+r (µ̂j) ∨ xij ∈ I−r (µ̂j) for some r ≤ r̂ − 2 √\n2 ln(2/β)}. Consider the following events:\n• FULLCHAIN(X1, X2): for all r ≤ r̂ − 2 √\n2 ln(2/β) and j ∈ {1, 2}, both the intervals I+r (µ̂j), I−r (µ̂j) contain at least one point in Xj ,\n• UARCHAIN(A, X1, X2): the points in Y are selected by the algorithm A with equal probabilities.\nLemma 5.4. Fix any δ-fair algorithm A for some δ < 0.0002. With probability at least 1/2 over the realizations of µ1, µ2, X1 and X2, the event FULLCHAIN(X1, X2) implies UARCHAIN(A, X1, X2).\nProof sketch for Theorem 5.1. The combination of Lemmas 5.3 and 5.4 shows that with constant probability over µ1, µ2 and X , A will need to select Ω(m) individuals with equal probabilities, which leads to an expected regret of Ω(1) over the draws of µ1, µ2 This means there exist distributionsF1 = N (µ∗1, 1) andF2 = N (µ∗2, 1) under which A incurs Ω(1) regret."
  }, {
    "heading": "6. Sequential Batch Setting",
    "text": "We briefly mention an extension to the sequential batch setting, in which the algorithm selects individuals in T rounds. In each round t, for each population j, there aremj new candidates with their observations drawn i.i.d. from the distribution Fj , At each round t, the algorithm needs to select k individuals from this pool. Let Stj be the set of observations from population j accumulated after the first t rounds. In particular, for any observation x and population j, let the historical CCDF value be q̂tj(x) =\n1 |mjt| ∑ x′∈Stj\n1[x′ < x]. As t grows large, the empirical CCDF values become better estimates for the true CCDF values. We give a variant of the FAIRTOP algorithm that achieves (ε, δ)-approximate fairness in every round, and incurs average regret over time diminishing as Õ ( 1\nε √ mT\n) ."
  }, {
    "heading": "7. Simulations",
    "text": "We conclude by discussing some illustrative simulation results for FAIRTOP, along with comparisons to simpler algorithms without fairness guarantees. The simulations were conducted on data in which the raw scores for each population i = 1, 2 were drawn from N (µi, 1) respectively, and the µi themselves were chosen randomly fromN (0, 1). Thus befitting the motivation for our model, the raw scores are not directly comparable between populations. While we varied the population sizes, they were held in the fixed ratio m1/m2 = 2 and k = d0.1(m1 +m2)e.\nFor such a simulation with population sizes m1 = 100 and m2 = 50, Figure 1(a) shows the underlying scores computed by FAIRTOP (which depend only on the empirical CDF values) for each member of both populations, but sorted according to their true CDF values so that the transpositions that occur between emprical and true CDFs are apparent; the red points are for the larger population and green for the smaller. Overlaid on this arc of underlying scores is a black plot illustrating sample post-noise scores\nwhen ε = 10. As we can see, re-sorting the points by their noisy scores will result in a significant amount of additional reshuffling.\nFigure 1(b) illustrates the induced distribution over chosen individuals; here we show the results of resampling the Laplace noise (again at ε = 10) for 100,000 trials, and choosing the top k post-noise scores across populations. The ordering is again by true CDF values and the same color coding is used. At this value of ε the distribution is biased towards better true CDF values but still enjoys strong fairness properties. For example, the “unfairness ratio” (maximum ratio of the number of times a worse CDF value is chosen to a better CDF value is chosen) is only 1.56 (note that this is substantially stronger than the bound of e10 guaranteed by our theorem). It is also visually clear that FAIRTOP is treating similar CDF values similarly, both within and between populations.\nNevertheless, the regret of FAIRTOP for these population sizes and ε is nontrivial (roughly 0.20 regret compared to the best k true CDF values). Of course, as per Theorem 3.7 by increasing ε we can reduce regret to any desired level at the expense of weakened fairness guarantees. However, as per Corollary 3.8 even for fixed ε (and therefore fixed fairness properties), regret diminishes rapidly in the natural scaling where the population sizes grow, but in a fixed ratio. This is illustrated empirically in Figure 1(c), where for varying choices of ε we plot regret as m1,m2 → ∞ with m1/m2 = 2.\nWe now briefly compare the properties of FAIRTOP to simpler approaches that generally enjoy lower regret but have no fairness properties. Perhaps the simplest is to pick the k highest ranked individuals by empirical CDF rank. This method will in general have very low regret, but since it is deterministic, any trial in which it doesn’t select the top k true CDF values has no fairness guarantee (i.e. the unfairness ratio will be infinite), and this happens in approx-\nimately approximately 87% of trials under the simulation parameters above (and approaches 100% as populations grow in fixed ratio).\nPerhaps the most natural “learning” approach is to use the raw scores to obtain estimated population means µ̂i (or more generally to estimate the unknown parameters of some known or assumed parametric form) and then use the CDFs of N (µ̂1, 1) and N (µ̂2, 1) to select the k best individuals across the two populations. This again has generally lower regret than FAIRTOP, but is deterministic and without fairness guarantees, with approximately 53% of trials resulting in unbounded unfairness ratio (approaching 100% as populations grow in fixed ratio).\nBut the main drawback of such a learning approach in comparison to the data-oblivious FAIRTOP is its need for realizability. For instance, if we change the population 2 scores to be drawn from the uniform distribution over a wide range, but the learning approach continues to assume normality in each population, it will virtually always choose only members of population 2, a clear and dramatic violation of any intuitive notion of fairness. This is of course due the fact that the highest scores in population 2 appear to have extraordinarily high CDF values when (incorrectly) assumed to have been drawn from a normal distribution. In contrast FAIRTOP, since it doesn’t even consider the actual scores but only generic properties of the relationship between empirical and true CDF values, will behave exactly the same, in both fairness and regret, regardless of how the underlying scores are generated."
  }],
  "year": 2017,
  "references": [{
    "title": "Gender and computing conference papers",
    "authors": ["Cohoon", "J McGrath", "Nigai", "Sergey", "Kaye", "Joseph Jofish"],
    "venue": "Communications of the ACM,",
    "year": 2011
  }, {
    "title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
    "authors": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"],
    "venue": "Ann. Math. Statist., 27(3):642–669,",
    "year": 1956
  }, {
    "title": "Calibrating noise to sensitivity in private data analysis",
    "authors": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam"],
    "venue": "In Theory of Cryptography Conference,",
    "year": 2006
  }, {
    "title": "Fairness through awareness",
    "authors": ["Dwork", "Cynthia", "Hardt", "Moritz", "Pitassi", "Toniann", "Reingold", "Omer", "Zemel", "Richard"],
    "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,",
    "year": 2012
  }, {
    "title": "Fair learning in Markovian environments",
    "authors": ["Jabbari", "Shahin", "Joseph", "Matthew", "Kearns", "Michael", "Morgenstern", "Jamie", "Roth", "Aaron"],
    "venue": "arXiv preprint arXiv:1611.03071,",
    "year": 2016
  }, {
    "title": "Fairness in learning: Classic and contextual bandits",
    "authors": ["Joseph", "Matthew", "Kearns", "Michael", "Morgenstern", "Jamie H", "Roth", "Aaron"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Mechanism design via differential privacy",
    "authors": ["McSherry", "Frank", "Talwar", "Kunal"],
    "venue": "In Foundations of Computer Science,",
    "year": 2007
  }, {
    "title": "Conjugate Bayesian analysis of the Gaussian distribution",
    "authors": ["Murphy", "Kevin P"],
    "year": 2007
  }, {
    "title": "Lipschitz extensions for node-private graph statistics and the generalized exponential mechanism",
    "authors": ["Raskhodnikova", "Sofya", "Smith", "Adam D"],
    "venue": "IEEE 57th Annual Symposium on Foundations of Computer Science,",
    "year": 2016
  }],
  "id": "SP:8e29794241bcfb6c896ae170366911448423b341",
  "authors": [{
    "name": "Michael Kearns",
    "affiliations": []
  }, {
    "name": "Aaron Roth",
    "affiliations": []
  }, {
    "name": "Zhiwei Steven Wu",
    "affiliations": []
  }],
  "abstractText": "We consider the problem of selecting a pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner. The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons. We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates. Specifically, we quantify the regret in quality imposed by “meritocratic” notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality. We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests.",
  "title": "Meritocratic Fairness for Cross-Population Selection"
}