{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Sequential data prediction is an important problem in machine learning spanning over a diverse set of applications ranging from text (Mikolov et al., 2010) to user behavior (Köck & Paramythis, 2011). For example, when applied to statistical language modeling, the goal is to predict the next word in textual data given context, very similar to that in user activity modeling where the aim is to predict the next\n1Carnegie Mellon University, Pittsburgh PA – work done while at Google 2Google Inc, Mountain View CA. Correspondence to: Manzil Zaheer <manzil@cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nactivity of the user given the history. Accurate user activity modeling is very important for serving relevant, personalized, and useful contents to the user. A good model of sequential data should be accurate, sparse, and interpretable. Unfortunately, none of the existing techniques for user or language modeling satisfy all of these requirements.\nThe state-of-the-art for modeling sequential data is to employ recurrent neural networks (RNN) (Lipton et al., 2015), such as LSTMs (Long-Short Term Memory) (Hochreiter & Schmidhuber, 1997). Such RNNs have been shown to be effective at capturing long and short patterns in data, e.g. token-level semantic as well as syntactic regularities in language (Jozefowicz et al., 2016). However, the neural network representations are generally uninterpretable and inaccessible to humans (Strobelt et al., 2016). Moreover, the number of parameters of the model is proportional to the number of observed word types or action types, which can grow to tens or hundreds of millions. Note that for user modeling task, character level RNN is not feasible because user actions are often not words but hash indices or URLs.\nOn the other hand of the spectrum, latent variable models with multi-task learning, such as LDA (Blei et al., 2002) and other topic model variants, which are strictly not sequence models, proved to be powerful tools for uncovering latent structure in both text and user data (Aly et al., 2012) with good commercial success (Hu et al., 2014). Topic models are popular for their ability to organize the data into a smaller set of prominent themes or topics through statistical strength sharing across users or documents. These topic representations are generally accessible to humans and easily lend themselves to being interpreted.\nIn this paper, we propose Latent LSTM Allocation (LLA), a model that bridges the gap between the sequential RNN’s and the non-sequential LDA. LLA borrows graph-\nical model techniques to infer topics (groups of related word or user activities) by sharing statistical strength across users/documents and recurrent deep networks to model the dynamics of topic evolution inside each sequence (document or user activities) rather than at user action/word level (Sec. 3.1). LLA inherits sparsity and interpretability from LDA, while borrowing accuracy from LSTM. We provide various variants of LLA that trade model size vs. accuracy without sacrificing interpretability (Sec. 3.3). As shown in Figure 1, for the task of language modeling on the Wikipedia dataset, LLA achieves comparable accuracy to LSTM while being as sparse as LDA in terms of models size. We give an efficient inference algorithm for parameter inference in LLA (Sec. 3.2) and show is efficacy and interpretability over several datasets (Sec. 4)."
  }, {
    "heading": "2. Background",
    "text": "In this section, we provide a brief review of user/language modeling and LSTMs."
  }, {
    "heading": "2.1. User/Language Modeling",
    "text": "User activity modeling and language modeling amounts to learning a function that computes the log probability, log p(w|model), of a user activity or sentence w = (w1, . . . , wn). Subsequently, this function can be used to predict the next set of actions or words. This function can be decomposed and learned in different ways under various assumptions. Imposing a bag of words assumption - as used in LDA - leads to ignoring the sequence information and yields ∑n i=1 log p(wi|model). Alternatively, one could decompose according to the chain rule into sum of the conditional log probabilities∑n i=1 log p(wi | w1, . . . , wi−1,model), thereby preserving temporal information and use some RNN to model log p(wi | w1, . . . , wi−1,model) (Mikolov et al., 2010; Sundermeyer et al., 2012)."
  }, {
    "heading": "2.2. Long Short-Term Memories",
    "text": "Temporal aspect is very important for user activity modeling. LSTM, a type of RNN, is well suited for the task as it can learn from experience to classify, process, and predict time series when there are very long time lags of unknown size between important events. LSTM is designed to cope with the vanishing gradient problem inherent in simple RNNs (Hochreiter & Schmidhuber, 1997). This is one of the main reasons why LSTMs outperform simple RNNs, hidden Markov models, and other sequence learning methods in numerous application.\nIn general, a RNN is a triplet (Σ, S, δ): • Σ ⊆ RD is the input alphabet • S ⊆ RH is the set of states • δ : S × Σ → S is the state transition function made\nup of a neural network. RNN maintains an internal state and at each time step take\nan input xt and updates its state st by applying the transition function made up of neural network δ the previous time step’s state st−1 and the input.\nOften the input is not available directly as elements of Σ and the output desired is not the state of the RNN. In such cases, input is appropriately transformed and desired output is produced at each time step from the state st:\nyt = g(st),\nwhere g is an arbitrary differentiable function.\nFor example, in a regular recurrent language model (RRLM) (Mikolov et al., 2010) a document is treated as a sequence and an LSTM is trained to predict directly the next word conditioned on the sequence of words before it, i.e. maximize p(wt|wt−1|, wt−2, ..., w0; model). In this case, the input transformation is done by using a word lookup table from word to a vector in Σ. This word representation is used to update the state of the LSTM. The output transformation is the projection of st into a vector of size of the vocabulary V followed by a softmax. However, this method will require two large matrices of dimension V ×H and cannot handle out of vocabulary words.\nTo overcome above mentioned shortcomings, another input transformation has been proposed, which looks at characters directly instead of words (Ling et al., 2015). On the spelling of the words another LSTM is run having characters as the input and the final state is used as the word representation. In this case, the memory requirement is reduced to half and the model can handle out-of-vocabulary words like typographical errors or new words made from composing existing ones. To use character level embedding, we can simply replace the word lookup table with the representation obtained from character-level LSTM. In case of natural languages, a similar trick of using a character-level LSTM to emit words can be applied as the output transformation as well. However, user modeling outputs (hash indices and URLs) lack morphological structure, and hence cannot leverage the technique."
  }, {
    "heading": "3. Latent LSTM Allocation",
    "text": "In this section, we provide a detailed description of the proposed LLA model and its variant for performing joint clustering and modeling non-linear dynamics. An ideal model for such a task should have three characteristics. First, it should have a low number of parameters. Second, it should be interpretable, allowing human analysis of the components. Lastly and most importantly, the model should be accurate in terms of predicting future events. We show how LLA satisfies all of these requirements."
  }, {
    "heading": "3.1. Generative model",
    "text": "In this model, we try to marry LDA with its strong interpretability capabilities with LSTM having excellent track record in capturing temporal information. In this regard,\nwe propose a factored model, i.e. we use the LSTM to model sequence of topics p(zt|zt−1, zt−2, ..., z1) and multinomial-Dirichlet to model word emissions p(wi|zi), similar to LDA. Suppose we have K topics, vocabulary size V , and a document collectionD where each document d ∈ D is composed of Nd words. With these notations, the complete generative process can be illustrated as in Figure 2a and can be described as:\n1. for k = 1 to K (a) Choose topic φk ∼ Dir(β) 2. for each document d in corpus D (a) Initialize LSTM with s0 = 0 (b) for each word index t from 1 to Nd\ni. Update st = LSTM(zd,t−1, st−1) ii. Get topic proportions at time t from the\nLSTM state, θ = softmaxK(Wpst + bp) iii. Choose a topic zd,t ∼ Categorical(θ) iv. Choose word wd,t ∼ Categorical(φzd,t)\nUnder this model, the marginal probability of observing a given document d can be written as:\np(wd|LSTM,φ) = ∑ zd p(wd, zd|LSTM,φ)\n= ∑ zd ∏ t p(wd,t|zd,t;φ)p(zd,t|zd,1:t−1; LSTM).\nHere p(zd,t|zd,1:t−1; LSTM) is the probability of generating topic for the next word in the document given topics of previous words and p(wd,t|zd,t;φ) is the probability of generating word given the topic, illustrating the simple modification of LSTM and LDA based language models.\nThe advantage of this modification is two fold. Firstly, we obtain a factored model as shown in Figure 3, thereby the number of parameters is reduced significantly compared to RRLM. This is because, unlike RRLM we operate at topic level instead of words directly and the number of topics is much smaller than the vocabulary size, i.e., K << V .\nThis allows us to get rid of large V ×H word embedding look-up table and V × H matrix used in softmax over the entire vocabulary. Instead we map from hidden to state to topics first using a K ×H matrix, which will be dense and then from topics to words using a V ×K matrix under the Dirichlet-multinomial model similar to LDA which will be very sparse. Secondly, this model is highly interpretable. We recover global themes present in the documents as φ. The LSTM output represents topic proportions for document/user at time t. The LSTM input over topics can capture semantic notion of topics."
  }, {
    "heading": "3.2. Inference",
    "text": "As the computation of the true posterior of LLA as described above is intractable, we have to resort to an approximate inference technique like mean field variational inference (Wainwright & Jordan, 2008) or stochastic EM (SEM) (Zaheer et al., 2016). Below we describe the generic aspects of the inference algorithm for LLA. Model specific aspects are relegated to the appendix. 1\nGiven a document collection, the inference task is to find the LSTM weights and word|topic probability matrix φ. This can be carried out using SEM. We begin by writing out the likelihood and lower bounding it as,∑\nd\nlog p(wd|LSTM,φ)\n≥ ∑ d ∑ zd q(zd) log p(zd; LSTM) ∏ t p(wd,t|zd,t;φ) q(zd) ,\n(1) for any distribution q. Then the goal is to ensure an increase in this evidence lower bound (ELBO) with each iteration in expectation. Following the suit of most EM based inference methods, each iteration involves two phases, each of which is described below:\nSE-step: For fixed LSTM parameters andφ, we sample the topic assignments z. This acts as an unbiased sample estimate of the expectation with respect to the conditional distribution of z required in traditional EM. This sampled estimate not only enjoys similar statistical convergence guarantees (Nielsen, 2000) but also provides many computational benefits like speed-up and reduced memory bandwidth requirements (Zaheer et al., 2016).\nUnder LLA, the conditional probability for topic at time step t given word at time t and past history of topics is, p(zd,t = k|wd,t, zd,1:t−1; LSTM,φ)\n∝ p(zd,t = k|zd,1:t−1; LSTM)p(wd,t|zd.t = k;φ) (2)\nThe first term represents probability of various topics predicted by LSTM dynamics after final softmax and second term comes from multinomial-Dirichlet word emission model given the topic. Naı̈vely sampling from this distribution would cost O(KH +H2) per word.\n1Available at http://manzil.ml/lla.html\nOptionally, one could speed up this sampling. Note that the second term in (2) has a sparse structure. For sampling from (2) the computation of the normalizer is not essential. To elaborate, let us define nwk as the number of times word w currently assigned to topic k and nk as the number of tokens assigned to topic k. Explicitly writing down first term:\np(wd,t = w|zd,t = k,φ) = φwk = nwk + β\nnk + V β\n= nwk\nnk + V β︸ ︷︷ ︸ sparse\n+ β\nnk + V β︸ ︷︷ ︸ dense\n(3)\nThere is an inherent sparsity present in nwk, as a given word would be typically about only a handful of topics, Kw K. The second term represents the global count of tokens in the topic and will be dense, regardless of the number of documents, words or topics.\nFollowing (Li et al., 2014), we devise an optional fast sampling strategy for exploiting this sparsity and construct a Metropolis-Hastings sampler. The idea is to replace the low weight dense term by a cheap approximation, while keeping the high weight sparse term exact. This leads to a proposal distribution that is close to (2), while at the same time allowing us to draw from it efficiently: • First draw a biased coin to decide whether to draw from\nthe sparse nwk term or from the dense term. The bias is\nb = p\np+ q , where p = ∑\nk:nwk 6=0\nnwk nk + V β p(zd,t = k|zd,1:t−1; LSTM)\nq = ∑ k\nβ\nnk + V β (pre-computed).\n• If we draw from the sparse term, the cost is O(KwH + H2), else the cost is O(H2) using the alias trick. • Finally, perform a MH accept/reject move by comparing the approximation with the true distribution.\nM-step: For fixed topic assignment z, update the φ and LSTM so as to improve the likelihood in expectation. As\nthe dependence of likelihood on LSTM parameters and φ are independent given z, we can update them independently and in parallel. For the former, we use the closed form expression for the maximizer,\nφwk = #{wd,t = w and zd,t = k}+ β #{nwk = k}+ V β = nwk + β nk + V β ,\nand for the latter we resort to stochastic gradient ascent which will increase the likelihood in expectation.\nThe execution of the whole inference and learning process includes several iterations involving the above mentioned two phases as outlined in Algorithm 1. This inference procedure is not an ad-hoc method, but an instantiation of the well studied SEM method. We ensure that in each iteration we increase the ELBO in expectation. Also, it is just a coincidence that for all such latent variable models the equations for SE-step looks like Gibbs sampling. (Exploiting this coincidence, in fact, one can prove that parallel Gibbs update will work for such models, cf (Tassarotti & Steele Jr, 2015; Zaheer et al., 2016) whereas in general parallel Gibbs sampling fails miserably e.g. for Ising models.) Moreover, we found empirically that augmenting the SEM inference with a beam search improved the performance.\nAutomatic differentiation software packages such as TensorFlow (Abadi et al., 2015) significantly speed up development time, but have limited control structures and indexing. Whereas random sampling requires varied control statements, thus was implemented separately on the CPU. Furthermore, the SEM saved us precious GPU-CPU bandwidth by transmitting only an integer per token instead of a K-dimensional variational parameter."
  }, {
    "heading": "3.3. Adding Context",
    "text": "Utilizing the word or user action, wd,t, directly would be more informative to model the dynamics and predict the next topic. For example, rather than only knowing the previous action belonged to “electronics purchase” topic, knowing exactly that a camera was bought, makes it easier to predict user’s next interest, e.g. camera lenses.\nAlgorithm 1 Stochastic EM for LLA Input: Document corpus D. 1: Initialize φ and LSTM with a few iterations of LDA 2: repeat\nSE-Step: 3: for all document d ∈ D in parallel do 4: for t← 1 to Nd (possibly with padding) do 5: ∀k ∈ {1, · · · ,K}, i.e., for every topic index\nobtain by LSTM forward pass: πk = φwd,tkp(zd,t = k|zd,1:t−1;LSTM).\n6: Sample zd,t ∼ Categorical(π) 7: end for 8: end for\nM-Step: 9: Collect sufficient statistics to obtain:\nφwk = nwk + β\nnk + V β , ∀w, k\n10: for mini-batch of documents B ⊂ D do 11: Compute the gradient by LSTM backward pass\n∂L ∂LSTM = ∑ d∈B Nd∑ t=1 ∂ log p(zd,t|zd,1:t−1;LSTM) ∂LSTM\n12: Update LSTM parameters by stochastic gradient descent methods such as Adam (Kingma & Ba, 2014). 13: end for 14: until Convergence\nIn order to incorporate the exact context, we construct a variant of LLA, called word LLA, where the LSTM is provided with an embedding of previous word wd,t−1 directly instead of zd,t−1. The corresponding graphical model is shown in Figure 2b and the joint likelihood is:∑\nd\nlog p(wd|LSTM, φ)\n= ∑ d ∑ t log ∑ zd,t p(wd,t|zd,t;φ)p(zd,t|wd,1:t−1; LSTM)\nA SEM inference strategy can be devised similar to that of topic LLA as presented in section 3.2.\nHowever, this modification brings back the model to the dense and high number of parameter regime as a large trainable V ×H lookup table for the word embeddings is needed for the input transformation. This problem can be mitigated by using char-level LSTM (char LSTM for short) to construct the input transformation. Such character-level LSTM have recently shown promising results in generating structured text (Karpathy et al., 2015) as well as in producing semantically valid word embeddings with a model we will refer to as char-LSTM) (Ling et al., 2015). The characterlevel models do not need the huge lookup table for words, as they operate directly on the characters and the number of distinct characters is extremely small. We chose the latter as our input transformation and briefly describe it below.\nThe input word w is decomposed into a sequence of characters c1, . . . , cm, where m is the length of w. Each character ci is transformed using a lookup table and fed into a bidirectional LSTM The forward LSTM evolves on the character sequence and produces a final state hfm. While\nthe backward LSTM receives as input the reverse sequence, and yields its own final state hb0. Both LSTMs use a different set of parameters. The representation ecw of word w is obtained by combining forward and backward final states:\neCw = D fsfm + D bsb0 + bd, (4)\nwhere Df , Db and bd are parameters that determine how the states are combined. This ecw is provided to the topic level LSTM as the input providing information about the word. Thus we are able to incorporate more context information by providing some information about the previous word without the need to have great number of parameters. We call this model, char LLA.\nThe different variants of LLA and LSTM as language model can be unified and thought of having different input and out transformations over LSTM for capturing the dynamics. The possible combinations are listed in Table 1. We will study each of them empirically next."
  }, {
    "heading": "4. Experimental Results",
    "text": "We perform a comprehensive evaluation of our model against several deep models, dynamic models, and topic models. For reproducibility we focus on the task of language modeling over the publicly available Wikipedia dataset, and for generality, we show additional experiments on the less-structured domain of user modeling."
  }, {
    "heading": "4.1. Setup",
    "text": "For all experiments we follow the standard setup for evaluating temporal models, i.e. divide each document (user history) into 60% for training and 40% for testing. The task is to predict the remaining 40% of the document (user data) based on the representation inferred from the first 60% of the document. We use perplexity as our metric (lower values are better) and compare our models (topic-LLA, wordLLA, and char-LLA) against the following baselines: • Autoencoder: A feed forward neural network that com-\nprises of encoder and decoder. The encoder projects the input data into a low-dimensional representation and the decoder reconstructs the input from this representation. The model is trained to minimize reconstruction error.\n• LSTMs: We consider both word-level LSTM language model (word-LSTM) and character-level hierarchical LSTM (char-LSTM) language model.\n(a) Wikipedia (b) User search click history\nFigure 4. Test perplexity and number of parameters of various models. Bars represent model sizes and the solid curve represents perplexity over testset (lower is better). Wikipedia\nFigure 5. The effect of the number of topics over the performance of LDA and LLA.\n• LDA: The vanilla version trained using collapsed Gibbs sampling over the bag of word representation.\n• ddLDA: Distance-dependent LDA is a variant of LDA incorporating the word sequence. It is based on a fixeddimensional approximaiton of the distance-dependent Dirichlet process (Blei & Frazier, 2011). In this model a word is assigned to a topic based on the popularity of the topics assigned to nearby words. Note that this model subsumes other LDA-based temporal models such as hidden Markov topic Model (Andrews & Vigliocco, 2010) and RCRP based models (Ahmed & Xing, 2008).\nWe trained all deep models using stochastic gradient decent with Adam (Kingma & Ba, 2014). All LSTM and LLA based models used the sampled softmax method for efficiency (Cho et al., 2015). Finally, all hyper-parameters of the models were tuned over a development set."
  }, {
    "heading": "4.2. Language modeling",
    "text": "We used the publicly available Wikipedia dataset to evaluate the models on the task of sequence prediction. Extremely short documents, i.e. documents having length less than 500 words were excluded. The dataset contains ~0.7 billion tokens and ~1 million documents. The most frequent 50k word-types were retained as our vocabulary. Unless otherwise stated, we used 1000 topics for LLA and LDA variants. For LSTM and LLA variants, we selected the dimensions of the input embedding (word or topic) and evolving latent state (over words or topics) in the range of {50, 150, 250}. In case of character-based models, we tuned the dimensions of the character embedding and latent state (over characters) in the range of {50, 100, 150}.\nAccuracy vs model size Figure 4(a) compares model size in terms of number of parameters with model accuracy in terms of test perplexity. As shown in the figure, LDA yields the smallest model size due to the sparsity bias implicit in the Dirichlet prior over the topic|word distributions. On the other hand, word-LSTM gives the best accuracy due to its ability to utilize word level statistics; however, the model size is order of magnitudes larger than LDA. Char-LSTM achieves almost 50% reduction in model size over word-LSTM at the expense of a slightly worse perplexity. The figure also shows that LLA variants (topicLLA, word-LLA, and char-LLA) can trade accuracy vs model size while still maintaining interpretability since the output of the LSTM component is always at the topic level.\nNote that the figure depicts the smallest word-LSTM at this perplexity level, as our goal is not to beat LSTM in terms of accuracy, but rather to provide a tuning mechanism that can trade-off perplexity vs model size while still maintaining interpretability. Finally, compared to LDA, which is a widely used tool, LLA variants achieve a significant perplexity reduction at a modest increase in model size while still maintaining topic sparsity. As shown in Figure 1, the autoencoder model performs poorly in terms of model size and perplexity thus we eliminated it from Figure 4(a) and the following figures to avoid cluttering the display.\nConvergence over time At first glance, LLA seems to be more involved than both LDA and LSTM. So, we raise the question whether the added complexity leads to a slower training. Figure 8 shows that compared to LSTM based models, LLA variants achieve comparable convergence speed. Moreover, compared to fast LDA samplers (Zaheer et al., 2017), our LLA variants introduce only a modest increase in training time. The figure also shows that character based models (char-LSTM and char-LLA) are slightly slower to train compared to word level variants due to their nested nature and the need to propagate gradients over both word and character level LSTMs.\nAblation study Since both LDA and LLA result in interpretable models, we want to explore if LDA can achieve a perplexity similar to a given LLA model by just increasing the number of topics in LDA. Figure 5 shows the performance of LDA and variants of LLA for different number of topics. As can be seen from the figure, even with 250 topics, all LLA based models achieve much lower perplexity than LDA with 1000 topics. In other words, LLA improves over LDA not because it uses a slightly larger model, but rather because it models the sequential order in the data.\nInterpretability Last but not least, we demonstrate the interpretability of our models. Similar to LDA, the proposed LLA also uncovers global themes, a.k.a topics prevalent in the dataset. We found qualitatively the topics produced by LLA to be cleaner. For example, in Table 2 we show a topic about funding, charity, and campaigns recovered by both. LDA includes spurious words like Iowa in the topic just because it co-occurs in the same documents. Whereas modeling the temporal aspect allows LLA to correctly switch topics at different parts of the sentence producing cleaner topic regarding the same subject.\nModeling based on only co-occurrences in LDA leads to further issues. For example, strikes among mine workers are common, so the two words will co-occur heavily but it does not imply that strikes and mining should be in the same topic. LDA assign both the words in the same topic as shown in Table 3. Modeling sentences as an ordered sequence allows distinction between the subjects and objects in the sentence. In the previous example, this leads to factoring into two separate topics of strikes and mine workers.\nThe topic LLA provides embedding of the topics in a Euclidean metric space. This embedding allows us to understand the temporal structure learned by the model: topics close in this space are expected to appear in close sequential proximity in documents. To understand this, we built a topic hierarchy using the embeddings which uncovers interesting facts about the data. For example in Figure 6, we show a portion of the topic hierarchy discovered by topic-LLA with 1000 topics. For each topic we list the top few words. The theme of the figure is countries in Asia. It groups topics relating to one country together and puts topics related to neighboring countries close by. Three prominent clusters are shown form top to bottom which corresponds to moving from west to east on the map of Asia. Top cluster is about Arab world, second one represent the Indian sub-continent, and the third one starts with southeast Asia like Philippines. (The topic abs gma cbn represents TV station in south-east Asia.) The complete hierarchy of topic is very meaningful and can be viewed at http://manzil.ml/lla.html."
  }, {
    "heading": "4.3. User modeling",
    "text": "We use an anonymized sample of user search click history to measure the accuracy of different models on predicting users’ future clicks. An accurate model would enable better user experience by presenting the user with relevant content. The dataset is anonymized by removing all items appearing less than a given threshold, this results in a dataset of ~50 million tokens, and 40K vocabulary size. This domain is less structured than the language modeling task since users’ click patterns are less predictable than the sequence of words which follow definite syntactic rules. We used a setup similar to the one used in the experiments over the Wikipedia dataset for parameters ranges and selections.\nFigure 4(b) gives the same conclusion as Figure 4(a): LLA variants achieve significantly lower perplexity compared to LDA and at the same time they are considerably smaller models compared to LSTM. Furthermore, even compared\nto ddLDA, an improved variant of LDA, our proposed LLA achieves better perplexity. ddLDA models time by introducing smoothness constraints that bias future actions to be generated from recent topics; however, it does not possess a predictive action model. As a result, it can neither model the fact that “booking a flight” topic should not be repeated over a short course of time nor that “booking a hotel” topic would likely follow shortly, but not necessarily immediately, after “booking a flight” topic. LLA, on the other hand, is capable of capturing this intricate dynamics by modeling the evolution of user topics via an LSTM – an extremely powerful dynamic model. This point is more evident if we consider the following hand-crafted2 user click trace in context of the topics depicted in Figure 7:\ntheknot.com zola.com weddingwire.com www.bridalguide.com pinterest.com food.com doityourself.com .....\nThere are four topics represented and color-coded (best viewed in color): wedding (sixth topic from top), social media (fourth from top), food (third form bottom) and home improvement (second form top) – all in Figure 7. We asked each model to predict what would be the three most likely topics that would appear next in current user’s session. LDA predicted wedding as the top topic followed by a tie for the remaining topics. ddLDA, whose output is based on exponential decay, yields wedding as most likely, followed by doityourself topic, and then the food topic as expected. In contrast, LLA ranks the most likely three topics as: doityourself topic, houzz topic (top topic in Figure 7), and the wedding topic. This is indicative of LLA learning that once a user starts in the doitourself topic, the user will stay longer in this part of the tree for the task and wander in the nearby topics for a while. Moreover, LLA still remembers the wedding topic, and unlike other models, LLA did not erroneously pick neighbors of the wedding topic among the three most likely topics to follow.\nFinally, to demonstrate the interpretability of our model, in Figure 7, we show a portion of the topic hierarchy discovered by topic-LLA with 250 topics. For each topic we list the top few clicked items. The theme of the figure is wedding and various house chores. Three prominent clusters are shown from top to bottom. The top cluster is about house renovations and wedding. Second cluster captures\n2For privacy concerns, we construct an artificial example manually for illustration purposes.\nFigure 6. Topic embedding discovered form the Wikipedia dataset\nFigure 7. Topic embedding discovered form the user search click history\nFigure 8. Convergence speed for various models.\nmakeup, designer shoes, and fashion. The bottom cluster capture kitchen, cooking and gardening. It is clear form the hierarchy that LLA groups topics into the embedding space based on sequential proximity in the search click history."
  }, {
    "heading": "4.4. Effect of Joint Training",
    "text": "One might wonder what would happen if we first train LDA and then simply train LSTM over the topic assignments from the LDA. We refer to this baseline as “independent learner” since LDA and LSTM are trained independently. In Table 4, we compare its performance against LLA, which jointly learns the topics and the evolution dynamics. As we can see, joint training is significantly better, since the LSTM will fit the random errors introduced by the topic assignments inferred form the LDA model, and in fact LSTM will learn to be as good as the sequence produced by an LDA model (which is time-oblivious to start with). The effect is more pronounced in the user history data due to the unstructured nature of this domain."
  }, {
    "heading": "5. Related Works & Discussions",
    "text": "In this paper we present LLA: Latent LSTM allocation. LLA leverages the powerful dynamic modeling capability of LSTM without blowing up the number of parameters while adding interpretability to the model. We achieve this by shifting from modeling the temporal dynamics at the observed word level to modeling the dynamics at a higher level of abstraction: topics. As the number of topics K is much smaller than the number of words V , it can act as a knob that can trade-off accuracy vs model size. In the extreme case, if we allow K → V in LLA then we recover LSTM. Furthermore, the topics provide an informative embedding that can reveal interesting temporal relationship as shown in Figure 6 and 7 – which is a novel contribution to the best of our knowledge.\nOur work is related to various dynamic topic models, however, previous works like ddLDA (ddCRP in general) or hidden Markov topic models provide only limited modeling capabilities of the temporal dynamics. Specifically, they impose smoothness constraints: i.e. topic of a word is biased toward nearby topics. This constraint cannot learn a predictive action model, e.g. after “booking a flight” topic, the “booking a hotel” topic is likely to follow. Moreover, Internet users often wander between related activities, e.g “booking a hotel” topic will happen shortly after “booking a flight”, but not necessarily immediately as the user might have been interrupted by something else. Thus we need a much richer temporal model such as an LSTM. Our quantitative results against ddLDA confirm this claim.\nAnother similar work would be lda2vec (Moody, 2016), where LDA is combined with local context in a fixed window size. This provides a sparse, interpretable model but lacks modeling the temporal aspect. The model cannot be used in a predictive-action setting. Also the sparsity is only in terms of per document parameters, whereas it suffers from huge dense of size vocabulary times embedding size.\nAnother relevant recent work is Sentence Level Recurrent Topic Model (SLRTM) (Tian et al., 2016). However, this model cannot capture long-range temporal dependencies, as the topic for each sentence is still decided using an exchangeable (non-sequential) Dirichlet-multinomial scheme similar to the LDA. It might be able to preserve local sentence structure or grammar, but that is particularly not very useful for the task of user modeling. Furthermore, as it contains RNN that operates on the entire vocabulary (not topic as in our case), the SLTRM has lots of parameters.\nFinally our work is related to recent work in recurrent latent variable models (Chung et al., 2015; Gemici et al., 2017) where a recurrent model is endowed with latent variables to model variability in the input data. However, they mainly focused on continuous input space such as images and speech data which enables the use of variational autoencoder techniques to learn the model parameters. Whereas in this paper, we focus on discrete data that are not amenable to the standard variational autoencoder techniques and as such we developed an efficient SEM algorithm instead."
  }],
  "year": 2017,
  "references": [{
    "title": "TensorFlow: Large-scale machine learning",
    "authors": ["Fernanda", "Vinyals", "Oriol", "Warden", "Pete", "Wattenberg", "Martin", "Wicke", "Yu", "Yuan", "Zheng", "Xiaoqiang"],
    "venue": "on heterogeneous systems,",
    "year": 2015
  }, {
    "title": "Dynamic non-parametric mixture models and the recurrent chinese restaurant process: with applications to evolutionary clustering",
    "authors": ["Ahmed", "Amr", "Xing", "Eric"],
    "venue": "In Proceedings of the 2008 SIAM International Conference on Data Mining,",
    "year": 2008
  }, {
    "title": "Web-scale user modeling for targeting",
    "authors": ["M. Aly", "A. Hatch", "V. Josifovski", "V.K. Narayanan"],
    "venue": "In Conference on World Wide Web,",
    "year": 2012
  }, {
    "title": "The hidden markov topic model: A probabilistic model of semantic representation",
    "authors": ["Andrews", "Mark", "Vigliocco", "Gabriella"],
    "venue": "Topics in Cognitive Science,",
    "year": 2010
  }, {
    "title": "Latent dirichlet allocation",
    "authors": ["D. Blei", "A. Ng", "M. Jordan"],
    "venue": "Advances in Neural Information Processing Systems 14,",
    "year": 2002
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Cho", "Sébastien Jean Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"],
    "year": 2015
  }, {
    "title": "A recurrent latent variable model for sequential data",
    "authors": ["Chung", "Junyoung", "Kastner", "Kyle", "Dinh", "Laurent", "Goel", "Kratarth", "Courville", "Aaron C", "Bengio", "Yoshua"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Generative temporal models with memory",
    "authors": ["Gemici", "Mevlana", "Hung", "Chia-Chun", "Santoro", "Adam", "Wayne", "Greg", "Mohamed", "Shakir", "Rezende", "Danilo J", "Amos", "David", "Lillicrap", "Timothy"],
    "venue": "arXiv preprint arXiv:1702.04649,",
    "year": 2017
  }, {
    "title": "Long shortterm memory",
    "authors": ["Hochreiter", "Sepp", "Schmidhuber", "Jürgen"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Exploring the limits of language modeling",
    "authors": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"],
    "venue": "arXiv preprint arXiv:1602.02410,",
    "year": 2016
  }, {
    "title": "Visualizing and understanding recurrent networks",
    "authors": ["Karpathy", "Andrej", "Johnson", "Justin", "Fei-Fei", "Li"],
    "venue": "arXiv preprint arXiv:1506.02078,",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Activity sequence modelling and dynamic clustering for personalized e-learning",
    "authors": ["Köck", "Mirjam", "Paramythis", "Alexandros"],
    "venue": "User Modeling and User-Adapted Interaction,",
    "year": 2011
  }, {
    "title": "Recurrent neural networks for customer purchase prediction on twitter",
    "authors": ["Korpusik", "Mandy", "Sakaki", "Shigeyuki", "Chen", "Francine Chen Yan-Ying"],
    "venue": "CBRecSys 2016,",
    "year": 2016
  }, {
    "title": "Reducing the sampling complexity of topic models",
    "authors": ["Li", "Aaron Q", "Ahmed", "Amr", "Ravi", "Sujith", "Smola", "Alexander J"],
    "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,",
    "year": 2014
  }, {
    "title": "Finding function in form: Compositional character models for open vocabulary word representation",
    "authors": ["Ling", "Wang", "Luı́s", "Tiago", "Marujo", "Astudillo", "Ramón Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"],
    "venue": "arXiv preprint arXiv:1508.02096,",
    "year": 2096
  }, {
    "title": "A critical review of recurrent neural networks for sequence learning",
    "authors": ["Lipton", "Zachary C", "Berkowitz", "John", "Elkan", "Charles"],
    "venue": "arXiv preprint arXiv:1506.00019,",
    "year": 2015
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Mikolov", "Tomas", "Karafiát", "Martin", "Burget", "Lukas", "Cernockỳ", "Jan", "Khudanpur", "Sanjeev"],
    "venue": "In Interspeech,",
    "year": 2010
  }, {
    "title": "Mixing dirichlet topic models and word embeddings to make lda2vec",
    "authors": ["Moody", "Christopher E"],
    "venue": "arXiv preprint arXiv:1605.02019,",
    "year": 2016
  }, {
    "title": "The stochastic em algorithm: estimation and asymptotic results",
    "authors": ["Nielsen", "Søren Feodor"],
    "venue": "Bernoulli, pp. 457–489,",
    "year": 2000
  }, {
    "title": "Visual analysis of hidden state dynamics in recurrent neural networks",
    "authors": ["Strobelt", "Hendrik", "Gehrmann", "Sebastian", "Huber", "Bernd", "Pfister", "Hanspeter", "Rush", "Alexander M"],
    "venue": "arXiv preprint arXiv:1606.07461,",
    "year": 2016
  }, {
    "title": "Lstm neural networks for language modeling",
    "authors": ["Sundermeyer", "Martin", "Schlüter", "Ralf", "Ney", "Hermann"],
    "venue": "In Interspeech, pp",
    "year": 2012
  }, {
    "title": "Improved recurrent neural networks for session-based recommendations",
    "authors": ["Tan", "Yong Kiam", "Xu", "Xinxing", "Liu", "Yong"],
    "venue": "In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,",
    "year": 2016
  }, {
    "title": "Efficient training of lda on a gpu by mean-for-mode estimation",
    "authors": ["Tassarotti", "Joseph", "Steele Jr.", "Guy L"],
    "venue": "In 32nd International Conference on Machine Learning ICML,",
    "year": 2015
  }, {
    "title": "Sentence level recurrent topic model: Letting topics speak for themselves",
    "authors": ["Tian", "Fei", "Gao", "Bin", "Liu", "Tie-Yan"],
    "venue": "arXiv preprint arXiv:1604.02038,",
    "year": 2016
  }, {
    "title": "Graphical models, exponential families, and variational inference",
    "authors": ["Wainwright", "Martin J", "Jordan", "Michael I"],
    "venue": "Foundations and Trends® in Machine Learning,",
    "year": 2008
  }, {
    "title": "Exponential stochastic cellular automata for massively parallel inference",
    "authors": ["Zaheer", "Manzil", "Wick", "Michael", "Tristan", "Jean-Baptiste", "Smola", "Alex", "Steele Jr.", "Guy L"],
    "venue": "In AISTATS: 19th Intl. Conf. Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Fast sampling algorithms for sparse latent variable models",
    "authors": ["Zaheer", "Manzil", "Ahmed", "Amr", "Ravi", "Sujith", "Smola", "Alex"],
    "venue": "arXiv preprint,",
    "year": 2017
  }],
  "id": "SP:55470ea8858d18e5c7675d2e717e2a0cd9a1e319",
  "authors": [{
    "name": "Manzil Zaheer",
    "affiliations": []
  }, {
    "name": "Amr Ahmed",
    "affiliations": []
  }, {
    "name": "Alexander J Smola",
    "affiliations": []
  }],
  "abstractText": "Recurrent neural networks, such as long-short term memory (LSTM) networks, are powerful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korpusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across different user types, LSTMs require a large number of parameters, notwithstanding the simplicity of the underlying dynamics, rendering it uninterpretable, which is highly undesirable in user modeling. The increase in complexity and parameters arises due to a large action space in which many of the actions have similar intent or topic. In this paper, we introduce Latent LSTM Allocation (LLA) for user modeling combining hierarchical Bayesian models with LSTMs. In LLA, each user is modeled as a sequence of actions, and the model jointly groups actions into topics and learns the temporal dynamics over the topic sequence, instead of action space directly. This leads to a model that is highly interpretable, concise, and can capture intricate dynamics. We present an efficient Stochastic EM inference algorithm for our model that scales to millions of users/documents. Our experimental evaluations show that the proposed model compares favorably with several state-of-the-art baselines.",
  "title": "Latent LSTM Allocation  Joint Clustering and Non-Linear Dynamic Modeling of Sequential Data"
}