{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-of-theart performances, ranging the fields of computer vision (Krizhevsky et al., 2012; He et al., 2016; Schroff et al., 2015; Taigman et al., 2014), audio and natural language processing and generation (Collobert & Weston, 2008; Hinton et al., 2012; Graves et al., 2013; van den Oord et al., 2016), as well as robotics (Mnih et al., 2015; Schulman et al., 2015), to name just a few. The list of success stories can be matched and surpassed by a list of practical “tips and tricks”, from different optimization algorithms, parameter tuning methods (Sutskever et al., 2013; Kingma & Ba, 2014), initialization schemes (Glorot & Bengio, 2010), architecture designs (Szegedy et al., 2016), loss functions, data augmentation (Krizhevsky et al., 2012) and so on.\nThe current theoretical understanding of deep learning is far from being sufficient for a rigorous analysis of the difficulties faced by practitioners. Progress must be made from\n1School of Computer Science and Engineering, The Hebrew University 2Weizmann Institute of Science. Correspondence to: Shai Shalev-Shwartz <shais@cs.huji.ac.il>, Ohad Shamir <ohad.shamir@weizmann.ac.il>, Shaked Shammah <shaked.shammah@mail.huji.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nboth parties: from a practitioner’s perspective, emphasizing the difficulties provides practical insights to the theoretician, which in turn, supplies theoretical insights and guarantees, further strengthening and sharpening practical intuitions and wisdom. In particular, understanding failures of existing algorithms is as important as understanding where they succeed.\nOur goal in this paper is to present and discuss families of simple problems for which commonly used methods do not show as exceptional a performance as one might expect. We use empirical results and insights as a ground on which to build a theoretical analysis, characterising the sources of failure. Those understandings are aligned, and sometimes lead to, different approaches, either for an architecture, loss function, or an optimization scheme, and explain their superiority when applied to members of those families. Interestingly, the sources for failure in our experiment do not seem to relate to stationary point issues such as spurious local minima or a plethora of saddle points, a topic of much recent interest (e.g. (Dauphin et al., 2014; Choromanska et al., 2015)), but rather to more subtle issues, having to do with informativeness of the gradients, signal-to-noise ratios, conditioning etc. The code for running all our experiments is available online1. In this version, due to the lack of space, we focus on two families of failures, and briefly describe two others in Section 4. We refer the reader to (Shalev-Shwartz et al., 2017) for an extended version of this paper.\nWe start off in Section 2 by discussing a class of simple learning problems for which the gradient information, central to deep learning algorithms, provably carries negligible information on the target function which we attempt to learn. This result is a property of the learning problems themselves, and holds for any specific network architecture one may choose for tackling the learning problem, implying that no gradient-based method is likely to succeed. Our analysis relies on tools and insights from the Statistical Queries literature, and underscores one of the main deficiencies of Deep Learning: its reliance on local properties of the loss function, with the objective being of a global nature.\n1 https://github.com/shakedshammah/\nfailures_of_DL. See command lines in Appendix D.\nNext, in Section 3, we tackle the ongoing dispute between two common approaches to learning. Most, if not all, learning and optimization problems can be viewed as some structured set of sub-problems. The first approach, which we refer to as the “end-to-end” approach, will tend to solve all of the sub-problems together in one shot, by optimizing a single primary objective. The second approach, which we refer to as the “decomposition” one, will tend to handle these sub-problems separately, solving each one by defining and optimizing additional objectives, and not rely solely on the primary objective. The benefits of the end-to-end approach, both in terms of requiring a smaller amount of labeling and prior knowledge, and perhaps enabling more expressive architectures, cannot be ignored. On the other hand, intuitively and empirically, the extra supervision injected through decomposition is helpful in the optimization process. We experiment with a simple problem in which application of the two approaches is possible, and the distinction between them is clear and intuitive. We observe that an end-to-end approach can be much slower than a decomposition method, to the extent that, as the scale of the problem grows, no progress is observed. We analyze this gap by showing, theoretically and empirically, that the gradients are much more noisy and less informative with the end-to-end approach, as opposed to the decomposition approach, explaining the disparity in practical performance."
  }, {
    "heading": "2. Parities and Linear-Periodic Functions",
    "text": "Most existing deep learning algorithms are gradient-based methods; namely, algorithms which optimize an objective through access to its gradient w.r.t. some weight vector w, or estimates of the gradient. We consider a setting where the goal of this optimization process is to learn some underlying hypothesis class H, of which one member, h 2 H, is responsible for labelling the data. This yields an optimization problem of the form\nmin\nw\nF h (w).\nThe underlying assumption is that the gradient of the objective w.r.t. w, rF\nh (w), contains useful information regarding the target function h, and will help us make progress.\nBelow, we discuss a family of problems for which with high probability, at any fixed point, the gradient, rF\nh (w), will be essentially the same regardless of the underlying target function h. Furthermore, we prove that this holds independently of the choice of architecture or parametrization, and using a deeper/wider network will not help. The family we study is that of compositions of linear and periodic functions, and we experiment with the classical problem of learning parities. Our empirical and theoretical study shows that indeed, if there’s little information in the\ngradient, using it for learning cannot succeed."
  }, {
    "heading": "2.1. Experiment",
    "text": "We begin with the simple problem of learning random parities: After choosing some v⇤ 2 {0, 1}d uniformly at random, our goal is to train a predictor mapping x 2 {0, 1}d to y = ( 1)hx,v⇤i, where x is uniformly distributed. In words, y indicates whether the number of 1’s in a certain subset of coordinates of x (indicated by v⇤) is odd or even.\nFor our experiments, we use the hinge loss, and a simple network architecture of one fully connected layer of width 10d > 3d2 with ReLU activations, and a fully connected output layer with linear activation and a single unit. Note that this class realizes the parity function corresponding to any v⇤ (see Lemma 3 in the appendix).\nEmpirically, as the dimension d increases, so does the difficulty of learning, which can be measured in the accuracy we arrive at after a fixed number of training iterations, to the point where around d = 30, no advance beyond random performance is observed after reasonable time. Figure 1 illustrates the results."
  }, {
    "heading": "2.2. Analysis",
    "text": "To formally explain the failure from a geometric perspective, consider the stochastic optimization problem associated with learning a target function h,\nmin\nw\nF h (w) := E x [`(p w (x), h(x))] , (1)\nwhere ` is a loss function, x are the stochastic inputs (assumed to be vectors in Euclidean space), and p\nw is some predictor parametrized by a parameter vector w (e.g. a neural network of a certain architecture). We will assume that F is differentiable. A key quantity we will be interested in studying is the variance of the gradient of F with respect to h, when h is drawn uniformly at random from a collection\nof candidate target functions H:\nVar(H, F,w) = E h rFh(w) E h 0 rF h 0 (w)\n2\n(2)\nIntuitively, this measures the expected amount of “signal” about the underlying target function contained in the gradient. As we will see later, this variance correlates with the difficulty of solving (1) using gradient-based methods2.\nThe following theorem bounds this variance term.\nTheorem 1 Suppose that\n• H consists of real-valued functions h satisfying E x\n[h2(x)]  1, such that for any two distinct h, h0 2 H, E\nx\n[h(x)h0(x)] = 0.\n• p w\n(x) is differentiable w.r.t. w, and satisfies E x ⇥k @ @w p w (x)k2⇤  G(w)2 for some scalar G(w). • The loss function ` in (1) is either the square loss `(ŷ, y) = 12 (ŷ y)2 or a classification loss of the form `(ŷ, y) = r(ŷ · y) for some 1-Lipschitz function r, and the target function h takes values in {±1}.\nThen\nVar(H, F,w)  G(w) 2\n|H| .\nThe proof is given in Appendix B.1. The theorem implies that if we try to learn an unknown target function, possibly coming from a large collection of uncorrelated functions, then the sensitivity of the gradient to the target function at any point decreases linearly with |H|. Before we make a more general statement, let us return to the case of parities, and study it through the lens of this framework. Suppose that our target function is some parity function chosen uniformly at random, i.e. a random element from the set of 2d functions H = {x 7! ( 1)hx,v⇤i : v\n⇤ 2 {0, 1}d}. These are binary functions, which are easily seen to be mutually orthogonal: Indeed, for any v,v0,\nE x\nh ( 1)hx,vi( 1)hx,v0i i = E\nx\nh ( 1)hx,v+v0i i\n=\ndY\ni=1\nE h ( 1)xi(vi+v0i) i = dY\ni=1\n( 1)vi+v0i + ( 1) (vi+v0i) 2\nwhich is non-zero if and only if v = v0. Therefore, by Theorem 1, we get that Var(H, F,w)  G(w)2/2d – that is, exponentially small in the dimension d. By Chebyshev’s inequality, this implies that the gradient at any point w will\n2This should not be confused with the variance of gradient estimates used by SGD, which we discuss in Section 3.\nbe extremely concentrated around a fixed point independent of h.\nThis phenomenon of exponentially-small variance can also be observed for other distributions, and learning problems other than parities. Indeed, in (Shamir, 2016), it was shown that this also holds in a more general setup, when the output y corresponds to a linear function composed with a periodic one, and the input x is sampled from a smooth distribution:\nTheorem 2 (Shamir 2016) Let be a fixed periodic function, and let H = {x 7! (v⇤>x) : kv⇤k = r} for some r > 0. Suppose x 2 Rd is sampled from an arbitrary mixture of distributions with the following property: The square root of the density function ' has a Fourier transform '̂ satisfying R x:kxk>r '̂\n2(x)dx R x '̂ 2(x)dx  exp( ⌦(r)). Then if"
  }, {
    "heading": "F denotes the objective function with respect to the squared loss,",
    "text": "Var(H, F,w)  O (exp( ⌦(d)) + exp( ⌦(r))) .\nThe condition on the Fourier transform of the density is generally satisfied for smooth distributions (e.g. arbitrary Gaussians whose covariance matrices are positive definite, with all eigenvalues at least ⌦(1/r)). Thus, the bound is extremely small as long as the norm r and the dimension d are moderately large, and indicates that the gradients contains little signal on the underlying target function.\nBased on these bounds, one can also formally prove that a gradient-based method, under a reasonable model, will fail in returning a reasonable predictor, unless the number of iterations is exponentially large in r and d 3 . This provides strong evidence that gradient-based methods indeed cannot learn random parities and linear-periodic functions. We emphasize that these results hold regardless of which class of predictors we use (e.g. they can be arbitrarily complex neural networks) – the problem lies in using a gradientbased method to train them. Also, we note that the difficulty lies in the random choice of v⇤, and the problem is not difficult if v⇤ is known and fixed in advance (for example, for a full parity v⇤ = (1, . . . , 1), this problem is known to be solvable with an appropriate LSTM network (Hochreiter & Schmidhuber, 1997)).\nFinally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning (Kearns, 1998; Blum et al., 1994). This refers to algorithms which are constrained to interact with\n3Formally, this requires an oracle-based model, where given a point w, the algorithm receives the gradient at w up to some arbitrary error much smaller than machine precision. See (Shamir, 2016), Theorem 4, for details.\ndata by receiving estimates of the expected value of some query over the underlying distribution (e.g. the expected value of the first coordinate), and it is well-known that parities cannot be learned with such algorithms. Recently, (Feldman et al., 2015) have formally shown that gradientbased methods with an approximate gradient oracle can be implemented as a statistical query algorithm, which implies that gradient-based methods are indeed unlikely to solve learning problems which are known to be hard in the statistical queries framework, in particular parities. In the discussion on random parities above, we have simply made the connection between gradient-based methods and parities more explicit, by direct examination of gradients’ variance w.r.t. the target function."
  }, {
    "heading": "3. Decomposition vs. End-to-end",
    "text": "Many practical learning problems, and more generally, algorithmic problems, can be viewed as a structured composition of sub-problems. Applicable approaches for a solution can either be tackling the problem in an end-to-end manner, or by decomposition. Whereas for a traditional algorithmic solution, the “divide-and-conquer” strategy is an obvious choice, the ability of deep learning to utilize big data and expressive architectures has made “end-to-end training” an attractive alternative. Prior results of end-toend (Mnih et al., 2015; Graves et al., 2013) and decomposition and added feedback (Gülçehre & Bengio, 2016; Hinton & Salakhutdinov, 2006; Szegedy et al., 2015; Caruana, 1998) approaches show success in both directions. Here, we try to address the following questions: What is the price of the rather appealing end-to-end approach? Is letting a network “learn by itself” such a bad idea? When is it necessary, or worth the effort, to “help” it?\nThere are various aspects which can be considered in this context. For example, (Shalev-Shwartz & Shashua, 2016) analyzed the difference between the approaches from the sample complexity point of view. Here, we focus on the optimization aspect, showing that an end-to-end approach might suffer from non-informative or noisy gradients, which may significantly affect the training time. Helping the SGD process by decomposing the problem leads to much faster training. We present a simple experiment, motivated by questions every practitioner must answer when facing a new, non trivial problem: What exactly is the required training data, what network architecture should be used, and what is the right distribution of development efforts. These are all correlated questions with no clear answer. Our experiments and analysis show that making the wrong choice can be expensive.\nFigure 2. Section 3.1’s experiment - examples of samples from X . The y values of the top and bottom rows are 1 and 1, respectively."
  }, {
    "heading": "3.1. Experiment",
    "text": "Our experiment compares the two approaches in a computer vision setting, where convolutional neural networks (CNN) have become the most widely used and successful algorithmic architectures. We define a family of problems, parameterized by k 2 N, and show a gap (rapidly growing with k) between the performances of the end-to-end and decomposition approaches.\nLet X denote the space of 28 ⇥ 28 binary images, with a distribution D defined by the following sampling procedure:\n• Sample ✓ ⇠ U([0,⇡]), l ⇠ U([5, 28 5]), (x, y) ⇠ U([0, 27])2.\n• The image x ✓,l,(x,y) associated with the above sample\nis set to 0 everywhere, except for a straight line of length l, centered at (x, y), and rotated at an angle ✓. Note that as the images space is discrete, we round the values corresponding to the points on the lines to the closest integer coordinate.\nLet us define an “intermediate” labeling function y : X ! {±1}, denoting whether the line in a given image slopes upwards or downwards, formally:\ny(x ✓,l,(x,y)) = ( 1 if ✓ < ⇡/2 1 otherwise .\nFigure 2 shows a few examples. We can now define the problem for each k. Each input instance is a tuple xk1 := (x1, . . . ,xk) of k images sampled i.i.d. as above. The target output is the parity over the image labels y(x1), . . . , y(xk), namely ỹ(xk1) = Q j=1...k y(xj).\nMany architectures of DNN can be used for predicting ỹ(xk1) given xk1 . A natural “high-level” choice can be:\n• Feed each of the images, separately, to a single CNN (of some standard specific architecture, for example, LeNet-like), denoted N (1)\nw1 and parameterized by its weights vector w1, outputting a single scalar, which can be regarded as a “score”.\n• Concatenate the “scores” of a tuple’s entries, transform them to the range [0, 1] using a sigmoid function, and feed the resulting vector into another network, N (2)\nw2 , of a similar architecture to the one defined in Section 2, outputting a single “tuple-score”, which can then be thresholded for obtaining the binary prediction.\nLet the whole architecture be denoted N w . Assuming that N (1) is expressive enough to provide, at least, a weak learner for y (a reasonable assumption), and that N (2) can express the relevant parity function (see Lemma 3 in the appendix), we obtain that this architecture has the potential for good performance.\nThe final piece of the experimental setting is the choice of a loss function. Clearly, the primary loss which we’d like to minimize is the expected zero-one loss over the prediction, N\nw\n(x\nk 1), and the label, ỹ(xk1), namely:\n˜L0 1(w) := E x\nk 1\n⇥ N\nw\n(x\nk 1) 6= ỹ(xk1) ⇤\nA “secondary” loss which can be used in the decomposition approach is the zero-one loss over the prediction of N (1)\nw1 (x k 1) and the respective y(xk1) value:\nL0 1(w1) := E x\nk 1\nh N (1)\nw1 (x\nk 1) 6= y(xk1) i\nLet ˜L,L be some differentiable surrogates for ˜L0 1, L0 1. A classical end-to-end approach will be to minimize ˜L, and only it; this is our “primary” objective. We have no explicit desire for N (1) to output any specific value, and hence L is, a priori, irrelevant. A decomposition approach would be to minimize both losses, under the assumption that L can “direct” w1 towards an “area” in which we know that the resulting outputs of N (1) can be separated by N (2). Note that using L is only possible when the y values are known to us.\nEmpirically, when comparing performances based on the “primary” objective, we see that the end-to-end approach is significantly inferior to the decomposition approach (see Figure 3). Using decomposition, we quickly arrive at a good solution, regardless of the tuple’s length, k (as long as k is in the range where perfect input to N (2) is solvable by SGD, as described in Section 2). However, using the end-to-end approach works only for k = 1, 2, and completely fails already when k = 3 (or larger). This may\nbe somewhat surprising, as the end-to-end approach optimizes exactly the primary objective, composed of two subproblems each of which is easily solved on its own, and with no additional irrelevant objectives."
  }, {
    "heading": "3.2. Analysis",
    "text": "We study the experiment from two directions: Theoretically, by analyzing the gradient variance (as in Section 2), for a somewhat idealized version of the experiment, and empirically, by estimating a signal-to-noise ratio (SNR) measure of the stochastic gradients used by the algorithm. Both approaches point to a similar issue: With the end-toend approach, the gradients do not seem to be sufficiently informative for the optimization process to succeed.\nBefore continuing, we note that a conceptually similar experiment to ours has been reported in (Gülçehre & Bengio, 2016) (also involving a composition of an image recognition task and a simple Boolean formula, and with qualitatively similar results). However, that experiment came without a formal analysis, and the failure was attributed to local minima. In contrast, our analysis indicates that the problem is not due to local-minima (or saddle points), but from the gradients being non-informative and noisy.\nWe begin with a theoretical result, which considers our experimental setup under two simplifying assumptions: First, the input is assumed to be standard Gaussian, and second, we assume the labels are generated by a target function of the form h\nu\n(x\nk 1) = Q k l=1 sign(u > x l\n). The first assumption is merely to simplify the analysis (similar results can\nbe shown more generally, but the argument becomes more involved). The second assumption is equivalent to assuming that the labels y(x) of individual images can be realized by a linear predictor, which is roughly the case for simple image labelling task such as ours.\nTheorem 3 Let xk1 denote a k-tuple (x1, . . . ,xk) of input instances, and assume that each x\nl is i.i.d. standard Gaussian in Rd. Define\nh u (x k\n1) =\nkY\nl=1\nsign(u>x l ),\nand the objective (w.r.t. some predictor p w parameterized by w)\nF (w) = E x\nk 1\n⇥ `(p\nw\n(x\nk 1), hu(x k 1) ⇤ .\nWhere the loss function ` is either the square loss `(ŷ, y) = 1 2 (ŷ y)2 or a classification loss of the form `(ŷ, y) = r(ŷ · y) for some 1-Lipschitz function r. Fix some w, and suppose that p\nw (x) is differentiable w.r.t. w and satisfies E\nx k 1 ⇥k @ @w p w (x k 1k2 ⇤  G(w)2. Then if\nH = {h u : u 2 Rd, kuk = 1}, then\nVar(H, F,w)  G(w)2 ·O r k log(d)\nd\n! k\n.\nThe proof is given in Appendix B.2. The theorem shows that the “signal” regarding h\nu (or, if applying to our experiment, the signal for learning N (1), had y been drawn uniformly at random from some set of functions over X) decreases exponentially with k. This is similar to the parity result in Section 2, but with an important difference: Whereas the base of the exponent there was 1/2, here it is the much smaller quantity k log(d)/ p d (e.g. in our experiment, we have k  4 and d = 282). This indicates that already for very small values of k, the information contained in the gradients about u can become extremely small, and prevent gradient-based methods from succeeding, fully according with our experiment.\nTo complement this analysis (which applies to an idealized version of our experiment), we consider a related “signalto-noise” (SNR) quantity, which can be empirically estimated in our actual experiment. To motivate it, note that a key quantity used in the proof of Theorem 3, for estimating the amount of signal carried by the gradient, is the squared norm of the correlation between the gradient of the predictor p\nw , g(xk1) := @\n@w\np w (x k 1) and the target function hu, which we denote by Sig\nu\n:\nSig u := E x\nk 1\n⇥ h u (x k 1)g(x k 1) ⇤\n2\n.\n1 2 3 4\nWe will consider the ratio between this quantity and a “noise” term Noi\nu , i.e. the variance of this correlation over the samples:\nNoi u := E x\nk 1\nhu(x k 1)g(x k 1) E x\nk 1\n⇥ h u (x k 1)g(x k 1) ⇤\n2\n.\nSince here the randomness is with respect to the data rather than the target function (as in Theorem 3), we can estimate this SNR ratio in our experiment. It is well-known (e.g. (Ghadimi & Lan, 2013)) that the amount of noise in the stochastic gradient estimates used by stochastic gradient descent crucially affects its convergence rate. Hence, smaller SNR should be correlated with worse performance.\nWe empirically estimated this SNR measure, Sig y /Noi y , for the gradients w.r.t. the weights of the last layer of N (1) (which potentially learns our intermediate labeling function y) at the initialization point in parameter space. The SNR estimate for various values of k are plotted in Figure 4. We indeed see that when k 3, the SNR appears to approach extremely small values, where the estimator’s noise, and the additional noise introduced by a finite floating point representation, can completely mask the signal, which can explain the failure in this case.\nIn Section A in the Appendix, we also present a second, more synthetic, experiment, which demonstrates a case where the decomposition approach directly decreases the stochastic noise in the SGD optimization process, hence benefiting the convergence rate."
  }, {
    "heading": "4. Additional Failure Families - Brief Discussion",
    "text": "In an extended version of this paper, (Shalev-Shwartz et al., 2017), we broadly discuss two additional families of failures. Here, due to lack of space, we present them briefly.\nFirst, we demonstrate the importance of both the network’s\narchitecture and the optimization algorithm on the training time. While the choice of architecture is usually studied in the context of its expressive power, we show that even when two architectures have the same expressive power for a given task, there may be a tremendous difference in the ability to optimize them. We analyze the required runtime of gradient descent for the two architectures through the lens of the condition number of the problem. We further show that conditioning techniques can yield additional orders of magnitude speedups. The experimental setup for this problem is around a seemingly simple problem — encoding a piece-wise linear one-dimensional curve. A summary of experimental results, when training with different architectures and conditioning techniques, is found in Figure 5. Despite the simplicity of this problem, we show that following the common rule of “perhaps I should use a deeper/wider network”4 does not significantly help here.\nFinally, we consider deep learning’s reliance on “vanilla” gradient information for the optimization process. We previously discussed the deficiency of using a local property of the objective in directing global optimization. We turn our focus to a simple case in which it is possible to solve the optimization problem based on local information, but not in the form of a gradient. We experiment with architectures that contain activation functions with flat regions, which leads to the well known vanishing gradient problem. Practitioners take great care when working with such activation functions, and many heuristic tricks are applied in order to initialize the network’s weights in non-flat areas of its activations. Here, we show that by using a different update rule, we manage to solve the learning problem efficiently. Moreover, one can show convergence guarantees for a family of such functions. This provides a clean example where non-gradient-based optimization schemes can overcome the limitations of gradient-based learning."
  }, {
    "heading": "5. Summary",
    "text": "In this paper, we considered different families of problems, where standard gradient-based deep learning approaches appear to suffer from significant difficulties. Our analysis indicates that these difficulties are not necessarily related to stationary point issues such as spurious local minima or a plethora of saddle points, but rather more subtle issues: Insufficient information in the gradients about the underlying target function; low SNR; bad conditioning; or flatness in the activations (see Figure 6 for a graphical illustration). We consider it as a first step towards a better understanding of where standard deep learning methods might fail, as well as what approaches might overcome these failures.\n4See http://joelgrus.com/2016/05/23/ fizz-buzz-in-tensorflow/ for the inspiration behind this quote."
  }],
  "year": 2017,
  "references": [{
    "title": "Multitask learning. In Learning to learn, pp. 95–133",
    "authors": ["Caruana", "Rich"],
    "year": 1998
  }, {
    "title": "The loss surfaces of multilayer networks",
    "authors": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "Gérard Ben", "LeCun", "Yann"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Collobert", "Ronan", "Weston", "Jason"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Statistical query algorithms for stochastic convex optimization",
    "authors": ["Feldman", "Vitaly", "Guzman", "Cristobal", "Vempala", "Santosh"],
    "venue": "arXiv preprint arXiv:1512.09170,",
    "year": 2015
  }, {
    "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
    "authors": ["Ghadimi", "Saeed", "Lan", "Guanghui"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["Glorot", "Xavier", "Bengio", "Yoshua"],
    "venue": "In Aistats,",
    "year": 2010
  }, {
    "title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing",
    "authors": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"],
    "venue": "(icassp), 2013 ieee international conference on,",
    "year": 2013
  }, {
    "title": "Knowledge matters: Importance of prior information for optimization",
    "authors": ["Gülçehre", "Çalar", "Bengio", "Yoshua"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2016
  }, {
    "title": "Reducing the dimensionality of data with neural networks",
    "authors": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"],
    "year": 2006
  }, {
    "title": "Long shortterm memory",
    "authors": ["Hochreiter", "Sepp", "Schmidhuber", "Jürgen"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Efficient noise-tolerant learning from statistical queries",
    "authors": ["Kearns", "Michael"],
    "venue": "Journal of the ACM (JACM),",
    "year": 1998
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "year": 2012
  }, {
    "title": "Facenet: A unified embedding for face recognition and clustering",
    "authors": ["Schroff", "Florian", "Kalenichenko", "Dmitry", "Philbin", "James"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2015
  }, {
    "title": "Trust region policy optimization",
    "authors": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"],
    "venue": "In ICML, pp",
    "year": 2015
  }, {
    "title": "On the sample complexity of end-to-end training vs. semantic abstraction training",
    "authors": ["Shalev-Shwartz", "Shai", "Shashua", "Amnon"],
    "venue": "arXiv preprint arXiv:1604.06915,",
    "year": 2016
  }, {
    "title": "Failures of deep learning",
    "authors": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Shammah", "Shaked"],
    "venue": "arXiv preprint arXiv:1703.07950,",
    "year": 2017
  }, {
    "title": "Distribution-specific hardness of learning neural networks",
    "authors": ["Shamir", "Ohad"],
    "venue": "arXiv preprint arXiv:1609.01037,",
    "year": 2016
  }, {
    "title": "On the importance of initialization and momentum in deep learning",
    "authors": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George E", "Hinton", "Geoffrey E"],
    "venue": "ICML (3),",
    "year": 2013
  }, {
    "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
    "authors": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"],
    "venue": "arXiv preprint arXiv:1602.07261,",
    "year": 2016
  }, {
    "title": "Deepface: Closing the gap to human-level performance in face verification",
    "authors": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc’Aurelio", "Wolf", "Lior"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2014
  }, {
    "title": "Wavenet: A generative model for raw audio",
    "authors": ["van den Oord", "Aäron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"],
    "venue": "CoRR abs/1609.03499,",
    "year": 2016
  }],
  "id": "SP:1c2576c7316516c385fb5c2f9f3faefafeaf8845",
  "authors": [{
    "name": "Shai Shalev-Shwartz",
    "affiliations": []
  }, {
    "name": "Ohad Shamir",
    "affiliations": []
  }, {
    "name": "Shaked Shammah",
    "affiliations": []
  }],
  "abstractText": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradientbased algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.",
  "title": "Failures of Gradient-Based Deep Learning"
}