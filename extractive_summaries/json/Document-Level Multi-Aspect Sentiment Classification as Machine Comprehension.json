{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2044–2054 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Document-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010). There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc. Most of reviews are very comprehensive and thus long documents. Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management. Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016). However, predicting an overall score for each long document is not enough, because the document can mention dif-\nferent aspects of the corresponding product or service. For example, in Figure 1, there could be different aspects for a review of hotel. These aspects help customer service better understand what are the major pros and cons of the product or service. Compared to the overall rating, users are less motivated to give aspect ratings. Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.\nOne straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997). For neural networks, we can simply treat each aspect (e.g., rating from one to five) as a classification task, and let different tasks use softmax classifier to extract task-specific representations at the top layer while share the input and hidden layers to mutually enhance the prediction results (Collobert et al., 2011; Luong et al., 2016). However, such approach ignores the fact that the aspects themselves have semantic meanings. For example, as human beings, if we were asked to evaluate the aspect rating of a document, we simply read the review, and find aspect-related keywords, and see around comments. Then, we aggregate all the related snippets to make a decision.\nIn this paper, we propose a novel approach to treat document-level multi-aspect sentiment clas-\n2044\nsification as a machine comprehension (Kumar et al., 2016; Sordoni et al., 2016) problem. To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect. For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc. Then we can ask pseudo questions: “How is the room?” “How is the bed?” “How is the view?” and provide an answer “Rating 5.” In this case, we can train a machine comprehension model to automatically attend corresponding text snippets in the review document to predict the aspect rating. Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations. We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions. At each level, the model consists of input encoders and iterative attention modules. The input encoder learns memories1 of documents and questions with Bi-directional LSTM (Bi-LSTM) model and non-linear mapping respectively. The iterative attention module takes into memories as input and attends them sequentially with a multiple hop mechanism, performing effective interactions between documents and aspect questions.\nTo evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines. We also analyze the effects of num-\n1Following the work (Weston et al., 2015; Sukhbaatar et al., 2015), we refer the memory to a set vectors which are stacked together and could be attended.\nbers of the hop and aspect words on performances. Moreover, a case study for attention results is performed at both word and sentence levels.\nThe contributions of this paper are two-fold. First, we study the document-level multi-aspect sentiment classification as a machine comprehension problem and introduce a hierarchical iterative attention model for it. Second, we demonstrate the effectiveness of proposed model on two datasets, showing that our model outperforms classical baselines. The code and data for this paper are available at https://github.com/ HKUST-KnowComp/DMSCMC."
  }, {
    "heading": "2 Method",
    "text": "In this section, we introduce our proposed method."
  }, {
    "heading": "2.1 Problem Definition and Hierarchical Framework",
    "text": "We first briefly introduce the problem we work on. Given a piece of review, our task is to predict the ratings of different aspects. For example, in Figure 1, we predict the ratings of Cleanliness, Room, and Value. To achieve this, we assume that there are existing reviews with aspect ratings for machines to learn. Formally, we denote the review document as d containing a set of Td sentences {s1, s2, . . . sTd}. For the t-th sentence st, we use a set of words { w1, w2, . . . w|st| } to represent it, and use wi, wwi and w p i as the one-hot encoding, word embedding, and phrase embedding for wi respectively. The phrase embedding encodes the semantics of phrases where the current word wi is the center (e.g., hidden vectors learned by Bi-LSTM shown in Section 2.2). For each qk of K aspects\n{q1, q2, . . . , qK}, we use Nk aspect-related keywords, { qk1 , qk2 . . . qkNk } , to represent it. Similarly, we use qki , q w ki\nas the one-hot encoding and word embedding for qki respectively.\nThere are several sophisticated methods for choosing aspect keywords (e.g., topic model). Here, we consider a simple way where five seeds were first manually selected for each aspect and then more words were obtained based on their cosine similarities with seeds2\nAs shown in Figure 2 (left), our framework follows the idea of multi-task learning, which learns different aspects simultaneously. In this case, all these tasks share the representations of words and architecture of semantic model for the final classifiers. Different from straightforward neural network based multi-task learning (Collobert et al., 2011), for each document d and an aspect qk, our model uses both the content of d and all the related keywords { qk1 , qk2 . . . qkNk } as input. Since the keywords can cover most of the semantic meanings of the aspect, and we do not know which document mentions which semantic meaning, we build an attention model to automatically decide it (introduced in Section 2.3). Assuming that the keywords have been decided, we use a hierarchical attention model to select useful information from the review documents. As shown in Figure 2 (right), the hierarchical attention of keywords is applied to both sentence level (to select meaningful words) and document level (to select meaningful sentence). Thus, our model builds aspectspecific representations in a bottom-up manner.\nSpecifically, we obtain sentence representations { sk1, sk2, . . . skT } using the input encoder (Section 2.2) and iterative attention module (Section 2.3) at the word level. Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification. As shown in Figure 2 (right), the attention model is applied twice at different levels of the representation."
  }, {
    "heading": "2.2 Input Encoder",
    "text": "The input module builds memory vectors for the iterative attention module and is performed both at word and sentence levels. For a document, it con-\n2For example, the words “value,” “price,” “worth,” “cost,” and “$” are selected as seeds for aspect Price. The information for seeds can be found in our released resource.\nverts word sequence into word level memory Mdw and sentence sequence into sentence level memory Mds respectively. For an aspect question qk, it takes a set of aspect-specific words {qki}1≤i≤Nk as input and derives word level memory Mqw and sentence level memory Mqs.\nTo construct Mdw, we obtain word embeddings{ ww1 , ww2 , . . . ww|st| } from an embedding matrix EA applied to all words shown in the corpus. Then, LSTM (Hochreiter and Schmidhuber, 1997) model is used as the encoder to produce hidden vectors of words based on the word embeddings. At each step, LSTM takes input wwt and derives a new hidden vector by ht = LSTM(wwt , ht−1). To preserve the subsequent context information for words, another LSTM is ran over word sequence in a reverse order simultaneously. Then the forward hidden vector −→ h t and backward hidden\nvector ←− h t are concatenated as phrase embedding wpt . We stack these phrase embeddings together as word level memory Mdw. Similarly, we feed sentence representations into another Bi-LSTM to derive the sentence level memory Mds . Note that, the sentence representations are obtained using the iterative attention module which is described as Eq. (5) in Section 2.3.\nSince we have question keywords as input, to allow the interactions between questions and documents, we also build question memory in following way. We obtain Qk = { qwki }\n1≤i≤Nk by looking up an embedding matrix 3 EB applied to all question keywords. Then a non-linear mapping is applied to obtain the question memory at word level:\nMqkw = tanh(QkW q w), (1)\nwhere Wqw is the parameter matrix to adapt qk at word level. Similarly, we use another mapping to obtain the sentence level memory:\nMqks = tanh(QkW q s), (2)\nwhere Wqs is the parameter matrix to adapt qk at sentence level."
  }, {
    "heading": "2.3 Iterative Attention Module",
    "text": "The iterative attention module (IAM) attends and reads memories of questions and documents alternatively with a multi-hop mechanism, deriving\n3EA and EB are initialized by the same pre-trained embeddings but are different embedding matrices with different updates.\naspect-specific sentence and document representations. As we discussed in the introduction, the set of selected question keywords may not best characterize the aspect for different documents. Thus, the IAM module introduces a backward attention to use document information (word or sentence) to select useful keywords of each aspect as the document-specific question to build attention model.\nThe illustration of IAM is shown in Figure 3. To obtain sentence representations, it takes Mdw and Mqw as the input and performs m iterations (hops). For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vectors into a single vector q̂; (2) updates the selective vector by the previous one and q̂; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vector ĉ; (4) updates the selective vector by the previous one and ĉ.\nWe unify operations (1) and (3) by an attention function x̂ = A(p, M), where M could be Mdw or Mqw which corresponds x̂ = ĉ or x̂ = q̂. The attention function A is decomposed as:\nH = tanh(MWa (1p)) a = softmax(HvTa )\nx̂ = ∑ aiMi,\n(3)\nwhere 1 is a vector with all elements are 1, which copies the selective vector to meet the dimension requirement. The Wa and va are parameters, a is attention weights for memory vectors, and Mi\nmeans i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively. We initialize p0 by a zero vector. The update function U can be a recurrent neural network (Xiong et al., 2017) or other heuristic weighting functions. In this paper, we introduce a simple strategy:\np2i−{l} = x̂, (4)\nwhich ignores the previous selective vector but succeeds to obtain comparable results with other more complicated function in the initial experiments.\nMulti-hop mechanism attends different memory locations in different hops (Sukhbaatar et al., 2015), capturing different interactions between documents and questions. In order to preserve the information of various kinds of interactions, we concatenate all ĉ’s in each hop as the final representations of sentences:\ns = [ĉ1; ĉ2; · · · ĉm]. (5) After obtaining sentence representations, we feed them into the sentence-level input encoder, deriving the memories Mds and Mqs. Then, the aspect-specific document representation dk is obtained by the sentence-level IAM in a similar way."
  }, {
    "heading": "2.4 Objective Function",
    "text": "For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K . All these representations are fed into classifiers, each of which includes a softmax layer. The softmax layer outputs the probability distribution over |Y| categories for the distributed representation, which is defined as:\np′(d, k) = softmax(Wclassk dk), (6)\nwhere Wclassk is the parameter matrix. We define the cross-entropy objective function between gold sentiment distribution p(d, k) and predicted sentiment distribution p′(d, k) as the classification loss function:\n− ∑ d∈D K∑ k=1 |Y|∑ i=1 p(d, k)log(p′(d, k)), (7)\nwhere p(d, k) is a one-hot vector, which has the same dimension as the number of classes, and only the dimension associated with the ground truth label is one, with others being zeros."
  }, {
    "heading": "3 Experiment",
    "text": "In this section, we show experimental results to demonstrate our proposed algorithm."
  }, {
    "heading": "3.1 Datasets",
    "text": "We conduct our experiments on TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016) datasets, which contain seven aspects (value, room, location, cleanliness, check in/front desk, service, and business service) and four aspects (feel, look, smell, and taste) respectively. We follow the processing step (Lei et al., 2016) by choosing the reviews with different aspect ratings and the new datasets are described in Table 1. We tokenize the datasets by Stanford corenlp4 and randomly split them into training, development, and testing sets with 80/10/10%."
  }, {
    "heading": "3.2 Baseline Methods",
    "text": "To demonstrate the effectiveness of the proposed method, we compare our model with following baselines:\nMajority uses the majority sentiment label in development sets as the predicted label.\nSVM uses unigram and bigram as text features and uses Liblinear (Fan et al., 2008) for learning.\nSLDA refers to supervised latent Dirichlet allocation (Blei and Mcauliffe, 2010) which is a statistical model of labeled documents.\nNBoW is a neural bag-of-words model averaging embeddings of all words in a document and feeds the resulted embeddings into SVM classifier.\nDAN is a deep averaging network model which consists of several fully connected layers with averaged word embeddings as input. One novel word dropout strategy is employed to boost model performances (Iyyer et al., 2015).\nCNN continuously performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer (Kim, 2014).\n4http://nlp.stanford.edu/software/corenlp.shtml\nLSTM is one variant of recurrent neural network and has been proved to be one of state-ofthe-art models for document-level sentiment classification (Tang et al., 2015a). We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information.\nHAN means the hierarchical attention network which is proposed in (Yang et al., 2016) for document classification. Note that, the original HAN depends GRU as the encoder. In our experiments, LSTM-based HAN obtains slightly better results. Thus, we report the results of HAN with LSTM as the encoder.\nWe extend DAN, CNN, LSTM with the hierarchical architecture and multi-task framework, the corresponding models are MHDAN, MHCNN and MHLSTM respectively. Besides, MHAN is also evaluated as one baseline, which is HAN with the multi-task learning."
  }, {
    "heading": "3.3 Implementation Details",
    "text": "We implement all neural models using Theano (Theano Development Team, 2016). The model parameters are tuned based on the development sets. We learn 200-dimensional word embeddings with Skip-gram model (Mikolov et al., 2013) on in-domain corpus, which follows (Tang et al., 2015a). The pre-trained word embeddings are used to initialize the embedding matrices EA and EB . The dimensions of all hidden vectors are set to 200. For TripAdvisor dataset, the hop numbers of word-level and sentence-level iterative attention modules are set to 4 and 2 respectively. For BeerAdvocate dataset, the hop numbers are set to 6 and 2. The number of selected keywords Nk = N is set to 20. To avoid model over-fitting, we use dropout and regularization as follows: (1) the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied to both sentence and document vectors. All parameters are trained by ADADELTA (Zeiler, 2012) without needing to set the initial learning rate. To ensure fair comparisons, we make baselines have same settings as the proposed model, such as word embeddings, dimensions of hidden vectors and optimization details and so on."
  }, {
    "heading": "3.4 Results and Analyses",
    "text": "We use accuracy and mean squared error (MSE) as the evaluation metrics and the results are shown in Table 2.\nCompared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets. All neural network models outperform NBoW. It shows the advantages of neural networks in the document sentiment classification.\nFrom the results of neural networks, we can observe that DAN performs worse than LSTM and CNN, and LSTM achieves slightly higher results than CNN. It can be explained that the simple composition method averaging embeddings of words in a document but ignoring word order, may not be as effective as other flexible composition models, such as LSTM and CNN, on aspect classification. Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks. Among all baselines, MHAN and MHLSTM achieve comparable results and outperform others.\nCompared with MHAN and MHLSTM, our method achieves improvements of 1.5% (3% relative improvement) and 1.0% (2.5% relative improvement) on TripAdvisor and BeerAdvocate re-\nspectively, which shows that the incorporation of iterative attention mechanism helps the deep neural network based model build up more discriminative aspect-aware representation. Note that BeerAdvocate is relatively more difficult since the predicted ratings are from 1 to 10 while TripAdvisor is 1 to 5. Moreover, t-test is conducted by randomly splitting datasets into train/dev/test sets and random initialization. The results on test sets are described in Table 3 which show performance of our model is stable."
  }, {
    "heading": "3.5 Case Study for Attention Results",
    "text": "In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study. Both word-level and sentence-level attention visualizations are shown in Figure 4. We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.\nFrom the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect. For example, in the first sentence, the words comfortable and bed are assigned higher\nweights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness. In the second sentence, the word internet is assigned a high attention value for Business. Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable. Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops. In the second sentence, we observe that the value of word internet increases with the number of hop. Thus, we can see that the more sensible weights are obtained for words through the proposed iterative attention mechanism. Similarly, the figures (c) and (d) show that the conclusion from words is also suitable for sentences. For the first sentence, the sentence weight regarding the aspect Room is lower than Cleanliness in the first hop, but surpasses Cleanliness in the second hop. For the second sentence, the weight for Business becomes higher in the second hop."
  }, {
    "heading": "3.6 Effects of Hop and Aspect Keywords",
    "text": "In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances. All the experiments are conducted\non the development set. Due to lack of space, we only present the results of TripAdvisor and the results of BeerAdvocate have a similar behavior as TripAdvisor.\nFor the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left). We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2. Moreover, we can see that the hop number of word level leads to larger variation than the hop number of sentence level.\nFor the size of aspect keywords, we vary N from 0 to 35, incremented by 5. Note that, we set a learnable vector to represent question memory when N = 0. The results are shown in Figure 5 (right). We observe that the performance increases when N ≤ 20, and has no improvement after N > 20. This indicates that a small number of keywords can help the proposed model achieve competitive results."
  }, {
    "heading": "4 Related Work",
    "text": "Multi-Aspect Sentiment Classification. Multiaspect sentiment classification has been studied extensively in literature. Lu et al. (2011) used support vector regression model based on hand-\ncrafted features to predict aspect ratings. To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective. There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014). However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect. Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments. With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work. Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair. However, these tasks are sentencelevel. Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task. This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure. However, they did not work on multiple aspects.\nMachine Comprehension. Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016). Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words. Hermann et al. (2015) used BiLSTM to encode document and query, and proposed Attentive Reader and Impatient Reader. The first one attends document based on the query representation, and the second one attends document by the representation of each token in query with an incremental manner. Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015) attend and reason document representation in a multihop fashion, enriching interactions between documents and questions. Dynamic Memory Network (Kumar et al., 2016) updates memories of documents by re-running GRU models based on derived attention weights. Meanwhile, the query representation is refined by another GRU model. Gated-Attention Reader (Dhingra et al., 2016) proposes a novel attention mechanism, which is based on multiplicative interactions between the query embeddings and the intermediate states of a recurrent neural network document reader. BiDirectional Attention Model (Xiong et al., 2017; Seo et al., 2017) fuses co-dependent representations of queries and documents in order to focus on relevant parts of both. Iterative Attention model (Sordoni et al., 2016) attends question and document sequentially, which is related to our model. Different from Iterative Attention model, our model focuses on the document-level multiaspect sentiment classification, which is proposed\nin a hierarchical architecture and has different procedures in the iterative attention module. Another related research problem is visual question answering which uses an image as question context rather than a set of keywords as question. Neural network based visual question answering (Lu et al., 2016; Xiong et al., 2016) is similar as the proposed models in text comprehension."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model. Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture."
  }, {
    "heading": "6 Acknowledgments",
    "text": "This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61472006 and 91646202) as well as the National Basic Research Program (973 Program No. 2014CB340405). This work was also supported by NVIDIA Corporation with the donation of the Titan X GPU, Hong Kong CERG Project 26206717, China 973 Fundamental R&D Program (No.2014CB340304), and the LORELEI Contract HR0011-15-2-0025 with DARPA. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript."
  }],
  "year": 2017,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of ICLR.",
    "year": 2014
  }, {
    "title": "A feature dependent method for opinion mining and classification",
    "authors": ["Alexandra Balahur", "Andres Montoyo."],
    "venue": "Natural Language Processing and Knowledge Engineering. pages 1–7.",
    "year": 2008
  }, {
    "title": "Supervised topic models",
    "authors": ["David M. Blei", "Jon D. Mcauliffe."],
    "venue": "Advances in Neural Information Processing Systems 3:327–332.",
    "year": 2010
  }, {
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Machine Learning 28(1):41–75.",
    "year": 1997
  }, {
    "title": "Neural sentiment classification with user and product attention",
    "authors": ["Huimin Chen", "Maosong Sun", "Cunchao Tu", "Yankai Lin", "Zhiyuan Liu."],
    "venue": "Proceedings of EMNLP. pages 1650–1659.",
    "year": 2016
  }, {
    "title": "Aspect extraction with automated prior knowledge learning",
    "authors": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu."],
    "venue": "ACL. pages 347–358.",
    "year": 2014
  }, {
    "title": "Exploiting domain knowledge in aspect extraction",
    "authors": ["Zhiyuan Chen", "Arjun Mukherjee", "Bing Liu", "Meichun Hsu", "Malu Castellanos", "Riddhiman Ghosh."],
    "venue": "EMNLP. pages 1655–1667.",
    "year": 2013
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Gated-attention readers for text comprehension",
    "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1606.01549 .",
    "year": 2016
  }, {
    "title": "Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)",
    "authors": ["Qiming Diao", "Minghui Qiu", "Chao-Yuan Wu", "Alexander J Smola", "Jing Jiang", "Chong Wang."],
    "venue": "Proceedings of KDD. ACM, pages 193–202.",
    "year": 2014
  }, {
    "title": "Liblinear: A library for large linear classification",
    "authors": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."],
    "venue": "Journal of machine learning research 9(Aug):1871–1874.",
    "year": 2008
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Proceedings of NIPS. pages 1693–1701.",
    "year": 2015
  }, {
    "title": "The goldilocks principle: Reading children’s books with explicit memory representations",
    "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of ICLR.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Deep unordered composition rivals syntactic methods for text classification",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daumé III."],
    "venue": "Proceedings of ACL. pages 1681–1691.",
    "year": 2015
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of EMNLP. pages 1746–1751.",
    "year": 2014
  }, {
    "title": "Ask me anything: Dynamic memory networks for natural language processing",
    "authors": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."],
    "venue": "Proceedings of ICML.",
    "year": 2016
  }, {
    "title": "Aspect specific sentiment analysis using hierarchical deep learning",
    "authors": ["Himabindu Lakkaraju", "Richard Socher", "Chris Manning."],
    "venue": "NIPS Workshop on Deep Learning and Representation Learning.",
    "year": 2014
  }, {
    "title": "Rationalizing neural predictions",
    "authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Proceedings of EMNLP. Austin, Texas, pages 107–117.",
    "year": 2016
  }, {
    "title": "Sentiment analysis and subjectivity",
    "authors": ["Bing Liu."],
    "venue": "Handbook of Natural Language Processing, Second Edition., pages 627–666.",
    "year": 2010
  }, {
    "title": "Multi-aspect sentiment analysis with topic models",
    "authors": ["Bin Lu", "Myle Ott", "Claire Cardie", "Benjamin K Tsou."],
    "venue": "ICDM Workshops. IEEE, pages 81–88.",
    "year": 2011
  }, {
    "title": "Hierarchical question-image coattention for visual question answering",
    "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."],
    "venue": "Proceedings of NIPS. pages 289–297.",
    "year": 2016
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "Proceedings of ICLR.",
    "year": 2016
  }, {
    "title": "Learning attitudes and attributes from multiaspect reviews",
    "authors": ["Julian McAuley", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "Proceedings of ICDM. IEEE, pages 1020–1025.",
    "year": 2012
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Proceedings of NIPS. pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Opinion mining and sentiment analysis",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Foundations and Trends in Information Retrieval 2(1-2):1–135.",
    "year": 2007
  }, {
    "title": "Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis",
    "authors": ["Nikolaos Pappas", "Andrei Popescu-Belis."],
    "venue": "Proceedings of EMNLP. pages 455–466.",
    "year": 2014
  }, {
    "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
    "authors": ["Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."],
    "venue": "Proceedings of SemEval. pages 27–35.",
    "year": 2014
  }, {
    "title": "A rule-based approach to aspect extraction from product reviews",
    "authors": ["Soujanya Poria", "Erik Cambria", "Lun-Wei Ku", "Chen Gui", "Alexander Gelbukh."],
    "venue": "Proceedings of the second workshop on natural",
    "year": 2014
  }, {
    "title": "Sentic lda: Improving on lda with semantic similarity for aspect-based sentiment analysis",
    "authors": ["Soujanya Poria", "Iti Chaturvedi", "Erik Cambria", "Federica Bisio."],
    "venue": "International Joint Conference on Neural Networks. pages 4465–4473.",
    "year": 2016
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Proceedings of EMNLP. pages 2383–2392.",
    "year": 2016
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "Proceedings of ICLR .",
    "year": 2017
  }, {
    "title": "Iterative alternating neural attention for machine reading",
    "authors": ["Alessandro Sordoni", "Philip Bachman", "Adam Trischler", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1606.02245 .",
    "year": 2016
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"],
    "venue": "In Proceedings of NIPS",
    "year": 2015
  }, {
    "title": "Document modeling with gated recurrent neural network for sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "EMNLP. pages 1422– 1432.",
    "year": 2015
  }, {
    "title": "Learning semantic representations of users and products for document level sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "ACL. pages 1014–1023.",
    "year": 2015
  }, {
    "title": "Aspect level sentiment classification with deep memory network",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proceedings of EMNLP. pages 214–224.",
    "year": 2016
  }, {
    "title": "Theano: A Python framework for fast computation of mathematical expressions",
    "authors": ["Theano Development Team."],
    "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
    "year": 2016
  }, {
    "title": "A joint model of text and aspect ratings for sentiment summarization",
    "authors": ["Ivan Titov", "Ryan T McDonald."],
    "venue": "Proceedings of ACL. Citeseer, volume 8, pages 308–316.",
    "year": 2008
  }, {
    "title": "Latent aspect rating analysis on review text data: a rating regression approach",
    "authors": ["Hongning Wang", "Yue Lu", "Chengxiang Zhai."],
    "venue": "Proceedings of KDD. ACM, pages 783–792.",
    "year": 2010
  }, {
    "title": "Memory networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "Proceedings of ICLR.",
    "year": 2015
  }, {
    "title": "Dynamic memory networks for visual and textual question answering",
    "authors": ["Caiming Xiong", "Stephen Merity", "Richard Socher."],
    "venue": "Proceedings of ICML. pages 1378–1387.",
    "year": 2016
  }, {
    "title": "Dynamic coattention networks for question answering",
    "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher."],
    "venue": "Proceedings of ICLR .",
    "year": 2017
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "Proceedings of NAACL-HLT . pages 1480–1489.",
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701 .",
    "year": 2012
  }],
  "id": "SP:2b6384bfa90dd68ff49335e75c825d667c654165",
  "authors": [{
    "name": "Yichun Yin",
    "affiliations": []
  }, {
    "name": "Yangqiu Song",
    "affiliations": []
  }, {
    "name": "Ming Zhang",
    "affiliations": []
  }],
  "abstractText": "Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo questionanswer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.",
  "title": "Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension"
}