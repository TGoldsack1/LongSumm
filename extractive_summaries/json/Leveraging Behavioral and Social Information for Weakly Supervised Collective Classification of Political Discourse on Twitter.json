{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 741–752 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1069"
  }, {
    "heading": "1 Introduction",
    "text": "The importance of understanding political discourse on social media platforms is becoming increasingly clear. In recent U.S. presidential elections, Twitter was widely used by all candidates to promote their agenda, interact with supporters, and attack their opponents. Social interactions on such platforms allow politicians to quickly react to current events and gauge interest in and support for their actions. These dynamic settings emphasize the importance of constructing automated tools for analyzing this content. However, these same dynamics make constructing such tools difficult, as the language used to discuss new events and political agendas continuously changes. Consequently, the rich social interactions on Twitter can be leveraged to help support such analysis by providing alternatives to direct supervision.\nIn this paper we focus on political framing, a very nuanced political discourse analysis task, on\na variety of issues frequently discussed on Twitter. Framing (Entman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue. For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue. While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase. Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles. Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.). These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter.\nTwitter issue framing is a challenging multilabel prediction task. Each tweet can be labeled as using one or more frames, out of 17 possibilities, while only providing 140 characters as input to the classifier. The main contribution of this work is to evaluate whether the social and behavioral information available on Twitter is sufficient for constructing a reliable classifier for this task. We approach this framing prediction task using a weakly supervised collective classification approach which leverages the dependencies between tweet frame predictions based on the interactions between their authors.\nThese dependencies are modeled by connecting Twitter users who have social connections or behavioral similarities. Social connections are di-\n741\nrected dependencies that represent the followers of each user as well as retweeting behavior (i.e., user A retweets user B’s content). Interestingly, such social connections capture the flow of influence within political parties; however, the number of connections that cross party lines is extremely low. Instead, we rely on capturing behavioral similarity between users to provide this information. For example, users whose Twitter activity peaks at similar times tend to discuss issues in similar ways, providing indicators of their frame usage for those issues. In addition to using social and behavioral information, our approach also incorporates each politician’s party affiliation and the frequent phrases (e.g., bigrams and trigrams) used by politicians on Twitter.\nThese lexical, social, and behavioral features are extracted from tweets via weakly supervised models and then declaratively compiled into a graphical model using Probabilistic Soft Logic (PSL), a recently introduced probabilistic modeling framework.1 As described in Section 4, PSL specifies high level rules over a relational representation of these features. These rules are then compiled into a graphical model called a hingeloss Markov random field (Bach et al., 2013), which is used to make the frame prediction. Instead of direct supervision we take a bootstrapping approach by providing a small seed set of keywords adapted from Boydstun et al. (2014), for each frame.\nOur experiments show that modeling social and behavioral connections improves F1 prediction scores in both supervised and unsupervised settings, with double the increase in the latter. We apply our unsupervised model to our entire dataset of tweets to analyze framing patterns over time by both party and individual politicians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party."
  }, {
    "heading": "2 Related Work",
    "text": "Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card\n1http://psl.cs.umd.edu\net al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter.\nIn recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012).\nExploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013).\nPredicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls\n2http://alt.qcri.org/semeval2016/ task6/\nbased on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012).\nSeveral works from political and social science research have studied the role of Twitter and framing in shaping public opinion of certain events, e.g. the Vancouver riots (Burch et al., 2015) and the Egyptian protests (Harlow and Johnson, 2011; Meraz and Papacharissi, 2013). Others have covered framing and sentiment analysis of opponents (Groshek and Al-Rawi, 2013) and network agenda modeling (Vargo et al., 2014) in the 2012 U.S. presidential election. Jang and Hart (2015) studied frames used by the general population specific to global warming. In contrast to these works, we predict the issue-independent general frames of tweets, by U.S. politicians, which discuss six different policy issues."
  }, {
    "heading": "3 Data Collection and Annotation",
    "text": "Data Collection and Preprocessing: We collected 184,914 of the most recent tweets of members of the U.S. Congress (both the House of Representatives and Senate). Using an average of ten keywords per issue, we filtered out tweets not related to the following six issues of interest: (1) limiting or gaining access to abortion, (2) debates concerning the Affordable Care Act (i.e., ACA or Obamacare), (3) the issue of gun rights versus gun control, (4) effects of immigration policies, (5) acts of terrorism, and (6) issues concerning the LGBTQ community. Forty politicians (10 Republicans and 10 Democrats, from both the House and Senate), were chosen randomly for annotation. Table 1 presents the statistics of our congressional tweets dataset, which is available for the community.3 Appendix A contains more details of our dataset and preprocessing steps.\nData Annotation: Two graduate students were trained in the use of the Policy Frames Codebook developed by Boydstun et al. (2014) for annotating each tweet with a frame. The general aspects of each frame are shown in Table 2. Frames are designed to generalize across issues and overlap of multiple frames is possible. Additionally, the Codebook is typically applied to newspaper ar-\n3The dataset and PSL scripts are available at: http://purduenlp.cs.purdue.edu/projects/ twitterframing.\nticles where discussion of policy can encompass other frames in the text. Consequently, annotators using the Codebook are advised to be careful when assigning Frame 13 to a text.\nBased on this guidance and the difficulty of labeling tweets (as discussed in Card et al. (2015)), annotators were instructed to use the following procedure: (1) attempt to assign a primary frame to the tweet if possible, (2) if not possible, assign two frames to the tweet where the first frame is chosen as the more accurate of the two frames, (3) when assigning frames 12 through 17, double check that the tweet cannot be assigned to any other frames. Annotators spent one month labeling the randomly chosen tweets. For all tweets with more than one frame, annotators met to come to a consensus on whether the tweet should have one frame or both. The labeled dataset has an inter-annotator agreement, calculated using Cohen’s Kappa statistic, of 73.4%.\nExtensions of the Codebook for Twitter Use: The first 14 frames outlined in Table 2 are directly applicable to the tweets of U.S. politicians. In our labeled set, Frame 15 (Other) was never used. Therefore, we drop its analysis from this paper. From our observations, we propose the addition of the 3 frames at the bottom of Table 2 for Twitter analysis: Factual, (Self) Promotion, and Personal Sympathy and Support. Tweets that present a fact, with no detectable political spin or twists, are labeled as having the Factual frame (15). Tweets that discuss a politician’s appearances, speeches, statements, or refer to political friends are considered to have the (Self) Promotion frame. Finally, tweets where a politician offers their “thoughts and prayers”, condolences, or stands in support of others, are considered to have the Personal frame.\nWe find that for many tweets, one frame is not enough. This is caused by the compound nature of many tweets, e.g., some tweets are two separate sentences, with each sentence having a different frame or tweets begin with one frame and end with another. A final problem, that may also be relevant to longer text articles, is that of subframes within a larger frame. For example, the tweet “We must bolster the security of our borders and craft an immigration policy that grows our economy.” has two frames: Security & Defense and Economic. However, both frames could fall under Frame 13 (Policy), if this tweet as a whole was a rebuttal point about an immigration policy. The lack of\navailable context for short tweets can make it difficult to determine if a tweet should have one primary frame or is more accurately represented by multiple frames."
  }, {
    "heading": "4 Global Models of Twitter Language and Activity",
    "text": "Due to the dynamic nature of political discourse on Twitter, our approach is designed to require as little supervision as possible. We implement 6 weakly supervised models which are datadependent and used to extract and format information from tweets into input for PSL predicates. These predicates are then combined into the probabilistic rules of each model as shown in Table 3. The only sources of supervision these models require includes: unigrams related to the issues, unigrams adapted from the Boydstun et al. (2014) Codebook for frames, and political party of the author of the tweets."
  }, {
    "heading": "4.1 Global Modeling Using PSL",
    "text": "PSL is a declarative modeling language which can be used to specify weighted, first-order logic rules. These rules are compiled into a hinge-loss Markov random field which defines a probability distribution over possible continuous value assignments to the random variables of the model (Bach et al.,\n2015).4 This probability density function is represented as:\nP (Y | X) = 1 Z exp\nMX\nr=1\nr r(Y , X)\n!\nwhere Z is a normalization constant, is the weight vector, and\nr(Y, X) = (max{lr(Y, X), 0})⇢r\nis the hinge-loss potential specified by a linear function lr. The exponent ⇢r 2 1, 2 is optional. Each potential represents the instantiation of a rule, which takes the following form:\n1 : P1(x) ^ P2(x, y) ! P3(y) 2 : P1(x) ^ P4(x, y) ! ¬P3(y)\nP1, P2, P3, and P4 are predicates (e.g., political party, issue, frame, and presence of n-grams) and x, y are variables. Each rule has a weight which reflects that rule’s importance and is learned using the Expectation-Maximization algorithm in our unsupervised experiments. Using concrete constants a, b (e.g., tweets and words) which instantiate the variables x, y, model atoms are mapped\n4Unlike other probabilistic logical models, e.g. MLNs, in which the model’s random variables are strictly true or false.\nto continuous [0,1] assignments. More important rules (i.e., those with larger weights) are given preference by the model."
  }, {
    "heading": "4.2 Language Based Models",
    "text": "Unigrams: Using the guidelines provided in the Policy Frames Codebook (Boydstun et al., 2014), we adapted a list of expected unigrams for each frame. For example, unigrams that should be related to Frame 12 (Political Factors & Implications) include: filibuster, lobby, Democrats, Republicans. We expect that if a tweet and frame contain a matching unigram, then that frame is likely present in that tweet. The information that tweet T has expected unigram U of frame F is represented with the PSL predicate: UNIGRAMF (T, U). This knowledge is then used as input to PSL Model 1 via the rule: UNIGRAMF (T, U) !FRAME(T, F) (shown in line 1 of Table 3).\nHowever, not every tweet will have a unigram that matches those in this list. Under the intuition that at least one unigram in a tweet should be similar to a unigram in the list, we designed the following MaxSim metric to compute the maximum similarity between a word in a tweet and a word from the list of unigrams.\nMAXSIM(T, F) = arg max u2F,w2T SIMILARITY(W,U)\n(1) T is a tweet, W is each word in T, and U is each unigram in the list of expected unigrams (per frame). SIMILARITY is the computed word2vec similarity (using pretrained embeddings) of each word in the tweet with every unigram in the list of unigrams for each frame. The frame F of the maximum scoring unigram is input to the PSL predicate: MAXSIMF (T, F), which indicates that tweet T has the highest similarity to frame F.\nBigrams and Trigrams: In addition to unigrams, we also explored the effects of political party slogans on frame prediction. Slogans are common catch phrases or sayings that people typically associate with different U.S. political parties. For example, Republicans are known for using the phrase “repeal and replace” when they discuss the ACA. Similarly, in the 2016 U.S. presidential election, Secretary Hillary Clinton’s campaign slogan became “Love Trumps Hate”. To visualize slogan usage by parties for different issues, we used the entire tweets dataset, including all unlabeled tweets, to extract the top bigrams\nand trigrams per party for each issue. The histograms in Figure 1 show these distributions for the top 100 bigrams and trigrams. Based on these results, we use the top 20 bigrams (e.g., women’s healthcare and immigration reform) and trigrams (e.g. prevent gun violence) as input to PSL predicates BIGRAMIP (T, B) and TRIGRAMIP (T, TG). These rules represent that tweet T has bigram B or trigram TG from the respective issue I phrase lists of either party P."
  }, {
    "heading": "4.3 Twitter Behavior Based Models",
    "text": "In addition to language based features of tweets, we also exploit the behavioral and social features of Twitter including similarities between temporal activity and network relationships.\nTemporal Similarity: We construct a temporal histogram for each politician which captures their Twitter activity over time. When an event happens politicians are most likely to tweet about that event within hours of its occurrence. Similarly, most politicians tweet about the event most frequently the day of the event and this frequency decreases over time. From these temporal histograms, we observed that the frames used the day of an event were similar and gradually changed over time. For example, once the public is notified of a shooting, politicians respond with Frame 17 to offer sympathy to the victims and their families. Over the next days or weeks, both parties slowly transition to using additional frames, e.g. Democrats use Frame 7 to argue for gun control legislation. To capture this behavior we use the PSL predicate SAMETIME(T1, T2). This indicates that tweet T1 occurs around the same time as tweet\nT2.5 This information is used in Model 4 via rules such as: SAMETIME(T1, T2) & FRAME(T1, F) !FRAME(T2, F), as shown in line 4 of Table 3.\nNetwork Similarity: Finally, we expect that politicians who share ideologies, and thus are likely to frame issues similarly, will retweet and/or follow each other on Twitter. Due to the compound nature of tweets, retweeting with additional comments can add more frames to the original tweet. Additionally, politicians on Twitter are more likely to follow members of their own party or similar non-political entities than those of the opposing party. To capture this network-based behavior we use two PSL predicates: RETWEETS(T1, T2) and FOLLOWS(T1, T2). These predicates indicate that the content of tweet T1 includes a retweet of tweet T2 and that the author of T1 follows the author of T2 on Twitter, respectively. The last two lines of Table 3 show examples of how network similarity is incorporated into PSL rules."
  }, {
    "heading": "5 Experiments",
    "text": "Evaluation Metrics: Since each tweet can have more than one frame, our prediction task is a multilabel classification task. The precision of a multilabel model is the ratio of how many predicted labels are correct:\nPrecision = 1\nT\nTX\nt=1\n|Yt \\ h(xt)| |h(xt)|\n(2)\nThe recall of this model is the ratio of how many of the actual labels were predicted:\nRecall = 1\nT\nTX\nt=1\n|Yt \\ h(xt)| |Yt|\n(3)\n5We conducted experiments with different hour and day limits and found that using a time frame of one hour results in the best accuracy while limiting noise.\nIn both formulas, T is the number of tweets, Yt is the true label for tweet t, xt is a tweet example, and h(xt) are the predicted labels for that tweet. The F1 score is computed as the harmonic mean of the precision and recall. Additionally, in Tables 4, 5, and 6 the reported average is the micro-weighted average F1 scores over all frames.\nExperimental Settings: We provide an analysis of our PSL models under both supervised and unsupervised settings. In the PSL supervised experiments, we used five-fold cross validation with randomly chosen splits.\nPrevious works typically use an SVM, with bagof-words features, which is not used in a multilabel prediction, i.e., each frame is predicted individually. The results of this approach on our dataset are shown in column 2 of Table 4. In this scenario, the SVM tends to prefer the majority class, which results in many incorrect labels. Column 3 shows the results of using an SVM with bag-of-words features to perform multilabel classification. This approach decreases the F1 score for a majority of frames. Both SVMs also result in F1 scores of 0 for some frames, further lowering the overall performance. Finally, columns 4 and 5 show the results of using our worst and best PSL models, respectively. PSL Model 1, which uses our adapted unigram features instead of the bag-of-words features for multilabel classification, serves as our baseline to improve upon. Additionally, Model 6 of the supervised, collective network setting represents the best results we can achieve.\nWe also explore the results of our PSL models in an unsupervised setting because the highly dynamic nature of political discourse on Twitter makes it unrealistic to expect annotated data to generalize to future discussions. The only source of supervision comes from the initial unigrams lists and party information as described in Section 4. The labeled tweets are used for evaluation only. As seen in Table 4, we are able to improve\nthe best unsupervised model to within an F1 score of 7.36 points of the unigram baseline of 66.02, and 19.13 points of the best supervised score of 77.79.\nAnalysis of Supervised Experiments: Table 5 shows the results of our supervised experiments. Here we can see that by adding Twitter behavior (beginning with Model 4), our behaviorbased models achieve the best F1 scores across all frames. Model 4 achieves the highest results on two frames, suggesting retweeting and network follower information do not help improve the prediction score for these frames. Similarly, Model 5 achieves the highest prediction for 5 of the frames, suggesting network follower information cannot further improve the score for these frames. Overall, the Twitter behavior based models are able to outperform language based models alone, including the best performing language model (Model 3) which combines unigrams, bigrams, and trigrams together to collectively infer the correct frames.\nAnalysis of Unsupervised Experiments: In the unsupervised setting, Model 6, the combination of language and Twitter behavior features achieves the best results on 16 of the 17 issues, as shown in Table 6. There are a few interesting aspects of the unsupervised setting which differ from the supervised setting. Six of the frame predictions do worse in Model 2, which is double that of the supervised version. This is likely due to the presence of overlapping bigrams across frames and issues, e.g., “women’s healthcare” could appear in both Frames 4 and 8 and the issues of ACA and abortion. However, all six are able to improve with the addition of trigrams (Model 3), whereas only 1 of 3 frames improves in the supervised setting. This suggests that bigrams may not be as useful as trigrams in an unsupervised setting. Finally, in Model 5, which adds retweet behaviors, we notice that 5 of the frames decrease in F1 score and 11\nof the frames have the same score as the previous model. These results suggest that retweet behaviors are not as useful as the follower network relationships in an unsupervised setting."
  }, {
    "heading": "6 Qualitative Analysis",
    "text": "To explore the usefulness of frame identification in political discourse analysis, we apply our best performing model (Model 6) on the unlabeled dataset to determine framing patterns over time, both by party and individual. Figure 2 shows the results of our frame analysis for both parties over time for two issues: ACA and terrorism.6 We compiled the predicted frames for tweets from 2014 to 2016 for each party. Figure 3 presents the results of frame prediction for 2015 tweets of aisle-crossing individual politicians for these two issues.\nParty Frames: From Figure 2(a) we can see that Democrats mainly use Frames 1, 4, 8, 9, and 15 to discuss ACA, while Figure 2(c) shows that Republicans predominantly use Frames 1, 8, 9, 12, and 13. Though the parties use similar frames, they are used to express different agendas. For example, Democrats use Frame 8 to indicate the positive effect that the ACA has had in granting more Americans health care access. Republicans, however, use Frame 8 (and Frame 13) to indicate their party’s agenda to replace the ACA with access to different options for health care. Additionally, Democrats use the Fairness & Equality Frame (Frame 4) to convey that the ACA gives minority groups a better chance at accessing health care.\n6Due to space, we omit the other 4 issues. These 2 were chosen because they are among the most frequently discussed issues in our dataset.\nThey also use Frame 15 to express statistics about enrollment of Americans under the ACA. Finally, Republicans use Frames 12 and 13 to bring attention to their own party’s actions to “repeal and replace” the ACA with different policies.\nFigures 2(b) and 2(d) show the party-based framing patterns over time for terrorism related tweets. For this issue both parties use similar frames: 3, 7, 10, 14, 16, and 17, but to express different views. For example, Democrats use Frame 3 to indicate a moral responsibility to fight ISIS. Republicans use Frame 3 to frame terrorists or their attacks as a result of “radical Islam”. An interesting pattern to note is seen in Frames 10 and 14 for both parties. In 2015 there is a large in-\ncrease in the usage of this frame. This seems to indicate that parties possibly adopt new frames simultaneously or in response to the opposing party, perhaps in an effort to be in control of the way the message is delivered through that frame.\nIndividual Frames: In addition to entire party analysis, we were interested in seeing if frames could shed light on the behavior of aisle-crossing politicians. These are politicians who do not vote the same as the majority vote of their party (i.e., they vote the same as the opposing party). Identifying such politicians can be useful in governments which are heavily split by party, i.e., governments such as the recent U.S. Congress (2015 to 2017), where politicians tend to vote the same\nas the rest of their party members. For this analysis, we collected five 2015 votes from the House of Representatives on both issues and compiled a list of the politicians who voted opposite to their party. The most important descriptor we noticed was that all aisle-crossing politicians tweet less frequently on the issue than their fellow party members. This is true for both parties. This behavior could indicate lack of desire to draw attention to one’s stance on the particular issue.\nFigure 3(a) shows the framing patterns of aislecrossing Republicans on ACA votes from 2015. Recall from Figure 2 that Democrats mostly use Frames 1, 4, 8, 9, and 15, while Republicans mainly use Frames 1, 8, and 9. In this example, these Republicans are considered aislecrossing votes because they have voted the same as Democrats on this issue. The most interesting pattern to note here is that these Republicans use the same framing patterns as the Republicans (Frames 1, 8, and 9), but they also use the frames that are unique to Democrats: Frames 4 and 15. These latter two frames appear significantly less in the Republican tweets of our entire dataset as well. These results suggest that to predict aisle-crossing\nRepublicans it would be useful to check for usage of typically Democrat-associated frames, especially if those frames are infrequently used by Republicans.\nFigure 3(b) shows the predicted frames for aisle-crossing Democrats on terrorism-related votes. We see here that there are very few tweets from these Democrats on this issue and that overall they use the same framing patterns as seen previously: Frames 3, 7, 10, 14, 16, and 17. However, given the small scale of these tweets, we can also consider Frames 12 and 13 to show peaks for this example. This suggests that for aisle-crossing Democrats the use of additional frames not often used by their party for discussing an issue might indicate potentially different voting behaviors."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper we present the task of collective classification of Twitter data for framing prediction. We show that by incorporating Twitter behaviors such as similar activity times and similar networks, we can increase F1 score prediction. We provide an analysis of our approach in both supervised and unsupervised settings, as well as a real world analysis of framing patterns over time. Finally, our global PSL models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their thoughtful comments and suggestions."
  }, {
    "heading": "A Supplementary Material",
    "text": "In this section we provide additional information about our congressional tweets dataset, as well as the lists of keywords and phrases used to filter tweets by issue and the unigrams used to extract information used for the Unigram and MaxSim PSL predicates. It is important to note that during preprocessing capitalization, stop words, URLs, and punctuation have been removed from tweets in our dataset. Additional word lists along with our PSL scripts and dataset are available at: http://purduenlp.cs.purdue.edu/ projects/twitterframing.\nDataset Statistics: Figure 4 shows the coverage of the labeled frames by party. From this, general patterns can be observed. For example, Republicans use Frames 12 and 17 more frequently than Democrats, while Democrats tend to use Frames 4, 9, 10, and 11. Table 7 shows the count of each type of frame that appears in each issue in our labeled dataset.\nWord Lists: Table 8 lists the keywords or phrases used to filter the entire dataset to only tweets related to the six issues studied in this paper. Table 9 lists the unigrams that were designed based on the descriptions for Frames 1 through 14\nprovided in the Policy Frames Codebook (Boydstun et al., 2014). These unigrams provide the initial supervision for our models as described in Section 4."
  }],
  "year": 2017,
  "references": [{
    "title": "How can you say such things?!?: Recognizing disagreement in informal political argument",
    "authors": ["Rob Abbott", "Marilyn Walker", "Pranav Anand", "Jean E. Fox Tree", "Robeson Bowmani", "Joseph King."],
    "venue": "Proc. of the Workshop on Language in Social Media.",
    "year": 2011
  }, {
    "title": "Identifying opinion subgroups in arabic online discussions",
    "authors": ["Amjad Abu-Jbara", "Ben King", "Mona Diab", "Dragomir Radev."],
    "venue": "Proc. of ACL.",
    "year": 2013
  }, {
    "title": "Hinge-loss markov random fields and probabilistic soft logic",
    "authors": ["Stephen H Bach", "Matthias Broecheler", "Bert Huang", "Lise Getoor."],
    "venue": "arXiv preprint arXiv:1505.04406 .",
    "year": 2015
  }, {
    "title": "Hinge-loss Markov random fields",
    "authors": ["Stephen H. Bach", "Bert Huang", "Ben London", "Lise Getoor"],
    "year": 2013
  }, {
    "title": "Sentiment analysis of political tweets: Towards an accurate classifier",
    "authors": ["Akshat Bakliwal", "Jennifer Foster", "Jennifer van der Puil", "Ron O’Brien", "Lamia Tounsi", "Mark Hughes"],
    "venue": "In Proc. of ACL",
    "year": 2013
  }, {
    "title": "Open extraction of fine-grained political statements",
    "authors": ["David Bamman", "Noah A Smith."],
    "venue": "Proc. of EMNLP.",
    "year": 2015
  }, {
    "title": "Testing and comparing computational approaches for identifying the language of framing in political news",
    "authors": ["Eric Baumer", "Elisha Elovic", "Ying Qin", "Francesca Polletta", "Geri Gay."],
    "venue": "Proc. of NAACL.",
    "year": 2015
  }, {
    "title": "On using twitter to monitor political sentiment and predict election results",
    "authors": ["Adam Bermingham", "Alan F Smeaton"],
    "year": 2011
  }, {
    "title": "Tracking the development of media frames within and across policy issues",
    "authors": ["Amber Boydstun", "Dallas Card", "Justin H. Gross", "Philip Resnik", "Noah A. Smith"],
    "year": 2014
  }, {
    "title": "Kissing in the carnage: An examination of framing on twitter during the vancouver riots",
    "authors": ["Lauren M. Burch", "Evan L. Frederick", "Ann Pegoraro."],
    "venue": "Journal of Broadcasting & Electronic Media 59(3):399–415.",
    "year": 2015
  }, {
    "title": "The media frames corpus: Annotations of frames across issues",
    "authors": ["Dallas Card", "Amber E. Boydstun", "Justin H. Gross", "Philip Resnik", "Noah A. Smith."],
    "venue": "Proc. of ACL.",
    "year": 2015
  }, {
    "title": "Hedge detection as a lens on framing in the gmo debates: A position paper",
    "authors": ["Eunsol Choi", "Chenhao Tan", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil", "Jennifer Spindel."],
    "venue": "Proc. of ACL Workshops.",
    "year": 2012
  }, {
    "title": "Framing theory",
    "authors": ["Dennis Chong", "James N Druckman."],
    "venue": "Annu. Rev. Polit. Sci. 10:103–126.",
    "year": 2007
  }, {
    "title": "Predicting the political alignment of twitter users",
    "authors": ["Michael D Conover", "Bruno Gonçalves", "Jacob Ratkiewicz", "Alessandro Flammini", "Filippo Menczer."],
    "venue": "Proc. of PASSAT .",
    "year": 2011
  }, {
    "title": "What does twitter have to say about ideology? In NLP 4 CMC",
    "authors": ["Sarah Djemili", "Julien Longhi", "Claudia Marinica", "Dimitris Kotzinos", "Georges-Elia Sarfati"],
    "year": 2014
  }, {
    "title": "Weakly supervised tweet stance classification by relational bootstrapping",
    "authors": ["Javid Ebrahimi", "Dejing Dou", "Daniel Lowd."],
    "venue": "Proc. of EMNLP.",
    "year": 2016
  }, {
    "title": "What to do about bad language on the internet",
    "authors": ["Jacob Eisenstein."],
    "venue": "Proc. of NAACL.",
    "year": 2013
  }, {
    "title": "Framing: Toward clarification of a fractured paradigm",
    "authors": ["Robert M Entman."],
    "venue": "Journal of communication 43(4):51–58.",
    "year": 1993
  }, {
    "title": "An empirical exploration of moral foundations theory in partisan news sources",
    "authors": ["Dean Fulgoni", "Jordan Carpenter", "Lyle Ungar", "Daniel Preotiuc-Pietro."],
    "venue": "Proc. of LREC.",
    "year": 2016
  }, {
    "title": "How they vote: Issue-adjusted models of legislative behavior",
    "authors": ["Sean Gerrish", "David M Blei."],
    "venue": "Advances in Neural Information Processing Systems. pages 2753–2761.",
    "year": 2012
  }, {
    "title": "More than words: Syntactic packaging and implicit sentiment",
    "authors": ["Stephan Greene", "Philip Resnik."],
    "venue": "Proc. of NAACL.",
    "year": 2009
  }, {
    "title": "Public sentiment and critical framing in social media content during the 2012 u.s. presidential campaign",
    "authors": ["Jacob Groshek", "Ahmed Al-Rawi"],
    "venue": "Social Science Computer Review 31(5):563–576",
    "year": 2013
  }, {
    "title": "The arab spring— overthrowing the protest paradigm? how the new york times, global voices and twitter covered the egyptian revolution",
    "authors": ["Summer Harlow", "Thomas Johnson."],
    "venue": "International Journal of Communication 5(0).",
    "year": 2011
  }, {
    "title": "Why are you taking this stance? identifying and classifying reasons in ideological debates",
    "authors": ["Kazi Saidul Hasan", "Vincent Ng."],
    "venue": "Proc. of EMNLP.",
    "year": 2014
  }, {
    "title": "Social group modeling with probabilistic soft logic",
    "authors": ["Bert Huang", "Stephen H. Bach", "Eric Norris", "Jay Pujara", "Lise Getoor."],
    "venue": "NIPS Workshops.",
    "year": 2012
  }, {
    "title": "Political ideology detection using recursive neural networks",
    "authors": ["Iyyer", "Enns", "Boyd-Graber", "Resnik."],
    "venue": "Proc. of ACL.",
    "year": 2014
  }, {
    "title": "Polarized frames on ”climate change” and ”global warming” across countries and states: Evidence from twitter big data",
    "authors": ["S. Mo Jang", "P. Sol Hart."],
    "venue": "Global Environmental Change 32:11–17.",
    "year": 2015
  }, {
    "title": "All i know about politics is what i read in twitter: Weakly supervised models for extracting politicians’ stances from twitter",
    "authors": ["Kristen Johnson", "Dan Goldwasser."],
    "venue": "Proc. of COLING.",
    "year": 2016
  }, {
    "title": "Major life event extraction from twitter based on congratulations/condolences speech acts",
    "authors": ["Jiwei Li", "Alan Ritter", "Claire Cardie", "Eduard H Hovy."],
    "venue": "Proc. of EMNLP.",
    "year": 2014
  }, {
    "title": "Weakly supervised user profile extraction from twitter",
    "authors": ["Jiwei Li", "Alan Ritter", "Eduard H Hovy."],
    "venue": "Proc. of ACL.",
    "year": 2014
  }, {
    "title": "Learning for microblogs with distant supervision: Political forecasting with twitter",
    "authors": ["Micol Marchetti-Bowick", "Nathanael Chambers."],
    "venue": "Proc. of EACL.",
    "year": 2012
  }, {
    "title": "Networked gatekeeping and networked framing on #egypt",
    "authors": ["Sharon Meraz", "Zizi Papacharissi."],
    "venue": "The International Journal of Press/Politics 18(2):138– 166.",
    "year": 2013
  }, {
    "title": "Tea party in the house: A hierarchical ideal point topic model and its application to republican legislators in the 112th congress",
    "authors": ["Viet-An Nguyen", "Jordan Boyd-Graber", "Philip Resnik", "Kristina Miler."],
    "venue": "Proc. of ACL.",
    "year": 2015
  }, {
    "title": "From tweets to polls: Linking text sentiment to public opinion time series",
    "authors": ["Brendan O’Connor", "Ramnath Balasubramanyan", "Bryan R Routledge", "Noah A Smith"],
    "venue": "In Proc. of ICWSM",
    "year": 2010
  }, {
    "title": "Political tendency identification in twitter using sentiment analysis techniques",
    "authors": ["Ferran Pla", "Lluı́s F Hurtado"],
    "venue": "In Proc. of COLING",
    "year": 2014
  }, {
    "title": "Linguistic models for analyzing and detecting biased language",
    "authors": ["Marta Recasens", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky."],
    "venue": "Proc. of ACL.",
    "year": 2013
  }, {
    "title": "Unsupervised modeling of twitter conversations",
    "authors": ["Alan Ritter", "Colin Cherry", "Bill Dolan."],
    "venue": "Proc. of NAACL.",
    "year": 2010
  }, {
    "title": "Measuring ideological proportions in political speeches",
    "authors": ["Sim", "Acree", "Gross", "Smith."],
    "venue": "Proc. of EMNLP.",
    "year": 2013
  }, {
    "title": "Recognizing stances in online debates",
    "authors": ["Swapna Somasundaran", "Janyce Wiebe."],
    "venue": "Proc. of ACL.",
    "year": 2009
  }, {
    "title": "Recognizing stances in ideological on-line debates",
    "authors": ["Swapna Somasundaran", "Janyce Wiebe."],
    "venue": "Proc. of NAACL Workshops.",
    "year": 2010
  }, {
    "title": "Joint models of disagreement and stance in online debate",
    "authors": ["Dhanya Sridhar", "James Foulds", "Bert Huang", "Lise Getoor", "Marilyn Walker."],
    "venue": "Proc. of ACL.",
    "year": 2015
  }, {
    "title": "The effect of wording on message propagation: Topicand author-controlled natural experiments on twitter",
    "authors": ["Chenhao Tan", "Lillian Lee", "Bo Pang."],
    "venue": "Proc. of ACL.",
    "year": 2014
  }, {
    "title": "A frame of mind: Using statistical models for detection of framing and agenda setting campaigns",
    "authors": ["Oren Tsur", "Dan Calacci", "David Lazer."],
    "venue": "Proc. of ACL.",
    "year": 2015
  }, {
    "title": "Predicting elections with twitter: What 140 characters reveal about political sentiment",
    "authors": ["Andranik Tumasjan", "Timm Oliver Sprenger", "Philipp G Sandner", "Isabell M Welpe."],
    "venue": "Proc. of ICWSM.",
    "year": 2010
  }, {
    "title": "Inferring latent user properties from texts published in social media",
    "authors": ["Svitlana Volkova", "Yoram Bachrach", "Michael Armstrong", "Vijay Sharma."],
    "venue": "Proc. of AAAI.",
    "year": 2015
  }, {
    "title": "Inferring user political preferences from streaming communications",
    "authors": ["Svitlana Volkova", "Glen Coppersmith", "Benjamin Van Durme."],
    "venue": "Proc. of ACL.",
    "year": 2014
  }, {
    "title": "Stance classification using dialogic properties of persuasion",
    "authors": ["Marilyn A. Walker", "Pranav Anand", "Robert Abbott", "Ricky Grant."],
    "venue": "Proc. of NAACL.",
    "year": 2012
  }, {
    "title": "Exploiting social network structure for person-to-person sentiment analysis",
    "authors": ["Robert West", "Hristo S Paskov", "Jure Leskovec", "Christopher Potts."],
    "venue": "TACL .",
    "year": 2014
  }, {
    "title": "Learning subjective language",
    "authors": ["Janyce Wiebe", "Theresa Wilson", "Rebecca Bruce", "Matthew Bell", "Melanie Martin."],
    "venue": "Computational linguistics .",
    "year": 2004
  }, {
    "title": "A penny for your tweets: Campaign contributions and capitol hill microblogs",
    "authors": ["Tae Yano", "Dani Yogatama", "Noah A Smith."],
    "venue": "Proc. of ICWSM.",
    "year": 2013
  }],
  "id": "SP:4118b4848601e8cbda94fd9ae1a207331eb0f30b",
  "authors": [{
    "name": "Kristen Johnson",
    "affiliations": []
  }, {
    "name": "Di Jin",
    "affiliations": []
  }, {
    "name": "Dan Goldwasser",
    "affiliations": []
  }],
  "abstractText": "Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.",
  "title": "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter"
}