{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 79–84 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2013"
  }, {
    "heading": "1 Introduction",
    "text": "NLP for studying people has grown rapidly as more than one-third of the human population use social media actively.1 While traditional NLP tasks (e.g. POS tagging, parsing, sentiment analysis) mostly work at the word, sentence, or document level, the increased focus on social scientific applications has shifted attention to new levels of analysis (e.g. user-level and communitylevel) (Koppel et al., 2009; Sarawgi et al., 2011; Schwartz et al., 2013a; Coppersmith et al., 2014; Flekova et al., 2016).\nFigure 1 shows the distribution of two unigrams, ‘the’ and ‘love’ at three levels of analysis. While both words have zero counts in most messages, ‘the’ starts to look Normal across\n1Social Insights; Global social media research summary 2017\nusers, and both words are approximately Normal at the county level. Methods performing optimally at the document level may suffer at the user or community level due to this shift in the distribution of lexical features.2\nIn this paper, we ask a fundamental statistical question: How does the shift in unit-of-analysis from document-level to user-or-community level shift lexical distributions in social media?3 The central limit theorem suggests that count data is better approximated by a Normal distribution as one increases the number of events, or as one aggregates more features (e.g. combining words using LDA topics or hand-built word sets). However, we do not know how far towards a Normal these new levels of analysis bring us.\nRelated work. The question we ask harks back to work from pioneers in corpus-based computational linguistics, including Shannon (1948) who suggested that probabilistic distributions of ngrams could be used to solve a range of communications problems, and Mosteller and Wallace (1963) who found that a negative binomial distribution seemed to model unigram usage by authors of the Federalist Papers. Numerous works have since continued the tradition of examining the distribution of lexical features. For example, McCallum et al. (1998) compares the results of probabilistic models based on multivariate Bernoulli with those based on multinomial distributions for document classification. Jansche\n2While the distribution of word frequencies (i.e. a Zipfian distribution) is often discussed in NLP, it is important to note that we are focused on the distribution of single features (e.g. words) over documents, users, or communities.\n3While other sources of corpora can also be aggregated to the user- or community-level (e.g. newswire, books), we believe the question of distributions is particularly important in social media because it often contains very short posts and a growing body of work in NLP for social science focuses on social media.\n79\n(2003) extended this line of work, observing lexical count data often display an extra probability mass concentrated at zero and suggesting ZeroInflated negative binomial distributions can capture this phenomenon better and are easier to implement than alternatives such as overdispersed binomial models. While these works are numerous, none, to the best of our knowledge, have focused on distributions across social media or at multiple levels of analysis.\nContributions. Our study is perhaps unconventional in modern computational linguistics due to the elementary nature of our contributions, focusing on understanding the empirical distributions of lexical features in Twitter. First, we use zeroinflated kernel density estimated plots to show how distributions of different language features (words, LDA topics, and hand-curated word sets) vary with level of analysis (message, user, and county). Second, we quantify which distributions best describe the different feature types and analysis levels of social media. Finally, we show the utility of such information, finding that using the appropriate model for each feature type improves Naive Bayes classification results across three common social scientific tasks: sarcasm detection at the message-level, gender identification at the user-level, and political ideology classification at the community-level."
  }, {
    "heading": "2 Methods",
    "text": "Examining data at three different levels of analysis and across three different lexical feature types (unigrams, data-driven topics, and manual lexica), we seek to (1) visually characterize distributions, (2) empirically test which distributions best fit the data, and (3) evaluate classification models utilizing multiple distributions at each level. Unigrams underlie all data where as each level of analysis\nand feature type represent a different degree of aggregation and covariance structure.\nData preparation. We start with a set of about two million Twitter posts and supplemental information about the users: their ID, county, and gender. The data was based on that of Volkova et al. (2013), who provide tweet ids and gender, and mapped to counties using the method of Schwartz et al. (2013a). We limit our data to users who have used at least 1000 words and counties that have at least 30 users and a total word count of 5000. Applying these constraints, the final set of data consists of 1,639,750 tweets (representing the message-level) from 5,226 users in 420 different counties (representing the community-level).\nWe consider three lexical features that are commonly used in NLP for social science: 1- grams (the top 10,000 most common unigrams found with happierFunTokenizing social media tokenizer), 2000 LDA topics downloaded from Schwartz et al. (2013b)), and lexica (64 categories from the linguistic inquiry and word count dictionary (Pennebaker et al., 2007)). Note that the features progress from most sparse (1grams) to least sparse (lexica).\nDistributions. Figure 2 shows the empirical distributions of different lexical features at different levels of analysis. 500 features were sampled from the top 20,000 unigrams 4, 2000 social media LDA topics (Schwartz et al., 2013a), and all 64 categories from the LIWC lexica (Pennebaker et al., 2007). To encode the variables continuously we used relative frequencies for unigrams and lexica (count of word or category divided by count of all words), and probability of topics, calculated from the posterior probabilities from the LDA models. Each line in the kernel density plot\n4In social media analyses, the top 20,000 features are often used (Schwartz and Ungar, 2015)\nis semi-transparent such that an aggregate trend across multiple features will emerge darkest. As we move along a row ranging specific features (unigrams) to generic features (lexicon), the empirical distribution gradually changes from resembling a “power law” (or binomial distribution with low number of trials and probability of success) to something more “Normal”. Similar shifts are also observed as we move across levels of modeling.\nWe investigate whether the best-fitting distributions vary across the three levels of analysis and three types of lexical features. We consider the following candidate distributions to see how well they fit each of these empirical distributions:\n• Continuous Distributions: (a) Power-law, (b), Log-normal and (c) Normal\n• Discrete Distributions: (a) Bernoulli, (b) Multinomial, (c) Poisson, and (d) Zero Inflated Poisson\nSince most of the distributions outlined above are standard distributions, we only briefly describe the zero-inflated variants which handle excess zero counts. Zero-inflated models explicitly model the idea that a distribution does not fully capture the mass at 0 in real world data. They assume that the data is generated from two components. The first\ncomponent is governed by a Bernoulli distribution that generates excess zeros, while the second component generates counts, some of which also could be zero (Jansche, 2003)."
  }, {
    "heading": "3 Evaluation",
    "text": "We evaluate the distributions we considered by first characterizing the goodness of fit at different levels of analyses and then by their predictive performance on social media prediction tasks, both of which we describe below."
  }, {
    "heading": "3.1 Goodness of fit",
    "text": "Following the central limit theorem, we seek to determine across the range levels of analysis and feature types, whether the distribution can be approximated by a Normal. Focusing just on the non-zero portions of data encoded as relative frequencies, we quantify the fit of each candidate distribution to the data.\nWe estimate the parameters for each distribution using MLE on a training data set (i.e. 80% of data). Then, we evaluate their likelihoods of a held-out test dataset, given the estimated parameters. Since we are trying to approximate the discrete distribution with a continuous model, all data were converted to relative frequencies. Finally, the distribution under which the test data is most likely\nis chosen as the ’best fit’ distribution. We repeat this 100 times and pick the most likely distribution over all these 100 independent runs.\nResults. Table 1 shows the percentage of features in each level that were best fit from an underlying distribution of Normal, Log-Normal, or Power Law. We see empirically that there is a trend toward Normal approximation moving from message to county level, as well as 1grams to lexica. In fact, a majority of lexica at the county-level were best approximated by a Normal distribution."
  }, {
    "heading": "3.2 Predictive Power",
    "text": "In the previous section, we showed that the distribution of lexical features depends on the scale of analysis considered (for example, the message level or the user level). Here, we demonstrate that predictive models which use these lexical features as co-variates can leverage this information to boost predictive performance. We consider three predictive tasks using a generative predictive model. The primary purpose of this evaluation is not to characterize the best distribution at a level or task, but to demonstrate that the choice of distribution assumed when modeling features significantly affects the predictive performance.\nPredictive Tasks : We consider the following common predictive tasks and also outline details of the datasets considered:\n1. Sarcasm Detection (Message level): This task consists of determining whether tweets contain a sarcastic expression (Bamman and Smith, 2015). The dataset consists of 16,833 messages with an average of 12 words per message.\n2. Gender Identification (User level): This task involves determining the gender of the author utilizing a previously described Twitter dataset (Volkova et al., 2013). This dataset consists of 5,044 users each of which have\nat least a 1,000 tokens as is standard in userlevel analyses (Schwartz et al., 2013b).\n3. Ideology Classification (Community level): We utilized county voting records from 2012 along with a dataset of tweets mapped to counties. This data consists of 2,175 counties with atleast 10,000 unigrams as is common in community level analyses (Eichstaedt et al., 2015).\nWe consider a Naive Bayes classifier (a generative model) which enables one to directly incorporate the inferred feature distribution at a particular level of analysis, the results of which we discuss in Table 2. Variable encoding for the classifiers varied from binary encoding of present or not (Bernoulli), to counts (Poisson, Zero-inflated Poisson), multivariate counts (Multinomial), and continuous relative frequencies (Normal). All distributions have closed form MLE solutions except for Zero-Inflated Poisson, in which case we used LBFGS optimization to fit both of its parameters (Head and Zerner, 1985).\nResults. We report macro F1-score for each of the underlying distributions in Table 2. For each of the tasks, we used 80% of the data for training and evaluate on the held-out 20%. We observe a similar pattern as that observed in the goodness of fit setting, with a shift in the best performing distribution from Bernoulli (which simply models if a feature exists or not) toward something more Gaussian (Poisson or Normal) as we move along from message-level to county-level analysis and from unigrams to lexica. Specifically note that at higher levels of analysis (at user and county levels) as the distribution of features becomes closer to Normal, modeling features as Bernoulli is clearly sub-optimal where as at the message level modeling unigrams as a Bernoulli is superior. These observations underscore the main insight that the distribution family used to model features can be con-\nsidered a function of level of analysis and featuretype considered and has a significant bearing on predictive performance."
  }, {
    "heading": "4 Conclusion",
    "text": "While computational linguistics has a long history of studying the distributions of lexical features, social media and social scientific studies have brought about a need to understand how these change at multiple levels of analyses. Here, we explored empirical distributions of different types of linguistic features (unigrams, topics, lexica) in three different levels of analysis in Twitter data (message, user, and community). To show which distribution can better describe features of different levels, we approached the problem in three different ways: (1) visualization of empirical distributions, (2) goodness-of-fit comparisons, and (3) for predictive tasks.\nWe showed that the best-fit distribution depends on feature-type (i.e. unigram versus lexica) and the level of analysis (i.e. message-, user-, or community-level). Following the central limit theorem, all user-level features were predominantly Log-normal, while a power law best fit unigrams at the message level and a Normal distribution best approximated lexica at the community level. Finally, we demonstrated that predictive performance can also vary considerably by the level of analysis and feature-type, following a similar trend from Bernoulli distributions at the messagelevel to Poisson or Normal at the community-level. Our results underscore the significance of the level of analysis for the ever-growing focus in NLP on social scientific problems which seek to not only better model words and documents but also the people and communities generating them."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported in part by the Templeton Religion Trust, Grant TRT-0048."
  }],
  "year": 2017,
  "references": [{
    "title": "Contextualized sarcasm detection on twitter",
    "authors": ["David Bamman", "Noah A Smith."],
    "venue": "Proceedings to the International Conference on Web-blogs and Social Media. pages 574–577.",
    "year": 2015
  }, {
    "title": "Quantifying mental health signals in twitter",
    "authors": ["Glen Coppersmith", "Mark Dredze", "Craig Harman."],
    "venue": "Proceedings of the ACL workshop on Computational Linguistics and Clinical Psychology.",
    "year": 2014
  }, {
    "title": "Psychological language on twitter predicts county",
    "authors": ["Johannes C Eichstaedt", "H Andrew Schwartz", "Margaret L Kern", "Gregory Park", "Darwin R Labarthe", "Raina M Merchant", "Sneha Jha", "Megha Agrawal", "Lukasz A Dziurzynski", "Maarten Sap"],
    "year": 2015
  }, {
    "title": "Analyzing Biases in Human Perception of User Age and Gender from Text",
    "authors": ["Lucie Flekova", "Jordan Carpenter", "Salvatore Giorgi", "Lyle Ungar", "Daniel Preoctiuc-Pietro."],
    "venue": "Proceedings of the 54th annual meeting of the Association for Computa-",
    "year": 2016
  }, {
    "title": "A broydenfletchergoldfarbshanno optimization procedure for molecular geometries",
    "authors": ["John D Head", "Michael C Zerner."],
    "venue": "Chemical physics letters 122(3):264–270.",
    "year": 1985
  }, {
    "title": "Parametric models of linguistic count data",
    "authors": ["Martin Jansche."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsVolume 1. Association for Computational Linguistics, pages 288–295.",
    "year": 2003
  }, {
    "title": "Computational methods in authorship attribution",
    "authors": ["Moshe Koppel", "Jonathan Schler", "Shlomo Argamon."],
    "venue": "Journal of the American Society for information Science and Technology 60(1):9–26.",
    "year": 2009
  }, {
    "title": "A comparison of event models for naive bayes text classification",
    "authors": ["Andrew McCallum", "Kamal Nigam"],
    "venue": "In AAAI-98 workshop on learning for text categorization. Citeseer,",
    "year": 1998
  }, {
    "title": "Inference in an authorship problem: A comparative study of discrimination methods applied to the authorship of the disputed federalist papers",
    "authors": ["Frederick Mosteller", "David L Wallace."],
    "venue": "Journal of the American Statistical Association 58(302):275–",
    "year": 1963
  }, {
    "title": "Liwc2007: Linguistic inquiry and word count",
    "authors": ["James W Pennebaker", "Roger J Booth", "Martha E Francis."],
    "venue": "Austin, Texas: LIWC.net .",
    "year": 2007
  }, {
    "title": "Gender attribution: tracing stylometric evidence beyond topic and genre",
    "authors": ["Ruchita Sarawgi", "Kailash Gajulapalli", "Yejin Choi."],
    "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Computational",
    "year": 2011
  }, {
    "title": "Characterizing geographic variation",
    "authors": ["H Andrew Schwartz", "Johannes C Eichstaedt", "Margaret L Kern", "Lukasz Dziurzynski", "Richard E Lucas", "Megha Agrawal", "Gregory J Park", "Shrinidhi K Lakshmikanth", "Sneha Jha", "Martin EP Seligman"],
    "year": 2013
  }, {
    "title": "Personality, gender, and age in the language",
    "authors": ["H Andrew Schwartz", "Johannes C Eichstaedt", "Margaret L Kern", "Lukasz Dziurzynski", "Stephanie M Ramones", "Megha Agrawal", "Achal Shah", "Michal Kosinski", "David Stillwell", "Martin EP Seligman"],
    "year": 2013
  }, {
    "title": "Datadriven content analysis of social media a systematic overview of automated methods",
    "authors": ["H Andrew Schwartz", "Lyle H Ungar."],
    "venue": "The ANNALS of the American Academy of Political and Social Science 659(1):78–94.",
    "year": 2015
  }, {
    "title": "A mathematical theory of communication, bell system technical journal 27: 379-423 and 623–656",
    "authors": ["Claude E Shannon."],
    "venue": "Mathematical Reviews (MathSciNet): MR10, 133e .",
    "year": 1948
  }, {
    "title": "Exploring demographic language variations to improve multilingual sentiment analysis in social media",
    "authors": ["Svitlana Volkova", "Theresa Wilson", "David Yarowsky."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural",
    "year": 2013
  }],
  "id": "SP:06cc885d1531ae54c8cc0b9204e15a89bc4a441d",
  "authors": [{
    "name": "Fatemeh Almodaresi",
    "affiliations": []
  }, {
    "name": "Lyle Ungar",
    "affiliations": []
  }, {
    "name": "Vivek Kulkarni",
    "affiliations": []
  }, {
    "name": "Mohsen Zakeri",
    "affiliations": []
  }, {
    "name": "Salvatore Giorgi",
    "affiliations": []
  }, {
    "name": "H. Andrew Schwartz",
    "affiliations": []
  }],
  "abstractText": "Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language. This move to working with data at the user or community level has presented the field with different characteristics of linguistic data. In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the central limit theorem to become much more Log-normal or even Normal at the userand county-levels. Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.",
  "title": "On the Distribution of Lexical Features at Multiple Levels of Analysis"
}