{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We are concerned with minimizing functions of the form\nmin x∈Rn\nF (x) = Eξf(x, ξ) (1)\nMany common problems in statistics and machine learning can be put into this form. For instance, in the empirical risk minimization framework, a model is learned from a set {y1, . . . , ym} of training data by minimizing an empirical loss function of the form\nmin x L(x) =\n1\nm m∑ i=1 f(x, yi) (2)\nIt is easy to see that this formulation is equivalent to taking ξ to be the uniform distribution on the points {y1, . . . , ym}.\nAn objective function of the form (1) is often impractical, as the distribution of ξ is generally unavailable, making it\n*Equal contribution 1Dept. of Industrial Engineering and Operations Research, Columbia University. Correspondence to: Chaoxu Zhou <cz2364@columbia.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ninfeasible to analytically compute Ef(x, ξ). This can be resolved by replacing the expectation Ef(x, ξ) by the estimate (2). The strong law of large numbers implies that the sample mean L(x) converges almost surely to F (x) as the number of samples m increases, provided that the samples yi are drawn independently from the distribution of ξ. However, even the concrete problem (2) is not a good target for classical optimization algorithms, as the amount of data m is frequently extremely large. A better strategy when optimizing (2) is to consider subsamples of the data to reduce the computational cost. This leads to stochastic algorithms where the objective function changes at each iteration by randomly selecting subsamples.\nMany stochastic algorithms have been proposed which use this approach for solving (2), notably stochastic gradient descent (SGD) (Bottou, 2010), and variance-reduced extensions of SGD, such as SVRG (Johnson & Zhang, 2013), SAG (Schmidt et al., 2013), and SAGA (Defazio et al., 2014). These methods are first-order methods, extending gradient descent to the stochastic setting, and the latter three (variance-reduced) methods can be shown to converge linearly for strongly convex objectives. Linearly convergent stochastic Limited Memory BFGS algorithms (Byrd et al., 2016)(Moritz et al., 2016)(Gower et al., 2016) have also been proposed. It is then natural to consider stochastic extensions of quasi-Newton and secondorder methods. One such method, the Newton Incremental Method (NIM) (Rodomanov & Kropotov, 2016), combines cyclic updating of a fixed collection of functions f(x, y1), . . . , f(x, ym) with Newton’s method, and attains local superlinear convergence.\nOne of the key obstacles to developing stochastic extensions of quasi-Newton methods is the necessity of selecting appropriate step sizes. The analysis of the global convergence of the BFGS method (Powell, 1976) and other members of Broyden’s convex class (Byrd et al., 1987) assumes that Armijo-Wolfe inexact line search is used. This is rather undesirable for a stochastic algorithm, as line search is both computationally expensive and difficult to analyze in a probabilistic setting. However, there is a special class of functions, the self-concordant functions, whose properties allow us to compute an adaptive step size and thereby avoid performing line searches. In (Gao & Goldfarb, 2016), it is shown that the BFGS method\n(Broyden, 1967) (Fletcher, 1970)(Goldfarb, 1970)(Shanno, 1970) with adaptive step sizes converges superlinearly when applied to self-concordant functions.\nIn this paper, our goal is to develop a stochastic quasiNewton algorithm for self-concordant functions. We propose an iterative method of the following form. At the k-th iteration, we drawmk i.i.d samples ξ1, . . . , ξmk . Define the empirical objective function at the k-th iteration to be\nFk(x) = 1\nmk mk∑ i=1 f(x, ξi) (3)\nLetHk be a positive definite matrix. The next step direction is given by dk = −Hk∇Fk(xk) and the step size by\ntk = αk\n1 + αkδk\nwhere\nαk = ∇Fk(xk)THk∇Fk(xk)\nδ2k (4) δk = √ ∇Fk(xk)THk∇2Fk(xk)Hk∇Fk(xk)\nThe motivation for this step size is described in Section 4.\nA key feature of these methods is that the step size tk can be computed analytically, using only local information, and adapts itself to the local curvature. A fixed step size η is typically used in variance-reduced first-order methods, and this step size must be determined experimentally. The theoretical analysis that has been provided for these methods is of little help in choosing η, as often η is constrained to be impractically small, and moreover, is related to unknown constants such as the Lipschitz parameter of the gradient. Furthermore, a fixed η which was effective in one regime may become ineffective as the algorithm progresses, and enters regions of varying curvature.\nOur new methods are also capable of solving general problems of the form (1). This is in contrast to popular incremental-type methods such as SAG, SAGA, and NIM, which, because of their stored updating scheme, can only be applied to problems of the form (2) with a fixed data set {y1, . . . , ym}. This opens up new avenues for the solutions of problems where new data can be sampled as the algorithm progresses, as opposed to having a fixed training set throughout. In this respect, our new method is akin to Streaming SVRG (Frostig et al., 2015), which is also able to solve (1) with similar conditions on the growth of the samples mk.\nBy choosing the matrices Hk appropriately, we obtain stochastic extensions of classical methods. In particular, two choices of Hk will be of interest:\n1. Taking Hk = I yields the stochastic adaptive gradient descent method (SA-GD).\n2. Fixing H0, and then taking Hk+1 to be the BFGS update of Hk, yields the stochastic adaptive BFGS method (SA-BFGS).\nOur methods can be proven to converge under a suitable sampling regime. When the number of samplesmk is fixed, and large enough, both SA-GD and SA-BFGS convergeRlinearly in expectation to an -optimal solution. To obtain R-superlinear convergence, the number of samples must be allowed to grow rapidly. This principle, of increasing the number of samples to match the desired rate of convergence, was previously applied to R-linear convergence in (Byrd et al., 2012). The SA-BFGS algorithm can be shown to converge R-superlinearly almost surely to the true optimal solution if the number of samples is increased so that m−1k converges R-superlinearly to 0.\nThis paper is organized as follows. In Section 2, we introduce our notation, and the technical assumptions needed for the analysis of our methods. In Section 3, we describe the relevant results from stochastic analysis which motivate our algorithms. In Section 4, we briefly describe the required theory of self-concordant functions. In Section 5, we formally define stochastic adaptive methods, and state the convergence results. In Section 6, we present preliminary numerical experiments, and conclude with a discussion in Section 7. Full proofs of the convergence theorems can be found in the supplementary materials."
  }, {
    "heading": "2. Assumptions and Notation",
    "text": "The number of variables is n. We write g(x) = ∇F (x) and G(x) = ∇2F (x), and gk(x) = ∇Fk(x) and Gk(x) = ∇2Fk(x) for the gradient and Hessian of the empirical objective function. In the context of a sequence of iterates {xk}∞k=0 generated by an algorithm, we also write gk with no argument to denote gk(xk), and Gk for Gk(xk). In the context of BFGS, Hk denotes the approximation of the inverse Hessian, and Bk = H−1k . The step direction (generally −Hkgk) is denoted dk, and the step size by tk, so the actual step taken is sk = tkdk.\nThe optimal solution of min x∈Rn\nF (x) is denoted x∗, and the\noptimal solution of the empirical problem min x∈Rn Fk(x) is denoted x∗k. Note that x ∗ k is a random variable.\nUnless otherwise specified, the norm ‖ · ‖ is the 2-norm, or the operator 2-norm. The Frobenius norm is indicated as ‖ · ‖F .\nWe make the following technical assumptions on F (x) and f(x, ξ). We will explain the motivation for these assumptions at the relevant points in the discussion.\nAssumptions:\n1. There exist constants L ≥ ` > 0 such that for every x ∈ Rn and every realization of ξ, the Hessian of f with respect to x satisfies\n`I ∇2xf(x, ξ) LI\nThat is, f(x, ξ) is strongly convex for all ξ, with the eigenvalues of ∇2xf(x, ξ) bounded by ` and L. The condition number, denoted κ, is given by L/`.\n2. Fk(x) is standard self-concordant for every possible sampling ξ1, . . . , ξmk .\n3. There exist compact sets D0 and D with x∗ ∈ D and D0 ⊆ D, such that if x0 is chosen in D0, then for all possible realizations of the samples ξ1, . . . , ξmk for every k, the sequence of iterates {xk}∞k=0 produced by the algorithm is contained within D. We use D = sup{‖x− y‖ : x, y ∈ D} for the diameter of D. Furthermore, we assume that the objective values and gradients are bounded:\nu = sup ξ sup x∈D\nf(x, ξ) <∞\nl = inf ξ inf x∈D\nf(x, ξ) > −∞\nγ = sup ξ sup x∈D ‖∇f(x, ξ)‖ <∞\n4. (For BFGS only) The Hessian G(x) is Lipschitz continuous with constant LH .\nNote that Assumption 1 is standard when analyzing stochastic algorithms. The function f(x, ξ) is commonly a loss function, and either f(x, ξ) is itself strongly convex, or each f(x, ξ) is weakly convex and the strong convexity is ensured by adding a quadratic regularization term to F (x)."
  }, {
    "heading": "3. Stochastic Framework",
    "text": "Our analysis is based on a uniform convergence law, a standard technique in learning theory and empirical processes. The central idea is that Fk(x) is an empirical mean estimating F (x), and we can closely control the error |Fk(x) − F (x)| over all x ∈ D by varying the sample size mk. The relevant stochastic analysis can be found in (W. van der Vaart & Wellner, 1996) and (Goldfarb et al., 2017). These powerful techniques are required to analyze second-order stochastic methods, since inverting the product of a sampled Hessian and sampled gradient generates both dependence and non-linearity. In comparison, firstorder stochastic methods can be analyzed by evaluating the expected value and variance of the sampled gradient. It\nis also useful to compare this approach with that used in (Byrd et al., 2012) and (Friedlander & Goh, 2013), where the variance of the gradient is controlled by pointwise estimates or pointwise tail bounds.\nTheorem 3.1 (Corollary 1, (Goldfarb et al., 2017)). Let δ > 0 and 0 < < min{D, δ2L}. Then\nP(sup x∈D |Fk(x)−F (x)| > δ) ≤ c( ) exp\n( −mk(δ − 2L ) 2\n2(u− l)2 ) where c( ) = 2nn/2Dn −n. If mk ≥ 3, then\nE sup x∈D |Fk(x)− F (x)| ≤ C √ logmk mk\nE|Fk(x∗k)− F (x∗)| ≤ C √\nlogmk mk\nwhere C is a constant (depending only on F ) given by 4(|u|+|l|)nn/2Dn exp [ −n (\nlog u− l 2 √ 2L\n)] +(u−l) √ n+ 1\nAssumption 3 is required for the uniform law of Theorem 3.1 to hold. The set D0 is assumed to be a region where, if x0 is chosen within D0, the path of the stochastic algorithm will remain within the larger set D. For practical purposes, we may takeD to be an arbitrarily large bounded set in order to ensure convergence, though this worsens the complexity bounds provided by Theorem 3.1. We note that the constant C is very much a ‘worst-case’ bound, and for almost every problem arising in practice, the expected difference will be much smaller than Theorem 3.1 suggests. Rather, the crucial implication is that the expected differ-\nence diminishes at the rate √\nlogmk mk , allowing a sharp level of control by adjusting mk."
  }, {
    "heading": "4. Self-Concordant Functions and Adaptive Methods",
    "text": "A convex function f : Rn → R is self-concordant if there exists a constant κ such that for every x ∈ Rn and every h ∈ Rn, we have\n|∇3f(x)[h, h, h]| ≤ κ(∇2f(x)[h, h])3/2\nIf κ = 2, f is standard self-concordant. Self-concordant functions were introduced by Nesterov and Nemirovski in the context of interior point methods (Nesterov & Nemirovski, 1994).\nMany common problems have self-concordant formulations. At least one method, the DiSCO algorithm of Zhang and Xiao (2015), is tailored for distributed self-concordant optimization and has been applied to many regression problems. Convex quadratic objective functions have third\nderivatives equal to zero, and are therefore trivially standard self-concordant. In particular, least squares regression is self-concordant. In (Zhang & Xiao, 2015), it is also shown that regularized regression, with either logistic loss or hinge loss, is self-concordant.\nFor self-concordant functions, the notion of a local norm is especially useful. Given a convex function f , the local norm with respect to f at the point x is given by\n‖h‖x = √ hT∇2f(x)h\nConsider an iterative method for minimizing selfconcordant functions with steps given as follows. On the kth step, the step direction dk is given by dk = −Hk∇f(xk) for some positive definite matrix Hk, and the step size is given by tk = αk1+αkδk , where δk = ‖dk‖xk and αk = gTk Hkgk δ2k .\nMethods of this type have been analyzed in (Tran-Dinh et al., 2015) and (Gao & Goldfarb, 2016). In (Gao & Goldfarb, 2016), the above choice of αk is shown to guarantees a decrease in the function value. Theorem 4.1 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let ρk = ∇f(xk)THk∇f(xk). If αk is chosen to be αk = ρkδ2k , then f(xk + tkdk) ≤ f(xk)− ω(ηk), where ηk = ρkδk and the function ω(z) = z − log(1 + z).\nWe make Assumption 2 in order to apply Theorem 4.1 to the empirical objective functions Fk(x). A natural question is whether Assumption 2 can be relaxed to the assumption that f(x, ξ) is self-concordant for all ξ, and F (x) is standard self-concordant. In the case where ξ is finitely supported, it is possible to scale F (x) so that we may use this weaker assumption. Lemma 4.2. Suppose that ξ is finitely supported on {a1, . . . , am}, with pi = P(ξ = ai). Suppose that f(x, ai) is self-concordant with constant κi. Let θ = 14 maxκ2i min pi\n. Then the scaled function F (x) = E[θf(x, ξ)] is standard selfconcordant, and every empirical objective function F k(x) is standard self-concordant.\nProof. Observe that θf(x, ai)pi is self-concordant with constant θ−1/2p−1/2i κi ≤ 2. We deduce that EF (x) =∑m i=1 θf(x, ai)pi is standard self-concordant. Further-\nmore, since ∑m i=1 pi = 1, we have min pi ≤ 1 m . Thus, 1 mθf(x, ai) is self-concordant with constant θ−1/2m−1/2κi ≤ 2, which implies that F k(x) is standard self-concordant for every possible F k(x).\nHowever, if we do not assume that each f(x, ξ) is selfconcordant, or if ξ is not compactly supported, it is un-\nAlgorithm 1 SA-GD Input: x0, {m0,m1, . . .} for k = 0, 1, 2, . . . do\nSample ξ1, . . . , ξmk i.i.d from ξ Compute:\ngk = ∇Fk(xk), dk = −gk, δk = √ gTk Gk(xk)gk,\nαk = gTk gk δ2k , tk = αk 1 + αkδk ,\nwhere Fk(x) = 1mk ∑mk i=1 f(x, ξi).\nSet xk+1 = xk + tkgk. end for\nclear whether every possible Fk(x) will be standard selfconcordant, even when F (x) is standard self-concordant. Thus, we impose Assumption 2 in our analysis, while observing that it is unnecessary in the practical case where ξ is derived from a finite data set."
  }, {
    "heading": "5. Stochastic Adaptive Methods",
    "text": "Our basic approach is to sample an empirical objective function Fk(x) at each step, and then compute the step direction and adaptive step size (4) using Fk. For particular choices of the matricesHk, we recover analogues of classical methods such as gradient descent, L-BFGS, and BFGS."
  }, {
    "heading": "5.1. Stochastic Adaptive GD",
    "text": "When Hk = I for every k, the resulting method is stochastic adaptive gradient descent (SA-GD), which is given as Algorithm 1. The number of samples drawn at each iteration is left as an input to the algorithm, and (weak) bounds on the required number mk can be inferred from the convergence analysis.\nTheorem 5.1. Let > 0. Suppose that at each iteration, we draw m i.i.d samples. We may choose k = Õ(log −1) and m = Õ( −2 log −1) so that the SA-GD method converges in expectation to an -optimal solution after k steps. Here, Õ(·) absorbs constants depending only on F , namely C and κ.\nA more precise quantitative form of Theorem 5.1, and its proof, appear as Theorem B.1 in the supplementary materials. The argument can be sketched as follows. Theorem 4.1 implies that the adaptive step size tk produces a linear decrease towards the optimal value of Fk; that is, Fk(xk+1)−Fk(x∗k) ≤ r(Fk(xk)−Fk(x∗k)). Here r is a deterministic constant depending only on `, L, and γ. By iterating, we can bound the true gap F (xk+1)−F (x∗) in terms\nAlgorithm 2 SA-BFGS Input: x0, H0, {m0,m1, . . .}, β < 1 for k = 0, 1, 2, . . . do\nSample ξ1, . . . , ξmk i.i.d from ξ Compute\ngk = ∇Fk(xk), dk = −Hkgk δk = √ dTkGk(xk)dk,\nαk = gTkHkgk δ2k , tk = αk 1 + αkδk ,\nwhere Fk(x) = 1mk ∑mk i=1 f(x, ξi). Set gk+1 = ∇Fk(xk + tkdk) Set yk = gk+1 − gk if gTk+1dk < βgTk dk then\nSet dk = gk Recompute δk, αk, tk Set Hk+1 = Hk else Set Hk+1 = (I − sky T k\nsTk yk )Hk(I − yks\nT k\nsTk yk ) +\nsks T k sTk yk\nend if Set xk+1 = xk + tkdk.\nend for\nof rk(F0(x1) − F0(x∗0)) and rj sup x∈D |Fk+1−j(x) − F (x)| for 1 ≤ j ≤ k. The first term can be made smaller than by taking k to be O(log −1), and the second can be bounded with Theorem 3.1."
  }, {
    "heading": "5.2. Stochastic Adaptive BFGS",
    "text": "By updating Hk using the BFGS formula, we obtain the stochastic adaptive BFGS (SA-BFGS) method, which is given in Algorithm 2.\nIn Algorithm 2, we use the standard BFGS update with yk = gk(xk+1) − gk(xk). Another option is to replace yk with the action of the Hessian on sk, so yk = Gk(xk)sk. In general, we must compute Gk(xk)dk when finding the adaptive step size, so we can re-use the result of that computation instead of computing an extra gradient gk(xk+1).\nFor technical reasons, our SA-BFGS procedure tests whether the Wolfe condition is satisfied for the adaptive step size tk. If not, we revert to taking a SA-GD step. This is an artifact of our analysis, and under suitable conditions on the growth of the samples mk, there will be some point after which the Wolfe condition is necessarily satisfied on every step. In practice, this test can be omitted.\nThere are also two possible ways to implement SA-BFGS. For problems with n at most medium-sized, it is possible to explicitly store the matrix Hk, and compute dk by a matrix\nproduct −Hkgk. For n very large, it is infeasible to store Hk, and we can instead store the pairs (sk, yk) and compute −Hkgk using a two-loop recursion (Nocedal, 1980). This corresponds to stochastic adaptive L-BFGS (SA-LBFGS) if we limit the number of past pairs (sk, yk) to only the h most recent, and to SA-BFGS if we store everything. The amount of storage used by SA-LBFGS surpasses that of SA-BFGS as h approaches n.\nTheorem 5.1 holds for SA-LBFGS, since Theorem B.1 holds for any method where {Hk} has uniformly bounded eigenvalues. Thus, SA-LBFGS also converges in expectation to an -optimal solution after k = Õ(log −1) steps given samples of size m = Õ( −2 log −1), though now the constants within the big-O are also dependent on h.\nAs with SA-GD, one can obtain an -optimal solution in expectation from SA-BFGS with a fixed amount of sampling:\nTheorem 5.2. Let > 0. Suppose that at each iteration, we draw m i.i.d samples. We may choose k = Õ(log −1) and m = Õ( −2(log −1)3) so that the SA-BFGS method converges in expectation to an -optimal solution after k steps. Here, Õ(·) absorbs constants depending only on F , namely C and κ.\nThis theorem follows from Lemma C.9 in the supplementary materials. Note that the complexity bound is in fact weaker than that of SA-GD. This occurs because the initial matrices H0, . . . ,Hk may be poor approximations of (∇2F (x))−1, in which case the BFGS directions may perform poorly. We have little theoretical control over Hk except in the limit, and this is reflected in the weaker iteration bounds for -optimality.\nFor convergence to the true optimal solution x∗, it is necessary for the number of samples mk to increase over time. In fact, by increasing the number of samples appropriately, the SA-BFGS algorithm can be shown to converge to x∗ R-superlinearly with probability 1.\nTheorem 5.3. Suppose that we drawmk samples on the kth step, where m−1k converges R-superlinearly to 0. Then the SA-BFGS algorithm converges R-superlinearly to the optimal solution x∗ almost surely.\nThe complete proof is left to Lemma C.1 in the supplementary materials. We briefly sketch it here.\nTaking mk to be an increasing sequence, we can guarantee that the sequence {ηk = ρkδk } converges to 0. From basic results on the adaptive step size, this implies that tk eventually satisfies the Armijo-Wolfe conditions, and therefore the algorithm eventually uses the BFGS direction, and performs a BFGS update on Hk, at every step. Our test for the Wolfe condition ensures that in every iteration, we either can control the progress by analyzing the BFGS up-\ndate Hk+1, or by using our earlier results on SA-GD steps. Together, these facts imply that SA-BFGS converges Rlinearly almost surely, and consequently, the sum of distances ∑∞ k=0 ‖xk − x∗‖ is finite almost surely.\nFollowing the classical argument, we analyze the evolution of Hk+1 to show that the steps taken by SA-BFGS converge to the Newton step. This is precisely the well-known Dennis-Moré condition (Dennis Jr. & Moré, 1974) for Qsuperlinear convergence; however, in the stochastic setting, we have an additional error term resulting from the variance in the sampling of the gradients and Hessians. We apply Theorem 3.1 to show that gk(x) and G(x) rapidly converge to the true gradient and Hessian, and thus SABFGS attains R-superlinear convergence.\nIncreasing mk at this rate is clearly infeasible in reality, and it is non-trivial to determine a level of sampling which produces (iteration-wise) superlinear convergence in a reasonable amount of real time. We note, for comparison, that the Streaming SVRG method (Frostig et al., 2015) requires sample sizes for the gradient that grow at a geometric rate, and that the dynamic batch method (Byrd et al., 2012) obtainsR-linear convergence in expectation by increasing the sample sizes at a geometric rate. A superlinear increase in the number of samples may be unavoidable as a condition for superlinear convergence, given the statistical error of the sample.\nIn practice, we are often uninterested in finding the true minimizer, and instead aim to quickly find an approximate solution. The theoretical R-superlinear rate of SA-BFGS suggests that SA-BFGS or SA-LBFGS may offer practical speedups over first-order stochastic methods, even without using substantial sampling. One intuitive heuristic would be to draw fewer samples in the early phase of the algorithm, when computing the gradient with high accuracy is less valuable, and then increasing the number of samples as the solutions approach optimality to reduce fluctuation."
  }, {
    "heading": "6. Numerical Experiments",
    "text": "We compared several implementations of SA-GD and SABFGS against the original SGD method and the robust SGD method (Nemirovski et al., 2009) on a penalized least squares problem with random design. That is, the objective function has the form\nmin w∈Rp L(w) = E(Y −XTw)2 + 1 2 ‖w‖22\nwhere X ∈ Rp and Y ∈ R are random variables with finite second moments. We base our model on a standard linear regression problem with Gaussian errors, so Y = XTβ+ for a deterministic vector β ∈ Rp and ∼ N(0, 1) is a noise component. X was drawn according to a multivariate N(0,Σ(ρ)), where Σ(ρ) = (1−ρ2)Ip+ρ2J (here J is the\nall-ones matrix). By varying ρ, we control the condition number of the expected Hessian. We tested problems of size p = 100 and p = 500.\nThe following eight algorithms were tested:\nSGD: SGD with fixed mk = p and diminishing step sizes tk = 1 k+1000 for problems with p = 100, and tk =\n1 k+5000 for p = 500.\nSA-GD: SA-GD with fixed mk = p.\nSA-GD-I: SA-GD with increasing samples mk = 12p + 1.01k.\nSA-BFGS: SA-BFGS with fixed mk = p. We do not test for the Wolfe condition at each step, and instead always take the adaptive BFGS step and perform a BFGS update.\nSA-BFGS-I: SA-BFGS with increasing samples mk = 1 2p+ 1.01\nk. We do not test for the Wolfe condition at each step, and instead always take the adaptive BFGS step and perform a BFGS update.\nSA-BFGS-GD: SA-BFGS with increasing samples mk = 1 2p+1.01\nk. We test for the Wolfe condition and switch to taking a SA-GD step when the adaptive step tk for SA-BFGS fails the test.\nR-S-GD-C: Robust SGD with constant step size tk = 1√ N\n, where N is the pre-determined number of steps. At the k-th iteration, output the average of the previous k/2 solutions.\nR-S-GD-V: Robust SGD with diminishing step size tk = 1√ k\n. At the k-th iteration, output the average of the previous k/2 solutions.\nWe also tested the Streaming SVRG algorithm with the hyperparameters specified in Corollary 4 (Frostig et al., 2015). However, this algorithm was difficult to tune and performed poorly, so we have omitted it from the figures.\nThe algorithms prefixed by “SA-” used the adaptive step size. For SGD, we followed the standard practice of using a diminishing and non-summable step size tk = ak+b . The values a = 1, b = {1000, 5000} in our test were chosen experimentally. The SA-BFGS algorithms were implemented using a two-loop recursion to compute Hkgk when p = 500. This significantly improved their performance compared to storing the matrix Hk explicitly.\nFigure 1 shows the performance of each algorithm on a series of problems with varying problem size p and parameter ρ. The y-axis measures the gap log(f(x(t)) − f(x∗)) of the solution x(t) obtained by the algorithm after using t\nseconds of CPU time. The algorithms were implemented in Matlab 2015a, and the system was an Intel i5-5200U running Ubuntu.\nThe increasing sample sizes used in SA-GD-I do not appear to reduce its variance, compared to SA-GD with mk fixed, which goes against our initial expectations. In plots (a), (b), (e), and (f), SA-GD-I appears to exhibit the same fluctuations as SA-GD when the points approach optimality. In plot (c), SA-GD-I briefly surpasses SA-GD before the objective value jumps again. It is only in plot (d) that SAGD-I appears to descend more consistently than SA-GD. This suggests that, for these problem instances, the rate of sampling mk = O(1.01k) could even be increased further, and that the tradeoff between taking more steps, versus taking fewer steps with more accurate estimates, favors more accurate steps.\nLikewise, SA-BFGS-I failed to consistently perform better than SA-BFGS with mk fixed. SA-BFGS-I seemed to perform relatively better when the dimension was larger, whereas SA-BFGS was faster when p = 100. One notable example was plot (f), where SA-BFGS initially descends rapidly before abruptly stalling, and is quickly surpassed by SA-BFGS-I. The data going beyond 60 seconds of CPU time verified that SA-BFGS continued to stall, and never managed to obtain a log-error of less than 1. Our interpretation is that for this problem, the fixed sample size is too small to obtain an accurate solution. It also appears that SA-BFGS requires larger samples to obtain a stable solution when the problem is particularly ill-conditioned. In the plots (e), and particularly (f), where ρ = 0.9, SABFGS rapidly reaches the point where variance in the gradient estimates introduces too much noise to make further progress.\nWhat is also interesting is that SA-GD often outperforms SA-BFGS if both algorithms use the same fixed sample size. While somewhat disappointing, there is a natural reason for this. BFGS optimizes a local quadratic model of the objective, and is performant when its approximation Hk resembles the true Hessian. The Hessian exhibits greater variance than the gradient, simply by virtue of having n2 components compared to n, and we generally expect that more sampling is needed to accurately estimate the Hessian. With the same amount of sampling, SA-BFGS is therefore ‘noisier’ than SA-GD relative to the true function.\nWe also see this reflected in the performance of SA-BFGSGD, which was able to outperform SA-BFGS and SABFGS-I when p = 100 despite the additional cost of checking the Wolfe condition. For p = 500, it is less clear whether a better strategy is simply to perform SA-BFGSI. One strategy that might be effective is a fixed pattern of ‘switching’ between GD and BFGS steps, with a greater number of GD steps in the early phase.\nPredictably, robust SGD was the most stable method (given the lack of variance-reduction in the other methods). Both R-S-GD-C and R-S-GD-V exhibited almost no fluctuations, while improving at a reasonable rate. The R-S-GD-C method with a constant step size outperformed R-S-GD-V, which we find counterintuitive given that R-S-GD-C used a smaller step size than R-S-GD-V. This method of variancereduction by averaging the solutions seems to be effective and can be applied to any iterative method.\nNumerical results for ERM (empirical risk minimization) problems can be found in section D in the supplementary material."
  }, {
    "heading": "7. Discussion",
    "text": "For self-concordant objective functions, adaptive methods eliminate the need to choose a step size, which is a big advantage. This is independent of a new difficulty which arises for stochastic methods, which is the choice of the sample size mk. From our experiments, we see that choosing mk appropriately is crucial. Unlike SGD and SVRG, where it is often most effective to use mini-batches of size 1, SA-BFGS and SA-GD work best with comparatively larger samples.\nNote that SA-GD and SA-BFGS are not purely first-order methods, as computing the adaptive step size requires calculating the Hessian-vector product Gk(xk)dk. However, it is often possible to calculate Hessian-vector products efficiently, and at far less cost than computing the full Hessian ∇2Fk(x). Consider, for example, a a logistic regression problem where the empirical loss function is\nL(x) = 1\nm m∑ i=1 log(1 + e−yia T i x)\nfor sampled data {(ai, yi) : ai ∈ Rn, yi ∈ {−1, 1}}. Let A denote the n × m matrix with columns ai. The gradient is given by ATβ, where β is the vector βi = −yie−yia T i x/(1 + e−yia T i x), and the Hessian is given by ABAT , where B is the diagonal matrix with entries Bii = e −yiaTi x/(1 + e−yia T i x)2. To compute the product\ndT∇2L(x)d, it suffices to compute ∑m i=1Bii(a T i d)\n2, and this requires approximately the same number of arithmetic operations as computing ∇L(x). Thus, computing δk for the adaptive step size requires roughly the same amount of work as one additional gradient calculation. Also, as mentioned in Section 5, we may replace the BFGS update in SA-BFGS with a modified update using the Hessian action yk = Gk(xk)dk to save effort.\nThis work represents only a preliminary step in the development of stochastic quasi-Newton methods. There are several key questions that remain open:\n1. Theorem 5.3 partially resolves a question posed by Moritz et al. (Moritz et al., 2016), by proving superlinear convergence of a stochastic algorithm under rather restrictive conditions. It would be strengthened greatly if we could prove superlinear convergence under weaker conditions on mk.\n2. The theory developed for adaptive step sizes only applies to self-concordant functions, but the adaptive step size itself can be interpreted for non-selfconcordant functions, as an adjustment based on the local curvature. It would be of great interest to extend adaptive methods to general convex functions, thereby replacing both inexact line search and fixed step sizes on a large class of problems.\n3. How can variance-reduction be applied to SA-GD and SA-BFGS? The control variates used in SVRG are effective, though costly, and perhaps difficult to compute for functions of the form Ef(x, ξ). Another possible technique is to output an average of the solutions, as is done by robust SGD, though this introduces an additional hyperparameter (the number of past solutions to average). Better variance-reduction strategies might compensate for noisy estimates, allowing us to reduce the number of samples needed in practice.\n4. What heuristics can be developed for the sample sizes mk to improve the stability and speed up performance of SA-GD and SA-BFGS?"
  }],
  "year": 2017,
  "references": [{
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["Bottou", "Léon"],
    "venue": "In Proceedings of COMPSTAT’2010,",
    "year": 2010
  }, {
    "title": "Quasi-Newton methods and their application to function minimisation",
    "authors": ["Broyden", "Charles G"],
    "venue": "Mathematics of Computation,",
    "year": 1967
  }, {
    "title": "Global convergence of a class of quasi-Newton methods on convex problems",
    "authors": ["Byrd", "Richard H", "Nocedal", "Jorge", "Yuan", "Ya-Xiang"],
    "venue": "Siam. J. Numer. Anal.,",
    "year": 1987
  }, {
    "title": "Sample size selection in optimization methods for machine learning",
    "authors": ["Byrd", "Richard H", "Chin", "Gillian M", "Nocedal", "Jorge", "Wu", "Yuchen"],
    "venue": "Mathematical Programming,",
    "year": 2012
  }, {
    "title": "A stochastic quasi-newton method for large-scale optimization",
    "authors": ["Byrd", "Richard H", "S.L. Hansen", "Nocedal", "Jorge", "Singer", "Yoram"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2016
  }, {
    "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
    "authors": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Characterization of superlinear convergence and its application to quasi-Newton methods",
    "authors": ["Dennis Jr.", "John E", "Moré", "Jorge J"],
    "venue": "Math. Comp.,",
    "year": 1974
  }, {
    "title": "A new approach to variable metric algorithms",
    "authors": ["Fletcher", "Roger"],
    "venue": "The Computer Journal,",
    "year": 1970
  }, {
    "title": "Tail bounds for stochastic approximation",
    "authors": ["Friedlander", "Michael P", "Goh", "Gabriel"],
    "venue": "arXiv preprint arXiv:1304.5586,",
    "year": 2013
  }, {
    "title": "Competing with the empirical risk minimizer in a single pass",
    "authors": ["Frostig", "Roy", "Ge", "Rong", "Kakade", "Sham M", "Sidford", "Aaron"],
    "venue": "In COLT, pp",
    "year": 2015
  }, {
    "title": "Quasi-Newton methods: Superlinear convergence without line search for self-concordant functions",
    "authors": ["Gao", "Wenbo", "Goldfarb", "Donald"],
    "venue": "in review",
    "year": 2016
  }, {
    "title": "A family of variable-metric methods derived by variational means",
    "authors": ["Goldfarb", "Donald"],
    "venue": "Mathematics of Computation,",
    "year": 1970
  }, {
    "title": "Linear convergence of stochastic Frank-Wolfe variants",
    "authors": ["Goldfarb", "Donald", "Iyengar", "Garud", "Zhou", "Chaoxu"],
    "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Stochastic block BFGS : Squeezing more curvature out of data",
    "authors": ["Gower", "Robert", "Goldfarb", "Donald", "Richtárik", "Peter"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["Johnson", "Rie", "Zhang", "Tong"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "A linearly-convergent stochastic L-BFGS algorithm",
    "authors": ["Moritz", "Philipp", "Nishihara", "Robert", "Jordan", "Michael I"],
    "venue": "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Robust stochastic approximation approach to stochastic programming",
    "authors": ["Nemirovski", "Arkadi", "Juditsky", "Anatoli", "Lan", "Guanghui", "Shapiro", "Alexander"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2009
  }, {
    "title": "Interior-point polynomial algorithms in convex programming",
    "authors": ["Nesterov", "Yurii", "Nemirovski", "Arkadi"],
    "venue": "Society for Industrial and Applied Mathematics, Philadelphia,",
    "year": 1994
  }, {
    "title": "Updating quasi-Newton matrices with limited",
    "authors": ["Nocedal", "Jorge"],
    "venue": "storage. Math. Comp.,",
    "year": 1980
  }, {
    "title": "Some global convergence properties of a variable metric algorithm for minimization without exact line searches",
    "authors": ["Powell", "Michael J. D"],
    "venue": "Nonlinear Programming, volume IX. SIAMAMS Proceedings,",
    "year": 1976
  }, {
    "title": "A superlinearly-convergent proximal newton-type method for the optimization of finite sums",
    "authors": ["Rodomanov", "Anton", "Kropotov", "Dmitry"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Minimizing finite sums with the stochastic average gradient",
    "authors": ["Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis"],
    "venue": "Mathematical Programming,",
    "year": 2013
  }, {
    "title": "Conditioning of quasi-Newton methods for function minimization",
    "authors": ["Shanno", "David F"],
    "venue": "Mathematics of Computation,",
    "year": 1970
  }, {
    "title": "Composite self-concordant minimization",
    "authors": ["Tran-Dinh", "Quoc", "Kyrillidis", "Anastasios", "Cevher", "Volkan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "Weak Convergence and Empirical Processes",
    "authors": ["W. van der Vaart", "Aad", "Wellner", "Jon A"],
    "year": 1996
  }, {
    "title": "Disco: Distributed optimization for self-concordant empirical loss",
    "authors": ["Zhang", "Yuchen", "Xiao", "Lin"],
    "venue": "In JMLR: Workshop and Conference Proceedings,",
    "year": 2015
  }],
  "id": "SP:02e0eb2254bf3ece9d02e05d2bc5cb82f05376ee",
  "authors": [{
    "name": "Chaoxu Zhou",
    "affiliations": []
  }, {
    "name": "Wenbo Gao",
    "affiliations": []
  }, {
    "name": "Donald Goldfarb",
    "affiliations": []
  }],
  "abstractText": "We propose a novel class of stochastic, adaptive methods for minimizing self-concordant functions which can be expressed as an expected value. These methods generate an estimate of the true objective function by taking the empirical mean over a sample drawn at each step, making the problem tractable. The use of adaptive step sizes eliminates the need for the user to supply a step size. Methods in this class include extensions of gradient descent (GD) and BFGS. We show that, given a suitable amount of sampling, the stochastic adaptive GD attains linear convergence in expectation, and with further sampling, the stochastic adaptive BFGS attains R-superlinear convergence. We present experiments showing that these methods compare favorably to SGD.",
  "title": "Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values"
}