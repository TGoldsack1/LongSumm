{
  "sections": [{
    "heading": "1 Introduction",
    "text": "The covariance matrix C of an n-dimensional distribution is an integral part of data analysis, with numerous occurrences in machine learning and signal processing. It is therefore crucial to understand how close is the sample covariance, i.e., the matrix C̃ estimated from a finite number of samples m, to the actual covariance matrix. Following developments in the tools for the concentration of measure, (Vershynin, 2012) showed that a sample size of m = O(n) is up to iterated logarithmic factors sufficient for all distributions with finite fourth moment supported in a centered Euclidean ball of radius O( √ n). Similar results hold for sub-exponential (Adamczak et al., 2010) and finite second moment distributions (Rudelson, 1999).\nWe take an alternative standpoint and ask if we can do\n1École Polytechnique Fédérale de Lausanne, Switzerland. Correspondence to: Andreas Loukas<andreas.loukas@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nbetter when only a subset of the spectrum is of interest. Concretely, our objective is to characterize how many samples are sufficient to guarantee that an eigenvector and/or eigenvalue of the sample and actual covariance matrices are, respectively, sufficiently close. Our approach is motivated by the observation that methods that utilize the covariance commonly prioritize the estimation of principal eigenspaces. For instance, in (local) principal component analysis we are usually interested in the direction of the first few eigenvectors (Berkmann & Caelli, 1994; Kambhatla & Leen, 1997), where in linear dimensionality reduction one projects the data to the span of the first few eigenvectors (Jolliffe, 2002; Frostig et al., 2016). In the nonasymptotic regime, an analysis of these methods hinges on characterizing how close are the principal eigenvectors and eigenspaces of the sample and actual covariance matrices.\nOur finding is that the “spectral leaking” occurring in the eigenvector estimation is strongly concentrated along the eigenvalue axis. In other words, the eigenvector ũi of the sample covariance is far less likely to lie in the span of an eigenvector uj of the actual covariance when the eigenvalue distance |λi − λj | is large, and the concentration of the distribution in the direction of uj is small. This agrees with the intuition that principal components are easier to estimate, exactly because they are more likely to appear in the samples of the distribution.\nWe provide a mathematical argument confirming this phenomenon. Under fairly general conditions, we prove that\nm = O\n( k2j\n(λi − λj)2\n) and m = O ( k2i λ2i ) (1)\nsamples are asymptotically almost surely (a.a.s.) sufficient to guarantee that |〈ũi, uj〉| and |λ̃i−λi|/λi, respectively, is small for all distributions with finite second moment. Here, k2j is a measure of the concentration of the distribution in the direction of uj . We also attain a high probability bound for sub-gaussian distributions supported in a centered Euclidean ball. Interestingly, our results lead to sample estimates for linear dimensionality reduction, and suggest that linear reduction is feasible even from few samples.\nTo the best of our knowledge, these are the first nonasymptotic results concerning the eigenvectors of the sam-\nple covariance of non-Normal distributions. Previous studies have intensively investigated the limiting distribution of the eigenvalues of a sample covariance matrix (Silverstein & Bai, 1995; Bai, 1999), such as the smallest and largest eigenvalues (Bai & Yin, 1993) and the eigenvalue support (Bai & Silverstein, 1998). Eigenvectors and eigenprojections have attracted less attention; the main research thrust entails using tools from the theory of large-dimensional matrices to characterize limiting distributions (Anderson, 1963; Girko, 1996; Schott, 1997; Bai et al., 2007) and it has limited applicability in the nonasymptotic setting where the sample size m is small and n cannot be arbitrary large.\nDifferently, we use techniques from perturbation analysis and non-asymptotic concentration of measure. However, in contrast to arguments commonly used to reason about eigenspaces (Davis & Kahan, 1970; Yu et al., 2015; Huang et al., 2009; Hunter & Strohmer, 2010), our bounds can characterize weighted linear combinations of 〈ũi, uj〉2 over i and j, and do not depend on the minimal eigenvalue gap separating two eigenspaces but rather on all eigenvalue differences. The latter renders them useful to many real datasets, where the eigenvalue gap is not significant but the eigenvalue magnitudes decrease sufficiently fast.\nWe also note two recent works targeting the nonassymptotic regime of Normal distributions. Shaghaghi and Vorobyov recently characterized the first two moments of the subspace projection error, a result which implies sample estimates (Shaghaghi & Vorobyov, 2015), but is restricted to specific projectors. A refined concentration analysis for spectral projectors of Normal distributions was also presented in (Koltchinskii & Lounici, 2015). Finally, we remark that there exist alternative estimators for the spectrum of the covariance with better asymptotic properties (Ahmed, 1998; Mestre, 2008). Instead, we here focus on the standard estimates, i.e., the eigenvalues and eigenvectors of the sample covariance."
  }, {
    "heading": "2 Problem Statement and Main Results",
    "text": "Let x ∈ Cn be a sample of a multivariate distribution and denote by x1, x2, . . . , xm the m independent samples used\nto form the sample covariance, defined as\nC̃ = m∑ p=1 (xp − x̄)(xp − x̄)∗ m , (2)\nwhere x̄ is the sample mean. Denote by ui the eigenvector of C associated with eigenvalue λi, and correspondingly for the eigenvectors ũi and eigenvalues λ̃i of C̃, such that λ1 ≥ λ2 ≥ . . . ≥ λn. We ask:\nProblem 1. How many samples are sufficient to guarantee that the inner product |〈ũi, uj〉| = |ũ∗i uj | and the eigenvalue gap |δλi| = |λ̃i − λi| is smaller than some constant t with probability larger than ?\nClearly, when asking that all eigenvectors and eigenvalues of the sample and actual covariance matrices are close, we will require at least as many samples as needed to ensure that ‖C̃ − C‖2 ≤ t. However, we might do better when only a subset of the spectrum is of interest. The reason is that inner products |〈ũi, uj〉| are strongly concentrated along the eigenvalue axis. To illustrate this phenomenon, let us consider the distribution constructed by the n = 784 pixel values of digit ‘1’ in the MNIST database. Figure 1, compares the eigenvectors uj of the covariance computed from all 6742 images, to the eigenvectors ũi of the sample covariance matrices C̃ computed from a random subset of m = 10, 100, 500, and 1000 samples. For each i = 1, 4, 20, 100, we depict at λj the average of |〈ũi, uj〉| over 100 sampling draws. We observe that: (i) The magnitude of 〈ũi, uj〉 is inversely proportional to their eigenvalue gap |λi−λj |. (ii) Eigenvector ũj mostly lies in the span of eigenvectors uj over which the distribution is concentrated.\nWe formalize these statements in two steps."
  }, {
    "heading": "2.1 Perturbation arguments",
    "text": "First, we work in the setting of Hermitian matrices and notice the following inequality:\nTheorem 3.2. For Hermitian matricesC and C̃ = δC+C, with eigenvectors uj and ũi respectively, the inequality\n|〈ũi, uj〉| ≤ 2 ‖δCuj‖2 |λi − λj | ,\nholds for sgn(λi − λj) 2λ̃i > sgn(λi − λj)(λi + λj) and λi 6= λj .\nThe above stands out from standard eigenspace perturbation results, such as the sin(Θ) Theorem (Davis & Kahan, 1970) and its variants (Huang et al., 2009; Hunter & Strohmer, 2010; Yu et al., 2015) for three main reasons:\nFirst, Theorem 3.2 characterizes the angle between any pair of eigenvectors allowing us to jointly bound any linear combination of inner-products. Though this often proves handy (c.f. Section 5), it is infeasible using sin(Θ)-type arguments. Second, classical bounds are not appropriate for a probabilistic analysis as they feature ratios of dependent random variables (corresponding to perturbation terms). In the analysis of spectral clustering, this complication was dealt with by assuming that |λi − λj | ≤ |λ̃i − λj | (Hunter & Strohmer, 2010). We weaken this condition at a cost of a multiplicative factor. In contrast to previous work, we also prove that the condition is met a.a.s. Third, previous bounds are expressed in terms of the minimal eigenvalue gap between eigenvectors lying in the interior and exterior of the subspace of interest. This is a limiting factor in practice as it renders the results only amenable to situations where there is a very large eigenvalue gap separating the subspaces. The proposed result improves upon this by considering every eigenvalue difference."
  }, {
    "heading": "2.2 Concentration of measure",
    "text": "The second part of our analysis focuses on the covariance and has a statistical flavor. In particular, we give an answer to Problem 1 for various families of distributions.\nIn the context of distributions with finite second moment, we prove in Section 4.1 that:\nTheorem 4.1. For any two eigenvectors ũi and uj of the sample and actual covariance respectively, and for any real number t > 0:\nP(|〈ũi, uj〉| ≥ t) ≤ 1\nm ( 2kj t |λi − λj | )2 , (3)\nsubject to the same conditions as Theorem 3.2.\nFor eigenvalues, we have the following corollary:\nCorollary 2.1. For any eigenvalues λi and λ̃i of C and C̃, respectively, and for any t > 0, we have\nP\n( |λ̃i − λi|\nλi ≥ t ) ≤ 1 m ( ki λi t )2 .\nTerm kj = (E [ ‖xx∗uj‖22 ] − λ2j )1/2 captures the tendency of the distribution to fall in the span of uj : the smaller the tail in the direction of uj the less likely we are going to confuse ũi with uj .\nFor normal distributions, we have that k2j = λ 2 j + λj tr(C) and the number of samples needed for |〈ũi, uj〉| to be small is m = O(tr(C)/λ2i ) when λj = O(1) and m = O(λ −2 i ) when λj = O(tr(C)−1). Thus for normal distributions, principal components ui and uj with min{λi/λj , λi} = Ω(tr(C)1/2) can be distinguished given a constant number of samples. On the other hand, estimating λi with small relative error requires m = O(tr(C)/λi) samples and can thus be achieved from very few samples when λi is large1.\nIn Section 4.2, we also give a sharp bound for the family of distributions supported within a ball (i.e., ‖x‖ ≤ r a.s.).\nTheorem 4.2. For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c s.t. for any real number t > 0,\nP(|〈ũi, uj〉| ≥ t) ≤ exp ( 1− cmΦij(t) 2\nλj ‖x‖2Ψ2\n) , (4)\nwhere Φij(t) = |λi−λj | t−2λj 2 (r2/λj−1)1/2\n−2 ‖x‖Ψ2 and subject to the same conditions as Theorem 3.2.\nAbove, ‖x‖Ψ2 is the sub-gaussian norm, for which we usually have ‖x‖Ψ2 = O(1) (Vershynin, 2010). As such, the theorem implies that, whenever λi λj = O(1), the sample requirement is with high probability m = O(r2/λ2i ).\nThese theorems solidify our experimental findings shown in Figure 1 and provide a concrete characterization of the relation between the spectrum of the sample and actual covariance matrix as a function of the number of samples, the eigenvalue gap, and the distribution properties. As exemplified in Section 5 for linear dimensionality reduction, we believe that our results carry strong implications for the non-asymptotic analysis of PCA-based methods."
  }, {
    "heading": "3 Perturbation Arguments",
    "text": "Before focusing on the sample covariance matrix, it helps to study 〈ũi, uj〉 in the setting of Hermitian matrices. The presentation of the results is split in three parts. Section 3.1 starts by studying some basic properties of inner products of the form 〈ũi, uj〉, for any i and j. The results are used in Section 3.2 to provide a first bound on the angle between two eigenvectors, and refined in Section 3.3."
  }, {
    "heading": "3.1 Basic observations",
    "text": "We start by noticing an exact relation between the angle of a perturbed eigenvector and the actual eigenvectors of C.\nLemma 3.1. For every i and j in 1, 2, . . . , n, the relation (λ̃i − λj) (ũ∗i uj) = ∑n `=1(ũ ∗ i u`) (u ∗ jδCu`) holds .\n1Though the same cannot be stated about the absolute error |δλi|, that is smaller for small λi.\nProof. The proof follows from a modification of a standard argument in perturbation theory. We start from the definition C̃ ũi = λ̃i ũi and write\n(C + δC) (ui + δui) = (λi + δλi) (ui + δui). (5)\nExpanded, the above expression becomes\nCδui + δCui + δCδui\n= λiδui + δλiui + δλiδui, (6)\nwhere we used the fact that Cui = λiui to eliminate two terms. To proceed, we substitute δui = ∑n j=1 βijuj , where βij = δu∗i uj , into (6) and multiply from the left by u∗j , resulting to:\nn∑ `=1 βiju ∗ jCu` + u ∗ jδCui + n∑ `=1 βiju ∗ jδCu`\n= λi n∑ `=1 βiju ∗ ju` + δλiu ∗ jui + δλi n∑ `=1 βiju ∗ ju` (7)\nCancelling the unnecessary terms and rearranging, we have\nδλiu ∗ jui + (λi + δλi − λj)βij\n= u∗jδCui + n∑ `=1 βiju ∗ jδCu`. (8)\nAt this point, we note that (λi + δλi − λj) = λ̃i − λj and furthermore that βij = ũ∗i uj − u∗i uj . With this in place, equation (8) becomes\nδλiu ∗ jui + (λ̃i − λj) (ũ∗i uj − u∗i uj)\n= u∗jδCui + n∑ `=1 (ũ∗i u`)u ∗ jδCu` − u∗jδCui. (9)\nThe proof completes by noticing that, in the left hand side, all terms other than (λ̃i − λj) ũ∗i uj fall-off, either due to u∗i uj = 0, when i 6= j, or because δλi = λ̃i−λj , o.w.\nAs the expression reveals, 〈ũi, uj〉 depends on the orientation of ũi with respect to all other u`. Moreover, the angles between eigenvectors depend not only on the minimal gap between the subspace of interest and its complement space (as in the sin(Θ) theorem), but on every difference λ̃i−λj . This is a crucial ingredient to a tight bound, that will be retained throughout our analysis."
  }, {
    "heading": "3.2 Bounding arbitrary angles",
    "text": "We proceed to decouple the inner products.\nTheorem 3.1. For any Hermitian matrices C and C̃ = δC +C, with eigenvectors uj and ũi respectively, we have that |λ̃i − λj | |〈ũi, uj〉| ≤ ‖δC uj‖2.\nProof. We rewrite Lemma 3.1 as\n(λ̃i − λj)2(ũ∗i uj)2 = ( n∑ `=1 (ũ∗i u`) (u ∗ jδCu`) )2 . (10)\nWe now use the Cauchy-Schwartz inequality\n(λ̃i − λj)2(ũ∗i uj)2 ≤ n∑ `=1 (ũ∗i u`) 2 n∑ `=1 (u∗jδCu`) 2\n= n∑ `=1 (u∗jδCu`) 2 = ‖δC uj‖22, (11)\nwhere in the last step we exploited Lemma 3.2. The proof concludes by taking a square root at both sides of the inequality. Lemma 3.2. n∑̀ =1 (u∗jδCu`) 2 = ‖δC uj‖22. Proof. We first notice that u∗jδCu` is a scalar and equal to its transpose. Moreover, δC is Hermitian as the difference of two Hermitian matrices. We therefore have that n∑ `=1 (u∗jδCu`) 2 = n∑ `=1 u∗jδCu`u ∗ `δCuj\n= u∗jδC n∑ `=1 (u`u ∗ ` )δCuj = u ∗ jδCδCuj = ‖δCuj‖22,\nmatching our claim."
  }, {
    "heading": "3.3 Refinement",
    "text": "As a last step, we move all perturbation terms to the numerator, at the expense of a multiplicative constant factor.\nTheorem 3.2. For Hermitian matricesC and C̃ = δC+C, with eigenvectors uj and ũi respectively, the inequality\n|〈ũi, uj〉| ≤ 2 ‖δCuj‖2 |λi − λj | ,\nholds for sgn(λi − λj) 2λ̃i > sgn(λi − λj)(λi + λj) and λi 6= λj .\nProof. Adding and subtracting λi from the left side of the expression in Lemma 3.1 and from definition we have\n(δλi + λi − λj) (ũ∗i uj) = n∑ `=1 (ũ∗i u`) (u ∗ jδCu`). (12)\nFor λi 6= λj , the above expression can be re-written as\n|ũ∗i uj | =\n∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`)− δλi (ũ∗i uj) ∣∣∣∣ |λi − λj |\n≤ 2 max  ∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`) ∣∣∣∣ |λi − λj | , |δλi| |ũ∗i uj | |λi − λj |  . (13)\nLet us examine the right-hand side inequality carefully. Obviously, when the condition |λi − λj | ≤ 2 |δλi| is not met, the right clause of (13) is irrelevant. Therefore, for |δλi| < |λi − λj | /2 the bound simplifies to\n|ũ∗i uj | ≤ 2\n∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`) ∣∣∣∣ |λi − λj | . (14)\nSimilar to the proof of Theorem 3.1, applying the CauchySchwartz inequality we have that\n|ũ∗i uj | ≤ 2\n√ n∑̀ =1 (ũ∗i u`) 2 n∑̀ =1 (u∗jδCu`) 2\n|λi − λj | = 2 ‖δCuj‖2 |λi − λj | ,\n(15)\nwhere in the last step we used Lemma 3.2. To finish the proof we notice that, due to Theorem 3.2, whenever |λi − λj | ≤ |λ̃i − λj |, one has\n|ũ∗i uj | ≤ ‖δC uj‖2 |λ̃i − λj | ≤ ‖δC uj‖2 |λi − λj | < 2 ‖δCuj‖2 |λi − λj | . (16)\nOur bound therefore holds for the union of intervals |δλi| < |λi − λj | /2 and |λi− λj | ≤ |λ̃i− λj |, i.e., for λ̃i > (λi + λj)/2 when λi > λj and for λ̃i < (λi + λj)/2 when λi < λj ."
  }, {
    "heading": "4 Concentration of Measure",
    "text": "This section builds on the perturbation results of Section 3 to characterize how far any inner product 〈ũi, uj〉 and eigenvalue λ̃i are from the ideal estimates.\nBefore proceeding, we remark on some simplifications employed in the following. W.l.o.g., we will assume that the mean E[x] is zero. In addition, we will assume the perspective of Theorem 3.2, for which the inequality sgn(λi− λj) 2λ̃i > sgn(λi−λj)(λi+λj) holds. This event is shown to occur a.a.s. when the gap and the sample size are sufficiently large, but it is convenient to assume that it happens almost surely. In fact, removing this assumption is possible (see Section 4.1.2), but it is largely not pursued here as it leads to less elegant and sharp estimates."
  }, {
    "heading": "4.1 Distributions with finite second moment",
    "text": "Our first flavor of results is based on a variant of the Tchebichef inequality and holds for any distribution with finite second moment, though only with moderate probability estimates."
  }, {
    "heading": "4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES",
    "text": "We start with the concentration of inner-products |〈ũi, uj〉|.\nTheorem 4.1. For any two eigenvectors ũi and uj of the sample and actual covariance respectively, with λi 6= λj , and for any real number t > 0, we have\nP(|〈ũi, uj〉| ≥ t) ≤ 1\nm ( 2 kj t |λi − λj | )2 for sgn(λi − λj) 2λ̃i > sgn(λi − λj)(λi + λj) and kj =( E [ ‖xx∗uj‖22 ] − λ2j )1/2 .\nProof. According to a variant of Tchebichef’s inequality (Sarwate, 2013), for any random variable X and for any real numbers t > 0 and α:\nP(|X − α| ≥ t) ≤ Var[X] + (E[X]− α) 2\nt2 . (17)\nSetting X = 〈ũi, uj〉 and α = 0, we have\nP(|〈ũi, uj〉| ≥ t) ≤ Var[〈ũi, uj〉] + E[〈ũi, uj〉]2\nt2 = E [ 〈ũi, uj〉2 ] t2 ≤ 4E [ ‖δCuj‖22 ] t2(λi − λj)2 , (18)\nwhere the last inequality follows from Theorem 3.2. We continue by expanding δC using the definition of the eigenvalue decomposition and substituting the expectation.\nE [ ‖δCuj‖22 ] = E [ ‖C̃uj − λjuj‖22 ] = E [ u∗j (C̃ − λj)(C̃ − λj)uj\n] = E [ u∗j C̃ 2uj ] + λ2j − 2λju∗jE [ C̃ ] uj\n= E [ u∗j C̃ 2uj ] − λ2j . (19)\nIn addition, E [ u∗j C̃ 2uj ] = m∑ p,q=1 u∗j E [ (xpx ∗ p)(xqx ∗ q) ] m2 uj\n= ∑ p 6=q u∗j E [ xpx ∗ p ] E [ xqx ∗ q ] m2 uj + m∑ p=1 u∗j E [ xpx ∗ pxpx ∗ p ] m2 uj\n= m(m− 1)\nm2 λ2j +\n1 m u∗jE[xx ∗xx∗]uj\n= (1− 1 m )λ2j + 1 m u∗jE[xx ∗xx∗]uj (20)\nand therefore\nE [ ‖δCuj‖22 ] = (1− 1\nm )λ2j +\n1 m u∗jE[xx ∗xx∗]uj − λ2j\n= u∗jE[xx ∗xx∗]uj − λ2j m\n= E [ ‖xx∗uj‖22 ] − λ2j\nm .\nPutting everything together, the claim follows.\nThe following corollary will be very useful when applying our results.\nCorollary 4.1. For any weights wij and real t > 0:\nP ∑ i 6=j wij〈ũi, uj〉2 > t  ≤∑ i6=j\n4wij k 2 j\nmt (λi − λj)2 ,\nwhere kj = ( E [ ‖xx∗uj‖22 ] − λ2j )1/2 and wij 6= 0 when\nλi 6= λj and sgn(λi − λj) 2λ̃i > sgn(λi − λj)(λi + λj).\nProof. We proceed as in the proof of Theorem 4.1:\nP (∑ i 6=j wij〈ũi, uj〉2 ) 1 2 > t  ≤ E [∑ i6=j wij〈ũi, uj〉2 ] t2\n≤ 4 t2 ∑ i 6=j wij E [ ‖δCuj‖22 ] (λi − λj)2\n(21)\nThe claim follows by computing E [ ‖δCuj‖22 ] (as before) and squaring both terms within the probability."
  }, {
    "heading": "4.1.2 EIGENVALUE CONCENTRATION",
    "text": "Though perhaps less sharp than what is currently known (e.g., see (Silverstein & Bai, 1995; Bai & Silverstein, 1998) for the asymptotic setting), it might be interesting to observe that a slight modification of the same argument can be used to characterize the eigenvalue relative difference, and as a consequence the main condition of Theorem 4.1.\nCorollary 4.2. For any eigenvalues λi and λ̃i of C and C̃, respectively, and for any t > 0, we have\nP\n( |λ̃i − λi|\nλi ≥ t ) ≤ 1 m ( ki λi t )2 ,\nwhere ki = (E [ ‖xx∗ui‖22 ] − λi)1/2.\nProof. Directly from the Bauer-Fike theorem (Bauer & Fike, 1960) one sees that\n|δλi| ≤ ‖C̃ui − λiui‖2 = ‖δCui‖2. (22)\nThe proof is then identical to that of Theorem 4.1.\nUsing this, we find that the eventE = {sgn(λi−λj) 2λ̃i > sgn(λi − λj)(λi + λj)} occurs with probability at least\nP(E) ≥ P ( |λ̃i − λi| <\n|λi − λj | 2\n) > 1− 2k 2 i\nm|λi − λj | .\nTherefore, one eliminates the condition from Theorem 4.1’s statement by relaxing the bound to\nP(|〈ũi, uj〉| ≥ t) ≤ P(|〈ũi, uj〉| ≥ t |E) + (1−P(E))\n< 2\nm|λi − λj | ( 2k2j t2|λi − λj | + k2i ) . (23)"
  }, {
    "heading": "4.1.3 THE INFLUENCE OF THE DISTRIBUTION",
    "text": "As seen by the straightforward inequality E [ ‖xx∗uj‖22 ] ≤\nE [ ‖x‖42 ] , kj connects to the kurtosis of the distribution. However, it also captures the tendency of the distribution to fall in the span of uj .\nTo see this, we will work with the whitened random vectors ε = C+1/2x, where C+ denotes the Moore–Penrose pseudoinverse of C. In particular,\nk2j = E [ u∗jC 1/2εε∗Cεε∗C1/2uj ] − λ2j\n= λj(E [ ‖Λ1/2U∗εε∗uj‖22 ] − λj)\n= λj ( n∑ `=1 λ`E [ ε̂(`)2ε̂(j)2 ] − λj ) , (24)\nwhere ε̂ = U∗ε. It is therefore easier to untangle the spaces spanned by ũi and uj when the variance of the distribution along the latter space is small (the expression is trivially minimized when λj → 0) or when the variance is entirely contained along that space (the expression is also small when λi = 0 for all i 6= j). In addition, it can be seen that distributions with fast decaying tails allow for better principal component identification (E [ ε̂(j)4 ] is a measure of kurtosis over the direction of uj).\nFor the particular case of a Normal distribution, we provide a closed-form expression.\nCorollary 4.3. For a Normal distribution, we have k2j = λj (λj + tr(C)).\nProof. For a centered and normal distribution with identity covariance, the choice of basis is arbitrary and the vector ε̂ = U∗ε is also zero mean with identity covariance. Moreover, for every ` 6= j we can write E [ ε̂(`)2ε̂(j)2 ] =\nE [ ε̂(`)2 ] E [ ε̂(j)2 ] = 1. This implies that\nE [ ‖xx∗uj‖22 ] = λ2j E [ ε̂(j)4 ] + λj n∑ ` 6=j λ`\n= λ2j (3− 1) + λj tr(C) = 2λ2j + λj tr(C) (25)\nand, accordingly, k2j = λj (λj + tr(C))."
  }, {
    "heading": "4.2 Distributions supported in a Euclidean ball",
    "text": "Our last result provides a sharper probability estimate for the family of sub-gaussian distributions supported in a centered Euclidean ball of radius r, with their Ψ2-norm\n‖x‖Ψ2 = sup y∈Sn−1 ‖〈x, y〉‖ψ2 , (26)\nwhere Sn−1 is the unit sphere and with the ψ2-norm of a random variable X defined as\n‖X‖ψ2 = sup p≥1\np−1/2E[|X|p]1/p . (27)\nOur setting is therefore similar to the one used to study covariance estimation (Vershynin, 2012). Due to space constraints, we refer the reader to the excellent review article (Vershynin, 2010) for an introduction to sub-gaussian distributions as a tool for non-asymptotic analysis of random matrices.\nTheorem 4.2. For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c, independent of the sample size, such that for any real number t > 0,\nP(|〈ũi, uj〉| ≥ t) ≤ exp ( 1− cmΦij(t) 2\nλj ‖x‖2Ψ2\n) , (28)\nwhere Φij(t) = |λi−λj | t−2λj 2 (r2/λj−1)1/2\n− 2 ‖x‖Ψ2 , λi 6= λj and sgn(λi − λj) 2λ̃i > sgn(λi − λj)(λi + λj).\nProof. We start from the simple observation that, for every upper bound B of |〈ũi, uj〉| the relation P(|〈ũi, uj〉| > t) ≤ P(B > t) holds. To proceed therefore we will construct a bound with a known tail. As we saw in Sections 3.3 and 4.1,\n|〈ũi, uj〉| ≤ 2 ‖δCuj‖2 |λi − λj |\n= 2 ∥∥∥(1/m)∑mp=1(xpx∗puj − λjuj)∥∥∥ 2\n|λi − λj | ≤ 2 ∑m p=1 ∥∥xpx∗puj − λjuj∥∥2 m |λi − λj |\n= 2 ∑m p=1 √ (u∗jxp)\n2(x∗pxp)− 2λj(u∗jxp)2 + λ2j m |λi − λj |\n= 2 ∑m p=1 √ (u∗jxp)\n2(‖xp‖22 − λj) + λ2j m |λi − λj |\n(29)\nAssuming further that ‖x‖2 ≤ r, and since the numerator is minimized when ‖xp‖22 approaches λj , we can write for every sample x = C1/2ε:√\n(u∗jx) 2(‖x‖22 − λj) + λ2j ≤ √ (u∗jx) 2(r2 − λj) + λ2j\n= √ λj(u∗jε) 2(r2 − λj) + λ2j\n≤ |u∗jε| √ λjr2 − λ2j + λj , (30)\nwhich is a shifted and scaled version of the random variable |ε̂(j)| = |u∗jε|. Setting a = (λjr2 − λ2j )1/2, we have\nP(|〈ũi, uj〉| ≥ t) ≤ P ( 2 ∑m p=1(|ε̂p(j)| a+ λj) m |λi − λj | ≥ t )\n= P ( m∑ p=1 (|ε̂p(j)| a+ λj) ≥ 0.5mt |λi − λj | )\n= P ( m∑ p=1 |ε̂p(j)| ≥ m (0.5 t |λi − λj | − λj) a ) . (31)\nBy Lemma 4.1 however, the left hand side is a sum of independent sub-gaussian variables. Since the summands are not centered, we expand each |ε̂p(j)| = zp + E[|ε̂p(j)|] in terms of a centered sub-gaussian zp with the same ψ2norm. Furthermore, by Jensen’s inequality and Lemma 4.1\nE[|ε̂p(j)|] ≤ E [ ε̂p(j) 2 ]1/2 ≤ 2\nλj ‖x‖Ψ2 . (32)\nTherefore, if we set Φij(t) = (0.5 |λi−λj | t−λj)\n(r2/λj−1)1/2 − 2 ‖x‖Ψ2\nP(|〈ũi, uj〉| ≥ t) ≤ P ( m∑ p=1 zp ≥ mΦij(t) λj ) . (33)\nMoreover, by the rotation invariance principle, the left hand side of the last inequality is a sub-gaussian with ψ2-norm smaller than (c1 ∑m p=1 ‖zp‖ 2 ψ2 )1/2 = (c1m) 1/2 ‖z‖ψ2 ≤ (c1m/λj) 1/2 ‖x‖Ψ2 , for some absolute constant c1. As a consequence, there exists an absolute constant c2, such that for each θ > 0:\nP (∣∣∣∣∣ m∑ p=1 zp ∣∣∣∣∣ ≥ θ ) ≤ exp ( 1− c2 θ 2λj m ‖x‖2Ψ2 ) . (34)\nSubstituting θ = mΦij(t)/λj , we have\nP(|〈ũi, uj〉| ≥ t) ≤ exp ( 1− c2m 2 Φij(t) 2λj\nmλ2j ‖x‖ 2 Ψ2\n)\n= exp ( 1− c2mΦij(t) 2\nλj ‖x‖2Ψ2\n) , (35)\nwhich is the desired bound.\nLemma 4.1. If x is a sub-gaussian random vector and ε = C+1/2x, then for every i, the random variable ε̂(i) = u∗i ε is also sub-gaussian, with ‖ε̂(i)‖ψ2 ≤ ‖x‖Ψ2 / √ λi.\nProof. Notice that\n‖x‖Ψ2 = sup y∈Sn−1 ‖〈x, y〉‖ψ2= sup y∈Sn−1 ∥∥∥∥∥∥ n∑ j=1 λ 1/2 j (u ∗ jy)(u ∗ jε) ∥∥∥∥∥∥ ψ2\n≥ ∥∥∥∥∥∥ n∑ j=1 λ 1/2 j (u ∗ jui)ε̂(j) ∥∥∥∥∥∥ ψ2 = λ 1/2 i ‖ε̂(i)‖ψ2 , (36)\nwhere, for the last inequality, we set y = ui."
  }, {
    "heading": "5 Application to Dimensionality Reduction",
    "text": "To emphasize the utility of our results, in the following we consider the practical example of linear dimensionality reduction. We show that a direct application of our bounds leads to upper estimates on the sample requirement.\nIn terms of mean squared error, the optimal way to reduce the dimension of a sample x of a distribution is by projecting it over the subspace of the covariance with maximum variance. Denote by Ik the diagonal matrix with the first k diagonal entries equal to one and the rest zero. When the actual covariance is known, the expected energy loss induced by the Pkx = IkU∗x projection is\nloss(Pk) = E [ ‖x‖22 − ‖Pkx‖22 ] E[‖x‖22] = ∑ i>k λi tr(C) . (37)\nHowever, when the projector P̃k = IkŨ∗ is constructed from the sample covariance, we have\nloss(P̃k) = E [ ‖x‖22 − ‖P̃kx‖22 ] E[‖x‖22]\n=\n∑n i=1 λi − tr(IkŨ∗UΛU∗Ũ)\ntr(C)\n=\n∑n i=1 λi − ∑ i≤k,j(ũ ∗ i uj) 2λj\ntr(C) (38)\nwith the expectation taken over the to-be-projected vectors x, but not the samples used to estimate the covariance. After slight manipulation, one finds that\nloss(P̃k) = loss(Pk) +\n∑ i≤k,j 6=i (ũ∗i uj) 2(λi − λj)\ntr(C) . (39)\nThe loss difference has an intuitive interpretation: when reducing the dimension with P̃k one looses either by discarding useful energy (terms j > k), or by displacing kept components within the permissible eigenspace (terms j ≤ k). Note also that all terms with j < i are negative and can be excluded from the sum if we are satisfied we an upper estimate2.\nIt is an implication of (39) and Corollary 4.1 that, when its conditions hold, for any distribution and t > 0\nP ( loss(P̃k) > loss(Pk) + t\ntr(C) ) ≤ ∑ i≤k j>i 4 k2j mt |λi − λj | .\nObserve that the loss difference becomes particularly small whenever k is small: (i) the terms in the sum are fewer and (ii) the magnitude of each term decreases (due to |λi−λj |).\n2A similar approach could also be utilized to derive a lower bound of the quantity loss(P̃k)− loss(Pk).\nThis phenomenon is also numerically verified in Figure 2 for the distribution of the images featuring digit ‘3’ in MNIST (total 6131 images with n = 784 pixels each). The figure depicts for different k how many samples are required such that the loss difference is smaller than a tolerance threshold, here 0.02, 0.05, and 0.1. Each point in the figure corresponds to an average over 10 sampling draws. The trends featured in these numerical results agree with our theoretical intuition. Moreover they illustrate that for modest k the sample requirement is far smaller than n.\nIt is also interesting to observe that for covariance matrices that are (approximately) low-rank, we obtain estimates reminiscent of compressed sensing (Candès et al., 2011), in the sense that the sample requirement becomes a function of the non-zero eigenvalues. Though intuitive, with the exception of (Koltchinskii et al., 2016), this dependency of the estimation accuracy on the rank was not transparent in known results for covariance estimation (Rudelson, 1999; Adamczak et al., 2010; Vershynin, 2012)."
  }, {
    "heading": "6 Conclusions",
    "text": "The main contribution of this paper was the derivation of non-asymptotic bounds for the concentration of innerproducts |〈ũi, uj〉| involving eigenvectors of the sample and actual covariance matrices. We also showed how these results can be extended to reason about eigenvalues and we applied them to the non-asymptotic analysis of linear dimensionality reduction.\nWe have identified two interesting directions for further research. The first has to do with obtaining tighter estimates. Especially with regards to our perturbation arguments, we believe that our current bounds on inner products could be sharpened by at least a constant multiplicative factor. The second direction involves using our results for the analysis of methods that utilize the eigenvectors of the covariance, such that principal component projection and regression (Jolliffe, 1982; Frostig et al., 2016)."
  }],
  "year": 2017,
  "references": [{
    "title": "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles",
    "authors": ["Adamczak", "Radosław", "Litvak", "Alexander", "Pajor", "Alain", "Tomczak-Jaegermann", "Nicole"],
    "venue": "Journal of the American Mathematical Society,",
    "year": 2010
  }, {
    "title": "Large-sample estimation strategies for eigenvalues of a wishart matrix. Metrika",
    "authors": ["Ahmed", "SE"],
    "year": 1998
  }, {
    "title": "Asymptotic theory for principal component analysis",
    "authors": ["Anderson", "Theodore Wilbur"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1963
  }, {
    "title": "Methodologies in spectral analysis of large dimensional random matrices, a review",
    "authors": ["Bai", "ZD"],
    "venue": "Statistica Sinica, pp",
    "year": 1999
  }, {
    "title": "Limit of the smallest eigenvalue of a large dimensional sample covariance matrix",
    "authors": ["ZD Bai", "Yin", "YQ"],
    "venue": "The annals of Probability,",
    "year": 1993
  }, {
    "title": "On asymptotics of eigenvectors of large sample covariance matrix",
    "authors": ["ZD Bai", "BQ Miao", "GM Pan"],
    "venue": "The Annals of Probability,",
    "year": 2007
  }, {
    "title": "No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices",
    "authors": ["Bai", "Zhi-Dong", "Silverstein", "Jack W"],
    "venue": "Annals of probability,",
    "year": 1998
  }, {
    "title": "Norms and exclusion theorems",
    "authors": ["Bauer", "Friedrich L", "Fike", "Charles T"],
    "venue": "Numerische Mathematik,",
    "year": 1960
  }, {
    "title": "Computation of surface geometry and segmentation using covariance techniques",
    "authors": ["Berkmann", "Jens", "Caelli", "Terry"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 1994
  }, {
    "title": "Robust principal component analysis",
    "authors": ["Candès", "Emmanuel J", "Li", "Xiaodong", "Ma", "Yi", "Wright", "John"],
    "venue": "Journal of the ACM,",
    "year": 2011
  }, {
    "title": "The rotation of eigenvectors by a perturbation",
    "authors": ["Davis", "Chandler", "Kahan", "William Morton"],
    "venue": "III. SIAM Journal on Numerical Analysis,",
    "year": 1970
  }, {
    "title": "Principal component projection without principal component analysis",
    "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Strong law for the eigenvalues and eigenvectors of empirical covariance matrices",
    "authors": ["V. Girko"],
    "year": 1996
  }, {
    "title": "Spectral clustering with perturbed data",
    "authors": ["Huang", "Ling", "Yan", "Donghui", "Taft", "Nina", "Jordan", "Michael I"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Performance analysis of spectral clustering on compressed, incomplete and inaccurate measurements",
    "authors": ["Hunter", "Blake", "Strohmer", "Thomas"],
    "venue": "arXiv preprint arXiv:1011.0997,",
    "year": 2010
  }, {
    "title": "Principal component analysis",
    "authors": ["Jolliffe", "Ian"],
    "venue": "Wiley Online Library,",
    "year": 2002
  }, {
    "title": "A note on the use of principal components in regression",
    "authors": ["Jolliffe", "Ian T"],
    "venue": "Applied Statistics, pp",
    "year": 1982
  }, {
    "title": "Dimension reduction by local principal component analysis",
    "authors": ["Kambhatla", "Nandakishore", "Leen", "Todd K"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Normal approximation and concentration of spectral projectors of sample covariance",
    "authors": ["Koltchinskii", "Vladimir", "Lounici", "Karim"],
    "venue": "arXiv preprint arXiv:1504.07333,",
    "year": 2015
  }, {
    "title": "Asymptotics and concentration bounds for bilinear forms of spectral projectors of sample covariance",
    "authors": ["Koltchinskii", "Vladimir", "Lounici", "Karim"],
    "venue": "In Annales de l’Institut Henri Poincaré, Probabilités et Statistiques,",
    "year": 2016
  }, {
    "title": "Improved estimation of eigenvalues and eigenvectors of covariance matrices using their sample estimates",
    "authors": ["Mestre", "Xavier"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2008
  }, {
    "title": "Random vectors in the isotropic position",
    "authors": ["Rudelson", "Mark"],
    "venue": "Journal of Functional Analysis,",
    "year": 1999
  }, {
    "title": "Asymptotics of eigenprojections of correlation matrices with some applications in principal components analysis",
    "authors": ["Schott", "James R"],
    "year": 1997
  }, {
    "title": "Subspace leakage analysis of sample data covariance matrix",
    "authors": ["Shaghaghi", "Mahdi", "Vorobyov", "Sergiy A"],
    "venue": "In ICASSP,",
    "year": 2015
  }, {
    "title": "On the empirical distribution of eigenvalues of a class of large dimensional random matrices",
    "authors": ["Silverstein", "Jack W", "Bai", "ZD"],
    "venue": "Journal of Multivariate analysis,",
    "year": 1995
  }, {
    "title": "Introduction to the non-asymptotic analysis of random matrices",
    "authors": ["Vershynin", "Roman"],
    "year": 2010
  }, {
    "title": "How close is the sample covariance matrix to the actual covariance matrix",
    "authors": ["Vershynin", "Roman"],
    "venue": "Journal of Theoretical Probability,",
    "year": 2012
  }, {
    "title": "A useful variant of the davis–kahan theorem for statisticians",
    "authors": ["Yu", "Yi", "Wang", "Tengyao", "Samworth", "Richard J"],
    "year": 2015
  }],
  "id": "SP:cfbb096d2439a5de9e8bfa2809ced2a78fd783db",
  "authors": [{
    "name": "Andreas Loukas",
    "affiliations": []
  }],
  "abstractText": "How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.",
  "title": "How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?"
}