{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 934–945 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n934"
  }, {
    "heading": "1 Introduction",
    "text": "Causation in the physical world has long been a central discussion to philosophers who study casual reasoning and explanation (Ducasse, 1926; Gopnik et al., 2007), to mathematicians or com-\nputer scientists who apply computational approaches to model cause-effect prediction (Pearl et al., 2009), and to domain experts (e.g., medical doctors) who attempt to understand the underlying cause-effect relations (e.g., disease and symptoms) for their particular inquires. Apart from this wide range of topics, this paper investigates a specific kind of causation, the very basic causal relations between a concrete action (expressed in the form of a verb-noun pair such as “cut-cucumber”) and the change of the physical state caused by this action. We call such relations naive physical action-effect relations.\nFor example, given an image as shown in Figure 1, we would have no problem predicting what actions can cause the state of the world depicted in the image, e.g., slicing an apple will likely lead to the state. On the other hand, given a statement “slice an apple”, it would not be hard for us to imagine what state change may happen to the apple. We can make such action-effect prediction because we have developed an understanding of this kind of basic action-effect relations at a very young age (Baillargeon, 2004). What happens to machines? Will artificial agents be able to make the same kind of predictions? The answer is not yet.\nDespite tremendous progress in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the understanding of naive causal relations regarding the physical world. This is one of the bottlenecks in machine intelligence. If artificial agents ever become capable of working with humans as partners, they will need to have this kind of physical action-effect understanding to help them reason, learn, and perform actions.\nTo address this problem, this paper introduces a new task on naive physical action-effect prediction. This task supports both cause predic-\ntion: given an image which describes a state of the world, identify the most likely action (in the form of a verb-noun pair, from a set of candidates) that can result in that state; and effect prediction: given an action in the form of a verb-noun pair, identify images (from a set of candidates) that depicts the most likely effects on the state of the world caused by that action. Note that there could be different ways to formulate this problem, for example, both causes and effects are in the form of language or in the form of images/videos. Here we intentionally frame the action as a language expression (i.e., a verb-noun pair) and the effect as depicted in an image in order to make a connection between language and perception. This connection is important for physical agents that not only can perceive and act, but also can communicate with humans in language.\nAs a first step, we collected a dataset of 140 verb-noun pairs. Each verb-noun pair is annotated with possible effects described in language and depicted in images (where language descriptions and image descriptions are collected separately). We have developed an approach that applies distant supervision to harness web data for bootstrapping action-effect prediction models. Our empirical results have shown that, using a simple bootstrapping strategy, our approach can combine the noisy web data with a small number of seed examples to improve action-effect prediction. In addition, for a new verb-noun pair, our approach can infer its effect descriptions and predict action-effect relations only based on 3 image examples.\nThe contributions of this paper are three folds. First, it introduces a new task on physical actioneffect prediction, a first step towards an under-\nstanding of causal relations between physical actions and the state of the physical world. Such ability is central to robots which not only perceive from the environment, but also act to the environment through planning. To our knowledge, there is no prior work that attempts to connect actions (in language) and effects (in images) in this nature. Second, our approach harnesses the large amount of image data available on the web with minimum supervision. It has shown that physical action-effect models can be learned through a combination of a few annotated examples and a large amount of un-annotated web data. This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have created a dataset for this task, which is available to the community 1. Our bootstrapping approach can serve as a baseline for future work on this topic.\nIn the following sections, we first describe our data collection effort, then introduce the bootstrapping approach for action-effect prediction, and finally present results from our experiments."
  }, {
    "heading": "2 Related Work",
    "text": "In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys-\n1This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html\nical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction.\nThe idea of modeling object physical state change has also been studied in the computer vision community (Fire and Zhu, 2016). Computational models have been developed to infer object states from observations and to further predict future state changes (Zhou and Berg, 2016; Wu et al., 2016, 2017). The action recognition task can be treated as detecting the transformation on object states (Fathi and Rehg, 2013; Yang et al., 2013; Wang et al., 2016). However these previous works only focus on the visual presentation of motion effects. Recent years have seen an increasing amount of work integrating language and vision, for example, visual question answering (Antol et al., 2015; Fukui et al., 2016; Lu et al., 2016), image description generation (Xu et al., 2015; Vinyals et al., 2015), and grounding language to perception (Yang et al., 2016; Roy, 2005; Tellex et al., 2011; Misra et al., 2017). While many approaches require a large amount of training data, recent works have developed zero/few shot learning for language and vision (Mukherjee and Hospedales, 2016; Xu et al., 2016, 2017a,b; Tsai and Salakhutdinov, 2017). Different from these previous works, this paper introduces a new task that connects language with vision for physical action-effect prediction.\nIn the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment (i.e., from images)."
  }, {
    "heading": "3 Action-Effect Data Collection",
    "text": "We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately.\nActions (verb-noun pairs). We selected 40 nouns that represent everyday life objects, most of them are from the COCO dataset (Lin et al., 2014), with a combination of food, kitchen ware, furniture, indoor objects, and outdoor objects. We also identified top 3000 most frequently used verbs from Google Syntactic N-gram dataset (Goldberg and Orwant, 2013) (Verbargs set). And we extracted top frequent verb-noun pairs containing a verb from the top 3000 verbs and a noun in the 40 nouns which hold a dobj (i.e., direct object) dependency relation. This resulted in 6573 candidate verbnoun pairs. As changes to an object can occur at various dimensions (e.g., size, color, location, attachment, etc.), we manually selected a subset of verb-noun pairs based on the following criteria: (1) changes to the objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation.\nEffects Described in Language. The basic knowledge about physical action-effect is so fundamental and shared among humans. It is often presupposed in our communication and not explicitly stated. Thus, it is difficult to extract naive action-effect relations from the existing textual data (e.g., web). This kind of knowledge is also not readily available in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012). To overcome this problem, we applied crowd-sourcing (Amazon Mechanical Turk) and collected a dataset of language descriptions describing effects for each of the 140 verb-noun pairs. The workers were shown a verb-noun pair, and were asked to use their own words and imag-\ninations to describe what changes might occur to the corresponding object as a result of the action. Each verb-noun pair was annotated by 10 different annotators, which has led to a total of 1400 effect descriptions. Table 1 shows some examples of collected effect descriptions. These effect language descriptions allow us to derive seed effect knowledge in a symbolic form.\nEffects Depicted in Images. For each action, three students searched the web and collected a set of images depicting potential effects. Specifically, given a verb-noun pair, each of the three students was asked to collect at least 5 positive images and 5 negative images. Positive images are those deemed to capture the resulting world state of the action. And negative images are those deemed to capture some state of the related object (i.e., the nouns in the verb-noun pairs), but are not the resulting state of the corresponding action. Then, each student was also asked to provide positive or negative labels for the images collected by the other two students. As a result each image has three positive/negative labels. We only keep the images whose labels are agreed by all three students. In total, the dataset contains 4163 images. On average, each action has 15 positive images, and 15 negative images. Figure 2 shows several examples of positive images and negative images of the action peel-orange. The positive images show an orange in a peeled state, while the negative images show oranges in different states (orange as a whole, orange slices, orange juice, etc.)."
  }, {
    "heading": "4 Action-Effect Prediction",
    "text": "Action-effect prediction is to connect actions (as causes) to the effects of actions. Specifically, given an image which depicts a state of the world, our task is to predict what concrete actions could cause the state of the world. This task is different from traditional action recognition as the underlying actions (e.g., human body posture/movement) are not captured by the images. In this regard, it is also different from image description generation.\nWe frame the problem as a few-shot learning task, by only providing a few human-labelled images for each action at the training stage. Given the very limited training data, we attempt to make use of web-search images. Web search has been adopted by previous computer vision studies to acquire training data (Fergus et al., 2005; Kennedy et al., 2006; Berg et al., 2010; Otani et al., 2016). Compared with human annotations, web-search comes at a much lower cost, but with a trade-off of poor data quality. To address this issue, we apply a bootstrapping approach that aims to handle data with noisy labels.\nThe first question is what search terms should be used for image search. There are two options. The first option is to directly use the action terms (i.e., verb-noun pairs) to search images and the downloaded web images are referred to as action web images. As desired images should be depicting effects of an action, terms describing effects become a natural choice. The second option is to use the key phrases extracted from language effect descriptions to search the web. The downloaded web images are referred to as effect web images."
  }, {
    "heading": "4.1 Extracting Effect Phrases from Language Data",
    "text": "We first apply chunking (shallow parsing) using the SENNA software (Collobert et al., 2011) to break an effect description into phrases such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), adjectives (ADJP), adverbs (ADVP), etc. After some examination, we found that most of the effect descriptions follow simple syntactic patterns. For a verb-noun pair, around 80% of its effect descriptions start with the same noun as the subject. In an effect description, the\nchange of state associated with the noun is mainly captured by some key phrases. For example, an adjective phrase usually describes a physical state; verbs like be, become, turn, get often indicate a description of change of the state. Based on these observations, we defined a set of patterns to identify phrases that describe physical states of an object. In total 1997 effect phrases were extracted from the language data. Table 2 shows some example patterns and example effect phrases that are extracted."
  }, {
    "heading": "4.2 Downloading Web Images",
    "text": "The purpose of querying search engine is to retrieve images of objects in certain effect states. To form image searching keywords, the effect phrases are concatenated with the corresponding noun phrases, for example, “apple + into thin pieces”. The image search results are downloaded and used as supplementary training data for the action-effect prediction models. However, web images can be noisy. First of all, not all of the automatically extracted effect phrases describe visible state of objects. Even if a phrase represents visible object states, the retrieved results may not be relevant. Figure 3 shows some example image search results using queries describing the object name “book”, and describing the object state such as “book is on fire”, “book is set aflame”. These state phrases were used by human annotators to describe the effect of the action “burn a book”. We can see that the images returned from the query “book is set aflame” are not depicting the physical effect state of “burn a book”. Therefore, it’s important to identify images with relevant effect states to train the model. To do that, we applied a bootstrapping method to handle the noisy web images as described in Section 4.3. For an action (i.e., a verb-noun pair), it has multiple corresponding effect phrases, and all of their image search results are treated as training images for this action.\nSince both the human annotated image data (Section 3) and the web-search image data were obtained from Internet search engines, they may\nhave duplicates. As part of the annotated images are used as test data to evaluate the models, it is important to remove duplicates. We designed a simple method to remove any images from the web-search image set that has a duplicate in the human annotated set. We first embed all images into feature vectors using pre-trained CNNs. For each web-search image, we calculate its cosine similarity score with each of the annotated images. And we simply remove the web images that have a score larger than 0.95."
  }, {
    "heading": "4.3 Models",
    "text": "We formulate the action-effect prediction task as a multi-class classification problem. Given an image, the model will output a probability distribution q over the candidate actions (i.e., verb-noun pairs) that can potentially cause the effect depicted in the image.\nSpecifically for model training, we are given a set of human annotated seeding image data {x, t} and a set of web-search image data {x′, t′}. Here x and x′ are the images (depicting effect states), and t and t′ are their classification targets (i.e., actions that cause the effects). Each target vector is the observed image label, t ∈ {0, 1}C , ∑ i ti = 1, and C is the number of classes (i.e., actions). The human annotated targets t can be trusted. But the targets of web-search images t′ are usually very noisy. Bootstrapping method has been shown to be an effective method to handle noisy labelled data (Rosenberg et al., 2005; Whitney and Sarkar, 2012; Reed et al., 2014). The objective of the\ncross-entropy loss is defined as follows:\nL(t,q) = C∑ i=1 ti log (qi), (1)\nwhere q are the predicted class probabilities, and C is the number of classes. To handle the noisy labels in the web-search data {x′, t′}, we adopt a bootstrapping objective following Reed’s work (Reed et al., 2014):\nL(t′,q) = C∑ i=1 [βt′i + (1− β)zi] log (qi), (2)\nwhere β ∈ [0, 1] is a model parameter to be assigned, z is the one-hot vector of the prediction q, zi = 1, if i = argmax qk, k = 1 . . . C.\nThe model architecture is shown in Figure 4. After each training batch, the current model will be used to make predictions q on images in the next batch. And the target probabilities is calculated as a linear combination of the current predictions q and the observed noisy labels t′. The idea behind this bootstrapping strategy is to ensure the consistency of the model’s predictions. By first initializing the model on the seeding image data, the bootstrapping approach allows the model to trust more on the web images that are consistent with the seeding data."
  }, {
    "heading": "4.4 Evaluation",
    "text": "We evaluate the models on the action-effect prediction task. Given an image that illustrates a state of the world, the goal is to predict what action could cause that state. Given an action in the form of a verb-noun pair, the goal is to identify images that depict the most likely effects on the state of the world caused by that action.\nFor each of the 140 verb-noun pairs, we use 10% of the human annotated images as the seeding image data for training, and use 30% for development and the rest 60% for test. The seeding image data set contains 408 images. On average, each verb-noun pair has less than 3 seeding images (including positive images and negative images). The development set contains 1252 images. The test set contains 2503 images. The model parameters were selected based on the performance on the development set.\nAs a given image may not be relevant to any effect, we add a background class to refer to images where effects are not caused by any action in the space of actions. So the total of classes for our evaluation model is 141. For each verb-noun pair and each of the effect phrases, around 40 images were downloaded from the Bing image search engine and used as candidate training examples. In total we have 6653 action web images and 59575 effect web images.\nMethods for Comparison All the methods compared are based on one neural network structure. We use ResNet (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009) to extract image features. The extracted image features are fed to a fully connected layer with rectified linear units and then to a softmax layer to make predictions. More specifically, we compare the following configurations: (1) BS+Seed+Act+Eff. The bootstrapping approach trained on the seeding images, the action web images, and the effect web images. During the training stage, the model was first trained on the seeding image data using vanilla cross-entropy objective (Equation 1). Then it was further trained on a combination of the seeding image data and web-search data using the bootstrapping objective (Equation 2). In the experiments we set β = 0.3. (2) BS+Seed+Act. The bootstrapping approach trained in the same fashion as (1). The only difference is that this method does not use the effect web images. (3) Seed+Act+Eff. A baseline method trained on a combination of the seeding images, the web action images, and the web effect images, using the vanilla cross-entropy objective. (4) Seed+Act. A baseline method trained on a combination of the seeding images and the action web images, using the vanilla cross-entropy objective.\n(5) Seed. A baseline method that was only trained on the seeding image data, using the vanilla crossentropy objective.\nEvaluation Results We apply the trained classification model to all of the test images. Based on the matrix of prediction scores, we can evaluate action-effect prediction from two angles: (1) given an action class, rank all the candidate images; (2) given an image, rank all the candidate action classes. Table 3 and 4 show the results for these two angels respectively. We report both mean average precision (MAP) and top prediction accuracy.\nOverall, BS+Seed+Act+Eff gives the best performance. By comparing the bootstrap approach with baseline approaches (i.e., BS+Seed+Act+Eff\nvs. Seed+Act+Eff, and BS+Seed+Act vs. Seed+Act), the bootstrapping approaches clearly outperforms their counterparts, demonstrating its ability in handling noisy web data. Comparing BS+Seed+Act+Eff with BS+Seed+Act, we can see that BS+Seed+Act+Eff performs better. This indicates the use of effect descriptions can bring more relevant images to train better models for action-effect prediction.\nIn Table 4, the poor performance of Seed+Act+Eff and Seed+Act shows that it is risky to fully rely on the noisy web search results. These two methods had trouble in distinguishing the background class from the rest.\nWe further trained another multi-class classifier with web effect images, using their corresponding effect phrases as class labels. Given a test image, we apply this new classifier to predict the effect descriptions of this image. Figure 5 shows some example images, their predicted actions based on our bootstrapping approach and their predicted effect phrases based on the new classifier. These examples also demonstrate another advantage of incorporating seed effect knowledge from language data: it provides state descriptions that can be used to better explain the perceived state. Such explanation can be crucial in human-agent communication for action planning and reasoning."
  }, {
    "heading": "5 Generalizing Effect Knowledge to New Verb-Noun Pairs",
    "text": "In real applications, it is very likely that we do not have the effect knowledge (i.e., language effect descriptions) for every verb-noun pair. And annotat-\ning effect knowledge using language (as shown in Section 3) can be very expensive. In this section, we describe how to potentially generalize seed effect knowledge to new verb-noun pairs through an embedding model."
  }, {
    "heading": "5.1 Action-Effect Embedding Model",
    "text": "The structure of our model is shown in Figure 6. It is composed of two sub-networks: one for verbnoun pairs (i.e., action) and the other one for effect phrases (i.e, effect). The action or effect is fed into an LSTM encoder and then to two fully-connected layers. The output is an action embedding vc and effect embedding ve. The networks are trained by minimizing the following cosine embedding loss function:\nL(vc,ve) = { 1− s(vc,ve), if (c, e) ∈ T max(0, s(vc,ve)), if (c, e) /∈ T\ns(·, ·) is the cosine similarity between vectors. T is a collection of action-effect pairs. Suppose c is an input for action and e is an input for effect, this loss function will learn an action and effect semantic space that maximizes the similarities between c and e if they have an action-effect relation (i.e., (c, e) ∈ T ). During training, the negative actioneffect pairs (i.e., (c, e) /∈ T ) are randomly sampled from data. In the experiments, the negative sampling ratio is set to 25. That is, for each positive action-effect pair, 25 negative pairs are created through random sampling.\nAt the inference step, given an unseen verbnoun pair, we embed it into the action and effect semantic space. Its embedding vector will be used to calculate similarities with all the embedding vectors of the candidate effect phrases."
  }, {
    "heading": "5.2 Evaluation",
    "text": "We divided the 140 verb-noun pairs into 70% training set (98 verb-noun pairs), 10% development set (14) and 20% test set (28). For the actioneffect embedding model, we use pre-trained GloVe word embeddings (Pennington et al., 2014) as input to the LSTM. The embedding model was trained using the language effect data corresponding to the training verb-noun pairs, and then it was applied to predict effect phrases for the unseen verb-noun pairs in the test set. For each unseen verb-noun pair, we collected its top five predicted effect phrases. Each predicted effect phrase was then used as query keywords to download web effect images. This set of web images are referred to as pEff and will be used in training the actioneffect prediction model.\nFor each of the 28 test (i.e., new) verb-noun pairs, we use the same ratio 10% (about 3 examples) of the human annotated images as the seeding images, which were combined with downloaded web images to train the prediction model. The remaining 30% and 60% are used as the development set, and the test set. We compare the following different configurations: (1) BS+Seed+Act+pEff. The bootstrapping approach trained on the seeding images, the action web images, and the web images downloaded using the predicted effect phrases. (2) BS+Seed+Act+Eff. The bootstrapping approach trained on the seeding images, the action web images, and the effect web images (downloaded using ground-truth effect phrases). (3) BS+Seed+Act. The bootstrapping approach trained on the seeding images and the action web\nimages.\n(4) Seed. A baseline only trained on the seeding images.\nTable 5 and 6 show the results for the action-effect prediction task for unseen verbnoun pairs. From the results we can see that BS+Seed+Act+pEff achieves close performance compared with BS+Seed+Act+Eff, which uses human annotated effect phrases. Although in most cases, BS+Seed+Act+pEff outperforms the baseline, which seems to point to the possibility that semantic embedding space can be employed to extend effect knowledge to new verb-noun pairs. However, the current results are not conclusive partly due to the small testing set. More in-depth evaluation is needed in the future.\nTable 7 shows top predicted effect phrases for several new verb-noun pairs. After analyzing the action-effect prediction results we notice that generalizing the effect knowledge to a verb-noun pair that contains an unseen verb tends to be more difficult than generalizing to a verb-noun pair that contains an unseen noun. Among the 28 test verbnoun pairs, 12 of them contain unseen verbs and known nouns, 7 of them contain unseen nouns and known verbs. For the task of ranking images given an action, the mean average precision is 0.447 for the unseen verb cases and 0.584 for the unseen noun cases. Although not conclusive, this might indicate that, verbs tend to capture more information about the effect states of the world than nouns."
  }, {
    "heading": "6 Discussion and Conclusion",
    "text": "When robots operate in the physical world, they not only need to perceive the world, but also need to act to the world. They need to understand the current state, to map their goals to the world state, and to plan for actions that can lead to the goals. All of these point to the importance of the ability to understand causal relations between actions and the state of the world. To address this issue, this paper introduces a new task on action-effect prediction.\nParticularly, we focus on modeling the connection between an action (a verb-noun pair) and its effect as illustrated in an image and treat natural language effect descriptions as side knowledge to help acquiring web image data and bootstrap training. Our current model is very simple and performance is yet to be improved. We plan to apply more advanced approaches in the future, for example, attention models that jointly capture actions, image states, and effect descriptions. We also plan to incorporate action-effect prediction to humanrobot collaboration, for example, to bridge the gap of commonsense knowledge about the physical world between humans and robots.\nThis paper presents an initial investigation on action-effect prediction. There are many challenges and unknowns, from problem formulation to knowledge representation; from learning and inference algorithms to methods and metrics for evaluations. Nevertheless, we hope this work can motivate more research in this area, enabling physical action-effect reasoning, towards agents which can perceive, act, and communicate with humans in the physical world."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by the National Science Foundation (IIS-1617682) and the DARPA XAI program under a subcontract from UCLA (N66001-17-2-4029). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions."
  }],
  "year": 2018,
  "references": [{
    "title": "Vqa: Visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425–2433.",
    "year": 2015
  }, {
    "title": "Infants’ physical world",
    "authors": ["Renée Baillargeon."],
    "venue": "Current directions in psychological science, 13(3):89–94.",
    "year": 2004
  }, {
    "title": "Automatic attribute discovery and characterization from noisy web data",
    "authors": ["Tamara L Berg", "Alexander C Berg", "Jonathan Shih."],
    "venue": "European Conference on Computer Vision, pages 663–676. Springer.",
    "year": 2010
  }, {
    "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management",
    "year": 2008
  }, {
    "title": "Mining semantic affordances of visual object categories",
    "authors": ["Yu-Wei Chao", "Zhan Wang", "Rada Mihalcea", "Jia Deng."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 4259–4267. IEEE.",
    "year": 2015
  }, {
    "title": "A lightweight tool for automatically extracting causal relationships from text",
    "authors": ["Stephen V Cole", "Matthew D Royal", "Marco G Valtorta", "Michael N Huhns", "John B Bowles."],
    "venue": "SoutheastCon, 2006. Proceedings of the IEEE, pages 125–129. IEEE.",
    "year": 2005
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Imagenet: A large-scale hierarchical image database",
    "authors": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."],
    "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.",
    "year": 2009
  }, {
    "title": "Minimally supervised event causality identification",
    "authors": ["Quang Xuan Do", "Yee Seng Chan", "Dan Roth."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294–303. Association for Computational Lin-",
    "year": 2011
  }, {
    "title": "Entity disambiguation for knowledge base population",
    "authors": ["Mark Dredze", "Paul McNamee", "Delip Rao", "Adam Gerber", "Tim Finin."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 277–285. Association for",
    "year": 2010
  }, {
    "title": "On the nature and the observability of the causal relation",
    "authors": ["Curt J Ducasse."],
    "venue": "The Journal of Philosophy, 23(3):57–68.",
    "year": 1926
  }, {
    "title": "Modeling actions through state changes",
    "authors": ["Alireza Fathi", "James M Rehg."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 2579–2586. IEEE.",
    "year": 2013
  }, {
    "title": "Learning object categories from google’s image search",
    "authors": ["Robert Fergus", "Li Fei-Fei", "Pietro Perona", "Andrew Zisserman."],
    "venue": "Computer Vision, 2005.",
    "year": 2005
  }, {
    "title": "Tenth IEEE International Conference on, volume 2, pages 1816–1823",
    "authors": ["ICCV"],
    "venue": "IEEE.",
    "year": 2005
  }, {
    "title": "Learning perceptual causality from video",
    "authors": ["Amy Fire", "Song-Chun Zhu."],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):23.",
    "year": 2016
  }, {
    "title": "Verb physics: Relative physical knowledge of actions and objects",
    "authors": ["Maxwell Forbes", "Yejin Choi."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 266–276.",
    "year": 2017
  }, {
    "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
    "authors": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."],
    "venue": "arXiv preprint arXiv:1606.01847.",
    "year": 2016
  }, {
    "title": "Physical causality of action verbs in grounded language understanding",
    "authors": ["Qiaozi Gao", "Malcolm Doering", "Shaohua Yang", "Joyce Y Chai."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), volume 1,",
    "year": 2016
  }, {
    "title": "A dataset of syntactic-ngrams over time from a very large corpus of english books",
    "authors": ["Yoav Goldberg", "Jon Orwant."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 1: Proceedings of the Main Conference and the",
    "year": 2013
  }, {
    "title": "Causal learning: Psychology, philosophy, and computation",
    "authors": ["Alison Gopnik", "Laura Schulz", "Laura Elizabeth Schulz."],
    "venue": "Oxford University Press.",
    "year": 2007
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778.",
    "year": 2016
  }, {
    "title": "To search or to label?: predicting the performance of search-based automatic image classifiers",
    "authors": ["Lyndon S Kennedy", "Shih-Fu Chang", "Igor V Kozintsev."],
    "venue": "Proceedings of the 8th ACM international workshop on Multimedia information re-",
    "year": 2006
  }, {
    "title": "Dbpedia–a large-scale, multilingual knowledge base",
    "authors": ["Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick Van Kleef", "Sören Auer"],
    "year": 2015
  }, {
    "title": "Microsoft coco: Common objects in context",
    "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick."],
    "venue": "European conference on computer vision, pages 740–755. Springer.",
    "year": 2014
  }, {
    "title": "Hierarchical question-image coattention for visual question answering",
    "authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."],
    "venue": "Advances In Neural Information Processing Systems, pages 289–297.",
    "year": 2016
  }, {
    "title": "Mapping instructions and visual observations to actions with reinforcement learning",
    "authors": ["Dipendra Misra", "John Langford", "Yoav Artzi."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1015–1026. Asso-",
    "year": 2017
  }, {
    "title": "Environment-driven lexicon induction for high-level instructions",
    "authors": ["Dipendra Kumar Misra", "Kejia Tao", "Percy Liang", "Ashutosh Saxena."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
    "year": 2015
  }, {
    "title": "Gaussian visual-linguistic embedding for zero-shot recognition",
    "authors": ["Tanmoy Mukherjee", "Timothy Hospedales."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 912–918.",
    "year": 2016
  }, {
    "title": "Learning joint representations of videos and sentences with web image search",
    "authors": ["Mayu Otani", "Yuta Nakashima", "Esa Rahtu", "Janne Heikkilä", "Naokazu Yokoya."],
    "venue": "European Conference on Computer Vision, pages 651–667. Springer.",
    "year": 2016
  }, {
    "title": "Causal inference in statistics: An overview",
    "authors": ["Judea Pearl"],
    "venue": "Statistics surveys, 3:96–146.",
    "year": 2009
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Combining supervised and unsupervised ensembles for knowledge base population",
    "authors": ["Nazneen Fatema Rajani", "Raymond J Mooney."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16).",
    "year": 2016
  }, {
    "title": "Training deep neural networks on noisy labels with bootstrapping",
    "authors": ["Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich."],
    "venue": "arXiv preprint arXiv:1412.6596.",
    "year": 2014
  }, {
    "title": "Semi-supervised self-training of object detection models",
    "authors": ["Chuck Rosenberg", "Martial Hebert", "Henry Schneiderman."],
    "venue": "Application of Computer Vision, 2005. WACV/MOTIONS’05 Volume 1. Seventh IEEE Workshops on, volume 1, pages 29–36.",
    "year": 2005
  }, {
    "title": "Grounding words in perception and action: computational insights",
    "authors": ["Deb Roy."],
    "venue": "Trends in cognitive sciences, 9(8):389–396.",
    "year": 2005
  }, {
    "title": "Creating causal embeddings for question answering with minimal supervision",
    "authors": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
    "year": 2016
  }, {
    "title": "Incremental acquisition of verb hypothesis space towards physical world interaction",
    "authors": ["Lanbo She", "Joyce Chai."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1,",
    "year": 2016
  }, {
    "title": "Interactive learning of grounded verb semantics towards human-robot communication",
    "authors": ["Lanbo She", "Joyce Chai."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages",
    "year": 2017
  }, {
    "title": "Back to the blocks world: Learning new actions through situated human-robot dialogue",
    "authors": ["Lanbo She", "Shaohua Yang", "Yu Cheng", "Yunyi Jia", "Joyce Chai", "Ning Xi."],
    "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and",
    "year": 2014
  }, {
    "title": "Representing general relational knowledge in conceptnet 5",
    "authors": ["Robert Speer", "Catherine Havasi."],
    "venue": "LREC, pages 3679–3686.",
    "year": 2012
  }, {
    "title": "Yago: a core of semantic knowledge",
    "authors": ["Fabian M Suchanek", "Gjergji Kasneci", "Gerhard Weikum."],
    "venue": "Proceedings of the 16th international conference on World Wide Web, pages 697–706. ACM.",
    "year": 2007
  }, {
    "title": "Understanding natural language commands for robotic navigation and mobile manipulation",
    "authors": ["Stefanie Tellex", "Thomas Kollar", "Steven Dickerson", "Matthew R Walter", "Ashis Gopal Banerjee", "Seth J Teller", "Nicholas Roy."],
    "venue": "AAAI, volume 1, page 2.",
    "year": 2011
  }, {
    "title": "Improving one-shot learning through fusing side information",
    "authors": ["Yao-Hung Hubert Tsai", "Ruslan Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1710.08347.",
    "year": 2017
  }, {
    "title": "Show and tell: A neural image caption generator",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156–3164. IEEE.",
    "year": 2015
  }, {
    "title": "Actions ̃ transformations",
    "authors": ["Xiaolong Wang", "Ali Farhadi", "Abhinav Gupta."],
    "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2658–2667.",
    "year": 2016
  }, {
    "title": "Bootstrapping via graph propagation",
    "authors": ["Max Whitney", "Anoop Sarkar."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 620–628. Association for Computational Linguis-",
    "year": 2012
  }, {
    "title": "Physics 101: Learning physical object properties from unlabeled videos",
    "authors": ["Jiajun Wu", "Joseph J Lim", "Hongyi Zhang", "Joshua B Tenenbaum", "William T Freeman."],
    "venue": "BMVC, volume 2, page 7.",
    "year": 2016
  }, {
    "title": "Learning to see physics via visual de-animation",
    "authors": ["Jiajun Wu", "Erika Lu", "Pushmeet Kohli", "Bill Freeman", "Josh Tenenbaum."],
    "venue": "Advances in Neural Information Processing Systems, pages 152–163.",
    "year": 2017
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."],
    "venue": "International Conference on Machine",
    "year": 2015
  }, {
    "title": "Transductive zero-shot action recognition by word-vector embedding",
    "authors": ["Xun Xu", "Timothy Hospedales", "Shaogang Gong."],
    "venue": "International Journal of Computer Vision, 123(3):309–333.",
    "year": 2017
  }, {
    "title": "Multi-task zero-shot action recognition with prioritised data augmentation",
    "authors": ["Xun Xu", "Timothy M Hospedales", "Shaogang Gong."],
    "venue": "European Conference on Computer Vision, pages 343–359. Springer.",
    "year": 2016
  }, {
    "title": "Few-shot object recognition from machine-labeled web images",
    "authors": ["Zhongwen Xu", "Linchao Zhu", "Yi Yang."],
    "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
    "year": 2017
  }, {
    "title": "Grounded semantic role labeling",
    "authors": ["Shaohua Yang", "Qiaozi Gao", "Changsong Liu", "Caiming Xiong", "Song-Chun Zhu", "Joyce Y Chai."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
    "year": 2016
  }, {
    "title": "Multi level causal relation identification using extended features",
    "authors": ["Xuefeng Yang", "Kezhi Mao."],
    "venue": "Expert Systems with Applications, 41(16):7171–7181.",
    "year": 2014
  }, {
    "title": "Detection of manipulation action consequences (mac)",
    "authors": ["Yezhou Yang", "Cornelia Fermüller", "Yiannis Aloimonos."],
    "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 2563–2570. IEEE.",
    "year": 2013
  }, {
    "title": "Stating the obvious: Extracting visual common sense knowledge",
    "authors": ["Mark Yatskar", "Vicente Ordonez", "Ali Farhadi."],
    "venue": "Proceedings of NAACL-HLT, pages 193–198.",
    "year": 2016
  }, {
    "title": "Zero-shot activity recognition with verb attribute induction",
    "authors": ["Rowan Zellers", "Yejin Choi."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2017
  }, {
    "title": "Learning temporal transformations from time-lapse videos",
    "authors": ["Yipin Zhou", "Tamara L Berg."],
    "venue": "European Conference on Computer Vision, pages 262–277. Springer.",
    "year": 2016
  }],
  "id": "SP:fa4f7ec0d1a3e67c6170638a9502cbad81873b9c",
  "authors": [{
    "name": "Qiaozi Gao",
    "affiliations": []
  }, {
    "name": "Shaohua Yang",
    "affiliations": []
  }, {
    "name": "Joyce Y. Chai",
    "affiliations": []
  }, {
    "name": "Lucy Vanderwende",
    "affiliations": []
  }],
  "abstractText": "Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic actioneffect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verbnoun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.",
  "title": "What Action Causes This? Towards Naive Physical Action-Effect Prediction"
}