{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 334–343 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealing for their single-model, end-to-end training process, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016). However, there are still many open problems in NMT (Koehn and Knowles, 2017). One particular issue is mistranslation of rare words. For example, consider the Uzbek sentence:\nSource: Ammo muammolar hali ko’p, deydi amerikalik olim Entoni Fauchi. Reference: But still there are many problems, says American scientist Anthony Fauci. Baseline NMT: But there is still a lot of problems, says James Chan.\nAt the position where the output should be Fauci, the NMT model’s top three candidates are Chan,\n1The code for this work can be found at https://github.com/tnq177/improving_lexical_ choice_in_nmt\nFauci, and Jenner. All three surnames occur in the training data with reference to immunologists: Fauci is the director of the National Institute of Allergy and Infectious Diseases, Margaret (not James) Chan is the former director of the World Health Organization, and Edward Jenner invented smallpox vaccine. But Chan is more frequent in the training data than Fauci, and James is more frequent than either Anthony or Margaret.\nBecause NMT learns word representations in continuous space, it tends to translate words that “seem natural in the context, but do not reflect the content of the source sentence” (Arthur et al., 2016). This coincides with other observations that NMT’s translations are often fluent but lack accuracy (Wang et al., 2017b; Wu et al., 2016).\nWhy does this happen? At each time step, the model’s distribution over output words e is\np(e) ∝ exp ( We · h̃ + be )\nwhere We and be are a vector and a scalar depending only on e, and h̃ is a vector depending only on the source sentence and previous output words. We propose two modifications to this layer. First, we argue that the term We · h̃, which measures how well e fits into the context h̃, favors common words disproportionately, and show that it helps to fix the norm of both vectors to a constant. Second, we add a new term representing a more direct connection from the source sentence, which allows the model to better memorize translations of rare words.\nBelow, we describe our models in more detail. Then we evaluate our approaches on eight language pairs, with training data sizes ranging from 100k words to 8M words, and show improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings. Finally, we provide some analysis to better understand why our modifications work well.\n334"
  }, {
    "heading": "2 Neural Machine Translation",
    "text": "Given a source sequence f = f1 f2 · · · fm, the goal of NMT is to find the target sequence e = e1e2 · · · en that maximizes the objective function:\nlog p(e | f ) = n∑\nt=1\nlog p(et | e<t, f ).\nWe use the global attentional model with general scoring function and input feeding by Luong et al. (2015a). We provide only a very brief overview of this model here. It has an encoder, an attention, and a decoder. The encoder converts the words of the source sentence into word embeddings, then into a sequence of hidden states. The decoder generates the target sentence word by word with the help of the attention. At each time step t, the attention calculates a set of attention weights at(s). These attention weights are used to form a weighted average of the encoder hidden states to form a context vector ct. From ct and the hidden state of the decoder are computed the attentional hidden state h̃t. Finally, the predicted probability distribution of the t’th target word is:\np(et | e<t, f ) = softmax(Woh̃t + bo). (1)\nThe rows of the output layer’s weight matrix Wo can be thought of as embeddings of the output vocabulary, and sometimes are in fact tied to the embeddings in the input layer, reducing model size while often achieving similar performance (Inan et al., 2017; Press and Wolf, 2017). We verified this claim on some language pairs and found out that this approach usually performs better than without tying, as seen in Table 1. For this reason, we always tie the target embeddings and Wo in all of our models."
  }, {
    "heading": "3 Normalization",
    "text": "The output word distribution (1) can be written as:\np(e) ∝ exp ( ‖We‖ ‖h̃‖ cos θWe,h̃ + be ) ,\nwhere We is the embedding of e, be is the e’th component of the bias bo, and θWe,h̃ is the angle between We and h̃. We can intuitively interpret the terms as follows. The term ‖h̃‖ has the effect of sharpening or flattening the distribution, reflecting whether the model is more or less certain in a particular context. The cosine similarity cos θWe,h̃ measures how well e fits into the context. The bias be controls how much the word e is generated; it is analogous to the language model in a log-linear translation model (Och and Ney, 2002).\nFinally, ‖We‖ also controls how much e is generated. Figure 1 shows that it generally correlates with frequency. But because it is multiplied by cos θWe,h̃, it has a stronger effect on words whose embeddings have direction similar to h̃, and less effect or even a negative effect on words in other directions. We hypothesize that the result is that the model learns ‖We‖ that are disproportionately large.\nFor example, returning to the example from Section 1, these terms are:\ne ‖We‖ ‖h̃‖ cos θWe,h̃ be logit Chan 5.25 19.5 0.144 −1.53 13.2 Fauci 4.69 19.5 0.154 −1.35 12.8 Jenner 5.23 19.5 0.120 −1.59 10.7\nObserve that cos θWe,h̃ and even be both favor the correct output word Fauci, whereas ‖We‖ favors the more frequent, but incorrect, word Chan. The most frequently-mentioned immunologist trumps other immunologists.\nTo solve this issue, we propose to fix the norm of all target word embeddings to some value r. Followingthe weight normalization approach of Salimans and Kingma (2016), we reparameterize We as r\nve ‖ve‖ , but keep r fixed.\nA similar argument could be made for ‖h̃t‖: because a large ‖h̃t‖ sharpens the distribution, causing frequent words to more strongly dominate rare words, we might want to limit it as well. We compared both approaches on a development set and found that replacing h̃t in equation (1) with r\nh̃t ‖h̃t‖\nindeed performs better, as shown in Table 1."
  }, {
    "heading": "4 Lexical Translation",
    "text": "The attentional hidden state h̃ contains information not only about the source word(s) corresponding to the current target word, but also the contexts of those source words and the preceding context of the target word. This could make the model prone to generate a target word that fits the context but doesn’t necessarily correspond to the source word(s). Count-based statistical models, by contrast, don’t have this problem, because they simply don’t model any of this context. Arthur et al. (2016) try to alleviate this issue by integrating a count-based lexicon into an NMT system. However, this lexicon must be trained separately using GIZA++ (Och and Ney, 2003), and its parameters form a large, sparse array, which can be difficult to store in GPU memory.\nWe propose instead to use a simple feedforward neural network (FFNN) that is trained jointly with the rest of the NMT model to generate a target word based directly on the source word(s). Let fs (s = 1, . . . ,m) be the embeddings of the source words. We use the attention weights to form a\nweighted average of the embeddings (not the hidden states, as in the main model) to give an average source-word embedding at each decoding time step t:\nf `t = tanh ∑\ns\nat(s) fs.\nThen we use a one-hidden-layer FFNN with skip connections (He et al., 2016):\nh`t = tanh(W f ` t ) + f ` t\nand combine its output with the decoder output to get the predictive distribution over output words at time step t:\np(yt | y<t, x) = softmax(Woh̃t + bo + W`h`t + b`).\nFor the same reasons that were given in Section 3 for normalizing h̃t and the rows of Wot , we normalize h`t and the rows of W\n` as well. Note, however, that we do not tie the rows of W` with the word embeddings; in preliminary experiments, we found this to yield worse results."
  }, {
    "heading": "5 Experiments",
    "text": "We conducted experiments testing our normalization approach and our lexical model on eight language pairs using training data sets of various sizes. This section describes the systems tested and our results."
  }, {
    "heading": "5.1 Data",
    "text": "We evaluated our approaches on various language pairs and datasets:\n• Tamil (ta), Urdu (ur), Hausa (ha), Turkish (tu), and Hungarian (hu) to English (en), using data from the LORELEI program.\n• English to Vietnamese (vi), using data from the IWSLT 2015 shared task.2\n• To compare our approach with that of Arthur et al. (2016), we also ran on their English to Japanese (ja) KFTT and BTEC datasets.3\nWe tokenized the LORELEI datasets using the default Moses tokenizer, except for Urdu-English, where the Urdu side happened to be tokenized using Morfessor FlatCat (w = 0.5). We used the preprocessed English-Vietnamese and EnglishJapanese datasets as distributed by Luong et al., and Arthur et al., respectively. Statistics about our data sets are shown in Table 2."
  }, {
    "heading": "5.2 Systems",
    "text": "We compared our approaches against two baseline NMT systems:\nuntied, which does not tie the rows of Wo to the target word embeddings, and tied, which does.\nIn addition, we compared against two other baseline systems:\nMoses: The Moses phrase-based translation system (Koehn et al., 2007), trained on the same data as the NMT systems, with the same maximum sentence length of 50. No additional data was used for training the language model. Unlike the NMT systems, Moses used the full vocabulary from the training data; unknown words were copied to the target sentence. Arthur: Our reimplementation of the discrete lexicon approach of Arthur et al. (2016). We only tried their auto lexicon, using GIZA++ (Och and Ney, 2003), integrated using their bias approach. Note that we also tied embedding as we found it also helped in this case.\nAgainst these baselines, we compared our new systems:\nfixnorm: The normalization approach described in Section 3. fixnorm+lex: The same, with the addition of the lexical translation module from Section 4.\n2https://nlp.stanford.edu/projects/nmt/ 3http://isw3.naist.jp/~philip-a/emnlp2016/"
  }, {
    "heading": "5.3 Details",
    "text": "Model For all NMT systems, we fed the source sentences to the encoder in reverse order during both training and testing, following Luong et al. (2015a). Information about the number and size of hidden layers is shown in Table 2. The word embedding size is always equal to the hidden layer size.\nFollowing common practice, we only trained on sentences of 50 tokens or less. We limited the vocabulary to word types that appear no less than 5 times in the training data and map the rest to UNK. For the English-Japanese and English-Vietnamese datasets, we used the vocabulary sizes reported in their respective papers (Arthur et al., 2016; Luong and Manning, 2015).\nFor fixnorm, we tried r ∈ {3, 5, 7} and selected the best value based on the development set performance, which was r = 5 except for EnglishJapanese (BTEC), where r = 7. For fixnorm+lex, because Wsh̃t+W`h`t takes on values in [−2r2, 2r2], we reduced our candidate r values by roughly a factor of √ 2, to r ∈ {2, 3.5, 5}. A radius r = 3.5 seemed to work the best for all language pairs.\nTraining We trained all NMT systems with Adadelta (Zeiler, 2012). All parameters were initialized uniformly from [−0.01, 0.01]. When a gradient’s norm exceeded 5, we normalized it to 5. We also used dropout on non-recurrent connections only (Zaremba et al., 2014), with probability 0.2. We used minibatches of size 32. We trained for 50 epochs, validating on the development set after every epoch, except on English-Japanese, where we validated twice per epoch. We kept the best checkpoint according to its BLEU on the development set.\nInference We used beam search with a beam size of 12 for translating both the development and test sets. Since NMT often favors short translations (Cho et al., 2014), we followed Wu et al. (2016) in using a modified score s(e | f ) in place of log-probability:\ns(e | f ) = log p(e | f ) lp(e)\nlp(e) = (5 + |e|)α (5 + 1)α\nWe set α = 0.8 for all of our experiments. Finally, we applied a postprocessing step to replace each UNK in the target translation with the\nsource word with the highest attention score (Luong et al., 2015b).\nEvaluation For translation into English, we report case-sensitive NIST BLEU against detokenized references. For English-Japanese and English-Vietnamese, we report tokenized, casesensitive BLEU following Arthur et al. (2016) and Luong and Manning (2015). We measure statistical significance using bootstrap resampling (Koehn, 2004)."
  }, {
    "heading": "6 Results and Analysis",
    "text": ""
  }, {
    "heading": "6.1 Overall",
    "text": "Our results are shown in Table 3. First, we observe, as has often been noted in the literature, that NMT tends to perform poorer than PBMT on low resource settings (note that the rows of this table are sorted by training data size).\nOur fixnorm system alone shows large improvements (shown in parentheses) relative to tied. Integrating the lexical module (fixnorm+lex) adds in further gains. Our fixnorm+lex models surpass Moses on all tasks except Urdu- and Hausa-English, where it is 1.6 and 0.7 BLEU short respectively.\nThe method of Arthur et al. (2016) does improve over the baseline NMT on most language pairs, but not by as much and as consistently as our models, and often not as well as Moses. Unfortunately, we could not replicate their approach for English-Japanese (KFTT) because the lexical table was too large to fit into the computational graph.\nFor English-Japanese (BTEC), we note that, due to the small size of the test set, all systems except for Moses are in fact not significantly different from tied (p > 0.01). On all other tasks, however, our systems significantly improve over tied (p < 0.01)."
  }, {
    "heading": "6.2 Impact on translation",
    "text": "In Table 4, we show examples of typical translation mistakes made by the baseline NMT systems. In the Uzbek example (top), untied and tied have confused 34 with UNK and 700, while in the Turkish one (middle), they incorrectly output other proper names, Afghan and Myanmar, for the proper name Kenya. Our systems, on the other hand, translate these words correctly.\nThe bottom example is the one introduced in Section 1. We can see that our fixnorm approach\ndoes not completely solve the mistranslation issue, since it translates Entoni Fauchi to UNK UNK (which is arguably better than James Chan). On the other hand, fixnorm+lex gets this right. To better understand how the lexical module helps in this case, we look at the top five translations for the word Fauci in fixnorm+lex:\ne cos θWe,h̃ cos θW le,hl be + b l e logit\nFauci 0.522 0.762 −8.71 7.0 UNK 0.566 −0.009 −1.25 5.6 Anthony 0.263 0.644 −8.70 2.4 Ahmedova 0.555 0.173 −8.66 0.3 Chan 0.546 0.150 −8.73 −0.2\nAs we can see, while cos θWe,h̃ might still be confused between similar words, cos θW le,hl significantly favors Fauci."
  }, {
    "heading": "6.3 Alignment and unknown words",
    "text": "Both our baseline NMT and fixnorm models suffer from the problem of shifted alignments noted by Koehn and Knowles (2017). As seen in Figure 2a and 2b, the alignments for those two systems seem to shift by one word to the left (on the source side). For example, nói should be aligned to said instead of Telekom, and so on. Although this is not a problem per se, since the decoder can decide to attend to any position in the encoder states as long as the state at that position holds the information the decoder needs, this becomes a real issue when we need to make use of the alignment information, as in unknown word replacement (Luong et al., 2015b). As we can see in Figure 2, because of the alignment shift, both tied and fixnorm incorrectly replace the two unknown words (in bold) with But Deutsche instead of Deutsche Telekom. In contrast, under fixnorm+lex and the model of Arthur et al. (2016), the alignment is corrected, causing the UNKs to be replaced with the correct source words."
  }, {
    "heading": "6.4 Impact of r",
    "text": "The single most important hyper-parameter in our models is r. Informally speaking, r controls how much surface area we have on the hypersphere to allocate to word embeddings. To better understand its impact, we look at the training perplexity and dev BLEUs during training with different values of r. Table 6 shows the train perplexity and best tokenized dev BLEU on Turkish-English for fixnorm and fixnorm+lex with different values of r. As we can see, a smaller r results in\nhu-en 244 244 (0.599) document (0.005) By (0.003) by (0.002) offices (0.001) befektetéseinek investments (0.151) investment (0.017) Investments (0.015) all (0.012) investing (0.003) kutatás-fejlesztésre research (0.227) Research (0.040) Development (0.014) researchers (0.008) development (0.007)\ntu-en ifade expression (0.109) expressed (0.061) express (0.056) speech (0.024) expresses (0.020) cumhurbaşkanı President (0.573) president (0.030) Republic (0.027) Vice (0.010) Abdullah (0.008) Göstericiler protesters (0.115) demonstrators (0.050) Protesters (0.033) UNK (0.004) police (0.003)\nworse training perplexity, indicating underfitting, whereas if r is too large, the model achieves better training perplexity but decrased dev BLEU, indicating overfitting."
  }, {
    "heading": "6.5 Lexicon",
    "text": "One byproduct of lex is the lexicon, which we can extract and examine simply by feeding each source word embedding to the FFNN module and calculating p`(y) = softmax(W`h`+b`). In Table 5, we show the top translations for some entries in the lexicons extracted from fixnorm+lex for Hungarian, Turkish, and Hausa-English. As expected, the lexical distribution is sparse, with a few top translations accounting for the most probability mass."
  }, {
    "heading": "6.6 Byte Pair Encoding",
    "text": "Byte-Pair-Encoding (BPE) (Sennrich et al., 2016) is commonly used in NMT to break words into word-pieces, improving the translation of rare words. For this reason, we reran our experiments using BPE on the LORELEI and EnglishVietnamese datasets. Additionally, to see if our proposed methods work in high-resource scenarios, we run on the WMT 2014 English-German (en-de) dataset,4 using newstest2013 as the development set and reporting tokenized, case-sensitive BLEU on newstest2014 and newstest2015.\nWe validate across different numbers of BPE operations; specifically, we try {1k, 2k, 3k} merge operations for ta-en and ur-en due to their small sizes, {10k, 12k, 15k} for the other LORELEI datasets and en-vi, and 32k for en-de. Using BPE results in much smaller vocabulary sizes, so we do not apply a vocabulary cut-off. Instead, we train on\n4https://nlp.stanford.edu/projects/nmt/\nan additional copy of the training data in which all types that appear once are replaced with UNK, and halve the number of epochs accordingly. Our models, training, and evaluation processes are largely the same, except that for en-de, we use a 4-layer decoder and 4-layer bidirectional encoder (2 layers for each direction).\nTable 7 shows that our proposed methods also significantly improve the translation when used with BPE, for both high and low resource language pairs. With BPE, we are only behind Moses on Urdu-English."
  }, {
    "heading": "7 Related Work",
    "text": "The closest work to our lex model is that of Arthur et al. (2016), which we have discussed already in Section 4. Recent work by Liu et al. (2016) has very similar motivation to that of our fixnorm model. They reformulate the output layer in terms of directions and magnitudes, as we do here. Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin. Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification.\nHandling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well.\nRecently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation. However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model. It would be interesting to see if our methods benefit the Trans-\nformer network and other models as well."
  }, {
    "heading": "8 Conclusion",
    "text": "In this paper, we have presented two simple yet effective changes to the output layer of a NMT model. Both of these changes improve translation quality substantially on low-resource language pairs. In many of the language pairs we tested, the baseline NMT system performs poorly relative to phrase-based translation, but our system surpasses it (when both are trained on the same data). We conclude that NMT, equipped with the methods demonstrated here, is a more viable choice for low-resource translation than before, and are optimistic that NMT’s repertoire will continue to grow."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by University of Southern California subcontract 67108176 under DARPA contract HR0011-15-C-0115. Nguyen was supported in part by a fellowship from the Vietnam Education Foundation. We would like to express our great appreciation to Sharon Hu for letting us use her group’s GPU cluster (supported by NSF award 1629914), and to NVIDIA corporation for the donation of a Titan X GPU. We also thank Tomer Levinboim for insightful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
    "year": 2014
  }, {
    "title": "A character-level decoder without explicit segmentation for neural machine translation",
    "authors": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin."],
    "venue": "arXiv:1705.03122.",
    "year": 2017
  }, {
    "title": "Incorporating copying mechanism in sequence-to-sequence learning",
    "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Pointing the unknown words",
    "authors": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "Proc. CVPR.",
    "year": 2016
  }, {
    "title": "Tying word vectors and word classifiers: A loss framework for language modeling",
    "authors": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."],
    "venue": "Proc. ICLR.",
    "year": 2017
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proc. ACL-IJCNLP.",
    "year": 2015
  }, {
    "title": "Is neural machine translation ready for deployment? A case study on 30 translation directions",
    "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."],
    "venue": "Proc. IWSLT .",
    "year": 2016
  }, {
    "title": "Statistical significance tests for machine translation evaluation",
    "authors": ["Philipp Koehn."],
    "venue": "Proc. EMNLP.",
    "year": 2004
  }, {
    "title": "Moses: Open source toolkit for statistical machine translation",
    "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"],
    "year": 2007
  }, {
    "title": "Six challenges for neural machine translation",
    "authors": ["Philipp Koehn", "Rebecca Knowles."],
    "venue": "Proc. Workshop on Neural Machine Translation.",
    "year": 2017
  }, {
    "title": "Context models for oov word translation in low-resource languages",
    "authors": ["Angli Liu", "Katrin Kirchhoff."],
    "venue": "Proceedings of AMTA 2018, vol. 1: MT Research Track. AMTA.",
    "year": 2018
  }, {
    "title": "Large-margin softmax loss for convolutional neural networks",
    "authors": ["Weiyang Liu", "Yandong Wen", "Zhiding Yu", "Meng Yang."],
    "venue": "Proc. ICML.",
    "year": 2016
  }, {
    "title": "Stanford neural machine translation systems for spoken language domain",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "Proc. IWSLT .",
    "year": 2015
  }, {
    "title": "Achieving open vocabulary neural machine translation with hybrid word-character models",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proc. EMNLP.",
    "year": 2015
  }, {
    "title": "Addressing the rare word problem in neural machine translation",
    "authors": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."],
    "venue": "Proc. ACL-IJCNLP.",
    "year": 2015
  }, {
    "title": "Vocabulary manipulation for neural machine translation",
    "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Discriminative training and maximum entropy models for statistical machine translation",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Proc. ACL.",
    "year": 2002
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Computational Linguistics 29(1).",
    "year": 2003
  }, {
    "title": "Using the output embedding to improve language models",
    "authors": ["Ofir Press", "Lior Wolf."],
    "venue": "Proc. EACL.",
    "year": 2017
  }, {
    "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
    "authors": ["T. Salimans", "D.P. Kingma."],
    "venue": "ArXiv e-prints .",
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proc. ACL.",
    "year": 2016
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "NIPS 27.",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in Neural Information Processing Systems. pages 6000–6010.",
    "year": 2017
  }, {
    "title": "Normface: L2 hypersphere embedding for face verification",
    "authors": ["Feng Wang", "Xiang Xiang", "Jian Cheng", "Alan L. Yuille."],
    "venue": "Proceedings of the 25th ACM international conference on Multimedia. ACM. https://doi.org/https://doi.",
    "year": 2017
  }, {
    "title": "Neural machine translation advised by statistical machine translation",
    "authors": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang."],
    "venue": "Proc. AAAI.",
    "year": 2017
  }, {
    "title": "Google’s neural machine translation system: Bridging the gap between human",
    "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"],
    "year": 2016
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."],
    "venue": "arXiv:1409.2329.",
    "year": 2014
  }, {
    "title": "ADADELTA: An adaptive learning rate method",
    "authors": ["Matthew D. Zeiler."],
    "venue": "arXiv:1212.5701v1.",
    "year": 2012
  }],
  "id": "SP:3acf8313546def523da334cb3324806ac229769f",
  "authors": [{
    "name": "Toan Q. Nguyen",
    "affiliations": []
  }, {
    "name": "David Chiang",
    "affiliations": []
  }],
  "abstractText": "We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value. Second, we integrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.1",
  "title": "Improving Lexical Choice in Neural Machine Translation"
}