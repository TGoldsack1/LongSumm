{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The choice of the right optimization method plays a major role in the success of training deep learning models. Although Stochastic Gradient Descent (SGD) often works well out of the box, more advanced optimization methods such as Adam (Kingma & Ba, 2015) or Adagrad (Duchi et al., 2011) can be faster, especially for training very deep networks. Designing optimization methods for deep learning, however, is very challenging due to the non-convex nature of the optimization problems.\nIn this paper, we consider an approach to automate the process of designing update rules for optimization methods, especially for deep learning architectures. The key insight is to use a controller in the form of a recurrent network to generate an update equation for the optimizer. The recur-\n*Equal contribution 1Google Brain. Correspondence to: Irwan Bello <ibello@google.com>, Barret Zoph <barretzoph@google.com>, Vijay Vasudevan <vrv@google.com>, Quoc V. Le <qvl@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrent network controller is trained with reinforcement learning to maximize the accuracy of a particular model architecture, being trained for a fixed number of epochs by the update rule, on a held-out validation set. This process is illustrated in Figure 1.\nOn CIFAR-10, our approach discovers several update rules that are better than many commonly used optimizers such as Adam, RMSProp, or SGD with and without Momentum on a small ConvNet model. Many of the generated update equations can be easily transferred to new architectures and datasets. For instance, update rules found on a small ConvNet architecture, when applied to the Wide ResNet architecture (Zagoruyko & Komodakis, 2016), improved accuracy over Adam, RMSProp, Momentum, and SGD by a margin up to 2% on the test set. The same update rules also work well for Google’s Neural Machine Translation system (Wu et al., 2016) giving an improvement of up to 0.7 BLEU on the WMT 2014 English to German task."
  }, {
    "heading": "2. Related Work",
    "text": "Neural networks are difficult and slow to train, and many methods have been designed to tackle this difficulty (e.g., Riedmiller & Braun (1992); LeCun et al. (1998); Schraudolph (2002); Martens (2010); Le et al. (2011); Duchi et al. (2011); Zeiler (2012); Martens & Sutskever (2012); Schaul et al. (2013); Pascanu & Bengio (2013); Pascanu et al. (2013); Kingma & Ba (2014); Ba et al. (2017)). More recent optimization methods combine insights from both stochastic and batch methods in that they use a small minibatch, similar to SGD, yet they implement many heuristics to estimate diagonal second-order informa-\ntion, similar to Hessian-free or L-BFGS (Liu & Nocedal, 1989). This combination often yields faster convergence for practical problems (Duchi et al., 2011; Dean et al., 2012; Kingma & Ba, 2014). For example, Adam (Kingma & Ba, 2014), a commonly-used optimizer in deep learning, implements simple heuristics to estimate the mean and variance of the gradient, which are used to generate more stable updates during training.\nMany of the above update rules are designed by borrowing ideas from convex analysis, even though optimization problems in neural networks are non-convex. Recent empirical results with non-monotonic learning rate heuristics (Loshchilov & Hutter, 2017) suggest that there are still many unknowns in training neural networks and that many ideas in non-convex optimization can be used to improve it.\nThe goal of our work is to search for better update rules for neural networks in the space of well known primitives. In other words, instead of hand-designing new update rules from scratch, we use a machine learning algorithm to search the update rules. This goal is shared with recentlyproposed methods by Andrychowicz et al. (2016); Ravi & Larochelle (2017); Wichrowska et al. (2017), which employ an LSTM to generate numerical updates for training neural networks. The key difference is that our approach generates a mathematical equation for the update instead of numerical updates. The main advantage of generating an equation is that it can easily be transferred to larger tasks and does not require training any additional neural networks for a new optimization problem. Finally, although our method does not aim to optimize the memory usage of update rules, our method discovers update rules that are on par with Adam or RMSProp while requiring less memory.\nThe concept of using a Recurrent Neural Network for meta-learning has been attempted in the past, either via genetic programming or gradient descent (Schmidhuber, 1992; Hochreiter et al., 2001). Similar to the above recent methods, these approaches only generate the updates, but not the update equations, as proposed in this paper.\nA related approach is using genetic programming to evolve update equations for neural networks (e.g., Bengio et al. (1994); Runarsson & Jonsson (2000); Orchard & Wang (2016)). Genetic programming however is often slow and requires many heuristics to work well. For that reason, many prior studies in this area have only experimented with very small-scale neural networks. For example, the neural networks used for experiments in Orchard & Wang (2016) have around 100 weights, which is quite small compared to today’s standards.\nOur approach is reminiscent of recent work in automated model discovery with Reinforcement Learning (Baker\net al., 2016), especially Neural Architecture Search (Zoph & Le, 2017), in which a recurrent network is used to generate the configuration string of neural architectures instead. In addition to applying the key ideas to different applications, this work presents a novel scheme to combine primitive inputs in a much more flexible manner, which makes the search for novel optimizers possible.\nFinally, our work is also inspired by the recent studies by Keskar et al. (2016); Zhang et al. (2017), in which it was found that SGD can act as a regularizer that helps generalization. In our work, we use the accuracy on the validation set as the reward signal, thereby implicitly searching for optimizers that can help generalization as well."
  }, {
    "heading": "3. Method",
    "text": ""
  }, {
    "heading": "3.1. A simple domain specific language for update rules",
    "text": "In our framework, the controller generates strings corresponding to update rules, which are then applied to a neural network to estimate the update rule’s performance; this performance is then used to update the controller so that the controller can generate improved update rules over time.\nTo map strings sampled by the controller to an update rule, we design a domain specific language that relies on a parenthesis-free notation (in contrast to the classic infix notation). Our choice of domain specific language (DSL) is motivated by the observation that the computational graph of most common optimizers can be represented as a simple binary expression tree, assuming input primitives such as the gradient or the running average of the gradient and basic unary and binary functions.\nWe therefore express each update rule with a string describing 1) the first operand to select, 2) the second operand to select, 3) the unary function to apply on the first operand, 4) the unary function to apply on the second operand and 5) the binary function to apply to combine the outputs of the unary functions. The output of the binary function is then either temporarily stored in our operand bank (so that it can be selected as an operand in subsequent parts of the string) or used as the final weight update as follows:\n∆w = λ ∗ b(u1(op1), u2(op2))\nwhere op1, op2, u1(.), u2(.) and b(., .) are the operands, the unary functions and the binary function corresponding to the string, w is the parameter that we wish to optimize and λ is the learning rate.\nWith a limited number of iterations, our DSL can only represent a subset of all mathematical equations. However we note that it recovers common optimizers within one iteration assuming access to simple primitives. Figure 2 shows\nhow some commonly used optimizers can be represented in the DSL. We also note that multiple strings in our prediction scheme can map to the same underlying update rule, including strings of different lengths (c.f. the two representations of Adam in Figure 2). This is both a feature of our action space corresponding to mathematical expressions (addition and multiplication are commutative for example) and our choice of domain specific language. We argue that this makes for interesting exploration dynamics because a competitive optimizer may be obtained by expressing a standard optimizer in an expanded fashion and modifying it slightly."
  }, {
    "heading": "3.2. Controller optimization with policy gradients",
    "text": "Our controller is implemented as a Recurrent Neural Network which samples strings of length 5n where n is a number of iterations fixed during training (see Figure 3). Since the operand bank grows as more iterations are computed, we use different softmax weights at every step of prediction.\nThe controller is trained to maximize the performance of its sampled update rules on a specified model. The training objective is formulated as follows:\nJ(θ) = E∆∼pθ(.)[R(∆)] (1)\nwhereR(∆) corresponds to the accuracy on a held-out val-\nidation set obtained after training a target network with update rule ∆.\nZoph & Le (2017) train their controller using a vanilla policy gradient obtained via REINFORCE (Williams, 1992), which is known to exhibit poor sample efficiency. We find that using the more sample efficient Trust Region Policy Optimization (Schulman et al., 2015) algorithm speeds up convergence of the controller. For the baseline function in TRPO, we use a simple exponential moving average of previous rewards."
  }, {
    "heading": "3.3. Accelerated Training",
    "text": "To further speed up the training of the controller RNN, we employ a distributed training scheme. In our distributed training scheme the samples generated from the controller RNN are added to a queue, and run on a set of distributed workers that are connected across a network. This scheme is different from (Zoph & Le, 2017) in that now a parameter server and controller replicas are not needed for the controller RNN, which simplifies training. At each iteration, the controller RNN samples a batch of update rules and adds them to the global worker queue. Once the training of the child network is complete, the accuracy on a held-out validation set is computed and returned to the controller RNN. whose parameters get updated with TRPO. New samples are then generated and this same process continues.\nIdeally, the reward fed to the controller would be the performance obtained when running a model with the sampled optimizer until convergence. However, such a setup requires significant computation and time. To help deal with these issues, we propose the following trade-offs to greatly reduce computational complexity. First, we find that searching for optimizers with a small two layer convolutional network provides enough of a signal for whether an optimizer would do well on much larger models such as the Wide ResNet model. Second, we train each model for a modest 5 epochs only, which also provides enough signal for whether a proposed optimizer is good enough for our needs. These techniques allow us to run experiments more quickly and efficiently compared to Zoph & Le (2017), with our controller experiments typically converging in less than a day using 100 CPUs, compared to 800 GPUs over several weeks."
  }, {
    "heading": "4. Experiments",
    "text": "We aim to answer the following questions:\n• Can the controller discover update rules that outperform other stochastic optimization methods?\n• Do the discovered update rules transfer well to other\narchitectures and tasks?\nIn this section, we will focus on answering the first question by performing experiments with the optimizer search framework to find optimizers on a small ConvNet model and compete with the existing optimizers. In the next section, we will transfer the found optimizers to other architectures and tasks thereby answering the second question."
  }, {
    "heading": "4.1. Search space",
    "text": "The operands, unary functions and binary functions that are accessible to our controller are as follows (details below):\n• Operands: g, g2, g3, m̂, v̂, γ̂, sign(g), sign(m̂), 1, small constant noise, 10−4w, 10−3w, 10−2w, 10−1w, ADAM and RMSProp.\n• Unary functions which map input x to: x, −x, ex, log |x|, clip(x, 10−5), clip(x, 10−4), clip(x, 10−3), drop(x, 0.1), drop(x, 0.3), drop(x, 0.5) and sign(x).\n• Binary functions which map (x, y) to x+y (addition), x − y (subtraction), x ∗ y (multiplication), xy+ (division) or x (keep left).\nHere, m̂, v̂, γ̂ are running exponential moving averages of g, g2 and g3, obtained with decay rates β1, β2 and β3 respectively, drop(.|p) sets its inputs to 0 with probability p and clip(.|l) clips its input to [−l, l]. All operations are applied element-wise.\nIn our experiments, we use binary trees with depths of 1, 2 and 3 which correspond to strings of length 5, 10 and 15 respectively. The above list of operands, unary functions and binary function is quite large, so to address this issue, we find it helpful to only work with subsets of the operands and functions presented above. This leads to typical search space sizes ranging from 106 to 1010 possible update rules.\nWe also experiment with several constraints when sampling an update rule, such as forcing the left and right operands to be different at each iteration, and not using addition as the final binary function. An additional constraint added is to force the controller to reuse one of the previously computed operands in the subsequent iterations. The constraints are implemented by manually setting the logits corresponding to the forbidden operands or functions to −∞."
  }, {
    "heading": "4.2. Experimental details",
    "text": "Across all experiments, our controller RNN is trained with the ADAM optimizer with a learning rate of 10−5 and a minibatch size of 5. The controller is a single-layer LSTM with hidden state size 150 and weights are initialized uniformly at random between -0.08 and 0.08. We also use an\nentropy penalty to aid in exploration. This entropy penalty coefficient is set to 0.0015.\nThe child network architecture that all sampled optimizers are run on is a small two layer 3x3 ConvNet. This ConvNet has 32 filters with ReLU activations and batch normalization applied after each convolutional layer. These child networks are trained on the CIFAR-10 dataset, one of the most benchmarked datasets in deep learning.\nThe controller is trained on a CPU and the child models are trained using 100 distributed workers which also run on CPUs (see Section 3.3). Once a worker receives an optimizer to run from the controller, it performs a basic hyperparameter sweep on the learning rate: 10i with i ranging from -5 to 1, with each learning rate being tried for 1 epoch of 10,000 CIFAR-10 training examples. The best learning rate after 1 epoch is then used to train our child network for 5 epochs and the final validation accuracy is reported as a reward to the controller. The child networks have a batch size of 100 and evaluate the update rule on a fixed held-out validation set of 5,000 examples. In this setup, training a child model with a sampled optimizer generally takes less than 10 minutes. Experiments typically converge within a day. All experiments are carried out using TensorFlow (Abadi et al., 2016).\nThe hyperparameter values for the update rules are inspired by standard values used in the literature. We set to 10−8, β1 to 0.9 and β2 = β3 to 0.999. Preliminary experiments indicate that the update rules are robust to slight changes in the hyperparameters they were searched over."
  }, {
    "heading": "4.3. Experimental results",
    "text": "Our results show that the controller discovers many different updates that perform well during training and the maximum accuracy also increases over time. In Figure 4, we show the learning curve of the controller as more optimizers are sampled.\nThe plots in Figure 5 show the results of two of our best\noptimizers being run for 300 epochs on the full CIFAR10 dataset. From the plots we observe that our optimizers slightly outperform Momentum and SGD, while greatly outperforming RMSProp and Adam.\nThe controller discovered update rules that work well, but also produced update equations that are fairly intuitive. For instance, among the top candidates is the following update function:\n• esign(g)∗sign(m) ∗ g\nBecause esign(g)∗sign(m) is positive, in each dimension the update follows the direction of g with some adjustments to the scale. The term esign(g)∗sign(m) means that if the signs of the gradient and its running average agree, we should make an update to the coordinate with the scale of e, otherwise make an update with the scale of 1/e. This expression appears in many optimizers that the model found, showing up in 5 of the top 10 candidate update rules:\n• [esign(g)∗sign(m) + clip(g, 10−4)] ∗ g\n• drop(g, 0.3) ∗ esign(g)∗sign(m)\n• ADAM ∗ esign(g)∗sign(m)\n• drop(g, 0.1) ∗ esign(g)∗sign(m)"
  }, {
    "heading": "5. Transferability experiments",
    "text": "A key advantage of our method of discovering update equations compared to the previous approach (Andrychowicz et al., 2016) is that update equations found by our method can be easily transferred to new tasks. In the following experiments, we will exercise some of the update equations found in the previous experiment on different network architectures and tasks. The controller is not trained again, and the update rules are simply reused."
  }, {
    "heading": "5.1. Control Experiment with Rosenbrock function",
    "text": "We first test one of the optimizers we found in the previous experiment on the famous Rosenbrock function against the commonly used deep learning optimizers in TensorFlow (Abadi et al., 2016): Adam, SGD, RMSProp and Momentum, and tune the value of in Adam in a log scale between 10−3 and 10−8. In this experiment, each optimizer\nis run for 4000 iterations with 4 different learning rates on a logarithmic scale and and the best performance is plotted. The update rule we test is the intuitive g ∗ esign(g)∗sign(m). The results in Figure 6 show that our optimizer outperforms Adam, RMSProp, SGD, and is close to matching the performance of Momentum on this task."
  }, {
    "heading": "5.2. CIFAR-10 with Wide ResNet",
    "text": "We further investigate the generalizability of the found update rules on a different and much larger model: the Wide ResNet architecture (Zagoruyko & Komodakis, 2016). Our controller finds many optimizers that perform well when run for 5 epochs on the small ConvNet. To filter optimizers that do well when run for many more epochs, we run dozens of our top optimizers for 300 epochs and aggressively stop optimizers that show less promise. The top optimizers identified by this process are also the top optimizers for the small ConvNet and the GNMT experiment.\nFigure 7 shows the comparison between SGD, Adam, RMSProp, Momentum and two of the top candidate optimizers. The plots reveal that these two optimizers outperform the other optimizers by a size-able margin on a competitive CIFAR-10 model.\nTable 1 shows more details of the comparison between our top 16 optimizers against the commonly used SGD, RMSProp, Momentum, and Adam optimizers. We note that although Momentum was used and tuned by Zagoruyko & Komodakis (2016), many of our updates outperform that setup. The margin of improvement is around 1%. Our method is also better than other optimizers, with a margin up to 2%."
  }, {
    "heading": "5.3. Neural Machine Translation",
    "text": "We run one particularly promising optimizer, g ∗ esign(g)∗sign(m), on the WMT 2014 English → German task. Our goal is to test the transferability of this optimizer on a completely different model and task, since before our optimizers were run on convolutional networks and the translation models are RNNs. Our optimizer in this experiment is compared against the Adam optimizer (Kingma & Ba, 2015). The architecture of interest is the Google\nNeural Machine Translation (GNMT) model (Wu et al., 2016), which was shown to achieve competitive translation quality on the English → German task. The GNMT network comprises 8 LSTM layers for both its encoder and decoder (Hochreiter & Schmidhuber, 1997), with the first layer of the encoder having bidirectional connections. This GNMT model also employs attention in the form of a 1 layer neural network.\nThe model is trained in a distributed fashion using a parameter server. Twelve workers are used, with each worker using 8 GPUs and a minibatch size of 128. Further details for this model can be found in Wu et al. (2016).\nIn our experiments, the only change we make to training is to replace Adam with the new update rule. We note that the GNMT model’s hyperparameters, such as weight initialization, were previously tuned to work well with Adam (Wu et al., 2016), so we expect more tuning can further improve the results of this new update rule.\nThe results in Table 2 show that our optimizer does indeed generalize well and achieves an improvement of 0.1 perplexity, which is considered to be a decent gain on this task. This gain in training perplexity enables the model to obtain\na 0.5 BLEU improvement over the Adam optimizer on the test set Wu et al. (2016). On the validation set, the averaged improvement of 5 points near the peak values is 0.7 BLEU.\nFinally, the update rule is also more memory efficient as it only keeps one running average per parameter, compared to two running averages for Adam. This has practical implications for much larger translation models where Adam cannot currently be used due to memory constraints (Shazeer et al., 2017)."
  }, {
    "heading": "6. Conclusion",
    "text": "This paper considers an approach for automating the discovery of optimizers with a focus on deep neural network architectures. We evaluate our discovered optimizers on widely used CIFAR-10 and NMT models for which we obtain competitive performance against common optimizers and validate to some extent that the optimizers generalize across architectures and datasets.\nOne strength of our approach is that it naturally encompasses the environment in which the optimization process happens. One may for example use our method for discovering optimizers that perform well in scenarios where computations are only carried out using 4 bits, or a distributed setup where workers can only communicate a few bits of information to a shared parameter server. Unlike previous approaches in learning to learn, the update rules in the form of equations can be easily transferred to other optimization tasks.\nFinally, one of the update rules found by our method, g ∗esign(g)∗sign(m), is surprisingly intuitive and particularly promising. Our experiments show that it performs well on a range of tasks that we have tried, from image classification with ConvNets to machine translation with LSTMs. In addition to opening up new ways to design update rules, this new update rule can now be used to improve the training of deep networks."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Samy Bengio, Jeff Dean, Vishy Tirumalashetty and the Google Brain team for the help with the project."
  }],
  "year": 2017,
  "references": [{
    "title": "Tensorflow: A system for large-scale machine learning",
    "authors": ["Tucker", "Paul", "Vasudevan", "Vijay", "Warden", "Pete", "Wicke", "Martin", "Yu", "Yuan", "Zheng", "Xiaoqiang"],
    "venue": "Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI),",
    "year": 2016
  }, {
    "title": "Learning to learn by gradient descent by gradient descent",
    "authors": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Distributed second-order optimization using Kroneckerfactored approximations",
    "authors": ["Ba", "Jimmy", "Grosse", "Roger", "Martens", "James"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Designing neural network architectures using reinforcement learning",
    "authors": ["Baker", "Bowen", "Gupta", "Otkrist", "Naik", "Nikhil", "Raskar", "Ramesh"],
    "venue": "arXiv preprint arXiv:1611.02167,",
    "year": 2016
  }, {
    "title": "Use of genetic programming for the search of a new learning rule for neural networks",
    "authors": ["Bengio", "Samy", "Yoshua", "Cloutier", "Jocelyn"],
    "venue": "In Evolutionary Computation,",
    "year": 1994
  }, {
    "title": "Large scale distributed deep networks",
    "authors": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Long shortterm memory",
    "authors": ["Hochreiter", "Sepp", "Schmidhuber", "Jürgen"],
    "venue": "Neural Computation,",
    "year": 1997
  }, {
    "title": "Learning to learn using gradient descent",
    "authors": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"],
    "venue": "In International Conference on Artificial Neural Networks,",
    "year": 2001
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2015
  }, {
    "title": "On optimization methods for deep learning",
    "authors": ["Le", "Quoc V", "Ngiam", "Jiquan", "Coates", "Adam", "Lahiri", "Ahbik", "Prochnow", "Bobby", "Ng", "Andrew Y"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning,",
    "year": 2011
  }, {
    "title": "Efficient backprop",
    "authors": ["LeCun", "Yann A", "Bottou", "Léon", "Orr", "Genevieve B", "Müller", "Klaus-Robert"],
    "venue": "In Neural networks: Tricks of the trade. Springer,",
    "year": 1998
  }, {
    "title": "On the limited memory BFGS method for large scale optimization",
    "authors": ["Liu", "Dong C", "Nocedal", "Jorge"],
    "venue": "Mathematical programming,",
    "year": 1989
  }, {
    "title": "SGDR: stochastic gradient descent with restarts",
    "authors": ["Loshchilov", "Ilya", "Hutter", "Frank"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Deep learning via Hessian-free optimization",
    "authors": ["Martens", "James"],
    "venue": "In Proceedings of the 27th International Conference on Machine Learning,",
    "year": 2010
  }, {
    "title": "Training deep and recurrent networks with Hessian-free optimization",
    "authors": ["Martens", "James", "Sutskever", "Ilya"],
    "venue": "In Neural networks: Tricks of the trade,",
    "year": 2012
  }, {
    "title": "The evolution of a generalized neural learning rule",
    "authors": ["Orchard", "Jeff", "Wang", "Lin"],
    "venue": "In 2016 International Joint Conference on Neural Networks (IJCNN),",
    "year": 2016
  }, {
    "title": "Revisiting natural gradient for deep networks",
    "authors": ["Pascanu", "Razvan", "Bengio", "Yoshua"],
    "venue": "arXiv preprint arXiv:1301.3584,",
    "year": 2013
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Optimization as a model for few-shot learning",
    "authors": ["Ravi", "Sachin", "Larochelle", "Hugo"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "RPROP - a fast adaptive learning algorithm",
    "authors": ["Riedmiller", "Martin", "Braun", "Heinrich"],
    "venue": "In Proc. of ISCIS VII, Universitat. Citeseer,",
    "year": 1992
  }, {
    "title": "Evolution and design of distributed learning rules",
    "authors": ["Runarsson", "Thomas P", "Jonsson", "Magnus T"],
    "venue": "In IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks,",
    "year": 2000
  }, {
    "title": "No more pesky learning rates",
    "authors": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Steps towards ’self-referential’ neural learning: A thought experiment",
    "authors": ["Schmidhuber", "Juergen"],
    "venue": "Technical report, University of Colorado Boulder,",
    "year": 1992
  }, {
    "title": "Fast curvature matrix-vector products for second-order gradient descent",
    "authors": ["Schraudolph", "Nicol N"],
    "venue": "Neural Computation,",
    "year": 2002
  }, {
    "title": "Trust region policy optimization",
    "authors": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Outrageously large neural networks: The sparselygated mixture-of-experts layer",
    "authors": ["Shazeer", "Noam", "Mirhoseini", "Azalia", "Maziarz", "Krzysztof", "Davis", "Andy", "Le", "Quoc", "Hinton", "Geoffrey", "Dean", "Jeff"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Learned optimizers that scale and generalize",
    "authors": ["Wichrowska", "Olga", "Maheswaranathan", "Niru", "Hoffman", "Matthew W", "Colmenarejo", "Sergio Gomez", "Denil", "Misha", "de Freitas", "Nando", "Sohl-Dickstein", "Jascha"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["Williams", "Ronald J"],
    "venue": "In Machine Learning,",
    "year": 1992
  }, {
    "title": "Wide residual networks",
    "authors": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"],
    "venue": "arXiv preprint arXiv:1605.07146,",
    "year": 2016
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Zeiler", "Matthew D"],
    "venue": "arXiv preprint arXiv:1212.5701,",
    "year": 2012
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Neural Architecture Search with reinforcement learning",
    "authors": ["Zoph", "Barret", "Le", "Quoc V"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }],
  "id": "SP:74ea9b95117fea617e168a7a1e2bea21604edd51",
  "authors": [{
    "name": "Irwan Bello",
    "affiliations": []
  }, {
    "name": "Barret Zoph",
    "affiliations": []
  }, {
    "name": "Vijay Vasudevan",
    "affiliations": []
  }, {
    "name": "Quoc V. Le",
    "affiliations": []
  }],
  "abstractText": "We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a specific domain language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. These optimizers can also be transferred to perform well on different neural network architectures, including Google’s neural machine translation system.",
  "title": "Neural Optimizer Search with Reinforcement Learning"
}