{
  "sections": [{
    "text": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 912–921, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics\nMethods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation."
  }, {
    "heading": "1 Introduction",
    "text": "Recent advances in deep neural networks (DNNs) have demonstrated the importance of learning vector-space representations of text, e.g., words and sentences, for a number of natural language processing tasks. For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word\n∗This research was conducted during the author’s internship at Microsoft Research.\nrepresentations learned from large corpora. Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features. A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast.\nHowever, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.\nOur contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks. Our model learns to map arbitrary text queries and documents into semantic vector representations in a low dimensional latent space. While the general concept of multi-task neural nets is not new, our model is novel in that it successfully combines tasks as disparate as operations necessary for classifica-\n912\ntion or ranking. Second, we demonstrate strong results on query classification and web search. Our multi-task representation learning consistently outperforms stateof-the-art baselines. Meanwhile, we show that our model is not only compact but it also enables agile deployment into new domains. This is because the learned representations allow domain adaptation with substantially fewer in-domain labels."
  }, {
    "heading": "2 Multi-Task Representation Learning",
    "text": ""
  }, {
    "heading": "2.1 Preliminaries",
    "text": "Our multi-task model combines classification and ranking tasks. For concreteness, throughout this paper we will use query classification as the classification task and web search as the ranking task. These are important tasks in commercial search engines:\nQuery Classification: Given a search query Q, the model classifies in the binary fashion as to whether it belongs to one of the domains of interest. For example, if the query Q is “Denver sushi”, the classifier should decide that it belongs to the “Restaurant” domain. Accurate query classification enables a richer personalized user experience, since the search engine can tailor the interface and results. It is however challenging because queries tend to be short (Shen et al., 2006). Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution. In this study, we classify queries into four domains of interest: (“Restaurant”, “Hotel”, “Flight”, “Nightlife”). Note that one query can belong to multiple domains. Therefore, a set of binary classifiers are built, one for each domain, to perform the classification. We frame the problem as four binary classification tasks. Thus, for domain Ct, our goal is binary classification based on P (Ct| Q) (Ct = {0, 1} ). For each domain t, we assume supervised data (Q, yt = {0, 1} with yt as binary labels.1\nWeb Search: Given a search queryQ and a document list L, the model ranks documents in the order\n1One could frame the problem as a a single multi-class classification task, but our formulation is more practical as it allows adding new domains without retraining existing classifiers. This will be relevant in domain adaptation (§3.3).\nof relevance. For example, if the queryQ is ”Denver sushi”, model returns a list of documents that satisfies such information need. Formally, we estimate P (D1|Q), P (D2|Q), . . . for each document Dn and rank according to these probabilities. We assume that supervised data exist; I.e., there is at least one relevant document Dn for each query Q."
  }, {
    "heading": "2.2 The Proposed Multi-Task DNN Model",
    "text": "Briefly, our proposed model maps any arbitrary queries Q or documents D into fixed lowdimensional vector representations using DNNs. These vectors can then be used to perform query classification or web search. In contrast to existing representation learning methods which employ either unsupervised or single-task supervised objectives, our model learns these representations using multi-task objectives.\nThe architecture of our multi-task DNN model is shown in Figure 1. The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs. Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300. This is the shared semantic representation that is trained by our multi-task objectives. In the following, we elaborate the model in detail:\nWord Hash Layer (l1): Traditionally, each word is represented by a one-hot word vector, where the dimensionality of the vector is the vocabulary size. However, due to the large size of vocabulary in realworld tasks, it is very expensive to learn such kind of models. To alleviate this problem, we adopt the word hashing method (Huang et al., 2013). We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k). For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol. Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.\nSemantic-Representation Layer (l2): This is a shared representation learned across different tasks. this layer maps the letter-trigram inputs into a 300-\n1\ndimensional vector by\nl2 = f(W1 · l1) (1)\nwhere f(·) is the tanh nonlinear activation f(z) = 1−e−2z 1+e−2z . This 50k-by-300 matrix W1 is responsible for generating the cross-task semantic representation for arbitrary text inputs (e.g., Q or D).\nTask-Specific Representation (l3): For each task, a nonlinear transformation maps the 300- dimension semantic representation l2 into the 128- dimension task-specific representation by\nl3 = f(Wt2 · l2) (2)\nwhere, t denotes different tasks (query classification or web search).\nQuery Classification Output: Suppose QC1 ≡ l3 = f(Wt=C12 · l2) is the 128-dimension taskspecific representation for a query Q. The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z) = 1\n1+e−z :\nP (C1|Q) = g(Wt=C13 ·QC1) (3)\nWeb Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd . Then, the relevance score is\nAlgorithm 1: Training a Multi-task DNN Initialize model Θ : {W1,Wt2,Wt3} randomly for iteration in 0...∞ do\n1. Pick a task t randomly 2. Pick sample(s) from task t\n(Q, yt = {0, 1}) for query classification (Q,L) for web search\n3. Compute loss: L(Θ) L(Θ)=Eq. 5 for query classification L(Θ)=Eq. 6 for web search 4. Compute gradient: ∇(Θ) 5. Update model: Θ = Θ− ∇(Θ)\nend The task t is one of the query classification tasks or web search task, as shown in Figure 1. For query classification, each training sample includes one query and its category label. For web search, each training sample includes query and document list.\ncomputed by cosine similarity as:\nR(Q,D) = cos(QSq , DSd) = QSq ·DSd ||QSq ||||DSd || (4)"
  }, {
    "heading": "2.3 The Training Procedure",
    "text": "In order to learn the parameters of our model, we use mini-batch-based stochastic gradient descent (SGD) as shown in Algorithm 1. In each iteration, a task t is selected randomly, and the model is updated ac-\ncording to the task-specific objective. This approximately optimizes the sum of all multi-task objectives. For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))} (5)\nwhere yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).\nThe objective for web search used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005). Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J . We then minimize the negative log likelihood of the clicked document (defined in Eq. 7) given queries across the training data\n− log ∏\n(Q,D+)\nP (D+|Q) (6)\nwhere the probability of a given document D+ is computed\nP (D+|Q) = exp(γR(Q,D +))∑\nD′∈L exp(γR(Q,D′)) (7)\nhere, γ is a tuning factor determined on held-out data. Additional training details: (1) Model parameters are initialized with uniform distribution in the range (−√6/(fanin + fanout),√6/(fanin + fanout)) (Montavon et al., 2012). Empirically, we have not observed better performance by initialization with layer-wise pre-training. (2) Moment methods and AdaGrad training (Duchi et al., 2011) speed up the convergence speed but gave similar results as plain SGD. The SGD learning rate is fixed at = 0.1/1024. (3) We run Algorithm 1 for 800K iterations, taking 13 hours on an NVidia K20 GPU."
  }, {
    "heading": "2.4 An Alternative View of the Multi-Task Model",
    "text": "Our proposed multi-task DNN (Figure 1) can be viewed as a combination of a standard DNN for classification and a Deep Structured Semantic Model (DSSM) for ranking, shown in Figure 2. Other ways to merge the models are possible. Figure 3 shows an alternative multi-task architecture, where only the query part is shared among all tasks and the DSSM\nretains independent parameters for computing the document representations. This is more similar to the original DSSM. We have attempted training this model using Algorithm 1, but it achieves good results on query classification at the expense of web search. This is likely due to unbalanced updates (i.e. parameters for queries are updated more often than that of documents), and implying that the amount of sharing is an important design choice in multi-task models.\n3"
  }, {
    "heading": "3 Experimental Evaluation",
    "text": ""
  }, {
    "heading": "3.1 Data Sets and Evaluation Metrics",
    "text": "We employ large-scale, real data sets in our evaluation. See Table 1 for statistics. The test data for query classification were sampled from one-year log files of a commercial search engine with labels (yes or no) judged by humans. The test data for web search contains 12,071 English queries, where each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad, fair,\ngood, excellent and perfect. The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997). For web search, we employ the Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2000)."
  }, {
    "heading": "3.2 Results on Accuracy",
    "text": "First, we evaluate whether our model can robustly improve performance, measured as accuracy across multiple tasks.\nTable 2 summarizes the AUC scores for query classification, comparing the following classifiers: • SVM-Word: a SVM model2 with unigram, bi-\ngram and trigram surface-form word features.\n• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM). • DNN: single-task deep neural net (Figure 2). • MT-DNN: our multi-task proposal (Figure 1). The results show that the proposed MT-DNN performs best in all four domains. Further, we observe:\n1. MT-DNN outperforms DNN, indicating the usefulness of the multi-task objective (that includes web search) over the single-task objective of query classification.\n2. Both DNN and MT-DNN outperform SVMLetter, which initially uses the same input features (l1). This indicates the importance of learning a semantic representation l2 on top of these letter trigrams.\n3. Both DNN and MT-DNN outperform a strong SVM-Word baseline, which has a large feature set that consists of 3 billion features.\nTable 3 summarizes the NDCG results on web search, comparing the following models: 2In this paper, we use the liblinear to build SVM classifiers and optimize the corresponding parameter C by using 5-fold cross-validation in training data. http://www.csie.ntu.edu.tw/ cjlin/liblinear/\n• Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA\n• DSSM: single-task ranking model (Figure 2) • MT-DNN: our multi-task proposal (Figure 1)\nAgain, we observe that MT-DNN performs best. For example, MT-DNN achieves NDCG@1=0.334, outperforming the current state-of-the-art single-task DSSM (0.327) and the classic methods like PLSA (0.308) and BM25 (0.305). This is a statistically significant improvement (p < 0.05) over DSSM and other baselines.\nTo recap, our MT-DNN robustly outperforms strong baselines across all web search and query classification tasks. Further, due to the use of larger training data (from different domains) and the regularization effort as we discussed in Section 1, we confirm the advantage of multi-task models over than single-task ones.3"
  }, {
    "heading": "3.3 Results on Model Compactness and Domain Adaptation",
    "text": "Important criteria for building practical systems are agility of deployment and small memory footprint and fast run-time. Our model satisfies both with 3We have also trained SVM using Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) features. Unfortunately, the results are poor at 60-70 AUC, indicating the sub-optimality of unsupervised representation learning objectives for actual prediction tasks. We optimized the Word2Vec features in the SVM baseline by scaling and normalizing as well, but did not observe much improvement.\nhigh model compactness. The key to the compactness is the aggressive compression from the 500kdimensional bag-of-words input to 300-dimensional semantic representation l2. This significantly reduces the memory/run-time requirements compared to systems that rely on surface-form features. The most expensive portion of the model is storage of the 50k-by-300 W1 and its matrix multiplication with l1, which is sparse: this is trivial on modern hardware. Our multi-task DNN takes < 150KB in memory whereas e.g. SVM-Word takes about 200MB.\nCompactness is particularly important for query classification, since one may desire to add new domains after discovering new needs from the query logs of an operational system. On the other hand, it is prohibitively expensive to collect labeled training data for new domains. Very often, we only have very small training data or even no training data.\nTo evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗. Train MTDNN on the remaining tasks (including Web Search\ntask) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗\nWe compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure. (2) Word3gram uses unigram, bigram and trigram word features. (3) Letter3gram uses letter-trigrams. Note that Word3gram and Letter3gram correspond to SVMWord and SVM-Letter respectively in Table 2.\nThe AUC results for different amounts of t∗ training data are shown in Figure 4. In the Hotel, Flight and Restaurant domains, we observe that our semantic representation dominated the other two feature representations (Word3gram and Letter3gram) in all cases except the extremely large-data regime (more than 1 million training samples in domain t∗). Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se-\nmanticRepresentation is recommended.4\nIn a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗; (2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗. The purpose is to see whether shared semantic representation is useful even under a DNN architecture. Figure 5 show the AUC results of DNN1 vs. DNN2 (the results SVM denotes the same system as SemanticRepresentation in Figure 4, plotted here for reference). We observe that when the training data is extremely large (millions of samples), one does best by training all parameters from scratch (DNN1). Otherwise, one is better off using a shared semantic representation trained by multitask objectives. Comparing DNN2 and SVM with SemanticRepresentation, we note that SVM works best for training data of several thousand samples; DNN2 works best in the medium data range."
  }, {
    "heading": "4 Related Work",
    "text": "There is a large body of work on representation learning for natural language processing, sometimes using different terminologies for similar concepts; e.g., feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Schölkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014).\nPopular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec-\n4The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples).\ntives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a).\nOur model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014).\nThe synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997). The main challenge is in designing the tasks and the network structure. For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system. While conceptually similar, our model is novel in that it combines tasks as disparate as classification and ranking. Further, considering that multi-task models often exhibit mixed results (i.e. gains in some tasks but degradation in others), our accuracy improvements across all tasks is a very satisfactory result."
  }, {
    "heading": "5 Conclusion",
    "text": "In this work, we propose a robust and practical representation learning algorithm based on multi-task objectives. Our multi-task DNN model successfully combines tasks as disparate as classification and ranking, and the experimental results demon-\nstrate that the model consistently outperforms strong baselines in various query classification and web search tasks. Meanwhile, we demonstrated compactness of the model and the utility of the learned query/document representation for domain adaptation.\nOur model can be viewed as a general method for learning semantic representations beyond the word level. Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks. A comprehensive exploration will be pursued as future work."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Xiaolong Li, Yelong Shen, Xinying Song, Jianshu Chen, Byungki Byun, Bin Cao and the anonymous reviewers for valuable discussions and comments."
  }],
  "year": 2015,
  "references": [{
    "title": "Don’t count, predict! a systematic comparison of context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
    "year": 2014
  }, {
    "title": "Latent dirichlet allocation",
    "authors": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."],
    "venue": "the Journal of machine Learning research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Joint learning of words and meaning representations for open-text semantic parsing",
    "authors": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."],
    "venue": "AISTATS.",
    "year": 2012
  }, {
    "title": "The use of the area under the roc curve in the evaluation of machine learning algorithms",
    "authors": ["Andrew P Bradley."],
    "venue": "Pattern recognition, 30(7):1145–1159.",
    "year": 1997
  }, {
    "title": "Learning to rank using gradient descent",
    "authors": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender."],
    "venue": "Proceedings of the 22nd international conference on Machine learning, pages 89–96. ACM.",
    "year": 2005
  }, {
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Machine Learning, 28.",
    "year": 1997
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical",
    "year": 2014
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa."],
    "venue": "Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Indexing by latent semantic analysis",
    "authors": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."],
    "venue": "Journal of the American Society for Information Science, 41(6).",
    "year": 1990
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis",
    "authors": ["E. Gabrilovich", "S. Markovitch."],
    "venue": "IJCAI.",
    "year": 2007
  }, {
    "title": "Clickthrough-based translation models for web search: from word models to phrase models",
    "authors": ["Jianfeng Gao", "Xiaodong He", "Jian-Yun Nie."],
    "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1139–1148. ACM.",
    "year": 2010
  }, {
    "title": "Clickthrough-based latent semantic models for web search",
    "authors": ["Jianfeng Gao", "Kristina Toutanova", "Wen-tau Yih."],
    "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 675–684. ACM.",
    "year": 2011
  }, {
    "title": "Learning continuous phrase representations for translation modeling",
    "authors": ["Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 699–709,",
    "year": 2014
  }, {
    "title": "Modeling interestingness with deep neural networks",
    "authors": ["Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2–13, Doha, Qatar,",
    "year": 2014
  }, {
    "title": "Probabilistic latent semantic indexing",
    "authors": ["Thomas Hofmann."],
    "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50–57. ACM.",
    "year": 1999
  }, {
    "title": "Learning deep structured semantic models for web search using clickthrough data",
    "authors": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."],
    "venue": "Proceedings of the 22nd ACM international conference on Conference on information &",
    "year": 2013
  }, {
    "title": "Deep recursive neural networks for compositionality in language",
    "authors": ["Ozan Irsoy", "Claire Cardie."],
    "venue": "NIPS. 920",
    "year": 2014
  }, {
    "title": "A neural network for factoid question answering over paragraphs",
    "authors": ["Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daumé III."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    "year": 2014
  }, {
    "title": "Ir evaluation methods for retrieving highly relevant documents",
    "authors": ["Kalervo Järvelin", "Jaana Kekäläinen."],
    "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 41–48. ACM.",
    "year": 2000
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655–665, Balti-",
    "year": 2014
  }, {
    "title": "Principal component neural networkstheory and applications",
    "authors": ["Juha Karhunen."],
    "venue": "Pattern Analysis & Applications, 1(1):74–75.",
    "year": 1998
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc Le", "Tomas Mikolov."],
    "venue": "Proceedings of the International Conference on Machine Learning (ICML).",
    "year": 2014
  }, {
    "title": "Frege in space: A program for compositional distributional semantics",
    "authors": ["R. Bernardi M. Baroni", "R. Zamparelli."],
    "venue": "Linguistic Issues in Language Technologies.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomás̆ Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"],
    "year": 2013
  }, {
    "title": "Evaluating neural word representations in tensor-based compositional settings",
    "authors": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process-",
    "year": 2014
  }, {
    "title": "Learning word embeddings efficiently with noise-contrastive estimation",
    "authors": ["Andriy Mnih", "Koray Kavukcuoglu."],
    "venue": "Advances in Neural Information Processing Systems 26 (NIPS 2013).",
    "year": 2013
  }, {
    "title": "Neural Networks: Tricks of the Trade 2nd ed",
    "authors": ["Gregoire Montavon", "Genevieve Orr", "Klaus-Robert Muller."],
    "venue": "springer.",
    "year": 2012
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, Octo-",
    "year": 2014
  }, {
    "title": "Nonlinear component analysis as kernel eigenvalue problem",
    "authors": ["B. Schölkopf", "A. Smola", "K.-R. Müller."],
    "venue": "Neural Computation, 10.",
    "year": 1998
  }, {
    "title": "Query enrichment for web-query classification",
    "authors": ["Dou Shen", "Rong Pan", "Jian-Tao Sun", "Jeffrey Junfeng Pan", "Kangheng Wu", "Jie Yin", "Qiang Yang."],
    "venue": "ACM Trans. Inf. Syst., 24(3):320–352, July.",
    "year": 2006
  }, {
    "title": "A latent semantic model with convolutional-pooling structure for information retrieval",
    "authors": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Grégoire Mesnil."],
    "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and",
    "year": 2014
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 Conference on Empirical Meth-",
    "year": 2013
  }, {
    "title": "Word representations: A simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala,",
    "year": 2010
  }, {
    "title": "A study of smoothing methods for language models applied to ad hoc information retrieval",
    "authors": ["Chengxiang Zhai", "John Lafferty."],
    "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,",
    "year": 2001
  }],
  "id": "SP:c3b8367a80181e28c95630b9b63060d895de08ff",
  "authors": [{
    "name": "Xiaodong Liu",
    "affiliations": []
  }, {
    "name": "Jianfeng Gao",
    "affiliations": []
  }, {
    "name": "Xiaodong He",
    "affiliations": []
  }, {
    "name": "Li Deng",
    "affiliations": []
  }, {
    "name": "Kevin Duh",
    "affiliations": []
  }, {
    "name": "Ye-yi Wang",
    "affiliations": []
  }],
  "abstractText": "Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",
  "title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval"
}