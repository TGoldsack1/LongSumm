{
  "sections": [{
    "heading": "1. Introduction",
    "text": "This paper is concerned with the problem of estimating entropy or mutual information of an unknown probability density p over RD, given n i.i.d. samples from p. Entropy and mutual information are fundamental information theoretic quantities, and consistent estimators for these quantities have a host of applications within machine learning, statistics, and signal processing. For example, entropy estimators have been used for goodness-of-fit testing (Goria et al., 2005), parameter estimation in semi-parametric models (Wolsztynski et al., 2005), texture classification and image registration (Hero et al., 2001; 2002), change point de-\n1Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Shashank Singh <sss1@andrew.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntection (Bercher & Vignat, 2000), and anomaly detection in networks (Noble & Cook, 2003; Nychis et al., 2008; Bereziński et al., 2015). Mutual information is a popular nonparametric measure of dependence, whose estimators have been used in feature selection (Peng et al., 2005; Shishkin et al., 2016), clustering (Aghagolzadeh et al., 2007), learning graphical models (Chow & Liu, 1968), fMRI data processing (Chai et al., 2009), prediction of protein structures (Adami, 2004), boosting and facial expression recognition (Shan et al., 2005), and fitting deep nonlinear models (Hunter & Hodas, 2016). Estimators for both entropy and mutual information have been used in independent component and subspace analysis (Learned-Miller & Fisher, 2003; Szabó et al., 2007a).\nMotivated by these and other applications, several very recent lines of work (discussed in Section 3) have studied information estimation,1 focusing largely on two settings:\n1. Gaussian Setting: If p is known to be Gaussian, there exist information estimators with mean squared error (MSE) at most −2 log ( 1− Dn ) and an (almost matching) minimax lower bound of 2D/n (Cai et al., 2015). 2. Nonparametric Setting: If p is assumed to lie in a nonparametric smoothness class, such an s-order2 Hölder or Sobolev class, then the minimax MSE is of asymptotic order max { n−1, n− 8s 4s+D } (Birgé & Massart, 1995).\nIn the Gaussian setting, consistent estimation is tractable even in the high-dimensional case whereD increases fairly quickly with n, as long as D/n → 0. However, optimal estimators for the Gaussian setting rely heavily on the assumption of joint Gaussianity, and their performance can degrade quickly when the data deviate from Gaussian. Especially in high dimensions, it is unlikely that data are jointly Gaussian, making these estimators brittle in practice. In the nonparametric setting, the theoretical convergence rate decays exponentially with D, and, it has been found empirically that information estimators for this setting fail to converge at realistic sample sizes in all but very low dimensions. Also, most nonparametric estimators are sensitive to tuning of bandwidth parameters, which is chal-\n1We will collectively call the closely related problems of entropy and mutual information estimation information estimation.\n2Here, s encodes the degree of smoothness, roughly corresponding to the number of continuous derivatives of p.\nlenging for information estimation, since no empirical error estimate is available for cross-validation.\nGiven these factors, though the Gaussian and nonparametric cases are fairly well understood in theory, there remains a lack of practical information estimators for the common case where data are neither exactly Gaussian nor very low dimensional. The main goal of this paper is to fill the gap between these two extreme settings by studying information estimation in a semiparametric compromise between the two, known as the “nonparanormal” (a.k.a. “Gaussian copula”) model (see Definition 2). The nonparanormal model, analogous to the additive model popular in regression (Friedman & Stuetzle, 1981), limits complexity of interactions among variables but makes minimal assumptions on the marginal distribution of each variable. The result scales better with dimension than nonparametric models, while being more robust than Gaussian models.\nProofs of our main results, as well as additional experiments and details are given in the extended version of this paper 3. All code can be found on GitHub4."
  }, {
    "heading": "2. Problem statement and notation",
    "text": "There are a number of distinct generalizations of mutual information to more than two variables. The definition we consider is simply the difference between the sum of marginal entropies and the joint entropy:\nDefinition 1. (Multivariate mutual information) Let X1, . . . , XD be R-valued random variables with a joint probability density p : RD → [0,∞) and marginal densities p1, ..., pD : R→ [0,∞). The multivariate mutual information I(X) of X = (X1, . . . , XD) is defined by\nI(X) := E X∼p\n[ log ( p(X)∏D\nj=1 pj(Xj)\n)]\n= D∑ j=1 H(Xj)−H(X), (1)\nwhere H(X) = −EX∼p[log p(X)] denotes entropy of X .\nThis notion of multivariate mutual information, originally due to Watanabe (1960) (who called it “total correlation”) measures total dependency, or redundancy, within a set of D random variables. It has also been called the “multivariate constraint” (Garner, 1962) and “multiinformation” (Studenỳ & Vejnarová, 1998). Many related information theoretic quantities can be expressed in\n3accessible at http://www.contrib.andrew. cmu.edu/˜sss1/publications/papers/ nonparanormal-information-estimation.pdf\n4https://github.com/sss1/ nonparanormal-information\nterms of I(X), and can thus be estimated using estimators of I(X). Examples include pairwise mutual information I(X,Y ) = I((X,Y ))−I(X)−I(Y ), which measures dependence between (potentially multivariate) random variables X and Y , conditional mutual information\nI(X|Z) = I((X,Z))− D∑ j=1 I((Xj , Z)),\nwhich is useful for characterizing how much dependence within X can be explained by a latent variable Z (Studenỳ & Vejnarová, 1998), and transfer entropy (a.k.a. “directed information”) TX → Y , which measures predictive power of one time series X on the future of another time series Y . I(X) is also related to entropy via Eq. (1), but, unlike the above quantities, this relationship depends on the marginal distributions of X , and hence involves some additional considerations, as discussed in Section 8.\nWe now define the class of nonparanormal distributions, from which we assume our data are drawn.\nDefinition 2. (Nonparanormal distribution, a.k.a. Gaussian copula model) A random vector X = (X1, . . . , XD)\nT is said to have a nonparanormal distribution (denoted X ∼ NPN (Σ; f)) if there exist functions {fj}Dj=1 such that each fj : R → R is a diffeomorphism 5 and f(X) ∼ N (0,Σ), for some (strictly) positive definite Σ ∈ RD×D with 1’s on the diagonal (i.e., each σj = Σj,j = 1). 6 Σ is called the latent covariance of X and f is called the marginal transformation of X .\nThe nonparanormal family relaxes many constraints of the Gaussian family. Nonparanormal distributions can be multi-modal, skewed, or heavy-tailed, can encode noisy nonlinear dependencies, and need not be supported on RD. Minimal assumptions are made on the marginal distributions; any desired continuously differentiable marginal cumulative distribution function (CDF) Fi of variableXi corresponds to marginal transformation fi(x) = Φ−1(Fi(x)) (where Φ is the standard normal CDF). As examples, for a Gaussian variable Z, the 2-dimensional case, X1 ∼ N (0, 1), and X2 = T (X1 + Z) is nonparanormal when T (x) = x3, T = tanh, T = Φ, or any other diffeomorphism. On the other hand, the limits of the Gaussian copula appear, for example, when T (x) = x2, which is not bijective; then, if E[Z] = 0, the Gaussian copula approximation of (X1, X2) models X1 and X2 as independent.\n5A diffeomorphism is a continuously differentiable bijection g : R→ R ⊆ R such that g−1 is continuously differentiable.\n6Setting E [f(X)] = 0 and each σj = 1 ensures model identifiability, but does not reduce the model space, since these parameters can be absorbed into the marginal transformation f .\nWe are now ready to formally state our problem:\nFormal Problem Statement: Given n i.i.d. samples X1, ..., Xn ∼ NPN (Σ; f), where Σ and f are both unknown, we would like to estimate I(X).\nOther notation: D denotes the dimension of the data (i.e., Σ ∈ RD×D and f : RD → RD). For a positive integer k, [k] = {1, ..., k} denotes the set of positive integers less than k (inclusive). For consistency, where possible, we use i ∈ [n] to index samples and j ∈ [D] to index dimensions (so that, e.g., Xi,j denotes the jth dimension of the ith sample). Given a data matrix X ∈ Rn×D, our estimators depend on the empirical rank matrix\nR ∈ [n]n×D with Ri,j := n∑ k=1 1{Xi,j≥Xk,j}. (2)\nFor a square matrixA ∈ Rk×k, |A| denotes the determinant of A, AT denotes the transpose of A, and\n‖A‖2 := max x ∈ Rk ‖x‖2 = 1\n‖Ax‖2 and ‖A‖F := √ ∑ i,j∈[k] A2i,j\ndenote the spectral and Frobenius norms ofA, respectively. When A is symmetric, λ1(A) ≥ λ2(A) ≥ · · · ≥ λD(A) are its eigenvalues."
  }, {
    "heading": "3. Related Work and Our Contributions",
    "text": ""
  }, {
    "heading": "3.1. The Nonparanormal",
    "text": "Nonparanormal models have been used for modeling dependencies among high-dimensional data in a number of fields, such as graphical modeling of gene expression data (Liu et al., 2012), of neural data (Berkes et al., 2009), and of financial time series (Malevergne & Sornette, 2003; Wilson & Ghahramani, 2010; Hernández-Lobato et al., 2013), extreme value analysis in hydrology (Renard & Lang, 2007; Aghakouchak, 2014), and informative data compression (Rey & Roth, 2012).\nWith one recent exception (Ince et al., 2017), previous information estimators for the nonparanormal case (Calsaverini & Vicente, 2009; Ma & Sun, 2011; Elidan, 2013), rely on fully nonparametric information estimators as subroutines, and hence suffer strongly from the curse of dimensionality. Very recently, Ince et al. (2017) proposed what we believe is the first mutual information estimator tailored specifically to the nonparanormal case; their estimator is equivalent to one of the estimators (IG, described in Section 4.1) we study. However, they focused on its applications to neuroimaging data analysis, and did not study its performance theoretically or empirically."
  }, {
    "heading": "3.2. Information Estimation",
    "text": "Our motivation for studying the nonparanormal family comes from trying to bridge two recent approaches to information estimation. The first has studied fully nonparametric entropy estimation, assuming only that data are drawn from a smooth probability density p, where smoothness is typically quantified by a Hölder or Sobolev exponent s ∈ (0,∞), roughly corresponding to the continuous differentiability of s. In this setting, the minimax optimal MSE rate has been shown by Birgé & Massart (1995) to be O ( max { n−1, n− 8s 4s+D }) . This rate slows exponen-\ntially with the dimension D, and, while many estimators have been proposed (Pál et al., 2010; Sricharan et al., 2011; 2013; Singh & Póczos, 2014a;b; Krishnamurthy et al., 2014; Moon & Hero, 2014b;a; Singh & Póczos, 2016a; Moon et al., 2017) for this setting, their practical use is limited to a few dimensions7.\nThe second area is in the setting where data are assumed to be drawn from a truly Gaussian distribution. Here the highdimensional case is far more optimistic. While this case had been studied previously (Ahmed & Gokhale, 1989; Misra et al., 2005; Srivastava & Gupta, 2008), Cai et al. (2015) recently provided a precise finite-sample analysis based on deriving the exact probability law of the logdeterminant log |Σ̂| of the scatter matrix Σ̂. From this, they derived a deterministic bias correction, giving an estimator for which they prove an MSE upper bound of −2 log ( 1− Dn ) and a high-dimensional central limit theorem for the case D →∞ as n→∞ (but D < n).\nCai et al. (2015) also prove a minimax lower bound of 2D/n on MSE, with several interesting consequences. First, consistent information estimation is possible only if D/n → 0. Second, since, for small x, − log(1 − x) ≈ x, this lower bound essentially matches the above upper bound when D/n is small. Third, they show this lower bound holds even when restricted to diagonal covariance matrices. Since the upper bound for the general case and the lower bound for the diagonal case essentially match, it follows that Gaussian information estimation is not made easier by structural assumptions such as Σ being bandable, sparse, or Toeplitz, as is common in, for example, stationary Gaussian process models (Cai & Yuan, 2012).\nThis 2D/n lower bound extends to our more general nonparanormal setting. However, we provide a minimax lower bound suggesting that the nonparanormal setting is strictly harder, in that optimal rates depend on Σ. Our results imply\n7“Few” depends on s and n, but Kandasamy et al. (2015) suggest nonparametric estimators should only be used withD at most 4-6. Rey & Roth (2012) tried using several nonparametric information estimators on the Communities and Crime UCI data set (n = 2195, D = 10), but found all too unstable to be useful.\nnonparanormal information estimation does become easier if Σ is assumed to be bandable or Toeplitz.\nA closely related point is that known convergence rates for the fully nonparametric case require the density p to be bounded away from 0 or have particular tail behavior, due to singularity of the logarithm near 0 and resulting sensitivity of Shannon information-theoretic functionals to regions of low but non-zero probability. In contrast, Cai et al. (2015) need no lower-bound-type assumptions in the Gaussian case. In the nonparanormal case, we show some such condition is needed to prove a uniform rate, but a weaker condition, a positive lower bound on λD(Σ), suffices.\nThe main contributions of this paper are the following:\n1. We propose three estimators, ÎG, Îρ, and Îτ ,8 for the mutual information of a nonparanormal distribution. 2. We prove upper bounds, of order O(D2/(λ2D(Σ)n)) on the mean squared error of Îρ, providing the first upper bounds for a nonparanormal information estimator. This bound suggests nonparanormal estimators scale far better with D than nonparametric estimators. 3. We prove a minimax lower bound suggesting that, unlike the Gaussian case, difficulty of nonparanormal information estimation depends on the true Σ. 4. We give simulations comparing our proposed estimators to Gaussian and nonparametric estimators. Besides confirming and augmenting our theoretical predictions, these help characterize the settings in which each nonparanormal estimator works best. 5. We present entropy estimators based on ÎG, Îρ, and Îτ . Though nonparanormal entropy estimation requires somewhat different assumptions from mutual information estimation, we show that entropy can also be estimated at the rate O(D2/(λ2D(Σ)n))."
  }, {
    "heading": "4. Nonparanormal Information Estimators",
    "text": "In this section, we present three different estimators, IG, Iρ, and Iτ , for the mutual information of a nonparanormal distribution. We begin with a lemma providing common motivation for all three estimators.\nSince mutual information is invariant to diffeomorphisms of individual variables, it is easy to see that the mutual information of a nonparanormal random variable is the same as that of the latent Gaussian random variable. Specifically:\nLemma 3. (Nonparanormal mutual information): Suppose X ∼ NPN (Σ; f). Then,\nI(X) = −1 2 log |Σ|. (3)\n8Ince et al. (2017) proposed ÎG for use in neuroimaging data analysis. To the best of our knowledge, Îρ and Îτ are novel.\nLemma 3 shows that mutual information of a nonparanormal random variable depends only the latent covariance Σ; the marginal transformations are nuisance parameters, allowing us to avoid difficult nonparametric estimation; the estimators we propose all plug different estimates of Σ into Eq. (3), after a regularization step described in Section 4.3."
  }, {
    "heading": "4.1. Estimating Σ by Gaussianization",
    "text": "The first estimator Σ̂G of Σ proceeds in two steps. First, the data are transformed to have approximately standard normal marginal distributions, a process Szabó et al. (2007b) referred to as “Gaussianization”. By the nonparanormal assumption, the Gaussianized data are approximately jointly Gaussian. Then, the latent covariance matrix is estimated by the empirical covariance of the Gaussianized data.\nMore specifically, letting Φ−1 denote the quantile function of the standard normal distribution and recalling the rank matrix R defined in (2), the Gaussianized data\nX̃i,j := Φ −1 ( Ri,j n+ 1 ) (for i ∈ [n], j ∈ [D])\nare obtained by transforming the empirical CDF of the each dimension to approximate Φ. Then, we estimate Σ by the empirical covariance Σ̂G := 1n ∑n i=1 X̃iX̃ T i ."
  }, {
    "heading": "4.2. Estimating Σ by rank correlation",
    "text": "The second estimator actually has two variants, Iρ and Iτ , respectively based on relating the latent covariance to two classic rank-based dependence measures, Spearman’s ρ and Kendall’s τ . For two random variablesX and Y with CDFs FX , FY : R→ [0, 1], ρ and τ are defined by\nρ(X,Y ) := Corr(FX(X), FY (Y ))\nand τ(X,Y ) := Corr(sign(X −X ′), sign(Y − Y ′)),\nrespectively, where\nCorr(X,Y ) = E[(X − E[X])(Y − E[Y ])]√\nV[X]V[Y ]\ndenotes the standard Pearson correlation operator and (X ′, Y ′) is an IID copy of (X,Y ). ρ and τ generalize to the D-dimensional setting in the form of rank correlation matrices ρ, τ ∈ [−1, 1]D×D with ρj,k = ρ(Xj , Xk) and τj,k = τ(Xj , Xk) for each j, k ∈ [D].\nIρ and Iτ are based on a classical result relating the correlation and rank-correlation of a bivariate Gaussian:\nTheorem 4. (Kruskal, 1958): Suppose (X,Y ) has a Gaussian joint distribution with covariance Σ. Then,\nCorr(X,Y ) = 2 sin (π\n6 ρ(X,Y )\n) = sin (π 2 τ(X,Y ) ) .\nρ and τ are often preferred over Pearson correlation for their relative robustness to outliers and applicability to nonnumerical ordinal data. While these are strengths here as well, the main reason for their relevance is that they are invariant to marginal transformations (i.e., for diffeomorphisms f, g : R → R, ρ(f(X), g(Y )) = ±ρ(X,Y ) and τ(f(X), g(Y )) = ±τ(X,Y )). As a consequence, the identity provided in Theorem 4 extends unchanged to the case (X,Y ) ∼ NPN (Σ; f). This suggests an estimate for Σ based on estimating ρ or τ and plugging this elementwise into the transform x 7→ 2 sin ( π 6x ) or x 7→ sin ( π 2x ) , respectively. Specifically, Σρ is defined by\nΣ̂ρ := 2 sin (π 6 ρ̂ ) , where ρ̂ = Ĉorr(R)\nis the empirical correlation of the rank matrix R, and sine is applied element-wise. Similarly, Σ̂τ := sin ( π 2 τ̂ ) , where\nτ̂j,k := 1( n 2 ) ∑ i6=`∈[n] sign(Xi,j −X`,j) sign(Xi,k −X`,k)."
  }, {
    "heading": "4.3. Regularization and estimating I",
    "text": "Unfortunately, unlike usual empirical correlation matrices, none of Σ̂G, Σ̂ρ, or Σ̂τ is almost surely strictly positive definite. As a result, directly plugging into the mutual information functional (3) may give ∞ or be undefined. To correct for this, we propose a regularization step, in which we project each estimated latent covariance matrix onto the (closed) cone S(z) of symmetric matrices with minimum eigenvalue z > 0. Specifically, for any z > 0, let\nS(z) := { A ∈ RD×D : A = AT , λD(A) ≥ z } .\nFor any symmetric matrix A ∈ RD×D with eigendecomposition Σ̂ = QΛQ−1 (i.e., QQT = QTQ = ID and Λ is diagonal), the projection Az of A onto S(z) is defined as Az := QΛzQ−1, where Λz is the diagonal matrix with jth nonzero entry (Λz)j,j = max{z,Λj,j}. We call this a “projection” because Az = argminB∈S(z)‖A−B‖F (see, e.g., Henrion & Malick (2012)).\nApplying this regularization to Σ̂G, Σ̂ρ, or Σ̂τ gives a strictly positive definite estimate Σ̂G,z , Σ̂ρ,z , or Σ̂τ,z , respectively, of Σ. We can then estimate I by plugging this into Equation (3), giving our three estimators:\nÎG,z := − 1 2 log ∣∣∣Σ̂G,z∣∣∣ , Îρ,z := −1 2 log ∣∣∣Σ̂ρ,z∣∣∣\nand Îτ,z := − 1 2 log ∣∣∣Σ̂τ,z∣∣∣ ."
  }, {
    "heading": "5. Upper Bounds on the Error of Îρ,z",
    "text": "Here, we provide finite-sample upper bounds on the error of the estimator Îρ based on Spearman’s ρ. Proofs are given\nin the Appendix. We first bound the bias of Îρ:\nProposition 5. Suppose X1, ..., Xn i.i.d.∼ NPN (Σ; f). Then, there exists a constant C > 0 such that, for any z > 0, the bias of Îρ,z is at most∣∣∣E [Îρ,z]− I∣∣∣ ≤ C ( D\nz √ n + log |Σz| |Σ|\n) ,\nwhere Σz is the projection of Σ onto S(z).\nThe first term of the bias stems from nonlinearity of the log-determinant function in Equation 3, which we analyze via Taylor expansion. The second term,\nlog |Σz| |Σ|\n= ∑\nλj(Σ)<z\nlog\n( z\nλj(Σ)\n) ,\nis due to the regularization step and is actually exact, but is difficult to simplify or bound without more assumptions on the spectrum of Σ and choice of z, which we discuss later. We now turn to bounding the variance of Îρ,z . We first provide an exponential concentration inequality for Îρ,z around its expectation, based on McDiarmid’s inequality:\nProposition 6. Suppose X1, ..., Xn i.i.d.∼ NPN (Σ; f). Then, for any z, ε > 0,\nP [∣∣∣Îρ,z − E [Îρ,z]∣∣∣ > ε] ≤ 2 exp(− nz2ε2\n18π2D2\n) .\nSuch exponential concentration bounds are useful when one wants to simultaneously bound the error of multiple uses of an estimator, and hence we present it separately as it may be independently useful. However, for the purpose of understanding convergence rates, we are more interested in the variance bound that follows as an easy corollary:\nCorollary 7. Suppose X1, ..., Xn i.i.d.∼ NPN (Σ; f). Then, for any z > 0, the variance of Îρ,z is at most\nV [ Îρ,z ] ≤ 36π 2D2\nz2n .\nGiven these bias and variance bounds, a bound on the MSE of Îρ,z follows via the usual bias-variance decomposition:\nTheorem 8. Suppose X ∼ NPN (Σ; f). Then, there exists a constant C such that\nE [( Îρ,z − I )2] ≤ C ( D2\nz2n + log2 |Σz| |Σ|\n) . (4)\nA natural question is now how to optimally select the regularization parameter z. While the bound (4) is clearly convex in z, it depends crucially on the unknown spectrum of Σ, and, in particular, on the smallest eigenvalues of Σ. As a result, it is difficult to choose z optimally in general, but we can do so for certain common subclasses of covariance matrices. For example, if Σ is Toeplitz or bandable (i.e., for some c ∈ (0, 1), all |Σi,j | ≤ c|i−j|), then the smallest eigenvalue of Σ can be bounded below (Cai & Yuan, 2012). When Σ is bandable, as we show in the Appendix, this bound can be independent of D. In these cases, the following somewhat simpler MSE bound can be used: Corollary 9. Suppose X ∼ NPN (Σ; f), and suppose z ≤ λD(Σ). Then, there exists a constant C > 0 such that\nE [( Îρ,z − I )2] ≤ CD 2\nz2n ."
  }, {
    "heading": "6. Lower Bounds in terms of Σ",
    "text": "If X1, ..., Xn\ni.i.d∼ N (0,Σ) are Gaussian, for the plug-in estimator\nÎ = − 12 log ∣∣∣Σ̂∣∣∣ (where Σ̂ = 1n∑ni=1XiXTi\nis the empirical covariance matrix), Cai et al. (2015) showed that the distribution of Î − I is independent of the true correlation matrix Σ. This follows from the “stability” of Gaussians (i.e., that nonsingular linear transformations of Gaussian random variables are Gaussian). In particular,"
  }, {
    "heading": "Î − I = log |Σ̂| − log |Σ| = log |Σ−1/2Σ̂Σ−1/2|,",
    "text": "and Σ−1/2Σ̂Σ−1/2 has the same distribution as log Σ̂ does in the special case that Σ = ID is the identity. This property is both somewhat surprising, given that I → ∞ as |Σ| → 0, and useful, leading to a tight analysis of the error of Î and confidence intervals that do not depend on Σ.\nIt would be convenient if any nonparanormal information estimators satisfied this property. Unfortunately, the main result of this section is a negative one, showing that this property is unlikely to hold without additional assumptions: Proposition 10. Consider the 2-dimensional case\nX1, ..., Xn i.i.d∼ N (0,Σ), with Σ = [ 1 σ σ 1 ] , (5)\nand let σ∗ ∈ (0, 1). Suppose an estimator Î = Î(R) of Iσ = − 12 log(1 − σ\n2) is a function of the empirical rank matrix R ∈ Nn×2 of X . Then, there exists a constant C > 0, depending only n, such that the worst-case MSE of Î over σ ∈ (0, σ∗) satisfies\nsup σ∈(0,σ∗)\nE [( Î(R)− Iσ )2] ≥ 1\n64\n( C − log(1− σ2∗) )2\nClearly, this lower bound tends to∞ as σ → 1. As written, this result lower bounds the error of rank-based estimators in the Gaussian case when σ ≈ 1. However, to the best of our knowledge, all methods for estimating Σ in the nonparanormal case are functions of R, and prior work (Hoff, 2007) has shown that the rank matrix R is a generalized sufficient statistic for Σ (and hence for I) in the nonparanormal model. Thus, it is reasonable to think of lower bounds for rank-based estimators in the Gaussian case as lower bounds for any estimator in the nonparanormal case.\nThe proof of this result is based on the simple observation that the rank matrix can take only finitely many values. Hence, as σ → 1, R tends to be perfectly correlated, providing little information about σ, whereas the dependence of the estimand Iσ on σ increases sharply. This is intuition is formalized in the Appendix using Le Cam’s lemma for lower bounds in two-point parameter estimation problems."
  }, {
    "heading": "7. Empirical Results",
    "text": "We compare 5 mutual information estimators:\n• Î: Gaussian plug-in estimator with bias-correction (see Cai et al. (2015)).\n• ÎG: Nonparanormal estimator using Gaussianization. • Îρ: Nonparanormal estimator using Spearman’s ρ.\n• Îτ : Nonparanormal estimator using Kendall’s τ . • ÎkNN: Nonparametric estimator using k-nearest neigh-\nbor (kNN) statistics.\nFor Iρ and Iτ , we used a regularization constant z = 10−3. We did not regularize for IG. Although this implies P[IG = ∞] > 0, this is extremely unlikely for even moderate values of n and never occurred during our experiments, which all use n ≥ 32. We thus omit denoting dependence on z. For IkNN, except as noted in Experiment 3, k = 2, based on recent analysis (Singh & Póczos, 2016b) suggesting that small values of k are best for estimation.\nSufficient details to reproduce experiments are given in the Appendix, and MATLAB source code is available at [Omitted for anonymity]. We report MSE based on 1000 i.i.d. trials of each condition. 95% confidence intervals were consistently smaller than plot markers and hence omitted to avoid cluttering plots. Except as specified otherwise, each experiment had the following basic structure: In each trial, a correlation matrix Σ was drawn by normalizing a random covariance matrix from a Wishart distribution, and data X1, ..., Xn\ni.i.d.∼ N (0,Σ) drawn. All 5 estimators were computed fromX1, ..., Xn and squared error from true mutual information (computed from Σ) was recorded. Unless specified otherwise, n = 100 and D = 25.\nSince our nonparanormal information estimators are func-\ntions of ranks of the data, neither the true mutual information nor our non-paranormal estimators depend on the marginal transformations. Thus, except in Experiment 2, where we show the effects of transforming marginals, and Experiment 3, where we add outliers to the data, we perform all experiments on truly Gaussian data, with the understanding that this setting favors the Gaussian estimator.\nAll experimental results are displayed in Figure 1.\nExperiment 1 (Dependence on n): We first show nonparanormal estimators have “parametric” O(n−1) dependence on n, unlike ÎkNN, which converges far more slowly. For large n, MSEs of ÎG, Îρ, and Îτ are close to that of Î .\nExperiment 2 (Non-Gaussian Marginals): Next, we show nonparanormal estimators are robust to nonGaussianity of the marginals, unlike Î . We applied a nonlinear transformation f to a fraction α ∈ [0, 1] of dimensions of Gaussian data. That is, we drew Z1, ..., Zn\ni.i.d.∼ N (0,Σ) and then used data X1, ..., Xn, where\nXi,j = { T (Zi,j) if j < αD Zi,j if j ≥ αD , ∀i ∈ [n], j ∈ [D],\nfor a diffeomorphism T . Here, we use T (z) = ez . The Appendix shows similar results for several other T . Î performs poorly even when α is quite small. Poor performance of ÎkNN may be due to discontinuity of the density at x = 0.\nExperiment 3 (Outliers): We now show that nonparanormal estimators are far more robust to the presence of outliers than Î or ÎkNN. To do this, we added outliers to the data according to the method of Liu et al. (2012). After drawing Gaussian data, we independently select bβnc samples in each dimension, and replace each i.i.d. uniformly at random from {−5,+5}. Performance of Î degrades rapidly even for small β. ÎkNN can fail for atomic distributions, ÎkNN = ∞ whenever at least k samples are identical. This mitigate this, we increased k to 20 and ignored trials where ÎkNN = ∞, but ÎkNN ceased to give any finite estimates when β was sufficiently large.\nFor small values of β, nonparanormal estimators surprisingly improve. We hypothesize this is due to convexity of the mutual information functional Eq. (3) in Σ. By Jensen’s inequality, estimators which plug-in an approximately unbiased estimate Σ̂ of Σ are biased towards overestimating I . Adding random (uncorrelated) noise reduces estimated dependence, moving the estimate closer to the true value. If this nonlinearity is indeed a major source of bias, it may be possible to derive a von Mises-type bias correction (see Kandasamy et al. (2015)) accounting for higher-order terms in the Taylor expansion of the log-determinant.\nExperiment 4 (Dependence on Σ): Here, we verify our results in Section 6 showing that MSE of rank-based estima-\ntors approaches∞ as |Σ| → 0, while MSE of Î is independent of Σ. Here, we set D = 2 and Σ as in Eq. (5), varying σ ∈ [0, 1]. Indeed, the MSE of Î does not change, while the MSEs of ÎG, Îρ, and Îτ all increase as σ → 1. This increase seems mild in practice, with performance worse than of Î only when σ > 0.99. Îτ appears to perform far better than ÎG and Îρ in this regime. Performance of IkNN degrades far more quickly as σ → 1. This phenomenon is explored by Gao et al. (2015), who lower bound error of IkNN in the presence of strong dependencies, and proposed a correction to improve performance in this case.\nIt is also interesting that errors of Îρ and Îτ drop as σ → 0. This is likely because, for small σ, the main source of error is the variance of ρ̂ and τ̂ (as − log(1 − σ2) ≈ σ2 when σ ≈ 0). When n → ∞ and D is fixed, both 2 sin(πρ̂/6) and sin(πτ̂/2) are asymptotically normal estimates of σ, with asymptotic variances proportional to (1−σ2)2 (Klaassen & Wellner, 1997). By the delta method, since dIdσ = σ 1−σ2 , Îρ and Îτ are asymptotically normal estimates of I , with asymptotic variances proportional to σ2 and hence vanishing as σ → 0."
  }, {
    "heading": "8. Estimating Entropy",
    "text": "Thus far, we have discussed estimation of mutual information I(X). Mutual information is convenient because it is invariant under marginal transformation, and hence I(X) = I(f(X)) depends only on Σ. While the entropy H(X) does depend on the marginal transform f , fortunately, by Eq. (1), H(X) differs from I(X) only by a sum of univariate entropies. Univariate nonparametric estimation of entropy in has been studied extensively, and there exist several estimators (e.g., based on sample spacings (Beirlant et al., 1997), kernel density estimates (Moon et al., 2016) or k-nearest neighbor methods (Singh & Póczos, 2016b)) that can estimate H(Xj) at the rate n−1 in MSE under relatively mild conditions on the marginal density pj . While the precise assumptions vary with the choice of estimator, they are mainly (a) that pj be lower bounded on its support or have particular (e.g., exponential) tail behavior, and (b) that pj be smooth, typically quantified by a Hölder or Sobolev condition. Details of these assumptions are in the Appendix.\nUnder these conditions, since there exist estimators Ĥ1, ..., ĤD and a constant C > 0 such that\nE[(Ĥj −H(Xj))2] ≤ C/n, ∀j ∈ [D]. (6)\nCombining these estimators with an estimator, say Îρ,z , of mutual information gives an estimator of entropy:\nĤρ,z := ∑D j=1 Ĥj − Îρ,z.\nIf we assume z = λ−1D (Σ) is bounded below by a positive\nconstant, combining inequality (6) with Corollary 9 gives E [( Ĥρ,z −H(X) )2] ≤ CD 2\nn ,\nwhere C differs from in (6) but is independent of n and D."
  }, {
    "heading": "9. Conclusions and Future Work",
    "text": "This paper suggests nonparanormal information estimation as a practical compromise between the intractable nonparametric case and the limited Gaussian case. We proposed three estimators for this problem and provided the first upper bounds for nonparanormal information estimation. We also gave lower bounds showing how dependence on Σ differs from the Gaussian case and demonstrated empirically that nonparanormal estimators are more robust than Gaussian estimators, even in dimensions too high for nonparametric estimators.\nCollectively, these results suggest that, by scaling to moderate or high dimensionality without relying on Gaussianity, nonparanormal information estimators may be effective tools with a number of machine learning applications. While the best choice of information estimator inevitably depends on context, as an off-the-shelf guide for practitioners, the estimators we suggest, in order of preference, are:\n• fully nonparametric if D < 6, n > max{100, 10D}. • Îρ if D2/n is small and data may have outliers. • Îτ if D2/n is small and dependencies may be strong. • ÎG otherwise. • Î only given strong belief that data are nearly Gaussian.\nThere are many natural open questions in this line of work. First, in the nonparanormal model, we focused on estimating mutual information I(X), which does not depend on marginal transforms f , and entropy, which decomposes into I(X) and 1-dimensional entropies. In both cases, additional structure imposed by the nonparanormal model allows estimation in higher dimensions than fully nonparametric models. Can nonparanormal assumptions lead to\nhigher dimensional estimators for the many other useful nonlinear functionals of densities (e.g., Lp norms/distances and more general (e.g., Rényi or Tsallis) entropies, mutual informations, and divergences) that do not decompose?\nSecond, there is a gap between our upper bound rate of ‖Σ−1‖22D2/n and the only known lower bound of 2D/n (from the Gaussian case), though we also showed that bounds for rank-based estimators depend on Σ. Is quadratic dependence on D optimal? How much do rates improve under structural assumptions on Σ? Upper bounds should be derived for other estimators, such as ÎG and Îτ . The 2D/n lower bound proof of Cai et al. (2015) for the Gaussian case, based on the Cramer-Rao inequality (Van den Bos, 2007), is unlikely to tighten in the nonparanormal case, since Fisher information is invariant to diffeomorphisms of the data. Hence, a new approach is needed if the lower bound in the nonparanormal case is to be raised.\nFinally, our work applies to estimating the log-determinant log |Σ| of the latent correlation in a nonparanormal model. Besides information estimation, the work of Cai et al. (2015) on estimating log |Σ| in the Gaussian model was motivated by the role of log |Σ| in other multivariate statistical tools, such as quadratic discriminant analysis (QDA) and MANOVA (Anderson, 1984). Can our estimators lead to more robust nonparanormal versions of these tools?"
  }, {
    "heading": "ACKNOWLEDGEMENTS",
    "text": "This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887 to B.P., and by an NSF Graduate Research Fellowship to S.S. under Grant No. DGE-1252522. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
  }],
  "year": 2017,
  "references": [{
    "title": "A hierarchical clustering based on mutual information maximization",
    "authors": ["Aghagolzadeh", "Mehdi", "Soltanian-Zadeh", "Hamid", "B Araabi", "Ali"],
    "venue": "In Image Processing,",
    "year": 2007
  }, {
    "title": "Entropy–copula in hydrology and climatology",
    "authors": ["Aghakouchak", "Amir"],
    "venue": "J. Hydrometeorology,",
    "year": 2014
  }, {
    "title": "Entropy expressions and their estimators for multivariate distributions",
    "authors": ["Ahmed", "Nabil Ali", "Gokhale", "DV"],
    "venue": "IEEE Trans. on Information Theory,",
    "year": 1989
  }, {
    "title": "Multivariate statistical analysis",
    "authors": ["Anderson", "TW"],
    "venue": "Wi1ey and Sons,",
    "year": 1984
  }, {
    "title": "Nonparametric entropy estimation: An overview",
    "authors": ["Beirlant", "Jan", "Dudewicz", "Edward J", "Györfi", "László", "Van der Meulen", "Edward C"],
    "venue": "International Journal of Mathematical and Statistical Sciences,",
    "year": 1997
  }, {
    "title": "Estimating the entropy of a signal with applications",
    "authors": ["Bercher", "J-F", "Vignat", "Christophe"],
    "venue": "IEEE Trans. on Signal Processing,",
    "year": 2000
  }, {
    "title": "An entropy-based network anomaly detection",
    "authors": ["Bereziński", "Przemysław", "Jasiul", "Bartosz", "Szpyrka", "Marcin"],
    "venue": "method. Entropy,",
    "year": 2015
  }, {
    "title": "Characterizing neural dependencies with copula models",
    "authors": ["Berkes", "Pietro", "Wood", "Frank", "Pillow", "Jonathan W"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2009
  }, {
    "title": "Estimation of integral functionals of a density",
    "authors": ["Birgé", "Lucien", "Massart", "Pascal"],
    "venue": "Annals of Stat., pp",
    "year": 1995
  }, {
    "title": "Adaptive covariance matrix estimation through block thresholding",
    "authors": ["Cai", "T Tony", "Yuan", "Ming"],
    "venue": "Annals of Stat.,",
    "year": 2014
  }, {
    "title": "Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions",
    "authors": ["Cai", "T Tony", "Liang", "Tengyuan", "Zhou", "Harrison H"],
    "venue": "J. of Multivariate Analysis,",
    "year": 2015
  }, {
    "title": "An informationtheoretic approach to statistical dependence: Copula information",
    "authors": ["Calsaverini", "Rafael S", "Vicente", "Renato"],
    "venue": "EPL (Europhysics Letters),",
    "year": 2009
  }, {
    "title": "Exploring functional connectivities of the human brain using multivariate information analysis",
    "authors": ["Chai", "Barry", "Walther", "Dirk", "Beck", "Diane", "Fei-Fei", "Li"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Approximating discrete probability distributions with dependence trees",
    "authors": ["C Chow", "Liu", "Cong"],
    "venue": "IEEE transactions on Information Theory,",
    "year": 1968
  }, {
    "title": "Copulas in machine learning",
    "authors": ["Elidan", "Gal"],
    "venue": "In Copulae in mathematical and quantitative finance,",
    "year": 2013
  }, {
    "title": "Projection pursuit regression",
    "authors": ["Friedman", "Jerome H", "Stuetzle", "Werner"],
    "venue": "JASA, 76(376):817–823,",
    "year": 1981
  }, {
    "title": "Efficient estimation of mutual information for strongly dependent variables",
    "authors": ["Gao", "Shuyang", "Ver Steeg", "Greg", "Galstyan", "Aram"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "Uncertainty and structure as psychological concepts",
    "authors": ["Garner", "Wendell R"],
    "year": 1962
  }, {
    "title": "A new class of random vector entropy estimators and its applications in testing statistical hypotheses",
    "authors": ["Goria", "Mohammed Nawaz", "Leonenko", "Nikolai N", "Mergel", "Victor V", "Novi Inverardi", "Pier Luigi"],
    "venue": "Nonparametric Statistics,",
    "year": 2005
  }, {
    "title": "Projection methods in conic optimization",
    "authors": ["Henrion", "Didier", "Malick", "Jérôme"],
    "venue": "In Handbook on Semidefinite, Conic and Polynomial Optimization,",
    "year": 2012
  }, {
    "title": "Gaussian process conditional copulas with applications to financial time series",
    "authors": ["Hernández-Lobato", "José Miguel", "Lloyd", "James R", "HernándezLobato", "Daniel"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Alpha-divergence for classification, indexing and retrieval (revised)",
    "authors": ["Hero", "Alfred O", "Ma", "Bing", "Michel", "Olivier", "Gorman", "John"],
    "year": 2001
  }, {
    "title": "Applications of entropic spanning graphs",
    "authors": ["Hero", "Alfred O", "Ma", "Bing", "Michel", "Olivier JJ", "Gorman", "John"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2002
  }, {
    "title": "Extending the rank likelihood for semiparametric copula estimation",
    "authors": ["Hoff", "Peter D"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2007
  }, {
    "title": "Mutual information for fitting deep nonlinear models",
    "authors": ["Hunter", "Jacob S", "Hodas", "Nathan O"],
    "venue": "arXiv preprint arXiv:1612.05708,",
    "year": 2016
  }, {
    "title": "A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula",
    "authors": ["Ince", "Robin AA", "Giordano", "Bruno L", "Kayser", "Christoph", "Rousselet", "Guillaume A", "Gross", "Joachim", "Schyns", "Philippe G"],
    "venue": "Human brain mapping,",
    "year": 2017
  }, {
    "title": "Nonparametric von mises estimators for entropies, divergences and mutual informations",
    "authors": ["Kandasamy", "Kirthevasan", "Krishnamurthy", "Akshay", "Poczos", "Barnabas", "Wasserman", "Larry", "Robins", "James M"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Efficient estimation in the bivariate normal copula model: normal margins are least favourable",
    "authors": ["Klaassen", "Chris AJ", "Wellner", "Jon A"],
    "year": 1997
  }, {
    "title": "Nonparametric estimation of renyi divergence and friends",
    "authors": ["Krishnamurthy", "Akshay", "Kandasamy", "Kirthevasan", "Poczos", "Barnabas", "Wasserman", "Larry"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Ordinal measures of association",
    "authors": ["Kruskal", "William H"],
    "venue": "JASA,",
    "year": 1958
  }, {
    "title": "ICA using spacings estimates of entropy",
    "authors": ["E.G. Learned-Miller", "J.W. Fisher"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "High-dimensional semiparametric gaussian copula graphical models",
    "authors": ["Liu", "Han", "Fang", "Yuan", "Ming", "Lafferty", "John", "Wasserman", "Larry"],
    "venue": "The Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Mutual information is copula entropy",
    "authors": ["Ma", "Jian", "Sun", "Zengqi"],
    "venue": "Tsinghua Science & Tech.,",
    "year": 2011
  }, {
    "title": "Testing the gaussian copula hypothesis for financial assets dependences",
    "authors": ["Malevergne", "Yannick", "Sornette", "Didier"],
    "venue": "Quantitative Finance,",
    "year": 2003
  }, {
    "title": "Estimation of the entropy of a multivariate normal distribution",
    "authors": ["Misra", "Neeraj", "Singh", "Harshinder", "Demchuk", "Eugene"],
    "venue": "J. Multivariate Analysis,",
    "year": 2005
  }, {
    "title": "Multivariate f-divergence estimation with confidence",
    "authors": ["Moon", "Kevin", "Hero", "Alfred"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Ensemble estimation of multivariate f-divergence",
    "authors": ["Moon", "Kevin R", "Hero", "Alfred O"],
    "venue": "In IEEE International Symposium on Information Theory (ISIT),",
    "year": 2014
  }, {
    "title": "Improving convergence of divergence functional ensemble estimators",
    "authors": ["Moon", "Kevin R", "Sricharan", "Kumar", "Greenewald", "Kristjan", "Hero", "Alfred O"],
    "venue": "In IEEE International Symposium on Information Theory (ISIT),",
    "year": 2016
  }, {
    "title": "Ensemble estimation of mutual information",
    "authors": ["Moon", "Kevin R", "Sricharan", "Kumar", "Hero III", "Alfred O"],
    "venue": "arXiv preprint arXiv:1701.08083,",
    "year": 2017
  }, {
    "title": "Graph-based anomaly detection",
    "authors": ["Noble", "Caleb C", "Cook", "Diane J"],
    "venue": "In KDD,",
    "year": 2003
  }, {
    "title": "An empirical evaluation of entropy-based traffic anomaly detection",
    "authors": ["Nychis", "George", "Sekar", "Vyas", "Andersen", "David G", "Kim", "Hyong", "Zhang", "Hui"],
    "venue": "In Proceedings of the 8th ACM SIGCOMM conference on Internet measurement,",
    "year": 2008
  }, {
    "title": "Estimation of Rényi entropy and mutual information based on generalized nearest-neighbor graphs",
    "authors": ["Pál", "Dávid", "Póczos", "Barnabás", "Szepesvári", "Csaba"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
    "authors": ["Peng", "Hanchuan", "Long", "Fuhui", "Ding", "Chris"],
    "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
    "year": 2005
  }, {
    "title": "Use of a Gaussian copula for multivariate extreme value analysis: some case studies in hydrology",
    "authors": ["Renard", "Benjamin", "Lang", "Michel"],
    "venue": "Advances in Water Resources,",
    "year": 2007
  }, {
    "title": "Meta-Gaussian information bottleneck",
    "authors": ["Rey", "Mélanie", "Roth", "Volker"],
    "venue": "In Advances in Neural Information Processing Systems, pp. 1916–1924,",
    "year": 2012
  }, {
    "title": "Conditional mutual infomation based boosting for facial expression recognition",
    "authors": ["Shan", "Caifeng", "Gong", "Shaogang", "McOwan", "Peter W"],
    "venue": "In BMVC,",
    "year": 2005
  }, {
    "title": "Exponential concentration of a density functional estimator",
    "authors": ["Singh", "Shashank", "Póczos", "Barnabás"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Generalized exponential concentration inequality for Rényi divergence estimation",
    "authors": ["Singh", "Shashank", "Póczos", "Barnabás"],
    "venue": "In ICML, pp",
    "year": 2014
  }, {
    "title": "Finite-sample analysis of fixed-k nearest neighbor density functional estimators",
    "authors": ["Singh", "Shashank", "Póczos", "Barnabás"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Analysis of k-nearest neighbor distances with application to entropy estimation",
    "authors": ["Singh", "Shashank", "Póczos", "Barnabás"],
    "venue": "arXiv preprint arXiv:1603.08578,",
    "year": 2016
  }, {
    "title": "k-nearest neighbor estimation of entropies with confidence",
    "authors": ["Sricharan", "Kumar", "Raich", "Raviv", "Hero", "Alfred O"],
    "venue": "In IEEE International Symposium on Information Theory (ISIT),",
    "year": 2011
  }, {
    "title": "Ensemble estimators for multivariate entropy estimation",
    "authors": ["Sricharan", "Kumar", "Wei", "Dennis", "Hero", "Alfred O"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2013
  }, {
    "title": "Bayesian estimation of the entropy of the multivariate Gaussian",
    "authors": ["Srivastava", "Santosh", "Gupta", "Maya R"],
    "venue": "In IEEE International Symposium on Information Theory (ISIT),",
    "year": 2008
  }, {
    "title": "The multiinformation function as a tool for measuring stochastic dependence",
    "authors": ["Studenỳ", "Milan", "Vejnarová", "Jirina"],
    "venue": "In Learning in graphical models,",
    "year": 1998
  }, {
    "title": "Undercomplete blind subspace deconvolution",
    "authors": ["Szabó", "Zoltán", "Póczos", "Barnabás", "Lőrincz", "András"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2007
  }, {
    "title": "Post nonlinear independent subspace analysis",
    "authors": ["Szabó", "Zoltán", "Póczos", "Barnabás", "Szirtes", "Gábor", "Lőrincz", "András"],
    "venue": "In International Conference on Artificial Neural Networks,",
    "year": 2007
  }, {
    "title": "Parameter estimation for scientists and engineers",
    "authors": ["Van den Bos", "Adriaan"],
    "year": 2007
  }, {
    "title": "Information theoretical analysis of multivariate correlation",
    "authors": ["Watanabe", "Satosi"],
    "venue": "IBM J. of research and development,",
    "year": 1960
  }, {
    "title": "Copula processes",
    "authors": ["Wilson", "Andrew", "Ghahramani", "Zoubin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Minimum-entropy estimation in semi-parametric models",
    "authors": ["E. Wolsztynski", "E. Thierry", "L. Pronzato"],
    "venue": "Signal Process.,",
    "year": 2005
  }],
  "id": "SP:3a80f4c553bd6095bc55c0f5125b7b635246d5ae",
  "authors": [{
    "name": "Shashank Singh",
    "affiliations": []
  }, {
    "name": "Barnabás Póczos",
    "affiliations": []
  }],
  "abstractText": "We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.",
  "title": "Nonparanormal Information Estimation"
}