{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 38–44 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2007"
  }, {
    "heading": "1 Introduction",
    "text": "In this work, we address multilingual semantic parsing – the task of mapping natural language sentences coming from multiple different languages into their corresponding formal semantic representations. We consider two multilingual scenarios: 1) the single-source setting, where the input consists of a single sentence in a single language, and 2) the multi-source setting, where the input consists of parallel sentences in multiple languages. Previous work handled the former by means of monolingual models (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), while the latter has only been explored by Jie and Lu (2014) who ensembled many monolingual models together. Unfortunately, training a model for each language separately ignores the shared information among the source languages, which may be potentially beneficial for typologically related languages. Practically, it is also inconvenient to train, tune, and configure a new model for each language, which can be a laborious process.\nIn this work, we propose a parsing architecture that accepts as input sentences in several\nlanguages. We extend an existing sequence-totree model (Dong and Lapata, 2016) to a multitask learning framework, motivated by its success in other fields, e.g., neural machine translation (MT) (Dong et al., 2015; Firat et al., 2016). Our model consists of multiple encoders, one for each language, and one decoder that is shared across source languages for generating semantic representations. In this way, the proposed model potentially benefits from having a generic decoder that works well across languages. Intuitively, the model encourages each source language encoder to find a common structured representation for the decoder. We further modify the attention mechanism (Bahdanau et al., 2015) to integrate multisource information, such that it can learn where to focus during parsing; i.e., which input positions in which languages.\nOur contributions are as follows:\n• We investigate semantic parsing in two multilingual scenarios that are relatively unexplored in past research,\n• We present novel extensions to the sequenceto-tree architecture that integrates multilingual information for semantic parsing, and\n• We release a new ATIS semantic dataset annotated in two new languages."
  }, {
    "heading": "2 Related Work",
    "text": "In this section, we summarize semantic parsing approaches from previous works. Wong and Mooney (2006) created WASP, a semantic parser based on statistical machine translation. Lu et al. (2008) proposed generative hybrid tree structures, which were augmented with a discriminative reranker. CCG-based semantic parsing systems have been developed, such as ZC07 (Zettlemoyer and Collins, 2007) and UBL (Kwiatkowski et al.,\n38\n2010). Researchers have proposed sequence-tosequence parsing models (Jia and Liang, 2016; Dong and Lapata, 2016; Kočiskỳ et al., 2016). Recently, Susanto and Lu (2017) extended the hybrid tree with neural features.\nRecent progress in multilingual NLP has moved towards building a unified model that can work across different languages, such as in multilingual dependency parsing (Ammar et al., 2016), multilingual MT (Firat et al., 2016), and multilingual word embedding (Guo et al., 2016). Nonetheless, multilingual approaches for semantic parsing are relatively unexplored, which motivates this work. Jones et al. (2012) evaluated an individuallytrained tree transducer on a multilingual semantic dataset. Jie and Lu (2014) ensembled monolingual hybrid tree models on the same dataset."
  }, {
    "heading": "3 Model",
    "text": "In this section, we describe our approach to multilingual semantic parsing, which extends the sequence-to-tree model by Dong and Lapata (2016). Unlike the mainstream approach that trains one monolingual parser per source language, our approach integrates N encoders, one for each language, into a single model. This model encodes a sentence from the n-th language X = x1, x2, ..., x|X| as a vector and then uses a shared decoder to decode the encoded vector into its corresponding logical form Y = y1, y2, ..., y|Y |. We consider two types of input: 1) a single sentence in one of N languages in the single-source setting and 2) parallel sentences in N languages in the multi-source setting. We elaborate on each setting in Section 3.1 and 3.2, respectively.\nThe encoder is implemented as a unidirectional RNN with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997), which takes a sequence of natural language tokens as input. Similar to previous multi-task frameworks, e.g., in neural MT (Firat et al., 2016; Zoph and Knight, 2016), we create one encoder per source language, i.e., {Ψnenc}Nn=1. For the n-th language, it updates the hidden vector at time step t by:\nhnt = Ψ n enc(h n t−1,E n x[xt]) (1)\nwhere Ψnenc is the LSTM function and E n x ∈ R|V |×d is an embedding matrix containing row vectors of the source tokens in the n-th language. Each encoder may be configured differently, such\nas by the number of hidden units and the embedding dimension for the source symbol.\nIn the basic sequence-to-sequence model, the decoder generates each target token in a linear fashion. However, in semantic parsing, such a model ignores the hierarchical structure of logical forms. In order to alleviate this issue, Dong and Lapata (2016) proposed a decoder that generates logical forms in a top-down manner, where they define a “non-terminal” token <n> to indicate subtrees. At each depth in the tree, logical forms are generated sequentially until the end-ofsequence token is output.\nUnlike in the single language setting, here we define a single, shared decoder Ψdec as opposed to one decoder per source language. We augment the parent non-terminal’s information p when computing the decoder state zt, as follows:\nzt = Ψdec(zt−1,Ey[ỹt−1],p) (2)\nwhere Ψdec is the LSTM function and ỹt−1 is the previous target symbol.\nThe attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) computes a timedependent context vector ct (as defined later in Section 3.1 and 3.2), which is subsequently used for computing the probability distribution over the next symbol, as follows:\nz̃t = tanh(Uzt + Vct) (3) p(yt|y<t, X) ∝ exp(Wz̃t) (4)\nwhere U, V, and W are weight matrices. Finally, the model is trained to maximize the following conditional log-likelihood:\nL(θ) = ∑\n(X,Y )∈D\n|Y |∑\nt=1\nlog p(yt|y<t, X) (5)\nwhere (X,Y ) refers to a ground-truth sentencesemantics pair in the training data D.\nWe use the same formulation above for the encoders and the decoder in both multilingual settings. Each setting differs in terms of: 1) the decoder state initialization, 2) the computation of the context vector ct, and 3) the training procedure, which are described in the following sections."
  }, {
    "heading": "3.1 Single-Source Setting",
    "text": "In this setting, the input is a source sentence coming from the n-th language. Figure 1 (a) depicts a scenario where the model is parsing Indonesian input, with English and Chinese being non-active.\nThe last state of the n-th encoder is used to initialize the first state of the decoder. We may need to first project the encoder vector into a suitable dimension for the decoder, i.e., z0 = φndec(h n |X|), where φndec can be an affine transformation. Similarly, we may do so before computing the attention scores, i.e., h̃nk = φ n att(h n k). Then, we compute the context vector cnt as a weighted sum of the hidden vectors in the n-th encoder:\nαnk,t = exp(h̃nk · zt)∑|X|\nk′=1 exp(h̃ n k′ · zt)\n(6)\ncnt =\n|X|∑\nk=1\nαnk,th̃ n k (7)\nWe set ct = cnt for computing Equation 3. We propose two variants of the model under this setting. In the first version, we define separate weight matrices for each language, i.e., {Un,Vn,Wn}Nn=1. In the second version, the three weight matrices are shared across languages, essentially reducing the number of parameters by a factor of N .\nThe training data consists of the union of sentence-semantics pairs in N languages, where the source sentences are not necessarily parallel. We implement a scheduling mechanism that cycles through all languages during training, one language at a time. Specifically, model parameters are updated after one batch from one language before moving to the next one. Similar to Firat et al. (2016), this mechanism prevents excessive updates from a specific language."
  }, {
    "heading": "3.2 Multi-Source Setting",
    "text": "In this setting, the input are semantically equivalent sentences in N languages. Figure 1 (b) depicts a scenario where the model is parsing English, Indonesian, and Chinese simultaneously. It\nincludes a combiner module (denoted by the grey box), which we will explain next.\nThe decoder state at the first time step is initialized by first combining the N final states from each encoder, i.e., z0 = φinit(h1|X|, · · · ,hN|X|), where we implement φinit by max-pooling.\nWe propose two ways of computing ct that integrates source-side information from multiple encoders. First, we consider word-level combination, where we combine N encoder states at every time step, as follows:\nαnk,t = exp(h̃nk · zt)∑N\nn′=1 ∑|X| k′=1 exp(h̃ n′ k′ · zt)\n(8)\nct = N∑\nn=1\n|X|∑\nk=1\nαnk,th̃ n k (9)\nAlternatively, in sentence-level combination, we first compute the context vector for each language in the same way as Equation 6 and 7. Then, we perform a simple concatenation of N context vectors: ct = [ c1t ; · · · ; cNt ] .\nUnlike the single-source setting, the training data consists of N -way parallel sentencesemantics pairs. That is, each training instance consists of N semantically equivalent sentences and their corresponding logical form."
  }, {
    "heading": "4 Experiments and Results",
    "text": ""
  }, {
    "heading": "4.1 Datasets and Settings",
    "text": "We conduct our experiments on two multilingual benchmark datasets, which we describe below. Both datasets use a meaning representation based on lambda calculus.\nThe GeoQuery (GEO) dataset is a standard benchmark evaluation for semantic parsing. The multilingual version consists of 880 instances of natural language queries related to US geography facts in four languages (English, German, Greek, and Thai) (Jones et al., 2012). We use the standard split which consists of 600 training examples and 280 test examples.\nThe ATIS dataset contains natural language queries to a flight database. The data is split into 4,434 instances for training, 491 for development, and 448 for evaluation, same as Zettlemoyer and Collins (2007). The original version only includes English. In this work, we annotate the corpus in Indonesian and Chinese. The Chinese corpus was\nannotated (with segmentations) by hiring professional translation service. The Indonesian corpus was annotated by a native Indonesian speaker.\nWe use the same pre-processing as Dong and Lapata (2016), where entities and numbers are replaced with their type names and unique IDs.1 English words are stemmed using NLTK (Bird et al., 2009). Each query is paired with its corresponding semantic representation in lambda calculus (Zettlemoyer and Collins, 2005).\nIn all experiments, following Dong and Lapata (2016), we use a one-layer LSTM with 200- dimensional cells and embeddings. We use a minibatch size of 20 with RMSProp updates (Tieleman and Hinton, 2012) for a fixed number of epochs, with gradient clipping at 5. Parameters are uniformly initialized at [-0.08,0.08] and regularized using dropout (Srivastava et al., 2014). Input sequences are reversed. See Appendix A for detailed experimental settings.\nFor each model configuration, all experiments are repeated 3 times with different random seed values, in order to make sure that our findings are reliable. We found empirically that the random seed may affect SEQ2TREE performance. This is especially important due to the relatively small dataset. As previously done in multitask sequence-to-sequence learning (Luong et al., 2016), we report the average performance for the baseline and our model. The evaluation metric is defined in terms of exact match accuracy with the ground-truth logical forms. See Appendix B for the accuracy of individual runs."
  }, {
    "heading": "4.2 Results",
    "text": "Table 1 compares the performance of the monolingual sequence-to-tree model (Dong and Lapata, 2016), SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1. On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO. Parameter sharing is shown to be helpful, in particular for GEO. We observe that the average performance increase on ATIS mainly comes from Chinese and Indonesian. We also learn that although including English is often helpful for the other languages, it may affect its individual performance.\nTable 2 shows the average performance on\n1See Section 3.6 of (Dong and Lapata, 2016).\nmulti-source parsing by combining 3 to 4 languages for GEO and 2 to 3 languages for ATIS. For RANKING, we combine the predictions from each language by selecting the one with the highest probability. Indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on GEO) than doing so at the output level. Combining at the word level and sentence level shows comparable performance on both datasets. It can be seen that the benefit is more apparent when we include English in the system combination.\nRegarding comparison to previous monolingual works, we want to highlight that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations: semantic tree and lambda calculus. As noted in Section 5 of Lu (2014), results obtained from these two versions are not comparable. We use lambda calculus same as Dong and Lapata (2016). Under the multilingual setting, the closest work is Jie and Lu (2014). Nonetheless, they used the semantic tree version of GeoQuery. They eval-\nuated extrinsically on a database query task while we use exact match accuracy, so their work is not directly comparable to ours."
  }, {
    "heading": "5 Analysis",
    "text": "In this section, we report a qualitative analysis of our multilingual model. Table 3 shows example output from the monolingual model, SINGLE, trained on the three languages in ATIS and the multilingual model, MULTI, with sentence-level combination. This example demonstrates a scenario when the multilingual model successfully parses the three input sentences into the correct logical form, whereas the individual models are unable to do so.\nFigure 2 shows the alignments produced by MULTI (sentence) when parsing ATIS in the multisource setting. Each cell in the alignment matrix corresponds to αnk,t which is computed by Equation 6. Semantically related words are strongly aligned, such as the alignments between ground (en), darat (id), 地面 (zh) and ground transport. This shows that such correspondences can be jointly learned by our multilingual model.\nIn Table 4, we summarize the number of parameters in the baseline and our multilingual model. The number of parameters in SINGLE and RANKING is equal to the sum of the number of parameters in their monolingual components. It can be seen that the size of our multilingual model is about 50-60% smaller than that of the baseline."
  }, {
    "heading": "6 Conclusion",
    "text": "We have presented a multilingual semantic parser that extends the sequence-to-tree model to a multitask learning framework. Through experiments, we show that our multilingual model performs better on average than 1) monolingual models in the single-source setting and 2) ensemble ranking in the multi-source setting. We hope that this work will stimulate further research in multilingual semantic parsing. Our code and data is available at http://statnlp.org/research/sp/."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Christopher Bryant, Li Dong, and the anonymous reviewers for helpful comments and feedback. This work is supported by MOE Tier 1 grant SUTDT12015008, and is partially supported by project 61472191 under the National Natural Science Foundation of China."
  }, {
    "heading": "A Hyperparameters",
    "text": "Table 5 lists the number of training epochs and the dropout probability used in the LSTM cell and the hidden layers before the softmax classifiers, which were chosen based on preliminary experiments on a held-out dataset. We use a training schedule where we switch to the next language after training one mini-batch for GEO and 500 for ATIS. For\nall multilingual models, we initialize the encoders using the encoder weights learned by the monolingual models. For the multi-source setting, we also initialize the decoder using the first language in the list of the combined languages."
  }, {
    "heading": "B Additional Experimental Results",
    "text": "In Table 6 and 7, we report the accuracy of the 3 runs for each model and dataset. In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI. This is the same conclusion that we reached when averaging the results over all runs."
  }],
  "year": 2017,
  "references": [{
    "title": "Many languages, one parser",
    "authors": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."],
    "venue": "Transactions of the Association for Computational Linguistics 4:431–444.",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of ICLR.",
    "year": 2015
  }, {
    "title": "Natural language processing with Python: analyzing text with the natural language toolkit",
    "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."],
    "venue": "” O’Reilly Media, Inc.”.",
    "year": 2009
  }, {
    "title": "Multi-task learning for multiple language translation",
    "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."],
    "venue": "Proceedings of ACL. https://doi.org/10.3115/v1/P15-1166.",
    "year": 2015
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of ACL. https://doi.org/10.18653/v1/P16-1004.",
    "year": 2016
  }, {
    "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism",
    "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of NAACL. https://doi.org/10.18653/v1/N16-1101.",
    "year": 2016
  }, {
    "title": "A representation learning framework for multi-source transfer parsing",
    "authors": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."],
    "venue": "Proceedings of AAAI.",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Data recombination for neural semantic parsing",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "Proceedings of ACL. https://doi.org/10.18653/v1/P16-1002.",
    "year": 2016
  }, {
    "title": "Multilingual semantic parsing: Parsing multiple languages into semantic representations",
    "authors": ["Zhanming Jie", "Wei Lu."],
    "venue": "Proceedings of COLING. http://aclweb.org/anthology/C14-1122.",
    "year": 2014
  }, {
    "title": "Semantic parsing with bayesian tree transducers",
    "authors": ["Bevan Keeley Jones", "Mark Johnson", "Sharon Goldwater."],
    "venue": "Proceedings of ACL. http://aclweb.org/anthology/P12-1051.",
    "year": 2012
  }, {
    "title": "Semantic parsing with semi-supervised sequential autoencoders",
    "authors": ["Tomáš Kočiskỳ", "Gábor Melis", "Edward Grefenstette", "Chris Dyer", "Wang Ling", "Phil Blunsom", "Karl Moritz Hermann."],
    "venue": "Proceedings of EMNLP.",
    "year": 2016
  }, {
    "title": "Inducing probabilistic ccg grammars from logical form with higherorder unification",
    "authors": ["Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of EMNLP. http://aclweb.org/anthology/D10-1119.",
    "year": 2010
  }, {
    "title": "Semantic parsing with relaxed hybrid trees",
    "authors": ["Wei Lu."],
    "venue": "Proceedings of EMNLP. http://aclweb.org/anthology/D14-1137.",
    "year": 2014
  }, {
    "title": "A generative model for parsing natural language to meaning representations",
    "authors": ["Wei Lu", "Hwee Tou Ng", "Wee Sun Lee", "Luke S Zettlemoyer."],
    "venue": "Proceedings of EMNLP. http://aclweb.org/anthology/D08-1082.",
    "year": 2008
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "Proceedings of ICLR.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "Proceedings of EMNLP. https://doi.org/10.18653/v1/D15-1166.",
    "year": 2015
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Semantic parsing with neural hybrid trees",
    "authors": ["Raymond Hendy Susanto", "Wei Lu."],
    "venue": "Proceedings of AAAI.",
    "year": 2017
  }, {
    "title": "Learning for semantic parsing with statistical machine translation",
    "authors": ["Yuk Wah Wong", "Raymond J Mooney."],
    "venue": "Proceedings of NAACL. http://aclweb.org/anthology/N06-1056.",
    "year": 2006
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of UAI.",
    "year": 2005
  }, {
    "title": "Online learning of relaxed ccg grammars for parsing to logical form",
    "authors": ["Luke S Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of EMNLP-CoNLL. http://aclweb.org/anthology/D07-1071.",
    "year": 2007
  }, {
    "title": "Multi-source neural translation",
    "authors": ["Barret Zoph", "Kevin Knight."],
    "venue": "Proceedings of NAACL. https://doi.org/10.18653/v1/N16-1004.",
    "year": 2016
  }],
  "id": "SP:fe433d6c26e4a5abb9eff26cacee0da79e6bfe2d",
  "authors": [{
    "name": "Raymond Hendy Susanto",
    "affiliations": []
  }, {
    "name": "Wei Lu",
    "affiliations": []
  }],
  "abstractText": "In this paper, we address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations. We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.",
  "title": "Neural Architectures for Multilingual Semantic Parsing"
}