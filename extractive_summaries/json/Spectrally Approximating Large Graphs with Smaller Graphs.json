{
  "sections": [{
    "heading": "1. Introduction",
    "text": "One of the most wide-spread techniques for sketching graph-structured data is coarsening. As with most sketching methods, instead of solving a large graph problem in its native domain, coarsening involves solving an akin problem of reduced size at a lower cost; the solution can then be inexpensively lifted and refined in the native domain.\nThe benefits of coarsening are well known both in the algorithmic and machine learning communities. There exists a long list of algorithms that utilize it for partitioning (Hendrickson & Leland, 1995; Karypis & Kumar, 1998a; Kushnir et al., 2006; Dhillon et al., 2007; Wang et al., 2014) and visualizing (Koren, 2002; Walshaw, 2006) large graphs in a computationally efficient manner. In addition, it has been frequently used to create multi-scale representations of graph-structured data, such as coarse-grained diffusion maps (Lafon & Lee, 2006), multi-scale wavelets (Gavish et al., 2010) and pyramids (Shuman et al., 2016).\nMore recently, coarsening is employed as a component of graph convolutional networks analogous to pooling (Bruna\n1École Polytechnique Fédérale Lausanne, Switzerland. Correspondence to: Andreas Loukas <andreas.loukas@epfl.ch>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\net al., 2014; Defferrard et al., 2016; Bronstein et al., 2017; Simonovsky & Komodakis, 2017). Combining the values of adjacent vertices reduces the spatial size of each layer’s output, prevents overfitting, and encourages a hierarchical scaling of representations.\nYet, much remains to be understood about the properties and limitations of graph coarsening.\nThe majority of theoretical work has so far focused on constructing fast linear solvers using multigrid techniques. These methods are especially relevant for approximating the solution of differential equations on grids and finiteelement meshes. Multigrids were also adapted to arbitrary graphs by Koutis et al. (2011) and later on by Livne and Brandt (2012). Based on an optimized version of the Galerkin coarsening, the authors demonstrate an algebraic multi-level approximation scheme that is numerically shown to solve symmetric diagonally dominant linear systems in almost linear time. Similar techniques have also been applied for approximating the Fiedler vector (Urschel et al., 2014; Gandhi, 2016) and solving least-squares problems (Hirani et al., 2015; Colley et al., 2017).\nDespite this progress, with the exception of certain interlacing results (Chung, 1997; Chen et al., 2004), it is currently an open question how coarsening affects the spectrum of a general graph. As a consequence, there is no rigorous way of determining to what extend one may coarsen a graph without significantly affecting the performance of spectral methods for graph partitioning and visualization. The absence of a fundamental understanding of what and how much information is lost also hinders our ability to design efficient learning algorithms for graph-structured data: e.g., coarsening is the least studied (and less optimized) component of graph convolutional networks.\nThis paper sheds light into some of these questions. Specifically, we consider a one-shot coarsening operation and ask how much it affects the eigenvalues and eigenvectors of the graph Laplacian. Key to our argument is the introduced restricted spectral similarity (RSS) property, asserting that the Laplacian of the coarsened and actual graphs behave similarly (up to some constants) with respect to an appropriate set of vectors. The RSS property is shown to hold for coarsenings constructed by contracting the edges contained in a randomized matching. Moreover, the attained\nconstants depend on the degree distribution and can be controlled by the ratio of the coarsened and actual graph sizes, i.e., the extend of dimensionality reduction.\nWe utilize the RSS property to provide spectrum approximation guarantees. It is proven that the principal eigenvalues and eigenspaces of the coarsened and actual Laplacian matrices are close when the RSS constants are not too large. Our results carry implications for non-linear methods for data clustering (Von Luxburg, 2007) and dimensionality reduction (Belkin & Niyogi, 2003). A case in point is spectral clustering: we show that lifted eigenvectors can be used to produce clustering assignments of good quality even without refinement. This phenomenon has been observed experimentally (Karypis & Kumar, 1998a; Dhillon et al., 2007), but up to now lacked formal justification.\nPaper organization. After introducing the RSS property in Section 2, we demonstrate in Section 3 how to generate coarsenings featuring small RSS constants. Sections 4 and 5 then link our results to spectrum preservation and spectral clustering, respectively. The paper concludes by briefly discussing the limitations of our analysis. The proofs can be found in a supplementary document."
  }, {
    "heading": "2. Graph coarsening",
    "text": "Consider a weighted graph G = (V, E ,W ) of N = |V| vertices and M = |E| edges, with the edge eij between vertices vi and vj weighed bywij ≤ 1. As usual, we denote by L the combinatorial Laplacian of G defined as\nL(i, j) =  di if i = j −wij if eij ∈ E 0 otherwise\n(1)\nand di the weighted degree of vi. Moreover, let λk be the k-th eigenvalue of L and uk the associated eigenvector."
  }, {
    "heading": "2.1. How to coarsen a graph?",
    "text": "At the heart of a coarsening lies a surjective (and therefore dimension reducing) mapping ϕ : V → Vc between the original vertex set V = {v1, . . . , vN} and the smaller vertex set Vc = {v′1, . . . , v′n}. In other words, the coarse graph Gc = (Vc, Ec) has m = |Ec| and contains every edge (i, j) ∈ E for which ϕ(vi) 6= ϕ(vj). We define the coarsened Laplacian as\nLc = CLC >, (2)\nwhere the fat n × N coarsening matrix C describes how different v ∈ V are mapped onto the vertex set Vc. Similarly, we may downsample a vector x ∈ RN supported on V by the linear transformation\nxc = Cx, (3)\nwhere now xc ∈ Rn. We here focus on coarsenings where each vertex vi is mapped into a single v′j . This is equivalent to only considering coarsening matrices with block-diagonal form C = blkdiag ( c>1 , . . . , c > n ) , where each c>j = [cj(1), . . . , cj(nj)] is the length nj coarsening weight vector associated with the j-th vertex v′j of Vc. In addition, we restrict our attention to constant coarsening weight vectors of unit norm ‖cj‖2 = 1 with entries equal to n−\n1/2 j .\nThough Lc is not a proper combinatorial Laplacian matrix (e.g., Lc1 6= 0 for 1 being the all ones vector), it can take the proper form using the simple re-normalization QLcQ, where Q = diag(C1). This might seem inconvenient at a first glance. We argue that it is not: it should not be the action of Lc in itself that matters, but its effect when combined with downsampling. When acting on xc, the desired nullspace property is regained since LcC1 = 0. Alternatively, one could define Lc′ = QCLC>Q and xc′ = Q\n−1Cx, where now Lc′ has the proper combinatorial Laplacian form. This construction however is equivalent to the one we consider here since x>c Lcxc = x>C>Q−1QCLC>QQ−1Cx = x>c′Lc′xc′ .\nWe will also utilize the notion of a coarsening frame:\nDefinition 1 (Coarsening frame). The coarsening frame GF = (VF , EF ,WF ) is the subgraph of G induced by set VF = {vi | ∃vj with ϕ(vi) = ϕ(vj)}.\nInformally, GF is the subgraph of G that is coarsened (see Figure 1c). We say that the coarsening corresponds to an edge contraction if no two edges of the coarsening frame are themselves adjacent—in other words, EF forms a matching on G.\nLifting. We write x̃ = C>xc to do an approximate inverse mapping from Vc to V , effectively lifting the dimension from Rn back to RN . To motivate this choice notice that, even though Π = C>C is not an identity matrix, it is block diagonal Π = blkdiag ( c1c > 1 , . . . , cnc > n ) . Moreover, Π is an identity mapping for all vectors in its range.\nProperty 1. Π = C>C is a rank n projection matrix.\nProof. For each block Πj in the diagonal of Π, we have Π2j = ΠjΠj = cjc > j cjc > j = cjc > j ‖cj‖2 = Πj . The rank of Π is n because each diagonal block Πj is of rank one.\nTherefore, if x is a vector in RN and xc = Cx is its coarsened counterpart, then x̃ = C>Cx = Πx is a localitypreserving approximation of x w.r.t. graph G.\nA toy example. Consider the example graph shown in Figure 1a and suppose that we want to coarsen the n1 = 3\ngray vertices VF = {v1, v2, v3} of G into vertex v′1, as shown in Figure 1b. Matrices C and Lc take the form:\nC = 1/√3 1/√3 1/√3 0 00 0 0 1 0 0 0 0 0 1  = [c>1 0 0 I2 ]\nLc = CLC > =  2/3 −1/√3 −1/√3−1/√3 1 0 −1/ √ 3 0 1  Above, the 2 × 2 identity matrix I2 preserves the neighborhood of all vertices not in VF . The coarsening frame is shown in Figure 1c."
  }, {
    "heading": "2.2. Restricted spectral similarity",
    "text": "The objective of coarsening is dual. First, we aim to attain computational acceleration by reducing the dimensionality of our problem. On the other hand, we must ensure that we do not loose too much valuable information, in the sense that the structure of the reduced and original problems should be as close as possible.\nSpectral similarity. One way to define how close a matrix B approximates the action of matrix A is to establish a spectral similarity relation of the form:\n(1− )x>Ax ≤ x>Bx ≤ (1 + )x>Ax, (4)\nfor all ∀x ∈ RN and with a positive constant. Stated in our context, (4) can be rewritten as:\n(1− )x>Lx ≤ x>c Lcxc ≤ (1 + )x>Lx (5)\nfor all x ∈ RN and with xc = Cx. If the equation holds, we say that matrix Lc is an -spectral approximation of L. In graph theory, the objective of constructing sparse spectrally similar graphs is the main idea of spectral graph sparsifiers, a popular method for accelerating the solution of linear systems involving the Laplacian, initially proposed by Spielman and co-authors (Spielman & Srivastava, 2011; Spielman & Teng, 2011).\nIn contrast to the sparsification literature however, here the dimension of the space changes and one needs to take into account both the Laplacian coarsening (L becomes Lc) and the vector downsampling operation (x becomes xc) in the\nsimilarity relation 1. Yet, from an analysis standpoint, an alternative interpretation is possible. Defining L̃ = ΠLΠ, we re-write\nx>c Lcxc = x >(C>C)L(C>C)x = x>ΠLΠx = x>L̃x.\nRemembering that C> acts as an approximate inverse of C, we interpret L̃ ∈ RN×N as an approximation of L that contains the same information as Lc ∈ Rn×n.\nRestricted spectral similarity (RSS). Equation (5) thus states that the rank n− 1 matrix L̃ is an -spectral approximation of L, a matrix of rank N − 1. Since the two matrices have different rank, the relation cannot hold for every x ∈ RN . To carry out a meaningful analysis, we focus on an appropriate subset of vectors.\nMore specifically, we restrict our attention to the first K eigenvectors of L and introduce the following property:\nDefinition 2 (Restricted spectral similarity). Suppose that there exists an integer K and positive constants k, such that for every k ≤ K,\n(1− k)λk ≤ u>k L̃uk ≤ (1 + k)λk. (6)\nThen Lc is said to satisfy the restricted spectral similarity (RSS) property with RSS constants { k}Kk=1.\nThe relation to spectral similarity is exposed by substituting u>k Luk = λk.\nFor every k, inequality (6) should intuitively capture how close is Cuk to being an eigenvector of Lc: When k = 0, vector Cuk is an eigenvector of Lc with eigenvalue λk. On the hand, for k > 0, Cuk is not an eigenvector of Lc, but matrices L and L̃ alter the length of vectors in the span of uk in a similar manner (up to 1± k). This intuition turns out to be valid. In the following we will demonstrate that the RSS property is a key ingredient in characterizing the relation between the first K eigenvalues and principal eigenspaces of the coarsened and actual Laplacian matrices. In particular, we will prove that the spectrum of Lc approximates that of L (up to lifting) when the constants k are sufficiently small. This line of thinking will be developed in Section 4.\nRemark 1. A uniform RSS constant = maxk≤K k is sufficient to guarantee spectrum preservation, however, individual constants { k}Kk=1 lead to tighter bounds."
  }, {
    "heading": "3. A randomized edge contraction algorithm",
    "text": "This section proposes an algorithm for coarsening a graph that provably produces coarsenings with bounded RSS con-\n1Coarsening could perhaps be interpreted as combining edge and vertex sparsification (Moitra, 2011).\nAlgorithm 1 Randomized Edge Contraction (REC) 1: input: G = (V, E), T, φ 2: output: Gc = (Vc, Ec) 3: C ← E , Gc ← G 4: Φ←∑eij∈E φij , t← 0. 5: while |C| > 0 and t < T do 6: t← t+ 1. 7: Select each eij from C with prob. pij = φij/Φ or\ncontinue with prob. 1−∑eij∈C pij . 8: C ← C \\ Nij 9: Gc ← contract(Gc, eij) as in (2)\n10: end while\nstants k.\nThe method, which we refer to as REC, is described in Algorithm 1. REC resembles the common greedy procedure of generating maximal matchings, in that it maintains a candidate set C containing all edges that can be added to the matching. At each iteration, a new edge eij is added and set C is updated by removing from it all edges in the edge neighborhood set Nij defined as follows:\nNij = {epq | epq ∈ E and p = i or q = j},\nwhich also includes the edge eij .\nYet, REC features two main differences. First, instead of selecting each new edge added to the matching uniformly at random, it utilizes a potential function φ defined on the edge set, i.e., φ : E → R+ with which it controls the probability pij that every edge is contracted. The second difference is that, at each iteration, REC does not select a valid edge with probability 1 −∑eij∈C pij . This choice is not driven by computational concerns, but facilitates the analysis, as it alleviates the need for updating the total potential Φ after every iteration.\nRemark 2. REC is equivalent to the O(M) complexity algorithm that samples from C directly in line 7 by updating Φ at every iteration such that its value is ∑ eij∈C φij . Though we suggest to use this latter algorithm in practice, it is easier to express our results using the number of iterations T of Algorithm 1.\nREC returns a maximal matching when T is sufficiently large. As we will see in the following, it is sufficient to consider T = O(N). The exact number of iterations will be chosen in order to balance the trade-off between the expected dimensionality reduction ratio\nr ∆ = E [ N − n N ] and the size of the RSS constants.\n3.1. Analysis of REC\nThe following theorem characterizes an individual RSS constant of an Lc generated by REC. As exemplified in Corollary 5.1, similar argumenrs can also be used to derive a uniform bound over all k for k ≤ K. Theorem 3.1. Let Lc be the coarsened Laplacian produced by REC and further suppose that\nλk ≤ 0.5 min eij∈E\n{ di + dj\n2 + wij\n} .\nFor any k ≥ 0, the relation λk ≤ u>k L̃uk ≤ λk(1 + k) holds with probability at least\n1− c2 1− e−c1T/N\n4 k max eij∈E χij  ∑ epq∈Nij wpq wij + 3− 4λk wij  where c1 = N maxeij∈E\n∑ epq∈Nij ppq ,\nc2 = c1/N 1− e−c1/N and χij = φij∑\nepq∈Nij φpq\n.\nThe theorem reveals that the dependency of k to some extremal properties implied by the potential function φ and the graph structure. It is noteworthy that\nc1 = O(1) implies lim N→∞ c2 = 1. (7)\nThese asymptotics can be taken at face value even for finite size problems: coarsening typically becomes computationally relevant for large N (typically N > 103), for which c2 has effectively converged to its limit.\nThe assumption that c1 is independent ofN can be satisfied either by assuming that G is a bounded degree graph, such that |Nij | N for every eij ∈ E , or by choosing potential functions φij that are inversely proportional to |Nij |. We can also incorporate the expected reduction ratio r in the bound, by noting that\nrN = ∑ eij∈E P (eij ∈ EF ) ≥ ∑ eij∈E pij 1− e−TPij Pij\n≥ 1− e −TPmax\nPmax = 1− e−c1T/N c1/N , (8)\n(see proof of Theorem 3.1 for definitions of Pij and Pmax) implying\n1− e−c1T/N ≤ rc1, (9)\nas well as that T = Nc1 log ( 1 1−rc1 ) iterations suffice to achieve any r < 1/c1. Nevertheless, this latter estimate is more pessimistic than the one presented in Theorem 3.1.\nThe norm ‖Π⊥uk‖22. For all k, one has\nP ( ‖Π⊥uk‖22 ≥ λk ) ≤ c2 1− e−c1T/N 2 max eij∈E χij wij ,\nwith constants defined as before (the derivation is not included as it resembles the one employed in the proof of Theorem 3.1). Thus, ‖Π⊥uk‖22 depends on the ratio r (through (9)) and is smaller for small k (due to λk). This is reasonable: by definition, eigenvectors corresponding to small eigenvalues are smooth functions on G; averaging some of their entries locally on the graph is unlikely to alter their values significantly."
  }, {
    "heading": "3.2. The heavy-edge potential function",
    "text": "Let us examine how the achieved results behave for a specific potential function. Setting φij = wij is a simple way to give preference to heavy frames—indeed, heavy-edge matchings have been utilized as a heuristic for coarsening (e.g., in combination with graph partitioning (Karypis & Kumar, 1998b)). It is perhaps interesting to note this particular potential function can be derived naturally from Theorem 3.1 if we require that\nχij\n∑ epq∈Nij wpq\nwij = 1 for all eij . (10)\nIt will be useful to denote respectively by %min and %max the minimum and maximum of (di + dj − wij)/2davg over all eij , with davg being the average degree. It is straightforward to calculate that in this case\nc1 = 2\n( di + dj − wij\ndavg\n) = 4%max.\nTherefore, c1 = O(1) for all graphs in which Ω(1) = %min ≤ %max = O(1), and given sufficiently large N and some manipulation the probability estimate of Theorem 3.1 reduces to\n1− 1− e −4%maxT/N\n4 k\n( 1 +\n1.5− 2λk davg %min\n) . (11)\nIn addition, P ( ‖Π⊥uk‖22 > λk ) ≤ 1−e−4%maxT/N2 %min davg .\nThe heavy-edge potential function is therefore more efficient for graphs with small degree variations. Such graphs are especially common in machine learning, where often the connecticity of each vertex is explicitly constructed such that all degrees are close to some target value (e.g., using a k-nearest neighbor graph construction (Muja & Lowe, 2014)).\nAs a proof of concept, Figure 2 compares the actual constants k with the bound of Theorem 3.1 when utilizing REC with a heavy-edge potential to coarsen the following benchmark graphs: (i) a point cloud representing a bunny obtained by re-sampling the Stanford bunny 3Dmesh (Turk & Levoy, 1994) and applying a k-nn construction (N = 1000, r = 0.4, k = 30), (ii) a k-nn similarity graph capturing the geometry of a 2D manifold usually referred to as Swiss roll (N = 1000, r = 0.4, k = 10), (iii) A network describing the interaction of yeast proteins (Rossi & Ahmed, 2015) (N = 1458, r = 0.25, davg = 2, dmax = 56), and (iv) a d-regular graph (N = 400, r = 0.4, d = 20). To derive the bounds, we started from Theorem 3.1 and identified for each k the smallest k such that the success probability is at least ps = {0.5, 0.7}. As predicted by our analysis, k decrease with k (the decrease is close to linear in λk) and with the variance of the degree distribution. The heavy-tailed yeast network and the regular graph constitute two extreme examples, with the latter featuring much smaller constants."
  }, {
    "heading": "3.3. Regular graphs",
    "text": "For regular graphs, (9) becomes asymptotically tight, leading to the following Corollary:\nCorollary 3.1. If G is a regular graph with combinatorial degree d and equal edge weights wij = w, then for any k such that λk ≤ (d + 1)/2 and for sufficiently large N , the relation λk ≤ u>k L̃uk ≤ λk(1 + k) holds with probability at least\n≥ 1− r 1− (2d) −1\nk\n( 1 +\n1.5− λk d− 0.5\n) d 1≈ 1− r\nk , (12)\ninequality ∥∥Π>uk∥∥22 ≥ rλ2 holds for all k with proba-\nbility at most 2/(d ) and T = N2(2−1/d) log ( 1 1−2(2−1/d)r ) iterations of REC suffice in expectation to achieve reduction r.\nAn other way to read Corollary 3.1 is that, for a sufficiently dense regular graph, there exists2 an edge contraction for which Lc satisfies the RSS property with constants bounded by r."
  }, {
    "heading": "4. The spectrum of the coarsened Laplacian",
    "text": "This section links the RSS property with spectrum preservation. Our results demonstrate that the distance between the spectrum of a coarsened Laplacian and of the combinatorial Laplacian it approximates is directly a function of RSS constants. This relation also extends to eigenspaces."
  }, {
    "heading": "4.1. Basic facts about the spectrum",
    "text": "Before delving into our main results, let us first consider the spectrum of a coarsened Laplacian which does not (necessarily) meet the RSS property.\nW.l.o.g., let G be connected and sort its eigenvalues as 0 = λ1 < λ2 ≤ . . . ≤ λN . Similarly, let λ̃k be the k-th largest eigenvalue of the coarsened Laplacian Lc and name ũk the associated eigenvector. As the following theorem shows, there is a direct relation between the eigenvalues λ̃ and λ.\nTheorem 4.1. Inequality λk ≤ λ̃k holds for all k ≤ n.\nWe remark the similarity of the above to a known result in spectral graph theory (Chung, 1997) (Lemma 1.15) assering that, if νk is the k-th eigenvalue of the normalized Laplacian ofG and ν̃k is the k-th eigenvalue of the normalized Laplacian of a graph Gc obtained by edge contraction, then νk ≤ ν̃k for all k = 1, 2, . . . , n. Despite this similarity however, Theorem 4.1 deals with the eigenvalues of the combinatorial Laplacian matrix and its coarsened counterpart Lc = CLC>.\nWe also notice that, when the weight vectors c are chosen to be constant over each connected component of GF (as we assume in this work) the nullspace of Lc spans the downsampled constant vector implying that\nC>ũ1 = u1 and 0 = λ̃1 < λ̃2. (13)\nThe above relations constitute the main reason why we utilize constant coarsening weights in our construction."
  }, {
    "heading": "4.2. From the RSS property to spectrum preservation",
    "text": "For eigenvalues, the RSS property implies an upper bound:\n2The existence is implied by the probabilistic method.\nTheorem 4.2. If Lc satisfies the RSS property, then\nλk ≤ λ̃k ≤ max { λ̃k−1,\n(1 + k)∑ i≥k(ũ > i Cuk) 2 λk\n} (14)\nfor all k ≤ K, where k is the k-th RSS constant.\nThe term ∑ i≥k(ũ > i Cuk)\n2 depends on the orientation of the eigenvectors of Lc with respect to those of L. We expect:\nλk ≤ λ̃k ≤ (1 + k)∑\ni≥k(ũ > i Cuk)\n2 λk ≈\n(1 + k) ‖Πuk‖22 λk.\nIndeed, for λ2 the above becomes an equality as∑ i≥2 (ũ>i Cu2) 2 = ‖Πu2‖ − (ũ>1 Cu2)2 = ‖Πu2‖,\nwhere the last equality follows from (13). In this case, the above results combined with the analysis presented in Section 3.1 imply the following corollary:\nCorollary 4.1. Consider a bounded degree graph with λ2 ≤ 0.5 mineij∈E\n{ di+dj\n2 + wij\n} and suppose that it is\ncoarsened by REC using a heavy-edge potential. For any feasible expected dimensionality reduction ratio r, sufficiently large N and any > 0\nλ̃2 ≤ 1 + r\n1− λ2 r λ2,\nwith probability at least 1 − c34 ( 1 + 1.5wmax+2(1−λ2)davg %min ) where c3 = r (1 − e−4%maxT/N ). For a d-regular graph this probability is at least 1− 1 (1 + 3−λ2d ).\nThe statement can be proved by taking a union bound with respect to the events {‖Π⊥u2‖22 > λ2 r } and {u>2 L̃u2 > (1 + r )λ2}, whose probabilities can be easily obtained from the results of Section 3.\nEigenspaces. We also analyze the angle between principal eigenspaces of L and Lc. We follow Li (1994) and split the (lifted) eigendecompositions of L and Lc as\nL = UΛU> = (Uk, Uk⊥)\n( Λk\nΛk⊥ )( U>k U>k⊥ ) C>LcC = (C >Ũ)Λ̃(Ũ>C)\n= (C>Ũk, C >Ũk⊥)\n( Λ̃k\nΛ̃k⊥\n)( Ũ>k C\nŨ>k⊥C\n) ,\nwhere Λk = diag(λ1, . . . , λk) and U1 = (u1, . . . , uk) (analogously for Λ̃k and Ũk). The canonical angles (Davis & Kahan, 1970; Stewart, 1990) between the eigenspaces\nspanned by Uk and C>Ũk are the singlular values of the matrix\nΘ(Uk, C >Ũk) ∆ = arccos(U>k C >ŨkŨ > k CUk) −1/2 (15)\nand moreover, the smaller the sinus of the canonical angles are, the closer the two subspaces lie.\nThe following theorem characterizes ϑk = ‖ sin Θ ( Uk, C >Ũk ) ‖2F , a measure of the miss-alignment of the eigenspaces spanned by Uk and C>Ũk.\nTheorem 4.3. If Lc satisfies the RSS property, then\nϑk ≤ min  ∑ 2≤i≤k iλi + λk‖Π⊥ui‖22 λ̃k+1 − λk ,\n∑ 2≤i≤k (1 + i)λi − λ2‖Πui‖22 λ̃k+1 − λ2  , for every k ≤ K.\nBoth bounds have something to offer: The first is applicable to situations where there is a significant eigenvalue separation between the subspace of interest and neighboring spaces (this condition also appears in classic perturbation analysis (Davis & Kahan, 1970)) and has the benefit of vanishing when n = N . The second bound on the other hand does not depend on the minimum eigengap between λk and λk+1, but on the gap between every eigenvalue λi in the subspace of interest and λk+1, which can be significantly smaller.\nWe obtain an end-to-end analysis of coarsening by combining Theorem 4.3 with Theorem 3.1 and taking a union bound over all k ≤ K. However, the reader is urged to consider the proof of Corollary 5.1 for a more careful analysis with significantly improved probability estimates.\nThe importance of the eigenvalue distribution can be seen in Figure 3, where we examine the alignment of Uk and C>Ũk for different k when r = 0.4. The figure summarizes the results for 10 stochastic block model graphs, each consisting of N = 1000 vertices. These graphs were built\nby uniformly assigning vertices into K = 10 communities and connecting any two vertices with probability p or q depending on whether they belong in the same or different communities, respectively. Such constructions are well known to produce eigenvalue distributions that feature a large gap between theK andK+1 eigenvalues and small gaps everywhere else.\nBelow K the eigenspaces are poorly aligned and not much better than chance (dotted line). As soon as the size of the subspace becomes equal to K however we observe a significant drop, signifying good alignment. This matches the prediction offered by our bounds (dashed line). The phenomenon is replicated for two parametrizations of the stochastic block model, one featuring a low q (and thus a large gap) and one with larger q. Due to the smaller gap, in the latter case the trend is slightly less exaggerated."
  }, {
    "heading": "5. Implications for spectral clustering",
    "text": "Spectral clustering is a non-linear method for partitioning N points z1, z2, . . . , zN ∈ RD into K sets S = {S1, S2, . . . , SK}. There exist many versions of the algorithm. We consider the “unnormalized spectral clustering” (Von Luxburg, 2007):\n1. Construct a similarity graph with wij = e−‖zi−zj‖ 2 2/σ 2\nbetween vertices vi and vj . Let L be the combinatorial Laplacian of the graph and write Ψ = UK ∈ RN×K to denote the matrix of its first K eigenvectors. 2. Among all cluster assignments S, search for the assignment S∗ that minimizes the k-means cost:\nFK(Ψ, S) = K∑ k=1 ∑ vi,vj∈Sk ‖Ψ(i, :)−Ψ(j, :)‖22 2 |Sk|\nThough a naive implementation of the above algorithm scales with O(N3), the acceleration of spectral clustering has been an active topic of research. A wide-range of sketching techniques have been proposed(Boutsidis et al., 2015a; Tremblay et al., 2016), arguably one of the fastest known algorithms utilizes coarsening. Roughly, the algorithm involves: (i) hierarchically coarsening the input graph (using edge contractions) until the latter reaches a target size; (ii) solving the clustering problem in the small dimension; (iii) lifting the solution back to the original domain; and (iv) performing some fast refinement to attain the final clustering.\nIn the following, we provide theoretical guarantees on the solution quality of the aforementioned scheme for a single coarsening level. To the extend of our knowledge, this is the first time that such an analysis has been carried out.\nTo perform the analysis, we suppose that\nS∗ = arg min S∈S FK(Ψ, S) and S̃∗ = arg min S∈S FK(Ψ̃, S)\nare the (optimal) clustering assignments obtained by solving the k-means using as input the original eigenvectors UK and the lifted eigenvectors Ψ̃ = C>ŨK of Lc, respectively. We then measure the quality of S̃∗ by examining how far the correct minimizer FK(Ψ, S∗) is to FK(Ψ, S̃∗). Note that the latter quantity utilizes the correct eigenvectors as points and necessarily FK(Ψ, S∗) ≤ FK(Ψ, S̃∗). Boutsidis et al. (2015a) noted that, if the two quantities are close then, despite the assignments themselves possibly being different, they both feature the same quality with respect to the k-means objective.\nWe prove the following approximation result:\nCorollary 5.1. Consider a bounded degree graph with λK ≤ 0.5 mineij∈E\n{ di+dj\n2 + wij\n} and suppose that it is\ncoarsened by REC using a heavy-edge potential. For sufficiently large N , any feasible ratio r, and > 0,\n[ FK(Ψ, S̃∗)1/2 −FK(Ψ, S∗)1/2 ]2 ≤ K∑ k=2 8 rλk δK\nwith probability at least 1− %max (\n1 + 6+4λK−8 c3davg%min\n) , where\nδK = λK+1 − λK and c3 = ∑K k=2 λ 2 k/ ∑K k=2 λk.\nThe corollary therefore provides conditions such that the clustering assignment produced with the aid of coarsening has quality that is close to that of the original in terms of absolute error, even without refinement. Practically, our result states that S̃∗ is a good candidate for the final solution as long as the graph has almost constant degree (such that %min ≈ 1 ≈ %max) and it is K clusterable (i.e., the gap δK = λK+1 − λK is large). We are unaware of any technique that provides meaningful lower bounds on FK(Ψ, S∗) and therefore cannot transform the bound to a relative error statement. However, from the algebraic formulation of the k-means cost it follows that, when the number of clusters is κ < K whereas the\nfeature matrix remains Ψ, then Fκ(Ψ, S∗) ≥ K−κ (this is because Ψ has exactly K unit singular values, whereas the k-means clustering cannot do better than a rank κ approximation of Ψ (Ding & He, 2004; Boutsidis et al., 2015b)). Under the conditions of Corollary 5.1 and with the same probability:FK(Ψ, S̃∗) 12 −FK(Ψ, S∗) 12\nFκ(Ψ, S∗) 1/2\n2≤ K∑ k=2 8 rλk δK (K − κ) ,\nwhich is a relaxed relative error guarantee.\nFigure 4 depicts the growth of the actual relative error with r. The particular experiment corresponds to a clustering problem involving N = 1000 images, each depicting a selected digit between 0 and 4 from the MNIST database (i.e., K = 5). We constructed a 12-nearest neighbor similarity graph and repeated the experiment 10 times, each time using a different image set, selected uniformly at random. This setting produces a simple, but non-trivial, clustering problem featuring some overlaps between clusters (see Figure 5 in the supplementary material).\nWe observed that most remaining error occurred at coarsened vertices lying at cluster boundaries. This can be eliminated by only a few iterations of local smoothing. Though more advanced techniques might be preferable from a computational perspective, such as Chebychev or ARMA graph filters (Shuman et al., 2011; Isufi et al., 2017), for illustration purposes, we here additionally perform t = {2, 10} steps of a simple power iteration scheme, yielding an O(tM(K − 1)) overhead. The experiment confirms that most errors are removed after few iterations."
  }, {
    "heading": "6. Discussion",
    "text": "The main message of our work is the following: coarsening locally perturbs individual eigenvectors; however, if carefully constructed, it can leave well-separated principal eigenspaces relatively untouched.\nThe main limitation of our analysis is that the concentration estimates given by Theorem 3.1 are conservative. We are currently considering methods to improve our bounds by taking into account the dependency structure of the binomial random variables in the sum. In addition, we are investigating how to generalize our analysis to the multi-level setting, where n can be as small as O(logN). Finally, we are considering the implications of our results to supervised methods for learning from graph-structured data in general, and graph convolutional neural networks in particular."
  }],
  "year": 2018,
  "references": [{
    "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
    "authors": ["M. Belkin", "P. Niyogi"],
    "venue": "Neural computation,",
    "year": 2003
  }, {
    "title": "Spectral clustering via the power method-provably",
    "authors": ["C. Boutsidis", "P. Kambadur", "A. Gittens"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Randomized dimensionality reduction for k-means clustering",
    "authors": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Geometric deep learning: Going beyond euclidean data",
    "authors": ["M.M. Bronstein", "J. Bruna", "Y. LeCun", "A. Szlam", "P. Vandergheynst"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2017
  }, {
    "title": "Spectral networks and locally connected networks on graphs",
    "authors": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. Lecun"],
    "venue": "In International Conference on Learning Representations (ICLR2014),",
    "year": 2014
  }, {
    "title": "An interlacing result on normalized laplacians",
    "authors": ["G. Chen", "G. Davis", "F. Hall", "Z. Li", "K. Patel", "M. Stewart"],
    "venue": "SIAM Journal on Discrete Mathematics,",
    "year": 2004
  }, {
    "title": "Spectral graph theory",
    "authors": ["F.R. Chung"],
    "venue": "Number 92. American Mathematical Soc.,",
    "year": 1997
  }, {
    "title": "Algebraic multigrid for least squares problems on graphs with applications to hodgerank",
    "authors": ["C. Colley", "J. Lin", "X. Hu", "S. Aeron"],
    "venue": "In Parallel and Distributed Processing Symposium Workshops (IPDPSW),",
    "year": 2017
  }, {
    "title": "The rotation of eigenvectors by a perturbation",
    "authors": ["C. Davis", "W.M. Kahan"],
    "venue": "iii. SIAM Journal on Numerical Analysis,",
    "year": 1970
  }, {
    "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
    "authors": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Weighted graph cuts without eigenvectors a multilevel approach",
    "authors": ["I.S. Dhillon", "Y. Guan", "B. Kulis"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2007
  }, {
    "title": "K-means clustering via principal component analysis",
    "authors": ["C. Ding", "X. He"],
    "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
    "year": 2004
  }, {
    "title": "Improvement of the cascadic multigrid algorithm with a gauss seidel smoother to efficiently compute the fiedler vector of a graph laplacian",
    "authors": ["S. Gandhi"],
    "venue": "arXiv preprint arXiv:1602.04386,",
    "year": 2016
  }, {
    "title": "Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning",
    "authors": ["M. Gavish", "B. Nadler", "R.R. Coifman"],
    "venue": "In ICML, pp",
    "year": 2010
  }, {
    "title": "A multi-level algorithm for partitioning",
    "authors": ["B. Hendrickson", "R.W. Leland"],
    "venue": "graphs. SC,",
    "year": 1995
  }, {
    "title": "Graph laplacians and least squares on graphs",
    "authors": ["A.N. Hirani", "K. Kalyanaraman", "S. Watts"],
    "venue": "In Parallel and Distributed Processing Symposium Workshop (IPDPSW),",
    "year": 2015
  }, {
    "title": "Autoregressive moving average graph filtering",
    "authors": ["E. Isufi", "A. Loukas", "A. Simonetto", "G. Leus"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }, {
    "title": "A fast and high quality multilevel scheme for partitioning irregular graphs",
    "authors": ["G. Karypis", "V. Kumar"],
    "venue": "SIAM Journal on scientific Computing,",
    "year": 1998
  }, {
    "title": "Multilevelk-way partitioning scheme for irregular graphs",
    "authors": ["G. Karypis", "V. Kumar"],
    "venue": "Journal of Parallel and Distributed computing,",
    "year": 1998
  }, {
    "title": "A fast multi-scale method for drawing large graphs",
    "authors": ["D.H.Y. Koren"],
    "venue": "Journal of graph algorithms and applications,",
    "year": 2002
  }, {
    "title": "Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing",
    "authors": ["I. Koutis", "G.L. Miller", "D. Tolliver"],
    "venue": "Computer Vision and Image Understanding,",
    "year": 2011
  }, {
    "title": "Fast multiscale clustering and manifold identification",
    "authors": ["D. Kushnir", "M. Galun", "A. Brandt"],
    "venue": "Pattern Recognition,",
    "year": 2006
  }, {
    "title": "Diffusion maps and coarsegraining: A unified framework for dimensionality reduction, graph partitioning, and data set parameterization",
    "authors": ["S. Lafon", "A.B. Lee"],
    "venue": "IEEE transactions on pattern analysis and machine intelligence,",
    "year": 2006
  }, {
    "title": "Relative perturbation theory:(ii) eigenspace variations",
    "authors": ["Li", "R.-C"],
    "venue": "Technical report,",
    "year": 1994
  }, {
    "title": "Lean algebraic multigrid (lamg): Fast graph laplacian linear solver",
    "authors": ["O.E. Livne", "A. Brandt"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2012
  }, {
    "title": "Vertex sparsification and universal rounding algorithms",
    "authors": ["A. Moitra"],
    "venue": "PhD thesis, Massachusetts Institute of Technology,",
    "year": 2011
  }, {
    "title": "Scalable nearest neighbor algorithms for high dimensional data",
    "authors": ["M. Muja", "D.G. Lowe"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2014
  }, {
    "title": "The network data repository with interactive graph analytics and visualization",
    "authors": ["R.A. Rossi", "N.K. Ahmed"],
    "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Chebyshev polynomial approximation for distributed signal processing",
    "authors": ["D.I. Shuman", "P. Vandergheynst", "P. Frossard"],
    "venue": "In Distributed Computing in Sensor Systems and Workshops (DCOSS), 2011 International Conference on,",
    "year": 2011
  }, {
    "title": "A multiscale pyramid transform for graph signals",
    "authors": ["D.I. Shuman", "M.J. Faraji", "P. Vandergheynst"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2016
  }, {
    "title": "Dynamic edgeconditioned filters in convolutional neural networks on graphs",
    "authors": ["M. Simonovsky", "N. Komodakis"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition",
    "year": 2017
  }, {
    "title": "Graph sparsification by effective resistances",
    "authors": ["D.A. Spielman", "N. Srivastava"],
    "venue": "SIAM Journal on Computing,",
    "year": 1913
  }, {
    "title": "Spectral sparsification of graphs",
    "authors": ["D.A. Spielman", "Teng", "S.-H"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Matrix perturbation theory",
    "authors": ["G.W. Stewart"],
    "year": 1990
  }, {
    "title": "Compressive spectral clustering",
    "authors": ["N. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Zippered polygon meshes from range images",
    "authors": ["G. Turk", "M. Levoy"],
    "venue": "In Proceedings of the 21st annual conference on Computer graphics and interactive techniques,",
    "year": 1994
  }, {
    "title": "A cascadic multigrid algorithm for computing the fiedler vector of graph laplacians",
    "authors": ["J.C. Urschel", "X. Hu", "J. Xu", "L.T. Zikatanov"],
    "venue": "arXiv preprint arXiv:1412.0565,",
    "year": 2014
  }, {
    "title": "A tutorial on spectral clustering",
    "authors": ["U. Von Luxburg"],
    "venue": "Statistics and computing,",
    "year": 2007
  }, {
    "title": "A multilevel algorithm for force-directed graph-drawing",
    "authors": ["C. Walshaw"],
    "venue": "Journal of Graph Algorithms and Applications,",
    "year": 2006
  }, {
    "title": "How to partition a billion-node graph",
    "authors": ["L. Wang", "Y. Xiao", "B. Shao", "H. Wang"],
    "venue": "In Data Engineering (ICDE),",
    "year": 2014
  }],
  "id": "SP:36ef0a410efa4347a3cb12997a989910afecf61c",
  "authors": [{
    "name": "Andreas Loukas",
    "affiliations": []
  }, {
    "name": "Pierre Vandergheynst",
    "affiliations": []
  }],
  "abstractText": "How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement—this phenomenon was previously observed, but lacked formal justification.",
  "title": "Spectrally Approximating Large Graphs with Smaller Graphs"
}