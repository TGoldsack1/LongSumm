{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 92–105 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "In commercial scenarios of neural machine translation (NMT), the one-best translation of a text is shown to multiple users who can reinforce highquality (or penalize low-quality) translations by explicit feedback (e.g., on a Likert scale) or implicit feedback (by clicking on a translated page). In such settings this type of feedback can be easily collected in large amounts. While bandit feedback1 in form of user clicks on displayed ads is the standard learning signal for response prediction in online advertising (Bottou et al., 2013), bandit learning for machine translation has so far been restricted to simulation experiments (Sokolov et al., 2016b; Lawrence et al., 2017b;\n∗The work for this paper was done while the first author was an intern at eBay.\n1The fact that only feedback for a single translation is collected constitutes the “bandit feedback” scenario where the name is inspired by “one-armed bandit” slot machines.\nNguyen et al., 2017; Kreutzer et al., 2017; Bahdanau et al., 2017).\nThe goal of our work is to show that the gold mine of cheap and abundant real-world human bandit feedback can be exploited successfully for machine learning in NMT. We analyze and utilize human reinforcements that have been collected from users of the eBay e-commerce platform. We show that explicit user judgments in form of fivestar ratings are not reliable and do not lead to downstream BLEU improvements in bandit learning. In contrast, we find that implicit task-based feedback that has been gathered in a cross-lingual search task can be used successfully to improve task-specific metrics and BLEU.\nAnother crucial difference of our work to previous research is the fact that we assume a counterfactual learning scenario where human feedback has been given to a historic system different from the target system. Learning is done offline from logged data, which is desirable in commercial settings where system updates need to be tested before deployment and the risk of showing inferior translations to users needs to be avoided. Our offline learning algorithms range from a simple bandit-to-supervised conversion (i.e., using translations with good feedback for supervised tuning) to transferring the counterfactual learning techniques presented by Lawrence et al. (2017b) from statistical machine translation (SMT) to NMT models. To our surprise, the bandit-to-supervised conversion proved to be very hard to beat, despite theoretical indications of poor generalization for exploration-free learning from logged data (Langford et al., 2008; Strehl et al., 2010). However, we show that we can further improve over this method by computing a task-specific reward scoring function, resulting in significant improvements in both BLEU and in task-specific metrics.\n92"
  }, {
    "heading": "2 Related Work",
    "text": "Sokolov et al. (2016a,b) introduced learning from bandit feedback for SMT models in an interactive online learning scenario: the MT model receives a source sentence from the user, provides a translation, receives feedback from the user for this translation, and performs a stochastic gradient update proportional to the feedback quality. Kreutzer et al. (2017) showed that the objectives proposed for log-linear models can be transferred to neural sequence learning and found that standard control variate techniques do not only reduce variance but also help to produce best BLEU results. Nguyen et al. (2017) proposed a very similar approach using a learned word-based critic in an advantage actor-critic reinforcement learning framework. A comparison of current approaches was recently performed in a shared task where participants had to build translation models that learn from the interaction with a service that provided e-commerce product descriptions and feedback for submitted translations (Sokolov et al., 2017). Lawrence et al. (2017b,a) were the first to address the more realistic problem of offline learning from logged bandit feedback, with special attention to the problem of exploration-free deterministic logging as is done in commercial MT systems. They show that variance reduction techniques used in counterfactual bandit learning (Dudı́k et al., 2011; Bottou et al., 2013) and off-policy reinforcement learning (Precup et al., 2000; Jiang and Li, 2016) can be used to avoid degenerate behavior of estimators under deterministic logging."
  }, {
    "heading": "3 User Feedback",
    "text": ""
  }, {
    "heading": "3.1 Explicit Feedback via Star Ratings",
    "text": "One way to collect reinforcement signals from human users of the eBay platform is by explicit ratings of product title translations on a five-point Likert scale. More specifically, when users visit product pages with translated titles, they can inspect the source when hovering with the mouse over the title. Then five stars are shown with the instruction to ‘rate this translation’. A screenshot of an implementation of this rating interface is shown in Figure 1. The original title, the translation and the given star rating are stored. For the experiments in this paper, we focus on translations from English to Spanish. The user star rating data set contains 69,412 rated product titles with 148k\nindividual ratings. Since 34% of the titles were rated more than once, the ratings for each title are averaged. We observe a tendency towards high ratings, in fact one half of the titles are rated with five stars (cf. Appendix C).\nTo investigate the reliability and validity of these ratings, we employed three bilingual annotators (‘experts’) to independently re-evaluate and give five-star ratings for a balanced subset of 1,000 product title translations. The annotators were presented the source title and the machine translation, together with instructions on the task provided in Appendix B. The inter-annotator agreement between experts is relatively low with Fleiss’ κ = 0.12 (Fleiss, 1971). Furthermore, there is no correlation of the averaged ‘expert’ ratings and the averaged user star ratings (Spearman’s ρ = −0.05). However, when we ask another three annotators to indicate whether they agree or disagree with a balanced subset of 2,000 user ratings, they agree with 42.3% of the ratings (by majority voting). In this binary meta-judgment task, the interannotator agreement between experts is moderate with κ = 0.45. We observe a strong tendency of the expert annotators to agree with high user ratings and to disagree with low user ratings. Two examples of user ratings, expert ratings and expert judgment are given in Table 1. In the first example, all raters agree that the translation is good, but in the second example, there is a strong disagreement between users and experts.\nThis analysis shows that it is generally not easy for non-professional users of the e-commerce platform, and even for expert annotators, to give star ratings of translations in the domain of usergenerated product titles with high reliability. This problem is related to low validity, i.e., we do not know whether the users’ response actually expresses translation quality, since we cannot control the influence of other factors on their judgment, e.g., the displayed image (see Figure 1), the prod-\nuct itself, or the users’ general satisfaction with the e-commerce transaction, nor can we exclude the possibility that the user judgment is given with an adversarial purpose. Furthermore, we do not have control over the quality of sources2, nor can we discern to which degree a user rating reflects fluency or adequacy of the translation."
  }, {
    "heading": "3.2 Task-Based Implicit Feedback",
    "text": "Another form of collecting human reinforcement signals via the eBay e-commerce platform is to embed the feedback collection into a cross-lingual information retrieval task. The product title translation system is part of the search interaction of a user with the e-commerce platform in the following way: When a user enters a query in Spanish, it is first translated to English (query translation), then a search engine retrieves a list of matching products, and their titles are translated to Spanish and displayed to the user. As soon as the user clicks on one of the translated titles, we store the original query, the translated query, the source product title and its translation. From this collection we filter the cases where (a) the original query and the translated query are the same, or (b) more than 90% of the words from the query translation are not contained in the retrieved source title. In this way, we attempt to reduce the propagation of errors in query translation and search. This leaves us with a dataset of 164,065 tuples of Spanish queries, English product titles and their Spanish translations (15% of the original collection). Note that this dataset is more than twice the size of the explicit feedback dataset. An example is given in Table 2.\nThe advantage of embedding feedback collection into a search task is that we can assume that users who formulate a search query have a genuine intent of finding products that fit their need, and are also likely to be satisfied with product title translations that match their query, i.e., contain\n2Most titles consist of a sequence of keywords rather than a fluent sentence. See Calixto et al. (2017) for a fluency analysis of product titles.\nterms from the query in their own language. We exploit this assumption in order to measure the quality of a product title translation by requiring a user to click on the translation when it is displayed as a result of the search, and then quantifying the quality of the clicked translation by the extent it matches the query that led the user to the product. For this purpose, we define a word-based matching function match(w,q) that evaluates whether a query q contains the word w:\nmatch(w,q) = { 1, ifw ∈ q 0, otherwise.\n(1)\nBased on this word-level matching, we compute a sequence-level reward for a sentence y of length T as follows:\nrecall(y,q) = 1\nT\nT∑\nt=1\nmatch(yt,q). (2)"
  }, {
    "heading": "4 Learning from User Feedback",
    "text": "Reward Functions. In reinforcement and bandit learning, rewards received from the environment are used as supervision signals for learning. In our experiments, we investigate several options to obtain a reward function ∆ : Y → [0, 1] from logged human bandit feedback:\n1. Direct User Reward: Explicit feedback, e.g., in the form of star ratings, can directly be used as reward by treating the reward function as a black box. Since human feedback is usually only available for one translation per input, learning from direct user rewards requires the use of bandit learning algorithms. In our setup, human bandit feedback has been collected for translations of a historic MT system different from the target system to be optimized. This restricts the learning setup to offline learning from logged bandit feedback.\n2. Reward Scoring Function: A possibility to use human bandit feedback to obtain rewards for more than a single translation per input is\nto score translations either against a logged reference or a logged query. The first option requires a bandit-to-supervised conversion of data where high-quality logged translations are used as references against which BLEU or other MT quality metrics can be measured. The second option uses logged queries to obtain a matching score as in Equation 2.\n3. Estimated Reward: Another option to extend bandit feedback to all translations is to learn a parametric model of rewards, e.g., by optimizing a regression objective. The reward function is known, but the model parameters need to be trained based on a history of direct user rewards or by evaluations of a reward scoring function.\nIn the following, we present how rewards can be integrated in various objectives for NMT training.\nMaximum Likelihood Estimation by Banditto-Supervised Conversion. Most commonly, NMT models are trained with Maximum Likelihood Estimation (MLE, Equation 3) on a given parallel corpus of source and target sequences D = {(x(s),y(s))}Ss=1\nLMLE(θ) =\nS∑\ns=1\nlog pθ(y (s)|x(s)). (3)\nThe MLE objective requires reference translations and is agnostic to rewards. However, in a banditto-supervised conversion, rewards can be used to filter translations to be used as pseudo-references for MLE training. We apply this scenario to explicit and implicit human feedback data in our experiments.\nReinforcement Learning by Minimum Risk Training. When rewards can be obtained for several translations per input instead of only for one as in the bandit setup, by using a reward estimate or scoring function, Minimum Risk Training (MRT, Equation 4) can be applied to optimize\nNMT from rewards.\nRMRT(θ) = S∑\ns=1\n∑\nỹ∈S(x(s)) qαθ (ỹ|x(s)) ∆(ỹ), (4)\nwhere sample probabilities are renormalized over a subset of translation samples S(x) ⊂ Y(x): qαθ (ỹ|x) = pθ(ỹ|x)α∑ y′∈S(x) pθ(y\n′|x)α . The hyperparameter α controls the sharpness of q (see Shen et al. (2016)).\nWith sequence-level rewards, all words of a translation of length T are reinforced to the same extent and are treated as if they contributed equally to the translation quality. A word-based reward function, such as the match with a given query (Equation 1), allows the words to have individual weights. The following modification of the sequence-level MRT objective (Equation 4) accounts for word-based rewards ∆(yt):\nRW-MRT(θ) = S∑\ns=1\n∑\nỹ∈S(x(s))\nT∏\nt=1 [ qαθ (ỹt|x(s), ỹ<t) ∆(yt) ] , (5)\nwhere ∆(yt) in our experiments is a matching score (1). In the following we use the bracketed prefix (W-) to subsume both sentence-level and word-level training objectives.\nWhen output spaces are large and reward functions sparse, (W-)MRT objectives typically benefit from a warm start, i.e., pre-training with MLE. Following Wu et al. (2016), we furthermore adopt a linear combination of MLE and (W-)MRT to stabilize learning:\nR(W-)MIX(θ) = λ ·RMLE(θ) +R(W-)MRT(θ).\nCounterfactual Learning by Deterministic Propensity Matching. Counterfactual learning attempts to improve a target MT system from a log of source sentences, translations produced by a historic MT system, and obtained feedback L = {(x(h),y(h),∆(y(h)))}Hh=1. For the special case of deterministically logged rewards\nLawrence et al. (2017b) introduced the Deterministic Propensity Matching (DPM) objective with self-normalization as a multiplicative control variate (Swaminathan and Joachims, 2015):3\nRDPM(θ) = 1\nH\nH∑\nh=1\n∆(y(h)) p̄θ(y (h)|x(h)), (6)\nwhere translation probabilities are reweighted over the current mini-batch B ⊂ H,B H: p̄θ(y\n(h)|x(h)) = pθ(y(h)|x(h))∑B b=1 pθ(y (b)|x(b)) . We addi-\ntionally normalize the log probability of a translation y by its length |y|: pnormθ (y|x) = exp ( log pθ(y|x)|y| ).\nCounterfactual Learning by Doubly Controlled Estimation. Lawrence et al. (2017b) furthermore propose the Doubly Controlled objective (DC, Equation 7) implementing the idea of doubly robust estimation (Dudı́k et al., 2011; Jiang and Li, 2016) for deterministic logs. In addition to learning from the historic reward for the logging system, the reward for other translations is estimated by a parametrized regression model that is trained on the log ∆̂φ : Y → [0, 1]. This objective contains both a multiplicative (probability reweighting) and an additive (reward estimate) control variate, hence the name.4\nRDC(θ) = 1\nH\nH∑\nh=1\n[( ∆(y(h))− ∆̂φ(y(h)) )\n× p̄θ(y(h)|x(h)) + ∑\ny∈S(x(h)) ∆̂φ(y) pθ(y|x(h))\n]\n(7)\nAs for MRT, the expectation over the full output space is approximated with a subset of k sample translations S(x) ⊂ Y(x).\nRelative Rewards. With the objectives as defined above, gradient steps are dependent on the magnitude of the reward for the current training instance. In reinforcement learning, an average reward baseline is commonly subtracted from the current reward with the primary goal to reduce variance (Williams, 1992). As a side effect, the\n3Lawrence et al. (2017b) propose reweighting over the whole log, but this is infeasible for NMT. For simplicty we refer to their DPM-R objective as DPM, and DC-R as DC.\n4We find empirically that estimating ĉ over the current batch as in objective ĉDC in (Lawrence et al., 2017b) does not improve over the simple setting with c = 1.\ncurrent reward is relativized, such that the gradient step is not only determined by the magnitude of the current rewards, but is put into relation with previous rewards. We found this effect to be particularly beneficial in experiments with suboptimal reward estimators or noisy rewards and therefore apply it to all instantiations of the DPM and DC objectives. For DPM, the running average of historic rewards ∆̄h = 1 h ∑h i=1 ∆(y\n(i)) is subtracted from the current reward. For DC we apply this to both types of rewards in Equation 7: 1) the logged reward ∆(y(h)), from which we subtract its running average ∆̄h instead of the estimated reward ∆̂φ(y(h)), and 2) the estimated reward ∆̂φ(y), from which we hence subtract the average estimated reward ¯̂ ∆h = 1 h ∑h i=1 1 k ∑ y′∈S(x(i)) ∆̂φ(y ′)."
  }, {
    "heading": "5 Experiments",
    "text": ""
  }, {
    "heading": "5.1 NMT Model",
    "text": "In our experiments, learning from feedback starts from a pre-trained English to Spanish NMT model that has not seen in-domain data (i.e., no product title translations). The NMT baseline model (BL) is a standard subword-based encoder-decoder architecture with attention (Bahdanau et al., 2015), implemented with TensorFlow (Abadi et al., 2015). The model is trained with MLE on 2.7M parallel sentences of out-of-domain data until the early stopping point which is determined on a small in-domain dev set of 1,619 product title translations. A beam of size 12 and length normalization (Wu et al., 2016) are used for beam search decoding. For significance tests we used approximate randomization (Clark et al., 2011), for BLEU score evaluation (lowercased) the multi-bleu script of the Moses decoder (Koehn et al., 2007), for TER computation the tercom tool (Snover et al., 2006). For MRT, DC and (W)MIX models we set k = 5, for (W-)MIX models λ = 0.5 and α = 0.05. For all NMT models involving random sampling, we report average results and standard deviation (in subscript) over two runs. Further details about training data and hyperparameters settings are described in Appendix D."
  }, {
    "heading": "5.2 Reward Estimator",
    "text": "The model architecture for the reward estimator used in the DC objective is a bilingual extension of the convolutional neural network (CNN) for\nsentence classification proposed by Kim (2014). Both source and target sequences are padded up to a pre-defined maximum sequence length Tmax, their embeddings are concatenated and further processed by a 1D-Convolution over the time dimension with several filters of sizes from 2 to 15, which is then followed by a max-over-time pooling and fed to a fully-connected output layer (Figure 2). The model is trained to minimize the mean squared error (MSE) on the training portion of the logged feedback data (60k for simulated sentenceBLEU feedback, 62,470 for star rating feedback). The word embeddings of the reward estimator are initialized by the word embeddings of the trained baseline NMT system and fine-tuned further together with the other CNN weights. The best parameters are identified by early-stopping on the validation portion of the feedback data (2,162 for the simulation, 6,942 for the star ratings). Please find a detailed description of the model’s hyperparameters in Appendix D.4.\nResults for a stand-alone evaluation of the reward estimator on the validation portions of the feedback data are given in Table 3. The estimator models sBLEU much more accurately than the user star ratings. This is due to large variance and skew of the user ratings. An MSE-trained estimator typically predicts values around the mean, which is not a suitable strategy for such a skewed distribution of labels, but is successful for the prediction of normal-distributed sBLEU."
  }, {
    "heading": "5.3 Explicit Star Rating Feedback",
    "text": "Counterfactual Bandit Learning. As shown in Table 4, counterfactual learning with DPM and DC on the logged star ratings as direct reward does not yield improvements over the baseline model in terms of corpus BLEU or TER. A randomization of feedback signals for translations gives the same results (DPM-random), showing that counterfactual learning from logged star ratings is equivalent to learning from noise. Evaluating the models in terms of estimated user reward, however, we find an improvement of +1.49 for DC, +0.04 for DPM over the baseline (53.93) (not shown in Table 4)— but these improvements do not transfer to BLEU because the reward model largely over-estimates the translation quality of translations with major faults. Hence it is not desirable to optimize towards this signal directly.\nBandit-to-Supervised Conversion. In the following setup, we utilize the user ratings to filter the log by using only five star rated translations, and perform supervised learning of MLE and MIX using sBLEU against pseudo-references as reward function. Table 4 shows that this filtering strategy leads to large improvements over the baseline, for MLE and even more for MIX, even though the data set size is reduced by 42%. However, around the same improvements can be achieved with a random selection of logged translations of the same size (MIX small, containing 55% fivestar ratings). Using all logged translations for training MIX achieves the best results. This suggests that the model does not profit from the feedback, but mostly from being exposed to in-domain translations of the logging system. This effect is similar to training on pseudo-references created by back-translation (Sennrich et al., 2016b,a)."
  }, {
    "heading": "5.4 Task-Based Implicit Feedback",
    "text": "Bandit-to-Supervised Conversion. We apply the same filtering technique to the logged implicit feedback by treating translations with recall = 1 as references for training MIX with sBLEU (reduction of the data set by 62%). The results in Table 5 show that large improvements over the baseline can be obtained even without filtering, BLEU and TER scores being comparable to the ones observed for training on explicit user ratings.\nTask-based Feedback. The key difference between the implicit feedback collected in the query-"
  }, {
    "heading": "Model Test BLEU Test TER",
    "text": ""
  }, {
    "heading": "Model Test BLEU Test TER",
    "text": "title data and the explicit user ratings, is that it can be used to define reward functions like recall or match (Equations 2, 1). For the experiments we train W-MIX, the word-based MRT objective (Equation 5) linearly combined with MLE, on the logged translations accompanying the queries (160k sentences). This combination is essential here, since the model would otherwise learn to produce translations that contain nothing but the query words. To account for usergenerated language in the queries and subwords in the MT model, we soften the conditions for a match, counting tokens as a match that are part of a word w that is either contained in the query, or has edit distance to a word in the query with dist(w,qi) < max(3, 0.3× |w|).\nTable 6 repeats the best MIX results from Table 4 and 5, and evaluates the models with respect to query recall. We also report the query recall for the logged translations and the out-of-domain baseline. These results are compared to W-MIX training on implicit feedback data described in Sec-"
  }, {
    "heading": "65.33 45.96 62.92±0.56 63.21±0.24 68.12±0.27",
    "text": "tion 3.2. The development portion of the querytitle dataset contains 4,065 sentences, the test set 2,000 sentences, which is used for query recall evaluation. The W-MIX model shows the largest improvement in query recall (12% points) and BLEU (6 points) over the baseline out of all tested learning approaches. It comes very close to the BLEU/TER results of the model trained on indomain references, but surpasses its recall by far. This is remarkable since the model does not use any human generated references, only logged data of task-based human feedback. Appendix F contains a set of examples illustrating what the WMIX learned."
  }, {
    "heading": "6 Conclusion",
    "text": "We presented methods to improve NMT from human reinforcement signals. The signals were logged from user activities of an e-commerce platform and consist of explicit ratings on a five-point Likert scale and implicit task-based feedback collected in a cross-lingual search task. We found that there are no improvements when learning from user star ratings, unless the noisy ratings themselves are stripped off in a bandit-to-supervised conversion. Implicit task-based feedback can be used successfully as a reward signal for NMT optimization, leading to improvements both in terms of enforcing individual word translations and in terms of automatic evaluation measures. In the future, we plan transfer these findings to production settings by performing regular NMT model updates with batches of collected user behavior data, especially focusing on improving translation of ambiguous and rare terms based on rewards from implicit partial feedback."
  }, {
    "heading": "Acknowledgements",
    "text": "The last author was supported in part by DFG Research Grant RI 2221/4-1. We would like to thank Pavel Petrushkov for helping with the NMT setup, and the anonymous reviewers for their insightful comments ."
  }, {
    "heading": "A Appendix Overview",
    "text": "Section B provides the instructions that were given to the annotators when judging MT quality. In Section C we provide histograms for simulated and explicit rewards. Section D contains details on the data and NMT model hyperparameters. In Section E we give results for simulation experiments on the e-commerce product title domain and a publicly available data set. Finally, we compare translation examples of different models in Section F."
  }, {
    "heading": "B Annotation Instructions",
    "text": ""
  }, {
    "heading": "B.1 Star Ratings",
    "text": "Please rate the translation quality of the segments on the scale from 1 to 5. Focus on whether or not the information contained in the source sentence is correctly and completely translated (ratings 1 - 4). Then, if you are ready to give a 4 based on the criteria below, check whether or not you can assign a 5 instead of the 4, focusing on remaining grammatical, morphological and stylistic errors. Remember that even a very fluent translation that looks like a human-produced sentence can receive a bad rating if it does not correctly convey all the information that was present in the source.\nAssign the following ratings from 1 to 5:\n1. Important information is missing and/or distorted in the translation, and the error is so severe that it may lead to erroneous perception of the described product. Or the translation contains profanities/insulting words.\n2. Information from the source is partially present in the translation, but important information is not translated or translated incorrectly.\n3. The most important information from the source is translated correctly, but some other less important information is missing or translated incorrectly.\n4. All of the information from the source is contained in the translation. This should be the only criterion to decide between 1-3 and 4. It is okay for a 4-rated translation to contain grammatical errors, disfluencies, or word choice that is not very appropriate to the style of the input text. There might be errors in casing of named entities when it is clear from the context that these are named entities.\n5. All of the information from the source is contained in the translation and is translated correctly. In contrast to a 4-rated translation, the translation is fluent, easy to read, and contains either no or very minor grammatical/morphological/stylistic errors. The brand names and other named entities have the correct upper/lower case."
  }, {
    "heading": "B.2 Binary Judgment",
    "text": "The customers of the eBay e-commerce platform, when presented with a title translation on the product page, can hover with the mouse over the translation of the title and see the original (source) title in a pop-up window. There, they have the possibility to rate the translation with 1 to 5 stars.\nThe goal of this evaluation is to check the ratings - you have to mark “Agree” when you agree with the rating and “Disagree” otherwise. The rating (number from 1 to 5) is shown in the Reference line.\nNote that eBay customers did not have any instructions on what the rating of 5 stars, 3 stars, or 4 stars means. Thus, the evaluation is subjective on their side. Please apply your common sense when agreeing or disagreeing with human judgment. The focus should be on adequacy (correct information transfer) as opposed to fluency."
  }, {
    "heading": "C Rewards",
    "text": ""
  }, {
    "heading": "C.1 Reward Distributions",
    "text": "Figure 3 shows the distribution of logged user star ratings, Figure 4 the distribution of sentence BLEU (sBLEU) scores for the simulation experiments with logged feedback. The logged translations for the user star ratings were generated by the production system, the logged translations for the simulation were generated by the BL NMT system."
  }, {
    "heading": "D Training Details",
    "text": ""
  }, {
    "heading": "D.1 Data",
    "text": "We conduct experiments on an English-to-Spanish e-commerce item titles translation task. The indomain data for training with simulated feedback is composed of in-house eBay data (item titles, descriptions, etc.). The out-of-domain data for training the baselines contains only publicly available parallel corpora, that is Europarl, TAUS, and OpenSubtitles released by the OPUS project (Tiedemann, 2009). The out-of-domain\ndata has been sub-sampled according to the similarity to the domain of the product title data, and 25% of the most similar sentence pairs have been selected. The corpus statistics for parallel data are shown in Table 7. Before calculating the corpus statistics, we apply pre-processing including tokenization and replacement of numbers and product specifications with a placeholder token (e.g., ‘6S’, and ‘1080p’). Table 8 gives an overview of the type and the size of the translations with feedback."
  }, {
    "heading": "D.2 NMT Model Architecture",
    "text": "The NMT has a bi-directional RNN encoder with one layer of 1000 GRUs, a decoder with 1000 GRUs, and source and target word embeddings of size 620. The vocabulary is generated from the out-of-domain training corpus with 40k bytepair merges (Sennrich et al., 2016c) and contains 40813 source tokens and 41050 target tokens. The full softmax is approximated by 1024 samples as proposed in (Jean et al., 2015). Dropout (Gal and Ghahramani, 2016) is applied with probability p = 0.1 to the embedding matrices, with p = 0.2 to the\ninput and recurrent connections of the RNNs."
  }, {
    "heading": "D.3 NMT Training Hyperparameters",
    "text": "The out-of-domain model is trained with minibatches of size 100 and L2 regularization with weight 1× 10−7, optimized with Adam (Kingma and Ba, 2014) with initial α = 0.0002, then decaying α by 0.9 each epoch.\nThe remaining models are trained with constant learning rates and mini-batch size 30, regularization and dropout stay the same. The settings for the other hyperparameters are listed in Table 9. The estimator loss weight is only relevant for DC, where the pre-trained estimator gets further finetuned during DC training."
  }, {
    "heading": "D.4 Reward Estimation",
    "text": "We find that for reward estimation a shallow CNN architecture with wide filters performs superior to a deeper CNN architecture (Le et al., 2017) and also to a recurrent architecture. Hence, we use one convolutional layer with ReLU activation of\nnf filters each for filter sizes from 2 to 15, capturing both local and more global features. For reward estimation on star ratings, nf = 100 and on simulated sBLEU nf = 20 worked best. Dropout with p = 0.5 is applied before the output layer for the simulation setting. We set Tmax = 60. The loss of each item in the batch is weighted by inverse frequency of its feedback in the current batch (counted in 10 buckets) to counterbalance skewed feedback distributions. The model is optimized with Adam (Kingma and Ba, 2014) (constant α = 0.001 for star ratings, α = 0.002 for the simulation) on minibatches of size 30. Note that the differences in hyper-parameters between both settings are the result of tuning and do not cause the difference in quality of the resulting estimators. We do not evaluate on a separate test set, since their final quality can be measured in how much well they serve as policy evaluators in counterfactual learning."
  }, {
    "heading": "E Simulated Bandit Feedback",
    "text": "Expected Loss. When rewards can be retrieved for sampled translations during learning, the Online Bandit Structured Prediction framework proposed by Sokolov et al. (2016a,b) can be applied for NMT, as demonstrated in Kreutzer et al.\n(2017); Sokolov et al. (2017). The Expected Loss objective (EL, Equation 8) maximizes5 the expectation of a reward over all source and target sequences, and does in principle not require references:\nREL(θ) =Ep(x)pθ(ỹ|x) [∆(ỹ)] . (8)\nWhile we could not apply it to the logged user feedback since it was obtained offline, we can compare to its performance in a simulation setting with simulated rewards instead of human feedback. It is expected to outperform methods learning with logged feedback due to the exploration during learning. In the following simulation experiments, ∆(ỹ) is computed by comparing a sampled translation ỹ ∼ pθ(y|x) to a given reference translation y with smoothed sentence-level BLEU (sBLEU)."
  }, {
    "heading": "E.1 E-commerce Product Titles",
    "text": "We test several of the proposed learning techniques with an in-domain parallel corpus (62,162 sentences) of product titles where bandit feedback is simulated by evaluating a sampled translation against a reference using sBLEU. Similar to previous studies on SMT (Lawrence et al., 2017b),\n5We use the terms reward or loss interchangeably depending on minimization or maximization contexts."
  }, {
    "heading": "Learning Model Test BLEU Test TER",
    "text": "this reward is deterministic and does not contain user-dependent noise.\nSupervised Fine-Tuning. When fine-tuning the baseline model on in-domain references (Luong and Manning, 2015), the model improves 3.34 BLEU (MLE in Table 10) on an in-domain test set (1,000 sentences). By tuning it on the same in-domain data for sBLEU with MIX, it gains another 3 BLEU points.\nBandit Learning. When feedback is given to only one translation per input (=online bandit feedback), the model (EL) achieves comparable performance to MLE training with references. When the feedback is logged offline for one round of deterministic outputs of the baseline model (=offline bandit feedback), we can still find improvements of 1.81 BLEU (DPM). With a reward estimator trained on this log, DC achieves even higher improvements of 3 BLEU. To test the contribution of the feedback in contrast to a simple in-domain training effect, we randomly perturbed the pairing of feedback signal and translation and retrain (DPM-random). This clearly degrades results, confirming feedback to be a useful signal rather than noise."
  }, {
    "heading": "E.2 Results on Publicly Available Data",
    "text": "Simulation experiments were also run on publicly available data. We use the same data, preprocessing and splits as (Lawrence et al., 2017b) to compare with their French-to-English news experiments on counterfactual learning with deterministically logged feedback for statistical machine translation (SMT). The baseline model is trained with MLE on 1.6M Europarl (EP) translations, bandit feedback is then simulated from 40k News Commentary (NC) translations. For the comparison of full supervision vs. weak feedback, we train in-domain models with MLE on in-domain NC references: training only on in-domain data (NC BL), and fine-tuning the out-of-domain baseline (EP BL) on in-domain data (MLE). The results are given in Table 11. The NMT baselines outperform the SMT equivalents. With fully supervised fine-tuning the NMT models improve over the out-of-domain baseline (EP BL) by 5 BLEU points, outperforming also the in-domain baseline (NC BL). Moving to weak feedback, we still find improvements over the baseline by 0.5 BLEU with beam search and 1.6 BLEU with greedy decoding for online feedback (EL), and 0.6 BLEU with beam search and 1 BLEU with greedy decoding for counterfactual learning with DC. However, DPM performs worse than for SMT and those not manage to improve over the out-of-domain baseline. Nevertheless these results confirm that – at least in simulation settings – the DC objective is very suitable for counterfactual learning from bandit feedback for NMT, almost reaching the gains of learning from online bandit feedback."
  }, {
    "heading": "F Examples",
    "text": "Table 12 gives an example where W-MIX training improved lexical translation choices. Table 13 lists two examples of W-MIX translations in comparison to the baseline and logged translations for given queries and product titles to illustrate the specific difficulties of the domain."
  }],
  "year": 2018,
  "references": [{
    "title": "TensorFlow: Largescale machine learning on heterogeneous systems. https://www.tensorflow.org",
    "authors": ["sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"],
    "year": 2015
  }, {
    "title": "An actor-critic algorithm for sequence prediction",
    "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."],
    "venue": "5th International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Third International Conference on Learning Representations. San Diego, California.",
    "year": 2015
  }, {
    "title": "Counterfactual reasoning and learning systems: The example",
    "authors": ["Léon Bottou", "Jonas Peters", "Joaquin QuiñoneroCandela", "Denis X. Charles", "D. Max Chickering", "Elon Portugaly", "Dipanakar Ray", "Patrice Simard", "Ed Snelson"],
    "year": 2013
  }, {
    "title": "Using images to improve machine-translating e-commerce product listings",
    "authors": ["Iacer Calixto", "Daniel Stein", "Evgeny Matusov", "Pintu Lohar", "Sheila Castilho", "Andy Way."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for",
    "year": 2017
  }, {
    "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Lin-",
    "year": 2011
  }, {
    "title": "Doubly robust policy evaluation and learning",
    "authors": ["Miroslav Dudı́k", "John Langford", "Lihong Li"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning",
    "year": 2011
  }, {
    "title": "Measuring nominal scale agreement among many raters",
    "authors": ["Joseph L Fleiss."],
    "venue": "Psychological bulletin 76(5):378.",
    "year": 1971
  }, {
    "title": "Dropout as a bayesian approximation: Representing model uncertainty",
    "authors": ["Y Gal", "Z Ghahramani"],
    "year": 2016
  }, {
    "title": "On using very large target vocabulary for neural machine translation",
    "authors": ["Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }, {
    "title": "Doubly robust offpolicy value evaluation for reinforcement learning",
    "authors": ["Nan Jiang", "Lihong Li."],
    "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML). New York, NY.",
    "year": 2016
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar.",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Moses: Open source toolkit for statistical machine translation",
    "authors": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"],
    "year": 2007
  }, {
    "title": "Bandit structured prediction for neural sequence-to-sequence learning",
    "authors": ["Julia Kreutzer", "Artem Sokolov", "Stefan Riezler."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Exploration scavenging",
    "authors": ["John Langford", "Alexander Strehl", "Jennifer Wortman."],
    "venue": "Proceedings of the 25th International Conference on Machine Learning (ICML). Helsinki, Finland.",
    "year": 2008
  }, {
    "title": "Counterfactual learning for machine translation: Degeneracies and solutions",
    "authors": ["Carolin Lawrence", "Pratik Gajane", "Stefan Riezler."],
    "venue": "Proceedings of the NIPS WhatIF Workshop. Long Beach, CA.",
    "year": 2017
  }, {
    "title": "Counterfactual learning from bandit feedback under deterministic logging : A case study in statistical machine translation",
    "authors": ["Carolin Lawrence", "Artem Sokolov", "Stefan Riezler."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural",
    "year": 2017
  }, {
    "title": "Do convolutional networks need to be deep for text classification",
    "authors": ["Hoa T. Le", "Christophe Cerisara", "Alexandre Denis"],
    "year": 2017
  }, {
    "title": "Stanford neural machine translation systems for spoken language domains",
    "authors": ["Minh-Thang Luong", "Christopher D Manning."],
    "venue": "Proceedings of the International Workshop on Spoken Language Translation. Da Nang, Vietnam.",
    "year": 2015
  }, {
    "title": "Reinforcement learning for bandit neural machine translation with simulated human feedback",
    "authors": ["Khanh Nguyen", "Hal Daumé III", "Jordan BoydGraber."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process-",
    "year": 2017
  }, {
    "title": "Eligibility traces for off-policy policy evaluation",
    "authors": ["Doina Precup", "Richard S. Sutton", "Satinder P. Singh."],
    "venue": "Proceedings of the Seventeenth International Conference on Machine Learning (ICML). San Francisco, CA.",
    "year": 2000
  }, {
    "title": "Edinburgh neural machine translation systems for wmt 16",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Improving neural machine translation models with monolingual data",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany.",
    "year": 2016
  }, {
    "title": "Minimum risk training for neural machine translation",
    "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Ger-",
    "year": 2016
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Ameri-",
    "year": 2006
  }, {
    "title": "Learning structured predictors from bandit feedback for interactive nlp",
    "authors": ["Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Ger-",
    "year": 2016
  }, {
    "title": "Stochastic structured prediction under bandit feedback",
    "authors": ["Artem Sokolov", "Julia Kreutzer", "Stefan Riezler", "Christopher Lo."],
    "venue": "Advances in Neural Information Processing Systems. Barcelona, Spain.",
    "year": 2016
  }, {
    "title": "Learning from logged implicit exploration data",
    "authors": ["Alexander L. Strehl", "John Langford", "Lihong Li", "Sham M. Kakade."],
    "venue": "Advances in Neural Information Processing Sytems (NIPS). Vancouver, Canada.",
    "year": 2010
  }, {
    "title": "The self-normalized estimator for counterfactual learning",
    "authors": ["Adith Swaminathan", "Thorsten Joachims."],
    "venue": "Advances in Neural Information Processing Systems (NIPS). Montreal, Canada.",
    "year": 2015
  }, {
    "title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces",
    "authors": ["Jörg Tiedemann."],
    "venue": "Recent advances in natural language processing. volume 5, pages 237–248.",
    "year": 2009
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Machine learning 8:229–256.",
    "year": 1992
  }, {
    "title": "Bandit Structured Prediction framework proposed by Sokolov et al. (2016a,b) can be applied for NMT, as demonstrated",
    "authors": ["Kreutzer"],
    "year": 2016
  }, {
    "title": "The Expected Loss objective (EL, Equation 8) maximizes5 the expectation of a reward over all source and target sequences, and does in principle not require references",
    "authors": ["Sokolov"],
    "year": 2017
  }],
  "id": "SP:270e9b0681e895ae5adf937ea0cca9eb3718c721",
  "authors": [{
    "name": "Julia Kreutzer",
    "affiliations": []
  }, {
    "name": "Shahram Khadivi",
    "affiliations": []
  }, {
    "name": "Evgeny Matusov",
    "affiliations": []
  }, {
    "name": "Stefan Riezler",
    "affiliations": []
  }],
  "abstractText": "We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay ecommerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments—five-star ratings of translation quality—and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit taskbased feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.",
  "title": "Can Neural Machine Translation be Improved with User Feedback?"
}