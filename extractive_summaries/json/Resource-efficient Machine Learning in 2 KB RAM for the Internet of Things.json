{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Objective: This paper develops a novel tree-based algorithm, called Bonsai, which can be trained on a laptop, or the cloud, and can then be shipped onto severely resource constrained Internet of Things (IoT) devices.\nResource constrained devices: The Arduino Uno board has an 8 bit ATmega328P microcontroller operating at 16 MHz with 2 KB SRAM and 32 KB read-only flash mem-\n1Microsoft Research, Bangalore, India 2CSE Department, IIT Delhi, India. Correspondence to: <manik@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nory. The BBC Micro:Bit has a 32 bit ARM Cortex M0 microcontroller operating at 16 MHz with 16 KB SRAM and 256 KB read-only flash. Neither provides hardware support for floating point operations. Billions of such tiny IoT microcontrollers have been deployed in the world (Meunier et al., 2014). Before deployment, the OS and all application code and data are burnt onto flash, leaving only a few KB for storing the trained ML model, prediction code, feature extraction code and associated data and parameters. After deployment, the only writable memory available is the 2 KB (Uno) or 16 KB (Micro:Bit) of SRAM which might not be sufficient to hold even a single feature vector.\nThe Internet of Things: A number of applications have been developed for consumer, enterprise and societal IoT including predictive maintenance, intelligent healthcare, smart cities and housing, etc. The dominant paradigm for these applications, given the severe resource constraints of IoT devices, has been that the IoT device is dumb – it just senses its environment and transmits the sensor readings to the cloud where all the decision making happens.\nMotivating scenarios: This paper proposes an alternative paradigm where the IoT device can make predictions locally without necessarily connecting to the cloud. This enables many scenarios, beyond the pale of the traditional paradigm, where it is not possible to transmit data to the cloud due to latency, bandwidth, privacy and energy concerns. For instance, consider a microcontroller implanted in the brain which warns patients about impending seizures so that they can call for help, pull over if they are driving, etc. Making predictions locally would allow the device to work everywhere irrespective of cloud connectivity. Furthermore, alerts could be raised more quickly with local predictions than if all the sensor readings had to be first transmitted to the cloud. In addition, since the energy required for executing an instruction might be much lower than the energy required to transmit a byte, making predictions locally would extend battery life significantly thereby avoiding repeated brain surgery and might also prevent brain tissue damage due to excess heat dissipation from the communicating radio. Finally, people might not be willing to transmit such sensitive data to the cloud. These characteristics are shared by many other scenarios including implants in the heart, precision agriculture on disconnected farms, smart spectacles for the visually impaired, etc.\nTree algorithms: Tree algorithms are general and can be used for classification, regression, ranking and other problems commonly found in the IoT setting. Even more importantly, they are ideally suited to IoT applications as they can achieve good prediction accuracies with prediction times and energies that are logarithmic in the number of training points. Unfortunately, they do not directly fit on tiny IoT devices as their space complexity is linear rather than logarithmic. In particular, learning shallow trees, or aggressively pruning deep trees or large ensembles, to fit in just a few KB often leads to poor prediction accuracy.\nBonsai: This paper develops a novel tree learner, called Bonsai, designed specifically for severely resource constrained IoT devices based on the following contributions. First, Bonsai learns a single, shallow, sparse tree so as to reduce model size but with powerful nodes for accurate prediction. Second, both internal and leaf nodes in Bonsai make non-linear predictions. Bonsai’s overall prediction for a point is the sum of the individual node predictions along the path traversed by the point. Path based prediction allows Bonsai to accurately learn non-linear decision boundaries while sharing parameters along paths to further reduce model size. Third, Bonsai learns a sparse matrix which projects all data points into a low-dimensional space in which the tree is learnt. This allows Bonsai to fit in a few KB of flash. Furthermore, the sparse projection is implemented in a streaming fashion thereby allowing Bonsai to tackle IoT applications where even a single feature vector might not fit in 2 KB of RAM. Fourth, rather than learning the Bonsai tree node by node in a greedy fashion, all nodes are learnt jointly, along with the sparse projection matrix, so as to optimally allocate memory budgets to each node while maximising prediction accuracy.\nImplementation: Another contribution is an efficient implementation of Bonsai which reduces its prediction costs on the Arduino and Micro:Bit to be even lower than that of an unoptimized linear classifier. This allows Bonsai to enjoy the prediction accuracy of a non-linear classifier while paying less than linear costs. This paper does not focus on the system and implementation details due to space limitations but the interested reader is referred to the publically available source code (BonsaiCode).\nResults: These contributions allow Bonsai to make predictions in milliseconds even on slow microcontrollers, fit in a few KB of flash and extend battery life beyond all other algorithms. Furthermore, it is demonstrated on multiple benchmark datasets that Bonsai’s prediction accuracies can approach those of uncompressed kNN classifiers, RBFSVMs, single hidden layer neural networks and gradient boosted decision tree ensembles whose models might take many MB of RAM. It is also demonstrated that Bonsai’s prediction accuracies for a given model size can be as much\nas 30% higher than state-of-the-art methods for resourceefficient machine learning. Finally, Bonsai is shown to generalize to other resource constrained settings beyond IoT by producing significantly better search results than Bing’s L3 ranker when the model size is restricted to 300 bytes."
  }, {
    "heading": "2. Related Work",
    "text": "The literature on resource-efficient machine learning is vast and specialized solutions have been developed for reducing the prediction costs of kNN algorithms (Kusner et al., 2014b; Wang et al., 2016), SVMs (Hsieh et al., 2014; Jose et al., 2013; Le et al., 2013; Li et al., 2016), deep learning (Iandola et al., 2016; Han et al., 2016; Yang et al., 2015; Denton et al., 2014; Wu et al., 2016; Rastegari et al., 2016; Hubara et al., 2016; Shankar et al., 2016; Ioannou et al., 2016a), model compression (Bucilua et al., 2006; Ba & Caruana, 2014), feature selection (Kusner et al., 2014a; Xu et al., 2013; 2012; Nan et al., 2015; Wang et al., 2015) and applications such as face detection (Viola & Jones, 2004).\nResource-efficient tree classifiers are particularly germane to this paper. The standard approach is to greedily grow the decision tree ensemble node by node until the prediction budget is exhausted. A popular alternative is to first learn the random forest or gradient boosted decision tree ensemble to maximize prediction accuracy and then use pruning techniques to meet the budget constraints (Duda et al., 2002; Dekel et al., 2016; Nan et al., 2016; Li, 2001; Breiman et al., 1984; Zhang & Huei-chuen, 2005; Sherali et al., 2009; Kulkarni & Sinha, 2012; Rokach & Maimon, 2014; Joly et al., 2012). Unfortunately, such techniques are fundamentally limited as they attempt to approximate complex non-linear decision boundaries using a small number of axis-aligned hyperplanes. This can lead to poor prediction accuracies as observed in Section 5.\nTree models have also been developed to learn more complex decision boundaries by moving away from learning axis-aligned hyperplanes at internal nodes and constant predictors at the leaves. For instance, (Breiman, 2001; Murthy et al., 1994; Kontschieder et al., 2015) learnt more powerful branching functions at internal nodes based on oblique cuts and full hyperplanes while (Utgoff, 1989; Hsieh et al., 2014) learnt more powerful leaf node predictors based on linear classifiers, kernelized SVMs, etc. Bonsai achieves better budget utilization than such models by learning shorter trees, typically depth 4 or lower, and by sharing the parameters between leaf node predictors.\nThe models closest to Bonsai are Decision Jungles (Shotton et al., 2013) and LDKL (Jose et al., 2013). Bonsai improves upon LDKL by learning its tree in a lowdimensional space, learning sparse branching functions and predictors and generalizing the model to multi-class classi-\nfication, ranking, etc. Decision Jungles are similar to Bonsai in that they share node parameters using a DAG structure. Unfortunately, Decision Jungles need to learn deep tree ensembles with many nodes as they use weak constant classifiers as leaf node predictors. Bonsai can have lower model size and higher accuracy as it learns a single, shallow tree in a low-dimensional space with non-linear predictors.\nNote that while tree based cost-sensitive feature selection methods are not directly relevant, their performance is nevertheless empirically compared to Bonsai’s in Section 5."
  }, {
    "heading": "3. The Bonsai Model for Efficient Prediction",
    "text": "Overview: Bonsai learns a single, shallow sparse tree whose predictions for a point x are given by\ny(x) = ∑ k Ik(x)W > k Zx ◦ tanh(σV>k Zx) (1)\nwhere ◦ denotes the elementwise Hadamard product, σ is a user tunable hyper-parameter, Z is a sparse projection matrix and Bonsai’s tree is parameterized by Ik, Wk and Vk where Ik(x) is an indicator function taking the value 1 if node k lies along the path traversed by x and 0 otherwise and Wk and Vk are sparse predictors learnt at node k. The prediction function is designed to minimize the model size, prediction time and prediction energy, while maintaining prediction accuracy, even at the expense of increased training costs. The function is also designed to minimize the working memory required as the Uno provides only 2 KB of writeable memory for storing the feature vector, programme variables and intermediate computations.\nStreaming sparse projection: Bonsai projects each D-dimensional input feature vector x into a low D̂dimensional space using a learnt sparse projection matrix ZD̂×D. Bonsai uses fixed point arithmetic for all math computation, including Zx, when implemented on the IoT device so as to avoid floating point overheads. Note that D̂ could be as low as 5 for many binary classification applications. This has the following advantages. First, it reduces Bonsai’s model size as all tree parameters are now learnt in the low-dimensional space. Second, when D̂ is small, Zx could be stored directly in the microcontroller’s registers thereby reducing prediction time and energy. Third, learning the projection matrix jointly with the tree parameters improves prediction accuracy. Fourth, since Zx can be computed in a streaming fashion, this allows Bonsai to tackle IoT applications where even a single feature vector cannot fit in 2 KB of SRAM. This is critical since standard tree implementations are unable to handle a streaming feature vector – the entire feature vector needs to be streamed for the root node to determine whether to pass the point down to the left or right child and therefore the vector is unavailable for processing at subsequent nodes. Some\nimplementations work around this limitation by simultaneously evaluating the branching function at all nodes as the vector is streamed but this increases the prediction costs from logarithmic to linear which might not be acceptable.\nBranching function at internal nodes: Bonsai computes Ik by learning a sparse vector θ at each internal node such that the sign of θ>Zx determines whether point x should be branched to the node’s left or right child. Using more powerful branching functions than the axis-aligned hyperplanes in standard decision trees allows Bonsai to learn shallow trees which can fit in a few KB. Of course, this is not a novel idea, and is insufficient in itself to allow a single, shallow decision tree to make accurate predictions.\nNode predictors: Decision trees, random forests and boosted tree ensembles are limited to making constant predictions at just the leaf nodes. This restricts their prediction accuracy when there are very few leaves. In contrast, for a multi-class, multi-label or regression problem with L targets, Bonsai learns matrices WD̂×L and VD̂×L at both leaf and internal nodes so that each node predicts the vector W>Zx◦tanh(σV>Zx). Note that the functional form of the node predictor was chosen as it was found to work well empirically (other forms could be chosen if found to be more appropriate). Further note that W and V will reduce to vectors for binary classification, ranking and singletarget regression. Bonsai’s overall predicted vector is given by (1) and is the sum of the individual vectors predicted by the nodes lying along the path traversed by x. This allows Bonsai to accurately learn non-linear decision boundaries using shallow trees with just a few nodes. Furthermore, path based prediction allows parameter sharing and therefore reduces model size as compared to putting independent classifiers of at least equal complexity in the leaf nodes alone. For instance, a depth 4 Bonsai tree with 15 internal and 16 leaf nodes stores 31 W and 31 V matrices with overall predictions being the sum of 4 terms depending on the path taken. If parameters were not shared and each leaf node independently learnt 4 W and 4 V matrices to make predictions of at least equal complexity, then a total of 16× 4 = 64 W and 64 V matrices would need to be stored thereby exceeding the memory budget. As an implementation detail, note that Bonsai uses the approximation tanh(x) ≈ x if |x| < 1 and signum(x) otherwise in order to avoid floating point computation."
  }, {
    "heading": "4. Training Bonsai",
    "text": "Notation: Bonsai learns a balanced tree of user specified height h with 2h − 1 internal nodes and 2h leaf nodes. The parameters that need to be learnt include: (a) Z: the sparse projection matrix; (b) θ = [θ1, . . . ,θ2h−1]: the parameters of the branching function at each internal node; and (c) W = [W1, . . . ,W2h+1−1] and V = [V1, . . . ,V2h+1−1]:\nthe predictor parameters at each node. Let Θ = [θ,W,V] denote a matrix obtained by stacking all the parameters together except for Z. Finally, it is assumed that N training points {(xi, y\n¯i )Ni=1} have been provided and that bud-\nget constraints BZ and BΘ on the projection matrix and tree parameters have been specified depending on the flash memory available on the IoT device.\nOptimization problem: Bonsai’s parameters are learnt as\nmin Z,Θ J (Z,Θ) = λθ 2 Tr(θ>θ) + λW 2 Tr(W>W)\n+ λV 2 Tr(V>V) + λZ 2 Tr(ZZ>)\n+ 1\nN N∑ i=1 L(xi,yi,y(xi);Z,Θ)\ns. t. ‖Z‖0 ≤ BZ, ‖Θ‖0 ≤ BΘ\n(2)\nwhere y(xi) is Bonsai’s prediction for point xi as given in (1) and L is an appropriately chosen loss function for classification, regression, ranking, etc. For instance, L = max(0, 1 − yiy(xi)) with yi ∈ {−1,+1} for binary classification and L = maxy∈Y((yi − y)>y(xi) + 1 − y>i y) with Y = {y|y ∈ {0, 1}L,1>y = 1} and yi ∈ Y for multi-class classification. It is worth emphasizing that the optimization problem is formulated such that all parameters are learnt jointly subject to the budget constraints. This leads to significantly higher prediction accuracies than if Z were first learnt independently, say using sparse PCA, and then Θ was learnt afterwards (see Section 5).\nAlgorithm: Optimizing (2) over the space of all balanced trees of height h is a hard, non-convex problem. Tree growing algorithms typically optimize such problems by greedily growing the tree a node at a time starting from the root. Unfortunately, this leads to a suboptimal utilization of the memory budget in Bonsai’s case as it is not clear a priori how much budget to allocate to each node. For instance, it is not apparent whether the budget should be distributed equally between all nodes or whether the root node should be allocated more budget and, if so, by how much.\nAlgorithm - Joint learning of nodes: Bonsai therefore learns all node parameters jointly with the memory budget for each node being determined automatically as part of the optimization. The difficulty with joint learning is that a node’s ancestors need to be learnt before it can be determined which training points will reach the node. Furthermore, the path traversed by a training point is a sharply discontinuous function of θ and Z thereby rendering gradient based techniques ineffective. Various approaches have been proposed in the literature for tackling these difficulties (Jose et al., 2013; Kontschieder et al., 2015; Norouzi et al., 2015; Xu et al., 2013; Ioannou et al., 2016b). Bonsai follows the approach of (Jose et al., 2013)\nand smooths the objective function by initially allowing points to traverse multiple paths in the tree. In particular, the indicator function Ik(x) is relaxed to Ik>1(x) = 1 2Ij(x) ( 1 + (−1)k−2j tanh(σIθ>j Zx) ) where j = ⌊ k 2 ⌋ is k’s parent node in a balanced tree, I1(x) = 1 and the parameter σI controls the fidelity of the approximation. Gradients can now be computed as\n∇θlIk(x) = σIIk(x)P lk(x)Zx (3) ∇ZIk(x) = ∑ l σIIk(x)P l k(x)θlx > (4)\nwhere P lk(x) = δ l k((−1)Ck(l) − tanh(σIθ>l Zx)), δlk = 1 if node l is an ancestor of node k and 0 otherwise and Ck(l) = 1 if node k is in the right subtree of node l and 0 otherwise. Of course, allowing a point to traverse multiple paths increases prediction costs. Some approaches therefore allow multiple paths during training but select a single path during prediction (Xu et al., 2013; Ioannou et al., 2016b). At each node, a point x is greedily branched to the child node having the greatest Ik(x). Unfortunately, this can lead to a drop in accuracy as the model learnt during training is different from the one used for prediction.\nBonsai therefore follows an alternative strategy where σI is tuned during training to ensure that points gradually start traversing at most a single path as optimization progresses. In particular, σI is initialized to a small value, such as 0.01, so as to ensure that tanh values are not saturated. As optimization progresses, σI is gradually increased so that tanh tends to the signum function and Ik(x) goes back to being an indicator function by the time convergence is reached. This allows Bonsai to directly use the learnt model for prediction and was found to empirically lead to good results.\nAlgorithm - Gradient descent with iterative hard thresholding: Various gradient based approaches, including those based on alternating minimization, were tried for optimizing (2). A gradient descent based algorithm with iterative hard thresholding (IHT) was empirically found to yield the best solutions. Gradient descent was chosen over stochastic gradient descent as it removed the burden of step size tuning, led to slightly better prediction accuracies while keeping training time acceptable. For instance, training times range from 2 minutes for USPS-2 to 15 minutes for MNIST-2 on a single core of a laptop with an Intel Core i7-3667U processor at 2 GHz with 8 GB of RAM. Stochastic gradient descent could be utilized for larger datasets or if training costs also needed to be minimized. The algorithm proceeds iteratively based on the following gradient and IHT steps in each iteration.\nAlgorithm - Gradient step: Given feasible Zt and Θt with a feasible allocation of the memory budget to various nodes at time step t, Bonsai applies M updates of gradient descent keeping the support of Z and Θ fixed so that\nthe budget allocations to nodes remain unchanged and the memory constraints are never violated. The update equations at each time step are\nZt+1 = Zt − ηtZ∇ZJ (Zt,Θt)|supp(Zt) (5) Θt+1 = Θt − ηtΘ∇ΘJ (Zt,Θt)|supp(Θt) (6)\nwith step sizes ηZ and ηΘ being chosen according to the Armijo rule and |supp indicating that the gradient was being computed only for the non-zero entries. M = 5 and M = 15 iterations were found to work well for binary and multi-class classification respectively. This allows Bonsai to decrease the objective function value without changing the budget allocation of various nodes.\nAlgorithm - IHT step: In order to improve the budget allocation, Bonsai performs a single gradient update with unrestricted support. This violates the memory constraints and Bonsai therefore projects the solution onto the feasible set by retaining the parameters with the largest magnitudes\nZt+M+1 = TBZ(Z t+M − ηt+MZ ∇ZJ (Z t+M ,Θt+M )) Θt+M+1 = TBΘ(Θ t+M − ηt+MΘ ∇ΘJ (Z t+M ,Θt+M ))\nwhere Tk is an operator returning k of its arguments which have the largest magnitudes while setting the rest to 0. This allows Bonsai to move to another feasible solution with even lower objective function value by improving the memory budget distribution across nodes.\nAlgorithm - Convergence: In general, projected gradient descent based algorithms might oscillate for non-convex problems. However, (Blumensath & Davies, 2008) prove that for smooth objective functions, gradient descent algorithms with IHT do indeed converge to a saddle point solution. Furthermore, if the objective function satisfies the Restricted Strong Convexity (RSC) property in a local region, then projected gradient descent with IHT will converge to the local minimum in that region (Jain et al., 2014). In practice, it was observed that the algorithm generally converged to a good solution soon and therefore was terminated after T = 300 iterations were reached.\nAlgorithm - Initialization & re-training: Z0 and Θ0 could be set randomly. Prediction accuracy gains of up to 1.5% could be observed if Bonsai was initialized by taking T steps of gradient descent without any budget constraints followed by a hard thresholding step. Further gains of 1.5% could be observed by taking another T steps of gradient descent with fixed support after termination. This helped in fine-tuning Bonsai’s parameters once the memory budget allocation had been finalized across the tree nodes.\nMore details about the optimization can be found in the supplementary material by clicking here."
  }, {
    "heading": "5. Experiments",
    "text": "Datasets: Experiments were carried out on a number of publically available binary and multi-class datasets including Chars4K (Campos et al., 2009), CIFAR10 (Krizhevsky, 2009), MNIST (LeCun et al., 1998), WARD (Yang et al., 2009), USPS (Hull, 1994), Eye (Kasprowski & Ober, 2004), RTWhale (RTW), and CUReT (Varma & Zisserman, 2005). Binary versions of these datasets were downloaded from (Jose et al., 2013). Bing’s L3 Ranking is a proprietary dataset where ground truth annotations specifying the relevance of query-document pairs have been provided on a scale of 0-4. Table 1 lists these datasets’ statistics.\nBaseline algorithms: Bonsai was compared to stateof-the-art algorithms for resource-efficient ML spanning tree, kNN, SVM and single hidden layer neural network algorithms including Decision Jungles (Shotton et al., 2013; Pohlen), Feature Budgeted Random Forests (BudgetRF) (Nan et al., 2015), Gradient Boosted Decision Tree Ensemble Pruning (Tree Pruning) (Dekel et al., 2016), Pruned Random Forests (BudgetPrune) (Nan et al., 2016), Local Deep Kernel Learning (LDKL) (Jose et al., 2013), Neural Network Pruning (NeuralNet Pruning) (Han et al., 2016) and Stochastic Neighbor Compression (SNC) (Kusner et al., 2014b). The differences between some of these algorithms and Bonsai is briefly discussed in Section 2. Publically available implementations of all algorithms were used taking care to ensure that published results could be reproduced thereby verifying the code and hyper-parameter settings. Note that Bonsai is not compared to deep convolutional neural networks as they have not yet been demonstrated to fit on such tiny IoT devices. In particular, convolutions are computationally expensive, drain batteries and produce intermediate results which do not fit in 2 KB RAM. Implementing them on tiny microcontrollers is still\n0 50 100 10\n20\n30\n40\n50\n60 Chars4K−62\nModel Size (KB)\nA cc\nur ac\ny (%\n)\nBonsaiOpt Bonsai NeuralNet Pruning SNC Decision Jungle BudgetPrune BudgetRF\n0 50 100 70\n80\n90\nCUReT−61\nModel Size (KB)\nA cc\nur ac\ny (%\n)\nBonsaiOpt Bonsai NeuralNet Pruning SNC Decision Jungle BudgetPrune BudgetRF\n0 50 100 80\n85\n90\n95\nMNIST−10\nModel Size (KB)\nA cc\nur ac\ny (%\n)\nBonsaiOpt Bonsai NeuralNet Pruning SNC Decision Jungle BudgetPrune BudgetRF\n0.2 0.4 0.6 0.8 1 42\n44\n46\n48\n50\n52\nL3 Ranking\nModel Size (KB)\nnD C\nG @\n1\nBonsai FastRank\n0 5 10 15 50\nModel Size (KB)\n0 5 10 15 80\nModel Size (KB)\n0 5 10 15 90\nModel Size (KB)\n0 5 10 15 88\nModel Size (KB)\n0 5 10 15 66\n68\n70\n72\n74\n76\n78 Chars4K−2\nModel Size (KB)\nA cc\nur ac\ny (%\n)\n0 5 10 15 68\n70\n72\n74\n76\nCIFAR10−2\nModel Size (KB)\nA cc\nur ac\ny (%\n)\n0 5 10 15 90\n92\n94\n96\nUSPS−2\nModel Size (KB)\nA cc\nur ac\ny (%\n)\nLegend\n−\n−\nBonsaiOpt Bonsai GBDT Tree Pruning LDKL LDKL−L1 NeuralNet Pruning SNC Decision Jungle BudgetPrune BudgetRF\nFigure 2: Binary Datasets - Bonsai dominates over state-of-the-art resource-efficient ML algorithms with gains of 8.6% on RTWhale-2 and 8.2% on Eye-2 in the 0-2 KB range. BonsaiOpt’s gains are even higher. Figure best viewed magnified.\nan open research problem. Bonsai’s performance was however compared to that of uncompressed single hidden layer neural networks without convolutions, Gradient Boosted Decision Trees (GBDT), kNN classifiers and RBF-SVMs.\nHyper-parameters: The publically provided training set for each dataset was subdivided into 80% for training and 20% for validation. The hyper-parameters of all algorithms were tuned on the validation set. Once the hyperparameters had been fixed, the algorithms were trained on the full training set and results were reported on the publically available test set.\nEvaluation: IoT applications would like to maximize their prediction accuracies using the best model that might fit within the available flash memory while minimizing their prediction times and energies. Accuracies of all algorithms are therefore presented for a range of model sizes. Some\nof the algorithms were implemented on the Uno and their prediction times and energies were compared to Bonsai’s.\nImplementation: Results are presented throughout for an unoptimized implementation of Bonsai for a fair comparison with the other methods. For instance, 4 bytes were used to store floating point numbers for all algorithms, all floating point operations were simulated in software, etc. However, results are also presented for an optimized implementation of Bonsai, called BonsaiOpt, where numbers were stored in a 1 byte fixed point format, tanh was approximated, all floating point operations were avoided, etc.\nComparison to uncompressed methods: The results in Tables 2 and 3 demonstrate that Bonsai’s prediction accuracies could compete with those of uncompressed kNN, GBDT, RBF-SVM and neural network classifiers with significantly larger model sizes. On RTWhale-2, Chars4K-62\nand Chars4K-2, Bonsai’s accuracies were higher than all other methods by 4.8%, 3.2% and 1.1% while its model size was lower by 977x, 13x and 157x respectively. Bonsai’s accuracies were lower by 1.0% - 5.0% on the other datasets with model size gains varying from 55x to 3996x. Note that, while BonsaiOpt’s accuracies were similar to Bonsai’s, its model sizes would be even lower.\nComparison to resource-efficient ML algorithms: The results in Figures 2 and 3 demonstrate that Bonsai’s prediction accuracies dominate those of state-of-the-art resourceefficient ML algorithms for all model sizes. In fact, Bonsai could outperform all other algorithms, including tree algorithms by as much as 30.7% on Char4K-62 and 28.9% on CUReT-61 for a given model size. For binary datasets, the largest gains were observed in the 0-2 KB regime including 8.6% on RTWhale-2 and 8.2% on Eye-2. Of course, BonsaiOpt’s gains were even higher on both binary and\nmulti-class datasets. These results validate Bonsai’s model, showing it to be accurate and compact and demonstrate that Bonsai’s optimization algorithm yields good solutions.\nL3 ranking: Bonsai was shown to generalise to other resource-constrained scenarios beyond IoT by ranking documents in response to queries on Bing. Bonsai was trained by replacing the classification gradients with rank-sensitive gradients approximating nDCG (Burges, 2010). As can be seen in Figure 1, using a 300 byte model, Bonsai could outperform Bing’s FastRank L3 ranker by 8.3%. In fact, Bonsai could achieve almost the same ranking accuracy as FastRank but with a 660x smaller model.\nPrediction on the Arduino Uno: Table 5 presents the prediction costs per test point for the highest accuracy models with size less than 2 KB for a few methods that were implemented on the Arduino Uno. The BonsaiOpt model was a more efficient implementation of the chosen Bonsai model. The results indicate that BonsaiOpt could be significantly more accurate, faster and energy-efficient as compared to other algorithms including an unoptimized linear classifier. Transmitting the test feature vector to the cloud, whenever possible, and running uncompressed GBDT might sometimes yield higher accuracies but would also consume 47x497x more energy which might not be feasible.\nBonsai’s components: The contribution of Bonsai’s components on the Chars4K-2 dataset is presented in Table 4. Modest reductions in accuracy were observed without proper initialization or re-training. Learning a projection matrix independently via sparse PCA before training reduced accuracy significantly as compared to Bonsai’s joint training of the projection matrix and tree parameters. Other tree and uncompressed methods also did not benefit much by training in the sparse PCA space."
  }, {
    "heading": "6. Conclusions",
    "text": "This paper proposed an alternative IoT paradigm, centric to the device rather than the cloud, where ML models run on tiny IoT devices without necessarily connecting to the cloud thereby engendering local decision making capabilities. The Bonsai tree learner was developed towards this end and demonstrated to be fast, accurate, compact and energy-efficient at prediction time. Bonsai was deployed on the Arduino Uno board as it could fit in a few KB of flash, required only 70 bytes of writable memory for binary classification and 500 bytes for a 62 class problem, handled streaming features and made predictions in milliseconds taking only milliJoules of energy. Bonsai’s prediction accuracies could be as much as 30% higher as com-\npared to state-of-the-art resource-efficient ML algorithms for a fixed model size and could even approach and outperform those of uncompressed models taking many MB of RAM. Bonsai achieved these gains by developing a novel model based on a single, shallow, sparse tree learnt in a low-dimensional space. Predictions made by both internal and leaf nodes and the sharing of parameters along paths allowed Bonsai to learn complex non-linear decision boundaries using a compact representation. Bonsai’s code is available from (BonsaiCode) and is part of Microsoft’s ELL machine learning compiler for IoT devices."
  }, {
    "heading": "Acknowledgements",
    "text": "We are grateful to Yeshwanth Cherapanamjeri, Ofer Dekel, Chirag Gupta, Prateek Jain, Ajay Manchepalli, Nagarajan Natarajan, Praneeth Netrapalli, Bhargavi Paranjape, Suresh Parthasarathy, Vivek Seshadri, Rahul Sharma, Harsha Vardhan Simhadri, Manish Singh and Raghavendra Udupa for many helpful discussions and feedback."
  }],
  "year": 2017,
  "references": [{
    "title": "Do deep nets really need to be deep",
    "authors": ["J. Ba", "R. Caruana"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Iterative thresholding for sparse approximations",
    "authors": ["T. Blumensath", "M.E. Davies"],
    "venue": "Journal of Fourier Analysis and Applications,",
    "year": 2008
  }, {
    "title": "Classification and regression trees",
    "authors": ["L. Breiman", "J. Friedman", "C.J. Stone", "R.A. Olshen"],
    "venue": "In CRC press,",
    "year": 1984
  }, {
    "title": "From ranknet to lambdarank to lambdamart: An overview",
    "authors": ["C.J. Burges"],
    "year": 2010
  }, {
    "title": "Character recognition in natural images",
    "authors": ["T.E. Campos", "B.R. Babu", "M. Varma"],
    "year": 2009
  }, {
    "title": "Pruning decision forests",
    "authors": ["O. Dekel", "C. Jacobbs", "L. Xiao"],
    "venue": "In Personal Communications,",
    "year": 2016
  }, {
    "title": "Exploiting linear structure within convolutional networks for efficient evaluation",
    "authors": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
    "authors": ["S. Han", "H. Mao", "W.J. Dally"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Fast prediction for largescale kernel machines",
    "authors": ["C.J. Hsieh", "S. Si", "I. Dhillon"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Quantized neural networks: Training neural networks with low precision weights and activations",
    "authors": ["I. Hubara", "M. Courbariaux", "D. Soudry", "R. El-Yaniv", "Y. Bengio"],
    "year": 2016
  }, {
    "title": "A database for handwritten text recognition research",
    "authors": ["J.J. Hull"],
    "venue": "IEEE PAMI,",
    "year": 1994
  }, {
    "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1MB model size",
    "authors": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"],
    "year": 2016
  }, {
    "title": "Deep roots: Improving cnn efficiency with hierarchical filter groups",
    "authors": ["Y. Ioannou", "D. Robertson", "R. Cipolla", "A. Criminisi"],
    "venue": "arXiv preprint arXiv:1605.06489,",
    "year": 2016
  }, {
    "title": "Decision forests, convolutional networks and the models inbetween",
    "authors": ["Y. Ioannou", "D. Robertson", "Kontschieder", "D. Zikicand P", "J. Shotton", "M. Brown", "A. Criminisi"],
    "venue": "arXiv preprint arXiv:1603.01250,",
    "year": 2016
  }, {
    "title": "On iterative hard thresholding methods for high-dimensional m-estimation",
    "authors": ["P. Jain", "A. Tewari", "P. Kar"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "L1based compression of random forest models",
    "authors": ["A. Joly", "F. Schnitzler", "P. Geurts", "L. Wehenkel"],
    "venue": "In ESANN,",
    "year": 2012
  }, {
    "title": "Local deep kernel learning for efficient non-linear svm prediction",
    "authors": ["C. Jose", "P. Goyal", "P. Aggrwal", "M. Varma"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Eye movement in biometrics",
    "authors": ["P. Kasprowski", "J. Ober"],
    "venue": "In eccv,",
    "year": 2004
  }, {
    "title": "Deep neural decision forests",
    "authors": ["P. Kontschieder", "M. Fiterau", "A. Criminisi", "S.R. Bulo"],
    "venue": "In ICCV,",
    "year": 2015
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky"],
    "venue": "Technical report,",
    "year": 2009
  }, {
    "title": "Pruning of random forest classifiers: A survey and future directions",
    "authors": ["V.Y. Kulkarni", "P.K. Sinha"],
    "venue": "In ICDSE,",
    "year": 2012
  }, {
    "title": "Feature-cost sensitive learning with submodular trees of classifiers",
    "authors": ["M.J. Kusner", "W. Chen", "Q. Zhou", "Z.E. Xu", "K.Q. Weinberger", "Y. Chen"],
    "venue": "In AAAI,",
    "year": 2014
  }, {
    "title": "Stochastic neighbor compression",
    "authors": ["M.J. Kusner", "S. Tyree", "K.Q. Weinberger", "K. Agrawal"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Fastfood-approximating kernel expansions in loglinear time",
    "authors": ["Q. Le", "T. Sarlós", "A. Smola"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Fast and accurate refined nyström-based kernel svm",
    "authors": ["Z. Li", "T. Yang", "L. Zhang", "R. Jin"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "The internet of things is now connecting the real economy",
    "authors": ["F. Meunier", "A. Wood", "K. Weiss", "K. Huberty", "S. Flannery", "J. Moore", "C. Hettenbach", "B. Lu"],
    "venue": "Technical report, Morgan Stanley,",
    "year": 2014
  }, {
    "title": "A system for induction of oblique decision",
    "authors": ["S.K. Murthy", "S. Kasif", "S. Salzberg"],
    "venue": "trees. JAIR,",
    "year": 1994
  }, {
    "title": "Feature-budgeted random forest",
    "authors": ["F. Nan", "J. Wang", "V. Saligrama"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Pruning random forests for prediction on a budget",
    "authors": ["F. Nan", "J. Wang", "V. Saligrama"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Efficient non-greedy optimization of decision trees",
    "authors": ["M. Norouzi", "M. Collins", "M.A. Johnson", "D.J. Fleet", "P. Kohli"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Xnor-net: Imagenet classification using binary convolutional neural networks",
    "authors": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"],
    "venue": "In ECCV,",
    "year": 2016
  }, {
    "title": "Data mining with decision trees: theory and applications",
    "authors": ["L. Rokach", "O. Maimon"],
    "venue": "World scientific,",
    "year": 2014
  }, {
    "title": "Refining architectures of deep convolutional neural networks",
    "authors": ["S. Shankar", "D. Robertson", "Y. Ioannou", "A. Criminisi", "R. Cipolla"],
    "year": 2016
  }, {
    "title": "An optimal constrained pruning strategy for decision trees",
    "authors": ["H.D. Sherali", "A.G. Hobeika", "C. Jeenanunta"],
    "venue": "INFORMS Journal on Computing,",
    "year": 2009
  }, {
    "title": "Decision jungles: Compact and rich models for classification",
    "authors": ["J. Shotton", "T. Sharp", "P. Kohli", "S. Nowozin", "J. Winn", "A. Criminisi"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Perceptron trees: A case study in hybrid concept representations",
    "authors": ["P.E. Utgoff"],
    "venue": "Connection Science,",
    "year": 1989
  }, {
    "title": "A statistical approach to texture classification from single images",
    "authors": ["M. Varma", "A. Zisserman"],
    "year": 2005
  }, {
    "title": "Robust real-time face detection",
    "authors": ["P. Viola", "M.J. Jones"],
    "year": 2004
  }, {
    "title": "Efficient learning by directed acyclic graph for resource constrained prediction",
    "authors": ["J. Wang", "K. Trapeznikov", "V. Saligrama"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Deep distance metric learning with data summarization",
    "authors": ["W. Wang", "C. Chen", "W. Chen", "P. Rai", "L. Carin"],
    "venue": "In ECML PKDD,",
    "year": 2016
  }, {
    "title": "Quantized convolutional neural networks for mobile devices",
    "authors": ["J. Wu", "C. Leng", "Y. Wang", "Q. Hu", "J. Cheng"],
    "year": 2016
  }, {
    "title": "The greedy miser: Learning under test-time budgets",
    "authors": ["Z. Xu", "K.Q. Weinberger", "O. Chapelle"],
    "venue": "In ICML,",
    "year": 2012
  }, {
    "title": "Cost-sensitive tree of classifiers",
    "authors": ["Z.E. Xu", "M.J. Kusner", "K.Q. Weinberger", "M. Chen"],
    "venue": "In ICML,",
    "year": 2013
  }, {
    "title": "Groupsensitive multiple kernel learning for object categorization",
    "authors": ["J. Yang", "Y. Li", "Y. Tian", "L. Duan", "W. Gao"],
    "venue": "In ICCV,",
    "year": 2009
  }, {
    "title": "Decision tree pruning via integer programming",
    "authors": ["Y. Zhang", "H. Huei-chuen"],
    "venue": "Technical report,",
    "year": 2005
  }],
  "id": "SP:db3e4b11a569fe17908417351678525f6304e7b7",
  "authors": [{
    "name": "Ashish Kumar",
    "affiliations": []
  }, {
    "name": "Saurabh Goyal",
    "affiliations": []
  }, {
    "name": "Manik Varma",
    "affiliations": []
  }],
  "abstractText": "This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30% higher than stateof-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing’s L3 ranker when the model size is restricted to 300 bytes. Bonsai’s code can be downloaded from (BonsaiCode).",
  "title": "Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things"
}