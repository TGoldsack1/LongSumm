{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 718–728 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1067\nAccurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutchik’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%."
  }, {
    "heading": "1 Introduction",
    "text": "According to the Oxford English Dictionary, emotion is defined as “[a] strong feeling deriving from one’s circumstances, mood, or relationships with others.” 1 This “standard” definition identifies emotions as constructs involving something innate that is often invoked in social interactions and that aids in communicating with others(Hwang and Matsumoto, 2016). It is no exaggeration that humans are emotional beings: Emotions are an integral part of human life, and affect our decision making as well as our mental and physical health. As such, developing emotion detection models is important; they have a wide array of applications, ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health.\n1https://en.oxforddictionaries.com/ definition/emotion.\nHowever, emotion detection has remained a challenging task, partly due to the limited availability of labeled data and partly due the controversial nature of what emotions themselves are (Aaron C. Weidman and Tracy, 2017).\nRecent advances in machine learning for natural language processing (NLP) suggest that, given enough labeled data, there should be an opportunity to build better emotion detection models. Manual labeling of data, however, is costly and so it is desirable to develop labeled emotion data without annotators. While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags (Mohammad and Kiritchenko, 2015), such labels are noisy and reliable.\nIn this work, we seek to enable deep learning by creating a large dataset of fine-grained emotions using Twitter data. More specifically, we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision (Mintz et al., 2009) (the use of hashtags as a surrogate for annotator-generated emotion labels) to build emotion models grounded in psychology. We construct such a dataset and exploit it using powerful deep learning methods to build accurate, high coverage models for emotion prediction. Overall, we make the following contributions: 1) Grounded in psychological theory of emotions, we build a large-scale, high quality dataset of tweets labeled with emotions. Key to this are methods to ensure data quality, 2) we validate the data collection method using human annotations, 3) we develop powerful deep learning models using a gated recurrent network to exploit the data, yielding new state-of-the-art on 24 fine-grained types of emotions, and 4) we extend the task beyond these emotion types to model Plutick’s 8 primary emotion dimensions.\n718\nOur emotion modeling relies on distant supervision (Read, 2005; Mintz et al., 2009), the approach of using cues in data (e.g., hashtags or emoticons) as a proxy for “ground truth” labels as we explained above. Distant supervision has been investigated by a number of researchers for emotion detection (Tanaka et al., 2005; Mohammad, 2012; Purver and Battersby, 2012; Wang et al., 2012; Pak and Paroubek, 2010; Yang et al., 2007) and for other semantic tasks such as sentiment analysis (Read, 2005; Go et al., 2009) and sarcasm detection (González-Ibánez et al., 2011). In these works, authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance. We take a similar approach, using a larger collection of tweets, richer emotion definitions, and stronger filtering for tweet quality.\nThe remainder of the paper is organized as follows: We first overview related literature in Section 2, describe our data collection in Section 3.1, and the annotation study we performed to validate our distant supervision method in Section 4. We then describe our methods in Section 5, provide results in Section 6, and conclude in Section 8."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Computational Treatment of Emotion",
    "text": "The SemEval-2007 Affective Text task (Strapparava and Mihalcea, 2007) [SEM07] focused on classification of emotion and valence (i.e., positive and negative texts) in news headlines. A total of 1,250 headlines were manually labeled with the 6 basic emotions of Ekman (Ekman, 1972) and made available to participants. Similarly, (Aman and Szpakowicz, 2007) describe an emotion annotation task of identifying emotion category, emotion intensity and the words/phrases that indicate emotion in blog post data of 4,090 sentences and a system exploiting the data. Our work differs from both that of SEM07 (Strapparava and Mihalcea, 2007) and (Aman and Szpakowicz, 2007) in that we focus on a different genre (i.e., Twitter) and investigate distant supervision as a way to acquire a significantly larger labeled dataset.\nOur work is similar to (Mohammad, 2012; Mohammad and Kiritchenko, 2015), (Wang et al., 2012), and (Volkova and Bachrach, 2016) who use distant supervision to acquire Twitter data with emotion hashtags and report analyses and experiments to validate the utility of this approach. For\nexample, (Mohammad, 2012) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval-2007 (Strapparava and Mihalcea, 2007) dataset. As the author points out, this is another premise that the selflabeled hashtags acquired from Twitter are consistent, to some degree, with the emotion labels given by the trained human judges who labeled the SemEval-2007 data. As pointed out earlier, (Wang et al., 2012) randomly sample a set of 400 tweets from their data and human-label as relevant/irrelevant, as a way to verify the distant supervision approach with the quality assurance heuristics they employ. The authors found that the precision on a test set is 93.16%, thus confirming the utility of the heuristics. (Wang et al., 2012) provide a number of important observations, as conclusions based on their work. These include that since they are provided by the tweets’ writers, the emotion hashtags are more natural and reliable than the emotion labels traditionally assigned by annotators to data by a few annotators. This is the case since in the lab-condition method annotators need to infer the writers emotions from text, which may not be accurate. Additionally, (Volkova and Bachrach, 2016) follow the same distant supervision approach and find correlations of users’ emotional tone and the perceived demographics of these users’ social networks exploiting the emotion hashtag-labeled data. Our dataset is more than an order of magnitude larger than (Mohammad, 2012) and (Volkova and Bachrach, 2016) and the range of emotions we target is much more fine grained than (Mohammad, 2012; Wang et al., 2012; Volkova and Bachrach, 2016) since we model 24 emotion types, rather than focus on ≤ 7 basic emotions.\n(Yan et al., 2016; Yan and Turtle, 2016a,b) develop a dataset of 15,553 tweets labeled with 28 emotion types and so target a fine-grained range as we do. The authors instruct human annotators under lab conditions to assign any emotion they feel is expressed in the data, allowing them to assign more than one emotion to a given tweet. A set of 28 chosen emotions was then decided upon and further annotations were performed using Amazon Mechanical Turk (AMT). The authors cite an agreement of 0.50 Krippendorff’s alpha (α) between the lab/expert annotators, and an (α) of 0.28 between experts and AMT workers. EmoTweet-\n28 is a useful resource. However, the agreement between annotators is not high and the set of assigned labels do not adhere to a specific theory of emotion. We use a much larger dataset and report an accuracy of the hashtag approach at 90% based on human judgement as reported in Section 4."
  }, {
    "heading": "2.2 Mood",
    "text": "A number of studies have also been performed to analyze and/or model mood in social media data. (De Choudhury et al., 2012) identify more than 200 moods frequent on Twitter as extracted from psychological literature and filtered by AMT workers. They then collect tweets which have one of the moods in their mood lexicon in the form of a hashtag. To verify the quality of the mood data, the authors run AMT studies where they ask workers whether a tweet displayed the respective mood hashtag or not and find that in 83% of the cases hashtagged moods at the end of posts did capture users’ moods, whereas for posts with mood hashtags anywhere in the tweet, only 58% of the cases capture the mood of users. Although they did not build models for mood detection, the annotation studies (De Choudhury et al., 2012) perform further support our specific use of hashtags to label emotions. (Mishne and De Rijke, 2006) collect user-labeled mood from blog post text on LiveJournal and exploit them for predicting the intensity of moods over a time span rather than at the post level. Similarly, (Nguyen, 2010) builds models to infer patterns of moods in a large collection of LiveJournal posts. Some of the moods in these LiveJournal studies (e.g., hungry, cold), as (De Choudhury et al., 2012) explain, would not fit any psychological theory. Our work is different in that it is situated in psychological theory of emotion."
  }, {
    "heading": "2.3 Deep Learning for NLP",
    "text": "In spite of the effectiveness of feature engineering for NLP, it is a labor intensive task that also needs domain expertise. More importantly, feature engineering falls short of extracting and organizing all the discriminative information from data (LeCun et al., 2015; Goodfellow et al., 2016). Neural networks (Goodfellow et al., 2016) have emerged as a successful class of methods that has the power of automatically discovering the representations needed for detection or classification and has been successfully applied to multiple NLP tasks. A line of studies in the literature (e.g., (Labutov and Lip-\nson, 2013; Maas et al., 2011; Tang et al., 2014b,a) aim to learn sentiment-specific word embeddings (Bengio et al., 2003; Mikolov et al., 2013) from neighboring text. Another thread of research focuses on learning semantic composition (Mitchell and Lapata, 2010), including extensions to phrases and sentences with recursive neural networks (a class of syntax-tree models) (Socher et al., 2013; Irsoy and Cardie, 2014; Li et al., 2015) and to documents with distributed representations of sentences and paragraphs (Le and Mikolov, 2014; Tang et al., 2015) for modeling sentiment.\nLong-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Neural Nets (GRNNs) (Cho et al., 2014; Chung et al., 2015), variations of recurrent neural networks (RNNs), a type of networks suitable for handling time-series data like speech (Graves et al., 2013) or handwriting recognition (Graves, 2012; Graves and Schmidhuber, 2009), have also been used successfully for sentiment analysis (Ren et al., 2016; Liu et al., 2015; Tai et al., 2015; Tang et al., 2015; Zhang et al., 2016). Convolutional neural networks (CNNs) have also been quite successful in NLP, and have been applied to a range of sentence classification tasks, including sentiment analysis (Blunsom et al., 2014; Kim, 2014; Zhang et al., 2015). Other architectures have also been recently proposed (e.g., (Bradbury et al., 2016)). A review of neural network methods for NLP can be found in (Goldberg, 2016)."
  }, {
    "heading": "3 Data",
    "text": ""
  }, {
    "heading": "3.1 Collection of a Large-Scale Dataset",
    "text": "To be able to use deep learning for modeling emotion, we needed a large dataset of labeled tweets. Since there is no such human-labeled dataset publicly available, we follow (Mohammad, 2012; Mintz et al., 2009; Purver and Battersby, 2012; González-Ibánez et al., 2011; Wang et al., 2012) in adopting distant supervision: We collect tweets with emotion-carrying hashtags as a surrogate for emotion labels. To be able to collect enough tweets to serve our need, we developed a list of hashtags representing each of the 24 emotions proposed by Robert Plutchick (Plutchik, 1980, 1985, 1994). Plutchik (Plutchik, 2001) organizes emotions in a three-dimensional circumplex model analogous to the colors on a color wheel. The cone’s vertical dimension represents intensity, and the 3 circle represent degrees of similarity\namong the various emotion types. The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of opposites. Emotions in the blank spaces are the primary emotion dyads (i.e., emotions that are mixtures of two of the primary emotions). For this work, we exclude the dyads in the exploded model from our treatment. For simplicity, we refer to the circles as plutchik-1: with the emotions {admiration, amazement, ecstasy, grief, loathing, rage, terror, vigilance}, plutchik-2: with the emotions {joy, trust, fear, surprise, sadness, disgust, anger, anticipation}, and plutchik-3: with the emotions {acceptance, annoyance, apprehension, boredom, distraction, interest, pensiveness, serenity}. The wheel is shown in Figure 1.\nFor each emotion type, we prepared a seed set of hashtags representing the emotion. We used Google synonyms and other online dictionaries and thesauri (e.g., www.thesaurus. com) to expand the initial seed set of each emotion. We acquire a total of 665 emotion hashtags across the 24 emotion types. For example, for the joy emotion, a subset of the seeds in our expanded set is {“happy”, “happiness”, “joy”, “joyful”, “joyfully”, “delighted”, “feelingsunny”, “blithe”, “beatific”, “exhilarated”, “blissful”, “walkingonair”, “jubilant”}. We then used the expanded set to extract tweets with hashtags from the set from a number of massive-scale in-house Twitter datasets. We also used Twitter API to crawl Twitter with hashtags from the expanded set. Using this method, we were able to acquire a dataset of about 1/4 billion tweets covering an extended time span from July 2009 till January 2017."
  }, {
    "heading": "3.2 Preprocessing and Quality Assurance",
    "text": "Twitter data are very noisy, not only because of use of non-standard typography (which is less of a problem here) but due to the many duplicate tweets and the fact that tweets often have multiple emotion hashtags. Since these reduce our ability to build accurate models, we need to clean the data and remove duplicates. Starting with > 1/4 billion tweets, we employ a rigorous and strict pipeline. This results in a vastly smaller set of about 1.6 million dependable labeled tweets.\nSince our goal is to create non-overlapping categories at the level of a tweet, we first removed all tweets with hashtags belonging to more than one emotion of the 24 emotion categories. Since it was observed (e.g., (Mohammad, 2012; Wang et al., 2012)) and also confirmed by our annotation study as described in Section 4, that hashtags in tweets with URLs are less likely to correlate with a true emotion label, we remove all tweets with URLs from our data. We filter out duplicates using a two-step procedure: 1) we remove all retweets (based on existence of the token “RT” regardless of case) and 2) we use the Python library pandas http://pandas. pydata.org/ “drop duplicates” method to compare the tweet texts of all the tweets after normalizing character repetitions [all consecutive characters of > 2 to 2] and user mentions (as detected by a string starting with an “@” sign). We then performed a manual inspection of a random sample of 1,000 tweets from the data and found no evidence of any remaining tweet duplicates.\nNext, even though the emotion hashtags themselves are exclusively in English, we observe the data do have tweets in languages other than English. This is due to code-switching, but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non-English languages until 2012. To filter out non-English, we use the langid (Lui and Baldwin, 2012) (https://github.com/ saffsd/langid.py) library to assign language tags to the tweets. Since the common wisdom in the literature (e.g., (Mohammad, 2012; Wang et al., 2012)) is to restrict data to hashtags occurring in final position of a tweet, we investigate correlations between a tweet’s relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position. We also only use tweets con-\ntaining at least 5 words. Table 2 shows statistics of the data after applying our cleaning, filtering, language identification, and deduplication pipeline. Since our focus is on English, we only show statistics for tweets tagged with an “en” (for “English”) label by langid. Table 2 provides three types of relevant statistics: 1) counts of all tweets, 2) counts of tweets with at least 5 words and the emotion hashtags occurring in the last quarter of the tweet text (based on character count), and 3) counts of tweets with at least 5 words and the emotion hashtags occurring as the final word in the tweet text. As the last column in Table 2 shows, employing our most strict criterion where an emotion hashtag must occur finally in a tweet of a minimal length 5 words, we acquire a total of 1,608,233 tweets: 205,125 tweets for plutchik-1, 790,059 for plutchik-2, and 613,049 for plutchik-3. 2"
  }, {
    "heading": "4 Annotation Study",
    "text": "In their work, (Wang et al., 2012) manually label a random sample of 400 tweets extracted with hash-\n2The data can be acquired by emailing the first author. The distribution is in the form of tweet ids and labels, to adhere to Twitter conditions.\ntags in a similar way as we acquire our data and find that human annotators agree 93% of the time with the hashtag emotion type if the hashtag occurs as the last word in the tweet. We wanted to validate our use of hashtags in a similar fashion and on a bigger random sample. We had human annotators label a random sample of 5,600 tweets that satisfy our preprocessing pipeline. Manual inspection during annotation resulted in further removing a negligible 16 tweets that were found to have problems. For each of the remaining 5,584 tweets, the annotators assign a binary tag from the set {relevant, irrelevant} to indicate whether a tweet carries an emotion category as assigned using our distant supervision method or not. Annotators assigned 61.37% (n = 3, 427) “relevant” tags and 38.63% (n = 2, 157) “irrelevant” tags. Our analysis of this manually labeled dataset also supports the findings of (Wang et al., 2012): When we limit position of the emotion hashtag to the end of a tweet, we acquire 90.57% relevant data. We also find that if we relax the constraint on the hashtag position such that we allow the hashtag to occur in the last quarter of a tweet (based on a total tweet character count), we acquire 85.43% relevant tweets. We also find that only 23.20% (n = 795 out of 3, 427) of the emotion carrying tweets have the emotion hashtags occurring in final position, whereas 31.75% (n = 1, 088 out of 3, 427) of the tweets have the emotion hashtags in the last quarter of the tweet string. This shows how enforcing a final hashtag location results in loss of a considerable number of emotion tweets. As shown in Table 2, only 1, 608, 233 tweets out of a total of 6, 851, 955 tweets (% = 23, 47) in our bigger dataset have emotion hashtags occurring in final position. Overall, we agree with (Mohammad, 2012; Wang et al., 2012) that the accuracy acquired by enforcing a strict pipeline and limiting to emotion hashtags to final position is a reasonable measure for warranting good-quality data for training supervised systems, an assumption we have also validated with our empirical findings here.\nOne advantage of using distant supervision under these conditions for labeling emotion data, as (Wang et al., 2012) also notes, is that the label is assigned by the writer of the tweet himself/herself rather than an annotator who could wrongly decide what category a tweet is. After all, emotion is a fuzzy concept and > 90% agreement as we\nreport here is higher than the human agreement usually acquired on many NLP tasks. Another advantage of this method is obviously that it enables us to acquire a sufficiently large training set to use deep learning. We now turn to describing our deep learning methods."
  }, {
    "heading": "5 Methods",
    "text": "For our core modeling, we use Gated Recurrent Neural Networks (GRNNs), a modern variation of recurrent neural networks (RNNs), which we now turn to introduce. For notation, we denote scalars with italic lowercase (e.g., x), vectors with bold lowercase (e.g.,x), and matrices with bold uppercase (e.g.,W).\nRecurrent Neural Network A recurrent neural network (RNN) is one type of neural network architecture that is particularly suited for modeling sequential information. At each time step t, an RNN takes an input vector xt IRn and a hidden state vector h t−1 IRm and produces the next hidden state h t by applying the recursive operation:\nht = f (Wxt + Uht−1 + b) (1)\nWhere the input to hidden matrix W IRmxn, the hidden to hidden matrix U IRmxm, and the bias vector b IRm are parameters of an affine transformation and f is an element-wise nonlinearity. While an RNN can in theory summarize all historical information up to time step ht, in practice it runs into the problem of vanishing/exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) while attempting to learn longrange dependencies.\nLSTM Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) addresses this exact problem of learning long-term dependencies by augmenting an RNN with a memory cell ct IRn at each time step. As such, in addition to the input vector xt, the hiddent vector ht−1, an LSTM takes a cell state vector ct−1 and produces ht and ct via the following calculations:\nit = σ ( Wixt + Uiht−1 + bi ) ft = σ ( Wfxt + Ufht−1 + bf )\not = σ (Woxt + Uoht−1 + bo) gt = tanh (W\ngxt + Ught−1 + bg) ct = ft ct−1 + it gt ht = ot tanh(ct)\n(2)\nWhere σ(·) and tanh(·) are the element-wise sigmoid and hyperbolic tangent functions, the element-wise multiplication operator, and it, ft, ot are the input, forget, and output gates. The gt is a new memory cell vector with candidates that could be added to the state. The LSTM parameters Wj , Uj , and bj are for j {i, f, o, g}.\nGRNNs (Cho et al., 2014; Chung et al., 2015) propose a variation of LSTM with a reset gate rt, an update state zt, and a new simpler hidden unit ht, as follows:\nrt = σ (Wrxt + Urht−1 + br) zt = σ (Wzxt + Uzht−1 + bz) h̃t = tanh ( Wxt + rt ∗ Uh̃ht−1 + bh̃ )\nht = zt ∗ ht−1 + (1− zt) ∗ h̃t\n(3)\nThe GRNN parameters Wj , Uj , and bj are for j {r, z, h̃}. In this set up, the hidden state is forced to ignore a previous hidden state when the reset gate is close to 0, thus enabling the network to forget or drop irrelevant information. Additionally, the update gate controls how much information carries over from a previous hidden state to the current hidden state (similar to an LSTM memory cell). We use GRNNs as they are simpler and faster than LSTM. For GRNNs, we use Theano (Theano Development Team, 2016).\nOnline Classifiers We compare the performance of the GRNNs to four online classifiers that are capable of handling the data size: Stochastic Gradient Descent (SGD), Multinomial Naive Bayes (MNB), Perceptron, and the Passive Agressive Classifier (PAC). These classifiers learn online from mini-batches of data. We use minibatches of 10,000 instances with all the four classifiers. We use the scikit-learn implementation of these classifiers (http://scikit-learn. org).\nSettings We aim to model Plutchik’s 24 finegrained emotions as well as his 8 primary emotion dimensions where each 3 related types of emotion (perceived as varying in intensity) are combined in one dimension. We now turn to describing our experiments experiments."
  }, {
    "heading": "6 Experiments",
    "text": ""
  }, {
    "heading": "6.1 Predicting Fine-Grained Emotions",
    "text": "As explained earlier, Plutchik organizes the 24 emotion types in the 3 main circles that we will refer to as plutchik-1, plutchik-2, and plutchik-3.\nWe model the set of emotions belonging to each of the 3 circles independently, thus casting each as an 8-way classification task. Inspired by observations from the literature and our own annotation study, we limit our data to tweets of at least 5 words with an emotional hashtag occurring at the end. We then split the data representing each of the 3 circles into 80% training (TRAIN), 10% development (DEV), and 10% testing (TEST). As mentioned above, we run experiments with a range of online, out-of-core classifiers as well as the\nGRNNs. To train the GRNNs, we optimize the hyper-parameters of the network on a development set as we describe below, choosing a vocabulary size of 80K words (a vocabulary size we also use for the out-of-core classifiers), a word embedding vector of size 300 dimensions learnt directly from the training data, an input maximum length of 30 words, 7 epochs, and the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 0.001. We use 3 dense layers each with 1, 000 units. We use dropout (Hinton et al., 2012) for regularization, with a dropout rate of 0.5. For our loss function, we use categorical cross-entropy. We use a minibatch (Cotter et al., 2011) size of 128. We found this architecture to work best with almost all the settings and so we fix it across the board for all experiments with GRNNs.\nResults with Traditional Classifiers Results with the online classifiers are presented in terms of F-score in Table 3. As the table shows, among this group of classifiers, the Passive Agressive classifier (PAC) acquires the best performance. PAC achieves an overall F-score of 64.86% on plutchik-1, 53.30% on plutchik-2, and 68.14% on plutchik-3, two of which are higher than an arbitrary baseline3 of 60%.\nResults with GRNNs Table 4 presents results with GRNNs, compared with the best results using the traditional classifiers as acquired with PAC. As the table shows, the GRNN models are very successful across all the 3 classification tasks. With GRNNs, we acquire an overall F-scores of: 91.21% on plutchik-1, 82.32% on plutchik-2, and 87.47% on plutchik-3. These results are 26.35%, 29.02%, and 25.37% higher than PAC, respectively.\nNegative Results We experiment with aug3The arbitrary baseline is higher than the majority class in\nthe training data in any of the 3 cases.\nmenting training data reported here in two ways: 1) For each emotion type, we concatenate the training data with training data of tweets that are more (or less) intense from the same sector/dimension in the wheel, and 2) for each emotion type, we add tweets where emotion hashtags occur in the last quarter of a tweet (which were originally filtered out from TRAIN). However, we gain no improvements based on either of these methods, thus reflecting the importance of using high-quality training data and the utility of our strict pipeline."
  }, {
    "heading": "6.2 Predicting 8 Primary Dimensions",
    "text": "We now investigate the task of predicting each of the 8 primary emotion dimensions represented by the sectors of the wheel (where the three degrees of intensity of a given emotion are reduced to a single emotion dimension [e.g., {ecstasy, joy, serenity} are reduced to the joy dimension]). We concatenate the 80% training data (TRAIN) from each of the 3 circles’ data into a single training set\n(TRAIN-ALL), the 10% DEV to form DEV-ALL, and the 10% TEST to form TEST-ALL. We test a number of hyper-parameters on DEV and find the ones we have identified on the fine-grained prediction to work best and so we adopt them as is with the exception of limiting to only 2 epochs. We believe that with a wider exploration of hyperparameters, improvements could be possible. As Table 5 shows, we are able to model the 8 dimensions with an overall superior accuracy of 95.68%. As far as we know, this is the first work on modeling these dimensions."
  }, {
    "heading": "7 Comparisons to Other Systems",
    "text": "We compare our results on the 8 basic emotions to the published literature. As Table 6 shows, on this subset of emotions, our system is 4.53% (acc) higher than the best published results (Volkova and Bachrach, 2016), facilitated by the fact that we have an order of magnitude more training data. As shown in Table 7, we also apply (Volkova and Bachrach, 2016)’s pre-trained model on our test set of the 6 emotions they predict (which belong to plutchik-2), and acquire an overall accuracy of 26.95%, which is significantly lower than our accuracy."
  }, {
    "heading": "8 Conclusion",
    "text": "In this paper, we built a large, automatically curated dataset for emotion detection using distant supervision and then used GRNNs to model finegrained emotion, achieving a new state-of-the-art performance. We also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion."
  }],
  "year": 2017,
  "references": [{
    "title": "The jingle and jangle of emotion assessment: Imprecise measurement, casual scale usage, and conceptual fuzziness in emotion research",
    "authors": ["Conor M. Steckler Aaron C. Weidman", "Jessica L. Tracy."],
    "venue": "Emotion .",
    "year": 2017
  }, {
    "title": "Identifying expressions of emotion in text",
    "authors": ["Saima Aman", "Stan Szpakowicz."],
    "venue": "Text, Speech and Dialogue. Springer, pages 196–205.",
    "year": 2007
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of machine learning research 3(Feb):1137–1155.",
    "year": 2003
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."],
    "venue": "IEEE transactions on neural networks 5(2):157–166.",
    "year": 1994
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual",
    "year": 2014
  }, {
    "title": "Quasi-recurrent neural networks",
    "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher."],
    "venue": "arXiv preprint arXiv:1611.01576 .",
    "year": 2016
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "arXiv preprint",
    "year": 2014
  }, {
    "title": "Gated feedback recurrent neural networks",
    "authors": ["Junyoung Chung", "Caglar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICML. pages 2067–2075.",
    "year": 2015
  }, {
    "title": "Better mini-batch algorithms via accelerated gradient methods",
    "authors": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan."],
    "venue": "Advances in neural information processing systems. pages 1647–1655.",
    "year": 2011
  }, {
    "title": "Not all moods are created equal! exploring human emotional states in social media",
    "authors": ["Munmun De Choudhury", "Scott Counts", "Michael Gamon"],
    "year": 2012
  }, {
    "title": "Universal and cultural differences in facial expression of emotion",
    "authors": ["P. Ekman."],
    "venue": "Nebraska Symposium on Motivation pages 207–283.",
    "year": 1972
  }, {
    "title": "Twitter sentiment classification using distant supervision",
    "authors": ["Alec Go", "Richa Bhayani", "Lei Huang."],
    "venue": "CS224N Project Report, Stanford 1(12).",
    "year": 2009
  }, {
    "title": "A primer on neural network models for natural language processing",
    "authors": ["Yoav Goldberg."],
    "venue": "Journal of Artificial Intelligence Research 57:345–420.",
    "year": 2016
  }, {
    "title": "Identifying sarcasm in twitter: a closer look",
    "authors": ["Roberto González-Ibánez", "Smaranda Muresan", "Nina Wacholder."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:",
    "year": 2011
  }, {
    "title": "Deep learning",
    "authors": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."],
    "venue": "MIT Press.",
    "year": 2016
  }, {
    "title": "Supervised sequence labelling",
    "authors": ["Alex Graves."],
    "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, Springer, pages 5–13.",
    "year": 2012
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."],
    "venue": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, pages 6645–6649.",
    "year": 2013
  }, {
    "title": "Offline handwriting recognition with multidimensional recurrent neural networks",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Advances in neural information processing systems. pages 545–552.",
    "year": 2009
  }, {
    "title": "Improving neural networks by preventing coadaptation of feature detectors",
    "authors": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1207.0580 .",
    "year": 2012
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Emotional expression",
    "authors": ["Hyisung C Hwang", "David Matsumoto."],
    "venue": "The Expression of Emotion: Philosophical, Psychological and Legal Perspectives page 137.",
    "year": 2016
  }, {
    "title": "Deep recursive neural networks for compositionality in language",
    "authors": ["Ozan Irsoy", "Claire Cardie."],
    "venue": "Advances in Neural Information Processing Systems. pages 2096–2104.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882 .",
    "year": 2014
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Re-embedding words",
    "authors": ["Igor Labutov", "Hod Lipson."],
    "venue": "ACL (2). pages 489–493.",
    "year": 2013
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc V Le", "Tomas Mikolov."],
    "venue": "ICML. volume 14, pages 1188–1196.",
    "year": 2014
  }, {
    "title": "Deep learning",
    "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."],
    "venue": "Nature 521(7553):436–444.",
    "year": 2015
  }, {
    "title": "When are tree structures necessary for deep learning of representations",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eudard Hovy"],
    "year": 2015
  }, {
    "title": "Multi-timescale long shortterm memory neural network for modelling sentences and documents",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."],
    "venue": "EMNLP. Citeseer, pages 2326–2335.",
    "year": 2015
  }, {
    "title": "langid",
    "authors": ["Marco Lui", "Timothy Baldwin."],
    "venue": "py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 system demonstrations. Association for Computational Linguistics, pages 25–30.",
    "year": 2012
  }, {
    "title": "Learning word vectors for sentiment analysis",
    "authors": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-",
    "year": 2011
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."],
    "venue": "Hlt-naacl. volume 13, pages 746–751.",
    "year": 2013
  }, {
    "title": "Distant supervision for relation extraction without labeled data",
    "authors": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference",
    "year": 2009
  }, {
    "title": "Capturing global mood levels using blog posts",
    "authors": ["Gilad Mishne", "Maarten De Rijke."],
    "venue": "AAAI spring symposium: computational approaches to analyzing weblogs. pages 145–152.",
    "year": 2006
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive science 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "emotional tweets",
    "authors": ["Saif M Mohammad."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International",
    "year": 2012
  }, {
    "title": "Using hashtags to capture fine emotion categories from tweets",
    "authors": ["Saif M Mohammad", "Svetlana Kiritchenko."],
    "venue": "Computational Intelligence 31(2):301–326.",
    "year": 2015
  }, {
    "title": "Mood patterns and affective lexicon access in weblogs",
    "authors": ["Thin Nguyen."],
    "venue": "Proceedings of the ACL 2010 Student Research Workshop. Association for Computational Linguistics, pages 43–48.",
    "year": 2010
  }, {
    "title": "Twitter as a corpus for sentiment analysis and opinion mining",
    "authors": ["Alexander Pak", "Patrick Paroubek."],
    "venue": "LREc. volume 10.",
    "year": 2010
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "ICML (3) 28:1310–1318.",
    "year": 2013
  }, {
    "title": "Emotion: A psychoevolutionary synthesis",
    "authors": ["Robert Plutchik."],
    "venue": "Harpercollins College Division.",
    "year": 1980
  }, {
    "title": "On emotion: The chickenand-egg problem revisited",
    "authors": ["Robert Plutchik."],
    "venue": "Motivation and Emotion 9(2):197–200.",
    "year": 1985
  }, {
    "title": "The psychology and biology of emotion",
    "authors": ["Robert Plutchik"],
    "venue": "HarperCollins College Publishers",
    "year": 1994
  }, {
    "title": "The nature of emotions human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
    "authors": ["Robert Plutchik."],
    "venue": "American scientist 89(4):344–350.",
    "year": 2001
  }, {
    "title": "Experimenting with distant supervision for emotion classification",
    "authors": ["Matthew Purver", "Stuart Battersby."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational",
    "year": 2012
  }, {
    "title": "Using emoticons to reduce dependency in machine learning techniques for sentiment classification",
    "authors": ["Jonathon Read."],
    "venue": "Proceedings of the ACL student research workshop. Association for Computational Linguistics, pages 43–48.",
    "year": 2005
  }, {
    "title": "Context-sensitive twitter sentiment classification using neural network",
    "authors": ["Yafeng Ren", "Yue Zhang", "Meishan Zhang", "Donghong Ji."],
    "venue": "AAAI. pages 215–221.",
    "year": 2016
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"],
    "venue": "In Proceedings of the conference on",
    "year": 2013
  }, {
    "title": "Semeval2007 task 14: Affective text",
    "authors": ["Carlo Strapparava", "Rada Mihalcea."],
    "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, pages 70–74.",
    "year": 2007
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1503.00075 .",
    "year": 2015
  }, {
    "title": "Extraction and classification of facemarks",
    "authors": ["Yuki Tanaka", "Hiroya Takamura", "Manabu Okumura."],
    "venue": "Proceedings of the 10th international conference on Intelligent user interfaces. ACM, pages 28–34.",
    "year": 2005
  }, {
    "title": "Document modeling with gated recurrent neural network for sentiment classification",
    "authors": ["Duyu Tang", "Bing Qin", "Ting Liu."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1422–1432.",
    "year": 2015
  }, {
    "title": "Building large-scale twitter-specific sentiment lexicon: A representation learning approach",
    "authors": ["Duyu Tang", "Furu Wei", "Bing Qin", "Ming Zhou", "Ting Liu."],
    "venue": "COLING. pages 172–182.",
    "year": 2014
  }, {
    "title": "Learning sentimentspecific word embedding for twitter sentiment classification",
    "authors": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."],
    "venue": "ACL (1). pages 1555–1565.",
    "year": 2014
  }, {
    "title": "Theano: A Python framework for fast computation of mathematical expressions",
    "authors": ["Theano Development Team."],
    "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
    "year": 2016
  }, {
    "title": "Inferring perceived demographics from user emotional tone and user-environment emotional contrast",
    "authors": ["Svitlana Volkova", "Yoram Bachrach."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL.",
    "year": 2016
  }, {
    "title": "Harnessing twitter” big data” for automatic emotion identification",
    "authors": ["Wenbo Wang", "Lu Chen", "Krishnaprasad Thirunarayan", "Amit P Sheth."],
    "venue": "Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012 Interna-",
    "year": 2012
  }, {
    "title": "Exploring fine-grained emotion detection in tweets",
    "authors": ["Jasy Liew Suet Yan", "Howard R Turtle."],
    "venue": "Proceedings of NAACL-HLT . pages 73–80.",
    "year": 2016
  }, {
    "title": "Exposing a set of fine-grained emotion categories from tweets",
    "authors": ["Jasy Liew Suet Yan", "Howard R Turtle."],
    "venue": "25th International Joint Conference on Artificial Intelligence. page 8.",
    "year": 2016
  }, {
    "title": "Emotweet-28: A fine-grained emotion corpus for sentiment analysis",
    "authors": ["Jasy Liew Suet Yan", "Howard R Turtle", "Elizabeth D Liddy"],
    "year": 2016
  }, {
    "title": "Emotion classification using web blog corpora",
    "authors": ["Changhua Yang", "Kevin Hsin-Yih Lin", "Hsin-Hsi Chen."],
    "venue": "Web Intelligence, IEEE/WIC/ACM International Conference on. IEEE, pages 275–278.",
    "year": 2007
  }, {
    "title": "Gated neural networks for targeted sentiment analysis",
    "authors": ["Meishan Zhang", "Yue Zhang", "Duy-Tin Vo."],
    "venue": "AAAI. pages 3087–3093.",
    "year": 2016
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "Advances in neural information processing systems. pages 649–657.",
    "year": 2015
  }],
  "id": "SP:6a6f1b8ef3eefaf8027a76134859b44886a5f795",
  "authors": [{
    "name": "Muhammad Abdul-Mageed",
    "affiliations": []
  }, {
    "name": "Lyle Ungar",
    "affiliations": []
  }],
  "abstractText": "Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutchik’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.",
  "title": "EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks"
}