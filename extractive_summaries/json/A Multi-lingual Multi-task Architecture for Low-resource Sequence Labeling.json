{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 799–809 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n799"
  }, {
    "heading": "1 Introduction",
    "text": "When we use supervised learning to solve Natural Language Processing (NLP) problems, we typically train an individual model for each task with task-specific labeled data. However, our target task may be intrinsically linked to other tasks. For example, Part-of-speech (POS) tagging and Name Tagging can both be considered as sequence labeling; Machine Translation (MT) and Abstractive Text Summarization both require the ability to understand the source text and generate natural language sentences. Therefore, it is valuable to transfer knowledge from related tasks to the target task. Multi-task Learning (MTL) is one of\n∗* Part of this work was done when the first author was on an internship at Facebook.\n1The code of our model is available at https://github. com/limteng-rpi/mlmt\nthe most effective solutions for knowledge transfer across tasks. In the context of neural network architectures, we usually perform MTL by sharing parameters across models (Ruder, 2017).\nPrevious studies (Collobert and Weston, 2008; Dong et al., 2015; Luong et al., 2016; Liu et al., 2018; Yang et al., 2017) have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing. However, most of these previous efforts focused on tasks and languages which have sufficient labeled data but hit a performance ceiling on each task alone. Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages. According to Ethnologue2, there are 7, 099 living languages in the world. It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements. Furthermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource languages. Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from related high-resource languages and tasks.\nIn (Yang et al., 2017), the authors simulated a low-resource setting for English and Spanish by downsampling the training data for the target task. However, for most low-resource languages, the data sparsity problem also lies in related tasks and languages. Under such circumstances, a single transfer model can only bring limited improvement. To tackle this issue, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing. In the first level, we share character embeddings,\n2https://www.ethnologue.com/guides/ how-many-languages\ncharacter-level convolutional neural networks, and word-level long-short term memory layer across all models. These components serve as a basis to connect multiple models and transfer universal knowledge among them. In the second level, we adopt different sharing strategies for different transfer schemes. For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge (e.g., I-PER3 should not be assigned to the first word in a sentence).\nTo illustrate our idea, we take sequence labeling as a case study. In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., POS tag) to each token in a sentence. It underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and chunking.\nExperiments show that our model can effectively transfer various types of knowledge from different auxiliary tasks and obtains up to 50.5% absolute F-score gains on Name Tagging compared to the mono-lingual single-task baseline. Additionally, our approach does not rely on a large amount of auxiliary task data to achieve the improvement. Using merely 1% auxiliary data, we already obtain up to 9.7% absolute gains in Fscore."
  }, {
    "heading": "2 Model",
    "text": ""
  }, {
    "heading": "2.1 Basic Architecture",
    "text": "The goal of sequence labeling is to assign a categorical label to each token in a given sentence. Though traditional methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Ratinov and Roth, 2009; Passos et al., 2014) achieved high performance on sequence labeling tasks, they typically relied on hand-crafted features, therefore it is difficult to adapt them to new tasks or languages. To avoid task-specific engineering, (Collobert et al., 2011) proposed a feed-forward neural network model that only requires word embeddings trained on a large scale corpus as features. After that, several neural models based on the combination of long-short term memory (LSTM) and CRFs (Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016) were proposed and\n3We adopt the BIOES annotation scheme. Prefixes B-, I, E-, and S- represent the beginning of a mention, inside of a mention, the end of a mention and a single-token mention respectively. The O tag is assigned to a word which is not part of any mention.\nachieved better performance on sequence labeling tasks.\nLSTM-CRFs-based models are well-suited for multi-lingual multi-task learning for three reasons: (1) They learn features from word and character embeddings and therefore require little feature engineering; (2) As the input and output of each layer in a neural network are abstracted as vectors, it is fairly straightforward to share components between neural models; (3) Character embeddings can serve as a bridge to transfer morphological and semantic information between languages with identical or similar scripts, without requiring cross-lingual dictionaries or parallel sentences.\nTherefore, we design our multi-task multilingual architecture based on the LSTM-CNNs model proposed in (Chiu and Nichols, 2016). The overall framework is illustrated in Figure 1. First, each word wi is represented as the combination xi of two parts, word embedding and character feature vector, which is extracted from character embeddings of the characters in wi using convolutional neural networks (CharCNN). On top of that, a bidirectional LSTM processes the sequence x = {x1, x2, ...} in both directions and encodes each word and its context into a fixed-size vector hi. Next, a linear layer converts hi to a score vector yi, in which each component represents the predicted score of a target tag. In order to model correlations between tags, a CRFs layer is added at the top to generate the best tagging path for the whole sequence. In the CRFs layer, given an input sentence x of length L and the output of the linear layer y, the score of a sequence of tags z is\ndefined as:\nS(x,y, z) = L∑\nt=1\n(Azt−1,zt + yt,zt),\nwhere A is a transition matrix in which Ap,q represents the binary score of transitioning from tag p to tag q, and yt,z represents the unary score of assigning tag z to the t-th word. Given the ground truth sequence of tags z, we maximize the following objective function during the training phase:\nO = logP (z|x) = S(x,y, z)− log ∑ z̃∈Z eS(x,y,z̃),\nwhere Z is the set of all possible tagging paths. We emphasize that our actual implementation differs slightly from the LSTM-CNNs model. We do not use additional word- and characterlevel explicit symbolic features (e.g., capitalization and lexicon) as they may require additional language-specific knowledge. Additionally, we transform character feature vectors using highway networks (Srivastava et al., 2015), which is reported to enhance the overall performance by (Kim et al., 2016) and (Liu et al., 2018). Highway networks is a type of neural network that can smoothly switch its behavior between transforming and carrying information."
  }, {
    "heading": "2.2 Multi-task Multi-lingual Architecture",
    "text": "MTL can be employed to improve performance on multiple tasks at the same time, such as MT and parsing in (Luong et al., 2016). However, in our scenario, we only focused on enhancing the performance of a low-resource task, which is our target task or main task. Our proposed architecture aims to transfer knowledge from a set of auxiliary tasks to the main task. For simplicity, we refer to a model of a main (auxiliary) task as a main (auxiliary) model.\nTo jointly train multiple models, we perform multi-task learning using parameter sharing. Let Θi be the set of parameters for model mi and Θi,j = Θi ∩Θj be the shared parameters between mi and mj . When optimizing model mi, we update Θi and hence Θi,j . In this way, we can partially train model mj as Θi,j ⊆ Θj . Previously, each MTL model generally uses a single transfer scheme. In order to merge different transfer models into a unified architecture, we employ two levels of parameter sharing as follows.\nOn the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models. This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages.\nCharacter Embeddings and Character-level CNNs. Character features can represent morphological and semantic information; e.g., the English morpheme dis- usually indicates negation and reversal as in “disagree” and “disapproval”. For low-resource languages lacking in data to suffice the training of high-quality word embeddings, character embeddings learned from other languages may provide crucial information for labeling, especially for rare and out-of-vocabulary words. Take the English word “overflying” (flying over) as an example. Even if it is rare or absent in the corpus, we can still infer the word meaning from its suffix over- (above), root fly, and prefix -ing (present participle form). In our architecture, we share character embeddings and the CharCNN between languages with identical or similar scripts to enhance word representation for low-resource languages.\nBidirectional LSTM. The bidirectional LSTM layer is essential to extract character, word, and contextual information from a sentence. However, with a large number of parameters, it cannot be fully trained only using the low-resource task data. To tackle this issue, we share the bidirectional LSTM layer across all models. Bear in mind that because our architecture does not require aligned cross-lingual word embeddings, sharing this layer across languages may confuse the model as it equally handles embeddings in different spaces. Nevertheless, under low-resource circumstances, data sparsity is the most critical factor that affects the performance.\nOn top of this basis, we adopt different parameter sharing strategies for different transfer schemes. For cross-task transfer, we use the same word embedding matrix across tasks so that they can mutually enhance word representations. For cross-lingual transfer, we share the linear layer and CRFs layer among languages to transfer taskspecific knowledge, such as the transition score between two tags.\nWord Embeddings. For most words, in addition to character embeddings, word embeddings are still crucial to represent semantic informa-\ntion. We use the same word embedding matrix for tasks in the same language. The matrix is initialized with pre-trained embeddings and optimized as parameters during training. Thus, task-specific knowledge can be encoded into the word embeddings by one task and subsequently utilized by another one. For a low-resource language even without sufficient raw text, we mix its data with a related high-resource language to train word embeddings. In this way, we merge both corpora and hence their vocabularies.\nRecently, Conneau et al. (2017) proposed a domain-adversarial method to align two monolingual word embedding matrices without crosslingual supervision such as a bilingual dictionary. Although cross-lingual word embeddings are not required, we evaluate our framework with aligned embeddings generated using this method. Experiment results show that the incorporation of crosslingual embeddings substantially boosts the performance under low-resource settings.\nLinear Layer and CRFs. As the tag set varies from task to task, the linear layer and CRFs can only be shared across languages. We share these layers to transfer task-specific knowledge to the main model. For example, our model corrects [SPER Charles] [S-PER Picqué] to [B-PER Charles] [E-PER Picqué] because the CRFs layer fully trained on other languages assigns a low score to the rare transition S-PER→S-PER and promotes B-PER→E-PER. In addition to the shared linear layer, we add an unshared language-specific linear layer to allow the model to behave differently\ntoward some features for different languages. For example, the suffix -ment usually indicates nouns in English whereas indicates adverbs in French.\nWe combine the output of the shared linear layer yu and the output of the language-specific linear layer ys using:\ny = g ⊙ ys + (1− g)⊙ yu,\nwhere g = σ(W gh + bg). W g and bg are optimized during training. h is the LSTM hidden states. As W g is a square matrix, y, ys, and yu have the same dimension. Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks with slight modification. For example, for text classification tasks, we can take the last hidden state of the forward LSTM as the sentence representation and replace the CRFs layer with a Softmax layer.\nIn our model, each task has a separate object function. To optimize multiple tasks within one model, we adopt the alternating training approach in (Luong et al., 2016). At each training step, we sample a task di with probability ri∑\nj rj , where ri\nis the mixing rate value assigned to di. In our experiments, instead of tuning ri, we estimate it by:\nri = µiζi √ Ni ,\nwhere µi is the task coefficient, ζi is the language coefficient, and Ni is the number of training examples. µi (or ζi) takes the value 1 if the task\n(or language) of di is the same as that of the target task; Otherwise it takes the value 0.1. For example, given English Name Tagging as the target task, the task coefficient µ and language coefficient ζ of Spanish Name Tagging are 0.1 and 1 respectively.\nWhile assigning lower mixing rate values to auxiliary tasks, this formula also takes the amount of data into consideration. Thus, auxiliary tasks receive higher probabilities to reduce overfitting when we have a smaller amount of main task data."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Data Sets",
    "text": "For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task (Tjong Kim Sang, 2002), English (ENG) data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003), Russian (RUS) data from LDC2016E95 (Russian Representative Language Pack), and Chechen (CHE) data from TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus4. We select Chechen as another target language in addition to Dutch and Spanish because it is a truly under-resourced language and its related language, Russian, also lacks NLP resources.\nFor POS Tagging, we use English, Dutch, Spanish, and Russian data from the CoNLL 2017 shared task (Zeman et al., 2017; Nivre et al., 2017). In this data set, each token is annotated with two POS tags, UPOS (universal POS tag) and XPOS (language-specific POS tag). We use UPOS because it is consistent throughout all languages."
  }, {
    "heading": "3.2 Experimental Setup",
    "text": "We use 50-dimensional pre-trained word embeddings and 50-dimensional randomly initialized character embeddings. We train word embeddings using the word2vec package5. English, Span-\n4https://tac.nist.gov/2017/KBP/data.html 5https://github.com/tmikolov/word2vec\nish, and Dutch embeddings are trained on corresponding Wikipedia articles (2017-12-20 dumps). Russian embeddings are trained on documents in LDC2016E95. Chechen embeddings are trained on documents in TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus. To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings, we use the unsupervised model in the MUSE library6 (Conneau et al., 2017). Although word embeddings are fine-tuned during training, we update the embedding matrix in a sparse way and thus do not have to update a large number of parameters.\nWe optimize parameters using Stochastic Gradient Descent with momentum, gradient clipping and exponential learning rate decay. At step t, the learning rate αt is updated using αt = α0 ∗ ρt/T , where α0 is the initial learning rate, ρ is the decay rate, and T is the decay step.7 To reduce overfitting, we apply Dropout (Srivastava et al., 2014) to the output of the LSTM layer.\nWe conduct hyper-parameter optimization by exploring the space of parameters shown in Table 2 using random search (Bergstra and Bengio, 2012). Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples. We select the set of parameters that achieves the best performance on the development set and apply it to all models."
  }, {
    "heading": "3.3 Comparison of Different Models",
    "text": "In Figure 3, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model (denoted as baseline), cross-task transfer model, and cross-lingual transfer model in low-resource settings with Dutch, Spanish, and Chechen Name Tagging as the main task respectively. We use English as the related language for Dutch and Spanish, and use Russian as the related language for\n6https://github.com/facebookresearch/MUSE 7Momentum β, gradient clipping threshold, ρ, and T are\nset to 0.9, 5.0, 0.9, and 10000 in the experiments.\nChechen. For cross-task transfer, we take POS Tagging as the auxiliary task. Because the CoNLL 2017 data does not include Chechen, we only use Russian POS Tagging and Russian Name Tagging as auxiliary tasks for Chechen Name Tagging.\nWe take Name Tagging as the target task for three reasons: (1) POS Tagging has a much lower requirement for the amount of training data. For example, using only 10 training sentences, our baseline model achieves 75.5% and 82.9% prediction accuracy on Dutch and Spanish; (2) Compared to POS Tagging, Name Tagging has been considered as a more challenging task; (3) Existing POS Tagging resources are relatively richer than Name Tagging ones; e.g., the CoNLL 2017 data set provides POS Tagging training data for 45 languages. Name Tagging also has a higher annotation cost as its annotation guidelines are usually more complicated.\nWe can see that our model substantially outperforms the mono-lingual single-task baseline model and obtains visible gains over single transfer models. When trained with less than 50 main tasks training sentences, cross-lingual transfer consistently surpasses cross-task transfer, which is not surprising because in the latter scheme, the linear layer and CRFs layer of the main model are not shared with other models and thus cannot be fully trained with little data.\nBecause there are only 20,400 sentences in Chechen documents, we also experiment with the data augmentation method described in Section 2.2 by training word embeddings on a mixture of Russian and Chechen data. This method yields additional 3.5%-10.0% absolute F-score gains. We also experiment with transferring from English to Chechen. Because Chechen uses Cyrillic alphabet , we convert its data set to Latin script. Surprisingly, although these two languages are not close, we get more improvement by using English as the auxiliary language.\nIn Table 3, we compare our model with state-ofthe-art models using all Dutch or Spanish Name Tagging data. Results show that although we design this architecture for low-resource settings, it also achieves good performance in high-resource settings. In this experiment, with sufficient training data for the target task, we perform another round of parameter sweeping. We increase the embedding sizes and LSTM hidden state size to 100 and 225 respectively."
  }, {
    "heading": "3.4 Qualitative Analysis",
    "text": "In Table 4, we compare Name Tagging results from the baseline model and our model, both trained with 100 main task sentences.\nThe first three examples show that shared character-level networks can transfer different levels of morphological and semantic information.\nIn example #1, the baseline model fails to identify “Palestijnen”, an unseen word in the Dutch data, while our model can recognize it because the shared CharCNN represents it in a way similar to its corresponding English word “Palestinians”, which occurs 20 times. In addition to mentions, the shared CharCNN can also improve representations of context words, such as “staat” (state) in the example. For some words dissimilar to corresponding English words, the CharCNN may enhance their word representations by transferring morpheme-level knowledge. For example, in sentence #2, our model is able to identify “Rusland” (Russia) as the suffix -land is usually associated with location names in the English data; e.g., Finland. Furthermore, the CharCNN is capable of capturing some word-level patterns, such as capitalized hyphenated compound and acronym as example #3 shows. In this sentence, neither “PMScentra” nor “MST” can be found in auxiliary task data, while we observe a number of similar expressions, such as American-style and LDP.\nThe transferred knowledge also helps reduce overfitting. For example, in sentence #4, the baseline model mistakenly tags “sección” (section) and “consellerı́a” (department) as organizations because their capitalized forms usually appear in Spanish organization names. With knowledge learned in auxiliary tasks that a lowercased word is rarely tagged as a proper noun, our model is able to avoid overfitting and correct these errors. Sentence #5 shows an opposite situation, where the capitalized word “campesinos” (farm worker) never appears in Spanish names.\nIn Table 5, we show differences between cross-\nlingual transfer and cross-task transfer. Although the cross-task transfer model recognizes “Ingeborg Marx” missed by the baseline model, it mistakenly assigns an S-PER tag to “Marx”. Instead, from English Name Tagging, the cross-lingual transfer model borrows task-specific knowledge through the shared CRFs layer that (1) B-PER→SPER is an invalid transition, and (2) even if we assign S-PER to “Ingeborg”, it is rare to have continuous person names without any conjunction or punctuation. Thus, the cross-lingual model promotes the sequence B-PER→E-PER.\nIn Figure 6, we depict the change of tag distribution with the number of training sentences. When trained with less than 100 sentences, the baseline model only correctly predicts a few tags dominated by frequent types. By contrast, our model has a visibly higher recall and better predicts infrequent tags, which can be attributed to the implicit data augmentation and inductive bias introduced by MTL (Ruder, 2017). For example, if all location names in the Dutch training data are single-token ones, the baseline model will inevitably overfit to the tag S-LOC and possibly label “Caldera de Taburiente” as [S-LOC Caldera] [S-LOC de] [S-LOC Taburiente], whereas with the shared CRFs layer fully trained on English Name Tagging, our model prefers B-LOC→I-LOC→ELOC, which receives a higher transition score."
  }, {
    "heading": "3.5 Ablation Studies",
    "text": "In order to quantify the contributions of individual components, we conduct ablation studies on Dutch Name Tagging with different numbers of training sentences for the target task. For the basic model, we we use separate LSTM layers and\nmodel (B), and result of our model (A). The GREEN ( RED ) highlight indicates a correct (incorrect) tag.\nremove the character embeddings, highway networks, language-specific layer, and Dropout layer. As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data. For example, the language-specific layer slightly impairs the performance with only 10 training sentences. However, this is unsurpris-\ning as it introduces additional parameters that are only trained by the target task data."
  }, {
    "heading": "3.6 Effect of the Amount of Auxiliary Task Data",
    "text": "For many low-resource languages, their related languages are also low-resource. To evaluate our model’s sensitivity to the amount of auxiliary task data, we fix the size of main task data and downsample all auxiliary task data with sample rates from 1% to 50%. As Figure 7 shows, the performance goes up when we raise the sample rate from\n1% to 20%. However, we do not observe significant improvement when we further increase the sample rate. By comparing scores in Figure 3 and Figure 7, we can see that using only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains. Due to space limitations, we only show curves for Dutch Name Tagging, while we observe similar results on other tasks. Therefore, we may conclude that our model does not heavily rely on the amount of auxiliary task data."
  }, {
    "heading": "4 Related Work",
    "text": "Multi-task Learning has been applied in different NLP areas, such as machine translation (Luong et al., 2016; Dong et al., 2015; Domhan and Hieber, 2017), text classification (Liu et al., 2017), dependency parsing (Peng et al., 2017), textual entailment (Hashimoto et al., 2017), text summarization (Isonuma et al., 2017) and sequence labeling (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Rei, 2017; Peng and Dredze, 2017; Yang et al., 2017; von Däniken and Cieliebak, 2017; Aguilar et al., 2017; Liu et al., 2018)\nCollobert and Weston (2008) is an early attempt that applies MTL to sequence labeling. The authors train a CNN model jointly on POS Tagging, Semantic Role Labeling, Name Tagging, chunking, and language modeling using parameter sharing. Instead of using other sequence labeling tasks, Rei (2017) and Liu et al. (2018) take language modeling as the secondary training objective to extract semantic and syntactic knowledge from large scale raw text without additional supervision. In (Yang et al., 2017), the authors propose three transfer models for crossdomain, cross-application, and cross-lingual trans-\nfer for sequence labeling, and also simulate a lowresource setting by downsampling the training data. By contrast, we combine cross-task transfer and cross-lingual transfer within a unified architecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously. In addition, because our model is designed for lowresource settings, we share components among models in a different way (e.g., the LSTM layer is shared across all models). Differing from most MTL models, which perform supervisions for all tasks on the outermost layer, (Søgaard and Goldberg, 2016) proposes an MTL model which supervised tasks at different levels. It shows that supervising low-level tasks such as POS Tagging at lower layer obtains better performance."
  }, {
    "heading": "5 Conclusions and Future Work",
    "text": "We design a multi-lingual multi-task architecture for low-resource settings. We evaluate the model on sequence labeling tasks with three language pairs. Experiments show that our model can effectively transfer different types of knowledge to improve the main model. It substantially outperforms the mono-lingual single-task baseline model, cross-lingual transfer model, and crosstask transfer model.\nThe next step of this research is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction. We also plan to explore the possibility of integrating incremental learning into this architecture to adapt a trained model for new tasks rapidly."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on."
  }],
  "year": 2018,
  "references": [{
    "title": "Random search for hyper-parameter optimization",
    "authors": ["James Bergstra", "Yoshua Bengio."],
    "venue": "Journal of Machine Learning Research, 13(Feb):281–305.",
    "year": 2012
  }, {
    "title": "Named entity recognition with bidirectional LSTM-CNNs",
    "authors": ["Jason P.C. Chiu", "Eric Nichols."],
    "venue": "TACL, 4:357–370.",
    "year": 2016
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "ICML.",
    "year": 2008
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, 12(Aug):2493–2537.",
    "year": 2011
  }, {
    "title": "Word translation without parallel data",
    "authors": ["Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou"],
    "venue": "arXiv preprint arXiv:1710.04087",
    "year": 2017
  }, {
    "title": "Transfer learning and sentence level features for named entity recognition on tweets",
    "authors": ["Pius von Däniken", "Mark Cieliebak."],
    "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text.",
    "year": 2017
  }, {
    "title": "Using targetside monolingual data for neural machine translation through multi-task learning",
    "authors": ["Tobias Domhan", "Felix Hieber."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Multi-task learning for multiple language translation",
    "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Multilingual language processing from bytes",
    "authors": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."],
    "venue": "NAACL HLT.",
    "year": 2016
  }, {
    "title": "A joint many-task model: Growing a neural network for multiple nlp tasks",
    "authors": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka", "Richard Socher"],
    "year": 2017
  }, {
    "title": "Extractive summarization using multi-task learning with document classification",
    "authors": ["Masaru Isonuma", "Toru Fujino", "Junichiro Mori", "Yutaka Matsuo", "Ichiro Sakata."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "Character-aware neural language models",
    "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."],
    "venue": "AAAI.",
    "year": 2016
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "ICML.",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "NAACL HLT.",
    "year": 2016
  }, {
    "title": "Empower sequence labeling with task-aware neural language model",
    "authors": ["Liyuan Liu", "Jingbo Shang", "Frank Xu", "Xiang Ren", "Huan Gui", "Jian Peng", "Jiawei Han."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "Adversarial multi-task learning for text classification",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "ICLR.",
    "year": 2016
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Universal dependencies 2.1. LINDAT/CLARIN digital library at the Institute of Formal and Ap",
    "authors": ["Wallin", "Jonathan North Washington", "Mats Wirén", "Tak-sum Wong", "Zhuoran Yu", "Zdeněk Žabokrtský", "Amir Zeldes", "Daniel Zeman", "Hanzhi Zhu"],
    "year": 2017
  }, {
    "title": "Lexicon infused phrase embeddings for named entity resolution",
    "authors": ["Alexandre Passos", "Vineet Kumar", "Andrew McCallum."],
    "venue": "CoNLL.",
    "year": 2014
  }, {
    "title": "Deep multitask learning for semantic dependency parsing",
    "authors": ["Hao Peng", "Sam Thomson", "Noah A Smith."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Multi-task domain adaptation for sequence tagging",
    "authors": ["Nanyun Peng", "Mark Dredze."],
    "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP.",
    "year": 2017
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["Lev Ratinov", "Dan Roth."],
    "venue": "CoNLL.",
    "year": 2009
  }, {
    "title": "Semi-supervised multitask learning for sequence labeling",
    "authors": ["Marek Rei."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "An overview of multi-task learning in deep neural networks",
    "authors": ["Sebastian Ruder."],
    "venue": "arXiv preprint arXiv:1706.05098.",
    "year": 2017
  }, {
    "title": "Deep multi-task learning with low level tasks supervised at lower layers",
    "authors": ["Anders Søgaard", "Yoav Goldberg."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "The Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "Highway networks",
    "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
    "authors": ["Erik F. Tjong Kim Sang."],
    "venue": "COLING.",
    "year": 2002
  }, {
    "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder."],
    "venue": "NAACL HLT.",
    "year": 2003
  }, {
    "title": "Transfer learning for sequence tagging with hierarchical recurrent networks",
    "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William W Cohen."],
    "venue": "ICLR.",
    "year": 2017
  }],
  "id": "SP:5332ba8db0c732f8cb9207375d10cfea45f27a72",
  "authors": [{
    "name": "Ying Lin",
    "affiliations": []
  }, {
    "name": "Shengqi Yang",
    "affiliations": []
  }, {
    "name": "Veselin Stoyanov",
    "affiliations": []
  }, {
    "name": "Heng Ji",
    "affiliations": []
  }],
  "abstractText": "We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute Fscore gains compared to the mono-lingual single-task baseline model. 1",
  "title": "A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling"
}