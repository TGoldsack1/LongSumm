{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Real-time and accurate prediction on resource-constrained devices is critical for several Machine Learning (ML) do-\n1Microsoft Research, India 2Carnegie Mellon University, Pittsburgh 3University of Michigan, Ann Arbor 4IIT Delhi, India. Correspondence to: Arun Sai Suggala <asuggala@andrew.cmu.edu>, Prateek Jain <prajain@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nmains. Internet-of-things (IoT) is one such rapidly growing domain. IoT devices have the potential to provide realtime, local, sensor-based solutions for a variety of areas like housing, factories, farming, even everyday utilities like toothbrushes and spoons. The ability to use machine learning on data collected from IoT sensors opens up a myriad of possibilities. For example, smart factories measure temperature, noise and various other parameters of their machines. ML based anomaly detection models can then be applied on this sensor data to preemptively schedule maintenance of a machine and avoid failure.\nHowever, machine learning in IoT scenarios is so far limited to cloud-based predictions where large deep learning models are deployed to provide accurate predictions. The sensors/embedded devices have limited compute/storage abilities and are tasked only with sensing and transmitting data to the cloud. Such a solution does not take into account several practical concerns like privacy, bandwidth, latency and battery issues. For example, consider the energy costs of communication if each IoT device on each machine in a smart factory has to continuously send data and receive predictions from the cloud.\nConsider a typical IoT device that has ≤ 32kB RAM and a 16MHz processor. Most existing ML models cannot be deployed on such tiny devices. Recently, several methods (Han et al., 2016; Nan et al., 2015; Kusner et al., 2014) have been proposed to produce models that are compressed compared to large DNN/kernel-SVM/decision-tree based classifiers. However, none of these methods work well at the scale of IoT devices. Moreover, they do not offer natural extensions to supervised learning problems other than the ones they were initially designed for.\nIn this paper, we propose a novel kNN based algorithm (ProtoNN) that can be deployed on the tiniest of devices, can handle general supervised learning problems, and can produce state-of-the-art accuracies with just ≈16kB of model size on many benchmark datasets. A key reason for selecting kNN as the algorithm of choice is due to its generality, ease of implementation on tiny devices, and small number of parameters to avoid overfitting. However, kNN suffers from three issues which limit its applicability in\npractice, especially in the small devices setting: a) Poor accuracy: kNN is an ill-specified algorithm as it is not a priori clear which distance metric one should use to compare a given set of points. Standard metrics like Euclidean distance, `1 distance etc. are not task-specific and lead to poor accuracies. b) Model size: kNN requires the entire training data for prediction, so its model size is too large for the IoT setting. c) Prediction time: kNN requires computing the distance of a given test point w.r.t. each training point, making it prohibitive for prediction in real-time.\nSeveral methods have been proposed to address some of these concerns. For example, metric learning (Weinberger & Saul, 2009) learns a task-specific metric that provides better accuracies but ends up increasing model-size and prediction time. KD-trees (Bentley, 1975) can decrease the prediction time, but they increase the model size and lead to loss in accuracy. Finally, recent methods like Stochastic Neighborhood Compression (SNC) (Kusner et al., 2014) can decrease model size and prediction time by learning a small number of prototypes to represent the entire training dataset. However, as our experiments show, their predictions are relatively inaccurate, especially in the tiny modelsize regime. Moreover, their formulations limit applicability to binary and multi-class classification problems (see Section 2 for a detailed comparison to SNC).\nIn contrast, ProtoNN is able to address the abovementioned concerns by using three key ideas:\na) Sparse low-d projection: we project the entire data in low-d using a sparse projection matrix that is jointly learned to provide good accuracy in the projected space.\nb) Prototypes: we learn prototypes to represent the entire training dataset. Moreover, we learn labels for each prototype to further boost accuracy. This provides additional flexibility, and allows us to seamlessly generalize ProtoNN for multi-label or ranking problems.\nc) Joint optimization: we learn the projection matrix jointly with the prototypes and their labels. Explicit sparsity constraints are imposed on our parameters during the optimization itself so that we can obtain an optimal model within the given model size de-facto, instead of post-facto pruning to force the model to fit in memory.\nUnfortunately, our optimization problem is non-convex with hard `0 constraints. Yet, we show that simple stochastic gradient descent (SGD) with iterative hard-thresholding (IHT) works well for optimization. ProtoNN can be implemented efficiently, can handle datasets with millions of points, and obtains state-of-the-art accuracies.\nWe analyze ProtoNN in a simple binary classification setting where the data is sampled from a mixture of two wellseparated Gaussians, each Gaussian representing one class.\nWe show that if we fix the projection matrix and prototype labels, the prototypes themselves can be learned optimally in polynomial time with at least a constant probability. Moreover, assuming a strong initialization condition we observe that our SGD+IHT method when supplied a small number of samples, proportional to the sparsity of means, converges to the global optima. Although the data model is simple, it nicely captures the main idea behind our problem formulation. Further, our analysis is the first such analysis for any method in this regime that tries to learn a compressed non-linear model for binary classification.\nFinally, we conduct extensive experiments to benchmark ProtoNN against existing state-of-the-art methods for various learning tasks. First, we show that on several binary (multi-class) problems, ProtoNN with a 2kB (16kB) memory budget significantly outperforms all the existing methods in this regime. Moreover, in the binary classification case, we show that ProtoNN with just ≈ 16kB of model-size, provides nearly the same accuracy as most popular methods like GBDT, RBF-SVM, 1-hidden layer NN, etc, which might require up to 1GB of RAM on the same datasets. Similarly, on multilabel datasets, ProtoNN can give 100× compression with ≤ 1% loss in accuracy. Finally, we demonstrate that ProtoNN can be deployed on a tiny Arduino Uno device1 and leads to better accuracies than existing methods while incurring significantly lesser energy and prediction time costs. We have implemented ProtoNN as part of an open source embedded device ML library and it can be downloaded online2."
  }, {
    "heading": "2. Related Works",
    "text": "kNN is a popular ML algorithm owing to its simplicity, generality, and interpretability (Cover & Hart, 2006). In particular, kNN can learn complex decision boundaries and has only one hyperparameter k. However, vanilla kNN suffers from several issues as mentioned in the previous section. A number of methods, which try to address these issues, exist in the literature. Broadly, these methods can be divided into three sub-categories.\nSeveral existing methods reduce prediction time of kNN using fast nearest neighbor retrieval. For example Bentley (1975); Beygelzimer et al. (2006) use tree data structures and Gionis et al. (1999); Weiss et al. (2008); Kulis & Darrell (2009); Norouzi et al. (2012); Liu et al. (2012) learn binary embeddings for fast nearest neighbor operations. These methods, although helpful in reducing the prediction time, lead to loss in accuracy and require the entire training data to be in memory leading to large model sizes that cannot be deployed on tiny IoT devices.\n1https://www.arduino.cc/en/Main/ArduinoBoardUno 2https://github.com/Microsoft/ELL\nAnother class of methods improve accuracy of kNN by learning a better metric to compare, given a pair of points (Goldberger et al., 2004; Davis et al., 2007). For example, (Weinberger & Saul, 2009) proposed a Large Margin Nearest Neighbor (LMNN) classifier which transforms the input space such that in the transformed space points from same class are closer compared to points from disparate classes. LMNN’s transformation matrix can map data into lower dimensions and reduce overall model size compared to kNN, but it is still too large for most resource-scarce devices.\nFinally, another class of methods constructs a set of prototypes to represent the entire training data. In some approaches (Angiulli, 2005; Devi & Murty, 2002), the prototypes are chosen from the original training data, while some other approaches (Mollineda et al., 2002) construct artificial points for prototypes. Of these approaches, SNC, Deep SNC (DSNC) (Wang et al., 2016), Binary Neighbor Compression (BNC) (Zhong et al., 2017) are the current state-of-the-art.\nSNC learns artificial prototypes such that the likelihood of a particular class probability model is maximized. Thus, SNC applies only to multi-class problems and its extension to multilabel/ranking problems is non-trivial. In contrast, we have a more direct discriminative formulation that can be applied to arbitrary supervised learning problems. To decrease the model size, SNC introduces a pre-processing step of low-d projection of the data via LMNN based projection matrix and then learns prototypes in the projected space. The SNC parameters (projection matrix, prototypes) might have to be hard-thresholded post-facto to fit within the memory budget. In contrast, ProtoNN’s parameters are de-facto learnt jointly with model size constraints imposed during optimization. This leads to significant improvements over SNC and other state-of-the-art methods in the small model-size regime; see Figure 1, 3.\nDSNC is a non-linear extension of SNC in that it learns a non-linear low-d transformation jointly with the prototypes. It has similar drawbacks as SNC: a) it only applies to multi-class problems and b) model size of DSNC can be significantly larger than SNC as it uses a feedforward network to learn the non-linear transformation.\nBNC is a binary embedding technique, which jointly learns a binary embedding and a set of artificial binary prototypes. Although BNC learns binary embeddings, its dimensionality can be significantly higher, so it need not result in significant model compression. Moreover, the optimization in BNC is difficult because of the discrete optimization space."
  }, {
    "heading": "3. Problem Formulation",
    "text": "Given n data points X = [x1,x2, . . .xn]T and the corresponding target output Y = [y1,y2 . . .yn]T , where xi ∈\nRd, yi ∈ Y , our goal is to learn a model that accurately predicts the desired output of a given test point. In addition, we also want our model to have small size. For both multilabel/multi-class problems with L labels, yi ∈ {0, 1}L but in multi-class ‖yi‖ = 1. Similarly, for ranking problems, the output yi is a permutation.\nLet’s consider a smooth version of kNN prediction function for the above given general supervised learning problem\nŷ = ρ(ŝ) = ρ ( n∑ i=1 σ(yi)K(x,xi) ) , (1)\nwhere ŷ is the predicted output for a given input x, ŝ =∑n i=1 σ(yi)K(x,xi) is the score vector for x. σ : Y → RL maps a given output into a score vector and ρ : RL → Y maps the score function back to the output space. For example, in the multi-class classification, σ is the identity function while ρ = Top1, where [Top1(s)]j = 1 if sj is the largest element and 0 otherwise. K : Rd × Rd → R is the similarity function, i.e., K(xi,xj) computes similarity between xi and xj . For example, standard kNN uses K(x,xi) = I[xi ∈ Nk(x)] where Nk(x) is the set of k nearest neighbors of x in X .\nNote that kNN requires entire X to be stored in memory for prediction, so its model size and prediction time are prohibitive for resource constrained devices. So, to bring down model and prediction complexity of kNN, we propose using prototypes that represent the entire training data. That is, we learn prototypes B = [b1, . . . ,bm] and the corresponding score vectors Z = [z1, . . . , zm] ∈ RL×m, so that the decision function is given by: ŷ = ρ (∑m j=1 zjK(x,bj) ) .\nExisting prototype based approaches like SNC, DSNC have a specific probabilistic model for multi-class problems with the prototypes as the model parameters. In contrast, we take a more direct discriminative learning approach that allows us to obtain better accuracies in several settings along with generalization to any supervised learning problem, e.g., multi-label classification, regression, ranking, etc.\nHowever, K is a fixed similarity function like RBF kernel which is not tuned for the task at hand and can lead to inaccurate results. We propose to solve this issue by learning a low-dimensional matrix W ∈ Rd̂×d that further brings down model/prediction complexity as well as transforms data into a space where prediction is more accurate.That is, our proposed algorithm ProtoNN uses the following prediction function that is based on three sets of learned parameters W ∈ Rd̂×d, B = [b1, . . . ,bm] ∈ Rd̂×m, and Z = [z1, . . . , zm] ∈ RL×m: ŷ = ρ (∑m j=1 zjK(Wx,bj) ) .\nTo further reduce the model/prediction complexity, we learn sparse set of Z,B,W . Selecting the correct simi-\nlarity function K is crucial to the performance of the algorithm. In this work we choose K to be the Gaussian kernel: Kγ(x, y) = exp{−γ2‖x − y‖22}, which is a popular choice in many non-parametric methods (including regression, classification, density estimation).\nNote that if m = n, and W = Id×d, then our prediction function reduces to the standard RBF kernel-SVM’s decision function for binary classification. That is, our function class is universal: we can learn any arbitrary function given enough data and model complexity. We observe a similar trend in our experiments, where even with reasonably small amount of model complexity, ProotNN nearly matches RBF-SVM’s prediction error.\nTraining Objective: We now provide the formal optimization problem to learn parameters Z,B,W . Let L(ŝ,y) be the loss (or) risk of predicting score vector ŝ for a point with label vector y. For example, the loss function can be standard hinge-loss for binary classification, or NDCG loss function for ranking problems.\nNow, define the empirical risk associated with Z,B,W as\nRemp(Z,B,W ) = 1\nn n∑ i=1 L yi, m∑ j=1 zjKγ(bj ,Wxi)  . In the sequel, to simplify the notation, we denote the risk at ith data point by Li(Z,B,W ) i.e., Li(Z,B,W ) = L ( yi, ∑m j=1 zjKγ(bj ,Wxi) ) . To jointly learn Z,B,W , we minimize the empirical risk with explicit sparsity constraints:\nmin Z:‖Z‖0≤sZ ,B:‖B‖0≤sB ,W :‖W‖0≤sW\nRemp(Z,B,W ), (2)\nwhere ‖Z‖0 is equal to the number of non-zero entries in Z. For all our expeirments (multi-class/multi-label), we used the squared `2 loss function as it helps us write down the gradients easily and allows our algorithm to converge faster and in a robust manner. That is, Remp(Z,B,W ) = 1 n ∑n i=1 ‖yi − ∑m j=1 zjKγ(bj ,Wxi)‖22. Note that the sparsity constraints in the above objective gives us explicit control over the model size. Furthermore, as we show in our experiments, jointly optimizing all the three parameters, Z,B,W , leads to better accuracies than optimizing only a subset of parameters."
  }, {
    "heading": "4. Algorithm",
    "text": "We now present our algorithm for optimization of (2). Note that the objective in (2) is non-convex and is difficult to optimize. However, we present a simple alternating minimization technique for its optimization. In this technique, we alternately minimizeZ,B,W while fixing the other two parameters. Note that the resulting optimization problem\nAlgorithm 1 ProtoNN: Train Algorithm Input: data (X,Y ), sparsities (sZ , sB , sW ), kernel parameter γ, projection dimension d̂, no. of prototypes m, iterations T , SGD epochs e. Initialize Z,B,W for t = 1 to T do {alternating minimization}\nrepeat {minimization of Z} randomly sample S ⊆ [1, . . . n] Z ← HTsZ ( Z − ηr ∑ i∈S ∇ZLi(Z,B,W ) ) until e epochs repeat {minimization of B}\nrandomly sample S ⊆ [1, . . . n] B ← HTsB ( B − ηr ∑ i∈S ∇BLi(Z,B,W ) ) until e epochs repeat {minimization of W}\nrandomly sample S ⊆ [1, . . . n] W ← HTsW ( W − ηr ∑ i∈S ∇WLi(Z,B,W ) ) until e epochs\nend for Output: Z,B,W\nin each of the alternating steps is still non-convex. To optimize these sub-problems we use projected Stochastic Gradient Descent (SGD) for large datasets and projected Gradient Descent (GD) for small datasets.\nSuppose we want to minimize the objective w.r.t Z by fixing B,W . Then in each iteration of SGD we randomly sample a mini-batch S ⊆ [1, . . . n] and update Z as: Z ← HTsZ ( Z − η ∑ i∈S ∇ZLi(Z,B,W ) ) , where HTsZ (A) is the hard thresholding operator that thresholds the smallest L ×m − sZ entries (in magnitude) of A and ∇ZLi(Z,B,W ) denotes the partial derivative of Li w.r.t Z. Note that GD procedure is just SGD with batch size |S| = n. Algorithm 1 presents pseudo-code for our entire training procedure.\nStep-size: Setting correct step-size is critical to convergence of SGD methods, especially for non-convex optimization problems. For our algorithm, we select the initial step size using Armijo rule. Subsequent step sizes are selected as ηt = η0/t where η0 is the initial step-size.\nInitialization: Since our objective function (2) is nonconvex, good initialization for Z,B,W is critical in converging efficiently to a good local optima. We used a randomly sampled Gaussian matrix to initialize W for binary and small multi-class benchmarks. However, for large multi class datasets (aloi) we use LMNN based initialization of W. Similarly, for multi-label datasets we use SLEEC (Bhatia et al., 2015) for initialization of W ; SLEEC is an embedding technique for large multi-label problems.\nFor initialization of prototypes, we experimented with two different approaches. In one, we randomly sample training\ndata points in the transformed space and assign them as the prototypes; this is a useful technique for multilabel problems. In the other approach, we run k-means clustering in the transformed space on data points belonging to each class and pick the cluster centers as our prototypes. We use this approach for binary and multi-class problems.\nConvergence: Although Algorithm 1 optimizes an `0 constrained optimization problem, we can still show that it converges to a local minimum due to smoothness of objective function (Blumensath & Davies, 2008). Moreover, if the objective function satisfies strong convexity in a small ball around optima, then appropriate initialization leads to convergence to that optima (Jain et al., 2014). In fact, our next section presents such a strong convexity result (wrtB) if the data is generated from a mixture of well-separated Gaussians. Finally, our empirical results (Section 6) indicate that the objective function indeed converges at a fast rate to a good local optimum leading to accurate models."
  }, {
    "heading": "5. Analysis",
    "text": "In this section, we present an analysis of our approach for when data is generated from the following generative model: let each point xi be sampled from a mixture of two Gaussians, i.e., xi\ni.i.d∼ 0.5·N (µ+, I)+0.5·N (µ−, I) ∈ Rd and the corresponding label yi be the indicator of the Gaussian from which xi is sampled. Now, it is easy to see that if the Gaussians are well-separated then one can design 2 prototypes b+∗, b−∗ such that the error of our method with W = I and fixed Z = [e1, e2] will lead to nearly Bayes’ optimal classifier; ei is the i-th canonical basis vector.\nThe goal of this section is to show that our method that optimizes the squared `2 loss objective (2) w.r.t. prototypes B, converges at a linear rate to a solution that is in a small ball around the global optima, and hence leads to nearly optimal classification accuracy.\nWe would like to stress that the goal of our analysis is to justify our proposed approach in a simple and easy to study setting. We do not claim new bounds for the mixture of Gaussians problem; it is a well-studied problem with several solid solutions. Our goal is to show that our method in this simple setting indeed converges to a nearly optimal solution at linear rate, thus providing some intuition for its success in practice. Also, our current analysis only studies optimization w.r.t. the prototypes B while fixing projection matrix W and prototype label vectors Z. Studying the problem w.r.t. all the three parameters is significantly more challenging, and is beyond the scope of this paper.\nDespite the simplicity of our model, ours is one of the first rigorous studies of a classification method that is designed for resource constrained problems. Typically, the proposed methods in this regime are only validated using empirical\nresults as theoretical study is quite challenging owing to the obtained non-convex optimization surface and complicated modeling assumptions.\nFor our first result, we ignore sparsity ofB, i.e., sB = 2 ·d. We consider the RBF-kernel for K with γ2 = 12 . Theorem 1. Let X = [x1, . . . ,xn] and Y = [y1, . . . ,yn] be generated from the above mentioned generative model. SetW = I , Z = [e1, e2] and let b+,b− be the prototypes. Let n → ∞, µ̄ := µ+ − µ−. Also, let ∆+ := b+ − µ+, ∆− := b+ − µ−, and let ∆+T µ̄ ≥ − (1−δ)2 ‖µ̄‖\n2 for some fixed constant δ > 0, and d ≥ 8(α − δ)‖µ̄‖2 for some constant α > 0. Then, the following holds for the gradient descent step b+\n′ = b+ − η∇b+R where R = E[Remp], and η ≥ 0 is appropriately chosen:\n‖b+′−µ+‖2 ≤ ‖b+−µ+‖2 ( 1− 0.01 exp { −α‖µ̄‖ 2\n4\n}) ,\nif ‖∆+‖ ≥ 8‖µ̄‖ exp { −α‖µ̄‖ 2\n4\n} .\nSee Appendix 8 for a detailed proof of this as well as the below given theorem. The above theorem shows that if the Gaussians are well-separated and the starting b+ is closer to µ+ than µ−, then the gradient descent step decreases the distance between b+ and µ+ geometrically until b+ converges to a small ball around µ+, the radius of the ball is exponentially small in ‖µ+−µ−‖. Note that our initialization method indeed satisfies the above mentioned assumption with at least a constant probability.\nIt is easy to see that in this setting, the loss function decomposes over independent terms from b+ and b−, and hence an identical result can be obtained for b−. For simplicity, we present the result for n → ∞ (hence, expected value). Extension to finite samples should be fairly straightforward using standard tail bounds. The tail bounds will also lead to a similar result for SGD but with an added variance term.\nNext, we show that if the b+ is even closer to µ+, then the objective function becomes strongly convex in b+,b−. Theorem 2. Let X,Y, µ̄,∆+,∆− be as given in Theorem 1. Also, let ∆+T µ̄ ≥ − (1−δ)2 ‖µ̄‖\n2, for some small constant δ > 0, ‖µ̄‖2 ≥ 4(ln 0.1)δ , and ‖∆+‖\n2 ≤ 0.5. Then, R with W = I and Z = [e1, e2] is a strongly convex function of B with condition number bounded by 20.\nNote that the initialization assumptions are much more strict here, but strong convexity with bounded condition number provides significantly faster convergence to optima. Moreover, this theorem also justifies our IHT based method. Using standard tail bounds, it is easy to show that if n grows linearly with sB rather than d, the condition number bound still holds over sparse set of vectors, i.e., for sparse µ+, µ− and sparse b+,b−. Using this restricted strong convexity with (Jain et al., 2014) guarantees\nthat with just O(sB log d) samples, our method will converge to a small ball around sparse µ+ in polynomial time. We skip these standard details as they are orthogonal to the main point of this analysis section."
  }, {
    "heading": "6. Experiments",
    "text": "In this section we present the performance of ProtoNN on various benchmark binary, multiclass and multilabel datasets with a goal to demonstrate the following aspects:\na) In severely resource constrained settings where we require model sizes to be less than 2kB (which occur routinely for IoT devices like Arduino Uno), we outperform all state-of-the art compressed methods. b) For model sizes in the range 16 − 32 kB, we achieve comparable accuracies to the best uncompressed methods. c) In multiclass and multilabel problems we achieve near state-of-the-art accuracies with an order of magnitude reduction in model size, thus showing our approach is flexible and general enough to handle a wide variety of problems.\nExperimental Settings: Datasets: Table 3 in Appendix 9.1 lists the binary, multiclass and multilabel datasets used in our experiments. For binary and multiclass datasets, we standardize each feature in the data to zero-mean and unit-variance. For multilabel datasets, we normalize the feature vector of each data point by projecting it onto a unit norm ball which preserves data sparsity. Hyperparameters: In all our experiments, we fix the no. of alternating minimization iterations(T) to 150. Each such iteration does e-many epochs each over the 3 parameters, W , B, and Z. For small binary and multiclass datasets we do GD with e set to 20. For multilabel and large multiclass (aloi) datasets, we do SGD with e set to 5, batch size to 512. Kernel parameter γ is computed after initializing B,W as 2.5median(D) , where D is the set of distances between prototypes and training points in the transformed space and\nis defined as D = {‖bj −Wxi‖2}i∈[n],j∈[m].\nProtoNN vs. Uncompressed Baselines: In this experiment we compare the performance of ProtoNN with uncompressed baselines and demonstrate that even with compression, ProtoNN achieves near state-of-the-art accuracies. We restrict the model size of ProtoNN to 16kB for binary datasets and to 64kB for multiclass datasets and don’t place any constraints on the model sizes of baselines. We compare ProtoNN with: GBDT, RBF-SVM, 1-Hidden Layer Neural Network (1-hidden NN), kNN, BNC and SNC. For baselines the optimal hyper-parameters are selected through cross-validation. For SNC, BNC we set projection dimensions to 100, 1280 respectively and compression ratios to 16%, 1%. For ProtoNN, hyper-parameters are set based on the following heuristics which ensure that the model size constraints are satisfied: Binary: d̂ = 10, sZ = sB = 0.8. m = 40 if sW = 1.0 gives model larger than 16kB. Else, sW = 1.0 and m is increased to reach 16 kB model. Multiclass: d̂ = 15, sZ = sB = 0.8. m = 5/class if sW = 1.0 gives model larger than 64kb. Else, m is increased to reach 64 kB model. CUReT which has 61 classes, requires smaller sZ to satisfy model size constraints. We use the above parameter settings for all binary, multiclass datasets except for binary versions of usps, character and eye which require 5-fold cross validation. Table 1 presents the results on binary datasets and Table 2 presents the results on multiclass datasets. For most of the datasets, ProtoNN gets to within 1−2% accuracy of the best uncompressed baseline with 1− 2 orders of magnitude reduction in model size. For example on character recognition, ProtoNN is 0.5% more accurate than the best method (RBFSVM) while getting ≈ 400× compression in model size. Similarly, on letter-26, our method is within 0.5% accuracy of RBF-SVM while getting ≈ 9× compression. Also note that ProtoNN with 16kB models is still able to outperform BNC, SNC on most of the datasets.\nProtoNN vs. Compressed Baselines: In this experiment we compare the performance of ProtoNN with other stateof-the-art compressed methods in the 2-16kB model size regime: BudgetRF (Nan et al., 2015), Decision Jungle (Shotton et al., 2013), LDKL (Jose et al., 2013), Tree Pruning (Dekel et al., 2016), GBDT (Friedman, 1999), Budget Prune (Nan et al., 2016), SNC and NeuralNet Pruning (Han et al., 2016). All baselines plots are obtained via cross-validation. Figure 1 presents the memory vs. accuracy plots. Hyper-parameters of ProtoNN are set as follows: Binary: sB = sZ = 0.8. For [2, 4, 8, 16] kB, d̂ = [5, 5, 10, 15]. sW , m are set using the same heuristic mentioned in the previous paragraph. Multiclass: sB = 0.8. For [16, 32, 64, 128] kB, d̂ = [10, 15, 15, 20]. sW , sZ , m are set as defined in the previous paragraph. ProtoNN values obtained with the above hyper-parameters\nare reported for all datasets, except usps and character recognition which require 5-fold cross validation. ProtoNN performs significantly better than the baselines on all the datasets. This is especially true in the 2kB regime, where ProtoNN is ≥ 5% more accurate on most of the datasets.\nProtoNN on Multilabel and Large Multiclass Datasets: We now present the performance of ProtoNN on larger datasets. Here, we experimented with the following datasets: aloi dataset which is a relatively large multiclass dataset , mediamill, delicious, eurlex which are smallmedium sized multilabel datasets. We set the hyper-parameters of ProtoNN as follows. d̂ is set to 30 for all datasets, except for eurlex for which we set it to 100. Other parameters are set as follows: sW = 1, sB = 1, sZ = 5/L for aloi and sZ = 2(avg. number of labels per training point)/L for multilabel datasets, m = 2 · L for multilabel datasets. For aloi, we compare ProtoNN with the following baselines: 1vsA L2 Logistic Regression (1vsA-Logi), RBFSVM, FastXML: a large-scale multilabel method (Prabhu & Varma, 2014), Recall Tree: a scalable method for large multiclass problems (Daume III et al., 2016). For 1vsALogi, Recall Tree we perform cross validation to pick the best tuning parameter. For FastXML we use the default parameters. For RBF-SVM we set γ to the default value 1/d\nand do a limited tuning of the regularization parameter. Left table of Figure 2 shows that ProtoNN (with m = 5000) gets to within 1% of the accuracy of RBF-SVM with just (1/50)th of its model size and 50 times fewer floating point computations per prediction. For a better comparison of ProtoNN with FastXML, we set the number of prototypes (m = 1500) such that computations/prediction of both the methods are almost the same. We can see that ProtoNN gets similar accuracy as FastXML but with a model size 2 orders of magnitude smaller than FastXML. Finally, our method has almost same prediction cost as Recall-Tree but with 10% higher accuracy and 4× smaller model size. Right table of Figure 2 presents preliminary results on multilabel datasets. Here, we compare ProtoNN with SLEEC, FastXML and DiSMEC (Babbar & Shölkopf, 2016), which learns a 1vsA linear-SVM in a distributed fashion. ProtoNN almost matches the performance of all baselines with huge reduction in model size. These results show that ProtoNN is very flexible and can handle a wide variety of problems very efficiently. SNC doesn’t have such flexibility. For example, it can’t be naturally extended to handle multilabel classification problems.\nProtoNN vs. BNC, SNC: In this experiment, we do a thorough performance comparison of ProtoNN with BNC and SNC. To show that ProtoNN learns better prototypes than BNC, SNC, we fix the projection dimension d̂ of all the methods and vary the number of prototypes m. To show that ProtoNN learns a better embedding, we fix m and vary d̂. For BNC, which learns a binary embedding, d̂ is chosen such that the #parameters in its transformation matrix is 32 times the #parameters in transformation matrices of ProtoNN, SNC. m is chosen similarly. Figure 3 presents the results from this experiment on mnist binary dataset. We use the following hyper parameters for ProtoNN: sW = 0.1, sZ = sB = 1.0. For SNC, we hard threshold the input transformation matrix so that it has sparsity 0.1. Note that for small d̂ our method is as much as\nFigure 2. Left Table: ProtoNN vs baselines on aloi dataset. For Recall Tree we couldn’t compute the avg. number of computations needed per prediction, instead we report the prediction time w.r.t 1vsA-Logi. Right Table: ProtoNN vs baselines on multilabel datasets. For SLEEC and FastXML we use the default parameters from the respective papers. Both the tables show that our method achieves similar accuracies as the baselines, but often with 1− 2 orders of magnitude compression in model size. On aloi our method is at most 2 slower than 1-vs-all while RBF-SVM is 115× slower.\n20% more accurate than SNC, 5% more accurate than BNC and reaches nearly optimal accuracy for small d̂ or m.\nRemark 1. Before we conclude the section we provide some practical guidelines for hyper-parameter selection in ProtoNN. Consider the following two cases: a) Small L (L / 0.1d): In this case, parameters d̂ and sW govern the model size. Given a model size constraint, fixing one parameter fixes the other, so that we effectively have one hyper-parameter to cross-validate. Choosing m such that 10 ≤ m/L ≤ 20 typically gives good accuracies. b) Large L (L ' 0.1d): In this case, sZ also governs the model size. sZ , sW and d̂ can be selected through crossvalidation. If the model size allows it, increasing d̂ typically helps. Fixing m/L to a reasonable value such as 3-10 for medium L, 1-2 for large L typically gives good accuracies."
  }, {
    "heading": "7. Experiments on tiny IoT devices",
    "text": "In the previous section, we showed that ProtoNN gets better accuracies than other compressed baselines at low model size regimes. For small devices, it is also critical to study other aspects like energy consumption, which severely impact the effectiveness of a method in practice. In this section, we study the energy consumption and prediction time of ProtoNN model of size 2kB when deployed on an Arduino Uno.The Arduino Uno has an 8 bit, 16 MHz Atmega328P microcontroller, with 2kB of SRAM and 32kB\nof read-only flash. We compare ProtoNN with 3 baselines (LDKL-L1, NeuralNet Pruning, L1 Logistic) on 4 binary datasets. Figure 4 presents the results from this experiment. ProtoNN shows almost the same characteristics as a simple linear model (L1-logistic) in most cases while providing significantly more accurate predictions. Further optimization: The Atmega328P microcontroller supports native integer arithmetic at ≈0.1µs/operation, software-based floating point arithmetic at≈6µs/operation; exponentials are a further order slower. It is thus desirable to perform prediction only using integers. We implemented an integer version of ProtoNN to leverage this. We factor out a common float value from the parameters and round the residuals by 1-byte integers. To avoid computing the exponentials, we store a pre-computed table of approximate exponential values. As can be seen in Figure 4, this optimized version of ProtoNN loses only a little accuracy, but obtains ≈ 2× reduction in energy and prediction cost."
  }],
  "year": 2017,
  "references": [{
    "title": "Fast condensed nearest neighbor rule",
    "authors": ["Angiulli", "Fabrizio"],
    "venue": "In ICML,",
    "year": 2005
  }, {
    "title": "Dismec-distributed sparse machines for extreme multi-label classification",
    "authors": ["Babbar", "Rohit", "Shölkopf", "Bernhard"],
    "venue": "In arXiv preprint arXiv:1609.02521, Accepted for Web Search and Data Mining Conference (WSDM)",
    "year": 2016
  }, {
    "title": "Multidimensional binary search trees used for associative searching",
    "authors": ["Bentley", "Jon Louis"],
    "venue": "Commun. ACM,",
    "year": 1975
  }, {
    "title": "Cover trees for nearest neighbor",
    "authors": ["Beygelzimer", "Alina", "Kakade", "Sham", "Langford", "John"],
    "venue": "In ICML,",
    "year": 2006
  }, {
    "title": "Sparse local embeddings for extreme multi-label classification",
    "authors": ["Bhatia", "Kush", "Jain", "Himanshu", "Kar", "Purushottam", "Varma", "Manik", "Prateek"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Iterative thresholding for sparse approximations",
    "authors": ["Blumensath", "Thomas", "Davies", "Mike E"],
    "venue": "Journal of Fourier Analysis and Applications,",
    "year": 2008
  }, {
    "title": "Nearest neighbor pattern classification",
    "authors": ["T. Cover", "P. Hart"],
    "venue": "IEEE Trans. Inf. Theor.,",
    "year": 2006
  }, {
    "title": "Logarithmic time one-against-some",
    "authors": ["Daume III", "Hal", "Karampatziakis", "Nikos", "Langford", "John", "Mineiro", "Paul"],
    "venue": "arXiv preprint arXiv:1606.04988,",
    "year": 2016
  }, {
    "title": "Information-theoretic metric learning",
    "authors": ["Davis", "Jason V", "Kulis", "Brian", "Jain", "Prateek", "Sra", "Suvrit", "Dhillon", "Inderjit S"],
    "venue": "In ICML,",
    "year": 2007
  }, {
    "title": "Pruning decision forests",
    "authors": ["O. Dekel", "C. Jacobbs", "L. Xiao"],
    "venue": "In Personal Communications,",
    "year": 2016
  }, {
    "title": "An incremental prototype set building technique",
    "authors": ["Devi", "V Susheela", "Murty", "M Narasimha"],
    "venue": "Pattern Recognition,",
    "year": 2002
  }, {
    "title": "Stochastic gradient boosting",
    "authors": ["Friedman", "Jerome H"],
    "venue": "Computational Statistics and Data Analysis,",
    "year": 1999
  }, {
    "title": "Similarity search in high dimensions via hashing",
    "authors": ["Gionis", "Aristides", "Indyk", "Piotr", "Motwani", "Rajeev"],
    "venue": "In VLDB,",
    "year": 1999
  }, {
    "title": "Neighbourhood components analysis",
    "authors": ["Goldberger", "Jacob", "Roweis", "Sam T", "Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan"],
    "venue": "In NIPS,",
    "year": 2004
  }, {
    "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
    "authors": ["S. Han", "H. Mao", "W.J. Dally"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "On iterative hard thresholding methods for high-dimensional m-estimation",
    "authors": ["Jain", "Prateek", "Tewari", "Ambuj", "Kar", "Purushottam"],
    "venue": "In NIPS, pp",
    "year": 2014
  }, {
    "title": "Local deep kernel learning for efficient nonlinear SVM prediction",
    "authors": ["Jose", "Cijo", "Goyal", "Prasoon", "Aggrwal", "Parv", "Varma", "Manik"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Learning to hash with binary reconstructive embeddings",
    "authors": ["Kulis", "Brian", "Darrell", "Trevor"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Stochastic neighbor compression",
    "authors": ["Kusner", "Matt J", "Tyree", "Stephen", "Weinberger", "Kilian", "Agrawal", "Kunal"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Supervised hashing with kernels",
    "authors": ["Liu", "Wei", "Wang", "Jun", "Ji", "Rongrong", "Jiang", "Yu-Gang", "Chang", "Shih-Fu"],
    "venue": "In Computer Vision and Pattern Recognition (CVPR),",
    "year": 2012
  }, {
    "title": "An efficient prototype merging strategy for the condensed 1-nn rule through class-conditional hierarchical clustering",
    "authors": ["Mollineda", "Ramón Alberto", "Ferri", "Francesc J", "Vidal", "Enrique"],
    "venue": "Pattern Recognition,",
    "year": 2002
  }, {
    "title": "Feature-budgeted random forest",
    "authors": ["F. Nan", "J. Wang", "V. Saligrama"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Pruning random forests for prediction on a budget",
    "authors": ["F. Nan", "J. Wang", "V. Saligrama"],
    "year": 2016
  }, {
    "title": "Hamming distance metric learning",
    "authors": ["Norouzi", "Mohammad", "Fleet", "David J", "Salakhutdinov", "Ruslan R"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2012
  }, {
    "title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning",
    "authors": ["Prabhu", "Yashoteja", "Varma", "Manik"],
    "venue": "In KDD,",
    "year": 2014
  }, {
    "title": "Decision jungles: Compact and rich models for classification",
    "authors": ["J. Shotton", "T. Sharp", "P. Kohli", "S. Nowozin", "J. Winn", "A. Criminisi"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Deep metric learning with data summarization",
    "authors": ["Wang", "Wenlin", "Chen", "Changyou", "Rai", "Piyush", "Carin", "Lawrence"],
    "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
    "year": 2016
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2009
  }, {
    "title": "Spectral hashing",
    "authors": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Robert"],
    "venue": "In NIPS,",
    "year": 2008
  }],
  "id": "SP:e703cba8751757e008a73f7f1d2c727971576ec3",
  "authors": [{
    "name": "Chirag Gupta",
    "affiliations": []
  }, {
    "name": "Arun Sai Suggala",
    "affiliations": []
  }, {
    "name": "Ankit Goyal",
    "affiliations": []
  }, {
    "name": "Harsha Vardhan Simhadri",
    "affiliations": []
  }, {
    "name": "Bhargavi Paranjape",
    "affiliations": []
  }, {
    "name": "Ashish Kumar",
    "affiliations": []
  }, {
    "name": "Saurabh Goyal",
    "affiliations": []
  }, {
    "name": "Raghavendra Udupa",
    "affiliations": []
  }, {
    "name": "Manik Varma",
    "affiliations": []
  }, {
    "name": "Prateek Jain",
    "affiliations": []
  }],
  "abstractText": "Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applications demand prediction models with small storage and computational complexity that do not compromise significantly on accuracy. In this work, we propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. ProtoNN is inspired by k-Nearest Neighbor (KNN) but has several orders lower storage and prediction complexity. ProtoNN models can be deployed even on devices with puny storage and computational power (e.g. an Arduino UNO with 2kB RAM) to get excellent prediction accuracy. ProtoNN derives its strength from three key ideas: a) learning a small number of prototypes to represent the entire training set, b) sparse low dimensional projection of data, c) joint discriminative learning of the projection and prototypes with explicit model size constraint. We conduct systematic empirical evaluation of ProtoNN on a variety of supervised learning tasks (binary, multi-class, multi-label classification) and show that it gives nearly state-of-the-art prediction accuracy on resource-scarce devices while consuming several orders lower storage, and using minimal working memory.",
  "title": "ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices"
}