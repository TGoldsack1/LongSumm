{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep neural networks have become the state-of-the-art systems for image recognition (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Qiao et al., 2018; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Wang et al., 2017; Zeiler & Fergus, 2013) as well as other vision tasks (Chen et al., 2015; Girshick et al., 2014; Long et al., 2015; Qiao et al., 2017; Ren et al., 2015; Shen et al., 2015; Xie & Tu, 2015). The architectures keep going deeper, e.g., from five convolutional layers (Krizhevsky et al., 2012) to 1001 layers (He et al., 2016b). The benefit of deep architectures is their strong learning capacities because each new layer can potentially introduce more non-linearities and typically uses larger receptive fields (Simonyan & Zisserman, 2014). In addition, adding certain types of layers (e.g. (He et al., 2016b)) will not harm the performance theoretically since they can just learn identity mapping. This makes stacking up layers more appealing in the network designs.\n1Johns Hopkins University 2Shanghai University 3Hikvision Research. Correspondence to: Siyuan Qiao <siyuan.qiao@jhu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nAlthough deeper architectures usually lead to stronger learning capacities, cascading convolutional layers (e.g. VGG (Simonyan & Zisserman, 2014)) or blocks (e.g. ResNet (He et al., 2016a)) is not necessarily the only method to achieve this goal. In this paper, we present a new way to increase the depth of the networks as an alternative to stacking up convolutional layers or blocks. Figure 2 provides an illustration that compares our proposed convolutional network that gradually updates the feature representations against the traditional convolutional network that computes its output simultaneously. By only adding an ordering to the channels without any additional computation, the later computed channels become deeper than the corresponding ones in the traditional convolutional network. We refer to the neural networks with the proposed computation orderings on the channels as Gradually Updated Neural Networks (GUNN). Figure 1 provides two examples of architecture designs based on cascading building blocks and GUNN. Without repeating the building blocks, GUNN increases the depths of the networks as well as their learning capacities.\nIt is clear that converting plain networks to GUNN increases the depths of the networks without any additional computations. What is less obvious is that GUNN in fact eliminates the overlap singularities inherent in the loss landscapes of the cascading-based convolutional networks, which have been shown to adversely affect the training of deep neural networks as well as their performances (Wei et al., 2008;\nOrhan & Pitkow, 2018). Overlap singularity is when internal neurons collapse into each other, i.e. they are unidentifiable by their activations. It happens in the networks, increases the training difficulties and degrades the performances (Orhan & Pitkow, 2018). However, if a plain network is converted to GUNN, the added computation orderings will break the symmetry between the neurons. We prove that the internal neurons in GUNN are impossible to collapse into each other. As a result, the effective dimensionality can be kept during training and the model will be free from the degeneracy caused by collapsed neurons. Reflected in the training dynamics and the performances, this means that converting to GUNN will make the plain networks easier to train and perform better. Figure 3 compares the training dynamics of a 15-layer plain network on CIFAR-10 dataset (Krizhevsky & Hinton, 2009) before and after converted to GUNN.\nIn this paper, we test our proposed GUNN on highly competitive benchmark datasets, i.e. CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015). Experimental results demonstrate that our proposed GUNNbased networks achieve the state-of-the-art performances compared with the previous cascading-based architectures."
  }, {
    "heading": "2. Related Work",
    "text": "The research focuses of image recognition have moved from feature designs (Dalal & Triggs, 2005; Lowe, 2004) to architecture designs (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Sermanet et al., 2014; Simonyan\n& Zisserman, 2014; Szegedy et al., 2015; Xie et al., 2017; Zeiler & Fergus, 2013) due to the recent success of the deep neural networks. Highway Networks (Srivastava et al., 2015) proposed architectures that can be trained end-to-end with more than 100 layers. The main idea of Highway Networks is to use bypassing paths. This idea was further investigated in ResNet (He et al., 2016a), which simplifies the bypassing paths by using only identity mappings. As learning ultra-deep networks became possible, the depths of the models have increased tremendously. ResNet with pre-activation (He et al., 2016b) and ResNet with stochastic depth (Huang et al., 2016) even managed to train neural networks with more than 1000 layers. FractalNet (Larsson et al., 2016) argued that in addition to summation, concatenation also helps train a deep architecture. More recently, ResNeXt (Xie et al., 2017) used group convolutions in ResNet and outperformed the original ResNet. DenseNet (Huang et al., 2017b) proposed an architecture with dense connections by feature concatenation. Dual Path Net (Chen et al., 2017) finds a middle point between ResNet and DenseNet by concatenating them in two paths. Unlike the above cascading-based methods, GUNN eliminates the overlap singularities caused by the architecture symmetry. The detailed analyses can be found in Section 4.3.\nAlternative to increasing the depth of the neural networks, another trend is to increase the widths of the networks. GoogleNet (Szegedy et al., 2015; 2016) proposed an Inception module to concatenate feature maps produced by different filters. Following ResNet (He et al., 2016a), the WideResNet (Zagoruyko & Komodakis, 2016) argued that compared with increasing the depth, increasing the width of the networks can be more effective in improving the performances. Besides varying the width and the depth, there are also other design strategies for deep neural networks (Hariharan et al., 2015; Kontschieder et al., 2015; Pezeshki et al., 2016; Rasmus et al., 2015; Yang & Ramanan, 2015). Deeply-Supervised Nets (Lee et al., 2014) used auxiliary classifiers to provide direct supervisions for the internal layers. Network in Network (Lin et al., 2013) adds micro perceptrons to the convolutional layers."
  }, {
    "heading": "3. Model",
    "text": ""
  }, {
    "heading": "3.1. Feature Update",
    "text": "We consider a feature transformation F : Rm×n → Rm×n, where n denotes the channel of the features and m denotes the feature location on the 2-D feature map. For example, F can be a convolutional layer with n channels for both the input and the output. Let x ∈ Rm×n be the input and y ∈ Rm×n be the output, we have\ny = F(x) (1)\nSuppose that F can be decomposed into channel-wise transformation Fc(·) that are independent with eath other, then for any location k and channel c we have\nykc = Fc(xr(k)) (2)\nwhere xr(k) denotes the receptive field of the location k and Fc denotes the transformation on channel c.\nLet UC denote a feature update on channel set C, i.e.,\nUC(x) : y k c = Fc(xr(k)),∀c ∈ C, k ykc = x k c ,∀c ∈ C, k\n(3)\nThen, UC = F when C = {1, ..., n}."
  }, {
    "heading": "3.2. Gradually Updated Neural Networks",
    "text": "By defining the feature update UC on channel set C, the commonly used one-layer CNN is a special case of feature updates where every channel is updated simultaneously. However, we can also update the channels gradually. For example, the proposed GUNN can be formulated by\nGUNN(x) = (Ucl ◦ Uc(l−1) ◦ ... ◦ Uc2 ◦ Uc1)(x)\nwhere l⋃\ni=1\nci = {1, 2, ..., n} and ci ∩ cj = Φ, ∀i 6= j\n(4) When l = 1, GUNN is equivalent to F .\nNote that the number of parameters and computation of GUNN are the same as those of the corresponding F for any partitions c1, ..., cl of {1, ..., n}. However, by decomposing F into channel-wise transformations and sequentially applying them, the later computed channels are deeper than the previous ones. As a result, the depth of the network can be increased, as well as the network’s learning capacity."
  }, {
    "heading": "3.3. Channel-wise Update by Residual Learning",
    "text": "We consider the residual learning proposed by ResNet (He et al., 2016a) in our model. Specifically, we consider the channel-wise transformation Fc : Rm×n → Rm×1 to be\nFc(x) = Gc(x) + xc (5)\nAlgorithm 1 Back-propagation for GUNN Input :U(·) = (Ucl ◦ Uc(l−1) ◦ ... ◦ Uc1)(·), input x,\noutput y = U(x), gradients ∂L/∂y, and parameters Θ for U .\nOutput :∂L/∂Θ, ∂L/∂x ∂L/∂x← ∂L/∂y for i← l to 1 do\nyc ← xc, ∀c ∈ ci ∂L/∂y, ∂L/∂Θci ← BP(y, ∂L/∂x, Uci ,Θci) (∂L/∂x)c ← (∂L/∂y)c, ∀c ∈ ci (∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c 6∈ ci\nend\nwhere Gc is a convolutional neural network Gc : Rm×n → Rm×1. The motivation of expressing F in a residual learning manner is to reduce overlap singularities (Orhan & Pitkow, 2018), which will be discussed in Section 4."
  }, {
    "heading": "3.4. Learning GUNN by Backpropagation",
    "text": "Here we show the backpropagation algorithm for learning the parameters in GUNN that uses the same amount of computations and memory as in F . In Eq. 4, let the feature update Uci be parameterized by Θci . Let BP(x, ∂L/∂y, f,Θ) be the back-propagation algorithm for differentiable function y = f(x; Θ) with the loss L and the parameters Θ. Algorithm 1 presents the back-propagation algorithm for GUNN. Since Uci has the residual structures (He et al., 2016a), the last two steps can be merged into\n(∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c (6)\nwhich further simplifies the implementation. It is easy to see that converting networks to GUNN-based does not increase the memory usage in feed-forwarding. Given Algorithm 1, converting networks to GUNN will not affect the memory in both the training and the evaluation."
  }, {
    "heading": "4. GUNN Eliminates Overlap Singularities",
    "text": "Overlap singularities are inherent in the loss landscapes of some network architectures which are caused by the nonidentifiability of subsets of the neurons. They are identified and discussed in previous work (Wei et al., 2008; Anandkumar & Ge, 2016; Orhan & Pitkow, 2018), and are shown to be harmful for the performances of deep networks. Intuitively, overlap singularities exist in architectures where the internal neurons collapse into each other. As a result, the models are degenerate and the effective dimensionality is reduced. (Orhan & Pitkow, 2018) demonstrated through experiments that residual learning (see Eq. 5) helps to reduce the overlap singularities in deep networks, which partly explains the exceptional performances of ResNet (He et al., 2016a) compared with plain networks. In the following, we first use linear transformation as an example to demonstrate\nhow GUNN-based networks break the overlap singularities. Then, we generalize the results to ReLU DNN. Finally, we compare GUNN with the previous state-of-the-art network architectures from the perspective of singularity elimination."
  }, {
    "heading": "4.1. Overlap Singularities in Linear Transformations",
    "text": "Consider a linear function y = f(x) : Rn → Rn such that\nyi = n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (7)\nSuppose that there exists a pair of collapsed neurons yp and yq (p < q). Then, for ∀x, yp = yq, and the equality holds after any number of gradient descents, i.e. ∆yp = ∆yq .\nEq. 7 describes a plain network. The solution for the existence of yp and yq is that ωp,j = ωq,j ,∀j. This is the case that is mostly discussed previously, which happens in the networks and degrades the performances.\nWhen we add the residual learning, Eq. 7 becomes\nyi = xi + n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (8)\nCollapsed neurons require that ωp,p + 1 = ωq,p, ωq,q + 1 = ωp,q. This will make the collapse of yp and yq very hard when ω is initialized from a normal distribution N (0, √ 2/n) as in ResNet, but still possible.\nNext, we convert Eq. 8 to GUNN, i.e.,\nyi = xi + i−1∑ j=1 ωi,jyj + n∑ j=i ωi,jxj , ∀i ∈ {1, .., n} (9)\nSuppose that yp and yq (p < q) collapse. Consider ∆y, the value difference at x after one step of gradient descent on ω with input x, ∂L/∂y and learning rate . When → 0,\n∆yi = ∂L\n∂yi ( i−1∑ j=1 y2j + n∑ j=i x2j ) + i−1∑ j=1 ωi,j∆yj (10)\nAs ∆yp = ∆yq,∀x, we have ωq,j = 0, ∀j : p < j < q. But this condition will be broken in the next update; thus, q = p + 1. Then, we derive that yp = yq = 0. But these will also be broken in the next step of gradient descent optimization. Hence, yp and yq cannot collapse into each other. The complete proof can be found in the appendix."
  }, {
    "heading": "4.2. Overlap Singularities in ReLU DNN",
    "text": "In practice, architectures are usually composed of several linear layers and non-linearity layers. Analyzing all the possible architectures is beyond our scope. Here, we discuss the commonly used ReLU DNN, in which only linear transformations and ReLUs are used by simple layer cascading.\nFollowing the notations in §3, we use y = G(x) + x, in which G(x) is a ReLU DNN. Note that G is continuous piecewise linear (PWL) function (Arora et al., 2018), which means that there exists a finite set of polyhedra whose union is Rn, and G is affine linear over each polyhedron.\nSuppose that we convert G(x)+x to GUNN and there exists a pair of collapsed neurons yp and yq (p < q). Then, the set of polyhedra for yp is the same as for yq. Let P be a polyhedron for yp and yq defined above. Then, ∀x,P, i,\nyi = xi + i−1∑ j=1 ωi,j(P)yj + n∑ j=i ωi,j(P)xj (11)\nwhere ω(P) denotes the parameters for polyhedron P. Note that on each P, y is a function of x in the form of Eq. 9; hence, yp and yq cannot collapse into each other. Since the union of all polyhedra is Rn, we conclude that GUNN eliminates the overlap singularities in ReLU DNN."
  }, {
    "heading": "4.3. Discussions and Comparisons",
    "text": "The previous two subsections consider the GUNN conversion where |ci| = 1,∀i (see Eq. 4). But this will slow down the computation on GPU due to the data dependency. Without specialized hardware or library support, we decide to increase |ci| to > 10. The resulted models run at the speed between ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b). But this change introduces singularities into the channels from the same set ci. Then, the residual learning helps GUNN to reduce the singularities within the same set ci since we initialize the parameters from a normal distributionN (0, √ 2/n). We will compare the results of GUNN with and without residual learning in the experiments.\nWe compare GUNN with the state-of-the-art architectures from the perspective of overlap singularities. ResNet (He et al., 2016a) and its variants use residual learning, which reduces but cannot eliminate the singularities. ResNeXt (Xie et al., 2017) uses group convolutions to break the symmetry between groups, which further helps to avoid neuron collapses. DenseNet (Huang et al., 2017b) concatenates the outputs of layers as the input to the next layer. DenseNet and GUNN both create dense connections, while DenseNet reuses the outputs by concatenating and GUNN by adding them back to the inputs. But the channels within the same layer of DenseNet are still possible to collapse into each other since they are symmetric. In contrast, adding back makes residual learning possible in GUNN. This makes residual learning indispensable in GUNN-based networks."
  }, {
    "heading": "5. Network Architectures",
    "text": "In this section, we will present the details of our architectures for the CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015) datasets."
  }, {
    "heading": "5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks",
    "text": "Since the proposed GUNN is a method for increasing the depths of the convolutional networks, specifying the architectures to be converted is equivalent to specifying the GUNN-based architectures. The architectures before conversion, the Simultaneously Updated Neural Networks (SUNN), become natural baselines for our proposed GUNN networks. We first study what baseline architectures can be converted.\nThere are two assumptions about the feature transformation F (see Eq. 1): (1) the input and the output sizes are the same, and (2) F is channel-wise decomposable. To satisfy the first assumption, we will first use a convolutional layer with Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to transform the feature space to a new space where the number of the channels is wanted. To satisfy the second assumption, instead of directly specifying the transform F , we focus on designing Fci , where ci is a subset of the channels (see Eq. 4). To be consistent with the term update used in GUNN and SUNN, we refer to Fci as the update units for channels ci.\nBottleneck Update Units In the architectures proposed in this paper, we adopt bottleneck neural networks as shown in Figure 4 for the update units for both the SUNN and GUNN. Suppose that the update unit maps the input features of channel size nin to the output features of size nout. Each unit contains three convolutional layers. The first convolutional layer transforms the input features to K × nout using a 1× 1 convolutional layer. The second convolutional layer is of kernel size 3× 3, stride 1, and padding 1, outputting the features of size K × nout. The third layer computes the features of size nout using a 1 × 1 convolutional layer. The output is then added back to the input, following the residual architecture proposed in ResNet (He et al., 2016a). We add batch normalization layer (Ioffe & Szegedy, 2015) and ReLU layer (Nair & Hinton, 2010) after the first and the second convolutional layers, while only adding batch\nnormalization layer after the third layer. Stacking up M update units also generates a new one. In total, we have two hyperparameters for designing an update unit: the expansion rate K and the number of the 3-layer update units M .\nOne Resolution, One Representation Our architectures will have only one representation at one resolution besides the pooling layers and the convolutional layers that initialize the needed numbers of channels. Take the architecture in Table 1 as an example. There are two processes for each resolution. The first one is the transition process, which computes the initial features with the dimensions of the next resolution, then down samples it to 1/4 using a 2×2 average pooling. A convolutional operation is needed here because F is assumed to have the same input and output sizes. The next process is using GUNN to update this feature space gradually. Each channel will only be updated once, and all channels will be updated after this process. Unlike most of the previous networks, after this two processes, the feature transformations at this resolution are complete. There will be no more convolutional layers or blocks following this feature representation, i.e., one resolution, one representation. Then, the network will compute the initial features for the next resolution, or compute the final vector representation of the entire image by a global average pooling. By designing networks in this way, SUNN networks usually have about 20 layers before converting to GUNN-based networks.\nChannel Partitions With the clearly defined update units, we can easily build SUNN and GUNN layers by using the units to update the representations following Eq. 4. The hyperparameters for the SUNN/GUNN layer are the number of the channels N and the partition over those channels. In our proposed architectures, we evenly partition the channels into P segments. Then, we can useN and P to represent the configuration of a layer. Together with the hyperparameters in the update units, we have four hyperparameters to tune for one SUNN/GUNN layer, i.e. {N,P,K,M}."
  }, {
    "heading": "5.2. Architectures for CIFAR",
    "text": "We have implemented two neural networks based on GUNN to compete with the previous state-of-the-art methods on CIFAR datasets, i.e., GUNN-15 and GUNN-24. Table 1 shows the big picture of GUNN-15. Here, we present the details of the hyperparameter settings for GUNN-15 and GUNN-24. For GUNN-15, we have three GUNN layers, Conv2, Conv3 and Conv4. The configuration for Conv2 is {N = 240, P = 20,K = 2,M = 1}, the configuration for Conv3 is {N = 300, P = 25,K = 2,M = 1}, and the configuration for Conv4 is {N = 360, P = 30,K = 2,M = 1}. For GUNN-24, based on GUNN-15, we change the number of output channels of Conv1 to 720, Trans1 to 900, Trans2 to 1080, and Trans3 to 1080. The hyperparameters are {N = 720, P = 20,K = 3,M = 2} for\nConv2, {N = 900, P = 25,K = 3,M = 2} for Conv3, and {N = 1080, P = 30,K = 3,M = 2} for Conv3. The number of parameters of GUNN-15 is 1585746 for CIFAR10 and 1618236 for CIFAR-100. The number of parameters of GUNN-24 is 29534106 for CIFAR-10 and 29631396 for CIFAR-100. The GUNN-15 is aimed to compete with the methods published in an early stage by using a much smaller model, while GUNN-24 is targeted at comparing with ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b) to get the state-of-the-art performance."
  }, {
    "heading": "5.3. Architectures for ImageNet",
    "text": "We implement a neural network GUNN-18 to compete with the state-of-the-art neural networks on ImageNet with a similar number of parameters. Table 2 shows the big picture of the neural network architecture of GUNN-18. Here, we present the detailed hyperparameters for the GUNN layers in GUNN-18. The GUNN layers include Conv2, Conv3, Conv4 and Conv5. The hyperparameters are {N = 400, P = 10,K = 2,M = 1} for Conv2,\n{N = 800, P = 20,K = 2,M = 1} for Conv3, {N = 1600, P = 40,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5. The number of parameters is 28909736. The GUNN-18 is targeted at competing with the previous state-of-the-art methods that have similar numbers of parameters, e.g., ResNet50 (Xie et al., 2017), ResNeXt-50 (Xie et al., 2017) and DenseNet-264 (Huang et al., 2017b).\nWe also implement a wider GUNN-based neural networks Wide-GUNN-18 for better capacities. The hyperparameters are {N = 1200, P = 30,K = 2,M = 1} for Conv2, {N = 1600, P = 40,K = 2,M = 1} for Conv3, {N = 2000, P = 50,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5. The number of parameters is 45624936. The Wide-GUNN-18 is targeted at competing with ResNet-101, ResNext-101, DPN (Chen et al., 2017) and SENet (Hu et al., 2017)."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we demonstrate the effectiveness of the proposed GUNN on several benchmark datasets."
  }, {
    "heading": "6.1. Benchmark Datasets",
    "text": "CIFAR CIFAR (Krizhevsky & Hinton, 2009) has two color image datasets: CIFAR-10 (C10) and CIFAR-100 (C100). Both datasets consist of natural images with the size of 32× 32 pixels. The CIFAR-10 dataset has 10 categories, while the CIFAR-100 dataset has 100 categories. For both of the datasets, the training and test set contain 50, 000 and 10, 000 images, respectively. To fairly compare our method with the state-of-the-arts (He et al., 2016a; Huang et al., 2017b; 2016; Larsson et al., 2016; Lee et al., 2014; Lin et al., 2013; Romero et al., 2014; Springenberg et al., 2014; Srivastava et al., 2015; Xie et al., 2017), we use the same training and testing strategies, as well as the data processing methods. Specifically, we adopt a commonly used data augmentation scheme, i.e., mirroring and shifting, for these two datasets. We use channel means and standard derivations to normalize the images for data pre-processing.\nImageNet The ImageNet dataset (Russakovsky et al., 2015) contains about 1.28 million color images for training and 50, 000 for validation. The dataset has 1000 categories. We adopt the same data augmentation methods as in the state-of-the-art architectures (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017) for training. For testing, we use single-crop at the size of 224× 224. Following the\nstate-of-the-arts (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017), we report the validation error rates."
  }, {
    "heading": "6.2. Training Details",
    "text": "We train all of our networks using stochastic gradient descents. On CIFAR-10/100 (Krizhevsky & Hinton, 2009), the initial learning rate is set to 0.1, the weight decay is set to 1e−4, and the momentum is set to 0.9 without dampening. We train the models for 300 epochs. The learning rate is divided by 10 at 150th epoch and 225th epoch. We set the batch size to 64, following (Huang et al., 2017b). All the results reported for CIFAR, regardless of the detailed configurations, were trained using 4 NVIDIA Titan X GPUs with the data parallelism. On ImageNet (Russakovsky et al., 2015), the learning rate is also set to 0.1 initially, and decreases following the schedule in DenseNet (Huang et al., 2017b). The batch size is set to 256. The network parameters are also initialized following (He et al., 2016a). We use 8 Tesla V100 GPUs with the data parallelism to get the reported results. Our results are directly comparable with ResNet, WideResNet, ResNeXt and DenseNet."
  }, {
    "heading": "6.3. Results on CIFAR",
    "text": "We train two models GUNN-15 and GUNN-24 for the CIFAR-10/100 dataset. Table 3 shows the comparisons between our method and the previous state-of-the-art methods. Our method GUNN achieves the best results in the test of both the single model and the ensemble test. Here, we use Snapshot Ensemble (Huang et al., 2017a).\nBaseline Methods Here we present the details of baseline methods in Table 3. The performances of ResNet (He et al., 2016a) are reported in Stochastic Depth (Huang et al., 2016) for both C10 and C100. The WideResNet (Zagoruyko & Komodakis, 2016) WRN-40-10 is reported in their official code repository on GitHub. The ResNeXt in the third group is of configuration 16 × 64d, which has the best result reported in the paper (Xie et al., 2017). The DenseNet is of configuration DenseNet-BC (k = 40), which achieves the best performances on CIFAR-10/100. The Snapshot Ensemble (Huang et al., 2017a) uses 6 DenseNet-100 to ensemble during inference. We do not compare with methods that use more data augmentation (e.g. (Zhang et al., 2017)) or stronger regularizations (e.g. (Gastaldi, 2017)) for the fairness of comparison.\nAblation Study For ablation study, we compare GUNN with SUNN, i.e., the networks before the conversion. Table 5 shows the comparison results, which demonstrate the effectiveness of GUNN. We also compare the performances of GUNN with and without residual learning."
  }, {
    "heading": "6.4. Results on ImageNet",
    "text": "We evaluate the GUNN on the ImageNet classification task, and compare our performances with the state-of-the-art methods. These methods include VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016a), ResNeXt (Xie\net al., 2017), DenseNet (Huang et al., 2017b), DPN (Chen et al., 2017) and SENet (Hu et al., 2017). The comparisons are shown in Table 4. The results of ours, ResNeXt, and DenseNet are directly comparable as these methods use the same framework for training and testing networks. Table 4 groups the methods by their numbers of parameters, except VGGNet which has 1.38× 108 parameters.\nThe results presented in Table 4 demonstrate that with the similar number of parameters, GUNN can achieve comparable performances with the previous state-of-the-art methods. For GUNN-18, we also conduct an ablation experiment by comparing the corresponding SUNN with GUNN of the same configuration. Consistent with the experimental results on the CIFAR-10/100 dataset, the proposed GUNN improves the accuracy on ImageNet dataset."
  }, {
    "heading": "7. Conclusions",
    "text": "In this paper, we propose Gradually Updated Neural Network (GUNN), a novel, simple yet effective method to increase the depths of neural networks as an alternative to cascading layers. GUNN is based on Convolutional Neural Networks (CNNs), but differs from CNNs in the way of computing outputs. The outputs of GUNN are computed gradually rather than simultaneously as in CNNs in order to increase the depth. Essentially, GUNN assumes the input and the output are of the same size and adds a computation ordering to the channels. The added ordering increases the receptive fields and non-linearities of the later computed channels. Moreover, it eliminates the overlap singularities inherent in the traditional convolutional networks. We test GUNN on the task of image recognition. The evaluations are done in three highly competitive benchmarks, CIFAR10, CIFAR-100 and ImageNet. The experimental results demonstrate the effectiveness of the proposed GUNN on image recognition. In the future, since the proposed GUNN can be used to replace CNNs in other neural networks, we will study the applications of GUNN in other visual tasks, such as object detection and semantic segmentation."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Wanyu Huang, Huiyu Wang and Chenxi Liu for their insightful comments and suggestions. We gratefully acknowledge funding supports from NSF award CCF-1317376 and ONR N00014-15-1-2356. This work was also supported in part by the National Natural Science Foundation of China under Grant 61672336."
  }],
  "year": 2018,
  "references": [{
    "title": "Efficient approaches for escaping higher order saddle points in non-convex optimization",
    "authors": ["A. Anandkumar", "R. Ge"],
    "venue": "In Proceedings of the Conference on Learning",
    "year": 2016
  }, {
    "title": "Understanding deep neural networks with rectified linear units",
    "authors": ["R. Arora", "A. Basu", "P. Mianjy", "A. Mukherjee"],
    "venue": "International Conference on Learning Representations,",
    "year": 2018
  }, {
    "title": "Semantic image segmentation with deep convolutional nets and fully connected crfs",
    "authors": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2015
  }, {
    "title": "Histograms of oriented gradients for human detection",
    "authors": ["N. Dalal", "B. Triggs"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2005
  }, {
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
    "authors": ["R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik"],
    "venue": "In IEEEConference on Computer Vision and Pattern Recognition,",
    "year": 2014
  }, {
    "title": "Hypercolumns for object segmentation and fine-grained localization",
    "authors": ["B. Hariharan", "P.A. Arbeláez", "R.B. Girshick", "J. Malik"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Identity mappings in deep residual networks. ECCV, 2016b",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "year": 2016
  }, {
    "title": "Deep networks with stochastic depth",
    "authors": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"],
    "year": 2016
  }, {
    "title": "Snapshot ensembles: Train 1, get M for free",
    "authors": ["G. Huang", "Y. Li", "G. Pleiss", "Z. Liu", "J.E. Hopcroft", "K.Q. Weinberger"],
    "venue": "CoRR, abs/1704.00109,",
    "year": 2017
  }, {
    "title": "Densely connected convolutional networks",
    "authors": ["G. Huang", "Z. Liu", "K.Q. Weinberger"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Deep neural decision forests",
    "authors": ["P. Kontschieder", "M. Fiterau", "A. Criminisi", "S.R. Bulò"],
    "venue": "In IEEE International Conference on Computer Vision, pp",
    "year": 2015
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "venue": "Master’s thesis,",
    "year": 2009
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Fractalnet: Ultra-deep neural networks without residuals",
    "authors": ["G. Larsson", "M. Maire", "G. Shakhnarovich"],
    "year": 2016
  }, {
    "title": "Fully convolutional networks for semantic segmentation",
    "authors": ["J. Long", "E. Shelhamer", "T. Darrell"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Distinctive image features from scale-invariant keypoints",
    "authors": ["D.G. Lowe"],
    "venue": "International Journal of Computer Vision,",
    "year": 2004
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["V. Nair", "G.E. Hinton"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2010
  }, {
    "title": "Deconstructing the ladder network architecture",
    "authors": ["M. Pezeshki", "L. Fan", "P. Brakel", "A.C. Courville", "Y. Bengio"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Scalenet: Guiding object proposal generation in supermarkets and beyond",
    "authors": ["S. Qiao", "W. Shen", "W. Qiu", "C. Liu", "A.L. Yuille"],
    "venue": "In IEEE International Conference on Computer Vision,",
    "year": 2017
  }, {
    "title": "Few-shot image recognition by predicting parameters from activations",
    "authors": ["S. Qiao", "C. Liu", "W. Shen", "A. Yuille"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2018
  }, {
    "title": "Semi-supervised learning with ladder networks",
    "authors": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
    "authors": ["S. Ren", "K. He", "R.B. Girshick", "J. Sun"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "Fitnets: Hints for thin deep",
    "authors": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"],
    "venue": "nets. CoRR,",
    "year": 2014
  }, {
    "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
    "authors": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Lecun"],
    "venue": "International Conference on Learning Representations,",
    "year": 2014
  }, {
    "title": "Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection",
    "authors": ["W. Shen", "X. Wang", "Y. Wang", "X. Bai", "Z. Zhang"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "CoRR, abs/1409.1556,",
    "year": 2014
  }, {
    "title": "Striving for simplicity: The all convolutional net",
    "authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"],
    "venue": "CoRR, abs/1412.6806,",
    "year": 2014
  }, {
    "title": "Going deeper with convolutions",
    "authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"],
    "venue": "In Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Rethinking the inception architecture for computer vision",
    "authors": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"],
    "venue": "In Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "SORT: Second-Order Response Transform for Visual Recognition",
    "authors": ["Y. Wang", "L. Xie", "C. Liu", "S. Qiao", "Y. Zhang", "W. Zhang", "Q. Tian", "A. Yuille"],
    "venue": "International Conference on Computer Vision,",
    "year": 2017
  }, {
    "title": "Dynamics of learning near singularities in layered networks",
    "authors": ["H. Wei", "J. Zhang", "F. Cousseau", "T. Ozeki", "S. Amari"],
    "venue": "Neural Computation,",
    "year": 2008
  }, {
    "title": "Holistically-nested edge detection",
    "authors": ["S. Xie", "Z. Tu"],
    "venue": "IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile,",
    "year": 2015
  }, {
    "title": "Aggregated residual transformations for deep neural networks",
    "authors": ["S. Xie", "R.B. Girshick", "P. Dollár", "Z. Tu", "K. He"],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Multi-scale recognition with dagcnns",
    "authors": ["S. Yang", "D. Ramanan"],
    "venue": "In IEEE International Conference on Computer Vision, pp",
    "year": 2015
  }, {
    "title": "Visualizing and understanding convolutional networks",
    "authors": ["M.D. Zeiler", "R. Fergus"],
    "venue": "CoRR, abs/1311.2901,",
    "year": 2013
  }],
  "id": "SP:123f9307da3d718c71af0ffb6f0cce74396e5759",
  "authors": [{
    "name": "Siyuan Qiao",
    "affiliations": []
  }, {
    "name": "Zhishuai Zhang",
    "affiliations": []
  }, {
    "name": "Wei Shen",
    "affiliations": []
  }, {
    "name": "Bo Wang",
    "affiliations": []
  }, {
    "name": "Alan Yuille",
    "affiliations": []
  }],
  "abstractText": "Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition. The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks. In this paper, we present an alternative method to increase the depth. Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner. The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better. Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.",
  "title": "Gradually Updated Neural Networks for Large-Scale Image Recognition"
}