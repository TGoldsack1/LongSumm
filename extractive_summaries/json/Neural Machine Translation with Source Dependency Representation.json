{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2846–2852 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT.\nIn this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in\n∗Kehai Chen was an internship research fellow at NICT when conducting this work.\n†Corresponding author.\nStatistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016).\nIn this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information. To this end, we create a dependency unit for each source word to capture long-distance dependency constraints. Then we design an Encoder with convolutional architecture to jointly learn SDRs (Section 3) and source dependency annotations, thus computing dependency context vectors and hidden states by a novel double-context based Decoder for word prediction (Section 4). Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves significant gains over the method by Sennrich and Haddow (2016), and thus delivers substantial improvements over the standard attentional NMT (Section 5)."
  }, {
    "heading": "2 Background",
    "text": "An NMT model consists of an Encoder process and a Decoder process, and hence it is often called Encoder-Decoder model (Sutskever et al., 2014; Bahdanau et al., 2014). Typically, each unit of source input xj ∈ (x1, . . . , xJ) is firstly embedded as a vector Vxj , and then represented as\n2846\nan annotation vector hj by\nhj = fenc(Vxj , hj−1), (1)\nwhere fenc is a bidirectional Recurrent Neural Network (RNN) (Bahdanau et al., 2014). These annotation vectors H = (h1, . . . , hJ) are used to generate the target word in the Decoder.\nAn RNN Decoder is used to compute the target word yi probability by a softmax layer g:\np(yi|y<i, x) = g(ŷi−1, si, ci), (2)\nwhere ŷi−1 is the previously emitted word, and si is an RNN hidden state for the current time step:\nsi = ϕ(ŷi−1, si−1, ci), (3)\nand the context vector ci is computed as a weighted sum of these source annotations hj :\nci = J∑\nj=1\nαijhj , (4)\nwhere the normalized alignment weight αij is computed by\nαij = exp(eij)∑J\nk=1 exp(eik) , (5)\nwhere eij is an alignment which indicates how well the inputs around position j and the output at the position i match:\neij = f(si−1, hj). (6)\nwhere f is a feedforward neural network."
  }, {
    "heading": "3 Source Dependency Representation",
    "text": "In order to capture source long-distance dependency constraints, we extract a dependency unit Uj for each source word xj from dependency tree, inspired by a dependency-based bilingual composition sequence for SMT (Chen et al., 2017). The extracted Uj is defined as the following:\nUj = 〈PAxj , SIxj , CHxj 〉, (7)\nwhere PAxj , SIxj , CHxj denote the parent, siblings and children words of source word xj in a dependency tree. Take x2 in Figure 2 as an example, the blue solid box U2 denotes its dependency unit: PAx2 = 〈x3〉, SIx2 = 〈x1, x4, x7〉 and CHx2 = 〈ε〉 (no child), that is, U2 = 〈x3, x1, x4, x7, ε〉.\nWe design a simplified neural network following Chen et al. (2017)’s Convolutional Neural Network (CNN) method, to learn the SDR for each source dependency unit Uj , as shown in Figure 1. Our neural network consists of an input layer, two convolutional layers, two pooling layers and an output layer:\n• Input layer: the input layer takes words of a dependency unitUj in the form of embedding vectors n×d, where n is the number of words in a dependency unit and d is vector dimension of each word. In our experiments, we set n to 10,1 and d is 620. For dependency units shorter than 10, we perform “/” padding at the ending of Uj . For example, the padded U2 is 〈x3, x1, x4, x7, ε, /, /, /, /, /〉.\n1We find that 99% of all the source dependency units contain no more than 10 words. So if the length is more than 10, the extra words are abandoned; if the length is less than 10, the rest positions are padded with “/”.\n• Convolutional layer: the first convolution consists of one 3×d convolution kernels (the stride is 1) to output an (n-2)×d matrix; the second convolution consists of one 3×d convolution kernels to output a n−22 ×d matrix.\n• Max-Pooling layer: the first pooling layer performs row-wise max over the two consecutive rows to output a n−24 ×d matrix; the second pooling layer performs row-wise max over the two consecutive rows to output a n−2\n8 ×d matrix. • Output layer: the output layer performs\nrow-wise average based on the output of the second pooling layer to learn a compact d-dimension vector VUj for Uj . In our experiment, the output of the output layer is 1× d-dimension vector.\nIt should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017). In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder. Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation."
  }, {
    "heading": "4 NMT with SDR",
    "text": "In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT.\n4.1 SDRNMT-1 Compared with standard attentional NMT, the Encoder of SDRNMT-1 model consists of a convolutional architecture and an bidirectional RNN, as shown in Figure 2. Therefore, the proposed Encoder can not only learn compositional representations for dependency units but also\ngreatly tackle the sparsity issues associated with large dependency units.\nMotivated by (Sennrich and Haddow, 2016), we concatenate the Vxj and VUj as input of the Encoder, as shown in the black dotted box in Figure 2. Source annotation vectors are learned based on the concatenated representation with dependency information:\nhj = fenc(Vxj : VUj , hj−1), (8)\nwhere “:” denotes the operation of vectors concatenation. Finally, these learned annotation vectors are as the input of the standard NMT Decoder to jointly learn alignment and translation. The only difference between our method and (Sennrich and Haddow, 2016)’s method is that they only use dependency label representation instead of VUj .\n4.2 SDRNMT-2\nIn SDRNMT-1, a single annotation, learned over concatenating word representation and SDR, is used to compute the context vector and the RNN hidden state for the current time step. To relieve more translation performance for NMT from the SDR, we propose a double-context mechanism, as shown in Figure 3.\nFirst, the Encoder of SDRNMT-2 consists of two independent annotations hj and dj :\nhj = fenc(Vxj , hj−1), dj = fenc(VUj , dj−1),\n(9)\nwhere H = [h1, · · · , hJ ] and D = [d1, · · · , dJ ] encode source sequential and long-distance dependency information, respectively.\nThe Decoder learns the corresponding alignment matrices and context vectors over the H and D, respectively. That is, according to eq.(6), given the previous hidden state ssi−1 and s d i−1, the current alignments esi,j and e d i,j are computed over source annotation vectors hj and dj , respectively:\nesi,j = f(s s i−1 + hj), edi,j = f(s d i−1 + dj).\n(10)\nAccording to eq.(5), we further compute the current alignment α̃:\nα̃i,j = exp(λesi,j + (1− λ)edi,j)∑J\nj=1 exp(λe s i,j + (1− λ)edi,j)\n, (11)\nwhere λ is a hyperparameter2 to control the importance of H and D. Note that compared with the original alignment model only depending on the sequential annotation vectorsH , the alignment weight α̃i,j jointly compute statistic over source sequential annotation vectors H and dependency annotation vectors D.\nThe current context vector csi and c d i are\ncompute by eq.(4), respectively:\ncsi = J∑\nj=1\nα̃i,jhj , and cdi = J∑\nj=1\nα̃i,jdj . (12)\nThe current hidden state ssi and s d i are computed by eq.(3), respectively:\nssi = ϕ(s s i−1, yi−1, c s i ), sdi = ϕ(s d i−1, yi−1, c d i ).\n(13)\nFinally, according to eq.(2), the probabilities for the next target word are computed using two hidden states ssi and s d i , the previously emitted word ŷi−1, the current sequential context vector csi and dependency context vector cdi :\np(yi|y<i, x, T ) = g(ŷi−1, ssi , sdi , csi , cdi ). (14)"
  }, {
    "heading": "5 Experiment",
    "text": ""
  }, {
    "heading": "5.1 Setting up",
    "text": "We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M\n2λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments.\nsentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4- gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test.\nThe baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017).\nFor all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80. The word embedding dimension is 620,5 and the hidden layer dimension\n3LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06.\n4https://github.com/EdinburghNLP/nematus 5For SDRNMT-1, the 360 dimensions are from Vxj and\nthe 260 dimensions are from VUj .\nis 1000, and all the layers use the dropout training technique (Hinton et al., 2012). We shuffle training set before training and the mini-batch size is 80. Training is conducted on a single Tesla P100 GPU. All NMT models train for 15 epochs using ADADELTA (Zeiler, 2012), and the train time is 6 days, which is 25% slower than the standard NMT."
  }, {
    "heading": "5.2 Results and Analyses",
    "text": "Table 1 shows the translation performances on test sets measured in BLEU score. The AttNMT significantly outperforms PBSMT by 2.74 BLEU points on average, indicating that it is a strong baseline NMT system. The baseline Sennrichdeponly improves the performance over the AttNMT by 0.58 BLEU points on average. This indicates that the proposed source dependency constraint is beneficial for improving the performance of NMT.\nMoreover, SDRNMT-1 gains improvements of 0.92 and 0.34 BLEU points on average than the AttNMT and Sennrich-deponly. These show that the proposed SDR can more effectively capture source dependency information than vector concatenation. Especially, the proposed SDRNMT2 outperforms the AttNMT and Sennrich-deponly on average by 1.64 and 1.03 BLEU points. These verify that the proposed double-context method is effective for word prediction."
  }, {
    "heading": "5.3 Effect of Translating Long Sentences",
    "text": "We follow (Bahdanau et al., 2014) to group sentences of similar lengths all the test sets (MT03-08), for example, “40” indicates that the length of sentences is between 30 and 40, and compute a BLEU score per group. As demonstrated in Figure 4, the proposed models outperform other baseline systems, especially in translating long sentences. These results show that the proposed models can effective encode longdistance dependencies to improve translation."
  }, {
    "heading": "6 Conclusion and Future Work",
    "text": "In this paper, we explored the source dependency information to improve the performance of NMT. We proposed a novel attentional NMT with source dependency representation to capture source longdistance dependencies. In the future, we will try to exploit a general framework for utilizing richer syntax knowledge."
  }, {
    "heading": "Acknowledgments",
    "text": "We are grateful to the anonymous reviewers for their insightful comments and suggestions. This work is partially supported by the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of MIC, Japan. Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and National High Technology Research & Development Program of China (863 program) via grant 2015AA015405."
  }],
  "year": 2017,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "CoRR, abs/1409.0473.",
    "year": 2014
  }, {
    "title": "Findings of the 2016 conference on machine translation",
    "authors": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."],
    "venue": "Proceedings of the First Conference on Machine Translation,",
    "year": 2016
  }, {
    "title": "Discriminative reordering with Chinese grammatical relations features",
    "authors": ["Pi-Chuan Chang", "Huihsin Tseng", "Dan Jurafsky", "Christopher D. Manning."],
    "venue": "Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation at",
    "year": 2009
  }, {
    "title": "Translation prediction with source dependency-based context representation",
    "authors": ["Kehai Chen", "Tiejun Zhao", "Muyun Yang", "Lemao Liu."],
    "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 3166–3172, California,",
    "year": 2017
  }, {
    "title": "Clause restructuring for statistical machine translation",
    "authors": ["Michael Collins", "Philipp Koehn", "Ivona Kucerova."],
    "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 531–540, Ann Arbor, Michigan.",
    "year": 2005
  }, {
    "title": "Fast and robust neural network joint models for statistical machine translation",
    "authors": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association",
    "year": 2014
  }, {
    "title": "Tree-to-sequence attentional neural machine translation",
    "authors": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 823–833, Berlin,",
    "year": 2016
  }, {
    "title": "Learning to parse and translate improves neural machine translation",
    "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism",
    "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Dependency-based bilingual language models for reordering in statistical machine translation",
    "authors": ["Ekaterina Garmash", "Christof Monz."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2014
  }, {
    "title": "To swap or not to swap? exploiting dependency word pairs for reordering in statistical machine translation",
    "authors": ["Christian Hadiwinoto", "Yang Liu", "Hwee Tou Ng."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages",
    "year": 2016
  }, {
    "title": "A dependency-based neural reordering model for statistical machine translation",
    "authors": ["Christian Hadiwinoto", "Hwee Tou Ng."],
    "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, California, USA. AAAI Press.",
    "year": 2017
  }, {
    "title": "Improving neural networks by preventing co-adaptation of feature detectors",
    "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "CoRR.",
    "year": 2012
  }, {
    "title": "Improved neural machine translation with a syntax-aware encoder and decoder",
    "authors": ["Chen Huadong", "Huang Shujian", "Chiang David", "Chen Jiajun."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
    "year": 2017
  }, {
    "title": "Recurrent continuous translation models",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700– 1709, Seattle, Washington, USA. Association for",
    "year": 2013
  }, {
    "title": "Dependency-based reordering model for constituent pairs in hierarchical smt",
    "authors": ["Arefeh Kazemi", "Antonio Toral", "Andy Way", "Amirhassan Monadjemi", "Mohammadali Nematbakhsh."],
    "venue": "Proceedings of the 18th Annual Conference of the European",
    "year": 2015
  }, {
    "title": "Modeling source syntax for neural machine translation",
    "authors": ["Junhui Li", "Xiong Deyi", "Tu Zhaopeng", "Zhu Muhua", "Zhou Guodong."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada.",
    "year": 2017
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the 26th International Conference on Neural Information",
    "year": 2013
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
    "year": 2002
  }, {
    "title": "Nematus: a toolkit for neural machine",
    "authors": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"],
    "year": 2017
  }, {
    "title": "Linguistic input features improve neural machine translation",
    "authors": ["Rico Sennrich", "Barry Haddow."],
    "venue": "Proceedings of the First Conference on Machine Translation, pages 83–91, Berlin, Germany. Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104–3112, Cambridge,",
    "year": 2014
  }, {
    "title": "ADADELTA: an adaptive learning rate method",
    "authors": ["Matthew D. Zeiler."],
    "venue": "CoRR, abs/1212.5701.",
    "year": 2012
  }, {
    "title": "Multisource neural translation",
    "authors": ["Barret Zoph", "Kevin Knight."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter",
    "year": 2016
  }],
  "id": "SP:ed98d706f512b5e40e684109b8f33419aa6c096c",
  "authors": [{
    "name": "Kehai Chen",
    "affiliations": []
  }, {
    "name": "Rui Wang",
    "affiliations": []
  }, {
    "name": "Masao Utiyama",
    "affiliations": []
  }, {
    "name": "Lemao Liu",
    "affiliations": []
  }, {
    "name": "Akihiro Tamura",
    "affiliations": []
  }, {
    "name": "Eiichiro Sumita",
    "affiliations": []
  }, {
    "name": "Tiejun Zhao",
    "affiliations": []
  }],
  "abstractText": "Source dependency information has been successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.",
  "title": "Neural Machine Translation with Source Dependency Representation"
}