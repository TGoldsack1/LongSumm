{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1033–1043 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1033"
  }, {
    "heading": "1 Introduction",
    "text": "Detection problems, aiming to identify occurrences of specific kinds of information (e.g., events, relations, or entities) in documents, are fundamental and widespread in information extraction (IE). For instance, an event detection (Walker et al., 2006) system may want to detect triggers for “Attack” events, such as “shot” in sentence “He was shot”. In relation detection (Hendrickx et al., 2009), we may want to identify all instances of a specific relation, such as “Jane joined Google” for “Employment” relation.\nRecently, a number of researches have employed neural network models to solve detection problems, and have achieved significant improvement in many tasks, such as event detection (Chen et al., 2015; Nguyen and Grishman, 2015), relation\ndetection (Zeng et al., 2014; Santos et al., 2015) and named entity recognition (Huang et al., 2015; Chiu and Nichols, 2015; Lample et al., 2016). These methods usually regard detection problems as standard classification tasks, with several positive classes for targets to detect and one negative class for irrelevant (background) instances. For example, an event detection model will identify event triggers in sentence “He was shot” by classifying word “shot” into positive class “Attack”, and classifying all other words into the negative class “NIL”. To optimize classifiers, cross-entropy loss function is commonly used in this paradigm.\nHowever, different from standard classification tasks, detection tasks have unique class inequality characteristic, which stems from both data distribution and applied evaluation metric. Table 1 shows their differences. First, positive instances are commonly sparsely distributed in detection tasks. For example, in event detection, less than 2% of words are a trigger of an event in RichERE dataset (Song et al., 2015). Furthermore, detection tasks are commonly evaluated using F-measure on positive classes, rather than accuracy or F-measure on all classes. Therefore positive and negative classes play different roles in the evaluation: the performance is evaluated by only considering how well we can detect positive instances, while correct predictions of negative instances are ignored.\nDue to the class inequality characteristic, reported results indicate that simply applying stan-\ndard classification paradigm to detection tasks will result in deficient performance (Anand et al., 1993; Carvajal et al., 2004; Lin et al., 2017). This is because minimizing cross-entropy loss function corresponds to maximize the accuracy of neural networks on all training instances, rather than Fmeasure on positive classes. Furthermore, due to the positive sparsity problem, training procedure will easily achieve a high accuracy on negative class, but is difficult to converge on positive classes and often leads to a low recall rate. Although simple sampling heuristics can alleviate this problem to some extent, they either suffer from losing inner class information or over-fitting positive instances (He and Garcia, 2009; Fernández-Navarro et al., 2011), which often result in instability during the training procedure.\nSome previous approaches (Joachims, 2005; Jansche, 2005, 2007; Dembczynski et al., 2011; Chinta et al., 2013; Narasimhan et al., 2014; Natarajan et al., 2016) tried to solve this problem by directly optimizing F-measure. Parambath et al. (2014) proved that it is sufficient to solve F-measure optimization problem via cost-sensitive learning, where class-specific cost factors are applied to indicate the importance of different classes to F-measure. However, optimal factors are not known a priori so ε-search needs to be applied, which is too time consuming for the optimization of neural networks.\nTo solve the class inequality problem for sparse detection model optimization, this paper proposes a theoretical framework to quantify the importance of positive/negative instances during training. We borrow the idea of marginal utility from Economics (Stigler, 1950), and regard the evaluation metric (i.e., F-measure commonly) as the utility to optimize. Based on the above idea, the importance of an instance is measured using the marginal utility of correctly predicting it. For standard classification tasks evaluated using accuracy, our framework proves that correct predictions of positive and negative instances will have equal and unchanged marginal utility, i.e., all instances are with the same importance. For detection problems evaluated using F-measure, our framework proves that the utility of correctly predicting one more positive instance (marginal positive utility) and that of correctly predicting one more negative instance (marginal negative utility) are different and dynamically changed during model training.\nThat is, the importance of instances of each class is not only determined by their data distribution, but also affected by how well the current model can converge on different classes.\nBased on the above framework, we propose adaptive scaling, a dynamic cost-sensitive learning algorithm which adaptively scales costs of instances of different classes with above quantified importance during the training procedure, and thus can make the optimization criteria consistent with the evaluation metric. Furthermore, a batchwise version of our adaptive scaling algorithm is proposed to make it directly applicable as a plug-in of conventional neural network training algorithms. Compared with previous methods, adaptive scaling is designed based on marginal utility framework and doesn’t introduce any additional hyper-parameter, and therefore is more efficient and stable to transfer among datasets and models.\nGenerally, the main contributions of this paper are:\n• We propose a marginal utility based framework for detection model optimization, which can dynamically quantify instance importance to different evaluation metrics.\n• Based on the above framework, we present adaptive scaling, a plug-in algorithm which can effectively resolve the class inequality problem in neural detection model optimization via dynamic cost-sensitive learning.\nWe conducted experimental studies1 on event detection, a typical sparse detection task in IE. We thoroughly compared various methods for adapting classical neural network models into detection problems. Experiment results show that our adaptive scaling algorithm not only achieves a better performance, but also is more stable and more adaptive for training neural networks on various models and datasets."
  }, {
    "heading": "2 Background",
    "text": "Relation between Accuracy Metric and CrossEntropy Loss. Recent neural network methods usually regard detection problems as standard classification tasks, with several positive classes to detect, and one negative class for other irrelevant\n1Our source code is openly available at github.com/ sanmusunrise/AdaScaling.\ninstances. Formally, given P positive training instances P = {(xi, yi)Pi=1}, and N negative instances N = {(xi, yi)Ni=1} (due to positive sparsity, P N ), the training of neural network classifiers usually involves in minimizing the softmax cross-entropy loss function regarding to model parameters θ:\nLCE(θ) = − 1\nP +N ∑ (xi,yi)∈P ⋃ N log p(yi|xi; θ) (1)\nand if P,N →∞, we have\nlim P,N→∞ LCE(θ) = −E[log p(y|x; θ)] = − log(Accuracy) (2) which reveals that minimizing cross-entropy loss corresponds to maximize the expected accuracy of the classifier on training data. Divergence between F-Measure and CrossEntropy Loss. However, detection tasks are mostly evaluated using F-measure computed on positive classes, which makes it unsuitable to optimize classifiers using cross-entropy loss. For instance, due to the positive sparsity, simply classifying all instances into negative class will achieve a high accuracy but zero F-measure.\nTo show where this divergence comes from, let c1, c2, ..., ck−1 denote k−1 positive classes and ck is the negative class, we define TP = ∑k−1 i=1 TPi, where TPi is the population of correctly predicted instances of positive class ci. TN denotes the number of correctly predicted negative instances. PE represents positive-positive error, where an instance is classified into one positive class ci but its golden label is another positive class cj . Then we have following metrics2:\nAccuracy = TP + TN\nP +N (3)\nPrecision = TP\nN − TN + PE + TP (4)\nRecall = TP\nP (5)\nFβ = (1 + β 2) Precision · Recall β2 · Precision + Recall\n= (1 + β2) TP\nβ2P +N − TN + PE + TP\n(6)\nwhere β in Fβ is a factor indicating the metric attaches β times as much importance to recall as\n2This paper considers micro-averaged metrics. But our conclusions can be easily extended to macro-averaged metrics by scaling above-mentioned coefficients with sample sizes of each class.\nprecision. We can easily see that for accuracy metric, correct predictions of positive and negative instances are equally regarded (i.e., TP and TN are symmetric), which is consistent with crossentropy loss function. However, when measuring using F-measure, this condition is no longer holding. The importance varies from different classes (i.e., TP and TN are asymmetric). Therefore, to make the training procedure consistent with F-measure, it is critical to take this importance difference into consideration. F-measure Optimization via Cost-sensitive Learning. Parambath et al. (2014) have shown that F-measure can be optimized via cost-sensitive learning, where a cost (importance) is set for each class for adjusting their impact on model learning. However, most previous studies set such costs manually (Anand et al., 1993; Domingos, 1999; Krawczyk et al., 2014) or search them on large scale dataset (Nan et al., 2012; Parambath et al., 2014), whose best settings are not transferable and very time-consuming to find for neural network models. This motivates us to develop a theoretical framework for measuring such importance."
  }, {
    "heading": "3 Adaptive Scaling for Sparse Detection",
    "text": "This section describes how to effectively optimize neural network detection models via dynamic cost-sensitive learning. Specifically, we first propose a marginal utility based theoretical framework for measuring the importance of positive/negative instances. Then we present our adaptive scaling algorithm, which can leverage the importance of each class for effective and robust training of neural network detection models. Finally, a batch-wise version of our algorithm is proposed to make it can be applied as a plug-in of batch-based neural network training algorithms."
  }, {
    "heading": "3.1 Marginal Utility based Importance Measuring",
    "text": "Conventional methods commonly deal with the class inequality problem in sparse detection by deemphasizing the importance of negative instances during training. This raises two questions: 1) How to quantify the importance of instances of each class? As mentioned by Parambath et al. (2014), that importance is related to the convergence ability of models, which means that this problem cannot be solved by only considering the distribution of training data. 2) Is the im-\nportance of positive/negative instances remaining unchanged during the entire training process? If not, how it changes according to the convergence of the model?\nTo this end, we borrow the idea of marginal utility from economics, which means the change of utility from consuming one more unit of product. In detection tasks, we regard its evaluation metric (F-measure) as the utility function. The increment of utility from correctly predicting one more positive instance (marginal positive utility) can be regarded as the relative importance of positive classes, and that from correctly predicting one more negative instance (marginal negative utility) is look upon as the relative importance of the negative class. If marginal positive utility overweighs marginal negative utility, positive instances should be considered more important during optimization because it can lead to more improvement on the evaluation metric. In contrast, if marginal negative utility is higher, training procedure should incline to negative instances since it is more effective for optimizing the evaluation metric.\nFormally, we derive marginal positive utility MU(TP ) and marginal negative utility MU(TN) by computing the partial derivative of the evaluation metric with respect to TP and TN respectively. For instance, the marginal positive utilityMUacc(TP ) and the marginal negative utility MUacc(TN) regarding to accuracy metric are:\nMUacc(TP ) = ∂(Accuracy) ∂(TP ) = 1 P +N (7)\nMUacc(TN) = ∂(Accuracy) ∂(TN) = 1 P +N (8)\nWe can see thatMUacc(TP ) andMUacc(TN) are equal and constant regardless of the values of TP and TN . This indicates that, to optimize accuracy, we can simply treat positive and negative instances equally during the training phase, and this is what we exactly do when optimizing cross-entropy loss in Equation 1. For detection problems evaluated using F-measure, we can obtain the marginal utilities from Equation 6 as:\nMUFβ (TP ) = (1 + β2)(β2P +N − TN + PE) (β2P +N − TN + PE + TP )2 (9)\nMUFβ (TN) = (1 + β2) · TP\n(β2P +N − TN + PE + TP )2 (10)\nThis result is different from that of accuracy metric. First, MUFβ (TP ) and MUFβ (TN) is\nno longer equal, indicating that the importance of positive/negative instances to F-measure are different. Besides, it is notable that MUFβ (TP ) and MUFβ (TN) are dynamically changed during the training phase and are highly related to how well current model can fit positive instances and negative instances, i.e., TP and TN ."
  }, {
    "heading": "3.2 Adaptive Scaling Algorithm",
    "text": "In this section, we describe how to incorporate the above importance measures into the training procedure of neural networks, so that it can dynamically adjust weights of positive and negative instances regarding to F-measure.\nSpecifically, given the current model of neural networks parameterized by θ, let wβ(θ) denote the relative importance of negative instances to positive instances for Fβ-measure. Then wβ(θ) can be computed as the ratio of marginal negative utility MUFβ (TN(θ)) to the marginal positive utility MUFβ (TP (θ)), where TP (θ) and TN(θ) are TP and TN on training data with respect to θ-parameterized model:\nwβ(θ) = MUFβ (TN(θ))\nMUFβ (TP (θ)) =\nTP (θ)\nβ2P +N − TN(θ) + PE (11)\nThen at each iteration of the model optimization (i.e., each step of gradient descending), we want the model to take next update step proportional to the gradient of the wβ-scaled cross-entropy loss function LAS(θ) at the current point:\nLAS(θ) =− ∑\n(xi,yi)∈P\nlog p(yi|xi; θ)\n− ∑\n(xi,yi)∈N\nwβ(θ) · log p(yi|xi; θ) (12)\nConsequently, based on the contributions that correctly predicting one more instances of each class bringing to F-measure, the training procedure dynamically adjusts its attention between positive and negative instances. Thus our adaptive scaling algorithm can take the class inequality characteristic of detection problems into consideration without introducing any additional hyper-parameter3."
  }, {
    "heading": "3.3 Properties and Relations to Previous Empirical Conclusions",
    "text": "In this section, we investigate the properties of our adaptive scaling algorithm. By investigating the\n3Note that β is set according to the applied Fβ evaluation metric and therefore is not a hyper-parameter.\nchange of scaling coefficient wβ(θ) during training, we find that our method has a tight relation to previous empirical conclusions on solving the class inequality problem.\nProperty 1. The relative importance of positive/negative instances is related to the ratio of the instance number of each class, as well as how well current model can fit each class. It is easy to derive that if we fix the accuracies of each classes, wβ(θ) will be smaller if the ratio of the size of negative instances to that of the positive instances (i.e., NP ) increases. This indicates that the training procedure should pay more attention to positive instances if the empirical distribution inclines more severely towards negative class, which is identical to conventional practice that we should deemphasize more on negative instances if the positive sparsity problem is more severe (Japkowicz and Stephen, 2002). Besides, wβ(θ) highly depends on TP and TN , which is identical to previous conclusion that the best cost factors are related to the convergence ability of models (Parambath et al., 2014).\nProperty 2. For micro-averaged F-measure, all positive instances are equally weighted regardless of the sample size of its class. Let MU(TPi) be the marginal utility of positive class ci, we have:\nMUFβ (TPi) = ∂(Fβ) ∂(TP ) · ∂(TP ) ∂(TPi) =MUFβ (TP )\n(13)\nThis corresponds to the applied micro-averaged F-measure, in which all positive instances are equally considered regardless of the sample size of its class. Thus correctly predicting one more positive instance of any class will result in the same increment of micro-averaged F-measure.\nProperty 3. The importance of negative instances increases with the rise of accuracy on positive classes. This is a straightforward consequence because if the model has higher accuracy on positive instances then it should shift more of its attention to negative ones. Besides, if the accuracy of positive class is close to zero, F-measure will also be close to zero no matter how high the accuracy on negative class is, i.e., correctly predicting negative instances can result in little F-measure increment. Therefore negative instances are inconsequential when the accuracy on positive class is low. And with the increment of positive accuracy, the importance of negative class also increases.\nProperty 4. The importance of negative instances increased with the rise of accuracy on the negative class. This can make the training procedure incline to hard negative instances, which is similar to Focal Loss (Lin et al., 2017). During model convergence, easy negative instances can be correctly classified at the very beginning of training and its loss (negative log probability) will reduce very quickly. This is analogical to removing easy negative instances out of the training procedure and the hard negative instances remaining become more balanced proportional to positive instances. Therefore the importance wβ of remaining hard negative instances are increased to make the model fit them better.\nProperty 5. The importance of negative instances increases when more attention is paid to precision than recall. We can see that wβ decreases with the rise of β, which indicates we focus more on recall than precision. This is identical to practice in sampling heuristics that models should attach more attention to negative instances and sub-sample more of them if evaluation metrics incline more to precision than recall."
  }, {
    "heading": "3.4 Batch-wise Adaptive Scaling",
    "text": "In large-scale machine learning, batch-wise gradient based algorithm is more popular and efficient for neural network training. This section presents a batch-wise version of our adaptive scaling algorithm, which uses batch-based estimator ŵβ(θ) to replace wβ(θ) in Equation 12.\nFirst, because the main challenge of detection tasks is to identify positive instances from background ones, rather than distinguish between positive classes, we ignore the positive-positive error PE in our experiments. In fact, we found that compared with P and N − TN , PE is much smaller and has very limited impact on the final result. Besides, for TP and TN , we approximate them using their expectation on the current batch, which can produce a robust estimation even when the batch size is not large enough. Specifically, let PB = {(xi, yi)P B\ni=1} denotes PB positive instances andNB = {(xi, yi)N B\ni=1} is NB negative instances in the batch, we estimate TP (θ) and TN(θ) as:\nTPB(θ) = ∑\n(xi,yi)∈PB p(yi|xi; θ) (14)\nTNB(θ) = ∑\n(xi,yi)∈NB p(yi|xi; θ) (15)\nThen we can compute the estimator ŵβ(θ) for wβ(θ) as:\nŵβ(θ) = TPB(θ)\nβ2PB +NB − TNB(θ) (16)\nwhere ŵβ(θ) is computed using only the instances in a batch, which makes it can be directly applied as a plug-in of conventional batch-based neural network optimization algorithm where the loss of negative instances in batch are scaled by ŵβ(θ)."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Data Preparation",
    "text": "To assess the effectiveness of our method, we conducted experiments on event detection, which is a typical detection task in IE. We used the official evaluation datasets of TAC KBP 2017 Event Nugget Detection Evaluation (LDC2017E55) as test sets, which contains 167 English documents and 167 Chinese documents annotated with Rich ERE annotation standard. For English, we used previously annotated RichERE datasets, including LDC2015E29, LDC2015E68, LDC2016E31 and TAC KBP 2015-2016 Evaluation datasets in LDC2017E02 as the training set. For Chinese, the training set includes LDC2015E105, LDC2015E112, LDC2015E78 and the Chinese part of LDC2017E02. For both Chinese and English, we sampled 20 documents from the evaluation dataset of 2016 year as the development set. Finally, there are 866/20/167 documents in English train/development/test set and 506/20/167 documents in Chinese train/development/test set respectively. We used Stanford CoreNLP toolkit (Manning et al., 2014) for sentence splitting and word segmentation in Chinese."
  }, {
    "heading": "4.2 Baselines",
    "text": "To verify the effectiveness of our adaptive scaling algorithm, we conducted experiments on two state-of-the-art neural network event detection models. The first one is Dynamic Multipooling Convolutional Neural network (DMCNN) proposed by Chen et al. (2015), a one-layer CNN model with a dynamic multi-pooling operation over convolutional feature maps. The second one is BiLSTM used by Feng et al. (2016) and Yang and Mitchell (2017), where a bidirectional LSTM layer is firstly applied to the input sentence and then word-wise classification is directly conducted on the output of the BiLSTM layer of each word.\nWe compared our method with following baselines upon above-mentioned two models:\n1) Vanilla models (Vanilla), which used the original cross-entropy loss function without any additional treatment for class inequality problem.\n2) Under-sampling (Sampling), which samples only part of negative instances as the training data. This is the most widely used solution in event detection (Chen et al., 2015).\n3) Static scaling (Scaling), which scales loss of negative instances with a constant. This is a simple but effective cost-sensitive learning method.\n4) Focal Loss (Focal) (Lin et al., 2017), which scales loss of an instance with a factor proportional to the probability of incorrectly predicting it. This method proves to be effective in some detection problems such as Object Detection.\n5) Softmax-Margin Loss (CLUZH) (Makarov and Clematide, 2017), which sets additional costs for false-negative error and positive-positive error. This method was used in the 5-model ensembling CLUZH system in TAC KBP 2017 Evaluation. Besides, it also introduced several strong handcraft features, which makes it achieve the best performance on Chinese and very competitive performance on English in the evaluation.\nWe evaluated all systems with micro-F1 metric computed using the official evaluation toolkit4. We reported the average performance of 10 runs (Mean) of each system on the official type classification task.5 We also reported the variance (Var) of the performance to evaluate the stabilities of different methods. As TAC KBP2017 allowed each team to submit 3 different runs, to make our results comparable with evaluation results, we selected 3 best runs of each system on the development set and reported the best test set performance among them, which is referred as Best3 in this paper. We applied grid search (Hsu et al., 2003) to find best hyper-parameters for all methods."
  }, {
    "heading": "4.3 Overall results",
    "text": "Table 2 shows the overall results on both English and Chinese. From this table, we can see that:\n1) The class inequality problem is crucial for sparse detection tasks and requires special consideration. Compared with vanilla models, all\n4github.com/hunterhector/EvmEval/ tarball/master\n5Realis classification, another task in the evaluation, can be regarded as a standard classification task without background class, so we didn’t include it here.\nother methods trying to tackle this problem have shown significant improvements on both models and both languages, especially on Chinese dataset where the positive sparsity problem is more severe (Makarov and Clematide, 2017).\n2) It is critical to take the different roles of classes into consideration for F-measure optimization. Even down-weighting the loss assigned to well-classified examples can alleviate the positive sparsity problem by deemphasizing easy negative instances during optimization, Focal Loss cannot achieve competitive performance because it does not distinguish between different classes.\n3) Marginal Utility based framework provides a solid foundation for measuring instance importance, thus makes our adaptive scaling algorithm steadily outperform all heuristic baselines. No matter on mean or Best3 metric, adaptive scaling steadily outperforms other baselines on both BiLSTM and DMCNN model. Furthermore, we can see that simple models with adaptive scaling outperform the state-of-the-art CLUZH system on Chinese (which has more severe positive sparsity problem) and achieve comparable results with it on English. Please note that CLUZH is an ensemble of five models and uses extra hand-crafted features. This verified the effectiveness of our adaptive scaling algorithm.\n4) Our adaptive scaling algorithm doesn’t need additional hyper-parameters and the importance of instances is dynamically estimated. This leads to a more stable and transferable solution for detection model optimization. First, we can see that adaptive scaling has the lowest\nvariance among all methods, which means that it is more stable than other methods. Besides, adaptive scaling doesn’t introduce any additional hyper-parameters. In contrast, in experiment we found that the best hyper-parameters for undersampling (the ratio of sampled negative instances to positive instances) and static scaling (the prior cost for negative instances) remarkably varied from models and datasets."
  }, {
    "heading": "4.4 Stability Analysis",
    "text": "This section investigated the stability of different methods. Table 2 have shown that adaptive scaling has a much smaller variance than other baselines. To investigate its reason, Figure 1 shows the box plots of adaptive scaling and other heuristic methods on both models and both languages.\nWe can see that interquartile ranges (i.e., the difference between 75th and 25th percentiles of data) of the performances of adaptive scaling are smaller than other methods. In all groups of experiments, the performances of our adaptive scaling algorithm are with a smaller fluctuation. This demonstrates the stability of adaptive scaling algorithm. Furthermore, we found that conventional methods are more instable on Chinese dataset where the data distribution is more skewed. We believe that this is because:\n1) Under-Sampling might undermine the inner sub-concept structure of negative class by simply dropping negative instances, and its performance depends on the quality of sampled data, which can result in the instability.\n2) Static scaling sets the importance of negative instances statically in the entire training procedure. However, as shown in Section 3, the rel-\native importance between different classes is dynamically changed during the training procedure, which makes static scaling incapable of achieving stable performance in different phases of training.\n3) Adaptive scaling achieves more stable performance during the entire training procedure. First, it doesn’t drop any instances, so it can maintain the inner structure of negative class without any information loss. Besides, our algorithm can dynamically adjust the scaling factor during training, therefore can automatically shift attention between positive and negative classes according to the convergence state of the model."
  }, {
    "heading": "4.5 Adaptability on Different β",
    "text": "Figure 2 shows the change of Precision, Recall and F1 measures regarding to different β. We can see that when β increases, the precision decreased and the recall increased by contrast. This is identical to the nature of Fβ where β represents the relative importance of precision and recall. Furthermore, adaptive scaling with β = 1 achieved the best performance on F1 measure. This further demonstrates that wβ derived from our marginal utility framework is a good and adaptive estimator for the relative importance of the negative class to positive classes of Fβ measure."
  }, {
    "heading": "5 Related Work",
    "text": "This paper proposes adaptive scaling algorithm for sparse detection problem. Related work to this paper mainly includes: Classification on Imbalanced Data. Conventional approaches addressed data imbalance from either data-level or algorithm-level. Data-level approaches resample the training data to maintain the balance between different classes (Japkowicz and Stephen, 2002; Drummond et al., 2003). Further improvements on this direction involve how to better sampling data with minimum in-\nformation loss (Carvajal et al., 2004; Estabrooks et al., 2004; Han et al., 2005; Fernández-Navarro et al., 2011). Algorithm-level approaches attempt to choose an appropriate inductive bias on models or algorithms to make them more suitable on data imbalance condition, including instance weighting (Ting, 2002; Lin et al., 2017), cost-sensitive learning (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014) and active learning approaches (Ertekin et al., 2007a,b; Zhu and Hovy, 2007). F-Measure Optimization. Previous research on F-measure optimization mainly fell into two paradigms (Nan et al., 2012): 1) Decisiontheoretic approaches (DTA), which first estimate a probability model and find the optimal predictions according to that model (Joachims, 2005; Jansche, 2005, 2007; Dembczynski et al., 2011; BusaFekete et al., 2015; Natarajan et al., 2016). The main drawback of these methods is that they need to estimate the joint probability with exponentially many combinations, thus make them hard to use in practice; 2) Empirical utility maximization (EUM) approaches, which adapt approximate methods to find a best classifier in hypothesises (Musicant et al., 2003; Chinta et al., 2013; Parambath et al., 2014; Narasimhan et al., 2014). However, EUM methods depend on thresholds or costs that are not known a priori so time-consuming searching on large development set is required. Our adaptive scaling algorithm is partially inspired by EUM approaches, but is based on the marginal utility framework, which doesn’t introduce any additional hyper-parameter or searching procedure. Neural Network based Event Detection. Event detection is a typical task of detection problems. Recently neural network based methods have achieved significant progress in Event Detection. CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) are two effective and widely used models. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) or introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016)."
  }, {
    "heading": "6 Conclusions",
    "text": "This paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive\nsparsity problem and directly optimize F-measure by adaptively scaling the influence of negative instances in loss function. Based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimization of neural networks without introducing additional hyper-parameters. Experiments on event detection verified the effectiveness and stability of our adaptive scaling algorithm.\nThe divergence between loss functions and evaluation metrics is common in NLP and machine learning. In the future we want to apply our marginal utility based framework to other metrics, such as Mean Average Precision (MAP)."
  }, {
    "heading": "Acknowledgments",
    "text": "We sincerely thank the reviewers for their valuable comments. Moreover, this work is supported by the National Natural Science Foundation of China under Grants no. 61433015, 61572477 and 61772505, and the Young Elite Scientists Sponsorship Program no. YESS20160177."
  }],
  "year": 2018,
  "references": [{
    "title": "An improved algorithm for neural network classification of imbalanced training sets",
    "authors": ["Rangachari Anand", "Kishan G Mehrotra", "Chilukuri K Mohan", "Sanjay Ranka."],
    "venue": "IEEE Transactions on Neural Networks, 4(6):962–969.",
    "year": 1993
  }, {
    "title": "Online f-measure optimization",
    "authors": ["Róbert Busa-Fekete", "Balázs Szörényi", "Krzysztof Dembczynski", "Eyke Hüllermeier."],
    "venue": "Advances in Neural Information Processing Systems, pages 595–603.",
    "year": 2015
  }, {
    "title": "Neural network method for failure detection with skewed class distribution",
    "authors": ["K Carvajal", "M Chacón", "D Mery", "G Acuna."],
    "venue": "Insight-Non-Destructive Testing and Condition Monitoring, 46(7):399–402.",
    "year": 2004
  }, {
    "title": "Event extraction via dynamic multi-pooling convolutional neural networks",
    "authors": ["Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao."],
    "venue": "Proceedings of ACL 2015.",
    "year": 2015
  }, {
    "title": "Optimizing f-measure with non-convex loss and sparse linear classifiers",
    "authors": ["Punya Murthy Chinta", "P Balamurugan", "Shirish Shevade", "M Narasimha Murty."],
    "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on, pages 1–8. IEEE.",
    "year": 2013
  }, {
    "title": "Named entity recognition with bidirectional lstm-cnns",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "arXiv preprint arXiv:1511.08308.",
    "year": 2015
  }, {
    "title": "An exact algorithm for f-measure maximization",
    "authors": ["Krzysztof J Dembczynski", "Willem Waegeman", "Weiwei Cheng", "Eyke Hüllermeier."],
    "venue": "Advances in",
    "year": 2011
  }, {
    "title": "Metacost: A general method for making classifiers cost-sensitive",
    "authors": ["Pedro M. Domingos."],
    "venue": "KDD.",
    "year": 1999
  }, {
    "title": "C4. 5, class imbalance, and cost sensitivity: why undersampling beats over-sampling",
    "authors": ["Chris Drummond", "Robert C Holte"],
    "venue": "In Workshop on learning from imbalanced datasets II,",
    "year": 2003
  }, {
    "title": "Learning on the border: active learning in imbalanced data classification",
    "authors": ["Seyda Ertekin", "Jian Huang", "Leon Bottou", "Lee Giles."],
    "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages",
    "year": 2007
  }, {
    "title": "Active learning for class imbalance problem",
    "authors": ["Seyda Ertekin", "Jian Huang", "C Lee Giles."],
    "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 823–824. ACM.",
    "year": 2007
  }, {
    "title": "A multiple resampling method for learning from imbalanced data sets",
    "authors": ["Andrew Estabrooks", "Taeho Jo", "Nathalie Japkowicz."],
    "venue": "Computational intelligence, 20(1):18–36.",
    "year": 2004
  }, {
    "title": "A languageindependent neural network for event detection",
    "authors": ["Xiaocheng Feng", "Lifu Huang", "Duyu Tang", "Bing Qin", "Heng Ji", "Ting Liu."],
    "venue": "Proceedings of ACL 2016.",
    "year": 2016
  }, {
    "title": "A dynamic over-sampling procedure based on sensitivity for multi-class problems",
    "authors": ["Francisco Fernández-Navarro", "César Hervás-Martı́nez", "Pedro Antonio Gutiérrez"],
    "venue": "Pattern Recognition,",
    "year": 2011
  }, {
    "title": "Event nugget detection with forward-backward recurrent neural networks",
    "authors": ["Reza Ghaeini", "Xiaoli Z Fern", "Liang Huang", "Prasad Tadepalli."],
    "venue": "Proceedings of ACL 2016.",
    "year": 2016
  }, {
    "title": "Borderline-smote: a new over-sampling method in imbalanced data sets learning",
    "authors": ["Hui Han", "Wen-Yuan Wang", "Bing-Huan Mao."],
    "venue": "International Conference on Intelligent Computing, pages 878– 887. Springer.",
    "year": 2005
  }, {
    "title": "Learning from imbalanced data",
    "authors": ["Haibo He", "Edwardo A Garcia."],
    "venue": "IEEE Transactions on knowledge and data engineering, 21(9):1263–1284.",
    "year": 2009
  }, {
    "title": "Semeval-2010 task 8: Multi-way classification of semantic relations",
    "authors": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"],
    "year": 2009
  }, {
    "title": "A practical guide to support vector classification",
    "authors": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"],
    "year": 2003
  }, {
    "title": "Bidirectional lstm-crf models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "arXiv preprint arXiv:1508.01991.",
    "year": 2015
  }, {
    "title": "Maximum expected f-measure training of logistic regression models",
    "authors": ["Martin Jansche."],
    "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 692–699. Association for",
    "year": 2005
  }, {
    "title": "A maximum expected utility framework for binary sequence labeling",
    "authors": ["Martin Jansche."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 736–743.",
    "year": 2007
  }, {
    "title": "The class imbalance problem: A systematic study",
    "authors": ["Nathalie Japkowicz", "Shaju Stephen."],
    "venue": "Intelligent data analysis, 6(5):429–449.",
    "year": 2002
  }, {
    "title": "A support vector method for multivariate performance measures",
    "authors": ["Thorsten Joachims."],
    "venue": "Proceedings of the 22nd international conference on Machine learning, pages 377–384. ACM.",
    "year": 2005
  }, {
    "title": "Cost-sensitive decision tree ensembles for effective imbalanced classification",
    "authors": ["Bartosz Krawczyk", "Michał Woźniak", "Gerald Schaefer."],
    "venue": "Applied Soft Computing, 14:554–562.",
    "year": 2014
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "arXiv preprint arXiv:1603.01360.",
    "year": 2016
  }, {
    "title": "Focal loss for dense object detection",
    "authors": ["Tsung-Yi Lin", "Priya Goyal", "Ross Girshick", "Kaiming He", "Piotr Dollár."],
    "venue": "arXiv preprint arXiv:1708.02002.",
    "year": 2017
  }, {
    "title": "UZH at TAC KBP 2017: Event nugget detection via joint learning with softmax-margin objective",
    "authors": ["Peter Makarov", "Simon Clematide."],
    "venue": "Proceedings of TAC 2017.",
    "year": 2017
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "In Proceedings of ACL 2014.",
    "year": 2014
  }, {
    "title": "Optimizing f-measure with support vector machines",
    "authors": ["David R Musicant", "Vipin Kumar", "Aysel Ozgur"],
    "venue": "In FLAIRS conference,",
    "year": 2003
  }, {
    "title": "Optimizing f-measure: A tale of two approaches",
    "authors": ["Ye Nan", "Kian Ming Chai", "Wee Sun Lee", "Hai Leong Chieu."],
    "venue": "arXiv preprint arXiv:1206.4625.",
    "year": 2012
  }, {
    "title": "On the statistical consistency of plugin classifiers for non-decomposable performance measures",
    "authors": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal."],
    "venue": "Advances in Neural Information Processing Systems, pages 1493–1501.",
    "year": 2014
  }, {
    "title": "Optimal classification with multivariate losses",
    "authors": ["Nagarajan Natarajan", "Oluwasanmi Koyejo", "Pradeep Ravikumar", "Inderjit Dhillon."],
    "venue": "International Conference on Machine Learning, pages 1530–1538.",
    "year": 2016
  }, {
    "title": "Joint event extraction via recurrent neural networks",
    "authors": ["Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman."],
    "venue": "Proceedings of NAACL-HLT 2016.",
    "year": 2016
  }, {
    "title": "Event detection and domain adaptation with convolutional neural networks",
    "authors": ["Thien Huu Nguyen", "Ralph Grishman."],
    "venue": "Proceedings of ACL 2015.",
    "year": 2015
  }, {
    "title": "Modeling skip-grams for event detection with convolutional neural networks",
    "authors": ["Thien Huu Nguyen", "Ralph Grishman."],
    "venue": "Proceedings of EMNLP 2016.",
    "year": 2016
  }, {
    "title": "Optimizing f-measures by cost-sensitive classification",
    "authors": ["Shameem Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet."],
    "venue": "Advances in Neural Information Processing Systems, pages 2123–2131.",
    "year": 2014
  }, {
    "title": "Classifying relations by ranking with convolutional neural networks",
    "authors": ["Cicero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou."],
    "venue": "arXiv preprint arXiv:1504.06580.",
    "year": 2015
  }, {
    "title": "From light to rich ere: annotation of entities, relations, and events",
    "authors": ["Zhiyi Song", "Ann Bies", "Stephanie Strassel", "Tom Riese", "Justin Mott", "Joe Ellis", "Jonathan Wright", "Seth Kulick", "Neville Ryant", "Xiaoyi Ma."],
    "venue": "Proceedings of the The 3rd Workshop on",
    "year": 2015
  }, {
    "title": "The development of utility theory",
    "authors": ["George J Stigler."],
    "venue": "i. Journal of Political Economy, 58(4):307– 327.",
    "year": 1950
  }, {
    "title": "Cost-sensitive boosting for classification of imbalanced data",
    "authors": ["Yanmin Sun", "Mohamed S Kamel", "Andrew KC Wong", "Yang Wang."],
    "venue": "Pattern Recognition, 40(12):3358–3378.",
    "year": 2007
  }, {
    "title": "An instance-weighting method to induce cost-sensitive trees",
    "authors": ["Kai Ming Ting."],
    "venue": "IEEE Transactions on Knowledge and Data Engineering, 14(3):659–665.",
    "year": 2002
  }, {
    "title": "Ace 2005 multilingual training corpus",
    "authors": ["Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda."],
    "venue": "Linguistic Data Consortium, Philadelphia, 57.",
    "year": 2006
  }, {
    "title": "Leveraging knowledge bases in lstms for improving machine reading",
    "authors": ["Bishan Yang", "Tom Mitchell."],
    "venue": "Proceedings of.",
    "year": 2017
  }, {
    "title": "Relation classification via convolutional deep neural network",
    "authors": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao."],
    "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,",
    "year": 2014
  }, {
    "title": "A convolution bilstm neural network model for chinese event extraction",
    "authors": ["Ying Zeng", "Honghui Yang", "Yansong Feng", "Zheng Wang", "Dongyan Zhao."],
    "venue": "Proceedings of NLPCC-ICCPOL 2016.",
    "year": 2016
  }, {
    "title": "Active learning for word sense disambiguation with methods for addressing the class imbalance problem",
    "authors": ["Jingbo Zhu", "Eduard Hovy."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
    "year": 2007
  }],
  "id": "SP:e8462bfb719f0665cc30779c69c77a4df79dd0bc",
  "authors": [{
    "name": "Hongyu Lin",
    "affiliations": []
  }, {
    "name": "Yaojie Lu",
    "affiliations": []
  }, {
    "name": "Xianpei Han",
    "affiliations": []
  }, {
    "name": "Le Sun",
    "affiliations": []
  }],
  "abstractText": "This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes. These characteristics often result in deficient performance of neural network based detection models. In this paper, we propose adaptive scaling, an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic costsensitive learning. To this end, we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyperparameters. Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models.",
  "title": "Adaptive Scaling for Sparse Detection in Information Extraction"
}