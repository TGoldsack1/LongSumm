{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2104–2115 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2104"
  }, {
    "heading": "1 Introduction",
    "text": "Language models (LMs) are statistical models that assign a probability over sequences of words. Language models can often help with other tasks, such as speech recognition (Mikolov et al., 2010; Prabhavalkar et al., 2017), machine translation (Luong et al., 2015; Gülçehre et al., 2017), text summarisation (Filippova et al., 2015; Gambhir and Gupta, 2017), question answering (Wang et al., 2017), semantic error detection (Rei and Yannakoudakis, 2017; Spithourakis et al., 2016a), and fact checking (Rashkin et al., 2017).\nNumeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively. Language models exhibit literacy by being able to assign higher probabilities to sentences that\nare both grammatical and realistic, as in this example:\n‘I eat an apple’ (grammatical and realistic) ‘An apple eats me’ (unrealistic)\n‘I eats an apple’ (ungrammatical)\nLikewise, a numerate language model should be able to rank numerical claims based on plausibility:\n’John’s height is 1.75 metres’ (realistic) ’John’s height is 999.999 metres’ (unrealistic)\nExisting approaches to language modelling treat numerals similarly to other words, typically using categorical distributions over a fixed vocabulary.\nHowever, this maps all unseen numerals to the same unknown type and ignores the smoothness of continuous attributes, as shown in Figure 1. In that respect, existing work on language modelling does not explicitly evaluate or optimise for numeracy. Numerals are often neglected and low-resourced, e.g. they are often masked (Mitchell and Lapata, 2009), and there are only 15,164 (3.79%) numerals among GloVe’s 400,000 embeddings pretrained on 6 billion tokens (Pennington et al., 2014). Yet, numbers appear ubiquitously, from children’s magazines (Joram et al., 1995) to clinical reports (Bigeard et al., 2015), and grant objectivity to sciences (Porter, 1996).\nPrevious work finds that numerals have higher out-of-vocabulary rates than other words and proposes solutions for representing unseen numerals as inputs to language models, e.g. using numerical magnitudes as features (Spithourakis et al., 2016b,a). Such work identifies that the perplexity of language models on the subset of numerals can be very high, but does not directly address the issue. This paper focuses on evaluating and improving the ability of language models to predict numerals. The main contributions of this paper are as follows:\n1. We explore different strategies for modelling numerals, such as memorisation and digit-bydigit composition, and propose a novel neural architecture based on continuous probability density functions.\n2. We propose the use of evaluations that adjust for the high out-of-vocabulary rate of numerals and account for their numerical value (magnitude).\n3. We evaluate on a clinical and a scientific corpus and provide a qualitative analysis of learnt representations and model predictions. We find that modelling numerals separately from other words can drastically improve the perplexity of LMs, that different strategies for modelling numerals are suitable for different textual contexts, and that continuous probability density functions can improve the LM’s prediction accuracy for numbers."
  }, {
    "heading": "2 Language Models",
    "text": "Let s1,s2,...,sL denote a document, where st is the token at position t. A language model estimates the probability of the next token given previous tokens, i.e. p(st|s1,...,st−1). Neural LMs estimate this probability by feeding embeddings, i.e. vectors that represent each token, into a Recurrent Neural Network (RNN) (Mikolov et al., 2010).\nToken Embeddings Tokens are most commonly represented by aD-dimensional dense vector that is unique for each word from a vocabulary V of known words. This vocabulary includes special symbols (e.g. ‘UNK’) to handle out-of-vocabulary tokens, such as unseen words or numerals. Let ws be the one-hot representation of token s, i.e. a sparse binary vector with a single element set to 1 for that token’s index in the vocabulary, andE∈RD×|V| be the token embeddings matrix. The token embedding for s is the vector etokens =Ews.\nCharacter-Based Embeddings A representation for a token can be build from its constituent characters (Luong and Manning, 2016; Santos and Zadrozny, 2014). Such a representation takes into account the internal structure of tokens. Let d1,d2,...,dN be the characters of token s. A character-based embedding for s is the final hidden state of a D-dimensional character-level RNN: echarss =RNN(d0,d1,...dL).\nRecurrent and Output Layer The computation of the conditional probability of the next token involves recursively feeding the embedding of the current token est and the previous hidden state ht−1 into a D-dimensional token-level RNN to obtain the current hidden state ht. The output probability is estimated using the softmax function, i.e.\np(st|ht)=softmax(ψ(st))= 1Ze ψ(st) Z= ∑ s′∈V eψ(s ′), (1)\nwhere ψ(.) is a score function.\nTraining and Evaluation Neural LMs are typically trained to minimise the cross entropy on the training corpus:\nHtrain=− 1\nN ∑ st∈train logp(st|s<t) (2)\nA common performance metric for LMs is per token perplexity (Eq. 3), evaluated on a test corpus. It can also be interpreted as the branching factor: the size of an equally weighted distribution with equivalent uncertainty, i.e. how many sides you need on a fair die to get the same uncertainty as the model distribution.\nPPtest=exp(Htest) (3)"
  }, {
    "heading": "3 Strategies for Modelling Numerals",
    "text": "In this section we describe models with different strategies for generating numerals and propose the\nuse of number-specific evaluation metrics that adjust for the high out-of-vocabulary rate of numerals and account for numerical values. We draw inspiration from theories of numerical cognition. The triple code theory (Dehaene et al., 2003) postulates that humans process quantities through two exact systems (verbal and visual) and one approximate number system that semantically represents a number on a mental number line. Tzelgov et al. (2015) identify two classes of numbers: i) primitives, which are holistically retrieved from long-term memory; and ii) non-primitives, which are generated online. An in-depth review of numerical and mathematical cognition can be found in Kadosh and Dowker (2015) and Campbell (2005)."
  }, {
    "heading": "3.1 Softmax Model and Variants",
    "text": "This class of models assumes that numerals come from a finite vocabulary that can be memorised and retrieved later. The softmax model treats all tokens (words and numerals) alike and directly uses Equation 1 with score function:\nψ(st)=h T t e token st =h T t Eoutwst, (4)\nwhere Eout ∈ RD×|V| is an output embeddings matrix. The summation in Equation 1 is over the complete target vocabulary, which requires mapping any out-of-vocabulary tokens to special symbols, e.g. ‘UNKword’ and ‘UNKnumeral’.\nSoftmax with Digit-Based Embeddings The softmax+rnn variant considers the internal syntax of a numeral’s digits by adjusting the score function:\nψ(st)=h T t e token st +h T t e chars st\n=hTt Eoutwst+h T t E RNN out wst,\n(5)\nwhere the columns of ERNNout are composed of character-based embeddings for in-vocabulary numerals and token embeddings for the remaining vocabulary. The character set comprises digits (0-9), the decimal point, and an end-of-sequence character. The model still requires normalisation over the whole vocabulary, and the special unknown tokens are still needed.\nHierarchical Softmax A hierarchical softmax (Morin and Bengio, 2005a) can help us decouple the modelling of numerals from that of words. The probability of the next token st is decomposed to that of its class ct and the probability of the exact token from within the class:\np(st|ht)= ∑ ct∈C p(ct|ht)p(st|ct,ht)\np(ct|ht)=σ ( hTt b ) (6)\nwhere the valid token classes are C = {word, numeral}, σ is the sigmoid function and b is a D-dimensional vector. Each of the two branches of p(st|ct,ht) can now be modelled by independently normalised distributions. The hierarchical variants (h-softmax and h-softmax+rnn) use two independent softmax distributions for words and numerals. The two branches share no parameters, and thus words and numerals will be embedded into separate spaces.\nThe hierarchical approach allows us to use any well normalised distribution to model each of its branches. In the next subsections, we examine different strategies for modelling the branch of numerals, i.e. p(st|ct=numeral,ht). For simplicity, we will abbreviate this to p(s)."
  }, {
    "heading": "3.2 Digit-RNN Model",
    "text": "Let d1,d2...dN be the digits of numeral s. A digit-bydigit composition strategy estimates the probability of the numeral from the probabilities of its digits:\np(s)=p(d1)p(d2|d1)...p(dN |d<N) (7)\nThe d-RNN model feeds the hidden state ht of the token-level RNN into a character-level RNN (Graves, 2013; Sutskever et al., 2011) to estimate this probability. This strategy can accommodate an open vocabulary, i.e. it eliminates the need for an UNKnumeral symbol, as the probability is normalised one digit at a time over the much smaller vocabulary of digits (digits 0-9, decimal separator, and end-of-sequence)."
  }, {
    "heading": "3.3 Mixture of Gaussians Model",
    "text": "Inspired by the approximate number system and the mental number line (Dehaene et al., 2003), our proposed MoG model computes the probability of numerals from a probability density function (pdf) over real numbers, using a mixture of Gaussians for the underlying pdf:\nq(v)= K∑ k=1 πkNk(v;µk,σ2k)\nπk=softmax ( BTht ) ,\n(8)\nwhere K is the number of components, πk are mixture weights that depend on hidden state ht of the token-level RNN, Nk is the pdf of the normal distribution with mean µk ∈R and variance σ2k ∈R, andB∈RD×K is a matrix.\nThe difficulty with this approach is that for any continuous random variable, the probability that it equals a specific value is always zero. To resolve this,\nwe consider a probability mass function (pmf) that discretely approximates the pdf:\nQ̃(v|r)= v+ r∫ v− r q(u)du=F(v+ r)−F(v− r), (9)\nwhere F(.) is the cumulative density function of q(.), and r =0.5×10−r is the number’s precision. The level of discretisation r, i.e. how many decimal digits to keep, is a random variable in N with distribution p(r). The mixed joint density is:\np(s)=p(v,r)=p(r)Q̃(v|r) (10)\nFigure 2 summarises this strategy, where we model the level of discretisation by converting the numeral into a pattern and use a RNN to estimate the probability of that pattern sequence:\np(r)=p(SOS INT_PART . r decimal digits︷ ︸︸ ︷ \\d ... \\d EOS) (11)"
  }, {
    "heading": "3.4 Combination of Strategies",
    "text": "Different mechanisms might be better for predicting numerals in different contexts. We propose a combination model that can select among different\nstrategies for modelling numerals: p(s)= ∑ ∀m∈M αmp(s|m)\nαm=softmax ( ATht ) ,\n(12)\nwhere M={h-softmax, d-RNN, MoG}, and A∈RD×|M|. Since both d-RNN and MoG are openvocabulary models, the unknown numeral token can now be removed from the vocabulary of h-softmax."
  }, {
    "heading": "3.5 Evaluating the Numeracy of LMs",
    "text": "Numeracy skills are centred around the understanding of numbers and numerals. A number is a mathematical object with a specific magnitude, whereas a numeral is its symbolic representation, usually in the positional decimal Hindu–Arabic numeral system (McCloskey and Macaruso, 1995). In humans, the link between numerals and their numerical values boosts numerical skills (Griffin et al., 1995).\nPerplexity Evaluation Test perplexity evaluated only on numerals will be informative of the symbolic component of numeracy. However, model comparisons based on naive evaluation using Equation 3 might be problematic: perplexity is sensitive to outof-vocabulary (OOV) rate, which might differ among models, e.g. it is zero for open-vocabulary models. As an extreme example, in a document where all words are out of vocabulary, the best perplexity is achieved by a trivial model that predicts everything as unknown.\nUeberla (1994) proposed Adjusted Perplexity (APP; Eq. 14), also known as unknown-penalised perplexity (Ahn et al., 2016), to cancel the effect of the out-of-vocabulary rate on perplexity. The APP is the perplexity of an adjusted model that uniformly redistributes the probability of each out-of-vocabulary class over all different types in that class:\np′(s)= { p(s) 1|OOVc| if s∈OOVc p(s) otherwise\n(13)\nwhere OOVc is an out-of-vocabulary class (e.g. words and numerals), and |OOVc| is the cardinality of each OOV set. Equivalently, adjusted perplexity can be calculated as:\nAPPtest=exp ( Htest+\n∑ c Hcadjust\n)\nHcadjust=− ∑ t |st∈OOVc| N log 1 |OOVc|\n(14)\nwhereN is the total number of tokens in the test set and |s∈OOVc| is the count of tokens from the test set belonging in each OOV set.\nEvaluation on the Number Line While perplexity looks at symbolic performance on numerals, this evaluation focuses on numbers and particularly on their numerical value, which is their most prominent semantic content (Dehaene et al., 2003; Dehaene and Cohen, 1995).\nLet vt be the numerical value of token st from the test corpus. Also, let v̂t be the value of the most probable numeral under the model st = argmax (p(st|ht,ct=num)). Any evaluation metric from the regression literature can be used to measure the models performance. To evaluate on the number line, we can use any evaluation metric from the regression literature. In reverse order of tolerance to extreme errors, some of the most popular are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Median Absolute Error (MdAE):\nei = vi−v̂i\nRMSE = √ 1 N N∑ i=1 e2i\nMAE = 1N N∑ i=1 |ei|\nMdAE = median{|ei|}\n(15)\nThe above are sensitive to the scale of the data. If the data contains values from different scales, percentage metrics are often preferred, such as the Mean/Median Absolute Percentage Error (MAPE/MdAPE):\npei = vi−v̂i vi\nMAPE = 1N N∑ i=1 |pei|\nMdAPE = median{|pei|}\n(16)"
  }, {
    "heading": "4 Data",
    "text": "To evaluate our models, we created two datasets with documents from the clinical and scientific domains, where numbers abound (Bigeard et al., 2015; Porter, 1996). Furthermore, to ensure that the numbers will be informative of some attribute, we only selected texts that reference tables.\nClinical Data Our clinical dataset comprises clinical records from the London Chest Hospital. The records where accompanied by tables with 20 numeric attributes (age, heart volumes, etc.) that they partially describe, as well as include numbers not found in the tables. Numeric tokens constitute only a small proportion of each sentence (4.3%), but account\nfor a large part of the unique tokens vocabulary (>40%) and suffer high OOV rates.\nScientific Data Our scientific dataset comprises paragraphs from Cornell’s ARXIV 1 repository of scientific articles, with more than half a million converted papers in 37 scientific sub-fields. We used the preprocessed ARXMLIV (Stamerjohanns et al., 2010; Stamerjohanns and Kohlhase, 2008) 2 version, where papers have been converted from LATEX into a custom XML format using the LATEXML 3 tool. We then kept all paragraphs with at least one reference to a table and a number.\nFor both datasets, we lowercase tokens and normalise numerals by omitting the thousands separator (\"2,000\" becomes \"2000\") and leading zeros (\"007\" becomes \"7\"). Special mathematical symbols are tokenised separately, e.g. negation (“-1” as “-”, “1”), fractions (“3/4” as “3”, “/”, “4”), etc. For this reason, all numbers were non-negative. Table 1 shows descriptive statistics for both datasets."
  }, {
    "heading": "5 Experimental Results and Discussion",
    "text": "We set the vocabularies to the 1,000 and 5,000 most frequent token types for the clinical and scientific datasets, respectively. We use gated token-character embeddings (Miyamoto and Cho, 2016) for the input of numerals and token embeddings for the input and output of words, since the scope of our paper is numeracy. We set the models’ hidden dimensions to D = 50 and initialise all token embeddings to pretrained GloVe (Pennington et al., 2014). All our\n1ARXIV.ORG. Cornell University Library at http://arxiv.org/, visited December 2016\n2ARXMLIV. Project home page at http://arxmliv.kwarc.info/, visited December 2016\n3LATEXML. http://dlmf.nist.gov, visited December 2016\nRNNs are LSTMs (Hochreiter and Schmidhuber, 1997) with the biases of LSTM forget gate were initialised to 1.0 (Józefowicz et al., 2015). We train using mini-batch gradient decent with the Adam optimiser (Kingma and Ba, 2014) and regularise with early stopping and 0.1 dropout rate (Srivastava, 2013) in the input and output of the token-based RNN.\nFor the mixture of Gaussians, we select the mean and variances to summarise the data at different granularities by fitting 7 separate mixture of Gaussian models on all numbers, each with twice as many components as the previous, for a total of 27+1− 1 = 256 components. These models are initialised at percentile points from the data and trained with the expectation-minimisation algorithm. The means and variances are then fixed and not updated when we train the language model."
  }, {
    "heading": "5.1 Quantitative Results",
    "text": "Perplexities Table 2 shows perplexities evaluated on the subsets of words, numerals and all tokens of\nthe test data. Overall, all models performed better on the clinical than on the scientific data. On words, all models achieve similar perplexities in each dataset.\nOn numerals, softmax variants perform much better than other models in PP, which is an artefact of the high OOV-rate of numerals. APP is significantly worse, especially for non-hierarchical variants, which perform about 2 and 4 orders of magnitude worse than hierarchical ones.\nFor open-vocabulary models, i.e. d-RNN, MoG, and combination, PP is equivalent to APP. On numerals, d-RNN performed better than softmax variants in both datasets. The MoG model performed twice as well as softmax variants on the clinical dataset, but had the third worse performance in the scientific dataset. The combination model had the best overall APP results for both datasets.\nEvaluations on the Number Line To factor out model specific decoding processes for finding the best next numeral, we use our models to rank a set\nof candidate numerals: we compose the union of in-vocabulary numbers and 100 percentile points from the training set, and we convert numbers into numerals by considering all formats up to n decimal points. We select n to represent 90% of numerals seen at training, which yields n=3 and n=4 for the clinical and scientific data, respectively.\nTable 3 shows evaluation results, where we also include two naive baselines of constant predictions: with the mean and median of the training data. For both datasets, RMSE and MAE were too sensitive to extreme errors to allow drawing safe conclusions, particularly for the scientific dataset, where both metrics were in the order of 109. MdAE can be of some use, as 50% of the errors are absolutely smaller than that.\nAlong percentage metrics, MoG achieved the best MAPE in both datasets (18% and 54% better that the second best) and was the only model to perform better than the median baseline for the clinical data. However, it had the worst MdAPE, which means that MoG mainly reduced larger percentage errors. The d-RNN model came third and second in the clinical and scientific datasets, respectively. In the latter it achieved the best MdAPE, i.e. it was effective at reducing errors for 50% of the numbers. The combination model did not perform better than its constituents. This is possibly because MoG is the only strategy that takes into account the numerical magnitudes of the numerals."
  }, {
    "heading": "5.2 Learnt Representations",
    "text": "Softmax versus Hierarchical Softmax Figure 3 visualises the cosine similarities of the output token embeddings of numerals for the softmax and h-softmax models. Simple softmax enforced high similarities among all numerals and the unknown numeral token, so as to make them more dissimilar to words, since the model embeds both in the same space. This is not the case for h-softmax that uses two different spaces: similarities are concentrated along the diagonal and fan out as the magnitude grows, with the exception of numbers with special meaning, e.g. years and percentile points.\nDigit embeddings Figure 4 shows the cosine similarities between the digits of the d-RNN output mode. We observe that each primitive digit is mostly similar to its previous and next digit. Similar behaviour was found for all digit embeddings of all models."
  }, {
    "heading": "5.3 Predictions from the Models",
    "text": "Next Numeral Figure 5 shows the probabilities of different numerals under each model for two\nexamples from the clinical development set. Numerals are grouped by number of decimal points. The h-softmax model’s probabilities are spiked, d-RNNs are saw-tooth like and MoG’s are smooth, with the occasional spike, whenever a narrow component allows for it. Probabilities rapidly decrease for more decimal digits, which is reminiscent of the theoretical expectation that the probability of en exact value for a continuous variable is zero.\nSelection of Strategy in Combination Model Table 4 shows development set examples with high selection probabilities for each strategy of the combination model, along with numerals with the highest average selection per mode. The h-softmax model is responsible for mostly integers with special functions,\ne.g. years, typical drug dosages, percentile points, etc. In the clinical data, d-RNN picks up two-digit integers (mostly dimensions) and MoG is activated for continuous attributes, which are mostly out of vocabulary. In the scientific data, d-RNN and MoG\nshowed affinity to different indices from catalogues of astronomical objects: d-RNN mainly to NGC (Dreyer, 1888) and MoG to various other indices, such as GL (Gliese, 1988) and HIP (Perryman et al., 1997). In this case, MoG was wrongly selected for numerals with a labelling function, which also highlights a limitation of evaluating on the number line, when a numeral is not used to represent its magnitude.\nSignificant Digits Figure 5 shows the distributions of the most significant digits under the d-RNN model\nand from data counts. The theoretical estimate has been overlayed, according to Benford’s law (Benford, 1938), also called the first-digit law, which applies to many real-life numerals. The law predicts that the first digit is 1 with higher probability (about 30%) than 9 (< 5%) and weakens towards uniformity at higher digits. Model probabilities closely follow estimates from the data. Violations from Benford’s law can be due to rounding (Beer, 2009) and can be used as evidence for fraud detection (Lu et al., 2006)."
  }, {
    "heading": "6 Related Work",
    "text": "Numerical quantities have been recognised as important for textual entailment (Lev et al., 2004; Dagan et al., 2013). Roy et al. (2015) proposed a quantity entailment sub-task that focused on whether a given quantity can be inferred from a given text and, if so, what its value should be. A common framework for acquiring common sense about numerical attributes of objects has been to collect a corpus of numerical values in pre-specified templates and then model attributes as a normal distribution (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010; Narisawa et al., 2013; de Marneffe et al., 2010). Our model embeds these approaches into a LM that has a sense for numbers.\nOther tasks that deal with numerals are numerical information extraction and solving mathematical problems. Numerical relations have at least one argument that is a number and the aim of the task is to extract all such relations from a corpus, which can range from identifying a few numerical attributes (Nguyen and Moschitti, 2011; Intxaurrondo et al., 2015) to generic numerical relation extraction (Hoffmann et al., 2010; Madaan et al., 2016). Our model does not extract values, but rather produces an probabilistic estimate.\nMuch work has been done in solving arithmetic (Mitra and Baral, 2016; Hosseini et al., 2014; Roy and Roth, 2016), geometric (Seo et al., 2015), and algebraic problems (Zhou et al., 2015; Koncel-Kedziorski et al., 2015; Upadhyay et al., 2016; Upadhyay and Chang, 2016; Shi et al., 2015; Kushman et al., 2014) expressed in natural language. Such models often use mathematical background knowledge, such as linear system solvers. The output of our model is not based on such algorithmic operations, but could be extended to do so in future work.\nIn language modelling, generating rare or unknown words has been a challenge, similar to our unknown numeral problem. Gulcehre et al. (2016) and Gu et al. (2016) adopted pointer networks (Vinyals et al., 2015)\nto copy unknown words from the source in translation and summarisation tasks. Merity et al. (2016) and Lebret et al. (2016) have models that copy from context sentences and from Wikipedia’s infoboxes, respectively. Ahn et al. (2016) proposed a LM that retrieves unknown words from facts in a knowledge graph. They draw attention to the inappropriateness of perplexity when OOV-rates are high and instead propose an adjusted perplexity metric that is equivalent to APP. Other methods aim at speeding up LMs to allow for larger vocabularies (Chen et al., 2015), such as hierarchical softmax (Morin and Bengio, 2005b), target sampling (Jean et al., 2014), etc., but still suffer from the unknown word problem. Finally, the problem is resolved when predicting one character at a time, as done by the character-level RNN (Graves, 2013; Sutskever et al., 2011) used in our d-RNN model."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we investigated several strategies for LMs to model numerals and proposed a novel openvocabulary generative model based on a continuous probability density function. We provided the first thorough evaluation of LMs on numerals on two corpora, taking into account their high out-of-vocabulary rate and numerical value (magnitude). We found that modelling numerals separately from other words through a hierarchical softmax can substantially improve the perplexity of LMs, that different strategies are suitable for different contexts, and that a combination of these strategies can help improve the perplexity further. Finally, we found that using a continuous probability density function can improve prediction accuracy of LMs for numbers by substantially reducing the mean absolute percentage metric.\nOur approaches in modelling and evaluation can be used in future work in tasks such as approximate information extraction, knowledge base completion, numerical fact checking, numerical question answering, and fraud detection. Our code and data are available at: https://github.com/uclmr/ numerate-language-models."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank the anonymous reviewers for their insightful comments and also Steffen Petersen for providing the clinical dataset and advising us on the clinical aspects of this work. This research was supported by the Farr Institute of Health Informatics Research and an Allen Distinguished Investigator award."
  }],
  "year": 2018,
  "references": [],
  "id": "SP:cb0ba1e9b5137f57a4cb52be84f30d2b0319f8cb",
  "authors": [{
    "name": "Georgios P. Spithourakis",
    "affiliations": []
  }, {
    "name": "Sebastian Riedel",
    "affiliations": []
  }],
  "abstractText": "Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over nonhierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.",
  "title": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers"
}