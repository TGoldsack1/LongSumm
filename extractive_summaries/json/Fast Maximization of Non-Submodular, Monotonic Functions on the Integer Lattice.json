{
  "sections": [{
    "heading": "1. Introduction",
    "text": "As a natural extension of finite sets S (equivalently, {0, 1}S), optimization of discrete functions on the integer lattice NS has received attention recently (Alon et al., 2012; Demaine et al., 2014; Soma & Yoshida, 2015). As an example, consider the placement of sensors in a water network (Krause et al., 2008a); in the set version, each sensor takes a value in {0, 1}, which corresponds to whether the sensor was placed. In the lattice version (Soma & Yoshida, 2015), each sensor has a power level in {0, . . . , b} ⊆ N, to which the sensitivity of the sensor is correlated. As a second example, consider the influence maximization problem (Kempe et al., 2003); instead of the binary seeding of a user, the lattice version enables partial incentives or discounts to be used (Demaine et al., 2014).\nAlthough many results from the optimization of submodular set functions have been generalized to the integer lattice (Soma & Yoshida, 2015; 2016; Ene & Nguyen, 2016), many objective functions arising from applications are not submodular (Bian et al., 2017b; Lin et al., 2017; Das &\n1University of Florida, Gainesville, Florida. Correspondence to: Alan Kuhnle <kuhnle@ufl.edu>, My T. Thai <mythai@ufl.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nKempe, 2011; Horel & Singer, 2016). In this work, we consider maximization subject to a cardinality constraint (MCC), where the function f to be maximized may be nonsubmodular. Let k ∈ N (the budget), b ∈ (N∪{∞})S (the box), and let f : {x ∈ NS : x ≤ b} → R+ (the objective) be a non-negative and monotonic1 function with f(0) = 0. Then determine\nmax ‖w‖1≤k f(w), (MCC)\nwhere w = (ws)s∈S ∈ NS , ‖w‖1 = ∑ s∈S |ws|.\nSince the integer lattice may be represented as a multiset of size k|S|, one may use results for Problem MCC with non-submodular set functions. In particular, the tight ratio 1α (1− e−αγs) of the standard greedy algorithm by Bian et al. (2017b), where α, γs are discussed below, applies with the lattice adaptation of the standard greedy algorithm (StandardGreedy) given in Alg. 1. However, this approach requires Ω(|S|k) queries of f , which is not polynomial in the input2 size O(|S| log k). Even for applications with set functions, Ω(|S|k) queries may be prohibitive, and researchers (Leskovec et al., 2007; Mirzasoleiman et al., 2015; Badanidiyuru & Vondrák, 2014) have sought ways to speed up the StandardGreedy algorithm. Unfortunately, these approaches rely upon the submodularity of f , and there has been no analogous effort for non-submodular functions.\nTo quantify the non-submodularity of a lattice function f , we generalize the following quantities defined for set functions to the lattice: (1) the diminishing-return (DR) ratio γd of f (Lehmann et al., 2006), (2) the weak DR ratio γs of f (Das & Kempe, 2011), and (3) the generalized curvature α of f (Bian et al., 2017b). Our main contributions are:\n• To speed up StandardGreedy (Alg. 1), we adapt the threshold greedy framework of Badanidiyuru & Vondrák (2014) to non-submodular functions; this yields an algorithm (ThresholdGreedy, Alg. 2) with approximation ratio (1 − e−γdγs − η), for\n1for all v ≤ w (coordinate-wise), f(v) ≤ f(w) 2The input is considered to be the vector b of length n = |S| and the number k represented in log k bits (w.l.o.g. each component of b is at most k); the function is regarded as an oracle and hence does not contribute to input size.\nAlgorithm 1 StandardGreedy 1: Input: f ∈ Fb, k ∈ N, b ∈ NS 2: Output: g ∈ NS 3: g← 0 4: for i = 1 to k do 5: g← g + arg maxs∈S:g+s≤b δs(g) 6: return g\nany η > 0, the first approximation algorithm with polynomial query complexity for Problem MCC on the lattice. The query complexity of the StandardGreedy algorithm is improved from Ω(nk) to O ( n log k logκ ( ε2/k )) , where κ, ε ∈ (0, 1) are parameters of ThresholdGreedy.\n• We introduce the novel approximation algorithm FastGreedy, which combines elements of StandardGreedy and ThresholdGreedy to improve the performance ratio to (1− e−β∗γs − η), where β∗ is at least γd and in many cases3 is determined by the algorithm. Furthermore, FastGreedy exploits the non-submodularity of the function to decrease its runtime in practice without sacrificing its performance guarantee, while maintaining the same worst-case query complexity as ThresholdGreedy up to a constant factor.\n• To demonstrate our algorithms, we introduce a general budget allocation problem for viral marketing, which unifies submodular influence maximization (IM) under the independent cascade model (Kempe et al., 2003) with the non-submodular boosting problem (Lin et al., 2017) and in addition allows partial incentives. We prove a lower bound on the DR and weak DR ratios for this unified framework, and we experimentally validate our proposed algorithms in this setting."
  }, {
    "heading": "2. Related Work",
    "text": "The study of optimization of submodular set functions is too extensive to give a comprehensive overview. On the integer lattice, there have been many efforts to maximize submodular functions, e.g Soma & Yoshida (2017); Bian et al. (2017a); Gottschalk & Peis (2016). To the best of our knowledge, we are the first to study the optimization of non-submodular functions on the integer lattice. In the following discussion, we primarily restrict our attention to the maximization of monotonic, submodular lattice functions subject to a cardinality constraint and the maximization of non-submodular set functions.\n3When the solution g returned by FastGreedy satisfies ‖g‖1 = k. Otherwise, an upper bound on β∗ is returned.\nReduction of Ene & Nguyen (2016). Ene & Nguyen (2016) have given a polynomial-time reduction from the lattice to a set that enables unified translation of submodular optimization strategies to DR-submodular (i.e. DR ratio γd = 1, see Section 3) functions on the integer lattice. Since this translation is designed for DR-submodular functions, it does not give a polynomial-time algorithm for Problem MCC when f is non-submodular. Specifically, for the case of maximization subject to a cardinality constraint, Ene & Nguyen (2016) rely upon the threshold greedy algorithm for submodular set functions (Badanidiyuru & Vondrák, 2014), which does not work for nonsubmodular functions without modifications such as the ones in our paper.\nThreshold Greedy and Lattice Optimization. To speed up the StandardGreedy for submodular set functions, Badanidiyuru & Vondrák (2014) introduced the threshold greedy framework, which speeds up the StandardGreedy algorithm for maximizing submodular set functions under cardinality constraint from O(nk) function evaluations to O ( n ε log n ε ) , and it maintains the approximation ratio (1− 1/e− ε), for ε > 0. Soma & Yoshida (2016) adapted the threshold approach for efficiently maximizing DRsubmodular functions on the integer lattice and provided (1 − 1/e − ε)-approximation algorithms. Other adaptations of the threshold approach of Badanidiyuru & Vondrák (2014) to the integer lattice include (Ene & Nguyen, 2016; Soma & Yoshida, 2015). Elenberg et al. (2017) recently extended a similar algorithm (Sieve Streaming of Badanidiyuru et al. (2014)) to non-submodular functions in a somewhat similar fashion. The key difference is Sieve Streaming makes one pass over the data using many geometrically spaced thresolds in parallel, whereas the approach of Badanidiyuru & Vondrák (2014) makes many sequential passes at geometrically spaced thresholds.\nOur ThresholdGreedy algorithm is an adaptation of the algorithm of Soma & Yoshida (2016) for DR-submodular maximization to non-submodular functions. The nonsubmodularity requires new analysis, in the following specific ways: (1) during the binary search phase, we cannot guarantee that we find the maximum number of copies whose average gain exceeds the threshold τ ; hence, we must settle for any number of copies whose average gain exceeds τ , while ensuring that the gain of adding one additional copy falls belows τ . (2) To prove the performance ratio, we require a combination of the DR ratio γd and the weak DR ratio γs.\nThe very recent work of Qian et al. (2018) considers the same problem as in our paper, and they define the same DR ratio as ours; their lattice submodularity ratio is different from the weak DR ratio, however. The algorithm (POMS) in Qian et al. (2018) is based upon a Pareto optimization\ntechnique, which is substantially different from our approach; our emphasis is on developing an approximation algorithm with runtime logarithmic in k, while POMS has running time Ω(k2n).\nOptimization of Non-Submodular Set Functions. For non-submodular set functions, the weak DR ratio γs was introduced by Das & Kempe (2011) under the name submodularity ratio; we generalize γs to lattice functions in Section 3, and we show the DR ratio γd ≤ γs. Bian et al. (2017b) introduced generalized curvature α of a set function, an analogous concept to the DR ratio as we discuss in Section 3. Bian et al. (2017b) extended the analysis of Conforti & Cornuéjols (1984) to non-submodular set functions; together with the weak DR ratio γs, they proved StandardGreedy has tight approximation ratio 1α (1− e−γsα) under cardinality constraint. The DR ratio γd is introduced as inverse curvature in Bogunovic et al. (2018), wherein robust maximization of set functions is considered; the DR ratio has also been introduced by Lehmann et al. (2006); Qian et al. (2018).\nMany other notions of non-submodular set functions have been introduced (Krause et al., 2008b; Horel & Singer, 2016; Borodin et al., 2014; Feige & Izsak, 2013). For a comprehensive discussion of the relation of these and additional notions to the weak DR ratio γs, we refer the reader to Bian et al. (2017b)."
  }, {
    "heading": "3. Non-Submodularity on the Lattice",
    "text": "In this section, we define the lattice versions of DR ratio γd, weak DR ratio γs, and generalized curvature α, which are used in the approximation ratios proved in Section 4.\nNotations. For each s ∈ S, let s be the unit vector with 1 in the coordinate corresponding to s, and 0 elsewhere. We write δw(v) = f(v + w) − f(v) for v,w ∈ NS . Given a box in the integer lattice b ∈ NS , let the set of all nonnegative, monotonic lattice functions with f(0) = 0, and domain {x ∈ NS : x ≤ b} be denoted Fb. It is often useful to think of a vector v ∈ NS as a multi-set containing vs copies of s ∈ S, where vs is the value of v’s coordinate corresponding to s. We use the notation {v} to represent the multiset corresponding to the vector v. Finally, we define v ∨w and v ∧w for v,w ∈ NS to be the vector with the coordinate-wise maximum and minimum respectively. Rather than an algorithm taking an explicit description of the function f as input, we consider the function f as an oracle and measure the complexity of an algorithm in terms of the number of oracle calls or queries.\nWe begin with the related concepts of DR ratio and generalized curvature.\nDefinition 1. Let f ∈ Fb. The diminishing-return (DR)\nratio of f , γd(f), is the maximum value in [0, 1] such that for any s ∈ S, and for all v ≤ w such that w + s ≤ b, γd(f)δs(w) ≤ δs(v). Definition 2. Let f ∈ Fb. The generalized curvature of f , α(f), is the minimum value in [0, 1] such that for any s ∈ S, and for all v ≤ w such that w + s ≤ b, δs(w) ≥ (1− α(f))δs(v).\nThe DR ratio extends the notion of DR-submodularity of Soma & Yoshida (2015), which is obtained as the special case γd = 1. Generalized curvature for set functions was introduced in Bian et al. (2017b). Notice that α results in lower bounds on the marginal gain of s to a vector w, while γd results in upper bounds on the same quantity: (1 − α)δs(v) ≤ δs(w) ≤ 1γd δs(v), whenever v ≤ w and the above expressions are defined. Next, we generalize the weak DR ratio of Das & Kempe (2011) to the integer lattice.\nDefinition 3. Let f ∈ Fb. The weak DR ratio of f , γs(f), is the maximum value in [0, 1] such that for all v,w, such that v ≤ w, γs(f)(f(w)− f(v)) ≤ ∑ s∈{w−v} δs(v).\nThe next proposition, proved in Appendix C, shows the relationship between DR ratio and weak DR ratio.\nProposition 1. For all f ∈ Fb, γd(f) ≤ γs(f). The function f is DR submodular iff γd(f) = γs(f) = 1.\nIn the rest of this work, we will parameterize functions by the non-submodularity ratios defined above and partition functions into the sets Fγd,γs,αb = {f ∈ Fb : γd(f) = γd, γs(f) = γs, α(f) = α}.\nGreedy versions. In the proofs of this paper, the full power of the parameters defined above is not required. It suffices to consider restricted versions, where the maximization is taken over only those vectors which appear in the ratio proofs. We define these greedy versions in Appendix B and include more discussion in Remark 1 of Section 4.1."
  }, {
    "heading": "4. Algorithms",
    "text": ""
  }, {
    "heading": "4.1. The ThresholdGreedy Algorithm",
    "text": "In this section, we present the algorithm ThresholdGreedy (Alg. 2) to approximate Problem MCC with ratio 1 − e−γgγs − η with polynomial query complexity. Appendix D contains the proofs of all lemmas, claims, and omitted details from this section.\nDescription. ThresholdGreedy operates by considering decreasing thresholds for the marginal gain in its outer for loop; for each threshold τ , the algorithm adds on line 2 elements whose marginal gain exceeds τ as described be-\nlow. The parameter κ ∈ (0, 1) determines the stepsize between successive thresholds; the algorithm continues until the budget k is met (line 2) or the threshold is below a minimum value dependent on the parameter ε ∈ (0, 1). Intuitively, the goal of the threshold approach (Badanidiyuru & Vondrák, 2014) for submodular set functions is as follows. At each threshold (i.e., iteration of the outer for loop), add all elements whose marginal gain exceeds τ to the solution g. On the lattice, adding all copies of s ∈ S whose average gain exceeds τ on line 2 would require the addition of the maximum multiple ls such that the average marginal gain exceeds τ :\nδls(g) ≥ lτ, (P1)\nas in the threshold algorithm of Soma & Yoshida (2016) for DR-submodular functions, in which the maximum l is identified by binary search. However, since f is not DRsubmodular, it is not always the case that δs(g + ls) ≥ δs(g + (l + 1)s), for each l. For this reason, we cannot find the maximum such l by binary search. Furthermore, even if we found the maximum l for each s ∈ S, we could not guarantee that all elements of marginal gain at least τ were added due to the non-submodularity of f : an element whose gain is less than τ when considered in the inner for loop might have gain greater than τ after additional elements are added to the solution.\nThresholdGreedy more conservatively ensures that the number l chosen for each s ∈ S satisfies both (P1) and\nδs(g + ls) < τ, (P2)\nbut it is not necessarily the maximum such l.\nPivot. Any l satisfying both (P1) and (P2) is termed a pivot4 with respect to g, s, τ . Perhaps surprisingly, a valid pivot can be found with binary search in O(log bmax) = O(log k) function queries, where bmax = maxs∈S bs; discussion of BinarySearchPivot and proof of this results is provided in Appendix D, Lemma 2. By finding a pivot for each s ∈ S, ThresholdGreedy does not attempt to add all elements exceeding the marginal gain of threshold τ ; instead, ThresholdGreedy maintains the following property at each threshold.\nProperty 1. Let gτ be the solution of ThresholdGreedy immediately after the iteration of the outer for loop corresponding to threshold τ . Then for each s ∈ S, there exists h ≤ gτ such that δs(h) < τ .\n4For convenience, we also define the maximum value of l, lmax = min{bs − gs, k − ‖g‖1} to be a pivot if lmax satisfies (P1) only, and set δs(g + lmaxs) = 0, so that all pivots satisfy both properties.\nAlgorithm 2 ThresholdGreedy 1: Input: f ∈ Fb, k ∈ N, κ, ε ∈ (0, 1). 2: Output: g ∈ NS 3: g← 0, M ← maxs∈S f(s). 4: for ( τ = M ; τ ≥ κε2Mk ; τ ← κτ ) do\n5: for s ∈ S do 6: l←BinarySearchPivot(f,g,b, s, k, τ) 7: g← g + ls 8: if ‖g‖1 = k then 9: return g\n10: return g\nPerformance ratios. Next, we present the main result of this section, the performance guarantee involving the DR and weak DR ratios. Observe that the query complexity of ThresholdGreedy is polynomial in the input size O(n log k).\nTheorem 1. Let an instance of Problem MCC be given, with f ∈ Fγd,γs,αb . If g is the solution returned by ThresholdGreedy and Ω is an optimal solution to this instance, then\nf(g) ≥ ( 1− e−κγdγs − ε ) f(Ω).\nThe query complexity of ThresholdGreedy is O ( n log k logκ ( ε2/k )) .\nIf η > 0 is given, the assignment κ = (1 − η/2), ε = η/2 yields performance ratio at least 1− e−γdγs − η.\nProof. If γd < ε, the ratio holds trivially; so assume γd ≥ ε. The proof of the following claim requires an application of the DR ratio.\nClaim 1. Let g be produced by a modified version of ThresholdGreedy that continues until ‖g‖1 = k. If we show f(g) ≥ (1− e−κγdγs)f(Ω), the results follows.\nThus, for the rest of the proof let g be as described in Claim 1. Let gt be the value of g after the tth execution of line 2 of ThresholdGreedy. Let lt be the tth pivot, such that gt = gt−1+ltst. The next claim lower bounds the marginal gain in terms of the DR ratio and the previous threshold.\nClaim 2. For each s ∈ {Ω− gt−1 ∧Ω},\nltγdκδs(g t−1) ≤ f(gt)− f(gt−1).\nProof. Let τ be the threshold at which ltst is added to gt−1; let s ∈ {Ω − gt−1 ∧ Ω}. If τ is the first threshold, γdδs(g\nt−1) ≤ δs(0) ≤ τ < τκ . If τ is not the first threshold, τ ′ = τ/κ is the previous threshold value of the previous iteration of the outer for loop. By Property 1, there\nexists h ≤ gτ ′ ≤ gt−1, such that δs(h) < τ ′. By the definition of DR ratio, γdδs(gt−1) ≤ δs(h) < τ ′ = τ/κ. In either case, by the fact that property (P1) of a pivot holds for lt, we have\nf(gt)− f(gt−1) ≥ ltτ ≥ ltγdκδs(gt−1).\nSince |Ω| ≤ k, we have by Claim 2\nf(gt)− f(gt−1) ≥ l tγdκ\nk ∑ s∈{Ω−(gt−1∧Ω)} δs(g t−1)\n= ltγdκ\nk ∑ s∈{Ω∨gt−1−gt−1} δs(g t−1)\n≥ l tγdγsκ\nk\n( f(Ω ∨ gt−1)− f(gt−1) ) ≥ l tγdγsκ\nk\n( f(Ω)− f(gt−1) ) ,\nwhere the equality follows from the lattice identity v ∨ w − v = w − v ∧ w for all v,w ∈ NS , the second inequality is by definition of the weak DR ratio, and the third inequality is from monotonicity. From here, we obtain f(g) ≥ ( 1−∏Tt=1 (1− ltγdγsκk )) f(Ω), from which the hypothesis of Claim 1 follows.\nQuery complexity. The for loop on line 2 (Alg. 2) iterates at most logκ ε\n2/k times; each iteration requires O(n log k) queries, by Lemma 2.\nFor additional speedup, the inner for loop of FastGreedy may be parallelized, which divides the factor of n in the query complexity by the number of threads but worsens the performance ratio; in addition to γd, γs, the generalized curvature α is required in the proof.\nCorollary 1. If the inner for loop of ThresholdGreedy is parallelized, the performance ratio becomes 1 − e−(1−α)γdγs − η, for η > 0. Remark 1. A careful analysis of the usage of γd,γs in the proof of Theorem 1 shows that the full power of the definitions of these quantities is not required. Rather, it is sufficient to consider ThresholdGreedy versions of these parameters, as defined in Appendix B. In the same way, we also have FastGreedy version of γs based upon the proof of Theorem 2. The FastGreedy version of the DR ratio is an integral part of how the algorithm works and is calculated directly by the algorithm, as we discuss in the next section."
  }, {
    "heading": "4.2. The FastGreedy Algorithm",
    "text": "The proof of the performance ratio of ThresholdGreedy requires both the submodularty ratio γs and the DR ratio γd. In this section, we provide an algorithm (FastGreedy, Alg.\n3) that achieves ratio 1− e−β∗γs − η, with factor β∗ ≥ γd that it can determine during its execution. Appendix E provides proofs for all lemmas, claims, and omitted details.\nDescription. FastGreedy employs a threshold framework analogous to ThresholdGreedy. Each iteration of the outer while loop of FastGreedy is analogous to an iteration of the outer for loop in ThresholdGreedy, in which elements are added whose marginal gain exceeds a threshold. FastGreedy employs BinarySearchPivot to find pivots for each s ∈ S for each threshold value τ . Finally, the parameter ε determines a minimum threshold value.\nAs its threshold, FastGreedy uses τ = βκm, where m is the maximum marginal gain found on line 3, parameter κ is the intended stepsize between thresholds as in ThresholdGreedy, and β is an upper bound on the DR ratio γd, as described below. This choice of τ has the following advantages over the approach of ThresholdGreedy: (1) since the threshold is related to the maximum marginal gain m, the theoretical performance ratio is improved; (2) the use of β to lower the threshold ensures the same5 worst-case query complexity as ThresholdGreedy and leads to substantial reduction of the number of queries in practice, as we demonstrate in Section 6.\nFastGreedy DR ratio β∗. If FastGreedy is modified6 to continue until ‖g‖1 = k, let the final, smallest value β∗ of β be termed the FastGreedy DR ratio on the instance. The FastGreedy DR ratio β∗ is at least the DR ratio γd of the function, up to the parameter δ:\nLemma 1. Let parameters κ, δ, ε ∈ (0, 1) be given. Throughout the execution of FastGreedy on an instance of Problem MCC with f ∈ Fγd,γsb , β ≥ γdδ. Since ε can be arbitrarily small, β∗ ≥ γdδ.\nProof. Initally, β = 1; it decreases by a factor of δ ∈ (0, 1) at most once per iteration of the while loop. Suppose β ≤ γd for some iteration i of the while loop, and let g have the value assigned immediately after iteration i, m have the value assigned after line 3 of iteration i. Since a valid pivot was found for each s ∈ S during iteration i, by property (P2) there exists gs ≤ g, δs(gs) < βκm ≤ γdκm. Hence δs(g) ≤ κm, by the definition of DR ratio. In iteration i + 1, m′ has the value of m from iteration i, so the value of m computed during iteration i+ 1 is at most κm′, and β does not decrease during iteration i+ 1.\nPerformance ratio. Next, we present the main result of this section. In contrast to ThresholdGreedy, the factor of\n5Up to a constant factor, which depends on γd. 6This modification can be accomplished by setting ε to ensure\nthe condition on line 3 is always true on this instance.\nAlgorithm 3 FastGreedy 1: Input: f ∈ Fb, k ∈ N, κ, δ, ε ∈ (0, 1). 2: Output: g ∈ NS 3: g ← 0, M ← maxs∈S f(s), m ← M,m′ ← M/κ, β ← 1\n4: while m ≥Mε2/k do 5: m← maxs∈S δs(g) 6: if m > κm′ then 7: β ← βδ 8: m′ ← m 9: τ ← βκm\n10: for s ∈ S do 11: l←BinarySearchPivot(f,g,b, s, k, τ) 12: g← g + ls 13: if ‖g‖1 = k then 14: return g 15: return g\nγd in the performance ratio has been replaced with β∗; at the termination of the algorithm, the value of β∗ is an output of FastGreedy if the solution g satisfies ‖g‖1 = k. In any case, by Lemma 1, the performance ratio is at worst the same as that of ThresholdGreedy.\nTheorem 2. Let an instance of Problem MCC be given, with f ∈ Fγd,γsb . Let g be the solution returned by FastGreedy with parameters κ, δ, ε ∈ (0, 1), and let Ω be an optimal solution to this instance; also, suppose γd ≥ ε. Let β∗ be the FastGreedy DR ratio on this instance. Then,\nf(g) ≥ ( 1− e−κβ∗γs − ε ) f(Ω)\nThe worst-case query complexity of FastGreedy is O (( logδ(γd) logκ(γd) + logκ ε 2/k ) n log k ) .\nIf η > 0 is given, the assignment κ = (1 − η/2), ε = η/2 yields performance ratio at least 1− e−β∗γs − η.\nProof of query complexity. The performance ratio is proved in Appendix E. Let m′1, . . . ,m ′ K be the sequence of m′ values in the order considered by the algorithm. By Lemma 1, m′j > κm ′ j−1 at most Γ = logδ γd times; label each such index j an uptick, and let j1, . . . , jl be the indices of each uptick in order of their appearance. Also, let ki be the first index after ji such that m′ki ≤ κm′ji−1, for each i ∈ {1, . . . , l}. Next, we will iteratively delete from the sequence of m′ values. Initially, let ` = l be the last uptick in the sequence; delete all termsm′j` , . . . ,m ′ k`−1 from them\n′ sequence. Set ` = `− 1 and repeat this process until ` = 0.\nClaim 3. For each ` selected in the iterative deletion above, there are at most logκ γd values deleted from the sequence.\nBy Claim 3 and the bound on the number of upticks, we have deleted at most logκ γd logδ γd thresholds m\n′ from the sequence; every term in the remaining sequence satisfies m′j ≤ κm′j−1; hence, the remaining sequence contains at most logκ ε\n2/k terms, by its initial and terminal values. The query complexity follows from the number of queries per value of m′, which is O(n log k) by Lemma 2."
  }, {
    "heading": "5. Influence Maximization: A General Framework",
    "text": "In this section, we provide a non-submodular framework for viral marketing on a social network that unifies the classical influence maximization (Kempe et al., 2003) with the boosting problem (Lin et al., 2017).\nOverview. The goal of influence maximization is to select seed users (i.e. initially activated users) to maximize the expected adoption in the social network, where the total number of seeds is restricted by a budget, such that the expected adoption in the social network is maximized. The boosting problem is, given a fixed seed set S, to incentivize (i.e. increase the susceptibility of a user to the influence of his friends) users within a budget such that the expected adoption with seed set S increases the most.\nOur framework combines the above two scenarios with a partial incentive: an incentive (say, x% off the purchase price) increases the probability a user will purchase the product independently and increases the susceptibility of the user to the influence of his friends. Hence, our problem asks how to best allocate the budget between (partially) seeding users and boosting the influence of likely extant seeds. Both the classical influence maximization and the non-submodular boosting problem can be obtained as special cases, as shown in Appendix F.\nOur model is related to the formulation of Demaine et al. (2014); however, they employ a submodular threshold-based model, while our model is inherently nonsubmodular due to the boosting mechanism (Lin et al., 2017). Also, GIM is related to the submodular budgeted allocation problem of Alon et al. (2012), in which the influence of an advertiser increases with the amount of budget allocated; the main difference with GIM is that we modify incoming edge weights with incentives instead of outgoing, which creates the boosting mechanism responsible for the non-submodularity.\nModel. Given a social network G = (V,E), and a product p, we define the following model of adoption. The allocation of budget to u is thought of as a discount towards purchasing the product; this discount increases the probability that this user will adopt or purchase the product. Furthermore, this discount increases the susceptibility of the\nuser to influence from its (incoming) social connections.\nFormally, an incentive level xu is chosen for each user u. With independent probability p(u,xu), user u initially activates or adopts the product; altogether, this creates a probabilistic initial set S of activated users. Next, through the classical Independent Cascade (IC) model7 of adoption, users influence their neighbors in the social network; wherein the weight p(v, u,xu) for edge (v, u) is determined by the incentive level xu of user u as well as the strength of the social connection from v to u.\nWe write px(H,T ) to denote the probability of full graph realization H and seed set T when x gives the incentive levels for each user. We write R(H,T ) to denote the size of the reachable set from T in realization H . The expected activation in the network given a choice x of incentive levels is given by I(x) = ∑ T⊆V ∑ H⊆G p\nx(H,T )R(H,T ), where an explicit formula for px(H,T ) is given in Appendix F. Finally, let A(x) = I(x)− I(0). Definition 4 (Generalized Influence Maximization (GIM)). Let social network G = (V,E) be given, together with the mappings i 7→ p(u, i), i 7→ p(u, v, i), for all u ∈ V, (u, v) ∈ E, for each i ∈ {0, . . . , L}, where L is the number of incentive levels. Given budget k, determine incentive levels x, with ‖x‖1 ≤ k, such that A(x) is maximized.\nBound on non-submodularity. Next, we provide a lower bound on the greedy DR ratios (see Appendix B). We emphasize that the assumption that the probability mappings as a function of incentive level be submodular does not imply the objective A(x) is DR-submodular. Theorem 3 is proved in Appendix F.\nTheorem 3. Let I be an instance of GIM, with budget k. Let ce = max(u,v)∈E,i∈L p(u,v,i+1) p(u,v,i) , cn = maxx∈V,i∈L p(x,i+1) p(x,i) . Suppose for all (u, v) ∈ E,w ∈ V , the mappings i 7→ p(u, v, i), i 7→ p(w, i) are submodular set functions. Then, the greedy DR ratios defined in Appendix B and the FastGreedy DR ratio are lower bounded by c−k∆e c −k n , where ∆ is the maximum in-degree in G."
  }, {
    "heading": "6. Experimental Evaluation",
    "text": "In this section, we evaluate our proposed algorithms for the GIM problem defined in Section 5. Source code for the implementation is available at https://gitlab. com/emallson/lace. We evaluate our algorithms as compared with StandardGreedy; by the naive reduction of the lattice to sets in exponential time, this algorithm is equivalent to performing this reduction and running the standard greedy for sets, the performance of which for\n7The IC model is defined in Appendix F.\nnon-submodular set functions was analyzed by Bian et al. (2017b).\nIn Section 6.1, we describe our methodology; in Section 6.2, we compare the algorithms and non-submodularity parameters. In Appendix G.1, we explore the behavior of FastGreedy as the parameters δ, κ, and ε are varied."
  }, {
    "heading": "6.1. Methodology",
    "text": "Our implementation uses Monte Carlo sampling to estimate the objective value A(x), with 10 000 samples used. As a result, each function query is relatively expensive.\nWe evaluate on two networks taken from the SNAP dataset (Leskovec & Krevl, 2014): ca-GrQc (“GrQc”; 15k nodes, 14.5K edges) and facebook (“Facebook”; 4k nodes, 176K edges). Unless otherwise specified, we use 10 repetitions per datapoint and display the mean. The width of shaded intervals is one standard deviation. Standard greedy is omitted from some figures where running time is prohibitive. Unless noted otherwise, we use default settings of ε = 0.05, δ = 0.9, κ = 0.95. We use a uniform box constraint and assign each user the same number of incentive levels; the maximum incentive level for a user corresponds to giving the product to the user for free and hence deterministically seeds the user; we adopt linear models for the mappings i 7→ p(u, i), i 7→ p(u, v, i). We often plot versus K, which is defined as the maximum number of deterministic seeds; for example, if k = 200 with 10 incentive levels, then K = 20."
  }, {
    "heading": "6.2. Results",
    "text": "In this section, we demonstrate the following: (1) our algorithms exhibit virtually identical quality of solution with StandardGreedy, (2) our algorithms query the function much fewer times, which leads to dramatic runtime improvement over StandardGreedy, (3) FastGreedy further reduces the number of queries of ThresholdGreedy while sacrificing little in solution quality, and (4) the nonsubmodularity parameters on a small instance are com-\nputed, which provides evidence that our theoretical performance ratios are useful.\nQuality of Solution In Fig. 1(a), we plot A(g) for the solution returned by each algorithm on the GrQc network with 10 incentive levels; the difference in quality of solution returned by the three algorithms is negligible. In Fig. 1(b), we plot the same for the Facebook network with 100 incentive levels; on Facebook, we drop StandardGreedy due to its prohibitive runtime. FastGreedy is observed to lose a small (up to 3%) factor, which we consider acceptable in light of its large runtime improvement, which we discuss next.\nNumber of Queries Next, we present in Fig. 2 the number of function queries8 each algorithm requires on the GrQc and Facebook networks. StandardGreedy required up to 20M queries on Facebook, hence it is not shown in Fig. 2(b). Both of our algorithms provide a large improvement over StandardGreedy; in particular, notice that StandardGreedy increases linearly with k, while both of the others exhibit logarithmic increase in agreement with the theoretical query complexity of each. Furthermore, FastGreedy uses at least 14.5% fewer function queries than ThresholdGreedy and up to 43% fewer as k grows.\n8Our implementation is in terms of the marginal gain. The number of function queries shown is the number of times the marginal gain function was called.\nNon-Submodularity Parameters The value of the FastGreedy DR ratio β∗ on GrQc is shown in Fig. 4(a); notice that it is relatively stable as the budget increases from K = 20 to 100, although there is substantial drop from 10 incentive levels to 100; this may be explained as an increase in the non-submodularity resulting from inaccurate sampling of A, since it is more difficult to detect differences between the finer levels. Still, on all instances tested, β∗ > 0.6, which suggests the worst-case performance ratio of FastGreedy is not far from that of StandardGreedy.\nFinally, we examine the various non-submodularity parameters on a very small instance which admits their computation: a random Barabasi-Albert network with 10 nodes and 10 incentive levels. We compute the FastGreedy version of the submodularity ratio γs defined in Appendix B by direct enumeration and consider the FastGreedy DR ratio β∗. Results are shown in Fig. 4(b). The value of β∗ is close to 1 and remains constant with increasing budget k, while the FastGreedy submodularity ratio decreases slowly with k. With β∗ and the FastGreedy γs, we can compute the worst-case performance ratio of FastGreedy across these instances: 0.449692."
  }, {
    "heading": "7. Conclusions",
    "text": "In this work, we provide two approximation algorithms for maximizing non-submodular functions with respect to a cardinality constraint on the integer lattice with polynomial query complexity. Since set functions are a special case, our work provides faster algorithms for the same problem with set functions than the standard greedy algorithm, although the performance ratio degrades from at least 1−e−γs to 1−e−β∗γs , where β∗ is the FastGreedy DR Ratio. We propose a natural application of non-submodular influence maximization, for which we lower bound the relevant non-submodularity parameters and validate our algorithms."
  }, {
    "heading": "Acknowledgement",
    "text": "This work was supported in part by US NSF EFRI 1441231, CCF 1422116, and DTRA HDTRA1-14-1-0055."
  }],
  "year": 2018,
  "references": [{
    "title": "Optimizing budget allocation among channels and influencers",
    "authors": ["Alon", "Noga", "Gamzu", "Iftah", "Tennenholtz", "Moshe"],
    "venue": "Proceedings of the 21st International Conference on World Wide Web (WWW), pp",
    "year": 2012
  }, {
    "title": "Fast algorithms for maximizing submodular functions",
    "authors": ["Badanidiyuru", "Ashwinkumar", "J. Vondrák"],
    "venue": "Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
    "year": 2014
  }, {
    "title": "Non-monotone Continuous DR-submodular Maximization: Structure and Algorithms",
    "authors": ["Bian", "An", "Levy", "Kfir Y", "Krause", "Andreas", "Buhmann", "Joachim M"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2017
  }, {
    "title": "Guarantees for Greedy Maximization of Non-submodular Functions with Applications",
    "authors": ["Bian", "Andrew An", "Buhmann", "Joachim M", "Krause", "Andreas", "Tschiatschek", "Sebastian"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Robust Maximization of Non-Submodular Objectives",
    "authors": ["Bogunovic", "Ilija", "Zhao", "Junyao", "Cevher", "Volkan"],
    "venue": "In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2018
  }, {
    "title": "Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem",
    "authors": ["Conforti", "Michele", "Cornuéjols", "Gérard"],
    "venue": "Discrete Applied Mathematics,",
    "year": 1984
  }, {
    "title": "Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection",
    "authors": ["Das", "Abhimanyu", "Kempe", "David"],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML),",
    "year": 2011
  }, {
    "title": "Streaming Weak Submodularity: Interpreting Neural Networks on the Fly",
    "authors": ["Elenberg", "Ethan R", "Dimakis", "Alexandros G", "Feldman", "Moran", "Karbasi", "Amin"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2017
  }, {
    "title": "A Reduction for Optimizing Lattice Submodular Functions with Diminishing Returns",
    "authors": ["Ene", "Alina", "Nguyen", "Huy L"],
    "venue": "arXiv preprint arXiv:1606.08362v1,",
    "year": 2016
  }, {
    "title": "Welfare Maximization and the Supermodular Degree",
    "authors": ["Feige", "Uriel", "Izsak", "Rani"],
    "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science (ITCS), pp",
    "year": 2013
  }, {
    "title": "Submodular Function Maximization over Distributive and Integer Lattices",
    "authors": ["Gottschalk", "Corinna", "Peis", "Britta"],
    "venue": "arXiv preprint arXiv:1505:05423,",
    "year": 2016
  }, {
    "title": "Maximization of Approximately Submodular Functions",
    "authors": ["Horel", "Thibaut", "Singer", "Yaron"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Maximizing the spread of influence through a social network",
    "authors": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "Éva"],
    "venue": "In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
    "year": 2003
  }, {
    "title": "Efficient sensor placement optimization for securing large water distribution networks",
    "authors": ["Krause", "Andreas", "Leskovec", "Jure", "Guestrin", "Carlos", "VanBriesen", "Jeanne M", "Faloutsos", "Christos"],
    "venue": "Journal of Water Resources Planning and Management,",
    "year": 2008
  }, {
    "title": "NearOptimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies",
    "authors": ["Krause", "Andreas", "Singh", "Ajit", "Guestrin", "Carlos"],
    "venue": "In Journal of Machine Learning Research,",
    "year": 2008
  }, {
    "title": "Combinatorial auctions with decreasing marginal utilities",
    "authors": ["Lehmann", "Benny", "Daniel", "Nisan", "Noam"],
    "venue": "Games and Economic Behavior,",
    "year": 2006
  }, {
    "title": "SNAP Datasets: Stanford large network dataset collection",
    "authors": ["Leskovec", "Jure", "Krevl", "Andrej"],
    "venue": "http://snap. stanford.edu/data,",
    "year": 2014
  }, {
    "title": "Boosting information spread: An algorithmic approach",
    "authors": ["Lin", "Yishi", "Chen", "Wei", "Lui", "John C.S"],
    "venue": "Proceedings of the International Conference on Data Engineering (ICDE),",
    "year": 2017
  }, {
    "title": "Lazier Than Lazy Greedy",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondrak", "Jan", "Krause", "Andreas"],
    "venue": "In Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2015
  }, {
    "title": "On Multiset Selection with Size Constraints",
    "authors": ["Qian", "Chao", "Zhang", "Yibo", "Tang", "Ke", "Yao", "Xin"],
    "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2018
  }, {
    "title": "A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice",
    "authors": ["Soma", "Tasuku", "Yoshida", "Yuichi"],
    "venue": "Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Maximizing Monotone Submodular Functions over the Integer Lattice",
    "authors": ["Soma", "Tasuku", "Yoshida", "Yuichi"],
    "venue": "Integer Programming and Combinatorial Optimization,",
    "year": 2016
  }, {
    "title": "Non-monotone DRSubmodular Function Maximization",
    "authors": ["Soma", "Tasuku", "Yoshida", "Yuichi"],
    "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2017
  }],
  "id": "SP:3b42d827593585c4c24a03b0620fdc982bbecb15",
  "authors": [{
    "name": "Alan Kuhnle",
    "affiliations": []
  }, {
    "name": "David Smith",
    "affiliations": []
  }, {
    "name": "Victoria G. Crawford",
    "affiliations": []
  }],
  "abstractText": "The optimization of submodular functions on the integer lattice has received much attention recently, but the objective functions of many applications are non-submodular. We provide two approximation algorithms for maximizing a nonsubmodular function on the integer lattice subject to a cardinality constraint; these are the first algorithms for this purpose that have polynomial query complexity. We propose a general framework for influence maximization on the integer lattice that generalizes prior works on this topic, and we demonstrate the efficiency of our algorithms in this context.",
  "title": "Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice"
}