{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 968–978 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n968\ntest and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance."
  }, {
    "heading": "1 Introduction",
    "text": "The choice of the words to express an opinion depends on the domain as users often use domainspecific words (Qiu et al., 2009; Sharma and Bhattacharyya, 2015). For example, entertaining and boring are frequently used in the movie domain to express an opinion; however, finding these words in the electronics domain is rare. Moreover, there are words which are likely to be used across domains in the same proportion, but may change their polarity orientation from one domain to another (Choi et al., 2009). For example, a word like unpredictable is positive in the movie domain (un-\npredictable plot), but negative in the automobile domain (unpredictable steering). Such a polarity changing word should be assigned positive orientation in the movie domain and negative orientation in the automobile domain.1 Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades.\nGenerally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus (Pang et al., 2002; Kanayama and Nasukawa, 2006; Pang and Lee, 2008; Esuli and Sebastiani, 2005; Breck et al., 2007; Li et al., 2009; Prabowo and Thelwall, 2009; Taboada et al., 2011; Cambria et al., 2013; Rosenthal et al., 2014). This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task (Bhattacharyya, 2015). On the other hand, domain adaptation techniques work in contrast to traditional supervised techniques on the principle of transferring learned knowledge across domains (Blitzer et al., 2007; Pan et al., 2010; Bhatt et al., 2015). The existing transfer learning based domain adaptation algorithms for cross-domain classification have generally been proven useful in reducing the labeled data requirement, but they do not consider words like unpredictable that change polarity orientation across domains. Transfer (reuse) of changing polarity words affects the cross-domain performance negatively. Therefore, one cannot use transfer learning as the proverbial hammer, rather one needs to gauge what to transfer from the source domain to the target domain.\nIn this paper, we propose that the words which\n1The word ‘unpredictable’ is a classic example of changing (inconsistent) polarity across domains (Turney, 2002; Fahrni and Klenner, 2008).\nare equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis. χ2 is a popularly used and reliable statistical test to identify significance and polarity of a word in an annotated corpus (Oakes et al., 2001; Al-Harbi et al., 2008; Cheng and Zhulyn, 2012; Sharma and Bhattacharyya, 2013). However, for an unlabeled corpus no such statistical technique is applicable. Therefore, identification of words which are significant with a consistent polarity across domains is a non-trivial task. In this paper, we present a novel technique based on χ2 test and cosine-similarity between context vector of words to identify Significant Consistent Polarity (SCP) words across domains.2 The major contribution of this research is as follows.\n1. Extracting significant consistent polarity words across domains: A technique which exploits cosine-similarity between context vector of words and χ2 test is used to identify SCP words across labeled source and unlabeled target domains.\n2. An ensemble-based adaptation algorithm: A classifier (Cs) trained on SCP words in the labeled source domain acts as a seed to initiate a classifier (Ct) on the target specific features. These classifiers are then combined in a weighted ensemble to further enhance the cross-domain classification performance.\nOur results show that our approach gives a statistically significant improvement over Structured Correspondence Learning (SCL) (Bhatt et al., 2015) and common unigrams in identification of transferable words, which eventually facilitates a more accurate sentiment classifier in the target domain. The road-map for rest of the paper is as follows. Section 2 describes the related work. Section 3 describes the extraction of the SCP and the ensemble-based adaptation algorithm. Section 4 elaborates the dataset and the experimental protocol. Section 5 presents the results and section 6 reports the error analysis. Section 7 concludes the paper.3\n2SCP words are words which are significant in both the domains with consistent polarity orientation.\n3Majority of this work is done at Conduent Labs India till February 2016."
  }, {
    "heading": "2 Related Work",
    "text": "The most significant efforts in the learning of transferable knowledge for cross-domain text classification are Structured Correspondence Learning (SCL) (Blitzer et al., 2007) and Structured Feature Alignment (SFA) (Pan et al., 2010). SCL aims to learn the co-occurrence between features from the two domains. It starts with learning pivot features that occur frequently in both the domains. It models correlation between pivots and all other features by training linear predictors to predict presence of pivot features in the unlabeled target domain data. SCL has shown significant improvement over a baseline (shift-unaware) model. SFA uses some domain-independent words as a bridge to construct a bipartite graph to model the co-occurrence relationship between domainspecific words and domain-independent words. Our approach also exploits the concept of cooccurrence (Pan et al., 2010), but we measure the co-occurrence in terms of similarity between context vector of words, unlike SCL and SFA, which literally look for the co-occurrence of words in the corpus. The use of context vector of words in place of words helps to overcome the data sparsity problem (Sharma et al., 2015).\nDomain adaptation for sentiment classification has been explored by many researchers (Jiang and Zhai, 2007; Ji et al., 2011; Saha et al., 2011; Glorot et al., 2011; Xia et al., 2013; Zhou et al., 2014; Bhatt et al., 2015). Most of the works have focused on learning a shared low dimensional representation of features that can be generalized across different domains. However, none of the approaches explicitly analyses significance and polarity of words across domains. On the other hand, Glorot et al., (2011) proposed a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Zhou et al., (2014) also proposed a deep learning approach to learn a feature mapping between cross-domain heterogeneous features as well as a better feature representation for mapped data to reduce the bias issue caused by the crossdomain correspondences. Though deep learning based approaches perform reasonably good, they don’t perform explicit identification and visualization of transferable features across domains unlike SFA and SCL, which output a set of words as transferable (reusable) features. Our approach explicitly determines the words which are equally\nsignificant with a consistent polarity across source and target domains. Our results show that the use of SCP words as features identified by our approach leads to a more accurate cross-domain sentiment classifier in the unlabeled target domain."
  }, {
    "heading": "3 Approach: Cross-domain Sentiment Classification",
    "text": "The proposed approach identifies words which are equally significant for sentiment classification with a consistent polarity across source and target domains. These Significant Consistent Polarity (SCP) words make a set of transferable knowledge from the labeled source domain to the unlabeled target domain for cross-domain sentiment analysis. The algorithm further adapts to the unlabeled target domain by learning target domain specific features. The following sections elaborate SCP features extraction (3.1) and the ensemblebased cross-domain adaptation algorithm (3.2)."
  }, {
    "heading": "3.1 Extracting SCP Features",
    "text": "The words which are not significant for classification in the labeled source domain, do not transfer useful knowledge to the target domain through a supervised classifier trained in the source domain. Moreover, words that are significant in both the domains, but have different polarity orientation transfer the wrong information to the target domain through a supervised classifier trained in the labeled source domain, which also downgrade the cross-domain performance.\nOur algorithm identifies the significance and the polarity of all the words individually in their respective domains. Then the words which are significant in both the domains with the consistent polarity orientation are used to initiate the crossdomain adaptation algorithm. The following sections elaborate how the significance and the polarity of the words are obtained in the labeled source and the unlabeled target domains."
  }, {
    "heading": "3.1.1 Extracting Significant Words with the Polarity Orientation from the Labeled Source Domain",
    "text": "Since we have a polarity annotated dataset in the source domain, a statistical test like χ2 test can be applied to find the significance of a word in the corpus for sentiment classification (Cheng and Zhulyn, 2012; Zheng et al., 2004). We have used goodness of fit chi2 test with equal number of reviews in positive and negative corpora. This test is\ngenerally used to determine whether sample data is consistent with a null hypothesis.4 Here, the null hypothesis is that the word is equally used in the positive and the negative corpora. The χ2 test is formulated as follows:\nχ2(w) = ((cwp − µw)2 + (cwn − µw)2)/µw (1)\nWhere, cwp is the observed count of a wordw in the positive documents and cwn is the observed count in the negative documents. µw represents an average of the word’s count in the positive and the negative documents. Here, µw is the expected count or the value of the null-hypothesis. There is an inverse relation between χ2 value and the p-value which is probability of the data given null hypothesis is true. In such a case where a word results in a pvalue smaller than the critical p-value (0.05), we reject the null-hypothesis. Consequently, we assume that the word w belongs to a particular class (positive or negative) in the data, hence it is a significant word for classification (Sharma and Bhattacharyya, 2013).\nPolarity of Words in the Labeled Source Domain: Chi-square test substantiates the statistically significant association of a word with a class label. Based on this association we assign a polarity orientation to a word in the domain. In other words, if a word is found significant by χ2 test, then the exact class of the word is determined by comparing cwp and c w n . For instance, if c w p is higher than cwn , then the word is positive, else negative."
  }, {
    "heading": "3.1.2 Extracting Significant Words with the Polarity Orientation from the Unlabeled Target Domain",
    "text": "Target domain data is unlabeled and hence, χ2 test cannot be used to find significance of the words. However, to obtain SCP words across domains, we take advantage of the fact that we have to identify significance of only those words in the target domain which are already proven to be significant in the source domain. We presume that a word which is significant in the source domain as per χ2 test and occurs with a frequency greater than a certain threshold (θ) in the target domain is significant in the target domain also.\ncountt(significants(w)) > θ ⇒ significantt(w) (2)\n4http://stattrek.com/chi-square-test/ goodness-of-fit.aspx?Tutorial=AP.\nEquation (2) formulates the significance test in the unlabeled target (t) domain. Here, function significants assures the significance of the word w in the labeled source (s) domain and countt gives the normalized count of the w in t.5 χ2 test has one key assumption that the expected value of an observed variable should not be less than 5 to be significant. Considering this assumption as a base, we fix the value of θ as 10.6\nPolarity of Words in the Unlabeled Target Domain: Generally, in a polar corpus, a positive word occurs more frequently in context of other positive words, while a negative word occurs in context of other negative words (Sharma et al., 2015).7 Based on this hypothesis, we explore the contextual information of a word that is captured well by its context vector to assign polarity to words in the target domain (Rill et al., 2012; Rong, 2014). Mikolov et al., (2013) showed that similarity between context vector of words in vicinity such as ‘go’ and ‘to’ is higher compared to distant words or words that are not in the neighborhood of each other. Here, the observed concept is that if a word is positive, then its context vector learned from the polar review corpus will give higher cosine-similarity with a known positive polarity word in comparison to a known negative polarity word or vice versa. Therefore, based on the cosine-similarity scores we can assign the label of the known polarity word to the unknown polarity word. We term known polarity words as Positivepivot and Negative-pivot.\nContext Vector Generation: To compute context vector (conV ec) of a word (w), we have used publicly available word2vec toolkit with the skip-gram model (Mikolov et al., 2013).8 In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given window are predicted (Faruqui et al., 2014). We construct a 100 dimensional vector for each can-\n5Normalized count of w in t shows the proportion of occurrences of w in t.\n6We tried with smaller values of theta also, but they were not found as effective as theta value of 10 for significant words identification.\n7For example, ‘excellent’ will be used more often in positive reviews in comparison to negative reviews, hence, it would have more positive words in its context. Likewise, ‘terrible’ will be used more frequently in negative reviews in comparison to positive reviews, hence, it would have more negative words in its context.\n8 Available at: https://radimrehurek.com/ gensim/models/word2vec.html\ndidate word from the unlabeled target domain data. The decision method given in Equation 3 defines the polarity assignment to the unknown polarity words of the target domain. If a word w gives a higher cosine-similarity with the PosPivot (Positive-pivot) than the NegPivot (Negative-pivot), the decision method assigns the positive polarity to the word w, else negative polarity to the word w.\nIf(cosine(conV ec(w), conV ec(PosPivot)) > cosine(conV ec(w), conV ec(NegPivot)))\n⇒ Positive If(cosine(conV ec(w), conV ec(PosPivot)) < cosine(conV ec(w), conV ec(NegPivot)))\n⇒ Negative\n(3)\nPivot Selection Method: We empirically observed that a polar word which has the highest frequency in the corpus gives more coverage to estimate the polarity orientation of other words while using context vector. Essentially, the frequent occurrence of the word in the corpus allows it to be in context of other words frequently. Therefore a polar word having the highest frequency in the target domain is observed to be more accurate as pivot for identification of polarity of input words.9 Table 1 shows the examples of a few words in the electronics domain whose polarity orientation is derived based on the similarity scores obtained with PosPivot and NegPivot words in the electronics domain. Transferable Knowledge: The proposed algorithm uses the above mentioned techniques to identify the significance and the polarity of words in the labeled source data (cf. Section 3.1.1) and the unlabeled target data (cf. Section 3.1.2). The words which are found significant in both the domains with the same polarity orientation form a set of SCP features for cross-domain sentiment classification. The weights learned for the SCP features in the labeled source domain by the classification algorithm can be reused for sentiment classification in the unlabeled target domain as SCP features have consistent impacts in both the domains.\n9 To obtain the highest frequency based pivots, words in the target corpus (unlabeled) were ordered based on their frequency in the corpus, then a few top words were manually observed (by three human annotators) to pick out a positive word and a negative word. The positive and negative polarity of pivots were confirmed manually to get rid of random high frequency words (for example, function words). These highest frequency polar words were set as Positive-pivot and Negative-pivot."
  }, {
    "heading": "3.2 Ensemble-based Cross-domain Adaptation Algorithm",
    "text": "Apart from the transferable SCP words (Obtained in Section 3.1), each domain has specific discriminating words which can be discovered only from that domain data. The proposed cross-domain adaptation approach (Algorithm 1) attempts to learn such domain specific features from the target domain using a classifier trained on SCP words in the source domain. An ensemble of the classifiers trained on the SCP features (transferred from the source) and domain specific features (learned within the target) further enhances the cross-domain performance. Table 2 lists the notations used in the algorithm. The working of the cross-domain adaptation algorithm is as follows:\n1. Identify SCP features from the labeled source and the unlabeled target domain data.\n2. A SVM based classifier is trained on SCP words as features using labeled source domain data, named as Cs.\n3. The classifier Cs is used to predict the labels for the unlabeled target domain instancesDut ,\nand the confidently predicted instances ofDut form a set of pseudo labeled instances Rnt .\n4. A SVM based classifier is trained on the pseudo labeled target domain instances Rnt , using unigrams in Rnt as features to include the target specific words, this classifier is named as Ct .\n5. Finally, a Weighted Sum Model (WSM) of Cs and Ct gives a classifier in the target domain.\nThe confidence in the prediction of Dut is measured in terms of the classification-score of the document, i.e., the distance of the input document from the separating hyper-plane given by the SVM classifier (Hsu et al., 2003). The top n confidently predicted pseudo labeled instances (Rnt ) are used to train classifier Ct, where n depends on a threshold that is empirically set to | ± 0.2|.10 The classifier Cs trained on the SCP features (transferred knowledge) from the source domain and the classifier Ct trained on self-discovered target specific features from the pseudo labeled target domain instances bring in complementary information from the two domains. Therefore, combiningCs andCt in a weighted ensemble (WSM) further enhances the cross-domain performance. Algorithm 1 gives the pseudo code of the proposed adaptation approach.\nInput: Dls = {r1s , r2s , r3s , ....rjs}, Dut = {r1t , r2t , r3t , ....rkt }, Vs = {w1s , w2s , w3s , ....wps}, Vt = {w1t , w2t , w3t , ....wqt }\nOutput: Sentiment Classifier in the Target Domain 1: SCP = sigPol(Dls) ∩ sigPol(Dut ) 2: Cs = Train-SVM(Dls), where f = SCP 3: Predict Label: Cs(Dut )→ Dlt 4: Select: Rnt | ∀rit ∈ Dut , Cs(rit) > φ, where i ∈ {1, 2....k} and n <= k 5: Ct = Train-SVM(Rnt ), where f = {unigrams(Rnt )} 6: WSM = (Cs ∗Ws + Ct ∗Wt)/(Ws +Wt) 7: Sentiment Classifier in the Target Domain = WSM\nALGORITHM 1: Building of target domain classifier from the source domain\nWeighted Sum Model (WSM): The weighted ensemble of classifiers helps to overcome the er-\n10Balamurali et al., (2013) have shown that 350 to 400 labeled documents are required to get a high accuracy classifier in a domain using supervised classification techniques, but beyond 400 labeled documents there is not much improvement in the classification accuracy. Hence, threshold on classification score is set such that it can give a sufficient number of documents for supervised classification. Threshold |±0.2| gives documents between 350 to 400.\nrors produced by the individual classifier. The formulation of WSM is given in step-6 of the Algorithm 1. If Cs has wrongly predicted a document at boundary point and Ct has predicted the same document confidently, then weighted sum of Cs and Ct predicts the document correctly or vice versa. For example, a document is classified by Cs as negative (wrong prediction) with a classification-score of −0.07, while the same document is classified by Ct as positive (correct prediction) with a classification-score of 0.33, the WSM of Cs and Ct will classify the document as positive with a classification-score of 0.12 (Equation 4).\nWSM = (−0.07 ∗ 0.765 + 0.33 ∗ 0.712)\n(0.765 + 0.712) = 0.12\n(4) Here 0.765 and 0.712 are the weights Ws and Wt to the classifiers Cs and Ct respectively. Weights to the Classifiers in WSM: The weights Ws and Wt are the classification accuracies obtained by Cs and Ct respectively on the crossvalidation data from the target domain. The weights Ws and Wt allow Cs and Ct to participate in the WSM in proportion of their accuracy on the cross-validation data. This restriction facilitates the domination of the classifier which is more accurate."
  }, {
    "heading": "4 Dataset & Experimental Protocol",
    "text": "In this paper, we show comparison between SCPbased domain adaptation (our approach) and SCLbased domain adaptation approach proposed by Bhatt el al. (2015) using four domains, viz., Electronics (E), Kitchen (K), Books (B), and DVD.11 We use SVM algorithm with linear kernel (Tong and Koller, 2002) to train a classifier in all the mentioned classification systems in the paper. To implement SVM algorithm, we have used the publicly available Python based Scikit-learn package (Pedregosa et al., 2011).12 Data in each domain is divided into three parts, viz., train (60%), validation (20%) and test (20%). The SCP words are extracted from the training data. The weights WS and Wt for the source and target classifiers are essentially accuracies obtained by Cs and Ct\n11The same multi-domain dataset is used by Bhatt et al. (2015), available at: http://www.cs.jhu.edu/ ˜mdredze/datasets/sentiment/index2.html.\n12Available at: http://scikit-learn.org/ stable/modules/svm.html.\nrespectively on validation dataset from the target domain. We report the accuracy for all the systems on the test data. Table 3 shows the statistics of the dataset."
  }, {
    "heading": "5 Results",
    "text": "In this paper, we compare our approach with Structured Correspondence Learning (SCL) and common unigrams. SCL is used by Bhatt et al., (2015) for identification of transferable information from the labeled source domain to the unlabeled target domain for cross-domain sentiment analysis. They showed that transferable features extracted by SCL provide a better cross-domain sentiment analysis system than the transferable features extracted by Structured Feature Alignment (Pan et al., 2010). The SCL-based sentiment classifier in the target domain proposed by Bhatt et. al., (2015) is state-of-the-art for cross-domain sentiment analysis. On the other hand, common unigrams of the source and target are the most visible transferable information.13\nGold standard SCP words: Chi-square test gives us significance and polarity of the word in the corpus by taking into account the polarity labels of the reviews. Application of chi-square test in both the domains, considering that the target domain is also labeled, gives us gold standard SCP words. There is no manual annotation involved.\nF-score for SCP Words Identification Task: The set of SCP words represent the usable information across domains for cross-domain classification, hence we compare the F-score for the SCP words identification task obtained with our approach, SCL and common-unigrams in Figure 1. It demonstrates that our approach gives a huge improvement in the F-score over SCL and common unigrams for all the 12 pairs of the source and target domains. To measure the statistical significance of this improvement, we applied t-test on the F-score distribution obtained with our approach, SCL and common unigrams. t-test is a\n13Common unigrams is a set of unique words which appear in both the domains.\nstatistical significance test. It is used to determine whether two sets of data are significantly different or not.14 Our approach performs significantly better than SCL and common unigrams, while SCL performs better than common unigrams as per ttest.\nComparison among Cs, Ct and WSM: Table 4 shows the comparison among classifiers obtained in the target domain using SCP given by our approach, SCL, common-unigrams, and gold standard SCP for electronics as the source and movie as the target domains. Since electronics and movie are two very dissimilar domains in terms of domain specific words, unlike, books and movie, getting a high accuracy classifier in the movie domain from the electronics domain is a challenging task (Pang et al., 2002). Therefore, in Table 4 results are reported with electronics as the source domain and movie as the target domain.15 In all four cases, there is difference in the transferred information from the source to the target, but the ensemblebased classification algorithm (Section 3.2) is the same. Table 4 depicts sentiment classification accuracy obtained with Cs, Ct and WSM. The weights Ws and Wt in WSM are normalized accuracies by Cs and Ct respectively on the validation set from the target domain. The fourth column (size) represents the feature set size. We observed that WSM gives the highest accuracy, which validates our assumption that a weighted sum of two classifiers is better than the performance of individual classifiers. The WSM accuracy obtained with SCP words given by our approach is comparable to the accuracy obtained with gold standard SCP words.\nThe motivation of this research is to learn shared representation cognizant of significant and polarity changing words across domains. Hence, we report cross-domain classification accuracy obtained with three different types of shared representations (transferable knowledge), viz., common-unigrams, SCL and our approach.16 System-1, system-2 and system-3 in Table 5 show the final cross-domain sentiment classification accuracy obtained with WSM in the target domain\n14The detail about the test is available at: http://www. socialresearchmethods.net/kb/stat_t.php.\n15The movie review dataset is a balanced corpus of 2000 reviews. Available at: http://www.cs.cornell. edu/people/pabo/movie-review-data/\n16The reported accuracy is the ratio of correctly predicted documents to that of the total number of documents in the test dataset.\nfor 12 pairs of source and target using commonunigrams, SCL and our approach respectively.\nSystem-1: This system considers commonunigrams of both the domains as shared representation. System-2: It differs from system-1 in the shared representation, which is learned using Structured Correspondence Learning (SCL) (Bhatt et al., 2015) to initiate the process. System3: This system implements the proposed domain adaptation algorithm. Here, the shared representation is the SCP words and the ensemble-based domain adaptation algorithm (Section 3.2) gives the final classifier in the target domain. Table 5 depicts that the system-3 is better than system-1 and system-2 for all pairs, except K to B and B to D. For these two pairs, system-2 performs better than system-3, though the difference in accuracy is very low (below 1%).\nTo enhance the final accuracy in the target domain, Bhatt et al., (2015) performed iterations over the pseudo labeled target domain instances (Rnt ). In each iteration, they obtained a new Ct trained on increased number of pseudo labeled documents. This process is repeated till all the training instances of the target domain are considered. The Ct obtained in the last iteration makes WSM with Cs which is trained on the transferable features given by SCL. Bhatt et al., (2015) have shown that iteration-based domain adaptation technique is more effective than one-shot\nadaptation approaches. System-4, system-5, and system-6 in Table 5 incorporate the iterative process into system-1, system-2, and system-3 respectively. We observed the same trend after the inclusion of the iterative process also, as the SCPbased system-6 performed the best in all 12 cases. On the other hand, SCL-based system-5 performs better than the common-unigrams based system4. Table 7 shows the results of significance test (ttest) performed on the accuracy distributions produced by the six different systems. The notice-\nable point is that the iterations over SCL (system5) and our approach (system-6) narrow down the difference in the accuracy between system-2 and system-3 as system-2 and system-3 have a statistically significant difference in accuracy with the p-value of 0.039 (Row-4 of Table 7), but the difference between system-5 and system-6 is not statistically significant. Essentially, system-3 does not give much improvement with iterations, unlike system-2. In other words, addition of the iterative process with the shared representation given by SCL overcomes the errors introduced by SCL. On the other hand, SCP given by our approach were able to produce a less erroneous system in oneshot. Table 6 shows the in-domain sentiment classification accuracy obtained with unigrams and significant words as features considering labeled data in the domain. System-6 tries to equalize the in-domain accuracy obtained with unigrams.\nTo validate our assertion that polarity preserving significant words (SCP) across source and target domains make a less erroneous set of transferable knowledge from the source domain to the target domain, we computed Pearson productmoment correlation between F-score obtained for our approach (cf. Figure 1) and cross-domain accuracy obtained with SCP (System-3, cf. Table 5). We observed a strong positive correlation (r) of 0.78 between F-score and cross-domain accuracy. Essentially, an accurate set of SCP words positively stimulates an improved classifier in the unlabeled target domain."
  }, {
    "heading": "6 Error Analysis",
    "text": "The pairs of domains which share a greater number of domain-specific words, result in a higher accuracy cross-domain classifier. For example, Electronics (E) and Kitchen (K) domains share many domain-specific words, hence pairing of such similar domains as the source and the target results into a higher accuracy classifier in the target domain. Table 5 shows that K→E outperforms B→E and D→E, and E→K outperforms B→K and D→K. On the other hand, DVD (D) and electronics are two very different domains unlike electronics and Kitchen, or DVD and books. The DVD dataset contains reviews about the music albums. This difference in types of reviews makes them to share less number of words. Table 8 shows the percent (%) of common words among the 4 domains. The percent of common unique words are common unique words divided by the summation\nof unique words in the domains individually."
  }, {
    "heading": "7 Conclusion",
    "text": "In this paper, we proposed that the Significant Consistent Polarity (SCP) words represent the transferable information from the labeled source domain to the unlabeled target domain for crossdomain sentiment classification. We showed a strong positive correlation of 0.78 between the SCP words identified by our approach and the sentiment classification accuracy achieved in the unlabeled target domain. Essentially, a set of less erroneous transferable features leads to a more accurate classification system in the unlabeled target domain. We have presented a technique based on χ2 test and cosine-similarity between context vector of words to identify SCP words. Results show that the SCP words given by our approach represent more accurate transferable information in comparison to the Structured Correspondence Learning (SCL) algorithm and common-unigrams. Furthermore, we show that an ensemble of the classifiers trained on the SCP features and target specific features overcomes the errors of the individual classifiers."
  }],
  "year": 2018,
  "references": [{
    "title": "A Almuhareb, A Al-Thubaity, MS Khorsheed, and A Al-Rajeh",
    "authors": ["S Al-Harbi"],
    "year": 2008
  }, {
    "title": "Lost in translation: viability of machine translation for cross language sentiment analysis",
    "authors": ["AR Balamurali", "Mitesh M Khapra", "Pushpak Bhattacharyya."],
    "venue": "Computational Linguistics and Intelligent Text Processing, pages 38–49. Springer.",
    "year": 2013
  }, {
    "title": "An iterative similarity based adaptation technique for cross-domain text classification",
    "authors": ["Himanshu S. Bhatt", "Deepali Semwal", "S. Roy."],
    "venue": "Proceedings of Conference on Natural Language Learning, pages 52–61.",
    "year": 2015
  }, {
    "title": "Multilingual Projections",
    "authors": ["Pushpak Bhattacharyya."],
    "venue": "Springer International Publishing, Cham.",
    "year": 2015
  }, {
    "title": "Identifying expressions of opinion in context",
    "authors": ["Eric Breck", "Yejin Choi", "Claire Cardie."],
    "venue": "Proceedings of International Joint Conference on Artificial Intelligence, pages 2683–2688.",
    "year": 2007
  }, {
    "title": "New avenues in opinion mining and sentiment analysis",
    "authors": ["Erik Cambria", "Bjorn Schuller", "Yunqing Xia", "Catherine Havasi."],
    "venue": "IEEE Intelligent Systems, (2):15–21.",
    "year": 2013
  }, {
    "title": "A system for multilingual sentiment learning on large data sets",
    "authors": ["Alex Cheng", "Oles Zhulyn."],
    "venue": "Proceedings of International Conference on Computational Linguistics, pages 577–592.",
    "year": 2012
  }, {
    "title": "Domain-specific sentiment analysis using contextual feature generation",
    "authors": ["Yoonjung Choi", "Youngho Kim", "Sung-Hyon Myaeng."],
    "venue": "Proceedings of the 1st international CIKM workshop on Topicsentiment analysis for mass opinion, pages 37–44.",
    "year": 2009
  }, {
    "title": "Determining the semantic orientation of terms through gloss classification",
    "authors": ["Andrea Esuli", "Fabrizio Sebastiani."],
    "venue": "Proceedings of International Conference on Information and Knowledge Management, pages 617–624.",
    "year": 2005
  }, {
    "title": "Old wine or warm beer: Target-specific sentiment analysis of adjectives",
    "authors": ["Angela Fahrni", "Manfred Klenner."],
    "venue": "Proc. of the Symposium on Affective Language in Human and Machine, AISB, pages 60–",
    "year": 2008
  }, {
    "title": "Retrofitting word vectors to semantic lexicons",
    "authors": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1411.4166.",
    "year": 2014
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520.",
    "year": 2011
  }, {
    "title": "A practical guide to support vector classification",
    "authors": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin."],
    "venue": "Technical report, Department of Computer Science, National Taiwan University.",
    "year": 2003
  }, {
    "title": "Transfer learning via multiview principal component analysis",
    "authors": ["Yang-Sheng Ji", "Jia-Jun Chen", "Gang Niu", "Lin Shang", "Xin-Yu Dai."],
    "venue": "Journal of Computer Science and Technology, 26(1):81–98.",
    "year": 2011
  }, {
    "title": "Instance weighting for domain adaptation in nlp",
    "authors": ["Jing Jiang", "ChengXiang Zhai."],
    "venue": "Proceedings of Association for Computational Linguistics, pages 264–271.",
    "year": 2007
  }, {
    "title": "Fully automatic lexicon expansion for domain-oriented sentiment analysis",
    "authors": ["Hiroshi Kanayama", "Tetsuya Nasukawa."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 355–363.",
    "year": 2006
  }, {
    "title": "A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge",
    "authors": ["Tao Li", "Yi Zhang", "Vikas Sindhwani."],
    "venue": "Proceedings of International Joint Conference on Natural Language Processing, pages 244–252.",
    "year": 2009
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "CoRR.",
    "year": 2013
  }, {
    "title": "A method based on the chi-square test for document classification",
    "authors": ["Michael Oakes", "Robert Gaaizauskas", "Helene Fowkes", "Anna Jonsson", "Vincent Wan", "Micheline Beaulieu."],
    "venue": "Proceedings of the 24th annual international ACM SIGIR confer-",
    "year": 2001
  }, {
    "title": "Cross-domain sentiment classification via spectral feature alignment",
    "authors": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen."],
    "venue": "Proceedings of International Conference on World Wide Web, pages 751–760.",
    "year": 2010
  }, {
    "title": "Opinion mining and sentiment analysis",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Foundations and Trends in Information Retrieval, 2(1-2):1–135.",
    "year": 2008
  }, {
    "title": "Thumbs up?: sentiment classification using machine learning techniques",
    "authors": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 79–86.",
    "year": 2002
  }, {
    "title": "Scikit-learn: Machine learning in python",
    "authors": ["Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"],
    "venue": "Journal of Machine",
    "year": 2011
  }, {
    "title": "Sentiment analysis: A combined approach",
    "authors": ["Rudy Prabowo", "Mike Thelwall."],
    "venue": "Journal of Informetrics, 3(2):143–157.",
    "year": 2009
  }, {
    "title": "Expanding domain sentiment lexicon through double propagation",
    "authors": ["Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen."],
    "venue": "IJCAI, volume 9, pages 1199– 1204.",
    "year": 2009
  }, {
    "title": "A generic approach to generate opinion lists of phrases for opinion mining applications",
    "authors": ["Sven Rill", "Jörg Scheidt", "Johannes Drescher", "Oliver Schütz", "Dirk Reinel", "Florian Wogenstein."],
    "venue": "Proceedings of the First International Workshop on Is-",
    "year": 2012
  }, {
    "title": "word2vec parameter learning explained",
    "authors": ["Xin Rong."],
    "venue": "arXiv preprint arXiv:1411.2738.",
    "year": 2014
  }, {
    "title": "Semeval-2014 task 9: Sentiment analysis in twitter",
    "authors": ["Sara Rosenthal", "Preslav Nakov", "Alan Ritter", "Veselin Stoyanov."],
    "venue": "Proceedings of SemEval, pages 73–80.",
    "year": 2014
  }, {
    "title": "Active supervised domain adaptation",
    "authors": ["Avishek Saha", "Piyush Rai", "Hal Daumé III", "Suresh Venkatasubramanian", "Scott L DuVall."],
    "venue": "Machine Learning and Knowledge Discovery in Databases, pages 97–112.",
    "year": 2011
  }, {
    "title": "Detecting domain dedicated polar words",
    "authors": ["Raksha Sharma", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of the International Joint Conference on Natural Language Processing, pages 661–666.",
    "year": 2013
  }, {
    "title": "Domain sentiment matters: A two stage sentiment analyzer",
    "authors": ["Raksha Sharma", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of the International Conference on Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Adjective intensity and sentiment analysis",
    "authors": ["Raksha Sharma", "Mohit Gupta", "Astha Agarwal", "Pushpak Bhattacharyya."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Lexicon-based methods for sentiment analysis",
    "authors": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede."],
    "venue": "Computational Linguistics, 37(2):267–307.",
    "year": 2011
  }, {
    "title": "Support vector machine active learning with applications to text classification",
    "authors": ["Simon Tong", "Daphne Koller."],
    "venue": "The Journal of Machine Learning Research, 2:45–66.",
    "year": 2002
  }, {
    "title": "Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews",
    "authors": ["Peter D. Turney."],
    "venue": "Proceedings of Association for Computational Linguistics, pages 417–424.",
    "year": 2002
  }, {
    "title": "Feature ensemble plus sample selection: domain adaptation for sentiment classification",
    "authors": ["Rui Xia", "Chengqing Zong", "Xuelei Hu", "Erik Cambria."],
    "venue": "IEEE Intelligent Systems, 28(3):10–18.",
    "year": 2013
  }, {
    "title": "Feature selection for text categorization on imbalanced data",
    "authors": ["Zhaohui Zheng", "Xiaoyun Wu", "Rohini Srihari."],
    "venue": "ACM Sig KDD Explorations Newsletter, 6(1):80–89.",
    "year": 2004
  }, {
    "title": "Hybrid heterogeneous transfer learning through deep learning",
    "authors": ["Joey Tianyi Zhou", "Sinno Jialin Pan", "Ivor W Tsang", "Yan Yan."],
    "venue": "AAAI, pages 2213–2220.",
    "year": 2014
  }],
  "id": "SP:d60064aa94b9ae004d4ea0361e2f425beca12b09",
  "authors": [{
    "name": "Raksha Sharma",
    "affiliations": []
  }, {
    "name": "Pushpak Bhattacharyya",
    "affiliations": []
  }, {
    "name": "Sandipan Dandapat",
    "affiliations": []
  }, {
    "name": "Himanshu Sharad Bhatt",
    "affiliations": []
  }],
  "abstractText": "Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, crossdomain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.",
  "title": "Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification"
}