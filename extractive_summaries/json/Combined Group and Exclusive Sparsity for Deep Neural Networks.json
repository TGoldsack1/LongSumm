{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Deep neural networks have shown tremendous success in recent years, achieving near-human performances on tasks such as visual recognition (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016). One of the key factors in this success of deep network is its expressive power, which is made possible by multiple layers of non-linear transformations. However, this expressive power comes at a cost: increased number of parameters. Due to large\n1UNIST, Ulsan, South Korea 2AITrics, Seoul, South Korea. Correspondence to: Sung Ju Hwang <sjhwang@unist.ac.kr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nnumber of parameters, deep networks require large amount of memory and computation power to train. Further, large number of parameters also mean that the model is highly susceptible to overfitting as well, if trained with insufficient data. To resolve such issues, researchers have sought ways to make the model more compact and lightweight by parameter reduction, via model compression (Ba & Caruana, 2014; Hinton et al., 2014), or removing unnecessary weights either by pruning (Reed, 1993; Han et al., 2015) and `1-regularization (Collins & Kohli, 2014). However, one of the main problems of these methods is that they often achieve such efficiency at the expense of accuracy.\nHow can we then obtain a compact deep network without sacrificing the prediction accuracy? One way to achieve this goal is better utilizing the capacity of the network, by reducing redundancies in the model parameters. In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set. However, since this is a difficult constraint to satisfy, in practice, such constraint is given only at the initialization stage (Saxe et al., 2014), or enforced implicitly through regularizations such as dropout (Srivastava et al., 2014) that prevents feature co-adaption. Contrary to these existing approaches, we propose to impose an explicit regularization to reduce redundancies. Our idea is to enforce network weights at each layer to fit to different sets of input features as much as possible. This exclusive feature learning is implemented by the exclusive sparsity regularization based on (1, 2)-norm (Zhou et al., 2010; Kong et al., 2014), which basically promotes network weights at each layer to compete for few meaningful features from the lower layer.\nHowever, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared. For example, if the lowerlayer feature is a wheel, and the upper layer weights are features describing car and bicycle respectively, then the two upper layer weights should share the common feature that describes the wheel. Thus, we also allow for sharing of some important features, by introducing an additional group sparsity regularizer based on (2, 1)-norm and combine the two regularization terms, balancing their effect at each layer of the network to adjust the degree of feature sharing and competition.\nOur combined regularizer can be applied to all layers of a generic deep neural network, including plain fullyconnected feedforward networks and convolutional networks. We validate our regularized network on four public datasets with different base networks, on which it achieves a compact, lighter model while achieving superior performance over networks trained with other sparsity-inducing regularizers, sometimes obtaining even better accuracy than the full model. As an example, on CIFAR-10 dataset, our network obtains 2.17% accuracy improvements while using 13.72% less number of parameters and 35.67% less floating point operations. Further empirical analysis shows that exclusive sparsity helps the network to converge faster to a given error rate, and learn less redundant features."
  }, {
    "heading": "2. Related Work",
    "text": "Sparsity for deep neural networks Obtaining compact deep networks by removing unnecessary weights, is a longstudied topic in deep learning research. The most simplest yet popular weight removal method is to prune out weak weights by simple thresholding (Reed, 1993; Han et al., 2015). Another way to induce sparsity on weights is by `1-regularization (Tibshirani, 1994). Collins & Kohli (2014) applied the `1-regularization to convolutional neural networks, demonstrating that it can obtain a compact, memory-efficient network at the expense of small reduction in the prediction accuracy. Few recent work applied group sparsity (Yuan & Lin, 2006) regularization to deep networks, as it has a number of nice properties. By removing an entire feature group, group sparsity can automatically decide the number of neurons (Alvarez & Salzmann, 2016). Further, if applied between the weights at different layers, it can also be used to decide optimal number of layers to use for the given network (Wen et al., 2016). In terms of efficiency, structured sparsity using (2, 1)-norm exhibits better data locality than the regular sparsity, and results in larger speedups (Wen et al., 2016; Alvarez & Salzmann, 2016). We also employ the group sparsity in our combined regularizer, but we mainly group the features across multiple filters, to promote feature sharing among the filters. While all the previously introduced models do help reduce number of parameters and result in certain amount of speedups, such memory and time efficiency is mostly obtained at the expense of reduced accuracy. Our combined group and exclusive sparsity regularization, on the other hand, do not degenerate performance, since its aim in learning sparse weights/features is in removing redundancy to better utilize the network capacity.\nExclusive feature learning There exists quite a number of work on imposing exclusivity among the learned model parameters/features. One popular way is to enforce orthogonality, as this will minimize the dependency and redun-\ndancy among the variables that are being regularized. Orthogonality at initialization stage has been much studied in the deep learning context (Saxe et al., 2014), as in such a non-convex optimization setting this can lead to convergence to a better local optimum. Zhou et al. (2011) enforced orthogonality via explicit dot product regularization to make the parameters for parent-level and childlevel classifiers in a hierarchical classifier to be orthogonal. However, the orthogonal regularizer is non-convex and does not scale well, since it scales quadratically to the number of participating vectors. Another way to enforce exclusivity is through (1, 2)-norm, which is basically the 2-norm over 1-norm groups, that results in promoting sparsity across different vectors. The (1, 2)-norm is first proposed in (Zhou et al., 2010), where it is used to promote competitions among the models jointly learned in a multitask learning framework. A similar regularizer was used in (Hwang et al., 2011) in a metric learning setting, with an additional `1-regularization that helps learn discriminative features for each metric. Kong et al. (2014) generalized the (1, 2)-norm to be used with arbitrary objective and handle overlapping groups. In deep learning context, Goo et al. (2016) proposed a difference pooling technique that has a similar motivation to exclusive lasso, which subtracts the common superclass level feature map from the classspecific feature maps to learn class-exclusive features for fine-grained classification. In all existing models, exclusivity is applied only at the class-level, and application of the exclusivity regularization to weights at any layers of deep networks through (1, 2)-norm, has not yet been explored. Further, our regularizer is a combined term of both group and exclusive lasso which allows sharing of important features while making each weight to be as different as possible, rather than purely exclusive feature learning that is impractical. The regularizer proposed in (Kim & Xing, 2010) is similar to ours, which proposes a weighted (2, 1)-norm that has a similar effect of varying the degree of competition and grouping, although our regularizer is more explicit in its effect and optimization."
  }, {
    "heading": "3. Approach",
    "text": "Our main objective is to implement a sparse deep neural network with significantly less number of parameters than what the original non-sparse network has, which at the same time obtains comparable or even better performance to the original model. The training objective for a generic (deep) neural network for classification1 is given as follows:\nmin {W (l)}\nL({W (l)},D) + λ L∑\nl=1\nΩ(W (l)) (1)\n1While our method is generic and can be also applied to regression, we only consider the classification task for simplicity.\nHere, D = {xi,yi}Ni=1 is a training dataset with N instances where xi ∈ Rd is a d-dimensional input feature and yi ∈ {1, . . . ,K} is its class label which is one of the K classes, {W (l)} is the set of weights across all layers, L(W ) is the loss parameterized by W , L is the total number of layers, W (l) is the weight matrix (or tensor) for layer l, Ω(W (l)) is some regularization term on the network weights at layer l, and λ is the regularization parameter that balances the loss with the regularization.\nThe usual and the most often used regularization term is the 2-norm: Ω(W (l)) = ‖W (l)‖22, which is also called as the `2-regularizer. The regularization has an effect of adding a bias term to reduce variance of the model, which in turn results in a lower generalization error.\nHowever, since our goal is in obtaining a sparse model where large portion of W (l) is zeroed out, we want Ω(W (l)) to be a sparsity-inducing regularizer. The most common regularizer for promoting sparsity is the 1-norm:\nΩ(W (l)) = ‖W (l)‖1 (2)\nThis 1-norm regularization results in obtaining a sparse weight matrix, since it requires the solution to be found at the corner of the 1-norm ball, thus eliminating unnecessary elements. The element-wise sparsity can be helpful when most of the features are irrelevant to the learning objective, as in the data-driven approaches. However, as aforementioned, when applied to a deep network it usually results in slight accuracy reduction. Further, element-wise sparsity, while achieving a memory-efficient model, usually do not result in meaningful speedups in practical network architectures such as CNNs, since the bottleneck is in the convolutional operations that do not reduce much when the number of filters stays the same (Wen et al., 2016).\nGroup sparsity, on the other hand, can help reduce the intrinsic complexity of the model by eliminating a neuron or a convolutional filter as a whole, and thus can help obtain practical speedups in deep neural networks (Wen et al., 2016; Alvarez & Salzmann, 2016). The group sparsity regularization is defined as follows:\nΩ(W (l)) = ∑ g ‖W (l)g ‖2 = ∑ g √∑ i w (l) g,i 2\n(3)\nwhere g ∈ G is a weight group, W (l)g is the weight matrix (or a vector) for group g that is defined on W (l), and wg,i is a weight at index i, for group g. Since `2-norm has the grouping effect that results in similar weights for correlated features, this will result in complete elimination of some groups, thus removing some input neurons (See Figure 3, (a)). This has an effect of automatically deciding how many neurons to use at each layer.\nStill, this group sparsity does not maximally utilize the capacity of the network since there still could be redundancy among the features that are selected. Thus, we propose to apply a sparsity-inducing regularization that obtains a sparse network weight matrix, while also minimizing the redundancy among network weights for better utilization of the network capacity."
  }, {
    "heading": "3.1. Exclusive Sparsity Regularization for Deep Neural Networks",
    "text": "Exclusive sparsity, or exclusive lasso was first introduced in (Zhou et al., 2010) in multi-task learning context. The main idea in the work is to enforce the model parameters for different tasks to compete for features, instead of sharing features as suggested by previous work on multi-task learning that leverages group lasso. When the task is a classification task, this makes sense since the objective is to differentiate between classes which can be achieved by identifying discriminative feature for each class.\nThe generic exclusive sparsity regularization is defined as follows:\nΩ(W (l)) = 1\n2 ∑ g ‖W (l)g ‖21 = 1 2 ∑ g (∑ i |w(l)g,i| )2 (4)\nwhere w(l)g,i is the ith instance of the submatrix (or the vector) W (l)g . This norm is often called as (1, 2)-norm, and is basically the 2-norm over 1-norm groups. The sparsity is now enforced within each group, as opposed to the group sparsity regularizer which promotes inter-group sparsity. Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight. In (Zhou et al., 2010), Wg is defined to be the model parameter for multiple tasks on the same feature, in which case the (1,2)-norm enforces each task predictor to fit to\nfew features that are most useful for it. Exclusive sparsity can be straightforwardly applied to fully connected layers of a deep network, by grouping network weights from the same neuron at each layer into one group and applying (1, 2)-norm on these groups (See Figure 1(b)). This will enforce each output neuron to compete for input neurons, which will result in learning largely disparate network weights at each layer.\nExclusive sparsity on convolutional filters For convolutional layers of a convolutional neural network, exclusive sparsity can be applied in the same manner as in fully connected layers, where we apply Eq. 4 on the convolutional filters, while defining each group g as the same feature across multiple convolutional filters. Figure 2(c) illustrates the feature groups and effect of exclusive sparsity on the convolutional filters. This will enforce the convolutional filters to be as different as possible from each other, removing any redundancies between them."
  }, {
    "heading": "3.2. Combined Group & Exclusive Sparsity Regularization",
    "text": "As mentioned earlier, our main intuition is that there are varying degree of sharing and exclusivity among different features. Exclusivity alone cannot result in learning an optimal set of features, since some features need to be shared across multiple higher-level features. Thus we need to allow for some degree of sharing across the features, while still making each weight to be sufficiently different in order for each feature to be meaningful. How can we then come up with a regularizer that can achieve the two seemingly conflicting goals?\nIn tree guided group lasso (Kim & Xing, 2010), each pair of weights are given different degree of sharing and competition based on the similarity between the tasks given by a taxonomy, which can be either semantically defined or obtained through clustering, through a regularization similar to an elastic-net formulation. While this model can be\napplied at the final softmax layer, on the softmax weight for each class, such taxonomy does not exist for the intermediate level network weights, and it is also not efficient to obtain them through clustering or other means.\nThus we propose to simply combine the group sparsity and the exclusive sparsity together, which will result in a similar effect, where network weights exhibit certain degree of sharing if they are correlated, but are learned to be different on other parts that are not shared. Our combined group and exclusive lasso regularizer is given as follows:\nΩ(W (l)) = ∑ g ( (1− µl)‖W (l)g ‖2 + µl 2 ‖W (l)g ‖21 ) (5)\nwhere λ is the parameter that decides the entire regularization effect, W l is the weight matrix for lth layer, and µl is the parameter for balancing the sharing and competition term at each layer.\nThen how should we set the balancing term µl at each layer? One simple solution is to set all µl to be a single constant, but a better way is to set them differently at each layer, based on the degree of sharing and competition required at each layer. At lower layers, features will be quite generic and might need to be shared across all high-level neurons for accurate expression of the input data, wheareas at the top layer softmax weights, it would be better to have the weights to select features as disjoint as possible for better discriminativity. Thus, we set µl = m+ (1− 2m) lL−1 , to reflect such intuition, where L is a total number of all layers, l ∈ {0, ..., L − 1} is an index of each layer, and 0 ≤ m ≤ 1 is the lowest parameter value for the exclusive sparsity term. If m = 0, the regularizer reduces to (2, 1)- norm regularizer with µ1 = 0 at the lowest layer, while at the topmost softmax layer, the regularizer is an (1, 2)-norm regularizer µ1 = 1."
  }, {
    "heading": "3.3. Numerical Optimization",
    "text": "Our regularized learning objective can be solved using proximal gradient descent, which is often used for optimizing objectives formed as a combination of both smooth\nand non-smooth terms. The proximal gradient algorithm for regularized objective first obtains the intermediate solution Wt+ 12 by taking a gradient step using the gradient computed on the loss only, and then optimize for the regularization term while performing Euclidean projection of it to the solution space, as in the following formulation:\nmin Wt+1\nΩ(Wt+1) + 1\n2λs ‖Wt+1 −Wt+ 12 ‖ 2 2 (6)\nwhere Wt+1 is the variable to obtain after the current iteration, λ is the regularization parameter, and s is the step size. When Ω(Wt+1) is a group sparsity regularizer or an exclusive sparsity regularizer, the above problem has a closedform solution.\nThe solution, or the proximal operator for the group sparsity regularizer, proxGL(W ) is given as follows:\nproxGL(W) = ( 1− λ ‖wg‖2 ) + wg,i (7)\nfor all g and i, where g is each group, and i is an element of in each group. The proximal operator for the exclusive sparsity regularizer, proxEL(W), is obtained as follows:\nproxEL(W) =\n( 1− λ‖wg‖1\n|wg,i| ) + wg,i\n= sign(wg,i)( ∣∣wg,i∣∣− λ‖wg‖1)+ (8)\nfor all g and i. The combined regularizer can be optimized simply by applying the two proximal operators in a row at each gradient step, after updating the variable with the lossbased gradient. Algorithm 1 describes the proximal gradient algorithm for optimizing our regularized objective.\nAlgorithm 1 Stochastic Proximal Gradient Algorithm for Combined (2,1)- and (1,2)- regularization Input: W, λ, µ, mini-batch size b, learning rate η\nInitialize W, t while Some predefined stopping criterion is satisfied do\nRandomly select b samples from p ∈ {1, 2, ..., n}, for each layer l, do\nW (l)\nt+ 1 2\n:= W(l)t − ηstb ∑ p5fp(W (l) t ) . Update the\nparameter with the gradient of a non-regularized objective W (l)\nt+ 1 2 ,GL\n= proxGL(W (l)\nt+ 1 2\n) . Apply proxGL in Eq. 7\nW (l) t+1 = proxEL(W (l)\nt+ 1 2 ,GL\n) . Apply proxEL in Eq. 8\nend for end while"
  }, {
    "heading": "4. Experiment",
    "text": "We perform all experiments with convolutional neural network as the base network model. The regularization is ap-\nplied at the network weights for all layers, excluding the bias term. All models are implemented and experimented using Tensorflow (Abadi et al., 2016) framework 2.\nBaselines and our models We compare our regularized networks against relevant baselines.\n1) `2. The network trained with `2-regularization.\n2) `1. The network trained with `1-regularization, which has elementwise sparse network weights.\n3) Group Sparsity-Filter. The network regularized with `2,1-norm on the weights, which groups each convolutional filter as a group at convolutional layers. This network is an implementation of the model in (Wen et al., 2016).\n4) Group Sparsity-Feature. The network that uses the same `2,1-regularization as in 3), but with each group defined as the same feature at different filters.\n5) Exclusive Sparsity. This is the network whose weights at each layer are regularized with `1,2-norm only.\n6) Combined Group and Exclusive Sparsity. The network regularized with our combined structured sparsity on the weights. The combination weight that balances both regularizations are dynamically set at each layer.\nDatasets and base networks We validate our method on four public datasets for classification, with four different convolutional networks.\n1) MNIST. This dataset contains 70, 000 28×28 grayscale images of handwritten digits for training example images, where there is 6, 000 training instances and 1, 000 test instances per class. As for the base network, we use a simple convolutional neural network with two convolutional layers and two fully connected layers.\n2) CIFAR-10. This dataset consists of 60, 000 images sized 32 × 32, from ten animal and vehicle classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). For each class, there are 5, 000 images for training and 1, 000 images for test. For the base network, we use LeNet (Lecun et al., 1998), that has two convolutional layers followed by three fully connected layers.\n3) CIFAR-100. This dataset also consists of 60, 000 images of 32× 32 pixels as in CIFAR-10, but has 100 generic object classes instead of 10. For each class, 500 images are used for training and 100 images are used for test. For the base network, we use a variant of Wide Residual Network (Zagoruyko & Komodakis, 2016), which has 16 layers with the widening factor of k = 10.\n4) ImageNet-1K. This is the dataset for 2012 ImageNet 2Codes available at https://github.com/jaehong-yoon93/CGES\nLarge Scale Visual Recognition Challange (Deng et al., 2009) that consists of 1, 281, 167 images from 1, 000 generic object categories. For evaluation, we used the validation set that consists of 50, 000 images, following the standard procedure. For the base network, we used an implementation of AlexNet (Krizhevsky et al., 2012).\nFor MNIST and CIFAR-10 experiment, we train all networks from the scratch; for CIFAR-100, and ImageNet1K experiment where we use larger networks (WRN and AlexNet) we fine-tune the network from the `2- regularized networks, since training them from scratch takes prohibitively long time."
  }, {
    "heading": "4.1. Quantitative analysis",
    "text": "We first validate whether our sparsity-inducing regularizations result in better accuracy-efficiency trade-off compared to baseline methods, by measuring the prediction accuracy over number of parameters, and number of floating point operations (FLOP) for each method.\nFigure 3 shows the prediction accuracy of the different models over number of parameters/FLOP, obtained by differentiating the sparsity-inducing regularization parameter for each method. As expected, `1-regularization greatly reduces the number of parameters, while maintaining a similar performance to the original model. The group sparsity regularization in general performs worse than `1, but\nachieves better accuracy in certain sparsity ranges. The exclusive sparsity improves the performance over the base `2- regularization model in low-sparsity range which is especially well shown in CIFAR-10 result, but degenerates performance as the sparsity increases. We attribute this to the fact that exclusive sparsity aims to make each weight/filter to fit to completely disjoint sets of low-level features, which is unrealistic as features may need to fit to the same set of low-level features for accurate representation.\nFinally, our combined group and exclusive sparsity, which allows for certain degree of sharing between the weights/features while enforcing exclusivity, achieves the best accuracy/parameter trade-off, achieving similar or better performance gain to the exclusive sparsity while also greatly reducing the number of parameters. Fig 3(a) shows the results on the MNIST dataset, on which our CGES obtains no accuracy reduction, using 36.48% less number of parameters and 14.46% less computation. On CIFAR-10 dataset, CGES improves the classification accuracy over the `2 baselines by 2.17%, using 13.72% less number of parameters using 35.67% less FLOP. CGES obtains slight accuracy reduction of 1.15% on CIFAR-100 dataset, using only 51.22% of parameters and 42.77% less FLOP.\nOn ImageNet (Table 1), CGES obtains similar or slightly worse performance to the full network while using 60% − 68% of its parameters, while `1 shows noticeable performance degeneration at the same sparsity level.\nIterative pruning Iterative pruning (Han et al., 2015) is another effective method for obtaining a sparse network while maintaining high accuracy. As iterative pruning is orthogonal to our method, we can couple the two methods to obtain even better performance per number of parameters used; specifically, we replace the usual weight decay regularizer used in (Han et al., 2015) with our CGES regularizer. We report the accuracy of this combined model on MNIST and CIFAR-10 dataset, when using 10% of the parameters of the full network (Table 2). The results show that CGES coupled with iterative pruning obtains similar or even better results to the original model using only a fraction of the parameters, significantly outperforming the base pruning model which suffers substantial accuracy loss.\nConvergence speed We further analyze the empirical convergence rate of our regularized network, since it will be impractical if the regularized network requires much longer iterations to reach the same accuracy. Interestingly, we empirically found that our exclusive sparsity regularizer also helps network achieve the same error using much fewer iterations (Figure 4(a)), compared to base `2- regularization. This faster convergence agrees with the observations in (Saxe et al., 2014), where networks whose weights are initialized as random orthgonal matrices con-\nverged faster than networks with random Gaussian initialization.\nConvolutional vs. fully connected layers To see how much effect our combined regularizer has on different types of layers, we experiment applying the model only to the fully connected layer, or convolutional layers, while applying usual `2-regularizer to other layers. Figure 4(b) shows the result of this experiment, where we plot the accuracy over percentage of parameters used, for models that applies ES only to fully connected layers, only to convolutional layers, and both. We observe improvements on all models, which shows the effectiveness of the exclusive sparsity regularizer to all types of network weights. Further, ES results in larger improvements on convolutional layers, which makes sense since lower-layer features are more important as they are more generic across different classes, than the features learned at fully connected layers. However, conv layers obtained the best accuracy at low-sparsity range, since strict enforcement of exclusivity hurts the representational power of the features, whereas FC layers obtained improvements even on high-sparsity range; this may be because loss of expressiveness could be compensated by better discriminativity of the features at high level.\nSharing vs. Competing for Features We further explore how varying the degree of sharing and competition affect the accuracy and efficiency of the model, by experimenting with different configurations of µl in Eq. 5 at each layer. We report the results in Figure 4(c). Specifically, we test two different approaches to balance the degree of sharing and competition at each layer. The first model, ES-Increasing, is the actual combination we have used in our method which increases the effect of exclusive sparsity with increasing l. This model reflects our intuition that competition will help at high layers, while sharing will help more at lower layers. The second model, ES-Constant combines the two terms with µl = 0.5 throughout all layers. We observe that ES-Increasing works better than ES-\nConstant across all sparsity ranges, which shows that our scheme of increasing exclusivity at higher layers indeed helps improve the model performance."
  }, {
    "heading": "4.2. Qualitative analysis",
    "text": "For further qualitative analysis, we visualize the weights and convolutional filters obtained using the baselines and our methods.\nFigure 5 visualizes the weights of the softmax layer for different regularization methods, from the network trained on the CIFAR-10 dataset. Each row is the softmax parameter for each class. `2 and `1 work as expected, resulting in non-sparse and elementwise sparse weights. The group sparsity regularizer results in the total elimination of certain features that are not shared across multiple classes. The exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class. However, when combined with the group lasso, it allows certain degree of feature reuse, while still obtaining parameters that are largely disparate across classes.\nTo show that such effect is not confined to the fully connected layer, we also visualize the convolutional filters in the first convolutional layer of the network trained on the CIFAR-10 dataset, in Figure 6. We observe that the combined group and exclusive sparsity regularizer results in filters that are much sharper than the ones that are obtained by `1 or group sparsity regularization, with some spatial features dropped altogether from the competition with other filters. Further, there is less redundancy among the filters,\nunlike the filters learned by other regularization methods. Note that we set the exclusivity factor µ1 = 0.8 just for visualization purpose, since our weighting scheme will set µ1 as a low value in the first convolutional layer."
  }, {
    "heading": "5. Conclusion",
    "text": "In this work, we proposed a novel regularizer for generic deep neural networks that effectively utilizes the capacity of the network, by exploiting the sharing and competing relationships among different network weights. Specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm on the network weights, along with group sparsity regularization using (2, 1)-norm, such that exclusive sparsity enforces the network weights to use input neurons that are as different as possible from the other weights, while the group sparsity allows for some degree of sharing among them, as it is impossible to make the network weights to fit to completely disjoint set of features. We validate our method on some public datasets for both the accuracy and efficiency against other sparsity-inducing regularizers, and the results show that our combined regularizer helps obtain even better performance than the original full network, while significantly reducing the memory and computation requirements.\nAcknowledgements This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF2016M3C4A7952634), and UAV-technology development\nprogram through the National Research Foundation of Korea (NRF) funded by the Korea Aerospace Research Institute (NRF-2016M1B3A1A01937742)."
  }],
  "year": 2017,
  "references": [{
    "title": "Tensorflow: Large-scale Machine",
    "authors": ["Abadi", "Martı́n", "Agarwal", "Ashish", "Barham", "Paul", "Brevdo", "Eugene", "Chen", "Zhifeng", "Citro", "Craig", "Corrado", "Greg S", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu"],
    "venue": "Learning on Heterogeneous Distributed Systems",
    "year": 2016
  }, {
    "title": "Learning the number of neurons in deep networks",
    "authors": ["Alvarez", "Jose M", "Salzmann", "Mathieu"],
    "venue": "In NIPS",
    "year": 2016
  }, {
    "title": "Do deep nets really need to be deep",
    "authors": ["Ba", "Jimmy", "Caruana", "Rich"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Memory bounded deep convolutional networks",
    "authors": ["Collins", "Maxwell D", "Kohli", "Pushmeet"],
    "venue": "arXiv preprint arXiv:1412.1442,",
    "year": 2014
  }, {
    "title": "Fei-F ̇ Imagenet: A Large-Scale Hierarchical Image Database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. ei"],
    "venue": "In CVPR,",
    "year": 2009
  }, {
    "title": "Taxonomy-Regularized Semantic Deep Convolutional Neural Networks",
    "authors": ["Goo", "Wonjoon", "Kim", "Juyong", "Gunhee", "Hwang", "Sung Ju"],
    "venue": "In ECCV,",
    "year": 2016
  }, {
    "title": "Learning both weights and connections for efficient neural network",
    "authors": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"],
    "venue": "In NIPS",
    "year": 2015
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"],
    "year": 2016
  }, {
    "title": "Distilling the knowledge in a neural network",
    "authors": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"],
    "venue": "In NIPS 2014 Deep Learning Workshop,",
    "year": 2014
  }, {
    "title": "Learning a tree of metrics with disjoint visual features",
    "authors": ["Hwang", "Sung Ju", "Grauman", "Kristen", "Sha", "Fei"],
    "venue": "In NIPS,",
    "year": 2011
  }, {
    "title": "Tree-guided group lasso for multitask regression with structured sparsity",
    "authors": ["S. Kim", "E.P. Xing"],
    "venue": "In ICML, pp",
    "year": 2010
  }, {
    "title": "Exclusive feature learning on arbitrary structures via `1, 2-norm",
    "authors": ["Kong", "Deguang", "Fujimaki", "Ryohei", "Liu", "Ji", "Nie", "Feiping", "Ding", "Chris"],
    "venue": "In NIPS",
    "year": 2014
  }, {
    "title": "Gradientbased Learning Applied to Document Recognition",
    "authors": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Pruning algorithms-a survey",
    "authors": ["R. Reed"],
    "venue": "IEEE Transactions on Neural Networks,",
    "year": 1993
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Going Deeper with Convolutions",
    "authors": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"],
    "venue": "In CVPR,",
    "year": 2015
  }, {
    "title": "Regression shrinkage and selection via the lasso",
    "authors": ["R. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society, Series B,",
    "year": 1994
  }, {
    "title": "Learning structured sparsity in deep neural networks",
    "authors": ["Wen", "Wei", "Wu", "Chunpeng", "Wang", "Yandan", "Chen", "Yiran", "Li", "Hai"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Model selection and estimation in regression with grouped variables",
    "authors": ["Yuan", "Ming", "Lin", "Yi"],
    "venue": "Journal of the Royal Statistical Society, Series B,",
    "year": 2006
  }, {
    "title": "Wide residual networks",
    "authors": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"],
    "venue": "In BMVC,",
    "year": 2016
  }, {
    "title": "Hierarchical Classification via Orthogonal Transfer",
    "authors": ["D. Zhou", "L. Xiao", "M. Wu"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Exclusive lasso for multi-task feature selection",
    "authors": ["Zhou", "Yang", "Jin", "Rong", "Hoi", "Steven C. H"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }],
  "id": "SP:2cd4cf879366499118752a0ed7bb19f3efbcf90a",
  "authors": [{
    "name": "Jaehong Yoon",
    "affiliations": []
  }, {
    "name": "Sung Ju Hwang",
    "affiliations": []
  }],
  "abstractText": "The number of parameters in a deep neural network is usually very large, which helps with its learning capacity but also hinders its scalability and practicality due to memory/time inefficiency and overfitting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the network. Specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm, which promotes competition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sparsity with the group sparsity based on (2, 1)-norm, to promote both sharing and competition for features in training of a deep neural network. We validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy.",
  "title": "Combined Group and Exclusive Sparsity for Deep Neural Networks"
}