{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics\nIn this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1"
  }, {
    "heading": "1 Introduction",
    "text": "Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules. There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by\n1Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list\nthe following words. An example is shown below that includes edited words and filler words.\nI want a flight to Boston︸ ︷︷ ︸ edited uh I mean︸ ︷︷ ︸ filler to Denver\nAutomatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection.\nMany models have been evaluated for this task. Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection. They showed that CRFs significantly outperformed Maximum Entropy models and HMMs. Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001). Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking. They obtained the best reported F-score of 83.8% on the Switchboard corpus. Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.\nFrom the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006). Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage. They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.\n820\nIn this paper, we propose a multi-step stacked learning approach for disfluency detection. In our method, we first perform filler word detection, then edited word detection. In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step. We also include a new type of features, called inbetween features, and incorporate them into the last step. For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance. On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection."
  }, {
    "heading": "2 Balancing Precision and Recall Using",
    "text": "Weighted M3Ns\nWe use a sequence labeling model for edit detection. Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other). For example, the previous sentence is represented as:\nI/O want/O a/O flight/O to/BE Boston/EE uh/O I/O mean/O to/O Denver/O\nWe use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:\nP = #correctly predicted edited words\n#predicted edited words\nR = #correctly predicted edited words\n#gold standard edited words\nF = 2× P ×R\nP + R\nThere are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006). Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.\nIn this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary\nresults showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).\nThe learning task for M3Ns can be represented as follows:\nmin α\n1 2 C∥ ∑ x,y αx,y∆f(x, y)∥22 + ∑ x,y αx,yL(x, y)\ns.t. ∑\ny\nαx,y = 1 ∀x\nαx,y ≥ 0, ∀x, y\nThe above shows the dual form for training M3Ns, where x is the observation of a training sample, y ∈ Y is a label. α is the parameter needed to be optimized, C > 0 is the regularization parameter. ∆f(x, y) is the residual feature vector: f(x, ỹ) − f(x, y), where ỹ is the true label of x. L(x, y) is the loss function. Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) = ∑ t δ(yt, ỹt), where δ(a, b) is the binary indicator function (it is 0 if a = b). In our work, we use the weighted hamming loss:\nL(x, y) = ∑\nt\nv(yt, ỹt)δ(yt, ỹt)\nwhere v(yt, ỹt) is the weighted loss for the error when ỹt is mislabeled as yt. Such a weighted loss function allows us to balance the model’s precision and recall rates. For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate. In our work, we tune the weight matrix v using the development dataset."
  }, {
    "heading": "3 Multi-step Stacked Learning for Edit Disfluency Detection",
    "text": "Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features. Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005). Its idea is to use two\n(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors. It has the advantage of incorporating non-local features as well as nonlinear classifiers. In our task, we do not just use the classifier’s output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers. We use 10 fold cross validation to train the low level predictors. The following describes the three steps in our approach."
  }, {
    "heading": "3.1 Step 1: Filler Word Detection",
    "text": "In the first step, we automatically detect filler words. Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy. For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency. Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.\nFor filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O. We use un-weighted hamming loss to learn M3Ns for this task. Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this. The features we used are listed in Table 1. All n-grams are extracted from the original text."
  }, {
    "heading": "3.2 Step 2: Edited Word Detection",
    "text": "In the second step, edited words are detected using M3Ns with the weighted-hamming loss. The features we used are listed in Table 2. All n-grams in the first step are also used here. Besides that, word n-grams, POS n-grams and logic n-grams extracted from filler word removed text are included. Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words."
  }, {
    "heading": "3.3 Step 3: Refined Edited Word Detection",
    "text": "In this step, we use n-gram features extracted from the text after removing edit disfluencies based on\nthe previous step. According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural. The following is an example:\n• Ref: The new type is prettier than what their/SE they used to look like.\n• Sys: The new type is prettier than what/BE their/EE they used to look like.\nUsing the system’s prediction, we would have bigram than they, which is odd. Usually, the pronoun following than is accusative case. We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis. This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,\n2011). They have the benefit of measuring the fluency of the cleaned text.\nAnother common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies. For example,\n• Coordinates: they ca n′t decide which are the good aspects and which are the bad aspects\n• Rough Copies: it/BE ′s/IE a/IE pleasure/IE to/EE it s good to get outside\nTo distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically. We extract all the pieces lying between identical word bigrams AB . . . AB. The observation is that coordinates are often longer than edited sequences. Hence we introduce the in-between features for each word. If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity). We also used other patterns such as A . . . A and ABC . . . ABC, but they are too noisy or infrequent and do not yield much performance gain.\nTable 3 lists the feature templates used in this last step."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Experimental Setup",
    "text": "We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004). We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and\nsuch kind of information is not available (Johnson and Charniak, 2004).\nWe tuned the weight matrix for hamming loss on the development dataset using simple grid search. The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time. The optimal weight matrix is shown in Table 4. Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ·E will be mapped to the class of edited words."
  }, {
    "heading": "4.2 Results",
    "text": "We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning. For each model, we tuned the parameters on the development data: Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1. For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training. Regularization penalty is C = 0.1 and iteration number is 30.\nTable 5 shows the results using different models and features. The baseline models use only the ngrams features extracted from the original text. We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models. Regarding the features, the gradually added n-gram features have consistent improvement for all models. Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words. In our analysis, we also noticed that the in-between fea-\ntures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words). We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001). There are no significant differences among CRFs, AP and PA. Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value < 0.001).\nWe also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6. We achieved the best F-score. The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models."
  }, {
    "heading": "5 Conclusion",
    "text": "In this paper, we proposed multi-step stacked learning to extract n-gram features step by step. The first level removes the filler words providing new ngrams for the second level to remove edited words. The\nthird level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection. To minimize the F-loss approximately, we modified the hamming loss in M3Ns. Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank three anonymous reviewers for their valuable comments. This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."
  }],
  "year": 2013,
  "references": [{
    "title": "Hidden markov support vector machines",
    "authors": ["Yasemin Altun", "Ioannis Tsochantaridis", "Thomas Hofmann."],
    "venue": "Proc. of ICML.",
    "year": 2003
  }, {
    "title": "Edit detection and parsing for transcribed speech",
    "authors": ["Eugene Charniak", "Mark Johnson."],
    "venue": "Proc. of NAACL.",
    "year": 2001
  }, {
    "title": "Stacked sequential learning",
    "authors": ["William W. Cohen", "Vitor Rocha de Carvalho."],
    "venue": "Proc. of IJCAI.",
    "year": 2005
  }, {
    "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "Proc. of EMNLP.",
    "year": 2002
  }, {
    "title": "Online passive-aggressive algorithms",
    "authors": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai ShalevShwartz", "Yoram Singer", "Yoram Singer."],
    "venue": "Journal of Machine Learning Research.",
    "year": 2006
  }, {
    "title": "Using integer linear programming for detecting speech disfluencies",
    "authors": ["Kallirroi Georgila."],
    "venue": "Proc. of NAACL.",
    "year": 2009
  }, {
    "title": "A TAGbased noisy-channel model of speech repairs",
    "authors": ["Mark Johnson", "Eugene Charniak."],
    "venue": "Proc. of ACL.",
    "year": 2004
  }, {
    "title": "Effective use of prosody in parsing conversational speech",
    "authors": ["Jeremy G. Kahn", "Matthew Lease", "Eugene Charniak", "Mark Johnson", "Mari Ostendorf."],
    "venue": "Proc. of HLT-EMNLP.",
    "year": 2005
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proc. of ICML.",
    "year": 2001
  }, {
    "title": "Enriching speech recognition with automatic detection",
    "authors": ["Yang Liu", "E. Shriberg", "A. Stolcke", "D. Hillard", "M. Ostendorf", "M. Harper"],
    "year": 2006
  }, {
    "title": "Max-margin markov networks",
    "authors": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller."],
    "venue": "Proc. of NIPS.",
    "year": 2004
  }, {
    "title": "Learning Structured Prediction Models: A Large Margin Approach",
    "authors": ["Ben Taskar."],
    "venue": "Ph.D. thesis, Stanford University.",
    "year": 2004
  }, {
    "title": "A progressive feature selection algorithm for ultra large feature spaces",
    "authors": ["Qi Zhang", "Fuliang Weng", "Zhe Feng."],
    "venue": "Proc. of ACL.",
    "year": 2006
  }, {
    "title": "The impact of language models and loss functions on repair disfluency detection",
    "authors": ["Simon Zwarts", "Mark Johnson."],
    "venue": "Proc. of ACL-HLT.",
    "year": 2011
  }],
  "id": "SP:2a665aad5ac98ddb2101b39897439c014dc19e86",
  "authors": [{
    "name": "Xian Qian",
    "affiliations": []
  }, {
    "name": "Yang Liu",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (MNs) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and MNs with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1",
  "title": "Disfluency Detection Using Multi-step Stacked Learning"
}