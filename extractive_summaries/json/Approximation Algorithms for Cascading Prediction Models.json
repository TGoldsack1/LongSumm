{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In any machine learning task, some examples are harder than others, and intuitively we should be able to get away with less computation on easier examples. Doing so has the potential to reduce serving costs in the cloud as well as energy usage on-device, which is important in a wide variety of applications (Guan et al., 2017).\nFollowing the tremendous empirical success of deep learning, much recent work has focused on making deep neural networks adaptive, typically via an end-to-end training approach in which the network learns to make exampledependent decisions as to which computations are performed during inference. At the same time, recent work on neural architecture search has demonstrated that optimizing over thousands of candidate model architectures can yield results that improve upon state-of-the-art architectures designed by humans (Zoph et al., 2017). It is natural to think that combining these ideas should lead to even better results, but how best to do so remains an open problem.\nOne of the motivations for our work is that for many problems, there are order-of-magnitude differences between the cost of a reasonably accurate model and that of a model\n1Google Research. Correspondence to: Matthew Streeter <mstreeter@google.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwith state-of-the-art accuracy. For example, the most accurate NasNet model achieves 82.7% accuracy on ImageNet using 23 billion multiplies per example, while a MobileNet model achieves 70.6% accuracy with only 569 million multiplies per example (Howard et al., 2017; Zoph et al., 2017). If we could identify the images on which the smaller model’s prediction is (with high probability) no less accurate than the larger one’s, we could use fewer multiplications on those images without giving up much accuracy.\nIn this work, we present a family of algorithms that can be used to create a cascaded model with the same accuracy as a specified reference model, but potentially lower averagecase cost, where cost is user-defined. This family is defined by a meta-algorithm with various pluggable components. In its most basic instantiation, the algorithm takes a pool of pre-trained models as input and produces a cascaded model in two steps:\n1. It equips each model with a set of possible rules for returning “don’t know” (denoted⊥) on examples where it is not confident. Each (model, rule) combination is called an abstaining model.\n2. It selects a sequence of abstaining models to try, in order, when making a prediction (stopping once we find a model that does not return ⊥).\nWe also present instantiations of the meta-algorithm that generate new prediction models on-the-fly, either using lightweight training of ensemble models or a full architecture search. We also discuss a variant that produces an adaptive policy tree rather than a fixed sequence of models.\nAn important feature of our algorithms is that they scale efficiently to a large number of models and (model, rule) combinations. They also allow for computations performed by one stage of the cascade to be re-used in later stages when possible (e.g., if two successive stages of the cascade are neural networks that share the same first k layers)."
  }, {
    "heading": "2. Abstaining models",
    "text": "Our cascade-generation algorithm requires as input a set of abstaining models, which are prediction models that return “don’t know” (denoted ⊥) on certain examples. For classification problems, such models are known as classifiers\nwith a reject option, and methods for training them have been widely studied (Yuan & Wegkamp, 2010). In this section we present a simple post-processing approach that can be used to convert a pre-trained model from any domain into an abstaining model.\nWe assume our prediction model is a function p : X → Y , and that its performance is judged by taking the expected value of an accuracy metric q : Y × Y → R, where q(ŷ, y) is the accuracy of prediction ŷ when the true label is y. Our goal is to create an abstaining model m : X → Y ∪ {⊥} that returns ⊥ on examples where p has low accuracy, and returns p(x) otherwise.\nToward this end, we create a model q̂ to predict q(p(x), y) given x. Typically, this model is based on the values of intermediate computations performed when evaluating p(x). We train the model to estimate the value of the accuracy metric, seeking to achieve q̂(x) ≈ q(p(x), y). We then return⊥ if the predicted accuracy falls below some threshold.\nAs an example, for a multi-class classification problem, we might use the entropy of the vector of predicted class probabilities as a feature, and q̂ might be a one-dimensional isotonic regression that predicts top-1 accuracy as a function of entropy. The rule would then return ⊥ on examples where entropy is too high.\nPseudo-code for the abstaining model is given in Algorithm 1. Here and elsewhere, we distinguish between an algorithm’s parameters and its input variables. Specifying values for the parameters defines a function of the input variables, for example ConfidentModel(·; p, q̂, t) denotes the abstaining model based on prediction model p, accuracy model q̂, and threshold t.\nThe accuracy model q̂ used in this approach is similar to the binary event forecaster used in the calibration scheme of Kuleshov and Liang (2015), and is interchangeable with it in the case where q(ŷ, y) ∈ {0, 1}.\nAlgorithm 1 ConfidentModel(x; p, q̂, t) Parameters: prediction model p : X → Y , accuracy model q̂ : X → R, threshold t ∈ R Input: example x ∈ X return p(x) if q̂(x) ≥ t, else ⊥"
  }, {
    "heading": "3. Cascade generation algorithm",
    "text": "Having created a set of abstaining models, we must next select a sequence of abstaining models to use in our cascade. Our goal is to generate a cascade that minimizes average cost as measured on a validation set, subject to an accuracy constraint (e.g., requiring that overall accuracy match that of some existing reference model).\nWe accomplish this using a greedy algorithm presented in §3.1. To make clear the flexibility of our approach, we present it as a meta-algorithm parameterized by several functions:\n1. An accuracy constraint a(p,R) determines whether a prediction model p is sufficiently accurate on a set R ⊆ X × Y of labeled examples.\n2. A cost function c(m,S) determines the cost of evaluating m(x), possibly making use of intermediate results computed when evaluating each model in S.\n3. An abstaining model generator g(R,S) returns a set of abstaining models, given the set S of models that have already been added to the cascade by the greedy algorithm as well as the set R of labeled examples remaining (those on which every model in S abstains).\nPossible choices for these functions are discussed in sections 3.2, 3.3, and 3.4, respectively. §3.5 presents theoretical results about the performance of the greedy algorithm. §3.6 discusses how to modify the greedy algorithm to return an adaptive policy tree rather than a linear cascade, and §3.7 discusses integrating the algorithm with model architecture search."
  }, {
    "heading": "3.1. The greedy algorithm",
    "text": "We now present the greedy cascade-generation algorithm. As already mentioned, the goal of the algorithm is to produce a cascade that minimizes cost, subject to an accuracy constraint. The high-level idea of the algorithm is to find the abstaining model that maximizes the number of non-⊥ predictions per unit cost, considering only those abstaining models that satisfy the accuracy constraint on the subset of examples for which they return a prediction. We then remove from the validation set the examples on which this model returns a prediction, and apply the same greedy rule to choose the next abstaining model, continuing in this manner until no examples remain.\nTo define the algorithm precisely, we denote the set of examples on which an abstaining model m returns a prediction by\nA(m,R) = {(x, y) ∈ R : m(x) 6= ⊥} .\nHere and elsewhere, we use the shorthand mj:k to denote the sequence {mi}ki=j . Algorithm 2 gives pseudo-code for the algorithm using this notation.\nOur greedy algorithm builds on earlier approximation algorithms for min-sum set cover and related problems (Feige et al., 2004; Munagala et al., 2005; Streeter et al., 2007). The primary difference is that our algorithm must worry\nAlgorithm 2 GreedyCascade(R; a, c, g) Parameters: accuracy constraint a, cost function c, abstaining model generator g Input: validation set R Initialize i = 1, R1 = R, while |Ri| > 0 do Mi = g(Ri,m1:i−1) M usefuli = {m ∈Mi : A(m,Ri) 6= ∅} M accuratei = { m ∈M usefuli : a(m,A(m,Ri))\n} If M accuratei = ∅, return ⊥. Define ri(m) ≡ |A(m,Ri)|c(m,m1:i−1) mi = argmaxm∈M accuratei {ri(m)} Ri+1 = Ri \\ A(mi, Ri) i = i+ 1\nreturn m1:i−1\nabout maintaining accuracy in addition to minimizing cost. We also consider a more general notion of cost, reflecting the possibility of reusing intermediate computations."
  }, {
    "heading": "3.2. Accuracy constraints",
    "text": "We first consider the circumstances under which Algorithm 2 returns a cascade that satisfies the accuracy constraint.\nLet S = m1:k be a cascade returned by the greedy algorithm, and let Ai = A(mi, Ri), where Ri is the set of examples remaining at the start of iteration i. Observe that Ri+1 = Ri \\Ai, which implies that Ai and Aj are disjoint for j 6= i. Also, because R1 = R and Rk = ∅, ⋃ iAi = R.\nBy construction, a(mi, Ai) holds for all i. Because S uses mi to make predictions on examples in Ai, this implies a(S,Ai) holds as well. Thus, the accuracy constraint a(S,R) will be satisfied so long as a(S,Ai) ∀i implies a(S, ⋃ iAi). A sufficient condition is that the accuracy constraint is decomposable, as captured in the following definition.\nDefinition 1. An accuracy constraint a is decomposable if, for any two disjoint sets A,B ⊆ X × Y , a(m,A) ∧ a(m,B) =⇒ a(m,A ∪B).\nAn example of a decomposable accuracy constraint is the MinRelativeAccuracy constraint shown in Algorithm 3, which requires that average accuracy according to some metric is at least α times that of a fixed reference model. When using this constraint, any cascade returned by the greedy algorithm is guaranteed to have overall accuracy at least α times that of the reference model pref.\nWe now consider the circumstances under which the greedy algorithm terminates successfully (i.e., does not return ⊥). This happens so long as M accuratei is always non-empty. A\nAlgorithm 3 MinRelativeAccuracy(p,R0;α, q, pref) Parameters: α ∈ (0, 1], accuracy metric q, prediction model pref Input: prediction model p, validation set R0 Define Q(p′) = ∑ (x,y)∈R0 q(p ′(x), y)\nreturn I [Q(p) ≥ α ·Q(pref)]\nsufficient condition is that the accuracy constraint is satisfiable, defined as follows. Definition 2. An accuracy constraint a is satisfiable with respect to an abstaining model generator g and validation set R if there exists a model m∗ that (a) never abstains, (b) is always returned by g, and (c) satisfies a(m∗, R0) ∀R0 ⊆ R.\nThe MinRelativeAccuracy constraint is satisfiable provided the reference model pref is always among the models returned by the model generator g.\nNote that MinRelativeAccuracy is not the same as simply requiring a fixed minimum average accuracy (e.g., 80% top-1 accuracy). Rather, the accuracy required depends on the reference model’s performance on the provided subset R0, which takes on many different values when running Algorithm 2. A constraint that requires accuracy ≥ qmin on R0 is generally not satisfiable, because R0 might contain only examples that all models misclassify."
  }, {
    "heading": "3.3. Cost functions",
    "text": "We now consider possible choices for the cost function c. In the simplest case, there is no reuse of computations and c(m,S) depends only on m, in which case we say the cost function is linear.\nTo allow for computation reuse, we define a weighted, directed graph with a vertex v(m) for each model m, plus a special vertex v∅. For each m, there is an edge (v∅, v(m)) whose weight is the cost of computing m(x) from scratch. An edge (v(m1), v(m2)) with weight w indicates that m2(x) can be computed at cost w if m1(x) has already been computed. The cost function is then:\nc(m,S) = min v∈V (S)\n{shortest path(v, v(m))} (1)\nwhere V (S) = {v(m) : m ∈ S} ∪ {v∅}.\nAs an example, suppose we have a set of models {mi : 1 ≤ i ≤ D}, where mi makes predictions by computing the output of the first i layers of a fixed neural network (e.g., a ResNet-like image classifier). In this case, the graph is a linear chain whose ith edge has weight equal to the cost of computing the output of layer i given its input.\nEquation (1) can also be generalized in terms of hypergraphs to allow reuse of multiple intermediate results. This\nis useful in the case of ensemble models, which take a weighted average of other models’ predictions."
  }, {
    "heading": "3.4. Abstaining model generators",
    "text": "We now discuss choices for the abstaining model generator g used in Algorithm 2. Given a set Ri of examples remaining in the validation set, and a sequence m1:i−1 of models that are already in the cascade, g returns a set of models to consider for the ith stage of the cascade.\nA simple approach to defining g is to take a fixed set P of prediction models, and for each one to return a ConfidentModel with the threshold set just high enough to satisfy the accuracy constraint, as illustrated in Algorithm 4.\nAlgorithm 4 ConfidentModelSet(R;P, Q̂, a) Parameters: set P of prediction models, set Q̂ of accuracy models, accuracy constraint a Input: validation set R Define a>(m) = a(m,A(m,R)) Define m(p, q̂, t) = ConfidentModel(·; p, q̂, t) Define tmin(p, q̂) = min { t ∈ R : a>(m(p, q̂, t))\n} return { m(p, q̂, tmin(p, q̂)) : p ∈ P, q̂ ∈ Q̂ }\nAnother lightweight approach is to fit an ensemble model that makes use of the already-computed predictions of the first i − 1 models. Assume that each abstaining model mj has a backing prediction model pj that never returns⊥. For each p in a fixed set P of prediction models, we fit an ensemble model p̄(x) = β0p(x) + ∑i−1 j=1 βjpj(x), where β is optimized to maximize accuracy on the remaining examplesRi. Each p̄ can then be converted to a ConfidentModel in the same manner as above.\nThe most thorough (but also most expensive) approach is to perform a search to find a model architecture that yields the best benefit/cost ratio, as discussed further in §3.7."
  }, {
    "heading": "3.5. Theoretical results",
    "text": "In this section we provide performance guarantees for Algorithm 2, showing that under reasonable assumptions it produces a cascade that satisfies the accuracy constraint and has cost within a factor of 4 of optimal. We also show that even in very special cases, the problem of finding a cascade whose cost is within a factor 4 − of optimal is NP-hard for any > 0.\nAs shown in §3.2, the greedy algorithm will return a cascade that satisfies the accuracy constraint provided the constraint is decomposable and satisfiable. This is a fairly weak assumption, and is satisfied by the MinRelativeAccuracy constraint given in Algorithm 3.\nWe now consider the conditions the cost function must satisfy, which are more subtle. Our guarantees hold for all linear cost functions, as well as a certain class of functions that allow for a limited form of computation reuse. To make this precise, we will use the following definitions.\nDefinition 3. A set M∗ of abstaining models dominates a sequence {mi}ki=1 of abstaining models with respect to a cost function c if two conditions hold:\n1. ∑ m∈M∗ c(m, ∅) ≤ ∑k i=1 c(mi,m1:i−1), and\n2. for any x ∈ X , ifmi(x) 6= ⊥ for some i, thenm(x) 6= ⊥ for some m ∈M∗.\nIf the cost function is linear, c(m,S) = c(m, ∅) ∀m,S, and any sequence of abstaining models is dominated by the corresponding set.\nDefinition 4. A cost function c is admissible with respect to a set of abstaining modelsM if, for any sequence of models in M , there exists a set M∗ ⊆M that dominates it.\nA linear cost function is always admissible. Cost functions of the form (1) are admissible under certain conditions. A sufficient condition is that the graph defining the cost function is a linear chain, and for each edge (v(mi), v(mi+1)), mi(x) 6= ⊥ =⇒ mi+1(x) 6= ⊥. If the graph is a linear chain but does not have this property, we can make the cost function admissible by including additional models. Specifically, for each k, we add a model m∗k that computes the output of models m1,m2, . . . ,mk in order (at cost c(m∗k, ∅) = ∑k i=1 c(mi,m1:i−1) = c(mk, ∅)), and then returns the prediction (if any) returned by the model with maximum index. The singleton set {m∗k} will then dominate any sequence composed of models in {m1,m2, . . . ,mk}. Similar arguments apply to graphs comprised of multiple linear chains. (Such graphs arise if we have multiple deep neural networks, each of which can return a prediction after evaluating only its first k layers.)\nWe also assume c(m,S) ≤ c(m, ∅) ∀m,S (i.e., reusing intermediate computations does not hurt).\nTo state our performance guarantees, we now introduce some additional notation. For any cascade S = m1:k and cost function c, let\ncΣ(S) ≡ k∑\ni=1\nc(mi,m1:i−1) (2)\nbe the cost of computing the output of all stages. For any example x and cascade S = m1:k, let τ(x, S) be the cost of computing the output of S, that is\nτ(x, S) = ∑\ni:mj(x)=⊥∀j<i\nc(mi,m1:i−1) . (3)\nFinally, for any set M of models, we define A(M,R) = ∪m∈MA(m,R).\nThe following lemma shows that, if the cost function is admissible, the number of examples that a cascade can answer per unit cost is bounded by the maximum number of examples any single model can answer per unit cost. Theorem 1 then uses this inequality to bound the approximation ratio of the greedy algorithm.\nLemma 1. For any setR ⊂ X×Y , any setM of abstaining models, any cost function c that is admissible with respect to M , and any sequence S of models in M ,\n|A (S,R)| ≤ r∗cΣ(S)\nwhere\nr∗ = max m∈M { |A (m,R)| c(m, ∅) } and cΣ is defined in Equation (2).\nProof. For any m ∈ M , |A (m,R)| ≤ r∗ · c(m, ∅) by definition of r∗. Thus, for any set M∗ ⊆M ,\n|A (M∗, R)| ≤ ∑\nm∈M∗ |A (m,R)|\n≤ r∗ ∑\nm∈M∗ c(m, ∅) .\nBecause c is admissible with respect to M , there exists an M∗ with |A (S,R)| ≤ |A (M∗, R)| and∑\nm∈M∗ c(m, ∅) ≤ cΣ(S). Combining this with the above inequality proves the lemma.\nThe proof of Theorem 1 (specifically the proof of claim 2) is along the same lines as the analysis of a related greedy algorithm for generating a task-switching schedule (Streeter et al., 2007), which in turn built on an elegant geometric proof technique developed by Feige et al. (2004).\nTheorem 1. Let SG = GreedyCascade(R; a, c, g). LetM be a set of models such thatM ⊆M accuratei for all i, and c is admissible with respect to M , where M accuratei is defined as in Algorithm 2. Define T(S) ≡ ∑ (x,y)∈R τ(x, S), where τ is defined in Equation (3). Then,\nGREEDY ≤ 4 · OPT\nwhere GREEDY = T(SG), OPT = minS∈M∞ {T(S)}.\nProof. We first introduce some notation. Let SG = m1:k, let ni = |Ri| denote the number of examples remaining at the start of the ith iteration of the greedy algorithm, and let Ci = c(mi,m1:i−1) be the cost of the abstaining model selected in the ith iteration. Let r∗i = ni−ni+1 Ci\nbe the maximum benefit/cost ratio on iteration i. Let S∗ be an optimal cascade.\nClaim 1: For any i, there are at least ni2 examples with τ(x, S∗) ≥ ni2r∗i .\nProof of claim 1: Let t = ni2r∗i , and let S0 be the maximal prefix of S∗ satisfying cΣ(S0) ≤ t. Any example x ∈ Ri \\ A(S0, Ri) must have τ(x, S∗) ≥ t. Thus, it suffices to show |A (S0, Ri)| ≤ ni2 . By Lemma 1, |A (S0, Ri)| ≤ r∗t, where r∗ = maxm∈M { |A(m,Ri)| c(m,∅) } . By assumption, c(m, ∅) ≥ c(m,m1:i−1) for all m ∈ M , which implies r∗ ≤ maxm∈M { |A(m,Ri)|\nc(m,m1:i−1)\n} ≤ r∗i , where\nthe last inequality uses the fact that M ⊆ M accuratei . Thus, |A (S0, Ri)| ≤ tr∗ ≤ tr∗i = ni2 .\nClaim 2: GREEDY ≤ 4 · OPT.\nProof of claim 2: The total cost of the cascade SG is the sum of the costs associated with each stage, that is\nGREEDY = ∑ x∈R τ(x, SG) = k∑ i=1 niCi .\nTo relate OPT to this expression, let {tj}nj=1 be the sequence that results from sorting the costs {τ(x, S∗) : x ∈ R} in descending order. Assuming for the moment that ni is even for all i, let J(i) = { j : ni+12 < j ≤ ni 2 } . Because {tj} is nonincreasing, and n1 = n while nk = 0,\nOPT = n∑\nj=1\ntj ≥ n 2∑\nj=1\ntj = k∑ i=1 ∑ j∈J(i) tj .\nThus, to show OPT ≥ 14GREEDY, it suffices to show that for any i, ∑\nj∈J(i)\ntj ≥ 1\n4 niCi . (4)\nTo see this, first note that for any i,∑ j∈J(i) tj ≥ |J(i)| · t(ni2 ) = r∗i · Ci 2 · t(ni2 )\nBy claim 1, t(ni2 ) ≥ ni 2r∗i . Combining this with the above inequality proves (4).\nFinally, if ni is odd for some i, we can apply the above argument to a set R′ which contains two copies of each example in R, in order to prove the equivalent inequality 2GREEDY ≤ 4 · 2OPT.\nFinally, we consider the computational complexity of the optimization problem Algorithm 2 solves. Given a validation set R, set of abstaining models M , and accuracy constraint a, we refer to the problem of finding a minimumcost cascade S that satisfies the accuracy constraint as\nMINIMUM COST CASCADE. This problem is NP-hard to approximate even in very special cases, as summarized in Theorem 2.\nTheorem 2. For any > 0, it is NP-hard to obtain an approximation ratio of 4− for MINIMUM COST CASCADE. This is true even in the special case where: (1) the cost function always returns 1, and (2) the accuracy constraint is always satisfied.\nProof (sketch). The theorem can be proved using a reduction from MIN-SUM SET COVER (Feige et al., 2004). In the reduction, each element e in the MIN-SUM SET COVER instance becomes an example xe in the validation set, and each set Z becomes a prediction model mZ where mZ(xe) = ⊥ iff. e /∈ Z. The cost function in the MINIMUM COST CASCADE instance always returns 1, and the accuracy constraint is always satisfied."
  }, {
    "heading": "3.6. Adaptive policies",
    "text": "In this section we discuss how to modify Algorithm 2 to return an adaptive policy tree rather than a linear cascade.\nThe greedy algorithm for set covering can be modified to produce an adaptive policy (Golovin & Krause, 2011), and a similar approach can be applied Algorithm 2. The resulting algorithm is similar to Algorithm 2, but instead of building up a list of abstaining models it builds a tree, where each node of the tree is labeled with an abstaining model and each edge is labeled with some feature of the parent node’s output (e.g., a discretized confidence score).\nIf theA function satisfies a technical condition called adaptive monotone submodularity, the resulting algorithm has an approximation guarantee analogous to the one stated in Theorem 1, but with respect to the best adaptive policy rather than merely the best linear sequence. This can be shown by combining the proof technique of Golovin and Krause (2011) with the proof of Theorem 1. Unfortunately, the A function is not guaranteed to have this property in general. However, it can be shown that the adaptive version of Algorithm 2 still has the guarantees described in Theorem 1 (i.e., adaptivity does not hurt)."
  }, {
    "heading": "3.7. Greedy architecture search",
    "text": "The GreedyCascade algorithm can be integrated with model architecture search in multiple ways. One way would be to simply take all the models evaluated by an architecture search as input to the greedy algorithm. A potentially much more powerful approach is to use architecture search as the model generator g used in the greedy algorithm’s inner loop.\nWith this approach, there is one architecture search for each stage of the generated cascade. The goal of the ith search is\nto maximize the benefit/cost ratio criterion used on the ith iteration of the greedy algorithm (subject to the accuracy constraint). Because the ith search only needs to consider examples not already classified by the first i−1 stages, later searches have potentially lower training cost. Furthermore, the ith model can make use of the intermediate layers of the first i− 1 models as input features, allowing computations to be reused across stages of the cascade."
  }, {
    "heading": "4. Experiments",
    "text": "In this section we evaluate our cascade generation algorithm by applying it to state-of-the-art pre-trained models for the ImageNet classification task. We first examine the efficacy of the abstention rules described in §2, then we evaluate the full cascade-generation algorithm."
  }, {
    "heading": "4.1. Accuracy versus abstention rate",
    "text": "As discussed in §2, we decide whether a model should abstain from making a prediction by training a second model to predict its accuracy on a given example, and checking whether predicted accuracy falls below some threshold.\nFor our ImageNet experiments, we take top-1 accuracy as the accuracy metric, and predict its value based on a vector of features derived from the model’s predicted class probabilities. We use as features (1) the entropy of the vector, (2) the maximum predicted class probability, and (3) the gap between the first and second highest predictions in logit space. Our accuracy model q̂ is fit using logistic regression on a validation set of 25,000 images.\nFigure 1 illustrates the tradeoffs between accuracy and response rate that can be achieved by applying this rule to Inception-v3, measured on a second disjoint validation set of 25,000 images. The horizontal axis is the fraction of examples on which Inception-v3 returns ⊥, and the ver-\nTable 1. Cascade of pre-trained MobileNet (Howard et al., 2017) models.\nSTAGE IMAGE SIZE # MULTS CONFIDENCE\nTHRESHOLD (LOGIT GAP)\n%(EXAMPLES CLASSIFIED)\nACCURACY (ON EXAMPLES CLASSIFIED BY STAGE)\n1 128 X 128 49M 1.98 40% 88% 2 160 X 160 77M 1.67 16% 73% 3 160 X 160 162M 1.23 18% 62% 4 224 X 224 150M 1.24 7% 45% 5 224 X 224 569M −∞ 19% 45%\ntical axis is top-1 accuracy on the remaining examples. For comparison, we also show for each feature the tradeoff curve obtained by simply thresholding the raw feature value. We also show the theoretically optimal tradeoff curve that would be achieved using an accuracy model that predicts top-1 accuracy perfectly (in which case we only return ⊥ on examples Inception-v3 misclassifies).\nOverall, Inception-v3 achieves 77% top-1 accuracy. However, if we set the logit gap cutoff threshold appropriately, we can achieve over 95% accuracy on the 44% of examples on which the model is most confident (while returning ⊥ on the remaining 56%). Perhaps surprisingly, using the learned accuracy model gives a tradeoff curve almost identical to that obtained by simply thresholding the logit gap."
  }, {
    "heading": "4.2. Cascades",
    "text": "Having shown the effectiveness of our abstention rules, we now evaluate our cascade-generation algorithm on a pool of state-of-the-art ImageNet classification models. Our pool consists of 23 models released as part of the TF-Slim library (Silberman & Guadarrama, 2016). The pool contains two recent NasNet models produced by neural architecture search (Zoph et al., 2017), five models based on the Inception architecture (Szegedy et al., 2016), and all 16 MobileNet models (Howard et al., 2017). We generate abstaining models by thresholding the logit gap value.\nFor each model p, and each α ∈ { 1− i100 : 0 ≤ i ≤ 5 }\n, we used Algorithm 2 to generate a cascade with low cost, subject to the constraint that accuracy was at least α times that of p. We use number of multiplications as the cost.\nWhen using Algorithm 2 it is important to use examples not seen during training, because statistics such as logit gap are distributed very differently for them. We used 25,000 images from the ILSVRC 2012 validation set (Russakovsky et al., 2015) to run the algorithm, and report results on the remaining 25,000 validation images.1\nFigure 2 shows the tradeoffs we achieve between accuracy\n1The cascade returned by the greedy algorithm always returns a non-⊥ prediction on unseen test images, because the final stage of the cascade always uses a confidence threshold of −∞.\nFigure 2. Cascades of pre-trained ImageNet models.\nand average cost. Relative to the large (server-sized) NasNet model, we obtain a 1.5x reduction with no loss in accuracy. Relative to Inception-v4, one of the cascades obtains a 1.8x cost reduction with a no loss in accuracy, while another obtains a 1.2x cost reduction with a 1.2% increase in accuracy. Relative to the largest MobileNet model, we achieve a 2x cost reduction with a 0.5% accuracy gain.\nWe now examine the structure of an auto-generated cascade. Table 1 shows a cascade generated using a pool of 16 MobileNet models, with the most accurate MobileNet model as the reference model. The ith row of the table describes the (model, rule) pair used in the ith stage of the cascade. The cascade has several intuitive properties:\n1. Earlier stages use cheaper models. Model used in earlier stages of the cascade have fewer parameters, use fewer multiplies, and have lower input image resolution.\n2. Cheaper models require higher confidence. The minimum logit gap required to make a prediction is higher for earlier stages, reflecting the fact that cheaper models must be more confident in order to achieve sufficiently high accuracy.\n3. Cheaper models handle easier images. Although\noverall model accuracy increases in later stages, accuracy on the subset of images actually classified by each stage is strictly decreasing (last column). This supports the idea that easier images are allocated to cheaper models."
  }, {
    "heading": "4.3. Cascades of approximations",
    "text": "A large number of techniques have been developed for reducing the cost of deep neural networks via postprocessing. Such techniques include quantization, pruning of weights or channels, and tensor factorizations (see (Han et al., 2015) and references therein for further discussion). In this section, we show how these techniques can be used to generate a larger pool of approximated models, which can then be used as input to our cascade-generation algorithm in order to achieve further cost reductions. This also provides a way to make use of the cascade-generation algorithm in the case where only a single pre-trained model is available.\nFor these experiments, we focus on quantization of model parameters as the compression technique. For each model m, and each number of bits b ∈ {1, 2, . . . , 16}, we generate a new model mb by quantizing all of m’s parameters to b bits. This yields a pool of 23 · 16 = 368 quantized models, which we use as input to the cascade-generation algorithm. Cost is the number of bits read from memory when classifying an example. Aside from these two changes, our experiments are identical to those in §4.2.\nFigure 3 shows accuracy as a function of the average number of bits that must be fetched from memory in order to classify an example. Though the cascades generated in §4.2 (which were optimized for number of multiplications) do not consistently improve on average memory I/O, the cascades of approximations reduce it by up to a factor of 6 with no loss in accuracy."
  }, {
    "heading": "5. Related work",
    "text": "The high-level goal of our work is to reduce average-case inference cost by spending less computation time on easier examples. This subject is the topic of a vast literature, with many distinct problem formulations spread across many domains.\nWithin the realm of computer vision, cascaded models received much attention following the seminal work of Viola and Jones (2001), who used a hand-crafted cascade of increasingly more expensive features to create a fast and accurate face detector. Zhang and Viola (2008) partially automated this approach by selecting the thresholds used in each stage of the cascade automatically, while later work selected both the thresholds and the set of features used in each stage automatically (Vasconcelos & Saberian, 2010; Chen et al., 2012). Though designed for binary classification, these approaches could potentially be generalized to apply to other problems using thresholds based on prediction confidence (see §2).\nIn problems such as speech recognition and machine translation, inference typically involves a heuristic search through the large space of possible outputs, and the cascade idea can be used to progressively filter the set of outputs under consideration (Weiss & Taskar, 2010).\nRecent work has sought to apply the cascade idea to deep networks. Most research involves training an adaptive model end-to-end (Graves, 2016; Guan et al., 2017; Hu et al., 2017; Huang et al., 2017). Though end-to-end training is appealing, it is sensitive to the choice of model architecture. Current approaches for image classification are based on ResNet architectures, and do not achieve results competitive with the latest NasNet models on ImageNet.\nAnother way to produce adaptive deep networks is to apply postprocessing to a pool of pre-trained models, as we have done. To our knowledge, the only previous work that has taken this route is that of Bolukbasi et al. (2017), who also present results for ImageNet. In contrast to the greedy approximation algorithm presented in this work, their approach does not have good performance in the worst case, and also requires the pre-trained input models to be arranged into a directed acyclic graph a priori as opposed to learning this structure as part of the optimization process.\nOur work is also similar in spirit to that of Grubb and Bagnell (2012), who focus on maximizing the accuracy achieved in a given time (whereas our work minimizes average-case time subject to an accuracy constraint).\nFinally, as already mentioned, our greedy approximation algorithm builds on previous greedy algorithms for minsum set cover and related problems (Feige et al., 2004; Munagala et al., 2005; Streeter et al., 2007)."
  }],
  "year": 2018,
  "references": [{
    "title": "Adaptive neural networks for efficient inference",
    "authors": ["T. Bolukbasi", "J. Wang", "O. Dekel", "V. Saligrama"],
    "venue": "In Proceedings of the Thirty-fourth International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Classifier cascade for minimizing feature evaluation cost",
    "authors": ["M. Chen", "Z. Xu", "K. Weinberger", "O. Chapelle", "D. Kedem"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2012
  }, {
    "title": "Approximating min sum set",
    "authors": ["U. Feige", "L. Lovász", "P. Tetali"],
    "venue": "cover. Algorithmica,",
    "year": 2004
  }, {
    "title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
    "authors": ["D. Golovin", "A. Krause"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2011
  }, {
    "title": "Adaptive computation time for recurrent neural networks",
    "authors": ["A. Graves"],
    "venue": "CoRR, abs/1603.08983,",
    "year": 2016
  }, {
    "title": "Speedboost: Anytime prediction with uniform near-optimality",
    "authors": ["A. Grubb", "D. Bagnell"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2012
  }, {
    "title": "Energyefficient amortized inference with cascaded deep",
    "authors": ["J. Guan", "Y. Liu", "Q. Liu", "J. Peng"],
    "venue": "classifiers. CoRR,",
    "year": 2017
  }, {
    "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
    "authors": ["S. Han", "H. Mao", "W.J. Dally"],
    "venue": "arXiv preprint arXiv:1510.00149,",
    "year": 2015
  }, {
    "title": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
    "authors": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"],
    "year": 2017
  }, {
    "title": "Anytime neural network: a versatile trade-off between computation and accuracy",
    "authors": ["H. Hu", "D. Dey", "J.A. Bagnell", "M. Hebert"],
    "year": 2017
  }, {
    "title": "Multi-scale dense networks for resource efficient image classification",
    "authors": ["G. Huang", "D. Chen", "T. Li", "F. Wu", "L. van der Maaten", "K.Q. Weinberger"],
    "year": 2017
  }, {
    "title": "Calibrated structured prediction",
    "authors": ["V. Kuleshov", "P.S. Liang"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "The pipelined set cover problem",
    "authors": ["K. Munagala", "S. Babu", "R. Motwani", "J. Widom", "E. Thomas"],
    "venue": "In ICDT 05: Proceedings of the 10th International Conference,",
    "year": 2005
  }, {
    "title": "Tf-slim: A high level library to define complex models in tensorflow. 2016",
    "authors": ["N. Silberman", "S. Guadarrama"],
    "venue": "URL https: //research.googleblog.com/2016/08/ tf-slim-high-level-library-to-define",
    "year": 2016
  }, {
    "title": "Combining multiple heuristics online",
    "authors": ["M. Streeter", "D. Golovin", "S.F. Smith"],
    "venue": "In Proceedings of the Twentysecond National Conference on Artificial Intelligence",
    "year": 2007
  }, {
    "title": "Rethinking the inception architecture for computer vision",
    "authors": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Boosting classifier cascades",
    "authors": ["N. Vasconcelos", "M.J. Saberian"],
    "venue": "In Advances in Neural Information Processing Systems, pp. 2047–2055,",
    "year": 2010
  }, {
    "title": "Rapid object detection using a boosted cascade of simple features",
    "authors": ["P. Viola", "M. Jones"],
    "venue": "In CVPR,",
    "year": 2001
  }, {
    "title": "Structured prediction cascades",
    "authors": ["D. Weiss", "B. Taskar"],
    "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Classification methods with reject option based on convex risk minimization",
    "authors": ["M. Yuan", "M. Wegkamp"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Multiple-instance pruning for learning efficient cascade detectors",
    "authors": ["C. Zhang", "P.A. Viola"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2008
  }, {
    "title": "Learning transferable architectures for scalable image recognition",
    "authors": ["B. Zoph", "V. Vasudevan", "J. Shlens", "Q.V. Le"],
    "year": 2017
  }],
  "id": "SP:46efe47c1d0102a42e2236ab5c063026681ab236",
  "authors": [{
    "name": "Matthew Streeter",
    "affiliations": []
  }],
  "abstractText": "We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost. Applied to state-of-the-art ImageNet classification models, this yields up to a 2x reduction in floating point multiplications, and up to a 6x reduction in average-case memory I/O. The auto-generated cascades exhibit intuitive properties, such as using lower-resolution input for easier images and requiring higher prediction confidence when using a computationally cheaper model.",
  "title": "Approximation Algorithms for Cascading Prediction Models"
}