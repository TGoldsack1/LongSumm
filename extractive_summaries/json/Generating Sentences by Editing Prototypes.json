{
  "sections": [{
    "heading": "1 Introduction",
    "text": "The ability to generate sentences is core to many NLP tasks, including machine translation, summarization, speech recognition, and dialogue. Most neural models for these tasks are based on recurrent neural language models (NLMs), which generate sentences from scratch, often in a left-to-right manner (Bengio et al., 2003). It is often observed that such NLMs suffer from the problem of favoring generic utterances such as “I don’t know” (Li et al., 2016). At the same time, naive strategies to increase diversity have been shown to compromise grammaticality (Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully represent the full diversity of complex utterances.\nIndeed, it is difficult even for humans to write complex text from scratch in a single pass; we often create an initial draft and incrementally revise it (Hayes and Flower, 1986). Inspired by this process,\nwe propose a new unconditional generative model of text which we call the prototype-then-edit model, illustrated in Figure 1. It first samples a random prototype sentence from the training corpus, and then invokes a neural editor, which draws a random “edit vector” and generates a new sentence by attending to the prototype while conditioning on the edit vector. The motivation is that sentences from the corpus provide a high quality starting point: they are grammatical, naturally diverse, and exhibit no bias towards shortness or vagueness. The attention mechanism (Bahdanau et al., 2015) of the neural editor strongly biases the generation towards the prototype, and therefore it needs to solve a much easier problem than generating from scratch.\nWe train the neural editor by maximizing an approximation to the generative model’s loglikelihood. This objective is a sum over lexically-\n437\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 437–450, 2018. Action Editor: Trevor Cohn. Submission batch: 9/2017; Revision batch: 12/2017; Published 7/2018.\nc©2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nsimilar sentence pairs in the training set, which we can scalably approximate using locality sensitive hashing. We also show empirically that most lexically similar sentences are also semantically similar, thereby endowing the neural editor with additional semantic structure. For example, we can use the neural editor to perform a random walk from a seed sentence to traverse semantic space.\nWe compare our prototype-then-edit model to approaches that generate from scratch on both language generation quality and semantic properties. For the former, our model generates higher quality generations according to human evaluations, and improves perplexity by 13 points on the Yelp corpus and 7 points on the One Billion Word Benchmark. For the latter, we show that latent edit vectors outperform standard sentence variational autoencoders (Bowman et al., 2016) on semantic similarity, locally-controlled text generation, and a sentence analogy task."
  }, {
    "heading": "2 Problem statement",
    "text": "Our primary goal is to learn a generative model of sentences for use as a language model.1 In particular, we model sentence generation as a prototypethen-edit process:\n1. Select prototype: Given a training corpus of sentencesX , randomly sample a prototype sentence x′ from a prototype distribution p(x′) (in our case, uniform over X ).\n2. Edit: Sample an edit vector z (encoding the type of edit to be performed) from an edit prior p(z). Then, feed the edit vector z and the previously selected prototype x′ into a neural editor pedit(x | x′, z), which generates a new sentence x.\nUnder this model, the likelihood of a sentence is:\np(x) = ∑\nx′∈X p(x | x′)p(x′) (1)\np(x | x′) = Ez∼p(z) [ pedit(x | x′, z) ] , (2)\n1 For many applications such as machine translation or dialogue generation, there is a context (e.g. foreign sentence, dialogue history), which can be supplied to both the prototype selector and the neural editor. This paper focuses on the unconditional case, proposing an alternative to LSTM based language models.\nwhere both prototype x′ and edit vector z are latent variables.\nOur formulation stems from the observation that many sentences in a large corpus can be represented as minor transformations of other sentences. For example, in the Yelp restaurant review corpus (Yelp, 2017) we find that 70% of the test set is within wordtoken Jaccard distance 0.5 of a training set sentence, even though almost no sentences are repeated verbatim. This implies that a neural editor which models lexically similar sentences should be an effective generative model for large parts of the test set.\nA secondary goal for the neural editor is to capture certain semantic properties; we focus on the following two in particular:\n1. Semantic smoothness: an edit should be able to alter the semantics of a sentence by a small and well-controlled amount, while multiple edits should make it possible to accumulate a larger change.\n2. Consistent edit behavior: the edit vector z should model/control the variation in the type of edit that is performed. When we apply the same edit vector on different sentences, the neural editor should perform semantically analogous edits across the sentences.\nIn Section 4, we show that the neural editor can successfully capture both properties, as reported by human evaluations."
  }, {
    "heading": "3 Approach",
    "text": "We would like to train our neural editor pedit(x | x′, z) by maximizing the marginal likelihood (Equation 1) via gradient ascent, but the objective cannot be computed exactly because it involves a sum over all prototypes x′ (expensive) and an expectation over the edit prior p(z) (no closed form).\nWe therefore propose two approximations to overcome these challenges:\n1. We lower bound the sum over latent prototypes x′ (in Equation 1) by only summing over x′ that are lexically similar to x.\n2. We lower bound the expectation over the edit prior (in Equation 2) using the evidence lower bound (ELBO) (Jordan et al., 1999; Doersch, 2016) which can be effectively approximated.\nWe describe and motivate these approximations in Sections 3.1 and 3.2, respectively. In Section 3.3, we combine the two approximations to give the final objective. Sections 3.4 and 3.5 drill down further into our specific model architecture."
  }, {
    "heading": "3.1 Approximate sum on prototypes, x′",
    "text": "Equation 1 defines the probability of generating a sentence x as the total probability of reaching x via edits from every prototype x′ ∈ X . However, most prototypes are unrelated and should have very small probability of transforming into x. Therefore, we approximate the summation over prototypes by only considering prototypes x′ that have high lexical overlap with x. To that end, define a lexical similarity neighborhood as:\nN (x) def= {x′ ∈ X : dJ(x, x′) < 0.5},\nwhere dJ(x, x′) is the Jaccard distance between x and x′ (treating each as a set of word tokens).\nWe will now lower bound log p(x) in two ways: (i) we will sum over only prototypes in the neighborhood N (x) rather than over the entire training set X as discussed above; (ii) we will push the log inside the summation using Jensen’s inequality, as is standard with variational lower bounds. Recall that the distribution over prototypes is uniform (p(x′) = 1/|X |), and define R(x) = log(|N (x)|/|X |). The derivation is as follows:\nlog p(x) = log\n[∑\nx′∈X p(x | x′)p(x′)\n]\n(i) ≥ log   ∑\nx′∈N (x) p(x | x′)p(x′)\n  (3)\n= log  |N (x)|−1 ∑\nx′∈N (x) p(x | x′)\n +R(x)\n(ii) ≥ |N (x)|−1 ∑\nx′∈N (x) log p(x | x′) ︸ ︷︷ ︸ def = LEX(x) +R(x).\nAssuming the neighborhood size |N (x)| is constant across x, then LEX(x) is a lower bound of log p(x) up to constants. For each x, the neighborhood N (x) can be efficiently precomputed with locality sensitive hashing (LSH) and minhashing. The full procedure is described in Appendix 6.\nNote that LEX(x) is still intractable to compute because each log p(x|x′) term involves an expectation over the edit prior p(z) (Equation 2). We address this in Section 3.2, but first, an interlude.\nInterlude: lexical similarity semantics. So far, we have motivated lexical similarity neighborhoods via computational considerations, but we found that lexical similarity training also captures semantic similarity. One can certainly construct sentences with small lexical distance that differ semantically (e.g., insertion of the word “not”). However, since we mine sentences from a corpus grounded in real world events, most lexically similar sentences are also semantically similar. For example, given “my son enjoyed the delicious pizza”, we are far more likely to see “my son enjoyed the delicious macaroni”, versus “my son hated the delicious pizza”.\nHuman evaluations of 250 edit pairs sampled from lexical similarity neighborhoods on the Yelp corpus support this conclusion. 35.2% of the sentence pairs were judged to be exact paraphrases, while 84% of the pairs were judged to be at least roughly equivalent. Sentence pairs were negated or change in topic only 7.2% of the time. Thus, a neural editor trained on this distribution should preferentially generate semantically similar edits.\nNote that semantic similarity is not needed if we are only interested in modeling the distribution p(x). But it does enable us to learn an edit model p(x|x′) that prefers semantically meaningful edits, which we explore in Section 4.3."
  }, {
    "heading": "3.2 Approximate expectation on edit vectors, z",
    "text": "In Section 3.1, we approximated the marginal likelihood log p(x) by LEX(x), which is a summation over terms of the form:\nlog p(x | x′) = logEz∼p(z) [ pedit(x | x′, z) ] . (4)\nUnfortunately the expectation over p(z) has no closed form, and naively approximating it by Monte Carlo sampling z ∼ p(z) will have unacceptably high variance, because pedit(x | x′, z) will be almost zero for nearly all z sampled from p(z), while being large for a few important but rare values.\nTo address this, we introduce an inverse neural editor q(z | x′, x): given a prototype x′ and a revised sentence x, it generates edit vectors that are likely to\nmap x′ to x, concentrating probability on the few rare but important values of z.\nWe can then use the evidence lower bound (ELBO) to lower bound Equation 4:\nlog p(x|x′) ≥ Ez∼q(z|x′,x) [ log pedit(x | x′, z) ] ︸ ︷︷ ︸\nLgen\n− KL(q(z | x′, x) ‖ p(z))︸ ︷︷ ︸ LKL\ndef = ELBO(x, x′).\nSince Lgen is an expectation over q(z | x′, x) instead of p(x), it can be effectively Monte Carlo estimated by sampling z ∼ q(z | x′, x). The second term, LKL, penalizes the difference between q(z | x′, x) and p(x), which is necessary for the lower bound to hold. A thorough introduction to the ELBO is provided in Doersch (2016).\nNote that q(z | x′, x) and pedit(x | x′, z) combine to form a variational autoencoder (VAE) (Kingma and Welling, 2014), where q(z | x′, x) is the variational encoder and pedit(x | x′, z) is the variational decoder."
  }, {
    "heading": "3.3 Final objective",
    "text": "Combining the lower bounds LEX(x) and ELBO(x, x′), our final approximation of the log-likelihood is\n∑\nx′∈N (x) ELBO(x, x′).\nWe optimize this objective using stochastic gradient ascent with respect to Θ = (Θp,Θq), where Θp are the parameters for the neural editor and Θq are the parameters for the inverse neural editor."
  }, {
    "heading": "3.4 Model architecture",
    "text": "To recap, our model features three components: the neural editor pedit(x | x′, z), the edit prior p(z), and the inverse neural editor q(z | x′, x). We detail each of these components below.\nNeural editor pedit(x | x′, z). We implement our neural editor as a left-to-right sequence-to-sequence model with attention, where the prototype x′ is the\ninput sequence and the revised sentence x is the output sequence. We employ an encoder-decoder architecture similar to Wu (2016), extending it to condition on an edit vector z by concatenating z to the input of the decoder at each time step.\nThe prototype encoder is a 3-layer bidirectional LSTM. The inputs to each layer are the concatenation of the forward and backward hidden states of the previous layer, with the exception of the first layer, which takes word vectors initialized using GloVe (Pennington et al., 2014).\nThe decoder is a 3-layer LSTM with attention. At each time step, the hidden state of the top layer is used to compute attention over the top-layer hidden states of the prototype encoder. The resulting attention context vector is then concatenated with the decoder’s top-layer hidden state and used to compute a softmax distribution over output tokens.\nEdit prior p(z). We sample the edit vector z from the prior by first sampling its scalar length znorm ∼ Unif(0, 10) and then sampling its direction zdir (a unit vector) from the uniform distribution on the unit sphere. The resulting z = znorm ·zdir. As we will see later, this particular choice of the prior enables us to easily compute LKL.\nInverse neural editor q(z | x′, x). Given an edit pair (x′, x), the inverse neural editor must infer what vectors z are likely to map x′ to x.\nSuppose that x′ and x only differed by a single word w. Then one might propose that the edit vector z should be equal to the word vector for w. Generalizing this intuition to multi-word edits, we would like multi-word insertions to be represented as the sum of the inserted word vectors, and similarly for deletions.\nFormally, define I = x\\x′ to be the set of words added to x′, and D = x′\\x to be the words deleted. We represent the difference between x′ and x using the following vector:\nf ( x, x′ ) = ∑\nw∈I Φ (w)⊕\n∑ w∈D Φ (w)\nwhere Φ(w) is the word vector for word w and ⊕ denotes concatenation. The word embeddings Φ are parameters of q. In our work, we initialize Φ(w) to be 300-dimensional GloVe vectors.\nSince we construct our edit vectors as the sum of word vectors, and similarities between word vectors have traditionally been measured with cosine similarity, we design q to add noise to perturb the direction of the vector f . In particular, a sample from q is simply a perturbed version of f : obtained by adding von-Mises Fisher (vMF) noise, and we perturb the magnitude of f by adding uniform noise. We visualize this perturbation process in Figure 2.\nFormally, let fnorm = ‖f‖ and fdir = f/fnorm. Let vMF (v;µ, κ) denote a vMF distribution over points v on the unit sphere (i.e., directions) with mean vector µ and concentration parameter κ (in such a distribution, the log-likelihood of a point decays linearly with its cosine similarity to µ, and the rate of decay is controlled by κ). Finally, define:\nq(zdir | x′, x) = vMF (zdir; fdir, κ) q(znorm | x′, x) = Unif(znorm; [f̃norm, f̃norm + ])\nwhere f̃norm = min(fnorm, 10 − ) is the truncated norm. The resulting edit vector is z = zdir · znorm.\nThe inverse neural editor q is parameterized by the word vectors Φ and has hyperparameters κ and . Further details are provided in Section 3.5."
  }, {
    "heading": "3.5 Details of the inverse neural editor",
    "text": "Differentiating w.r.t. Θq. To maximize our training objective, we must be able to compute ∇ΘqELBO(x, x′) = ∇ΘqLgen −∇ΘqLKL.\nTo compute ∇ΘqLgen, we use a reparameterization trick. Specifically, we can rewrite z ∼ q(z | x′, x) as z = h(α) where h is a deterministic function differentiable with respect to Θq and α ∼ p(α) is an auxiliary random variable not depending on Θq (the details of h and α are given in Appendix 6). We\ncan then write:\n∇ΘqLgen = ∇ΘqEz∼q(z|x′,x) [ log pedit(x | x′, z) ]\n= Eα∼p(α) [ ∇Θq log pedit(x | x′, h(α)) ] .\nThis moves the derivative inside the expectation. The inner derivative can now be computed via standard backpropagation.\nNext, we turn to∇ΘqLKL. First, note that:\nLKL = KL(q(znorm|x′, x)‖p(znorm)) + KL(q(zdir|x′, x)‖p(zdir)). (5)\nIt is easy to verify that the first KL term does not depend on Θq. The second term has the closed form\nKL(vMF(µ, κ)‖vMF(µ, 0)) = κ Id/2(κ) + Id/2−1(κ)\nd−2 2κ\nId/2−1(κ)− d−22κ − log(Id/2−1(κ))− log(Γ(d/2)) + log(κ)(d/2− 1)− (d− 2) log(2)/2, (6)\nwhere In(κ) is the modified Bessel function of the first kind, Γ is the gamma function, and d is the dimensionality of f . We can see that this too is constant with respect to Θq via the following intuition: both the KL divergence and the prior do not change under rotations, and thus we can see KL(vMF(µ, κ)‖vMF(µ, 0))) = KL(vMF(e1, κ)‖vMF(e1, 0))) by rotating µ to the first canonical basis vector. Hence∇ΘqLKL = 0.\nComparison with existing VAE encoders. Our design of q differs from the typical choice of a standard normal distribution (Bowman et al., 2016; Kingma and Welling, 2014) for two reasons:\nFirst, by construction, edit vectors are sums of word vectors and since cosine distances are traditionally used to measure distances between word vectors, it would be natural to encode distances between edit vectors by the cosine distance. The vonMises Fisher distribution captures this idea, as the log likelihood decays with cosine similarity.\nSecond, our design of q allows us to explicitly control the tradeoff between the two terms in our objective, Lgen and LKL. Note from equations 5 and 6 that LKL is purely a function of the hyperparameters and κ, and can thus be controlled exactly. By taking κ→ 0 and to the maximum norm, we can drive\nLKL arbitrarily close to 0. As a tradeoff, smaller values of κ produce a noisier edit vector, leading to a smaller Lgen. We find a good balance by tuning κ.\nIn contrast, when using a Gaussian variational encoder, the KL term takes a different value per example and cannot be explicitly controlled. Consequently, Bowman et al. (2016) and others have observed that training tends to aggressively drive these KL terms to zero, leading to uninformative values of z — even when multiplying LKL by a carefully tuned and annealed importance weight."
  }, {
    "heading": "4 Experiments",
    "text": "We divide our experimental results into two parts. In Section 4.2, we evaluate the merits of the prototypethen-edit model as a generative modeling strategy, measuring its improvements on language modeling (perplexity) and generation quality (human evaluations of diversity and plausibility). In Section 4.3, we focus on the semantics learned by the model and its latent edit vector space. We demonstrate that it possesses interpretable semantics, enabling us to smoothly control the magnitude of edits, incrementally optimize sentences for target properties, and perform analogy-style sentence transformations."
  }, {
    "heading": "4.1 Datasets",
    "text": "We evaluate perplexity on the Yelp review corpus (YELP, Yelp (2017)) and the One Billion Word Language Model Benchmark (BILLIONWORD, Chelba (2013)). For qualitative evaluations of generation quality and semantics, we focus on YELP as our primary test case, as we found that human judgments of semantic similarity were much better calibrated in this focused setting.\nFor both corpora, we used the named-entity recognizer (NER) in spaCy2 to replace named entities with their NER categories. We replaced tokens outside the top 10,000 most frequent tokens with an “out-of-vocabulary” token."
  }, {
    "heading": "4.2 Generative modeling",
    "text": "We compare NEURALEDITOR as a language model against the following baseline language models:\n1. NLM: a standard left-to-right neural language model generating from scratch. For fair com-\n2honnibal.github.io/spaCy\nparison, we use the exact same architecture as the decoder of NEURALEDITOR.\n2. KN5: a standard 5-gram Kneser-Ney language model in KenLM (Heafield et al., 2013).\n3. MEMORIZATION: generates by sampling a sentence from the training set.\nPerplexity. We start by evaluating NEURALEDITOR’s value as a language model, measured in terms of perplexity. We use the likelihood lower bound in Equation 3, where we sum over training set instances within Jaccard distance < 0.5, and for the VAE term in NEURALEDITOR, we use the onesample approximation to the lower bound used in Kingma (2014) and Bowman (2016).\nTo evaluate NEURALEDITOR’s perplexity, we use linear smoothing with NLM to account for rare sentences not within our Jaccard distance threshold. This smoothing corresponds to occasionally sampling a special prototype sentence that can be edited into any other sentence and we use a smoothing weight of 0.1 (for full details, see Appendix 6). We find NEURALEDITOR improves perplexity over NLM and KN5. Table 1 shows that this is the case for both YELP and the more general BILLIONWORD, which contains substantially fewer test-set sentences close to the training set. On YELP, we surpass even the best ensemble of NLM and KN5, while on BILLIONWORD we nearly match their performance.\nComparing each model at a per-sentence level, we see that NEURALEDITOR drastically improves loglikelihood for a significant number of sentences in the test set (Figure 3). Proximity to a prototype seems to be the chief determiner of NEURALEDITOR’s performance.\nSince NEURALEDITOR draws its strength from sentences in the training set, we also compared against a simpler alternative, in which we ensemble NLM and MEMORIZATION (retrieval without edits). NEURALEDITOR performs dramatically better than this alternative. Table 2 also qualitatively demonstrates that sentences generated by NEURALEDITOR are substantially different from the original prototypes.\nHuman evaluation. We now turn to human evaluation of generation quality, focusing on grammaticality and plausibility. We evaluated plausibility by asking human raters, “How plausible is it for this sentence to appear in the corpus?” on a scale of 1– 3. We evaluate generations from NEURALEDITOR against an NLM with a temperature parameter on the per-token softmax3 as well as a baseline which generates sentences by randomly sampling from the training set and replacing synonyms, where the probability of substitution follows exp(sij/τ), where sij is the cosine similarity between the original word and its synonym according to GloVe word vectors.\nDecreasing the temperature parameter below 1 is 3 If si is the softmax logit for tokenwi and τ is a temperature parameter, the temperature-adjusted distribution is p(wi) ∝ exp(si/τ).\na popular technique for suppressing incoherent and ungrammatical sentences. Many NLM systems have noted an undesirable tradeoff between grammaticality and diversity, where a temperature low enough to enforce grammaticality results in short and generic utterances (Li et al., 2016).\nFigure 4 illustrates that both the grammaticality and plausibility of NEURALEDITOR without any temperature annealing is on par with the best tuned temperature for NLM, with a far higher diversity, as measured by the discrete entropy over unigram frequencies. We also find that decreasing the temperature of NEURALEDITOR can be used to slightly improve the grammaticality, without substantially reducing the diversity of the generations.\nComparing with the synonym substitution model, we find both models have high plausibility, since synonym substitution maintains most of the words, but low grammaticality compared to both NEURALEDITOR and the NLM. Additionally, applying synonym substitutions to training examples has extremely low coverage – none of the sentences in the test set can be generated via synonym substitution, and thus this baseline has higher perplexity than all other baselines in Table 1.\nA key advantage of edit-based models thus emerges: Prototypes sampled from the training set organically inject diversity into the generation process, even if the temperature of the decoder in NEURALEDITOR is zero. Hence, we can keep the decoder at a very low temperature to maximize grammaticality and plausibility, without sacrificing diversity. In contrast, a zero temperature NLM would collapse to outputting one generic sentence.\nThis also suggests that the temperature parameter for NEURALEDITOR captures a more natural notion of diversity — a temperature of 1.0 encourages more aggressive extrapolation from the training set\nwhile lower temperatures favor more conservative mimicking. This is likely to be more useful than the tradeoff for generation-from-scratch, where low temperature also affects the diversity of generations.\nCategorizing edits. To better understand the behavior of NEURALEDITOR, we measured the frequency with which random edits from NEURALEDITOR matched known syntactic transformations.\nWe use the rule-based transformations defined in He (2015) as our set of transformations to test, and search the corpus for sentences where these rules can be applied. We then apply the rule-based transformation, and measure the log-likelihood that NEURALEDITOR generates the transformed outputs. We find that the edit model assigns relatively high probability to the identity map (no edits), followed by simple reordering transformations such as reordering to/that Clauses (It is ADJP to/that SBAR/S → To S/BARS is ADJP). Of the rules, active / passive receives the lowest probability, partially due to the rarity of passive voice sentences in the Yelp corpus (Table 3).\nIn all cases, the model assigns substantially higher probability to these rule-based transformations over editing to random sentences or shuffling the tokens randomly to match the Levenstein distance of each rule-based transform."
  }, {
    "heading": "4.3 Semantics of NEURALEDITOR",
    "text": "In this section, we investigate the learned semantics of NEURALEDITOR, focusing on the two desiderata discussed in Section 2: semantic smoothness, and consistent edit behavior.\nIn order to establish a baseline for these properties, we consider existing sentence generation techniques which can sample semantically similar sentences. The most similar language modeling approach which can capture semantics is the sentence\nvariational autoencoder (SVAE) which imposes semantic structure onto a latent vector space, but uses the latent vector to represent the entire sentence, rather than just an edit.\nTo use the SVAE to “edit” a target sentence into a semantically similar sentence, we perturb its underlying latent sentence vector and then decode the result back into a sentence — the same method used in Bowman et al. (2016).\nSemantic smoothness. A good editing system should have fine-grained control over the semantics of a sentence: i.e., each edit should only alter the semantics of a sentence by a small and well-controlled amount. We call this property semantic smoothness.\nTo study smoothness, we first generate an “edit sequence” by randomly selecting a prototype sentence, and then repeatedly editing via NEURALEDITOR (with edits drawn from the edit prior p(z)) to produce a sequence of revisions. We then ask human annotators to rate the size of the semantic changes between revisions. An example is given in Table 4.\nWe compare to two baselines, one based upon the sentence variational autoencoder (SVAE) and another baseline which simply samples similar sentences from the training set according to average word vector similarity (COSINE).\nFor SVAE, we generate a similar sequence of sentences by first encoding the prototype sentence, and then decoding after the addition of a random Gaussian with variance 0.4.4 This process is repeated to produce a sequence of sentences which we can view as the SVAE equivalent of the edit sequence.\nFor COSINE, we generate sentences from the training set using exponentiated cosine similarity be-\n4The variance was selected so that SVAE and NEURALEDITOR have the same average human similarity judgement between two successive sentences. This avoids situations where SVAE produces completely unrelated sentence due to the perturbation size.\ntween averaged word vectors. The temperature parameter for the exponential was selected as before to match the average human similarity judgement.\nFigure 5 shows that NEURALEDITOR frequently generates paraphrases despite being trained on lexical similarity, and only 1% of edits are unrelated from the prototype. In contrast, SVAE often repeats sentences exactly, and when it makes an edit it is equally likely to generate unrelated sentences. COSINE performs even worse likely due to the difficulty of retrieving similar sentences for rare and long sentences.\nLess Similar By Turk EvaluationDegenerate\nFigure 5: Compared with baselines, NEURALEDITOR frequently generates paraphrases and similar sentences while avoiding unrelated and degenerate ones.6\nQualitatively (Table 4), NEURALEDITOR seems\nto generate long, diverse sentences which smoothly change over time, while the SVAE biases towards short sentences with several semantic jumps, presumably due to the difficulty of training a sufficiently informative SVAE encoder.\nSmoothly controlling sentences. We now show that we can selectively choose edits sampled from NEURALEDITOR to incrementally optimize a sentence towards desired attributes. This task serves as a useful measure of semantic coverage: if an edit model has high coverage over sentences that are semantically similar to a prototype, it should be able to satisfy the target attribute while deviating minimally from the prototype’s original meaning.\nWe focus on controlling two simple attributes: compressing a sentence to below a desired length (e.g., 7 words), and inserting a target keyword into the sentence (e.g., “service” or “pizza”).\nGiven a prototype sentence, we try to discover a semantically similar sentence satisfying the target\n6 545 similarity assessments pairs were collected through Amazon Mechanical Turk following Agirre (2014), with the same scale and prompt. Similarity judgements were converted to descriptions by defining Paraphrase (5), Roughly Equivalent (4-3), Same Topic (2-1), Unrelated (0).\nattribute using the following procedure: First, we generate 1,000 edit sequences using the procedure described earlier. Then, we select the sequence with highest likelihood whose endpoint possesses the target attribute. We repeat this process for a large number of prototypes.\nWe use almost the same procedure for the SVAE, but instead of selecting by highest likelihood, we select the sequence whose endpoint has shortest latent vector distance from the prototype (as this is the SVAE’s metric of semantic similarity).\nIn Figure 6, we then aggregate the sentences from the collected edit sequences, and plot their semantic similarity to the prototype against their success in satisfying the target attribute. Not surprisingly, as target attribute satisfaction rises, semantic similarity drops. However, we also see that NEURALEDITOR sacrifices less semantic similarity to achieve the same level of attribute satisfaction as SVAE. SVAE is reasonable on tasks involving common words (such as the word service), but fails when the model is asked to generate rarer words such as pizza. Examples from these word inclusion problems show that SVAE often becomes stuck generating short, generic sentences (Table 5).\nConsistent edit behavior: sentence analogies. In the previous results, we showed that edit models learn to generate semantically similar sentences. We now assess whether the edit vector possesses glob-\nally consistent semantics. Specifically, applying the same edit vector to different sentences should result in semantically analogous edits.\nFor example, if we have an edit vector which edits the sentence x1 = “this was a good restaurant” into x2 = “this was the best restaurant”. Given a new sentence y1 = “The cake was great”, we expect applying the same edit vector to result in y2 = “The cake was the greatest”.\nFormally, suppose we have two sentences, x1 and x2, which are related by some underlying semantic relation r. Given a new sentence y1, we would like to find a y2 such that the same relation r holds between y1 and y2.\nOur approach is to estimate the edit vector between x1 and x2 as ẑ = f(x1, x2) — the mode of the inverse neural editor q. We then apply this edit vector to y1 using the neural editor to yield ŷ2 = argmaxxpedit(x | y1, ẑ).\nSince it is difficult to output ŷ2 exactly matching y2, we take the top k candidate outputs of pedit (using beam search) and evaluate whether the gold y2 appears among the top k elements.\nWe generate the semantic relations r using prior evaluations for word analogies (Mikolov et al., 2013a; Mikolov et al., 2013b). We leverage these to generate a new dataset of sentence analogies, using a simple strategy: given an analogous word pair (w1, w2), we mine the Yelp corpus for sentence pairs\n(x1, x2) such that x1 is transformed into x2 by insertingw1 and removingw2 (allowing for reordering and inclusion/exclusion of stop words).\nFor this task, we initially compared against the SVAE, but it had a top-k accuracy close to zero. Hence, we instead compare to SAMPLING which is a baseline which randomly samples an edit vector ẑ ∼ p(z), instead using ẑ derived from f(x1, x2).\nWe also compare our accuracies to the simpler task of solving the word, rather than sentence-level analogies in (Mikolov et al., 2013a) using GloVe. This task is substantially simpler, since the goal is to identify a single word (such as “good:better::bad:?”) instead of an entire sentence. Despite this, the top10 performance of our model in Table 6 is nearly as good as the performance of GloVe vectors on the simpler lexical analogy task. In some categories, NEURALEDITOR at top-10 actually performs better than word vectors, since NEURALEDITOR has an understanding of which words are likely to appear in the context of a Yelp review. Examples in Table 7 show the model is accurate and captures lexical analogies requiring word reorderings."
  }, {
    "heading": "5 Related work and discussion",
    "text": "Our work connects with a broad literature on attention-based neural models, retrieval-augmented text generation, semantically meaningful representations, and nonparametric statistics.\nBased upon recurrent neural networks and sequence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their flexibility and performance across a wide range of NLP tasks\n(Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more information from the input sequence (Vaswani et al., 2017). Our work extends the applicability of attention mechanisms beyond sequence-to-sequence models by allowing models to attend to randomly sampled sentences.\nThere is a growing literature on applying retrieval mechanisms to augment text generation models. For example, in the image captioning literature, Hodosh (2013), Kuznetsova (2013) and Mason (2014) proposed to generate image captions by first retrieving a prototype caption based on an image context, and then applying sentence compression to tailor the prototype to a particular image. More recently, Song (2016) ensembled a retrieval system and an NLM for dialogue, using the NLM to transform the retrieved utterance, and Gu (2017) used an off-the-shelf search engine system to retrieve and condition on training set examples. Although these approaches also edit text from the training set, these papers solve a fundamentally different problem since they solve conditional generation problems, and retrieve prototypes based on a context, where as our task is unconditional and thus there is no context which we can use to retrieve.\nOur work treats the prototype x′ as a latent variable rather than being given by a retrieval mechanism, and marginalizes over all possible prototypes — a challenge which motivates our new lexical similarity training method in Section 3.1. Practically,\nmarginalization over x′ makes our model attend to training examples based on similarity of output sequences, while prior retrieval models attend to examples based on similarity of the input sequences.\nIn terms of generation techniques that capture semantics, the sentence variational autoencoder (SVAE) (Bowman et al., 2016) is closest to our work in that it attempts to impose semantic structure on a latent vector space. However, the SVAE’s latent vector is meant to represent the entire sentence, whereas the neural editor’s latent vector represents an edit. Our results from Section 4.3 suggest that local variation over edits is easier to model than global variation over sentences.\nOur use of lexical similarity neighborhoods is comparable to context windows in word vector training (Mikolov et al., 2013a). More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics (Tenenbaum et al., 2000; Hashimoto et al., 2016).\nFrom a generative modeling perspective, editing randomly sampled training sentences closely resembles nonparametric kernel density estimation (Parzen, 1962) where one samples points from a training set, and adds noise to smooth the density. Our edit model is the text equivalent of Gaussian noise, and our training mechanism is a type of learned smoothing kernel.\nPrototype-then-edit is a semi-parametric approach that remembers the entire training set and uses a neural editor to generalize meaningfully beyond the training set. The training set provides a strong inductive bias — that the corpus can be characterized by prototypes surrounded by semantically similar sentences reachable by edits. Beyond improvements on generation quality as measured by perplexity, the approach also reveals new semantic structures via the edit vector.\nReproducibility. All code, data and experiments are available on the CodaLab platform at https: //bit.ly/2rHsWAX.\nAcknowledgements. We thank the reviewers and editor for their insightful comments. This work was funded by DARPA CwC program under ARO prime contract no. W911NF-15-1-0462."
  }, {
    "heading": "6 Appendix",
    "text": "Construction of the LSH. The LSH maps a sentence to lexically similar sentences in the corpus, representing a graph over sentences. We apply breadth-first search (BFS) over the LSH sentence graph started at randomly selected seed sentences and uniformly sample this set to form the training set.\nReparameterization trick for q. First, note that we can write znorm ∼ q(znorm|x′, x) as znorm = hnorm(αnorm) def = f̃norm + αnorm where αnorm ∼ Unif(0, ). Furthermore, Wood (1994) present a function hdir and auxiliary random variable αdir, such that zdir = hdir(αdir) is distributed according to a vMF with mean f and concentration κ. We can then define z = h(α) def= hdir(αdir) · hnorm(αnorm).\nSmoothing for language models. As a language model, NEURALEDITOR does not place probability on any test sentence which is sufficiently dissimilar from all training set sentences. In order to avoid this problem, we can consider a special prototype sentence ‘∅’ which can be edited into any sentence, and draw this special prototype with probability p∅. Concretely, we write:\np(x) = ∑\nx′∈X∪{∅} pedit(x|x′)pprior(x′)\n= (1− p∅) ∑\nx′∈X\n1\n|X |pedit(x|x ′) + p∅ pNLM(x).\nThis linearly smoothes between our edit model (pedit) and the NLM (pNLM) since our decoder is identical to the NLM, and thus conditioning on the special ∅ token reduces to using a NLM.\nEmpirically, we observe that even small values of p∅ produces low perplexity (Figure 7) corresponding to the observation that smoothing of NEURALEDITOR is only necessary to avoid degenerate loglikelihoods on a very small subset of the test set."
  }],
  "year": 2018,
  "references": [{
    "title": "SemEval-2014 Task 10: Multilingual semantic textual similarity",
    "authors": ["E. Agirre", "C. Banea", "C. Cardie", "D.M. Cer", "M.T. Diab", "A. Gonzalez-Agirre", "W. Guo", "R. Mihalcea", "G. Rigau", "J. Wiebe."],
    "venue": "International Conference on Computational Linguistics (COLING),",
    "year": 2014
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2015
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."],
    "venue": "Journal of machine learning research, 3(0):1137–1155.",
    "year": 2003
  }, {
    "title": "Generating sentences from a continuous space",
    "authors": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio."],
    "venue": "Computational Natural Language Learning (CoNLL), pages 10–21.",
    "year": 2016
  }, {
    "title": "One billion word benchmark for measuring progress in statistical language modeling",
    "authors": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson."],
    "venue": "arXiv preprint arXiv:1312.3005.",
    "year": 2013
  }, {
    "title": "Tutorial on variational autoencoders",
    "authors": ["C. Doersch."],
    "venue": "arXiv preprint arXiv:1606.05908.",
    "year": 2016
  }, {
    "title": "Search engine guided non-parametric neural machine translation",
    "authors": ["J. Gu", "Y. Wang", "K. Cho", "V.O. Li."],
    "venue": "arXiv preprint arXiv:1705.07267.",
    "year": 2017
  }, {
    "title": "The challenges of automatic summarization",
    "authors": ["U. Hahn", "I. Mani."],
    "venue": "Computer, 33.",
    "year": 2000
  }, {
    "title": "Word embeddings as metric recovery in semantic spaces",
    "authors": ["T.B. Hashimoto", "D. Alvarez-Melis", "T.S. Jaakkola."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL), 4:273–286.",
    "year": 2016
  }, {
    "title": "Writing research and the writer",
    "authors": ["J.R. Hayes", "L.S. Flower."],
    "venue": "American psychologist, 41(10):1106–1113.",
    "year": 1986
  }, {
    "title": "Syntax-based rewriting for simultaneous machine translation",
    "authors": ["H. He", "A.G. II", "J. Boyd-Graber", "H.D. III."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 55–64.",
    "year": 2015
  }, {
    "title": "Scalable modified Kneser-Ney language model estimation",
    "authors": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn."],
    "venue": "Association for Computational Linguistics (ACL), pages 690–696.",
    "year": 2013
  }, {
    "title": "Framing image description as a ranking task: Data, models and evaluation metrics",
    "authors": ["M. Hodosh", "P. Young", "J. Hockenmaier."],
    "venue": "Journal of Artificial Intelligence Research (JAIR), 47:853–899.",
    "year": 2013
  }, {
    "title": "An introduction to variational methods for graphical models",
    "authors": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul."],
    "venue": "Machine Learning, 37:183–233.",
    "year": 1999
  }, {
    "title": "Recurrent continuous translation models",
    "authors": ["N. Kalchbrenner", "P. Blunsom."],
    "venue": "Empirical Methods in",
    "year": 2013
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D.P. Kingma", "M. Welling."],
    "venue": "arXiv preprint arXiv:1312.6114.",
    "year": 2014
  }, {
    "title": "Generalizing image captions for image-text parallel corpus",
    "authors": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi."],
    "venue": "Association for Computational Linguistics (ACL), pages 790–796.",
    "year": 2013
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan."],
    "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages 110–119.",
    "year": 2016
  }, {
    "title": "Domain-specific image captioning",
    "authors": ["R. Mason", "E. Charniak."],
    "venue": "Computational Natural Language Learning (CoNLL), pages 2–10.",
    "year": 2014
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["T. Mikolov", "K. Chen", "G. Corrado", "Jeffrey."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Linguistic regularities in continuous space word representations",
    "authors": ["T. Mikolov", "W. Yih", "G. Zweig."],
    "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), volume 13, pages 746–751.",
    "year": 2013
  }, {
    "title": "On estimation of a probability density function and mode",
    "authors": ["E. Parzen."],
    "venue": "Annals of Mathematical Statistics, 33:1065–1076.",
    "year": 1962
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C.D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Data-driven response generation in social media",
    "authors": ["A. Ritter", "C. Cherry", "W.B. Dolan."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 583–593.",
    "year": 2011
  }, {
    "title": "Generating high-quality and informative conversation responses with sequence-tosequence models",
    "authors": ["L. Shao", "S. Gouws", "D. Britz", "A. Goldie", "B. Strope", "R. Kurzweil."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 2210–2219.",
    "year": 2017
  }, {
    "title": "Two are better than one: An ensemble of retrievaland generation-based dialog systems",
    "authors": ["Y. Song", "R. Yan", "X. Li", "D. Zhao", "M. Zhang."],
    "venue": "arXiv preprint arXiv:1610.07149.",
    "year": 2016
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["I. Sutskever", "O. Vinyals", "Q.V. Le."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3104–3112.",
    "year": 2014
  }, {
    "title": "A global geometric framework for nonlinear dimensionality reduction",
    "authors": ["J.B. Tenenbaum", "V.D. Silva", "J.C. Langford."],
    "venue": "Science, pages 2319–2323.",
    "year": 2000
  }, {
    "title": "Simulation of the von mises fisher distribution",
    "authors": ["A.T. Wood."],
    "venue": "Communications in statistics-simulation and computation, pages 157–164.",
    "year": 1994
  }, {
    "title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
    "authors": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"],
    "year": 2016
  }, {
    "title": "Yelp Dataset Challenge, Round 8",
    "authors": ["Yelp."],
    "venue": "https: //www.yelp.com/dataset_challenge.",
    "year": 2017
  }],
  "id": "SP:95d46f1ad908554e50f2a05f0e45cf9f805a32fe",
  "authors": [{
    "name": "Kelvin Guu",
    "affiliations": []
  }, {
    "name": "Tatsunori B. Hashimoto",
    "affiliations": []
  }, {
    "name": "Yonatan Oren",
    "affiliations": []
  }, {
    "name": "Percy Liang",
    "affiliations": []
  }],
  "abstractText": "We propose a new generative language model for sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional language models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-thenedit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.",
  "title": "Generating Sentences by Editing Prototypes"
}