{
  "sections": [{
    "text": "∑k i=1 max{0,w>i x} with respect\nto the squared loss. We provide a computerassisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once 6 ≤ k ≤ 20. By a concentration of measure argument, this implies that in high input dimensions, nearly all target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an overparameterization assumption is necessary to get a positive result in this setting."
  }, {
    "heading": "1. Introduction",
    "text": "One of the biggest mysteries of deep learning is why neural networks are successfully trained in practice using gradient-based methods, despite the inherent nonconvexity of the associated optimization problem. For example, non-convex problems can have poor local minima, which will cause any local search method (and in particular, gradient-based ones) to fail. Thus, it is natural to ask what types of assumptions, in the context of training neural networks, might mitigate such problems. For example, recent work has shown that other non-convex learning problems, such as phase retrieval, matrix completion, dictionary\n1Weizmann Institute of Science, Rehovot, Israel. Correspondence to: Itay Safran <itay.safran@weizmann.ac.il>, Ohad Shamir <ohad.shamir@weizmann.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nlearning, and tensor decomposition, do not have spurious local minima under suitable assumptions, in which case local search methods have a chance of succeeding (e.g., (Ge et al., 2015; Sun et al., 2015; Ge et al., 2016; Bhojanapalli et al., 2016)). Is it possible to prove similar positive results for neural networks?\nIn this paper, we focus on perhaps the simplest non-trivial ReLU neural networks, namely predictors of the form\nx 7→ k∑ i=1 [w>i x]+\nfor some k > 1, where [z]+ = max{0, z} is the ReLU function, x is a vector in Rd, and w1, . . . ,wk are parameter vectors. We consider directly optimizing the expected squared loss, where the input is standard Gaussian, and in the realizable case – namely, that the target values are generated by a network of a similar architecture:\nmin w1,...,wk Ex∼N (0,I) 1 2 ( k∑ i=1 [w>i x]+ − k∑ i=1 [v>i x]+ )2  . (1) Note that here, the choice wi = vσ(i) (for all i = 1, . . . , k and any permutation σ) is a global minimum with zero expected loss. Several recent papers analyzed such objectives, in the hope of showing that it does not suffer from spurious local minima (see related work below for more details).\nOur main contribution is to prove that unfortunately, this conjecture is false, and that Eq. (1) indeed has spurious local minima once 6 ≤ k ≤ 20. Moreover, this is true even if the dimension is arbitrarily large, and even if we assume that v1, . . . ,vk are orthonormal vectors. In fact, since in high dimensions randomly-chosen vectors are approximately orthogonal, and the landscape of the objective function is robust to small perturbations, we can show that spurious local minima exist for nearly all neural network problems as in Eq. (1), in high enough dimension (with respect to, say, a Gaussian distribution over v1, . . . ,vk). Moreover, we show experimentally that these local minima are not pathological, and that standard gradient descent can easily get trapped in them, with a probability which seems to increase towards 1 with the network size.\nOur proof technique is a bit unorthodox. Although it is possible to write down the gradient of Eq. (1) in closed form (without the expectation), it is not clear how to get analytical expressions for its roots, and hence characterize the stationary points of Eq. (1). As far as we know, an analytical expression for the roots might not even exist. Instead, we employed the following strategy: We ran standard gradient descent with random initialization on the objective function, until we reached a point which is both suboptimal (function value being significantly higher than 0); approximate stationary (gradient norm very close to 0); and with a strictly positive definite Hessian (with minimal eigenvalue significantly larger than 0). We use a computer to verify these conditions in a formal manner, avoiding floatingpoint arithmetic and the possibility of rounding errors. Relying on these numbers, we employ a Taylor expansion argument, to show that we must have arrived at a point very close to a local (non-global) minimum of Eq. (1), hence establishing the existence of such minima.\nOn the more positive side, we show that an additional overparameterization assumption appears to be very effective in mitigating these local minima issues: Namely, we use a network larger than that needed with unbounded computational power, and replace Eq. (1) with\nmin w1,...,wk Ex∼N (0,I) 1 2 ( n∑ i=1 [w>i x]+ − k∑ i=1 [v>i x]+ )2  , (2) where n > k. In our experiments with k, n up to size 20, we observe that whereas n = k leads to plenty of local minima, n = k+ 1 leads to much fewer local minima, whereas no local minima were encountered once n ≥ k + 2 (although those might still exist for larger values of k, n than those we tried). Thus, although Eq. (1) has local minima, we conjecture that Eq. (2) might still be proven to have no bad local minima, but this would necessarily require n to be sufficiently larger than k.\nThe paper is structured as follows: After surveying related work below, we provide our main results and proof ideas in Sec. 2. Sec. 3 provides additional experimental details about the local minima found, as well empirical evidence about the likelihood of reaching them using gradient descent. Some of the proofs are provided in Sec. 4, with the rest provided in the appendix."
  }, {
    "heading": "1.1. Related Work",
    "text": "There is a large and rapidly increasing literature on the optimization theory of neural networks, surveying all of which is well outside our scope. Thus, in this subsection, we only briefly survey the works most relevant to ours.\nWe begin by noting that when minimizing the average\nloss over some arbitrary finite dataset, it is easy to construct problems where even for a single neuron (k = 1 in Eq. (1)), there are many spurious local minima (e.g., (Auer et al., 1996; Swirszcz et al., 2016)). Moreover, the probability of starting at a basin of such local minima is exponentially high in the dimension (Safran & Shamir, 2016). On the other hand, it is known that if the network is over-parameterized, and large enough compared to the data size, then there are no local minima (Poston et al., 1991; Livni et al., 2014; Haeffele & Vidal, 2015; Zhang et al., 2016; Soudry & Carmon, 2016; Soltanolkotabi et al., 2017; Nguyen & Hein, 2017; Boob & Lan, 2017). In any case, neither these positive nor negative results apply here, as we are interested in the expected (population) loss with respect to the Gaussian distribution, which is of course nondiscrete. Also, several recent works have studied learning neural networks under a Gaussian distribution assumption (e.g., Janzamin et al. (2015); Brutzkus & Globerson (2017); Du et al. (2017); Li & Yuan (2017); Feizi et al. (2017); Zhang et al. (2017); Ge et al. (2017)), but using a network architecture different than ours, or focusing on algorithms rather than the geometry of the optimization problem. Finally, Shamir (2016) provides hardness results for training neural networks even under distributional assumptions, but these do not apply when making strong assumptions on both the input distribution and the network generating the data, as we do here.\nFor Eq. (1), a few works have shown that there are no spurious local minima, or that gradient descent will succeed in reaching a global minimum, provided the vi vectors are in general position or orthogonal (Zhong et al., 2017; Soltanolkotabi et al., 2017; Tian, 2017). However, these results either apply only to k = 1, assume the algorithm is initialized close to a global optimum, or analyze the geometry of the problem only on some restricted subset of the parameter space.\nThe empirical observation that gradient-based methods may not work well on Eq. (1) has been made in Livni et al. (2014), and more recently in Ge et al. (2017). Moreover, Livni et al. (2014) empirically observed that overparameterization seems to help. However, our focus here is to prove the existence of such local minima, as well as more precisely quantify their behavior as a function of the network sizes."
  }, {
    "heading": "2. Main Result and Proof Technique",
    "text": "Before we begin, a small note on terminology: When referring to local minima of a function F on Euclidean space, we always mean spurious local minima (i.e., points w such that infw F (w) < F (w) ≤ F (w′) for all w′ in some open neighborhood of w).\nOur basic result is the following:\nTheorem 1. Consider the optimization problem\nmin w1,...,wn∈Rk\nEx 1 2 ( n∑ i=1 [w>i x]+ − k∑ i=1 [v>i x]+ )2  , where x ∼ N (0, I) and v1, . . . ,vk are orthogonal unit vectors in Rk. Then for n = k ∈ {6, 7, . . . , 20}, as well as (k, n) ∈ {(8, 9) , (10, 11) , (11, 12) , . . . , (19, 20)}, the objective function above has spurious local minima.\nRemark 1. For k, n smaller than 6, we were unable to find local minima using our proof technique, since gradient descent always seemed to converge to a global minimum. Also, although we have verified the theorem only up to k, n ≤ 20, the result strongly suggests that there are local minima for larger values as well. See Sec. 3 for some examples of the local minima found.\nThe theorem assumes a fixed input dimension, and a particular choice of v1, . . . ,vk. However, due to the fact that gradient descent is invariant to orthonormal reparameterizations, these assumptions are not necessary and can be relaxed, as demonstrated by the following corollary:\nCorollary 1. Thm. 1 also applies if the space Rk is replaced by Rd for any d > k (with x distributed as a standard Gaussian in that space). Moreover, if v1, . . . ,vk are chosen i.i.d. from a Gaussian distribution N (0, cI) (for any c > 0), the theorem still holds with probability at least 1− exp(−Ω(d)). Remark 2. The corollary is not specific to a Gaussian distribution over v1, . . . ,vk, and can be generalized to any distribution for which v1, . . . ,vk are approximately orthogonal and of the same norm in high dimensions (see below for details).\nWe now turn to explain how these results are derived, starting with Thm. 1. In what follows, we let wn1 = (w1, . . . ,wn) ∈ Rkn be the vector of parameters, and let F (wn1 ) be the objective function defined in Thm. 1 (assuming k, n are fixed). We will also assume that F is thrice-differentiable in a neighborhood of wn1 (which will be shown to be true as part of our proofs), with a gradient ∇F (·) and a Hessian ∇2F (·).\nClearly, a global minimum of F is obtained by wi = vi for all i = 1, . . . , k (and wi = 0 otherwise), in which case F attains a global minimum of 0. Thus, to prove Thm. 1, it is sufficient to find a point wn1 ∈ Rkn such that ∇F (wn1 ) = 0, ∇2F (wn1 ) 0, and F (wn1 ) > 0. The major difficulty is showing the existence of points where the first condition is fulfilled: Gradient descent allows us to find points where ∇F (wn1 ) ≈ 0, but it is very unlikely to return a point where ∇F (wn1 ) = 0 exactly. Instead, we\nuse a Taylor-expansion argument (detailed below), to show that if we found a point such that ∇F (wn1 ) is sufficiently close to 0, as well as ∇2F (wn1 ) 0 and F (wn1 ) > 0, then wn1 must be close to a local minimum.\nThe second-order Taylor expansion of a multivariate, thrice-differentiable function F about a point wn1 ∈ Rkn, in a direction given by a unit vector u ∈ Rkn and using a Lagrange remainder term, is given by\nF (wn1 + tu) =F (wn1 ) + t ∑ i1 ∂ ∂wn1,i1 F (wn1 )ui1\n+ 1 2 t2 ∑ i1,i2\n∂2\n∂wn1,i1∂w n 1,i2\nF (wn1 )ui1ui2\n+ 1 6 t3 ∑ i1,i2,i3\n∂3\n∂wn1,i1∂w n 1,i2 ∂wn1,i3 F (wn1 + ξu)ui1ui2ui3 ,\nfor some ξ ∈ (0, t), and where wn1,i denotes the i-th coordinate of wn1 . Denoting the remainder term as Rwn1 ,u, we have\nF (wn1 + tu) = F (w n 1 ) + t∇F (wn1 )\n> u\n+ 1\n2 t2u>∇2F (wn1 )u +\n1 6 t3Rwn1 ,u . (3)\nNow, suppose that the point wn1 we obtain by gradient descent satisfies ||∇F (wn1 )|| ≤ , ∇2F (wn1 ) λmin · I and |Rwn1 ,u| ≤ B (for some positive λmin, , B), uniformly for all unit vectors u. By the Taylor expansion above, this implies that for all unit u,\nF (wn1 + tu) ≥ F (wn1 )− t ||∇F (wn1 )|| · ||u||\n+ t2\n2 λmin ||u||2 −\nt3 6 B\n= F (wn1 )− t+ λmint\n2\n2 − Bt\n3\n6\n= F (wn1 ) + t\n( λmin\n2 t− B 6 t2 −\n) .\nAn elementary calculation reveals that the term t ( λmin 2 t− B 6 t 2 − )\nis strictly positive for any t in the open interval of\n3λmin ± √\n9λ2min − 24B 2B\n(and in particular, in the closed interval of 3λmin± √ 9λ2min−25B 2B ). This implies that there is some small closed ball B̄t of radius t > 0 centered at wn1 (and with boundary S), such that F (wn1 ) < minw′n1 ∈S F (w ′n 1 ). Moreover, since F is continuous, it is minimized over B̄t\nat some point w∗n1 . But then\nF (w∗n1 ) = min w′n1 ∈B F (w′n1 ) ≤ F (wn1 ) < min w′n1 ∈S F (w′n1 ),\n(4) so w∗n1 must reside in the interior of B̄t. Thus, it is minimal in an open neighborhood containing it, hence it is a local minimum. Overall, we have arrived at the following key lemma:\nLemma 1. Assume that ||∇F (wn1 )|| ≤ , ∣∣Rwn1 ,u∣∣ ≤ B for some , B > 0 and all unit vectors u, and let λmin > 0 denote the smallest eigenvalue of ∇2F (wn1 ). If 9λ2min − 25B ≥ 0, then the function F contains a local minimum, within a distance of at most\nr := 3λmin − √ 9λ2min − 25B 2B\nfrom wn1 .\nThe only missing element is that the local minimum might be a global minimum. To rule this out, one can simply use the fact that F is a Lipschitz function, so that if F (wn1 ) is much larger than 0, the neighboring local minimum can’t have a value of 0, and hence cannot be global:\nLemma 2. Under the conditions of Lemma 1, if it also holds that\nF (wn1 ) > r2 ( 1 2 + ( n2 − n )( (maxi ||wi||+ r) 2π (mini ||wi|| − r) + 1 2 ) +\nnk ·maxi ||vi|| 2π (mini ||wi|| − r) + r\n) , (5)\nthen the local minimum is non-global.\nThe formal proof of this lemma appears in Subsection A.3 in the appendix.\nMost of the technical proof of Thm. 1 consists in rigorously verifying the conditions of Lemma 1 and Lemma 2. A major hurdle is that floating-point calculations are not guaranteed to be accurate (due to the possibility of round-off and other errors), so for a formal proof, one needs to use software that comes with guaranteed numerical accuracy. In our work, we chose to use variable precision arithmetic (VPA), a standard package of MATLAB which is based on symbolic arithmetic, and allows performing elementary numerical computations with an arbitrary number of guaranteed digits of precision. The main technical issue we faced is that some calculations are not easily done with a few elementary arithmetical operations (in particular, the standard way to compute λmin would be via a spectral decomposition of the Hessian matrix). The bulk of the proof consists of showing how we bound the quantities relevant to Lemma 1 in an elementary manner.\nFinally, we turn to discuss how Corollary 1 is proven, given Thm. 1 (see Subsection 4.3 for a more formal derivation). The proof idea is that the objective does not have any “nontrivial” structure outside the span of v1, . . . ,vk. Therefore, if we take a local minima for Rk, and pad it with d− k zeros, we get a point in Rd for which the gradient’s norm is unchanged, the Hessian has the same spectrum for any d ≥ k + 1, and the third derivatives are still bounded. Hence, that point is a local minimum in the higher-dimensional problem as well. As to the second part of the corollary, the only property of the Gaussian distribution we need is that in high dimensions, if we sample v1, . . . ,vk, then we are overwhelmingly likely to get approximately orthogonal vectors with approximately the same norm. Hence, up to rotation and scaling, we get a small perturbation F̃ of the objective F considered in Thm. 1. Moreover, for large enough d, we can make the perturbation arbitrarily small, uniformly in some compact domain. Now, recall that we prove the existence of some local minimum w∗n1 , by showing that F (wn1 ) < minw′n1 ∈S F (w ′n 1 ) in some small sphere S enclosing wn1 . If the perturbations are small enough, we also have F̃ (wn1 ) < minw′n1 ∈S F̃ (w ′n 1 ), which by arguments similar to before, imply that wn1 is close to a local minimum of F̃ ."
  }, {
    "heading": "3. Experiments",
    "text": "So far, we proved the existence of local minima for the objective function in Eq. (2). However, this does not say anything about the likelihood of gradient descent to reach them. We now turn to study this question empirically.\nFor each value of (k, n), where k ∈ [20] and n ∈ {k, . . . , 20}, we ran 1000 instantiations of gradient descent on the objective in Eq. (2), each starting from a different random initialization 1. Each instantiation was ran with a fixed step size of 0.1, until reaching a candidate stationary point / local minima (the stopping criterion was that the gradient norm w.r.t. any wi is at most 10−9). Points obtaining objective values less than 10−3 were ignored as those are likely to be close to a global minimum. Interestingly, no points with value between 10−3 and 10−2 were found. For all remaining candidate points, we verified that the conditions in Lemmas 1 and 2 are met2 to\n1We used standard Xavier initialization: Each weight vector wi was samples i.i.d. from a Gaussian distribution in Rk, with zero mean and covariance 1\nk I.\n2 Since running our algorithm for all suspicious points found on all architectures is time consuming, we instead identified points that are equivalent up to permutations on the order of neurons and of the data coordinates, since the objective is invariant under such permutations. By bounding the maximal Euclidean distance between these points and using the Lipschitzness of the objective and its Hessian (see Thm. 4 and Lemma 7), this allowed us to run the algorithm on a single representative from a family of equivalent points and speed up the running time drastically. Also,\nconclude that these points are indeed close to spurious local minima (in all cases, the distance turned out to be less than 2 · 10−6). Our verification process included verifying thrice-differentiability in the enclosing balls containing the minima by asserting they contain no singular points, hence the objective is an analytical expression when restricted to these balls where differentiability follows.\nIn Tables 1 and 2, we summarize the percentage of instantiations which were verified to converge close to a spurious local minimum, as a function of k, n. We note that among candidate points found, only a tiny fraction could not be verified to be local minima (this only occured for network sizes (k, n) ∈ {(15, 16) , (17, 18) , (20, 20)}, and consist only 0.1%, 2.4%, 0.9% of the instantiations respectively). In the tables, we also provide the minimal eigenvalue of the Hessian of the objective, and the objective value (or equivalently, the optimization error) at the points found, averaged over the instantiations3. Note that since the minimal eigenvalue is strictly positive and varies slightly inside the enclosing ball, this indicates that these are in fact strict local minima. As the tables demonstrate, the probability of converging to a spurious local minimum increases rapidly with k, n, and suggests that it eventually becomes overwhelming as long as n ≈ k. However, on a positive note, mild overparameterization seems to remedy this, as no local minima were found for n ≥ k+ 2 where n ≤ 20, and local minima for n = k + 1 are much more scarce than for n = k. We leave the investigation of local minima for larger values of k, n to future work.\nIn Fig. 1, we show the distribution of the objective values obtained in the points found, over the 1000 instantiations of several architectures. The figure clearly indicates that apart from a higher chance of converging to local minima, larger architectures also tend to have worse values attained on these minima.\nFinally, in examples 1 and 2 below, we present some specific local minima found for n = k = 6 and k = 8, n = 9, and discuss their properties. We note that these are the smallest networks (with n = k and n 6= k respectively) for which we were able to find such points.\nExample 1. Out of 1000 gradient descent instantiations for n = k = 6, three converged close to a local minimum. All three were verified to be essentially identical (after permuting the neurons and up to an Euclidean distance\nthe objective was tested to be thrice-differentiable in all enclosing balls of radii returned by the algorithm. Specifically, we ensured that no two such balls intersect (which results in two identical neurons, where the objective is not thrice-differentiable) and that no ball contains the origin (which results in a neuron with weight 0, where again the objective is not thrice-differentiable).\n3Since all points are extremely close to a local minimum, the objective at the minimum is essentially the same, up to a deviation on order less than 1.1 · 10−9. Also, the minimal eigenvalues vary by at most 5.7 · 10−4.\nwhere the parameter vector of each of the 6 neurons corresponds to a column of the matrix denoted w61. The Hessian of the objective at w61, ∇2F ( w61 ) , was confirmed to have\nminimal eigenvalue λmin ( ∇2F ( w61 )) ≥ 0.004699. This implied that all three suspicious points found for n = k = 6 are of distance at most r = 1.12 · 10−7 from a local minimum with objective value at least 0.02508.\nExample 2. Out of 1000 gradient descent initializations for k = 8, n = 9, one converged to a local minimum. The\npoint found, denoted w91, is given below: 0.99 −0.03 . . . −0.03 −0.03 0.13 0.07 −0.03 0.99 . . . −0.03 −0.03 0.13 0.07 ... ... . . . ... ... ...\n... −0.03 −0.03 . . . 0.99 −0.03 0.13 0.07 −0.03 −0.03 . . . −0.03 0.99 0.13 0.07 0.23 0.23 . . . 0.23 0.23 −0.19 −0.49\n ,\nwhere the parameter vector of each of the 9 neurons corresponds to a column of w91. The Hessian of the objective at w91,∇2F ( w91 ) , was confirmed to have minimal eigenvalue\nλmin ( ∇2F ( w91 )) ≥ 0.005944. This implied that w91 is of distance at most r = 7.8 · 10−8 from a local minimum with objective value at least 0.02056.\nIt is interesting to note that the points found in examples 1 and 2, as well as all other local minima detected, have a nice symmetric structure: We see that most of the trained neurons are very close to the target neurons in most of the dimensions. Also, many of the entries appear to be the same. Surprisingly, although such constructions might seem brittle, these are indeed strict local minima. Moreover, the probability of converging to such points becomes very large as the network size increases as demonstrated by our experiments."
  }, {
    "heading": "4. Proofs of Thm. 1 and Corollary 1",
    "text": "In this section, we provide a formal proof of Corollary 1, as well as an outline of the proof of Thm. 1. We also provide closed-form expressions for the objective and its derivatives. Missing parts of the proofs are provided in the appendix.\nIn the proofs, we use bold-faced letters (e.g., w) to denote vectors, barred bold-faced letters (e.g., w̄) to denote vectors normalized to unit Euclidean norm, and capital letters to generally denote matrices. Given a natural number k, we let [k] be shorthand for {1, . . . , k}. Given a matrix M , ||M ||sp denotes its spectral norm."
  }, {
    "heading": "4.1. Proof of Thm. 1",
    "text": "To prove Thm. 1 for some (k, n), it is enough to consider some particular choice of orthogonal v1, . . . ,vk, since any other choice amounts to rotating or reflecting the same objective function (which of course does not change the existence or non-existence of its local minima). In particular, we chose these vectors to simply be the standard basis vectors in Rk.\nAs we show in Subsection 4.2 below, the objective function in Eq. (2) can be written in an explicit form (without the expectation term), as well as its gradients and Hessians. We first ran standard gradient descent, starting from random initialization and using a fixed step size of 0.1, till we reached a point wn1 , such that the gradient norm w.r.t. any wi is at most 10−9. Given this point, we use Lemma 1 and Lemma 2 to prove that it is close to a local minimum. Specifically, we built code which does the following:\n1. Provide a rigorous upper bound on the norm of the gradient at a given point wn1 (since we have a closedform expression for the gradient, this only requires elementary calculations).\n2. Provide a rigorous lower bound on the minimal eigenvalue of ∇2F (wn1 ): This is the technically most demanding part, and the derivation of the algorithm is presented in Subsection A.1 in the appendix.\n3. Provide a rigorous upper bound B on the remainder term Rwn1 ,u (see Subsection A.2 in the appendix for the relevant calculations).\n4. Provide a rigorous Lipschitz bound on the objective F (wn1 ), establishing Lemma 2 (see Subsection A.3 in the appendix for the relevant calculations).\nWe used MATLAB (version 2017b) to perform all floating-point computations, and its associated MATLAB VPA package to perform the exact symbolic computations. The code we used is freely available at\nhttps://github.com/ItaySafran/OneLayerGDconvergence.git. For any candidate local minimum, the verification took from less than a minute up to a few hours, depending on the size of k, n, when running on Intel Xeon E5 processors (ranging from E5-2430 to E5-2660)."
  }, {
    "heading": "4.2. Closed-form Expressions for F,∇F and ∇2F",
    "text": "For convenience, we will now state closed-form expressions (without an expectation) for the objective function F in Eq. (2), its gradient and its Hessian. These are also the expressions used in the code we built to verify the conditions of Lemma 1 and Lemma 2. First, we have that\nF (wn1 ) = 1\n2 n∑ i,j=1 f (wi,wj) − ∑ i∈[n] j∈[k] f (wi,vj) + 1 2 k∑ i,j=1 f (vi,vj) , (6)\nwhere f (w,v) := Ex∼N (0,I) [[ w>x ] + [ v>x ] + ] = 1\n2π ||w|| ||v|| (sin (θw,v) + (π − θw,v) cos (θw,v)) , (7)\nand\nθw,v := cos −1 ( w>v\n||w|| · ||v|| ) is the angle between two vectors w,v. The latter equality in Eq. (7) was shown in Cho & Saul (2009, section 2).\nUsing the above representation, Brutzkus & Globerson (2017) compute the gradient of f (w,v) with respect to w, given by\ng (w,v) := ∂\n∂w f (w,v)\n= 1\n2π (||v|| sin (θw,v) w̄ + (π − θw,v)v) . (8)\nWhich implies that∇F (wn1 ), the gradient of the objective with respect to wn1 , equals\n∇F (wn1 ) =\n1 2 wn1 + n∑ i,j=1 i 6=j g̃ (wi,wj)− ∑ i∈[n] j∈[k] g̃ (wi,vj) ,\nwhere g̃ (wi,u) ∈ Rkn equals g (wi,u) ∈ Rk on entries k(i− 1) + 1 through ki, and zero elsewhere. We now provide the Hessian of Eq. (7) based on the computation of the\ngradient in Eq. (8) (see Subsection A.4.1 in the appendix for the full derivation)\nh1 (w,v) := ∂2\n∂w2 f (w,v)\n= sin (θw,v) ||v|| 2π ||w|| ( I− w̄w̄> + n̄v,wn̄>v,w ) ,\nh2 (w,v) := ∂2\n∂w∂v f (w,v)\n= 1\n2π\n( (π − θw,v) I + n̄w,vv̄> + n̄v,ww̄> ) ,\nwhere nv,w = v̄ − cos (θv,w) w̄ (9)\nand n̄v,w = nv,w ||nv,w|| . To formally define the Hessian of F (a kn×knmatrix), we partition it into n×n blocks, each of size k×k. Define h̃1 (wi,u) ∈ Rkn×kn to equal h1 (wi,u) on the i-th d × d diagonal block and zero elsewhere. For wi,wj define h̃2 (wi,wj) ∈ Rkn×kn to equal h2 (wi,wj) on the i, j-th k×k block and zero elsewhere. We now have that the Hessian is given by\n∇2F (wn1 ) = 1\n2 I + n∑ i,j=1 i 6=j h̃1 (wi,wj)\n− ∑ i∈[n] j∈[k] h̃1 (wi,vj) + n∑ i,j=1 i 6=j h̃2 (wi,wj) .\n(10)"
  }, {
    "heading": "4.3. Proof of Corollary 1",
    "text": "To show the first part of Corollary 1, we will use the following lemma:\nLemma 3. Let wn1 = (w1, . . . ,wn), V = (v1, . . . ,vk) where wi,vj ∈ Rk for all i ∈ [n] , j ∈ [k]. Denote for any natural m ≥ 0, w̃n1,m = (w̃1, . . . , w̃n), w̃i = (wi,0) ∈ Rk+m, Ṽm = (ṽ1, . . . , ṽk), ṽi = (vi,0) ∈ Rk+m and let M ∈ Rn×n be the matrix with entries Mij = 1 2 + n∑ l=1 l 6=i sin(θwi,wl)||wl|| 2π||wi|| − k∑ l=1 sin(θwi,vl)||vl|| 2π||wi|| , i = j\n1 2π ( π − θwi,wj ) , i 6= j .\nThen the spectrum of ∇2F ( w̃n1,m ) is comprised of the spectrum of ∇2F (wn1 ) and the spectrum of M with multiplicity m. In particular, if ∇2F ( w̃n1,1 ) λmin · I then\n∇2F ( w̃n1,m ) λmin · I, for any m > 1.\nProof. A straightforward substitution of w̃n1,m and Ṽm in Eq. (10), and a permutation of the rows and columns of the\nresulting matrix reveals that\n∇2F ( w̃n1,m ) =  ∇2F (wn1 ) 0 0 · · · 0 0 M 0 · · · 0 0 0 M · · · 0 ... ... ... . . . ...\n0 0 0 · · · M\n .\nNow, diagonalizing the block diagonal ∇2F ( w̃n1,m ) completes the proof of the lemma.\nBack to the first part of Corollary 1, we have from Lemma 3 that the lower bound on the smallest eigenvalue of ∇2F ( w̃n1,1 ) holds for ∇2F ( w̃n1,m ) for any m ≥ 1. Furthermore, since ‖wn1 ‖2 = ‖w̃n1,m‖2 for any m ≥ 0 we have that the upper bound on the third order derivatives from Subsection A.2 in the appendix and the Lipschitz bound on the objective from Subsection A.3 in the appendix still hold, as well as the bound on the norm of the gradient. Therefore by running the simulations in Sec. 3 on w̃n1,1 instead of w n 1 , the results apply in any optimization space Rn(k+m), for natural m ≥ 0, since the conditions for invoking Lemma 1 and Lemma 2 are met with the same exact constants4, completing the first part of the corollary.\nFor the second part of the corollary, we note that if v1, . . . ,vk are chosen i.i.d. from N (0, cI), then by standard concentration arguments, for any > 0 and high enough dimension d (depending on k, ), it holds with probability at least 1−exp(−Ω(d)) that | 1√\ncd ||vi||−1| ≤\nand | 1cdv > i vi′ | ≤ for all i, i′ ∈ {1, . . . , k} (see Ledoux (2005)). Therefore, regardless of which distribution we are considering, with probability at least 1− exp(−Ω(d)), we can find a scalar a > 0 and an orthogonal matrix M , such that ||aMvi − ei|| ≤ for all i, where ei is the i-th standard basis vector. Note that this strongly uses the orthonormal reparameterization invariance of gradient descent.\nLetting F be our objective function (w.r.t. the randomly chosen v1, . . . ,vk), and using the rotational symmetry of the Gaussian distribution and the positive-homogeneity of\n4Note that for m = 0 the eigenvalue lower bound constant may change, since the spectrum of M has no impact on the spectrum of∇2F ( w̃n1,0 ) . This, however, can only result in a stronger lower bound and does not affect the validity on the results obtained when running the experiments in Sec. 3 with m = 1.\nthe ReLU function, we have that F (wn1 ) equals\n1 2 Ex ( n∑ i=1 [w>i x]+ − k∑ i=1 [v>i x]+ )2 = 1\n2 Ex ( n∑ i=1 [w>i (M >x)]+ − k∑ i=1 [v>i (M >x)]+ )2 = 1\n2a2 Ex ( n∑ i=1 [(aMwi) >x]+ − k∑ i=1 [(aMvi) >x]+ )2 , where x ∼ N (0, I). It follows that F has the same local minima as\nF̃ (wn1 ) :=\n1 2 Ex∼N (0,I)  ( n∑ i=1 [w>i x]+ − k∑ i=1 [(aMvi) >x]+ )2  , since they are equivalent after scaling and rotation, as F̃ (wn1 ) = a 2F ( 1aM >wn1 ). Thus, it is enough to prove existence of local minima for F̃ .\nBy the argument above, we can rewrite F̃ (wn1 ) as\nF̃ (wn1 ) := F̃ẽ1,...,ẽk(w n 1 )\n= 1\n2 Ex∼N (0,I) ( n∑ i=1 [w>i x]+ − k∑ i=1 [ẽ>i x]+ )2 , where (with high probability) each ẽi is -close to the standard basis vector ei. If ei = ẽi, we have already shown that there is some local minimum w∗n1 , which is in the interior of a sphere S such that F (w∗n1 ) < minwn1 ∈S F (w n 1 ), and moreover, the ballB enclosed by S does not contain global minima (see Eq. (4)) since Thm. 4 in the appendix and the condition in Eq. (5) imply that the minimal value in the ball enclosing wn1 is strictly positive. In particular, let 0 > 0 be such that\nF (w∗n1 ) < min wn1 ∈S F (wn1 )− 0\nand min wn1 ∈B F (wn1 ) > inf wn1 F (wn1 ) + 0.\nIt is easily verified that by setting small enough (depending only on w∗n1 , B, 0 which are all fixed), we can ensure that\nmax wn1 ∈B |F̃ẽ1,...,ẽk(wn1 )− F̃e1,...,ek(wn1 )| ≤ 0 3 ,\ntherefore F̃ẽ1,...,ẽk(w ∗n 1 ) < minwn1 ∈S F̃ẽ1,...,ẽk(w n 1 ), as well as minwn1 ∈B F̃ẽ1,...,ẽk(w n 1 ) > infwn1 F̃ẽ1,...,ẽk(w n 1 ), which implies that any minimizer of F̃ẽ1,...,ẽk over B must be a local (non-global) minimum."
  }],
  "year": 2018,
  "references": [{
    "title": "Exponentially many local minima for single neurons",
    "authors": ["P. Auer", "M. Herbster", "M.K. Warmuth"],
    "venue": "In NIPS,",
    "year": 1996
  }, {
    "title": "Global optimality of local search for low rank matrix recovery",
    "authors": ["S. Bhojanapalli", "B. Neyshabur", "N. Srebro"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Theoretical properties of the global optimizer of two layer neural network",
    "authors": ["D. Boob", "G. Lan"],
    "venue": "arXiv preprint arXiv:1710.11241,",
    "year": 2017
  }, {
    "title": "Globally optimal gradient descent for a convnet with gaussian inputs",
    "authors": ["A. Brutzkus", "A. Globerson"],
    "venue": "arXiv preprint arXiv:1702.07966,",
    "year": 2017
  }, {
    "title": "Kernel methods for deep learning",
    "authors": ["Y. Cho", "L.K. Saul"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Gradient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima",
    "authors": ["S.S. Du", "J.D. Lee", "Y. Tian", "B. Poczos", "A. Singh"],
    "venue": "arXiv preprint arXiv:1712.00779,",
    "year": 2017
  }, {
    "title": "Porcupine neural networks:(almost) all local optima are global",
    "authors": ["S. Feizi", "H. Javadi", "J. Zhang", "D. Tse"],
    "venue": "arXiv preprint arXiv:1710.02196,",
    "year": 2017
  }, {
    "title": "Escaping from saddle pointsonline stochastic gradient for tensor decomposition",
    "authors": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"],
    "venue": "In Conference on Learning Theory, pp",
    "year": 2015
  }, {
    "title": "Matrix completion has no spurious local minimum",
    "authors": ["R. Ge", "J.D. Lee", "T. Ma"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Learning one-hidden-layer neural networks with landscape design",
    "authors": ["R. Ge", "J.D. Lee", "T. Ma"],
    "venue": "arXiv preprint arXiv:1711.00501,",
    "year": 2017
  }, {
    "title": "Global optimality in tensor factorization, deep learning, and beyond",
    "authors": ["B.D. Haeffele", "R. Vidal"],
    "venue": "arXiv preprint arXiv:1506.07540,",
    "year": 2015
  }, {
    "title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
    "authors": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"],
    "venue": "CoRR abs/1506.08473,",
    "year": 2015
  }, {
    "title": "The concentration of measure phenomenon",
    "authors": ["M. Ledoux"],
    "venue": "Number 89. American Mathematical Society,",
    "year": 2005
  }, {
    "title": "Convergence analysis of two-layer neural networks with relu activation",
    "authors": ["Y. Li", "Y. Yuan"],
    "venue": "arXiv preprint arXiv:1705.09886,",
    "year": 2017
  }, {
    "title": "On the computational efficiency of training neural networks",
    "authors": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"],
    "venue": "In NIPS, pp",
    "year": 2014
  }, {
    "title": "The loss surface of deep and wide neural networks",
    "authors": ["Q. Nguyen", "M. Hein"],
    "venue": "arXiv preprint arXiv:1704.08045,",
    "year": 2017
  }, {
    "title": "Local minima and back propagation",
    "authors": ["T. Poston", "Lee", "C.-N", "Y. Choie", "Y. Kwon"],
    "venue": "In Neural Networks,",
    "year": 1991
  }, {
    "title": "On the quality of the initial basin in overspecified neural networks",
    "authors": ["I. Safran", "O. Shamir"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Distribution-specific hardness of learning neural networks",
    "authors": ["O. Shamir"],
    "venue": "arXiv preprint arXiv:1609.01037,",
    "year": 2016
  }, {
    "title": "Theoretical insights into the optimization landscape of overparameterized shallow neural networks",
    "authors": ["M. Soltanolkotabi", "A. Javanmard", "J.D. Lee"],
    "venue": "arXiv preprint arXiv:1707.04926,",
    "year": 2017
  }, {
    "title": "No bad local minima: Data independent training error guarantees for multilayer neural networks",
    "authors": ["D. Soudry", "Y. Carmon"],
    "venue": "arXiv preprint arXiv:1605.08361,",
    "year": 2016
  }, {
    "title": "When are nonconvex problems not scary",
    "authors": ["J. Sun", "Q. Qu", "J. Wright"],
    "venue": "arXiv preprint arXiv:1510.06096,",
    "year": 2015
  }, {
    "title": "Local minima in training of deep networks",
    "authors": ["G. Swirszcz", "W.M. Czarnecki", "R. Pascanu"],
    "venue": "arXiv preprint arXiv:1611.06310,",
    "year": 2016
  }, {
    "title": "An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis",
    "authors": ["Y. Tian"],
    "venue": "arXiv preprint arXiv:1703.00560,",
    "year": 2017
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "venue": "arXiv preprint arXiv:1611.03530,",
    "year": 2016
  }, {
    "title": "Electron-proton dynamics in deep learning",
    "authors": ["Q. Zhang", "R. Panigrahy", "S. Sachdeva", "A. Rahimi"],
    "venue": "arXiv preprint arXiv:1702.00458,",
    "year": 2017
  }, {
    "title": "Recovery guarantees for one-hidden-layer neural networks",
    "authors": ["K. Zhong", "Z. Song", "P. Jain", "P.L. Bartlett", "I.S. Dhillon"],
    "venue": "arXiv preprint arXiv:1706.03175,",
    "year": 2017
  }],
  "id": "SP:aca4ae18b6dc365bdf619514ea733762efc0281d",
  "authors": [{
    "name": "Itay Safran",
    "affiliations": []
  }, {
    "name": "Ohad Shamir",
    "affiliations": []
  }],
  "abstractText": "We consider the optimization problem associated with training simple ReLU neural networks of the form x 7→ ∑k i=1 max{0,w> i x} with respect to the squared loss. We provide a computerassisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once 6 ≤ k ≤ 20. By a concentration of measure argument, this implies that in high input dimensions, nearly all target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an overparameterization assumption is necessary to get a positive result in this setting.",
  "title": "Spurious Local Minima are Common in Two-Layer ReLU Neural Networks"
}