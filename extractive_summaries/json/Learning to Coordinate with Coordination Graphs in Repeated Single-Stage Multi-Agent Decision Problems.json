{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Many decision problems can be phrased as coordination problems of many artificial intelligent agents (Boutilier, 1996). Examples include robot soccer (Kok et al., 2003), warehouse commissioning (Claes et al., 2017), and traffic light control (Wiering, 2000). We consider the cooperative case, where there is a single goal to be optimised. A naive approach could be to consider a super agent that decides on the actions of all agents involved, which could easily result in an action space which is prohibitively large. However, many coordination tasks have loose couplings. This means that the total reward to optimise can be decomposed into a sum of local rewards that only depend on (possibly\n1Dept. of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium 2Fac. of Science, Vrije Universiteit Amsterdam, The Netherlands 3DeepMind, London, UK. Correspondence to: Eugenio Bargiacchi <svalorzen@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\noverlapping) subsets of agents. Then, each agent’s action can only directly affects the rewards of few small subsets of agents. Key to making coordination efficient is exploiting such loose couplings.\nFor an example of such a coordination task, consider an autonomously controlled wind farm in which each agent represents a wind turbine that is able to adjust the alignment of its blades to the wind (see Section 6.3). Each turbine can maximize its own power output by aligning the blades exactly perpendicular to the wind, but doing so may hinder turbines that are behind it due to turbulence (Van Dijk et al., 2016). It should be possible to do better through coordination. However, considering the full joint action over all turbines leads to a high-dimensional action space, which would be hard to optimise. Instead, we can see that this problem is loosely coupled, by noting that the power output of each turbine only directly depends on a small subset of other turbines — the turbines upwind within a certain distance. This means that the total output can be phrased as a sum of local rewards that depend on small subsets of agents.\nIn this paper, we formalize multi-agent multi-armed bandits (MAMABs) and investigate how to balance exploration and exploitation in the joint action taken by the agents, such that the loss due to taking suboptimal joint actions during learning is bounded. Building on the upper confidence bound (UCB) framework (Auer et al., 2002) for single-agent multi-armed bandits, we formulate a new algorithm that we call multi-agent upper confidence exploration (MAUCE) (Section 4). MAUCE balances exploitation and exploration using local estimates and local upper confidence bounds.\nWe prove in Section 5 that MAUCE achieves a regret bound that depends on the harmonic mean of the local upper confidence bounds, rather than their sum, as we would get by applying the combinatorial bandit framework (Cesa-Bianchi & Lugosi, 2012; Chen et al., 2013). This leads to a regret logarithmic in the number of arm-pulls and linear in the number of agents. In contrast, the naive approach of considering the full joint action is exponential in the number of agents. In Section 6 we compare empirically the performance of MAUCE to other approaches from the literature, and show that it achieves much less regret in various settings, including wind farm control."
  }, {
    "heading": "2. Related Work",
    "text": "Multi-agent reinforcement learning and planning with loose couplings has mainly been studied in sequential problems (Guestrin et al., 2002; Kok & Vlassis, 2006; De Hauwere et al., 2010; Scharpff et al., 2016). In such sequential settings however, the value function does not permit an exact factorization. Therefore, only in the planning setting (Scharpff et al., 2016), some guarantees can be provided. For learning (Kok & Vlassis, 2006), the focus has been on empirical performance. In this paper, we focus on MAMAB, which permit an exact factorization of the value function.\nThis work is related to combinatorial bandits (Bubeck & Cesa-Bianchi, 2012; Cesa-Bianchi & Lugosi, 2012; Gai et al., 2012; Chen et al., 2013), in which sets of arms can be pulled simultaneously. In our setting, these variables correspond to the different agents, and similarly to the combinatorial bandit framework, the action space grows exponentially with the size of the sets of rewards. We consider a specific variant, called the semi-bandit problem (Audibert et al., 2011), in which local components of the global reward are observable. Chen et al. (2013) considered this variant and constructed an algorithm. However, that algorithm assumes access to an (α, β)-oracle that provides a joint action that outputs an α fraction of the optimal expected reward with a certain probability β. Instead, we assume the availability of a coordination graph, which is often a more reasonable assumption in multi-agent settings."
  }, {
    "heading": "3. Background",
    "text": "Before introducing our new algorithm, we first need to define our learning problem. This problem, the multi-agent multi-armed bandit, is a repeated fully cooperative multiagent game. We first define the single-agent version of our setting, and then add the multi-agent elements. The single-agent version of our setting is commonly known as the multi-armed bandit (MAB): Definition 1. A single-agent multi-armed bandit (MAB) (Thompson, 1933) is a tuple 〈A, F 〉 where\n• A is a set of actions or arms, and • F (a), called the reward function, is a random function\ntaking an arm, a ∈ A, as input. Specifically, for each a ∈ A, F (a) is a random variable associated with a probability distribution Pa : R → [0, 1] over realvalued rewards r.\nWe refer to the mean reward of an arm as µa = EPa [r] =∫∞ −∞ rPa(r)dr, and to the optimal reward as the mean reward of the best arm µ∗ = maxa µa.\nThe goal of an agent interacting with a MAB is to minimize the expected regret. Definition 2. The expected cumulative regret of pulling\na sequence of arms for timestep t = 1 to the horizon T (following the definition of Agrawal & Goyal, 2012), is\nE [ T∑ t=1 µ∗ − µa(t) ] ,\nwhere a(t) is the arm pulled at time t, and na(t) is the number of times arm a is pulled until timestep t.\nIn a multi-agent multi-armed bandit (MAMAB) there are multiple agents, and the rewards are factored:\nDefinition 3. A multi-agent multi-armed bandit (MAB) is a tuple 〈A,D, F 〉 where\n• D is the set of m enumerated agents, • A = A1 × · · · × Am is a set of joint actions, which is\nthe Cartesian product of the sets of individual actions, Ai, for each of the m agents in D, and • F (a), called the global reward function, is a random function taking a joint action, a ∈ A, as input, but with added structure. Specifically, there are ρ possibly overlapping subsets of agents, and the global reward is decomposed into ρ local noisy reward functions: F (a) = ∑ρ e=1 f\ne(ae) where fe(ae) ∈ [0, remax]. A local function fe only depends on the joint action ae of the subset De of agents.\nWe refer to the mean reward of a joint action as µa, which in turn is factorized into the same local reward components as F (a): µa = ∑ρ e=1 µ(a\ne). For simplicity, we refer to an agent i by its index.\nµa thus maps joint actions to real-valued expected rewards via real-valued local expected rewards, i.e., it is a coordination graph (CoG) (Guestrin et al., 2002; Kok & Vlassis, 2006). When µa and all its components are known, it can be used to extract the optimal reward µ∗. A naive way to do so would be to ‘flatten’ the CoG, i.e., enumerate all joint actions, compute their associated mean reward, and then maximize. However, this is typically infeasible, as the number of joint actions, A ≡ |A|, is exponential in the number of agents. For instance, if each agent has two actions, then A = 2m. Therefore, extracting the optimal reward and associated actions is typically done via algorithms like variable elimination (VE). In VE, agents are eliminated from the CoG sequentially, thus solving the maximization problem as a series of local subproblems: one per agent. When an agent is eliminated, VE computes its best responses to all possible actions of its neighbors, i.e., the agents with which it shares a local reward function. The local values of these best responses are then used to create a new local mean reward, replacing those to which the eliminated agent was connected. This exploits the graphical structure resulting from the factorization, and the size of the local subproblems depends only on the induced width, i.e., how many agents\nAlgorithm 1 MAUCE 1: Input: An MAMAB with a factorized reward function, F (a) = ∑ρ e=1 f\ne(ae), a time horizon T 2: Initialize µ̂e(ae) and ne(ae) to zero. 3: for i = 1 to T do 4: at = arg maxa µ̂t(a) + ct(a) where,\nµ̂et (a) = ∑ρ e=1 µ̂t(a e) and,\nct(a) = √ 1 2 ( ∑ρ e=1 n e t (a e)−1(remax) 2) log(tA)\n5: rt= ∑ρ e=1r e t (a\ne) (execute a, obtain local rewards) 6: Update µ̂et (a e) using ret (a e) for all ae ⊂ at 7: Increment net (a e) by 1 for all ae ⊂ at 8: end for\nthe eliminated agent shares a local reward function with at the time of its elimination. When the coordination graph is sparse, i.e., agents are only involved in a small number of local reward functions, the induced width is typically much smaller than the size of the joint action space, making the maximization problem tractable.\nWhen we are not simply maximizing over the joint actions to extract the optimal reward, but also need to explore to learn what the values of the mean rewards are, the situation becomes more complex. Again, we could ‘flatten’ the MAMAB by treating each joint action as a separate arm in a single-agent MAB, but this quickly leads to too many arms to be able to learn effectively with popular algorithms such as UCB (Auer et al., 2002) of which the regret bounds depend on the number of arms. Furthermore, just adding the standard exploration bonuses to each of the local mean rewards leads to over-exploration, as we will show experimentally in Section 6. Instead, we propose to treat exploration and exploitation as separate objectives during a VE-like scheme, and taking inspiration from the multiobjective literature (Roijers et al., 2015), define a new VElike subroutine, that allows us to define a MAUCE (Section 4) for which we can prove a much tighter regret-bound."
  }, {
    "heading": "4. Multi-Agent Upper Confidence Exploration",
    "text": "In this section we propose our new algorithm for MAMABs: Multi-Agent Upper Confidence Exploration (MAUCE) (Algorithm 1).\nMAUCE executes a joint action at every timestep that maximizes the estimated mean reward for a given factorization of the reward function, µ̂(a), plus an exploration bonus, ct(a), that is computed using the same factorization. To do so, it keeps mean estimates of local rewards µ̂e(ae), and local counts net (a\ne) for each subset of agents. These local estimates depend only on the subset of actions ae ⊂ a for this group of agents De ⊂ D. Not all joint actions have to\nbe selected often, or even at all. Note that the counts net (a e) used to compute the bonus for an action a can change over time, even if the joint action a has never been selected, because MAUCE observes and uses the local rewards, ret (a\ne). This enables the algorithm to exploit the graphical structure to compute tighter exploration bonuses while guaranteeing a tight regret bound. Despite not guaranteeing to explore all joint actions, the algorithm achieves guaranteed logarithmic regret. The proof for this regret bound is given in Section 5.\nBesides the local counts, the exploration bonus also depends on the maximum value of the local rewards remax, the time index t, and A. We note that A is exponential in the number of agents. Contrary to single-agent MABs, it is not trivial to maximize over µ̂(a) + ct(a), as we need to maximize over a A efficiently, and ct(a) is a non-linear function in the local counts net (a\ne). Hence, MAUCE requires a special algorithm to perform this maximization.\n4.1. Maximizing µ̂(a) + c(a)\nWe observe that we can express the estimated mean as the sum of local estimated means, and that ct(a) can be expressed as a function over the inverse counts: y( ∑ρ e=1 n e t (a e)−1(remax) 2). Hence, when we write down the local estimates as two-element vectors: an estimated mean component and a weighted inverse counts component,\nve(ae) = (µ̂e(ae), net (a e)−1(remax) 2), (1)\nwe can express the mean plus exploration bonus as a function applied to the sum of these vectors:\nzt(v(a)) = µ̂(a) + ct(a) = v[1] +\n√ 1\n2 v[2] log(tA), (2)\nwhere\nv(a) = ρ∑ e=1 ve(ae). (3)\nVector formulations of reward, as those of Equations 1–3, are often used in the multi-objective decision making literature (Roijers & Whiteson, 2017). Consider multi-objective variable elimination (MOVE) (Rollón & Larrosa, 2006; Roijers et al., 2015), a multi-objective framework based on variable elimination that is able to handle vectors. Instead of single best responses for eliminated agents, MOVE produces sets of vectors that are possibly optimal as intermediate results. At each agent elimination, MOVE computes all possible (local) value vectors for the subproblem of eliminating the agent i, and prunes away those that are locally dominated. After MOVE eliminates the last agent it outputs a possibly very large set of possibly optimal vectors, e.g., a Pareto front or convex coverage set.\nIn contrast to MOVE, we only want to output a single vector, i.e., the one that maximizes zt (Equation 2). To do this we\ntighten MOVE’s simple domination pruning by introducing lower and upper bounds on the exploration part of the vector. This results in an algorithm in which the number of vectors in the intermediate solution sets steeply decreases in the last agent eliminations (in contrast to MOVE, in which the intermediate sets typically continue to grow in size). We call this algorithm upper confidence variable elimination (UCVE).\nFirst, we define the input of UCVE. Specifically, to be able to work with sets of vectors as intermediate results, we first reformulate the problem of finding the optimal joint action in these terms. Specifically, we define the input to UCVE as a set F of local upper confidence vector set functions (UCVSFs). For each fe of F (a), F contains an identically scoped UCVSF ue. Each ue initially contains a singleton set, ue(ae) = {ve(ae)}, where ve(ae) is defined as in Equation 1. Eliminating an agent i, is performed by replacing all ue(ae) which have i in scope, i.e., i ∈ De, by a new function that incorporates the possibly optimal responses of i. These possibly optimal responses are again vectors in the form of Equation 1.\nAlgorithm 2 UCVE(F) Input : A set of local upper confidence vector set functions F and an elimination order q (a queue with all agents) Output : An optimal joint action, a∗\n1: while q is not empty do 2: i← q.dequeue() 3: Fi ← the subset of UCVSFs inF that have i in scope 4: xu, xl ← compute upper and lower bounds on the\nexploration part of the vectors for the remaining factors in F \\ Fi\n5: unew(·)← a new UCVSF 6: for all ae−i ∈ ADe\\{i} do 7: V ← ⋃ ai∈Ai ⊕ ue∈Fi ue(ae−i × {ai}) 8: unew(ae−i)←prune(V, xu, xl) 9: end for 10: F ← F \\ Fi ∪ {unew} 11: end while 12: u← retrieve final factor from F 13: return the optimal joint action from u\nUCVE is provided in Algorithm 2. Note that we only describe what is traditionally known as the forward pass of the variable elimination scheme. This is because to retrieve the optimal joint action, we make use of the tagging scheme of MOVE (Roijers et al., 2015), where vectors are tagged with the appropriate action of an agent during its elimination.\nUCVE eliminates all agents in a predetermined order, q, in the main loop (line 1–11). On line 2 the next agent i is\npopped off the queue, and on line 3 the factors that have i in scope, Fi are collected. The functions inFi will be replaced in F by a new UCVSF, unew, incorporating the possible best responses to every possible local joint action of the neighbors of i. This new UCVSF has all the neighboring agents De \\ {i} of agent i in scope.\nFirst, all possible vectors V that can be made with the UCVSFs in Fi are computed (on line 7), across all actions of i, for a given ae−i:\nV = ⋃\nai∈Ai ⊕ ue∈Fi ue(ae−i × {ai}),\nwhere Ai is the action space of agent i, and the cross-sum operator A ⊕ B is defined as A ⊕ B = {a + b : a ∈ A ∧ b ∈ B}. Note that the resulting actions always include the appropriate actions ai (which is under the union) and the appropriate actions from ae−i. After V is computed, the vectors in V that cannot lead to an optimal joint action need to be pruned.\nEach vector in V consists of an estimated mean and a weighted inverse counts part that will lead to the exploration bonus. Because the weighted inverse counts cannot be linearly added to the estimated mean, we cannot a priori tell whether a vector v ∈ V is better than another vector v′ ∈ V when v[1] > v′[1] but v[2] < v′[2]. We thus define a pruning operator that satisfies the two conditions necessary for correctness in a multi-objective variable elimination-scheme (Roijers, 2016), i.e., (1) no excess values are kept, and (2) no unnecessary values are returned after the last agent elimination. We compute an upper and a lower bound on the exploration bonus for the remaining functions in F \\Fi, using the sums of the maximum, resp. minimum, values for the exploration part, xu = ∑ ue∈F\\Fi maxae maxv∈ue(ae) v[2]\nand xl = ∑ ue∈F\\Fi minae minv∈ue(ae) v[2]. Specifically, a vector v ∈ V cannot contribute to the optimal value if there is another vector v′ ∈ V such that\nv[1]+\n√ 1\n2 (v[2]+xu) log(tA)<v\n′[1]+\n√ 1\n2 (v′[2]+xl) log(tA)\nHence, prune removes those candidate local upper confidence vectors that cannot contribute to finding the maximal mean plus exploration bonus. This immediately satisfies correctness condition (1), as it follows from the definition of upper and lower bounds.\nAfter all agents have been eliminated, there is only one UCVSF left, containing a single local upper confidence vector. UCVE retrieves the optimal vector— which maximizes the µ̂(a) + ct(a) —and the associated joint action, at, from the final UCVSF, satisfying correctness condition 2. We thus have defined an efficient algorithm that correctly outputs the joint action that maximizes µ̂(a) + ct(a), and can therefore be used to select joint actions inside of MAUCE."
  }, {
    "heading": "5. Linear Regret Bound for Collaborative Multi-Agent Settings",
    "text": "The efficiency of our method is achieved by exploiting localized structures within the global reward. If there are no such structures, then a worst-case regret of O(A log T ) can be achieved by employing an upper-confidence bound (UCB) algorithm (Auer et al., 2002; Auer & Ortner, 2010). As A grows exponentially with the number of agents, the global action space is simply too large to make this bound of practical use. However, we show that when the global reward can be decomposed into local reward functions over subsets of agents, the regret obtained when using our method becomes much smaller. In fact, when the local rewards all have the same range, the regret of our method becomes linear in the number of agents.\nAssume there are ρ subsets of agents, called groups, and that there is a decomposition of the global reward F (a) =∑ρ e=1 f\ne(ae) where fe(ae) ∈ [0, remax]. W.l.o.g., let us also consider non-identical groups and that ∑ρ e=1 r e max = 1. The local function fe only depends on the actions of a group De. We maintain the sample mean reward µ̂et (ae) and number of pulls net (a e) for each local joint action ae taken by group De at time t. Finally, we define the gap between the true expected rewards of the optimal action a∗ and action a to be ∆(a) = E [F (a∗)]− E [F (a)]. Theorem 1. If at each time t we choose at such that\nat = arg max a wt(a)\n= arg max a µ̂t(a) + ct(a) ,\nwith\nµ̂t(a) = ρ∑ e=1 µ̂et (a e) ,\nct(a) = √√√√1 2 ( ρ∑ e=1 net (a e)−1(remax) 2 ) log(tA) ,\nthen the expected global regret is bounded by\nE [ T∑ t=1 ∆(at) ] ≤ 2N ∑ρ e=1(r e max) 2 log(TA) mina ∆(a)2 +log T+1 ,\nwhere N ≡ ∑ρ e=1 ∏ i∈De Ai is the total number of local joint actions and Ai = |Ai|.\nProof. Let Ct(a) be the event that ∆(a) > 2ct(a) holds and Ct(a) its negation. By the law of the excluded middle, we can then write\nE [∆(at)] = E [∆(at) |Ct(at)]P (Ct(at)) + E [ ∆(at) ∣∣Ct(at)]P (Ct(at))\nwhich implies\nE [ T∑ t=1 ∆(at) ] ≤ T∑ t=1 P (Ct(at))\n+ E [ ∆(at) | Ct(at) ] E [ I { Ct(at) }] where I{·} is the indicator function. We first look at all time steps on which Ct(at) holds. Specifically, we bound the probability that this event occurs. Using the law of total probability and chain rule, we can derive\nP (Ct(at)) ≤ ∑ a∈A P (a = at |Ct(a)) (4)\nBy definition, action at maximizes the upper bound wt(·). Therefore,\nP (a = at | Ct(a)) = P (wt(a) = wt(at) |Ct(a)) ≤ P (wt(a) ≥ wt(a∗) |Ct(a)) = P (µ̂t(a)− µ̂t(a∗) ≥ ct(a∗)− ct(a) |Ct(a))\n≤ exp ( − 2(∆(a) + ct(a∗)− ct(a))\n2∑ρ e=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) ) In the last step, we used Hoeffding’s inequality. This is possible, as µ̂t(a) is a sum of i.i.d. random variables bounded within the interval [ 0,\nremax net (a e)\n] . We now apply condition\nCt(a) such that ∆(a) > 2ct(a) and derive\nP (a = at | Ct(a)) ≤ exp ( − 2(ct(a) + ct(a∗))\n2∑ρ e=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) ) ≤ exp ( − 2ct(a) 2 + 2ct(a∗) 2∑ρ\ne=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) ) = exp (− log(tA)) ≤ (tA)−1\nUsing (4), we can conclude\nT∑ t=1 P (Ct(at)) ≤ T∑ t=1 (tA)−1A ≤ log T + 1 (5)\nwhere for the last step we used ∑T t=1 t\n−1 < log T + γ + 3 6T+2 < log T + 1 (Chen & Qi, 2003), where γ is Euler’s constant.\nNow, we look at the time steps where Ct(at) holds. Either at = a∗ and ∆(at) = 0, or\n∆(at) ≤ 2ct(at)\n∆(at) 2 ≤ 2 log(tA) ρ∑ e=1 (remax) 2(net (a e t )) −1\n1 ≤ 2 log(tA) mine net (a e t )\n∑ρ e=1(r e max) 2\nmina6=a∗ ∆(a) 2\nmin e net (a e t ) ≤ 2 log(TA)\n∑ρ e=1(r e max) 2\nmina6=a∗ ∆(a) 2\n(6)\nNote that as there are at most N = ∑ρ e=1 ∏ i∈De Ai local joint actions, the left-hand side will increase every at most N time steps. Since the right-hand side is fixed and does not depend on t, (6) can only be true on at most 2N log(TA) ∑ρ e=1(r e max) 2/mina6=a∗ ∆(a) 2 different time steps. This implies that T∑ t=1 E [ ∆(at) | Ct(at) ] E [ I { Ct(at)\n}] ≤ E\n[ T∑ t=1 I { Ct(at) ∧ at 6= a∗ }]\n≤ 2N log(TA)\n∑ρ e=1(r e max) 2\nmina6=a∗ ∆(a) 2\nTogether with (4) and (5), this implies\nE [ T∑ t=1 ∆(at) ] ≤ 2N log(TA) ∑ρ e=1(r e max) 2 mina6=a∗ ∆(a) 2 + log T + 1\nCorollary 1. If Ai ≤ k for all agents i, and if |De| ≤ d for all groups De, then\nE [ T∑ t=1 ∆(at) ] ≤ 2ρk d (log T +m log k) mina6=a∗ ∆(a) 2 + log T + 1 .\nProof. ∑ρ e=1 r e max = 1 implies ∑ρ e=1(r e max)\n2 ≤ 1. Additionally, logA = ∑m i=1 logAi ≤ m log k. Finally,\nN = ∑ρ e=1 ∏ i∈De Ai ≤ ρkd.\nThe important thing to note is that the given regret bound is linear in the number of agents m and in the number of functions ρ, which implies — since ρ ≤ ( m d ) < md — that it is polynomial in m, with degree at most d+ 1. This is a huge improvement over the naive ‘flattened’ regret bound, which is exponential in the number of agents. Corollary 2. If, in addition to the assumptions in Corollary 1, each local function has the same range such that remax = ρ−1, then\nE [ T∑ t=1 ∆(at) ] ≤ 2k d (log T +m log k) mina6=a∗ ∆(a) 2 + log T + 1 .\nProof. If remax = ρ −1 for each e, then ∑ρ e=1(r e max)\n2 = ρ−1 and therefore N ∑ρ e=1(r e max) 2 = kd.\nNote that this implies that under the assumption that each local function has the same range, 1) the regret no longer depends on ρ and 2) the regret is linear in the number of agents."
  }, {
    "heading": "6. Experiments",
    "text": "In order to test the performance of MAUCE, and compare it to competing approaches, we tested it on three different settings of increasing complexity, which are described below. We compared our results against several baselines: a uniformly random action selector, Sparse Cooperative QLearning (SCQL) (Kok & Vlassis, 2006), and Learning with Linear Rewards (LLR) (Gai et al., 2012).\nSCQL is a multi-agent Q-Learning based algorithm that can leverage domain knowledge about agents’ interdependencies to lower its sample requirements. SCQL was originally proposed in the context of multi-agent MDPs, but does apply to MAMABs. To allow for exploration we use both optimistic initialization and an ε-greedy policy, with the ε parameter linearly decreasing over time: ε = 0.05− 10−5t.\nLLR is a UCB algorithm from the combinatorial bandit literature that applies most to MAMABs, as it assumes that the rewards are a linear combination of what we refer to as local reward functions. Contrary to MAUCE however, it computes upper confidence bounds on the local reward components separately, before summing them, rather than our vector-based formulation of Equations 1–3. LLR is parameterless, aside from the knowledge of which agents depend on each other.\nIn all experiments the rewards were normalized so that the maximum possible regret per timestep is one. The maximum possible reward was computed by directly solving non-stochastic versions of the problems with Variable Elimination (or brute-force enumeration). Reward normalization enables directly comparing the output results to see how each approach performs across different settings.\nWe now describe each of our problem settings: the 0101- Chain, which is simple but illustrates the fast learning properties of MAUCE; Gem Mining, which is real-world inspired and adapted from an established benchmark multiobjective coordination graph; and Wind Farm, a real-world coordination problem, in which we connect our learning problem to a state-of-the-art wind farm simulator.\nAll the code needed to run the experiments can be found at https://bitbucket.org/Svalorzen/ mauce-experiments/src/master/"
  }, {
    "heading": "6.1. 0101-Chain",
    "text": "The 0101-Chain is a simple MAMAB, with a known optimal action. The problem consists of n agents, and n− 1 local reward functions. Each local reward function f i(ai, ai+1) is connected to the agent with the same index, i, and to i+1.\nThe optimal action in the 0101-Chain problem is ai = 0 if i is even, and ai = 1 is i is odd. The reward tables for each local group are given in Table 1.\n6.2. Gem Mining\nOur Gem Mining problem is adapted from the Mining Day problem from (Roijers et al., 2015), which is a multiobjective coordination graph benchmark problem.\nIn Gem Mining, a mining company mines gems from a set of mines (local reward functions) located in the mountains (see Figure 1). The mine workers live in villages at the foot of the mountains. The company has one van in each village (agents) for transporting workers and must determine every morning to which mine each van should go (actions), but vans can only travel to nearby mines (graph connectivity). Workers are more efficient when there are more workers at a mine: the probability of finding a gem in a mine is x ·1.03w−1, where x is the base probability of finding a gem in a mine and w is the number of workers at the mine. To generate an instance with v villages (agents), we randomly assign 1-5 workers to each village and connect it to 2–4 mines. Each village is only connected to mines with a greater or equal index, i.e., if village i is connected to m mines, it is connected to mines i to i + m − 1. The last village is connected to 4 mines and thus the number of mines is v + 3."
  }, {
    "heading": "6.3. Wind Farm",
    "text": "In our wind farm experiment, we used a state-of-the-art simulator (Van Dijk et al., 2016) to mimic the energy production of a series of wind turbines when exposed to a global incoming wind vector. In the real world, turbines can often be oriented at certain angles to maximize production. This is a non-trivial control task, as the turbulence caused by a turbine will negatively affect turbines downwind. The\ndirection of this generated turbulence depends on the angle that the turbine has w.r.t. the incoming wind vector.\nWe setup our simulated wind farm using 11 turbines (see Figure 2). Each turbine has a choice between three different actions (angles) that it can turn to. The last 4 turbines downwind (2, 5, 8, and a) are set directly against the wind and are not controlled by agents, as they cannot generate turbulence that can impact power production. However, the remaining 7 turbines do influence the rest of the farm, and so must cooperate to maximize power production.\nWe vary the wind speed in the simulator at each timestep, following a truncated normal distribution with mean 8.1 m/s. The overall reward is normalized to a [0, 1] interval using the maximum possible overall reward at the highest wind strength and the minimum possible reward per turbine at the minimum wind strength. While this makes it impossible to compute the true regret, as choosing the optimal action does not result in a 0 regret in expectation, it avoids having to calculate the true expected reward for all actions in this scenario, which is non-trivial.\nDifferently from the previous experiments, rewards in this settings are obtained per-agent from the simulator rather than per-group. Thus, we use single-agent local groups to prevent dependencies between the reward functions of each group. The reward for agents in more than one group is given solely to their single-agent group, and none to the others (rather than splitting)."
  }, {
    "heading": "6.4. Results",
    "text": "We tested the performance of MAUCE on the higly structured 0101-Chain problem with 11 agents for 10,000 joint action executions, and compare its performance against random, SCQL and LLR. The results (Figure 3) indicate that both SCQL and MAUCE can learn effectively, far outclassing random joint action selection and LLR.\nWhen comparing MAUCE and SCQL (Figure 3(b)), MAUCE achieves considerably less regret than SCQL. This is because MAUCE’s exploration strategy is based on the aggregation of local exploration bounds, while SCQL uses an ε-greedy exploration strategy. On the other hand, SCQL does learn the optimal joint action quickly, thanks to optimistic initialization and this aggressive exploration strategy. We note that after a while, we decreased ε to 0, i.e., only exploit, making the regret graph a flat line from that point onward. We note that the annealing of ε needed to be fine-tuned. We thus conclude that MAUCE is an effective algorithm that can exploit the graphical structure, leading to superior performance for this highly-structured problem.\nWe then tested MAUCE against the other algorithms on randomly generated Gem Mining instances with 5 villages and 8 mines, to compare performances on a more challenging problem. Figure 3(c) represents the average regret over multiple different scenarios. We observe that, while SCQL and LLR are all able to achieve sublinear regret curves, MAUCE handles the exploration-exploitation tradeoffs best, resulting in the lowest regret over time.\nFinally, to test the performance of MAUCE on a real-world problem, we run the algorithms on a Wind Farm instance (Figure 3(d)). Due to the high computational costs of running the simulator, we perform only ten runs. As explained before, the measure shown is not an exact form of regret, as the optimal action will not result in a 0 regret in expectation.\nThe MAUCE algorithm once again performs best, with less cumulative regret than both LLR and SCQL. The LLR algorithm also doesn’t seem to achieve any significant learning with respect to the random policy. Note that MAUCE keeps learning and fine tuning this policy over the whole duration of the experiment, which allows it to increasingly achieve lower regret than SCQL. At timestep 10000, the difference between the two is 43.258, while at timestep 40000 it is 81.373. It is important to note that SCQL could probably be made to perform better by finely tuning the initialization values and epsilon updates, but this would take significant human time and repetitive trials. MAUCE can instead directly\nmanage the exploration-exploitation trade-off by using its local bounds for each local joint action.\nWe thus conclude that MAUCE is an effective algorithm for trading off exploration versus exploitation in MAMABs, and has superior performance w.r.t. the alternative algorithms."
  }, {
    "heading": "7. Conclusion",
    "text": "In this paper, we proposed the multi-agent upper confidence exploration (MAUCE) algorithm for multi-agent multi-armed bandits (MAMABs). While learning, MAUCE leverages the graphical properties of the MAMAB by treating as separate objectives both exploration, expressed as a function of the sum over weighted inverse local counts, and exploitation, i.e., the sum over estimated mean local rewards. Via a subroutine, upper confidence variable elimination (UCVE), that can handle these objectives, MAUCE selects the action that best balances exploration and exploitation according to the joint overall mean reward plus (upper confidence) exploration bound. We have proven a regret bound for MAUCE that is only linear in the number of agents, rather than exponential, as it would be if we were to flatten the MAMAB to a single-agent MAB. Furthermore, the regret bound is logarithmic in the number of arm pulls. We compared MAUCE empirically to state-of-the-art algorithms in multi-agent reinforcement learning and combinatorial bandits, and have shown that MAUCE achieves much lower empirical regret than these approaches.\nWe note that the range parameters remax for MAUCE, which represent the difference between the maximum and minimum possible reward for each local joint action, can be difficult to guess in advance when the problem is not exactly known, as in the Wind Farm experiments. One way to mitigate this, could be to estimate them from the coordination graph of expected mean rewards learnt while running the algorithm, rather than running preliminary experiments as we did for the Wind Farm. We will test this in future work. Furthermore, we aim to build on MAUCE to achieve quality guarantees for reinforcement learning in multi-agent MDPs."
  }, {
    "heading": "Acknowledgements",
    "text": "The first author was supported by Flanders Innovation & Entrepreneurship (VLAIO), SBO project 140047: Stable MultI-agent LEarnIng for neTworks (SMILE-IT), second author was supported by an FWO PhD grant (Fonds Wetenschappelijk Onderzoek - Vlaanderen), third author was a Postdoctoral Fellow with the FWO (grant #12J0617N)."
  }],
  "year": 2018,
  "references": [{
    "title": "Analysis of Thompson sampling for the multi-armed bandit problem",
    "authors": ["S. Agrawal", "N. Goyal"],
    "venue": "In COLT, pp",
    "year": 2012
  }, {
    "title": "Minimax policies for combinatorial prediction games",
    "authors": ["Audibert", "J.-Y", "S. Bubeck", "G. Lugosi"],
    "venue": "In COLT,",
    "year": 2011
  }, {
    "title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
    "authors": ["P. Auer", "R. Ortner"],
    "venue": "Periodica Mathematica Hungarica,",
    "year": 2010
  }, {
    "title": "Finite-time analysis of the multiarmed bandit problem",
    "authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"],
    "venue": "Machine learning,",
    "year": 2002
  }, {
    "title": "Planning, learning and coordination in multiagent decision processes",
    "authors": ["C. Boutilier"],
    "venue": "TARK",
    "year": 1996
  }, {
    "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "arXiv preprint:1204.5721,",
    "year": 2012
  }, {
    "title": "Combinatorial bandits",
    "authors": ["N. Cesa-Bianchi", "G. Lugosi"],
    "venue": "Journal of Computer and System Sciences,",
    "year": 2012
  }, {
    "title": "The best lower and upper bounds of harmonic sequence",
    "authors": ["Chen", "C.-P", "F. Qi"],
    "venue": "RGMIA research report collection,",
    "year": 2003
  }, {
    "title": "Combinatorial multiarmed bandit: General framework, results and applications",
    "authors": ["W. Chen", "Y. Wang", "Y. Yuan"],
    "venue": "In Proceedings of the 30th international conference on machine learning,",
    "year": 2013
  }, {
    "title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations",
    "authors": ["Y. Gai", "B. Krishnamachari", "R. Jain"],
    "venue": "IEEE/ACM Transactions on Networking (TON),",
    "year": 2012
  }, {
    "title": "Multiagent planning with factored MDPs",
    "authors": ["C. Guestrin", "D. Koller", "R. Parr"],
    "venue": "In NIPS 2002: Advances in Neural Information Processing Systems",
    "year": 2002
  }, {
    "title": "Collaborative multiagent reinforcement learning by payoff propagation",
    "authors": ["J. Kok", "N. Vlassis"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2006
  }, {
    "title": "Multi-robot decision making using coordination graphs",
    "authors": ["J.R. Kok", "M.T.J. Spaan", "N Vlassis"],
    "venue": "In Proceedings of the 11th International Conference on Advanced Robotics, ICAR,",
    "year": 2003
  }, {
    "title": "Computing convex coverage sets for faster multi-objective coordination",
    "authors": ["D. Roijers", "S. Whiteson", "F. Oliehoek"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2015
  }, {
    "title": "Multi-Objective Decision-Theoretic Planning",
    "authors": ["D.M. Roijers"],
    "venue": "PhD thesis, University of Amsterdam,",
    "year": 2016
  }, {
    "title": "Multi-objective decision making",
    "authors": ["D.M. Roijers", "S. Whiteson"],
    "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
    "year": 2017
  }, {
    "title": "Bucket elimination for multiobjective optimization problems",
    "authors": ["E. Rollón", "J. Larrosa"],
    "venue": "Journal of Heuristics,",
    "year": 2006
  }, {
    "title": "Solving transition-independent multi-agent MDPs with sparse interactions",
    "authors": ["J. Scharpff", "D.M. Roijers", "F.A. Oliehoek", "M. Spaan", "M.M. de Weerdt"],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
    "authors": ["W.R. Thompson"],
    "year": 1933
  }, {
    "title": "Yaw-misalignment and its impact on wind turbine loads and wind farm power output",
    "authors": ["M.T. Van Dijk", "J.W. Wingerden", "T. Ashuri", "Y. Li", "M. Rotea"],
    "venue": "Journal of Physics: Conference Series,",
    "year": 2016
  }, {
    "title": "Multi-agent reinforcement learning for traffic light control",
    "authors": ["M. Wiering"],
    "venue": "In Machine Learning: Proceedings of the Seventeenth International Conference (ICML’2000),",
    "year": 2000
  }],
  "id": "SP:b10ba92e07ea3ab350fd35364b537f3c13401898",
  "authors": [{
    "name": "Eugenio Bargiacchi",
    "affiliations": []
  }, {
    "name": "Timothy Verstraeten",
    "affiliations": []
  }, {
    "name": "Diederik M. Roijers",
    "affiliations": []
  }, {
    "name": "Ann Nowé",
    "affiliations": []
  }, {
    "name": "Hado van Hasselt",
    "affiliations": []
  }],
  "abstractText": "Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents. In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.",
  "title": "Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems"
}