{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Generative adversarial networks (GANs) (Goodfellow et al., 2014) have emerged as a powerful framework for modeling the draw of samples from complex data distributions. When trained on datasets of natural images, significant progress has been made on synthesizing realistic and sharp-looking images (Radford et al., 2016). Recent work has also extended the GAN framework for the challenging task of text generation (Yu et al., 2017; Zhang et al., 2017b). However, in its standard form, GAN models distributions in one domain, i.e., for a single random variable.\nThis work was done when Yunchen Pu, Zhe Gan and Yizhe Zhang were Ph.D. students at Duke University. 1Facebook, Menlo Park, CA, USA 2Duke University, Durham, NC, USA 3Microsoft Research, Redmond, WA, USA. Correspondence to: Yunchen Pu <pyc40@fb.com>, Shuyang Dai <shuyang.dai@duke.edu>, Zhe Gan <zhe.gan@microsoft.com>, Lawrence Carin <lcarin@duke.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nThere has been recent interest in employing GAN ideas to learn conditional distributions for two random variables. This setting is of interest when one desires to synthesize (or infer) one random variable given an instance of another random variable. Example applications include generative models with (stochastic) latent variables (Mescheder et al., 2017; Tao et al., 2018), and conditional data synthesis (Isola et al., 2017; Reed et al., 2016), when both domains consist of observed pairs of random variables.\nIn this paper we focus on learning the joint distribution of multiple random variables using adversarial training. For the case of two random variables, conditional GAN (Mirza & Osindero, 2014) and Triangle GAN (Gan et al., 2017a) have been utilized for this task in the case that paired data are available. Further, adversarially learned inference (ALI) (Dumoulin et al., 2017; Donahue et al., 2017) and CycleGAN (Zhu et al., 2017; Kim et al., 2017; Yi et al., 2017) were developed for unsupervised learning, where the twoway mappings between two domains are learned without any paired data. These models are unified as the joint distribution matching problem by Li et al. (2017a). However, in all previous approaches the joint distributions are not fully learned, i.e., the model only learns to sample from the conditional distributions, assuming access to the marginal distributions, which are typically instantiated as empirical samples from each individual domain (see Figure 1(b) for illustration). Therefore, only conditional data synthesis can be achieved due to the lack of a learned sample mechanism for the marginals.\nIt is desirable to build a generative-model learning framework from which one may sample from a fully-learned joint distribution. We design a new GAN framework that learns the joint distribution by decomposing it into the product of a marginal and a conditional distribution(s), each learned via adversarial training (see Figure 1(c) for illustration). The resulting model may then be employed in several distinct applications: (i) synthesis of draws from any of the marginals; (ii) synthesis of draws from the conditionals when other random variables are observed, i.e., imputation; (iii) or we may simultaneously draw all random variables from the joint distribution.\nFor the special case of two random variables, the proposed model consists of four generators and a softmax critic function. The design includes two generators for the marginals, two for the conditionals, and a single 5-way critic (discriminator) trained to distinguish pairs of real data from four different kinds of synthetic data. These five modules are implemented as neural networks, which share parameters for efficiency and are optimized jointly via adversarial learning. We also consider an example with three random variables.\nThe contributions of this work are summarized as follows. (i) We present the first GAN-enabled framework that allows for full joint-distribution learning of multiple random variables. Unlike existing models, the proposed framework learns marginals and conditionals simultaneously. (ii) We share parameters of the generator models, and thus the resulting model does not have a significant increase in the number of parameters relative to prior work that only considered conditionals (ALI, Triangle GAN, CycleGAN, etc.) or marginals (traditional GAN). (iii) Unlike existing approaches, we consolidate all real vs. artificial sample comparisons into a single softmax-based critic function. (iv) While the main focus is on the case of two random variables, we extend the proposed model to learning the joint distribution of three or more random variables. (v) We apply the proposed model for both unsupervised and supervised learning paradigms."
  }, {
    "heading": "2. Background",
    "text": "To simplify the presentation, we first consider joint modeling of two random variables, with the setup generalized in Sec. 3.2 to the case of more than two domains. For the two-random-variable case, consider marginal distributions q(x) and q(y) defined over two random variables x ∈ X and y ∈ Y , respectively. Typically, we have samples but not an explicit density form for q(x) and q(y), i.e., ensembles {xi}Ni=1 and {yj}Mj=1 are available for learning. In general, their joint distribution can be written as the product of a marginal and a conditional in two ways: q(x,y) = q(x)q(y|x) = q(y)q(x|y). One random variable can be synthesized (or inferred) given the other using conditional distributions, q(x|y) and q(y|x)."
  }, {
    "heading": "2.1. Generative Adversarial Networks",
    "text": "Nonparametric sampling from marginal distributions q(x) and q(y) can be accomplished via adversarial learning (Goodfellow et al., 2014), which provides a sampling mechanism that only requires gradient backpropagation, avoiding the need to explicitly adopt a form for the marginals. Specifically, instead of sampling directly from an (assumed) parametric distribution for the desired marginal, the target random variable is generated as a deterministic transformation of an easy-to-sample, independent\nnoise source, e.g., Gaussian distribution. The sampling procedure for the marginals, implicitly defined as x ∼ pα(x) and y ∼ pβ(y), is carried out through the following two generative processes:\nx̃ = fα( 1), 1 ∼ p( 1) , (1) ỹ = fβ( ′ 1), ′ 1 ∼ p( ′1) , (2)\nwhere fα(·) and fβ(·) are two marginal generators, specified as neural networks with parameters α and β, respectively. p( 1) and p( ′1) are assumed to be simple distributions, e.g., isotropic Gaussian. The generative processes manifested by (1) and (2) are illustrated in Figure 1(a). Within the models, stochasticity in x and y is manifested via draws from p( 1) and p( ′1), and respective neural networks fα(·) and fβ(·) transform draws from these simple distributions such that they are approximately consistent with draws from q(x) and q(y).\nFor this purpose, GAN trains a ω-parameterized critic function gω(x), to distinguish samples generated from pα(x) and q(x). Formally, the minimax objective of GAN is\nmin α max ω LGAN(α,ω) = Ex∼q(x)[log σ(gω(x))] (3)\n+ E 1∼p( 1)[log(1− σ(gω(x̃)))] ,\nwith expectations Ex∼q(x)[·] and E 1∼p( 1)[·] approximated via sampling, and σ(·) is the sigmoid function. As shown in Goodfellow et al. (2014), the equilibrium of the objective in (3) is achieved if and only if pα(x) = q(x).\nSimilarly, we can design a corresponding minimax objective that is similar to (3) to match the marginal pβ(y) to q(y)."
  }, {
    "heading": "2.2. Adversarially Learned Inference",
    "text": "In the same spirit, sampling from conditional distributions q(x|y) and q(y|x) can be also achieved as a deterministic transformation of two inputs, the variable in the source domain as a covariate, plus an independent source of noise. Specifically, the sampling procedure for the conditionals y ∼ pθ(y|x) and x ∼ pφ(x|y) is modeled as\nỹ = fθ(x, 2), x ∼ q(x), 2 ∼ p( 2) , (4) x̃ = fφ(y, ′ 2), y ∼ q(y), ′2 ∼ p( ′2) , (5)\nwhere fθ(·) and fφ(·) are two conditional generators, specified as neural networks with parameters θ and φ, respectively. In practice, the inputs of fθ(·) and fφ(·) are concatenated. As in GAN, p( 2) and p( ′2) are two simple distributions that provide the stochasticity when generating y given x, and vice versa. The conditional generative processes manifested in (4) and (5) are illustrated in Figure 1(b),\nALI (Dumoulin et al., 2017) learns the two desired conditionals by matching joint distributions pθ(x,y) = q(x)pθ(y|x) and pφ(x,y) = q(y)pφ(x|y), using a critic function gω(x,y) similar to (3). The minimax objective of ALI can be written as\nmin θ,φ max ω E(x,ỹ)∼pθ(x,y)[log σ(gω(x, ỹ))] (6)\n+ E(x̃,y)∼pφ(x,y)[log(1− σ(gω(x̃,y)))] .\nThe equilibrium of the objective in (6) is achieved if and only if pθ(x,y) = pφ(x,y).\nWhile ALI is able to match joint distributions using (6), only conditional distributions pθ(y|x) and pφ(x|y) are learned, thus assuming access to (samples from) the true marginal distributions q(x) and q(y), respectively."
  }, {
    "heading": "3. Adversarial Joint Distribution Learning",
    "text": "Below we discuss fully learning the joint distribution of two random variables in both supervised and unsupervised settings. By “supervised” it is meant that we have access to joint empirical data (x,y), and by “unsupervised” it is meant that we have access to empirical draws ofx and y, but not paired observations (x,y) from the joint distribution."
  }, {
    "heading": "3.1. JointGAN",
    "text": "Since q(x,y) = q(x)q(y|x) = q(y)q(x|y), a simple way to achieve joint-distribution learning is to first learn models\nfor the two marginals separately, using a pair of traditional GANs, followed by training an independent ALI model for the two conditionals. However, such a two-step training procedure is suboptimal, as there is no information flow between marginals and conditionals during training. This suboptimality is demonstrated in the experiments. Additionally, a two-step learning process becomes cumbersome when considering more than two random variables.\nAlternatively, we consider learning to sample from conditionals via pθ(y|x) and pφ(x|y), while also learning to sample from marginals via pα(x) and pβ(y). All model training is performed simultaneously. We term our model JointGAN for full GAN analysis of joint random variables.\nAccess to Paired Empirical Draws In this setting, we assume access to samples from the true (empirical) joint distribution q(x,y). The models we seek to learn constitute two means of approximating draws from the true distribution q(x,y), i.e., pα(x)pθ(y|x) and pβ(y)pφ(x|y), as shown in Figure 1(c):\nx̃ = fα( 1), ỹ = fθ(x̃, 2), (7) ŷ = fβ( ′ 1), x̂ = fφ(ŷ, ′ 2), (8)\nwhere fα(·), fβ(·), fθ(·) and fφ(·) are neural networks as defined previously. 1, 2, ′1 and ′ 2 are independent noise. Note that the only difference between fα(·) and fφ(·) is that the function fφ(·) has another conditional input y when compared with fα(·). Therefore, in implementation, we couple the parameters α and φ together. Similarly, β and θ are also coupled together. Specifically, fα(·) and fβ(·) are implemented as\nfα(·) = fφ(0, ·), fβ(·) = fθ(0, ·) , (9)\nwhere 0 is an all-zero tensor which has the same size as x or y. As a result, (7) and (8) have almost the same number of parameters as ALI-like approaches.\nThe following notation is introduced for simplicity of illustration:\np1(x,y) = q(x)pθ(y|x), p2(x,y) = q(y)pφ(x|y) p3(x,y) = pα(x)pθ(y|x), p4(x,y) = pβ(y)pφ(x|y) p5(x,y) = q(x,y) , (10)\nwhere p1(x,y), p2(x,y), p3(x,y) and p4(x,y) are implicitly defined in (4), (5), (7) and (8). Note that p5(x,y) is simply the empirical joint distribution.\nWhen learning, we wish to impose that the five distributions in (10) should be identical. Toward this end, an adversarial objective is specified. Joint pairs (x,y) are drawn from the five distributions in (10), and a critic function is learned to discriminate among them, while the four generators are\ntrained to mislead the critic. Naively, for JointGAN, one can use 4 binary critics to mimic a 5-class classifier. Departing from previous work such as Gan et al. (2017a), here the discriminator is implemented directly as a 5-way softmax classifier. Compared with using multiple binary classifiers, this design is more principled in that we avoid multiple critics resulting in possibly conflicting (real vs. synthetic) assessments.\nLet the critic gω(x,y) ∈ ∆4 (in the 4-simplex) be a ωparameterized neural network with softmax on the top layer, i.e., ∑5 k=1 gω(x,y)[k] = 1 and gω(x,y)[k] ∈ (0, 1), where gω(x,y)[k] is an entry of gω(x,y). The minimax objective for JointGAN, LJointGAN(θ,φ,ω), is given by\nmin θ,φ max ω LJointGAN(θ,φ,ω) (11)\n= ∑5\nk=1Epk(x,y)[log gω(x,y)[k]] .\nThe above objective (11) has taken into consideration the model design that α and φ are coupled together, with the same for β and θ; thusα and β are not present in (11). Note that expectation Ep5(x,y)[·] is approximated using empirical joint samples, expectations Ep3(x,y)[·] and Ep4(x,y)[·] are both approximated with purely synthesized joint samples, while Ep1(x,y)[·] and Ep2(x,y)[·] are approximated using conditionally synthesized samples, given samples from the empirical marginals. The following proposition characterizes the solution of (11) in terms of the joint distributions.\nProposition 1 The equilibrium for the minimax objective in (11) is achieved if and only if p1(x,y) = p2(x,y) = p3(x,y) = p4(x,y) = p5(x,y).\nThe proof is provided in Appendix A.\nNo Access to Paired Empirical Draws When paired data samples are not available, we do not have access to draws from p5(x,y) = q(x,y), so this term is not considered in (11). Instead, we wish to impose “cycle consistency” (Zhu et al., 2017), i.e., q(x) → x → pθ(y|x) → y → pφ(x|y)→ x̂ yields small ||x− x̂||, for an appropriate norm. Similarly, we impose q(y)→ y → pφ(x|y)→ x→ pθ(y|x)→ ŷ resulting in small ||y − ŷ||.\nIn this case, the discriminator becomes a 4-class classifier. Let g′ω(x,y) ∈ ∆3 denote a new critic, with softmax on the top layer, i.e., ∑4 k=1 g ′ ω(x,y)[k] = 1 and g′ω(x,y)[k] ∈ (0, 1). To encourage cycle consistency, we modify the objective in (11) as\nmin θ,φ max ω LJointGAN(θ,φ,ω) (12)\n= ∑4\nk=1Epk(x,y)[log g′ω(x,y)[k]] +Rθ,φ(x,y) ,\nwhere Rθ,φ(x,y) in (12) is a cycle-consistency regulariza-\ntion term specified as\nRθ,φ(x,y) = Ex∼q(x),y∼pθ(y|x),x̂∼pφ(x|y)||x− x̂|| + Ey∼q(y),x∼pφ(x|y),ŷ∼pθ(y|x)||y − ŷ|| ."
  }, {
    "heading": "3.2. Extension to multiple domains",
    "text": "The above formulation may be extended to the case of three or more joint random variables. However, for m random variables, there are m! different ways in which the joint distribution can be factorized. For example, for joint random variables (x,y, z), there are possibly six different forms of the model. One must have access to all the six instantiations of these models, if the goal is to be able to generate (impute) samples from all conditionals. However, not all modeled forms of p(x,y, z) need to be considered, if there is not interest in the corresponding form of the conditional. Below, we consider two specific forms of the model:\np(x,y, z) = pα(x)pν(y|x)pγ(z|x,y) (13) = pβ(z)pψ(y|z)pη(x|y, z) . (14)\nTypically, the joint draws from q(x,y, z) may not be easy to access; therefore, we assume that only empirical draws from q(x,y) and q(y, z) are available. For the purpose of adversarial learning, we let the critic be a 6-class softmax classifier that aims to distinguish samples from the following 6 distributions:\npα(x)pν(y|x)pγ(z|x,y) , pβ(z)pψ(z|y)pη(x|y, z) q(x)pν(y|x)pγ(z|x,y) , q(x)pψ(z|y)pη(x|y, z)\nq(x,y)pγ(z|x,y) , q(y, z)pη(x|y, z) .\nAfter training, one may synthesize (x,y, z), impute (y, z) from observed x, or impute z from (x,y), etc. Examples of this learning paradigm is demonstrated in the experiments. Interestingly, when implementing a sampling-based method for the above models, skip connections are manifested naturally as a result of the partitioning of the joint distribution,\ne.g., via pγ(z|x,y) and pη(x|y, z). This is illustrated in Figure 2, for pα(x)pν(y|x)pγ(z|x,y)."
  }, {
    "heading": "4. Related work",
    "text": "Adversarial methods for joint distribution learning can be roughly divided into two categories, depending on the application: (i) generation and inference if one of the domains consists of (stochastic) latent variables, and (ii) conditional data synthesis if both domains consists of observed pairs of random variables. Below, we review related work from these two perspectives.\nGeneration and inference The joint distribution of data and latent variables or codes can be considered in two (symmetric) forms: (i) from observed data samples fed through the encoder to yield codes, i.e., inference, and (ii) from codes drawn from a simple prior and propagated through the decoder to manifest data samples, i.e., generation. ALI (Dumoulin et al., 2017) and BiGAN (Donahue et al., 2017) proposed fully adversarial methods for this purpose. There are also many recent works concerned with integrating variational autoencoder (VAE) (Kingma & Welling, 2013; Pu et al., 2016) and GAN concepts for improved data generation and latent code inference (Hu et al., 2017). Representative work includes the AAE (Makhzani et al., 2015), VAE-GAN (Larsen et al., 2015), AVB (Mescheder et al., 2017), AS-VAE (Pu et al., 2017), SVAE (Chen et al., 2018), etc.\nConditional data synthesis Conditional GAN can be readily used for conditional-data synthesis if paired data are available. Multiple conditional GANs have been proposed to generate the images based on class labels (Mirza & Osindero, 2014), attributes (Perarnau et al., 2016), text (Reed et al., 2016; Xu et al., 2017) and other images (Isola et al., 2017). Often, only the mapping from one direction (a single conditional) is learned. Triangle GAN (Gan et al., 2017a) and Triple GAN (Li et al., 2017b) can be used to learn bi-directional mappings (both conditionals) in a semisupervised learning setup. Unsupervised learning methods were also developed for this task. CycleGAN (Zhu et al., 2017) proposed to use two generators to model the conditionals and two critics to decide whether a generated sample is synthesized, in each individual domain. Further, additional reconstruction losses were introduced to impose cycle consistency. Similar work includes DiscoGAN (Kim et al., 2017), DualGAN (Yi et al., 2017) and UNIT (Liu et al., 2017).\nCoGAN (Liu & Tuzel, 2016) can be used to achieve joint distribution learning. However, the joint distribution is only roughly approximated by the marginals, via sharing lowlayer weights of the generators, hence not learning the true (empirical) joint distributions in a principled way.\nAll the other previously proposed models focus on learning\nto sample from the conditionals given samples from one of the true (empirical) marginals, while the proposed model, to the best of the authors’ knowledge, is the first attempt to learn a full joint distribution of two or more observed random variables. Moreover, this paper presents the first consolidation of multiple binary critics into a single unified softmax-based critic.\nWe observe that the proposed model, JointGAN, may follow naturally in concept from GAN (Goodfellow et al., 2014) and ALI (Donahue et al., 2017; Dumoulin et al., 2017). However, there are several keys to obtaining good performance. Specifically, (i) the condition distribution setup naturally yields skip connections in the architecture. (ii) Compared with using multiple binary critics, the softmaxbased critic can be considered as sharing the parameters among all the binary critics except the top layer. This also imposes the critic embedding the generated samples from different ways into a common latent space and reduces the number of parameters. (iii) The weight-sharing constraint among generators enforces that synthesized images from the marginal and conditional generator share a common latent space, and also further reduces the number of parameters in the network."
  }, {
    "heading": "5. Experiments",
    "text": "Adam (Kingma & Ba, 2014) with learning rate 0.0002 is utilized for optimization of the JointGAN objectives. All noise vectors 1, 2, ′1 and ′ 2 are drawn from a N (0, I) distribution, with the dimension of each set to 100. Besides the results presented in this section, more results can be found in Appendix C.2. The code can be found at https: //github.com/sdai654416/Joint-GAN."
  }, {
    "heading": "5.1. Joint modeling multi-domain images",
    "text": "Datasets We present results on five datasets: edges↔ shoes (Yu & Grauman, 2014), edges↔handbags (Zhu et al., 2016), Google maps↔aerial photos (Isola et al., 2017), labels↔facades (Tyleček & Šára, 2013) and labels ↔cityscapes (Cordts et al., 2016). All of these datasets are two-domain image pairs.\nFor three-domain images, we create a new dataset by combining labels↔facades pairs and labels↔cityscapes pairs into facades↔labels↔cityscapes tuples. In this dataset, only empirical draws from q(x,y) and q(y, z) are available. Another new dataset is created based on MNIST, where the three image domains are the MNIST images, clockwise transposed ones, and anticlockwise transposed ones.\nBaseline As described in Sec. 3.1, a two-step model is implemented as the baseline. Specifically, WGAN-GP (Gulrajani et al., 2017) is employed to model the two marginals; Pix2pix (Isola et al., 2017) and CycleGAN (Zhu et al., 2017) are utilized to model the conditionals for the case with and\nwithout access to paired empirical draws, respectively.\nNetwork Architectures For generators, we employed the U-net (Ronneberger et al., 2015) which has been demonstrated to achieve impressive results for image-to-image translation. Following Isola et al. (2017), PatchGAN is employed for the discriminator, which provides real vs. synthesized prediction on 70× 70 overlapping image patches."
  }, {
    "heading": "5.1.1. QUALITATIVE RESULTS",
    "text": "Figures 3 and 4 show the results trained on paired data. All the image pairs are generated from random noise. For Figure 4, we first draw ( 1, 2) and ( ′1, ′ 2) to generate the top-left image pairs and bottom-right image pairs according to (7). All remaining image pairs are generated from the noise pair made by linear interpolation between 1 and ′1, and between 2 and ′ 2, respectively, also via (7). For Figure 3, in each row of the left block, the column is first\ngenerated from pα(x), and then the images of the right part are generated based on the leftmost image and an additional noise vector linear-interpolated between two random points 2 and ′2. The images in the right block are produced in a similar way.\nThese results demonstrate that our model is able to generate both realistic and highly coherent image pairs. In addition, the interpolation experiments illustrate that our model maintains smooth transitions in the latent space, with each point in the latent space corresponding to a plausible image. For example, in the edges↔handbags dataset, it can be seen that the edges smoothly transforming from complicated structures into simple ones, and the color of the handbags transforming from black to red. The quality of images generated from the baseline is much worse than ours, and are provided in Appendix C.1.\nFigure 5 shows the generated samples trained on unpaired data. Our model is able to produce image pairs whose quality are close to the samples trained on paired data.\nFigures 6 and 7 show the generated samples from models trained on three-domain images. The generated images in each tuple are highly correlated. Interestingly, in Figure 7, the synthesized labels strive to be consistent with both the generated street scene and facade photos."
  }, {
    "heading": "5.1.2. QUANTITATIVE RESULTS",
    "text": "We perform a detailed quantitative analysis on the twodomain image-pair task.\nHuman Evaluation We perform human evaluation using Amazon Mechanical Turk (AMT), and present human eval-\nuation results on the relevance and realism of generated pairs in both the cases with or without access to paired empirical samples. In each survey, we compare JointGAN and the two-step baseline by taking a random sample of 100 generated image pairs (5 datasets, 20 samples on each dataset), and ask the human evaluator to select which sample is more realistic and the content of which pairs are more relevant. We obtained roughly 44 responses per data sample (4378 samples in total) and the results are shown in Table 1. Clearly, human analysis suggest that our JointGAN produces higher-quality samples when compared with the\ntwo-step baseline, verifying the effectiveness of learning the marginal and conditional simultaneously.\nRelevance Score We use relevance score to evaluate the quality and relevance of two generated images. The relevance score is calculated as the cosine similarity between two images that are embedded into a shared latent space, which are learned via training a ranking model (Huang et al., 2013). Details are provided in Appendix B. The final relevance score is the average over all the individual relevance scores on each pair. Results are summarized in Table 2. Our JointGAN provides significantly better results than the two-step baselines, especially when we do not have access to the paired empirical samples.\nBesides the results of our model and baselines, we also present results on three types of real images: (i) True pairs: this is the real image pairs from the same dataset but not used for training the ranking model; (ii) Random pairs: the images are from the same dataset but the content of two images are not correlated; (iii) Other pairs: the images are correlated but sampled from a dataset different from the training set. We can see in Table 2 that the first one obtains a high relevance score while the latter two have a very low score, which shows that the relevance score metric assigns a low value when either the content of generated image pairs is not correlated or the images are not plausibly like the training set. It demonstrates that this metric correlates well with the quality of generated image pairs."
  }, {
    "heading": "5.2. Joint modeling caption features and images",
    "text": "Setup Our model is next evaluated on the Caltech-UCSD Birds dataset (Welinder et al., 2010), in which each image of bird is paired with 10 different captions. Since generating realistic text using GAN itself is a challenging task, in this work, we train our model on pairs of caption features and images. The caption features are obtained from a pretrained word-level CNN-LSTM autoencoder (Gan et al., 2017b), which aims to achieve a one-to-one mapping between the captions and the features. We then train JointGAN based on the caption features and their corresponding images (the paired data for training JointGAN use CNN-generated text\nfeatures, which avoids issues of training GAN for text generation). Finally to visualize the results, we use the pretrained LSTM decoder to decode the generated features back to captions. We employ StackGAN-stage-I (Zhang et al., 2017a) for generating images from caption features while a CNN is\nutilized to generate caption features from images. Details are provided in Appendix D.\nQualitative Results Figure 8 shows the qualitative results of JointGAN: (i) generate images from noise and then conditionally generate caption features, and (ii) generate caption features from noise and then conditionally generate images. The results show high-quality and diverse image generation, and strong coherent relationship between each pair of the caption feature and image. It demonstrates the robustness of our model, in that it not only generates realistic multidomain images but also handles well different datasets such as caption feature and image pairs."
  }, {
    "heading": "6. Conclusion",
    "text": "We propose JointGAN, a new framework for multi-domain joint distribution learning. The joint distribution is learned via decomposing it into the product of a marginal and a conditional distribution(s), each learned via adversarial training. JointGAN allows interesting applications since it provides freedom to draw samples from various marginalized or conditional distributions. We consider joint analysis of two and three domains, and demonstrate that JointGAN achieves significantly better results than a two-step baseline model, both qualitatively and quantitatively."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by DARPA, DOE, NIH, ONR and NSF."
  }],
  "year": 2018,
  "references": [{
    "title": "Symmetric variational autoencoder and connections to adversarial learning",
    "authors": ["L. Chen", "S. Dai", "Y. Pu", "E. Zhou", "C. Li", "Q. Su", "C. Chen", "L. Carin"],
    "year": 2018
  }, {
    "title": "The cityscapes dataset for semantic urban scene understanding",
    "authors": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"],
    "year": 2016
  }, {
    "title": "Adversarial feature learning",
    "authors": ["J. Donahue", "P. Krähenbühl", "T. Darrell"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Adversarially learned inference",
    "authors": ["V. Dumoulin", "I. Belghazi", "B. Poole", "O. Mastropietro", "A. Lamb", "M. Arjovsky", "A. Courville"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Triangle generative adversarial networks",
    "authors": ["Z. Gan", "L. Chen", "W. Wang", "Y. Pu", "Y. Zhang", "H. Liu", "C. Li", "L. Carin"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Learning generic sentence representations using convolutional neural networks",
    "authors": ["Z. Gan", "Y. Pu", "R. Henao", "C. Li", "X. He", "L. Carin"],
    "venue": "In EMNLP,",
    "year": 2017
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 2014
  }, {
    "title": "Improved training of Wasserstein GANs",
    "authors": ["I. Gulrajani", "F. Ahmed", "M. Arjovsky", "V. Dumoulin", "A.C. Courville"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "On unifying deep generative models",
    "authors": ["Z. Hu", "Z. Yang", "R. Salakhutdinov", "E.P. Xing"],
    "venue": "arXiv preprint arXiv:1706.00550,",
    "year": 2017
  }, {
    "title": "Learning deep structured semantic models for web search using clickthrough data",
    "authors": ["Huang", "P.-S", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"],
    "venue": "In CIKM,",
    "year": 2013
  }, {
    "title": "Image-toimage translation with conditional adversarial networks",
    "authors": ["P. Isola", "Zhu", "J.-Y", "T. Zhou", "A.A. Efros"],
    "year": 2017
  }, {
    "title": "Learning to discover cross-domain relations with generative adversarial networks",
    "authors": ["T. Kim", "M. Cha", "H. Kim", "J. Lee", "J. Kim"],
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Autoencoding beyond pixels using a learned similarity metric",
    "authors": ["A.B.L. Larsen", "S.K. Sønderby", "H. Larochelle", "O. Winther"],
    "venue": "arXiv preprint arXiv:1512.09300,",
    "year": 2015
  }, {
    "title": "Towards understanding adversarial learning for joint distribution matching",
    "authors": ["C. Li", "H. Liu", "C. Chen", "Y. Pu", "L. Chen", "R. Henao", "Carin", "L. Alice"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Triple generative adversarial nets",
    "authors": ["C. Li", "K. Xu", "J. Zhu", "B. Zhang"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Coupled generative adversarial networks",
    "authors": ["Liu", "M.-Y", "O. Tuzel"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Unsupervised imageto-image translation networks",
    "authors": ["Liu", "M.-Y", "T. Breuel", "J. Kautz"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks",
    "authors": ["L. Mescheder", "S. Nowozin", "A. Geiger"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Conditional generative adversarial nets",
    "authors": ["M. Mirza", "S. Osindero"],
    "venue": "In arXiv preprint arXiv:1411.1784,",
    "year": 2014
  }, {
    "title": "Invertible conditional gans for image editing",
    "authors": ["G. Perarnau", "J. van de Weijer", "B. Raducanu", "J.M. Álvarez"],
    "venue": "arXiv preprint arXiv:1611.06355,",
    "year": 2016
  }, {
    "title": "Variational autoencoder for deep learning of images, labels and captions",
    "authors": ["Y. Pu", "Z. Gan", "R. Henao", "X. Yuan", "C. Li", "A. Stevens", "L. Carin"],
    "year": 2016
  }, {
    "title": "Adversarial symmetric variational autoencoder",
    "authors": ["Y. Pu", "W. Wang", "R. Henao", "L. Chen", "Z. Gan", "C. Li", "L. Carin"],
    "year": 2017
  }, {
    "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
    "authors": ["A. Radford", "L. Metz", "S. Chintala"],
    "year": 2016
  }, {
    "title": "Generative adversarial text to image synthesis",
    "authors": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"],
    "year": 2016
  }, {
    "title": "U-net: Convolutional networks for biomedical image segmentation",
    "authors": ["O. Ronneberger", "P. Fischer", "T. Brox"],
    "venue": "In MICCAI,",
    "year": 2015
  }, {
    "title": "Chisquare generative adversarial network",
    "authors": ["C. Tao", "L. Chen", "R. Henao", "J. Feng", "L. Carin"],
    "venue": "In ICML,",
    "year": 2018
  }, {
    "title": "Spatial pattern templates for recognition of objects with regular structure",
    "authors": ["R. Tyleček", "R. Šára"],
    "venue": "In GCPR,",
    "year": 2013
  }, {
    "title": "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
    "authors": ["T. Xu", "P. Zhang", "Q. Huang", "H. Zhang", "Z. Gan", "X. Huang", "X. He"],
    "venue": "arXiv preprint arXiv:1711.10485,",
    "year": 2017
  }, {
    "title": "Dualgan: Unsupervised dual learning for image-to-image translation",
    "authors": ["Z. Yi", "H. Zhang", "P. Tan", "M. Gong"],
    "year": 2017
  }, {
    "title": "Fine-grained visual comparisons with local learning",
    "authors": ["A. Yu", "K. Grauman"],
    "venue": "In CVPR,",
    "year": 2014
  }, {
    "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
    "authors": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
    "authors": ["H. Zhang", "T. Xu", "H. Li", "S. Zhang", "D. Metaxas"],
    "venue": "In ICCV,",
    "year": 2017
  }, {
    "title": "Adversarial feature matching for text generation",
    "authors": ["Y. Zhang", "Z. Gan", "K. Fan", "Z. Chen", "R. Henao", "D. Shen", "L. Carin"],
    "venue": "In ICML,",
    "year": 2017
  }, {
    "title": "Generative visual manipulation on the natural image manifold",
    "authors": ["Zhu", "J.-Y", "P. Krähenbühl", "E. Shechtman", "A.A. Efros"],
    "venue": "In ECCV,",
    "year": 2016
  }, {
    "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
    "authors": ["Zhu", "J.-Y", "T. Park", "P. Isola", "A.A. Efros"],
    "year": 2017
  }],
  "id": "SP:b701f11ecf5d465c7d5c427914db2ad8c97bb8a9",
  "authors": [{
    "name": "Yunchen Pu",
    "affiliations": []
  }, {
    "name": "Shuyang Dai",
    "affiliations": []
  }, {
    "name": "Zhe Gan",
    "affiliations": []
  }, {
    "name": "Weiyao Wang",
    "affiliations": []
  }, {
    "name": "Guoyin Wang",
    "affiliations": []
  }, {
    "name": "Yizhe Zhang",
    "affiliations": []
  }, {
    "name": "Ricardo Henao",
    "affiliations": []
  }, {
    "name": "Lawrence Carin",
    "affiliations": []
  }],
  "abstractText": "A new generative adversarial network is developed for joint distribution matching. Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain. The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning. From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.",
  "title": "JointGAN: Multi-Domain Joint Distribution Learning with  Generative Adversarial Nets"
}