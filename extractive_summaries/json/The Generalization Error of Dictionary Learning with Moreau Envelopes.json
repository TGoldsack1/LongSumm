{
  "sections": [{
    "text": "Φ(x) = inf z∈Rm\n||x− z||22 + h(||z||2)\nand h is an even and univariate function on the real line. Connections are drawn between Φ and the Moreau envelope of h. A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works. Finally comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O( √ log n/n)."
  }, {
    "heading": "1. Introduction",
    "text": "The dictionary learning problem, also known as sparse coding, was initially studied in the context of Neuroscience (Olshausen & Field, 1997); the relevant literature has grown enormously since; see (Zhang et al., 2015) and references therein. The problem is described as follows: given set {Xi}ni=1 ⊂ Rm with n points sampled from an unknown fixed probability measure µ, a dictionary matrixD ∈ Rm×d is to be constructed so that any sample from µ can be approximated well by linear combinations of columns of D. The quality of approximation, for a given dictionary D, is measured by some function fD while D usually belongs\n1School of Electrical and Computer Engineering, Technical University of Crete, Greece. Correspondence to: Alexandros Georgogiannis <alexandrosgeorgogiannis@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nto some predefined family of matrices. From the statistical learning theory perspective, the aim is to minimize the population risk\nR(D) := ∫ fD(X)dµ = ∫ fDdµ, (1)\nwhen the only accessible information is a set of n training samples, say {Xi}ni=1, usually independent and identically distributed. Notation X is used for random vectors sampled from µ and notation x for real vectors in Rm.\nThe empirical risk minimization principle (ERM) is a natural approach in search of the best dictionary (Vapnik, 1998). It suggests that since the only availiable information is the set of training samples, one should search for the matrix D̂n that minimizes the empirical risk\nRn(D) := 1\nn n∑ i=1 fD(Xi). (2)\nThe empirical estimate D̂n is not of much use unless |Rn(D̂n) − R(D̂n)| decreases as the number of samples n increases. Subsuming all computational difficulties on computing the global minimizing argument of (2), the problem addressed here is a “generalization problem”. Given the family D of all m× d matrices with unit-norm columns, we design a loss function fD that measures the quality of approximation x ' Da and ask: Does the difference\n|R(D̂n)− inf D∈D R(D)| = ∣∣∣∣∫ fD̂ndµ− infD∈D ∫ fDdµ ∣∣∣∣ (3) decrease as the number of samples n increases, and if so, at what rate? Or even further, if R(D̂n) is close to infD∈DR(D), is D̂n close to the global minimizing argument of R(D)? Intuitively, the decrement of the absolute difference in expression (3) guarantees that by increasing the amount of data the population risk, with high probability, is within a very small distance of the optimal achievable gets arbitrarily close to one. The answers to the previous questions of course depend on the number of samples, the predefined family of dictionaries and the loss function.\nThe proposed loss functions in the literature of dictionary learning vary according to the application but it would not be an exaggeration to say that almost all of them may be\ndescribed by a function of the form:\nfD(x) := inf a∈Rd\nΦ(x−Da) + g(a), (4)\nwith Φ : Rm → [0,+∞) and g : Rd → [0,+∞]. This article focuses on the generalization properties of dictionary learning when Φ has the form:\nΦ(x−Da) := inf z∈Rm\n1 2 ||x−Da− z||22 + h(||z||2). (5)\nFunction h takes values on [0,+∞] and is described in further detail later on. Definition (5) is not novel and has been used in many applications of sparse coding, robust linear regression and dictionary learning (Adler et al., 2015; Amini et al., 2014; Forero et al., 2015; 2017; Jiang et al., 2015; Liu et al., 2015; Zhao & Tan, 2017). Although there is no formal robustness analysis yet to justify the superiority of (5) over the common square Euclidean loss ||x−Da||22, experimental evaluations in the previous applications suggest that this modification is a computationally “cheap” alternative, achieving better reconstruction error in some cases.\nAs can be seen from (4), if g is a sparsity promoting penalty then approximations that are linear combinations of a few columns of D are favored. The rationale behind the choice of h in (5) is not so obvious but if h satisfies a set of assumptions, then the following simplification holds true:\nΦ(x−Da) := eh(||x−Da||2). (6)\nHere, eh is a univariate continuous function with special name and properties, the so called Moreau envelope of h (Rockafellar & Wets, 2009). Interestingly enough, the epigraphical form of eh is completely determined by the generating function h. Roughly speaking, with a suitably chosen h, we can design loss functions fD able to ignore the influence of points x, the distance of which from their approximation Da is above a predefined threshold. The consistency results of this study should be regarded as complementary extensions−and in some cases refinements−of the generalization bounds in (Gribonval et al., 2015b) and (Vainsencher et al., 2011). Contrary to previous works, all bounds presented here are valid for the whole of space of dictionaries with unit-norm columns.\nIn Section 3 is considered the case where g is a separable function, that is, g is of the form g(a) = ∑d i=1 ĝ(ai), and ĝ : R→ [0,+∞) is univariate, continuous, even, and strictly increasing with minimum value ĝ(0) = 0. These assumptions are valid for many coordinate-separable regularizers, e.g., the lp-norms and variants of the logarithmic function. Let us point out here that if h = 0, using the results of Section 3 we revert to previously known bounds for the penalized squared Euclidean loss fD(x) = infa∈Rd (1/2)‖x−Da‖22 + g(a).\nSection 4 is an attempt to cover, beyond the class of strictly increasing penalties ĝ of Section 3, continuous and bounded\npenalties from robust statistics, such as the MCP or SCAD. This type of penalty functions have achieved widespread use, and to the best of our knowledge, the bounds presented here are among the first that consider them.\nHowever, the extended bounds of Section 4 turn out to be of limited applicability and do not work when g is the indicator function of all k-sparse vectors. To overcome this difficulty, in Section 5, we remove the continuity assumption from g and rely on combinatorial tools from VapnikChervonenkis (VC) theory in order to present bounds valid for any bounded, lower semicontinuous function g. Whenever possible, the sample complexity bounds presented here are compared to similar ones in literature. Next follows a brief overview of the relevant literature."
  }, {
    "heading": "1.1. Related Work and Contribution",
    "text": "The authors in (Gribonval et al., 2015b; Vainsencher et al., 2011) derive sample complexity bounds for the rate of convergence towards 0 of the absolute difference in (3) when Φ(x) = ||x||22, D is a general constraint set, and g(a) ranges from the lp-norms and characteristic functions of compact sets to the indicator function of non-negative vectors or k-sparse vectors. The results in (Maurer & Pontil, 2010) are independent of dimension m, as well as some results in (Vainsencher et al., 2011).\nA closer look on results of (Gribonval et al., 2015b) and (Vainsencher et al., 2011) concerning the finite case for dimension d, reveals that those are valid under joint assumptions on g and D. For instance, if g is the indicator function of k-sparse vectors in Rd, then the generalization bounds in (Vainsencher et al., 2011) are valid under an incoherence assumption on D while in (Gribonval et al., 2015b) under a “restricted isometry”-like property. General non-asymptotic results can be extracted from the previous analyses, as the case Φ(x) = ω(||x||), for any convex function ω : R→ [0,+∞) and any norm || · || on Rm. In (Liu & Tao, 2016) authors focus on the l1-non-negative matrix factorization problem where Φ(x) = ||x||1 and g is the indicator function of the non-negative orthant in Rd.\nThe main contribution of our work is the addition of generalization bounds concerning loss functions that are combinations of Moreau envelopes with bounded and lower semicontinuous regularizers. Some results are refinements of previously known ones, meaning that a spurious assumption on dictionary D has been removed."
  }, {
    "heading": "2. Preliminaries and some Technical Remarks",
    "text": "This is mainly a technical section where we take a closer look at the loss function fD and describe the statistical framework for the analysis. The value of fD at point x ∈ Rm, in light of equations (4) and (5), is expressed through\nthe solution of the minimization problem:\ninf a∈Rd  := Φ(x−Da)︷ ︸︸ ︷ inf z∈Rm { 1 2 ||x−Da− z||22 + h(||z||2) } +g(a) ︸ ︷︷ ︸ fD(x) . (7) The close connection between Φ and h is captured in Lemma 1 that, among others, gives a description of the set of points z ∈ Rm that achieve the minimum in (5). Lemma 1. Let h : R → [0,+∞] be a lower semicontinuous (lsc) and even function with its restriction on [0,+∞) non-decreasing and h(0) = 0. Assume that the multivalued map Ph : R→→ R, defined as\nPh(t) := argminu∈R 1\n2 (t− u)2 + h(u), (8)\n(H1) is odd, i.e., Ph(−t) = −Ph(t), (H2) compact-valued, (H3) non-decreasing, (H4) has a closed graph and (H5) satisfies Ph(t) ≤ t for all t ∈ R. Then function Φ in (5) becomes\nΦ(x−Da) = eh(||x−Da||2), (9)\nwhere eh : R→ [0,+∞) is defined as\neh(t) := inf u∈R\n1 2 (t− u)2 + h(u), t ∈ R (10)\nand is continuous with its restriction on [0,+∞) being nondecreasing. Furthermore, map Ph : Rm →→ Rm,\nPh(x−Da) := argminz∈Rm 1\n2 ||x−Da−z||22 +h(||z||2),\n(11) is equivalently represented as\nPh(x−Da) = x−Da ||x−Da||2 Ph(||x−Da||2). (12)\nAccording to Lemma 1, if h satisfies a certain set of assumptions, then Φ is equal to the composition of the Moreau envelope of h with the Euclidean norm. Although the restrictions surrounding h and its proximal map seem to be strict, the lemma is valid for a large number of h and Ph pairs; see Section 3.1 in (Antoniadis, 2007) for various examples. Hereafter, any univariate function h in this article satisfies assumptions (H1) through (H5).\nExample 1. The case of the l0-norm on R is an example that clearly describes the influence of h on the boundedness properties of Φ. Let h be the l0-(pseudo)norm on the real line defined as l0(t;λ) = λ 2\n2 1{t 6=0} for some λ > 0. The values of 1{t 6=0} alternate between zero and one according to whether t 6= 0 or not. The l0-norm satisfies all\nassumptions of Lemma 1: it is even, non-decreasing and lower semicontinuous while its proximal map Pl0 equals Pl0(t) = argminu∈R 1 2 (t−u) 2 + l0(u;λ) and is defined as\nPl0(t) =  0, |t| < λ, {0, t}, |t| = λ, t, |t| > λ.\n(13)\nNow function fD : Rm → [0,+∞) reads as\nfD(x) = inf a∈Rd  12 min{||x−Da||22, λ2}︸ ︷︷ ︸ := el0 (||x−Da||2) +g(a)  . (14) Boundedness of el0 implies that whenever the distance ||x−Da∗D(x)||2 between a point x and its best linear approximation Da∗D(x) is greater than the predefined value λ, then el0(||x − Da∗D(x)||2) = λ2/2; here a∗D(x) is the (possibly multivalued) map\na∗D(x) := argmin a∈Rd\n{ el0(||x−Da||22) + g(a) } . (15)\nAs long as g is globally upper bounded by some M > 0, if ||x−Da∗D(x)||2 > λ and a∗D(x) is sufficiently large, then fD(x) = λ\n2/2+M . Since the empirical optimal dictionary D̂n is defined through the minimization of the empirical risk Rn(D) in (2), and Rn is solely a function of D, points x for which f(x) = λ2/2 + M have no influence on the estimation of D̂n, and in that sense are “outliers”.\nThe previous example is merely used to build some intuition behind the popularity of fidelity term (9) in the presence of “outliers”. As “outliers” are considered points, the distance of which from their approximation Da∗D(x) is larger than a predefined threshold, say γ > 0. Note that any function h with proximal map satisfying Ph(t) = t when |t| > γ behaves like the l0-norm in Example 1.\nRemark 1. The simple example described above may serve to anchor intuition, but it should be kept in mind that although we use the term “outlier”, this is rather a study that focuses on the generalization error of dictionary learning. We do not provide robustness analysis of dictionary learning, since this would require a detailed mathematical definition of the notion “outlier”. Robustness analysis results for Moreau envelope losses using notions from robust statistics, as the breakdown value, are provided in (Georgogiannis, 2016) for the generalized k-means problem; k-means is an unstructured dictionary learning problem−as Dm×d does not have unit-norm columns−with m d, h(t) = 0, and g(·) the indicator of the basis vectors in Rd.\nA robustness analysis different from the previous one has already been developed in (Gribonval et al., 2015a); the authors show that under coherence-based assumptions on D,\nit is highly probable that the empirical risk 1n ∑n i=1 fD(Xi), when fD(x) = infa∈Rd 12 ||x − Da|| 2 2 + g(a), has a guaranteed empirical local minimum around the neighborhood of a population global minimum dictionary. A study motivated by the above references is of great interest and would fill the gap between theoretical and actual performance of dictionary learning algorithms using Moreau envelopes.\nNext is introduced the statistical learning framework. Denote as X , X1, X2, . . . , independent and identically distributed random vectors with values in a closed ball in Rm, say BRm(T ) with radius T centered at the origin, and denote as P̄ the set of all probability distributions µ on the Borel σ-algebra B(BRm(T )) generated by this ball.1 The aim is to show that the family of functions\nFD = { fD(x) : Rd → R; D ∈ D } (16)\nhas the uniform convergence of empirical means property on the measure space (BRm(T ),B(BRm(T )), µ), µ ∈ P̄ . Here D is the set of all m× d real matrices with unit Euclideannorm columns and fD is of the form (4). The collection of functions FD has the uniform convergence of empirical means (UCEM) property if the following convergence\nP  supfD∈FD︸ ︷︷ ︸ supD∈D ∣∣∣∣∣∣∣∣∣∣ 1 n n∑ i=1 fD(Xi)︸ ︷︷ ︸ Rn(D) − ∫ fDdµ︸ ︷︷ ︸ R(D) ∣∣∣∣∣∣∣∣∣∣ > ε  n→∞−→ 0\n(17) is valid for every positive number ε and probability measure µ ∈ P̄ on BRm(T ) (Vidyasagar, 2002).2 This asymptotic result immediately answers the question raised in the introduction: if (17) holds true, then an application of inequality\nR(D̂n)− inf D∈D R(D) . sup D∈D |Rn(D)−R(D)|\nassures that R(D̂n) tends to the optimal value infD∈DR(D) as the number of samples increases.\nIn most of our proofs, standard arguments from empirical processes theory are followed. In Sections 3 and 4 an appropriate form for h and g is chosen and then are used techniques based on either deterministic (Kolmogorov & Širjaev, 1993) or random ε-covers of the function class FD (Györfi et al., 2006); let us recall their definitions.\nDefinition 1 (ε-cover). Let ε > 0 and let F be a class of functions from A ⊆ Rm to R. Every finite collection\n1 The Borel σ-algebra B(Y ) of a subset Y of a metric space S is the one generated by B(Y ) = {Y ∩ E : E ∈ B(S)}. Thus the Borel σ-algebra B(BRm(T )) is precisely the class of all subsets of BRm(T ) which are Borel sets in Rm (Folland, 2013).\n2Symbol P in (17) denotes the product measure µ×∞1 on the product σ-algebra ⊗∞ 1 B(BRm(T )) (Folland, 2013).\nof functions f̃1, . . . , f̃N : Rm → R , for which for each f ∈ F there is a j(f) ∈ {1, . . . , N} such that\n||f − f̃j ||∞ := sup x∈A |f(x)− f̃j(x)| < ε, (18)\nis called ε-cover of F under the supremum norm.\nLet FD,ε = {f1, . . . , fN} be a ε-cover of FD with respect to || · ||∞. As intuitively expected, the fewer the balls needed to cover FD, the smaller the FD. Definition 2 (ε-covering number). Let ε > 0 and let F be a class of functions from a set A ⊆ Rm to R. Let N (ε,F , || · ||∞) be the size of the smallest ε-cover of F under the supremum norm in (18). If no finite ε-cover exists, takeN (ε,F , ||·||∞) =∞. ThenN (ε,F , ||·||∞) is named the ε-covering number of F , abbreviated to N∞(ε,F ).\nThe method of proof used in Sections 3 and 4 to establish the UCEM property for FD when g is continuous is based on deterministic ε-covers, basic exponential inequalities and the Borel-Cantelli lemma. Unfortunately, this approach does not work when g is the indicator function of all k-sparse vectors; see Section 5. To overcome this difficulty, we rely on tools from VC theory, such as the shatter coefficient of the family of subgraphs of a function class. Definition 3 (subgraphs of a function class). Consider a function class F with functions f : Rm → R+. The set\nF+ := { {(x, t) ∈ Rm+1 : f(x) ≥ t}; f ∈ F } (19)\nis the collection of all subgraphs of functions f in F .\nA family of subgraphs is a family of sets for which the shatter coefficient and VC dimension are defined as follows. Definition 4 (shatter coefficient). Let A be a family of sets. For {x1, . . . , xn} ⊂ Rm, let NA(x1, . . . , xn) be the number of different sets in {{x1, . . . , xn} ∩ A; A ∈ A} . The n-th shatter coefficient s(A, n) of A is\ns(A, n) := max x1,...,xn NA(x1, . . . , xn).\nThe shatter coefficient is the maximal number of different subsets of n points that can be picked out by sets of A. Definition 5 (VC dimension). Let A be a collection of sets with |A| ≥ 2. The largest integer k ≥ 1 for which s(A, k) = 2k is denoted by VA and is called the VC dimension of the class A.\nIf for some hypothetical function class F the corresponding shatter coefficient s(F+, n) is a polynomial of degree b with respect to n, i.e., s(F+, n) = O(nb), then the popular Vapnik-Chervonenkis’s inequality (Theorem 12.5 in (Devroye et al., 1997)) implies UCEM for F . Later on, in Section 5, we show that this is the case for s(F+D , n) as well, where F+D denotes the collection of all subgraphs of functions in FD with g the indicator of k-sparse vectors in Rd−recall the definitions of fD and FD in (4) and (16)."
  }, {
    "heading": "3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",
    "text": "In this section we prove the UCEM property for the function class FD in (16) when fD is defined as\nfD(x) := inf a∈Rd\n{eh(||x−Da||2) + g(a)} (20)\nand g has the following form:\ng(a) = d∑ i=1 ĝ(ai). (21)\nHere is assumed that ĝ : R→ [0,+∞) is a univariate, continuous, even, and strictly increasing function on [0,+∞) with minimum value ĝ(0) = 0. The aforementioned assumptions on g are valid for many coordinate-separable regularizers, e.g., the lp norms on Rd, g(a) = λ||a||p, 0 < p < +∞ for some λ > 0, and the log penalty function g(a) = ∑d i=1 λ log(γ+1) log(γ|ai| + 1), γ > 0. From now on, a separable function of the previous form is called (strictly) increasing if for all i, ĝ(ai) is (strictly) increasing as |ai| → +∞. The main result is the following theorem. Theorem 1. Let ε > 0 and consider the function class FD in (16) with fD : BRm(T ) → [0, eh(T )] and g : Rd → [0,+∞) defined as in (20) and (21) respectively. Then\nP { sup\nfD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ > ε }\n≤ 2 ( 9dĝ−1(eh(T ))\n2ε\n)md e − 2nε2 9eh(T ) 2 .\n(22) Furthermore,\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣→ 0 (n→∞) (23) almost surely, for any µ ∈ P̄ . Hence, the function class FD has the UCEM property with respect to P̄ .\nAn outline of Theorem’s 1 proof is the following:\n1. We define map F that maps any m× p matrix to some function of the form (20). Using appropriate metrics, F is shown to be globally Lipschitz.\n2. The Lipschitz continuity of F and the covering number of D generate an upper bound for N∞(ε,FD).\n3. Standard theorems from the empirical process theory imply the concentration result in (22) and finally prove the UCEM property for the function class FD.\nThe above outline makes clear that the main difficulty in proving Theorem 1 is the verification of the Lipschitz continuity of map F . Let us mention that factor dĝ\n−1(eh(T )) 2\nappearing on the right hand side (rhs) of (22) is an upper bound for the Lipschitz constant of the aforementioned map.\nThere exist other approaches that do not require any form of continuity on F to prove the UCEM property for FD. However, theoretical questions regarding the existence of the optimal dictionary are answered quite easily if we manage to construct such a map. For example, as well known, a continuous map maps compact sets to compact sets. If F is continuous, the compactness of D implies the compactness of FD. This in turn implies the existence of the optimal solution f∗D of minimization problem inffD∈FD ∫ fDdµ; indeed, the integral is a linear operator and FD is compact. Remark 2. Another theoretical question, of great importance for the measure theory enthusiasts, concerns the measurability of the supremum appearing on the left hand side (lhs) of (22). This is a random variable of which the measurability stems from total boundedness of FD with respect to the supremum norm ||f ||∞ := sup{x:||x||2≤T} |f(x)|. Proposition 1. Assume a set up as the one in Theorem 1. Then for any δ > 0,\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ ≤ O (√ log(nd) n ) (24)\nwith probability at least 1− δ.\nThe term log(d) in (24), responsible for the sub-optimality of the bound in case of convex Moreau envelopes, results from our proof method; similar bounds in the literature are of order O( √ log n/n) (Gribonval et al., 2015b; Vainsencher et al., 2011). This term is eliminated in Lemma 2 below to end up with a same order upper bound. The latter is in alignment with the sample complexity results presented in (Gribonval et al., 2015b) and (Liu & Tao, 2016) for the cases where fD(x) equals infa∈Rd 12 ||x −Da|| 2 2 + g(a) and infa∈Rd 12 ||x−Da||1 + g(a) respectively. Lemma 2. Let L > dĝ −1(eh(T ))\n2 and define β > 0 as β := mdmax{log(6L √ 8), 1}. Assume that n satisfies condition\nn\nlog(n) ≥ max\n{ 8, ( 1\n2 √ 8L\n)2 β } (25)\nand consider the same set up as in Theorem 1. Then,\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ ≤ 2√8 √ β log n n\n+ 1√ 8\n√ β + t\nn ,\n(26) with probability at least 1− 2e−t.\nThe rationale behind this lemma is to find conditions, under which for large values of the sample size n, an exponential tail for the error kicks in but without the term log(d) of inequality (24). Although the analysis seems finer, the result is valid only if the sample size satisfies the quite strict and complex inequality (25)."
  }, {
    "heading": "4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",
    "text": "Analysis of Section 3 covers a broad range of regularizers g but it does not cover popular penalty functions from robust statistics, like SCAD, gscad(a) = ∑d i=1 ĝscad(ai) or MCP,\ngmcp(a) = ∑d i=1 ĝmcp(ai) (Mazumder et al., 2012):\nĝscad(t;λ, γ) =  λt, t ≤ λ λγt− 12 (t 2+λ2)\nγ−1 , λ < t ≤ γλ λ2(γ2−1) 2(γ−1) , t > λγ, (27)\nand\nĝmcp(t;λ, γ) =\n{ λt− t 2\n2γ , t ≤ λ 1 2γλ 2, t > γλ. (28)\nAlthough the previous univariate functions are continuous, even, and satisfy the assumptions of Lemma 1, they fail to satisfy the assumptions of Theorem 1 because they are bounded above and thus not strictly increasing.\nThis section is an attempt to extend the results of Section 3 and handle a very special case of coordinate-separable regularizers: those g(a) = ∑d i=1 ĝ(ai), where ĝ : R → [0,+∞) is not only continuous and symmetric around zero, but also strictly increasing up to some point in [0,+∞) and then constant. For this purpose, we require that ĝ satisfies the additional (strict) inequality\neh(T ) < sup t∈R ĝ(t). (29)\nUnder assumption (29), all results presented in Section 3 remain valid; see the relevant discussion in Appendix A.5. Example 2 describes the impact of this assumption on penalty function ĝmcp while the same applies to ĝscad.\nExample 2. Let h be the l0-norm on the real line and ĝ(a) = ĝmcp(a; γ, λ2), λ2 > 0; recall the definition of the l0-norm on the real line: l0(t;λ1) = λ12 1{t 6=0}, λ1 > 0 . In this case, the Moreau envelope is\nel0(t;λ1) = 1\n2 min{t2, λ21}\nand gmcp(a) = ∑d i=1 ĝmcp(ai; γ, λ2). Now assumption (29) reads as\nsup t∈R\nĝmcp(t;λ2, γ) > 1\n2 min{T 2, λ21} (30)\nor after some simple algebraic calculations,\n1 2 λ22γ > 1 2 min{T 2, λ21} ⇔ λ2 >\n√ 1\nγ min{T 2, λ21}.\n(31) Thus, function class FD in (16) with fD(x) defined as\nfD(x) = inf a∈Rd\n{ el0(||x−Da||2) + d∑ i=1 ĝmcp(ai; γ, λ2) }\nhas the UCEM property only for pairs of values (λ1, λ2) with λ2 > √ 1 γ min{T 2, λ 2 1}.\nExample 2 reveals that the ease with which we extend the results of Section 3 has great impact on the diversity of functions ĝ that we could handle. In order to use the upper bounds in Proposition 1 or Lemma 2, our focus needs to be restricted on families FD where the rightmost inequality in (31) holds. This artificial restriction on the available pair of values (λ1, λ2) makes this extension quite useless; in many applications, when setting up λ1 and λ2, we search on a wider grid of values.\nIn the next section, we remove the continuity assumption from g and derive generalization bounds valid for any bounded lsc function, such as SCAD, MCP or the indicator function of all k-sparse vectors in Rd."
  }, {
    "heading": "5. The case of the indicator function of all",
    "text": "k-sparse vectors in Rd and its extension\nDenote as Σk = {a ∈ Rd : |{i : ai 6= 0}| = k} the set of all k-sparse vectors in Rd. The approach followed in Sections 3 and 4 to prove the UCEM property for FD heavily relies on the assumption that g is continuous. Consequently, it does not work for the function\ng(a) = { 0, if a ∈ Σk +∞, otherwise,\n(32)\nthe non-separable and lsc indicator function of all k-sparse vectors in Rd. Using combinatorial tools from VC theory, we remove the spurious condition on the coherence of D ∈ D appearing in previous works (Gribonval et al., 2015b; Vainsencher et al., 2011) and prove the UCEM property when g is bounded and lsc. Starting the analysis with function (32), the results are then extended to cover any bounded lsc function on Rd with range in [0,+∞).\nNext is presented Proposition 2, a modification of Theorem 20 in (Vainsencher et al., 2011): it states that map F from metric space (D, || · ||1,2) to metric space (FD, || · ||∞),\nFD := { min a∈Σk eh(||x−Da||2); D ∈ D } , (33)\nis not uniformly Lipschitz for any Lipschitz constant.3 This is the main reason we resign (ourselves) from previous proof techniques. Without an explicit upper bound for the Lipschitz constant of map F , we cannot infer a bound for the covering number of FD in terms of the one of D.\nProposition 2. Consider the family of functions FD in (33). Then, there exist γ > 0 and q ∈ BRm(T ) such that for every ε > 0, there exist D,D′ ∈ D such that\nmax 1≤j≤d\n||D·,j −D′·,j ||2 ≤ ε but |fD(q)− fD′(q)| > γ.\nIn other words, map F from D to FD with D ∈ D 7→ F (D) ∈ FD is not globally Lipschitz.\nProposition 2 suggests that there are two ways to overcome the limitations when dealing with k-sparse vectors: either more restrictions shall be imposed on the class of dictionaries D or a different proof method has to be followed. The former approach was adopted by (Vainsencher et al., 2011) and (Gribonval et al., 2015b), who both use deterministic ε-net arguments under an incoherence assumption on D and a lower RIP-property, respectively. In such a way, the authors restrict their analysis on a subspace of original space of all unit-norm column dictionaries.\nHere the latter approach is adopted: without additional assumptions on the dictionaries, standard tools from VC theory verify the UCEM property of FD. The main result is Proposition 3 which delivers an upper bound for s(F+D , n), the shatter coefficient of\nF+D := { {(x, t) ∈ Rm+1 : fD(x) ≥ t}; fD ∈ FD } ;\n(34) the previous set collection is the family of all subgraphs of functions fD which belong to FD (as defined in (33)).\nProposition 3. The shatter coefficient s(F+D , n) of the collection of sets F+D , as defined in (34), is bounded above as\ns(F+D , n) ≤ ( en α(m,d) )α(m,d) with α(m, d) independent of n and α(m, d) = ((m+ d)2 + 3(m+ d))/2 + 1.\nA direct use of Proposition’s 3 bound in the popular VapnikChervonenkis’s theorem (Theorem 12.5, (Devroye et al., 1997)) generates Theorem 2 and its byproduct Proposition 4. The latter characterizes the rate of convergence to zero of the difference of the sample average from the true mean of fD(X). All random variables appearing in Theorems 2, 3 and Proposition 4 below are assumed measurable.\nTheorem 2. Let fD : BRm(T ) → [0, eh(T )] for each fD 3 Although Proposition 2 has the same formulation as Theorem 20 of (Vainsencher et al., 2011), the latter cannot apply directly in our case except for k = 2. Proposition 2 clarifies through minor modifications what happens when k > 2.\nin the function class FD in (33) and let ε > 0. Then\nP { sup\nfD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ > }\n≤ 8s(F+D , n)e − nε2 32eh(T ) 2 .\n(35) Furthermore,\n∞∑ n=1 ( en α(m, d) )α(m,d) e − 2nε2 32eh(T ) 2 <∞ (36)\nfor all ε > 0, and by the Borel-Cantelli lemma,\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣→ 0 (almost surely). (37)\nHence, function class FD has the UCEM property. Proposition 4. Assume the same setup as in Theorem 2 and let δ > 0. With probability at least 1− δ, holds true that\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ ≤ O (√ log n n ) .\n(38)\nWhen fD(x) = eh(||x||2) and eh(t) = t2, the bounds for the absolute difference in the rhs of (38) in (Gribonval et al., 2015b) and (Vainsencher et al., 2011) are of order\nO (√\nlogn n\n) and O (√ log( √ n)\nn\n) respectively.\nAlthough Proposition 4 is suboptimal compared to the latter, let us recall that Proposition 4 is valid for all dictionaries with unit-norm columns, in contrast to the last referenced bounds that do not cover the whole of space D. With slight modifications, Theorem 2 extends to Theorem 3 which covers any bounded lsc function g, including MCP or SCAD. Theorem 3. Let g : Rd → [0,+∞) bounded and lsc, and FD the function class with functions\nfD(x) := inf a∈Rd\neh(||x−Da||2) + g(a). (39)\nLet ε > 0. Then\nP { sup\nfD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ > }\n≤ 8s(F+D , n)e − nε2 32eh(T ) 2 ,\n(40) where s(F+D , n) ≤ ( en α(m,d) )α(m,d) and α(m, d) := ((m+ d)2 + 3(m+ d))/2 + 1. Furthermore, with probability at least 1− δ, holds true that\nsup fD∈FD ∣∣∣∣∣ 1n n∑ i=1 fD(Xi)− ∫ fDdµ ∣∣∣∣∣ ≤ O (√ log n n ) .\n(41)"
  }, {
    "heading": "6. On the approximation error when m d",
    "text": "As already mentioned, our aim is to analyze the expected reconstruction error of the learned bases D̂n, R(D̂n) :=∫ fD̂ndµ, when D̂n is the (ERM)-estimator D̂n := argminD∈DRn(D). This reconstruction error decomposes into the estimation error est and the approximation error app as follows:\nR(D̂n) = R(D̂n)−R(D∗)︸ ︷︷ ︸ := est +R(D∗)︸ ︷︷ ︸ := app , (42)\nwhere D∗ := argminD∈DR(D) is the optimal dictionary, the global minimizer of the population risk. The estimation error exists because D̂n is just an estimate for D∗. The approximation error measures the risk of restricting ourselves to D rather than to a larger family of matrices. The optimal choice for D̂n guarantees that both est and app are the smallest possible. The estimation error is bounded as\nest := R(D̂n)−R(D∗) ≤ 2 sup D∈D |Rn(D)−R(D)|.\n(43) In previous sections was proven that the rhs of (43) approaches zero as n → +∞ and that, in view of (42), the reconstruction errorR(D̂n) is asymptotically equal to app. The approximation error does not depend on the sample size n; it is determined by the family of losses under study and the probability distribution of the data. In the k-sparse case, app is rarely zero, even for well behaved probability measures µ. The authors in (Vovk, 2016), Section 24, show that the two objectives, of good data approximation and of sparsity of the combination vector a, are incompatible if the data distribution puts its mass far from any low dimensional subspace and in such cases app 6= 0.\nIn this section, assuming m d, app is considered a function of d. An upper bound for app as d→ m, valid for any probability measure µ ∈ P̄ , gives insights to the problem of approximating points in Rm with combinations of points lying on subspaces of dimension d. Following the approach in (Liu & Tao, 2016), we relate the optimal population risk R(D∗) to the quantization error of probability measure µ.\nNext proposition is meaningful only in the case where g is the indicator function of special compact subsets of Rd, i.e., g(a) = χK(a) withK ⊂ Rd; χK(a) alternates between zero and infinity according to whether its actual argument belongs in K or not. Specifically, K is assumed to contain the basis vectors of the positive orthant. Assumptions (H1) to (H5) in Lemma 1 regarding h and its proximal map Ph remain valid, but is also required that\n(H6) Ph(t) = 0, when t ∈ [−τ, τ ], (44)\nfor some predefined value τ > 0. These assumptions simplify the proof of Proposition 5: if they are true, then the\nMoreau envelope behaves like the quadratic function t2 in a neighborhood around zero. Although assumptions (H1) through (H6) may seem strict, they are valid for many univariate penalty functions and compact sets, such as the closed unit-norm balls in Rd. Proposition 5. Assume m d. Let the family of losses FD be defined as FD := {fD(x); D ∈ D} with\nfD(x) := inf a∈Rd\neh(||x−Da||2) + χK(a), (45)\nwhere χK is the indicator function of some compact setK ⊂ Rd that contains all basis vectors of the positive orthant, i.e., {ej}d1 ∈ K and ej is the j-th column of the identity matrix.\nIf h satisfies the assumptions of Lemma 1 while its proximal map Ph satisfies assumptions (H1)-(H6), then for the approximation error it holds true that\nR(D∗) := inf D∈D\n∫ fD(x)dµ ≤ O(d−2/m). (46)\nThe bound in (46) depends on m and d. Despite being “weak”, as m−2/m → 1, this upper bound provides an insight to the problem: when m is fixed, but sufficiently large, and d → m, the approximation error decreases as d increases, at rate O(d−2/m). Let us note here that the UCEM property for the family of risk functions FD as defined in Proposition 5 can be proved using elements from the proof of Theorem 1 in Section 3."
  }, {
    "heading": "7. Conclusions",
    "text": "This article is a theoretical analysis on the sample complexity of dictionary learning when the loss function to be minimized is the sum of the Moreau envelope of some univariate lsc function h on the real line and a regularization function g. We derive generalization bounds for a wide range of g, including the case of the indicator function of all k-sparse vectors. As a byproduct of this analysis is provided some intuition behind the popularity of loss functions under study in the context of “gross outliers”, that is, samples with arbitrary “large” values. Finally, we comment on the approximation error of an ideal family of losses when the dimension m d, where d is the size of the dictionary. In the future, it would be interesting to characterize the differentiability properties of the losses under study. Such an analysis would have direct practical applications on the design of numerical optimization algorithms."
  }, {
    "heading": "Acknowledgements",
    "text": "The author thanks Athanasios P. Liavas and the anonymous reviewers for helpful comments and suggestions that improved the quality of the article."
  }],
  "year": 2018,
  "references": [{
    "title": "Sparse coding with anomaly detection",
    "authors": ["A. Adler", "M. Elad", "Y. Hel-Or", "E. Rivlin"],
    "venue": "Journal of Signal Processing Systems,",
    "year": 2015
  }, {
    "title": "VC dimension of ellipsoids",
    "authors": ["Y. Akama", "K. Irie"],
    "venue": "arXiv preprint arXiv:1109.4347,",
    "year": 2011
  }, {
    "title": "Outlier-aware dictionary learning for sparse representation",
    "authors": ["S. Amini", "M. Sadeghi", "M. Joneidi", "M. Babaie-Zadeh", "C. Jutten"],
    "venue": "In Machine Learning for Signal Processing (MLSP),",
    "year": 2014
  }, {
    "title": "Wavelet methods in statistics: Some recent developments and their applications",
    "authors": ["A. Antoniadis"],
    "venue": "Statistics Surveys,",
    "year": 2007
  }, {
    "title": "Quantization of probability distributions under norm-based distortion measures",
    "authors": ["S. Delattre", "S. Graf", "H. Luschgy", "G. Pages"],
    "venue": "Statistics & Decisions,",
    "year": 2004
  }, {
    "title": "A Probabilistic Theory of Pattern Recognition. Stochastic Modelling and Applied Probability",
    "authors": ["L. Devroye", "L. Györfi", "G. Lugosi"],
    "venue": "URL https://books.google. gr/books?id=uDgXoRkyWqQC",
    "year": 1997
  }, {
    "title": "Real analysis: modern techniques and their applications",
    "authors": ["G.B. Folland"],
    "year": 2013
  }, {
    "title": "Structured outlier models for robust dictionary learning",
    "authors": ["P.A. Forero", "S. Shafer", "J. Harguess"],
    "venue": "In Information Sciences and Systems (CISS),",
    "year": 2015
  }, {
    "title": "Sparsity-driven laplacian-regularized outlier identification for dictionary learning",
    "authors": ["P.A. Forero", "S. Shafer", "J.D. Harguess"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }, {
    "title": "Robust k-means: a theoretical revisit",
    "authors": ["A. Georgogiannis"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Sparse and spurious: dictionary learning with noise and outliers",
    "authors": ["R. Gribonval", "R. Jenatton", "F. Bach"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Sample complexity of dictionary learning and other matrix factorizations",
    "authors": ["R. Gribonval", "R. Jenatton", "F. Bach", "M. Kleinsteuber", "M. Seibert"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "A distribution-free theory of nonparametric regression",
    "authors": ["L. Györfi", "M. Kohler", "A. Krzyzak", "H. Walk"],
    "venue": "Springer Science & Business Media,",
    "year": 2006
  }, {
    "title": "Robust dictionary learning with capped l1-norm",
    "authors": ["W. Jiang", "F. Nie", "H. Huang"],
    "venue": "In IJCAI, pp",
    "year": 2015
  }, {
    "title": "Robust kernel dictionary learning using a whole sequence convergent algorithm",
    "authors": ["H. Liu", "J. Qin", "H. Cheng", "F. Sun"],
    "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "On the performance of manhattan nonnegative matrix factorization",
    "authors": ["T. Liu", "D. Tao"],
    "venue": "IEEE transactions on neural networks and learning systems,",
    "year": 2016
  }, {
    "title": "k-dimensional coding schemes in hilbert spaces",
    "authors": ["A. Maurer", "M. Pontil"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2010
  }, {
    "title": "Sparsenet: Coordinate descent with nonconvex penalties",
    "authors": ["R. Mazumder", "J.H. Friedman", "T. Hastie"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2012
  }, {
    "title": "Sparse coding with an overcomplete basis set: A strategy employed by V1",
    "authors": ["B.A. Olshausen", "D.J. Field"],
    "venue": "Vision research,",
    "year": 1997
  }, {
    "title": "Convergence of stochastic processes",
    "authors": ["D. Pollard"],
    "venue": "Springer Science & Business Media,",
    "year": 1984
  }, {
    "title": "Variational analysis, volume 317",
    "authors": ["R.T. Rockafellar", "Wets", "R.J.-B"],
    "venue": "Springer Science & Business Media,",
    "year": 2009
  }, {
    "title": "The sample complexity of dictionary learning",
    "authors": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "A theory of learning and generalization",
    "authors": ["M. Vidyasagar"],
    "year": 2002
  }, {
    "title": "Measures of Complexity: Festschrift for Alexey Chervonenkis",
    "authors": ["V. Vovk"],
    "venue": "ISBN 3319357786,",
    "year": 2016
  }, {
    "title": "Minimizing nonconvex non-separable functions",
    "authors": ["Y. Yu", "X. Zheng", "M. Marchetti-Bowick", "E.P. Xing"],
    "venue": "In AISTATS,",
    "year": 2015
  }, {
    "title": "A survey of sparse representation: algorithms and applications",
    "authors": ["Z. Zhang", "Y. Xu", "J. Yang", "X. Li", "D. Zhang"],
    "venue": "IEEE access,",
    "year": 2015
  }, {
    "title": "Online nonnegative matrix factorization with outliers",
    "authors": ["R. Zhao", "V.Y.F. Tan"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }],
  "id": "SP:b8354caba43c0567e687d2e2a89a58a4d4a90e21",
  "authors": [{
    "name": "Alexandros Georgogiannis",
    "affiliations": []
  }],
  "abstractText": "This is a theoretical study on the sample complexity of dictionary learning with general type of reconstruction losses. The goal is to estimate a m × d matrix D of unit-norm columns when the only available information is a set of training samples. Points x in R are subsequently approximated by the linear combination Da after solving the problem mina∈Rd Φ(x−Da) + g(a) with function g being either an indicator function or a sparsity promoting regularizer. Here is considered the case where Φ(x) = inf z∈Rm ||x− z||2 + h(||z||2) and h is an even and univariate function on the real line. Connections are drawn between Φ and the Moreau envelope of h. A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works. Finally comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O( √ log n/n).",
  "title": "The Generalization Error of Dictionary Learning with Moreau Envelopes"
}