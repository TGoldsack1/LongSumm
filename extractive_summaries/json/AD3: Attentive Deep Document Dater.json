{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Many natural language processing tasks require document creation time (DCT) information as a useful additional metadata. Tasks such as information retrieval (Li and Croft, 2003; Dakka et al., 2008), temporal scoping of events and facts (Allan et al., 1998; Talukdar et al., 2012b), document summarization (Wan, 2007) and analysis (de Jong et al., 2005a) require precise and validated creation time of the documents. Most of the documents obtained from the Web either contain DCT that cannot be trusted or contain no DCT information at all (Kanhabua and Nørvåg, 2008). Thus, predicting the time of these documents based on their content is an important task, often referred to as Document Dating.\nA few generative approaches (de Jong et al., 2005b; Kanhabua and Nørvåg, 2008) as well as a discriminative model (Chambers, 2012) have been previously proposed for this task. Kotsakos et al. (2014) employs term-burstiness resulting in improved precision on this task.\nRecently proposed NeuralDater (Vashishth et al., 2018) uses a graph convolution network (GCN) based approach for document dating, outperforming all previous models by a significant margin. NeuralDater extensively uses the syntactic and temporal graph structure present within the document itself. Motivated by NeuralDater, we explicitly develop two different methods: a) Attentive Context Model, and b) Ordered Event Model. The first component tries to accumulate knowledge across documents, whereas the latter uses the temporal structure of the document for predicting its DCT.\nMotivated by the effectiveness of attention based models in different NLP tasks (Yang et al., 2016a; Bahdanau et al., 2014), we incorporate attention in our method in a principled fashion. We use attention not only to capture context but also for feature aggregation in the graph convolution network (Hamilton et al., 2017). Our contributions are as follows.\n• We propose Attentive Deep Document Dater (AD3), the first attention-based neural model for time-stamping documents.\n• We devise a novel method for label based attentive graph convolution over directed graphs and use it for the document dating task.\n• Through extensive experiments on multiple real-world datasets, we demonstrate AD3’s effectiveness over previously proposed methods.\nAD3 source code and datasets used in the paper are available at https://github.com/malllabiisc/AD3"
  }, {
    "heading": "2 Related Work",
    "text": "Document Time-Stamping: Initial attempts\nar X\niv :1\n90 2.\n02 16\n1v 1\n[ cs\n.C L\n] 2\n1 Ja\nn 20\n19\nmade for document time-stamping task include statistical language models proposed by de Jong et al. (2005b) and Kanhabua and Nørvåg (2008). (Chambers, 2012) use temporal and hand-crafted features extracted from documents to predict DCT. They propose two models, one of which learns the probabilistic constraints between year mentions and the actual creation time, whereas the other one is a discriminative model trained on hand-crafted features. Kotsakos et al. (2014) propose a termburstiness (Lappas et al., 2009) based statistical method for the task. Vashishth et al. (2018) propose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN).\nEvent Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task use sieve-based archi-\ntectures, where multiple classifiers are ranked according to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a).\nGraph Convolutional Network (GCN): GCN (Kipf and Welling, 2016) is the extension of convolutional networks over graphs. In different NLP tasks such as semantic-role labeling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017), and event detection (Nguyen and Grishman, 2018), GCNs have proved to be effective. We extensively use GCN for capturing both syntactic and temporal aspect of the document.\nAttention Network: Attention networks have been well exploited for various tasks such as document classification (Yang et al., 2016b), question answering (Yang et al., 2016a), machine translation (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, attention over graph structure has been\nshown to work well by Veličković et al. (2018). Taking motivation from them, we deploy an attentive convolutional network on temporal graph for the document dating problem."
  }, {
    "heading": "3 Background: GCN & NeuralDater",
    "text": "The task of document dating can be modeled as a multi-class classification problem. Following prior work, we shall focus on DCT prediction at the year-granularity in this paper. In this section, we summarize the previous state-of-the-art model NeuralDater (Vashishth et al., 2018), before moving onto our method. An overview of graph convolutional network (GCN) (Kipf and Welling, 2016) is also necessary as it is used in NeuralDater as well as in our model."
  }, {
    "heading": "3.1 Graph Convolutional Network",
    "text": "GCN for Undirected Graph: Consider an undirected graph, G = (V, E), where V and E are the set of n vertices and set of edges respectively. Matrix X ∈ Rn×m, whose rows are input representation of node u, where xu ∈ Rm, ∀ u ∈ V , is the input feature matrix. The output hidden representation hv ∈ Rd of a node v after a single layer of graph convolution operation can be obtained by considering only the immediate neighbours of v, as formulated in (Kipf and Welling, 2016). In order to capture information at multi-hop distance, one can stack layers of GCN, one over another. GCN for Directed Graph: Consider a labelled edge from node u to v with label l(u, v), denoted collectively as (u, v, l(u, v)). Based on the assumption that information in a directed edge need not only propagate along its direction, Marcheggiani and Titov (2017) added opposite edges viz., for each (u, v, l(u, v)), (v, u, l(u, v)−1) is added to the edge list. Self loops are also added for passing the current embedding information. When GCN is applied over this modified directed graph, the embedding of the node v after kth layer will be,\nhk+1v = f  ∑ u∈N (v) ( W kl(u,v)h k u + b k l(u,v) ) . We note that the parameters W kl(u,v) and b k l(u,v) in this case are edge label specific. hku is the input to the kth layer. Here, N (v) refers to the set of neighbours of v, according to the updated edge list and f is any non-linear activation function (e.g., ReLU: f(x) = max(0, x))."
  }, {
    "heading": "3.2 NeuralDater",
    "text": "In this sub-section, we provide a brief overview of the components of the NeuralDater (Vashishth et al., 2018). Given a document D with n tokens w1, w2, · · ·wn, NeuralDater extracts a temporally rich embedding of the document in a principled way as explained below:"
  }, {
    "heading": "3.2.1 Context Embedding",
    "text": "Bi-directional LSTM is employed for embedding each word with its context. The GloVe representation of the words X ∈ Rn×k is transformed to a context aware representationHcntx ∈ Rn×k to get the context embedding. This is essentially shown as the Bi-LSTM in Figure 1."
  }, {
    "heading": "3.2.2 Syntactic Embedding",
    "text": "In this step, the context embeddings are further processed using GCN over the dependency parse tree of the sentences in the document, in order to capture long range connection among words. The syntactic dependency structure is extracted by Stanford CoreNLP’s dependency parser (Manning et al., 2014). NeuralDater follows the same formulation of GCN for directed graph as described in Section 3.1, where additional edges are added to the graph to model the information flow. Again following (Marcheggiani and Titov, 2017), NeuralDater does not allocate separate weight matrices for different types of dependency edge labels, rather it considers only three type of edges: a) edges that exist originally, b) the reverse edges that are added explicitly, and c) self loops. The S-GCN portion of Figure 1 represents this component.\nMore formally, Hcntx ∈ Rn×k is transformed to Hsyn ∈ Rn×ksyn by applying S-GCN."
  }, {
    "heading": "3.2.3 Temporal Embedding",
    "text": "In this layer, NeuralDater exploits the Event-Time graph structure present in the document. CATENA (Mirza and Tonelli, 2016), current state-of-the-art temporal and causal relation extraction algorithm, produces the temporal graph from the event time annotation of the document. GCN applied over this Event-Time graph, namely T-GCN, chooses nT number of tokens out of total n tokens from the document for further revision in their embeddings. Note that T is the total number of events and time mentions present in the document. A special node DCT is added to the graph and its embedding is jointly learned. Note that this layer learns both label and direction specific parameters."
  }, {
    "heading": "3.2.4 Classifier",
    "text": "Finally, the DCT embedding concatenated with the average pooled syntactic embedding is fed to a softmax layer for classification. This whole procedure is trained jointly."
  }, {
    "heading": "4 Attentive Deep Document Dater (AD3): Proposed Method",
    "text": "In this section, we describe Attentive Deep Document Dater (AD3), our proposed method. AD3 is inspired by NeuralDater, and shares many of its components. Just like in NeuralDater, AD3 also leverages two main types of signals from the document – syntactic and event-time – to predict the document’s timestamp. However, there are crucial differences between the two systems. Firstly, instead of concatenating embeddings learned from these two sources as in NeuralDater, AD3 treats these two models completely separate and combines them at a later stage. Secondly, unlike NeuralDater, AD3 employs attention mechanisms in each of these two models. We call the resulting models Attentive Context Model (AC-GCN) and Ordered Event Model (OE-GCN). These two models are described in Section 4.1 and Section 4.2, respectively."
  }, {
    "heading": "4.1 Attentive Context Model (AC-GCN)",
    "text": "Recent success of attention-based deep learning models for classification (Yang et al., 2016b), question answering (Yang et al., 2016a), and machine translation (Bahdanau et al., 2014) have motivated us to use attention during document dating. We extend the syntactic embedding model of NeuralDater (Section 3.2.2) by incorporating an attentive pooling layer. We call the resulting model ACGCN. This model (right side in Figure 1) has two major components.\n• Context Embedding and Syntactic Embedding: Following NeuralDater, we used Bi-LSTM and S-GCN to capture context and long-range syntactic dependencies in the document (Please refer to Section 3.2.1, Section 3.2.2 for brief description). The syntactic embedding, Hsyn ∈ Rn×ksyn is then fed to an Attention Network for further processing. Note that, ksyn is the dimension of the output of Syntactic-GCN and n is the number of tokens in the document.\n• Attentive Embedding: In this layer, we\nlearn the representation for the whole document through word level attention network. We learn a context vector, us ∈ Rs with respect to which we calculate attention for each token. Finally, we aggregate the token features with respect to their attention weights in order to represent the document. More formally, let hsynt ∈ Rksyn be the syntactic representation of the tth token in the document. We take non-linear projection of it in Rs with Ws ∈ Rs×ksyn . Attention weight αt for tth token is calculated with respect to the context vector uTt as follows.\nut = tanh(Wsh syn t ),\nαt = exp(uTt us)∑ t exp(u T t us) .\nFinally, the document representation for the AC-GCN is computed as shown below.\ndAC−GCN = ∑ t αth syn t\nThis representation is fed to a softmax layer for the final classification.\nThe final probability distribution over years predicted by the AC-GCN is given below.\nPAC−GCN(y|D) = Softmax(W · dAC−GCN + b)."
  }, {
    "heading": "4.2 Ordered Event Model (OE-GCN)",
    "text": "The OE-GCN model is shown on the left side of Figure 1. Just like in AC-GCN, context and syntactic embedding is also part of OE-GCN. The syntactic embedding is fed to the Attentive Graph Convolution Network (AT-GCN) where the graph is obtained from the time-event ordering algorithm CATENA (Mirza and Tonelli, 2016). We describe these components in detail below."
  }, {
    "heading": "4.2.1 Temporal Graph",
    "text": "We use the same process used in NeuralDater (Vashishth et al., 2018) for procuring the Temporal Graph from the document. CATENA (Mirza and Tonelli, 2016) generates 9 different temporal links between events and time expressions present in the document. Following Vashishth et al. (2018), we choose 5 most frequent ones - AFTER, BEFORE, SIMULTANEOUS, INCLUDES, and IS INCLUDED – as labels. The temporal graph\nis constructed from the partial ordering between event verbs and time expressions.\nLet ET be the edge list of the Temporal Graph. Similar to (Marcheggiani and Titov, 2017; Vashishth et al., 2018), we also add reverse edges for each of the existing edge and self loops for passing current node information as explained in Section 3.1. The new edge list E ′T is shown below.\nE ′T = ET ∪ {(j, i, l(i, j)−1) | (i, j, l(i, j)) ∈ ET} ∪ {(i, i, self) | i ∈ V)}.\nThe reverse edges are added with reverse labels like AFTER−1, BEFORE−1 etc . Finally, we get 10 labels for our temporal graph and we denote the set of edge labels by L."
  }, {
    "heading": "4.2.2 Attentive Graph Convolution (AT-GCN)",
    "text": "Since the temporal graph is automatically generated, it is likely to have incorrect edges. Ideally, we would like to minimize the influence of such noisy edges while computing temporal embedding. In order to suppress the noisy edges in the Temporal Graph and detect important edges for reasoning, we use attentive graph convolution (Hamilton et al., 2017) over the Event-Time graph. The attention mechanism learns the aggregation function jointly during training. Here, the main objective is to calculate the attention over the neighbouring nodes with respect to the current node for a given label. Then the embedding of the current node is updated by mixing neighbouring node embedding according to their attention scores. In this respect, we propose a label-specific attentive graph convolution over directed graphs.\nLet us consider an edge in the temporal graph from node i to node j with type l, where l ∈ L and L is the label set. The label set L can be divided broadly into two coarse labels as done in Section 3.2.2. The attention weights are specific to only these two type of edges to reduce parameter and prevent overfitting. For illustration, if there exists an edge from node i to j then the edge types will be,\n• L(i, j) =→ if (i, j, l(i, j)) ∈ E ′T , i.e., if the edge is an original event-time edge.\n• L(i, j) =← if (i, j, l(i, j)−1) ∈ E ′T , i.e., if the edge is added later.\nFirst, we take a linear projection (W attenL(i,j) ∈ RF×ksyn) of both the nodes in RF in order to map\nboth of them in the same direction-specific space. The concatenated vector [W attenL(i,j) × hi;W atten L(i,j) × hj ], signifies the importance of the node j w.r.t. node i. A non linear transformation of this concatenation can be treated as the importance feature vector between i and j.\neij = tanh[W atten L(i,j) × hi;W atten L(i,j) × hj ].\nNow, we compute the attention weight of node j for node i with respect to a direction-specific context vector aL(i,j) ∈ R2F , as follows.\nα l(i,j) ij =\nexp ( aTL(i,j)eij ) ∑\nk∈N l(i,·)i\nexp ( aTL(i,j)eik ) ,\nwhere, αl(i,j)ij = 0 if node i and j is not connected through label l. N l(i,·) denotes the subset of the neighbourhood of node i with label l only. Please note that, although the linear transform weight (W attenL(i,j) ∈ R\nF×ksyn) is specific to the coarse labels L, but for each finer label l ∈ L we get these convex weights of attentions. Figure 2 illustrates the above description w.r.t. edge type BEFORE.\nFinally, the feature aggregation is done according to the attention weights. Prior to that, another label specific linear transformation is taken to perform the convolution operation. Then, the updated feature for node i is calculated as follows. hk+1i = f (∑ l∈L ∑\nj∈N l(i,·)i α l(i,j) ij\n( Wl(i,j)hj + bl(i,j) )) .\nwhere, αii = 1, N l(i,·) denotes the subset of the neighbourhood of node i with label l only. Note that, αl(i,j)ij = 0 when j /∈ N l(i,·). To illustrate formally, from Figure 2, we see that weight α1 and α2 is calculated specific to label type BEFORE and the neighbours which are connected through BEFORE is being multiplied with Wbefore prior to aggregation in the ReLU block.\nNow, after applying attentive graph convolution network, we only consider the representation of Document Creation Time (DCT), hDCT , as the document representation itself. hDCT is now passed through a fully connected layer prior to softmax. Prediction of the OE-GCN for the document D will be given as\nPOE−GCN(y|D) = Softmax(W · dDCT + b)."
  }, {
    "heading": "4.3 AD3: Attentive Deep Document Dater",
    "text": "In this section, we propose an unified model by mixing both AC-GCN and OE-GCN. Even on validation data, we see that performance of both the models differ to a large extent. This significant difference (McNemar test p < 0.000001) motivated the unification. We take convex combination of the output probabilities of the two models\nas shown below.\nPjoint(y|D) = λPAC−GCN(y|D) + (1− λ)POE−GCN(y|D).\nThe combination hyper-parameter λ is tuned on the validation data. We obtain the value of λ to be 0.52 (Figure 3) and 0.54 for APW and NYT datasets, respectively. This depicts that the two models are capturing significantly different aspects of documents, resulting in a substantial improvement in performance when combined."
  }, {
    "heading": "5 Experimental Setup",
    "text": "Dataset: Experiments are carried out on the Associated Press Worldstream (APW) and New York Times (NYT) sections of the Gigaword corpus (Parker et al., 2011). We have used the same 8:1:1 split as Vashishth et al. (2018) for all the models. For quantitative details please refer to Table 1.\nEvaluation Criteria: In accordance with prior work (Chambers, 2012; Kotsakos et al., 2014; Vashishth et al., 2018) the final task is to predict the publication year of the document. We give a brief description of the baselines below.\nBaseline Methods:\n• MaxEnt-Joint (Chambers, 2012): This method engineers several hand-crafted temporally influenced features to classify the document using MaxEnt Classifier.\n• BurstySimDater (Kotsakos et al., 2014): This is a purely statistical method which uses lexical similarity and term burstiness (Lappas et al., 2009) for dating documents in arbitrary length time frame. For our experiments, we used a time frame length of 1 year.\n• NeuralDater (Vashishth et al., 2018): This is the first deep neural network based approach for the document dating task. Details are provided in Section 3.2.\nIsrael's consumer price index increased by 1.2 percent  in December, bringing the overall inflation rate for 1995 to 8.1 percent, well within the government's target rate for the year, officials said Friday. Israel radio said that it was the lowest annual inflation rate in twenty years.\nFigure 5: Visualization of the attention of AC-GCN. ACGCN captures the intuitive tokens as seen in the figure. Darker shade implies higher attention. The correct DCT is 1996.\nHyperparameters: We use 300-dimensional GloVe embeddings and 128-dimensional hidden state for both GCNs and BiLSTM with 0.8 dropout. We use Adam (Kingma and Ba, 2014) with 0.001 learning rate for training. For OE-GCN we use 2-layers of AT-GCN. 1-layer of S-GCN is used for both the models."
  }, {
    "heading": "6 Results",
    "text": ""
  }, {
    "heading": "6.1 Performance Analysis",
    "text": "In this section, we compare the effectiveness of our method with that of prior work. The deep network based NeuralDater model in (Vashishth et al., 2018) outperforms previous feature engi-\nneered (Chambers, 2012) and statistical methods (Kotsakos et al., 2014) by a large margin. We observe a similar trend in our case. Compared to the state-of-the-art model NeuralDater, we gain, on an average, a 3.7% boost in accuracy on both the datasets (Table 2).\nAmong individual models, OE-GCN performs at par with NeuralDater, while AC-GCN outperforms it. The empirical results imply that ACGCN by itself is effective for this task. The relatively worse performance of OE-GCN can be attributed to the fact that it only focuses on the Event-Time information and leaves out most of the contextual information. However, it captures various different (p < 0.000001, McNemar’s test, 2-tailed) aspects of the document for classification, which motivated us to propose an ensemble of the two models. This explains the significant boost in performance of AD3 over NeuralDater as well as the individual models. It is worth mentioning that although AC-GCN and OE-GCN do not provide significant boosts in accuracy, their predictions have considerably lower mean-absolute-\ndeviation as shown in Figure 4. We concatenated the DCT embedding provided by OE-GCN with the document embedding provided by AC-GCN and trained in an end to end joint fashion like NeuralDater. We see that even with a similar training method, the Attentive NeuralDater model on an average, performs 1.6% better in terms of accuracy, once again proving the efficacy of attention based models over normal models."
  }, {
    "heading": "6.2 Effectiveness of Attention",
    "text": "Attentive Graph Convolution (Section 4.2.2) proves to be effective for OE-GCN, giving a 2% accuracy improvement over non-attentive T-GCN of NeuralDater (Table 3). Similarly the efficacy of word level attention is also prominent from Table 3. We have also analyzed our models by visualizing attentions over words and attention over graph nodes. Figure 5 shows that AC-GCN focuses on temporally informative words such as ”said” (for tense) or time mentions like “1995”, alongside important contextual words like “inflation”, “Israel” etc. For OE-GCN, from Figure 6 we observe that “DCT” and time-mention ‘1995’ grabs the highest attention. Attention between “DCT” and other event verbs indicating past tense are quite prominent, which helps the model to infer 1996 (which is correct) as the most likely time-stamp of the document. These analyses provide us with a good justification for the performance of our attentive models."
  }, {
    "heading": "7 Discussion",
    "text": "Apart from empirical improvements over previous models, we also perform a qualitative analysis of the individual models. Figure 7 shows that the performance of AC-GCN improves with the length of documents, thus indicating that richer context leads to better model prediction. Figure 8 shows how the performance of OE-GCN improves with the number of event-time mentions in the document, thus further reinforcing our claim that more temporal information improves model performance.\nVashishth et al. (2018) reported that their model got confused by the presence of multiple misleading time mentions. AD3 overcomes this limitation using attentive graph convolution, which successfully filters out noisy time mentions as is evident\nfrom Figure 8."
  }, {
    "heading": "8 Conclusion",
    "text": "We propose AD3, an ensemble model which exploits both syntactic and temporal information in a document explicitly to predict its creation time (DCT). To the best of our knowledge, this is the first application of attention based deep models for dating documents. Our experimental results demonstrate the effectiveness of our model over all previous models. We also visualize the attention weights to show that the model is able to choose what is important for the task and filter out noise inherent in language. As part of future work, we would like to incorporate external knowledge as a side information for improved time-stamping of documents."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported by the Ministry of Human Resource Development (MHRD), Government of India."
  }],
  "year": 2019,
  "references": [{
    "title": "On-line new event detection and tracking",
    "authors": ["James Allan", "Ron Papka", "Victor Lavrenko."],
    "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98, pages 37–45,",
    "year": 1998
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "CoRR, abs/1409.0473.",
    "year": 2014
  }, {
    "title": "Graph convolutional encoders for syntax-aware neural machine translation. CoRR, abs/1704.04675",
    "authors": ["Joost Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Sima’an"],
    "year": 2017
  }, {
    "title": "Labeling documents with timestamps: Learning from their time expressions",
    "authors": ["Nathanael Chambers."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 98–106, Strouds-",
    "year": 2012
  }, {
    "title": "Answering general time sensitive queries",
    "authors": ["Wisam Dakka", "Luis Gravano", "Panagiotis G. Ipeirotis."],
    "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 1437–1438, New York, NY, USA. ACM.",
    "year": 2008
  }, {
    "title": "Temporal Language Models for the Disclosure of Historical Text",
    "authors": ["Franciska M.G. de Jong", "H. Rode", "Djoerd Hiemstra."],
    "venue": "KNAW. Imported from EWI/DB PMS [db-utwente:inpr:0000003683].",
    "year": 2005
  }, {
    "title": "Temporal Language Models for the Disclosure of Historical Text",
    "authors": ["Franciska M.G. de Jong", "H. Rode", "Djoerd Hiemstra."],
    "venue": "KNAW. Imported from EWI/DB PMS [db-utwente:inpr:0000003683].",
    "year": 2005
  }, {
    "title": "Inductive representation learning on large graphs",
    "authors": ["William L. Hamilton", "Rex Ying", "Jure Leskovec."],
    "venue": "CoRR, abs/1706.02216.",
    "year": 2017
  }, {
    "title": "Improving temporal language models for determining time of non-timestamped documents",
    "authors": ["Nattiya Kanhabua", "Kjetil Nørvåg."],
    "venue": "Proceedings of the 12th European Conference on Research and Advanced Technology for Digital Libraries, ECDL",
    "year": 2008
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Semisupervised classification with graph convolutional networks",
    "authors": ["Thomas N Kipf", "Max Welling."],
    "venue": "arXiv preprint arXiv:1609.02907.",
    "year": 2016
  }, {
    "title": "A burstiness-aware approach for document dating",
    "authors": ["Dimitrios Kotsakos", "Theodoros Lappas", "Dimitrios Kotzias", "Dimitrios Gunopulos", "Nattiya Kanhabua", "Kjetil Nørvåg."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "On burstiness-aware search for document sequences",
    "authors": ["Theodoros Lappas", "Benjamin Arai", "Manolis Platakis", "Dimitrios Kotsakos", "Dimitrios Gunopulos."],
    "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery",
    "year": 2009
  }, {
    "title": "Time-based language models",
    "authors": ["Xiaoyan Li", "W. Bruce Croft."],
    "venue": "Proceedings of the Twelfth International Conference on Information and Knowledge Management, CIKM ’03, pages 469–475, New York, NY, USA. ACM.",
    "year": 2003
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Association for Computational Linguistics (ACL) System Demonstrations,",
    "year": 2014
  }, {
    "title": "Encoding sentences with graph convolutional networks for semantic role labeling",
    "authors": ["Diego Marcheggiani", "Ivan Titov."],
    "venue": "CoRR, abs/1703.04826.",
    "year": 2017
  }, {
    "title": "Event ordering with a generalized model for sieve prediction ranking",
    "authors": ["Bill McDowell", "Nathanael Chambers", "Alexander Ororbia II", "David Reitter."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1:",
    "year": 2017
  }, {
    "title": "Catena: Causal and temporal relation extraction from natural language texts",
    "authors": ["Paramita Mirza", "Sara Tonelli."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 64–75. The",
    "year": 2016
  }, {
    "title": "Graph convolutional networks with argument-aware pooling for event detection",
    "authors": ["Thien Huu Nguyen", "Ralph Grishman"],
    "year": 2018
  }, {
    "title": "English gigaword fifth edition ldc2011t07",
    "authors": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."],
    "venue": "dvd. Philadelphia: Linguistic Data Consortium.",
    "year": 2011
  }, {
    "title": "Acquiring temporal constraints between relations",
    "authors": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell."],
    "venue": "Proceedings of the 21st ACM international conference on Information and knowledge management, pages 992–1001. ACM.",
    "year": 2012
  }, {
    "title": "Coupled temporal scoping of relational facts",
    "authors": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell."],
    "venue": "Proceedings of the fifth ACM international conference on Web search and data mining, pages 73–82. ACM.",
    "year": 2012
  }, {
    "title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations",
    "authors": ["Naushad UzZaman", "Hector Llorens", "Leon Derczynski", "James Allen", "Marc Verhagen", "James Pustejovsky."],
    "venue": "Second Joint Conference on Lexical",
    "year": 2013
  }, {
    "title": "Dating documents using graph convolution networks",
    "authors": ["Shikhar Vashishth", "Shib Sankar Dasgupta", "Swayambhu Nath Ray", "Partha Talukdar."],
    "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational",
    "year": 2018
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin."],
    "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
    "year": 2017
  }, {
    "title": "Graph attention networks",
    "authors": ["Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Semeval-2010 task 13: Tempeval-2",
    "authors": ["Marc Verhagen", "Roser Sauri", "Tommaso Caselli", "James Pustejovsky."],
    "venue": "Proceedings of the 5th international workshop on semantic evaluation, pages 57–62. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Timedtextrank: Adding the temporal dimension to multi-document summarization",
    "authors": ["Xiaojun Wan."],
    "venue": "Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages",
    "year": 2007
  }, {
    "title": "Stacked attention networks for image question answering",
    "authors": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June",
    "year": 2016
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alexander J. Smola", "Eduard H. Hovy."],
    "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association",
    "year": 2016
  }],
  "id": "SP:899b63f2893015c57cbe749a8b693697bfbf6548",
  "authors": [{
    "name": "Swayambhu Nath Ray",
    "affiliations": []
  }, {
    "name": "Shib Sankar Dasgupta",
    "affiliations": []
  }, {
    "name": "Partha Talukdar",
    "affiliations": []
  }],
  "abstractText": "Knowledge of the creation date of documents facilitates several tasks such as summarization, event extraction, temporally focused information extraction etc. Unfortunately, for most of the documents on the Web, the time-stamp metadata is either missing or can’t be trusted. Thus, predicting creation time from document content itself is an important task. In this paper, we propose Attentive Deep Document Dater (AD3), an attention-based neural document dating system which utilizes both context and temporal information in documents in a flexible and principled manner. We perform extensive experimentation on multiple real-world datasets to demonstrate the effectiveness of AD3 over neural and non-neural baselines.",
  "title": "AD3: Attentive Deep Document Dater"
}