{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931–2937 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics\nWe present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text."
  }, {
    "heading": "1 Introduction",
    "text": "Words in news media and political discourse have a considerable power in shaping people’s beliefs and opinions. As a result, their truthfulness is often compromised to maximize impact. Recently, fake news has captured worldwide interest, and the number of organized efforts dedicated solely to fact-checking has almost tripled since 2014.1 Organizations, such as PolitiFact.com, actively investigate and rate the veracity of comments made by public figures, journalists, and organizations.\nFigure 1 shows example quotes rated for truthfulness by PolitiFact. Per their analysis, one component of the two statements’ ratings is the misleading phrasing (bolded in green in the figure). For instance, in the first example, the statement is true as stated, though only because the speaker hedged their meaning with the quantifier just. In the second example, two correlated events – Brexit\n1https://www.poynter.org/2017/there-are-now-114-factchecking-initiatives-in-47-countries/450477/\n“By declaring that Pluto was no longer a planet, the (International Astronomical Union) put into place a planetary definition that would have even declassified Earth as a planet if it existed as far from the sun as Pluto does.”\nHalf TrueTrue False\n-Rated Half True by PunditFact, (July 2015)\nand Google search trends – are presented ambiguously as if they were directly linked.\nImportantly, like above examples, most factchecked statements on PolitiFact are rated as neither entirely true nor entirely false. Analysis indicates that falsehoods often arise from subtle differences in phrasing rather than outright fabrication (Rubin et al., 2015). Compared to most prior work on deception literature that focused on binary categorization of truth and deception, political fact-checking poses a new challenge as it involves a graded notion of truthfulness.\nWhile political fact-checking generally focuses on examining the accuracy of a single quoted statement by a public figure, the reliability of general news stories is also a concern (Connolly et al., 2016; Perrott, 2016). Figure 2 illustrates news types categorized along two dimensions: the intent of the authors (desire to deceive) and the content of the articles (true, mixed, false).\n2931\nIn this paper, we present an analytic study characterizing the language of political quotes and news media written with varying intents and degrees of truth. We also investigate graded deception detection, determining the truthfulness on a 6-point scale using the political fact-checking database available at PolitiFact.2"
  }, {
    "heading": "2 Fake News Analysis",
    "text": "News Corpus with Varying Reliability To analyze linguistic patterns across different types of articles, we sampled standard trusted news articles from the English Gigaword corpus and crawled articles from seven different unreliable news sites of differing types. Table 1 displays sources identified under each type according to US News & World Report.3 These news types include: • Satire: mimics real news but still cues the reader\nthat it is not meant to be taken seriously • Hoax: convinces readers of the validity of a\nparanoia-fueled story • Propaganda: misleads readers so that they be-\nlieve a particular political/social agenda Unlike hoaxes and propaganda, satire is intended to be notably different from real news so that audiences will recognize the humorous intent. Hoaxes and satire are more likely to invent stories, while propaganda frequently combines truths, falsehoods, and ambiguities to confound readers.\nTo characterize differences between news types, we applied various lexical resources to trusted and fake news articles. We draw lexical resources from prior works in communication theory and stylistic analysis in computational linguistics. We tokenize\n2All resources created for this paper including corpus of news articles from unreliable sources, collection of Politifact ratings, and compiled Wiktionary lexicons have been made publicly available at homes.cs.washington. edu/˜hrashkin/factcheck.html\n3www.usnews.com/news/national-news/articles/2016-1114/avoid-these-fake-news-sites-at-all-costs\nthe text with NLTK (Bird et al., 2009) and compute per-document count for each lexicon, and report averages per article of each type.\nFirst among these lexicons is the Linguistic Inquiry and Word Count (LIWC), a lexicon widely used in social science studies (Pennebaker et al., 2015). In addition, we estimate the use of strongly and weakly subjective words with a sentiment lexicon (Wilson et al., 2005). Subjective words can be used to dramatize or sensationalize a news story. We also use lexicons for hedging from (Hyland, 2015) because hedging can indicate vague, obscuring language. Lastly, we introduce intensifying lexicons that we crawled from Wiktionary based on a hypothesis that fake news articles try to enliven stories to attract readers. We compiled five lists from Wiktionary of words that imply a degree a dramatization (comparatives, superlatives, action adverbs, manner adverbs, and modal adverbs) and measured their presence.\nDiscussion Table 2 summarizes the ratio of averages between unreliable news and truthful news for a handful of the measured features. Ratios greater than one denote features more prominent in fake news, and ratios less than one denote features more prominent in truthful news. The ratios between unreliable/reliable news reported are statistically significant (p < 0.01) with Welsch t-test after Bonferroni correction.\nOur results show that first-person and secondperson pronouns are used more in less reliable or deceptive news types. This contrasts studies in other domains (Newman et al., 2003), which found fewer self-references in people telling lies about their personal opinions. Unlike that domain, news writers are trying to appear indifferent. Editors at trustworthy sources are possibly more\nrigorous about removing language that seems too personal, which is one reason why this result differs from other lie detection domains. This finding instead corroborates previous work in written domains found by Ott et al. (2011) and Rayson et al. (2001), who found that such pronouns were indicative of imaginative writing. Perhaps imaginative storytelling domains is a closer match to detecting unreliable news than lie detection on opinions.\nOur results also show that words that can be used to exaggerate – subjectives, superlatives, and modal adverbs – are all used more by fake news. Words used to offer concrete figures – comparatives, money, and numbers – appear more in truthful news. This also builds on previous findings by Ott et al. (2011) on the difference between superlative/comparative usage.\nTrusted sources are more likely to use assertive words and less likely to use hedging words, indicating that they are less vague about describing events, as well. This relates to psychology theories (Buller and Burgoon, 1996) that deceivers show more “uncertainty and vagueness” and “indirect forms of expression”. Similarly, the trusted sources use the hear category words more often, possibly indicating that they are citing primary sources more often.\nThe last column in Table 2 shows the fake news type that uses the corresponding lexicon most\nprominently. We found that one distinctive feature of satire compared to other types of untrusted news is its prominent use of adverbs. Hoax stories tend to use fewer superlatives and comparatives. In contrast, compared to other types of fake news, propaganda uses relatively more assertive verbs and superlatives.\nNews Reliability Prediction We study the feasibility of predicting the reliability of the news article into four categories: trusted, satire, hoax, or propaganda. We split our collected articles into balanced training (20k total articles from the Onion, American News, The Activist, and the Gigaword news excluding ‘APW’, ‘WPB’ sources) and test sets (3k articles from the remaining sources). Because articles in the training and test set come from different sources, the models must classify articles without relying on author-specific cues. We also use 20% of the training articles as an in-domain development set. We trained a Max-Entropy classifier with L2 regularization on n-gram tf-idf feature vectors (up to trigrams).4\nThe model achieves F1 scores of 65% on the out-of-domain test set (Table 3). This is a promising result as it is much higher than random, but still leaves room for improvement compared to the\n4N-gram tfidf vectors have acted as competitive means of cross-domain text-classification. Zhang et al. (2015) found that for data sets smaller than a million examples, this was the best model, outperforming neural models.\nperformance on the development set consisting of articles from in-domain sources.\nWe examined the 50 highest weighted n-gram features in the MaxEnt classifier for each class. The highest weighted n-grams for trusted news were often specific places (e.g., “washington”) or times (“on monday”). Many of the highest weighted from satire were vaguely facetious hearsay (“reportedly”, “confirmed”). For hoax articles, heavily weighted features included divisive topics (“liberals”, “trump”) and dramatic cues (“breaking”). Heavily weighted features for propaganda tend towards abstract generalities (“truth”, “freedom”) as well as specific issues (“vaccines”, “syria”). Interestingly, “youtube” and “video” are highly weighted for the propaganda and hoax classes respectively; indicating that they often rely on video clips as sources."
  }, {
    "heading": "3 Predicting Truthfulness",
    "text": "Politifact Data Related to the issue of identifying the truthfulness of a news article is the factchecking of individual statements made by public figures. Misleading statements can also have a variety of intents and levels of reliability depending on whom is making the statement.\nPolitiFact5 is a site led by Tampa Bay Times journalists who actively fact-check suspicious statements. One unique quality of PolitiFact is that each quote is evaluated on a 6-point scale of truthfulness ranging from “True” (factual) to “Pantson-Fire False” (absurdly false). This scale allows for distinction between categories like mostly true (the facts are correct but presented in an incomplete manner) or mostly false (the facts are not correct but are connected to a small kernel of truth).\nWe collected labelled statements from PolitiFact and its spin-off sites (PunditFact, etc.) (10,483 statements in total). We analyze a subset of 4,366 statements that are direct quotes by the original speaker. The distributions of ratings on the PolitiFact scale for this subset are shown\n5www.politifact.com/\nin Table 4. Most statements are labeled as neither completely true nor false.\nWe formulate a fine-grained truthfulness prediction task with Politifact data. We split quotes into training/development/test set of {2575, 712, 1074} statements, respectively, so that all of each speaker’s quotes are in a single set. Given a statement, the model returns a rating for how reliable the statement is (Politifact ratings are used as gold labels). We ran the experiment in two settings, one considering all 6 classes and the other considering only 2 (treating the top three truthful ratings as true and the lower three as false).\nModel We trained an LSTM model (Hochreiter and Schmidhuber, 1997) that takes the sequence of words as the input and predicts the Politifact rating. We also compared this model with Maximum Entropy (MaxEnt) and Naive Bayes models, frequently used for text categorization.\nFor input to the MaxEnt and Naive Bayes models, we tried two variants: one with the word tfidf vectors as input, and one with the LIWC measurements concatenated to the tf-idf vectors. For the LSTM model, we used word sequences as input and also a version where LSTM output is concatenated with LIWC feature vectors before undergoing the activation layer. The LSTM word embeddings are initialized with 100-dim embeddings from GLOVE (Pennington et al., 2014) and fine-tuned during training. The LSTM was implemented with Theano and Keras with 300-dim hidden state and a batch size of 64. Training was done with ADAM to minimize categorical crossentropy loss over 10 epochs.\nClassifier Results Table 5 summarizes the performance on the development set. We report macro averaged F1 score in all tables. The LSTM outperforms the other models when only using text as input; however the other two models improve substantially with adding LIWC features, particu-\nlarly in the case of the multinomial naive Bayes model. In contrast, the LIWC features do not improve the neural model much, indicating that some of this lexical information is perhaps redundant to what the model was already learning from text.\nWe report results on the test set in Table 6. We again find that LIWC features improves MaxEnt and NB models to perform similarly to the LSTM model. As in the dev. set results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly."
  }, {
    "heading": "4 Related Work",
    "text": "Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth. Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning.\nLinguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004). In these applications, people purposefully tell lies to receive an extrinsic payoff. In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity.\nFact-Checking and Fake News There is research in political science exploring how effective fact-checking is at improving people’s awareness\n(Lord et al., 1979; Thorson, 2016; Nyhan and Reifler, 2015). Prior computational works (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact-checking through entailment from knowledge bases. Our work takes a more linguistic approach, performing lexical analysis over varying types of falsehood.\nBiyani et al. (2016) examine the unique linguistic styles found in clickbait articles, and Kumar et al. (2016) also characterize hoax documents on Wikipedia. The differentiation between these fake news types is also proposed in previous work (Rubin et al., 2015). Our paper extends this work by offering a quantitative study of linguistic differences found in articles of different types of fake news, and build predictive models for graded deception across multiple domains – PolitiFact and news articles. More recent work (Wang, 2017) has also investigated PolitiFact data though they investigated meta-data features for prediction whereas our investigation is focused on linguistic analysis through stylistic lexicons."
  }, {
    "heading": "5 Conclusion",
    "text": "We examine truthfulness and its contributing linguistic attributes across multiple domains e.g., online news sources and public statements. We perform multiple prediction tasks on fact-checked statements of varying levels of truth (graded deception) as well as a deeper linguistic comparison of differing types of fake news e.g., propaganda, satire and hoaxes. We have shown that factchecking is indeed a challenging task but that various lexical features can contribute to our understanding of the differences between more reliable and less reliable digital news sources."
  }, {
    "heading": "6 Acknowledgements",
    "text": "We would like to thank anonymous reviewers for providing insightful feedback. The research described in this paper was conducted under the Laboratory Directed Research and Development Program at Pacific Northwest National Laboratory, a multiprogram national laboratory operated by Battelle for the U.S. Department of Energy, the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1256082, in part by NSF grants IIS-1408287, IIS-1714566, and gifts by Google and Facebook."
  }],
  "year": 2017,
  "references": [{
    "title": "Natural Language Processing with Python",
    "authors": ["Steven Bird", "Ewan Klein", "Edward Loper."],
    "venue": "O’Reilly Media.",
    "year": 2009
  }, {
    "title": "8 amazing secrets for getting more clicks”: Detecting clickbaits in news streams using article informality",
    "authors": ["Prakhar Biyani", "Kostas Tsioutsiouliklis", "John Blackmer."],
    "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "Interpersonal deception theory",
    "authors": ["David B. Buller", "Judee K. Burgoon."],
    "venue": "Communication Theory 6(3):203–242. https://doi.org/10.1111/j.14682885.1996.tb00127.x.",
    "year": 1996
  }, {
    "title": "Hedge detection as a lens on framing in the gmo debates: A position paper",
    "authors": ["Eunsol Choi", "Chenhao Tan", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil", "Jennifer Spindel."],
    "venue": "Proceedings of the Workshop on Extra-Propositional Aspects of",
    "year": 2012
  }, {
    "title": "Computational fact checking from knowledge networks",
    "authors": ["Giovanni Luca Ciampaglia", "Prashant Shiralkar", "Luis M. Rocha", "Johan Bollen", "Filippo Menczer", "Alessandro Flammini."],
    "venue": "PLOS ONE 10(6):e0128193.",
    "year": 2015
  }, {
    "title": "Fake news: An insidious trend that’s fast becoming a global problem",
    "authors": ["Kate Connolly", "Angelique Chrisafis", "Poppy McPherson", "Stephanie Kirchgaessner", "Benjamin Haas", "Dominic Phillips", "Elle Hunt", "Michael Safi"],
    "year": 2016
  }, {
    "title": "Psycholinguistic features for deceptive role detection in werewolf",
    "authors": ["Codruta Girlea", "Roxana Girju", "Eyal Amir."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Metadiscourse",
    "authors": ["Ken Hyland."],
    "venue": "The International Encyclopedia of Language and Social Interaction, John Wiley & Sons, Inc., pages 1–11.",
    "year": 2015
  }, {
    "title": "Opinion spam and analysis",
    "authors": ["Nitin Jindal", "Bing Liu."],
    "venue": "Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, pages 219–230.",
    "year": 2008
  }, {
    "title": "Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes",
    "authors": ["Srijan Kumar", "Robert West", "Jure Leskovec."],
    "venue": "Proceedings",
    "year": 2016
  }, {
    "title": "Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence",
    "authors": ["Charles G Lord", "Lee Ross", "Mark R Lepper."],
    "venue": "Journal of Personality and Social Psychology 37(11):2098–2109.",
    "year": 1979
  }, {
    "title": "The lie detector: Explorations in the automatic recognition of deceptive language",
    "authors": ["Rada Mihalcea", "Carlo Strapparava."],
    "venue": "Proceedings of the ACLIJCNLP 2009 Conference Short Papers. Association for Computational Linguistics, pages 309–312.",
    "year": 2009
  }, {
    "title": "Lying words: Predicting deception from linguistic styles",
    "authors": ["Matthew L Newman", "James W Pennebaker", "Diane S Berry", "Jane M Richards."],
    "venue": "Personality and social psychology bulletin 29(5):665–675.",
    "year": 2003
  }, {
    "title": "The effect of fact-checking on elites: A field experiment on US state legislators",
    "authors": ["Brendan Nyhan", "Jason Reifler."],
    "venue": "American Journal of Political Science 59(3):628–640.",
    "year": 2015
  }, {
    "title": "Finding deceptive opinion spam by any stretch of the imagination",
    "authors": ["Myle Ott", "Yejin Choi", "Claire Cardie", "Jeffrey T. Hancock."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2011
  }, {
    "title": "Linguistic Inquiry and Word Count: LIWC2015",
    "authors": ["James W. Pennebaker", "Roger J. Booth", "Ryan L. Boyd Boyd", "Martha E. Francis."],
    "venue": "PennebakerConglomerates, Austin, TX. www.liwc.net.",
    "year": 2015
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
    "year": 2014
  }, {
    "title": "Fake news’ on social media influenced US election voters, experts say",
    "authors": ["Kathryn Perrott."],
    "venue": "http://www.abc.net.au/news/2016-11-14/fakenews-would-have-influenced-us-election-expertssay/8024660. Accessed: 2017-01-30.",
    "year": 2016
  }, {
    "title": "Grammatical word class variation within the british national corpus sampler",
    "authors": ["Paul Rayson", "Andrew Wilson", "Geoffrey Leech."],
    "venue": "Language and Computers 36(1):295–306.",
    "year": 2001
  }, {
    "title": "Linguistic models for analyzing and detecting biased language",
    "authors": ["Marta Recasens", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
    "year": 2013
  }, {
    "title": "Deception detection for news: Three types of fakes",
    "authors": ["Victoria L. Rubin", "Yimin Chen", "Niall J. Conroy."],
    "venue": "Proceedings of the Association for Information Science and Technology 52(1):1–4.",
    "year": 2015
  }, {
    "title": "Belief echoes: The persistent effects of corrected misinformation",
    "authors": ["Emily Thorson."],
    "venue": "Political Communication 33(3):460–480.",
    "year": 2016
  }, {
    "title": "Fact checking: Task definition and dataset construction",
    "authors": ["Andreas Vlachos", "Sebastian Riedel."],
    "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science. Association for Computational Linguistics,",
    "year": 2014
  }, {
    "title": "Liar, liar pants on fire”: A new benchmark dataset for fake news detection",
    "authors": ["William Yang Wang."],
    "venue": "Proceedings of the Association for Computational Linguistics Short Papers. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Recognizing contextual polarity in phraselevel sentiment analysis",
    "authors": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."],
    "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Pro-",
    "year": 2005
  }, {
    "title": "Character-level convolutional networks for text classification",
    "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."],
    "venue": "Advances in Neural Information Processing Systems. pages 649–657.",
    "year": 2015
  }, {
    "title": "Automating linguisticsbased cues for detecting deception in text-based asynchronous computer-mediated communications",
    "authors": ["Lina Zhou", "Judee K. Burgoon", "Jay F. Nunamaker", "Doug Twitchell."],
    "venue": "Group Decision and Negotiation 13(1):81–106.",
    "year": 2004
  }],
  "id": "SP:7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22",
  "authors": [{
    "name": "Hannah Rashkin",
    "affiliations": []
  }, {
    "name": "Eunsol Choi",
    "affiliations": []
  }, {
    "name": "Jin Yea Jang",
    "affiliations": []
  }, {
    "name": "Svitlana Volkova",
    "affiliations": []
  }, {
    "name": "Yejin Choi",
    "affiliations": []
  }],
  "abstractText": "We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.",
  "title": "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking"
}