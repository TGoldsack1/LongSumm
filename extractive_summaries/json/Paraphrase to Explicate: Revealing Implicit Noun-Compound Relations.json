{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200–1211 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1200\nRevealing the implicit semantic relation between the constituents of a nouncompound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks."
  }, {
    "heading": "1 Introduction",
    "text": "Noun-compounds hold an implicit semantic relation between their constituents. For example, a ‘birthday cake’ is a cake eaten on a birthday, while ‘apple cake’ is a cake made of apples. Interpreting noun-compounds by explicating the relationship is beneficial for many natural language understanding tasks, especially given the prevalence of nouncompounds in English (Nakov, 2013).\nThe interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relationships (e.g. Nastase and Szpakowicz, 2003) or by generating various free text paraphrases that describe the relation in a more expressive manner (e.g. Hendrickx et al., 2013).\nMethods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013). Such methods are unable to generalize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge. For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job.\nWe follow the paraphrasing approach and propose a semi-supervised model for paraphrasing noun-compounds. Differently from previous methods, we train the model to predict either a paraphrase expressing the semantic relation of a noun-compound (predicting ‘[w2] made of [w1]’ given ‘apple cake’), or a missing constituent given a combination of paraphrase and noun-compound (predicting ‘apple’ given ‘cake made of [w1]’). Constituents and paraphrase templates are represented as continuous vectors, and semantically-similar paraphrase templates are embedded in proximity, enabling better generalization. Interpreting ‘parsley cake’ effectively reduces to identifying paraphrase templates whose “selectional preferences” (Pantel et al., 2007) on each constituent fit ‘parsley’ and ‘cake’.\nA qualitative analysis of the model shows that the top ranked paraphrases retrieved for each noun-compound are plausible even when the constituents never co-occur (Section 4). We evaluate our model on both the paraphrasing and the classification tasks (Section 5). On both tasks, the model’s ability to generalize leads to improved performance in challenging evaluation settings.1\n1The code is available at github.com/vered1986/panic"
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 Noun-compound Classification",
    "text": "Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations.\nEarly work on the task leveraged information derived from lexical resources and corpora (e.g. Girju, 2007; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016).\nThe datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone is labeled as CONTAINED (zone contains business), although it could also be labeled as PURPOSE (zone whose purpose is business)."
  }, {
    "heading": "2.2 Noun-compound Paraphrasing",
    "text": "As an alternative to the strict classification to predefined relation classes, Nakov and Hearst (2006) suggested that the semantics of a noun-compound could be expressed with multiple prepositional and verbal paraphrases. For example, apple cake is a cake from, made of, or which contains apples.\nThe suggestion was embraced and resulted in two SemEval tasks. SemEval 2010 task 9 (Butnariu et al., 2009) provided a list of plausible human-written paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text.\nVarious approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents\nfrom a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013).\nOne of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The approach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositional distributional vector (Mitchell and Lapata, 2010) and used it to predict paraphrases from the corpus. Similar noun-compounds are expected to have similar distributional representations and therefore yield the same paraphrases. For example, if the corpus does not contain paraphrases for plastic spoon, the model may predict the paraphrases of a similar compound such as steel knife.\nIn terms of sharing information between semantically-similar paraphrases, Nulty and Costello (2010) and Surtani et al. (2013) learned “is-a” relations between paraphrases from the co-occurrences of various paraphrases with each other. For example, the specific ‘[w2] extracted from [w1]’ template (e.g. in the context of olive oil) generalizes to ‘[w2] made from [w1]’. One of the drawbacks of these systems is that they favor more frequent paraphrases, which may co-occur with a wide variety of more specific paraphrases."
  }, {
    "heading": "2.3 Noun-compounds in other Tasks",
    "text": "Noun-compound paraphrasing may be considered as a subtask of the general paraphrasing task, whose goal is to generate, given a text fragment, additional texts with the same meaning. However, general paraphrasing methods do not guarantee to explicate implicit information conveyed in the original text. Moreover, the most notable source for extracting paraphrases is multiple translations of the same text (Barzilay and McKeown,\n2001; Ganitkevitch et al., 2013; Mallinson et al., 2017). If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead.\nAnother related task is Open Information Extraction (Etzioni et al., 2008), whose goal is to extract relational tuples from text. Most system focus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. Pal and Mausam (2016) focused on segmenting multi-word nouncompounds and assumed an is-a relation between the parts, as extracting (Francis Collins, is, NIH director) from “NIH director Francis Collins”. Xavier and Lima (2014) enriched the corpus with compound definitions from online dictionaries, for example, interpreting oil industry as (industry, produces and delivers, oil) based on the WordNet definition “industry that produces and delivers oil”. This method is very limited as it can only interpret noun-compounds with dictionary entries, while the majority of English noun-compounds don’t have them (Nakov, 2013)."
  }, {
    "heading": "3 Paraphrasing Model",
    "text": "As opposed to previous approaches, that focus on predicting a paraphrase template for a given nouncompound, we reformulate the task as a multitask learning problem (Section 3.1), and train the model to also predict a missing constituent given the paraphrase template and the other constituent. Our model is semi-supervised, and it expects as input a set of noun-compounds and a set of constrained part-of-speech tag-based templates that make valid prepositional and verbal paraphrases.\nSection 3.2 details the creation of training data, and Section 3.3 describes the model."
  }, {
    "heading": "3.1 Multi-task Reformulation",
    "text": "Each training example consists of two constituents and a paraphrase (w2, p, w1), and we train the model on 3 subtasks: (1) predict p given w1 and w2, (2) predict w1 given p and w2, and (3) predict w2 given p and w1. Figure 1 demonstrates the predictions for subtasks (1) (right) and (2) (left) for the training example (cake, made of, apple). Effectively, the model is trained to answer questions such as “what can cake be made of?”, “what can be made of apple?”, and “what are the possible relationships between cake and apple?”.\nThe multi-task reformulation helps learning better representations for paraphrase templates, by embedding semantically-similar paraphrases in proximity. Similarity between paraphrases stems either from lexical similarity and overlap between the paraphrases (e.g. ‘is made of’ and ‘made of’), or from shared constituents, e.g. ‘[w2] involved in [w1]’ and ‘[w2] in [w1] industry’ can share [w1] = insurance and [w2] = company. This allows the model to predict a correct paraphrase for a given noun-compound, even when the constituents do not occur with that paraphrase in the corpus."
  }, {
    "heading": "3.2 Training Data",
    "text": "We collect a training set of (w2, p, w1, s) examples, where w1 and w2 are constituents of a nouncompound w1w2, p is a templated paraphrase, and s is the score assigned to the training instance.2\n2We refer to “paraphrases” and “paraphrase templates” interchangeably. In the extracted templates, [w2] always precedes [w1], probably because w2 is normally the head noun.\nWe use the 19,491 noun-compounds found in the SemEval tasks datasets (Butnariu et al., 2009; Hendrickx et al., 2013) and in Tratz (2011). To extract patterns of part-of-speech tags that can form noun-compound paraphrases, such as ‘[w2] VERB PREP [w1]’, we use the SemEval task training data, but we do not use the lexical information in the gold paraphrases.\nCorpus. Similarly to previous noun-compound paraphrasing approaches, we use the Google Ngram corpus (Brants and Franz, 2006) as a source of paraphrases (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013). The corpus consists of sequences of n terms (for n ∈ {3, 4, 5}) that occur more than 40 times on the web. We search for n-grams following the extracted patterns and containing w1 and w2’s lemmas for some noun-compound in the set. We remove punctuation, adjectives, adverbs and some determiners to unite similar paraphrases. For example, from the 5-gram ‘cake made of sweet apples’ we extract the training example (cake, made of, apple). We keep only paraphrases that occurred at least 5 times, resulting in 136,609 instances.\nWeighting. Each n-gram in the corpus is accompanied with its frequency, which we use to assign scores to the different paraphrases. For instance, ‘cake of apples’ may also appear in the corpus, although with lower frequency than ‘cake from apples’. As also noted by Surtani et al. (2013), the shortcoming of such a weighting mechanism is that it prefers shorter paraphrases, which are much more common in the corpus (e.g. count(‘cake made of apples’) count(‘cake of apples’)). We overcome this by normalizing the frequencies for each paraphrase length, creating a distribution of paraphrases in a given length.\nNegative Samples. We add 1% of negative samples by selecting random corpus words w1 and w2 that do not co-occur, and adding an example (w2, [w2] is unrelated to [w1], w1, sn), for some predefined negative samples score sn. Similarly, for a word wi that did not occur in a paraphrase p we add (wi, p, UNK, sn) or (UNK, p, wi, sn), where UNK is the unknown word. This may help the model deal with non-compositional noun-compounds, where w1 and w2 are unrelated, rather than forcibly predicting some relation between them."
  }, {
    "heading": "3.3 Model",
    "text": "For a training instance (w2, p, w1, s), we predict each item given the encoding of the other two.\nEncoding. We use the 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014), which are fixed during training. In addition, we learn embeddings for the special words [w1], [w2], and [p], which are used to represent a missing component, as in “cake made of [w1]”, “[w2] made of apple”, and “cake [p] apple”.\nFor a missing component x ∈ {[p], [w1], [w2]} surrounded by the sequences of words v1:i−1 and vi+1:n, we encode the sequence using a bidirectional long-short term memory (bi-LSTM) network (Graves and Schmidhuber, 2005), and take the ith output vector as representing the missing component: bLS(v1:i, x, vi+1:n)i.\nIn bi-LSTMs, each output vector is a concatenation of the outputs of the forward and backward LSTMs, so the output vector is expected to contain information on valid substitutions both with respect to the previous words v1:i−1 and the subsequent words vi+1:n.\nPrediction. We predict a distribution of the vocabulary of the missing component, i.e. to predict w1 correctly we need to predict its index in the word vocabulary Vw, while the prediction of p is from the vocabulary of paraphrases in the training set, Vp. We predict the following distributions:\np̂ = softmax(Wp · bLS( ~w2, [p], ~w1)2) ŵ1 = softmax(Ww · bLS( ~w2, ~p1:n, [w1])n+1) ŵ2 = softmax(Ww · bLS([w2], ~p1:n, ~w1)1) (1) where Ww ∈ R|Vw|×2d, Wp ∈ R|Vp|×2d, and d is the embeddings dimension.\nDuring training, we compute cross-entropy loss for each subtask using the gold item and the prediction, sum up the losses, and weight them by the instance score. During inference, we predict the missing components by picking the best scoring index in each distribution:3\np̂i = argmax(p̂)\nŵ1i = argmax(ŵ1)\nŵ2i = argmax(ŵ2)\n(2)\nThe subtasks share the pre-trained word embeddings, the special embeddings, and the biLSTM parameters. Subtasks (2) and (3) also share Ww, the MLP that predicts the index of a word.\n3In practice, we pick the k best scoring indices in each distribution for some predefined k, as we discuss in Section 5.\nImplementation Details. The model is implemented in DyNet (Neubig et al., 2017). We dedicate a small number of noun-compounds from the corpus for validation. We train for up to 10 epochs, stopping early if the validation loss has not improved in 3 epochs. We use Momentum SGD (Nesterov, 1983), and set the batch size to 10 and the other hyper-parameters to their default values."
  }, {
    "heading": "4 Qualitative Analysis",
    "text": "To estimate the quality of the proposed model, we first provide a qualitative analysis of the model outputs. Table 1 displays examples of the model outputs for each possible usage: predicting the paraphrase given the constituent words, and predicting each constituent word given the paraphrase and the other word.\nThe examples in the table are from among the top 10 ranked predictions for each componentpair. We note that most of the (w2, paraphrase, w1) triplets in the table do not occur in the training\ndata, but are rather generalized from similar examples. For example, there is no training instance for “company in the software industry” but there is a “firm in the software industry” and a company in many other industries.\nWhile the frequent prepositional paraphrases are often ranked at the top of the list, the model also retrieves more specified verbal paraphrases. The list often contains multiple semanticallysimilar paraphrases, such as ‘[w2] involved in [w1]’ and ‘[w2] in [w1] industry’. This is a result of the model training objective (Section 3) which positions the vectors of semantically-similar paraphrases close to each other in the embedding space, based on similar constituents.\nTo illustrate paraphrase similarity we compute a t-SNE projection (Van Der Maaten, 2014) of the embeddings of all the paraphrases, and draw a sample of 50 paraphrases in Figure 2. The projection positions semantically-similar but lexicallydivergent paraphrases in proximity, likely due to\nmany shared constituents. For instance, ‘with’, ‘from’, and ‘out of’ can all describe the relation between food words and their ingredients."
  }, {
    "heading": "5 Evaluation: Noun-Compound Interpretation Tasks",
    "text": "For quantitative evaluation we employ our model for two noun-compound interpretation tasks. The main evaluation is on retrieving and ranking paraphrases (§5.1). For the sake of completeness, we also evaluate the model on classification to a fixed inventory of relations (§5.2), although it wasn’t designed for this task."
  }, {
    "heading": "5.1 Paraphrasing",
    "text": "Task Definition. The general goal of this task is to interpret each noun-compound to multiple prepositional and verbal paraphrases. In SemEval 2013 Task 4,4 the participating systems were asked to retrieve a ranked list of paraphrases for each noun-compound, which was automatically evaluated against a similarly ranked list of paraphrases proposed by human annotators.\nModel. For a given noun-compound w1w2, we first predict the k = 250 most likely paraphrases: p̂1, ..., p̂k = argmaxk p̂, where p̂ is the distribution of paraphrases defined in Equation 1.\nWhile the model also provides a score for each paraphrase (Equation 1), the scores have not been optimized to correlate with human judgments. We therefore developed a re-ranking model that receives a list of paraphrases and re-ranks the list to better fit the human judgments.\nWe follow Herbrich (2000) and learn a pairwise ranking model. The model determines which of two paraphrases of the same noun-compound should be ranked higher, and it is implemented as an SVM classifier using scikit-learn (Pedregosa et al., 2011). For training, we use the available training data with gold paraphrases and ranks provided by the SemEval task organizers. We extract the following features for a paraphrase p: 1. The part-of-speech tags contained in p 2. The prepositions contained in p 3. The number of words in p 4. Whether p ends with the special [w1] symbol 5. cosine(bLS([w2], p, [w1])2, ~Vp p̂i ) · p̂p̂i\nwhere ~Vp p̂i is the biLSTM encoding of the predicted paraphrase computed in Equation 1 and p̂p̂i\n4 https://www.cs.york.ac.uk/semeval-2013/task4\nis its confidence score. The last feature incorporates the original model score into the decision, as to not let other considerations such as preposition frequency in the training set take over.\nDuring inference, the model sorts the list of paraphrases retrieved for each noun-compound according to the pairwise ranking. It then scores each paraphrase by multiplying its rank with its original model score, and prunes paraphrases with final score < 0.025. The values for k and the threshold were tuned on the training set.\nEvaluation Settings. The SemEval 2013 task provided a scorer that compares words and ngrams from the gold paraphrases against those in the predicted paraphrases, where agreement on a prefix of a word (e.g. in derivations) yields a partial scoring. The overall score assigned to each system is calculated in two different ways. The ‘isomorphic’ setting rewards both precision and recall, and performing well on it requires accurately reproducing as many of the gold paraphrases as possible, and in much the same order. The ‘non-isomorphic’ setting rewards only precision, and performing well on it requires accurately reproducing the top-ranked gold paraphrases, with no importance to order.\nBaselines. We compare our method with the published results from the SemEval task. The SemEval 2013 baseline generates for each nouncompound a list of prepositional paraphrases in an arbitrary fixed order. It achieves a moderately good score in the non-isomorphic setting by generating a fixed set of paraphrases which are both common and generic. The MELODI system performs similarly: it represents each nouncompound using a compositional distributional vector (Mitchell and Lapata, 2010) which is then used to predict paraphrases from the corpus. The performance of MELODI indicates that the system was rather conservative, yielding a few common paraphrases rather than many specific ones. SFS and IIITH, on the other hand, show a more balanced trade-off between recall and precision.\nAs a sanity check, we also report the results of a baseline that retrieves ranked paraphrases from the training data collected in Section 3.2. This baseline has no generalization abilities, therefore it is expected to score poorly on the recall-aware isomorphic setting.\nResults. Table 2 displays the performance of the proposed method and the baselines in the two evaluation settings. Our method outperforms all the methods in the isomorphic setting. In the nonisomorphic setting, it outperforms the other two systems that score reasonably on the isomorphic setting (SFS and IIITH) but cannot compete with the systems that focus on achieving high precision.\nThe main advantage of our proposed model is in its ability to generalize, and that is also demonstrated in comparison to our baseline performance. The baseline retrieved paraphrases only for a third of the noun-compounds (61/181), expectedly yielding poor performance on the isomorphic setting. Our model, which was trained on the very same data, retrieved paraphrases for all nouncompounds. For example, welfare system was not present in the training data, yet the model predicted the correct paraphrases “system of welfare benefits”, “system to provide welfare” and others.\nError Analysis. We analyze the causes of the false positive and false negative errors made by the model. For each error type we sample 10 nouncompounds. For each noun-compound, false positive errors are the top 10 predicted paraphrases which are not included in the gold paraphrases, while false negative errors are the top 10 gold paraphrases not found in the top k predictions made by the model. Table 3 displays the manu-\nally annotated categories for each error type.\nMany false positive errors are actually valid paraphrases that were not suggested by the human annotators (error 1, “discussion by group”). Some are borderline valid with minor grammatical changes (error 6, “force of coalition forces”) or too specific (error 2, “life of women in community” instead of “life in community”). Common prepositional paraphrases were often retrieved although they are incorrect (error 3). We conjecture that this error often stem from an n-gram that does not respect the syntactic structure of the sentence, e.g. a sentence such as “rinse away the oil from baby ’s head” produces the n-gram “oil from baby”.\nWith respect to false negative examples, they consisted of many long paraphrases, while our model was restricted to 5 words due to the source of the training data (error 1, “holding done in the case of a share”). Many prepositional paraphrases consisted of determiners, which we conflated with the same paraphrases without determiners (error 2, “mutation of a gene”). Finally, in some paraphrases, the constituents in the gold paraphrase appear in inflectional forms (error 3, “holding of shares” instead of “holding of share”)."
  }, {
    "heading": "5.2 Classification",
    "text": "Noun-compound classification is defined as a multiclass classification problem: given a pre-defined set of relations, classify w1w2 to the relation that holds between w1 and w2. Potentially, the corpus co-occurrences of w1 and w2 may contribute to the classification, e.g. ‘[w2] held at [w1]’ indicates a TIME relation. Tratz and Hovy (2010) included such features in their classifier, but ablation tests showed that these features had a relatively small contribution, probably due to the sparseness of the paraphrases. Recently, Shwartz and Waterson (2018) showed that paraphrases may contribute to the classification when represented in a continuous space.\nModel. We generate a paraphrase vector representation ~par(w1w2) for a given noun-compound w1w2 as follows. We predict the indices of the k most likely paraphrases: p̂1, ..., p̂k = argmaxk p̂, where p̂ is the distribution on the paraphrase vocabulary Vp, as defined in Equation 1. We then encode each paraphrase using the biLSTM, and average the paraphrase vectors, weighted by their confidence scores in p̂:\n~par(w1w2) =\n∑k i=1 p̂\np̂i · ~Vp p̂i∑k\ni=1 p̂ p̂i\n(3)\nWe train a linear classifier, and represent w1w2 in a feature vector f(w1w2) in two variants: paraphrase: f(w1w2) = ~par(w1w2), or integrated: concatenated to the constituent word embeddings f(w1w2) = [ ~par(w1w2), ~w1, ~w2]. The classifier type (logistic regression/SVM), k, and the penalty are tuned on the validation set. We also provide a baseline in which we ablate the paraphrase component from our model, representing a nouncompound by the concatenation of its constituent embeddings f(w1w2) = [ ~w1, ~w2] (distributional).\nDatasets. We evaluate on the Tratz (2011) dataset, which consists of 19,158 instances, labeled in 37 fine-grained relations (Tratz-fine) or 12 coarse-grained relations (Tratz-coarse).\nWe report the performance on two different dataset splits to train, test, and validation: a random split in a 75:20:5 ratio, and, following concerns raised by Dima (2016) about lexical memorization (Levy et al., 2015), on a lexical split in which the sets consist of distinct vocabularies. The lexical split better demonstrates the scenario in which a noun-compound whose constituents have not been observed needs to be interpreted based on similar observed noun-compounds, e.g. inferring the relation in pear tart based on apple cake and other similar compounds. We follow the random and full-lexical splits from Shwartz and Waterson (2018).\nBaselines. We report the results of 3 baselines representative of different approaches: 1) Feature-based (Tratz and Hovy, 2010): we reimplement a version of the classifier with features from WordNet and Roget’s Thesaurus. 2) Compositional (Dima, 2016): a neural architecture that operates on the distributional representations of the noun-compound and its constituents. Noun-compound representations are learned with\nthe Full-Additive (Zanzotto et al., 2010) and Matrix (Socher et al., 2012) models. We report the results from Shwartz and Waterson (2018). 3) Paraphrase-based (Shwartz and Waterson, 2018): a neural classification model that learns an LSTM-based representation of the joint occurrences of w1 and w2 in a corpus (i.e. observed paraphrases), and integrates distributional information using the constituent embeddings.\nResults. Table 4 displays the methods’ performance on the two versions of the Tratz (2011) dataset and the two dataset splits. The paraphrase model on its own is inferior to the distributional model, however, the integrated version improves upon the distributional model in 3 out of 4 settings, demonstrating the complementary nature of the distributional and paraphrase-based methods. The contribution of the paraphrase component is especially noticeable in the lexical splits.\nAs expected, the integrated method in Shwartz and Waterson (2018), in which the paraphrase representation was trained with the objective of classification, performs better than our integrated model. The superiority of both integrated models in the lexical splits confirms that paraphrases are beneficial for classification.\nAnalysis. To analyze the contribution of the paraphrase component to the classification, we focused on the differences between the distributional and integrated models on the Tratz-Coarse lexical split. Examination of the per-relation F1 scores revealed that the relations for which performance improved the most in the integrated model were TOPICAL (+11.1 F1 points), OBJECTIVE (+5.5), ATTRIBUTE (+3.8) and LOCATION/PART WHOLE (+3.5).\nTable 5 provides examples of noun-compounds that were correctly classified by the integrated model while being incorrectly classified by the distributional model. For each noun-compound, we provide examples of top ranked paraphrases which are indicative of the gold label relation."
  }, {
    "heading": "6 Compositionality Analysis",
    "text": "Our paraphrasing approach at its core assumes compositionality: only a noun-compound whose meaning is derived from the meanings of its constituent words can be rephrased using them. In §3.2 we added negative samples to the training data to simulate non-compositional nouncompounds, which are included in the classification dataset (§5.2). We assumed that these compounds, more often than compositional ones would consist of unrelated constituents (spelling bee, sacred cow), and added instances of random unrelated nouns with ‘[w2] is unrelated to [w1]’. Here, we assess whether our model succeeds to recognize non-compositional noun-compounds.\nWe used the compositionality dataset of Reddy et al. (2011) which consists of 90 nouncompounds along with human judgments about their compositionality in a scale of 0-5, 0 being non-compositional and 5 being compositional. For each noun-compound in the dataset, we predicted the 15 best paraphrases and analyzed the errors. The most common error was predicting paraphrases for idiomatic compounds which may have\na plausible concrete interpretation or which originated from one. For example, it predicted that silver spoon is simply a spoon made of silver and that monkey business is a business that buys or raises monkeys. In other cases, it seems that the strong prior on one constituent leads to ignoring the other, unrelated constituent, as in predicting “wedding made of diamond”. Finally, the “unrelated” paraphrase was predicted for a few compounds, but those are not necessarily non-compositional (application form, head teacher). We conclude that the model does not address compositionality and suggest to apply it only to compositional compounds, which may be recognized using compositionality prediction methods as in Reddy et al. (2011)."
  }, {
    "heading": "7 Conclusion",
    "text": "We presented a new semi-supervised model for noun-compound paraphrasing. The model differs from previous models by being trained to predict both a paraphrase given a noun-compound, and a missing constituent given the paraphrase and the other constituent. This results in better generalization abilities, leading to improved performance in two noun-compound interpretation tasks. In the future, we plan to take generalization one step further, and explore the possibility to use the biLSTM for generating completely new paraphrase templates unseen during training."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported in part by an Intel ICRI-CI grant, the Israel Science Foundation grant 1951/17, the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), and Theo Hoffenberg. Vered is also supported by the Clore Scholars Programme (2017), and the AI2 Key Scientific Challenges Program (2017)."
  }],
  "year": 2018,
  "references": [{
    "title": "Extracting paraphrases from a parallel corpus",
    "authors": ["Regina Barzilay", "R. Kathleen McKeown."],
    "venue": "Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P01-1008.",
    "year": 2001
  }, {
    "title": "Web 1t 5-gram version 1",
    "authors": ["Thorsten Brants", "Alex Franz"],
    "year": 2006
  }, {
    "title": "Semeval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions",
    "authors": ["Cristina Butnariu", "Su Nam Kim", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Stan Szpakowicz", "Tony Veale."],
    "venue": "Proceed-",
    "year": 2009
  }, {
    "title": "Proceedings of the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds",
    "authors": ["Corina Dima"],
    "year": 2016
  }, {
    "title": "Automatic noun compound interpretation using deep neural networks and word embeddings",
    "authors": ["Corina Dima", "Erhard Hinrichs."],
    "venue": "IWCS 2015 page 173.",
    "year": 2015
  }, {
    "title": "Open information extraction from the web",
    "authors": ["Oren Etzioni", "Michele Banko", "Stephen Soderland", "Daniel S Weld."],
    "venue": "Communications of the ACM 51(12):68–74.",
    "year": 2008
  }, {
    "title": "PPDB: The paraphrase database",
    "authors": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
    "year": 2013
  }, {
    "title": "Improving the interpretation of noun phrases with cross-linguistic information",
    "authors": ["Roxana Girju."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguis-",
    "year": 2007
  }, {
    "title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
    "authors": ["Alex Graves", "Jürgen Schmidhuber."],
    "venue": "Neural Networks 18(5-6):602–610.",
    "year": 2005
  }, {
    "title": "Semeval-2013 task 4: Free paraphrases of noun compounds",
    "authors": ["Iris Hendrickx", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Stan Szpakowicz", "Tony Veale."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM),",
    "year": 2013
  }, {
    "title": "Large margin rank boundaries for ordinal regression",
    "authors": ["Ralf Herbrich."],
    "venue": "Advances in large margin classifiers pages 115–132.",
    "year": 2000
  }, {
    "title": "Largescale noun compound interpretation using bootstrapping and the web as a corpus",
    "authors": ["Nam Su Kim", "Preslav Nakov."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2011
  }, {
    "title": "Interpreting noun compounds using bootstrapping and sense collocation",
    "authors": ["Su Nam Kim", "Timothy Baldwin."],
    "venue": "Proceedings of Conference of the Pacific Association for Computational Linguistics. pages 129–136.",
    "year": 2007
  }, {
    "title": "Do supervised distributional methods really learn lexical inference relations",
    "authors": ["Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"],
    "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the As-",
    "year": 2015
  }, {
    "title": "Ucd-goggle: A hybrid system for noun compound paraphrasing",
    "authors": ["Guofu Li", "Alejandra Lopez-Fernandez", "Tony Veale."],
    "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics,",
    "year": 2010
  }, {
    "title": "Paraphrasing revisited with neural machine translation",
    "authors": ["Jonathan Mallinson", "Rico Sennrich", "Mirella Lapata."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers.",
    "year": 2017
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive science 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "On the interpretation of noun compounds: Syntax, semantics, and entailment",
    "authors": ["Preslav Nakov."],
    "venue": "Natural Language Engineering 19(03):291–330.",
    "year": 2013
  }, {
    "title": "Using verbs to characterize noun-noun relations",
    "authors": ["Preslav Nakov", "Marti Hearst."],
    "venue": "International Conference on Artificial Intelligence: Methodology, Systems, and Applications. Springer, pages 233– 244.",
    "year": 2006
  }, {
    "title": "Exploring noun-modifier semantic relations",
    "authors": ["Vivi Nastase", "Stan Szpakowicz."],
    "venue": "Fifth international workshop on computational semantics (IWCS-5). pages 285–301.",
    "year": 2003
  }, {
    "title": "A method of solving a convex programming problem with convergence rate o (1/k2)",
    "authors": ["Yurii Nesterov."],
    "venue": "Soviet Mathematics Doklady. volume 27, pages 372–376.",
    "year": 1983
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"],
    "year": 2017
  }, {
    "title": "Ucd-pn: Selecting general paraphrases using conditional probability",
    "authors": ["Paul Nulty", "Fintan Costello."],
    "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, pages 234–237.",
    "year": 2010
  }, {
    "title": "Using lexical and relational similarity to classify semantic relations",
    "authors": ["Diarmuid Ó Séaghdha", "Ann Copestake."],
    "venue": "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for Computa-",
    "year": 2009
  }, {
    "title": "Demonyms and compound relational nouns in nominal open ie",
    "authors": ["Harinder Pal", "Mausam."],
    "venue": "Proceedings of the 5th Workshop on Automated Knowledge Base Construction. Association for Computational Linguistics, San Diego, CA, pages 35–39.",
    "year": 2016
  }, {
    "title": "ISP: Learning inferential selectional preferences",
    "authors": ["Patrick Pantel", "Rahul Bhagat", "Bonaventura Coppola", "Timothy Chklovski", "Eduard Hovy."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Asso-",
    "year": 2007
  }, {
    "title": "Interpreting compound noun phrases using web search queries",
    "authors": ["Marius Pasca."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Associa-",
    "year": 2015
  }, {
    "title": "Identifying 1950s american jazz musicians: Fine-grained isa extraction via modifier composition",
    "authors": ["Ellie Pavlick", "Marius Pasca."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
    "year": 2017
  }, {
    "title": "Scikit-learn: Machine learning in Python",
    "authors": ["E. Duchesnay."],
    "venue": "Journal of Machine Learning Research 12:2825–2830.",
    "year": 2011
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
    "year": 2014
  }, {
    "title": "An empirical study on compositionality in compound nouns",
    "authors": ["Siva Reddy", "Diana McCarthy", "Suresh Manandhar."],
    "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language",
    "year": 2011
  }, {
    "title": "Olive oil is made of olives, baby oil is made for babies: Interpreting noun compounds using paraphrases in a neural model",
    "authors": ["Vered Shwartz", "Chris Waterson."],
    "venue": "The 16th Annual Conference of the North American Chapter of the Association for",
    "year": 2018
  }, {
    "title": "Semantic compositionality through recursive matrix-vector spaces",
    "authors": ["Richard Socher", "Brody Huval", "D. Christopher Manning", "Y. Andrew Ng."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
    "year": 2012
  }, {
    "title": "Iiit-h: A corpus-driven co-occurrence based probabilistic model for noun compound paraphrasing",
    "authors": ["Nitesh Surtani", "Arpita Batra", "Urmi Ghosh", "Soma Paul."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2:",
    "year": 2013
  }, {
    "title": "Semantically-enriched parsing for natural language understanding",
    "authors": ["Stephen Tratz."],
    "venue": "University of Southern California.",
    "year": 2011
  }, {
    "title": "A taxonomy, dataset, and classifier for automatic noun compound interpretation",
    "authors": ["Stephen Tratz", "Eduard Hovy."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computa-",
    "year": 2010
  }, {
    "title": "Melodi: A supervised distributional approach for free paraphrasing of noun compounds",
    "authors": ["Tim Van de Cruys", "Stergos Afantenos", "Philippe Muller."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Vol-",
    "year": 2013
  }, {
    "title": "Accelerating t-sne using tree-based algorithms",
    "authors": ["Laurens Van Der Maaten."],
    "venue": "Journal of machine learning research 15(1):3221–3245.",
    "year": 2014
  }, {
    "title": "Sfs-tue: Compound paraphrasing with a language model and discriminative reranking",
    "authors": ["Yannick Versley."],
    "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop",
    "year": 2013
  }, {
    "title": "Uvt: Memory-based pairwise ranking of paraphrasing verbs",
    "authors": ["Sander Wubben."],
    "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, pages 260–263.",
    "year": 2010
  }, {
    "title": "Boosting open information extraction with noun-based relations",
    "authors": ["Clarissa Xavier", "Vera Lima."],
    "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan",
    "year": 2014
  }, {
    "title": "Estimating linear models for compositional distributional semantics",
    "authors": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar."],
    "venue": "Proceedings of the 23rd International Conference on Computational Linguistics.",
    "year": 2010
  }],
  "id": "SP:a1b8da0b0f1d22c99191721661808df0c2d0d92f",
  "authors": [{
    "name": "Vered Shwartz",
    "affiliations": []
  }, {
    "name": "Ido Dagan",
    "affiliations": []
  }],
  "abstractText": "Revealing the implicit semantic relation between the constituents of a nouncompound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.",
  "title": "Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations"
}