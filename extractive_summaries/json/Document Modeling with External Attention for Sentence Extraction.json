{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n2020"
  }, {
    "heading": "1 Introduction",
    "text": "Recurrent neural networks have become one of the most widely used models in natural language processing (NLP). A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit networks (GRU; Cho et al., 2014) have been designed to model text capturing long-term dependencies in problems such as language modeling. However, document modeling, a key to many natural language\n∗The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh.\n1Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information.\nunderstanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level.\nIt is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals.\nWe present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words. Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document. Our model explicitly biases the extractor with external cues and\nimplicitly biases the encoder through training.\nWe demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information. These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document. Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query. For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information. For answer selection, we use word overlap features, such as the inverse sentence frequency (ISF, Trischler et al., 2016) and the inverse document frequency (IDF) together with the query, all formulated as external cues.\nOur main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.\nSecond, while external information has been shown to be useful for summarization systems using traditional hand-crafted features (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001), our model is the first to exploit such information in deep learning-based summarization. We evaluate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset (Hermann et al., 2015). Experimental results show that our summarizer, informed with title and image captions, consistently outperforms summarizers that do not use this information. We also conduct a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete.\nLastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often\nmeasure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other."
  }, {
    "heading": "2 Document Modeling For Sentence Extraction",
    "text": "Given a document D consisting of a sequence of n sentences (s1, s2, ..., sn) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017). The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below. The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues.\nSentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence. In Figure 1, ker-\nnels of size 2 (red) and 4 (blue) are applied three times each. The max-pooling over time operation yields two feature lists fK2 and fK4 ∈ R3. The final sentence embeddings have six dimensions.\nDocument Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1, s2, . . . , sn), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).\nSentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues. It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer. Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder). Instead, our extractor attends to a sequence of p pieces of external information E : (e1, e2, ..., ep) relevant for the task (e.g., ei is a title or an image caption for summarization) for cues. At time ti, it reads sentence si and makes a binary prediction, conditioned on the document representation (obtained from the document encoder), the previously labeled sentences and the external information. This way, our labeler is able to identify locally and globally important sentences within the document which correlate well with the external information.\nGiven sentence st at time step t, it returns a probability distribution over labels as:\np(yt|st, D,E) = softmax(g(ht, h′t)) (1) g(ht, h ′ t) = Uo(Vhht +W ′ hh ′ t) (2)\nht = LSTM(st, ht−1)\nh′t = p∑ i=1 α(t,i)ei,\nwhere α(t,i) = exp(htei)∑ j exp(htej)\nwhere g(·) is a single-layer neural network with parameters Uo, Vh and W ′h. ht is an intermedi-\nate RNN state at time step t. The dynamic context vector h′t is essentially the weighted sum of the external information (e1, e2, . . . , ep). Figure 1 summarizes our model."
  }, {
    "heading": "3 Sentence Extraction Applications",
    "text": "We validate our model on two sentence extraction problems: extractive document summarization and answer selection for machine reading comprehension. Both these tasks require local and global contextual reasoning about a given document. As such, they test the ability of our model to facilitate document modeling using external information.\nExtractive Summarization An extractive summarizer aims to produce a summary S by selecting m sentences from D (where m < n). In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si should be included in the summary) by assigning score p(yi|si,D ,E , θ) quantifying the relevance of si to the summary. We assemble a summary S by selecting m sentences with top p(yi = 1|si,D ,E , θ) scores.\nWe formulate external information E as the sequence of the title and the image captions associated with the document. We use the convolutional sentence encoder to get their sentence-level representations.\nAnswer Selection Given a question q and a document D , the goal of the task is to select one candidate sentence si ∈ D in which the answer exists. In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si contains the answer) and assign score p(yi|si,D ,E , θ) quantifying si’s relevance to the query. We return as answer the sentence si with the highest p(yi = 1|si,D ,E , θ) score.\nWe treat the question q as external information and use the convolutional sentence encoder to get its sentence-level representation. This simplifies Eq. (1) and (2) as follow:\np(yt|st, D, q) = softmax(g(ht, q)) (3) g(ht, q) = Uo(Vhht +Wqq),\nwhere Vh and Wq are network parameters. We exploit the simplicity of our model to further assimilate external features relevant for answer selection: the inverse sentence frequency (ISF, (Trischler et al., 2016)), the inverse document frequency (IDF) and a modified version of the ISF score which we call local ISF. Trischler et al. (2016) have shown that a simple ISF baseline (i.e., a sentence with the highest ISF score as an answer) correlates well with the answers. The ISF score αsi for the sentence si is computed as αsi =∑\nw∈si∩q IDF(w), where IDF is the inverse document frequency score of word w, defined as: IDF(w) = log NNw , whereN is the total number of sentences in the training set and Nw is the number of sentences in which w appears. Note that, si ∩ q\nrefers to the set of words that appear both in si and in q. Local ISF is calculated in the same manner as the ISF score, only with setting the total number of sentences (N ) to the number of sentences in the article that is being analyzed.\nMore formally, this modifies Eq. (3) as follows:\np(yt|st, D, q) = softmax(g(ht, q, αt, βt, γt)),(4)\nwhere αt, βt and γt are the ISF, IDF and local ISF scores (real values) of sentence st respectively . The function g is calculated as follows:\ng(ht, q, αt, βt, γt) =Uo (Vhht+\nWqq +Wisf(αt · 1)+ Widf(βt · 1) +Wlisf(γt · 1) ) ,\nwhere Wisf , Widf and Wlisf are new parameters added to the network and 1 is a vector of 1s of size equal to the sentence embedding size. In Figure 1, these external feature vectors are represented as 6-dimensional gray vectors accompanied with dashed arrows."
  }, {
    "heading": "4 Experiments and Results",
    "text": "This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups. In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation."
  }, {
    "heading": "4.1 Extractive Document Summarization",
    "text": "Summarization Dataset We evaluated our models on the CNN news highlights dataset (Hermann et al., 2015).2 We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents). We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the\n2Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset. We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document. As image captions often do not have sentence boundaries, they blend with the sentences of the document unnoticeably. This leads to the production of erroneous summaries.\n“story highlights” associated with each article are gold-standard abstractive summaries. We trained our network on a named-entity-anonymized version of news articles. However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.\nTo train our model, we need documents annotated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy). We followed Nallapati et al. (2017) and automatically extracted ground truth labels such that all positively labeled sentences from an article collectively give the highest ROUGE (Lin and Hovy, 2003) score with respect to the gold summary.\nWe used a modified script of Hermann et al. (2015) to extract titles and image captions, and we associated them with the corresponding articles. All articles get associated with their titles. The availability of image captions varies from 0 to 414 per article, with an average of 3 image captions. There are 40% CNN articles with at least one image caption.\nAll sentences, including titles and image captions, were padded with zeros to a sentence length of 100. All input documents were padded with zeros to a maximum document length of 126. For each document, we consider a maximum of 10 image captions. We experimented with various numbers (1, 3, 5, 10 and 20) of image captions on the validation set and found that our model performed best with 10 image captions. We refer the reader to the supplementary material for more implementation details to replicate our results.\nComparison Systems We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary. We refer to this baseline as LEAD in the rest of the paper.\nWe also compared our system against the sentence extraction system of Cheng and Lapata (2016). We refer to this system as POINTERNET as the neural attention architecture in Cheng and Lapata (2016) resembles the one of Pointer Networks (Vinyals et al., 2015).3 It does not exploit any external information.4 Cheng and Lap-\n3The architecture of POINTERNET is closely related to our model without external information.\n4Adding external information to POINTERNET is an in-\nata (2016) report only on the DailyMail dataset. We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5\nAutomatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6 Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously.\nteresting direction of research but we do not pursue it here. It requires decoding with multiple types of attentions and this is not the focus of this paper.\n5We are unable to compare our results to the extractive system of Nallapati et al. (2017) because they report their results on the DailyMail dataset and their code is not available. The abstractive systems of Chen et al. (2016) and Tan and Wan (2017) report their results on the CNN dataset, however, their results are not comparable to ours as they report on the full-length F1 variants of ROUGE to evaluate their abstractive summaries. We report ROUGE recall scores which is more appropriate to evaluate our extractive summaries.\n6We used pyrouge, a Python package, to compute all our ROUGE scores with parameters “-a -c 95 -m -n 4 -w 1.2.”\nWe report our results on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries. For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences on average in the gold highlights of the training set. We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.\nWe experimented with two types of external information: title (TITLE) and image captions (CAPTION). In addition, we experimented with the first sentence (FS) of the document as external information. Note that the latter is not external information, it is a sentence in the document. However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016). XNET with FS acts as a baseline for XNET with title and image captions.\nWe report the performance of several variants of XNET on the validation set in Table 1. We also compare them against the LEAD baseline and POINTERNET. These two systems do not use any additional information. Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET. When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information. Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001). The performance with TITLE and CAPTION is better than that with FS. We also tried possible combinations of TITLE, CAPTION and FS. All XNET models are superior to the ones without any external information. XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively). It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document. We use this model for testing purposes.\nOur final results on the test set are shown in Table 2. It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior\nto XNET. This result could be because LEAD (always) and POINTERNET (often) include the first sentence in their summaries, whereas, XNET is better capable at selecting sentences from various document positions. This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where XNET performs best across all ROUGE scores. We note that POINTERNET outperforms LEAD for 75-byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it outperforms LEAD for full length summaries on the metrics R1, R2 and RL. It shows that POINTERNET with its attention over sentences in the document is capable of exploring more than first few sentences in the document, but it is still behind XNET which is better at identifying salient sentences in the document. XNET performs significantly better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores.\nHuman Evaluation We complement our automatic evaluation results with human evaluation. We randomly selected 20 articles from the test set.\nAnnotators were presented with a news article and summaries from four different systems. These include the LEAD baseline, POINTERNET, XNET and the human authored highlights. We followed the guidelines in Cheng and Lapata (2016), and asked our participants to rank the summaries from best (1st) to worst (4th) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We did not allow any ties and we only sampled articles with nonidentical summaries. We assigned this task to five annotators who were proficient English speakers. Each annotator was presented with all 20 articles. The order of summaries to rank was randomized per article. An example of summaries our subjects ranked is provided in the supplementary material.\nThe results of our human evaluation study are shown in Table 3. As one might imagine, HUMAN gets ranked 1st most of the time (41%). However, it is closely followed by XNET which ranked 1st 28% of the time. In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places. We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01). It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN. On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN. The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries."
  }, {
    "heading": "4.2 Answer Selection",
    "text": "Question Answering Datasets We run experiments on four datasets collected for open domain question-answering tasks: WikiQA (Yang et al., 2015), SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMarco (Nguyen et al., 2016).\nNewsQA was especially designed to present lexical and syntactic divergence between questions and answers. It contains 119,633 questions posed by crowdworkers on 12,744 CNN articles previously collected by Hermann et al. (2015). In a similar manner, SQuAD associates 100,000+\nquestion with a Wikipedia article’s first paragraph, for 500+ previously chosen articles. WikiQA was collected by mining web-searching query logs and then associating them with the summary section of the Wikipedia article presumed to be related to the topic of the query. A similar collection procedure was followed to create MSMarco with the difference that each candidate answer is a whole paragraph from a different browsed website associated with the query.\nWe follow the widely used setup of leaving out unanswered questions (Trischler et al., 2016; Yang et al., 2015) and adapt the format of each dataset to our task of answer sentence selection by labeling a candidate sentence with 1 if any answer span is contained in that sentence. In the case of MSMarco, each candidate paragraph comes associated with a label, hence we treat each one as a single long sentence. Since SQuAD keeps the official test dataset hidden and MSMarco does not provide labels for its released test set, we report results on their official validation sets. For validation, we set apart 10% of each official training set.\nOur dataset splits consist of 92,525, 5,165 and 5,124 samples for NewsQA; 79,032, 8,567, and 10,570 for SQuAD; 873, 122, and 237 for WikiQA; and 79,704, 9,706, and 9,650 for MSMarco, for training, validation, and testing respectively.\nComparison Systems We compared the output of our model against the ISF (Trischler et al., 2016) and LOCALISF baselines. Given an article, the sentence with the highest ISF score is selected as an answer for the ISF baseline and the sentence with the highest local ISF score for the LOCALISF baseline. We also compare our model against a neural network (PAIRCNN) that encodes (question, candidate) in an isolated manner as in previous work (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). The architecture uses the sentence encoder explained in earlier sections to learn the question and candidate representations. The distribution over labels is given by p(yt|q) = p(yt|st, q) = softmax(g(st, q)) where g(st, q) = ReLU(Wsq · [st; q] + bsq). In addition, we also compare our model against APCNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (Wang and Jiang, 2017), KVMemNN (Miller et al., 2016), and COMPAGGR, a state-of-the-art system by Wang et al. (2017).\nWe experiment with several variants of our model. XNET is the vanilla version of our sen-\ntence extractor conditioned only on the query q as external information (Eq. (3)). XNET+ is an extension of XNET which uses ISF, IDF and local ISF scores in addition to the query q as external information (Eqn. (4)). We also experimented with a baseline XNETTOPK where we choose the top k sentences with highest ISF score, and then among them choose the one with the highest probability according to XNET. In our experiments, we set k = 5. In the end, we experimented with an ensemble network LRXNET which combines the XNET score, the COMPAGGR score and other word-overlap-based scores (tweaked and optimized for each dataset separately) for each sentence using a logistic regression classifier. It uses ISF and LocalISF scores for NewsQA, IDF and ISF scores for SQuAD, sentence length, IDF and ISF scores for WikiQA, and word overlap and ISF score for MSMarco. We refer the reader to the supplementary material for more implementation and optimization details to replicate our results.\nEvaluation Metrics We consider metrics that evaluate systems that return a ranked list of candidate answers: mean average precision (MAP), mean reciprocal rank (MRR), and accuracy (ACC).\nResults Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco. Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions,\ninstead of only observing each candidate in isolation.\nSecondly, we can observe that ISF is indeed a strong baseline that outperforms XNET. This means that just “reading” the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering. Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR. Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.\nThis consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection. Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique. Using it as a hard constraint, with XNETTOPK, does not achieve the best result. We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself. As such, XNET+ is capable of using this feature in datasets with richer context.\nIt is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern. For the SQuAD dataset, the results are comparable (less than 1%). However, the improvement for WikiQA reaches ∼3% and then the gap shrinks again for NewsQA, with an improvement of ∼1%. This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and\nto an entire article for NewsQA. Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises.7\nInterestingly, our model lags behind COMPAGGR on the MSMarco dataset. It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets. As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly. This can be observed by the fact that XNET and PAIRCNN obtain comparable results. COMPAGGR performs better because comparing each candidate independently is a better strategy."
  }, {
    "heading": "5 Conclusion",
    "text": "We describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document. We implement our approach through an attention mechanism of a neural network architecture for modeling documents.\nOur experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks. Our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information. Second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal. For answer selection, we show that inserting the query with word overlap features using our external attention mechanism outperforms state-of-the-art systems that naturally also have access to this information."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Jianpeng Cheng for providing us with the CNN dataset and the implementation of Point-\n7See the supplementary material for an example supporting our hypothesis.\nerNet. We also thank the members of the Edinburgh NLP group for participating in our human evaluation experiments. This work greatly benefitted from discussions with Jianpeng Cheng, Annie Louis, Pedro Balage, Alfonso Mendes, Sebastião Miranda, and members of the Edinburgh NLP group. We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760), the European Union under the Horizon 2020 SUMMA project (Narayan, Cohen; grant agreement 688139), and Huawei Technologies (Cohen)."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the 3rd International Conference on Learning Representations. San Diego, California, USA.",
    "year": 2015
  }, {
    "title": "Distraction-based neural networks for modeling documents",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang."],
    "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence. New York, USA, pages 2754–2760.",
    "year": 2016
  }, {
    "title": "Neural summarization by extracting sentences and words",
    "authors": ["Jianpeng Cheng", "Mirella Lapata."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 484–494.",
    "year": 2016
  }, {
    "title": "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings",
    "year": 2014
  }, {
    "title": "Attentive pooling networks",
    "authors": ["Cıcero Nogueira dos Santos", "Ming Tan", "Bing Xiang", "Bowen Zhou."],
    "venue": "CoRR abs/1602.03609.",
    "year": 2016
  }, {
    "title": "New methods in automatic extracting",
    "authors": ["Harold P. Edmundson."],
    "venue": "Journal of the Association for Computing Machinery 16(2):264–285.",
    "year": 1969
  }, {
    "title": "Sentence compression by deletion with LSTMs",
    "authors": ["Katja Filippova", "Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon,",
    "year": 2015
  }, {
    "title": "Contextual LSTM (CLSTM) models for large scale NLP tasks",
    "authors": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck."],
    "venue": "CoRR abs/1602.06291.",
    "year": 2016
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Advances in Neural Information Processing Systems 28. pages 1693–",
    "year": 2015
  }, {
    "title": "Long Short-Term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Reinforced mnemonic reader for machine comprehension",
    "authors": ["Minghao Hu", "Yuxing Peng", "Xipeng Qiu."],
    "venue": "CoRR abs/1705.02798.",
    "year": 2017
  }, {
    "title": "Document context language models",
    "authors": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein."],
    "venue": "CoRR abs/1511.03962.",
    "year": 2015
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 1746–1751.",
    "year": 2014
  }, {
    "title": "Character-aware neural language models",
    "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence. Phoenix, Arizona USA, pages 2741–2749.",
    "year": 2016
  }, {
    "title": "A trainable document summarizer",
    "authors": ["Julian Kupiec", "Jan Pedersen", "Francine Chen."],
    "venue": "Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Seattle, Washington, USA, pages",
    "year": 1995
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Thang Luong", "Dan Jurafsky."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
    "year": 2015
  }, {
    "title": "Automatic evaluation of summaries using N-gram cooccurrence statistics",
    "authors": ["Chin-Yew Lin", "Eduard Hovy."],
    "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association",
    "year": 2003
  }, {
    "title": "Hierarchical recurrent neural network for document modeling",
    "authors": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing. Lisbon, Portugal, pages",
    "year": 2015
  }, {
    "title": "Automatic Summarization",
    "authors": ["Inderjeet Mani."],
    "venue": "Natural language processing. John Benjamins Publishing Company.",
    "year": 2001
  }, {
    "title": "Context dependent recurrent neural network language model",
    "authors": ["Tomas Mikolov", "Geoffrey Zweig."],
    "venue": "Proceedings of the Spoken Language Technology Workshop. IEEE, pages 234–239.",
    "year": 2012
  }, {
    "title": "Key-value memory networks for directly reading documents",
    "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods on Natural",
    "year": 2016
  }, {
    "title": "SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents",
    "authors": ["Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou."],
    "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence. San Francisco, Cali-",
    "year": 2017
  }, {
    "title": "Abstractive text summarization using sequence-tosequence RNNs and beyond",
    "authors": ["Ramesh Nallapati", "Bowen Zhou", "Cı́cero Nogueira dos Santos", "Çaglar Gülçehre", "Bing Xiang"],
    "venue": "In Proceedings of the 20th SIGNLL Conference on Computational Natural",
    "year": 2016
  }, {
    "title": "Ranking sentences for extractive summarization with reinforcement learning",
    "authors": ["Shashi Narayan", "Shay B. Cohen", "Mirella Lapata."],
    "venue": "Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2018
  }, {
    "title": "MS Marco: A human generated machine reading comprehension dataset",
    "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."],
    "venue": "Proceedings of the Workshop on Cognitive Computation: Inte-",
    "year": 2016
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 2383–2392.",
    "year": 2016
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 379–389.",
    "year": 2015
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada, pages 1073–1083.",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Advances in Neural Information Processing Systems 27. pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Abstractive document summarization with a graph-based attentional neural model",
    "authors": ["Jiwei Tan", "Xiaojun Wan."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Vancouver, Canada, pages 1171–1181.",
    "year": 2017
  }, {
    "title": "Inter-document contextual language model",
    "authors": ["Quan Hung Tran", "Ingrid Zukerman", "Gholamreza Haffari."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
    "year": 2016
  }, {
    "title": "Newsqa: A machine comprehension dataset",
    "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."],
    "venue": "CoRR abs/1611.09830.",
    "year": 2016
  }, {
    "title": "Pointer networks",
    "authors": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."],
    "venue": "Advances in Neural Information Processing Systems 28. pages 2692–2700.",
    "year": 2015
  }, {
    "title": "A compareaggregate model for matching text sequences",
    "authors": ["Shuohang Wang", "Jing Jiang."],
    "venue": "Proceedings of the 5th International Conference on Learning Representations. Toulon, France.",
    "year": 2017
  }, {
    "title": "Larger-context language modelling with recurrent neural network",
    "authors": ["Tian Wang", "Kyunghyun Cho."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 1319–1329.",
    "year": 2016
  }, {
    "title": "Gated self-matching networks for reading comprehension and question answering",
    "authors": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "Sentence similarity learning by lexical decomposition and composition",
    "authors": ["Zhiguo Wang", "Haitao Mi", "Abraham Ittycheriah."],
    "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.",
    "year": 2016
  }, {
    "title": "Making neural QA as simple as possible but not simpler",
    "authors": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."],
    "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning. Vancouver, Canada, pages 271–280.",
    "year": 2017
  }, {
    "title": "WikiQA: A challenge dataset for open-domain question answering",
    "authors": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 2013–2018.",
    "year": 2015
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "ABCNN: Attention-based convolutional neural network for modeling sentence pairs",
    "authors": ["Wenpeng Yin", "Hinrich Schtze", "Bing Xiang", "Bowen Zhou."],
    "venue": "Transactions of the Association for Computational Linguistics 4:259–272.",
    "year": 2016
  }],
  "id": "SP:822469ed2719b20ba85fa96f4cfcf15e8f976831",
  "authors": [{
    "name": "Shashi Narayan",
    "affiliations": []
  }, {
    "name": "Ronald Cardenas",
    "affiliations": []
  }, {
    "name": "Nikos Papasarantopoulos",
    "affiliations": []
  }, {
    "name": "Shay B. Cohen",
    "affiliations": []
  }, {
    "name": "Mirella Lapata",
    "affiliations": []
  }, {
    "name": "Jiangsheng Yu",
    "affiliations": []
  }, {
    "name": "Yi Chang",
    "affiliations": []
  }],
  "abstractText": "Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.1",
  "title": "Document Modeling with External Attention for Sentence Extraction"
}