{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 153–161 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Natural language understanding (NLU) is a key component of dialog systems for commercial personal digital assistants (PDAs) such as Amazon Alexa, Google Home, Microsoft Cortana and Apple Siri. The task of the NLU component is to map input user utterances into a semantic frame consisting of domain, intent and slots (Kurata et al., 2016). The semantic frame is used by the dialog manager for state tracking and action selection.\nSlot tagging can be formulated as a sequence classification task where each input word in the user utterance must be classified as belonging to one of the slot types in a predefined schema (Sarikaya et al., 2016). In a standard NLU architecture, each new domain defines a new domainspecific schema for its slots. Figure 1 shows examples of annotated queries from three different domains relevant to a typical commercial digital\nassistant. Since the schemas for different domains can vary, the usual strategy is to train a separate slot tagging model for each new domain. However, the number of domains increases rapidly as the PDAs are required to support new scenarios and training a separate slot tagging model for each new domain becomes prohibitively expensive in terms of annotation costs.\nEven though different domains have different slot tagging schemas, some classes of slots appear across a number of domains, as suggested by the examples in Figure 1. Both travel and flight status have date and time related slots, and all three domains have the location slot. Reusing annotated data for these common slots would allow us to train models with better accuracy using less data. However, since both the input distribution and the label distribution are different across domains, we must use domain adaptation methods to train on the joint data (Daume, 2007; Kim et al.,\n153\n2016c; Blitzer et al., 2006). In this data-driven adaptation approach, we build a repository of annotated data containing date, time, location and other reusable slots. We then combine relevant data from the reusable repository with the domain specific data during model training. Figure 2(a) shows an example of this architecture where reusable date/time data is used for training travel domain.\nA drawback of the data-driven adaptation approach is that as the repository of data for reusable slots grows, the training time for new domains increases. The training data for a new domain might be in the hundreds of samples, while the training data for the reusable slots might contain hundreds of thousands of samples. This increase in training time makes iterative refinement difficult in the initial design of new domains, which is when the ability to deploy new models quickly is crucial.\nAn alternative strategy is to use model-driven adaptation approaches (Kim et al., 2017b) as shown in Figure 2(b). Here, instead of retraining on the data for the reusable slots, we train “expert” models for these slots, and use the output of these models directly when training new domains. Using model-driven adaptation ensures that model training time is proportional to the data size of new\ntarget domains, as opposed to the large data size for reusable slots, allowing for faster training.\nIn this paper, we present a model-driven adaptation approach for slot tagging called Bag of Experts (BoE). In Section 2, we first describe how this approach can be applied to two popular machine learning methods used for slot tagging: Long Short Term Memory (LSTM) and Conditional Random Fields (CRF) models. We then describe a dataset of 10 target domains and 2 reusable domains that we’ve collected for use in a commercial digital assistant, in Section 3. Using this data, we conduct experiments comparing the BoE models with their non-expert counterparts, and show that BoE models can lead to significant F1-score improvements. The experimental setup is described in Section 4.1 and the results are discussed in Section 4.3. This is followed by a survey of related work in Section 5 and the conclusion in Section 6."
  }, {
    "heading": "2 Approaches",
    "text": "We first describe our LSTM and CRF models for slot tagging, followed by their BoE variants: LSTM-BoE and CRF-BoE. Tensorflow (Abadi et al., 2015) was used for implementing the LSTM models, while a custom C++ implementation was\nused for the CRF models."
  }, {
    "heading": "2.1 LSTM",
    "text": "For our LSTM model, we follow a standard bidirectional LSTM architecture (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Let w1...wn denote the input word sequence. For every input word wi, let fCi and b C i be the outputs of the forward and backward character level LSTMs respectively, and let mi be the word embedding (initialized either randomly or with pretrained embeddings). The input to the word level LSTMs, gi, is the concatenation of these three vectors:\ngi = [f C i ; b C i ;mi]\nwhere both fCi , b C i ∈ R25 and mi has the same dimensions as the pre-trained embeddings. The forward and backward word level LSTMs take gi as input and produce fWi and b W i , which are then concatenated to produce hi:\nhi = [f W i , b W i ]\nwhere fWi , b W i ∈ R100, making hi ∈ R200. hi is then input to a dense feed forward layer with a softmax activation to predict the label probabilities for each word. We train using stochastic gradient descent with Adam (Kingma and Ba, 2015). To avoid overfitting, we also use dropout on top of mi and hi layers, with a default dropout keep probability of 0.8. We experiment with some variations\nof this default LSTM architecture, the results are described in Section 4.2."
  }, {
    "heading": "2.2 LSTM-BoE",
    "text": "We now describe the LSTM Bag of Experts (LSTM-BoE) architecture. Let e1...ek ∈ E be the set of reusable expert domains. For each expert ej , we train a separate LSTM with the architecture described in Section 2.1. Let heji be the bi-directional word LSTM output for expert ej on word wi.\nWhen training on a target domain, for each word wi, we first compute the character level LSTMs fCi , b C i similarly to Section 2.1. We then compute a BoE representation for this word as:\nhE = ∑\nei∈E h ej i\nThe input to the word level LSTM for word wi in the target domain is now a concatenation of the character level LSTM outputs (fCi , b C i ), the word embedding mi, and hE :\ngi = [f C i ; b C i ;mi;h E ]\ngi is then input to the word level LSTM for the target domain to produce hi in the same way as Section 2.1. This architecture is similar to the one presented in (Kim et al., 2017b), with the exception that in their architecture, hE is concatenated with the word level LSTM output hi for the target\ndomain. In our architecture, we add hE before the word-level LSTM in order to capture long-range dependencies of label prediction for a word on expert predictions for context words."
  }, {
    "heading": "2.3 CRF",
    "text": "Conditional Random Fields (CRF) are a popular family of models that have been proven to work well in a variety of sequence tagging NLP applications (Lafferty et al., 2001). For our experiments, we use a standard linear-chain CRF architecture with n-gram and context features.\nIn particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words. We also use a skip bigram feature created by concatenating the current unigram and skip-one unigram.\nWe train our CRF using stochastic gradient descent with L1 regularization to prevent overfitting. The L1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling (Tsuruoka et al., 2009)."
  }, {
    "heading": "2.4 CRF-BoE",
    "text": "Similar to the LSTM-BoE model, we first train a CRF model cj for each of the reusable expert domains ej ∈ E. When training on a target domain, for every query word wi, a one-hot label vector l j i is emitted by each expert CRF model cj . The length of the label vector lji is the number of labels in the expert domain, with the value corresponding to the label predicted by cj for word wi set to 1, and values for all other labels set to 0. For each word, the label vectors for all the expert CRF models are concatenated and provided as features for the target domain CRF training, along with the n-gram features."
  }, {
    "heading": "3 Data",
    "text": ""
  }, {
    "heading": "3.1 Target Domains",
    "text": "We built a dataset of 10 target domains for experimentation. Table 1 shows the list of domains as well as some statistics and example utterances. We treated these as new domains - that is, we do not have real interaction data with users for these domains. The annotated data is therefore prepared in two steps.\nFirst, utterances are obtained using crowdsourcing, where workers are provided with prompts for different intents of a domain and asked to generate\nnatural language utterances corresponding to those intents. Next, the generated utterances are annotated by a different set of crowd workers, using the slot schema for each domain. Inter-annotator agreement as well as manual inspection are used to ensure data quality in both stages.\nThe amount of data collected varies for each domain based on its complexity and business priority. Dataset size statistics for the data used in our experiments are presented in section 4.1. Test and dev data are sampled at 10% of the total annotated data, with stratified sampling used in order to preserve the distribution of the intents."
  }, {
    "heading": "3.2 Reusable Domains",
    "text": "We experiment with two domains containing reusable slots: timex and location. The timex domain consists of utterances containing the slots date, time and duration. The location domain consists of utterances containing location, location type and place name slots. Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse.1\n1Several other candidate reusable domains exist, including: the name domain containing the slot contact name; the number domain containing the slots rating, quantity and price; and the reference domain containing the slots ordinal (whose values include “first”, “second” or “third”) and order ref (with values such as “before” or “after”). All of these slots appear in more than 25% of the available domains.\nData for these domains was sampled from the input utterances from our commercial digital assistant. Each reusable domain contains about a million utterances. There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains. The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains.\nGrouping the reusable slots into domains in this way provides additional opportunities for a commercial system: the trained reusable domain models can be used in other related products which need to identify time and location related entities. Models trained on the timex and location data have F1-scores of 96% and 89% respectively on test data from their respective domains."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Experimental Setup",
    "text": "We want to verify if BoE models can improve slot tagging performance by using the information from reusable domains. To simulate the low data scenario for the initial model training, we create three training datasets by sampling 2000, 1000 and 500 training examples from every domain. We use stratified sampling to maintain the input distribution of the intents across the three training datasets.\nFor each training dataset, we train the four models as described in Section 2 and compute the precision, recall and F1-score on the test data. Fixed seeds are used when training all models to make the results reproducible. Table 3 summarizes these results, with only F1-scores reported to save space. We describe these results in Section 4.3."
  }, {
    "heading": "4.2 LSTM architecture variants",
    "text": "Using the dev data set for the 10 domains, we experimented with using different pretrained embeddings, dropout probabilities and a CRF output layer in our LSTM architecture. The results are summarized in Table 2. For each of the 10 domains, we trained using each variant with 10 different seeds, and computed the mean F1-score for each domain. For comparing two variants, we computed the mean difference in the F1-scores over the 10 domains and its p-value.\nWe tried word level Glove embeddings of 100, 200 and 300 dimensions as well as 500- dimensional word embeddings trained over the ut-\nterances from our commercial PDA logs. Both 100 and 200 dimensional Glove embeddings led to statistically significant improvements, but the word embeddings trained over our logs led to the biggest improvement. We also tried using a CRF output layer (Lample et al., 2016) and different values of dropout keep probability, but none of them gave statistically significant improvements over the default model. Based on this, we used PDA trained 500-dimensional word embeddings for our final experiments on test data."
  }, {
    "heading": "4.3 Results and Discussion",
    "text": "Table 3(a) shows the F1-scores obtained by the different methods for the training data set of 2000 training instances for each of the 10 domains. LSTM based models in general perform better than the CRF based models. The LSTM models have a statistically significant average improvement of 3.14 absolute F1-score over the CRF models. The better performance of LSTM over CRF can be explained by the LSTM being able to use information over longer contexts to make predictions, while the CRF model is limited to at most the previous and next 3 words.\nThe results in Table 3(a) also show that both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. LSTM-BoE has a statistically significant mean improvement of 1.92 points over LSTM. CRF-BoE also shows an average improvement of 2.19 points over the CRF model, but the results are not statistically significant. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of timex and location slots, as shown in Table 4.\nThe shopping model shows a regression for BoE models, and a reason could be the low frequency of expert slots (Table 4). However, low frequency of expert slots does not always mean that BoE methods can’t help, as shown by the improvement in the purchase domain. Finally, for sports, social network and deals domains, the LSTM-BoE improves over LSTM, while CRFBoE does not improve over CRF. Our hypothesis is that given the query patterns for these domains, the dense vector output used by LSTM-BoE is able to transfer some information, while the categorical label output used by CRF-BoE is not.\nTable 3(b) shows the results with 500 and 1000\ntraining data instances. Note that the improvements are even higher for the experiments with smaller training data. In particular, LSTM-BoE shows an improvement of 4.63 in absolute F1score over LSTM when training with 500 instances. Thus, as we reduce the amount of training data in the target domain, the performance improvement from BoE models is even higher.\nAs an example, in the purchase domain, the LSTM-BoE model achieves an F1-score of 70.66% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 66.24%. Thus the LSTM-BoE model achieves better F1-score with only one-fourth the training data. Similarly, for flight status, travel, and transportation domains, the LSTM-BoE model gets better performance with 500 training instances, compared to a CRF model with 2000 training instances. The LSTMBoE architecture, therefore, allows us to reuse the domain experts to produce better performing mod-\nels with much lower data annotation costs. As the target domain training data increases, the contribution due to domain experts goes down, but more experimentation is needed to establish the threshold at which it is no longer useful to add experts."
  }, {
    "heading": "5 Related Work",
    "text": "Early methods for slot-tagging used rule-based approaches (Ward and Issar, 1994). Much of the later work on supervised learning focused on CRFs, for example (Sarikaya et al., 2016), or neural networks (Deoras and Sarikaya, 2013; Yao et al., 2013; Liu et al., 2015; Celikyilmaz and HakkaniTur, 2015). Unsupervised (or weakly-supervised) methods also were used for NLU tasks, primarily leveraging search query click logs (Hakkani-Tur et al., 2011a,b, 2013) and knowledge graphs (Tur et al., 2012; Heck and Hakkani-Tur, 2012; Heck et al., 2013); hybrid methods, for example as described in (Kim et al., 2015a; Celikyilmaz et al., 2015; Chen et al., 2016), also exist. Our approach\nin this paper is a purely supervised one. Transfer learning is a vast area of research, with too many publications for an exhaustive list. We discuss some of the recent work most relevant to our methods. In (Kim et al., 2015b), the slot labels from across different domains are mapped into a shared space using Canonical Correlation Analysis (CCA) and automatically-induced embeddings over the label space. These label representations allow mapping of label types between different domains, which makes it possible to apply standard data-driven domain adaptation approaches (Daume, 2007). They also introduce a model-driven adaptation technique based on training a hidden unit CRF (HUCRF) on the source domain, which is then used to initialize the training for the target domain. The limitation of this approach is that only one source domain can be used, while multiple experts can be used in the proposed BoE approach.\n(Kim et al., 2016a) build a single, universal slot tagging model, and constrain the decoding process to subsets of slots for various domains; this process assumes that a mapping of slot tags in the new domain to the ones in the universal slot model has already been generated. A related work by (Kim et al., 2016b) directly predicts the required schema prior to performing the constrained decoding. These approaches are attractive because only one universal model needs to be trained, but do not work in cases when a new domain contains a mixture of new and existing slots. Our approach allows transfer of partial knowledge in such cases.\n(Kim et al., 2016c) uses a neural version of the approach first described in (Daume, 2007), by using existing annotated data in a variety of domains\nto adapt the slot tag models of new domains where the tag space is partly shared. The drawback of such data-driven domain adaptation is the increase in training time as more experts are added.\nAn expert-based adaptation, similar to the techniques applied in this paper, was first described in (Kim et al., 2017b). (Jaech et al., 2016) use multitask learning, training a bidirectional LSTM with character-level embeddings, trained jointly to produce slot tags for a number of travel-related domains. Finally, (Kim et al., 2017a) frame the problem of temporal shift in data of a single domain (and the related problem of bootstrapping a new domain with imperfectly-matched synthetic data) as one of domain adaptation, applying adversarial training approaches.\nA number of researchers also investigated bootstrapping NLU systems using zero-shot learning. (Dauphin et al., 2014; Kumar et al., 2017) both investigated domain classification; most relevant to us is the work by (Bapna et al., 2017), who studied full semantic frame tagging using zero-shot learning, by projecting the tags into a shared embedding space, similar to work done by (Kim et al., 2015b)."
  }, {
    "heading": "6 Conclusion",
    "text": "We experimented with Bag of Experts (BoE) architectures for CRF and LSTM based slot tagging models. Our experimental results over a set of 10 domains show that BoE architectures are able to use the information from reusable expert models to perform significantly better than their nonexpert counterparts. In particular, the LSTM-BoE model shows a statistically significant improvement of 1.92% over the LSTM model on average when training with 2000 instances. When training with 500 instances, the improvement of LSTM-BoE model over LSTM is even higher at 4.63%. For multiple domains, an LSTM-BoE model trained on only 500 instances is able to outperform a baseline CRF model trained over 4 times the data. Thus, the BoE approach produces high performing models for slot tagging at much lower annotation costs."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Ahmed El Kholy for his comments and feedback on an earlier version of this paper. Also, thanks to Kyle Williams and Zhaleh Feizollahi for their help with code and data collection."
  }],
  "year": 2018,
  "references": [{
    "title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
    "authors": ["van", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"],
    "year": 2015
  }, {
    "title": "Toward zero-shot frame semantic parsing for domain scaling",
    "authors": ["Ankur Bapna", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."],
    "venue": "Proc. Interspeech.",
    "year": 2017
  }, {
    "title": "Domain adaptation with structural correspondence learning",
    "authors": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."],
    "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 120–128, Strouds-",
    "year": 2006
  }, {
    "title": "Convolutional neural network based semantic tagging with entity embeddings",
    "authors": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."],
    "venue": "Proc. NIPS Workshop on Machine Learning for SLU and Interaction.",
    "year": 2015
  }, {
    "title": "Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems",
    "authors": ["Asli Celikyilmaz", "Dilek Hakkani-Tur", "Panupong Pasupat", "Ruhi Sarikaya."],
    "venue": "In Proc. AAAI.",
    "year": 2015
  }, {
    "title": "Syntax or semantics? knowledge-guided joint semantic frame parsing",
    "authors": ["Yun-Nung (Vivian) Chen", "Dilek Hakkani-Tur", "Gokhan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng"],
    "venue": "In IEEE Workshop on Spoken Language Technology",
    "year": 2016
  }, {
    "title": "Frustratingly Easy Domain Adaptation",
    "authors": ["Hal Daume", "III."],
    "venue": "Proc. ACL, pages 256–263, Prague, Czech Republic. Association for Computational Linguistics.",
    "year": 2007
  }, {
    "title": "Zero-shot learning and clustering for semantic utterance classification",
    "authors": ["Yann Dauphin", "Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."],
    "venue": "International Conference on Learning Representations.",
    "year": 2014
  }, {
    "title": "Deep belief network based semantic taggers for spoken language understanding",
    "authors": ["Anoop Deoras", "Ruhi Sarikaya."],
    "venue": "Proc. Interspeech.",
    "year": 2013
  }, {
    "title": "Employing web search query click logs for multi-domain spoken language understanding",
    "authors": ["D. Hakkani-Tur", "G. Tur", "L. Heck", "A. Celikyilmaz", "A. Fidler", "D. Hillard", "R. Iyer", "S. Parthasarathy."],
    "venue": "Proc. ASRU.",
    "year": 2011
  }, {
    "title": "Bootstrapping domain detection using query click logs for new domains",
    "authors": ["D. Hakkani-Tur", "G. Tur", "L. Heck", "E. Shriberg."],
    "venue": "Proc. Interspeech.",
    "year": 2011
  }, {
    "title": "A weakly supervised approach for discovering new user intents from search query logs",
    "authors": ["Dilek Hakkani-Tur", "Asli Celikyilmaz", "Larry Heck", "Gokhan Tur."],
    "venue": "Proc. Interspeech.",
    "year": 2013
  }, {
    "title": "Exploiting the semantic web for unsupervised spoken language understanding",
    "authors": ["Larry Heck", "Dilek Hakkani-Tur."],
    "venue": "Proc. SLT.",
    "year": 2012
  }, {
    "title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing",
    "authors": ["Larry Heck", "Dilek Hakkani-Tur", "Gokhan Tur."],
    "venue": "Proc. Interspeech.",
    "year": 2013
  }, {
    "title": "Bidirectional LSTM-CRF models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "CoRR, abs/1508.01991.",
    "year": 2015
  }, {
    "title": "Domain adaptation of recurrent neural networks for natural language understanding",
    "authors": ["Aaron Jaech", "Larry Heck", "Mari Ostendorf."],
    "venue": "Proc. Interspeech.",
    "year": 2016
  }, {
    "title": "Weakly supervised slot tagging with partially labeled sequences from web search click logs",
    "authors": ["Young-Bum Kim", "Minwoo Jeong", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proc. NAACL.",
    "year": 2015
  }, {
    "title": "Natural language model reusability for scaling to different domains",
    "authors": ["Young-Bum Kim", "Alexandre Rochette", "Ruhi Sarikaya."],
    "venue": "Proc. EMNLP.",
    "year": 2016
  }, {
    "title": "Adversarial adaptation of synthetic or stale data",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Proc. ACL.",
    "year": 2017
  }, {
    "title": "Domain attention with an ensemble of experts",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Dongchan Kim."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 643–653.",
    "year": 2017
  }, {
    "title": "Domainless adaptation by constrained decoding on a schema lattice",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "Proc. COLING.",
    "year": 2016
  }, {
    "title": "Frustratingly easy neural domain adaptation",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya."],
    "venue": "COLING, pages 387–396.",
    "year": 2016
  }, {
    "title": "New transfer learning techniques for disaparate label sets",
    "authors": ["Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya", "Minwoo Jeong."],
    "venue": "Proc. ACL.",
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Zero-shot learning across heterogeneous overlapping domains",
    "authors": ["A. Kumar", "P.R. Muddireddy", "M. Dreyer", "B. Hoffmeister."],
    "venue": "Proc. Interspeech.",
    "year": 2017
  }, {
    "title": "Leveraging sentence-level information with encoder lstm for semantic slot filling",
    "authors": ["Gakuto Kurata", "Bing Xiang", "Bowen Zhou", "Mo Yu."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2077–2083,",
    "year": 2016
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."],
    "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "NAACL-HLT.",
    "year": 2016
  }, {
    "title": "Deep contextual language understanding in spoken dialog systems",
    "authors": ["C. Liu", "P. Xu", "R. Sarikaya."],
    "venue": "Proc. Interspeech.",
    "year": 2015
  }, {
    "title": "End-toend sequence labeling via bi-directional lstm-cnnscrf",
    "authors": ["Xuezhe Ma", "Eduard H. Hovy."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, abs/1603.01354.",
    "year": 2016
  }, {
    "title": "An overview of end-to-end language understanding and dialog management for personal digital assistants",
    "authors": ["Ramesh", "Hisami Suzuki", "Roman Holenstein", "Elizabeth Krawczyk", "Vasiliy Radostev."],
    "venue": "IEEE.",
    "year": 2016
  }, {
    "title": "Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty",
    "authors": ["Yoshimasa Tsuruoka", "Jun’ichi Tsujii", "Sophia Ananiadou"],
    "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th In-",
    "year": 2009
  }, {
    "title": "Exploiting the semantic web for unsupervised natural language semantic parsing",
    "authors": ["Gokhan Tur", "Minwoo Jeong", "Ye-Yi Wang", "Dilek Hakkani-Tur", "Larry Heck."],
    "venue": "Proc. Interspeech.",
    "year": 2012
  }, {
    "title": "Recent improvements in the CMU spoken language understanding system",
    "authors": ["W. Ward", "S. Issar."],
    "venue": "Proc. ACL.",
    "year": 1994
  }, {
    "title": "Recurrent neural networks for language understanding",
    "authors": ["K. Yao", "J. Zweig", "M. Hwang", "Y. Shi", "D. Yu."],
    "venue": "Proc. Interspeech. 161",
    "year": 2013
  }],
  "id": "SP:9342b54f38f85574a112d447a9cc5d8409b6fece",
  "authors": [{
    "name": "Rahul Jha",
    "affiliations": []
  }, {
    "name": "Alex Marin",
    "affiliations": []
  }, {
    "name": "Suvamsh Shivaprasad",
    "affiliations": []
  }, {
    "name": "Imed Zitouni",
    "affiliations": []
  }],
  "abstractText": "Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants. Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow. To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models. Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.",
  "title": "Bag of Experts Architectures for Model Reuse in Conversational Language Understanding"
}