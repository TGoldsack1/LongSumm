{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2326–2335, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language.\nRecently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al.,\n∗Corresponding author\n2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013). However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The longterm dependencies need to be transmitted one-byone along the sequence. Some important features could be lost in transmission process. Besides, the error signal is also back-propagated one-byone through multiple time steps in the training phase with back-propagation through time (BPTT) (Werbos, 1990) algorithm. The learning efficiency could also be decreased for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.\nIn this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales. Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), we partition the hidden states of the standard LSTM into several groups. Each group is activated and updated at different time periods. The fast-speed groups keep the short-term memories, while the slow-speed groups keep the long-term memories. We evaluate our model on four benchmark datasets of text classification. Experimental results show that our model can not only handle short texts, but can model long texts.\nOur contributions can be summarized as follows.\n• With the multiple different timescale memories, MT-LSTM easily carries the crucial information over a long distance. MT-LSTM\n2326\ncan well model both short and long texts.\n• MT-LSTM has faster convergence speed than the standard LSTM since the error signal can be back-propagated through multiple timescales in the training phase."
  }, {
    "heading": "2 Neural Models for Sentences and Documents",
    "text": "The primary role of the neural models is to represent the variable-length sentence or document as a fixed-length vector. These models generally consist of a projection layer that maps words, subword units or n-grams to vector representations (often trained beforehand with unsupervised methods), and then combine them with the different architectures of neural networks. Most of these models for distributed representations of sentences or documents can be classified into four categories.\nBag-of-words models A simple and intuitive method is the Neural Bag-of-Words (NBOW) model, in which the representation of sentences or documents can be generated by averaging constituent word representations. However, the main drawback of NBOW is that the word order is lost. Although NBOW is effective for general document classification, it is not suitable for short sentences.\nSequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) (Mikolov et al., 2010) or the gated versions of RNN (Sutskever et al., 2014; Chung et al., 2014). Sequence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN.\nTopological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts\na more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree.\nConvolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can maintain the word order information and learn more abstract characteristics."
  }, {
    "heading": "3 Long Short-Term Memory Networks",
    "text": "A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input sequence. The activation of the hidden state ht at time-step t is computed as a function f of the current input symbol xt and the previous hidden state ht−1\nht = { 0 t = 0 f(ht−1,xt) otherwise\n(1)\nIt is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1.\nTraditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014).\nUnfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence.\nLong short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies. The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary. A number of minor modifications to the standard LSTM unit have been made. While there are numerous LSTM variants, here we describe the implementation used by Graves (2013).\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The entries of the gating vectors it, ft and ot are in [0, 1]. The LSTM transition equations are the following:\nit = σ(Wixt + Uiht−1 + Vict−1) (2) ft = σ(Wfxt + Ufht−1 + Vfct−1), (3) ot = σ(Woxt + Uoht−1 + Voct), (4) c̃t = tanh(Wcxt + Ucht−1), (5) ct = f it ⊙ ct−1 + it ⊙ c̃t, (6) ht = ot ⊙ tanh(ct), (7)\nwhere xt is the input at the current time step, σ denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state.\nFigure 1 shows the structure of a LSTM unit. In\nparticular, these gates and the memory cell allow a LSTM unit to adaptively forget, memorize and expose the memory content. If the detected feature, i.e., the memory content, is deemed important, the forget gate will be closed and carry the memory content across many time-steps, which is equivalent to capturing a long-term dependency. On the other hand, the unit may decide to reset the memory content by opening the forget gate."
  }, {
    "heading": "4 Multi-Timescale Long Short-Term Memory Neural Network",
    "text": "h1 h2 h3 h4 · · · hT softmax\nx1 x2 x3 x4 xT y\n(a) Unfolded LSTM\nLSTM can capture the long-term and short-term dependencies in a sequence. But the long-term dependencies need to be transmitted one-by-one along the sequence. Some important information could be lost in transmission process for long texts, such as documents. Besides, the error signal is back-propagated through multiple time steps when we use the back-propagation through time (BPTT) (Werbos, 1990) algorithm. The training efficiency could also be low for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.\nInspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), which use de-\nlayed connections and units operating at different timescales to improve the simple RNN, we separate the LSTM units into several groups. Different groups capture different timescales dependencies.\nMore formally, the LSTM units are partitioned into g groups {G1, · · · , Gg}. Each group Gk, (1 ≤ k ≤ g) is activated at different time periods Tk. Accordingly, the gates and weight matrices are also partitioned to maintain the corresponding LSTM groups. The MT-LSTM with just one group is the same to the standard LSTM.\nAt each time step t, only the groups Gk that satisfy (tMOD Tk) = 0 are executed. The choice of the set of periods Tk ∈ {T1, · · · , Tg} is arbitrary. Here, we use the exponential series of periods: group Gk has the period of Tk = 2k−1. The group G1 is the fastest one and can be executed at every time step, which works like the standard LSTM. The group Gk is the slowest one.\nAt time step t, the memory cell vector and hidden state vector of group Gk are calculate in two cases:\n(1) When group Gk is activated at time step t, the LSMT units of this group are calculated by the following equations:\nikt = σ(W k i xt + g∑ j=1 Uj→ki h j t−1 + g∑ j=1 Vj→ki c j t−1), (8)\nfkt = σ(W k fxt + g∑ j=1 Uj→kf h j t−1 + g∑ j=1 Vj→kf c j t−1), (9)\nokt = σ(W k oxt + g∑ j=1 Uj→ko h j t−1 + g∑ j=1 Vj→ko c j t), (10)\nc̃kt = tanh(W k cxt + g∑ j=1 Uj→kc h j t−1), (11) ckt = f k t ⊙ ckt−1 + ikt ⊙ c̃kt , (12) hkt = o k t ⊙ tanh(ckt ), (13)\nwhere ikt , f k t and o k t are the vectors of input gates, forget gates, and output gates of group Gk at time step t respectively; ckt and h k t are the memory cell vector and hidden state vector of group Gk at time step t respectively.\n(2) When group Gk is non-activated at time step t, its LSMT units keep unchanged.\nckt = c k t−1, (14) hkt = h k t−1. (15)\nFigure 3 shows the different between the standard LSTM and MT-LSTM."
  }, {
    "heading": "4.1 Two Feedback Strategies",
    "text": "The feedback mechanism of LSTM is implemented by the recurrent connections from time step t − 1 to t. Since the MT-LSTM groups are updated with the different frequencies, we can regard the different group as the human memory. The fast-speed groups are short-term memories, while the slow-speed groups are long-term memories. Therefore, an important consideration is what feedback mechanism is between the shortterm and long-term memories.\nFor the proposed MT-LSTM, we consider two feedback strategies to define the connectivity patterns among the different groups.\nFast-to-Slow (F2S) Strategy Intuitively, when we accumulate the short-term memory to a certain degree, we store some valuable information from the short-term memory into the long-term memory. Therefore, we firstly define a fast to slow strategy, which updates the slower group using the faster group. The connections from group j to group k exist if and only if Tj ≤ Tk. The weight matrices Uj→ki , U j→k f , U j→k o , U j→k c , V j→k i , Vj→kf , V j→k o are set to zero when Tj > Tk.\nThe F2S updating strategy is shown in Figure 3a.\nSlow-to-Fast (S2F) Strategy Following the work of (Koutnik et al., 2014), we also investigate another update scheme from slow-speed group to fast-speed group. The motivation is that a long term memory can be “distilled” into a short-term memory. The connections from group j to group i exist only if Tj ≥ Ti. The weight matrices Uj→ki , Uj→kf , U j→k o , U j→k c , V j→k i , V j→k f , V j→k o are set to zero when Tj < Tk. The S2F update strategy is shown in Figure 3b."
  }, {
    "heading": "4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups",
    "text": "Another consideration is how many groups need to be used. An intuitive way is that we need more groups for long texts than short texts. The number of the group depends the length of the texts.\nHere, we use a simple dynamic strategy to choose the maximum number of groups, and then the best g is chosen as a hyperparameter according to different tasks. The upper bound of the number of groups is calculated by\ng = log2 L− 1, (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice."
  }, {
    "heading": "5 Training",
    "text": "In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters. The network is trained with backpropagation and the gradientbased optimization is performed using the Adagrad update rule (Duchi et al., 2011).\nThe back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error."
  }, {
    "heading": "6 Experiments",
    "text": "In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models."
  }, {
    "heading": "6.1 Datasets",
    "text": "We evaluate our model on four different datasets. The first three datasets are sentence-level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows.\n• SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences."
  }, {
    "heading": "6.2 Competitor Models",
    "text": "We compare our model with the following models:\n• NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).\n1http://nlp.stanford.edu/sentiment. 2http://cogcomp.cs.illinois.edu/Data/\nQA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/\n• RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). • AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015). • DCNN Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014). • CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014). • PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). Here, we use the popular open source implementation of PV in Gensim4. • LSTM The standard LSTM for text classification. We use the implementation of Graves (2013). The unfolded illustration is shown in Figure 2a."
  }, {
    "heading": "6.3 Hyperparameters and Training",
    "text": "In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabulary size is about 500,000. The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters for the LSTM and MTLSTM are set as Figure 2."
  }, {
    "heading": "6.4 Results",
    "text": "Table 3 shows the classification accuracies of the standard LSTM, MT-LSTM compared with the competitor models.\nFirstly, we compare two feedback strategies of MT-LSTM. The fast-to-slow feedback strat-\n4https://github.com/piskvorky/gensim/\negy (MT-LSTM (F2S)) is better than the slow-tofast strategy (MT-LSTM (S2F)), which indicates that MT-LSTM benefits from periodically storing some valuable information “purified” from the short-term memory into the long-term memory. In the following discussion, we use fast-to-slow feedback strategy as the default setting of MT-LSTM.\nCompared with the standard LSTM, MT-LSTM results in significantly improvements with the same size of hidden layers.\nMT-LSTM outperforms the competitor models on the SST-1, QC and IMDB datasets, and is close to the two best CNN based models on the SST-2 dataset. But MT-LSTM uses much fewer parameters than the CNN based models. The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN.\nMoreover, MT-LSTM can not only handle short texts, but can model long texts in classification task.\nDocuments Modeling Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents). For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to working on sentences instead of paragraphs or documents. Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents. The first level uses a DCNN to trans-\nform embeddings for the words in each sentence into an embedding for the entire sentence. The second level uses another DCNN to transform sentence embeddings from the first level into a single embedding vector that represents the entire document. However, their result is unsatisfactory and they reported that the IMDB dataset is too small to train a CNN model.\nThe standard LSTM has an advantage to model documents due to its simplification. However, it is also difficult to train LSTM since the error signals need to be back-propagated over a long distance\nwith the BPTT algorithm. Our MT-LSTM can alleviate this problem with multiple timescale memories. The experiment on IMDB dataset demonstrates this advantage. MTLSTM achieves the accuracy of 92.1% , which are better than the other models.\nMoreover, MT-LSTM converges at a faster rate than the standard LSTM. Figure 4 plots the convergence on the IMDB dataset. In practice, MTLSTM is approximately three times faster than the standard LSTM since the hidden states of lowspeed group often keep unchanged and need not to be re-calculated at each time step.\nImpact of the Different Number of Memory Groups In our model, the number of memory groups is a hyperparameter. Here we plotted the accuracy curves of our model with the different numbers of memory groups in Figure 5 to show its impacts on the four datasets.\nWhen the length of text (SST-1, SST-2 and QC) is small, not all memory groups can be activated if we set too many groups, which may harm the performance. When dealing with the long texts (IMBD), more groups lead to a better performance. The performance can be improved with the increase of the number of memory groups.\nAccording to our dynamic strategy, the maximum numbers of groups is 3, 3, 2, 7 for the four datasets. The best numbers of groups from experiments are 3, 3, 3, 5 respectively. Therefor, our dynamic strategy is reasonable. All the datasets except QC, the best number of groups is equal to or smaller than our calculated upper bound. MTLSMT suffers underfitting when the number of groups is larger than the upper bound."
  }, {
    "heading": "6.5 Case Study",
    "text": "To get an intuitive understanding of what is happening when we use LSTM or MT-LSTM to predict the class of text, we design an experiment to analyze the output of LSTM and MT-LSTM at each time step.\nWe sample three sentences from the SST-2 test dataset, and the dynamical changes of the predicted sentiment score over time are shown in Figure 6. It is intriguing to notice that our model can handle the rhetorical question well.\nThe first sentence “Is this progress ?” has a negative sentiment. Although the word “progress” is positive, our model can adjust the sentiment correctly after seeing the question mark “?”, and finally gets a correct prediction.\nThe second sentence “He ’d create a movie better than this .” also has a negative sentiment. The word “better” is positive. Our model finally gets a correct negative prediction after seeing “than this”, while LSTM gets a wrong prediction.\nThe third sentence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well."
  }, {
    "heading": "7 Related Work",
    "text": "There are many previous works to model the variable-length text as a fixed-length vector. Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task.\nOur proposed MT-LSTM can handle short texts as well as long texts in classification task."
  }, {
    "heading": "8 Conclusion",
    "text": "In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multiple different timescale memories. Intuitively, MTLSTM easily carries the crucial information over a long distance. Another advantage of MT-LSTM is that the training speed is faster than the standard LSTM (approximately three times faster in practice).\nIn future work, we would like to investigate the other feedback mechanism between the short-term and long-term memories."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200)."
  }],
  "year": 2015,
  "references": [{
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."],
    "venue": "Neural Networks, IEEE Transactions on, 5(2):157–166.",
    "year": 1994
  }, {
    "title": "A neural probabilistic language model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin."],
    "venue": "The Journal of Machine Learning Research, 3:1137–1155.",
    "year": 2003
  }, {
    "title": "Long short-term memory neural networks for chinese word segmentation",
    "authors": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
    "year": 2015
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of EMNLP.",
    "year": 2014
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1412.3555.",
    "year": 2014
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "The Journal of Machine Learning Research, 12:2493–2537.",
    "year": 2011
  }, {
    "title": "Modelling, visualising and summarising documents with a single convolutional neural network",
    "authors": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas."],
    "venue": "arXiv preprint arXiv:1406.3830.",
    "year": 2014
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Hierarchical recurrent neural networks for long-term dependencies",
    "authors": ["Salah El Hihi", "Yoshua Bengio."],
    "venue": "NIPS, pages 493–499. Citeseer.",
    "year": 1995
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L Elman."],
    "venue": "Cognitive science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "Generating sequences with recurrent neural networks",
    "authors": ["Alex Graves."],
    "venue": "arXiv preprint arXiv:1308.0850.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
    "authors": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jürgen Schmidhuber"],
    "year": 2001
  }, {
    "title": "Convolutional neural network architectures for matching natural language sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "Advances in Neural Information Processing Systems.",
    "year": 2014
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of ACL.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882.",
    "year": 2014
  }, {
    "title": "A clockwork rnn",
    "authors": ["Jan Koutnik", "Klaus Greff", "Faustino Gomez", "Juergen Schmidhuber."],
    "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 1863–1871.",
    "year": 2014
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc V. Le", "Tomas Mikolov."],
    "venue": "Proceedings of ICML.",
    "year": 2014
  }, {
    "title": "Learning question classifiers",
    "authors": ["Xin Li", "Dan Roth."],
    "venue": "Proceedings of the 19th International Conference on Computational Linguistics, pages 556– 562.",
    "year": 2002
  }, {
    "title": "Learning word vectors for sentiment analysis",
    "authors": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-",
    "year": 2011
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur."],
    "venue": "INTERSPEECH.",
    "year": 2010
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Composition in distributional models of semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive science, 34(8):1388–1429.",
    "year": 2010
  }, {
    "title": "Recursive distributed representations",
    "authors": ["Jordan B Pollack."],
    "venue": "Artificial Intelligence, 46(1):77–105.",
    "year": 1990
  }, {
    "title": "Parsing natural scenes and natural language with recursive neural networks",
    "authors": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.",
    "year": 2011
  }, {
    "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-",
    "year": 2011
  }, {
    "title": "Semantic compositionality through recursive matrix-vector spaces",
    "authors": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
    "year": 2012
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the conference on",
    "year": 2013
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."],
    "venue": "Advances in Neural Information Processing Systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for",
    "year": 2010
  }, {
    "title": "Baselines and bigrams: Simple, good sentiment and topic classification",
    "authors": ["Sida Wang", "Christopher D Manning."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. As-",
    "year": 2012
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["Paul J Werbos."],
    "venue": "Proceedings of the IEEE, 78(10):1550–1560.",
    "year": 1990
  }, {
    "title": "Spoken language understanding using long short-term memory neural networks",
    "authors": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."],
    "venue": "IEEE SLT.",
    "year": 2014
  }, {
    "title": "Self-adaptive hierarchical sentence model",
    "authors": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."],
    "venue": "arXiv preprint arXiv:1504.05070.",
    "year": 2015
  }],
  "id": "SP:f112b8e6ec85cb741475337262132ed4eb14ddfb",
  "authors": [{
    "name": "Pengfei Liu",
    "affiliations": []
  }, {
    "name": "Xipeng Qiu",
    "affiliations": []
  }, {
    "name": "Xinchi Chen",
    "affiliations": []
  }, {
    "name": "Shiyu Wu",
    "affiliations": []
  }, {
    "name": "Xuanjing Huang",
    "affiliations": []
  }],
  "abstractText": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, it is still a challenge task to model long texts, such as sentences and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task.",
  "title": "Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents"
}