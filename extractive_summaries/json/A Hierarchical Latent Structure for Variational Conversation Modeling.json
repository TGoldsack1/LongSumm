{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 1792–1801 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Conversation modeling has been a long interest of natural language research. Recent approaches for data-driven conversation modeling mostly build upon recurrent neural networks (RNNs) (Vinyals and Le, 2015; Sordoni et al., 2015b; Shang et al., 2015; Li et al., 2017; Serban et al., 2016). Serban et al. (2016) use a hierarchical RNN structure to model the context of conversation. Serban et al. (2017) further exploit an utterance latent\nvariable in the hierarchical RNNs by incorporating the variational autoencoder (VAE) framework (Kingma and Welling, 2014; Rezende et al., 2014).\nVAEs enable us to train a latent variable model for natural language modeling, which grants us several advantages. First, latent variables can learn an interpretable holistic representation, such as topics, tones, or high-level syntactic properties. Second, latent variables can model inherently abundant variability of natural language by encoding its global and long-term structure, which is hard to be captured by shallow generative processes (e.g. vanilla RNNs) where the only source of stochasticity comes from the sampling of output words.\nIn spite of such appealing properties of latent variable models for natural language modeling, VAEs suffer from the notorious degeneration problem (Bowman et al., 2016; Chen et al., 2017) that occurs when a VAE is combined with a powerful decoder such as autoregressive RNNs. This issue makes VAEs ignore latent variables, and eventually behave as vanilla RNNs. Chen et al. (2017) also note this degeneration issue by showing that a VAE with a RNN decoder prefers to model the data using its decoding distribution rather than using latent variables, from bits-back coding perspective. To resolve this issue, several heuristics have been proposed to weaken the decoder, enforcing the models to use latent variables. For example, Bowman et al. (2016) propose some heuristics, including KL annealing and word drop regularization. However, these heuristics cannot be a complete solution; for example, we observe that they fail to prevent the degeneracy in VHRED (Serban et al., 2017), a conditional VAE model equipped with hierarchical RNNs for conversation modeling.\nThe objective of this work is to propose a novel VAE model that significantly alleviates the degen-\n1792\neration problem. Our analysis reveals that the causes of the degeneracy are two-fold. First, the hierarchical structure of autoregressive RNNs is powerful enough to predict a sequence of utterances without the need of latent variables, even with the word drop regularization. Second, we newly discover that the conditional VAE structure where an utterance is generated conditioned on context, i.e. a previous sequence of utterances, induces severe data sparsity. Even with a large-scale training corpus, there only exist very few target utterances when conditioned on the context. Hence, the hierarchical RNNs can easily memorize the context-to-utterance relations without relying on latent variables.\nWe propose a novel model named Variational Hierarchical Conversation RNN (VHCR), which involves two novel features to alleviate this problem. First, we introduce a global conversational latent variable along with local utterance latent variables to build a hierarchical latent structure. Second, we propose a new regularization technique called utterance drop. We show that our hierarchical latent structure is not only crucial for facilitating the use of latent variables in conversation modeling, but also delivers several additional advantages, including gaining control over the global context in which the conversation takes place.\nOur major contributions are as follows: (1) We reveal that the existing conditional VAE model with hierarchical RNNs for conversation modeling (e.g. (Serban et al., 2017)) still suffers from the degeneration problem, and this problem is caused by data sparsity per context that arises from the conditional VAE structure, as well as the use of powerful hierarchical RNN decoders.\n(2) We propose a novel variational hierarchical conversation RNN (VHCR), which has two distinctive features: a hierarchical latent structure and a new regularization of utterance drop. To the best of our knowledge, our VHCR is the first VAE conversation model that exploits the hierarchical latent structure.\n(3) With evaluations on two benchmark datasets of Cornell Movie Dialog (Danescu-NiculescuMizil and Lee, 2011) and Ubuntu Dialog Corpus (Lowe et al., 2015), we show that our model improves the conversation performance in multiple metrics over state-of-the-art methods, including HRED (Serban et al., 2016), and VHRED (Serban et al., 2017) with existing degeneracy solu-\ntions such as the word drop (Bowman et al., 2016), and the bag-of-words loss (Zhao et al., 2017)."
  }, {
    "heading": "2 Related Work",
    "text": "Conversation Modeling. One popular approach for conversation modeling is to use RNN-based encoders and decoders, such as (Vinyals and Le, 2015; Sordoni et al., 2015b; Shang et al., 2015). Hierarchical recurrent encoder-decoder (HRED) models (Sordoni et al., 2015a; Serban et al., 2016, 2017) consist of utterance encoder and decoder, and a context RNN which runs over utterance representations to model long-term temporal structure of conversation.\nRecently, latent variable models such as VAEs have been adopted in language modeling (Bowman et al., 2016; Zhang et al., 2016; Serban et al., 2017). The VHRED model (Serban et al., 2017) integrates the VAE with the HRED to model Twitter and Ubuntu IRC conversations by introducing an utterance latent variable. This makes a conditional VAE where the generation process is conditioned on the context of conversation. Zhao et al. (2017) further make use of discourse act labels to capture the diversity of conversations.\nDegeneracy of Variational Autoencoders. For sequence modeling, VAEs are often merged with the RNN encoder-decoder structure (Bowman et al., 2016; Serban et al., 2017; Zhao et al., 2017) where the encoder predicts the posterior distribution of a latent variable z, and the decoder models the output distributions conditioned on z. However, Bowman et al. (2016) report that a VAE with a RNN decoder easily degenerates; that is, it learns to ignore the latent variable z and falls back to a vanilla RNN. They propose two techniques to alleviate this issue: KL annealing and word drop. Chen et al. (2017) interpret this degeneracy in the context of bits-back coding and show that a VAE equipped with autoregressive models such as RNNs often ignores the latent variable to minimize the code length needed for describing data. They propose to constrain the decoder to selectively encode the information of interest in the latent variable. However, their empirical results are limited to an image domain. Zhao et al. (2017) use an auxiliary bag-of-words loss on the latent variable to force the model to use z. That is, they train an auxiliary network that predicts bag-of-words representation of the target utterance based on z. Yet this loss works in an opposite di-\nrection to the original objective of VAEs that minimizes the minimum description length. Thus, it may be in danger of forcibly moving the information that is better modeled in the decoder to the latent variable."
  }, {
    "heading": "3 Approach",
    "text": "We assume that the training set consists of N i.i.d samples of conversations {c1, c2, ..., cN} where each ci is a sequence of utterances (i.e. sentences) {xi1,xi2, ...,xini}. Our objective is to learn the parameters of a generative network θ using Maximum Likelihood Estimation (MLE):\nargmax θ\n∑\ni\nlog pθ(ci) (1)\nWe first briefly review the VAE, and explain the degeneracy issue before presenting our model."
  }, {
    "heading": "3.1 Preliminary: Variational Autoencoder",
    "text": "We follow the notion of Kingma and Welling (2014). A datapoint x is generated from a latent variable z, which is sampled from some prior distribution p(z), typically a standard Gaussian distribution N (z|0, I). We assume parametric families for conditional distribution pθ(x|z). Since it is intractable to compute the log-marginal likelihood log pθ(x), we approximate the intractable true posterior pθ(z|x) with a recognition model qφ(z|x) to maximize the variational lower-bound:\nlog pθ(x) ≥ L(θ,φ;x) (2) = Eqφ(z|x)[− log qφ(z|x) + log pθ(x, z)] = −DKL(qφ(z|x)‖p(z))+Eqφ(z|x)[log pθ(x|z)]\nEq. 2 is decomposed into two terms: KL divergence term and reconstruction term. Here, KL divergence measures the amount of information encoded in the latent variable z. In the extreme where KL divergence is zero, the model completely ignores z, i.e. it degenerates. The expectation term can be stochastically approximated by sampling z from the variational posterior qφ(z|x). The gradients to the recognition model can be efficiently estimated using the reparameterization trick (Kingma and Welling, 2014)."
  }, {
    "heading": "3.2 VHRED",
    "text": "Serban et al. (2017) propose Variational Hierarchical Recurrent Encoder Decoder (VHRED) model\nfor conversation modeling. It integrates an utterance latent variable zuttt into the HRED structure (Sordoni et al., 2015a) which consists of three RNN components: encoder RNN, context RNN, and decoder RNN. Given a previous sequence of utterances x1, ...xt−1 in a conversation, the VHRED generates the next utterance xt as:\nhenct−1 = f enc θ (xt−1) (3)\nhcxtt = f cxt θ (h cxt t−1,h enc t−1) (4)\npθ(z utt t |x<t) = N (z|µt,σtI) (5)\nwhere µt = MLPθ(h cxt t ) (6)\nσt = Softplus(MLPθ(h cxt t )) (7)\npθ(xt|x<t) = fdecθ (x|hcxtt , zuttt ) (8)\nAt time step t, the encoder RNN f encθ takes the previous utterance xt−1 and produces an encoder vector henct−1 (Eq. 3). The context RNN f cxt θ models the context of the conversation by updating its hidden states using the encoder vector (Eq. 4). The context hcxtt defines the conditional prior pθ(z utt t |x<t), which is a factorized Gaussian distribution whose mean µt and diagonal variance σt are given by feed-forward neural networks (Eq. 5-7). Finally the decoder RNN fdecθ generates the utterance xt, conditioned on the context vector hcxtt and the latent variable zuttt (Eq. 8). We make two important notes: (1) the context RNN can be viewed as a high-level decoder, and together with the decoder RNN, they comprise a hierarchical RNN decoder. (2) VHRED follows a conditional VAE structure where each utterance xt is generated conditioned on the context hcxtt (Eq. 5-8).\nThe variational posterior is a factorized Gaussian distribution where the mean and the diagonal variance are predicted from the target utterance and the context as follows:\nqφ(z utt t |x≤t) = N (z|µ′t,σ′tI) (9)\nwhere µ′t = MLPφ(xt,h cxt t ) (10)\nσ′t = Softplus(MLPφ(xt,h cxt t )) (11)"
  }, {
    "heading": "3.3 The Degeneration Problem",
    "text": "A known problem of a VAE that incorporates an autoregressive RNN decoder is the degeneracy that ignores the latent variable z. In other words, the KL divergence term in Eq. 2 goes to zero and the decoder fails to learn any dependency between the latent variable and the data. Eventually, the model behaves as a vanilla RNN. This problem is\nfirst reported in the sentence VAE (Bowman et al., 2016), in which following two heuristics are proposed to alleviate the problem by weakening the decoder.\nFirst, the KL annealing scales the KL divergence term of Eq. 2 using a KL multiplier λ, which gradually increases from 0 to 1 during training:\nL̃(θ,φ;x) = −λDKL(qφ(z|x)‖p(z)) (12) +Eqφ(z|x)[log pθ(x|z)]\nThis helps the optimization process to avoid local optima of zero KL divergence in early training. Second, the word drop regularization randomly replaces some conditioned-on word tokens in the RNN decoder with the generic unknown word token (UNK) during training. Normally, the RNN decoder predicts each next word in an autoregressive manner, conditioned on the previous sequence of ground truth (GT) words. By randomly replacing a GT word with an UNK token, the word drop regularization weakens the autoregressive power of the decoder and forces it to rely on the latent variable to predict the next word. The word drop probability is normally set to 0.25, since using a higher probability may degrade the model performance (Bowman et al., 2016).\nHowever, we observe that these tricks do not solve the degeneracy for the VHRED in conversation modeling. An example in Fig. 1 shows that the VHRED learns to ignore the utterance latent variable as the KL divergence term falls to zero."
  }, {
    "heading": "3.4 Empirical Observation on Degeneracy",
    "text": "The decoder RNN of the VHRED in Eq. 8 conditions on two information sources: deterministic hcxtt and stochastic z\nutt. In order to check whether the presence of deterministic source hcxtt causes the degeneration, we drop the deterministic hcxtt and condition the decoder only on the stochastic utterance latent variable zutt:\npθ(xt|x<t) = fdecθ (x|zuttt ) (13)\nWhile this model achieves higher values of KL divergence than original VHRED, as training proceeds it again degenerates with the KL divergence term reaching zero (Fig. 2).\nTo gain an insight of the degeneracy, we examine how the conditional prior pθ(zuttt |x<t) (Eq. 5) of the utterance latent variable changes during training, using the model above (Eq. 13). Fig. 2 plots the ratios of E[σ2t ]/Var(µt), where E[σ2t ] indicates the within variance of the priors, and Var(µt) is the between variance of the priors. Note that traditionally this ratio is closely related to Analysis of Variance (ANOVA) (Lomax and Hahs-Vaughn, 2013). The ratio gradually falls to zero, implying that the priors degenerate to separate point masses as training proceeds. Moreover, we find that the degeneracy of priors coincide with the degeneracy of KL divergence, as shown in (Fig. 2). This is intuitively natural: if the prior is already narrow enough to specify the target utterance, there is little pressure to encode any more information in the variational posterior for reconstruction of the target utterance.\nThis empirical observation implies that the fundamental reason behind the degeneration may originate from combination of two factors: (1) strong expressive power of the hierarchical RNN decoder and (2) training data sparsity caused by the conditional VAE structure. The VHRED is trained to predict a next target utterance xt conditioned on the context hcxtt which encodes information about previous utterances {x1, . . . ,xt−1}. However, conditioning on the context makes the range of training target xt very sparse; even in a large-scale conversation corpus such as Ubuntu Dialog (Lowe et al., 2015), there exist one or very few target utterances per context. Therefore, hierarchical RNNs, given their autoregressive power, can easily overfit to training data without using the latent variable. Consequently, the VHRED will not encode any information in the latent variable, i.e. it degenerates. It explains why the word drop fails to prevent the degeneracy in the VHRED. The word drop only regularizes the decoder RNN; however, the context RNN is also powerful enough to predict a next utterance in a given context even with the weakened decoder RNN. Indeed we observe that using a larger word drop probability such as 0.5 or 0.75 only slows down, but fails to stop the KL divergence from vanishing."
  }, {
    "heading": "3.5 Variational Hierarchical Conversation RNN (VHCR)",
    "text": "As discussed, we argue that the two main causes of degeneration are i) the expressiveness of the hierarchical RNN decoders, and ii) the conditional VAE structure that induces data sparsity. This finding hints us that in order to train a nondegenerate latent variable model, we need to design a model that provides an appropriate way to\nregularize the hierarchical RNN decoders and alleviate data sparsity per context. At the same time, the model should be capable of modeling complex structure of conversation. Based on these insights, we propose a novel VAE structure named Variational Hierarchical Conversation RNN (VHCR), whose graphical model is illustrated in Fig. 3. Below we first describe the model, and discuss its unique features.\nWe introduce a global conversation latent variable zconv which is responsible for generating a sequence of utterances of a conversation c = {x1, . . . ,xn}:\npθ(c|zconv) = pθ(x1, . . . ,xn|zconv) (14)\nOverall, the VHCR builds upon the hierarchical RNNs, following the VHRED (Serban et al., 2017). One key update is to form a hierarchical latent structure, by using the global latent variable zconv per conversation, along with local the latent variable zuttt injected at each utterance (Fig. 3):\nhenct = f enc θ (xt) (15)\nhcxtt = { MLPθ(zconv), if t = 0 f cxtθ (h cxt t−1,h enc t−1, z conv), otherwise pθ(xt|x<t, zuttt , zconv) = fdecθ (x|hcxtt , zuttt , zconv) pθ(z\nconv) = N (z|0, I) (16) pθ(z utt t |x<t, zconv) = N (z|µt,σtI) (17)\nwhere µt = MLPθ(h cxt t , z conv) (18)\nσt = Softplus(MLPθ(h cxt t , z conv)). (19)\nFor inference of zconv, we use a bi-directional RNN denoted by f conv, which runs over the utterance vectors generated by the encoder RNN:\nqφ(z conv|x1, ...,xn) = N (z|µconv,σconvI) (20)\nwhere hconv = f conv(henc1 , ...,h enc n ) (21)\nµconv = MLPφ(hconv) (22) σconv = Softplus(MLPφ(hconv)). (23)\nThe posteriors for local variables zuttt are then conditioned on zconv:\nqφ(z utt t |x1, ...,xn, zconv) = N (z|µ′t,σ′tI) (24)\nwhere µ′t = MLPφ(xt,h cxt t , z conv) (25)\nσ′t = Softplus(MLPφ(xt,h cxt t , z conv)).\nOur solution of VHCR to the degeneration problem is based on two ideas. The first idea is to build a hierarchical latent structure of zconv for\na conversation and zuttt for each utterance. As z conv is independent of the conditional structure, it does not suffer from the data sparsity problem. However, the expressive power of hierarchical RNN decoders makes the model still prone to ignore latent variables zconv and zuttt . Therefore, our second idea is to apply an utterance drop regularization to effectively regularize the hierarchical RNNs, in order to facilitate the use of latent variables. That is, at each time step, the utterance encoder vector henct is randomly replaced with a generic unknown vector hunk with a probability p. This regularization weakens the autoregressive power of hierarchical RNNs and as well alleviates the data sparsity problem, since it induces noise into the context vector hcxtt which conditions the decoder RNN. The difference with the word drop (Bowman et al., 2016) is that our utterance drop depresses the hierarchical RNN decoders as a whole, while the word drop only weakens the lower-level decoder RNNs. Fig. 4 confirms that with the utterance drop with a probability of 0.25, the VHCR effectively learns to use latent variables, achieving a significant degree of KL divergence."
  }, {
    "heading": "3.6 Effectiveness of Hierarchical Latent Structure",
    "text": "Is the hierarchical latent structure of the VHCR crucial for effective utilization of latent variables? We investigate this question by applying the utterance drop on the VHRED which lacks any hierarchical latent structure. We observe that the KL divergence still vanishes (Fig. 4), even though\nthe utterance drop injects considerable noise in the context hcxtt . We argue that the utterance drop weakens the context RNN, thus it consequently fail to predict a reasonable prior distribution for zutt (Eq. 5-7). If the prior is far away from the region of zutt that can generate a correct target utterance, encoding information about the target in the variational posterior will incur a large KL divergence penalty. If the penalty outweighs the gain of the reconstruction term in Eq. 2, then the model would learn to ignore zutt, in order to maximize the variational lower-bound in Eq. 2.\nOn the other hand, the global variable zconv allows the VHCR to predict a reasonable prior for local variable zuttt even in the presence of the utterance drop regularization. That is, zconv can act as a guide for zutt by encoding the information for local variables. This reduces the KL divergence penalty induced by encoding information in zutt to an affordable degree at the cost of KL divergence caused by using zconv. This trade-off is indeed a fundamental strength of hierarchical models that provide parsimonious representation; if there exists any shared information among the local variables, it is coded in the global latent variable reducing the code length by effectively reusing the information. The remaining local variability is handled properly by the decoding distribution and local latent variables.\nThe global variable zconv provides other benefits by representing a latent global structure of a conversation, such as a topic, a length, and a tone of the conversation. Moreover, it allows us to control such global properties, which is impossible for models without hierarchical latent structure."
  }, {
    "heading": "4 Results",
    "text": "We first describe our experimental setting, such as datasets and baselines (section 4.1). We then report quantitative comparisons using three different metrics (section 4.2–4.4). Finally, we present qualitative analyses, including several utterance control tasks that are enabled by the hierarchal latent structure of our VHCR (section 4.5). We defer implementation details and additional experiment results to the supplementary file."
  }, {
    "heading": "4.1 Experimental Setting",
    "text": "Datasets. We evaluate the performance of conversation generation using two benchmark datasets: 1) Cornell Movie Dialog Corpus (Danescu-\nNiculescu-Mizil and Lee, 2011), containing 220,579 conversations from 617 movies. 2) Ubuntu Dialog Corpus (Lowe et al., 2015), containing about 1 million multi-turn conversations from Ubuntu IRC channels. In both datasets, we truncate utterances longer than 30 words.\nBaselines. We compare our approach with four baselines. They are combinations of two state-ofthe-art models of conversation generation with different solutions to the degeneracy. (i) Hierarchical recurrent encoder-decoder (HRED) (Serban et al., 2016), (ii) Variational HRED (VHRED) (Serban et al., 2017), (iii) VHRED with the word drop (Bowman et al., 2016), and (iv) VHRED with the bag-of-words (bow) loss (Zhao et al., 2017).\nPerformance Measures. Automatic evaluation of conversational systems is still a challenging problem (Liu et al., 2016). Based on literature, we report three quantitative metrics: i) the negative log-likelihood (the variational bound for variational models), ii) embedding-based metrics (Serban et al., 2017), and iii) human evaluation via Amazon Mechanical Turk (AMT)."
  }, {
    "heading": "4.2 Results of Negative Log-likelihood",
    "text": "Table 1 summarizes the per-word negative loglikelihood (NLL) evaluated on the test sets of two datasets. For variational models, we instead present the variational bound of the negative loglikelihood in Eq. 2, which consists of the reconstruction error term and the KL divergence term. The KL divergence term can measure how much each model utilizes the latent variables.\nWe observe that the NLL is the lowest by the HRED. Variational models show higher NLLs, because they are regularized methods that are forced to rely more on latent variables. Independent of NLL values, we later show that the latent variable models often show better generalization performance in terms of embedding-based metrics and human evaluation. In the VHRED, the KL divergence term gradually vanishes even with the word drop regularization; thus, early stopping is necessary to obtain a meaningful KL divergence. The VHRED with the bag-of-words loss (bow) achieves the highest KL divergence, however, at the cost of high NLL values. That is, the variational lower-bound minimizes the minimum description length, to which the bow loss works in an opposite direction by forcing latent variables to encode bag-of-words representation of utterances. Our VHCR achieves stable KL divergence without any auxiliary objective, and the NLL is lower than the VHRED + bow model.\nTable 2 summarizes how global and latent variable are used in the VHCR. We observe that VHCR encodes a significant amount of information in the global variable zconv as well as in the local variable zutt, indicating that the VHCR successfully exploits its hierarchical latent structure."
  }, {
    "heading": "4.3 Results of Embedding-Based Metrics",
    "text": "The embedding-based metrics (Serban et al., 2017; Rus and Lintean, 2012) measure the textual similarity between the words in the model response and the ground truth. We represent words using Word2Vec embeddings trained on the Google News Corpus1. The average metric projects each utterance to a vector by taking the mean over word embeddings in the utterance, and computes the cosine similarity between the model response vector and the ground truth vector. The extrema metric is similar to the average metric, only except that it takes the extremum of each di-\n1https://code.google.com/archive/p/word2vec/.\nmension, instead of the mean. The greedy metric first finds the best non-exclusive word alignment between the model response and the ground truth, and then computes the mean over the cosine similarity between the aligned words.\nTable 3 compares the different methods with three embedding-based metrics. Each model generates a single response (1-turn) or consecutive three responses (3-turn) for a given context. For 3-turn cases, we report the average of metrics measured for three turns. We use the greedy decoding for all the models.\nOur VHCR achieves the best results in most metrics. The HRED is the worst on the Cornell Movie dataset, but outperforms the VHRED and VHRED + bow on the Ubuntu Dialog dataset. Although the VHRED + bow shows the highest KL divergence, its performance is similar to that of VHRED, and worse than that of the VHCR model. It suggests that a higher KL divergence does not necessarily lead to better performance; it is more important for the models to balance the modeling powers of the decoder and the latent variables. The VHCR uses a more sophisticated hierarchical latent structure, which better reflects the structure of\nnatural language conversations."
  }, {
    "heading": "4.4 Results of Human Evaluation",
    "text": "Table 4 reports human evaluation results via Amazon Mechanical Turk (AMT). The VHCR outperforms the baselines in both datasets; yet the performance improvement in Cornell Movie Dialog are less significant compared to that of Ubuntu. We empirically find that Cornell Movie dataset is small in size, but very diverse and complex in content and style, and the models often fail to generate sensible responses for the context. The performance gap with the HRED is the smallest, suggesting that the VAE models without hierarchical latent structure have overfitted to Cornell Movie dataset."
  }, {
    "heading": "4.5 Qualitative Analyses",
    "text": "Comparison of Predicted Responses. Table 5 compares the generated responses of algorithms. Overall, the VHCR creates more consistent responses within the context of a given conversation. This is supposedly due to the global latent variable zconv that provides a more direct and effective way to handle the global context of a conversation. The context RNN of the baseline models can handle long-term context to some extent, but not as much as the VHCR.\nInterpolation on zconv. We present examples of one advantage by the hierarchical latent structure of the VHCR, which cannot be done by the other existing models. Table 6 shows how the generated responses vary according to the interpolation on zconv. We randomly sample two zconv from a standard Gaussian prior as references (i.e. the top and the bottom row of Table 6), and interpolate points between them. We generate 3-turn conversations conditioned on given zconv. We see that zconv controls the overall tone and content of conversations; for example, the tone of the response is friendly in the first sample, but gradually becomes hostile as zconv changes.\nGeneration on a Fixed zconv. We also study how fixing a global conversation latent variable zconv affects the conversation generation. Table 7 shows an example, where we randomly fix a reference zconv from the prior, and generate multiple examples of 3-turn conversation using randomly sampled local variables zutt. We observe that zconv heavily affects the form of the first utterance; in the examples, the first utterances all start with a “where” phrase. At the same time, responses show\nvariations according to different local variables zutt. These examples show that the hierarchical latent structure of VHCR allows both global and fine-grained control over generated conversations."
  }, {
    "heading": "5 Discussion",
    "text": "We introduced the variational hierarchical conversation RNN (VHCR) for conversation modeling. We noted that the degeneration problem in existing VAE models such as the VHRED is persistent, and proposed a hierarchical latent variable model with the utterance drop regularization. Our VHCR obtained higher and more stable KL divergences than various versions of VHRED models without using any auxiliary objective. The empir-\nical results showed that the VHCR better reflected the structure of natural conversations, and outperformed previous models. Moreover, the hierarchical latent structure allowed both global and finegrained control over the conversation generation."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by Kakao and Kakao Brain corporations, and Creative-Pioneering Researchers Program through Seoul National University. Gunhee Kim is the corresponding author."
  }],
  "year": 2018,
  "references": [{
    "title": "Generating sentences from a continuous space",
    "authors": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."],
    "venue": "CoNLL. https://doi.org/10. 18653/v1/K16-1002.",
    "year": 2016
  }, {
    "title": "Variational lossy autoencoder",
    "authors": ["Xi Chen", "Diederik P Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs",
    "authors": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee."],
    "venue": "CMCL Workshop.",
    "year": 2011
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "ICLR.",
    "year": 2014
  }, {
    "title": "Adversarial learning for neural dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1701.06547 .",
    "year": 2017
  }, {
    "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
    "authors": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Statistical concepts: A second course",
    "authors": ["Richard G Lomax", "Debbie L Hahs-Vaughn."],
    "venue": "Routledge.",
    "year": 2013
  }, {
    "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."],
    "venue": "SIGDIAL.",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."],
    "venue": "ICML.",
    "year": 2014
  }, {
    "title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics",
    "authors": ["Vasile Rus", "Mihai Lintean."],
    "venue": "Building Educational Applications Using NLP Workshop. ACL. http://www.aclweb.org/",
    "year": 2012
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C Courville", "Joelle Pineau."],
    "venue": "AAAI.",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "AAAI.",
    "year": 2017
  }, {
    "title": "Neural responding machine for short-text conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "ACL. https://doi.org/10.3115/ v1/P15-1152.",
    "year": 2015
  }, {
    "title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
    "authors": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."],
    "venue": "CIKM.",
    "year": 2015
  }, {
    "title": "A neural network approach to contextsensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "ICML Deep Learning Workshop.",
    "year": 2015
  }, {
    "title": "Variational neural machine translation",
    "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan", "Min Zhang."],
    "venue": "EMNLP. https://doi.org/10. 18653/v1/D16-1050.",
    "year": 2016
  }, {
    "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
    "authors": ["Tiancheng Zhao", "Ran Zhao", "Maxine Eskenazi."],
    "venue": "ACL. https://doi.org/10. 18653/v1/P17-1061.",
    "year": 2017
  }],
  "id": "SP:5b62ddc50cd337f73c96d4715b8371f2450a02aa",
  "authors": [{
    "name": "Yookoon Park",
    "affiliations": []
  }, {
    "name": "Jaemin Cho",
    "affiliations": []
  }, {
    "name": "Gunhee Kim",
    "affiliations": []
  }],
  "abstractText": "Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling. However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly due to two reasons. First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables. Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization. With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation. Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.",
  "title": "A Hierarchical Latent Structure for Variational Conversation Modeling"
}