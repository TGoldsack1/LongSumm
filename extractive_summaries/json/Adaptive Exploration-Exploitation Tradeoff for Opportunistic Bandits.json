{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In existing studies of multi-armed bandits (MABs) (Auer et al., 2002; Bubeck & Cesa-Bianchi, 2012), pulling a suboptimal arm results in a constant regret. While this is a valid assumption in many existing applications, there exists a variety of applications where the actual regret of pulling a suboptimal arm may vary depending on external conditions. Consider the following application scenarios.\n1Twitter Inc., San Francisco, California, USA; 2University of California, Davis, California, USA. This work was partially completed while the first author was a postdoctoral researcher at University of California, Davis. Correspondence to: Xin Liu <xinliu@ucdavis.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nMotivating scenario 1: price variation. MAB has been widely used in studying effective procedures and treatments (Lai, 1987; Press, 2009; Villar et al., 2015), including in agriculture. In agriculture, price often varies significantly for produce and livestock. For example, the pork price varied from $0.46/lb to $1.28/lb, and orange $608/ton to $1140/ton, in 2014-2017 (Index Mundi). Commodity price forecast has achieved high accuracy and been widely used for production decisions (Brandt & Bessler, 1983). In this scenario, different treatments can be considered as arms. The effectiveness of a particular treatment is captured by the value of the arm, and is independent of the market price of the product. (The latter is true because an experiment in one farm, among tens of thousands of such farms in the US, has negligible impact on the overall production and thus the commodity’s market price.) The monetary reward is proportional to price and to the effectiveness of the treatment. The goal of a producer is to minimize the overall monetary regret, compared to the oracle. Therefore, intuitively, when the product price is low, the monetary regret of pulling a suboptimal arm is low, and vice versa.\nMotivating scenario 2: load variation. Network configuration is widely used in wireless networks, data-center networks, and the Internet, in order to control network topology, routing, load balancing, and thus improve the overall performance. For example, in a cellular network, a cell tower has a number of parameters to configure, including radio spectrum, transmission power, antenna angle and direction, etc. The configuration of such parameters can greatly impact the overall performance, e.g., coverage, throughput, and service quality. A network configuration can be considered as an arm, where its performance needs to be learned. Networks are typically designed and configured to handle the peak load, and thus we hope to learn the best configuration for the peak load.\nNetwork traffic load fluctuates over time. When the network load is low, we can inject dummy traffic into the network so that the total load, the real load plus the dummy load, resembles the peak load. It allows us to learn the performance of the configuration under the peak load. At the same time, the regret of using a suboptimal configuration is low because the real load affected is low. Furthermore, in practice, we can set the priority of the dummy traffic to be lower than that of the real traffic. Because networks handle high priority\ntraffic first, low priority traffic results in little or no impact on the high priority traffic (Walraevens et al., 2003). In this case, the regret on the actual load is further reduced, or even negligible (when the suboptimal configuration is sufficient to handle the real load).\nOpportunistic bandits. Motivated by these application scenarios, we study opportunistic bandits in this paper. Specifically, we define opportunistic bandit as a bandit problem with the following characteristics: 1) The best arm does not change over time. 2) The exploration cost (regret) of a suboptimal arm varies depending on a time-varying external condition that we refer to as load (which is the price in the first scenario). 3) The load is revealed before an arm is pulled, so that one can decide which arm to pull depending on the load. As its name suggests, in opportunistic bandits, one can leverage the opportunities of load variation to achieve a lower regret. In addition to the previous two examples, opportunistic bandit algorithms can be applied to other scenarios that share the above characteristics.\nWe note that opportunistic bandits significantly differs from non-stationary bandits (Garivier & Moulines, 2011; Besbes et al., 2014). In non-stationary bandits, the expected reward of each arm varies and the optimal arm may change over time, e.g., because of the shift of interests. In opportunistic bandits, the optimal arm does not change over time, but the regret of trying a suboptimal arm changes depending on the load. In other words, in non-stationary bandits, the dynamics of the optimal arm make finding the optimal arm more challenging. In contrast, in opportunistic bandits, the time-varying nature of the load provides opportunities to reduce the regret of finding the fixed optimal arm. Because of such fundamental differences, in non-stationary bandits, one can show polynomial regret (e.g., Ω(T 2/3) (Besbes et al., 2014)) because one has to keep track of the optimal arm. In opportunistic bandits, we can show O(log T ) (or even O(1) in certain special cases) regret because we can push more exploration to slots when the regret is lower.\nWe also note the connection and difference between opportunistic bandits and contextual bandits (Zhou, 2015; Wu et al., 2015; Li et al., 2010; Chu et al., 2011). Broadly speaking, opportunistic bandits can be considered as a special case of contextual bandits where we can consider the load as the context. However, general contextual bandits do not take advantages of the unique properties of opportunistic bandits, in particular, the optimal bandit remains the same, and regrets differ under different contexts (i.e., load). To follow this line, the performance of contextual bandits has been compared in Appendix D.3.\nContributions. In this paper, we propose an Adaptive Upper-Confidence-Bound (AdaUCB) algorithm to dynamically balance the exploration-exploitation tradeoff in opportunistic bandits. The intuition is clear: we should explore\nmore when the load is low and exploit more when the load is high. The design challenge is to quantify the right amount of exploration and exploitation depending on the load. The analysis challenge is due to the inherent coupling over time and thus over bandits under different conditions. In particular, due to the randomness nature of bandits, the empirical estimates of the expected rewards could deviate from the true values, which could lead to suboptimal actions when the load is high. We address these challenges by studying the lower bounds on the number of pulls of the suboptimal arms under low load. Because the exploration factor is smaller under high load than that under low load, it requires less information accuracy to make the optimal decision under high load. Thus, with an appropriate lower bound on the number of pulls of the suboptimal arms under low load, we can show that the information obtained from the exploration under the low load is sufficient for accurate decisions under the high load. As a result, the exploration under high load is reduced and thus so does the overall regret.\nTo the best of our knowledge, this is the first work proposing and studying opportunistic bandits that aims to adaptively balance the exploration-exploitation tradeoff considering load-dependent regrets. We propose AdaUCB, an algorithm that adjusts the exploration-exploitation tradeoff according to the load level. We prove that AdaUCB achieves O(log T ) regret with a smaller coefficient than the traditional UCB algorithm. Furthermore, AdaUCB achieves O(1) regret with respect to T in the case where the exploration cost is zero when the load level is smaller than a certain threshold. Using both synthetic and real-world traces, we show that AdaUCB significantly outperforms other bandit algorithms, such as UCB and TS (Thompson Sampling), under large load fluctuations."
  }, {
    "heading": "2. System Model",
    "text": "We study an opportunistic bandit problem, where the exploration cost varies over time depending on an external condition, called load here. Specifically, consider a Karmed stochastic bandit system. At time t, each arm has a random nominal reward Xk,t, where Xk,t ∈ [0, 1] are independent across arms, and i.i.d. over time, with mean value E[Xk,t] = uk. Let u∗ = maxk uk be the maximum expected reward and k∗ = arg maxuk be the best arm. The arm with the best nominal reward does not depend on the load and does not change over time.\nLet Lt ≥ 0 be the load at time t. For simplicity, we assume Lt ∈ [0, 1]. The agent observes the value of Lt before making the decision; i.e., the agent pulls an arm at based on both Lt and the historical observations, i.e., at = Γ(Lt,Ht−1), where Ht−1 = (L1, a1, Xa1,1, . . . , Lt−1, at−1, Xat−1,t−1) represents the historical observations. The agent then receives an actual reward LtXat,t. While the underlying\nnominal reward Xat,t is independent of Lt conditioned on at, the actual reward depends on Lt. We also assume that the agent can observe the value of Xat,t after pulling arm at at time t.\nThis model captures the essence of opportunistic bandits and its assumptions are reasonable. For example, in the agriculture scenario, Xat,t captures the effectiveness of a treatment, e.g., the survival rate or the yield of an antibiotic treatment. The value of Xat,t can always be observed by the agent after applying treatment at at time t. Conditioned on at, Xat,t is also independent of Lt, the price of the commodity. Meanwhile, the actual reward, i.e., the monetary reward, is modulated by Lt (the price) as LtXat,t. In the network configuration example, Xat,t captures the impact of a configuration at the peak load, e.g., success rate, throughput, or service quality score. Because the total load (the real load plus the dummy load) resembles the peak load, Xat,t is independent of the real load Lt conditioned on at, and can always be observed. Further, because the real load is a portion of the total load and the network can identify real traffic from dummy traffic, the actual reward is thus a portion of the total reward, modulated by the real load as LtXat,t.\nIf system statistics are known a priori, then the agent will always pull the best arm and obtain the expected total reward u∗E[ ∑T t=1 Lt]. Thus, the regret of a policy Γ is defined as\nRΓ(T ) = u ∗E [ T∑ t=1 Lt ] − T∑ t=1 E[LtXat,t]. (1)\nIn particular, when Lt is i.i.d. over time with mean value E[Lt] = L̄, the total expected reward for the oracle solution is u∗L̄T and the regret is RΓ(T ) = u∗L̄T −∑T t=1 E[LtXat,t]. Because the action at can depend on Lt, it is likely that E[LtXat,t] 6= L̄E[Xat,t]."
  }, {
    "heading": "3. Adaptive UCB",
    "text": "We first recall a general version of the classic UCB1 (Auer et al., 2002) algorithm, referred to as UCB(α), which always selects the arm with the largest index defined in the following format:\nûk(t) = ūk(t) +\n√ α log t\nCk(t− 1) , 1 ≤ k ≤ K,\nwhere α is a constant, Ck(t − 1) is the number of pulls for arm-k before t, and ūk(t) = 1Ck(t−1) ∑t−1 τ=1 1(aτ = k)Xk,τ . It has been shown that UCB(α) achieves logarithmic regret in stochastic bandits when α > 1/2 (Bubeck, 2010). UCB1 in (Auer et al., 2002) is a special case with α = 2.\nAlgorithm 1 AdaUCB 1: Init: α > 0.5, Ck(t) = 0, ūk(t) = 1. 2: for t = 1 to K do 3: Pull each arm once and update Ck(t) and ūk(t) ac-\ncordingly; 4: end for 5: for t = K + 1 to T do 6: Observe Lt; 7: Calculate UCB: for k = 1, 2, . . . ,K,\nûk(t) = ūk(t) + √ α(1− L̃t) log t Ck(t− 1) , (2)\nwhere L̃t is the normalized load defined in Eq. (4); 8: Pull the arm with the largest ûk(t):\nat = arg max 1≤k≤K ûk(t); (3)\n9: Update ūk(t) and Ck(t); 10: end for\nIn this work, we propose an AdaUCB algorithm for opportunistic bandits. In order to capture different ranges of Lt, we first normalize Lt to be within [0, 1]:\nL̃t = [Lt]\nl(+) l(−) − l(−)\nl(+) − l(−) , (4)\nwhere l(−) and l(+) are the lower and upper thresholds for truncating the load level, and [Lt]l (+)\nl(−) =\nmax{l(−),min(Lt, l(+))}. Load normalization reduces the impact of different load distributions. It also restricts the coefficient of the exploration term in the UCB indices, which avoids under or over explorations. To achieve good performance, the truncation thresholds should be appropriately chosen and can be learned online in practice, as discussed in Sec. 4.3. We note that L̃t is only used in AdaUCB algorithm. The rewards and regrets are based on Lt, not L̃t.\nThe AdaUCB algorithm adjusts the tradeoff between exploration and exploitation based on the load level Lt. Specifically, as shown in Algorithm 1, AdaUCB makes decisions based on the sum of the empirical reward (the exploitation term) ūk(t) and the confidence interval width (the exploration term). The latter term is proportional to √ 1− L̃t. In other words, AdaUCB uses an exploration factor α(1− L̃t) that is linearly decreasing in L̃t. Thus, when the load level is high, the exploration term is relatively small and AdaUCB tends to emphasize exploitation, i.e., choosing the arms that perform well in the past. In contrast, when the load level is low, AdaUCB uses a larger exploration term and gives more opportunities to the arms with less explorations. Intuitively, with this load-awareness, AdaUCB explores more when the\nload is low and leverages the learned statistics to make better decisions when the load is high. Since the actual regret is scaled with the load level, AdaUCB can achieve an overall lower regret. Note that we have experimented a variety of load adaptation functions. The current one achieves superior empirical performance and is amenable to analyze, and thus adopted here."
  }, {
    "heading": "4. Regret Analysis",
    "text": "Although the intuition behind AdaUCB is natural, the rigorous analysis of its regret is challenging. To analyze the decision in each slot, we require the statistics for the number of pulls of each arm. Unlike traditional regret analysis, we care about not only the upper bound, but also the lower bound for calculating the confidence level. However, even for fixed load levels, it is difficult to characterize the total number of pulls for suboptimal arms, i.e., obtaining tight lower and upper bounds for the regret. The gap between the lower and upper bounds makes it more difficult to evaluate the properties of UCB for general random load levels. To make the intuition more clear and analyses more readable, we start with the case of squared periodic wave load and Dirac rewards to illustrate the behavior of AdaUCB in Sec. 4.1. Then, we extend the results to the case with random binary-value load and random rewards in Sec. 4.2, and finally analyze the case with continuous load in Sec. 4.3.\nSpecifically, we first consider the case with binary-valued load, i.e., Lt ∈ { 0, 1 − 1}, where 0, 1 ∈ [0, 0.5). For this case, we let l(−) = 0 and l(+) = 1. Then, L̃t = 0 if Lt = 0, and L̃t = 1− 0− 11− 0 = 1 − 1 1− 0 if Lt = 1 − 1. Therefore, the indices used by AdaUCB are given as follows:\nûk(t) = ūk(t) + √ α log t Ck(t−1) , if Lt = 0, ūk(t) + √\nα 1 log t (1− 0)Ck(t−1) , if Lt = 1− 1.\n(5)\nWe investigate the regret of AdaUCB under the binaryvalued load described above in Sec. 4.1 and Sec. 4.2, and then study its performance under continuous load in Sec. 4.3 with the insights obtained from the binary-valued load case."
  }, {
    "heading": "4.1. AdaUCB under Periodic Square Wave Load and Dirac Rewards",
    "text": "We first study a simple case with periodic square wave load and Dirac rewards. In this scenario, the evolution of the system under AdaUCB is deterministic. The analysis of this deterministic system allows us to better understand AdaUCB and quantify the benefit of load-awareness. In addition, we focus on 2-armed bandits in analysis for easy illustration in this section.\nSpecifically, we assume the load is Lt = 0 if t is even, and 1− 1 if t is odd. Moreover, the rewards are fixed, i.e., Xk,t = uk for all k and t, but unknown a priori. Without loss of generality, we assume arm-1 has higher reward, i.e., 1 ≥ u1 > u2 ≥ 0, and let ∆ = u1 − u2 be the reward difference.\nUnder these settings, we can obtain the bounds for the number of pulls for each arm by borrowing the idea from (Salomon et al., 2011; 2013). The proofs of these results are included in Appendix A, which are similar to (Salomon et al., 2011; 2013), except for the effort of addressing the case of Lt = 1− 1.\nWe first characterize the upper and lower bounds on the total number of pulls for the suboptimal arm.\nLemma 1. In the opportunistic bandit with periodic square wave load and Dirac rewards, the number of pulls for arm-2 under AdaUCB is bounded as follows: 1) Upper bound for any t ≥ 1: C2(t) ≤ αlogt∆2 + 1; 2) Lower bound for any t = 2τ ≥ 2: C2(2τ) ≥ f(τ) = ∫ τ 2\nmin(h′(s), 1)ds − h(2), where h(s) = α log s∆2 ( 1 + √ 2α log s (2s−1)∆2 )−2 .\nNote that C2(2τ) provides the information for making decision in slot 2τ + 1, when Lt = 1 − 1. With the lower bound in Lemma 1, we can show that after a certain time, AdaUCB will always pull the better arm when Lt = 1− 1 with the information provided by C2(2τ). Combining with the upper bound on C2(t), we can obtain the regret bound for AdaUCB:\nTheorem 1. In the opportunistic bandit with periodic square wave load and Dirac rewards, the regret of AdaUCB is bounded as: RAdaUCB(T ) ≤ 0α log T∆ +O(1).\nRemark 1: According to (Salomon et al., 2011), the regret of UCB(α) is lower bounded by α log T∆ for fixed load Lt = 1. Without load-awareness, we can expect that the explorations occur roughly uniformly under different load levels. Thus, the regret of UCB(α) in this opportunistic bandit is roughly α(1+ 0− 1) log T2∆ , and is much larger than the regret of AdaUCB for small 0 and 1. As an extreme case, when 0 = 0, the regret of AdaUCB is O(1), while that of UCB(α) is O(log T ).\nRemark 2: The above analysis provides us insights about the benefit of load-awareness in opportunistic bandits. With load-awareness, AdaUCB forces exploration to the slots with lower load and the information obtained there is sufficient to make good decisions in higher-load slots. Thus, the overall regret of AdaUCB is much smaller than traditional load-agnostic algorithms."
  }, {
    "heading": "4.2. AdaUCB under Random Binary-Valued Load and Random Rewards",
    "text": "We now consider the more general case with random binaryvalued load and random rewards. We assume that load Lt ∈ { 0, 1 − 1} and P{Lt = 0} = ρ ∈ (0, 1). We consider i.i.d random reward Xk,t ∈ [0, 1] and E[Xk,t] = uk, where 1 ≥ u1 > u2 ≥ u3 ≥ ... ≥ uK ≥ 0. Let ∆k = u1 − uk, and ∆∗ = mink>1 ∆k = ∆2 be the minimum gap between the suboptimal arms and the optimal arm.\nCompared with the deterministic case in Sec. 4.1, the analysis under random load and rewards is much more challenging. In particular, due to the reward randomness, the empirical value ūk(t) will deviate from its true value uk. Unlike Dirac reward, this deviation could result in suboptimal decisions even when 0 and 1 are small. Thus, we need to carefully lower bound the number of pulls for each arm so that the deviation is bounded with high probability. We only provide sketches for the proofs here due to the space limit and refer readers to Appendix B for more detailed analyses.\nWe consider a larger α (α > 2 in general, or larger when explicitly stated) for theoretical analysis purpose, similarly to earlier UCB papers such as (Auer et al., 2002). As we will see in the simulations, AdaUCB with α > 1/2 works well under general random load.\nWe first propose a loose but useful bound for the number of pulls for the optimal arm. Let C(0)k (t) be the number of slots where arm-k is pulled when Lt = 0, i.e., C\n(0) k (t) =∑t\nτ=1 1(Lτ = 0, aτ = k). Lemma 2. In the opportunistic bandit with random binaryvalued load and random rewards, for a constant η ∈ (0, ρ), there exists a constant T2, such that under AdaUCB, for all t ≥ T2\nP { C\n(0) 1 (t) < (ρ− η)t 2 } ≤ e−2η 2t + [2(K − 1)]2α−1\n2α− 2 [ (ρ− η)t ]−2α+2 .\nSketch of Proof: The key intuition of proof is that when C\n(0) 1 (t) is too small, the optimal arm will be pulled with high probability. Specifically, let k′ > 1 be the index of arm that has been pulled for the most time among the suboptimal arms before t, and t′ < t be the last slot when k′ is pulled under load Lt = 0 for the last time. If C (0) 1 (t) < (ρ−η)t 2 , then Ck′(t′− 1) ≥ C(0)k′ (t′− 1) = Θ(t) with high probability. Using the fact that log tt → 0 as t→∞, we know there exists a constant T2 such that for t ≥ T2, the confidence width √ α log t′\nCk′ (t ′−1) will be sufficiently small compared with\nthe minimum gap ∆∗ ≤ ∆k. Moreover, the algorithm will pull the best arm when the UCB deviation is sufficiently small. Then, we can bound the probability of the event\nC (0) 1 (t) < (ρ−η)t 2 by bounding the deviation of UCBs.\nNext we bound the total number of pulls of the suboptimal arm as follows. Lemma 3. In the opportunistic bandit with random binaryvalued load and random rewards, under AdaUCB, we have\nE[Ck(T )] ≤ 4α log T\n∆2k +O(1), 1 < k ≤ K. (6)\nSketch of Proof: To prove this lemma, we discuss the slots when the suboptimal arm is pulled under low and high load levels, respectively. When the load is low, i.e., Lt = 0, AdaUCB becomes UCB(α) and thus we can bound the probability of pulling the suboptimal arm similarly to (Auer et al., 2002). When the load is high, i.e., Lt = 1− 1, the index becomes ûk(t) = ūk(t)+ √ α 1 log t\n(1− 0)Ck(t−1) . In this case, with high probability, the index of the optimal arm is lower\nbounded by u1 − ( 1− √\n1 (1− 0) )√ α log t C1(t−1) according to\nLemma 2. With similar adjustment on the UCB index for the suboptimal arm, we can bound the probability of pulling the suboptimal arm under high load. The conclusion of the lemma then follows by combining the above two cases.\nNow we further lower bound the pulls of the suboptimal arm with high probability. Lemma 4. In the opportunistic bandit with random binaryvalued load and random rewards, for a positive number δ ∈ (0, 1), we have for any k > 1,\nP { Ck(t) < α log t\n4(∆k + δ)2 } = O ( t−(2α−3) + t−(2α( 1−δ 2−δ )\n2−2)). Sketch of Proof: Although the analysis is more difficult, the intuition of proving this lemma is similar to that of Lemma 2: if Ck(t) is too small at a certain slot, then we will pull the suboptimal arm instead of the optimal arm with high probability. To be more specific, we focus on the slot t′ when the optimal arm is pulled for the last time before t under load Lt = 0. According to Lemma 2, C1(t) ≥ C(0)1 (t) ≥ (ρ−η)t 2 with high probability, indicating t′ ≥ (ρ− η)t/2 with high probability. Moreover, the index for the optimal arm û1(t′) ≤ u1 + δ with high probability for a sufficiently large t′, because √ log t t → 0 as t → ∞. On the other hand, we can show that for the suboptimal arm, ûk(t\n′) > u1 + δ = uk + (∆k + δ) with high probability when Ck(t′ − 1) < α log t4(∆k+δ)2 . Thus, the probability of pulling the optimal arm at t′ is bounded by a small value, implying the conclusion of the lemma.\nUsing the above lemmas, now we can further refine the upper bound on the regret of AdaUCB and show that AdaUCB achieves smaller regret than traditional UCB.\nTheorem 2. Using AdaUCB in the opportunistic bandit with random binary-valued load and random rewards, if α > 16 and √ 1\n1− 0 < 1 8 , we have\nRAdaUCB(T ) ≤ 4 0α log T ∑ k>1 1 ∆k +O(1). (7)\nSketch of Proof: The key idea of the proof is to find an appropriate δ ∈ (0,∆∗), such that α > 16(1 + δ∆∗ )\n2 and√ 1\n1− 0 < ∆∗ 8(∆∗+δ) . In fact, the existence of this δ is guaranteed under the assumptions α > 16 and √\n1 1− 0 < 1 8 .\nUsing this δ, we can then use Lemma 4 to bound the probability of pulling the suboptimal arm when the load is high. This indicates most explorations occur when the load is low, i.e., Lt = 0. The conclusion of this theorem then follows according to Lemma 3.\nRemark 3: Although there is no tight lower bound for the regret of UCB(α), we know that for traditional (loadoblivious) bandit algorithms, E[Ck(T )] is lower bounded by log TKL(uk,u1) (Lai & Robbins, 1985) for large T , where KL(uk, u1) is the Kullback-Leibler divergence. Without load-awareness, the regret will be roughly lower bounded by (1− 0− 1) log T2 ∑ k>1 ∆k KL(uk,u1)\n. In contrast, with loadawareness, AdaUCB can achieve much lower regret than load-oblivious algorithms, when the load fluctuation is large, i.e., 0 and 1 are small.\nTheorem 2 directly implies the following result.\nCorollary 1. Using AdaUCB in the opportunistic bandit with random reward under i.i.d. random binary load where 0 = 0, if α > 16 and 1 < √ 2\n4 , we have RAdaUCB(T ) = O(1).\nRemark 4: We note that this O(1) bound is in the sense of expected regret, which is different from the high probability O(1) regret bound (Abbasi-Yadkori et al., 2011). Specifically, while the opportunistic bandits can model the whole spectrum of load-dependent regret, Corollary 1 highlights one end of the spectrum where there are “free” learning opportunities. In this case, we push most explorations to the “free” exploration slots and result in an O(1) expected regret. Note that even under “free” exploration, we assume here that the value of the arms can be observed as discussed in Sec. 2.\nIt is worth noting that there are realistic scenarios where the exploration cost of a suboptimal arm is zero or close to zero. Consider the network configuration case where we use throughput as the reward. In this case, Xat,t is the percentage of the peak load that configuration at can handle. Because of the dummy low-priority traffic injected into the network, we can learn the true value of Xat,t under the\npeak load. At the same time, configuration at, although suboptimal, may completely satisfy the real load Lt because it is high priority and thus served first. Therefore, although a suboptimal arm, at sacrifices no throughput on the real load Lt, and thus generates a real regret of zero. In other words, even if the system load is always positive, the chance of zero regret under a suboptimal arm is greater than zero, and in practice, can be non-negligible. To capture this effect, we can modify the regret defined in Eq. (1) by replacing Lt with 0 when Lt is smaller than a threshold.\nLast, we note that, under the condition of Corollary 1, it is easy to design other heuristic algorithms that can perform well. For example, one can do round-robin exploration when the load is zero and chooses the best arm when the load is non-zero. However, such naive strategies are difficult to extend to more general cases. In contrast, AdaUCB applies to a wide range of situations, with both theoretical performance guarantees and desirable empirical performance.\nDependence on ρ: In the regret analysis, we focus on the asymptotic behavior of the regret as T goes to infinity. In the bound, the constant term contains the impact of other factors, in particular the ratio of low load ρ, as shown in Appendix B.5. From the analysis, one can see that the constant term increases as ρ → 0. It suggests that one should use the traditional UCB when ρ is small because there exists little load fluctuation. In practice, AdaUCB achieves much smaller regret than traditional UCB and TS algorithms, even for small values of ρ such as ρ = 0.05 under binary load and ρ = 0.001 under continuous load. Such analysis and evaluations establish guidelines on when to use UCB or AdaUCB. More discussions can be found in Appendix D."
  }, {
    "heading": "4.3. AdaUCB under Continuous Load",
    "text": "Inspired by the insights obtained from the binary-valued load case, we discuss AdaUCB in opportunistic bandits under continuous load in this section.\nSelection of truncation thresholds. When the load is continuous, we need to choose appropriate l(−) and l(+) for AdaUCB. We first assume that the load distribution is a priori known, and discuss how to choose the thresholds under unknown load distribution later. The analysis under binary-valued load indicates that, the explorations mainly occur in low load slots. To guarantee sufficient explorations for a logarithmic regret, we propose to select the thresholds such that:\n• The lower threshold l(−) satisfies P{Lt ≤ l(−)} = ρ > 0;\n• The upper threshold l(+) ≥ l(−).\nIn the special case of l(+) = l(−), we redefine the normalized load L̃t in (4) as L̃t = 0 when Lt ≤ l(−) and L̃t = 1 when Lt > l(−).\nRegret analysis. Under continuous load, it is hard to obtain regret bound as that in Theorem 2 for general l(−) and l(+) chosen above. Instead, we first show logarithmic regret for general l(−) and l(+), and then illustrate the advantages of AdaUCB for the special case with l(−) = l(+).\nFirst, we show that AdaUCB with appropriate truncation thresholds achieves logarithmic regret as below. This lemma is similar to Lemma 3, and the detailed outline of proof can be found in Appendix C.\nLemma 5. In the opportunistic bandit with random continuous load and random rewards, under AdaUCB with P{Lt ≤ l(−)} = ρ > 0 and l(+) ≥ l(−), we have\nE[Ck(T )] ≤ 4α log T\n∆2k +O(1). (8)\nNext, we illustrate the advantages of AdaUCB under continuous load by studying the regret bound for AdaUCB with special thresholds l(+) = l(−).\nTheorem 3. In the opportunistic bandit with random continuous load and random rewards, under AdaUCB with P{Lt ≤ l(−)} = ρ > 0 and l(+) = l(−) , we have RAdaUCB(T ) ≤ 4α log TE[Lt|Lt ≤ l(−)] ∑ k>1 1 ∆k +O(1), (9)\nwhere E[Lt|Lt ≤ l(−)] is the expectation of Lt conditioned on Lt ≤ l(−).\nSketch of Proof: Recall that for this special case l(+) = l(−), we let L̃t = 0 for Lt ≤ l(−) and L̃t = 1 for Lt > l(+). Then we can prove the theorem analogically to the proof of Theorem 2 for the binary-valued case. Specially, when Lt ≤ l(−), we have L̃t = 0 and it corresponds to the case of Lt = 0 (L̃t = 0) in the binary-valued load case. Similarly, the case of Lt > l(+) (L̃t = 1) corresponds to the case of Lt = 1 − 1 under binary-valued load with 1 = 0. Then, we can obtain results similar to Lemma 4 and thus show that the regret under load Lt > l(+) is O(1). Furthermore, the number of pulls under load level Lt ≤ l(−) is bounded according to Lemma 5. The conclusion of the theorem then follows by using the fact that all load below l(−) are treated the same by AdaUCB, i.e., L̃t = 0 for all Lt ≤ l(−).\nRemark 5: We compare the regret of AdaUCB and conventional bandit algorithms by an example, where the load level Lt is uniformly distributed in [0, 1]. In this simple example, the regret of AdaUCB with thresholds l(+) = l(−)\nis bounded by RAdaUCB(T ) ≤ 4α log T ∑ k>1 1 ∆k · ρ2 + O(1), since E[Lt|Lt ≤ l(−)] = ρ/2 and E[Lt|Lt >\nl(−)] < 1. However, for any load-oblivious bandit algorithm such as UCB(α) , the regret is lower bounded by log T ∑ k>1 ∆k KL(uk,u1)\n· 12 +O(1). Thus, AdaUCB achieves much smaller regret when T is large and ρ is relatively small.\nRemark 6: From the above analysis, we can see that the selection of l(+) does not affect the order of the regret (O(log T )). However, for a fixed l(−), we can further adjust l(+) to control the explorations for the load in the range of (l(−), l(+)). Specifically, with a larger l(+), more explorations happen under the load between l(−) and l(+). These explorations accelerate the learning speed but may increase the long term regret because we allow more explorations under load l(−) < Lt < l(+). The behavior is opposite if we use a smaller l(+). In addition, appropriately chosen thresholds also handle the case when the load has little or no fluctuation, i.e., Lt ≈ c. For example, if we set l(−) = c and l(+) = 2c, AdaUCB degenerates to UCB(α).\nE-AdaUCB. In practice, the load distribution may be unknown a priori and may change over time. To address this issue, we propose a variant, named Empirical-AdaUCB (E-AdaUCB), which adjusts the thresholds l(−) and l(+) based on the empirical load distribution. Specifically, the algorithm maintains the histogram for the load levels (or its moving average version for non-stationary cases), and then select l(−) and l(+) accordingly. For example, we can select l(−) and l(+) such that the empirical probability P̃{Lt ≤ l(−)} = P̃{Lt ≥ l(+)} = 0.05. We can see that, in most simulations, E-AdaUCB performs closely to AdaUCB with thresholds chosen offline."
  }, {
    "heading": "5. Experiments",
    "text": "In this section, we evaluate the performance of AdaUCB using both synthetic data and real-world traces. We use the classic UCB(α) and TS (Thompson Sampling) algorithms as comparison baselines. In both AdaUCB and UCB(α), we set α as α = 0.51, which is close to 1/2 and performs better than a larger α. We note that the gap between AdaUCB and the classic UCB(α) clearly demonstrates the impact of opportunistic learning. On the other hand, TS is one of the most popular and robust bandit algorithms applied to a wide range of application scenarios. So we apply it here as a reference. However, because AdaUCB and TS (or other bandit algorithms) improve UCB on different fronts, so their comparison does not clearly show the impact of opportunistic bandit.\nAdaUCB under synthetic scenarios. We consider a 5- armed bandit with Bernoulli rewards, where the expected reward vector is [0.05, 0.1, 0.15, 0.2, 0.25]. Fig. 1(a) shows the regrets for different algorithms under random binaryvalue load with 0 = 1 = 0 and ρ = 0.5. AdaUCB significantly reduces the regret in opportunistic bandits.\n100 102 104 106\nTime t\n0\n50\n100\n150\nR eg\nre t\nUCB( ) TS AdaUCB\n(a) Binary-valued load\n100 102 104 106\nTime t\n0\n20\n40\n60\n80\n100\n120\n140\n160\nR eg\nre t\nUCB( ) TS AdaUCB E-AdaUCB AdaUCB(l(-)=l(+))\n(b) Beta distributed load\nFigure 1. Regret under Synthetic Scenarios. In (a), 0 = 1 = 0, ρ = 0.5. In (b), for AdaUCB, l(−) = l(−)0.05, l (+) = l (+) 0.05; for AdaUCB(l (−) = l(+)), l(−) = l(+) = l(−)0.05.\nSpecifically, the exploration cost in this case can be zero and AdaUCB achieves O(1) regret. For continuous load, Fig. 1(b) shows the regrets for different algorithms with beta distributed load. AdaUCB still outperforms the UCB(α) or TS algorithms. Here, we define l(−)ρ as the lower threshold such that P{Lt ≤ l(−)ρ } = ρ, and l(+)ρ as the upper threshold such that P{Lt ≥ l(+)ρ } = ρ. These simulation results demonstrate that, with appropriately chosen parameters, the proposed AdaUCB and E-AdaUCB algorithms achieve good performance by leveraging the load fluctuation in opportunistic bandits. As a special case, with a single threshold l(+) = l(−) = l(−)0.05, AdaUCB still outperforms UCB(α) and TS, although it may have higher regret at the beginning. More simulation results can be found in Appendix D.1, where we study the impact of environment and algorithm parameters such as load fluctuation and the thresholds for load truncation. In particular, the results show that AdaUCB works well in continuous load when ρ is very small.\nAdaUCB applied in MVNO systems. We now evaluate the proposed algorithms using real-world traces. In an MVNO (Mobile Virtual Network Operator) system, a virtual operator, such as Google Fi (Project Fi, https://fi.google.com), provides services to users by leasing network resources from real mobile operators. In such a system, the virtual operator would like to provide its users high quality service by accessing the network resources of the real operator with the best network performance. Therefore, we view each real mobile operator as an arm, and the quality of user experienced on that operator network as the reward. We use experiment data from Speedometer (Speedometer, https://storage.cloud.google.com/speedometer) and another anonymous operator to conduct the evaluation. More details about the MVNO system can be found in Appendix D.2. Here, using insights obtained from simulations based on the synthetic data, we choose l(−) and l(+) such that P{Lt ≤ l(−)} = P{Lt ≥ l(+)} = 0.05. As shown in Fig. 2, the regret of AdaUCB is only about 1/3 of UCB(α),\nand the performance of E-AdaUCB is indistinguishable from that of AdaUCB. This experiment demonstrates the effectiveness of AdaUCB and E-AdaUCB in practical situations, where the load and the reward are continuous and are possibly non-stationary. It also demonstrates the practicality of E-AdaUCB without a priori load distribution information."
  }, {
    "heading": "6. Conclusions and Future Work",
    "text": "In this paper we study opportunistic bandits where the regret of pulling a suboptimal arm depends on external conditions such as traffic load or produce price. We propose AdaUCB that opportunistically chooses between exploration and exploitation based on the load level, i.e., taking the slots with low load level as opportunities for more explorations. We analyze the regret of AdaUCB, and show that AdaUCB can achieve provable lower regret than the traditional UCB algorithm, and even O(1) regret with respect to time horizon T , under certain conditions. Experimental results based on both synthetic and real data demonstrate the significant benefits of opportunistic exploration under large load fluctuations.\nThis work is a first attempt to study opportunistic bandits, and several open questions remain. First, although AdaUCB achieves promising experimental performance under general settings, rigorous analysis with tighter performance bound remains challenging. Furthermore, opportunistic TS-type algorithms are also interesting because TS-type algorithms often performs better than UCB-type algorithms in practice. Last, we hope to investigate more general relations between the load and actual reward."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by NSF Grants CCF1423542, CNS-1547461, CNS-1718901. The authors would like to thank Prof. Peter Auer (University of Leoben) for his helpful suggestions, and the reviewers for their valuable feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Improved algorithms for linear stochastic bandits",
    "authors": ["Y. Abbasi-Yadkori", "D. Pál", "C. Szepesvári"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "Finite-time analysis of the multiarmed bandit problem",
    "authors": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"],
    "venue": "Machine learning,",
    "year": 2002
  }, {
    "title": "Stochastic multi-armedbandit problem with non-stationary rewards",
    "authors": ["O. Besbes", "Y. Gur", "A. Zeevi"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Price forecasting and evaluation: An application in agriculture",
    "authors": ["J.A. Brandt", "D.A. Bessler"],
    "venue": "Journal of Forecasting,",
    "year": 1983
  }, {
    "title": "Bandits games and clustering foundations",
    "authors": ["S. Bubeck"],
    "venue": "PhD thesis, Université des Sciences et Technologie de LilleLille I,",
    "year": 2010
  }, {
    "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "Machine Learning,",
    "year": 2012
  }, {
    "title": "Contextual bandits with linear payoff functions",
    "authors": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"],
    "venue": "In International Conference on Artificial Intelligence and Statistics,",
    "year": 2011
  }, {
    "title": "On upper-confidence bound policies for non-stationary bandit problems",
    "authors": ["A. Garivier", "E. Moulines"],
    "venue": "In International Conference on Algorithmic Learning Theory,",
    "year": 2011
  }, {
    "title": "Adaptive treatment allocation and the multi-armed bandit problem",
    "authors": ["T.L. Lai"],
    "venue": "The Annals of Statistics,",
    "year": 1987
  }, {
    "title": "Asymptotically efficient adaptive allocation rules",
    "authors": ["T.L. Lai", "H. Robbins"],
    "venue": "Advances in Applied Mathematics,",
    "year": 1985
  }, {
    "title": "A contextual-bandit approach to personalized news article recommendation",
    "authors": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"],
    "venue": "In ACM International Conference on World Wide Web (WWW), pp",
    "year": 2010
  }, {
    "title": "Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research",
    "authors": ["W.H. Press"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2009
  }, {
    "title": "Regret lower bounds and extended upper confidence bounds policies in stochastic multi-armed bandit problem",
    "authors": ["A. Salomon", "Audibert", "J.-Y", "I.E. Alaoui"],
    "venue": "arXiv preprint arXiv:1112.3827,",
    "year": 2011
  }, {
    "title": "Lower bounds and selectivity of weak-consistent policies in stochastic multi-armed bandit problem",
    "authors": ["A. Salomon", "Audibert", "J.-Y", "I.E. Alaoui"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges",
    "authors": ["S.S. Villar", "J. Bowden", "J Wason"],
    "venue": "Statistical Science,",
    "year": 2015
  }, {
    "title": "Performance analysis of a single-server atm queue with a priority scheduling",
    "authors": ["J. Walraevens", "B. Steyaert", "H. Bruneel"],
    "venue": "Computers & Operations Research,",
    "year": 1829
  }, {
    "title": "Algorithms with logarithmic or sublinear regret for constrained contextual bandits",
    "authors": ["H. Wu", "R. Srikant", "X. Liu", "C. Jiang"],
    "venue": "In The 29th Annual Conference on Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "A survey on contextual multi-armed bandits",
    "authors": ["L. Zhou"],
    "venue": "arXiv preprint arXiv:1508.03326,",
    "year": 2015
  }],
  "id": "SP:881443012b9cd5c1d2b62033789af3fb9e4b2077",
  "authors": [{
    "name": "Huasen Wu",
    "affiliations": []
  }, {
    "name": "Xueying Guo",
    "affiliations": []
  }, {
    "name": "Xin Liu",
    "affiliations": []
  }],
  "abstractText": "In this paper, we propose and study opportunistic bandits a new variant of bandits where the regret of pulling a suboptimal arm varies under different environmental conditions, such as network load or produce price. When the load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g., trying a suboptimal network configuration). Therefore, intuitively, we could explore more when the load/price is low and exploit more when the load/price is high. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound (AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff for opportunistic bandits. We prove that AdaUCB achieves O(log T ) regret with a smaller coefficient than the traditional UCB algorithm. Furthermore, AdaUCB achieves O(1) regret with respect to T if the exploration cost is zero when the load level is below a certain threshold. Last, based on both synthetic data and real-world traces, experimental results show that AdaUCB significantly outperforms other bandit algorithms, such as UCB and TS (Thompson Sampling), under large load/price fluctuations.",
  "title": "Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits"
}