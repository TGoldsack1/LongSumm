{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 694–699 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n694\nMany corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time."
  }, {
    "heading": "1 Introduction",
    "text": "Language, and therefore data derived from language, changes over time (Ullmann, 1962). Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016). However, little is known about how shifts in text over time affect the performance of language processing systems.\nThis paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time. We consider both long-term variations in text over time and seasonal variations which change throughout a year but repeat across years. Our empirical study considers corpora contain-\ning formal text spanning decades as well as usergenerated content spanning only a few years.\nAfter describing the datasets and experiment design, this paper has two main sections, respectively addressing the following research questions:\n1. In what ways does document classification depend on the timestamps of the documents?\n2. Can document classifiers be adapted to perform better in time-varying corpora?\nTo address question 1, we train and test on data from different time periods, to understand how performance varies with time. To address question 2, we apply a domain adaptation approach, treating time intervals as domains. We show that in most cases this approach can lead to improvements in classification performance, even on future time intervals."
  }, {
    "heading": "1.1 Related Work",
    "text": "Time is implicitly embedded in the classification process: classifiers are often built to be applied to future data that doesn’t yet exist, and performance on held-out data is measured to estimate performance on future data whose distribution may have changed. Methods exist to adjust for changes in the data distribution (covariate shift) (Shimodaira, 2000; Bickel et al., 2009), but time is not typically incorporated into such methods explicitly.\nOne line of work that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørvåg, 2008; Chambers, 2012; Kotsakos et al., 2014). However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps."
  }, {
    "heading": "2 Datasets and Experimental Setup",
    "text": "Our study experiments with six corpora:\n• Reviews: Three corpora containing reviews labeled with sentiment: music reviews from Amazon (He and McAuley, 2016), and hotel reviews and restaurant reviews from Yelp.1 We discarded reviews that had fewer than 10 tokens or a helpfulness/usefulness score of zero. The reviews with neutral scores were removed.\n• Politics: Sentences from the American party platforms of Republicans and Democrats from 1948 to 2016, available every four years.2\n• News: Newspaper articles from 1950-2014, labeled with whether the article is relevant to the US economy.3\n• Twitter: Tweets labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (Huang et al., 2017).\nOur experiments require documents to be grouped into time intervals. Table 1 shows the intervals for each corpus. Documents that fall outside of these time intervals were removed. We grouped documents into two types of intervals:\n• Seasonal: Time intervals within a year (e.g., January through March) that may be repeated across years.\n• Non-seasonal: Time intervals that do not repeat (e.g., 1997-1999).\nFor each dataset, we performed binary classification, implemented in sklearn (Pedregosa et al., 2011). We built logistic regression classifiers with TF-IDF weighted n-gram features (n ∈ {1, 2, 3}), removing features that appeared in less than 2 documents. Except when otherwise specified, we held out a random 10% of documents as\n1https://www.yelp.com/dataset 2https://www.comparativeagendas.net/\ndatasets_codebooks 3https://www.crowdflower.com/ data-for-everyone/\nvalidation data for each dataset. We used Elastic Net (combined `1 and `2) regularization (Zou and Hastie, 2005), and tuned the regularization parameters to maximize performance on the validation data. We evaluated the performance using weighted F1 scores."
  }, {
    "heading": "3 How Does Classification Performance Vary with Time?",
    "text": "We first conduct an analysis of how classifier performance depends on the time intervals in which it is trained and applied. For each corpus, we train the classifier on each time interval and test on each time interval. We downsampled the training data within each time interval to match the number of documents in the smallest interval, so that differences in performance are not due to the size of the training data.\nIn all experiments, we train a classifier on a partition of 80% of the documents in the time interval, and repeat this five times on different partitions, averaging the five F1 scores to produce the final estimate. When training and testing on the same interval, we test on the held-out 20% of documents in that interval (standard cross-validation). When testing on different time intervals, we test on all documents, since they are all held-out from the training interval; however, we still train on five subsets of 80% of documents, so that the training data is identical across all experiments.\nFinally, to understand why performance varies, we also qualitatively examined how the distribution of content changes across time intervals. To measure the distribution of content, we trained a topic model with 20 topics using gensim (Řehůřek and Sojka, 2010) with default parameters. We associated each document with one topic (the most probable topic in the document), and then calculated the proportion of each topic within a time period as the proportion of documents in that time period assigned to that topic. We can then visualize the extent to which the distribution of 20 topics varies by time.\nJan-M ar Apr-Ju n Jul-Se p Oct-D ec\nTrain\nJanMar Apr -Jun Jul-S ep\nOct -De\nc\nTe st\n0.948 0.912 0.913 0.910\n0.916 0.949 0.914 0.909\n0.916 0.912 0.952 0.910\n0.916 0.914 0.918 0.945\nReviews data - music\nJan-M ar Apr-Ju n Jul-Se p Oct-D ec\nTrain\nJanMar Apr -Jun Jul-S ep\nOct -De\nc\nTe st\n0.865 0.862 0.862 0.861\n0.863 0.862 0.861 0.858\n0.862 0.859 0.866 0.861\n0.863 0.863 0.863 0.858\nReviews data - hotels\nJan-M ar Apr-Ju n Jul-Se p Oct-D ec\nTrain\nJanMar Apr -Jun Jul-S ep\nOct -De\nc\nTe st\n0.898 0.806 0.750 0.769\n0.795 0.876 0.745 0.787\n0.794 0.795 0.900 0.767\n0.791 0.790 0.731 0.891\nNews data - economy\nJan-M ar Apr-Ju n Jul-Se p Oct-D ec\nTrain\nJanMar Apr -Jun Jul-S ep\nOct -De\nc\nTe st\n0.896 0.894 0.891 0.856\n0.808 0.940 0.853 0.829\n0.836 0.904 0.917 0.845\n0.849 0.891 0.884 0.902\nTwitter data - vaccine\n2006 -08 2009 -11 2012 -14 2015 -17\nTrain\n200 6-08 200 9-11 201 2-14 201 5-17 Te st\n0.823 0.828 0.825 0.859\n0.799 0.843 0.830 0.858\n0.800 0.819 0.833 0.869\n0.790 0.813 0.835 0.880\nReviews data - hotels\n2006 -08 2009 -11 2012 -14 2015 -17\nTrain\n200 6-08 200 9-11 201 2-14 201 5-17 Te st\n0.829 0.838 0.869 0.883\n0.814 0.856 0.870 0.883\n0.815 0.842 0.884 0.894\n0.814 0.839 0.875 0.902\nReviews data - restaurants\n1948 -56 1960 -68 1972 -80 1984 -92 1996 -20042008 -16\nTrain\n194 8-56 196 0-68 197 2-80 198 4-92\n199 6-20\n04 200 8-16\nTe st\n0.659 0.567 0.518 0.544 0.525 0.532 0.551 0.800 0.529 0.477 0.474 0.495 0.545 0.506 0.678 0.635 0.573 0.523 0.515 0.473 0.565 0.866 0.594 0.569 0.435 0.404 0.490 0.618 0.848 0.684 0.435 0.416 0.480 0.606 0.674 0.819\nPolitics - US political data\n1985 -89 1990 -94 1995 -99 2000 -04 2005 -09 2010 -14\nTrain\n198 5-89 199 0-94 199 5-99 200 0-04 200 5-09 201 0-14 Te st\n0.876 0.758 0.783 0.794 0.777 0.756 0.764 0.883 0.771 0.802 0.789 0.748 0.759 0.760 0.905 0.798 0.806 0.763 0.760 0.756 0.770 0.926 0.805 0.771 0.773 0.767 0.783 0.826 0.900 0.778 0.773 0.750 0.778 0.810 0.786 0.897\nNews data - economy\nFigure 1: Document classification performance when training and testing on different times of year (top) and different years (bottom). Some corpora are omitted for space."
  }, {
    "heading": "3.1 Seasonal Variability",
    "text": "The top row of Figure 1 shows the test scores from training and testing on each pair of seasonal time intervals for four of the datasets. We observe very strong seasonal variations in the economic news corpus, with a drop in F1 score on the order of 10 when there is a mismatch in the season between training and testing. There is a similar, but weaker, effect on performance in the music reviews from Amazon and the vaccine tweets. There was virtually no difference in performance in any of the pairs in both review corpora from Yelp (restaurants, not pictured, and hotels).\nTo help understand why the performance varies, Figure 2 (left) shows the distribution of topics in each seasonal interval for two corpora: Amazon music reviews and Twitter. We observe very little variation in the topic distribution across seasons in the Amazon corpus, but some variation in the Twitter corpus, which may explain the large performance differences when testing on held-out seasons in the Twitter data as compared to the Amazon corpus.\nFor space, we do not show the descriptions of the topics, but instead only the shape of the distributions to show the degree of variability. We did qualitatively examine the differences in word features across the time periods, but had difficulty interpreting the observations and were unable to draw clear conclusions. Thus, characterizing the ways in which content distributions vary over time, and why this affects performance, is still an open question."
  }, {
    "heading": "3.2 Non-seasonal Variability",
    "text": "The bottom row of Figure 1 shows the test scores from training and testing on each pair of nonseasonal time intervals. A strong pattern emerges in the political parties corpus: F1 scores can drop by as much as 40 points when testing on different time intervals. This is perhaps unsurprising, as this collection spans decades, and US party positions have substantially changed over time. The performance declines more when testing on time intervals that are further away in time from the training interval, suggesting that changes in party platforms shift gradually over time. In contrast, while there was a performance drop when testing outside the training interval in the economic news corpus, the drop was not gradual. In the Twitter dataset (not pictured), F1 dropped by an average of 4.9 points outside the training interval.\nWe observe an intriguing non-seasonal pattern that is consistent in both of the review corpora from Yelp, but not in the music review corpus from Amazon (not pictured), which is that the classification performance fairly consistently increases over time. Since we sampled the dataset so that the time intervals have the same number of reviews, this suggests something else changed over time about the way reviews are written that makes the sentiment easier to detect.\nThe right side of Figure 2 shows the topic distribution in the Amazon and Twitter datasets across non-seasonal intervals. We observe higher levels of variability across time in the non-seasonal intervals as compared to the seasonal intervals."
  }, {
    "heading": "3.3 Discussion",
    "text": "Overall, it is clear that classifiers generally perform best when applied to the same time interval they were trained. Performance diminishes when applied to different time intervals, although different corpora exhibit differ patterns in the way in which the performance diminishes. This kind of analysis can be applied to any corpus and could provide insights into characteristics of the corpus that may be helpful when designing a classifier."
  }, {
    "heading": "4 Making Classification Robust to Temporality",
    "text": "We now consider how to improve classifiers when working with datasets that span different time intervals. We propose to treat this as a domain adaptation problem. In domain adaptation, any partition of data that is expected to have a different distribution of features can be treated as a domain (Joshi et al., 2013). Traditionally, domain adaptation is used to adapt models to a common task across rather different sets of data, e.g., a sentiment classifier for different types of products (Blitzer et al., 2007). Recent work has also applied domain adaptation to adjust for potentially more subtle differences in data, such as adapting for differences in the demographics of authors (Volkova et al., 2013; Lynn et al., 2017). We follow the same approach, treating time intervals as domains.\nIn our experiments, we use the feature augmentation approach of Daumé III (2007) to perform domain adaptation. Each feature is duplicated to have a specific version of the feature for every domain, as well as a domain-independent version of the feature. In each instance, the domainindependent feature and the domain-specific feature for that instance’s domain have the same feature value, while the value is zeroed out for the domain-specific features for the other domains.\nThis is equivalent to a model where the feature weights are domain specific but share a Gaussian prior across domains (Finkel and Manning, 2009). This approach is widely used due to its simplicity, and derivatives of this approach have been used in similar work (e.g., (Lynn et al., 2017)). Following Finkel and Manning (2009), we separately adjust the regularization strength for the domain-independent feature weights and the domain-specific feature weights."
  }, {
    "heading": "4.1 Seasonal Adaptation",
    "text": "We first examine classification performance on the datasets when grouping the seasonal time intervals (January-March, April-June, July-August, September-December) as domains and applying the feature augmentation approach for domain adaptation. As a baseline comparison, we apply the same classifier, but without domain adaptation.\nResults are shown in Table 2. We see that applying domain adaptation provides a small boost in three of the datasets, and has no effect on two of the datasets. If this pattern holds in other corpora, then this suggests that it does not hurt performance to apply domain adaptation across different times of year, and in some cases can lead to a small performance boost."
  }, {
    "heading": "4.2 Non-seasonal Adaptation",
    "text": "We now consider the non-seasonal time intervals (spans of years). In particular, we consider the scenario when one wants to apply a classifier trained on older data to future data. This requires a modification to the domain adaptation approach, because future data includes domains that did not exist in the training data, and thus we cannot learn domain-specific feature weights. To solve this, we train in the usual way, but when testing on future data, we only include the domain-independent features. The intuition is that the domain-independent parameters should be applicable to all domains, and so using only these features should lead to better generalizability to new domains. We test this hypothesis by training the classifiers on all but the last time interval, and testing on the final interval. For hyperparameter tuning, we used the final time interval of the training data (i.e., the penultimate interval) as the validation set. The intuition is that the penultimate interval is the closest to the test data and thus is expected to be most similar to it.\nResults are shown in the first three columns of Table 3. We see that this approach leads to a small performance boost in all cases except the Twitter dataset. This means that this simple feature augmentation approach has the potential to make classifiers more robust to future changes in data.\nHow to apply the feature augmentation technique to unseen domains is not well understood. By removing the domain-specific features, as we did here, the prediction model has changed, and so its behavior may be hard to predict. Nonetheless, this appears to be a successful approach."
  }, {
    "heading": "4.2.1 Adding Seasonal Features",
    "text": "We also experimented with including the seasonal features when performing non-seasonal adaptation. In this setting, we train the models with two domain-specific features in addition to the domain-independent features: one for the season,\nand one for the non-seasonal interval. As above, we remove the non-seasonal features at test time; however, we retain the season-specific features in addition to the domain-independent features, as they can be reused in future years.\nThe results of this approach are shown in the last column of Table 3. We find that combining seasonal and non-seasonal features together leads to an additional performance gain in most cases."
  }, {
    "heading": "5 Conclusion",
    "text": "Our experiments suggest that time can substantially affect the performance of document classification, and practitioners should be cognizant of this variable when developing classifiers. A simple analysis comparing pairs of time intervals can provide insights into how performance varies with time, which could be a good practice to do when initially working with a corpus. Our experiments also suggest that simple domain adaptation techniques can help account for this variation.4\nWe make two practical recommendations following the insights from this work. First, evaluation will be most accurate if the test data is as similar as possible to whatever future data the classifier will be applied to, and one way to achieve this is to select test data from the chronological end of the corpus, rather than randomly sampling data without regard to time. Second, we observed that performance on future data tends to increase when hyperparameter tuning is conducted on later data; thus, we also recommend sampling validation data from the chronological end of the corpus."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors thank the anonymous reviews for their insightful comments and suggestions. This work was supported in part by the National Science Foundation under award number IIS-1657338.\n4Our code is available at: https://github.com/ xiaoleihuang/Domain_Adaptation_ACL2018"
  }],
  "year": 2018,
  "references": [{
    "title": "Discriminative learning under covariate shift",
    "authors": ["Steffen Bickel", "Michael Brckner", "Tobias Scheffer."],
    "venue": "Journal of Machine Learning Research, 10:2137– 2155.",
    "year": 2009
  }, {
    "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
    "authors": ["John Blitzer", "Mark Dredze", "Fernando Pereira."],
    "venue": "Association for Computational Linguistics (ACL), pages 440–447.",
    "year": 2007
  }, {
    "title": "Labeling documents with timestamps: Learning from their time expressions",
    "authors": ["Nathanael Chambers."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 98–106.",
    "year": 2012
  }, {
    "title": "Frustratingly easy domain adaptation",
    "authors": ["Hal Daumé III."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2007
  }, {
    "title": "Diffusion of lexical change in social media",
    "authors": ["Jacob Eisenstein", "Brendan O’Connor", "Noah A. Smith", "Eric P. Xing"],
    "venue": "PLoS ONE,",
    "year": 2014
  }, {
    "title": "Hierarchical Bayesian domain adaptation",
    "authors": ["Jenny R. Finkel", "Christopher D. Manning."],
    "venue": "North American Chapter of the Association for Computational Linguistics (ACL).",
    "year": 2009
  }, {
    "title": "The social dynamics of language change in online networks",
    "authors": ["Rahul Goel", "Sandeep Soni", "Naman Goyal", "John Paparrizos", "Hanna Wallach", "Fernando Diaz", "Jacob Eisenstein."],
    "venue": "The International Conference on Social Informatics (SocInfo).",
    "year": 2016
  }, {
    "title": "Diachronic word embeddings reveal statistical laws of semantic change",
    "authors": ["William L. Hamilton", "Jure Leskovec", "Dan Jurafsky."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2016
  }, {
    "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
    "authors": ["Ruining He", "Julian McAuley."],
    "venue": "Proceedings of the 25th International Conference on World Wide Web (WWW), pages 507–517. Interna-",
    "year": 2016
  }, {
    "title": "Examining patterns of influenza vaccination in social media",
    "authors": ["Xiaolei Huang", "Michael C Smith", "Michael J Paul", "Dmytro Ryzhkov", "Sandra C Quinn", "David A Broniatowski", "Mark Dredze."],
    "venue": "Proceedings of the AAAI Joint Workshop on Health",
    "year": 2017
  }, {
    "title": "What’s in a domain? multidomain learning for multi-attribute data",
    "authors": ["Mahesh Joshi", "Mark Dredze", "William W. Cohen", "Carolyn P. Rose."],
    "venue": "North American Chapter of the Association for Computational Linguistics (NAACL) (short paper), pages",
    "year": 2013
  }, {
    "title": "Improving temporal language models for determining time of nontimestamped documents",
    "authors": ["N. Kanhabua", "K. Nørvåg."],
    "venue": "European Conference on Digital Libraries (ECDL).",
    "year": 2008
  }, {
    "title": "A burstiness-aware approach for document dating",
    "authors": ["Dimitrios Kotsakos", "Theodoros Lappas", "Dimitrios Kotzias", "Dimitrios Gunopulos", "Nattiya Kanhabua", "Kjetil Nørvåg."],
    "venue": "Proceedings of the 37th International ACM SIGIR Conference on Re-",
    "year": 2014
  }, {
    "title": "Human centered NLP with user-factor adaptation",
    "authors": ["Veronica E. Lynn", "Youngseo Son", "Vivek Kulkarni", "Niranjan Balasubramanian", "H. Andrew Schwartz."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1155.",
    "year": 2017
  }, {
    "title": "Scikit-learn: Machine learning in Python",
    "authors": ["Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"],
    "year": 2011
  }, {
    "title": "Software Framework for Topic Modelling with Large Corpora",
    "authors": ["Radim Řehůřek", "Petr Sojka."],
    "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA. http://is.muni.cz/",
    "year": 2010
  }, {
    "title": "Improving predictive inference under covariate shift by weighting the loglikelihood function",
    "authors": ["Hidetoshi Shimodaira."],
    "venue": "Journal of Statistical Planning and Inference, 90(2):227 – 244.",
    "year": 2000
  }, {
    "title": "Semantics: an introduction to the science of meaning",
    "authors": ["Stephen Ullmann."],
    "venue": "Basil Blackwell, Oxford.",
    "year": 1962
  }, {
    "title": "Exploring demographic language variations to improve multilingual sentiment analysis in social media",
    "authors": ["Svitlana Volkova", "Theresa Wilson", "David Yarowsky."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages",
    "year": 2013
  }, {
    "title": "Understanding semantic change of words over centuries",
    "authors": ["Derry Tanti Wijaya", "Reyyan Yeniterzi."],
    "venue": "Proceedings of the 2011 International Workshop on DETecting and Exploiting Cultural diversiTy on the Social Web.",
    "year": 2011
  }, {
    "title": "From Part to Person: Natural Tendencies of Semantic Change and the Search for Cognates",
    "authors": ["D.P. Wilkins."],
    "venue": "Cognitive Anthropology Research Group at the Max Planck Institute for Psycholinguistics.",
    "year": 1993
  }, {
    "title": "Regularization and variable selection via the elastic net",
    "authors": ["Hui Zou", "Trevor Hastie."],
    "venue": "Journal of the Royal Statistical Society, Series B, 67:301–320.",
    "year": 2005
  }],
  "id": "SP:69825241b4bf759127e87aa9ee9b50b33ae90aa1",
  "authors": [{
    "name": "Xiaolei Huang",
    "affiliations": []
  }, {
    "name": "Michael J. Paul",
    "affiliations": []
  }],
  "abstractText": "Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",
  "title": "Examining Temporality in Document Classification"
}