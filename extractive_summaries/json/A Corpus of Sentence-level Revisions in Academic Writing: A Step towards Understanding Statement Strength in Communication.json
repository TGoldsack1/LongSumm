{
  "sections": [{
    "text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403–408, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "It is important for authors and speakers to find the appropriate “pitch” to convey a desired message to the public. Indeed, sometimes heated debates can arise around the choice of statement strength. For instance, on March 1, 2014, an attack at Kunming’s railway station left 29 people dead and more than 140 others injured.1 In the aftermath, Chinese media accused Western media of “softpedaling the attack and failing to state clearly that it was an act of terrorism”.2 In particular, regarding the statement by the US embassy that referred to this incident as the “terrible and senseless act of violence in Kunming”, a Weibo user posted “If you say that the Kunming attack is a ‘terrible and\n1http://en.wikipedia.org/wiki/2014_ Kunming_attack\n2http://sinosphere.blogs.nytimes. com/2014/03/03/u-n-security-councilcondemns-terrorist-attack-in-kunming/\nsenseless act of violence’, then the 9/11 attack can be called a ‘regrettable traffic incident”’.3\nThis example is striking but not an isolated case, for settings in which one party is trying to convince another are pervasive; scenarios range from court trials to conference submissions. Since the strength and scope of an argument can be a crucial factor in its success, it is important to understand the effects of statement strength in communication.\nA first step towards addressing this question is to be able to distinguish between strong and weak statements. As strength is inherently relative, it is natural to look at revisions that change statement strength, which we refer to as “strength changes”. Though careful and repeated revisions are presumably ubiquitous in politics, legal systems, and journalism, it is not clear how to collect them; on the other hand, revisions to research papers may be more accessible, and many researchers spend significant time on editing to convey the right message regarding the strength of a project’s contributions, novelty, and limitations. Indeed, statement strength in science communication matters to writers: understating contributions can affect whether people recognize the true importance of the work; at the same time, overclaiming can cause papers to be rejected.\nWith the increasing popularity of e-print services such as the arXiv4, strength changes in scientific papers are becoming more readily available. Since the arXiv started in 1991, it has become “the standard repository for new papers in mathematics, physics, statistics, computer science, biology, and other disciplines” (Krantz, 2007). An intriguing observation is that many researchers submit multiple versions of the same paper on arXiv. For instance, among the 70K papers submitted in\n3http://www.huffingtonpost.co.uk/2014/ 03/03/china-kunming-911_n_4888748.html\n4http://arxiv.org/\n403\n2011, almost 40% (27.7K) have multiple versions. Many differences between these versions constitute a source of valid and motivated strength differences, as can be seen from the sentential revisions in Table 1. Pair 1 makes the contribution seem more impressive by replacing “studied” with “proposed”. Pair 2 downgrades “human communication activity” to “mobile phone communication”. Pair 3 removes “significantly” and the emphasis on “same privacy guarantees”. Pair 4 shows an insertion of hedging, a relatively well-known type of strength reduction. Pair 5 is an interesting case that shows the complexity of this problem: on the one hand, S2 claims that something is “inefficient”, which is an absolute statement, compared to “efficiency loss” in S1, where the possibility of efficiency still exists; on the other hand, S1 employs an active tone that emphasizes a causal relationship.\nThe main contribution of this work is to provide the first large-scale corpus of sentence-level revisions for studying a broad range of variations in statement strength. We collected labels for a subset of these revisions. Given the possibility of all kinds of disagreement, the fair level of agreement (Fleiss’ Kappa) among our annotators was decent. But in some cases, the labels differed from our expectations, indicating that the general public can interpret the strength of scientific statements differently from researchers. The participants’ comments may further shed light on science communication and point to better ways to define and understand strength differences."
  }, {
    "heading": "2 Related Work and Data",
    "text": "Hedging, which can lead to strength differences, has received some attention in the study of science\ncommunication (Salager-Meyer, 2011; Lewin, 1998; Hyland, 1998; Myers, 1990). The CoNLL 2010 Shared Task was devoted to hedge detection (Farkas et al., 2010). Hedge detection was also used to understand scientific framing in debates over genetically-modified organisms in food (Choi et al., 2012).\nRevisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al., 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). But none of the categories of Wikipedia revisions previously examined (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012; Mola-Velasco, 2011; Potthast et al., 2008; Daxenberger and Gurevych, 2012) relate to statement strength. After all, the objective of editing on Wikipedia is to present neutral and objective articles.\nPublic datasets of science communication are available, such as the ACL Anthology,5 collections of NIPS papers,6 and so on. These datasets are useful for understanding the progress of disciplines or the evolution of topics. But the lack of edit histories or revisions makes them not immediately suitable for studying strength differences. Recently, there have been experiments with open peer review.7 Records from open reviewing can provide additional insights into the revision process once enough data is collected.\n5http://aclweb.org/anthology/ 6http://nips.djvuzone.org/txt.html 7http://openreview.net"
  }, {
    "heading": "3 Dataset Description",
    "text": "Our main dataset was constructed from all papers submitted in 2011 on the arXiv. We first extracted the textual content from papers that have multiple versions of tex source files. All mathematical environments were ignored. Section titles were not included in the final texts but are used in alignment.\nIn order to align the first version and the final version of the same paper, we first did macro alignment of paper sections based on section titles. Then, for micro alignment of sentences, we employed a dynamic programming algorithm similar to that of Barzilay and Elhadad (2003). Instead of cosine similarity, we used an idf-weighted longestcommon-subsequence algorithm to define the similarity between two sentences, because changes in word ordering can also be interesting. Formally, the similarity score between sentence i and sentence j is defined as\nSimpi, jq “ Weighted-LCSpSi, Sjq maxp ř\nwPSi idfpwq, ř wPSj idfpwqq ,\nwhere Si and Sj refer to sentence i and sentence j. Since it is likely that a new version adds or deletes a large sequence of sentences, we did not impose a skip penalty. We set the mismatch penalty to 0.1.8\nIn the end, there are 23K papers where the first version was different from the last version.9 We\n8We did not allow cross matching (i.e., i Ñ j´1, i´1 Ñ j), since we thought matching this case as pi ´ 1, iq Ñ j or i Ñ pj, j ´ 1q can provide context for annotation purposes. But in the end, we focused on labeling very similar pairs. This decision had little effect.\n9 This differs from the number in Section 1 because articles may not have the tex source available, or the differences between versions may be in non-textual content.\ncategorize sentential revisions into the following three types:\n• Deletion: we cannot find a match in the final version.\n• Typo: all sequences in a pair of matched sentences are typos, where a sequence-level typo is one where the edit distance between the matched sequences is less than three.\n• Rewrite: matched sentences that are not typos. This type is the focus of this study.\nWhat kinds of changes are being made? One might initially think that typo fixes represent a large proportion of revisions, but this is not correct, as shown in Figure 1a. Deletions represent a substantial fraction, especially in the middle section of a paper. But it is clear that the majority of changes are rewrites; thus revisions on the arXiv indeed provide a great source for potential strength differences.\nWho makes changes? Figure 1b shows that the Math subarchive makes the largest number of changes. This is consistent with the mathematics community’s custom of using the arXiv to get findings out early. In terms of changes per sentence (Figure 1c), statistics and quantitative studies are the top subareas.\nFurther, Figure 2 shows the effect of the number of authors. It is interesting that both in terms of sheer number and percentage, single-authored papers have the most changes. This could be because a single author enjoys greater freedom and has stronger motivation to make changes, or because multiple authors tend to submit a more polished initial version. This echoes the finding in Posner\nand Baecker (1992) that the collaborative writing process differs considerably from individual writing. Also, more than 25% of the first versions are changed, which again shows that substantive edits are being made in these resubmissions."
  }, {
    "heading": "4 Annotating Strength Differences",
    "text": "In order to study statement strength, reliable strength-difference labels are needed. In this section, we describe how we tried to define strength differences, compiled labeling instructions, and gathered labels using Amazon Mechanical Turk.\nLabel definition and collection procedure. We focused on matched sentences from abstracts and introductions to maximize the proportion of strength differences (as opposed to factual/no strength changes). We required pairs to have similarity score larger than 0.5 in our labeling task to make pairs more comparable. We also replaced\nall math environments with “[MATH]”.10 We obtained 108K pairs that satisfy the above conditions, available at http://chenhaot.com/ pages/statement-strength.html. To create the pool of pairs for labeling, we randomly sampled 1000 pairs and then removed pairs that we thought were processing errors.\nWe used Amazon Mechanical Turk. It may initially seem surprising to have annotations of technical statements not done by domain experts; we did this intentionally because it is common to communicate unfamiliar topics to the public in political and science communication (we comment on non-expert rationales later). We use the following set of labels: Stronger, Weaker, No Strength Change, I can’t tell. Table 2 gives our definitions. The instructions included 8 pairs as examples and 10 pairs to label as a training exercise. Participants were then asked to choose labels and write mandatory comments for 50 pairs. According to the comments written by participants, we believe that they did the labeling in good faith.\nQuantitative overview. We collected 9 labels each for 500 pairs. Among the 500 pairs, Fleiss’ Kappa was 0.242, which indicates fair agreement (Landis and Koch, 1977). We took a conservative approach and only considered pairs with an absolute majority label, i.e., at least 5 of 9 labelers chose the same label. There are 386 pairs that satisfy this requirement (93 weaker, 194 stronger, 99 no change). On this subset of pairs, Fleiss’ Kappa is 0.322, and 74.4% of pairs were strength changes. Considering all the possible disagreement, this result was acceptable.\nQualitative observations. We were excited about the labels from these participants: despite\n10These decisions were made based on the results and feedback that we got from graduate students in an initial labeling.\nthe apparent difficulty of the task, we found that many labels for the 386 pairs were reasonable. However, in some cases, the labels were counterintuitive. Table 3 shows some representative examples.\nFirst, participants tend to take details as evidence even when these details are not germane to the statement. For pair 1, while one turker pointed out the decline in number of experiments, most turkers simply labeled it as stronger because it was more specific. “Specific” turned out to be a common reason used in the comments, even though we said in the instructions that only additional justification and evidence matter. This echoes the finding in Bell and Loftus (1989) that even unrelated details influenced judgments of guilt.\nSecond, participants interpret constraints/conditions not in strictly logical ways, seeming to care little about scope at times. For instance, the majority labeled pair 2 as “stronger”. But in S2 for that pair, the result holds for strictly fewer possible worlds. But it should be said that there are cases that labelers interpreted logically, e.g., “compelling evidence” subsumes “compelling experimental evidence”.\nBoth of the above cases share the property that they seem to be correlated with a tendency to judge lengthier statements as stronger. Another interesting case that does not share this characteristic is that participants can have a different understanding of domain-specific terms. For pair 3, the majority thought that “vectors” sounds more impressive than “images”; for pair 4, the majority considered “adapt” stronger than “discover”. This issue is common when communicating new topics to the public not only in science commu-\nnication but also in politics and other scenarios. It may partly explain miscommunications and misinterpretations of scientific studies in journalism.11"
  }, {
    "heading": "5 Looking ahead",
    "text": "Our observations regarding the annotation results raise questions regarding what is a generalizable way to define strength differences, how to use the labels that we collected, and how to collect labels in the future. We believe that this corpus of sentence-level revisions, together with the labels and comments from participants, can provide insights into better ways to approach this problem and help further understand strength of statements.\nOne interesting direction that this enables is a potentially new kind of learning problem. The comments indicate features that humans think salient. Is it possible to automatically learn new features from the comments?\nThe ultimate goal of our study is to understand the effects of statement strength on the public, which can lead to various applications in public communication."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank J. Baldridge, J. Boyd-Graber, C. Callison-Burch, and the reviewers for helpful comments; P. Ginsparg for providing data; and S. Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park, K. Raman, M. Reitblatt, S. Roy, A. Sharma, R. Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yang and the anonymous annotators for all their labeling help. This work was supported in part by NSF grant IIS-0910664 and a Google Research Grant.\n11http://www.phdcomics.com/comics/ archive.php?comicid=1174"
  }],
  "year": 2014,
  "references": [{
    "title": "Sentence alignment for monolingual comparable corpora",
    "authors": ["Regina Barzilay", "Noemie Elhadad."],
    "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 25–",
    "year": 2003
  }, {
    "title": "Trivial persuasion in the courtroom: The power of (a few) minor details",
    "authors": ["Brad E Bell", "Elizabeth F Loftus."],
    "venue": "Journal of Personality and Social Psychology, 56(5):669.",
    "year": 1989
  }, {
    "title": "User Edits Classification Using Document Revision Histories",
    "authors": ["Amit Bronner", "Christof Monz."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356–366.",
    "year": 2012
  }, {
    "title": "Hedge detection as a lens on framing in the GMO debates: A position paper",
    "authors": ["Eunsol Choi", "Chenhao Tan", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil", "Jennifer Spindel."],
    "venue": "Proceedings of the Workshop on Extra-Propositional Aspects of",
    "year": 2012
  }, {
    "title": "A Corpus-Based Study of Edit Categories in Featured and Non-Featured Wikipedia Articles",
    "authors": ["Johannes Daxenberger", "Iryna Gurevych."],
    "venue": "COLING, pages 711–726.",
    "year": 2012
  }, {
    "title": "Automatically Classifying Edit Categories in Wikipedia Revisions",
    "authors": ["Johannes Daxenberger", "Iryna Gurevych."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "The CoNLL2010 shared task: Learning to detect hedges and their scope in natural language text",
    "authors": ["Richárd Farkas", "Veronika Vincze", "György Móra", "János Csirik", "György Szarvas."],
    "venue": "CoNLL— Shared Task, pages 1–12.",
    "year": 2010
  }, {
    "title": "Hedging in scientific research articles",
    "authors": ["Ken Hyland."],
    "venue": "John Benjamins Pub. Co., Amsterdam; Philadelphia.",
    "year": 1998
  }, {
    "title": "How to Write Your First Paper",
    "authors": ["Steven G. Krantz."],
    "venue": "Notices of the AMS.",
    "year": 2007
  }, {
    "title": "The Measurement of Observer Agreement for Categorical Data",
    "authors": ["J. Richard Landis", "Gary G. Koch."],
    "venue": "Biometrics, 33(1):159–174.",
    "year": 1977
  }, {
    "title": "Hedging: Form and function in scientific research texts",
    "authors": ["Beverly A. Lewin."],
    "venue": "Genre Studies in English for Academic Purposes, volume 9, pages 89–108. Universitat Jaume I.",
    "year": 1998
  }, {
    "title": "Mining Naturally-occurring Corrections and Paraphrases from Wikipedia’s Revision History",
    "authors": ["Aurlien Max", "Guillaume Wisniewski."],
    "venue": "Proceedings of The seventh international conference on Language Resources and Evaluation.",
    "year": 2010
  }, {
    "title": "Wikipedia Vandalism Detection",
    "authors": ["Santiago M Mola-Velasco."],
    "venue": "Proceedings of the 20th International Conference Companion on World Wide Web, pages 391–396.",
    "year": 2011
  }, {
    "title": "Writing biology: Texts in the social construction of scientific knowledge",
    "authors": ["Greg Myers."],
    "venue": "University of Wisconsin Press, Madison, Wis.",
    "year": 1990
  }, {
    "title": "How people write together [groupware",
    "authors": ["Ilona R Posner", "Ronald M Baecker."],
    "venue": "System Sciences, 1992. Proceedings of the Twenty-Fifth Hawaii International Conference on, pages 127– 138.",
    "year": 1992
  }, {
    "title": "Automatic Vandalism Detection in Wikipedia",
    "authors": ["Martin Potthast", "Benno Stein", "Robert Gerling."],
    "venue": "Advances in Information Retrieval, pages 663–668. Springer Berlin Heidelberg.",
    "year": 2008
  }, {
    "title": "Scientific discourse and contrastive linguistics: hedging",
    "authors": ["Françoise Salager-Meyer."],
    "venue": "European Science Editing, 37(2):35–37.",
    "year": 2011
  }, {
    "title": "Mining Wikipedia Revision Histories for Improving Sentence Compression",
    "authors": ["Elif Yamangil", "Rani Nelken."],
    "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technolo-",
    "year": 2008
  }, {
    "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia",
    "authors": ["Mark Yatskar", "Bo Pang", "Cristian Danescu-NiculescuMizil", "Lillian Lee."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North",
    "year": 2010
  }, {
    "title": "Expanding textual entailment corpora from Wikipedia using co-training",
    "authors": ["Fabio Massimo Zanzotto", "Marco Pennacchiotti."],
    "venue": "Proceedings of the 2nd Workshop on Collaboratively Constructed Semantic Resources.",
    "year": 2010
  }, {
    "title": "Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History",
    "authors": ["Torsten Zesch."],
    "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529–538.",
    "year": 2012
  }],
  "id": "SP:1e295f4e195cf3d63a90ce85ca7ce29e0b42d8b7",
  "authors": [{
    "name": "Chenhao Tan",
    "affiliations": []
  }, {
    "name": "Lillian Lee",
    "affiliations": []
  }],
  "abstractText": "The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task.",
  "title": "A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication"
}