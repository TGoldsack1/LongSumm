{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1092–1102 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Multimedia data (including text, image, audio and video) have increased dramatically recently, which makes it difficult for users to obtain important information efficiently. Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watching videos from beginning to end.\n1http://www.nlpr.ia.ac.cn/cip/jjzhang.htm\nThe existing applications related to MMS include meeting record summarization (Erol et al., 2003; Gross et al., 2000), sport video summarization (Tjondronegoro et al., 2011; Hasan et al., 2013), movie summarization (Evangelopoulos et al., 2013; Mademlis et al., 2016), pictorial storyline summarization (Wang et al., 2012), timeline summarization (Wang et al., 2016b) and social multimedia summarization (Del Fabro et al., 2012; Bian et al., 2013; Schinas et al., 2015; Bian et al., 2015; Shah et al., 2015, 2016). When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions. For the summarization of pictorial storylines, the input is a set of images with text descriptions. None of these applications focus on summarizing multimedia data that contain asynchronous information about general topics.\nIn this paper, as shown in Figure 1, we propose an approach to a generate textual summary from a set of asynchronous documents, images, audios and videos on the same topic.\nSince multimedia data are heterogeneous and contain more complex information than pure text does, MMS faces a great challenge in addressing the semantic gap between different modalities. The framework of our method is shown in Figure 1. For the audio information contained in videos, we obtain speech transcriptions through Automatic Speech Recognition (ASR) and design a method to use these transcriptions selectively. For visual information, including the key-frames extracted from videos and the images that appear in documents, we learn the joint representations of texts and images by using a neural network; we then can identify the text that is relevant to the image. In this way, audio and visual information can be integrated into a textual summary.\nTraditional document summarization involves two essential aspects: (1) Salience: the summa-\n1092\nry should retain significant content of the input documents. (2) Non-redundancy: the summary should contain as little redundant content as possible. For MMS, we consider two additional aspects: (3) Readability: because speech transcriptions are occasionally ill-formed, we should try to get rid of the errors introduced by ASR. For example, when a transcription provides similar information to a sentence in documents, we should prefer the sentence to the transcription presented in the summary. (4) Coverage for the visual information: images that appear in documents and videos often capture event highlights that are usually very important. Thus, the summary should cover as much of the important visual information as possible. All of the aspects can be jointly optimized by the budgeted maximization of submodular functions (Khuller et al., 1999).\nOur main contributions are as follows:\n• We design an MMS method that can automatically generate a textual summary from a set of asynchronous documents, images, audios and videos related to a specific topic.\n• To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions.\n• We introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Multi-document Summarization",
    "text": "Multi-document summarization (MDS) attempts to extract important information for a set of documents related to a topic to generate a short sum-\nmary. Graph based methods (Mihalcea and Tarau, 2004; Wan and Yang, 2006; Zhang et al., 2016) are commonly used. LexRank (Erkan and Radev, 2011) first builds a graph of the documents, in which each node represents a sentence and the edges represent the relationship between sentences. Then, the importance of each sentence is computed through an iterative random walk."
  }, {
    "heading": "2.2 Multi-modal Summarization",
    "text": "In recent years, much work has been done to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia.\nErol et al. (2003) aim to create important segments of a meeting recording based on audio, text and visual activity analysis. Tjondronegoro et al. (2011) propose a way to summarize a sporting event by analyzing the textual information extracted from multiple resources and identifying the important content in a sport video. Evangelopoulos et al. (2013) use an attention mechanism to detect salient events in a movie. Wang et al. (2012) and Wang et al. (2016b) use image-text pairs to generate a pictorial storyline and timeline summarization. Li et al. (2016) develop an approach for multimedia news summarization for searching results on the Internet, in which the hLDA model is introduced to discover the topic structure of the news documents. Then, a news article and an image are chosen to represent each topic. For social media summarization, Fabro et al. (2012) and Schinas et al. (2015) propose to summarize the real-life events based on multimedia content such as photos from Flickr and videos from YouTube. Bian et al. (2013; 2015) propose a multimodal LDA to detect topics by capturing the correlations between textual and visual features of microblogs with embedded images. The output of their method is a set of representative images that describe the events. Shah et al. (2015; 2016) introduce EventBuilder\nwhich produces text summaries for a social event leveraging Wikipedia and visualizes the event with social media activities.\nMost of the above studies focus on synchronous multi-modal content, i.e., in which images are paired with text descriptions and videos are paired with subtitles. In contrast, we perform summarization from asynchronous (i.e., there is no given description for images and no subtitle for videos) multi-modal information about news topics, including multiple documents, images and videos, to generate a fixed length textual summary. This task is both more general and more challenging."
  }, {
    "heading": "3 Our Model",
    "text": ""
  }, {
    "heading": "3.1 Problem Formulation",
    "text": "The input is a collection of multi-modal dataM = {D1, ..., D|D|, V1, ..., V|V |} related to a news topic T , where each document Di = {Ti, Ii} consists of text Ti and image Ii (there may be no image for some documents). Vi denotes video. | · | denotes the cardinality of a set. The objective of our work is to automatically generate textual summary to represent the principle content ofM."
  }, {
    "heading": "3.2 Model Overview",
    "text": "There are many essential aspects in generating a good textual summary for multi-modal data. The salient content in documents should be retained, and the key facts in videos and images should be covered. Further, the summary should be readable and non-redundant and should follow the fixed length constraint. We propose an extraction-based method in which all these aspects can be jointly optimized by the budgeted maximization of submodular functions defined as follows:\nmax S⊆T {F(S) : ∑ s∈S ls ≤ L} (1)\nwhere T is the set of sentences, S is the summary, ls is length (number of words) of sentence s, L is budget, i.e., length constraint for the summary, and submodular function F(S) is the summary score related to the above-mentioned aspects.\nText is the main modality of documents, and in some cases, images are embedded in documents. Videos consist of at least two types of modalities: audio and visual. Next, we give overall processing methods for different modalities.\nAudio, i.e., speech, can be automatically transcribed into text by using an ASR system2. Then, we can leverage a graph-based method to calculate the salience score for all of the speech transcriptions and for the original sentences in documents. Note that speech transcriptions are often ill-formed; thus, to improve the readability, we should try to avoid the errors introduced by ASR. In addition, audio features including acoustic confidence (Valenza et al., 1999), audio power (Christel et al., 1998) and audio magnitude (Dagtas and Abdel-Mottaleb, 2001) have proved to be helpful for speech and video summarization which will benefit our method.\nFor visual, which is actually a sequence of images (frames), because most of the neighboring frames contain redundant information, we first extract the most meaningful frames, i.e., the keyframes, which can provide the key facts for the whole video. Then, it is necessary to perform semantic analysis between text and visual. To this end, we learn the joint representations for textual and visual modalities and can then identify the sentence that is relevant to the image. In this way, we can guarantee the coverage of generated summary for the visual information."
  }, {
    "heading": "3.3 Salience for Text",
    "text": "We apply a graph-based LexRank algorithm (Erkan and Radev, 2011) to calculate salience score of the text unit, including the sentences in documents and the speech transcriptions from videos. LexRank first constructs a graph based on the text units and their relationship and then conducts an iteratively random walk to calculate the salience score of the text unit, sa(ti), until convergence using the following equation:\nSa(ti) = µ ∑\nj\nSa(tj) ·Mji + 1− µ N (2)\nwhere µ is the damping factor that is set to 0.85. N is the total number of the text units. Mji is the relationship between text unit ti and tj , which is computed as follows:\nMji = sim(tj , ti) (3)\nThe text unit ti is represented by averaging the embeddings of the words (except stop-words) in ti. sim(·) denotes cosine similarity between two texts (negative similarities are replaced with 0).\n2We use IBM Watson Speech to Text service: www.ibm.com/watson/developercloud/speech-to-text.html\nFor MMS task, we propose two guidance strategies to amend the affinity matrix M and calculate salience score of the text as shown in Figure 2."
  }, {
    "heading": "3.3.1 Readability Guidance Strategies",
    "text": "The random walk process can be understood as a recommendation: Mji in Equation 2 denotes that tj will recommend ti to the degree of Mji. The affinity matrix M in the LexRank model is symmetric, which means Mij = Mji. In contrast, for MMS, considering the unsatisfactory quality of speech recognition, symmetric affinity matrices are inappropriate. Specifically, to improve the readability, for a speech transcription, if there is a sentence in document that is related to this transcription, we would prefer to assign the text sentence a higher salience score than that assigned to the transcribed one. To this end, the process of a random walk should be guided to control the recommendation direction: when a document sentence is related to a speech transcription, the symmetric weighted edge between them should be transformed into a unidirectional edge, in which we invalidate the direction from document sentence to the transcribed one. In this way, speech transcriptions will not be recommended by the corresponding document sentences. Important speech transcriptions that cannot be covered by documents still have the chance to obtain high salience scores. For the pair of a sentence ti and a speech transcription tj , Mij is computed as follows:\nMij = {\n0, if sim(ti, tj) > Ttext sim(ti, tj), otherwise\n(4) where threshold Ttext is used to determine whether a sentence is related to others. We obtain the proper semantic similarity threshold by testing on Microsoft Research Paraphrase (MSRParaphrase) dataset (Quirk et al., 2004). It is a publicly avail-\nable paraphrase corpus that consists of 5801 pairs of sentences, of which 3900 pairs are semantically equivalent."
  }, {
    "heading": "3.3.2 Audio Guidance Strategies",
    "text": "Some audio features can guide the summarization system to select more important and readable speech transcriptions. Valenza et al. (1999) use acoustic confidence to obtain accurate and readable summaries of broadcast news programs. Christel et al. (1998) and Dagtas and AbdelMottaleb (2001) apply audio power and audio magnitude to find significant audio events. In our work, we first balance these three feature scores for each speech transcription by dividing their respective maximum values among the whole amount of audio, and we then average these scores to obtain the final audio score for speech transcription. For each adjacent speech transcription pair (tk, tk′ ), if the audio score a(tk) for tk is smaller than a certain threshold while a(tk′ ) is greater, which means that tk′ is more important and readable than tk, then tk should recommend tk′ , but tk′ should not recommend tk. We formulate it as follows: {\nMkk′ = sim(tk, tk′ ) Mk′k = 0\nif a(tk) < Taudio and a(tk′ ) > Taudio (5)\nwhere the threshold Taudio is the average audio score for all the transcriptions in the audio.\nFinally, affinity matrices are normalized so that each row adds up to 1."
  }, {
    "heading": "3.4 Text-Image Matching",
    "text": "The key-frames contained in videos and the images embedded in documents often captures news highlights in which the important ones should be covered by the textual summary. Before measuring the coverage for images, we should train the model to bridge the gap between text and image, i.e., to match the text and image.\nWe start by extracting key-frames of videos based on shot boundary detection. A shot is defined as an unbroken sequence of frames. The abrupt transition of RGB histogram features often indicates shot boundaries (Zhuang et al., 1998). Specifically, when the transition of the RGB histogram feature for adjacent frames is greater than a certain ratio3 of the average transition for the whole video, we segment the shot. Then, the frames\n3The ratio is determined by testing on the\nin the middle of each shot are extracted as keyframes. These key-frames and images in documents make up the image set that the summary should cover.\nNext, it is necessary to perform a semantic analysis between the text and the image. To this end, we learn the joint representations for textual and visual modalities by using a model trained on the Flickr30K dataset (Young et al., 2014), which contains 31,783 photographs of everyday activities, events and scenes harvested from Flickr. Each photograph is manually labeled with 5 textual descriptions. We apply the framework of Wang et al. (2016a), which achieves state-of-the-art performance for text-image matching task on the Flickr30K dataset. The image is encoded by the VGG model (Simonyan and Zisserman, 2014) that has been trained on the ImageNet classification task following the standard procedure (Wang et al., 2016a). The 4096-dimensional feature from the pre-softmax layer is used to represent the image. The text is first encoded by the Hybrid GaussianLaplacian mixture model (HGLMM) using the method of Klein et al. (2014). Then, the HGLMM vectors are reduced to 6000 dimensions through PCA. Next, the sentence vector vs and image vector vi are mapped to a joint space by a two-branch neural network as follows:{\nx = W2 · f(W1 · vs + bs) y = V2 · f(V1 · vi + bi) (6)\nwhere W1 ∈ R2048×6000, bs ∈ R2048, W2 ∈ R512×2048, V1 ∈ R2048×4096, bi ∈ R2048, V2 ∈ R512×2048, f is Rectified Linear Unit (ReLU).\nThe max-margin learning framework is applied to optimize the neural network as follows:\nL = ∑ i,k max[0,m+ s(xi, yi)− s(xi, yk)]\n+ λ1 ∑ i,k max[0,m+ s(xi, yi)− s(xk, yi)] (7)\nwhere for positive text-image pair (xi, yi), the top K most violated negative pairs (xi, yk) and (xk, yi) in each mini-batch are sampled. The objective function L favors higher matching score s(xi, yi) (cosine similarity) for positive text-image pairs than for negative pairs4.\nshot detection dataset of TRECVID. http://wwwnlpir.nist.gov/projects/trecvid/\n4In the experiments, K = 50, m = 0.1 and λ1 = 2. Wang et al. (2016a) also proved that structure-preserving constraints can make 1% Recall@1 improvement.\nNote that the images in Flickr30K are similar to our task. However, the image descriptions are much simpler than the text in news, so the model trained on Flickr30K cannot be directly used for our task. For example, some of the information contained in the news, such as the time and location of events, cannot be directly reflected by images. To solve this problem, we simplify each sentence and speech transcription based on semantic role labelling (Gildea and Jurafsky, 2002), in which each predicate indicates an event and the arguments express the relevant information of this event. ARG0 denotes the agent of the event, and ARG1 denotes the action. The assumption is that the concepts including agent, predicate and action compose the body of the event, so we extract “ARG0+predicate+ARG1” as the simplified sentence that is used to match the images. It is worth noting that there may be multiple predicateargument structures for one sentence and we extract all of them.\nAfter the text-image matching model is trained and the sentences are simplified, for each textimage pair (Ti, Ij) in our task, we can identify the matched pairs if the score s(Ti, Ij) is greater than a threshold Tmatch. We set the threshold as the average matching score for the positive text-image pair in Flickr30K, although the matching performance for our task could in principle be improved by adjusting this parameter."
  }, {
    "heading": "3.5 Multi-modal Summarization",
    "text": "We model the salience of a summary S as the sum of salience scores Sa(ti)5 of the sentence ti in the summary, combining a λ-weighted redundancy penalty term:\nFs(S) = ∑ ti∈S Sa(ti)− λs|S| ∑ ti,tj∈S sim(ti, tj) (8)\nWe model the summary S coverage for the image set I as the weighted sum of image covered by the summary:\nFc(S) = ∑ pi∈I Im(pi)bi (9)\nwhere the weight Im(pi) for the image pi is the length ratio between the shot pi and the whole videos. bi is a binary variable to indicate\n5Normalized by the maximum value among all the sentences.\nwhether an image pi is covered by the summary, i.e., whether there is at least one sentence in the summary matching the image.\nFinally, considering all the modalities, the objective function is defined as follows:\nFm(S) = 1 Ms ∑ ti∈S Sa(ti) + 1 Mc ∑ pi∈I Im(pi)bi\n− λm|S| ∑ i,j∈S sim(ti, tj)\n(10) where Ms is the summary score obtained by Equation 8 and Mc is the summary score obtained by Equation 9. The aim of Ms and Mc is to balance the aspects of salience and coverage for images. λs, and λm are determined by testing on development set. Note that to guaranteed monotone of F , λs, and λm should be lower than the minimum salience score of sentences. To further improve non-redundancy, we make sure that similarity between any pair of sentences in the summary is lower than Ttext.\nEquations 8,9 and 10 are all monotone submodular functions under the budget constraint. Thus, we apply the greedy algorithm (Lin and Bilmes, 2010) guaranteeing near-optimization to solve the problem."
  }, {
    "heading": "4 Experiment",
    "text": ""
  }, {
    "heading": "4.1 Dataset",
    "text": "There is no benchmark dataset for MMS. We construct a dataset as follows. We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese. We set 5 topics for each language as a development set. For each topic, we collect 20 documents within the same period using Google News search6 and 5-10 videos in CCTV.com7 and Youtube8. More details of the corpus are illustrated in Table 1. Some examples of news topics are provided Table 2.\nWe employ 10 graduate students to write reference summaries after reading documents and watching videos on the same topic. We keep 3 reference summaries for each topic. The criteria for summarizing documents lie in: (1) retaining important content of the input documents and videos; (2) avoiding redundant information; (3) having a\n6http://news.google.com/ 7http://www.cctv.com/ 8https://www.youtube.com/\ngood readability; (4) following the length limit. We set the length constraint for each English and Chinese summary to 300 words and 500 characters, respectively."
  }, {
    "heading": "4.2 Comparative Methods",
    "text": "Several models are compared in our experiments, including generating summaries with different modalities and different approaches to leverage images.\nText only. This model generates summaries only using the text in documents.\nText + audio. This model generates summaries using the text in documents and the speech transcriptions but without guidance strategies.\nText + audio + guide. This model generates summaries using the text in documents and the speech transcriptions with guidance strategies.\nThe following models generate summaries using both documents and videos but take advantage of images in different ways. The salience scores for text are obtained with guidance strategies.\nImage caption. The image is first captioned using the model of Vinyals et al. (2016) which achieved first place in the 2015 MSCOCO Image Captioning Challenge. This model generates summaries using text in documents, speech transcription and image captions.\nNote that the above-mentioned methods generate summaries by using Equation 8 and the follow-\ning methods using Equation 8 ,9 and 10. Image caption match. This model uses generated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold Ttext, the image and the sentence match.\nImage alignment. The images are aligned to the text in the following ways: The images in a document are aligned to all the sentences in this document and the key-frames in a shot are aligned to all the speech transcriptions in this shot.\nImage match. The texts are matched with images using the approach introduced in Section 3.4."
  }, {
    "heading": "4.3 Implementation Details",
    "text": "We perform sentence9 and word tokenization, and all the Chinese sentences are segmented by Stanford Chinese Word Segmenter (Tseng et al., 2005). We apply Stanford CoreNLP toolkit (Levy and D. Manning, 2003; Klein and D. Manning, 2003) to perform lexical parsing and use semantic role labelling approach proposed by Yang and Zong (2014). We use 300-dimension skipgram English word embeddings which are publicly available10. Given that text-image matching model and image caption generation model are trained in English, to create summaries in Chinese, we first translate the Chinese text into English via Google Translation11 and then conduct text and image matching."
  }, {
    "heading": "4.4 Multi-modal Summarization Evaluation",
    "text": "We use the ROUGE-1.5.5 toolkit (Lin and Hovy, 2003) to evaluate the output summaries. This evaluation metric measures the summary quality by matching n-grams between generated summary and reference summary. Table 3 and Table 4 show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regarding to the three reference summaries for each topic in English and Chinese.\nFor the results of the English MMS, from the first three lines in Table 3 we can see that when summarizing without visual information, the method with guidance strategies performs slightly better than do the first two methods. Because Rouge mainly measures word overlaps, manual evaluation is needed to confirm the impact of guidance strategies on improving readability. It is in-\n9We exclude sentences containing less than 5 words. 10https://code.google.com/archive/p/word2vec/ 11https://translate.google.com\ntroduced in Section 4.5. The rating ranges from 1 (the poorest) to 5 (the best). When summarizing with textual and visual modalities, performances are not always improved, which indicates that the models of image caption, image caption match and image alignment are not suitable to MMS. The image match model has a significant advantage over other comparative methods, which illustrates that it can make use of multi-modal information.\nTable 4 shows the Chinese MMS results, which are similar to the English results that the image match model achieves the best performance. We find that the performance enhancement for the image match model is smaller in Chinese than it is in English, which may be due to the errors introduced by machine translation.\nWe provides a generated summary in English using the image match model, which is shown in Figure 3."
  }, {
    "heading": "4.5 Manual Summary Quality Evaluation",
    "text": "The readability and informativeness for summaries are difficult to evaluate formally. We ask five graduate students to measure the quality of summaries generated by different methods. We calculate the average score for all of the topics, and the results are displayed in Table 5. Overall, our method with guidance strategies achieves higher scores than do the other methods, but it is still obviously poorer than the reference sum-\nmaries. Specifically, when speech transcriptions are not considered, the informativeness of the summary is the worst. However, adding speech transcriptions without guidance strategies decreases readability to a large extent, which indicates that guidance strategies are necessary for MMS. The image match model achieves higher informativeness scores than do the other methods without using images.\nWe give two instances of readability guidance that arise between document text (DT) and speech transcriptions (ST) in Table 6. The errors introduced by ASR include segmentation (instance A) and recognition (instance B) mistakes."
  }, {
    "heading": "4.6 How Much is the Image Worth",
    "text": "Text-image matching is the toughest module for our framework. Although we use a state-of-the-art approach to match the text and images, the performance is far from satisfactory. To find a somewhat strong upper-bound of the task, we choose five topics for each language to manually label the text-image matching pairs. The MMS results on these topics are shown in Table 7 and Table 8. The experiments show that with the ground truth textimage matching result, the summary quality can be promoted to a considerable extent, which indicates visual information is crucial for MMS.\nAn image and the corresponding texts obtained using different methods are given in Figure 4 an d Figure 5. We can conclude that the image caption\nand the image caption match contain little of the image’s intrinsically intended information. The image alignment introduces more noise because it is possible that the whole text in documents or the speech transcriptions in shot are aligned to the document images or the key-frames, respectively. The image match can obtain similar results to the image manually match, which illustrates that the image match can make use of visual information to generate summaries."
  }, {
    "heading": "5 Conclusion",
    "text": "This paper addresses an asynchronous MMS task, namely, how to use related text, audio and video information to generate a textual summary. We formulate the MMS task as an optimization problem with a budgeted maximization of submodular functions. To selectively use the transcription of audio, guidance strategies are designed using the graph model to effectively calculate the salience score for each text unit, leading to more readable and informative summaries. We investigate various approaches to identify the relevance between the image and texts, and find that the image match model performs best. The final experimental results obtained using our MMS corpus in both English and Chinese demonstrate that our system can benefit from multi-modal information.\nAdding audio and video does not seem to improve dramatically over text only model, which indicates that better models are needed to capture the interactions between text and other modalities, especially for visual. We also plan to enlarge our MMS dataset, specifically to collect more videos."
  }, {
    "heading": "Acknowledgments",
    "text": "The research work has been supported by the Natural Science Foundation of China under Grant No. 61333018 and No. 61403379."
  }],
  "year": 2017,
  "references": [{
    "title": "Multimedia summarization for trending topics in microblogs",
    "authors": ["References Jingwen Bian", "Yang Yang", "Tat-Seng Chua."],
    "venue": "Proceedings of the 22nd ACM international conference on Conference on information &",
    "year": 2013
  }, {
    "title": "Multimedia summarization for social events in microblog stream",
    "authors": ["Jingwen Bian", "Yang Yang", "Hanwang Zhang", "TatSeng Chua."],
    "venue": "IEEE Transactions on Multimedia, 17(2):216–228.",
    "year": 2015
  }, {
    "title": "Evolving video skims into useful multimedia abstractions",
    "authors": ["Michael G Christel", "Michael A Smith", "C Roy Taylor", "David B Winkler."],
    "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems, pages 171–178. ACM",
    "year": 1998
  }, {
    "title": "Extraction of tv highlights using multimedia features",
    "authors": ["Serhan Dagtas", "Mohamed Abdel-Mottaleb."],
    "venue": "Multimedia Signal Processing, 2001 IEEE Fourth Workshop on, pages 91–96. IEEE.",
    "year": 2001
  }, {
    "title": "Summarization of real-life events based on community-contributed content",
    "authors": ["Manfred Del Fabro", "Anita Sobe", "Laszlo Böszörmenyi."],
    "venue": "The Fourth International Conferences on Advances in Multimedia, pages 119–126.",
    "year": 2012
  }, {
    "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
    "authors": ["Gunes Erkan", "Dragomir R. Radev."],
    "venue": "Journal of Qiqihar Junior Teachers College, 22:2004.",
    "year": 2011
  }, {
    "title": "Multimodal summarization of meeting recordings",
    "authors": ["Berna Erol", "D-S Lee", "Jonathan Hull."],
    "venue": "Multimedia and Expo, 2003. ICME’03. Proceedings. 2003 International Conference on, volume 3, pages III–25. IEEE.",
    "year": 2003
  }, {
    "title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textu",
    "authors": ["Georgios Evangelopoulos", "Athanasia Zlatintsi", "Alexandros Potamianos", "Petros Maragos", "Konstantinos Rapantzikos", "Georgios Skoumas", "Yannis Avrithis"],
    "year": 2013
  }, {
    "title": "Automatic labeling of semantic roles",
    "authors": ["Daniel Gildea", "Daniel Jurafsky."],
    "venue": "Computational Linguistics, Volume 28, Number 3, September 2002.",
    "year": 2002
  }, {
    "title": "Towards a multimodal meeting record",
    "authors": ["Ralph Gross", "Michael Bett", "Hua Yu", "Xiaojin Zhu", "Yue Pan", "Jie Yang", "Alex Waibel."],
    "venue": "Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference on, volume 3, pages 1593–1596. IEEE.",
    "year": 2000
  }, {
    "title": "Multi-modal highlight generation for sports videos using an informationtheoretic excitability measure",
    "authors": ["Taufiq Hasan", "Hynek Bořil", "Abhijeet Sangwan", "John HL Hansen."],
    "venue": "EURASIP Journal on Advances in Signal Processing, 2013(1):173.",
    "year": 2013
  }, {
    "title": "The budgeted maximum coverage problem",
    "authors": ["Samir Khuller", "Anna Moss", "Joseph Seffi Naor."],
    "venue": "Information Processing Letters, 70(1):39–45.",
    "year": 1999
  }, {
    "title": "Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation",
    "authors": ["Benjamin Klein", "Guy Lev", "Gil Sadeh", "Lior Wolf."],
    "venue": "arXiv preprint arXiv:1411.7399.",
    "year": 2014
  }, {
    "title": "Accurate unlexicalized parsing",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "Is it harder to parse chinese, or the chinese treebank",
    "authors": ["Roger Levy", "Christopher D. Manning"],
    "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
    "year": 2003
  }, {
    "title": "Multimedia news summarization in search",
    "authors": ["Zechao Li", "Jinhui Tang", "Xueming Wang", "Jing Liu", "Hanqing Lu."],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 7(3):33.",
    "year": 2016
  }, {
    "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
    "authors": ["Chin-Yew Lin", "Eduard Hovy."],
    "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational",
    "year": 2003
  }, {
    "title": "Multi-document summarization via budgeted maximization of submodular functions",
    "authors": ["Hui Lin", "Jeff Bilmes."],
    "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2010
  }, {
    "title": "Multimodal stereoscopic movie summarization conforming to narrative characteristics",
    "authors": ["Ioannis Mademlis", "Anastasios Tefas", "Nikos Nikolaidis", "Ioannis Pitas."],
    "venue": "IEEE Transactions on Image Processing, 25(12):5828–5840.",
    "year": 2016
  }, {
    "title": "Multimodal graph-based event detection and summarization in social media streams",
    "authors": ["Manos Schinas", "Symeon Papadopoulos", "Georgios Petkos", "Yiannis Kompatsiaris", "Pericles A Mitkas."],
    "venue": "Proceedings of the 23rd ACM international confer-",
    "year": 2015
  }, {
    "title": "Eventbuilder: Real-time multimedia event",
    "authors": ["Rajiv Ratn Shah", "Anwar Dilawar Shaikh", "Yi Yu", "Wenjing Geng", "Roger Zimmermann", "Gangshan Wu"],
    "year": 2015
  }, {
    "title": "Leveraging multimodal information for event summarization and concept-level sentiment analysis",
    "authors": ["Rajiv Ratn Shah", "Yi Yu", "Akshay Verma", "Suhua Tang", "Anwar Dilawar Shaikh", "Roger Zimmermann."],
    "venue": "Knowledge-Based Systems, 108:102–109.",
    "year": 2016
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["Karen Simonyan", "Andrew Zisserman."],
    "venue": "arXiv preprint arXiv:1409.1556.",
    "year": 2014
  }, {
    "title": "Multi-modal summarization of key events and top players in sports tournament videos",
    "authors": ["Dian Tjondronegoro", "Xiaohui Tao", "Johannes Sasongko", "Cher Han Lau."],
    "venue": "Applications of Computer Vision (WACV), 2011 IEEE Workshop on, pages 471–478.",
    "year": 2011
  }, {
    "title": "A conditional random field word segmenter",
    "authors": ["H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning"],
    "year": 2005
  }, {
    "title": "Summarisation of spoken audio through information extraction",
    "authors": ["Robin Valenza", "Tony Robinson", "Marianne Hickey", "Roger Tucker."],
    "venue": "ESCA Tutorial and Research Workshop (ETRW) on Accessing Information in Spoken Audio.",
    "year": 1999
  }, {
    "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
    "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."],
    "venue": "IEEE transactions on pattern analysis and machine intelligence.",
    "year": 2016
  }, {
    "title": "Improved affinity graph based multi-document summarization",
    "authors": ["Xiaojun Wan", "Jianwu Yang."],
    "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers.",
    "year": 2006
  }, {
    "title": "Generating pictorial storylines via minimum-weight connected dominating set approximation in multiview graphs",
    "authors": ["Dingding Wang", "Tao Li", "Mitsunori Ogihara."],
    "venue": "AAAI.",
    "year": 2012
  }, {
    "title": "Learning deep structure-preserving image-text embeddings",
    "authors": ["Liwei Wang", "Yin Li", "Svetlana Lazebnik."],
    "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5005–5013.",
    "year": 2016
  }, {
    "title": "A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization",
    "authors": ["Yang William Wang", "Yashar Mehdad", "R. Dragomir Radev", "Amanda Stent."],
    "venue": "Proceedings of the 2016 Conference of",
    "year": 2016
  }, {
    "title": "Multipredicate semantic role labeling",
    "authors": ["Haitong Yang", "Chengqing Zong."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363–373. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "From image descriptions to visual denotations",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the Association of Computational Linguistics, 2:67–78.",
    "year": 2014
  }, {
    "title": "Abstractive cross-language summarization via translation model enhanced predicate argument structure fus",
    "authors": ["Jiajun Zhang", "Yu Zhou", "Chengqing Zong"],
    "year": 2016
  }, {
    "title": "Adaptive key frame extraction using unsupervised clustering",
    "authors": ["Yueting Zhuang", "Yong Rui", "Thomas S Huang", "Sharad Mehrotra."],
    "venue": "Image Processing, 1998. ICIP 98. Proceedings. 1998 International Conference on, volume 1, pages 866–870.",
    "year": 1998
  }],
  "id": "SP:25e3a70f95e4c636e4ddd602dcf5a6d3318a0504",
  "authors": [{
    "name": "Haoran Li",
    "affiliations": []
  }, {
    "name": "Junnan Zhu",
    "affiliations": []
  }, {
    "name": "Cong Ma",
    "affiliations": []
  }, {
    "name": "Jiajun Zhang",
    "affiliations": []
  }, {
    "name": "Chengqing Zong",
    "affiliations": []
  }],
  "abstractText": "The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. In this work, we propose an extractive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal content. For audio information, we design an approach to selectively use its transcription. For visual information, we learn the joint representations of text and images using a neural network. Finally, all of the multimodal aspects are considered to generate the textual summary by maximizing the salience, non-redundancy, readability and coverage through the budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese, which is released to the public1. The experimental results obtained on this dataset demonstrate that our method outperforms other competitive baseline methods.",
  "title": "Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video"
}