{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2015 Conference, pages 232–236, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Speech summarization has been of great interest to the community because speech is the principal modality of human communications, and it is not as easy to skim, search or browse speech transcripts as it is for textual messages. Speech recorded from call centres offers a great opportunity to study goal-oriented and focused conversations between an agent and a caller. The Call Centre Conversation Summarization (CCCS) task consists in automatically generating summaries of spoken conversations in the form of textual synopses that shall inform on the content of a conversation and might be used for browsing a large database of recordings. Compared to news summarization where extractive approaches have been very successful, the CCCS task’s objective is to foster work on abstractive summarization in order\nto depict what happened in a conversation instead of what people actually said.\nThe track leverages conversations from the Decoda and Luna corpora of French and Italian call centre recordings, both with transcripts available in their original language as well as English translation (both manual and automatic). Recordings duration range from a few minutes to 15 minutes, involving two or sometimes more speakers. In the public transportation and help desk domains, the dialogs offer a rich range of situations (with emotions such as anger or frustration) while staying in a coherent and focused domain.\nGiven transcripts, participants to the task shall generate abstractive summaries informing a reader about the main events of the conversations, such as the objective of the caller, whether and how it was solved by the agent, and the attitude of both parties. Evaluation has been performed by comparing submissions to reference synopses written by quality assurance experts from call centres. Both conversations and reference summaries are kindly provided by the SENSEI project.\nThis paper reports on the results of the CCCS task in term ROUGE-2 evaluation metric. Two participants have submitted four systems to the task. In addition, we provide three baselines which frame the performance that would be obtained by extractive systems. The results are analysed according to language, human annotator coherence and the impact of automatic translation.\nThe remaining of the paper is organized as follows: Section 2 describes the synopsis generation task. Section 3 describes the CCCS corpus. Section 4 describes the results from the systems of the participants. Section 5 discusses future research avenues."
  }, {
    "heading": "2 Task",
    "text": "The CCCS task consists in creating systems that can analyse call centre conversations and generate\n232\nwritten summaries reflecting why the customer is calling, how the agent answers that query, what are the steps to solve the problem and what is the resolution status of the problem.\nUnlike news summarization which focuses on locating facts in text written by journalists and selecting the most relevant facts, conversation synopses require an extra level of analysis in order to achieve abstraction. Turn taking from the speakers has to be converted to generic expression of their needs, beliefs and actions. Even though extractive systems might give a glimpse of the dialogs, only abstraction can yield synopses that tell the story of what happens in the conversations.\nContrary to previous research on meeting summarization (Gillick et al., 2009; Erol et al., 2003; Lai and Renals, 2014; Wang and Cardie, 2012) (among others), we expect that the fact that conversations are focused and goal oriented will enable to foster research on more abstractive methods, such as (Murray, 2015; Mehdad et al., 2013) and deeper analysis of the conversations.\nParticipants to the CCCS task could submit system output in any of the supported languages, and could submit a maximum of three runs per language. For each conversation, they had to submit one synopsis of length 7% of the number of words of the transcript of that conversation."
  }, {
    "heading": "3 Corpus description",
    "text": "The CCCS task draws from two call centre conversation corpora, the Decoda corpus in French and the Luna corpus in Italian. Subsets from both corpora have been translated to English.\nDecoda corpus The French DECODA corpus consists in conversations between customers and one or more agent recorded in 2009 in a call centre of the public transport authority in Paris (Bechet et al., 2012). The topics of the conversations range from itinerary and schedule requests, to lost and found, to complaints (the calls were recorded during strikes). The dialogues, recorded in ecological conditions, are very spontaneous and focused on the objective of the caller. They are very challenging for Automatic Speech Recognition due to harsh acoustic conditions such as calling from mobile phones directly from the metro. For the CCCS task, manual transcripts were provided to the participants.\nWhile the original language of the conversations is French, the SENSEI project provided man-\nual translations in English by professional translators which were trained to keep the spontaneous aspects of the originals (a very challenging task according to them). 97 conversations were manually translated, on which an automatic translation system based on Moses was trained in order to produce automatic translations for the remaining of the corpus.\nThe original corpus consists of 1513 conversations (about 70h of speech). 1000 conversations have been distributed without synopses for unsupervised system training. 50 conversations were distributed with multiple synopses from up to five annotators. The test set consists of 47 manually translated conversations and corresponding synopses, and 53 automatically translated conversations and corresponding synopses. The data for training and testing is also provided in French.\nThe human written synopses are very diverse and show a high degree of abstraction from the words of the conversation with third person writing, telegraphic style and analysis of the conversations. Examples:\n• A man is calling cause he got a fine. He is waiting for a new card so he used his wife’s card. He must now write a letter asking for clemency.\n• A user wants to go to the Ambroise Paré clinic but the employee misunderstands and gives her the wrong itinerary. Luckily the employee realises her mistake and gives the passenger the right information in the end.\n• School bag lost on line 4, not found.\nLuna corpus The Italian human-human Luna corpus (Dinarelli et al., 2009) consists of 572 dialogs (≈ 26.5K turns & 30 hours of speech) in the hardware/software help desk domain, where a\nclient and an agent are engaged in a problem solving task over the phone. The dialogs are organised in transcriptions and annotations created within the FP6 LUNA project. For the CCCS shared task, manual transcriptions were used.\nWithin the FP7 SENSEI project, 100 dialogs were translated from Italian to English using professional translation services according to the methodology described in (Stepanov et al., 2014). For more accurate translations, manual transcriptions were converted to an ‘annotated’ text format, which contained mark-up for overlapping turns, fillers, pauses, noise, partial words, etc.; and translators received detailed guidelines on how to handle each phenomenon in translation. Additionally, the translators were required to translate the speech phenomena such as disfluencies as closely as possible to the source language maintaining ‘naturalness’ in the target language.\nFive native Italian speakers have annotated 200 Luna dialogs with synopses so that each dialog was processed by every annotator.1 Synopses of the 100 translated dialogs were also manually translated to English.\nThe translated and annotated dialogs were equally split into training and test sets for the CCCS task. The training dialogs were used to automatically translate additional Luna dialogs and synopses for both training and testing. Similar to the DECODA corpus, for the unsupervised training of the systems a supplementary set of 261 dialogs was automatically translated and provided to the participants without synopses. Dialogs and their associated synopses were provided both in English and Italian. The statistics for Luna manual English test set are provided in Table 2.\n1Few (2) synopses were found to address dialog dimensions other than the task and were removed."
  }, {
    "heading": "4 Results",
    "text": "Metric Evaluation is performed with the ROUGE-2 metric (Lin, 2004). ROUGE-2 is the recall in term of word bigrams between a set of reference synopses and a system submission. The ROUGE 1.5.5 toolkit was adapted to deal with a conversation-dependent length limit of 7%, had lemmatization disabled and stop-words kept, to be as language independent as possible 2. Jackknifing and resampling is used in order to compute confidence estimate intervals.\nParticipation Seven research groups had originally expressed their intention to participate to the CCCS task. Four groups downloaded the test data, and two groups actually submitted system output at the deadline. Those two groups generated four runs: NTNU:1, NTNU:2, NTNU:3, LIA-RAG:1. The technical details of these submissions are described in their own papers.\nIn addition to those four runs, we provide three baselines which serve to calibrate participant performance. The first baseline is Maximal Marginal Relevance (Baseline-MMR) (Carbonell and Goldstein, 1998) with λ = 0.7. The second baseline is the first words of the longest turn in the conversation, up to the length limit (Baseline-L). The third baseline is the words of the longest turn in the first 25% of the conversation, which usually corresponds to the description of the caller’s problem (Baseline-LB). Those baselines are described in more details in (Trione, 2014).\nIn order to estimate the overlap between human synopses, we remove each of the human synopses in turn from the reference and compute their performance as if they were systems. Across languages, 11 annotators (denoted human-1 to human-5 for IT/EN, and human-A to human-G for FR/EN) produced from 5 to 100 synopses. Note that some annotators only worked on English conversations.\nPerformance Performance of the systems is reported in Table 3. It shows that in the source languages, the extractive baselines were difficult to beat while one of the systems significantly outperformed the baselines on English (the EN test set\n2The options for running ROUGE 1.5.5 are -a -l 10000 -n 4 -x -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0\ncorresponds to the union of manual and automatic translations).\nAn analysis of the consistency of human synopsis writers is outlined in Table 4. Consistency is computed by considering in turn each of the human synopses as system output, and computing ROUGE-2 performance. Humans have much better scores than the systems, showing that they are consistent in producing the gold standard. However, human annotators suffer from a much higher performance variance than systems (for which confidence intervals are 4-5 times smaller). This partly comes from the low number of manual synopses which is greater impacted by resampling than if there were hundreds of references for each conversation. It also comes from local inconsistencies between humans on a given conversation, resulting in diverging choices in term of which information is important.\nTable 5 shows the impact of automatic translation on system performance for the English set. This experiment is hard to interpret as the set of conversations for automatic and manual transla-\ntions is different. However, it seems that processing MT results leads to better ROUGE scores, probably due to the consistency with which the MT system translates words for both conversations and synopses (reference synopses are automatic translations of source language synopses for those conversations)."
  }, {
    "heading": "5 Conclusion",
    "text": "The objective of the CCCS pilot task at Multiling’15 was to allow work on abstractive summarization of goal-oriented spoken conversations. This task involved generating synopses from French and Italian call centre recording transcripts, and English translations of those transcripts. Four systems were submitted by two participants, and obtained reasonable results but had trouble exceeding the performance of the extractive baselines.\nClearly, ROUGE evaluation is limited for abstractive summarization in that the wording of generated text might be very different from system to system, and from reference to reference, while conveying the same meaning. In addition, ROUGE does not assess fluency and readability of the summaries.\nFuture work will focus on proposing better evaluation metrics for the task, probably involving the community for manually evaluating the fluency and adequacy of the submitted system output. In addition, work will be conducted in evaluating and insuring the consistency of the human experts who create the gold standard for the task."
  }, {
    "heading": "Acknowledgments",
    "text": "The research leading to these results has received funding from the European Union - Seventh Framework Programme (FP7/2007-2013) under grant agreement n.610916 - SENSEI."
  }],
  "year": 2015,
  "references": [{
    "title": "Decoda: a call-centre human-human spoken conversation corpus",
    "authors": ["Frederic Bechet", "Benjamin Maza", "Nicolas Bigouroux", "Thierry Bazillon", "Marc El-Beze", "Renato De Mori", "Eric Arbillot."],
    "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri,",
    "year": 2012
  }, {
    "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
    "authors": ["Jaime Carbonell", "Jade Goldstein."],
    "venue": "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information",
    "year": 1998
  }, {
    "title": "Annotating spoken dialogs: from speech segments to dialog acts and frame semantics",
    "authors": ["Marco Dinarelli", "Silvia Quarteroni", "Sara Tonelli", "Alessandro Moschitti", "Giuseppe Riccardi."],
    "venue": "Proceedings of EACL Workshop on the Semantic Repre-",
    "year": 2009
  }, {
    "title": "Multimodal summarization of meeting recordings",
    "authors": ["Berna Erol", "D-S Lee", "Jonathan Hull."],
    "venue": "Multimedia and Expo, 2003. ICME’03. Proceedings. 2003 International Conference on, volume 3, pages III–25. IEEE.",
    "year": 2003
  }, {
    "title": "A global optimization framework for meeting summarization",
    "authors": ["Daniel Gillick", "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur."],
    "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on,",
    "year": 2009
  }, {
    "title": "Incorporating lexical and prosodic information at different levels for meeting summarization",
    "authors": ["Catherine Lai", "Steve Renals."],
    "venue": "Fifteenth Annual Conference of the International Speech Communication Association.",
    "year": 2014
  }, {
    "title": "Rouge: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.",
    "year": 2004
  }, {
    "title": "Abstractive meeting summarization with entailment and fusion",
    "authors": ["Yashar Mehdad", "Giuseppe Carenini", "Frank W Tompa", "Raymond T NG."],
    "venue": "Proceedings of the 14th European Workshop on Natural Language Generation, pages 136–146.",
    "year": 2013
  }, {
    "title": "Abstractive meeting summarization as a markov decision process",
    "authors": ["Gabriel Murray."],
    "venue": "Advances in Artificial Intelligence, pages 212–219. Springer.",
    "year": 2015
  }, {
    "title": "The development of the multilingual luna corpus for spoken language system porting",
    "authors": ["Evgeny A. Stepanov", "Giuseppe Riccardi", "Ali Orkan Bayer."],
    "venue": "In",
    "year": 2014
  }, {
    "title": "Mthodes par extraction pour le rsum automatique de conversations parles provenant de centres dappel",
    "authors": ["Jeremy Trione."],
    "venue": "RECITAL.",
    "year": 2014
  }, {
    "title": "Focused meeting summarization via unsupervised relation extraction",
    "authors": ["Lu Wang", "Claire Cardie."],
    "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 304–313. Association for Computational Lin-",
    "year": 2012
  }],
  "id": "SP:b12d65e7a9bc6c9bb832f42376b1863fc86dbbc5",
  "authors": [{
    "name": "Benoit Favre",
    "affiliations": []
  }, {
    "name": "Evgeny Stepanov",
    "affiliations": []
  }, {
    "name": "Jérémy Trione",
    "affiliations": []
  }, {
    "name": "Frédéric Béchet",
    "affiliations": []
  }, {
    "name": "Giuseppe Riccardi",
    "affiliations": []
  }],
  "abstractText": "This paper describes the results of the Call Centre Conversation Summarization task at Multiling’15. The CCCS task consists in generating abstractive synopses from call centre conversations between a caller and an agent. Synopses are summaries of the problem of the caller, and how it is solved by the agent. Generating them is a very challenging task given that deep analysis of the dialogs and text generation are necessary. Three languages were addressed: French, Italian and English translations of conversations from those two languages. The official evaluation metric was ROUGE-2. Two participants submitted a total of four systems which had trouble beating the extractive baselines. The datasets released for the task will allow more research on abstractive dialog summarization.",
  "title": "Call Centre Conversation Summarization: A Pilot Task at Multiling 2015"
}