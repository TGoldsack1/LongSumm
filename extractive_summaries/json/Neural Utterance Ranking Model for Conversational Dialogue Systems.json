{
  "sections": [{
    "text": "Proceedings of the SIGDIAL 2016 Conference, pages 393–403, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The study of conversational dialogue systems (also known as non-task-oriented or chat-oriented dialogue systems) has a long history. To construct such systems, rule-based methods have long been used (Weizenbaum, 1966; Colby, 1981; Wallace, 2008); however, construction and maintenance costs are very high because these rules are manually created. Moreover, intuition tells us that the performance of such systems depends on the number of established rules, though reports indicate that performance did not improve much even\nif the number of rules was doubled (Higashinaka et al., 2015b), indicating that performance of rulebased systems is limited.\nRecently, the study of statistical-based methods that use statistical processing with large volumes of web data has become increasingly active. The key benefit of this approach is that manual response creation is not necessary; thus, construction and maintenance costs are low; however, since web data contains noise, this approach has the potential to output grammatically or semantically incorrect sentences. To tackle this problem, some studies extract correct sentences as utterances for dialogue systems from web data (Inaba et al., 2014; Higashinaka et al., 2014)．These studies focus solely on extraction and do not indicate how replies are generated using extracted sentences.\nIn our study, we propose a neural utterance ranking (NUR) model that ranks candidate utterances by their suitability in a given context using neural networks. Previously, we proposed an utterance selection model (Koshinda et al., 2015) in the framework same as that of the NUR model, which ranks utterances in order of suitability to given context. In section 4, we experimentally show that the performance of the NUR model exceeds that of our previous model.\nOur proposed method processes the word sequences in utterances and utterance sequences in context via multiple recurrent neural networks (RNNs). More specifically, the RNN encodes both utterances in a given context and candidates into fixed-length vectors. Such encoding enables suitable feature extraction for ranking. Next, another RNN receives these utterance-encoded vectors in chronological order, and our proposed NUR model ranks candidates using the output of this RNN. Our model considers the order of utterances in a given context; this architecture makes it pos-\n393\nsible to handle distant semantic relationships between context and candidates."
  }, {
    "heading": "2 Related Work",
    "text": "Statistical-based response methods incorporate two major approaches.\nThe first approach is the example-based method (Murao et al., 2003), which searches a large database of previously recorded dialogue for given user input selecting an utterance identified as the most similar. Well-known dialogue systems based on this approach include Jabberwacky (De Angeli and Carpenter, 2005) which won the Loebner prize contesta (i.e., a conversational dialogue system competition) in 2005 and 2006. In addition, Banch and Li. proposed a model based on the vector space model (Banchs and Li, 2012) and Nio et al. constructed a dialogue system that uses movie scripts and Twitter data (Nio et al., 2014). A disadvantage of example-based methods is that it is difficult to consider context. If the implemented approach searches for user input with context in a database, it can be difficult to find a suitable context because of the diversity of contexts; in such cases, system replies become unsuitable. In contrast, our NUR model can select responses while also taking into account a flexible set of contexts.\nThe second statistical-based response approach is the machine translation (MT) method. Ritter et al. first introduced the MT technique into response generation (Ritter et al., 2011). They used tweetreply pairs in Twitter data, regarding a tweet as the source language sentence and the reply as a target one in MT. In other words, the MT method translates user input into system responses. More recently, response generation using neural networks has been widely studied, most work grounded in the MT method (Cho et al., 2014; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). A problem with this method is that it might generate utterances containing syntax errors; further, it tends to generate utterances with broad utility that frequently appear in training data, e.g., “I don’t know.” or “I’m OK.” (Li et al., 2016).\nOur proposed method is not categorized into either of the above two methods. Some hard-toclassify statistical-based response methods similar to our model have been proposed, e.g., Shibata et al. proposed a method that selects a suitable sentence extracted from webpages as a response\nahttp://www.loebner.net/Prizef/loebner-prize.html\nto user input (Shibata et al., 2009). Sugiyama et al. generated responses using templates and dependency structures of sentences gathered from Twitter (Sugiyama et al., 2013). There are only few common points, although most of the hardto-classify methods use not only dialogue data but also non-dialogue data such as webpages or normal tweets (not pairs of tweet reply) on Twitter."
  }, {
    "heading": "3 Neural Utterance Ranking Model",
    "text": "For our ranking model, we first define sequences of utterances from the beginning of a dialogue to a certain point of time in context c = (u1, u2, . . . , ul) Each ui(i = 1, 2, ..., l) denotes an utterance in the context, and l denotes the number of utterances. We assume here that a dialogue system and user speak alternately and last utterance ul is given by the system. We define candidate utterance list ac = (ac1, a c 2, . . . , a c m) generated depending on context c, and score tc = (tc1, t c 2, . . . , t c m). Herein, m denotes the number of candidate utterances. We define utterance ranking to sort given candidate utterance list ac in order of suitability to context c. The correct order is defined by score tc with sorting based on the model’s output yac = (y1, y2, . . . , ym) corresponding to ac.\nOur proposed utterance ranking model, i.e., the NUR model illustrated in in Figure 1, receives context c and candidate utterance list ac, then outputs yac . Details of our NUR model are described below."
  }, {
    "heading": "3.1 Utterance Encoding",
    "text": "To extract information from context and candidate utterances for suitable utterance selection, our NUR model utilizes an RNN encoder.\nPrevious work utilized an RNN encoder for MT (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015) and response generation in dialogue systems (Cho et al., 2014; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). In these studies, the encoder reads as input a variable-length word sequence and outputs a fixed-length vector. Next, another RNN decodes a given fixedlength vector, producing an objective variablelength word sequence. Therefore, the encoder has learned to embed necessary information to generate objective sentences and place them into vectors. The RNN in our model does not generate sentences using this RNN decoder approach. Results of encoding are used for features to rank candidate utterances. The RNN encoder in our NUR model has a similar architecture, but the characteristics of the output vector are profoundly different, because our model learns to extract important features for utterance ranking.\nOur model first converts word sequence w = (w1, w2, . . . , wn) in an utterance into a distributed representation of word sequence, i.e., x = (x1, x2, . . . , xn) which the RNN encoder then reads. To convert into a distributed representation here, a neural network for word embedding (as shown in Figure 1) learns via the skip-gram model (Mikolov et al., 2013). This network has two layers, i.e., an input layer that reads a one-hot-vector representing each word and a certain denominational hidden layer.\nThe RNN encoder has two networks, i.e., a forward and a backward network. The forward RNN reads x at the beginning of a sentence and outputs −→ h = ( −→ h1, −→ h2, . . . , −→ hn) correspond to input sequence. The backward RNN reads x in reverse, then outputs ←− h = ( ←− h1, ←− h2, . . . , ←− hn). By joining the outputs of these forward and backward RNNs, we acquire objective encoded utterance vector v = [ −→ hn; ←− hn]; note that [x; y]the concatenation of vectors x and y. In the following experiments, we used twolayer long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks as our RNN encoders. The effective features extracted from utterances for candidate ranking are different between the user and the system. Therefore, our\nNUR model has two RNN encoders, one for user utterances, the other for system utterances, as illustrated in Figure 1"
  }, {
    "heading": "3.2 Ranking Candidate Utterances",
    "text": "Another RNN is used to rank candidate utterances, as illustrated in 2. This RNN has two LSTM layers and two linear layers; further, we use rectified linear unit (ReLU) as the activation function. Thus, this RNN reads encoded utterance sequences and outputs scores."
  }, {
    "heading": "3.2.1 Context-Candidate Vector Sequence",
    "text": "To select suitable responses, we not only must evaluate suitability of utterances based on the last utterance in the given context, but also must consider prior dialogue. The RNN for ranking utterances in our model reads vector sequences constructed by context and candidate utterances in chronological order, then outputs scores for the candidate in relation to the context.\nThus, context-candidate vector sequence vcai is constructed using context vector sequence vc = (vu1 , vu2 , . . . , vul), with ith candidate utterance vector vai defined as follows:\nvcai =  ([vu1 ; vu2 ], [vu3 ; vu4 ], . . . , [vul ; v c ai ]), if l is odd ([0; vu1 ], [vu2 ; vu3 ], . . . , [vul ; v c ai ]),\nif l is even\nHere, 0 denotes the zero vector. Our model inputs user and system utterances at one time so that it can consider dialogue history in a given context along with the relevance between candidate utterances and the last response given by a user."
  }, {
    "heading": "3.2.2 Loss Function in Learning",
    "text": "In cases where a neural network outputs a onedimensional value, like our model, the mean\nsquared error (MSE) between training data and the model’s output is generally used as a loss function; however, our objective is not to model scores, but rather for ranking, thus we use the distance between rank data based on training data and that based on the model’s outputs as a loss function. Several methods for modeling rank data have been proposed, including the Bradley-Terry-Luce model (Bradley and Terry, 1952; Luce, 1959), the Mallows model (Mallows, 1957) and the PlackettLuce model (Plackett, 1975; Luce, 1959). In our study, to calculate ranking distance, we selected the Plackett-Luce model, which has been used in various ranking models, such as ListNet (Cao et al., 2007), BayesRank (Kuo et al., 2009), etc.\nThe Plackett-Luce model transforms a score list for ranking into a probability distribution wherein higher scores in the given list are allocated higher probabilities. Probability of score tci in score list tc = (tc1, t c 2, . . . , t c m) ranked on the top is calculated by the Plackett-Luce model as follows:\np(tci ) = exp(tci )∑m\nk=1 exp(t c k)\nUsing the same equation, the output scores of our NUR model are transformed into probability distributions. We use cross-entropy between probability distributions as our loss function."
  }, {
    "heading": "4 Experiments",
    "text": "We conducted experiments to verify the performance of ranking given candidate utterances and given contexts. For comparison, we also tested a few baseline methods."
  }, {
    "heading": "4.1 Datasets",
    "text": "For our experiments, we used dialogue data between a conversational dialogue system and a user for both training and test data. We released a conversational dialogue system called KELDIC on Twitter (screen name: @KELDIC)b. KELDIC selects an appropriate response from candidates extracted by the utterance acquisition method of (Inaba et al., 2014) using ListNet(Cao et al., 2007). The utterance acquisition method extracted suitable sentences for system utterances related to given keywords from Twitter data by filtering inappropriate sentences. Details of the response algorithm of KELDIC is further described in (Koshinda et al., 2015).\nbhttps://twitter.com/KELDIC\nWe collected training and test data by first collecting pairs of context and candidate utterances that the system used for reply on Twitter. Next, annotators evaluated the suitability of each candidate utterance in relation to the given context. Here annotators must evaluate utterances that were actually used by the system on Twitter.\nEvaluation criterion was based on the Dialogue Breakdown Detection Challenge (DBDC) (Higashinaka et al., 2016). Each system’s utterances were annotated using one of the following three breakdown labels:\n(NB) Not a breakdown It is easy to continue the conversation.\n(PB) Possible breakdown It is difficult to continue the conversation smoothly.\n(B) Breakdown It is difficult to continue the conversation.\nAnnotators evaluated dialogue data on a tool we prepared. They were first shown a context and 10 candidate utterances, including how KELDIC actually replied on Twitter, as well as labels for each candidate. We instructed them to assign at least one NB label to given candidate utterances. If there were no suitable candidates for the NB label, they could optionally add candidate utterances. If they were still not able to find a suitable response, we allowed them to skip the evaluation. We recruited annotators on crowd-sourcing site CrowdWorksc.\nIn our evaluation, we regard candidates with 50% or more annotators decided as NB as correct utterances and others as incorrect.\nWe used 1581 data points (i.e., 1581 contexts and 17533 candidate utterances), each evaluated by three or more annotators. We choose 300 data points that contained at least one correct candidate for the given test data; the remaining 1057 data points were used for training data. Table 1 shows statistics for our data.\nIn learning the model, we need scores for candidate utterances to define ranking. Score yci of candidate utterance aci is calculated as follows:\nyci = sNB nNB N + sPB nPB N + sB nB N\nN = nNB + nPB + nB chttps://crowdworks.jp\nHere, nNB, nPB and nB denote the numbers of annotators assigned as NB, PB and B, respectively, and sNB, sPB and sB denote scoring parameters of NB, PB and B, respectively. In our experiments, we set (sNB, sPB, sB) = (10.0,−5.0,−10.0)."
  }, {
    "heading": "4.2 Experimental Settings",
    "text": "In the word-embedding neural network of our NUR model, we used 1000 embedding cells, a skip-gram window size of five, and learned via 100GB of Twitter data (Other layers were learned by 1281 data points).\nIn our encoding and ranking RNNs, we used LSTM layers with 1000 hidden cells in each layer. The dropout rate was set to 0.5, and the model was trained via AdaGrad (Duchi et al., 2011).\nTo validate our NUR model, we conducted experiments with the following two settings:.\nProposed using limited context To verify the effectiveness of context sequence processing by the ranking RNN, this setting causes our system to only use the last user utterance as context, discarding the rest.\nProposed using MSE To verify the effectiveness of the PlackettLuce model, this setting causes our system to learn using the MSE of utterance scores instead of the Plackett-Luce model.\nWe also compared performance to the following three methods:\nBoW + DNN This method ranks candidate utterances using deep neural networks (DNNs) and bag-ofwords (BoW) features. The DNN consisted of six layers, excluding input and output layers optimized by MSE. The input vector is made by concatenating three BoW vectors, i.e., candidate utterance, last user utterance in the given context, and the given context without the last user utterance. In the BoW vector,\nwe used 6203 words that occur at least two times in the training data, thus, the input layer of the DNN has 18609 cells. Each hidden layer has 5000 cells, with ReLU as the activation function, the dropout rate set to 0.5, and the model trained by AdaGrad (Duchi et al., 2011). The score for training was the same as the model proposed in Section 4.1.\nKELDIC The second comparative approach used the output of our KELDIC system. This dialogue system ranks utterances using ListNet (Cao et al., 2007) and selects the top-ranked utterance to reply. The feature vector for ranking is generated from context and candidate utterance. It primarily utilizes n-gram pairs between utterances in context and candidates as features.\nRandom This approach randomly shuffles candidates and uses them as a ranking list, thus serving as a baseline for ranking performance."
  }, {
    "heading": "4.3 Results",
    "text": "To evaluate ranking performance, we used mean average precision (MAP) and mean reciprocal rank (MRR) measures. Figure 3 shows MAP results over the top n ranked candidate utterances, while Figure 4 shows MRR results. Using the MAP measure, our proposed method showed the highest performance as compared to the other methods. The proposed using limited context and MSE follow this, suggesting that utterance encoding by RNN is effective to extract features for ranking.\nBoW + DNN did not provide strong performance results, because it could not handle the order relation of utterances in context and syntax due to the use of BoW features. KELDIC showed higher performance than that of Random, but lower than that of BoW + DNN, because it also has the problem of context processing and its generalization capability is lower than that of DNNs.\nHere, n = 1 of MAP indicates that the rate of correct utterance ranked on the top (The maximum value of n = 1 of MAP is 1.0 because each data points in the test data contains at least one correct candidate utterance). Since the top-ranked utterance is selected as a response in dialogue systems, it was found that our proposed method correctly replied with a probability of approximately 60%.\nResults of MRR (i.e., Figure 4) showed very similar results, i.e., our proposed method ranked suitable utterances higher.\nTable 2 shows an example of context in the test data and Table 3 shows candidate utterances to the context shown in Table 2, plus ranking results for the applied methods and NB rates of annotations for candidates. These results indicate that our proposed method ranked correct utterances higher and incorrect utterances lower, as desired."
  }, {
    "heading": "5 Dialogue Experiment",
    "text": "In the previous section, since test data must contain correct candidate utterances, the ability of our NUR model in terms of actual dialogue is uncertain, thus we developed a conversational dialogue system based on our proposed method and conducted dialogue experiments with human subjects.\nThe dialogue format and rules were fully compliant with the Dialogue Breakdown Detection Challenge (DBDC) (see (Higashinaka et al., 2015a)). A dialogue is started by a system utterance, then user and the system communicate with\none another. When a system speaks 11 times, the dialogue is finished. Therefore, a dialogue contains 11 system and 10 human utterances.\nOur dialogue system and subjects chat on our website; we collected 120 text chat dialogues. Annotators then labeled 1200 system utterances (excluding initial greetings) using breakdown labels NB, PB, and B. We again recruited subjects and annotators via CrowdWorks.\nFor comparison, we used DBDC development/test datad collected by chatting with a system based on NTT Docomo’s chat APIe (see (Onishi and Yoshimura, 2014) . Since the DBDC system selects a suitable response from large-scale utterance data, the architecture is similar to our model and therefore suitable as a comparative system.\nDBDC data has been annotated by 30 annotators using the breakdown labels and we use them without any change in this experiment. Therefore, the annotation rule is same but the annotators are different between our dialogue data and DBDC data."
  }, {
    "heading": "5.1 Dialogue System",
    "text": "A conversational dialogue system based on our NUR model selects an utterance as a response from candidates generated by the acquisition method of (Inaba et al., 2014). The system extracts nouns from the last user and system utterances, generating candidate utterances related to nouns. We used approximately one billion Japanese tweets collected from January through February 2015 for utterance acquisition. Our NUR model ranked candidates, and the system used topranked utterances as responses. If there were less than five acquired utterances, the system retroactively extracted nouns in context one by one to acquire further candidates.\nThe first utterance in the beginning of a dialogue was randomly selected from 16 manually created open question utterances, such as “What is your favorite website?” or “What kind of part-time job do you have?”. If the user’s response does not contain any nouns or the number of acquired utterances is less than five, the system randomly selects the 16 utterances again."
  }, {
    "heading": "5.2 Results",
    "text": "Table 4 shows statistics of the data, annotations, and experimental results. Dialogue data used in\ndhttps://sites.google.com/site/dialoguebreakdowndetection/ ehttps://www.nttdocomo.co.jp/service/\nour system were annotated by 34 human annotators. Fleiss’s K measure for our system’s data was lower than that of the DBDC dataset, but both are low. “PB + B” indicates that PB and B are treated as a single label. The table also shows the ratio of NB, PB, and B labels. These annotation results indicate that output probabilities of PB and B utterances by our system were significantly lower, while NB was higher than that of the DBDC system (p < 0.01).\nThe Breakdown ratio (B) and (PB + B) values are calculated by the labels of majority vote in 34 (proposed) or 30 (DBDC) annotators in each system’s utterance. Breakdown ratio (B) is the ratio of the B majority label to all majority labels. Breakdown ratio (PB + B) is the ratio of PB and B majority labels (treated as a single label). This indicates that our system can offer a response that does not provoke a critical dialogue breakdown with a probability of approximately 90% and a\nvery natural response with a probability of 60%. Both breakdown ratios showed significant differences between our system and the DBDC system (p < 0.001).\nTable 4 also shows the number of words per utterance and the number of vocabularies. These results are important for system evaluation, because if a system always use innocuous responses, such as “I don’t know” or “That’s true”, it is relatively easy to avoid dialogue breakdown. By these values, we can find whether a system frequently uses innocuous responses or not; however, to increase user satisfaction with a dialogue system, it is important not only to avoid dialogue breakdown, but also to offer flexible replies. From Table 4, we also observe that the number of words per utterance and the number of vocabularies in our system were bigger than that of the DBDC system, indicating that our system infrequently used innocuous responses and had a good vocabulary for generating responses. Indeed, our system rarely used such utterances, but the DBDC system sometimes used them.\nThe number of words per utterance by user between both datasets was almost the same, but the number of vocabularies by user of the DBDC system was lower than that of our system. This was attributable to the DBDC system’s utterances that increased the incident of dialogue breakdown.\nfcalculated using 100 dialogues\nWhen the DBDC system uses such utterances, the user responds with formulaic responses, such as “What do you mean?”. Since the DBDC system frequently caused dialogue breakdowns, users used formulaic replies, and as a result, the number of vocabularies decreased."
  }, {
    "heading": "6 Conclusions",
    "text": "In this study, we proposed a new utterance selection method called the NUR model for conversational dialogue systems. Our model ranks candidate utterances by their suitability in given contexts using neural networks. Our proposed model encodes utterances in context and candidates into fixed-length vectors, then processes these encoded vectors in chronological order to rank utterances. Experimental results showed that our proposed model ranked utterances more accurately than that of deep learning and other existing methods. In addition, we constructed a conversational dialogue system based on our proposed method and conducted experiments to evaluate its performance via dialogue with human subjects. By comparing the dialogue system of DBDC, we found our system able to conduct conversations more naturally than DBDC.\nThe dialogue system used in the experiment acquired topic words from given context in a simple manner. Because of this, there are some cases that the system selects inappropriate topics and fails in changing topics. Thus, future work includes topic management. Moreover, the system is unskilled at answering questions, and it often provokes dialogue breakdown. It requires a question-answering method corresponding to conversational dialogue systems."
  }, {
    "heading": "Acknowledgements",
    "text": "This study received a grant of JSPS Grants-in-aid for Scientific Research 16H05880 and a research grant from Kayamori Foundation of Informational Science Advancement."
  }, {
    "heading": "A Dialogue Example",
    "text": "Table 5 shows example dialogue between our NUR-based dialogue system and a user. In the table, NB, PB, and B indicate respective rates of each label by 34 annotators."
  }],
  "year": 2016,
  "references": [{
    "title": "Iris: a chat",
    "authors": ["Rafael E Banchs", "Haizhou Li"],
    "year": 2012
  }, {
    "title": "Rank analysis of incomplete block designs: I",
    "authors": ["Ralph Allan Bradley", "Milton E Terry."],
    "venue": "the method of paired comparisons. Biometrika, 39(3/4):324– 345.",
    "year": 1952
  }, {
    "title": "Learning to rank: from pairwise approach to listwise approach",
    "authors": ["Z. Cao", "T. Qin", "T.Y. Liu", "M.F. Tsai", "H. Li."],
    "venue": "Proceedings of the 24th international conference on Machine learning, pages 129–136.",
    "year": 2007
  }, {
    "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Modeling a paranoid mind",
    "authors": ["K.M. Colby."],
    "venue": "Behavioral and Brain Sciences, 4(4):515–560.",
    "year": 1981
  }, {
    "title": "Stupid computer! abuse and social identities",
    "authors": ["Antonella De Angeli", "Rollo Carpenter."],
    "venue": "Proceedings of the INTERACT 2005 workshop Abuse: The darker side of Human-Computer Interaction.",
    "year": 2005
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Syntactic filtering and content-based retrieval of twitter sentences for the generation of system utterances",
    "authors": ["Ryuichiro Higashinaka", "Nozomi Kobayashi", "Toru Hirano", "Chiaki Miyazaki", "Toyomi Meguro", "Toshiro Makino", "Yoshihiro Matsuo"],
    "year": 2014
  }, {
    "title": "Towards taxonomy of errors in chat-oriented dialogue systems",
    "authors": ["Ryuichiro Higashinaka", "Kotaro Funakoshi", "Masahiro Araki", "Hiroshi Tsukahara", "Yuka Kobayashi", "Masahiro Mizukami."],
    "venue": "16th Annual Meeting of the Special Interest Group on Dis-",
    "year": 2015
  }, {
    "title": "On the difficulty of improving hand-crafted rules in chat-oriented dialogue systems",
    "authors": ["Ryuichiro Higashinaka", "Toyomi Meguro", "Hiroaki Sugiyama", "Toshiro Makino", "Yoshihiro Matsuo."],
    "venue": "2015 Asia-Pacific Signal and Information Processing As-",
    "year": 2015
  }, {
    "title": "The dialogue breakdown detection challenge: Task description, datasets, and evaluation metrics",
    "authors": ["Ryuichiro Higashinaka", "Kotaro Funakoshi", "Kobayashi Yuka", "Michimasa Inaba."],
    "venue": "10th edition of the Language Resources and Evaluation Confer-",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Candidate utterance acquisition method for non-task-oriented dialogue systems from twitter",
    "authors": ["Michimasa Inaba", "Sayaka Kamizono", "Kenichi Takahashi."],
    "venue": "Transactions of the Japanese Society for Artificial Intelligence, 29(1):21–31.",
    "year": 2014
  }, {
    "title": "Recurrent continuous translation models",
    "authors": ["Nal Kalchbrenner", "Phil Blunsom."],
    "venue": "Proc. EMNLP, 3(39):413.",
    "year": 2013
  }, {
    "title": "Machine-learned ranking based non-task-oriented dialogue agent using twitter data",
    "authors": ["Makoto Koshinda", "Michimasa Inaba", "Kenichi Takahashi."],
    "venue": "2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technol-",
    "year": 2015
  }, {
    "title": "Learning to rank from bayesian decision inference",
    "authors": ["Jen-Wei Kuo", "Pu-Jen Cheng", "Hsin-Min Wang."],
    "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, pages 827–836.",
    "year": 2009
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings of the NAACL-HLT 2016.",
    "year": 2016
  }, {
    "title": "Individual choice behavior: A theoretical analysis",
    "authors": ["R.D. Luce."],
    "venue": "Wiley, New York.",
    "year": 1959
  }, {
    "title": "Non-null ranking models",
    "authors": ["Colin L Mallows."],
    "venue": "i. Biometrika, 44(1/2):114–130.",
    "year": 1957
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Example-based spoken dialogue system using woz system log",
    "authors": ["Hiroya Murao", "Nobuo Kawaguchi", "Shigeki Matsubara", "Yukiko Yamaguchi", "Yasuyoshi Inagaki."],
    "venue": "4th SIGdial Workshop on Discourse and Dialogue, pages 140–148.",
    "year": 2003
  }, {
    "title": "Utilizing human-to-human conversation examples for a multi domain chat-oriented dialog system",
    "authors": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Toda Tomoki", "Satoshi Nakamura."],
    "venue": "IEICE TRANSACTIONS on Information and Systems, 97(6):1497–",
    "year": 2014
  }, {
    "title": "Casual conversation technology achieving natural dialog with computers",
    "authors": ["Kanako Onishi", "Takeshi Yoshimura."],
    "venue": "NTT DOCOMO Technical Jouranl, 15(4):16–21.",
    "year": 2014
  }, {
    "title": "The analysis of permutations",
    "authors": ["RL Plackett."],
    "venue": "Applied Statistics, pages 193–202.",
    "year": 1975
  }, {
    "title": "Data-driven response generation in social media",
    "authors": ["Alan Ritter", "Colin Cherry", "William B Dolan."],
    "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 583–593.",
    "year": 2011
  }, {
    "title": "Neural Responding Machine for Short Text Conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "Proceedings of the 53th Annual Meeting of Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
    "year": 2015
  }, {
    "title": "Dialog system for open-ended conversation using web documents",
    "authors": ["Masahiro Shibata", "Tomomi Nishiguchi", "Yoichi Tomiura."],
    "venue": "Informatica, 33(3).",
    "year": 2009
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "Proceedings",
    "year": 2015
  }, {
    "title": "Opendomain utterance generation for conversational dialogue systems using web-scale dependency structures",
    "authors": ["Hiroaki Sugiyama", "Toyomi Meguro", "Ryuichiro Higashinaka", "Yasuhiro Minami."],
    "venue": "Proc. SIGDIAL, pages 334–338.",
    "year": 2013
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "Proceedings of the ICML Deep Learning Workshop, pages 1–7.",
    "year": 2015
  }, {
    "title": "The anatomy of A.L.I.C.E",
    "authors": ["R.S. Wallace"],
    "venue": "Parsing the Turing Test,",
    "year": 2008
  }, {
    "title": "ELIZA-a computer program for the study of natural language communication between man and machine",
    "authors": ["J. Weizenbaum."],
    "venue": "Communications of the ACM, 9(1):36–45.",
    "year": 1966
  }],
  "id": "SP:38f17696b684070f15d6d07d5e0ffa4631e5661d",
  "authors": [{
    "name": "Michimasa Inaba",
    "affiliations": []
  }, {
    "name": "Kenichi Takahashi",
    "affiliations": []
  }],
  "abstractText": "In this study, we present our neural utterance ranking (NUR) model, an utterance selection model for conversational dialogue agents. The NUR model ranks candidate utterances with respect to their suitability in relation to a given context using neural networks; in addition, a dialogue system based on the model converses with humans using highly ranked utterances. Specifically, the model processes word sequences in utterances and utterance sequences in context via recurrent neural networks. Experimental results show that the proposed model ranks utterances with higher precision relative to deep learning and other existing methods. Furthermore, we construct a conversational dialogue system based on the proposed method and conduct experiments on human subjects to evaluate performance. The experimental result indicates that our system can offer a response that does not provoke a critical dialogue breakdown with a probability of 92% and a very natural response with a probability of 58%.",
  "title": "Neural Utterance Ranking Model for Conversational Dialogue Systems"
}