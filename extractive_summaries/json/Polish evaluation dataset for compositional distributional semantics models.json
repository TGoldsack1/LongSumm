{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784–792 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1073"
  }, {
    "heading": "1 Introduction and related work",
    "text": ""
  }, {
    "heading": "1.1 Distributional semantics",
    "text": "The basic idea of distributional semantics, i.e. determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957). John R. Firth drew attention to the contextdependent nature of meaning especially with his\n1The dataset is obtainable at: http://zil.ipipan.waw.pl/Scwad/CDSCorpus\nfamous maxim “You shall know a word by the company it keeps” (Firth, 1957, p. 11).\nNowadays, distributional semantics models are estimated with various methods, e.g. word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013). To ascertain the purport of a word, e.g. bath, you can use the context of other words that surround it. If we assume that the meaning of this word expressed by its lexical context is associated with a distributional vector, the distance between distributional vectors of two semantically similar words, e.g bath and shower, should be smaller than between vectors representing semantically distinct words, e.g. bath and tree."
  }, {
    "heading": "1.2 Compositional distributional semantics",
    "text": "Based on empirical observations that distributional vectors encode certain aspects of word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors. The idea of semantic composition is not new. It is well known as the principle of compositionality:2 “The meaning of a compound expression is a function of the meaning of its parts and of the way they are syntactically combined.” (Janssen, 2012, p. 19).\nModelling the meaning of textual units larger than words using compositional and distributional information is the main subject of compositional distributional semantics (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, to name a few studies). The fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic. Apart from the papers, it was the SemEval-2014 Shared Task 1\n2As the principle of compositionality is attributed to Gottlob Frege, it is often called Frege’s principle.\n784\n(Marelli et al., 2014) that essentially contributed to the expansion of CDS and increased an interest in this domain. The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus."
  }, {
    "heading": "1.3 The SICK corpus",
    "text": "The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena. It builds on two external data sources – the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012). Each sentence pair is human-annotated for relatedness in meaning and entailment.\nThe relatedness score corresponds to the degree of semantic relatedness between two sentences and is calculated as the average of ten human ratings collected for this sentence pair on the 5-point Likert scale. This score indicates the extent to which the meanings of two sentences are related.\nThe entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral. According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label."
  }, {
    "heading": "1.4 Motivation and organisation of the paper",
    "text": "Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g. training or testing data) stimulates the development of NLP tools and the estimation of NLP models. English is undoubtedly the most prominent in this regard and English resources are the most numerous. Therefore, NLP methods are mostly designed for English and tested on English data, even if there is no guarantee that they are universal. In order to verify whether an NLP algorithm is adequate, it is not enough to evaluate it solely for English. It is also valuable to have high-quality resources for languages typologically different to English. Hence, we aim at building datasets for the evaluation of CDS models in languages other than English, which are often underresourced. We strongly believe that the availability of test data will encourage development of CDS models in these languages and allow to better test the universality of CDS methods.\nWe start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions. First, it is a rather under-resourced language in contrast to the resource-rich English. Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order. If some heuristics is tested on e.g. Polish, the evaluation results can be approximately generalised to other Slavic languages. We hope the Slavic NLP community will be interested in designing and evaluating methods of semantic modelling for Slavic languages.\nThe procedure of building an evaluation dataset for validating compositional distributional semantics models of Polish generally builds on steps designed to assemble the SICK corpus (described in Section 1.3) because we aim at building an evaluation dataset which is comparable to the SICK corpus. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for Polish (see Section 2.1) and the need for Polish-specific transformation rules (see Section 2.2). Furthermore, the rules of arranging sentences into pairs (see Section 2.3) are defined anew taking into account the characteristic of data and bidirectional entailment annotations, since an entailment relation between two sentences must not be symmetric. Even if our assumptions of annotating sentence pairs coincide with the SICK principles to a certain extent (see Section 3.1), the annotation process differs from the SICK procedure, in particular by introducing an element of human verification of correctness of automatically transformed sentences (see Section 3.2) and some additional post-corrections (see Section 3.3). Finally, a summary of the dataset is provided in Section 4.1 and the dataset evaluation is given in Section 4.2."
  }, {
    "heading": "2 Procedure of collecting data",
    "text": ""
  }, {
    "heading": "2.1 Selection and description of images",
    "text": "The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012). Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images.\nImages are selected from the 8K ImageFlickr\ndataset (Rashtchian et al., 2010). At first we wanted to take only these images the descriptions of which were selected for the SICK corpus. However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions. Therefore, we independently extract 1K images and split them into 46 thematic groups (e.g. children, musical instruments, motorbikes, football, dogs). The numbers of images within individual thematic groups vary from 6 images in the volleyball and telephoning groups to 94 images in the various people group. The second largest groups are children and dogs with 50 images each.\nThe chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction. The authors are instructed to write one single sentence (with a sentence predicate) describing the action in a displayed image. They should not describe an imaginable context or an interpretation of what may lie behind the scene in the picture. If some details in the picture are not obvious, they should not be described either. Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, because those are not compositional linguistic phenomena. Finally, descriptions should contain Polish diacritics and proper punctuation."
  }, {
    "heading": "2.2 Transformation of descriptions",
    "text": "The second step of building the SICK corpus consisted in pre-processing extracted sentences, i.e. normalisation and expansion (Bentivogli et al., 2014, p. 3–4). Since the authors of Polish descriptions are asked to follow the guidelines (presented in Section 2.1), the normalisation step is not essential for our data. The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings. The following transformations are implemented:\n1. dropping conjunction concerns sentences with coordinated predicates sharing a subject, e.g. Rowerzysta odpoczywa i obserwuje morze. (Eng. ‘A cyclist is resting and watching the sea.’). The finite form of one of the coordinated predicates is transformed into:\n• an active adjectival participle, e.g. Odpoczywający rowerzysta obserwuje\nmorze. (Eng. ‘A resting cyclist is watching the sea.’) or Obserwujący morze rowerzysta odpoczywa. (Eng. ‘A cyclist, who is watching the sea, is resting.’), • a contemporary adverbial participle,\ne.g. Rowerzysta, odpoczywając, obserwuje morze. (Eng. ‘A cyclist is watching the sea, while resting.’) or Rowerzysta odpoczywa, obserwując morze. (Eng. ‘A cyclist is resting, while watching the sea.’).\n2. removing conjunct in adjuncts, i.e. the deletion of one of coordinated elements of an adjunct, e.g. Mały, ale zwinny kot miauczy. (Eng. ‘A small but agile cat miaows.’) can be changed into either Mały kot miauczy. (Eng. ‘A small cat miaows.’) or Zwinny kot miauczy. (Eng. ‘An agile cat miaows.’).\n3. passivisation, e.g. Człowiek ujeżdża byka. (Eng. ‘A man is breaking a bull in.’) can be transformed into Byk jest ujeżdżany przez człowieka. (Eng. ‘A bull is being broken in by a man.’).\n4. removing adjuncts, e.g. Dwa białe króliki siedzą na trawie. (Eng. ‘Two small rabbits are sitting on the grass.’) can be changed into Króliki siedzą. (Eng. ‘The rabbits are sitting.’).\n5. swapping relative clause for participles, i.e. a relative clause swaps with a participle (and vice versa), e.g. Kobieta przytula psa, którego trzyma na smyczy. (Eng. ‘A woman hugs a dog which she keeps on a leash.’). The relative clause is interchanged for a participle construction, e.g. Kobieta przytula trzymanego na smyczy psa. (Eng. ‘A woman hugs a dog kept on a leash.’).\n6. negation, e.g. Mężczyźni w turbanach na głowach siedzą na słoniach. (Eng. ‘Men in turbans on their heads are sitting on elephants.’) can be transformed into Nikt nie siedzi na słoniach. (Eng. ‘Nobody is sitting on elephants.’), Żadni mężczyźni w turbanach na głowach nie siedzą na słoniach. (Eng. ‘No men in turbans on their heads are sitting on elephants.’), and Mężczyźni w turbanach na głowach nie siedzą na słoniach. (Eng. ‘Men in turbans on their heads are not sitting on elephants.’).\n7. constrained mixing of dependents from various sentences, e.g. Dwoje dzieci siedzi na wielbłądach w pobliżu wysokich gór. (Eng. ‘Two children are sitting on camels near high mountains.’) can be changed into Dwoje dzieci siedzi przy zastawionym stole w pobliżu wysokich gór. (Eng. ‘Two children are sitting at the table laid with food near high mountains.’).\nThe first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning. All transformations are performed on the dependency structures of input sentences (Wróblewska, 2014).\nSome of the transformations are very productive (e.g. mixing dependents). Other, in turn, are sparsely represented in the output (e.g. dropping conjunction). The number of transformed sentences randomly selected to build the dataset is in the second column of Table 1."
  }, {
    "heading": "2.3 Data ensemble",
    "text": "The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs. Since our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus. The general idea behind the pair-ensembling procedure was to introduce sentence pairs with different levels of relatedness into the dataset. Apart from pairs connecting two sentences originally written by humans (as described in Section 2.1), there are also pairs in which an original sentence is connected with\na transformed sentence. For each of the 1K images, the following 10 pairs are constructed (for A being the set of all sentences originally written by the first author, B being the set of all sentences originally written by the second author, a ∈ A and b ∈ B being the original descriptions of the picture):\n1. (a,b)\n2. (a,a1), where a1 ∈ t(a), and t(a) is the set of all transformations of the sentence a\n3. (b,b1), where b1 ∈ t(b)\n4. (a,b2), where b2 ∈ t(b)\n5. (b,a2), where a2 ∈ t(a)\n6. (a,a3), where a3 ∈ t(a′),a′ ∈ A, T (a′) = T (a),a′ 6= a, for T (a) being the thematic group3 of a\n7. (b,b3), where b3 ∈ t(b′),b′ ∈ B, T (b′) = T (b),b′ 6= b\n8. (a,a4), where a4 ∈ A, T (a4) 6= T (a)4\n9. (b,b4), where b4 ∈ B, T (b4) 6= T (b)\n10. (a,a5), where a5 ∈ t(a),a5 6= a1 for 50% images, (b,b5) (analogously) for other 50%.5\nFor each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus. As a result, the working set consists of 20K sentence pairs."
  }, {
    "heading": "3 Corpus annotation",
    "text": ""
  }, {
    "heading": "3.1 Annotation assumptions",
    "text": "The degree of semantic relatedness between two sentences is calculated as the average of all human ratings on the Likert scale with the range from 0 to 5. Since we do not want to excessively influence\n3The thematic group of a sentence a corresponds to the thematic group of an image being the source of a (as described in Section 2.1).\n4The pairs (a,a4) of the same authors’ descriptions of two images from different thematic groups are expected to be unrelated. The same applies to (b,b4).\n5A repetition of point 2 with a restriction that a different pair is created (pairs of very related sentences are expected). We alternate between authors A and B to obtain equal author proportions in the final ensemble of pairs.\nthe annotations, the guidelines given to annotators are mainly example-based:6\n• 5 (very related): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Na płocie jest duży kot. (Eng. ‘There is a large cat on the fence.’),\n• 1–4 (more or less related): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Kot nie siedzi na płocie. (Eng. ‘A cat is not sitting on the fence.’); Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Właściciel dał kotu chrupki. (Eng. ‘The owner gave kibble to his cat.’); Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Kot miauczy pod płotem. (Eng. ‘A cat miaows by the fence.’).\n• 0 (unrelated): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Zaczął padać deszcz. (Eng. ‘It started to rain.’).\nApart from these examples, there is a note in the annotation guidelines indicating that the degree of semantic relatedness is not equivalent to the degree of semantic similarity. Semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one.\nPolish entailment labels correspond directly to the SICK labels (i.e. entailment, contradiction, neutral). The entailment label assigned by the majority of human judges is selected as the gold label. The entailment labels are defined as follows:\n• a wynika z b (b entails a) – if a situation or an event described by sentence b occurs, it is recognised that a situation or an event described by a occurs as well, i.e. a and b refer to the same event or the same situation,\n• a jest zaprzeczeniem b (a is the negation of b) – if a situation or an event described by b occurs, it is recognised that a situation or an event described by a may not occur at the same time,\n6We realise that the boundary between semantic perception of a sentence by various speakers is fuzzy (it depends on speakers’ education, origin, age, etc.). It was thus our wellthought-out decision to draw only general annotation frames and to enable annotators to rely on their feel for language.\n• a jest neutralne wobec b (a is neutral to b) – the truth of a situation described by a cannot be determined on the basis of b."
  }, {
    "heading": "3.2 Annotation procedure",
    "text": "Similar to the SICK corpus, each Polish sentence pair is human-annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.7 Since for each annotated pair (a,b), its reverse (b,a) is also subject to annotation, the entailment relation is in practice determined ‘in both directions’ for 10K sentence pairs. For the task of relatedness annotation, the order of sentences within pairs seems to be irrelevant, we can thus assume to obtain 6 relatedness scores for 10K unique pairs.\nSince the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences. In order to avoid annotating erroneous sentences, the annotation process is divided into two stages:\n1. a sentence pair is sent to a judge with the leader role, who is expected to edit and to correct the transformed sentence from this pair before annotation, if necessary,\n2. the verified and possibly enhanced sentence pair is sent to the other two judges, who can only annotate it.\nThe leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes. Unusual sentences which could be accepted by Polish speakers should not be modified. Moreover, the modified sentence may not be identical with the other sentence in the pair. The classification and statistics of distinct corrections made by the leader judges are provided in Table 2.\nA strict classification of error types is quite hard to provide because some sentences contain more than one error. We thus order the error types from the most serious errors (i.e. ‘sense’ errors) to the redundant corrections (i.e. ‘other’ type). If a sentence contains several errors, it is qualified for the higher order error type.\nIn the case of sentences with ‘sense’ errors, the need for correction is uncontroversial and\n7Our annotators have relatively strong linguistic background. Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate.\narises from an internal logical contradiction.8 The sentences with ‘semantic’ changes are syntactically correct, but deemed unacceptable by the leader annotators from the semantic or pragmatic point of view.9 The ‘grammatical’ errors mostly concern missing agreement.10 The majority of ‘word order’ corrections are unnecessary, but we found some examples which can be classified as actual word or phrase order errors.11 The correction of punctuation consists in adding or deleting a comma.12 The sentences in the ‘other’ group, in turn, could as well have been left unchanged because they are proper Polish sentences, but were apparently considered odd by the leader annotators.\n8An example of ‘sense’ error: the sentence Chłopak w zielonej bluzie i czapce zjeżdża na rolkach na leżąco. (Eng. ‘A boy in a green sweatshirt and a cap roller-skates downhill in a lying position.’) is corrected into Chłopak w zielonej bluzie i czapce zjeżdża na rolkach. (Eng. ‘A boy in a green sweatshirt and a cap roller-skates downhill.’).\n9An example of ‘semantic’ correction: the sentence Dziewczyna trzyma w pysku patyk. (Eng. ‘A girl holds a stick in her muzzle.’) is corrected into Dziewczyna trzyma w ustach patyk. (Eng. ‘A girl holds a stick in her mouth.’).\n10An example of ‘grammatical’ error: the sentence Grupasg.nom uśmiechających się ludzi tańcząpl. (Eng. *‘A group of smiling people are dancing.’) is corrected into Grupasg.nom uśmiechających się ludzi tańczysg . (Eng. ‘A group of smiling people is dancing.’).\n11An example of word order error: the sentence Samochód, który jest uszkodzony, koloru białego stoi na lawecie dużego auta. (lit. ‘A car that is damaged, of the white color stands on the trailer of a large car.’, Eng. ‘A white car that is damaged is standing on the trailer of a large car.’) is corrected into Samochód koloru białego, który jest uszkodzony, stoi na lawecie dużego auta.\n12An example of punctuation correction: the wrong comma in the sentence Nad brzegiem wody, stoją dwaj mężczyźni z wędkami. (lit. ‘On the water’s edge, two men are standing with rods.’; Eng. ‘Two men with rods are standing on the water’s edge.’) should be deleted, i.e. Nad brzegiem wody stoją dwaj mężczyźni z wędkami."
  }, {
    "heading": "3.3 Impromptu post-corrections",
    "text": "During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators. We thus decided to garner annotators’ comments and suggestions for improving sentences. After validation of these suggestions by an experienced linguist, it turns out that most of these proposals concern punctuation errors (e.g. missing comma) and typos in 312 distinct sentences. These errors are fixed directly in the corpus because they should not impact the annotations of sentence pairs. The other suggestions concern more significant changes in 29 distinct sentences (mostly minor grammatical or semantic problems overlooked by the leader annotators). The annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them."
  }, {
    "heading": "4 Corpus summary and evaluation",
    "text": ""
  }, {
    "heading": "4.1 Corpus statistics",
    "text": "Tables 3 and 4 summarise the annotations of the resulting 10K sentence pairs corpus. Table 3 aggregates the occurrences of 6 possible relatedness scores, calculated as the mean of all 6 individual annotations, rounded to an integer.\nTable 4 shows the number of the particular entailment labels in the corpus. Since each sentence pair is annotated for entailment in both directions, the final entailment label is actually a pair of two labels:\n• entailment+neutral points to ‘one-way’ entailment,\n• contradiction+neutral points to ‘one-way’ contradiction,\n• entailment+entailment, contradiction+contradiction, and neutral+neutral point to equivalence.\nWhile the actual corpus labels are ordered in the sense that there is a difference between e.g. entailment+neutral and neutral+entailment (the entailment occurs in different directions), we treat all labels as unordered for the purpose of this summary (e.g. entailment+neutral covers neutral+entailment as well, representing the same type of relation between two sentences)."
  }, {
    "heading": "4.2 Inter-annotator agreement",
    "text": "The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen’s kappa (Cohen, 1960). However, this coefficient is designed to measure agreement between two annotators only. Since there are three annotators of each pair of ordered sentences, we decided to apply Fleiss’ kappa13 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items. An additional advantage of this measure is that different items can be rated by different human judges, which doesn’t impact measurement. The normalised Fleiss’ measure of inter-annotator agreement is:\nκ = P̄ − P̄e 1− P̄e\nwhere the quantity P̄ − P̄e measures the degree of agreement actually attained in excess of chance, while “[t]he quantity 1 − P̄e measures the degree of agreement attainable over and above what would be predicted by chance” (Fleiss, 1971, p. 379).\nWe recognise Fleiss’ kappa as particularly useful for measuring inter-annotator agreement with respect to entailment labelling in our evaluation dataset. First, there are more than two raters. Second, entailment labels are categorically. Measured\n13As Fleiss’ kappa is actually the generalisation of Scott’s π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π, cf. Artstein and Poesio (2008).\nwith Fleiss’ kappa, there is an inter-annotator agreement of κ = 0.734 for entailment labels in Polish evaluation dataset, which is quite satisfactory as for a semantic labelling task.\nRelative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle. This is also reflected in the inter-annotator agreement scores measured with Fleiss’ kappa. Inter-annotator agreement measured for six semantic relatedness groups corresponding to points on the Likert scale is quite low: κ = 0.337. If we measure interannotator agreement for three classes corresponding to the three relatedness groups from the annotation guidelines (see Section 3.1), i.e. <0>, <1, 2, 3, 4>, and <5>, the Fleiss’ score is significantly higher: κ = 0.543. Hence, we conclude that Fleiss’ kappa is not a reliable measure of inter-annotator agreement in relation to relatedness scores. Therefore, we decided to use Krippendorff’s α instead.\nKrippendorff’s α (Krippendorff, 1980, 2013) is a coefficient appropriate for measuring the interannotator agreement of a dataset which is annotated with multiple judges and characterised by different magnitudes of disagreement and missing values. Krippendorff proposes distance metrics suitable for various scales: binary, nominal, interval, ordinal, and ratio. In ordinal measurement14 the attributes can be rank-ordered, but distances between them do not have any meaning. Measured with Krippendorff’s ordinal α, there is an inter-annotator agreement of α = 0.780 for relatedness scores in the Polish evaluation dataset, which is quite satisfactory as well. Hence, we conclude that our dataset is a reliable resource for the purpose of evaluating compositional distributional semantics model of Polish."
  }, {
    "heading": "5 Conclusions",
    "text": "The goal of this paper is to present the procedure of building a Polish evaluation dataset for the validation of compositional distributional semantics models. As we aim at building an evalua-\n14Nominal measurement is useless for measuring agreement between relatedness scores (α = 0.340 is the identical value as Fleiss’ kappa, since all disagreements are considered equal). We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed. The interval score measured for relatedness annotations is quite high α = 0.785, but we doubt whether the distance between relatedness scores is meaningful in this case.\ntion dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus. However, the procedure of building the SICK corpus cannot be adapted without modifications. First, the Polish seed-sentences have to be written based on the images which are selected from 8K ImageFlickr dataset and split into thematic groups, since usable datasets are not publicly available. Second, since the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch. Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations. The discrepancies relative to the SICK procedure also concern the annotation process itself. Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions. Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections.\nThe presented procedure of building a dataset was tested on Polish. However, it is very likely that the annotation framework will work for other Slavic languages (e.g. Czech with an excellent dependency parser).\nThe presented procedure results in building the Polish test corpus of relatively high quality, confirmed by the inter-annotator agreement coefficients of κ = 0.734 (measured with Fleiss’ kappa) for entailment labels and of α = 0.780 (measured with Krippendorff’s ordinal alpha) for relatedness scores."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the reliable and tenacious annotators of our dataset: Alicja DziedzicRawska, Bożena Itoya, Magdalena Król, Anna Latusek, Justyna Małek, Małgorzata Michalik, Agnieszka Norwa, Małgorzata Szajbel-Keck, Alicja Walichnowska, Konrad Zieliński, and some other. The research presented in this paper was supported by SONATA 8 grant no 2014/15/D/HS2/03486 from the National Science Centre Poland."
  }],
  "year": 2017,
  "references": [{
    "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity",
    "authors": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."],
    "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM). pages 385–393.",
    "year": 2012
  }, {
    "title": "Inter-Coder Agreement for Computational Linguistics",
    "authors": ["Ron Artstein", "Massimo Poesio."],
    "venue": "Computational Linguistics 34:557–596.",
    "year": 2008
  }, {
    "title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
    "authors": ["Marco Baroni", "Roberto Zamparelli."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pages",
    "year": 2010
  }, {
    "title": "A Neural Probabilistic Language Model",
    "authors": ["Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin."],
    "venue": "Journal of Machine Learning Research 3:1137–1155.",
    "year": 2003
  }, {
    "title": "Neural Probabilistic Language Models",
    "authors": ["Yoshua Bengio", "Holger Schwenk", "Jean-Sébastien Senécal", "Fréderic Morin", "Jean-Luc Gauvain."],
    "venue": "D.E. Holmes and L.C. Jain, editors, Innovations in Machine Learning. Theory and Applications,",
    "year": 2006
  }, {
    "title": "SICK through the SemEval Glasses",
    "authors": ["Luisa Bentivogli", "Raffaella Bernardi", "Marco Marelli", "Stefano Menini", "Marco Baroni", "Roberto Zamparelli."],
    "venue": "Lesson learned from the evaluation of compositional distributional semantic models on full sen-",
    "year": 2014
  }, {
    "title": "A coefficient of agreement for nominal scales",
    "authors": ["Jacob Cohen."],
    "venue": "Educational and Psychological Measurement 20:37–46.",
    "year": 1960
  }, {
    "title": "A synopsis of linguistic theory, 1930-1955",
    "authors": ["John Rupert Firth."],
    "venue": "Studies in Linguistic Analysis. Special volume of the Philological Society pages 1–32.",
    "year": 1957
  }, {
    "title": "Measuring nominal scale agreement among many raters",
    "authors": ["Joseph L. Fleiss."],
    "venue": "Psychological Bulletin 75:378–382.",
    "year": 1971
  }, {
    "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning",
    "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP",
    "year": 2011
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig Harris."],
    "venue": "Word 10:146–162. 791",
    "year": 1954
  }, {
    "title": "Compositionality: its historic context",
    "authors": ["Theo M.V. Janssen."],
    "venue": "Wolfram Hinzen, Edouard Machery, and Markus Werning, editors, The Oxford Handbook of Compositionality, Oxford University Press, Studies in Fuzziness and Soft Computing, pages 19–",
    "year": 2012
  }, {
    "title": "Content Analysis: An Introduction to Its Methodology",
    "authors": ["Klaus Krippendorff."],
    "venue": "Sage Publications, Beverly Hills.",
    "year": 1980
  }, {
    "title": "Content Analysis: An Introduction to Its Methodology",
    "authors": ["Klaus Krippendorff."],
    "venue": "Sage Publication, Thousand Oaks, 3rd edition.",
    "year": 2013
  }, {
    "title": "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness",
    "authors": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"],
    "year": 2014
  }, {
    "title": "Distributed Representations of Words and Phrases and their Compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Advances in Neural Information Processing Systems 26. Proceedings of Neural Information Pro-",
    "year": 2013
  }, {
    "title": "Composition in Distributional Models of Semantics",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Cognitive Science 34:1388–1429.",
    "year": 2010
  }, {
    "title": "Collecting Image Annotations Using Amazon’s Mechanical Turk",
    "authors": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s",
    "year": 2010
  }, {
    "title": "Reliability of Content Analysis: The Case of Nominal Scale Coding",
    "authors": ["William A. Scott."],
    "venue": "Public Opinion Quarterly 19:321–325.",
    "year": 1955
  }, {
    "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    "authors": ["Richard Socher", "Brody Huval", "Christopher Manning", "Andrew Ng."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
    "year": 2012
  }, {
    "title": "Polish Dependency Parser Trained on an Automatically Induced Dependency Bank",
    "authors": ["Alina Wróblewska."],
    "venue": "Ph.D. dissertation, Institute of Computer Science, Polish Academy of Sciences, Warsaw.",
    "year": 2014
  }],
  "id": "SP:d390e524fc523199a0094b9681148d8ecad0ecbc",
  "authors": [{
    "name": "Alina Wróblewska",
    "affiliations": []
  }, {
    "name": "Katarzyna Krasnowska-Kieraś",
    "affiliations": []
  }],
  "abstractText": "The paper presents a procedure of building an evaluation dataset1. for the validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.",
  "title": "Polish evaluation dataset for compositional distributional semantics models"
}