{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2278–2282, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "The neural encoder-decoder framework for machine translation (Neco and Forcada, 1997; Castaño and Casacuberta, 1997; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) provides new tools for addressing the field’s difficult challenges. In this framework (Figure 1), we use a recurrent neural network (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another recurrent network (decoder) to convert that vector into a target sentence. In this paper, we train long shortterm memory (LSTM) neural units (Hochreiter and Schmidhuber, 1997) trained with back-propagation through time (Werbos, 1990).\nA remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the system on previously unseen test data, using BLEU (Papineni et al., 2002), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is incurred. This behavior seems to come for free, without special design.\nBy contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the\nIBM SMT group, whose famous Models 1-5 included a learned table (y|x), with x and y being the lengths of source and target sentences (Brown et al., 1993). But they did not deploy this table when decoding a foreign sentence f into an English sentence e; it did not participate in incremental scoring and pruning of candidate translations. As a result (Brown et al., 1995):\n“However, for a given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are improperly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with clength(e) · P(f|e) for some empirically chosen constant c. This modification is treatment of the symptom rather than treatment of the disease itself, but it offers some temporary relief. The cure lies in better modeling.”\nMore temporary relief came from Minimum Error-Rate Training (MERT) (Och, 2003), which automatically sets c to maximize BLEU score. MERT also sets weights for the language model P(e), translation model P(f|e), and other features. The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time.\nNMT’s ability to correctly model length is remarkable for these reasons: • SMT relies on maximum BLEU training to ob-\ntain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training. • Standard SMT models explicitly “cross off”\n2278\nsource words and phrases as they are translated, so it is clear when an SMT decoder has finished translating a sentence. NMT systems lack this explicit mechanism. • SMT decoding involves heavy search, so if one\nMT output path delivers an infelicitous ending, another path can be used. NMT decoding explores far fewer hypotheses, using a tight beam without recombination.\nIn this paper, we investigate how length regulation works in NMT."
  }, {
    "heading": "2 A Toy Problem for Neural MT",
    "text": "We start with a simple problem in which source strings are composed of symbols a and b. The goal of the translator is simply to copy those strings. Training cases look like this:\na a a b b a <EOS> → a a a b b a <EOS> b b a <EOS> → b b a <EOS> a b a b a b a a <EOS> → a b a b a b a a <EOS> b b a b b a b b a <EOS> → b b a b b a b b a <EOS> The encoder must summarize the content of any\nsource string into a fixed-length vector, so that the decoder can then reconstruct it.1 With 4 hidden LSTM units, our NMT system can learn to solve this problem after being trained on 2500 randomly chosen strings of lengths up to 9.2 3\nTo understand how the learned system works, we encode different strings and record the resulting LSTM cell values. Because our LSTM has four hidden units, each string winds up at some point in four-\n1We follow Sutskever et al. (2014) in feeding the input string backwards to the encoder.\n2Additional training details: 100 epochs, 100 minibatch size, 0.7 learning rate, 1.0 gradient clipping threshold.\n3We use the toolkit: https://github.com/isi-nlp/Zoph RNN\ndimensional space. We plot the first two dimensions (unit1 and unit2) in the left part of Figure 2, and we plot the other two dimensions (unit3 and unit4) in the right part. There is no dimension reduction in these plots. Here is what we learn: • unit1 records the approximate length of the\nstring. Encoding a string of length 7 may generate a value of -6.99 for unit1. • unit2 records the number of b’s minus the num-\nber of a’s, thus assigning a more positive value to b-heavy strings. It also includes a +1 bonus if the string ends with a. • unit3 records a prefix of the string. If its value\nis less than 1.0, the string starts with b. Otherwise, it records the number of leading a’s. • unit4 has a more diffuse function. If its value is\npositive, then the string consists of all b’s (with a possible final a). Otherwise, its value correlates with both negative length and the preponderance of b’s.\nFor our purposes, unit1 is the interesting one. Figure 3 shows the progression of “a b a b b b” as it gets encoded (top figure), then decoded (bottom two figures). During encoding, the value of unit1 decreases by approximately 1.0 each time a letter is read. During decoding, its value increases each time a letter is written. When it reaches zero, it signals the decoder to output <EOS>.\nThe behavior of unit1 shows that the translator incorporates explicit length regulation. It also explains two interesting phenomena: • When asked to transduce previously-unseen\nstrings up to length 14, the system occasionally makes a mistake, mixing up an a or b. However, the output length is never wrong.4\n4Machine translation researchers have also noticed that\n• When we ask the system to transduce very long strings, beyond what it has been trained on, its output length may be slightly off. For example, it transduces a string of 28 b’s into a string of 27 b’s. This is because unit1 is not incremented and decremented by exactly 1.0."
  }, {
    "heading": "3 Full-Scale Neural Machine Translation",
    "text": "Next we turn to full-scale NMT. We train on data from the WMT 2014 English-to-French task, consisting of 12,075,604 sentence pairs, with 303,873,236 tokens on the English side, and 348,196,030 on the French side. We use 1000 hidden LSTM units. We also use two layers of LSTM units between source and target.5\nAfter the LSTM encoder-decoder is trained, we send test-set English strings through the encoder portion. Every time a word token is consumed, we record the LSTM cell values and the length of the\nwhen the translation is completely wrong, the length is still correct (anonymous).\n5Additional training details: 8 epochs, 128 minibatch size, 0.35 learning rate, 5.0 gradient clipping threshold.\nstring so far. Over 143,379 token observations, we investigate how the LSTM encoder tracks length.\nWith 1000 hidden units, it is difficult to build and inspect a heat map analogous to Figure 3. Instead, we seek to predict string length from the cell values, using a weighted, linear combination of the 1000 LSTM cell values. We use the least-squares method to find the best predictive weights, with resulting R2 values of 0.990 (for the first layer, closer to source text) and 0.981 (second layer). So the entire network records length very accurately.\nHowever, unlike in the toy problem, no single unit tracks length perfectly. The best unit in the second layer is unit109, which correlates with R2=0.894.\nWe therefore employ three mechanisms to locate"
  }, {
    "heading": "1 109 0.894",
    "text": ""
  }, {
    "heading": "2 334, 109 0.936",
    "text": ""
  }, {
    "heading": "3 334, 442, 109 0.942",
    "text": ""
  }, {
    "heading": "4 334, 442, 109, 53 0.947",
    "text": ""
  }, {
    "heading": "5 334, 442, 109, 53, 46 0.951",
    "text": ""
  }, {
    "heading": "6 334, 442, 109, 53, 46, 928 0.953",
    "text": ""
  }, {
    "heading": "7 334, 442, 109, 53, 46, 433, 663 0.955",
    "text": "a subset of units responsible for tracking length. We select the top k units according to: (1) individual R2 scores, (2) greedy search, which repeatedly adds the unit which maximizes the set’s R2 value, and (3) beam search. Table 1 shows different subsets we obtain. These are quite predictive of length. Table 2 shows how R2 increases as beam search augments the subset of units."
  }, {
    "heading": "4 Mechanisms for Decoding",
    "text": "For the toy problem, Figure 3 (middle part) shows how the cell value of unit1 moves back to zero as the target string is built up. It also shows (lower part) how the probability of target word <EOS> shoots up once the correct target length has been achieved.\nMT decoding is trickier, because source and target strings are not necessarily the same length, and\ntarget length depends on the words chosen. Figure 4 shows the action of unit109 and unit334 for a sample sentence. They behave similarly on this sentence, but not identically. These two units do not form a simple switch that controls length—rather, they are high-level features computed from lower/previous states that contribute quantitatively to the decision to end the sentence.\nFigure 4 also shows the log P(<EOS>) curve, where we note that the probability of outputting <EOS> rises sharply (from 10−8 to 10−4 to 0.998), rather than gradually."
  }, {
    "heading": "5 Conclusion",
    "text": "We determine how target length is regulated in NMT decoding. In future work, we hope to determine how other parts of the translator work, especially with reference to grammatical structure and transformations."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was supported by ARL/ARO (W911NF10-1-0533), DARPA (HR0011-15-C-0115), and the Scientific and Technological Research Council of Turkey (TÜBİTAK) (grants 114E628 and 215E201)."
  }],
  "year": 2016,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio."],
    "venue": "Proc. ICLR.",
    "year": 2014
  }, {
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["P. Brown", "S. della Pietra", "V. della Pietra", "R. Mercer."],
    "venue": "Computational Linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "Method and system for natural language translation",
    "authors": ["P.F. Brown", "J. Cocke", "S. della Pietra", "V. della Pietra", "F. Jelinek", "J.C. Lai", "R.L. Mercer."],
    "venue": "US Patent 5,477,451.",
    "year": 1995
  }, {
    "title": "A connectionist approach to machine translation",
    "authors": ["M.A. Castaño", "F. Casacuberta."],
    "venue": "EUROSPEECH.",
    "year": 1997
  }, {
    "title": "Lstm can solve hard long time lag problems",
    "authors": ["S. Hochreiter", "J. Schmidhuber."],
    "venue": "Advances in neural information processing systems, pages 473–479.",
    "year": 1997
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["M. Luong", "H. Pham", "C. Manning."],
    "venue": "Proc. EMNLP.",
    "year": 2015
  }, {
    "title": "Asynchronous translations with recurrent neural nets",
    "authors": ["R. Neco", "M. Forcada."],
    "venue": "International Conf. on Neural Networks, volume 4, pages 2535–2540.",
    "year": 1997
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["F.J. Och."],
    "venue": "Proc. ACL.",
    "year": 2003
  }, {
    "title": "BLEU: a method for automatic evaluation of machine translation",
    "authors": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu."],
    "venue": "Proc. ACL.",
    "year": 2002
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["I. Sutskever", "O. Vinyals", "Q.V. Le."],
    "venue": "Proc. NIPS.",
    "year": 2014
  }, {
    "title": "Backpropagation through time: what it does and how to do it",
    "authors": ["P.J. Werbos."],
    "venue": "Proceedings of the IEEE, 78(10):1550–1560.",
    "year": 1990
  }],
  "id": "SP:2759976e7d8a27c788fb52a24830fc63ce0de570",
  "authors": [{
    "name": "Xing Shi",
    "affiliations": []
  }, {
    "name": "Kevin Knight",
    "affiliations": []
  }, {
    "name": "Deniz Yuret",
    "affiliations": []
  }],
  "abstractText": "We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.",
  "title": "Why Neural Translations are the Right Length"
}