{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–739 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n732"
  }, {
    "heading": "1 Introduction",
    "text": "Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) has become the de-facto recurrent neural network (RNN) for learning representations of sequences in NLP. Like simple recurrent neural networks (S-RNNs) (Elman, 1990), LSTMs are able to learn non-linear functions of arbitrary-length input sequences. However, they also introduce an additional memory cell to mitigate the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994). This memory is controlled by a mechanism of gates, whose additive connections allow long-distance dependencies to be learned more easily during backpropagation. While this view is mathematically accurate, in this paper we argue that it does not provide a complete picture of why LSTMs work in practice.\n∗The first two authors contributed equally to this paper.\nWe present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously realized. To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN’s outputs over time, which is implicitly computed by the gates. We hypothesize that, for many practical NLP problems, the weighted sum serves as the main modeling component. The SRNN, while theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model. By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing.\nWe test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al., 2017). We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation. Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance. We also show that, in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum. Together, these results suggest that the gates’ ability to compute an element-wise weighted sum, rather than the non-linear transition dynamics of S-RNNs, are the driving force behind LSTM’s success."
  }, {
    "heading": "2 What Do Memory Cells Compute?",
    "text": "LSTMs are typically motivated as an augmentation of simple RNNs (S-RNNs), defined as:\nht = tanh(Whhht−1 +Whxxt + bh) (1)\nS-RNNs suffer from the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994) due to compounding multiplicative updates of the hidden state. By introducing a memory cell and an output layer controlled by gates, LSTMs enable shortcuts through which gradients can flow when learning with backpropagation. This mechanism enables learning of long-distance dependencies while preserving the expressive power of recurrent nonlinear transformations provided by S-RNNs.\nRather than viewing the gates as simply an auxiliary mechanism to address a learning problem, we present an alternate view that emphasizes their modeling strengths. We argue that the LSTM should be interpreted as a hybrid of two distinct recurrent architectures: (1) the S-RNN which provides multiplicative connections across timesteps, and (2) the memory cell which provides additive connections across timesteps. On top of these recurrences, an output layer is included that simply squashes and filters the memory cell at each step.\nThroughout this paper, let {x1, . . . ,xn} be the sequence of input vectors, {h1, . . . ,hn} be the sequence of output vectors, and {c1, . . . , cn} be the memory cell’s states. Then, given the basic LSTM definition below, we can formally identify three sub-components.\nc̃t = tanh(Wchht−1 +Wcxxt + bc) (2)\nit = σ(Wihht−1 +Wixxt + bi) (3)\nft = σ(Wfhht−1 +Wfxxt + bf ) (4) ct = it ◦ c̃t + ft ◦ ct−1 (5) ot = σ(Wohht−1 +Woxxt + bo) (6) ht = ot ◦ tanh(ct) (7)\nContent Layer (Equation 2) We refer to c̃t as the content layer, which is the output of an SRNN. Evaluating the need for multiplicative recurrent connections in the content layer is the focus of this work. The content layer is passed to the memory cell, which decides which parts of it to store.\nMemory Cell (Equations 3-5) The memory cell ct is controlled by two gates. The input gate it controls what part of the content (c̃t) is written to the memory, while the forget gate ft controls\nwhat part of the memory is deleted by filtering the previous state of the memory (ct−1). Writing to the memory is done by adding the filtered content (it ◦ c̃t) to the retained memory (ft ◦ ct−1).\nOutput Layer (Equations 6-7) The output layer ht passes the memory cell through a tanh activation function and uses an output gate ot to read selectively from the squashed memory cell.\nOur goal is to study how much each of these components contribute to the empirical performance of LSTMs. In particular, it is worth considering the memory cell in more detail to reveal why it could serve as a standalone powerful model of long-distance context. It is possible to show that it implicitly computes an element-wise weighted sum of all the previous content layers by expanding the recurrence relation in Equation 5:\nct = it ◦ c̃t + ft ◦ ct−1\n= t∑ j=0 ( ij ◦ t∏ k=j+1 fk ) ◦ c̃j\n= t∑\nj=0\nwtj ◦ c̃j\n(8)\nEach weight wtj is a product of the input gate ij (when its respective input c̃j was read) and every subsequent forget gate fk. An interesting property of these weights is that, like the gates, they are also soft element-wise binary filters."
  }, {
    "heading": "3 Standalone Memory Cells are Powerful",
    "text": "The restricted space of element-wise weighted sums allows for easier mathematical analysis, visualization, and perhaps even learnability. However, constrained function spaces are also less expressive, and a natural question is whether these models will work well for NLP problems that involve understanding context. We hypothesize that the memory cell (which computes weighted sums) can function as a standalone contextualizer. To test this hypothesis, we present several simplifications of the LSTM’s architecture (Section 3.1), and show on a variety of NLP benchmarks that there is a qualitative performance difference between models that contain a memory cell and those that do not (Section 3.2). We conclude that the content and output layers are relatively minor contributors, and that the space of element-wise weighted sums is sufficiently powerful to compete with fully parameterized LSTMs (Section 3.3)."
  }, {
    "heading": "3.1 Simplified Models",
    "text": "The modeling power of LSTMs is commonly assumed to derive from the S-RNN in the content layer, with the rest of the model acting as a learning aid to bypass the vanishing gradient problem. We first isolate the S-RNN by ablating the gates (denoted as LSTM – GATES for consistency).\nTo test whether the memory cell has enough modeling power of its own, we take an LSTM and replace the S-RNN in the content layer from Equation 2 with a simple linear transformation (c̃t = Wcxxt) creating the LSTM – S-RNN model.\nWe further simplify the LSTM by removing the output gate from Equation 7 (ht = tanh(ct)), leaving only the activation function in the output layer (LSTM – S-RNN – OUT). After removing the S-RNN and the output gate from the LSTM, the entire ablated model can be written in a modular, compact form:\nht = OUTPUT ( t∑\nj=0\nwtj ◦ CONTENT(xj) ) (9)\nwhere the content layer CONTENT(·) and the output layer OUTPUT(·) are both context-independent functions, making the entire model highly constrained and mathematically simpler. The complexity of modeling contextual information is needed only for computing the weights wtj . As we will see in Section 3.2, both of these ablations perform on par with LSTMs on several tasks.\nFinally, we ablate the hidden state from the gates as well, by computing each gate gt via σ(Wgxxt+bg). In this model, the only recurrence is the additive connection in the memory cell; it has no multiplicative recurrent connections at all. It can be seen as a type of QRNN (Bradbury et al., 2016) or SRU (Lei et al., 2017b), but for consistency we label it as LSTM – S-RNN – HIDDEN."
  }, {
    "heading": "3.2 Experiments",
    "text": "We compare model performance on four NLP tasks, with an experimental setup that is lenient towards LSTMs and harsh towards its simplifications. In each case, we use existing implementations and previously reported hyperparameter settings. Since these settings were tuned for LSTMs, any simplification that performs equally to (or better than) LSTMs under these LSTM-friendly settings provides strong evidence that the ablated component is not a contributing factor. For each\ntask we also report the mean and standard deviation of 5 runs of the LSTM settings to demonstrate the typical variance observed due to training with different random initializations.\nLanguage Modeling We evaluate the models on the Penn Treebank (PTB) (Marcus et al., 1993) language modeling benchmark. We use the implementation of Zaremba et al. (2014) from TensorFlow’s tutorial while replacing any invocation of LSTMs with simpler models. We test two of their configurations: medium and large (Table 1).\nQuestion Answering For question answering, we use two different QA systems on the Stanford question answering dataset (SQuAD) (Rajpurkar et al., 2016): the Bidirectional Attention Flow model (BiDAF) (Seo et al., 2016) and DrQA (Chen et al., 2017). BiDAF contains 3 LSTMs, which are referred to as the phrase layer, the modeling layer, and the span end encoder. Our experiments replace each of these LSTMs with their simplified counterparts. We directly use the implementation of BiDAF from AllenNLP (Gardner et al., 2017), and all experiments reuse the existing hyperparameters that were tuned for LSTMs. Likewise, we use an open-source implementation of DrQA1 and replace only the LSTMs, while leaving everything else intact. Table 2 shows the results.\nDependency Parsing For dependency parsing, we use the Deep Biaffine Dependency Parser (Dozat and Manning, 2016), which relies on stacked bidirectional LSTMs to learn contextsensitive word embeddings for determining arcs between a pair of words. We directly use their released implementation, which is evaluated on the Universal Dependencies English Web Treebank v1.3 (Silveira et al., 2014). In our experiments, we use the existing hyperparameters and only replace the LSTMs with the simplified architectures. Table 3 shows the results.\nMachine Translation For machine translation, we used OpenNMT (Klein et al., 2017) to train English to German translation models on the multimodal benchmarks from WMT 2016 (used in OpenNMT’s readme file). We use OpenNMT’s default model and hyperparameters, replacing the stacked bidirectional LSTM encoder with the sim-\n1https://github.com/hitvoice/DrQA\nplified architectures.2 Table 4 shows the results."
  }, {
    "heading": "3.3 Discussion",
    "text": "We showed four major ablations of the LSTM. In the S-RNN experiments (LSTM – GATES), we ablate the memory cell and the output layer. In the LSTM – S-RNN and LSTM – S-RNN – OUT experiments, we ablate the S-RNN. In the LSTM – SRNN – HIDDEN, we remove not only the S-RNN in the content layer, but also the S-RNNs in the gates, resulting in a model whose sole recurrence is in the memory cell’s additive connection.\nAs consistent with previous literature, removing the memory cell degrades performance drastically. In contrast, removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP.\nEven after removing every multiplicative recurrence from the memory cell itself, the model’s performance remains well above the vanilla S-\n2For the S-RNN baseline (LSTM – GATES), we had to tune the learning rate to 0.1 because the default value (1.0) resulted in exploding gradients. This is the only case where hyperparameters were modified in all of our experiments.\nRNN’s, and falls within the standard deviation of an LSTM’s on some tasks (see Table 3). This latter result indicates that the additive recurrent connection in the memory cell – and not the multiplicative recurrent connections in the content layer or in the gates – is the most important computational element in an LSTM. As a corollary, this result also suggests that a weighted sum of context words, while mathematically simple, is a powerful model of contextual information."
  }, {
    "heading": "4 LSTM as Self-Attention",
    "text": "Attention mechanisms are widely used in the NLP literature to aggregate over a sequence (Cho et al., 2014; Bahdanau et al., 2015) or contextualize tokens within a sequence (Cheng et al., 2016; Parikh et al., 2016) by explicitly computing weighted sums. In the previous sections, we demonstrated that LSTMs implicitly compute weighted sums as well, and that this computation is central to their success. How, then, are these two computations related, and in what ways do they differ?\nAfter simplifying the content layer and removing the output gate (LSTM – S-RNN – OUT), the model’s computation can be expressed as a weighted sum of context-independent functions of the inputs (Equation 9 in Section 3.1). This formula abstracts over both the simplified LSTM and the family of attention mechanisms, and through this lens, the memory cell’s computation can be seen as a “cousin” of self-attention. In fact, we can also leverage this abstraction to visualize the\nsimplified LSTM’s weights as is commonly done with attention (see Appendix A for visualization).\nHowever, there are three major differences in how the weights wtj are computed.\nFirst, the LSTM’s weights are vectors, while attention typically computes scalar weights; i.e. a separate weighted sum is computed for every dimension of the LSTM’s memory cell. Multiheaded self-attention (Vaswani et al., 2017) can be seen as a middle ground between the two approaches, allocating a scalar weight for different subsets of the dimensions.\nSecond, the weighted sum is accumulated with a dynamic program. This enables a linear rather than quadratic complexity in comparison to selfattention, but reduces the amount of parallel computation. This accumulation also creates an inductive bias of attending to nearby words, since the weights can only decrease over time.\nFinally, attention has a probabilistic interpretation due to the softmax normalization, while the sum of weights in LSTMs can grow up to the sequence length. In variants of the LSTM that tie the input and forget gate, such as coupled-gate LSTMs (Greff et al., 2016) and GRUs (Cho et al., 2014), the memory cell instead computes a weighted average with a probabilistic interpretation. These variants compute locally normalized distributions via a product of sigmoids rather than globally normalized distributions via a single softmax."
  }, {
    "heading": "5 Related Work",
    "text": "Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) have been previously explored. These typically consist of a different parameterization of the gates, such as LSTMs with peephole connections (Gers and Schmidhuber, 2000), or a rewiring of the connections, such as GRUs (Cho et al., 2014). However, these modifications invariably maintain the recurrent content layer. Even more systematic explorations (Józefowicz et al., 2015; Greff et al., 2016; Zoph and Le, 2017) do not question the importance of the embedded SRNN. This is the first study to provide applesto-apples comparisons between LSTMs with and without the recurrent content layer.\nSeveral other recent works have also reported promising results with recurrent models that are vastly simpler than LSTMs, such as quasirecurrent neural networks (Bradbury et al., 2016), strongly-typed recurrent neural networks (Bal-\nduzzi and Ghifary, 2016), recurrent additive networks (Lee et al., 2017), kernel neural networks (Lei et al., 2017a), and simple recurrent units (Lei et al., 2017b), making it increasingly apparent that LSTMs are over-parameterized. While these works indicate an obvious trend, they do not focus on explaining what LSTMs are learning. In our carefully controlled ablation studies, we propose and evaluate the minimal changes required to test our hypothesis that LSTMs are powerful because they dynamically compute element-wise weighted sums of content layers."
  }, {
    "heading": "6 Conclusion",
    "text": "We presented an alternate view of LSTMs: they are a hybrid of S-RNNs and a gated model that dynamically computes weighted sums of the S-RNN outputs. Our experiments investigated whether the S-RNN is a necessary component of LSTMs. In other words, are the gates alone as powerful of a model as an LSTM? Results across four major NLP tasks (language modeling, question answering, dependency parsing, and machine translation) indicate that LSTMs suffer little to no performance loss when removing the S-RNN. This provides evidence that the gating mechanism is doing the heavy lifting in modeling context. We further ablate the recurrence in each gate and find that this incurs only a modest drop in performance, indicating that the real modeling power of LSTMs stems from their ability to compute element-wise weighted sums of context-independent functions of their inputs.\nThis realization allows us to mathematically relate LSTMs and other gated RNNs to attention-based models. Casting an LSTM as a dynamically-computed attention mechanism enables the visualization of how context is used at every timestep, shedding light on the inner workings of the relatively opaque LSTM."
  }, {
    "heading": "Acknowledgements",
    "text": "The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google, Tencent, and Nvidia, and an Allen Distinguished Investigator Award. We also thank Yoav Goldberg, Benjamin Heinzerling, Tao Lei, and the UW NLP group for helpful conversations and comments on the work."
  }, {
    "heading": "A Weight Visualization",
    "text": "Given the empirical evidence that LSTMs are effectively learning weighted sums of the content layers, it is natural to investigate what weights the model learns in practice. Using the more mathematically transparent simplification of LSTMs, we can visualize the weights wtj that are placed on every input j at every timestep t (see Equation 9).\nUnlike attention mechanisms, these weights are vectors rather than scalar values. Therefore, we can only provide a coarse-grained visualization of the weights by rendering their L2-norm, as shown in Table 5. In the visualization, each column indicates the word represented by the weighted sum, and each row indicates the word over which the weighted sum is computed. Dark horizontal streaks indicate the duration for which a word was remembered. Unsurprisingly, the weights on the diagonal are always the largest since it indicates the weight of the current word. More interesting task-specific patterns emerge when inspecting the off-diagonals that represent the weight on the context words.\nThe first visualization uses the language model. Due to the language modeling setup, there are only non-zero weights on the current or previous words. We find that the common function words are quickly forgotten, while infrequent words that\nsignal the topic are remembered over very long distances.\nThe second visualization uses the dependency parser. In this setting, since the recurrent architectures are bidirectional, there are non-zero weights on all words in the sentence. The top-right triangle indicates weights from the forward direction, and the bottom-left triangle indicates from the backward direction. For syntax, we see a significantly different pattern. Function words that are useful for determining syntax are more likely to be remembered. Weights on head words are also likely to persist until the end of a constituent.\nThis illustration provides only a glimpse into what the model is capturing, and perhaps future, more detailed visualizations that take the individual dimensions into account can provide further insight into what LSTMs are learning in practice.\n739\nLanguage model weights Dependency parser weights\nThe hym n was sun g at my first inau gura\nl\nchu rch serv ice as gov erno\nr\nThe hymn\nwas\nsung\nat\nmy first inaugural\nchurch service\nas governor\nThe hym n was sun g at my first inau gura\nl\nchu rch serv ice as gov erno\nr\nThe hymn\nwas\nsung\nat\nmy first inaugural\nchurch service\nas governor\nUS troo ps ther e clas hed with gue rrilla s in a figh t that left one Iraq i dea d\nUS troops there clashed with guerrillas in a\nfight that left one Iraqi dead\nUS troo ps ther e clas hed with gue rrilla s in a figh t that left one Iraq i dea d\nUS troops there clashed with guerrillas in a\nfight that left one Iraqi dead"
  }],
  "year": 2018,
  "references": [{
    "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
    "authors": ["Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Strongly-typed recurrent neural networks",
    "authors": ["David Balduzzi", "Muhammad Ghifary."],
    "venue": "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016. pages 1292–",
    "year": 2016
  }, {
    "title": "Learning long-term dependencies with gradient descent is difficult",
    "authors": ["Yoshua Bengio", "Patrice Y. Simard", "Paolo Frasconi."],
    "venue": "IEEE Transactions on Neural Networks 5(2):157–166.",
    "year": 1994
  }, {
    "title": "Quasi-recurrent neural networks",
    "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher."],
    "venue": "CoRR abs/1611.01576.",
    "year": 2016
  }, {
    "title": "Reading wikipedia to answer open-domain questions",
    "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-",
    "year": 2017
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
    "year": 2016
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings",
    "year": 2014
  }, {
    "title": "Deep biaffine attention for neural dependency parsing",
    "authors": ["Timothy Dozat", "Christopher D. Manning."],
    "venue": "CoRR abs/1611.01734.",
    "year": 2016
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L. Elman."],
    "venue": "Cognitive Science 14:179–211.",
    "year": 1990
  }, {
    "title": "Recurrent nets that time and count",
    "authors": ["Felix A. Gers", "Jürgen Schmidhuber."],
    "venue": "IJCNN.",
    "year": 2000
  }, {
    "title": "Lstm: A search space odyssey",
    "authors": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutnı́k", "Bas R Steunebrink", "Jürgen Schmidhuber"],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "year": 2016
  }, {
    "title": "Deep semantic role labeling: What works and whats next",
    "authors": ["Luheng He", "Kenton Lee", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Untersuchungen zu dynamischen neuronalen netzen",
    "authors": ["Sepp Hochreiter."],
    "venue": "Diploma, Technische Universität München 91.",
    "year": 1991
  }, {
    "title": "Long Short-term Memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["Rafal Józefowicz", "Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Opennmt: Open-source toolkit for neural machine translation",
    "authors": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush."],
    "venue": "Proc. ACL. https://doi.org/10.18653/ v1/P17-4012.",
    "year": 2017
  }, {
    "title": "Recurrent additive networks",
    "authors": ["Kenton Lee", "Omer Levy", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1705.07393 .",
    "year": 2017
  }, {
    "title": "Deriving neural architectures from sequence and graph kernels",
    "authors": ["Tao Lei", "Wengong Jin", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "ICML.",
    "year": 2017
  }, {
    "title": "Training rnns as fast as cnns",
    "authors": ["Tao Lei", "Yu Zhang", "Yoav Artzi."],
    "venue": "arXiv preprint arXiv:1709.02755 .",
    "year": 2017
  }, {
    "title": "Assessing the ability of lstms to learn syntaxsensitive dependencies",
    "authors": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg."],
    "venue": "TACL 4:521–535.",
    "year": 2016
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."],
    "venue": "Computational Linguistics 19:313–330.",
    "year": 1993
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. As-",
    "year": 2016
  }, {
    "title": "Squad: 100, 000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "CoRR abs/1611.01603.",
    "year": 2016
  }, {
    "title": "A gold standard dependency corpus for English",
    "authors": ["Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer", "Christopher D. Manning."],
    "venue": "Proceedings of the Ninth International Conference",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "arXiv preprint arXiv:1706.03762 .",
    "year": 2017
  }, {
    "title": "Recurrent neural network regularization",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."],
    "venue": "arXiv preprint arXiv:1409.2329 .",
    "year": 2014
  }, {
    "title": "Neural architecture search with reinforcement learning",
    "authors": ["Barret Zoph", "Quoc V Le."],
    "venue": "ICLR.",
    "year": 2017
  }],
  "id": "SP:2ccd5ae02fe745d640a7909a8b99b57a9fc788e6",
  "authors": [{
    "name": "Omer Levy",
    "affiliations": []
  }, {
    "name": "Kenton Lee",
    "affiliations": []
  }, {
    "name": "Nicholas FitzGerald",
    "affiliations": []
  }, {
    "name": "Luke Zettlemoyer",
    "affiliations": []
  }, {
    "name": "Paul G. Allen",
    "affiliations": []
  }],
  "abstractText": "LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections. We present an alternative view to explain the success of LSTMs: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated. We do this by decoupling the LSTM’s gates from the embedded simple RNN, producing a new class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input. Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM in most settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.",
  "title": "Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum"
}