{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 633–639 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n633"
  }, {
    "heading": "1 Introduction",
    "text": "In natural language processing fields, full neural network based methods are suitable for joint modeling as they can simultaneously utilize multiple task data sets or multiple language data sets to improve the performance achieved for individual tasks or languages (Collobert and Weston, 2008). It is known that joint modeling can address the data scarcity problem.\nKey natural language processing technologies for spoken dialogue systems include utterance intent classification, which is needed to detect intent labels such as dialogue act (Stolcke et al., 2000; Khanpour et al., 2016), domain (Xu and Sarikaya, 2014), and question type (Wu et al., 2005) from input utterances (Ravuri and Stolcke,\n2015a,b, 2016). One problem is that the training data are often limited or unbalanced among different tasks or different languages. Therefore, our motivation is to leverage both multi-task joint modeling and multi-lingual joint modeling to enhance utterance intent classification.\nThe multi-task and multi-lingual joint modeling can be composed by introducing both task-specific networks, which are shared among different languages, and language-specific networks, which are shared among different tasks (Masumura et al., 2018; Lin et al., 2018). Although joint modeling is mainly intended to improve classification performance in resource-poor tasks or languages, its classification performance is degraded in some minor data sets. This is because the languagespecific networks often depend on majority tasks, while the task-specific networks often depend on majority languages. What are needed are taskspecific networks that are invariant to languages, and language-specific networks that are invariant to tasks.\nIn order to explicitly improve the invariance of language and task-specific networks, this paper introduces adversarial training (Goodfellow et al., 2014). Our idea is to train language-specific networks so as to be insensitive to the target task, while training task-specific networks to be insensitive to language. To this end, we introduce multiple domain adversarial networks (Ganin et al., 2016), language-specific task adversarial networks, and task-specific language adversarial networks, into a state-of-the-art fully neural network based joint modeling; we adopt the bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) with attention mechanism (Yang et al., 2016; Zhou et al., 2016). To the best of our knowledge, this paper is the first study to employ adversarial training for multi-input and multi-output joint modeling.\nExperiments on Japanese and English data sets demonstrate the effectiveness of the adversarial training proposal. To support spoken dialogue systems, three different utterance intent classification tasks are examined: dialogue act, topic type, and question type classification."
  }, {
    "heading": "2 Related Work",
    "text": "Joint Modeling: In natural language processing research, joint modeling is usually split into multitask joint modeling and multi-lingual joint modeling. Multi-task joint modeling has been shown to effectively improve individual tasks (Collobert and Weston, 2008; Liu et al., 2016a,b; Zhang and Weng, 2016; Liu et al., 2016c). In addition, multi-lingual joint modeling is achieved by learning common semantic representations among different languages (Guo et al., 2016; Duong et al., 2016; Zhang et al., 2016, 2017b). In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018). Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling. Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training. Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 2017; Chen and Cardie, 2018). Most natural language processing papers adopted either the language invariant approach (Chen et al., 2016; Zhang et al., 2017a) or the task invariant approach (Ganin et al., 2016; Liu et al., 2017; Chen and Cardie, 2018). This paper aims to fully utilize both task adversarial training and language adversarial training. To this end, we simultaneously introduce language-specific task adversarial networks and task-specific language adversarial networks."
  }, {
    "heading": "3 Proposed Method",
    "text": "This section details our adversarial training method for multi-task and multi-lingual joint modeling of utterance intent classification.\nIn the j-th task utterance intent classification for the i-th language input utterance, intent label l(j) ∈ {1, · · · ,K(j)} is estimated from input utterance W(i) = {w(i)1 , · · · , w (i) T } where i ∈ {1, · · · , I} and j ∈ {1, · · · , J}. Utter-\nance intent classification is followed by estimation of the probabilities of each intent label given input utterance, P (l(j)|W(i),Θ(i,j)) where Θ(i,j) is the trainable model parameter for the combination of the i-th language and the j-th task. In multi-task and multi-lingual joint modeling, {Θ(1,1), · · · ,Θ(I,J)} are jointly trained from I language and J task data sets."
  }, {
    "heading": "3.1 Main Joint Network",
    "text": "The proposed method is founded on a fully neural network that employs I language-specific networks, J task-specific networks, and J classification networks as well as Masumura et al. (2018).\nThe language-specific network can be shared between multiple tasks, where words in the input utterance are converted into language-specific hidden representations. Each word in the i-th language input utterance W(i) is first converted into a continuous representation. Next, each word representation is converted into a hidden representation that uses BLSTM-RNNs to take neighboring word context information into account. The t-th language-specific hidden representation for the ith language is given by:\nw (i) t = EMBED(w (i) t ;θ (i) h ), (1)\nh (i) t = BLSTM({w (i) 1 , · · · ,w (i) T }, t;θ (i) h ), (2)\nwhere EMBED() is a linear transformational function for word embedding, BLSTM() is a function of the BLSTM-RNN layer, and θ(i)h is the trainable parameter for the i-th language-specific network.\nIn addition, task-specific networks can be shared between multiple languages, where the language-specific hidden representations are converted into task-specific hidden representations. The t-th language-specific hidden representation for the j-th task is given by:\nu (j) t = BLSTM({h (i) 1 , · · · ,h (i) T }, t;θ (j) u ), (3)\nwhere θ(j)u is the trainable parameter for the j-th task-specific network.\nIn classification networks for each task, the task-specific hidden representations are summarized as sentence representation s(j) by using a self-attention mechanism that can consider the importance of individual hidden representations (Yang et al., 2016; Zhou et al., 2016; Sawada et al., 2017). Next, predicted probabilities of the j-th\ntask intent labels, o(j) ∈ RK(j) , are given by:\ns(j) = ATTENSUM({h(i)1 , · · · ,h (i) T };θ (j) o ), (4) o(j) = SOFTMAX(s(j);θ(j)o ), (5)\nwhere ATTENSUM() is a weighted sum function with self-attention, SOFTMAX() is a transformational function with softmax activation, and θ(j)o is the trainable parameter for the j-th classification network. In the main joint networks of the proposal, Θ(i,j) corresponds to {θ(i)h , θ (j) u ,θ (j) o }."
  }, {
    "heading": "3.2 Adversarial Networks",
    "text": "The proposed method combines a languagespecific task adversarial network with a taskspecific language adversarial network. The task adversarial network is used for training the language-specific networks to be insensitive to target task labels, and the language adversarial network is used for training the task-specific networks to be insensitive to target language labels. In order to efficiently use stochastic gradient descent based training for optimizing the adversarial networks, we use gradient reversal layers, which allow the input vectors during forward propagation, and sign inversion of the gradients during back propagation, to be utilized (Ganin et al., 2016).\nThe i-th language-specific task adversarial network estimates task labels from the i-th languagespecific hidden representations. The predicted probabilities of task labels, x(i) ∈ RJ , are given by:\nh̃ (i) t = GRL(h (i) t ), (6) h̃(i) = ATTENSUM({h̃(i)1 , · · · , h̃ (i) T };θ (i) x ), (7) x(i) = SOFTMAX(h̃(i),θ(i)x ), (8)\nwhere GRL() represents the gradient reversal layer, and θ(i)x is the trainable parameter. The j-th taskspecific language adversarial network estimates language labels from the j-th task-specific hidden representations. The predicted probabilities of language labels, y(j) ∈ RI , are given by:\nũ (j) t = GRL(u (j) t ), (9) ũ(j) = ATTENSUM({ũ(j)1 , · · · , ũ (j) T };θ (j) y ), (10) y(j) = SOFTMAX(ũ(j),θ(j)y ), (11)\nwhere θy is the trainable parameter.\nThe proposed network structure shown in Figure 1 includes both joint networks and adversarial networks for two tasks and two languages. The red components are language-specific networks, the orange components are task-specific networks, and the purple components are classification networks. In addition, green components are language-specific task adversarial networks, and blue components are task-specific language adversarial networks."
  }, {
    "heading": "3.3 Training",
    "text": "Our adversarial training proposal jointly optimizes all parameters in both the main joint networks and the adversarial networks by using all training data sets {D(1,1), · · · ,D(I,J)} where D(i,j) represents the sets of the input utterances and the reference. The cross-entropy loss functions of each network are defined as:\nLo = − I∑\ni=1 J∑ j=1 |D(i,j)|∑ n=1 K(j)∑ k=1 ô (j) n,k log o (j) n,k, (12)\nLx = − I∑\ni=1 J∑ j=1 |D(i,j)|∑ n=1 J∑ j′=1 x̂ (i) n,j′ logx (i) n,j′ , (13)\nLy = − I∑\ni=1 J∑ j=1 |D(i,j)|∑ n=1 I∑ i′=1 ŷ (j) n,i′ log y (j) n,i′ , (14)\nwhere Lo, Lx, and Ly are the cross entropy loss terms for the classification networks, the task adversarial networks, and the language adversarial networks. ô(j)n,k, x̂ (i) n,j′ , and ŷ (j) n,i′ are the reference probabilities, and on,k, xn,j′ , and yn,i′ are the estimated probabilities of the k-th label in the j-th task classification network, the j′-th task in the ith language-specific task adversarial network, and\nthe i′-th language in the j-th task-specific language adversarial network forWn, respectively.\nDue to use of gradient reversal layers, individual parameters are gradually updated as follows:\nθ(j)o ← θ(j)o − ∂Lo\n∂θ (j) o\n, (15)\nθ(j)y ← θ(j)y − β ∂Ly\n∂θ (j) y\n, (16)\nθ(j)u ← θ(j)u − ( ∂Lo\n∂θ (j) u − β ∂Ly ∂θ (j) u ), (17)\nθ(i)x ← θ(i)x − α ∂Lx\n∂θ (i) x\n, (18)\nθ (i) h ← θ (i) h − (\n∂Lo ∂θ (i) h − α ∂Lx ∂θ (i) h − β ∂Ly ∂θ (i) h ),\n(19)\nwhere α and β are hyper parameters of the parameter update, and is the learning rate. Note that adversarial training is suppressed by setting α and β to 0.0. In training, we prepared optimizers for individual data sets. The individual learning rates fall when the validation loss of the target classification network increases."
  }, {
    "heading": "4 Experiments",
    "text": "Our experiments employed Japanese and English data sets created for three different utterance intent classification tasks. The tasks, dialogue act (DA) classification, topic type (TT) classification, and question type (QT) classification, are intended to support spoken dialogue systems. For example, the task of English DA classification is to obtain a DA label from an input utterance. We used natural language texts as the input utterances and individual label sets were unified between Japanese and English. Data sets employed in experiments were corpora that were made for constructing spoken dialogue systems (Masumura et al., 2018). Each of the data sets were divided into training (Train),\nvalidation (Valid), and test (Test) sets. Table 1 shows the number of utterances in individual data sets where #labels represents the number of labels. Table 2 shows English utterances and label examples for individual tasks."
  }, {
    "heading": "4.1 Setups",
    "text": "We examined single-task and mono-lingual modeling, multi-task joint modeling, multi-lingual join modeling, and multi-task and multi-lingual joint modeling with or without adversarial training.\nWe unified network configurations as follows. Word representation size was set to 128, BLSTMRNN unit size was set to 400, and sentence representation was set to 400. Dropout was used for EMBED() and BLSTM(), and the dropout rate was set to 0.5. Words that appeared only once in the training data sets were treated as unknown words. We used mini-batch stochastic gradient descent, in which initial learning rate was set to 0.1. We optimized hyper-parameters of adversarial training (α and β) for the validation sets by varying them from 0.001 to 1.0. Other hyper parameters were also optimized for the validation sets."
  }, {
    "heading": "4.2 Results",
    "text": "Table 3 shows the results in terms of utterance classification accuracy. For each setup, we constructed five models by varying the initial parameters and evaluated the average accuracy. Line (1) shows baseline results: single-task and monolingual modeling. Lines (2) and (3) show results\nwith only performing multi-task joint modeling, and lines (4) and (5) show results with only performing multi-lingual joint modeling. Note that lines (3) and (5) show the results achieved with adversarial training. Line (6) shows multi-task and multi-lingual joint modeling results: adversarial training was suppressed by setting both α and β to 0.0. Lines (7)–(9) shows the results achieved with adversarial training. Note that setting with bold values achieved the highest performance in our evaluation.\nFirst, in lines (2) and (4), the classification performance deteriorated in some cases, while performance improvements were achieved in other cases. On the other hand, in lines (3) and (5), classification performance in each data sets was improved by introducing adversarial training. This indicates that adversarial training was effective in improving the performance of joint modeling.\nNext, line (6) shows that, relative to line 1, multi-task and multi-lingual joint modeling can improve the classification performance for Japanese TT, Japanese QT, and English TT, but classification performance was degraded for English DA and English QT. This indicates that it is difficult to simultaneously improve the classification performance for all data sets because joint modeling often depends on majority tasks or majority languages. In addition, lines (7) and (8) show the introduction of either task adversarial networks or language adversarial networks yielded better performance than line (6) for all data sets. This indicates that adversarial training was effective in improving the performance of multi-task and multi-lingual joint modeling. The best results were achieved by using both language-specific task adversarial networks and task-specific language adversarial networks, line (9). These results confirm that task adversarial\nnetworks and language adversarial networks well complement each other. Of particular benefit, the proposed method demonstrated greater classification performance improvements when the number of training utterances per label was small."
  }, {
    "heading": "5 Conclusions",
    "text": "We have proposed an adversarial training method for the multi-task and multi-lingual joint modeling needed to enhance utterance intent classification. Our adversarial training proposal utilizes both task adversarial networks and language adversarial networks for improving task-invariance in languagespecific networks and language-invariance in taskspecific networks. Experiments showed that the adversarial training proposal could well realize the benefits of joint modeling in all data sets."
  }],
  "year": 2018,
  "references": [{
    "title": "Multinomial adversarial networks for multi-domain text classification",
    "authors": ["Xilun Chen", "Claire Cardie."],
    "venue": "arXiv preprint arXiv:1802.05694.",
    "year": 2018
  }, {
    "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
    "authors": ["Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger."],
    "venue": "arXiv preprint arXiv:1606.01614.",
    "year": 2016
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proc. International Conference on Machine Learning (ICML).",
    "year": 2008
  }, {
    "title": "Learning crosslingual word embeddings without bilingual corpora",
    "authors": ["Long Duong", "Hiroshi Kanayama", "Tengfei Ma", "Steven Bird", "Trevor Cohn."],
    "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1285–1295.",
    "year": 2016
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), pages 2672–2680.",
    "year": 2014
  }, {
    "title": "A representation learning framework for multi-source transfer parsing",
    "authors": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."],
    "venue": "Proc. AAAI Conference on Artificial Intelligence (AAAI), pages 2734–2740.",
    "year": 2016
  }, {
    "title": "Dialogue act classification in domain-independent conversations using a deep recurrent neural network",
    "authors": ["Hamed Khanpour", "Nishitha Guntakandla", "Rodney Nielsen."],
    "venue": "Proc. International Conference on Computational Linguistics (COLING),",
    "year": 2016
  }, {
    "title": "A multi-lingual multi-task architecture for low-resource sequence labeling",
    "authors": ["Ying Lin", "Shengqi Yang", "Veselin Stoyanov", "Heng Ji."],
    "venue": "Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages pp.799–809.",
    "year": 2018
  }, {
    "title": "Deep multi-task learning with shared memory",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 118–127.",
    "year": 2016
  }, {
    "title": "Recurrent neural network for text classification with multi-task learning",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proc. International Joint Conference on Artificial Intelligence (IJCAI), pages 2873–2879.",
    "year": 2016
  }, {
    "title": "Adversarial multi-task learning for text classification",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–10.",
    "year": 2017
  }, {
    "title": "Implicit discourse relation classification via multi-task neural networks",
    "authors": ["Yang Liu", "Sujian Li", "Xiaodong Zhang", "Zhifang Sui."],
    "venue": "Proc. AAAI Conference on Artificial Intelligence (AAAI), pages 2750– 2756.",
    "year": 2016
  }, {
    "title": "Multi-task and multi-lingual joint learning of neural lexical utterance classification based on partially-shared modeling",
    "authors": ["Ryo Masumura", "Tomohiro Tanaka", "Ryuichiro Higashinaka", "Hirokazu Masataki", "Yushi Aono."],
    "venue": "Proc. International",
    "year": 2018
  }, {
    "title": "Adversarial training methods for semisupervised text classification",
    "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow."],
    "venue": "Proc. International Conference on Learning Representation (ICLR).",
    "year": 2017
  }, {
    "title": "A comparative study of neural network models for lexical intent classification",
    "authors": ["Suman Ravuri", "Andreas Stolcke."],
    "venue": "Proc. Automatic Speech Recognition and Understanding Workshop (ASRU), pages 368–374.",
    "year": 2015
  }, {
    "title": "Recurrent neural network and LSTM models for lexical utterance classification",
    "authors": ["Suman Ravuri", "Andreas Stolcke."],
    "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), pages 135–139.",
    "year": 2015
  }, {
    "title": "A comparative study of recurrent neural network models for lexical domain classification",
    "authors": ["Suman Ravuri", "Andreas Stolcke."],
    "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6075–6079.",
    "year": 2016
  }, {
    "title": "Parallel hierarchical attention networks with shared memory reader for multi-stream conversational document classification",
    "authors": ["Naoki Sawada", "Ryo Masumura", "Hiromitsu Nishizaki."],
    "venue": "Proc. Annual Conference of the International Speech Com-",
    "year": 2017
  }, {
    "title": "Dialogue act modeling for automatic tagging and recognition",
    "authors": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martion", "Carol Van Ess-Dykema", "Marie Metter"],
    "year": 2000
  }, {
    "title": "Domain-specific FAQ retrieval using independent aspects",
    "authors": ["Chung-Hsien Wu", "Jui-Feng Yeh", "Ming-Jun Chen."],
    "venue": "ACM Transactions on Asian Language Information Processing, 4(1):1–17.",
    "year": 2005
  }, {
    "title": "Contextual domain classification in spoken language understanding systems using recurrent neural network",
    "authors": ["Puyang Xu", "Ruhi Sarikaya."],
    "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 136–140.",
    "year": 2014
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alexander J. Smola", "Eduard H. Hovy."],
    "venue": "Proc. Annual Conference of the North American Chapter of the Association for Computa-",
    "year": 2016
  }, {
    "title": "Inducing bilingual lexica from non-parallel data with earth mover’s distance regularization",
    "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Yiqun Liu", "Maosong Sun."],
    "venue": "Proc. International Conference on Computational Linguistics (COLING), pages 3188–",
    "year": 2016
  }, {
    "title": "Adversarial training for unsupervised bilingual lexicon induction",
    "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages 1959–1970.",
    "year": 2017
  }, {
    "title": "Bilingual lexicon induction from non-parallel data with minimum supervision",
    "authors": ["Meng Zhang", "Haoruo Peng", "Yang Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proc. AAAI Conference on Artificial Intelligence (AAAI), pages 3379–3384.",
    "year": 2017
  }, {
    "title": "A joint model of intent determination and slot filling for spoken language understanding",
    "authors": ["Xiaodong Zhang", "Houfeng Weng."],
    "venue": "Proc. International Joint Conference on Artificial Intelligence (IJCAI), pages 2993–2999.",
    "year": 2016
  }, {
    "title": "Attentionbased bidirectional long short-term memory networks for relation classification",
    "authors": ["Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu."],
    "venue": "Proc. Annual Meeting of the Association for Computational Lin-",
    "year": 2016
  }],
  "id": "SP:f0d1e36cd626485c7ce549843c16a892272894fa",
  "authors": [{
    "name": "Ryo Masumura",
    "affiliations": []
  }, {
    "name": "Yusuke Shinohara",
    "affiliations": []
  }, {
    "name": "Ryuichiro Higashinaka",
    "affiliations": []
  }, {
    "name": "Yushi Aono",
    "affiliations": []
  }],
  "abstractText": "This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification. In joint modeling, common knowledge can be efficiently utilized among multiple tasks or multiple languages. This is achieved by introducing both languagespecific networks shared among different tasks and task-specific networks shared among different languages. However, the shared networks are often specialized in majority tasks or languages, so performance degradation must be expected for some minor data sets. In order to improve the invariance of shared networks, the proposed method introduces both language-specific task adversarial networks and task-specific language adversarial networks; both are leveraged for purging the task or language dependencies of the shared networks. The effectiveness of the adversarial training proposal is demonstrated using Japanese and English data sets for three different utterance intent classification tasks.",
  "title": "Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification"
}