{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider finite-sum problems of the form\nmin w∈W\nF (w) = 1\nn n∑ i=1 fi(w), (1)\nwhere W is a closed convex subset of some Euclidean or Hilbert space, each fi is convex and µ-smooth, and F is λ-strongly convex1. Such problems are ubiquitous in machine learning, for example in order to perform empirical risk minimization using convex losses.\n1Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel. Correspondence to: Yossi Arjevani<yossi.arjevani@weizmann.ac.il>, Ohad Shamir <ohad.shamir@weizmann.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n1For a twice-differentiable function f , it is µ-smooth and λstrongly convex if λI ∇2f(w) µI for all w ∈ W .\nTo study the complexity of this and other optimization problems, it is common to consider an oracle model, where the optimization algorithm has no a-priori information about the objective function, and obtains information from an oracle which provides values and derivatives of the function at various domain points (Nemirovsky and Yudin, 1983). The complexity of the algorithm is measured in terms of the number of oracle calls required to optimize the function to within some prescribed accuracy.\nExisting lower bounds for finite-sum problems show that using a first-order oracle, which given a point w and index i = 1, . . . , n returns fi(w) and ∇fi(w), the number of oracle queries required to find an -optimal solution is at least of order\nΩ ( n+ √ nµ\nλ log\n( 1 )) ,\neither under algorithmic assumptions or assuming the dimension is sufficiently large2 (Agarwal and Bottou, 2014; Lan, 2015; Woodworth and Srebro, 2016; Arjevani and Shamir, 2016a). This is matched (up to log factors) by existing approaches, and cannot be improved in general.\nAn alternative to first-order methods are second-order methods, which also utilize Hessian information. A prototypical example is the Newton method, which given a (single) function F , performs iterations of the form\nwt+1 = wt − αt ( ∇2F (w) )−1∇F (w), (2) where ∇F (w),∇2F (w) are the gradient and the Hessian of F at w, and αt is a step size parameter. Second-order methods can have extremely fast convergence, better than those of first-order methods (i.e. quadratic instead of linear). Moreover, they can be invariant to affine transformations of the objective function, and provably independent of its strong convexity and smoothness parameters (assuming e.g. self-concordance) (Boyd and Vandenberghe, 2004). A drawback of these methods, however, is that they can be computationally prohibitive. In the context of machine\n2Depending on how -optimality is defined precisely, and where the algorithm is assumed to start, these bounds may have additional factors inside the log. For simplicity, we present the existing bounds assuming is sufficiently small, so that a log(1/ ) term dominates.\nlearning, we are often interested in high-dimensional problems (where the dimension d is very large), and the Hessians are d × d matrices which in general may not even fit into computer memory. However, for optimization problems as in Eq. (1), the Hessians of individual fi often have a special structure. For example, a very common special case of finite-sum problems in machine learning is empirical risk minimization for linear predictors, where\nfi(w) = `i(〈w,xi〉),\nwhere xi is a training instance and `i is some loss function. In that case, assuming `i is twice-differentiable, the Hessian has the rank-1 form `′′i (〈w,xi〉)xix>i . Therefore, the memory and computational effort involved with storing and manipulating the Hessian of this function is merely linear (rather than quadratic) in d. Thus, it is tractable even for high-dimensional problems.\nBuilding on this, several recent papers proposed and analyzed second-order methods for finite-sum problems, which utilize Hessians of the individual functions fi (see for instance Erdogdu and Montanari (2015); Agarwal et al. (2016); Pilanci and Wainwright (2015); Roosta-Khorasani and Mahoney (2016a;b); Bollapragada et al. (2016); Xu et al. (2016) and references therein). These can all be viewed as approximate Newton methods, which replace the actual Hessian∇2F (w) = 1n ∑n i=1∇2fi(w) in Eq. (2) by some approximation, based for instance on the Hessians of a few individual functions fi sampled at random. One may hope that such methods can inherit the favorable properties of second-order methods, and improve on the performance of commonly used first-order methods.\nIn this paper, we consider the opposite direction, and study lower bounds on the number of iterations required by algorithms using second-order (or possibly even higher-order) information, focusing on finite-sum problems which are strongly-convex and smooth. We make the following contributions:\n• First, as a more minor contribution, we prove that in the standard setting of optimizing a single smooth and strongly convex function, second-order information cannot improve the oracle complexity compared to first-order methods (at least in high dimensions). Although this may seem unexpected at first, the reason is that the smoothness constraint must be extended to higher-order derivatives, in order for higher-order information to be useful. We note that this observation in itself is not new, and is briefly mentioned (without proof) in Nemirovsky and Yudin (1983, Section 7.2.6). Our contribution here is in providing a clean, explicit statement and proof of this result.\n• We then turn to present our main results, which state\n(perhaps surprisingly) that under some mild algorithmic assumptions, and if the dimension is sufficiently large, the oracle complexity of second-order methods for finite-sum problems is no better than first-order methods, even if the finite-sum problem is composed of quadratics (which are trivially smooth to any order).\n• Despite this pessimistic conclusion, our results also indicate what assumptions and algorithmic approaches might be helpful in circumventing it. In particular, it appears that better, dimension-dependent performance may be possible, if the dimension is moderate and the n individual functions in Eq. (1) are accessed adaptively, in a manner depending on the functions rather than fixed in advance (e.g. sampling them from a non-uniform distribution depending on their Hessians, as opposed to sampling them uniformly at random). This provides evidence to the necessity of adaptive sampling schemes, and a dimension-dependent analysis, which indeed accords with some recently proposed algorithms and derivations, e.g. (Agarwal et al., 2016; Xu et al., 2016). We note that the limitations arising from oblivious optimization schemes (in a somewhat stronger sense) was also explored in (Arjevani and Shamir, 2016a;b).\nThe paper is structured as follows: We begin in Sec. 2 with a lower bound for algorithms utilizing second-order information, in the simpler setting where there is a single function F to be optimized, rather than a finite-sum problem. We then turn to provide our main lower bounds in Sec. 3, and discuss their applicability to some existing approaches in Sec. 4. We conclude in Sec. 5, where we also discuss possible approaches to circumvent our lower bounds. The formal proofs of our results appear in Appendix A."
  }, {
    "heading": "2. Strongly Convex and Smooth Optimization with a Second-Order Oracle",
    "text": "Before presenting our main results for finite-sum optimization problems, we consider the simpler problem of minimizing a single strongly-convex and smooth function F (or equivalently, Eq. (1) when n = 1), and prove a result which may be of independent interest.\nTo formalize the setting, we follow a standard oracle model, and assume that the algorithm does not have apriori information on the objective function F , except the strong-convexity parameter λ and smoothness parameter µ. Instead, it has access to an oracle, which given a point w ∈ W , returns values and derivatives of F at w (either ∇F (w) for a first-order oracle, or ∇F (w),∇2F (w) for a second-order oracle). The algorithm sequentially queries the oracle using w1,w2, . . . ,wT−1, and returns the point wT . Our goal is to lower bound the number of oracle calls\nT , required to ensure that wT is an -suboptimal solution.\nGiven a first-order oracle and a strongly convex and smooth objective in sufficiently high dimensions, it is well-known that the worst-case oracle complexity is\nΩ( √ µ/λ · log(1/ ))\n(Nemirovsky and Yudin, 1983). What if we replace this by a second-order oracle, which returns both ∇2F (w) on top of F (w),∇F (w)?\nPerhaps unexpectedly, it turns out that this additional information does not substantially improve the worst-case oracle complexity bound, as evidenced by the following theorem:\nTheorem 1. For any µ, λ such that µ > 8λ > 0, any ∈ (0, 1), and any deterministic algorithm, there exists a µ-smooth, λ strongly-convex function F on Rd (for d = Õ( √ µ/λ), hiding factors logarithmic in µ, λ, ), such that the number of calls T to a second-order oracle, required to ensure that F (wT ) − minw∈Rd F (w) ≤ · (F (0)−minw∈Rd F (w)), must be at least\nc\n(√ µ 8λ − 1 ) · log ( (λ/µ)3/2 c′ ) ,\nwhere c, c′ are positive universal constants.\nFor sufficiently large µλ and small , this complexity lower bound is Ω (√ µ λ · log ( 1 )) , which matches existing lower and upper bounds for optimizing strongly-convex and smooth functions using first-order methods. As mentioned earlier, the observation that such first-order oracle bounds can be extended to higher-order oracles is also briefly mentioned (without proof) in Nemirovsky and Yudin (1983, Section 7.2.6). Also, the theorem considers deterministic algorithms (which includes standard second-order methods, such as the Newton method), but otherwise makes no assumption on the algorithm. Generalizing this result to randomized algorithms should be quite doable, based on the techniques developed in Woodworth and Srebro (2016). We leave a formal derivation to future work.\nAlthough this result may seem surprising at first, it has a simple explanation: In order for Hessian information, which is local in nature, to be useful, there should be some regularity constraint on the Hessian, which ensures that it cannot change arbitrarily quickly as we move around the domain. A typical choice for a constraint of this kind is Lipschitz continuity which dictates that\n‖∇2F (w)−∇2F (w′)‖ ≤ L‖w −w′‖,\nfor some constant L. Indeed, the construction relies on a function which does not have Lipschitz Hessians: It is\nbased on a standard lower bound construction for firstorder oracles, but the function is locally “flattened” in certain directions around points which are to be queried by the algorithm. This is done in such a way, that the Hessian observed by the algorithm does not provide more information than the gradient, and cannot be used to improve the algorithm’s performance."
  }, {
    "heading": "3. Second-Order Oracle Complexity Bounds for Finite-Sum Problems",
    "text": "We now turn to study finite-sum problems of the form given in Eq. (1), and provide lower bounds on the number of oracle calls required to solve them, assuming a second-order oracle. To adapt the setting to a finite-sum problem, we assume that the second-order oracle is given both a point w and an index i ∈ {1, . . . , n}, and returns {fi(w),∇fi(w),∇2fi(w)}. The algorithm iteratively produces and queries the oracle with point-index pairs {(wt, it)}Tt=1, with the goal of making the suboptimality (or expected suboptimality, if the algorithm is randomized) smaller than using a minimal number of oracle calls T .\nIn fact, the lower bound construction we use is such that each function fi is quadratic. Unlike the construction of the previous section, such functions have a constant (and hence trivially Lipschitz) Hessian. Moreover, since any porder derivative of a quadratic for p > 2 is zero, this means that our lower bounds automatically hold even if the oracle provides p-th order derivatives at any w, for arbitrarily large p.\nHowever, in order to provide a lower bound using quadratic functions, it is necessary to pose additional assumptions on the structure of the algorithm (unlike Thm. 1 which is purely information-based). To see why, note that without computational constraints, the algorithm can simply query the Hessians and gradients of each fi(w) at w = 0, take the average to get ∇F (0) = 1n ∑n i=1∇fi(0) and\n∇2F (0) = 1n ∑n i=1∇2fi(0), and return the exact optimum, which for quadratics equals −∇2F (0)−1∇F (0). Therefore, with second-order information, the best possible information-based lower bound for quadratics is no better than Ω(n). This is not a satisfying bound, since in order to attain it we need to invert the (possibly high-rank) d× d matrix ∇2F (0). Therefore, if we are interested in bounds for computationally-efficient algorithms, we need to forbid such operations.\nSpecifically, we will consider two algorithmic assumptions, which are stated below (their applicability to existing algorithms is discussed in the next section). The first assumption constrains the algorithm to query and return points w which are computable using linear-algebraic manipula-\ntions of previous points, gradients and Hessians. Moreover, these manipulations can only depend on (at most) the last bn/2c Hessians returned by the oracle. As discussed previously, this assumption is necessary to prevent the algorithm from computing and inverting the full Hessian of F , which is computationally prohibitive. Formally, the assumption is the following:\nAssumption 1 (Linear-Algebraic Computations). wt belongs to the set Wt ⊆ Rd, defined recursively as follows: W1 = {0}, and Wt+1 is the closure of the set of vectors derived fromWt ∪ {∇fit(wt)} by a finite number of operations of the following form:\n• w,w′ → αw + α′w′, where α, α′ are arbitrary scalars.\n• w→ Hw, whereH is any d×d matrix which has the same block-diagonal structure as\nt∑ τ=max{1,t−bn/2c+1} ατ∇2fiτ (wτ ), (3)\nfor some arbitrary {ατ}.\nThe first bullet allows to take arbitrary linear combinations of previous points and gradients, and already covers standard first-order methods and their variants. As to the second bullet, by “same block-diagonal structure”, we mean that if the matrix in Eq. (3) can be decomposed to r diagonal blocks of size d1, . . . , dr in order, then H can also be decomposed into r blocks of size d1, . . . , dr in order (note that this does not exclude the possibility that each such block is composed of additional sub-blocks). To give a few examples, if we let Ht be the matrix in Eq. (3), then we may have:\n• H = Ht,\n• H = H−1t if Ht is invertible, or its pseudoinverse,\n• H = (Ht+D)−1 (whereD is some arbitrary diagonal matrix, possibly acting as a regularizer),\n• H is a truncated SVD decomposition of Ht (or again, Ht + D or (Ht + D)−1 for some arbitrary diagonal matrix D) or its pseudoinverse.\nMoreover, for quadratic functions, it is easily verified that the assumption also allows prox operations (i.e. returning arg minw fi(w) + ρ 2‖w − w\n′‖2 for some ρ, i and previously computed point w′). Also, note that the assumption places no limits on the number of such operations allowed\nbetween oracle calls. However, crucially, all these operations can be performed starting from a linear combination of at most bn/2c recent Hessians. As mentioned earlier, this is necessary, since if we could compute the average of all Hessians, then we could implement the Newton method. The assumption that the algorithm only “remembers” the last bn/2c Hessians is also realistic, as existing computationally-efficient methods seek to use much fewer than n individual Hessians at a time. We note that the choice of bn/2c is rather arbitrary, and can be replaced by αn for any constant α ∈ (0, 1). Also, the way the assumption is formulated, the algorithm is assumed to be initialized at the origin 0. However, this is merely for simplicity, and can be replaced by any other fixed vector (the lower bound will hold by shifting the constructed “hard” function appropriately).\nThe second (optional) assumption we will consider constrains the indices chosen by the algorithm to be oblivious, in the following sense:\nAssumption 2 (Index Obliviousness). The indices i1, i2, . . . chosen by the algorithm are independent of f1, . . . , fn.\nTo put this assumption differently, the indices may just as well be chosen before the algorithm begins querying the oracle. This can include, for instance, sampling functions fi uniformly at random from f1, . . . , fn, and performing deterministic passes over f1, . . . , fn in order. As we will see later on, this assumption is not strictly necessary, and can be removed at the cost of a somewhat weaker result. Nevertheless, the assumption covers all optimal first-order algorithms, as well as most second-order methods we are aware of (see Sec. 4 for more details).\nWith these assumptions stated, we can finally turn to present the main result of this section:\nTheorem 2. For any n > 1, any µ > λ > 0, any ∈ (0, c) (for some universal constant c > 0), and any (possibly randomized) algorithm satisfying Assumptions 1 and 2, there exists µ-smooth, λ-strongly convex quadratic functions f1, . . . , fn : Rd → R (for d = Õ(1 + √ µ/λn), hiding factors logarithmic in n, µ, λ, ), such that the number of calls T to a second-order oracle, so that\nE [ F (wT )− min\nw∈Rd F (w)\n] ≤ · ( F (0)− min\nw∈Rd F (w)\n) ,\nmust be at least\nΩ ( n+ √ nµ\nλ · log\n( (λ/µ)3/2 √ n )) .\nComparing this with the (tight) first-order oracle complexity bounds discussed in the introduction, we see that the\nlower bound is the same up to log-factors, despite the availability of second-order information. In particular, the lower bound exhibits none of the favorable properties associated with full second-order methods, which can compute and invert Hessians of F : Whereas the full Newton method can attain O(log log(1/ )) rates, and be independent of µ, λ if F satisfies a self-concordance property (Boyd and Vandenberghe, 2004), here we only get a linear O(log(1/ )) rate, and there is a strong dependence on µ, λ, even though the function is quadratic and hence self-concordant.\nThe proof of the theorem is based on a randomized construction, which can be sketched as follows: We choose indices j1, . . . , jd−1 ∈ {1, . . . , n} independently and uniformly at random, and define\nfi(w) = a · w21 + â · d−1∑ l=1 1jl=i(wl − wl+1)2\n+ ā · w2d − ã · w1 + λ\n2 ‖w‖2,\nwhere 1A is the indicator function of the event A, and a, â, ā, ã are parameters chosen based on λ, µ, n. The average function F (w) = 1n ∑n i=1 fi(w) equals\nF (w) = a·w21+ â n · d−1∑ l=1 (wl−wl+1)2+ā·w2d−ã·w1+ λ 2 ‖w‖2.\nBy setting the parameters appropriately, it can be shown that F is λ-strongly convex and each fi is µ-smooth. Moreover, the optimum of F has the form (q, q2, q3, . . . , qd) for\nq = √ κ− 1√ κ+ 1 ,\nwhere\nκ = µ λ − 1 n + 1 (4)\nis the so-called condition number of F . The proof is based on arguing that after T oracle calls, the points computable by any algorithm satisfying Assumptions 2 and 1 must have 0 values at all coordinates larger than some lT , hence the squared distance of wT from the optimum must be at least ∑d i=lT+1\nq2i, which leads to our lower bound. Thus, the proof revolves around upper bounding lT . We note that a similar construction of F was used in some previous first-order lower bounds under algorithmic assumptions (e.g. Nesterov (2013); Lan (2015), as well as Arjevani and Shamir (2015) in a somewhat different context). The main difference is in how we construct the individual functions fi, and in analyzing the effect of second-order rather than just first-order information.\nTo upper bound lT , we let lt (where t = 1, . . . , T ) be the largest non-zero coordinate in wt, and track how lt\nincreases with t. The key insight is that if w1, . . . ,wt−1 are zero beyond some coordinate l, then any linear combinations of them, as well as multiplying them by matrices based on second-order information, as specified in Assumption 1, will still result in vectors with zeros beyond coordinate l. The only way to “advance” and increase the set of non-zero coordinates is by happening to query the function fjl . However, since the indices of the queried functions are chosen obliviously, whereas each jl is chosen uniformly at random, the probability of this happening is quite small, of order 1/n. Moreover, we show that even if this event occurs, we are unlikely to “advance” by more than O(1) coordinates at a time. Thus, the algorithm essentially needs to make Ω(n) oracle calls in expectation, in order to increase the number of non-zero coordinates by O(1). It can be shown that the number of coordinates needed to get an -optimal solution is Ω̃( √ µ/nλ·log(1/ )) (hiding some log-factors). Therefore, the total number of oracle calls is about n times larger, namely Ω̃( √ nµ/λ · log(1/ )). To complete the theorem, we also provide a simple and separate Ω(n) lower bound, which holds since each oracle call gives us information on just one of the n individual functions f1, . . . , fn, and we need some information on most of them in order to get a close-to-optimal solution.\nWhen considering non-oblivious (i.e., adaptive) algorithms, the construction used in Thm. 2 fails as soon as the algorithm obtains the Hessians of all the individual functions (potentially, after n oracle queries). Indeed, knowing the Hessians of fi, one can devise an index-schedule which gains at least one coordinate at every iteration (by querying the function which holds the desired 2 × 2 block), as opposed to O(1/n) on average in the oblivious case. Nevertheless, as mentioned before, we can still provide a result similar to Thm. 2 even if the indices are chosen adaptively, at the cost of a much larger dimension: Theorem 3. Thm. 2 still holds if one omits Assumption 2, and with probability 1 rather than in expectation, at the cost of requiring an exponentially larger dimensionality of\nd = n Õ ( 1+ √ µ/λn ) .\nThe proof is rather straightforward: Making the dependence on the random indices j1, . . . , jd−1 explicit, the quadratic construction used in the previous theorem can be written as\nF j1,...,jd−1(w) = 1\nn n∑ i=1 f j1,...,jd−1 i (w)\n= 1\nn n∑ i=1 w>A j1,...,jd−1 i w − ã〈e1,w〉+ λ 2 ‖w‖2\nfor some d × d matrix Aj1,...,jd−1i dependent on j1, . . . , jd−1, and a fixed parameter ã. Now, we create n huge block-diagonal matrices A1, . . . , An, where each Ai\ncontains Aj1,...,jd−1i for each of the n d−1 possible choices of j1, . . . , jd−1 along its diagonal (in some canonical order), and one huge vector\ne = e1 + ed+1 + . . .+ e(nd−1−1)d+1.\nWe then let\nF (w) = 1\nn n∑ i=1 fi(w)\n= 1\nn n∑ i=1 w>Aiw − ã〈e,w〉+ λ 2 ‖w‖2.\nThis function essentially combines all nd−1 problems F j1,...,jd−1 simultaneously, where each F j1,...,jd−1 is embedded in a disjoint set of coordinates. Due to the blockdiagonal structure of each Ai, this function inherits the strong-convexity and smoothness properties of the original construction. Moreover, to optimize this function, the algorithm needs to “solve” all nd−1 problems simultaneously, using the same choice of indices i1, i2, . . .. Using a combinatorial argument which parallels the probabilistic argument in the proof of Thm. 2, we can show that no matter how these indices are chosen, the average number of nonzero coordinates of the iterates cannot grow too rapidly, and lead to the same bound as in Thm. 2. Since the construction is deterministic, and applies no matter how the indices are chosen, the lower bound holds deterministically, rather than in expectation as in Thm. 2.\nLastly, it is useful to consider how the bounds stated in Thm. 2 and Thm. 3 differ when the dimension d is fixed and finite. Inspecting the proofs of both theorems reveals that in both cases the suboptimality, as a function of the iteration number T , has a linear convegence rate bounded from below by\nE [ F (wT )− F (w?) F (0)− F (w?) ] ≥ Ω(1) (√ κ− 1√ κ+ 1 )O(Tn ) (5)\n(where κ is as defined in Eq. (4), and Ω(1) hides dependencies on the problem parameters, but is independent of T ). However, whereas the bound established in Thm. 2 is valid forO(d) number of iterations, Thm. 3 applies to a much restricted range of roughly log(d)/ log(n) iterations. This indicate that adaptive optimization algorithms might be able to gain a super-linear convergence rate after a significantly smaller number of iterations in comparison to oblivious algorithms (see (Arjevani and Shamir, 2016a) for a similar discussion regarding first-order methods). That being said, trading obliviousness for adaptivity may increase the periteration cost and reduce numerical stability."
  }, {
    "heading": "4. Comparison to Existing Approaches",
    "text": "As discussed in the introduction, there has been a recent burst of activity involving second-order methods for solving finite-sum problems, relying on Hessians of individual functions fi. In this section, we review the main algorithmic approaches and compare them to our results. The bottom line is that most existing approaches satisfy the assumptions stated in Sec. 3, and therefore our lower bounds will apply, at least in a worst-case sense. A possible exception to this is the Newton sketch algorithm (Pilanci and Wainwright, 2015), which relies on random projections, but on the flip side is computationally expensive.\nTurning to the details, existing approaches are based on taking the standard Newton iteration for such problems, wt+1 = wt − αt ( ∇2F (wt)\n)−1∇F (wt) = wt − αt ( 1\nn n∑ i=1 ∇2fi(wt)\n)−1( 1\nn n∑ i=1 ∇fi(wt)\n) ,\nand replacing the inverse Hessian term( 1 n ∑n i=1∇2fi(w) )−1 (and sometimes the vector term 1 n ∑n i=1∇fi(w) as well) by some approximation which is computationally cheaper to compute. One standard and well-known approach is to use only gradient information to construct such an approximation, leading to the family of quasi-Newton methods (Nocedal and Wright, 2006). However, as they rely on first-order rather than secondorder information, they are orthogonal to the topic of our work, and are already covered by existing complexity lower bounds for first-order oracles.\nTurning to consider Hessian approximation techniques using second-order information, perhaps the simplest and most intuitive approach is sampling: Since the Hessian equals the average of many individual Hessians,\n∇2F (w) = 1 n n∑ i=1 ∇2fi(w),\nwe can approximate it by taking a sample S of indices in {1, . . . , n} uniformly at random, compute the Hessians of the corresponding individual functions, and use the approximation\n∇2F (w) ≈ 1 |S| ∑ i∈S ∇2fi(w).\nIf |S| is large enough, then by concentration of measure arguments, this sample average should be close to the actual Hessian ∇2F (w). On the other hand, if |S| is not too large, then the resulting matrix is easier to invert (e.g. because it has a rank of only O(|S|), if each individual Hessian has rank O(1), as in the case of linear predictors). Thus, one can hope that the right sample size will lead\nto computational savings. There have been several rigorous studies of such “subsampled Newton” methods, such as Erdogdu and Montanari (2015); Roosta-Khorasani and Mahoney (2016a;b); Bollapragada et al. (2016) and references therein. However, our lower bound in Thm. 2 holds for such an approach, since it satisfies both Assumption 2 and 1. As expected, the existing worst-case complexity upper bounds are no better than our lower bound.\n(Xu et al., 2016) recently proposed a subsampled Newton method, together with non-uniform sampling, which assigns more weight to individual functions which are deemed more “important”. This is measured via properties of the Hessians of the functions, such as their norms or via leverage scores. This approach breaks Assumption 2, as the sampled indices are now chosen in a way dependent on the individual functions. However, our lower bound in Thm. 3, which does not require this assumption, still applies to such a method.\nA variant of the subsampled Newton approach, studied in Erdogdu and Montanari (2015), uses a low-rank approximation of the sample Hessian (attained by truncated SVD), in lieu of the sample Hessian itself. However, this still falls in the framework of Assumption 1, and our lower bound still applies.\nA different approach to approximate the full Hessian is via randomized sketching techniques, which replace the Hessian ∇2F (w) by a low-rank approximation of the form\n(∇2F (w))1/2SS>(∇2F (w))1/2,\nwhere S ∈ Rd×m,m d is a random sketching matrix, and (∇2F (w))1/2 is the matrix square root of ∇2F (w). This approach forms the basis of the Newton sketch algorithm proposed in Pilanci and Wainwright (2015). This approach currently escapes our lower bound, since it violates Assumption 1. That being said, this approach is inherently expensive in terms of computational resources, as it requires us to compute the square root of the full Hessian matrix. Even under favorable conditions, this requires us to perform a full pass over all functions f1, . . . , fn at every iteration. Moreover, existing iteration complexity upper bounds have a strong dependence on both µ/λ as well as the dimension d, and are considerably worse than the lower bound of Thm. 2. Therefore, we conjecture that this approach cannot lead to better worst-case results.\nAgarwal et al. (2016) develop another line of stochastic second-order methods, which are based on the observation that the Newton step (∇2F (w))−1∇F (w) is the solution of the system of linear equations\n∇2F (w)x = ∇F (w).\nThus, one can reduce the optimization problem to solving this system as efficiently as possible. The basic variant of\ntheir algorithm (denoted as LiSSA) relies on operations of the form\nw 7→ (I −∇2fi(w))w\n(for i sampled uniformly at random), as well as linear combinations of such vectors, which satisfy our assumptions. A second variant, LiSSA-Quad, re-phrases this linear system as the finite-sum optimization problem\nmin x\nx>∇2F (w)x +∇F (w)>x\n= 1\nn n∑ i=1 x>∇2fi(w)x +∇fi(w)>x,\nand uses some first-order method for finite-sum problems in order to solve it. Since individual gradients of this objective are of the form ∇2fi(w)x +∇fi(w), and most stateof-the-art first-order methods pick indices i obliviously, this approach also satisfies our assumptions, and our lower bounds apply. Yet another proposed algorithm, LiSSASample, is based on replacing the optimization problem above by\nmin x\nx>∇2F (w)B−1x +∇F (w)>x, (6)\nwhere B is some invertible matrix, solving it (with the optimum being equal to B(∇2F (w))−1∇F (w)), and multiplying the solution by B−1 to recover the solution (∇2F (w))−1∇F (w) to the original problem. In order to get computational savings, B is chosen to be a linear combination of O(d log(d)) sampled individual hessians ∇2fi(w), where it is assumed that d log(d) n, and the sampling and weighting is carefully chosen (based on the Hessians) so that Eq. (6) has strong convexity and smoothness parameters within a constant of each other. As a result, Eq. (6) can be solved fast using standard gradient descent, taking steps along the gradient, which equals ∇2F (w)B−1x +∇F (w) at any point x. This gradient is again computable under 1 (using O(n) oracle calls), since B is a linear combination of d log(d) n sampled individual Hessians. Thus, our lower bound (in the form of Thm. 3) still applies to such methods.\nThat being said, it is important to note that the complexity upper bound attained in Agarwal et al. (2016) for LiSSASample is on the order of\nÕ((n+ √ dµ/λ) · polylog(1/ ))\n(at least asymptotically as → 0), which can be better than our lower bound if d n. There is no contradiction, since the lower bound in Thm. 3 only applies for a dimension d much larger than n. Interestingly, our results also indicate that an adaptive index sampling scheme is necessary to get this kind of improved performance when d n: Otherwise, it could violate Thm. 2, which establishes a lower\nbound of Õ(n + √ nµ/λ) even if the dimension is quite\nmoderate (d = Õ(1 + √ µ/λn), which is n under the mild assumption that µ/λ n3).\nThe observation that an adaptive scheme (breaking assumption 2) can help performance when d n is also seen in the lower bound construction used to prove Thm. 2: If µ, λ, n are such that the required dimension d is n, then it means that only the functions fj1 , . . . , fjd−1 , which are a small fraction of all n individual functions, are informative and help us reduce the objective value. Thus, sampling these functions in an adaptive manner is imperative to get better complexity than the bound in Thm. 2. Based on the fact that only at most d−1 out of n functions are relevant in the construction, we conjecture that the possible improvement in the worst-case oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d, which is indeed the type of improvement attained (for small enough ) in Agarwal et al. (2016).\nFinally, we note that Agarwal et al. (2016) proposes another algorithm tailored to self-concordant functions, with runtime independent of the smoothness and strong convexity parameters of the problem. However, it requires performing ≥ 1 full Newton steps, so the runtime is prohibitive for large-scale problems (indeed, for quadratics as used in our lower bounds, even a single Newton step suffices to compute an exact solution)."
  }, {
    "heading": "5. Summary and Discussion",
    "text": "In this paper, we studied the oracle complexity for optimization problems, assuming availability of a second-order oracle. This is in contrast to most existing oracle complexity results, which focus on a first-order oracle. First, we formally proved that in the standard setting of stronglyconvex and smooth optimization problems, second-order information does not significantly improve the oracle complexity, and further assumptions (i.e. Lipschitzness of the Hessians) are in fact necessary. We then presented our main lower bounds, which show that for finite-sum problems with a second-order oracle, under some reasonable algorithmic assumptions, the resulting oracle complexity is – again – not significantly better than what can be obtained using a first-order oracle. Moreover, this is shown using quadratic functions, which have 0 derivatives of order larger than 2. Hence, our lower bounds apply even if we have access to an oracle returning derivatives of order p for all p ≥ 0, and the function is smooth to any order. In Sec. 4, we studied how our framework and lower bounds are applicable to most existing approaches.\nAlthough this conclusion may appear very pessimistic, they are actually useful in pinpointing potential assumptions and approaches which may circumvent these lower bounds. In\nparticular:\n• Our lower bound for algorithms employing adaptive index sampling schemes (Thm. 3) only hold when the dimension d is very large. This leaves open the possibility of better (non index-oblivious) algorithms when d is moderate, as was recently demonstrated in the context of the LiSSA-Sample algorithm of Agarwal et al. (2016) (at least for small enough ). As discussed in the previous section, we conjecture that the possible improvement in the worst-case oracle complexity of such schemes may amount to replacing dependencies on n with dependencies on d.\n• It might be possible to construct algorithms breaking Assumption 1, e.g. by using operations which are not linear-algebraic. That being said, we currently conjecture that this assumptions can be significantly relaxed, and similar results would hold for any algorithm which has “significantly” cheaper iterations (in terms of runtime) compared to the Newton method.\n• Our lower bounds are worst-case over smooth and strongly-convex individual functions fi. It could be that by assuming more structure, better bounds can be obtained. For example, as discussed in the introduction, an important special case is when fi(w) = `i(x > i w) for some scalar function `i and vector xi.\nOur construction in Thm. 2 does not quite fit this structure, although it is easy to show that we still get functions of the form fi(w) = `i(X>i w), where Xi has O(1 + d/n) = Õ(1 + √ µ/λn3) rows in expectation, which is Õ(1) under a broad parameter regime. We believe that the difference between Õ(1) rows and 1 row is not significant in terms of the attainable oracle complexity, but we may be wrong. Another possibility is to provide results depending on more delicate spectral properties of the function, beyond its strong convexity and smoothness, which may lead to better results and algorithms under favorable assumptions.\n• Our lower bounds in Sec. 3, which establish a linear convergence rate (logarithmic dependence on log(1/ )), are non-trivial only if the optimization error is sufficiently small. This does not preclude the possibility of attaining better initial performance when is relatively large.\nIn any case, we believe that our work lays the foundation for a more comprehensive study of the complexity of efficient second-order methods, for finite-sum and related optimization and learning problems."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "This research is supported in part by an FP7 Marie Curie CIG grant and an Israel Science Foundation grant 425/13."
  }],
  "year": 2017,
  "references": [{
    "title": "A lower bound for the optimization of finite sums",
    "authors": ["Alekh Agarwal", "Leon Bottou"],
    "venue": "arXiv preprint arXiv:1410.0723,",
    "year": 2014
  }, {
    "title": "Second order stochastic optimization in linear time",
    "authors": ["Naman Agarwal", "Brian Bullins", "Elad Hazan"],
    "venue": "arXiv preprint arXiv:1602.03943,",
    "year": 2016
  }, {
    "title": "Communication complexity of distributed convex learning and optimization",
    "authors": ["Yossi Arjevani", "Ohad Shamir"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Dimension-free iteration complexity of finite sum optimization problems",
    "authors": ["Yossi Arjevani", "Ohad Shamir"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "On the iteration complexity of oblivious first-order optimization algorithms",
    "authors": ["Yossi Arjevani", "Ohad Shamir"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Exact and inexact subsampled newton methods for optimization",
    "authors": ["Raghu Bollapragada", "Richard Byrd", "Jorge Nocedal"],
    "venue": "arXiv preprint arXiv:1609.08502,",
    "year": 2016
  }, {
    "title": "Convergence rates of sub-sampled newton methods",
    "authors": ["Murat A Erdogdu", "Andrea Montanari"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "An optimal randomized incremental gradient method",
    "authors": ["Guanghui Lan"],
    "venue": "arXiv preprint arXiv:1507.02000,",
    "year": 2015
  }, {
    "title": "Problem Complexity and Method",
    "authors": ["A. Nemirovsky", "D. Yudin"],
    "venue": "Efficiency in Optimization. Wiley-Interscience,",
    "year": 1983
  }, {
    "title": "Introductory lectures on convex optimization: A basic course, volume 87",
    "authors": ["Yurii Nesterov"],
    "venue": "Springer Science & Business Media,",
    "year": 2013
  }, {
    "title": "Newton sketch: A linear-time optimization algorithm with linear-quadratic convergence",
    "authors": ["Mert Pilanci", "Martin J Wainwright"],
    "venue": "arXiv preprint arXiv:1505.02250,",
    "year": 2015
  }, {
    "title": "Subsampled newton methods i: globally convergent algorithms",
    "authors": ["Farbod Roosta-Khorasani", "Michael W Mahoney"],
    "venue": "arXiv preprint arXiv:1601.04737,",
    "year": 2016
  }, {
    "title": "Subsampled newton methods ii: Local convergence rates",
    "authors": ["Farbod Roosta-Khorasani", "Michael W Mahoney"],
    "venue": "arXiv preprint arXiv:1601.04738,",
    "year": 2016
  }, {
    "title": "Tight complexity bounds for optimizing composite objectives",
    "authors": ["Blake Woodworth", "Nathan Srebro"],
    "venue": "arXiv preprint arXiv:1605.08003,",
    "year": 2016
  }, {
    "title": "Sub-sampled newton methods with non-uniform sampling",
    "authors": ["Peng Xu", "Jiyan Yang", "Farbod Roosta-Khorasani", "Christopher Ré", "Michael W Mahoney"],
    "venue": "arXiv preprint arXiv:1607.00559,",
    "year": 2016
  }],
  "id": "SP:d673568da56de9a4762becbf4cf3e3f01ea0d815",
  "authors": [{
    "name": "Yossi Arjevani",
    "affiliations": []
  }, {
    "name": "Ohad Shamir",
    "affiliations": []
  }],
  "abstractText": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer – perhaps surprisingly – is negative, at least in terms of worst-case guarantees. We also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.",
  "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems"
}