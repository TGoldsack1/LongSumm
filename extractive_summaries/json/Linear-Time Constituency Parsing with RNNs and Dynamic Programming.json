{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 477–483 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n477"
  }, {
    "heading": "1 Introduction",
    "text": "Span-based neural constituency parsing (Cross and Huang, 2016; Stern et al., 2017a) has attracted attention due to its high accuracy and extreme simplicity. Compared with other recent neural constituency parsers (Dyer et al., 2016; Liu and Zhang, 2016; Durrett and Klein, 2015) which use neural networks to model tree structures, the spanbased framework is considerably simpler, only using bidirectional RNNs to model the input sequence and not the output tree. Because of this factorization, the output space is decomposable\nwhich enables efficient dynamic programming algorithm such as CKY. But existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser (Cross and Huang, 2016) is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Björkelund et al., 2016).\nWe propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n − 1) actions (i.e., steps) for a sentence of nwords. While previous non-neural work in this area requires sophisticated features (Huang and Sagae, 2010; Mi and Huang, 2015) and thus high time complexity such as O(n11), our states are as simple as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features. This gives a running time ofO(n4), with the extraO(n) for step index. We further employ beam search to have a practical runtime of O(nb2) at the cost of exact search where b is the beam size. However, on the Penn Treebank, most sentences are less than 40 words (n < 40), and even with a small beam size of b = 10, the observed complexity of an O(nb2) parser is not exactly linear in n (see Experiments). To solve this problem, we apply cube pruning (Chiang, 2007; Huang and Chiang, 2007) to improve the runtime toO(nb log b) which\nrenders an observed complexity that is linear in n (with minor extra inexactness).\nWe make the following contributions:\n• We design the first neural parser that is both linear time and capable of searching over exponentially large space.1\n• We are the first to apply cube pruning to incremental parsing, and achieves, for the first time, the complexity of O(nb log b), i.e., linear in sentence length and (almost) linear in beam size. This leads to an observed complexity strictly linear in sentence length n.\n• We devise a novel loss function which penalizes wrong spans that cross gold-tree spans, and employ max-violation update (Huang et al., 2012) to train this parser with structured SVM and beam search.\n• Compared with chart parsing baselines, our parser is substantially faster for long sentences on the Penn Treebank, and orders of magnitude faster for end-to-end discourse parsing. It also achieves the highest F1 score on the Penn Treebank among single model end-to-end systems.\n• We devise a new formulation of graphstructured stack (Tomita, 1991) which requires no extra bookkeeping, proving a new theorem that gives deep insight into GSS."
  }, {
    "heading": "2 Preliminaries",
    "text": ""
  }, {
    "heading": "2.1 Span-Based Shift-Reduce Parsing",
    "text": "A span-based shift-reduce constituency parser (Cross and Huang, 2016) maintains a stack of spans (i, j), and progressively adds a new span each time it takes a shift or reduce action. With (i, j) on top of the stack, the parser can either shift to push the next singleton span (j, j + 1) on the stack, or it can reduce to combine the top two spans, (k, i) and (i, j), forming the larger span (k, j). After each shift/reduce action, the top-most span is labeled as either a constituent or with a null label ∅, which means that the subsequence is not a subtree in the final decoded parse. Parsing initializes with an empty stack and continues until (0, n) is formed, representing the entire sentence.\n1 https://github.com/junekihong/beam-span-parser"
  }, {
    "heading": "2.2 Bi-LSTM features",
    "text": "To get the feature representation of a span (i, j), we use the output sequence of a bi-directional LSTM (Cross and Huang, 2016; Stern et al., 2017a). The LSTM produces f0, ..., fn forwards and bn, ...,b0 backwards outputs, which we concatenate the differences of (fj−fi) and (bi−bj) as the representation for span (i, j). This eliminates the need for complex feature engineering, and can be stored for efficient querying during decoding."
  }, {
    "heading": "3 Dynamic Programming",
    "text": ""
  }, {
    "heading": "3.1 Score Decomposition",
    "text": "Like Stern et al. (2017a), we also decompose the score of a tree t to be the sum of the span scores:\ns(t) = ∑\n(i,j,X)∈t\ns(i, j,X) (1)\n= ∑\n(i,j)∈t\nmax X\ns((fj − fi;bi − bj), X) (2)\nNote that X is a nonterminal label, a unary chain (e.g., S-VP), or null label ∅.2 In a shift-reduce setting, there are 2n − 1 steps (n shifts and n − 1 reduces) and after each step we take the best label for the resulting span; therefore there are exactly\n2The actual code base of Stern et al. (2017b) forces s(i, j,∅) to be 0, which simplifies their CKY parser and slightly improves their parsing accuracy. However, in our incremental parser, this change favors shift over reduce and degrades accuracy, so our parser keeps a learned score for ∅.\n2n−1 such (labeled) spans (i, j,X) in tree t. Also note that the choice of the label for any span (i, j) is only dependent on (i, j) itself (and not depending on any subtree information), thus the max over label X is independent of other spans, which is a nice property of span-based parsing (Cross and Huang, 2016; Stern et al., 2017a)."
  }, {
    "heading": "3.2 Graph-Struct. Stack w/o Bookkeeping",
    "text": "We now reformulate this DP parser in the above section as a shift-reduce parser. We maintain a step index ` in order to perform action-synchronous beam search (see below). Figure 1 shows how to represent a parsing stack using only the top span (i, j). If the top span (i, j) shifts, it produces (j, j + 1), but if it reduces, it needs to know the second last span on the stack, (k, i), which is not represented in the current state. This problem can be solved by graph-structure stack (Tomita, 1991; Huang and Sagae, 2010), which maintains, for each state p, a set of predecessor states π(p) that p can combine with on the left.\nThis is the way our actual code works (π(p) is implemented as a list of pointers, or “left pointers”), but here for simplicity of presentation we devise a novel but easier-to-understand formulation in Fig. 1, where we explicitly represent the set of predecessor states that state ` : (i, j) can combine with as `′ : (k, i) where `′ = `−2(j− i) + 1, i.e., (i, j) at step ` can combine with any (k, i) for any k at step `′. The rationale behind this new formulation is the following theorem:\nTheorem 1 The predecessor states π(` : (i, j)) are all in the same step `′ = `− 2(j − i) + 1. Proof. By induction.\nThis Theorem bring new and deep insights and suggests an alternative implementation that does not require any extra bookkeeping. The time complexity of this algorithm is O(n4) with the extra O(n) due to step index.3"
  }, {
    "heading": "3.3 Action-Synchronous Beam Search",
    "text": "The incremental nature of our parser allows us to further lower the runtime complexity at the cost of inexact search. At each time step, we maintain the top b parsing states, pruning off the rest. Thus, a candidate parse that made it to the end of decoding had to survive within the top b at every step.\n3The word-synchronous alternative does not need the step index ` and enjoys a cubic time complexity, being almost identical to CKY. However, beam search becomes very tricky.\nWith O(n) parsing actions our time complexity becomes linear in the length of the sentence."
  }, {
    "heading": "3.4 Cube Pruning",
    "text": "However, Theorem 1 suggests that a parsing state p can have up to b predecessor states (“left pointers”), i.e., |π(p)| ≤ b because π(p) are all in the same step, a reduce action can produce up to b subsequent new reduced states. With b items on a beam and O(n) actions to take, this gives us an overall complexity of O(nb2). Even though b2 is a constant, even modest values of b can make b2 dominate the length of the sentence. 4\nTo improve this at the cost of additional inexactness, we introduce cube pruning to our beam search, where we put candidate actions into a heap and retrieve the top b states to be considered in the next time-step. We heapify the top b shiftmerged states and the top b reduced states. To avoid inserting all b2 reduced states from the previous beam, we only consider each state’s highest scoring left pointer,5 and whenever we pop a reduced state from the heap, we iterate down its left pointers to insert the next non-duplicate reduced state back into the heap. This process finishes when we pop b items from the heap. The initialization of the heap takes O(b) and popping b items takes O(b log b), giving us an overall improved runtime of O(nb log b)."
  }, {
    "heading": "4 Training",
    "text": "We use a Structured SVM approach for training (Stern et al., 2017a; Shi et al., 2017). We want the model to score the gold tree t∗ higher than any other tree t by at least a margin ∆(t, t∗):\n∀t, s(t∗)− s(t) ≥ ∆(t, t∗).\nNote that ∆(t, t) = 0 for any t and ∆(t, t∗) > 0 for any t 6= t∗. At training time we perform lossaugmented decoding:\nt̂ = arg max t s∆(t) = arg max t s(t) + ∆(t, t∗).\n4The average length of a sentence in the Penn Treebank training set is about 24. Even with a beam size of 10, we already have b2 = 100, which would be a significant factor in our runtime. In practice, each parsing state will rarely have the maximum b left pointers so this ends up being a loose upper-bound. Nevertheless, the beam search should be performed with the input length in mind, or else as b increases we risk losing a linear runtime.\n5If each previous beam is sorted, and if the beam search is conducted by going top-to-bottom, then each state’s left pointers will implicitly be kept in sorted order.\nwhere s∆(·) is the loss-augmented score. If t̂ = t∗, then all constraints are satisfied (which implies arg maxt s(t) = t\n∗), otherwise we perform an update by backpropagating from s∆(t̂)− s(t∗)."
  }, {
    "heading": "4.1 Cross-Span Loss",
    "text": "The baseline loss function from Stern et al. (2017a) counts the incorrect labels (i, j,X) in the predicted tree:\n∆base(t, t ∗) = ∑ (i,j,X)∈t 1 ( X 6= t∗(i,j) ) .\nNote that X can be null ∅, and t∗(i,j) denotes the gold label for span (i, j), which could also be ∅.6 However, there are two cases where t∗(i,j) = ∅: a subspan (i, j) due to binarization (e.g., a span combining the first two subtrees in a ternary branching node), or an invalid span in t that crosses a gold span in t∗. In the baseline function above, these two cases are treated equivalently; for example, a span (3, 5,∅) ∈ t is not penalized even if there is a gold span (4, 6,VP) ∈ t∗. So we revise our loss function as:\n∆new(t, t ∗) = ∑ (i,j,X)∈t 1 ( X 6= t∗(i,j)\n∨ cross(i, j, t∗) )\n6Note that the predicted tree t has exactly 2n − 1 spans but t∗ has much fewer spans (only labeled spans without ∅).\nwhere cross(i, j, t∗) = ∃ (k, l) ∈ t∗, and i < k < j < l or k < i < l < j."
  }, {
    "heading": "4.2 Max Violation Updates",
    "text": "Given that we maintain loss-augmented scores even for partial trees, we can perform a training update on a given example sentence by choosing to take the loss where it is the greatest along the parse trajectory. At each parsing time-step `, the violation is the difference between the highest augmented-scoring parse trajectory up to that point and the gold trajectory (Huang et al., 2012; Yu et al., 2013). Note that computing the violation gives us the max-margin loss described above. Taking the largest violation from all time-steps gives us the max-violation loss."
  }, {
    "heading": "5 Experiments",
    "text": "We present experiments on the Penn Treebank (Marcus et al., 1993) and the PTB-RST discourse treebank (Zhao and Huang, 2017). In both cases, the training set is shuffled before each epoch, and dropout (Hinton et al., 2012) is employed with probability 0.4 to the recurrent outputs for regularization. Updates with minibatches of size 10 and 1 are used for PTB and the PTB-RST respectively. We use Adam (Kingma and Ba, 2014) with default settings to schedule learning rates for all the weights. To address unknown words during training, we adopt the strategy described by Kiperwasser and Goldberg (Kiperwasser and Goldberg, 2016); words in the training set are replaced with the unknown word symbol UNK with probability punk = 1 1+f(w) , with f(w) being the number of\noccurrences of word w in the training corpus. Our system is implemented in Python using the DyNet neural network library (Neubig et al., 2017)."
  }, {
    "heading": "5.1 Penn Treebank",
    "text": "We use the Wall Street Journal portion of the Penn Treebank, with the standard split of sections 2-21 for training, 22 for development, and 23 for testing. Tags are provided using the Stanford tagger with 10-way jackknifing.\nTable 1 shows our development results and overall speeds, while Table 2 compares our test results. We show that a beam size of 20 can be fast while still achieving state-of-the-art performances."
  }, {
    "heading": "5.2 Discourse Parsing",
    "text": "To measure the tractability of parsing on longer sequences, we also consider experiments on the\nPTB-RST discourse Treebank, a joint discourse and constituency dataset with a combined representation, allowing for parsing at either level (Zhao and Huang, 2017). We compare our runtimes out-of-the-box in Figure 3. Without any pre-processing, and by treating discourse examples as constituency trees with thousands of words, our trained models represent end-to-end discourse parsing systems.\nFor our overall constituency results in Table 3, and for discourse results in Table 4, we adapt the split-point feature described in (Zhao and Huang, 2017) in addition to the base parser. We find that larger beamsizes are required to achieve good discourse scores."
  }, {
    "heading": "6 Conclusions",
    "text": "We have developed a new neural parser that maintains linear time, while still searching over an exponentially large space. We also use cube pruning to further improve the runtime to O(nb log b). For training, we introduce a new loss function, and achieve state-of-the-art results among singlemodel end-to-end systems."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Dezhong Deng who contributed greatly to Secs. 3.2 and 4 (he deserves co-authorship), and Mitchell Stern for releasing his code and and suggestions. This work was supported in part by NSF IIS-1656051 and DARPA N66001-17-2-4030."
  }],
  "year": 2018,
  "references": [{
    "title": "A reranking model for discourse segmentation using subtree features",
    "authors": ["Ngo Xuan Bach", "Nguyen Le Minh", "Akira Shimazu."],
    "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Com-",
    "year": 2012
  }, {
    "title": "How to train dependency parsers with inexact search for joint sentence boundary detection and parsing of entire documents",
    "authors": ["Anders Björkelund", "Agnieszka Faleńska", "Wolfgang Seeker", "Jonas Kuhn."],
    "venue": "Proceedings of the 54th Annual Meeting of the As-",
    "year": 2016
  }, {
    "title": "Hierarchical phrase-based translation",
    "authors": ["David Chiang."],
    "venue": "Computational Linguistics 33(2):201–208.",
    "year": 2007
  }, {
    "title": "Parsing as language modeling",
    "authors": ["Do Kook Choe", "Eugene Charniak."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 2331–2336.",
    "year": 2016
  }, {
    "title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles",
    "authors": ["James Cross", "Liang Huang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
    "year": 2016
  }, {
    "title": "Neural CRF parsing",
    "authors": ["Greg Durrett", "Dan Klein."],
    "venue": "arXiv preprint arXiv:1507.03641 .",
    "year": 2015
  }, {
    "title": "Recurrent neural network grammars",
    "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1602.07776 .",
    "year": 2016
  }, {
    "title": "Improving neural parsing by disentangling model combination and reranking effects",
    "authors": ["Daniel Fried", "Mitchell Stern", "Dan Klein."],
    "venue": "Proceedings of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Hilda: a discourse parser using support vector machine classification",
    "authors": ["Hugo Hernault", "Helmut Prendinger", "David A DuVerle", "Mitsuru Ishizuka", "Tim Paek."],
    "venue": "Dialogue and Discourse 1(3):1–33.",
    "year": 2010
  }, {
    "title": "Improving neural networks by preventing coadaptation of feature detectors",
    "authors": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."],
    "venue": "arXiv preprint arXiv:1207.0580 .",
    "year": 2012
  }, {
    "title": "Forest rescoring: Fast decoding with integrated language models",
    "authors": ["Liang Huang", "David Chiang."],
    "venue": "Proceedings of ACL 2007. Prague, Czech Rep.",
    "year": 2007
  }, {
    "title": "Structured perceptron with inexact search",
    "authors": ["Liang Huang", "Suphan Fayong", "Yang Guo."],
    "venue": "Proceedings of NAACL. http://www.isi.edu/ lhuang/perc-inexact.pdf.",
    "year": 2012
  }, {
    "title": "Dynamic programming for linear-time incremental parsing",
    "authors": ["Liang Huang", "Kenji Sagae."],
    "venue": "Proceedings of ACL 2010. Uppsala, Sweden.",
    "year": 2010
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
    "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."],
    "venue": "CoRR abs/1603.04351. http://arxiv.org/abs/1603.04351.",
    "year": 2016
  }, {
    "title": "Shift-reduce constituent parsing with neural lookahead features",
    "authors": ["Jiangming Liu", "Yue Zhang."],
    "venue": "arXiv preprint arXiv:1612.00567 .",
    "year": 2016
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Computational linguistics 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Shift-reduce constituency parsing with dynamic programming and pos tag lattice",
    "authors": ["Haitao Mi", "Liang Huang."],
    "venue": "Proceedings of NAACL 2015.",
    "year": 2015
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."],
    "venue": "arXiv preprint",
    "year": 2017
  }, {
    "title": "Fast(er) exact decoding and global training for transition-based dependency parsing via a minimal feature set",
    "authors": ["Tianze Shi", "Liang Huang", "Lillian Lee."],
    "venue": "Proceedings of EMNLP 2017 (to appear).",
    "year": 2017
  }, {
    "title": "Parsing with compositional vector grammars",
    "authors": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics, volume 1, pages",
    "year": 2013
  }, {
    "title": "A minimal span-based neural constituency parser",
    "authors": ["Mitchell Stern", "Jacob Andreas", "Dan Klein."],
    "venue": "Proceedings of the Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "A minimal span-based neural constituency parser (code base)",
    "authors": ["Mitchell Stern", "Jacob Andreas", "Dan Klein."],
    "venue": "https://github.com/ mitchellstern/minimal-span-parser.",
    "year": 2017
  }, {
    "title": "Effective inference for generative neural parsing",
    "authors": ["Mitchell Stern", "Daniel Fried", "Dan Klein."],
    "venue": "Proceedings of Empirical Methods in Natural Language Processing. pages 1695–1700.",
    "year": 2017
  }, {
    "title": "Generalized LR Parsing",
    "authors": ["Masaru Tomita", "editor"],
    "year": 1991
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "Advances in Neural Information Processing Systems. pages 2773–2781.",
    "year": 2015
  }, {
    "title": "Max-violation perceptron and forced decoding for scalable mt training",
    "authors": ["Heng Yu", "Liang Huang", "Haitao Mi", "Kai Zhao."],
    "venue": "Proceedings of EMNLP 2013.",
    "year": 2013
  }, {
    "title": "Joint syntactodiscourse parsing and the syntacto-discourse treebank",
    "authors": ["Kai Zhao", "Liang Huang."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pages 2117–2123.",
    "year": 2017
  }],
  "id": "SP:4a8446fe5008df3a2f64bb6ca7998349b660965d",
  "authors": [{
    "name": "Juneki Hong",
    "affiliations": []
  }, {
    "name": "Liang Huang",
    "affiliations": []
  }],
  "abstractText": "Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model “spans”. However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, O(n3), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-toend discourse parsing and joint sentence boundary detection and parsing. We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time O(nb2) where b is the beam size. We further speed this up to O(nb log b) by integrating cube pruning. Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.",
  "title": "Linear-Time Constituency Parsing with RNNs and Dynamic Programming"
}