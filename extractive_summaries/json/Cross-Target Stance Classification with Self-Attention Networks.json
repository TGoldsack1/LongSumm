{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 778–783 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n778"
  }, {
    "heading": "1 Introduction",
    "text": "Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017). Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern. Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016). This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target.\nAn alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets. For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country. It might be useful to start with a classifier that is adapted from a related target such as climate change is concern (presumably available and annotated), as in both cases\nusers could discuss the impacts from the targets to some common issues, such as the environment or communities.\nCross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets. However, for some targets that can be recognized as being related to the same and more general domains, it could be possible to generalize through certain aspects of the domains that reflect users’ major concerns. For example, from the following sentence, whose stance is against the approval of a mining project, “Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef”, it can be seen that both this sentence and the one in Table 1 mention the same aspect “reef destruction/damage”, which is closely related to the “environment” domain.\nIn this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets1. The basic idea is to learn a set of domainspecific aspects from a source target, and then apply them to prediction on a destination target. To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism. Our preliminary analysis shows that the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains.\n1In this work, the source target is chosen based on common sense. Exploring more sophisticated source target selection methods will be our future work."
  }, {
    "heading": "2 Model",
    "text": "In this section, we introduce the proposed model, CrossNet, for cross-target stance classification. Figure 1 shows the architecture of CrossNet. It consists of four layers from the Embedding Layer (bottom) to the Prediction Layer (top). It works by taking a stance-bearing sentence and a target as input and yielding the predicted stance label as output. In the following, we present the implementation of each layer in CrossNet."
  }, {
    "heading": "2.1 Embedding Layer",
    "text": "There are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1). We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector. The output of this layer are two sequences of vectors P = {p1, ...,p|P |} and T = {t1, ..., t|T |}, where p, t are word vectors."
  }, {
    "heading": "2.2 Context Encoding Layer",
    "text": "In this layer, we encode the contextual information in the input sentence and target. We use a bi-directional Long Short-Term Memory Network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to capture the left and right contexts of each word in the input. Moreover, to account for the impact of the target on stance inference, we borrow the idea of conditional encoding (Augenstein et al., 2016) to model the dependency of the sentence on the target. Formally, we first use a BiLSTMT to encode the target:\n[ −→ h Ti −→c Ti ] = −−−−→ LSTMT (ti, −→ h Ti−1, −→c Ti−1) [ ←− h Ti ←−c Ti ] = ←−−−− LSTMT (ti, ←− h Ti+1, ←−c Ti+1) (1)\nwhere h ∈ Rh and c ∈ Rh are the hidden state and cell state of LSTM. The symbol −→(←−) indicates the forward (backward) pass. ti is the input word vector at time step i.\nThen, we learn a conditional encoding of the sentence P , by initializing BiLSTMP (a different BiLSTM) with the final states of BiLSTMT :\n[ −→ h P1 −→c P1 ] = −−−−→ LSTMP (p1, −→ h T|T |, −→c T|T |)\n[ ←− h P|P | ←−c P|P |] = ←−−−− LSTMP (p|P |, ←− h T1 , ←−c T1 )\n(2)\nIt can be seen that the initialization is done by aligning the forward (backward) pass of the two BiLSTMs. The output is a contextually-encoded sequence, HP = {hP1 , ...,hP|P |}, where h = [ −→ h ; ←− h ] ∈ R2h with [; ] as the vector concatenation operation."
  }, {
    "heading": "2.3 Aspect Attention Layer",
    "text": "In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference. In particular, the key observation we make is that the domain aspects that reflect users’ major concerns are usually the core of understanding their stances, and could be mentioned by multiple users in a discussion. For example, we find that many users in our corpus mention the aspect “reef” to express their concerns about the impact of a mining project on the Great Barrier Reef. Based on this observation, the perception of the domain aspects can be boiled down to finding the sentence parts that not only carry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus.\nFirst, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects.\nThen, we utilize self-attention to signal the core parts of a stance-bearing sentence. Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016). In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence. To this end, we introduce a compatibility function to score the semantic compatibility between the encoded se-\nquence HP and each of its hidden states hP :\nci = w > 2 σ(W1h P i + b1) + b2 (3)\nwhere W1 ∈ Rd×2h, w2 ∈ Rd, b1 ∈ Rd, and b2 ∈ R are trainable parameters, and σ is the activation function. Note that all the above parameters are shared by every hidden state in HP . Next, we compute the attention weight ai for each hPi based on its compatibility score via softmax operation:\nai = exp(ci)∑|P | j=1 exp(cj)\n(4)\nFinally, we can obtain the domain aspect encoded representation based on the attention weights:\nAP = |P |∑ i=1 aih P i (5)\nwhere AP ∈ R2h is the domain aspect encoding for sentence P and also the output of this layer."
  }, {
    "heading": "2.4 Prediction Layer",
    "text": "We predict the stance label of the sentence based on its domain aspect encoding:\nŷ = softmax(MLP(AP )) (6)\nwhere we use a multilayer perceptron (MLP) to consume the domain aspect encoding AP and apply the softmax to get the predicted probability for each of the C classes, ŷ = {y1, ..., yC}."
  }, {
    "heading": "2.5 Model Training",
    "text": "For model training, we use multi-class crossentropy loss,\nJ (θ) = − N∑ i C∑ j y (i) j log ŷ (i) j + λ‖Θ‖ (7)\nwhereN is the size of training set. y is the groundtruth label indicator for each class, and ŷ is the predicted probability. λ is the coefficient for L2regularization. Θ denotes the set of all trainable parameters in our model."
  }, {
    "heading": "3 Experiments",
    "text": "This section reports the results of quantitative and qualitative evaluations of the proposed model."
  }, {
    "heading": "3.1 Datasets",
    "text": "SemEval-2016: the first dataset is from SemEval2016 Task 6 on Twitter stance detection, which contains stance-bearing tweets on different targets. We use the following five targets for our experiments: Climate Change is Concern (CC), Feminist Movement (FM), Hillary Clinton (HC), Legalization of Abortion (LA), and Donald Trump (DT). The class labels are favor, against, and neither, and their distributions are shown in Table 2. Tweets on an Australian mining project (AM): the second is our collection of tweets on a mining project in Australia obtained using Twitter API. It includes 220,067 tweets posted from January 2016 to June 2017 that contain the project name in the text. We remove all URL-only tweets and duplicate tweets, and obtain a set of 40,852 (unlabeled) tweets. Due to the lack of annotation, this dataset is only used for our qualitative evaluation.\nTo align with our scenario, the above targets can be categorized into three different domains: Women’s Rights (FM, LA), American Politics (HC, DT), and Environments (CC, AM)."
  }, {
    "heading": "3.2 Metric",
    "text": "We use F1-score to measure the classification performance. Due to the imbalanced class distributions of the SemEval dataset, we compute both micro-averaged (large classes dominate) and macro-averaged (small classes dominate) F1scores (Manning et al., 2008), and use their average as the metric, i.e., F = 12(Fmicro + Fmacro).\nTo evaluate the effectiveness of target adaptation, we use the metric transfer ratio (Glorot et al., 2011) to compare the cross-target and in-target performance of a model: Q = F (S,D)Fb(D,D) , where F (S,D) is the cross-target F1-score of a model trained on the source target S and tested on the destination target D, and Fb(D,D) is the in-target F1-score of a baseline model trained and tested on the same target D, which serves as the performance calibration for target adaptation."
  }, {
    "heading": "3.3 Training setup",
    "text": "The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus (Pennington et al., 2014), and fixed during training. The model is trained (90%) and validated (10%) on a source target, and tested on a destination target. The following model settings are selected based on a small grid search on the validation set: the LSTM hidden size of 60, the MLP layer size of 60, and dropout 0.1. The L2-regularization coefficient λ in the loss is 0.01. ADAM (Kingma and Ba, 2014) is used as the optimizer, with a learning rate of 10−3. Stratified 10-fold cross-validation is conducted to produce averaged results."
  }, {
    "heading": "3.4 Classification Performance",
    "text": "This section reports the results of our model and two baseline approaches on cross-target stance classification. BiLSTM: this is a base model for our task. It has two BiLSTMs for encoding the sentence and target separately. Then, the concatenation of the resulting encodings is fed into the final Prediction Layer to generate predicted stance labels. In our evaluation, this model is treated as the baseline model for deriving the in-target performance calibration Fb(D,D). MITRE (Augenstein et al., 2016): this is the\nbest system in SemEval-2016 Task 6. It utilizes the conditional encoding to learn a targetdependent representation for the input sentence. The conditional encoding is realized in the same way as the Context Encoding Layer does in our model, namely by using the hidden states of the target-encoding BiLSTM to initialize the sentence-encoding BiLSTM.\nTable 3 shows the results (in-target and crosstarget) on the two domains: Women’s Rights and American Politics. First, it is observed that MITRE outperforms BiLSTM over all target configurations, suggesting that, compared to simple concatenation, the conditional encoding of the target information could be more helpful to capture the dependency of the sentence on the target.\nSecond, our model is shown to achieve better results than the two baselines in almost all cases (only slightly worse than MITRE on LA under the in-target setting, and the difference is not statistically significant), which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance. Moreover, by comparing the performance of our model under different target configurations, we see that the improvements brought by our model are more significant on the cross-target task than they are on the intarget task, with an average improvement of 6.6% (cross-target) vs. 3.0% (in-target) over MITRE in F1-score, which demonstrates a greater advantage of our model in the cross-target task.\nFinally, according to the transfer ratio results, the general drop from the in-target to cross-target performance (26% averaged over all cases) could imply that while the target-independent information (i.e., the domain-specific aspects) is shown to benefit generalization, it could be important to also consider the information that is specific to the destination target for model building (which has not yet been explored in this work)."
  }, {
    "heading": "3.5 Visualization of Attention",
    "text": "To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model in Table 4.\nWe can see that the most highlighted parts in each example are relevant to the respective domain. For example, “feminist”, “rights”, and “equality” are commonly used when talking about women’s rights, and “president” and “dreams” of-\nten appear in text about politics. It is also interesting to note that words that are specific to the destination target may not be captured by the model learned from the source target, such as “abortion” in sentence 1 and “trumps” in sentence 3. This makes sense because those words are rare in the source target corpus and thus not well noticed by the model.\nFinally, for our project, we can see from the last two sentences that the model learned from climate change is concern is able to concentrate on words that are central to understanding the authors’ stances on the approval of the mining project, such as “reef”, “destroy”, “environmental”, and “disaster”. Overall, the above visualization demonstrates that our model could benefit stance inference across related targets through capturing domain-specific information."
  }, {
    "heading": "3.6 Learned Domain-Specific Aspects",
    "text": "Finally, it is also possible to show the learned domain aspects by extracting all sentence parts in a corpus that are highly attended by our model. Table 5 presents a number of samples from the intersections between the sets of highly-attended words on the respective targets in the three domains. Again, we see that these highly-attended words are specific to the respective domains. We\nalso notice that besides the domain-aspect words, our model can find words that carry sentiments as well, such as “great”, “crazy”, and “beautiful”, which contribute to stance prediction."
  }, {
    "heading": "4 Conclusion and Future Work",
    "text": "In this work, we study cross-target stance classification and propose a novel self-attention neural model that can extract target-independent information for model generalization. Experimental results show that the proposed model can perceive high-level domain-specific information in a sentence and achieves superior results over a number of baselines in certain domains. In the future, there are several ways of extending our model.\nFirst, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets. One possibility could be to learn certain correlations between target closeness and generalization performance, which could further be used for guiding the target selection process. Second, our current model for identifying users’ stances on mining projects only generalizes from one source target (i.e., Climate Change is Concern). However, a mining project in general could affect other aspects of our society such as community and economics. It could be useful to also consider other related sources for knowledge transfer. Finally, it would be interesting to evaluate our model in a multilingual scenario (Taulé et al., 2017), in order to examine its generalization ability (whether it can attend to useful domain-specific information in a new language) and multilingual scope."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank all anonymous reviewers for their valuable comments. We would also like to thank Keith Vander Linden for his helpful comments on drafts of this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Stance detection with bidirectional conditional encoding",
    "authors": ["Isabelle Augenstein", "Tim Rocktäschel", "Andreas Vlachos", "Kalina Bontcheva."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages",
    "year": 2016
  }, {
    "title": "Long short-term memory-networks for machine reading",
    "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 551–561.",
    "year": 2016
  }, {
    "title": "Weakly supervised tweet stance classification by relational bootstrapping",
    "authors": ["Javid Ebrahimi", "Dejing Dou", "Daniel Lowd."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 1012–1017.",
    "year": 2016
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th international conference on machine learning (ICML). pages 513–520.",
    "year": 2011
  }, {
    "title": "Stance classification of ideological debates: Data, models, features, and constraints",
    "authors": ["Kazi Saidul Hasan", "Vincent Ng."],
    "venue": "Proceedings of the 6th International Joint Conference on Natural Language Processing. pages 1348–1356.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980 .",
    "year": 2014
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in neural information processing systems (NIPS). pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Semeval-2016 task 6: Detecting stance in tweets",
    "authors": ["Saif Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiao-Dan Zhu", "Colin Cherry."],
    "venue": "SemEval@ NAACL-HLT . pages 31–41.",
    "year": 2016
  }, {
    "title": "Stance and sentiment in tweets",
    "authors": ["Saif M Mohammad", "Parinaz Sobhani", "Svetlana Kiritchenko."],
    "venue": "ACM Transactions on Internet Technology (TOIT) 17(3):26.",
    "year": 2017
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Overview of the task on stance and gender detection in tweets on catalan independence at ibereval 2017",
    "authors": ["Mariona Taulé", "M Antonia Martı", "Francisco M Rangel", "Paolo Rosso", "Cristina Bosco", "Viviana Patti"],
    "year": 2017
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin."],
    "venue": "Advances in neural information processing systems (NIPS). pages 6000–6010.",
    "year": 2017
  }],
  "id": "SP:f187834a4fcd6dc1ad022814626ed8020cb36613",
  "authors": [{
    "name": "Chang Xu",
    "affiliations": []
  }, {
    "name": "Cécile Paris",
    "affiliations": []
  }, {
    "name": "Surya Nepal",
    "affiliations": []
  }, {
    "name": "Ross Sparks",
    "affiliations": []
  }],
  "abstractText": "In stance classification, the target on which the stance is made defines the boundary of the task, and a classifier is usually trained for prediction on the same target. In this work, we explore the potential for generalizing classifiers between different targets, and propose a neural model that can apply what has been learned from a source target to a destination target. We show that our model can find useful information shared between relevant targets which improves generalization in certain scenarios.",
  "title": "Cross-Target Stance Classification with Self-Attention Networks"
}