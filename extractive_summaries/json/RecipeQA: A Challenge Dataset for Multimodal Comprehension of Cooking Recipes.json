{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1358–1368 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1358"
  }, {
    "heading": "1 Introduction",
    "text": "There is a rich literature in natural language processing (NLP) and information retrieval on question answering (QA) (Hirschman and Gaizauskas, 2001), but recently deep learning has sparked interest in a special kind of QA, commonly referred to as reading comprehension (RC) (Vanderwende, 2007). The aim in RC research is to build intelligent systems with the abilities to read and understand natural language text and answer questions related to it (Burges, 2013). Such tests are appealing as they require joint understanding of the question and the related passage (i.e. context), and moreover, they can analyze many different types of skills in a rather objective way (Sugawara et al., 2017).\nDespite the progress made in recent years, there is still a significant performance gap between humans and deep neural models in RC, and researchers are pushing forward our understanding of\nthe limitations and capabilities of these approaches by introducing new datasets. Existing tasks for RC mainly differ in two major respects: the questionanswer formats, e.g. cloze (fill-in-the-blank), span selection or multiple choice, and the text sources they use, such as news articles (Hermann et al., 2015; Trischler et al., 2017), fictional stories (Hill et al., 2016), Wikipedia articles (Kočiský et al., 2018; Hewlett et al., 2016; Rajpurkar et al., 2016) or other web sources (Joshi et al., 2017). A popular topic in computer vision closely related to RC is Visual Question Answering (VQA) in which context takes the form of an image in the comprehension task, where recent datasets have also been compiled, such as (Antol et al., 2015; Yu et al., 2015; Johnson et al., 2017; Goyal et al., 2017), to name a few.\nMore recently, research in QA has been extended to focus on the multimodal aspects of the problem where different modalities are being explored. Tapaswi et al. (2016) introduced MovieQA where they concentrate on evaluating automatic story comprehension from both video and text. In COMICS, Iyyer et al. (2017) turned to comic books to test understanding of closure, transitions in the narrative from one panel to the next. In AI2D (Kembhavi et al., 2016) and FigureQA (Kahou et al., 2018), the authors addressed comprehension of scientific diagrams and graphical plots. Last but not least, Kembhavi et al. (2017) has proposed another comprehensive and challenging dataset named TQA, which comprised of middle school science lessons of diagrams and texts.\nIn this study, we focus on multimodal machine comprehension of cooking recipes with images and text. To this end, we introduce a new QA dataset called RecipeQA that consists of recipe instructions and related questions (see Fig. 1 for an example text cloze style question). There are a handful of reasons why understanding and reasoning about\nText Cloze Style Question Context Modalities: Images and Descriptions of Steps\nRecipe: Last-Minute Lasagna\n1. Heat oven to 375 degrees F. Spoon a thin layer of sauce over the bottom of a 9-by-13-inch baking dish.\n2. Cover with a single layer of ravioli. 3. Top with half the spinach half the mozzarella and a third\nof the remaining sauce. 4. Repeat with another layer of ravioli and the remaining\nspinach mozzarella and half the remaining sauce. 5. Top with another layer of ravioli and the remaining sauce\nnot all the ravioli may be needed. Sprinkle with the Parmesan.\n6. Cover with foil and bake for 30 minutes. Uncover and bake until bubbly, 5 to 10 minutes.\n7. Let cool 5 minutes before spooning onto individual plates.\nStep 1 Step 2 Step 3 Step 4\nStep 5 Step 6 Step 7\nQuestion Choose the best text for the missing blank to correctly complete the recipe Cover. . Bake. Cool, serve.\nAnswer A. Top, sprinkle B. Finishing touches C. Layer it up D. Ravioli bonus round\nFigure 1: An illustrative text cloze style question (context, question and answer triplet). The context is comprised of recipe description and images where the question is generated using the question titles. Each paragraph in the context is taken from another step, as also true for the images. Bold answer is the correct one.\nrecipes is interesting. Recipes are written with a specific goal in mind, that is to teach others how to prepare a particular food. Hence, they contain immensely rich information about the real world. Recipes consist of instructions, wherein one needs to follow each instruction to successfully complete the recipe. As a classical example in introductory programming classes, each recipe might be seen as a particular way of solving a task and in that regard can also be considered as an algorithm. We believe that recipe comprehension is an elusive challenge and might be seen as important milestone in the long-standing goal of artificial intelligence and machine reasoning (Norvig, 1987; Bottou, 2014).\nAmong previous efforts towards multimodal machine comprehension (Tapaswi et al., 2016; Kembhavi et al., 2016; Iyyer et al., 2017; Kembhavi et al., 2017; Kahou et al., 2018), our study is closer to what Kembhavi et al. (2017) envisioned in TQA. Our task primarily differs in utilizing substantially larger number of images – the average number of images per recipe in RecipeQA is 12 whereas TQA has only 3 images per question on average. Moreover, in our case, each image is aligned with the text of a particular step in the corresponding recipe. Another important difference is that TQA contains mostly diagrams or textbook images whereas\nRecipeQA consists of natural images taken by users in unconstrained environments.\nSome of the important characteristics of RecipeQA are as follows:\n• There are arbitrary numbers of steps in recipes and images in steps, respectively.\n• There are different question styles, each requiring a specific comprehension skill.\n• There exists high lexical and syntactic divergence between contexts, questions and answers.\n• Answers require understanding procedural language, in particular keeping track of entities and/or actions and their state changes.\n• Answers may need information coming from multiple steps (i.e. multiple images and multiple paragraphs).\n• Answers inherently involve multimodal understanding of image(s) and text.\nTo sum up, we believe RecipeQA is a challenging benchmark dataset which will serve as a test bed for evaluating multimodal comprehension systems. In this paper, we present several statistical analyses on RecipeQA and also obtain baseline performances for a number of multimodal comprehension tasks that we introduce for cooking recipes."
  }, {
    "heading": "2 RecipeQA Dataset",
    "text": "The Recipe Question Answering (RecipeQA) dataset is a challenging multimodal dataset that evaluates reasoning over real-life cooking recipes. It consists of approximately 20K recipes from 22 food categories, and over 36K questions. Fig. 2 shows an illustrative cooking recipe from our dataset. Each recipe includes an arbitrary number of steps containing both textual and visual elements. In particular, each step of a recipe is accompanied by a ‘title’, a ‘description’ and a set of illustrative ‘images’ that are aligned with the title and the description. Each of these elements can be considered as a different modality of the data. The questions in RecipeQA explore the multimodal aspects of the step-by-step instructions available in the recipes through a number of specific tasks that are described in Sec. 3, namely textual cloze, visual cloze, visual coherency and visual ordering."
  }, {
    "heading": "2.1 Data Collection",
    "text": "We consider cooking recipes as the main data source for our dataset. These recipes were collected from Instructables1, which is a how-to web site where users share all kinds of instructions including but not limited to recipes.\nWe employed a set of heuristics that helped us collect high quality data in an automatic manner. For instance, while collecting the recipes, we downloaded only the most popular recipes by considering the popularity as an objective measure for assessing the quality of a recipe. Our assumption is that the mostly viewed recipes contain less noise and include easy-to-understand instructions with high-quality illustrative images.\nIn total, we collected about 20K unique recipes from the food category of Instructables. We filtered out non-English recipes using a language identification (Lui and Baldwin, 2012), and automatically removed the ones with unreadable contents such as the ones that only contain recipe videos. Finally, as a post processing step, we normalized the description text by removing non-ASCII characters from the text."
  }, {
    "heading": "2.2 Questions and Answers",
    "text": "For machine comprehension and reasoning, forming the questions and the answers is crucial for evaluating the ability of a model in understanding\n1All materials from the instructables.com were downloaded in April 2018.\nthe content. Prior studies employed natural language questions either collected via crowdsourcing platforms such as SQuAD (Rajpurkar et al., 2016) or generated synthetically as in CNN/Daily Mail (Hermann et al., 2015). Using natural language questions is a good approach in terms of capturing human understanding, but crowdsourcing is often too costly and does not scale well as the size of the dataset grows. Synthetic question generation is a low-cost solution, but the quality of the generated questions is subject to question.\nRecipeQA includes structured data about the cooking recipes that consists of step-by-step instructions, which helps us generate questions in a fully automatic manner without compromising the quality. Our questions test the semantics of the instructions of the recipes from different aspects through the tasks described in Sec. 3. In particular, we generate a set of multiple choice questions (the number of choices is fixed as four) by following a simple procedure which apply to all of our tasks with slight modifications.\nIn order to generate question-answer-context triplets, we first filtered out recipes that contain less than 3 steps or more than 25 steps. We also ignored the initial step of the recipes as our preliminary analysis showed that the first step of the recipes almost always is used by the authors to provide a narrative, e.g. why they love making that particular food, or how it makes sense to prepare a food for some occasion, and often is not relevant to the recipe instructions. In addition, we automatically removed some indicators such as step numbers that explicitly emphasize temporal order from the step titles while generating questions.\nGiven a task, we first randomly select a set of steps from each recipe and construct our questions and answers from these steps according to the task at hand. In particular, we employ the modality that the comprehension task is built upon to generate the candidate answers and use the remaining content as the necessary context for our questions. For instance, if the step titles are used within the candidate answers, the context becomes the descriptions and the images of the steps. As the average number of steps per recipe is larger than four, using this strategy, we can generate multiple context-questionanswer triplets from a single recipe.\nCandidate answers can be generated by selecting the distractors at random from the steps of other recipes. To make our dataset more challenging, we\nemploy a different strategy and select the distractors from the relevant modalities (titles, descriptions or images), which are not too far or too close from the correct answer. Specifically, we employ the following simple heuristic. We first find k nearest neighbors (k = 100) from other recipes. We then define an adaptive neighborhood by finding the closest distance to the query and remove the candidates that are too close. The remaining candidates are similar enough to be adversarial but not too similar to semantically substitute for the groundtruth. Finally, we randomly sample distractors from that pool. Details of the question generation procedure for each of the tasks are given in Sec. 3."
  }, {
    "heading": "2.3 Dataset Statistics",
    "text": "RecipeQA dataset contains approximately 20K cooking recipes and over 36K question-answer pairs divided into four major question types reflecting each of the task at hand. The data is split into non-overlapping training, validation and test sets so that one set does not include a recipe and/or questions about that recipe which are available in other sets. There are 22 different food categories\nacross our dataset whose distribution is shown in Fig. 3. While splitting the recipes into sets, we take into account these categories so that all the sets have a similar distribution of recipes across all the categories. In Table 1, we show the detailed statistics about our RecipeQA dataset. Moreover, to visualize the token frequencies, we also provide the word clouds of the titles and the descriptions from the recipes in Fig. 4."
  }, {
    "heading": "3 Tasks",
    "text": "RecipeQA includes four different types of tasks: (1) Textual cloze, (2) Visual cloze, (3) Visual coherence, and (4) Visual ordering. Each of these tasks requires different reasoning skills as discussed in (Sugawara et al., 2017), and considers different modalities in their contexts and candidate answer sets. By modalities, we refer to the following pieces of information: (i) titles of steps, (ii) descriptions of steps and (iii) illustrative images of steps. While generating the questions for these tasks, we rather employ fixed templates as will be discussed below, which helps us to automatically construct questionanswers pairs from the recipes with no human intervention. Using these tasks, we can easily evaluate complex relationships between different steps of a recipe via their titles, their descriptions and/or their illustrative images. Hence, our question-answers pairs are multimodal in nature. In the following, we provide a detailed description of each one of these tasks and discuss our strategies while selecting candidate answers."
  }, {
    "heading": "3.1 Textual Cloze",
    "text": "Textual cloze style questions test the ability to infer missing text either in the title or in the step description by taking into account the question’s context which includes a set of illustrative images besides text. While generating the question-answer pairs for this task, we randomly select a step from the candidate steps of a given recipe, hide its title and description, and ask for identifying this text amongst the multiple choices from the remaining modalities. To construct the distractor answers, we use the strategy in Sec. 2.2 that depends on the WMD (Kusner et al., 2015) distance measure. In Fig. 1, we provide a sample text cloze question from RecipeQA generated automatically in this way."
  }, {
    "heading": "3.2 Visual Cloze",
    "text": "Visual cloze style questions test a skill similar to that of textual cloze task with the difference that the missing information in this task reside in the visual domain. Here, just like the textual cloze task, for a recipe we randomly select a step, hide its representative image, and ask to infer this image amongst the multiple choices. The context for this task is all textual and is in the form of a sequence of titles and descriptions. To construct the distractor images, we use Euclidean distances of 2048-d pool5 features extracted from a ResNet-50 (He et al., 2016) pre-trained on ImageNet classification task. We show a sample visual cloze style question in Fig. 5 (second row)."
  }, {
    "heading": "3.3 Visual Coherence",
    "text": "Visual coherence style questions test the capability to identify an incoherent image in an ordered set of images given the titles and descriptions of the corresponding recipe as the context. Hence, to be successful at this task, a system needs to not only understand the relations between candidate steps, but also align and relate different modalities existing in the context and the answers. While generating the answer candidates for this task, we randomly select a single representative image from a single step and replace this image with a distractor image via employing the distractor selection strategy used for visual cloze task. In Fig. 5 (third row), we provide a sample visual coherence style question from RecipeQA."
  }, {
    "heading": "3.4 Visual Ordering",
    "text": "Visual ordering questions test the ability of a system in finding a correctly ordered sequence given a jumbled set of representative images of a recipe. As in the previous visual tasks, the context of this task consists of the titles and descriptions of a recipe. To\nContext Modalities: Titles and Descriptions of Steps\nRecipe: Bacon Sushi\nsuccessfully complete this task, the system needs to understand the temporal occurrence of a sequence of recipe steps and infer temporal relations between candidates, i.e. boiling the water first, putting the\nspaghetti next, so that the ordered sequence of images aligns with the given recipe. To generate answer choices, we simply use random permutations of the illustrative images in the recipe steps. In\nFig. 5 (last row), we illustrate this visual ordering task through an example question. Here, we should note that a similar task has been previously investigated by Agrawal et al. (2016) for visual stories where the task is to order a jumbled set of aligned image-description pairs."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Data Preparation",
    "text": "Ingredient Detection. We employed the method proposed in (Salvador et al., 2017) to detect recipe ingredients. To learn more effective word embeddings, we transformed the ingredients with compound words such as olive oil into single word ingredients with a proper hyphenation as olive oil.\nTextual Embeddings. We trained a distributed memory model, namely Doc2Vec (Le and Mikolov, 2014) and used it to learn word level and document level embeddings while encoding the semantic similarity by taking into account the word order within the provided context. In this way, we can represent each word, sentence or paragraph by a fixed sized vector. In our experiments, we employed 100-d vectors to represent all of the textual modalities (titles and descriptions). We made sure that the embeddings encode semantically useful information by exploring nearest neighbors (see Fig. 6 for some examples.)\nVisual Features. We used the final activation of the ResNet-50 (He et al., 2016) model trained on the ImageNet dataset (Russakovsky et al., 2015) to extract 2048-d dense visual representations. Then, we further utilized an autoencoder to decrease the dimension of the visual features to 100-d so that they become compatible in size with the text embeddings."
  }, {
    "heading": "4.2 Baseline Models",
    "text": "Neural Baselines. For our neural baselines, we adapted the Impatient Reader model in (Hermann et al., 2015), which was originally developed only for the cloze style text comprehension questions in the CNN/Daily Mail dataset. In our implementation, we used a uni-directional stacked LSTM architecture with 3 layers, in which we feed the context of the question to the network in a sequential manner. Particularly, we preserve the temporal order of the steps of the recipe while feeding it to the neural model, by mimicking the most common reading strategy – reading from top to bottom. For the multimodal setting, since images are represented with vectors which are of the same size with the text embeddings, we also feed the images to the network in the same order they are presented in the recipe.\nIn order to account for different question types, we employ a modular architecture, which requires small adjustments to be made for each task. For instance, we place the candidate answers into query for the cloze style questions or remove the candidate answer from the query for the visual coherence type questions. In training our Impatient Reader baseline model, we use a cosine similarity function and employed the hinge ranking loss (Collobert et al., 2011) as follows:\nL = max{0,M − cos(q, a+) + cos(q, a−)} (1)\nwhere M is a scalar denoting the margin, a+ represents the ground truth answer, and a− corresponds to an incorrect answer which is sampled randomly from the whole answer space. For all of our experiments, we select M as 1.5 and employ a simple heuristic to prevent overfitting by following an early stopping scheme with patience set to 10 against the validation set accuracy after the initial epoch. For the optimizer, we use ADAM and set the learning rate to 1e−3. The training took around 18 to 24 hours on GTX 1080Ti on a single GPU. We did not perform any hyperparameter tuning.\nSimple Baselines. We adapt the Hasty Student model described in (Tapaswi et al., 2016), which does not consider the provided context and simply answers questions by only looking at the similarities or the dissimilarities between the elements in questions and the candidate answers.\nFor the textual close task, each candidate answer is compared against the titles or descriptions of\nthe steps by using WMD (Kusner et al., 2015) distance, where such distances are averaged. Then, the choice closest to all of the question steps is selected as the final answer. For the visual cloze task, a similar approach is carried out by considering images instead of text using deep visual features. For the visual coherence task, since the aim is to find the incoherent image among other images, the final answer is chosen as the most dissimilar one to the remaining images on average. Lastly, for the visual ordering task, first, the distances between each consecutive image pair in a candidate ordering of the jumbled image set is estimated. Then, each candidate ordering is scored based on the average of these pairwise distances and the choice with the minimum average distance is set as the final answer. In all these simple baseline models, we use the cosine distance to rank the candidates."
  }, {
    "heading": "4.3 Baseline Results",
    "text": "We report the performance of the baseline models in Table 2 which indicates the ratio of correct answers against the total questions in the test. For the textual cloze, the comparison between text-only and multimodal Impatient Reader models shows that the additional visual modality helps the model to understand the question better and to provide more accurate answers. While for the cloze style questions, the Impatient Reader outperforms the Hasty student, for the visual coherence and visual ordering style questions Hasty student gives way better results. This demonstrates that better neural models are needed to be able to effectively deal with this kind of questions. Some qualitative examples are provided in the supplementary material."
  }, {
    "heading": "5 Related Work",
    "text": "Question Answering has been studied extensively in the literature. With the success of deep learning approaches in question answering, comprehension and reasoning aspects of the task has attracted researchers to investigate QA as a medium to measure intelligence. Various datasets and methods\nhave been proposed for measuring different aspects of the comprehension and reasoning problem. Each dataset has its own merits as well as weaknesses. Recently, a thorough analysis by (Chen et al., 2016) revealed that the required reasoning and inference level was quite simple for CNN/Daily Mail dataset (Hermann et al., 2015). To make reasoning task more realistic, new datasets such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), MSMARCO (Nguyen et al., 2016), CLEVR (Johnson et al., 2017), COMICS (Iyyer et al., 2017) and FigureQA (Kahou et al., 2018) have been proposed.\nIn the following, we briefly discuss the publicly available datasets that are closely related to our problem and provide an overview in Table 3.\nThe closest works to ours are (Iyyer et al., 2017), (Tapaswi et al., 2016) and (Kembhavi et al., 2017) where data multi-modality is the key aspect. COMICS dataset (Iyyer et al., 2017) focus on comic book narratives and explore visual cloze style questions, introducing a dataset consisting of drawings from comic books. The dataset is constructed from 4K Golden Age (1938-1954) comic books from the Digital Comics Museum and contains 1.2M panels with 2.5M textboxes. Three tasks are evaluated in this context, namely text cloze, visual cloze, character coherence. MovieQA dataset (Tapaswi et al., 2016), comprises of 15K crowdsourced questions about 408 movies. It consists of movie clips, subtitles, and snapshots, is about comprehending stories about movies. TQA dataset (Kembhavi et al., 2017), have 26K questions about 1K middle school science lessons with 3.5K im-\nages, mostly of diagrams and aims at addressing middle school knowledge acquisition using both images and text. Since the audience is middle school children, it requires limited reasoning.\nRecipeQA substantially differentiates from the previous work in the following way. Our dataset consists of natural images that are taken by anonymous users in unconstrained environments, which is a major diversion from COMICS and TQA datasets.\nIt should also be noted that there has been a long history of research involving cooking recipes. Recent examples include parsing of recipes (Malmaud et al., 2014; Jermsurawong and Habash, 2015), aligning instructional text to videos (Malmaud et al., 2015; Sener et al., 2015), recipe text generation (Kiddon et al., 2016), learning cross-modal embeddings (Salvador et al., 2017), tracking entities and action transformations in recipes (Bosselut et al., 2018).\nFinally, to our best knowledge, there is no dataset focusing on “how-to” instructions or recipes; hence, this work will be the first to serve multimodal comprehension of recipes having an arbitrary number of steps aligned with multiple images and multiple sentences."
  }, {
    "heading": "6 Conclusion",
    "text": "We present RecipeQA, a dataset for multimodal comprehension of cooking recipes, which consists of roughly 20K cooking recipes with over 36K context-question-answer triplets. To our knowledge, RecipeQA is the first machine comprehension dataset that deals with understanding procedural knowledge in a multimodal setting. Each one of the four question styles in our dataset is specifically tailored to evaluate a particular skill and requires connecting the dots between different modalities. Results of our baseline models demonstrate that RecipeQA is a challenging dataset and we plan make it publicly available for other researchers to promote the development of new methods for multimodal machine comprehension. In the future, we also intend to extend the dataset by collecting natural language questions-answer pairs via crowdsourcing. We also hope that RecipeQA will serve other purposes for related research problems on cooking recipes as well."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank our anonymous reviewers for their insightful comments and suggestions, which helped us improve the paper, Taha Sevim and Kenan Hagverdiyev for their help in building the RecipeQA challenge website, and NVIDIA Corporation for the donation of GPUs used in this research. This work was supported in part by a Hacettepe BAP fellowship (FBB-2016-11653) awarded to Erkut Erdem. Semih Yagcioglu was partly sponsored by STM A.Ş."
  }],
  "year": 2018,
  "references": [{
    "title": "Sort story: Sorting jumbled images and captions into stories",
    "authors": ["Harsh Agrawal", "Arjun Chandrasekaran", "Dhruv Batra", "Devi Parikh", "Mohit Bansal."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 925–931.",
    "year": 2016
  }, {
    "title": "VQA: Visual question answering",
    "authors": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."],
    "venue": "IEEE International Conference on Computer Vision (ICCV), pages 2425–2433.",
    "year": 2015
  }, {
    "title": "Simulating action dynamics with neural process networks",
    "authors": ["Antoine Bosselut", "Corin Ennis", "Omer Levy", "Ari Holtzman", "Dieter Fox", "Yejin Choi."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2018
  }, {
    "title": "From machine learning to machine reasoning - an essay",
    "authors": ["Léon Bottou."],
    "venue": "Machine Learning, 94(2):133– 149.",
    "year": 2014
  }, {
    "title": "Towards the machine comprehension of text: An essay",
    "authors": ["Christopher JC Burges."],
    "venue": "Technical report, Technical report, Microsoft Research Technical Report MSR-TR-2013-125.",
    "year": 2013
  }, {
    "title": "A thorough examination of the CNN/Daily Mail reading comprehension task",
    "authors": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2016
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "Journal of Machine Learning Research, pages 2493—-2537.",
    "year": 2011
  }, {
    "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
    "authors": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recog-",
    "year": 2017
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
    "year": 2016
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1693–1701.",
    "year": 2015
  }, {
    "title": "WikiReading: A novel large-scale language understanding task over Wikipedia",
    "authors": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot."],
    "venue": "Association for Computational Lin-",
    "year": 2016
  }, {
    "title": "The goldilocks principle: Reading children’s books with explicit memory representations",
    "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "Natural language question answering: The view from here",
    "authors": ["Lynette Hirschman", "Robert Gaizauskas."],
    "venue": "Natural Language Engineering, 7(4):275– 300.",
    "year": 2001
  }, {
    "title": "The amazing mysteries of the gutter: Drawing inferences between panels in comic book narratives",
    "authors": ["Mohit Iyyer", "Varun Manjunatha", "Anupam Guha", "Yogarshi Vyas", "Jordan Boyd-Graber", "Hal Daumé III", "Larry Davis."],
    "venue": "IEEE Conference on Computer",
    "year": 2017
  }, {
    "title": "Predicting the structure of cooking recipes",
    "authors": ["Jermsak Jermsurawong", "Nizar Habash."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 781–786.",
    "year": 2015
  }, {
    "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
    "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick."],
    "venue": "IEEE Conference on Computer Vision and Pattern",
    "year": 2017
  }, {
    "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
    "authors": ["Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer."],
    "venue": "Association for Computational Linguistics (ACL), pages 1601–1611, Vancouver,",
    "year": 2017
  }, {
    "title": "FigureQA: An annotated figure dataset for visual reasoning",
    "authors": ["Samira Ebrahimi Kahou", "Adam Atkinson", "Vincent Michalski", "Akos Kadar", "Adam Trischler", "Yoshua Bengio."],
    "venue": "International Conference on Learning Representations (ICLR) Work-",
    "year": 2018
  }, {
    "title": "A diagram is worth a dozen images",
    "authors": ["Aniruddha Kembhavi", "Mike Salvato", "Eric Kolve", "Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi."],
    "venue": "European Conference on Computer Vision, pages 235– 251. Springer.",
    "year": 2016
  }, {
    "title": "Are you smarter than a sixth grader",
    "authors": ["Aniruddha Kembhavi", "Minjoon Seo", "Dustin Schwenk", "Jonghyun Choi", "Ali Farhadi", "Hannaneh Hajishirzi"],
    "year": 2017
  }, {
    "title": "Globally coherent text generation with neural checklist models",
    "authors": ["Chloé Kiddon", "Luke Zettlemoyer", "Yejin Choi."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 329—-339.",
    "year": 2016
  }, {
    "title": "The NarrativeQA reading comprehension challenge",
    "authors": ["Tomáš Kočiský", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette."],
    "venue": "Transactions of the Association for Computational Linguistics.",
    "year": 2018
  }, {
    "title": "From word embeddings to document distances",
    "authors": ["Matt J. Kusner", "Yu Sun", "Nicholas I. Kolkin", "Kilian Q. Weinberger."],
    "venue": "International Conference on Machine Learning (ICML), pages 957–966.",
    "year": 2015
  }, {
    "title": "Distributed representations of sentences and documents",
    "authors": ["Quoc Le", "Tomas Mikolov."],
    "venue": "International Conference on Machine Learning (ICML), pages 1188–1196.",
    "year": 2014
  }, {
    "title": "langid.py: An off-the-shelf language identification tool. In Association for Computational Linguistics (ACL",
    "authors": ["Marco Lui", "Timothy Baldwin"],
    "venue": "Demo Session,",
    "year": 2012
  }, {
    "title": "What’s cookin’? Interpreting cooking videos using text, speech and vision",
    "authors": ["Jonathan Malmaud", "Jonathan Huang", "Vivek Rathod", "Nick Johnston", "Andrew Rabinovich", "Kevin Murphy."],
    "venue": "North American Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Cooking with semantics",
    "authors": ["Jonathan Malmaud", "Earl Wagner", "Nancy Chang", "Kevin Murphy."],
    "venue": "ACL 2014 Workshop on Semantic Parsing, pages 33–",
    "year": 2014
  }, {
    "title": "MS MARCO: A human generated machine reading comprehension dataset",
    "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."],
    "venue": "NIPS 2016 Workshop on Cognitive Computation.",
    "year": 2016
  }, {
    "title": "A unified theory of inference for text understanding",
    "authors": ["Peter Norvig."],
    "venue": "Technical report, University of California at Berkeley, Berkeley, CA, USA.",
    "year": 1987
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2016
  }, {
    "title": "ImageNet: Large scale visual recognition challenge",
    "authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei."],
    "venue": "Interna-",
    "year": 2015
  }, {
    "title": "Learning cross-modal embeddings for cooking recipes and food images",
    "authors": ["Amaia Salvador", "Nicholas Hynes", "Yusuf Aytar", "Javier Marin", "Ferda Ofli", "Ingmar Weber", "Antonio Torralba."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
    "year": 2017
  }, {
    "title": "Unsupervised semantic parsing of video collections",
    "authors": ["Ozan Sener", "Amir Zamir", "Silvio Savarese", "Ashutosh Saxena."],
    "venue": "IEEE International Conference on Computer Vision (ICCV).",
    "year": 2015
  }, {
    "title": "Prerequisite skills for reading comprehension: Multi-perspective analysis of mctest datasets and systems",
    "authors": ["Saku Sugawara", "Hikaru Yokono", "Akiko Aizawa."],
    "venue": "Association for the Advancement of Artificial Intelligence (AAAI), pages 3089–3096.",
    "year": 2017
  }, {
    "title": "MovieQA: Understanding stories in movies through question-answering",
    "authors": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."],
    "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
    "year": 2016
  }, {
    "title": "Newsqa: A machine comprehension dataset",
    "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."],
    "venue": "2nd Workshop on Representation Learning for NLP.",
    "year": 2017
  }, {
    "title": "Answering and questioning for machine reading",
    "authors": ["Lucy Vanderwende."],
    "venue": "AAAI Spring Symposium: Machine Reading, page 91.",
    "year": 2007
  }, {
    "title": "Visual madlibs: Fill in the blank description generation and question answering",
    "authors": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L Berg."],
    "venue": "IEEE International Conference on Computer Vision (ICCV), pages 2461–2469.",
    "year": 2015
  }],
  "id": "SP:e04428ce77d6d459b7063d6bda7a8f72a539f284",
  "authors": [{
    "name": "Semih Yagcioglu",
    "affiliations": []
  }, {
    "name": "Aykut Erdem",
    "affiliations": []
  }, {
    "name": "Erkut Erdem",
    "affiliations": []
  }, {
    "name": "Nazli Ikizler-Cinbis",
    "affiliations": []
  }],
  "abstractText": "Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa.",
  "title": "RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes"
}