{
  "sections": [{
    "text": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 231–237, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Question Answering (QA) is a challenging task that draws upon many aspects of NLP. Unlike search or information retrieval, answers infrequently contain lexical overlap with the question (e.g. What should we eat for breakfast? – Zoe’s Diner has good pancakes), and require QA models to draw upon more complex methods to bridge this ”lexical chasm” (Berger et al., 2000). These methods range from robust shallow models based on lexical semantics, to deeper, explainably-correct, but much more brittle inference methods based on first order logic.\nBerger et al. (2000) proposed that this ”lexical chasm” might be partially bridged by repurposing statistical machine translation (SMT) models for QA. Instead of translating text from one language to another, these monolingual alignment models learn to translate from question to answer1, learning common associations from question terms such as eat or breakfast to answer terms like kitchen, pancakes, or cereal.\nWhile monolingual alignment models have enjoyed a good deal of recent success in QA (see related work), they have expensive training data requirements, requiring a large set of aligned indomain question-answer pairs for training. For lowresource languages or specialized domains like science or biology, often the only option is to enlist a domain expert to generate gold QA pairs – a process that is both expensive and time consuming. All of this means that only in rare cases are we accorded the luxury of having enough high-quality QA pairs to properly train an alignment model, and so these models are often underutilized or left struggling for resources.\nMaking use of recent advancements in discourse parsing (Feng and Hirst, 2012), here we address this issue, and investigate whether alignment models for QA can be trained from artificial question-answer pairs generated from discourse structures imposed on free text. We evaluate our methods on two corpora, generating alignment models for an opendomain community QA task using Gigaword2, and for a biology-domain QA task using a biology textbook.\n1In practice, alignment for QA is often done from answer to question, as answers tend to be longer and provide more opportunity for association (Surdeanu et al., 2011).\n2LDC catalog number LDC2012T21\n231\nThe contributions of this work are: 1. We demonstrate that by exploiting the dis-\ncourse structure of free text, monolingual alignment models can be trained to surpass the performance of models built from expensive indomain question-answer pairs. 2. We compare two methods of discourse parsing: a simple sequential model, and a deep model based on Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). We show that the RST-based method captures within and across-sentence alignments and performs better than the sequential model, but the sequential model is an acceptable approximation when a discourse parser is not available. 3. We evaluate the proposed methods on two corpora, including a low-resource domain where training data is expensive (biology). 4. We experimentally demonstrate that monolingual alignment models trained using our method considerably outperform state-of-theart neural network language models in low resource domains."
  }, {
    "heading": "2 Related Work",
    "text": "Lexical semantic models have shown promise in bridging Berger et al.’s (2000) ”lexical chasm.” In general, these models can be classified into alignment models (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013) which require structured training data, and language models (Jansen et al., 2014; Sultan et al., 2014; Yih et al., 2013), which operate over free text. Here, we close this gap in resource availability by developing a method to train an alignment model over free text by making use of discourse structures.\nDiscourse has been previously applied to QA to help identify answer candidates that contain explanatory text (e.g. Verberne et al. (2007)). Jansen et al. (2014) proposed a reranking model that used both shallow and deep discourse features to identify answer structures in large answer collections across different tasks and genres. Here we use discourse to impose structure on free text to create inexpensive knowledge resources for monolingual alignment. Our work is conceptually complementary to that of Jansen et al. – where they explored\nlargely unlexicalized discourse structures to identify explanatory text, we use discourse to learn lexicalized models for semantic similarity.\nOur work is conceptually closest to that of Hickl et al. (2006), who created artificially aligned pairs for textual entailment. Taking advantage of the structure of news articles, wherein the first sentence tends to provide a broad summary of the article’s contents, Hickl et al. aligned the first sentence of each article with its headline. By making use of automated discourse parsing, here we go further and impose alignment structure over an entire text."
  }, {
    "heading": "3 Approach",
    "text": "A written text is not simply a collection of sentences, but rather a flowing narrative where sentences and sentence elements depend on each other for meaning – a concept known as cohesion (Halliday and Hasan, 2014). Here we examine two methods for generating alignment training data from free text that make use of cohesion: a shallow method that uses only intersentence structures, and a deep method that uses both intrasentence and intersentence structures. We additionally attempt to separate the contribution of discourse from that of alignment in general by comparing these models against a baseline alignment model which aligns sentences at random.\nThe first model, the sequential discourse model (SEQ), considers that each sentence continues the\nnarrative of the previous one, and creates artificial question-answer pairs from all pairs of consecutive sentences. Thus, this model takes advantage of intersentence cohesion by aligning the content words3 in each sentence with the content words in the following sentence. For example, in the passage in Figure 1, this model would associate cider in the first sentence with apples and orchard in the second sentence.\nThe second model uses RST to capture discourse cohesion both within and across sentence boundaries. We extracted RST discourse structures using an in-house parser (Surdeanu et al., 2015), which follows the architecture introduced by Hernault et al. (2010) and Feng and Hirst (2012). The parser first segments text into elementary discourse units (EDUs), which may be at sub-sentence granularity, then recursively connects neighboring units with binary discourse relations, such as Elaboration or Contrast.4 Our parser differs from previous work with respect to feature generation in that we implement all features that rely on syntax using solely dependency syntax. For example, a crucial feature used by the parser is the dominance relations of Soricut and Marcu (2003), which capture syntactic dominance between discourse units located in the same sentence. While originally these dominance relations were implemented using constituent syntax, we provide an equivalent implementation that relies on dependency syntax. The main advantage to this approach is speed: the resulting parser performs at least an order of magnitude faster than the parser of Feng and Hirst (2012).\nImportantly, we generate artificial alignment pairs from this imposed structure by aligning the governing text (nucleus) with its dependent text (satellite).5 Turning again to the example in Figure 1, this RSTbased model captures additional alignments that are both intrasentence, e.g., apples–orchard, and intersentence, e.g., cider–autumn.\n3In pilot experiments, we found that aligning only nouns, verbs, adjectives, and adverbs yielded higher performance.\n4The RST parser performs better on relations which occur more frequently. We use only relations that occurred at least 1% of the time. This amounted to six relations: elaboration, attribution, background, contrast, same-unit, and joint. Using all relations slightly improves performance by 0.3% P@1.\n5Pilot experiments showed that this direction of alignment performed better than aligning from satellite to nucleus."
  }, {
    "heading": "4 Models and Features",
    "text": "We evaluate the contribution of these alignment models using a standard reranking architecture (Jansen et al., 2014). The initial ranking of candidate answers is done using a shallow candidate retrieval (CR) component.6 Then, these answers are reranked using a more expressive model that incorporates alignment features alongside the CR score. As a learning framework we use SVMrank, a Support Vector Machine tailored for ranking.7 We compare this alignment-based reranking model against one that uses a state-of-the-art recurrent neural network language model (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2013), which has been successfully applied to QA previously (Yih et al., 2013).\nAlignment Model: The alignment matrices were generated with IBM Model 1 (Brown et al., 1993) using GIZA++ (Och and Ney, 2003), and the corresponding models were implemented as per Surdeanu et al. (2011) with a global alignment probability. We extend this alignment model with features from Fried et al. (In press) that treat each (source) word’s probability distribution (over destination words) in the alignment matrix as a distributed semantic representation, and make use the Jensen-Shannon distance (JSD)8 between these conditional distributions. A summary of all these features is shown in Table 1.\nRNNLM: We learned word embeddings using the word2vec RNNLM of Mikolov et al. (2013), and include the cosine similarity-based features described in Table 1."
  }, {
    "heading": "5 Experiments",
    "text": "We tested our approach in two different domains, open-domain and cellular biology. For consistency we use the same corpora as Jansen et al. (2014), which are described briefly here.\nYahoo! Answers (YA): Ten thousand open-domain how questions were randomly chosen from the Ya-\n6We use the same cosine similarity between question and answer lemmas as Jansen et al. (2014), weighted using tf.idf.\n7http://www.cs.cornell.edu/people/tj/ svm_light/svm_rank.html\n8Jensen-Shannon distance is based on Kullback-Liebler divergence but is a distance metric (finite and symmetric).\nhoo! Answers9 community question answering corpus and divided: 50% for training, 25% for development, and 25% for test. Candidate answers for a given question are selected from the corresponding answers proposed by the community (each question has an average of 9 answers).\nBiology QA (Bio): 183 how and 193 why questions in the cellular biology domain were hand-crafted by a domain expert, and paired with gold answers in the Campbell’s Biology textbook (Reece et al., 2011). Each paragraph in the textbook was considered as a candidate answer. As there were few questions, five fold cross-validation was used with three folds for training, one for development, and one for test.\nAlignment Corpora: To train the alignment models we generated alignment pairs from two different resources: Annotated Gigaword (Napoles et al., 2012) for YA, and the textbook for Bio. Each was discourse parsed with the RST discourse parser described in Section 3, which is implemented in the FastNLPProcessor toolkit10, using the MaltParser11 for syntactic analysis."
  }, {
    "heading": "5.1 Results and Discussion",
    "text": "Figure 2 shows the performance of the discourse models against the number of documents used to train the alignment model.12 We used the standard implementation for P@1 (Manning et al., 2008) with the adaptations for Bio described in Jansen et al. (2014). We address the following questions.\n9http://answers.yahoo.com 10http://github.com/sistanlp/processors 11http://www.maltparser.org/ 12For space reasons the graph for Bio how is not shown, but\nthe pattern is essentially identical to Bio why.\nHow does the performance of the RST and SEQ models compare? Comparing the two principal alignment models, the RST-based model significantly outperforms the SEQ model by about 0.5% P@1 in both domains (p < 0.001 for Bio and p < 0.01 for YA)13. This shows that deep discourse anal-\n13All reported statistics were performed at the endpoints, i.e., when all training data is used, using bootstrap resampling with\nysis (as imperfect as it is today) is beneficial.\nHow does the performance of the RST model compare to a model trained on in-domain pairs? Both the RST and SEQ results for YA are higher than that of an alignment model trained on explicit in-domain question-answer pairs. Fried et. al (In press) trained an identical alignment model using approximately 65k QA pairs from the YA corpus, and report a performance of 27.24% P@1, or nearly 2 points lower than our model trained using 10,000 Gigaword documents. This is an encouraging result, which further demonstrates that: (a) discourse analysis can be exploited to generate artificial semistructured data for alignment, and (b) the sequential model, which also outperforms Fried et. al, can be used as a reasonable proxy for discourse when a parser is not available.\nHow does the performance of the RST model compare to previous work? Comparing our work to Jansen et al. (2014), the most relevant prior work, we notice two trends. First, our discourse-based alignment models outperform their CR + RNNLM model, which peaks at 26.6% P@1 for YA and 31.7% for Bio why. While some of this difference can be assigned to implementation differences (e.g., we consider only content words for both alignment and RNNLM, where they used all words), this result again emphasizes the value of our approach. Second, the partially lexicalized discourse structures used by Jansen et. al to identify explanatory text in candidate answers perform better than our approach, which relies solely on lexicalized alignment. However, we expect that our two approaches are complementary, because they address different aspects of the QA task (structure vs. similarity).\nHow do the RST and SEQ models compare to the non-alignment baselines? In Bio, both the RST and SEQ alignment models significantly outperform the RNNLM and CR baselines (p < 0.001). In YA, the RST and SEQ models significantly outperform the CR baseline (p < 0.001), and though they considerably outperform the the RNNLM baseline for most training document sizes, when all 10,000 documents are used for training, they do not perform better. This shows that alignment models are more\n10,000 iterations.\nrobust to little training data, but RNNLMs catch up when considerable data is available.\nHow does the SEQ model compare to the RND baseline? In Bio, the SEQ model significantly outperforms the RND baseline (p < 0.001) but in YA it does not. This is likely due to differences in the size of the document which was randomized. In YA, the sentences were randomized within Gigaword articles, which are relatively short (averaging 19 sentences), whereas in Bio the randomization was done at the textbook level. In practice, as document size decreases, the RND model approaches the SEQ model.\nWhy does performance plateau in YA and not in Bio? With Bio, we exploit all of the limited indomain training data, and continue to see performance improvements. With YA, however, performance asymptotes for the alignment models when trained beyond 10,000 documents, or less than 1% of the Gigaword corpus. Similarly, when trained over the entirety of Gigaword (two orders of magnitude more data), our RNNLM improves only slightly, peaking at approximately 30.5% P@1 (or, a little over 1% P@1 higher). We hypothesize that this limitation comes from failing to take context into account. In open domains, alignments such as apple – orchard may interfere with those from different contexts, e.g., apple – computer, and add noise to the answer selection process."
  }, {
    "heading": "6 Conclusion",
    "text": "We propose two inexpensive methods for training alignment models using solely free text, by generating artificial question-answer pairs from discourse structures. Our experiments indicate that these methods are a viable solution for constructing stateof-the-art QA systems for low-resource domains, or languages where training data is expensive and/or limited. Since alignment models have shown utility in other tasks (e.g. textual entailment), we hypothesize that these methods for creating inexpensive and highly specialized training data could be useful for tasks other than QA."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the Allen Institute for AI for funding this work."
  }],
  "year": 2015,
  "references": [{
    "title": "Bridging the lexical chasm: Statistical approaches to answer finding",
    "authors": ["Adam Berger", "Rich Caruana", "David Cohn", "Dayne Freytag", "Vibhu Mittal."],
    "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research & Development on Infor-",
    "year": 2000
  }, {
    "title": "The mathematics of statistical machine translation: Parameter estimation",
    "authors": ["Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."],
    "venue": "Computational Linguistics, 19(2):263–311.",
    "year": 1993
  }, {
    "title": "A noisy-channel approach to question answering",
    "authors": ["Abdessamad Echihabi", "Daniel Marcu."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 16–23. Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "Text-level discourse parsing with rich linguistic features",
    "authors": ["Vanessa Wei Feng", "Graeme Hirst."],
    "venue": "Proceedings of the Association for Computational Linguistics.",
    "year": 2012
  }, {
    "title": "Cohesion in english",
    "authors": ["Michael Alexander Kirkwood Halliday", "Ruqaiya Hasan."],
    "venue": "Routledge.",
    "year": 2014
  }, {
    "title": "HILDA: A discourse parser using support vector machine classification",
    "authors": ["H. Hernault", "H. Prendinger", "D. duVerle", "M. Ishizuka."],
    "venue": "Dialogue and Discourse, 1(3):1–33.",
    "year": 2010
  }, {
    "title": "Recognizing textual entailment with lccs groundhog system",
    "authors": ["Andrew Hickl", "John Williams", "Jeremy Bensley", "Kirk Roberts", "Bryan Rink", "Ying Shi."],
    "venue": "Proceedings of the Second PASCAL Challenges Workshop.",
    "year": 2006
  }, {
    "title": "Discourse complements lexical semantics for nonfactoid answer reranking",
    "authors": ["Peter Jansen", "Mihai Surdeanu", "Peter Clark."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2014
  }, {
    "title": "Rhetorical structure theory: Toward a functional theory of text organization",
    "authors": ["William C. Mann", "Sandra A. Thompson."],
    "venue": "Text, 8(3):243–281.",
    "year": 1988
  }, {
    "title": "Introduction to Information Retrieval",
    "authors": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Schütze."],
    "venue": "Cambridge University Press.",
    "year": 2008
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["Tomas Mikolov", "Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur."],
    "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH",
    "year": 2010
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the International Conference on Learning Representations (ICLR).",
    "year": 2013
  }, {
    "title": "Annotated gigaword",
    "authors": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme."],
    "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX ’12, pages 95–100, Strouds-",
    "year": 2012
  }, {
    "title": "A systematic comparison of various statistical alignment models",
    "authors": ["Franz Josef Och", "Hermann Ney."],
    "venue": "Computational Linguistics, 29(1):19–51.",
    "year": 2003
  }, {
    "title": "Campbell Biology",
    "authors": ["J.B. Reece", "L.A. Urry", "M.L. Cain", "S.A. Wasserman", "P.V. Minorsky."],
    "venue": "Pearson Benjamin Cummings.",
    "year": 2011
  }, {
    "title": "Statistical machine translation for query expansion in answer retrieval",
    "authors": ["Stefan Riezler", "Alexander Vasserman", "Ioannis Tsochantaridis", "Vibhu Mittal", "Yi Liu."],
    "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),",
    "year": 2007
  }, {
    "title": "Automatic question answering using the web: Beyond the factoid",
    "authors": ["Radu Soricut", "Eric Brill."],
    "venue": "Journal of Information Retrieval - Special Issue on Web Information Retrieval, 9(2):191–206.",
    "year": 2006
  }, {
    "title": "Sentence level discourse parsing using syntactic and lexical information",
    "authors": ["R. Soricut", "D. Marcu."],
    "venue": "Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.",
    "year": 2003
  }, {
    "title": "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence",
    "authors": ["Md. Arafat Sultan", "Steven Bethard", "Tamara Sumner."],
    "venue": "Transactions of the Association for Computational Linguistics, 2:219–230.",
    "year": 2014
  }, {
    "title": "Learning to rank answers to nonfactoid questions from web collections",
    "authors": ["Mihai Surdeanu", "Massimiliano Ciaramita", "Hugo Zaragoza."],
    "venue": "Computational Linguistics, 37(2):351–383.",
    "year": 2011
  }, {
    "title": "Two practical rhetorical structure theory parsers",
    "authors": ["Mihai Surdeanu", "Thomas Hicks", "Marco A. Valenzuela-Escárcega."],
    "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL): Software Demonstra-",
    "year": 2015
  }, {
    "title": "Discourse-based answering of why-questions",
    "authors": ["Susan Verberne", "Lou Boves", "Nelleke Oostdijk", "PeterArno Coppen"],
    "venue": "Traitement Automatique des Langues, Discours et document: traitements automatiques,",
    "year": 2007
  }, {
    "title": "Semi-markov phrasebased monolingual alignment",
    "authors": ["Xuchen Yao", "Benjamin Van Durme", "Chris CallisonBurch", "Peter Clark."],
    "venue": "Proceedings of EMNLP.",
    "year": 2013
  }, {
    "title": "Question answering using enhanced lexical semantic models",
    "authors": ["Wen-tau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2013
  }],
  "id": "SP:d8c2bae7eee9a2e5011bab6b9c3888b23ea8c6c8",
  "authors": [{
    "name": "Rebecca Sharp",
    "affiliations": []
  }, {
    "name": "Peter Jansen",
    "affiliations": []
  }, {
    "name": "Mihai Surdeanu",
    "affiliations": []
  }, {
    "name": "Peter Clark",
    "affiliations": []
  }],
  "abstractText": "Monolingual alignment models have been shown to boost the performance of question answering systems by ”bridging the lexical chasm” between questions and answers. The main limitation of these approaches is that they require semistructured training data in the form of question-answer pairs, which is difficult to obtain in specialized domains or lowresource languages. We propose two inexpensive methods for training alignment models solely using free text, by generating artificial question-answer pairs from discourse structures. Our approach is driven by two representations of discourse: a shallow sequential representation, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We show that these alignment models trained directly from discourse structures imposed on free text improve performance considerably over an information retrieval baseline and a neural network language model trained on the same data.",
  "title": "Spinning Straw into Gold: Using Free Text to Train Monolingual Alignment Models for Non-factoid Question Answering"
}