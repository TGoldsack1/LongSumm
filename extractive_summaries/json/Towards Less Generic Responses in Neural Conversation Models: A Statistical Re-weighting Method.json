{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2769–2774 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2769"
  }, {
    "heading": "1 Introduction",
    "text": "Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq\n∗This work was done while Yahui Liu was with Tencent AI Lab.\n†Corresponding author\nmodels are used for dialogue generation tasks. One of the most important problems is that such models are inclined to generate generic and dull responses (e.g., I don’t know), rather than meaningful and specific answers (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,c; Kannan et al., 2016; Li et al., 2017; Xie, 2017; Wei et al., 2017; Mou et al., 2017).\nUntil now, it has attracted increasing studies to address the issue of generating generic response. For example, Li et al. (2016a) used the mutual information theory to reconstruct MLE, but this model is easy to generate ungrammatical outputs. They further proposed a fast diverse decoding approach (Li et al., 2016b), which modifies the beam search to re-rank meaningful responses into higher positions. Similar works explore different ways to encourage response diversity for picking less generic responses in the decoding search (Vijayakumar et al., 2016; Li and Jurafsky, 2016). In the reinforcement learning framework (Li et al., 2016c), the reward function used in the decoding considers the ease of answering, which is measured by a distance towards a set of 8 generic responses. Thus, it can also alleviate the problem of generating generic responses to some extent. Lison and Bibauw (2017) proposed to add a weighting model to learn the “quality” of the query and response pair, but it relies heavily on additional inputs. All these works tried to add extra optimized terms in the encoding or decoding modules in Seq2Seq, making the training or prediction more complicated.\nIn this work, we consider the reason why Seq2Seq often generates generic responses by analyzing the MLE objective function directly. We notice that multiple responses are often associated with one single input query. As shown in Figure 1, the relationship between queries and responses is much looser in conversation models than that in NMT, since the space of possible responses is much\nlarger than the space of possible translations for a given sentence. On one hand, the information of these responses is only required to be relevant to the input query but usually differs from it. On the other hand, a query accepts large semantic diversity among its responses. Hence, it is a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017). Meanwhile, we can see there is a m-to-n relationship between all queries and responses in the training corpus. Then, we find that MLE, which learns a 1-to-1 mapping in response generation, naturally puts more emphasis on optimizing the frequent patterns. Thus, the converged local optimum is easy to output these patterns or their combinations, leading to generic responses.\nInspired by this observation, we propose a statistical re-weighting method which modifies MLE by re-weighting the multiple responses for each query such that MLE will not be dominated by the frequent patterns or their combinations. The proposed method calculates the weights of a response with the consideration of two statistical features: similarity frequency and sentence length. Our model is simple and efficient to optimize without adding additional terms into the original Seq2Seq objective function. We validate the performance of our proposed method on a large Chinese dialogue corpus. Results show that it can improve the acceptance rate of the generated responses and significantly suppress the number of generic responses."
  }, {
    "heading": "2 Proposed Method",
    "text": "Standard Seq2Seq models for NMT and dialogue generation aim at estimating the conditional probability p(y|x) where x = (x1, . . . , xT ) is an input sequence and y = (y1, . . . , yT ′) is its corresponding output sequence whose length T ′ may differ from T . During training, we learn all the model parameters θ by summing the negative log likelihood\nof each sample pair (x,y) in the training corpus C:\n`(x,y, θ) = − T ′∑ t=1 log p(yt|x,y[t−1];θ), (1)\nL(C, θ) = ∑\n(x,y)∈C\n`(x,y, θ). (2)\nRecall that generic responses are those that are safe and universal for many queries and thus frequently appear in the training corpus. Hence, if we have two responses of x in which one is generic and the other one contains more meaningful content, using L(C, θ) in Eq. 1 will put the same emphasis on optimizing each of their loss terms. Therefore, L(C, θ) contains a large amount of patterns from the generic responses, thus it is not surprised to see that the trained models are stuck into local optimum that are inclined to generate these patterns or their combinations.\nBased on this observation, we argue that a good loss function of Seq2Seq for dialogue generation should not be dominated by the patterns from generic responses. Here, we propose a reweighting method for responses of a query x. Specifically, `(x,y, θ) in Eq. 1 is modified to be:\n`w(x,y, θ) = w(y|x)`(x,y, θ), (3)\nwhere w(y|x) ∈ (0, 1] is a soft weight for a response y of a query x. In the implementation, we make the normalization of this loss at the mini-batch level for better computational efficiency. Hence, the loss of Eq. 2 for a mini-batch L(B, θ) takes the form:\nL(B, θ) = ∑\nx,y∈B `w(x,y, θ)∑ x,y∈Bw(y|x) . (4)\nWe summarize two common properties for the responses:\n• Responses with the patterns of frequently appearing in the training corpus tend to be generic. Here, the patterns refer to both the whole sentence or n-grams which can be described by similarities among responses.\n• Very short and long responses should be avoid. Owing to the MLE objective function, the Seq2Seq frameworks are inclined to generate short responses that are universal replies. While long responses usually contain more specific information which may not be generalized to most conversation scenarios. Hence,\nhigh-quality responses tend to be with moderate length.\nWe propose an estimator by considering these two properties:\nw(y|x,R,C) = Φ(y) maxr∈R{Φ(r)} , (5)\nwhere R denotes all collected responses of x in C. For each response, the estimator gives a weight by:\nΦ(y) = αE(y) + βF(y). (6)\nHere, E(y) and F(y) correspond to the mentioned two properties respectively:\n• E(y) = e−af(y), where f(y) is a function related to the frequency of response y. It could be formulated as\nf(y) = max{0,Count(D(y,yj) ≥ τ)− b} ∀j ∈ |C|,\nwhere D(·) refers to the similarity between two sentences, a is a scale factor, b is bias and τ ∈ [0, 1] is a threshold specifying the similarity that two responses will be considered identical. For instance, it could be the simplest strictly matching, which is used in our experiments. Other methods like cosine distance of TF-IDF (token or n-grams) can also be applied, but may encounter computational issues for large corpus. A response with a higher frequency will be assigned with a smaller E(y).\n• F(y) = e−c||y|−|ŷ||, where |y| denotes the number of tokens in y, |ŷ| = 1|C| ∑ r∈C |r|\nrefers to the average length of responses in the total training corpus, and c is a scale factor. Here, the “moderate length” is set to the average length of responses of the total training corpus. In practice, we have tried to use long responses (longer than average length) to fine-tune the Seq2Seq model. Though it slightly increases the average length of generated responses, the generated responses suffer from more ungrammatical and influent issues. Hence, if a response is too short or long, it will receive a low score of F(y).\nMentioned hyper-parameters {α, β, a, b, τ, c} are constant values in the following experiments, which are set to {0.5, 0.5, 0.33, 3, 1.0, 0.33}. When\nwe performed our experiments, we tried several hyper-parameter settings and found that our method is not sensitive to different hyper-parameters and achieves stable results in general. Hence, we do not spend many efforts to specifically tune these hyper-parameters.\nTo validate that our design function in Eq. 5 and Eq. 6 are effective to weight the responses, Table 1 shows the weights of 8 responses for a query “其实 单身也挺好的 (It’s pretty good to be single)”. As can be seen, the weights are reasonable, in which the higher-ranked responses are more informative ones with low similarity frequency and moderate length."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Corpus and Evaluation",
    "text": "We crawl conversation pairs from some popular Chinese social media websites1, and select 7M high-quality pairs as our training corpus. Conventional metrics such as BLEU (Papineni et al., 2002) and perplexity, are improper to be used for response generation tasks. Following previous works (Li et al., 2016c, 2017), we apply human annotations. We randomly sample 500 queries (not used in training) as our test samples, and recruit 3 annotators to evaluate each generated response from two aspects:\n• Fluency: 0 (unreadable), 1 (readable but with some grammar mistakes), 2 (fluent);\n• Relevance: 0 (not relevant at all), 1 (relevant at a distant level), 2 (relevant, including the\n1Weibo: www.weibo.com, Baidu Tieba: tieba. baidu.com, and Zhihu: www.zhihu.com\ngeneric responses), 3 (relevant as well as interesting).\nAcceptance is then automatically calculated as a metric reflecting whether the response is acceptable to real users. A response will be assigned 1 when it gets Fluency≥1 and Relevance≥2, otherwise it will be assigned 0.\nWe implement our baseline Seq2Seq model using its standard objective function in Eq. 1 with two LSTM layers for encoding/decoding and a standard beam search with a beam size of 5 (the best setting), termed as Seq2Seq. We also compare several Seq2Seq variants:\n• Seq2Seq-RS: training with a subset by randomly sampling only one from the multiple responses for each query;\n• Seq2Seq-MMI: applying the maximum mutual information (Li et al., 2016a) (only the MMI-bidi);\n• Seq2Seq-DD: applying the diverse decoding algorithm (Li et al., 2016b);\n• Ours-RW: calculating weights via our reweighting method proposed in Section 2. Without applying any other tricks, we implement three versions of our method by using E(·) only, F(·) only, a linear combination of E(·) and F(·) in Eq 6, termed as OursRW{E,F,EF}."
  }, {
    "heading": "3.2 Results and Discussion",
    "text": "Human annotation results are shown in Table 2. Several observations can be made. First, Seq2SeqRS performs slightly worse than the baseline model. This means that it does not work to simply discard a large amount of training data to construct a 1- to-1 query-response subset for training. Second, Seq2Seq-MMI not only provides no improvement for the baseline but also inclines to generate generic response. Third, Seq2Seq-DD obtains higher relevance and acceptance scores than the baseline, which shows its effectiveness by re-ranking more meaningful responses into higher positions in beam search. Fourth, our method achieves the best performance on almost all metrics. When we use strictly matched frequency of each response, Ours-RWE does not perform better than the baseline model because that the percentage of responses with frequency higher than 3 is about 0.5% in our training corpus. However, it still enhances the performance\nin Ours-RWEF, which performs the best and increases the acceptance of the baseline model from 0.42 to 0.55. This validates that the properties about similarity frequency and sentence length play important roles in generating better responses.\nSpecifically, the average percentage of the generated responses that are assigned to relevance rating 2 (relevant, including the generic responses) and 3 (relevant as well as interesting) are presented in Table 4. It shows that our method achieves higher relevance score owing to generating more highquality responses with rating 3.\nTo validate that our method is effective to reduce the number of generated generic responses, we calculated the distinct-1 and distinct-2 (Li et al., 2016a) for the compared methods respectively, which are the number of distinct unigrams and bigrams divided by total number of generated words respectively. As shown in Table 5, Ours-RWEF achieves the best performance on the two metrics. This indicates that our model often outputs more meaningful and relevant responses than the other compared methods.\nWe further randomly sample another 100K queries (not used in training) and use the various models to generate responses. We compare the frequencies of several common generic responses appearing in the generated results, as shown in Table 6. It shows that our method can significantly reduce the number of generic responses. For instance, we reduce about 75% of the case “我也 不知道 (I don’t know, either.)” and 77% of the case “我也想知道 (I want to know, too)” to be generated."
  }, {
    "heading": "4 Conclusion",
    "text": "In this paper, we propose a statistical re-weighting method to weight multiple responses differently and optimize the MLE objective function. The weight of each response is calculated based on\ntwo terms according to the similarity frequency and its length. Experiments show that our approach improves the performance over the baseline models and reduces the number of generated generic responses significantly. It indicates that mismatching issue of objective function can be alleviated through such similar re-weighting methods, by which current encoder-decoder architectures can take full use of the m-to-n training corpus and model the dialogue generation tasks better."
  }],
  "year": 2018,
  "references": [{
    "title": "Smart reply: Automated response suggestion for email",
    "authors": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "László Lukács", "Marina Ganea", "Peter Young"],
    "year": 2016
  }, {
    "title": "A diversity-promoting objective function for neural conversation models",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."],
    "venue": "The North American Chapter of the Association for Computational Linguistics (NAACL).",
    "year": 2016
  }, {
    "title": "Mutual information and diverse decoding improve neural machine translation",
    "authors": ["Jiwei Li", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1601.00372.",
    "year": 2016
  }, {
    "title": "A simple, fast diverse decoding algorithm for neural generation",
    "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."],
    "venue": "arXiv preprint arXiv:1611.08562.",
    "year": 2016
  }, {
    "title": "Deep reinforcement learning for dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2016
  }, {
    "title": "Adversarial learning for neural dialogue generation",
    "authors": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."],
    "venue": "Conference on Empirical Methods in Neural Language Processing (EMNLP).",
    "year": 2017
  }, {
    "title": "Not all dialogues are created equal: instance weighing",
    "authors": ["Pierre Lison", "Serge Bibauw"],
    "year": 2017
  }, {
    "title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation",
    "authors": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."],
    "venue": "Proceedings of the International Conference on Computational Linguis-",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL).",
    "year": 2002
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C Courville", "Joelle Pineau."],
    "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "AAAI Conference on Artificial Intelli-",
    "year": 2017
  }, {
    "title": "Neural responding machine for short-text conversation",
    "authors": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2015
  }, {
    "title": "A conditional variational framework for dialog generation",
    "authors": ["Xiaoyu Shen", "Hui Su", "Yanran Li", "Wenjie Li", "Shuzi Niu", "Yang Zhao", "Akiko Aizawa", "Guoping Long."],
    "venue": "arXiv preprint arXiv:1705.00316.",
    "year": 2017
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "the North",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in Neural Information Processing Systems (NIPS).",
    "year": 2014
  }, {
    "title": "Diverse beam search: Decoding diverse solutions from neural sequence models",
    "authors": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra."],
    "venue": "arXiv preprint arXiv:1610.02424.",
    "year": 2016
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "arXiv preprint arXiv:1506.05869.",
    "year": 2015
  }, {
    "title": "Why do neural dialog systems generate short and meaningless replies? a comparison between dialog and translation",
    "authors": ["Bolin Wei", "Shuai Lu", "Lili Mou", "Hao Zhou", "Pascal Poupart", "Ge Li", "Zhi Jin."],
    "venue": "arXiv preprint arXiv:1712.02250.",
    "year": 2017
  }, {
    "title": "Neural text generation: A practical guide",
    "authors": ["Ziang Xie."],
    "venue": "arXiv preprint arXiv:1711.09534.",
    "year": 2017
  }, {
    "title": "Neural response generation via gan with an approximate embedding layer",
    "authors": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang", "Zhouran Wang", "Chao Qi."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2017
  }, {
    "title": "Seqgan: Sequence generative adversarial nets with policy gradient",
    "authors": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."],
    "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
    "year": 2017
  }, {
    "title": "Mechanism-aware neural machine for dialogue response generation",
    "authors": ["Ganbin Zhou", "Ping Luo", "Rongyu Cao", "Fen Lin", "Bo Chen", "Qing He."],
    "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
    "year": 2017
  }],
  "id": "SP:360d463449377cbc543ec0a9c4dc2b2cafc30438",
  "authors": [{
    "name": "Yahui Liu",
    "affiliations": []
  }, {
    "name": "Victoria Bi",
    "affiliations": []
  }, {
    "name": "Jun Gao",
    "affiliations": []
  }, {
    "name": "Xiaojiang Liu",
    "affiliations": []
  }, {
    "name": "Jian Yao",
    "affiliations": []
  }, {
    "name": "Shuming Shi",
    "affiliations": []
  }],
  "abstractText": "Sequence-to-sequence neural generation models have achieved promising performance on short text conversation tasks. However, they tend to generate generic/dull responses, leading to unsatisfying dialogue experience. We observe that in conversation tasks, each query could have multiple responses, which forms a 1-to-n or m-to-n relationship in the view of the total corpus. The objective function used in standard sequence-to-sequence models will be dominated by loss terms with generic patterns. Inspired by this observation, we introduce a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses.",
  "title": "Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method"
}