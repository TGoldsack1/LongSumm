{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Decentralized partially observable Markov decision processes (Dec-POMDPs) emerged as the standard framework for sequential decision making by a team of collaborative agents (Bernstein et al., 2000). A key assumption of DecPOMDPs is that agents can neither see the actual state of the system nor explicitly communicate their noisy observations with each other due to communication cost, latency or noise, hence providing a partial explanation of the double exponential growth at every control interval of the required memory in optimal algorithms (Hansen et al., 2004; Szer et al., 2005; Oliehoek et al., 2008; Amato et al., 2009; Oliehoek et al., 2013; Dibangoye et al., 2015; 2016). While planning methods for finite Dec-POMDPs made substantial progress in recent years, the formal treatment of the corresponding reinforcement learning problems received little attention so far. The literature of multi-agent reinforcement learning (MARL) can be divided into two main categories: concurrent and team approaches (Tan, 1998; Panait & Luke, 2005).\n1Univ Lyon, INSA Lyon, INRIA, CITI, F-69621 Villeurbanne, France 2INRIA / Université de Lorraine, Nancy, France. Correspondence to: Jilles S. Dibangoye <jilles.dibangoye@inria.fr>, Olivier Buffet <olivier.buffet@loria.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nPerhaps the dominant paradigm in MARL is the concurrent approach, which involves multiple simultaneous learners: typically, each agent has its learning process. Self-interested learners, for example, determine their best-response behaviors considering their opponents are part of the environment, often resulting in local optima (Brown, 1951; Hu & Wellman, 1998; Littman, 1994). While concurrent learning can apply in Dec-POMDPs, a local optimum may lead to severely suboptimal performances (Peshkin et al., 2000; Zhang & Lesser, 2011; Kraemer & Banerjee, 2016; Nguyen et al., 2017). Also, methods of this family face two conceptual issues that limit their applicability. The primary concern is that of the co-adaptation dilemma, which arises when each attempt to modify an agent behavior can ruin learned behaviors of its teammates. Another major problem is that of the multi-agent credit assignment, that is, how to split the collective reward among independent learners.\nAlternatively, the team approach involves a single learner acting on behalf of all agents to discover a collective solution (Salustowicz et al., 1998; Miconi, 2003). Interestingly, this approach circumvents the difficulties arising from both the co-adaptation and the multi-agent credit assignment. Coordinated agents, for example, simultaneously learn their control choices and the other agent strategies assuming instantaneous and free explicit communications (Guestrin et al., 2002; Kok & Vlassis, 2004). While methods of this family inherit from standard single-agent techniques, they need to circumvent two significant drawbacks: the explosion in the state space size; and the centralization of all learning resources in a single place. Recently, team algorithms ranging from Q-learning to policy-search have been introduced for finite Dec-POMDPs, but with no guaranteed global optimality (Wu et al., 2013; Liu et al., 2015; 2016; Kraemer & Banerjee, 2016). So, it seems one can either compute local optima with arbitrary bad performances or calculate optimal solutions but assuming noise-free, instantaneous and explicit communications.\nA recent approach to optimally solving Dec-POMDPs suggests recasting them into occupancy-state MDPs (oMDPs) and then applying (PO)MDP solution methods (Dibangoye et al., 2013; 2014a;b; 2016). In these oMDPs, the states called occupancy states are distributions over hidden states and joint histories of the original problem, and actions called decision rules are mappings from joint histories to con-\ntrols (Nayyar et al., 2011; Oliehoek, 2013; Dibangoye et al., 2016). This approach achieves scalability gains by exploiting the piece-wise linearity and convexity of the optimal value function. Since this methodology was successfully applied for planning in Dec-POMDPs, it is natural to wonder which benefits it could bring to the corresponding MARL problem. Unfortunately, a straightforward application of standard RL methods to oMDPs will face three severe limitations. First, occupancy states are unknown, and hence must be estimated. Second, they lie in a continuum making tabular RL methods inapplicable. Finally, the greedy maximization is computationally demanding in decentralized stochastic control problems (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010).\nThis paper extends the methodology of Dibangoye et al. to MARL, focussing on the three major issues that limit its applicability. Our primary result is the proof that, by restricting attention to plans instead of policies, a linear function over occupancy states and joint decision rules, which is simple to store and update, can capture the optimal performance for Dec-POMDPs. We further use plans instead of policies in a policy iteration algorithm, with the plan always being improved with respect to a linear function and a linear function always being driven toward the linear function for the plan. Under accurate estimation of the occupancy states, the resulting algorithm, called occupancystate SARSA (oSARSA) (Rummery, G. A. and Niranjan, 1994), is guaranteed to converge with probability one to a near-optimal plan for any finite Dec-POMDP. To extend its applicability to higher-dimensional domains, oSARSA replaces the greedy (or soft) maximization by a mixed-integer linear program for finite settings. Altogether, we obtain a MARL algorithm that can apply to finite Dec-POMDPs. Experiments show our approach can learn to act near-optimally in many finite domains from the literature.\nWe organize the remainder of this paper as follows. Section 2 extends a recent planning theory, starting with a formal definition of finite Dec-POMDPs. We proceed with the introduction of a framework for centralized MARL in Dec-POMDPs along with solutions to the three limitations mentioned above in Section 3. We further present the resulting algorithm oSARSA along with convergence guarantees in Section 4. Finally, we conduct experiments in Section 5, demonstrating our approach learns to act optimally in many finite domains from the literature. Proofs are provided in the companion technical report (Dibangoye & Buffet, 2018).\n2. Planning in Dec-POMDPs as oMDPs"
  }, {
    "heading": "2.1. Finite Dec-POMDPs",
    "text": "A finite Dec-POMDP is a tuple M .“ pn,X, tU iu, tZiu, p, r, `, γ, b0q, where n denotes the\nnumber of agents involved in the decentralized stochastic control process; X is a finite set of hidden world states, denoted x or y; U i is a finite private control set of agent i P v1;nw, where U “ U1 ˆ ¨ ¨ ¨ ˆ Un specifies the set of controls u “ pu1, . . . , unq; Zi is a finite private observation set of agent i, where Z “ Z1 ˆ ¨ ¨ ¨ ˆ Zn specifies the set of observations z “ pz1, . . . , znq; p describes a transition function with conditional density pu,zpx, yq; r is a reward model with immediate reward rpx, uq, we assume rewards are two-side bounded, i.e., for some c P R`, @x P X,u P U : |rpx, uq| ď c; ` is the planning horizon; γ P r0, 1s denotes the discount factor; and b0 is the initial belief state with density b0px0q. We shall restrict attention to finite planning horizon ` ă 8 since an infinite planning horizon solution is within a small scalar ą 0 of a finite horizon solution where ` “ rlogγpp1´ γq {cqs.\nBecause we are interested in MARL, we assume an incomplete knowledge aboutM , i.e., p and r are either unavailable or only through a generative model. Hence, the goal of solving M is to find a plan, i.e., a tuple of individual decision rules, one for each agent and time step: ρ .“ pa10:`, . . . , an0:`q. A tth individual decision rule ait : O i t ÞÑ PpU iq of agent i prescribes private controls based on the whole information available to the agent up to the tth time step, i.e., history of controls and observations oit “ pui0:t´1, zi1:tq, where oi0 “ H and oit P Oit. A tth joint decision rule, denoted at : Ot ÞÑ PpUq, can be specified as atpu|oq\n.“ śn i“1 a i tpui|oiq, where Ot\n.“ O1t ˆ ¨ ¨ ¨ ˆ Ont , oi P Oit and o .“ po1, . . . , onq P Ot. From control interval t onward, agents collectively receive discounted cumulative rewards, denoted by random variable Rt\n.“ λ1rt` ¨ ¨ ¨`λ`r`, where λt denotes the time-step dependent weighting factors, often set to λt “ γt for discounted problems. For any control interval t, joint plans a0:t of interest are those that achieve the highest performance measure Jpa0:tq\n.“ Ea0:t tR0 | b0u starting at b0, where Ea0:tt¨u denotes the expectation with respect to the probability distribution over state-action pairs joint plan a0:t induces, in particular Jpρq\n.“ Jpa0:`´1q for ρ .“ a0:`´1. One can show that, in Dec-POMDPs, there always exists a deterministic plan that is as good as any stochastic plan (see Puterman, 1994, Lemma 4.3.1). Unfortunately, there is no direct way to apply the theory developed for Markov decision processes (Bellman, 1957; Puterman, 1994) to Dec-POMDPs, including: the Bellman optimality equation; or the policy improvement theorem. To overcome these limitations, we rely on a recent theory by Dibangoye et al. that recasts M into an MDP, thereby allowing knowledge transfer from the MDP setting to Dec-POMDPs."
  }, {
    "heading": "2.2. Occupancy-State MDPs",
    "text": "To overcome the fact that agents can neither see the actual state of the system nor explicitly communicate their noisy observations with each other, Szer et al. (2005) and later\non Dibangoye et al. (2016) suggest formalizing M from the perspective of a centralized algorithm. A centralized algorithm acts on behalf of the agents by selecting a joint decision rule to be executed at each control interval based on all data available about the system, namely the information state. The information state at the end of control interval t, denoted ιt`1\n.“ pb0, a0:tq, is a sequence of joint decision rules the centralized algorithm selected starting at the initial belief state. Hence, the information state satisfies the following recursion: ι0 .“ pb0q and ιt`1 .“ pιt, atq for all control interval t, resulting in an ever-growing sequence. To generalize the value from one information state to another one, Dibangoye et al. introduced the concept of occupancy states. The occupancy state at control interval t, denoted st\n.“ Ppxt, ot|ιtq, is a distribution over hidden states and joint histories conditional on information state ιt at control interval t. Interestingly, the occupancy state has many important properties. First, it is a sufficient statistic of the information state when estimating the (current and future) reward to be gained by executing a joint decision rule:\nRpst, atq .“ ÿ\nx\nÿ\no\nstpx, oq ÿ\nu\natpu|oq ¨ rpx, uq.\nIn addition, it describes a deterministic and fully observable Markov decision process, where the next occupancy state depends only on the current occupancy state and next joint decision rule, for all y P X, o P O, u P U, z P Z:\nT pst, atq .“ st`1\nst`1py, po, u, zqq .“ atpu|oq ÿ\nx\nstpx, oq ¨ pu,zpx, yq.\nThe process the occupancy states describe is known as the occupancy-state Markov decision process (oMDP), and denoted M 1 .“ pS,A,R, T, `, γ, s0q. Similarly to POMDPs, it was proven that Dec-POMDPs can be recasted into MDPs, called oMDPs, and an optimal solution of the resulting oMDP is also an optimal solution for the original DecPOMDP (Dibangoye et al., 2016). M 1 is an `-steps deterministic and continuous MDP with respect to M , where S .“ YtPv0;`´1w St is the set of occupancy states up to control interval ` ´ 1; A .“ YtPv0;`´1w At is the set of joint decision rules up to control interval `´ 1; R is the reward model; and T is the transition rule; s0 is the initial occupancy state, which is essentially the initial belief in M ; γ and ` are as in M . It is worth noticing that there is no need to construct explicitly M 1; instead we use M (when available) as a generative model for the occupancy states T pst, atq and rewards Rpst, atq, for all control intervals t.\nTo better understand why we use plans instead of policies and how they relate, consider the MDP case. The solution of any finite MDP called a policy π : S ÞÑ A can be represented as a decision tree, where nodes are labelled with\nactions and arcs are labelled with states. Since an oMDP is also an MDP, its policies can also be represented as decision trees, except that actions are decision rules and states are occupancy states. In contrast to standard MDPs, oMDPs are deterministic. This means that only a single branch in the decision-tree representation—i.e., a sequence of actions—is necessary to act optimally in oMDPs. A single branch of a decision tree is called a plan. Hence policies are more general than plans, but in deterministic MDPs, both can be employed while achieving optimal performance (plans inducing an open-loop approach, and policies a closed-loop approach). We shall restrict attention to plans because they are more concise than policies. Below, we review a closedloop approach based on the dynamic programming theory (Bellman, 1957).\nFor any finiteM , the Bellman equation is written as follows: for all occupancy state st P St, and some fixed policy π,\nV πt pstq .“ Rpst, πpstqq ` λ1V πt`1pT pst, πpstqqq (1)\nwith boundary condition V π` p¨q .“ 0, describes the return of a particular occupancy state st when taking decision rule at “ πpstq prescribed by π. The equation for an optimal policy π˚ is referred to as the Bellman optimality equation: for any control interval t, and occupancy state st,\nV ˚t pstq .“ maxatPA Rpst, atq ` λ1V ˚t`1pT pst, atqq (2)\nwith boundary condition V ˚` p¨q .“ 0. Unfortunately, occupancy states lie in a continuum, which makes exact dynamic programming methods infeasible. Interestingly, when optimized exactly, the value function solution of (2) along with the boundary condition is always piece-wise linear and convex in the occupancy-state space (Dibangoye et al., 2016).\nLemma 1. For any arbitrary M 1, the solution V ˚0:` of (2) is convex in the occupancy-state space. If we restrict attention to deterministic policies and finite M (and corresponding M 1), the solution of (2) is piece-wise linear and convex in the occupancy-state space. Hence, the optimal value at any occupancy state st is as follows:\nV ˚t pstq .“ maxαtPΓt xst, αty, (3)\nwhere xst, αty is used to express the expectation of a linear function αt (also called α-vectorin the probability space defined by sample space X ˆO, the σ-algebra X ˆO and the probability distribution st; and Γt is the set of all tth α-vectors.\nLemma 1 shows that for any arbitrary M and corresponding M 1, the solution of (2), represented by sets Γ0:`, is convex in the occupancy-state space. Each α-vector defines the value function over a bounded region of the occupancy-state space. In addition, it is associated with a plan, defining the\noptimal plan for a bounded region of the occupancy-state space. Sets Γ0:` are iteratively improved by adding a new α-vector that dominates current ones over certain regions of the occupancy-state space. The α-vector to be added is computed using point-based Bellman backup operator H:\nrHΓt`1spstq “ arg max αat : aPAt,αt`1PΓt`1 xst, αat y,\nwhere αat px, oq .“ Etrpx, uq ` λ1αt`1py, po, u, zqq|au, for each hidden state x P X , and joint history o P O. To keep the number of α-vectors manageable, one can prune those that are dominated over the entire occupancy-state space. All in all, the oMDP reformulation permits us to solve finite M by means of M 1 using near-optimal planning methods leveraging on the special structure of the optimal value function (Shani et al., 2013). This methodology results in the current state-of-the-art algorithm to optimally solving finite Dec-POMDPs (Dibangoye et al., 2016). So it seems natural to wonder if the same methodology can also succeed when applied to the corresponding reinforcement-learning problem. In other words, how can a centralized algorithm learn to coordinate a team of agents with possibly contradicting perceptual information?\n3. Learning in Dec-POMDPs as oMDPs Using the oMDP reformulation, a natural approach to achieve centralized RL for decentralized stochastic control suggests applying exact RL methods. In the Q-learning algorithm (Watkins & Dayan, 1992), for example, one would learn directly the Q-value function when following a fixed policy π: for any control interval t P v0; `´ 1w,\nQπt pst, atq .“ Rpst, atq ` λ1V πt`1pT pst, atqq (4)\nwith boundary condition Qπ` p¨, ¨q “ 0. The policy improvement theorem provides a procedure to change a sub-optimal policy π into an improved one π̄ (Howard, 1960): for any control interval t P v0; `´ 1w,\nπ̄pstq .“ arg maxatPAt Q π t pst, atq. (5)\nUnfortunately, this approach has three severe limitations. First, the occupancy states are unknown and must be estimated. Second, even if we assume a complete knowledge of the occupancy states, they lie in a continuum, which precludes exact RL methods to accurately predict α-vectors even in the limit of infinite time and data. Finally, the greedy maximization required to improve the value function proved to be NP-hard in finite settings (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010)."
  }, {
    "heading": "3.1. Addressing Estimation Issues",
    "text": "Although mappings T and R in M 1 are unknown to either agents or a centralized algorithm, one can instead estimate\non the fly both T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq for some fixed plan ρ .“ a0:`´1 through successive interactions of agents with the environment. To this end, we shall distinguish between two settings. The first one assumes a generative model is available during the centralized learning phase, e.g. a black box simulator; and the second does not. In both cases, we build on the concept of replay pool (Mnih et al., 2015), except that we extend it from stationary single-agent domains to non-stationary multi-agent domains.\nIf a generative model is available during the learning phase, then a Monte Carlo method can approximate T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq arbitrary closely. To this end, the generative model allows the agents to sample experiences generated from M . An `-steps experience is a 4-tuple ξ .“ px0:`´1, u0:`´1, r0:`´1, z1:`q, where x0:`´1 are sampled hidden states, u0:`´1 are controls made, r0:`´1 are reward signals drawn from the reward model, and z1:` are the resulting observations, drawn from the dynamics model. If we let Dρ .“ tξrisuiPv1:Kw be the replay pool of K i.i.d random samples created through successive interactions with the generative model, then empirical occupancy state ŝt « T ps0, a0:t´1q and reward R̂t « RpT ps0, a0:t´1q, atq corresponding to the current Dρ are given by: for any control interval t P v0 : `´ 1w,\nŝtpx, oq .“ 1K řK i“1δxpx ris t q ¨ δopu ris 0:t, z ris 1:tq (6)\nand R̂t .“ 1K řK i“1r ris t , (7)\nwhere δxp¨q and δop¨q denote the delta-Dirac mass located in hidden state and joint history pair, respectively. By the law of large numbers the sequence of averages of these estimates converges to their expected values, and the standarddeviation of its error falls as 1{ ? K (Sutton & Barto, 1998, chapter 5). The error introduced by Monte Carlo when estimating T pŝt´1, at´1q instead of T ps0, a0:t´1q is upper bounded by 2`{ ? K. The proof follows from the performance guarantee of the policy-search algorithm by Bagnell et al. (2004). Hence, to ensure the learned value function is within ą 0 of the optimal one, one should set the replaypool size to K “ Θp4 ` 2 2 q.\nWhen no generative model is available, the best we can do is to store samples agents collected during the learning phase into replay pools Dρ, one experience for each episode within the limit size of K. We maintain only the K recent experiences, and may discard1 hidden states since they are unnecessary for the updates of future replay pools and the performance measure. The rationale behind this approach is that it achieves the same performances as a Monte Carlo method for the task of approximating T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq given a fixed plan ρ\n.“ a0:`´1. In fact, if we let Dρ be a replay pool of K i.i.d. samples\n1Note that one should keep hidden states when available since they often speed up the convergence.\ngenerated according to ρ, the empirical occupancy state ŝt « T ps0, a0:t´1q and reward R̂t « RpT ps0, a0:t´1q, atq corresponding to Dρ are given by (6) and (7), respectively. One can further show this approach preserves performance guarantees similar to those obtained when using a generative model."
  }, {
    "heading": "3.2. Addressing Prediction Issues",
    "text": "The key issue with large spaces of occupancy states and decision rules is that of generalization, that is, how experiences with a limited subset of occupancy states and decision rules can produce a good approximation over a much larger space. Fortunately, a fundamental property of oMDPs is the convexity of the optimal value function over the occupancystate space, see Lemma 1. Building on this property, we demonstrate a simple yet important preliminary result before stating the main result of this section.\nLemma 2. For any arbitrary M 1 (resp. M ), the optimal Qvalue function is the upper envelope of sets Ω˚0:` of α-vectors over occupancy states and joint decision rules: for any control interval t, Q˚t pst, atq “ maxqtPΩ˚t xsτ d aτ , qty, where qt P Ω˚t are appropriate α-vectors, and sτ d aτ denotes the Hadamard product2.\nLemma 2 generalizes the convexity property demonstrated in Lemma 1 from optimal value functions over occupancy states to optimal value functions over occupancy states and decision rules. As a consequence, finite sets Ω˚0:`´1 of αvectors can produce solutions arbitrarily close to the optimal Q-value function Q˚0:`´1. Though Q-value function Q ˚ 0:`´1 generalizes from a pair of occupancy state and decision rule to another one, storing and updating a convex hull is non trivial. Instead of learning the optimal Q-value function over all occupancy states and decision rules, we explore a simpler yet tractable alternative, which will prove sufficient to preserve ability to eventually find an optimal plan starting at initial occupancy state s0. Theorem 1. For any arbitrary M 1 (resp. M ), the Q-value functionQρ ˚\n0:`´1 under an optimal plan ρ ˚ .“ a˚0:`´1 starting\nat initial occupancy state s0 is linear in occupancy states and decision rules: Qρ ˚ t pst, atq “ xst d at, q ρ˚\nt y where qρ ˚\nt .“ arg maxqtPΩ˚t xT ps0, a ˚ 0:t´1q d a˚t , qty .\nTheorem 1 proves that the Q-function for a given optimal joint plan achieves performance at the initial occupancy state s0 as good as the Q-value function for an optimal joint policy. Standard policy iteration algorithms search for an optimal joint policy, which requires a finite set of αvectors to approximate V ˚/Q˚, hence the resulting PWLC approximator is tight almost everywhere. Building upon Theorem 1, we search for an optimal ρ, which requires\n2@px, o, uq : rsτ d aτ spx, o, uq .“ sτ px, oq ¨ aτ pu|oq.\nonly a single α-vector to approximate V ρ{Qρ, thus the resulting linear approximator is loose everywhere except in the neighborhood of a few points. The former approach may require less iterations before convergence to an optimal joint policy, but the computational cost of each iteration shall increase with the number of α-vectors maintained. The latter approach may require much more iterations, but all iteration shares the same computational cost."
  }, {
    "heading": "3.3. Addressing Plan Improvement Issues",
    "text": "A fundamental theorem in many RL algorithms is the policy improvement theorem, which helps improving policies over time until convergence. This section introduces a procedure to improve a plan starting with a sub-optimal one.\nSuppose we have determined the value function V ρ0:`´1 for any arbitrary ρ .“ a0:`´1. For some control interval t P v0; ` ´ 1w, we would like to know whether or not we should change decision rules a0:t to choose ā0:t ‰ a0:t. We know how good it is to follow the current plan from control interval t onward—that is V ρt —but would it be better or worse to change to the new plan? One way to answer this question is to consider selecting ā0:t at control interval t and thereafter following decision rules at`1:`´1 of the existing ρ. The value of the resulting joint plan is given by Jpā0:t´1q ` λ1V ρt`1pT ps0, ā0:t´1qq. The key criterion is whether this quantity is greater or less than Jpρq. Next, we state the plan improvement theorem for oMDPs. Theorem 2. Let ρ .“ a0:`´1 and ρ̄ .“ ā0:`´1 be any pair of plans and J0:` be a sequence of α-vectors such that, for all t, Jtpxt, otq\n.“ Etα0r0 ` . . .` αtrt|b0, xt, ot, a0:t´1u. Let s̄t .“ T ps0, ā0:t´1q and st .“ T ps0, a0:t´1q be occupancy states at any control interval t P v0; ` ´ 1w under ρ̄ and ρ, respectively. Then, xā0:t˚´1, at˚:`´1y such that t˚ “ arg maxtPv0;`´1w xs̄t ´ st, Jt ´ λ1V ρ t y is as good as, or better than, ρ.\nProof. The proof follows from the difference between the performance measure of ρ .“ a0:`´1 and ρ̄\n.“ ā0:`´1. Let ςtpρ̄, ρq be the advantage of taking plan xā0:t´1, at:`´1y instead of ρ: for any control interval t P v0; `´ 1w,\nςtpρ̄, ρq “ Jpā0:t´1q ` λ1V ρt pT ps0, ā0:t´1qq ´ Jpρq “ Jpā0:t´1q ´ Jpa0:t´1q ` λ1pV ρt ps̄tq ´ V ρ t pstqq\n“ xs̄t ´ st, Jt ´ λ1V ρt y.\nIf we let t˚ .“ arg maxt“0,1,...,`´1 ςtpρ̄, ρq, then plan xā0:t˚´1, at˚:`´1y achieves the highest advantage among plan set txā0:t´1, at:`´1yutPv0;`´1w constructed based on ρ̄. If t˚ “ 0, then xā0:t˚´1, at˚:`´1y “ ρ, and no improved plans were found from plan set generated from ρ̄. Otherwise, new xā0:t˚´1, at˚:`´1y must be better than ρ.\nTheorem 2 plays the same role in the plan space as does\nthe policy improvement theorem in the policy space. More precisely, after sampling a plan, i.e., a sequence of decision rules, it tells us which of these decision rules will improve the current plan. More specifically, it shows how, given ρ .“ a0:`´1 and α-vector qρ0:`´1, we can easily evaluate a change in ρ at any control interval to a particular (possibly improved) plan. To ease exploration towards promising plans, we investigate the -greedy maximization (or softmaximization). At each control interval t and occupancy state st, it randomly selects ât with probability ; otherwise, it greedily selects ât w.r.t. the current Q-value function Q ρ t :\nât .“ arg maxat : a1tPA1t ,...,ant PAnt Q ρ t pst, atq,\nwhere ρ̂ .“ â0:`´1. Unfortunately, this operation proved to be NP-hard for finite M (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010). Searching for the best decision rule requires enumerating all of them, which is not possible in large planning horizons. Instead, we present a mixed-integer linear programming (MILP) method, which successfully performs the greedy maximization for finite M . Though MILP is NP-hard in the worst case, the solution of its LP relaxation, which is in P, is often integral in our experiments. In other words, the solution of the LP relaxation is already a solution of the MILP. A similar observation was done before by MacDermed & Isbell. Mixed-Integer Linear Program 1 builds on (MacDermed & Isbell, 2013), which introduced an integer program for the greedy maximization in finite M . We also exploit the occupancy state estimation, in which ŝt replaces st, and the current α-vector q ρ t . Mixed-Integer Linear Program 1 (For finite M ).\nMaximize at,a1t ,...,a n t\nř\nx\nř oŝtpx, oq ÿ\nu\natpu|oq ¨ qρt po, uq (8)\ns.t.: ř ujatpu j , ui|oq “ aitpui|oiq, @i, ui, o (9)\nř\nuatpu|oq “ 1, @o (10)\nwhere tatpu|oqu and taitpui|oiqu are positive and boolean variables, respectively.\nMixed-Integer Linear program 1 optimizes positive variables tatpu|oquuPU,oPOt , one positive variable for each control-history pair. More precisely, each variable represents the probability atpu|oq of control u being taken given that agents experienced joint history o. Constraints must be imposed on these variables to ensure they form proper probability distributions (10), and that they result from the product of independent probability distributions (9), one independent probability distribution for each agent. In order to make the description of the conditional independence,\natpu|oq “ a1t pu1|o1q ˆ ¨ ¨ ¨ ˆ ant pun|onq, (11)\nwe use additional variables taitpui|oiquiPv1;nw,uiPUi,oiPOit . Marginalizing out both sides of (11) over all control-history\npairs of all agents except agent i, denoted ´i, leads to (9). That is not sufficient to ensure conditional independence in general. If we further constrain taitpui|oiqu to be boolean, then system of equations (9) implies (11). Given (9) and (10), agent variables taitpui|oiquuiPUi,oiPOit describe a proper probability distribution, so we omit corresponding constraints. Our greedy maximization approach is fundamentally different from previous ones, including the integer program by (MacDermed & Isbell, 2013) and the constraint optimization program by (Kumar & Zilberstein, 2009; Dibangoye et al., 2016). First, while previous approaches made use of boolean variables, we use both positive and boolean variables instead. Next, prior approaches optimize a value function represented as a convex hull; we optimize an α-vector instead."
  }, {
    "heading": "4. The oSARSA Algorithm",
    "text": "This section presents the oSARSA algorithm with tabular representations and function approximations (using either linear functions or deep neural networks) along with convergence guarantees. oSARSA algorithms are specializations of Policy Iteration, except that we use plans instead of policies. For the sake of conciseness, we describe a generic algorithm, which can fit to either tabular or approximate representations.\nIn Dec-POMDPs, the goal of oSARSA is to learn q˚0:`´1, a sequence of α-vectors of an optimal plan ρ˚. In particular, we must estimate qtpx, o, uq for the current plan ρ and for all reachable state x, joint history o, control u, and any control interval t. At the same time, the algorithm changes ρ towards improved plans according to the plan improvement theorem. The improved plans are constructed by exploring the occupancy-state space according to -greedy plans (see Section 3.3). To provide good estimations, we store all experiences in data set Dρ, from which we estimate the occupancy states and returns under ρ for any control interval (see Section 3.1). Upon estimating occupancy state ŝ and selecting joint decision rule a, we update parametrized αvector qt with parameter θt using qt`1, Dρ and at`1 by means of temporal difference learning: for all px, o, uq,\nθ rτ`1s t .“ θrτst ` βτEŝ,a,D,at`1tδt∇q rτs t px, o, uqu (12)\nδt “ r ` λ1qrτst`1py, o1, u1q ´ q rτs t px, o, uq,\nwhere βτ is a step size, and quantity ∇qtpx, o, uq denotes the gradient of qt at px, o, uq w.r.t. some parameter θt. Using tabular representations (e.g., finite/small M ), θt “ qt and thus ∇qtpx, o, uq is a unit vector ex,o,u whose value at px, o, uq is one and zero otherwise. Using linear function approximations (e.g., continuous/large M ), qtpx, o, uq\n.“ φtpx, o, uqJθt, where ∇qtpx, o, uq “ φtpx, o, uq is the feature vector at px, o, uq. Algorithm 1 shows the pseudocode of oSARSA.\nAlgorithm 1 The oSARSA Algorithm Initialize ḡ “ ´8, ρ̄ and q0:`´1 arbitrary, and Dρ̄. while q0:`´1 has not converged do\nSelect -greedily ρ w.r.t. q0:`´1 and Dρ̄. Compose Dρ with N trajectories tξrτsuNτ“1. Estimate pg, ςq from r\nř`´1 t“0 R̂t|Dρ, ŝ0 “ s0s.\nIf g ´ ς ě ḡ then pρ̄, ḡ,Dρ̄q “ pρ, g ` ς,Dρq. Update α-vectors q0:`´1 as described in (12).\nend while\nTo establish the convergence of oSARSA, we introduce the following assumptions.\nTheorem 3. Consider assumptions: (1) The stepsizes tβτuτ“1,2,... satisfy Robbins & Monro’s conditions; (2) The occupancy states ŝ0:`´1 and immediate returns R̂0:`´1 are accurately estimated; and (3) Every pair of reachable occupancy state and joint decision rule is visited infinitely often. Under these assumptions, the sequence qrτs0:`´1 generated by oSARSA converges with probability 1 to q˚0:`´1.\nProof. Under these assumptions, we define Hρ that maps a sequence of α-vectors q0:`´1 to a new sequence of α-vectors Hρq0:`´1 according to the formula: for all hidden state x, joint history o and control u, at control interval t,\npHρq0:`´1qpx, o, uq “ rpx, uq ` λ1Etvt`1py, o‘ pu, zqqu,\nwhere vtpx, oq .“ qtpx, o, ρpoqq and ρpoq is the control prescribed by ρ at joint history o. Then, the plan evaluation step of the oSARSA algorithm is of the form\nq rτ`1s t px, o, uq “ p1´ βtqq rτs t px, o, uq ` βtκ rτs t px, o, uq,\nκ rτs t px, o, uq “ pHρq rτs 0:`´1qpx, o, uq ` wtpx, o, uq,\nwhere wtpx, o, uq “ rpx, uq ` λ1vrτst`1py, o ‘ pu, zqq ´ pHρqrτs0:`´1qpx, o, uq is a zero mean noise term. Using this temporal-difference update-rule, see (12), we converge with probability 1 to qρ0:`´1. It now remains to be verified that the plan improvement step of the oSARSA algorithm changes the current plan for an improved one. Initially, ḡ is arbitrarily bad, so any new plan is an improved one. Then, ḡ “ Jpρq for the current best plan ρ since occupancy state and return are accurately estimated. Hence, when ever g ě ḡ, we know that the new plan ρ̄ yields a performance measure Jpρ̄q superior to Jpρq, thus ρ̄ improves ρ. We conclude the proof noticing that in finite M , the number of deterministic plans is finite. As a consequence, by visiting infinitely often every pair of occupancy state and decision rule we are guaranteed to visit all deterministic plans, hence an optimal one.\nIt is now important to observe that we meet assumption (2) in Theorem 3 only when M is available. Otherwise, we rely\non confidence bounds rg ´ ς, g ` ςs, e.g. Hoeffding’s inequality, on estimate g « Jpρq. In particular, we use lowerbounds g ´ ς on sample means instead of the sample means g themselves, to limit situations where g is overestimated. Small data sets often lead to suboptimal solutions, but as the number of experiences in data set Dρ increases, sample means and corresponding lower bounds get close to the mean, i.e., ς tends to 0. We require an accurate estimation of a plan’s performance to know for sure its performance is above that of any other plans we may encounter. However, an estimation of a sample plan’s performance can be refined over time until it becomes accurate. For example, if the algorithm samples a promising plan—i.e., its confidence bounds suggest it might achieve a better performance than that of the current plan—the algorithm can progressively refine it until it becomes accurate. Of course, having a good initial estimate can significantly speed up the convergence. In the extreme case, the initial estimate is the true value. It is also worth noticing that the memory complexity of the oSARSA algorithms is linear with the size of an α-vector, i.e., Θp|Dρ|q; and its time complexity is linear with the episodes."
  }, {
    "heading": "5. Experiments",
    "text": "We ran the oSARSA algorithm on a Mac OSX machine with 3.8GHz Core i5 and 8GB of available RAM. We solved the MILPs using ILOG CPLEX Optimization Studio. We define features to use sequences ofK last joint observations instead of joint histories, hence the dimension of the parameter vector θ is |X|p|U ||Z|qK for finite M .\nWe evaluate our algorithm on multiple 2-agent benchmarks from the literature all available at masplan.org: Mabc, Recycling, Gridsmall, Grid3x3corners, Boxpushing, and Tiger. These are the largest and the most challenging benchmarks from the Dec-POMDP literature. For each of them, we compare our algorithm to the state-of-the-art algorithms based on either a complete or a generative model: FBHSVI (Dibangoye et al., 2016), RLaR (Kraemer & Banerjee, 2016), and MCEM (Wu et al., 2013). We also reported results of the state-of-the-art model-free solver: (distributed) REINFORCE (Peshkin et al., 2000). For REINFORCE and oSARSA, we used hyper-parameters and β ranging from 1 to 10´3 with a decaying factor of 104, sample size |D| “ 104. We use maximum episodes and time limit 105 and 5 hours, respectively, as our stopping criteria.\nSurprisingly, REINFORCE performs very well on domains that consist of weakly coupled agents, see Figure 1. However, for domains with strongly coupled agents, e.g., Tiger or BoxPushing, it often gets stuck at some local optima. In contrast, oSARSA converges to near-optimal solutions when enough resources are available over all domains, see Figure 1 and Table 1. Regarding the most challenging benchmarks, which require more resources, oSARSA stops before\nthe convergence to a near-optimal solution; yet, it often outperforms the other RL algorithms. RLaR can achieve near-optimal result for small domains and short planning horizon (` ď 5q, assuming there exists a unique optimal plan. As for MCEM, it can solve infinite horizon problems, but similarly to REINFORCE may get stuck in local optima; this is essentially as they both use a form of gradient descent in a parametrized policy space."
  }, {
    "heading": "3 5.19 N.A. 5.0 5.19 5.19",
    "text": ""
  }, {
    "heading": "4 4.46 N.A. 4.6 4.80 4.80",
    "text": ""
  }, {
    "heading": "5 6.65 N.A. 2.2 6.99 7.02",
    "text": ""
  }, {
    "heading": "6 – N.A. 0.3 2.34 10.38",
    "text": ""
  }, {
    "heading": "7 – N.A. -1.7 2.25 9.99",
    "text": ""
  }, {
    "heading": "8 N.A. -10 -19.9 -0.2 13.44",
    "text": ""
  }, {
    "heading": "6 – N.A. 1.46 1.49 1.49",
    "text": ""
  }, {
    "heading": "7 – N.A. 2.17 2.19 2.19",
    "text": ""
  }, {
    "heading": "8 – N.A. 2.96 2.95 2.96",
    "text": ""
  }, {
    "heading": "9 – N.A. 3.80 3.80 3.80",
    "text": ""
  }, {
    "heading": "10 – N.A. 4.66 4.69 4.68",
    "text": ""
  }, {
    "heading": "3 66.08 N.A. 17.6 65.27 66.08",
    "text": ""
  }, {
    "heading": "4 98.59 N.A. 18.1 98.16 98.59",
    "text": ""
  }, {
    "heading": "5 – N.A. 35.2 107.64 107.72",
    "text": ""
  }, {
    "heading": "6 – N.A. 36.4 120.26 120.67",
    "text": ""
  }, {
    "heading": "7 – N.A. 36.4 155.21 156.42",
    "text": ""
  }, {
    "heading": "8 – N.A. 52.9 186.04 191.22",
    "text": ""
  }, {
    "heading": "9 – N.A. 54.5 206.75 210.27",
    "text": ""
  }, {
    "heading": "10 – N.A. 54.7 218.39 223.74",
    "text": ""
  }, {
    "heading": "8 N.A. 59.1 58.9 144.57 224.43",
    "text": ""
  }, {
    "heading": "6. Discussion",
    "text": "This paper extends a recent but growing (deep) MARL paradigm (Szer et al., 2005; Dibangoye et al., 2016; Kraemer & Banerjee, 2016; Mordatch & Abbeel, 2017; Foerster et al., 2017), namely RL for decentralized control, from modelbased to model-free settings. This paradigm allows a centralized algorithm to learn on behalf of all agents how to select an optimal joint decision rule to be executed at each control interval based on all data available about the system during a learning phase, while still preserving ability for each agent to act based solely on its private histories at the execution phase. In particular, we introduced tabular and approximate oSARSA algorithms, which demonstrated promising results often outperforming state-of-the-art MARL approaches for Dec-POMDPs. To do so, oSARSA learns a value function that maps pairs of occupancy state and joint decision rule to reals. To ease the generalization in such high-dimensional continuous spaces, we restrict attention to plans rather than policies, which in turn restricts value functions of interest to linear functions. To speed up the greedy maximization, we used a MILP for finite settings—we shall use a gradient approach instead of a MILP for continuous settings in future works. Finally, we present a proof of optimality for a MARL algorithm when the estimation error is neglected. We shall investigate an approach to relax this somewhat restrictive assumption, perhaps within the probably approximately correct learning framework.\nThe RL for decentralized control paradigm is significantly different from the standard RL paradigm, in which agents have the same amount of information during both the learning and the execution phases. Another major difference lies in the fact that learned value functions in standard (deep) RL algorithms are mapping from histories (or states) to reals. In contrast, oSARSA learns a value function that maps occupancy-state/decision-rule pairs to reals—spaces of occupancy states and joint decision rules are multiple orders of magnitude larger than history or state spaces. As a consequence, standard (MA)RL methods, e.g. REINFORCE and MCEM, and recent actor-critic methods (Bono et al., 2018), may converge towards a local optimum faster than oSARSA, but the latter often converges towards a nearoptimal solution. oSARSA uses occupancy states instead of joint histories mainly because occupancy states are (so far minimal) sufficient statistics for optimal decision-making in Dec-POMDPs—using joint histories instead of occupancy states may lead to suboptimal solutions except in quite restrictive settings. For example, RLaR learns value functions mapping history/action pairs to reals, but convergence towards an optimal solution is guaranteed only for domains that admit a unique optimal joint plan—which essentially restricts to POMDPs (Kraemer & Banerjee, 2016)."
  }],
  "year": 2018,
  "references": [{
    "title": "Incremental Policy Generation for Finite-Horizon DEC-POMDPs",
    "authors": ["C. Amato", "J.S. Dibangoye", "S. Zilberstein"],
    "venue": "In ICAPS,",
    "year": 2009
  }, {
    "title": "Policy Search by Dynamic Programming",
    "authors": ["J.A. Bagnell", "S.M. Kakade", "J.G. Schneider", "A.Y. Ng"],
    "venue": "In NIPS",
    "year": 2004
  }, {
    "title": "The Complexity of Decentralized Control of Markov Decision Processes",
    "authors": ["D.S. Bernstein", "S. Zilberstein", "N. Immerman"],
    "venue": "In UAI,",
    "year": 2000
  }, {
    "title": "On the Study of Cooperative Multi-Agent Policy Gradient",
    "authors": ["G. Bono", "J.S. Dibangoye", "L. Matignon", "F. Pereyron", "O. Simonin"],
    "venue": "Research Report RR-9188,",
    "year": 2018
  }, {
    "title": "Iterative Solutions of Games by Fictitious Play",
    "authors": ["G.W. Brown"],
    "venue": "In Activity Analysis of Production and Allocation",
    "year": 1951
  }, {
    "title": "Learning to Act in Decentralized Partially Observable MDPs. Research report, INRIA Grenoble - Rhone-Alpes ",
    "authors": ["J.S. Dibangoye", "O. Buffet"],
    "venue": "CHROMA Team ; INRIA Nancy, équipe LARSEN,",
    "year": 2018
  }, {
    "title": "Pointbased incremental pruning heuristic for solving finitehorizon DEC-POMDPs",
    "authors": ["J.S. Dibangoye", "Mouaddib", "A.-I", "B. Chaib-draa"],
    "venue": "In AAMAS, pp",
    "year": 2009
  }, {
    "title": "Optimally Solving Dec-POMDPs As Continuous-state MDPs",
    "authors": ["J.S. Dibangoye", "C. Amato", "O. Buffet", "F. Charpillet"],
    "venue": "In IJCAI, pp",
    "year": 2013
  }, {
    "title": "Optimally solving Dec-POMDPs as Continuous-State MDPs: Theory and Algorithms",
    "authors": ["J.S. Dibangoye", "C. Amato", "O. Buffet", "F. Charpillet"],
    "venue": "Research Report RR8517,",
    "year": 2014
  }, {
    "title": "ErrorBounded Approximations for Infinite-Horizon Discounted Decentralized POMDPs",
    "authors": ["J.S. Dibangoye", "O. Buffet", "F. Charpillet"],
    "venue": "In ECML, pp",
    "year": 2014
  }, {
    "title": "Structural Results for Cooperative Decentralized Control Models",
    "authors": ["J.S. Dibangoye", "O. Buffet", "O. Simonin"],
    "venue": "In IJCAI, pp",
    "year": 2015
  }, {
    "title": "Optimally Solving Dec-POMDPs as Continuous-State MDPs",
    "authors": ["J.S. Dibangoye", "C. Amato", "O. Buffet", "F. Charpillet"],
    "venue": "Journal of AI Research,",
    "year": 2016
  }, {
    "title": "Counterfactual Multi-Agent Policy Gradients",
    "authors": ["J.N. Foerster", "G. Farquhar", "T. Afouras", "N. Nardelli", "S. Whiteson"],
    "year": 2017
  }, {
    "title": "Coordinated Reinforcement Learning",
    "authors": ["C. Guestrin", "M.G. Lagoudakis", "R. Parr"],
    "year": 2002
  }, {
    "title": "Dynamic Programming for Partially Observable Stochastic Games",
    "authors": ["E.A. Hansen", "D.S. Bernstein", "S. Zilberstein"],
    "venue": "In AAAI,",
    "year": 2004
  }, {
    "title": "Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm",
    "authors": ["J. Hu", "M.P. Wellman"],
    "year": 1998
  }, {
    "title": "Sparse Cooperative Q-learning",
    "authors": ["J.R. Kok", "N. Vlassis"],
    "venue": "In ICML,",
    "year": 2004
  }, {
    "title": "Multi-agent reinforcement learning as a rehearsal for decentralized planning",
    "authors": ["L. Kraemer", "B. Banerjee"],
    "year": 2016
  }, {
    "title": "Constraint-based dynamic programming for decentralized POMDPs with structured interactions",
    "authors": ["A. Kumar", "S. Zilberstein"],
    "venue": "In AAMAS,",
    "year": 2009
  }, {
    "title": "Markov games as a framework for multiagent reinforcement learning",
    "authors": ["M.L. Littman"],
    "venue": "In ICML,",
    "year": 1994
  }, {
    "title": "Stickbreaking policy learning in Dec-POMDPs",
    "authors": ["M. Liu", "C. Amato", "X. Liao", "L. Carin", "J.P. How"],
    "venue": "In IJCAI. AAAI,",
    "year": 2015
  }, {
    "title": "Learning for Decentralized Control of Multiagent Systems in Large, Partially-Observable Stochastic Environments",
    "authors": ["M. Liu", "C. Amato", "E.P. Anesta", "J.D. Griffith", "J.P. How"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs",
    "authors": ["L.C. MacDermed", "C. Isbell"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "When Evolving Populations is Better Than Coevolving Individuals: The Blind Mice Problem",
    "authors": ["T. Miconi"],
    "venue": "In IJCAI,",
    "year": 2003
  }, {
    "title": "Emergence of Grounded Compositional Language in Multi-Agent Populations",
    "authors": ["I. Mordatch", "P. Abbeel"],
    "year": 2017
  }, {
    "title": "Optimal Control Strategies in Delayed Sharing Information Structures",
    "authors": ["A. Nayyar", "A. Mahajan", "D. Teneketzis"],
    "venue": "Automatic Control, IEEE Transactions on,",
    "year": 2011
  }, {
    "title": "Policy gradient with value function approximation for collective multiagent planning",
    "authors": ["D.T. Nguyen", "A. Kumar", "H.C. Lau"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Sufficient Plan-Time Statistics for Decentralized POMDPs",
    "authors": ["F.A. Oliehoek"],
    "venue": "In IJCAI,",
    "year": 2013
  }, {
    "title": "Optimal and Approximate Q-value Functions for Decentralized POMDPs",
    "authors": ["F.A. Oliehoek", "M.T.J. Spaan", "N.A. Vlassis"],
    "venue": "Journal of AI Research,",
    "year": 2008
  }, {
    "title": "Heuristic search for identical payoff Bayesian games",
    "authors": ["F.A. Oliehoek", "M.T.J. Spaan", "J.S. Dibangoye", "C. Amato"],
    "venue": "In AAMAS,",
    "year": 2010
  }, {
    "title": "Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs",
    "authors": ["F.A. Oliehoek", "M.T.J. Spaan", "C. Amato", "S. Whiteson"],
    "venue": "Journal of AI Research,",
    "year": 2013
  }, {
    "title": "Cooperative multi-agent learning: The state of the art",
    "authors": ["L. Panait", "S. Luke"],
    "venue": "Autonomous Agents and Multi-Agent Systems,",
    "year": 2005
  }, {
    "title": "Learning to Cooperate via Policy Search",
    "authors": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"],
    "venue": "In UAI,",
    "year": 2000
  }, {
    "title": "Team Decision Problems",
    "authors": ["R. Radner"],
    "venue": "Ann. Math. Statist.,",
    "year": 1962
  }, {
    "title": "A stochastic approximation method",
    "authors": ["H. Robbins", "S. Monro"],
    "venue": "The annals of mathematical statistics,",
    "year": 1951
  }, {
    "title": "On-line Q-learning using connectionist systems",
    "authors": ["G.A. Rummery", "M. Niranjan"],
    "venue": "Technical report,",
    "year": 1994
  }, {
    "title": "Learning Team Strategies",
    "authors": ["R. Salustowicz", "M. Wiering", "J. Schmidhuber"],
    "venue": "Soccer Case Studies. ML,",
    "year": 1998
  }, {
    "title": "A survey of pointbased POMDP solvers",
    "authors": ["G. Shani", "J. Pineau", "R. Kaplow"],
    "venue": "Journal of Autonomous Agents and Multi-Agent Systems,",
    "year": 2013
  }, {
    "title": "Introduction to Reinforcement Learning",
    "authors": ["R.S. Sutton", "A.G. Barto"],
    "year": 1998
  }, {
    "title": "A Heuristic Search Algorithm for Solving Decentralized POMDPs",
    "authors": ["D. Szer", "F. Charpillet", "Zilberstein", "S. MAA"],
    "venue": "In UAI,",
    "year": 2005
  }, {
    "title": "Multi-agent Reinforcement Learning: Independent vs. Cooperative Agents",
    "authors": ["M. Tan"],
    "venue": "In Readings in Agents. San Francisco, CA,",
    "year": 1998
  }, {
    "title": "Monte-Carlo Expectation Maximization for Decentralized POMDPs",
    "authors": ["F. Wu", "S. Zilberstein", "N.R. Jennings"],
    "venue": "In IJCAI,",
    "year": 2013
  }, {
    "title": "Coordinated Multi-Agent Reinforcement Learning in Networked Distributed POMDPs",
    "authors": ["C. Zhang", "V. Lesser"],
    "year": 2011
  }],
  "id": "SP:7348935ef56075270536ae14dd4cf94d4aab83f5",
  "authors": [{
    "name": "Jilles S. Dibangoye",
    "affiliations": []
  }, {
    "name": "Olivier Buffet",
    "affiliations": []
  }],
  "abstractText": "We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized policy iteration, which at best led to local optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under certain conditions, the first near-optimal cooperative multi-agent reinforcement learning algorithm. To achieve significant scalability gains, we replace the greedy maximization by mixed-integer linear programming. Experiments show our approach can learn to act near-optimally in many finite domains from the literature.",
  "title": "Learning to Act in Decentralized Partially Observable MDPs"
}