{
  "sections": [{
    "heading": "1 Introduction",
    "text": "A fundamental problem in the theory of deep learning is to study the functional space of deep neural networks. A network can be modeled as a composition of elementary maps, however the family of all functions that can be obtained in this way is extremely complex. Many recent papers paint an accurate picture for the case of shallow networks (e.g., using mean field theory [7, 27]) and of deep linear networks [2, 3, 21], however a similar investigation of deep nonlinear networks appears to be significantly more challenging, and require very different tools.\nIn this paper, we consider a general model for deep polynomial neural networks, where the activation function is a polynomial (r-th power) exponentiation. The advantage of this framework is that the functional space associated with a network architecture is algebraic, so we can use tools from algebraic geometry [17] for a precise investigation of deep neural networks. Indeed, for a fixed activation degree r and architecture d = (d0, . . . , dh) (expressed as a sequence of widths), the family of all networks with varying weights can be identified with an algebraic variety Vd,r, embedded in a finite-dimensional Euclidean space. In this setting, an algebraic variety can be thought of as a manifold that may have singularities.\nIn this paper, our main object of study is the dimension of Vd,r as a variety (in practice, as a manifold), which may be regarded as a precise measure of the architecture’s expressiveness. Specifically, we prove that this dimension stabilizes when activations are high degree, and we provide an exact dimension formula for this case (Theorem 14). We also investigate conditions under which Vd,r fills its ambient space. This question is important from the vantage point of optimization, since an architecture is “filling” if and only if it corresponds to a convex functional space (Proposition 6). In\n⇤Equal contribution.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nthis direction, we prove a bottleneck property, that if a width is not sufficiently large, the network can never fill the ambient space regardless of the size of other layers (Theorem 19).\nIn a broader sense, our work introduces a powerful language and suite of mathematical tools for studying the geometry of network architectures. Although this setting requires polynomial activations, it may be used as a testing ground for more general situations and, e.g., to verify rules of thumb rigorously. Finally, our results show that polynomial neural networks are intimately related to the theory of tensor decompositions [22]. In fact, representing a polynomial as a deep network corresponds to a type of decomposition of tensors which may be viewed as a composition of decompositions of a recently introduced sort [24]. Using this connection, we establish general non-trivial upper bounds on filling widths (Theorem 10). We believe that our work can serve as a first step towards many interesting research challenges in developing the theoretical underpinnings of deep learning."
  }, {
    "heading": "1.1 Related work",
    "text": "The study of the expressive power of neural networks dates back to seminal work on the universality of networks as function approximators [10, 19]. More recently, there has been research supporting the hypothesis of “depth efficiency”, i.e., the fact that deep networks can approximate functions more efficiently than shallow networks [11, 25, 8, 9]. In contrast to this line of work, we study the class of functions that can be expressed exactly using a network. Our analysis may of course be used to investigate the problem of approximation, however this is not the focus of this paper.\nMost of the aforementioned studies make strong hypotheses on the network architecture. In particular, [11, 25] focus on arithmetic circuits, or sum-product networks [29]. These are networks composed of units that compute either the product or a weighted sum of their inputs. In [8], the authors introduce a model of convolutional arithmetic circuits. This is a particular class of arithmetic circuits that includes networks with layers of 1D convolutions and product pooling. This model does not allow for non-linear activations (beside the product pooling), although the follow-up paper [9] extends some results to ReLU activations with sum pooling. Interestingly, these networks are related to Hierarchical Tucker (HT) decomposition of tensors.\nThe polynomial networks studied in this paper are not arithmetic circuits, but feedforward deep networks with polynomial r-th power activations. This is a vast generalization of a setting considered in several recent papers [33, 14, 31], that study shallow (two layer) networks with quadratic activations (r = 2). These papers show that if the width of the intermediate layer is at least twice the input dimension, then the quadratic loss has no “bad” local minima. This result in line with our Proposition 5, which explains in this case the functional space is convex and fills the ambient space. We also point out that polynomial activations are required for the functional space of the network to span a finite dimensional vector space [23, 33].\nThe polynomial networks considered in this paper do not correspond to HT tensor decompositions as in [8, 9], rather they are related to a different polynomial/tensor decomposition attracting very recent interest [16, 24]. These generalize usual decompositions, however their algorithmic and theoretical understanding are, mostly, wide open. Neural networks motivate several questions in this vein.\nFinally, we mention other recent works that study neural networks from the perspective of algebraic geometry [26, 32, 20].\nMain contributions. Our main contributions can be summarized as follows.\n• We give a precise formulation of the expressiveness of polynomial networks in terms of the algebraic dimension of the functional space as an algebraic variety.\n• We spell out the close, two-way relationship between polynomial networks and a particular family of decompositions of tensors.\n• We prove several theoretical results on the functional space of polynomial networks. Notably, we give a formula for the dimension that holds for sufficiently high activation degrees (Theorem 14) and we prove a tight lower bound on the width of the layers for the network to be “filling” in the functional space (Theorem 19).\nNotation. We use Symd(Rn) to denote the space of homogeneous polynomials of degree d in n variables with coefficients in R. This set is a vector space over R of dimension Nd,n = n+d 1 d , spanned by all monomials of degree d in n variables. In practice, Symd(Rn) is isomorphic to RNd,n , and our networks will correspond to points in this high dimensional space. The notation Symd(Rn) expresses the fact that a polynomial of degree d in n variables can always be identified with a symmetric tensor in (Rn)⌦d that collects all of its coefficients."
  }, {
    "heading": "2 Basic setup",
    "text": "A polynomial network is a function p✓ : Rd0 ! Rdh of the form\np✓(x) = Wh⇢rWh 1⇢r . . . ⇢rW1x, Wi 2 Rdi⇥di 1 ,\nwhere the activation ⇢r(z) raises all elements of z to the r-th power (r 2 N). The parameters ✓ = (Wh, . . . ,W1) 2 Rd✓ (with d✓ = Ph i=1 didi 1) are the network’s weights, and the network’s architecture is encoded by the sequence d = (d0, . . . , dh) (specifying the depth h and widths di). Clearly, p✓ is a homogeneous polynomial mapping Rd0 ! Rdh of degree rh 1, i.e., p✓ 2 Symrh 1(Rd0)dh . For fixed degree r and architecture d = (d0, . . . , dh), there exists an algebraic map\nd,r : ✓ 7! p✓ =\n2 64 p✓1\n... p✓dh+1\n3\n75 , (1)\nwhere each p✓i is a polynomial in d0 variables. The image of d,r is a set of vectors of polynomials, i.e., a subset Fd,r of Symrh 1(Rd0)dh , and it is the functional space represented by the network. In this paper, we consider the “Zariski closure” Vd,r = Fd,r of the functional space.1 We refer to Vd,r as functional variety of the network architecture, as it is in fact an irreducible algebraic variety. In particular, Vd,r can be studied using powerful machinery from algebraic geometry.\nRemark 1. The functional variety Vd,r may be significantly larger than the actual functional space Fd,r, since the Zariski closure is typically larger than the closure with respect to the standard the Euclidean topology. On the other hand, the dimensions of the spaces Vd,r and Fd,r agree, and the set Vd,r is usually “nicer” (it can be described by polynomial equations, whereas an exact implicit description of Fd,r may require inequalities)."
  }, {
    "heading": "2.1 Examples",
    "text": "We present some examples that describe the functional variety Vd,r in simple cases. Example 2. A linear network is a polynomial network with r = 1. In this case, the network map d,r : Rd✓ ! Sym1(Rd0)dh ⇠= Rdh⇥d0 is simply matrix multiplication:\n✓ = (Wh,Wh 1, . . . ,W1) 7! p✓ = WhWh 1 . . .W1x.\nThe functional space Fd,r ✓ Rdh⇥d0 is the set of matrices with rank at most dmin = mini{di}. This set is already characterized by polynomial equations, as the common zero set of all (1+ dmin)⇥ (1+ dmin) minors, so Fd,r = Vd,r in this case. The dimension of Vd,r ⇢ Rdh⇥d0 is dmin(d0+dh dmin). Example 3. Consider d = (2, 2, 3) and r = 2. The input variables are x = [x1, x2]T , and the parameters ✓ are the weights\nW1 =\n w111 w112\nw121 w122\n, W2 =\n2 64 w211 w212 w221 w222\nw231 w232\n3\n75 .\n1The Zariski closure of a set X is the smallest set containing X that can be described by polynomial equations.\nThe network map p✓ is a triple of quadratic polynomials in x1, x2, that can be written as\nW2⇢2W1x =\n2 64 w211(w111x1 + w112x2)2 + w212(w121x1 + w122x2)2 w221(w111x1 + w112x2)2 + w222(w121x1 + w122x2)2\nw231(w111x1 + w112x2)2 + w232(w121x1 + w122x2)2\n3\n75 . (2)\nThe map (2,2,3),2 in (1) takes W1,W2 (that have d✓ = 10 parameters) to the three quadratics in x1, x2 displayed above. The quadratics have a total of dim Sym2(R2)3 = 9 coefficients, however these coefficients are not arbitrary, i.e., not all possible triples of polynomials occur in the functional space. Writing c(k)ij for the coefficient of xixj in p✓k in (2) (with k = 1, 2, 3 i, j = 1, 2) then it is a simple exercise to show that\ndet\n2\n664\nc (1) 11 c (1) 12 c (1) 22\nc (2) 11 c (2) 12 c (2) 22\nc (3) 11 c (3) 12 c (3) 22\n3\n775 = 0.\nThis cubic equation describes the functional variety V(2,3,3),2, which is in this case an eightdimensional subset (hypersurface) of Sym2(R2)3 ⇠= R9."
  }, {
    "heading": "2.2 Objectives",
    "text": "The main goal of this paper is to study the dimension of Vd,r as the network’s architecture d and the activation degree r vary. This dimension may be considered a precise and intrinsic measure of the polynomial network’s expressivity, quantifying degrees of freedom of the functional space. For example, the dimension reflects the number of input/output pairs the network can interpolate, as each sample imposes one linear constraint on the variety Vd,r.\nIn general, the variety Vd,r lives in the ambient space Symrh 1(Rd0)dh , which in turn only depends on the activation degree r, network depth h, and the input/output dimensions d0 and dh. We are thus interested in the role of the intermediate widths in the dimension of Vd,r. Definition 4. A network architecture d = (d0, . . . , dh) has a filling functional variety for the activation degree r if Vd,r = Symrh 1(Rd0)dh .\nIt is important to note that if the functional variety Vd,r is filling, then actual functional space Fd,r (before taking closure) is in general only thick, i.e., it has positive Lebesgue measure in Symrh 1(Rd0)dh (see Remark 1). On the other hand, given an architecture with a thick functional space, we can find another architecture whose functional space is the whole ambient space. Proposition 5 (Filling functional space). Fix r and suppose d = (d0, d1, . . . , dh 1, dh) has a filling functional variety Vd,r. Then the architecture d0 = (d0, 2d1, . . . , 2dh 1, dh) has a filling functional space, i.e., Fd0,r = Symrh 1(Rd0)dh .\nIn summary, while an architecture with a filling functional variety may not necessarily have a filling functional space, it is sufficient to double all the intermediate widths for this stronger condition to hold. As argued below, we expect architectures with thick/filling functional spaces to have more favorable properties in terms of optimization and training. On the other hand, non-filling architectures may lead to interesting functional spaces for capturing patterns in data. In fact, we show in Section 3.2 that non-filling architectures generalize families of low-rank tensors."
  }, {
    "heading": "2.3 Connection to optimization",
    "text": "The following two results illustrate that thick/filling functional spaces are helpful for optimization. Proposition 6. If the closure of a set C ⇢ Rn is not convex, then there exists a convex function f on Rn whose restriction to C has arbitrarily “bad” local minima (that is, there exist local minima whose value is arbitrarily larger than that of a global minimum).\nProposition 7. If a functional space Fd,r is not thick, then it is not convex.\nThese two facts show that if the functional space is not thick, we can always find a convex loss function and a data distribution that lead to a landscape with arbitrarily bad local minima. There is also an obvious weak converse, namely that if the functional space is filling Fd,r = Symrh 1(Rd0)dh , then any convex loss function Fd,r will have a unique global minimum (although there may be “spurious” critical points that arise from the non-convex parameterization)."
  }, {
    "heading": "3 Architecture dimensions",
    "text": "In this section, we begin our study of the dimension of Vd,r. We describe the connection between polynomial networks and tensor decompositions for both shallow (Section 3.1) and deep (Section 3.2) networks, and we present some computational examples (Section 3.3)."
  }, {
    "heading": "3.1 Shallow networks and tensors",
    "text": "Polynomial networks with h = 2 are closely related to CP tensor decomposition [22]. Indeed in the shallow case, we can verify the network map (d0,d1,d2),r sends W1 2 Rd1⇥d0 ,W2 2 Rd2⇥d1 to:\nW2⇢rW1x = ⇣ d1X\ni=1\nW2(:, i)⌦W1(i, :) ⌦r ⌘ · x ⌦r =: (W2,W1) · x ⌦r .\nHere (W2,W1) 2 Rd2 ⇥ Symr(Rd0) is a partially symmetric d2 ⇥ d⇥r0 tensor, expressed as a sum of d1 partially symmetric rank 1 terms, and · denotes contraction of the last r indices. Thus the functional space F(d0,d1,d2),r is the set of rank  d1 partially symmetric tensors. Algorithms for low-rank CP decomposition could be applied to (W2,W1) to recover W2 and W1. In particular, when d2 = 1, we obtain a symmetric d⇥r0 tensor. For this case, we have the following. Lemma 8. A shallow architecture d = (d0, d1, 1) is filling for the activation degree r if and only if every symmetric tensor T 2 Symr(Rd0) has rank at most d1.\nFurthermore, the celebrated Alexander-Hirschowitz Theorem [1] from algebraic geometry provides the dimension of Vd,r for all shallow, single-output architectures. Theorem 9 (Alexander-Hirschowitz). If d = (d0, d1, 1), the dimension of Vd,r is given by min ⇣ d0d1, d0+r 1 r ⌘ , except for the following cases:\n• r = 2, 2  d1  d0 1,\n• r = 3, d0 = 5, d1 = 7,\n• r = 4, d0 = 3, d1 = 5,\n• r = 4, d0 = 4, d1 = 9,\n• r = 4, d0 = 5, d1 = 15."
  }, {
    "heading": "3.2 Deep networks and tensors",
    "text": "Deep polynomial networks also relate to a certain iterated tensor decomposition. We first note the map d,r may be expressed via the so-called Khatri-Rao product from multilinear algebra. Indeed ✓ maps to:\nSymRow Wh((Wh 1 . . . (W2(W •r1 )) •r . . . )•r). (3)\nHere the Khatri-Rao product operates on rows: for M 2 Ra⇥b, the power M•r 2 Ra⇥br replaces each row, M(i, :), by its vectorized r-fold outer product, vec(M(i, :)⌦r). Also in (3), SymRow denotes symmetrization of rows, regarded as points in (Rd0)⌦rh 1 , a certain linear operator. Another viewpoint comes from using polynomials and inspecting the layers in reverse order. Writing [p✓1, . . . , p✓dh 1 ] T for the output polynomials at depth h 1, the top output at depth h is:\nwh11 p r ✓1 + wh12 p r ✓2 + . . .+ wh1dh 1 p r ✓dh 1 . (4)\nThis expresses a polynomial as a weighted sum of r-th powers of other (nonlinear) polynomials. Recently, a study of such decompositions has been initiated in the algebra community [24]. Such expressions extend usual tensor decompositions, since weighted sums of powers of homogeneous linear forms correspond to CP symmetric decompositions. Accounting for earlier layers, our neural network expresses each p✓i in (4) as r-th powers of lower-degree polynomials at depth h 2, so forth. Iterating the main result in [16] on decompositions of type (4), we obtain the following bound on filling intermediate widths. Theorem 10 (Bound on filling widths). Suppose d = (d0, d1, . . . , dh) and r 2 satisfy\ndh i min ✓ dh · r i(d0 1), ✓ r h i + d0 1\nrh i\n◆◆\nfor each i = 1, . . . , h 1. Then the functional variety Vd,r is filling."
  }, {
    "heading": "3.3 Computational investigation of dimensions",
    "text": "We have written code2 in the mathematical software SageMath [12] that computes the dimension of Vd,r for a general architecture d and activation degree r. Our approach is based on randomly selecting parameters ✓ = (Wh, . . . ,W1) and computing the rank of the Jacobian of d,r(✓) in (1). This method is based on the following lemma, coming from the fact that the map d,r is algebraic.\nLemma 11. For all ✓ 2 Rd✓ , the rank of the Jacobian matrix Jac d,r(✓) is at most the dimension of the variety Vd,r. Furthermore, there is equality for almost all ✓ (i.e., for a non-empty Zariski-open subset of Rd✓ ).\nThus if Jac d,r(✓) is full rank at any ✓, this witnesses a mathematical proof Vd,r is filling. On the other hand if the Jacobian is rank-deficient at random ✓, this indicates with “probability 1\" that Vd,r is not filling. We have implemented two variations of this strategy, by leveraging backpropagation. Both work over a finite field F = Z/pZ to avoid floating-point computations (for almost all primes p, this provides the correct dimension over R).\n1. Backpropagation over a polynomial ring. We defined a network class over a ring F[x1, . . . , xd0 ], taking as input a vector variables x = (x1, . . . , xd0). Performing automatic differentiation (backpropagation) of the output function yields polynomials corresponding to dp✓(x)/dw, for any entry w of a weight matrix Wi. Extracting the coefficients of the monomials in x, we recover the entries of the Jacobian of d,r(✓).\n2. Backpropagation over a finite field. We defined a network class over the finite field F = Z/pZ. After performing backpropagation at a sufficient number of random sample points x, we can recover the entries of the Jacobian of d,r(✓) by solving a linear system (this system is overdetermined, but it will have an exact solution in finite field arithmetic).\nThe first algorithm is simpler and does not require interpolation, but is generally slower. We present examples of some of our computations in Tables 1 and 2. Table 1 shows minimal architectures d = (d0, . . . , dh) that are filling, as the depth h varies. Here, “minimal” is with respect to the partial ordering comparing all widths. It is interesting to note that for deeper networks, there is not a unique\n2Available at https://github.com/mtrager/polynomial_networks.\nminimally filling network. Also conspicuous is that minimal filling widths are “unimodal\", (weakly) increasing and then (weakly) decreasing. Arguably, this pattern conforms with common wisdom. Conjecture 12 (Minimal filling widths are unimodal). Fix r, h, d0 and dh. If d = (d0, d1, . . . , dh) is a minimal filling architecture, there is i such that d0  d1  . . .  di and di di+1 . . . dh.\nTable 2 shows examples of computed dimensions, for varying architectures and degrees. Notice that the dimension of an architecture stabilizes as the degree r increases."
  }, {
    "heading": "4 General results",
    "text": "This section presents general results on the dimension of Vd,r. We begin by pointing out symmetries in the network map d,r, under suitable scaling and permutation.\nLemma 13 (Multi-homogeneity). For arbitrary invertible diagonal matrices Di 2 Rdi⇥di and permutation matrices Pi 2 Zdi⇥di (i = 1, . . . , h 1), the map d,r returns the same output under the replacement:\nW1 P1D1W1\nW2 P2D2W2D r 1 P T 1\nW3 P3D3W3D r 2 P T 2\n...\nWh WhD r h 1P T h 1.\nThus the dimension of a generic fiber (pre-image) of d,r is at least Ph 1 i=1 di.\nOur next result deduces a general upper bound on the dimension of Vd,r. Conditional on a standalone conjecture in algebra, we prove that equality in the bound is achieved for all sufficiently high activation degrees r. An unconditional result is achieved by varying the activation degrees per layer.\nTheorem 14 (Naive bound and equality for high activation degree). If d = (d0, . . . , dh), then\ndimVd,r  min dh +\nhX\ni=1\n(di 1 1)di, dh\n✓ d0 + rh 1 1\nrh 1\n◆! . (5)\nConditional on Conjecture 16, for fixed d satisfying di > 1 (i = 1, . . . , h 1), there exists r̃ = r̃(d) such that whenever r > r̃, we have an equality in (5). Unconditionally, for fixed d satisfying di > 1 (i = 1, . . . , h 1), there exist infinitely many (rh 1, rh 2, . . . , r1) such that the image of (Wh, . . . ,W1) 7!Wh⇢rh 1Wh 1⇢rh 2 . . . ⇢1W1x has dimension dh + P i(di 1 1)di.\nProposition 15. Given positive integers d, k, s, there exists r̃ = r̃(d, k, s) with the following property. Whenever p1, . . . , pk 2 R[x1, . . . , xd] are k homogeneous polynomials of the same degree s in d variables, no two of which are linearly dependent, then pr1, . . . , prk are linearly independent if r > r̃.\nConjecture 16. In the setting of Proposition 15, r̃ may be taken to depend only on d and k.\nProposition 15 and Conjecture 16 are used in induction on h for the equality statements in Theorem 14. We remark that following our arXiv version of this paper, progress toward Conjecture 16 was made\nin [30]. There, it is shown that there exists r between 1 and k! such that pr1, . . . , prk are linearly independent; however, it remains open whether there exists r̃ as we conjecture.\nThe next result uses the iterative nature of neural networks to provide a recursive dimension bound.\nProposition 17 (Recursive Bound). For all (d0, . . . , dk, . . . , dh) and r, we have: dimV(d0,...,dh),r  dimV(d0,...,dk),r + dimV(dk,...,dh),r dk.\nUsing the recursive bound, we can prove an interesting bottleneck property for polynomial networks.\nDefinition 18. The width di in layer i is an asymptotic bottleneck (for r, d0 and i) if there exists h̃ such that for all h > h̃ and all d1, . . . , di 1, di+1, . . . , dh, then the widths (d0, d1, . . . , di, . . . , dh) are non-filling.\nThis expresses our finding that too narrow a layer can “choke\" a polynomial network, such that there is no hope of filling the ambient space, regardless of how wide elsewhere or how deep the network is.\nTheorem 19 (Bottlenecks). If r 2, d0 2, i 1, then di = 2d0 2 is an asymptotic bottleneck. Moreover conditional on Conjecture 2 in [28], then di = 2d0 is not an asymptotic bottleneck.\nProposition 17 affords a simple proof that di = d0 1 is an asymptotic bottleneck. However to obtain the full statement of Theorem 19, we seem to need more powerful tools from algebraic geometry."
  }, {
    "heading": "5 Conclusion",
    "text": "We have studied the functional space of neural networks from a novel perspective. Deep polynomial networks furnish a framework for nonlinear networks, to which the powerful mathematical machinery of algebraic geometry may be applied. In this respect, we believe polynomial networks can help us access a better understanding of deep nonlinear architectures, for which a precise theoretical analysis has been extremely difficult to obtain. Furthermore, polynomials can be used to approximate any continuous activation function over any compact support (Stone-Weierstrass theorem). For these reasons, developing a theory of deep polynomial networks is likely to pay dividends in building understanding of general neural networks.\nIn this paper, we have focused our attention on the dimension of the functional space of polynomial networks. The dimension is the first and most basic descriptor of an algebraic variety, and in this context it provides an exact measure of the expressive power of an architecture. Our novel theoretical results include a general formula for the dimension of the architecture attained in high degree, as well as a tight lower bound and nontrivial upper bounds on the width of layers in order for the functional variety to be filling. We have also demonstrated intriguing connections with tensor and polynomial decompositions, including some which appear in very recent literature in algebraic geometry.\nThe tools and concepts introduced in this work for fully connected feedforward polynomial networks can be applied in principle to more general algebraic network architectures. Variations of our algebraic model could include multiple polynomial activations (rather than just single exponentiations) or more complex connectivity patterns of the network (convolutions, skip connections, etc.). The functional varieties of these architectures could be studied in detail and compared. Another possible research direction is a geometric study of the functional varieties, beyond the simple dimension. For example, the degree or the Euclidean distance degree [13] of these varieties could be used to bound the number of critical points of a loss function. Additionally, motivated by Section 3.2, we would like to develop computational methods for constructing a network architecture that represents an assigned polynomial mapping. Such algorithms might lead to “closed form” approaches for learning using polynomial networks (similar to SVD or tensor decomposition), as a provable counterpoint to gradient descent methods. Our research program might also shed light on the practical problem of choosing an appropriate architecture for a given application."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Justin Chen, Amit Moscovich, Claudiu Raicu and Steven Sam for helpful conversations. JK was partially supported by the Simons Collaboration on Algorithms and Geometry. MT and JB were partially supported by the Alfred P. Sloan Foundation, NSF RI-1816753 and Samsung Electronics."
  }],
  "year": 2019,
  "references": [{
    "title": "Polynomial interpolation in several variables",
    "authors": ["James Alexander", "André Hirschowitz"],
    "venue": "Journal of Algebraic Geometry,",
    "year": 1995
  }, {
    "title": "A convergence analysis of gradient descent for deep linear neural networks",
    "authors": ["Sanjeev Arora", "Nadav Cohen", "Noah Golowich", "Wei Hu"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2019
  }, {
    "title": "On the optimization of deep networks: implicit acceleration by overparameterization",
    "authors": ["Sanjeev Arora", "Nadav Cohen", "Elad Hazan"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2018
  }, {
    "title": "On hitting sets for special depth-4 circuits",
    "authors": ["Pranav Bisht"],
    "venue": "Master’s thesis, Indian Institute of Technology Kanpur,",
    "year": 2017
  }, {
    "title": "On maximum, typical and generic ranks",
    "authors": ["Grigoriy Blekherman", "Zach Teitler"],
    "venue": "Mathematische Annalen,",
    "year": 2015
  }, {
    "title": "Cohen-Macaulay rings, volume 39 of Cambridge Studies in Advanced Mathematics",
    "authors": ["Winfried Bruns", "Jürgen Herzog"],
    "year": 1993
  }, {
    "title": "On the global convergence of gradient descent for overparameterized models using optimal transport",
    "authors": ["Lenaic Chizat", "Francis Bach"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2018
  }, {
    "title": "On the expressive power of deep learning: a tensor analysis",
    "authors": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"],
    "venue": "In Conference on Learning Theory,",
    "year": 2016
  }, {
    "title": "Convolutional rectifier networks as generalized tensor decompositions",
    "authors": ["Nadav Cohen", "Amnon Shashua"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Approximation by superpositions of a sigmoidal function",
    "authors": ["George Cybenko"],
    "venue": "Mathematics of Control, Signals and Systems,",
    "year": 1989
  }, {
    "title": "Shallow vs. deep sum-product networks",
    "authors": ["Olivier Delalleau", "Yoshua Bengio"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "The Euclidean distance degree of an algebraic variety",
    "authors": ["Jan Draisma", "Emil Horobeţ", "Giorgio Ottaviani", "Bernd Sturmfels", "Rekha R. Thomas"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2016
  }, {
    "title": "On the power of over-parametrization in neural networks with quadratic activation",
    "authors": ["Simon S. Du", "Jason D. Lee"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2018
  }, {
    "title": "Commutative algebra: with a view toward algebraic geometry, volume 150 of Graduate Texts in Mathematics",
    "authors": ["David Eisenbud"],
    "year": 1995
  }, {
    "title": "On the Waring problem for polynomial rings",
    "authors": ["Ralf Fröberg", "Giorgio Ottaviani", "Boris Shapiro"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2012
  }, {
    "title": "Algebraic geometry: a first course, volume 133 of Graduate Texts in Mathematics",
    "authors": ["Joe Harris"],
    "year": 1995
  }, {
    "title": "Algebraic geometry, volume 52 of Graduate Texts in Mathematics",
    "authors": ["Robin Hartshorne"],
    "venue": "8th print edition,",
    "year": 1997
  }, {
    "title": "Multilayer feedforward networks are universal approximators",
    "authors": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"],
    "venue": "Neural Networks,",
    "year": 1989
  }, {
    "title": "Learning algebraic models of quantum entanglement",
    "authors": ["Hamza Jaffali", "Luke Oeding"],
    "venue": "arXiv preprint arXiv:1908.10247,",
    "year": 2019
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["Kenji Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Tensors: geometry and applications, volume 128 of Graduate Studies in Mathematics",
    "authors": ["J.M. Landsberg"],
    "venue": "American Mathematical Society,",
    "year": 2012
  }, {
    "title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
    "authors": ["Moshe Leshno", "Vladimir Ya. Lin", "Allan Pinkus", "Shimon Schocken"],
    "venue": "Neural Networks,",
    "year": 1993
  }, {
    "title": "On generic and maximal k-ranks of binary forms",
    "authors": ["Samuel Lundqvist", "Alessandro Oneto", "Bruce Reznick", "Boris Shapiro"],
    "venue": "Journal of Pure and Applied Algebra,",
    "year": 2079
  }, {
    "title": "On the expressive efficiency of sum product networks",
    "authors": ["James Martens", "Venkatesh Medabalimi"],
    "venue": "arXiv preprint arXiv:1411.7717,",
    "year": 2014
  }, {
    "title": "The loss surface of deep linear networks viewed through the algebraic geometry lens",
    "authors": ["Dhagash Mehta", "Tianran Chen", "Tingting Tang", "Jonathan D. Hauenstein"],
    "year": 2018
  }, {
    "title": "A mean field view of the landscape of two-layer neural networks",
    "authors": ["Song Mei", "Andrea Montanari", "Phan-Minh Nguyen"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2018
  }, {
    "title": "On the Hilbert series of ideals generated by generic forms",
    "authors": ["Lisa Nicklasson"],
    "venue": "Communications in Algebra,",
    "year": 2017
  }, {
    "title": "Sum-product networks: a new deep architecture",
    "authors": ["Hoifung Poon", "Pedro Domingos"],
    "venue": "arXiv preprint arXiv:1202.3732,",
    "year": 2012
  }, {
    "title": "Linear independence of power",
    "authors": ["Steven V. Sam", "Andrew Snowden"],
    "venue": "arXiv preprint arXiv:1907.02659,",
    "year": 2019
  }, {
    "title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks",
    "authors": ["Mahdi Soltanolkotabi", "Adel Javanmard", "Jason D. Lee"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2019
  }, {
    "title": "Pure and spurious critical points: a geometric study of linear networks",
    "authors": ["Matthew Trager", "Kathlén Kohn", "Joan Bruna"],
    "year": 1910
  }, {
    "title": "Spurious valleys in two-layers neural network optimization landscapes",
    "authors": ["Luca Venturi", "Afonso S. Bandeira", "Joan Bruna"],
    "venue": "arXiv preprint arXiv:1802.06384,",
    "year": 2018
  }],
  "id": "SP:e35e6a8b476d34cb41e3f229d4a1b5c28fdb9172",
  "authors": [{
    "name": "Joe Kileel",
    "affiliations": []
  }, {
    "name": "Matthew Trager",
    "affiliations": []
  }, {
    "name": "Joan Bruna",
    "affiliations": []
  }],
  "abstractText": "We study deep neural networks with polynomial activations, particularly their expressive power. For a fixed architecture and activation degree, a polynomial neural network defines an algebraic map from weights to polynomials. The image of this map is the functional space associated to the network, and it is an irreducible algebraic variety upon taking closure. This paper proposes the dimension of this variety as a precise measure of the expressive power of polynomial neural networks. We obtain several theoretical results regarding this dimension as a function of architecture, including an exact formula for high activation degrees, as well as upper and lower bounds on layer widths in order for deep polynomials networks to fill the ambient functional space. We also present computational evidence that it is profitable in terms of expressiveness for layer widths to increase monotonically and then decrease monotonically. Finally, we link our study to favorable optimization properties when training weights, and we draw intriguing connections with tensor and polynomial decompositions.",
  "title": "On the Expressive Power of Deep Polynomial Neural Networks"
}