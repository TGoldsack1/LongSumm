{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Reasoning about other agents’ intentions and being able to predict their behavior is important in multi-agent systems, in which the agents might have different, and sometimes competing, goals. In this paper, we introduce a new approach for estimating other agents’ unknown goals from their behavior and using those estimates to choose actions. We demonstrate that in the proposed tasks, using an explicit model of the other player leads to better performance than simply considering the other agent as part of the environment.\nWe frame the problem as a two-player stochastic game (Shapley, 1953), in which each agent is randomly assigned a different goal from a fixed set, which is shared between the agents. Players have full visibility of the environment, but no direct knowledge of the other’s goal and no communication channel. The reward obtained by each agent at the end of an episode depends on the goals of both agents, so an optimal policy must take into account both of their goals.\nThe key idea of this work is that as a first approximation\n1New York University, New York City, USA 2Facebook AI Research, New York City, USA. Correspondence to: Roberta Raileanu <raileanu@cs.nyu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nof understanding what the other player is trying to achieve, an agent should ask itself “what would be my goal if I had acted as the other player had?”. We instantiate this idea by parametrizing the agent’s action and value functions with a neural network that takes as input the observation state and a goal. As the agent plays the game, it uses its own policy (with the input expressed in the other agent’s frame of reference) to maximize the likelihood of the other’s observed actions and optimize directly over the goal representation to infer the other agent’s unknown goal. In contrast with the current literature, our approach does not require building any model of the other agent in order to infer its intention and predict its behavior."
  }, {
    "heading": "2. Approach",
    "text": "Background: A two-player Markov game is defined by a set of states S describing the possible configurations of all agents, a set of actions A1,A2 and a set of observations S1,S2 for each agent, and a transition function T : S × A1 ×A2 → S which gives the probability distribution on the next state as a function of current state and actions. Each agent i chooses actions by sampling from a stochastic policy πi : S × Ai → [0, 1]. The reward function of each agent is: ri : S ×A1 ×A2 → R. Each agent i aims to maximize its discounted return from time t onward: Rit = ∑∞ t=0 γ\ntrit, where rit is the reward obtained by agent i at time t and γ ∈ (0, 1] is the discount factor. In this work, we consider both cooperative and adversarial settings. In cooperative games, the agents have the same reward function: r1 = r2.\nWe now describe Self Other-Modeling (SOM), a new approach for inferring other agents’ goals in an online fashion and using these estimates to choose actions. To decide an action and to estimate the value of a state, we use a neural network f that takes as input its own goal zself , an estimate of the other player’s goal z̃other, and the observation state sself , and outputs a probability distribution over actions π and a value estimate V , such that for each agent i playing the game we have:[\nπi V i ] = f(siself , z i self , z̃ i other; θ i) .\nHere θi are agent i’s parameters for f , which has one softmax output for the policy, one linear output for the value function, and all the non-output layers shared. The actions\nare sampled from policy πi. The state siself contains the observation features from agent i’s viewpoint.\nWe propose that each agent models the behavior of the other player using its own policy. Thus, each agent uses its own network f in two ways: acting mode, in which the agent uses f to choose its actions and inference mode, in which the agent uses f to infer the other agent’s goal. For notation purposes, whenever f is used in acting mode (inference mode) we will refer to it as fself (fother):\nacting mode: fself (sself , zself , z̃other; θ) (1)\ninference mode: fother(sother, z̃other, zself ; θ). (2)\nThe two modes have different relative placements of the network’s inputs zself and z̃other. Additionally, since the environment is fully observed, the observation state of the two agents differs only by the specification of the agent’s identity on the map (i.e. each agent will be able to distinguish between its own location and the other’s location). Hence, in acting mode, the network fself will take as input sself (with the identity of the acting agent at the location of the self ) and in inference mode, the network fother will take as input sother (with the identity of the acting agent at the location of the other).\nAt each step, the agent uses equation (2) to output an estimate of the probability distribution over the other agent’s actions. Then, the agent uses supervision of the other’s true action to backpropagate through fother (without updating its paramters) and directly optimize over its input z̃other, the estimate of the other agent’s goal. The number of optimization steps used to update z̃other is a hyperparameter that can vary with the game. The new estimate z̃other is used as input to fself in (1) for choosing the self agent’s next action. Figure 1 illustrates this technique.\nNote that the network f is never updated during inference mode (i.e. using supervision of the other agent’s actions), f ’s parameters θ are updated only at the end of each episode using Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) with reward signal obtained by the self agent. In contrast, z̃other is updated (multiple times) at each step in the game.\nAlgorithm 1 represents the pseudo-code for training a SOM agent for one episode. The procedure is formulated from the viewpoint of a single agent. Since the goals are discrete in all the tasks considered here, the agent’s goal zself is encoded as a one-hot vector of dimension equal to the total number of possible goals in the game. In line 6, siself is the self’s observation state from the perspective of agent i, which is the same as the other’s observation state from the perspective of agent j, sjother.\nWe consider a continuous vector z̃other of the same dimension as zself , such that the estimate of the other agent’s\nAlgorithm 1 SOM training for one episode 1: procedure SELF OTHER-MODELING 2: for k := 1, num players do 3: z̃kother ← 1ngoals1ngoals 4: game.reset() 5: for step := 1, episode length do 6: siself = s j other ← game.get state()\n7: z̃OH,iother = one hot(argmax(softmax(z̃ i other)) 8: πiself , V i self ← f iself (siself , ziself , z̃ OH,i other; θ i)\n9: aiself ∼ πiself 10: game.action(aiself ) 11: for k : = 1, num inference steps do 12: z̃GS,jother = gumbel soft(softmax(z̃ j other)) 13: π̃jother← f j other(s j other, z̃ GS,j other, z j self ; θ j) 14: loss = cross entropy loss(π̃jother, a i self ) 15: loss.backward() 16: update(z̃jother) 17: for k := 1, num players do 18: policy.update(θk)\ngoal is a sample from the Categorical distribution with class probabilities softmax(z̃other). Thus, the estimate of the other’s goal is given by the one-hot vector z̃OHother, as shown in line 7. At the beginning of each game, the estimate of the other’s goal z̃OHother is randomly initialized, as illustrated in line 3, where 1ngoals represents a vector of all ones with the size equal to the number of possible goals.\nIn inference mode, the estimate of the other agent’s goal is expressed as a sample from the Gumbel-Softmax distribution (Jang et al., 2016; Maddison et al., 2016), z̃GSother, as shown in line 12, where gumbel soft(p) = softmax[g + log(p)]), with g sampled from the Gumbel distribution and the softmax temperature τ = 1. To update the estimate of the other’s goal, we directly optimize z̃other by using the cross-entropy loss to backpropagate through fother (lines 14, 15, 16).\nThe agents’ policies are parametrized by long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997)\nwith two fully-connected linear layers, and exponential linear unit (ELU) (Clevert et al., 2015) activations. The weights of the networks are initialized with semi-orthogonal matrices, as described in Saxe et al. (2013) and zero bias."
  }, {
    "heading": "3. Related Work",
    "text": "Multi-Agent Learning. Recent work in deep multi-agent RL focuses on partially visible, fully cooperative settings (Foerster et al., 2016a;b; Omidshafiei et al., 2017) and emergent communication (Lazaridou et al., 2016; Foerster et al., 2016a; Sukhbaatar et al., 2016; Das et al., 2017; Mordatch & Abbeel, 2017). Lerer & Peysakhovich (2017) design RL agents that are able to maintain cooperation in complex social dilemmas by generalizing a well-known game theoretic strategy called tit-for-tat (Axelrod, 2006), to multiagent Markov games. Leibo et al. (2017) considers semicooperative multi-agent environments in which the agents develop cooperative or competitive strategies depending on the task type and reward structure. Similarly, Lowe et al. (2017) proposes a centralized actor-critic architecture for efficient training in settings with such mixed strategies. Our setting is different since we do not allow communication between the agents, so the players have to indirectly reason about others’ intentions from their observed behavior.\nIntent Recognition. Research on plan, activity, and intent recognition has a long history, but it usually assumes domain knowledge or a form of rationality and uses techniques such as Bayesian inference or Hidden Markov Models (Sukthankar et al., 2014). The field of inverse reinforcement learning (IRL) (Russell, 1998; Ng et al., 2000; Abbeel & Ng, 2004) is also related to the problem considered here. IRL’s aim is to infer the reward function of an agent by observing its behavior, which is assumed to be nearly optimal. In contrast, our approach uses the observed actions of the other player to directly infer its goal in an online manner, which is then used by the agent when acting in the environment. This avoids the need for collecting offline samples of the other’s (state, action) pairs in order to estimate its reward function and use it to learn a policy. The more recent papers by Hadfield-Menell et al. (2016; 2017) are also concerned with the problem of inferring intentions, but their focus is on human-robot interaction and value alignment. Motivated by similar goals, Chandrasekaran et al. (2017) consider the problem of building a theory of AI’s mind, in order to improve human-AI interaction and the interpretability of AI systems. Recent work in cognitive science attempts to understand human decision-making by using a hierarchical model of social agency that infers human intentions for choosing a strategy (Kleiman-Weiner et al., 2016). However, none of these papers design algorithms that explicitly model other artificial agents in the environment or estimate their intentions, with the purpose of improving their decision\nmaking.\nModeling Other Agents. Opponent modeling has been extensively studied in games of imperfect information. Yet most previous approaches focuses on developing models with domain-specific probabilistic priors or strategy parametrizations. In contrast, our work proposes a more general framework for opponent modeling. Davidson (1999) uses an MLP to predict opponent actions given a game history, but the agents cannot adapt to their opponents’ behavior online. Lockett et al. (2007) designs a neural network architecture to identify the opponent type by learning a mixture of weights over a given set of cardinal opponents, but the game does not unfold within the RL framework.\nThe closest work to ours is Foerster et al. (2017) and He et al. (2016). Foerster et al. (2017) designs RL agents that take into account the learning of other agents in the environment when updating their own policies. This enables the agents to discover self-interested yet collaborative strategies such as tit-for-tat in the iterated prisoner’s dilemma. While our work does not explicitly attempt to shape the learning of other agents, it has the advantage that agents can update their beliefs during an episode and change their strategies online to gain more reward. Our setting is also different in that it considers that each agent has some hidden information needed by the other player to maximize its return.\nOur work is very much in line with He et al. (2016), where the authors build a general framework for modeling other agents in the reinforcement learning setting. He et al. (2016) proposes a model that jointly learns a policy and the behavior of opponents by encoding observations of the opponent into a DQN. Their Mixture of Experts architecture is able to discover different opponent strategy patterns in two competitive tasks. In our approach, rather than using hand designed features of the other agent’s behavior, the agent models others using its own policy. Another difference is that in this work, the agent runs an optimization over the input vector to infer the other agent’s hidden goal, rather than using a feed-forward network. In the experiments below, we show that SOM outperforms an adaptation of the method of He et al. (2016) to our setting."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, we evaluate our model SOM on three tasks:\n• The coin game, in Section 4.2, which is a fully cooperative task where the agents’ roles are symmetric.\n• The recipe game, in Section 4.3, which is adversarial, but with symmetric roles.\n• The door game, in Section 4.4, which is fully cooperative but has asymmetric roles for the two players.\nWe compare SOM to three other baselines and to a model that has access to the ground truth of the other agent’s goal. All the tasks considered are created in the Mazebase gridworld environment (Sukhbaatar et al., 2015)."
  }, {
    "heading": "4.1. Baselines",
    "text": "TRUE-OTHER-GOAL (TOG): We provide an upper bound on the performance of our model given by a policy network which takes the other agent’s true goal as input zother, as well as the state features sself and its own goal zself . Since this model has direct access to the true goal of the other agent, it does not need a separate network to model the behavior of the other agent. The architecture of TOG is the same as the one of SOM’s policy network, f .\nNO-OTHER-MODEL (NOM): The first baseline we use only takes as inputs the observation states sself and its own goal zself . NOM has the same architecture as the one used for SOM’s policy network, fself . This baseline does not explicitly model the other agent’s policy, goal, or actions.\nINTEGRATED-POLICY-PREDICTOR (IPP): Starting with the architecture and inputs of NOM, we construct a stronger baseline, IPP, which has an additional final linear layer that outputs a probability distribution over the next action of the other agent. Besides the A3C loss used to train the policy of this network, we also add a cross-entropy loss to train the prediction of the other agent’s action, using observations of its true actions.\nSEPARATE-POLICY-PREDICTOR (SPP): He et al. (2016) propose an opponent modeling framework based on DQN. In their approach, a neural network (separate from the learned Q-network) is trained to predict the opponents actions given hand crafted state information specific to the opponent. An intermediate hidden representation from this network is given as input to the Q-network.\nWe adapt the model of He et al. (2016) to our setting. In particular, we use A3C instead of DQN and we do not use the task-specific features used to represent the hidden goal of the opponent.\nThe resulting model, SPP, consists of two separate networks, a policy network for deciding the agent’s actions, and an opponent network for predicting the other agent’s actions. The opponent network takes as input its own state observation sself and goal zself , and outputs a probability distribution for the action taken by the other agent at the next step, as well as its hidden (recurrent) state. As in IPP, we train the opponent policy predictor with a cross-entropy loss using the true actions of the other agent. At each step, the hidden (recurrent) state outputted by this network is taken as input by the agent’s policy network, along with the observation state and its own goal. Both the policy network and the opponent policy predictor are LSTMs with the same\narchitecture as SOM.\nIn contrast to SOM, SPP does not explicitly infer the other agent’s goal. Rather, it builds an implicit model of the opponent by predicting the agent’s actions at each time step. In SOM, an inferred goal is given as additional input to the policy network. The analog of the inferred goal in SPP is the hidden (recurrent) state obtained from the opponent policy predictor which is given as an additional input to the policy network.\nTraining Details. In all our experiments, we train the agents’ policies using A3C (Mnih et al., 2016) with an entropy coefficient of 0.01, a value loss coefficient of 0.5, and a discount factor of 0.99. The parameters of the agents’ policies are optimized using Adam (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999, = 1×10−8, and weight decay 0. SGD with a learning rate of 0.1 was used for inferring the other agent’s goal, z̃other.\nThe hidden layer dimension of the policy network was 64 for the Coin and Recipe Games and 128 for the Door Game. We use a learning rate of 1×10−4 for all games and models.\nThe observation state s is represented by few-hot vectors indicating the locations of all the objects in the environment (including the other player). The dimension of this input state is 1 × nfeatures, where the number of features is 384, 192, and 900 for the Coin, Recipe, and Door games, respectively.\nFor each experiment, we trained the models using 5 different random seeds. All the results shown are for 10 optimization updates of z̃other at each step in the game, unless mentioned otherwise."
  }, {
    "heading": "4.2. Coin Game.",
    "text": "First, we evaluate the model on a fully cooperative task, in which the agents can gain more reward when using both of their goals rather than only their own goal. So it is in the best interest of each agent to estimate the other player’s goal and use that information when taking actions. The game, shown in the left diagram of Figure 4, takes place on a 8× 8 grid containing 12 coins of 3 different colors (4 coins of each color). At the beginning of each episode, the agents are randomly assigned one of the three colors. The action space consists of: go up, down, left, right, or pass. Once an agent steps on a coin, that coin disappears from the grid. The game ends after 20 steps. The reward received by both agents at the end of the game is given by the formula below:\nR(cself , cother) = (n self Cself + notherCself ) 2 + (nselfCother + n other Cother )2\n− (nselfCneither + n other Cneither )2,\nwhere notherCself is the number of coins of the self’s goal-color, which were collected by the other agents, and nselfCneither is\nthe number of coins corresponding to neither of the agents’ goals, collected by the self. For the example in Figure 4, agent 1 has Cself = orange and Cother = cyan, while agent 2’s Cself is cyan and Cother is orange. Cneither is red for both agents.\nThe role of the penalty for collecting coins that do not correspond to any of the agents’ goals is to avoid convergence to a greedy policy in which the agents can gain a non-negligible amount of reward by collecting all the coins in their proximity, without any regard to their color.\nTo maximize its return, each agent needs maximize the number of collected coins of its own or its collaborator’s color, and minimize the number of coins of the remaining color. Hence, when both agents are able to infer their collaborators’ goals with high accuracy and as early as possible in the game, they can use that information to maximize their\nshared utility.\nFigure 3 shows the mean and standard deviation of the reward across 5 runs with different random seeds obtained by SOM. Our model clearly outperforms all other baselines on this task. We also show the empirical upper bound on the reward using the model which takes as input the true color assigned to the other agent.\nFigure 2 analyzes the strategies of the different models by looking at the proportion of coins of each type collected by the agents. The optimal strategy is for each agent to maximize nselfCself + n self Cother\nand minimize nselfCneither . Due to the randomness in the initial conditions (in particular, the locations of coins in the environment), this amounts to each agent collecting an equal number of coins of its own and of the other’s color on average, across a large number of episodes (i.e. n̄selfCself = n̄ self Cother ).\nIndeed, this is the strategy learned by the model with perfect information of the other agent’s goal (TOG). SOM also learns to collect significantly more Other than Neither coins (although not as many as Self coins), indicating its ability to distinguish between the two types, at least during some of the episodes. This means that SOM can accurately infer the other agent’s goal early enough during the episode and use that information to collect more Other Coins, thus gaining more reward than if it were only using its own goal to direct its actions.\nIn contrast, the agents trained with the three baseline models collect significantly more Self coins, and as many Other as Neither coins on average. This shows that they learn to use their own goal for gaining reward, but they are unable to use the hidden goal of the other agent for further increasing their returns. Even if IPP and SPP are able to predict the actions of the other player with an accuracy of about 50%, they do not learn to distinguish between the coins that would increase (Other) and those that would decrease (Neither)\ntheir reward. This shows the weaknesses of using an implicit model of the other agent to maximize reward on certain tasks."
  }, {
    "heading": "4.3. Recipe Game.",
    "text": "Agents in adversarial scenarios have competing goals, so the ability of inferring the opponent’s goal could better inform the agent’s actions. With this motivation in mind, we evaluate our model on a game in which the agents have to craft certain compositional recipes, each containing multiple items found in the environment. The agents are given as input the names of their goal-recipes, without the corresponding components needed to make it. The resources in the environment are scarce, so only one of the agents can craft its recipe within one episode.\nAs illustrated in Figure 4 (center), there are 4 types of items: {sun, star, moon, lightning} and 4 recipes: {sun, sun, star}; {star, star, moon}; {moon, moon, lightning}; {lightning, lightning, sun}. The game is played in a 4× 6 grid, which contains 8 items in total, 2 of each type.\nAt the beginning of each episode, we randomly assign a recipe to one of the agents, and then we randomly pick a recipe for the other agent so that it has overlapping items with the recipe of the first agent. This ensures that the agents are competing for resources within each episode. At the end of the episode, each agent receives a reward of +1 for crafting its own recipe and a penalty of -0.1 for each item it picked up not needed for making its recipe.\nWe designed the layout of the grid so that neither agent has an initial advantage by being closer to the scarce resource. At the beginning of each episode, one of the agents starts on the left-most column of the grid, while the other one starts on the right-most column, at the same y-coordinate. Their initial y-coordinate as well as which agent starts on the left/right is randomized. Similarly, one item of each of the 4 different types is placed at random in the grid formed\nby the second and third columns of the maze, from left to right. The rest of the items are placed in the forth and fifth columns, so that the symmetry with respect to the vertical axis is preserved (i.e. items of the same type are placed at the same y-coordinate, and symmetric x-coordinates).\nAgents have six actions to choose from: pass, go up, down, left, right, or pick up (for picking up an item, which then disappears from the grid). The first agent to take an action is randomized. The game ends after 50 steps.\nWe pretrain all baselines on a version of the game which does not have overlapping recipes, in order to ensure that all the models learn to pick up the corresponding items, given a recipe as goal. All of the models learn to craft their assigned recipes ∼ 90% of the time on this simpler task. Then, we continue training the models on the adversarial task in which their recipes overlap in each episode. SOM is initialized with a pretrained NOM network.\nFigure 5 shows the winning fraction for different pairs played against each other in the Recipe game. For the first 100k episodes, the models are not being trained. We can see that SOM significantly outperfroms NOM, IPP, and SPP, winning ∼ 75 − 80% of the time, while the baselines can only win ∼ 15− 20% of the games. SPP ties against NOM, and TOG outperforms SOM by a large margin. We also played the same types of agents against each other and they all win ∼ 40− 50% of the games."
  }, {
    "heading": "4.4. Door Game.",
    "text": "In this section, we show that on a collaborative task with asymmetric roles and multiple possible partners, the agents can learn to figure out what role they should be playing in each game based on their partners’ actions.\nIn the Door game, two agents are located in a 5 × 9 grid, with 5 goals behind 5 doors on the left wall, and 5 switches on the right wall of the grid. The game starts with the two players in random squares on the grid, except for the ones occupied by the goals, doors, or switches, as illustrated in Figure 4. Agents can take any of the five actions: go up, down, left, right or pass. An action is invalid if it moves the player outside of the border or to a square occupied by a block or closed door. Both agents receive +3 reward when either one of them steps on its goal and they are penalized -0.1 for each step they take. The game ends when one of them gets to its goal or after 22 steps. All the goals are behind doors which are open only as long as one of the agents sits on the corresponding switch for that door.\nAt the beginning of an episode, each of the two players is randomly selected from a pool of 5 agents and receives as input a random number from 1 to 5 corresponding to its goal. Each of the 5 agents has its own policy which gets updated at the end of each episode they play. Note that the agents’\nidentities are not visible (i.e. there is no indication in the state features that specifies the id’s of the agents playing during a given episode). This restriction is important in order to ensure that the agents cannot gain advantage by specializing into the two roles needed to win (i.e. goal-goer and switch-puller) and identifying the specialization of the other player by simply observing its unique id.\nThe agents need to cooperate in order to receive reward. In contrast to our previous tasks, the two players must take different roles. In fact, the player who sits on the switch should ignore its own goal and instead infer the other’s goal, while the player who goes to its goal does not need to infer the other’s goal, but only use its own. In order to sit on the correct switch, an agent has to infer the other player’s goal from their observed actions. The only way in which an agent can use its own policy to model the other player is if each agent learns to play both roles of the game, i.e. go to its own goal and also open its collaborator’s door by sitting on the corresponding switch. Indeed, we see that the agents learn to play both roles and they are able to use their own policies to infer the other player’s goals when needed.\nFig 6 shows the mean and standard deviation of the winning fraction obtained by one of the agents on the Door game. While our model is still able to outperform the three baselines, the gap between the performance of our model and that of IPP or SPP (an approximate version of (He et al., 2016)) is smaller than in the previous tasks. However, this is a more difficult task for our model since it needs the agent to learn both roles before effectively using its own policy to infer the other agent’s goal. The plot shows that SOM actually performs worse than IPP and SPP during the initial part of training, before outperforming them. Nevertheless, we see that SOM training allows the agents to play both roles in an asymmetric cooperative game, and to infer the goal and role of the other player."
  }, {
    "heading": "4.5. Analyzing the goal inference",
    "text": "In this section we further analyze the ability of the SOM models to infer other’s intended goals.\nFigure 7 shows the fraction of episodes in which the goal of the other agent is correctly inferred. We consider that the goal is correctly inferred only when the estimate of the other’s goal remains accurate until the end of the game, so that we avoid counting the episodes in which the agent might infer the correct goal by chance at some intermediate step in the game. In all the games, the SOM agent learns to infer the other player’s goal with a mean accuracy ranging\nfrom ∼ 60− 80%. Comparing the second plot in Figure 2 with the left plot in Figure 7, one can observe that the SOM agent starts distinguishing Other from Neither coins after approximately 2M training episodes, which coincides with the time when the mean accuracy of the inferred goal converges to ∼ 75%. The Door Game (right) presents higher variance since the agents learn to use and infer the other’s goal at different stages during training.\nFigure 8 shows the cumulative distribution of the step at which the goal of the other player is correctly inferred (and remains the same until the end of the game). The cumulative distribution is computed over the episodes in which the goal is correctly inferred before the end of the game. In the Coin (blue) and Recipe (red) games, 80% of the times the agent correctly infers the goal of the other, it does so in the first five steps. The distribution for the Door (green) game indicates that the agent needs more steps on average to correctly infer the goal. This explains in part why the SOM agent only slightly outperforms the SPP baseline. If the agent does not infer the other’s goal early enough in the episode, it cannot efficiently use it to maximize its return.\nFigure 9 shows how the performance of the agent varies with\nthe number of optimization updates performed on z̃other at each step in the game. As expected, the agent’s reward (blue) generally increases with the number of inference steps, as does the fraction of episodes in which the goal is correctly inferred. One should note that increasing the number of inference steps from 10 to 20 only translates into less than 0.45% performance gain, while increasing it from 1 to 5 translates into a performance gain of 6.9% on the Coin game, suggesting that there is a certain threshold above which increasing the number of inference steps will not significantly improve performance."
  }, {
    "heading": "5. Discussion",
    "text": "Summary. In this paper, we introduced a new approach for inferring other agents’ hidden goals from their behavior and using those estimates to choose actions. We demonstrated that the agents are able to estimate others’ hidden goals in both cooperative and competitive settings, which enables them to converge to better policies. In the proposed tasks, using an explicit model of the other player led to better performance than simply considering the other agent as part of the environment.\nStrengths. Some of the main advantages of our method are its simplicity and flexibility. This method does not require any extra parameters to model other agents in the environment, can be trained with any reinforcement learning algorithm, and can be easily integrated with any network architecture. SOM can also be adapted to settings with more than two players, since the agent can use its own policy to model the behavior of any number of agents and infer their goals. Moreover, it can be easily generalized to numerous other environments and tasks.\nLimitations. Our approach is based on the assumption that the agents are identical or that their transition functions are independent and identically distributed. Hence, the framework is expected to be more suitable for symmetric games, in which the agents share a fixed set of goals and have similar abilities, and we expect a degradation of performance for asymmetric games. Our experiments confirm this observation. Another limitation of SOM is that it requires a longer training time than other baselines, since we backpropagate through the network at each step. However, their online nature is essential in adapting to the behavior of other agents in the environment.\nFuture Work. We plan to extend this work by evaluating the models on more complex environments and model deviations from the assumption that the players have identical policies, given a certain goal and state of the world. Another important avenue for future research is to design models that can adapt to non-stationary strategies of others in the environment, as well as to tasks with hierarchical goals."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was partially supported by ONR grant N0001413-1-0646. The authors wish to thank Alex Peysakhovich and Adam Lerer for helpful discussions."
  }],
  "year": 2018,
  "references": [{
    "title": "Apprenticeship learning via inverse reinforcement learning",
    "authors": ["P. Abbeel", "A.Y. Ng"],
    "venue": "In Proceedings of the twenty-first international conference on Machine learning,",
    "year": 2004
  }, {
    "title": "Agent-based modeling as a bridge between disciplines",
    "authors": ["R. Axelrod"],
    "venue": "Handbook of computational economics,",
    "year": 2006
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Clevert", "D.-A", "T. Unterthiner", "S. Hochreiter"],
    "venue": "arXiv preprlint arXiv:1511.07289,",
    "year": 2015
  }, {
    "title": "Learning cooperative visual dialog agents with deep reinforcement learning",
    "authors": ["A. Das", "S. Kottur", "J.M. Moura", "S. Lee", "D. Batra"],
    "venue": "arXiv preprint arXiv:1703.06585,",
    "year": 2017
  }, {
    "title": "Using artificial neural networks to model opponents in texas holdem",
    "authors": ["A. Davidson"],
    "venue": "Unpublished manuscript,",
    "year": 1999
  }, {
    "title": "Learning to communicate with deep multi-agent reinforcement learning",
    "authors": ["J. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Learning to communicate to solve riddles with deep distributed recurrent q-networks",
    "authors": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"],
    "venue": "arXiv preprint arXiv:1602.02672,",
    "year": 2016
  }, {
    "title": "Learning with opponentlearning awareness",
    "authors": ["J.N. Foerster", "R.Y. Chen", "M. Al-Shedivat", "S. Whiteson", "P. Abbeel", "I. Mordatch"],
    "venue": "arXiv preprint arXiv:1709.04326,",
    "year": 2017
  }, {
    "title": "Cooperative inverse reinforcement learning",
    "authors": ["D. Hadfield-Menell", "S.J. Russell", "P. Abbeel", "A. Dragan"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2016
  }, {
    "title": "Inverse reward design",
    "authors": ["D. Hadfield-Menell", "S. Milli", "P. Abbeel", "S.J. Russell", "A. Dragan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Opponent modeling in deep reinforcement learning",
    "authors": ["H. He", "J. Boyd-Graber", "K. Kwok", "H. Daumé III"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "Categorical reparameterization with gumbel-softmax",
    "authors": ["E. Jang", "S. Gu", "B. Poole"],
    "venue": "arXiv preprint arXiv:1611.01144,",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Coordinate to cooperate or compete: abstract goals and joint intentions in social interaction",
    "authors": ["M. Kleiman-Weiner", "M.K. Ho", "J.L. Austerweil", "M.L. Littman", "J.B. Tenenbaum"],
    "venue": "COGSCI,",
    "year": 2016
  }, {
    "title": "Multiagent cooperation and the emergence of (natural) language",
    "authors": ["A. Lazaridou", "A. Peysakhovich", "M. Baroni"],
    "venue": "arXiv preprint arXiv:1612.07182,",
    "year": 2016
  }, {
    "title": "Maintaining cooperation in complex social dilemmas using deep reinforcement learning",
    "authors": ["A. Lerer", "A. Peysakhovich"],
    "venue": "arXiv preprint arXiv:1707.01068,",
    "year": 2017
  }, {
    "title": "Evolving explicit opponent models in game playing",
    "authors": ["A.J. Lockett", "C.L. Chen", "R. Miikkulainen"],
    "venue": "In Proceedings of the 9th annual conference on Genetic and evolutionary computation,",
    "year": 2007
  }, {
    "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
    "authors": ["R. Lowe", "Y. Wu", "A. Tamar", "J. Harb", "P. Abbeel", "I. Mordatch"],
    "venue": "arXiv preprint arXiv:1706.02275,",
    "year": 2017
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables",
    "authors": ["C.J. Maddison", "A. Mnih", "Y.W. Teh"],
    "venue": "arXiv preprint arXiv:1611.00712,",
    "year": 2016
  }, {
    "title": "Asynchronous methods for deep reinforcement learning",
    "authors": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Emergence of grounded compositional language in multi-agent populations",
    "authors": ["I. Mordatch", "P. Abbeel"],
    "venue": "arXiv preprint arXiv:1703.04908,",
    "year": 2017
  }, {
    "title": "Algorithms for inverse reinforcement learning",
    "authors": ["A.Y. Ng", "Russell", "S. J"],
    "venue": "In Icml, pp",
    "year": 2000
  }, {
    "title": "Deep decentralized multi-task multi-agent rl under partial observability",
    "authors": ["S. Omidshafiei", "J. Pazis", "C. Amato", "J.P. How", "J. Vian"],
    "venue": "arXiv preprint arXiv:1703.06182,",
    "year": 2017
  }, {
    "title": "Learning agents for uncertain environments",
    "authors": ["S. Russell"],
    "venue": "In Proceedings of the eleventh annual conference on Computational learning theory,",
    "year": 1998
  }, {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"],
    "venue": "arXiv preprint arXiv:1312.6120,",
    "year": 2013
  }, {
    "title": "Mazebase: A sandbox for learning from games",
    "authors": ["S. Sukhbaatar", "A. Szlam", "G. Synnaeve", "S. Chintala", "R. Fergus"],
    "venue": "arXiv preprint arXiv:1511.07401,",
    "year": 2015
  }, {
    "title": "Learning multiagent communication with backpropagation",
    "authors": ["S. Sukhbaatar", "R Fergus"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }],
  "id": "SP:c8e9687ae76041b501b3660fd895227fde327d85",
  "authors": [{
    "name": "Roberta Raileanu",
    "affiliations": []
  }, {
    "name": "Emily Denton",
    "affiliations": []
  }, {
    "name": "Arthur Szlam",
    "affiliations": []
  }, {
    "name": "Rob Fergus",
    "affiliations": []
  }],
  "abstractText": "We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.",
  "title": "Modeling Others using Oneself in Multi-Agent Reinforcement Learning"
}