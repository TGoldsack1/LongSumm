{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Object tracking has gained much attention in recent decades (Bertinetto et al., 2016a; Danelljan et al., 2017; Zhu et al., 2016; Cui et al., 2016). The aim of object tracking is to localize an object in continuous video frames given an initial annotation in the first frame. Much of the existing work is, however, on the passive tracker, where it is presumed that the object of interest is always in the image scene so\n*Equal contribution 1Tencent AI Lab 2Peking University. Correspondence to: Wenhan Luo <whluo.china@gmail.com>, Peng Sun <pengsun000@gmail.com>, Fangwei Zhong <zfw@pku.edu.cn>, Wei Liu <wl2223@columbia.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>, Yizhou Wang <yizhou.Wang@pku.edu.cn>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nthat there is no need to handle the camera control during tracking. This fashion is inapplicable to some use-cases, e.g., the tracking performed by a mobile robot with a camera mounted or by a drone. To this end, one should seek a solution of active tracking, which composes two sub-tasks, i.e., the object tracking and the camera control (Fig. 1, Right).\nUnfortunately, it is hard to jointly tune the pipeline with the two separate sub-tasks. The tracking task may also involve many human efforts for bounding box labeling. Moreover, the implementation of camera control is non-trivial and can incur many expensive trial-and-errors happening in realworld. To address these issues, we propose an end-to-end active tracking solution via deep reinforcement learning. To be specific, we adopt a ConvNet-LSTM network, taking as input raw video frames and outputting camera movement actions (e.g., move forward, turn left, etc.).\nWe leverage virtual environments to conveniently simulate active tracking, saving the expensive human labeling or realworld trial-and-error. In a virtual environment, an agent (i.e., the tracker) observes a state (a visual frame) from a first-person perspective and takes an action, and then the environment returns the updated state (next visual frame). We adopt the modern Reinforcement Learning (RL) algorithm A3C (Mnih et al., 2016) to train the agent, where a customized reward function is designed to encourage the\nagent to be closely following the object.\nWe also adopt an environment augmentation technique to boost the tracker’s generalization ability. For this purpose, much engineering is devoted to preparing various environments in different object appearances, different backgrounds, and different object trajectories. We manage this by either using a simulator’s plug-in or developing specific APIs to communicate with a simulator engine. See Sec. 3.1.\nTo our slight surprise, the trained tracker shows good generalization capability. In testing, it performs robust active tracking in the case of unseen object movement path, unseen object appearance, unseen background, and distracting object. Additionally, the tracker can restore tracking when it occasionally loses the target due to, e.g., abrupt object movement.\nIn our experiments, the proposed tracking approach also outperforms a few representative conventional passive trackers which are equipped with a hand-tuned camera-control module. While we are not pursuing a state-of-the-art passive tracker in this work, the experimental results do show that a passive tracker is not indispensable in active tracking. Alternatively, a direct end-to-end solution can be effective. As far as we know, there has not yet been any attempt to deal with active tracking in an end-to-end manner.\nFinally, we perform qualitative evaluation on some video clips taken from the VOT dataset (Kristan et al., 2016). The results show that the tracking ability, obtained purely from simulators, can potentially transfer to real-world scenarios."
  }, {
    "heading": "2. Related Work",
    "text": "Object Tracking. Roughly, object tracking (Wu et al., 2013) is conducted in both passive and active ways. As mentioned in Sec. 1, passive object tracking has gained more attention due to its relatively simpler problem settings. In recent decades, passive object tracking has achieved a great progress (Wu et al., 2013). Many approaches (Hu et al., 2012) have been proposed to overcome difficulties resulted from the issues such as occlusion and illumination variations. In (Ross et al., 2008) subspace learning was adopted to update the appearance model of an object and integrated into a particle filter framework for object tracking. Babenko et al. (Babenko et al., 2009) employed multiple instance learning to track an object. Correlation filter based object tracking (Valmadre et al., 2017; Choi et al., 2017b) has also achieved a success in real-time object tracking (Bolme et al., 2010; Henriques et al., 2015). In (Hare et al., 2016), structured output prediction was used to constrain object tracking, avoiding converting positions to labels of training samples. In (Kalal et al., 2012), Tracking, Learning and Detection (TLD) were integrated into one framework for long-term tracking, where a detection module\ncan re-initialize the tracker once a missing object reappears. Recent years have witnessed the success of deep learning in object tracking (Wang et al., 2016; Bertinetto et al., 2016b). For instance, a stacked autoencoder was trained to learn good representations for object tracking in (Wang & Yeung, 2013). Both low-level and high-level representations were adopted to gain both accuracy and robustness (Ma et al., 2015).\nActive object tracking additionally considers camera control compared with traditional object tracking. There exists not much research attention in the area of active tracking. Conventional solutions dealt with object tracking and camera control in separate components (Denzler & Paulus, 1994; Murray & Basu, 1994; Kim et al., 2005), but these solutions are difficult to tune. Our proposal is completely different from them as it tackles object tracking and camera control simultaneously in an end-to-end manner.\nReinforcement Learning. Reinforcement Learning (RL) (Sutton & Barto, 1998) intends for a principled approach to temporal decision making problems. In a typical RL framework, an agent learns from the environment a policy function that maps state to action at each discrete time step, where the objective is to maximize the accumulated rewards returned by the environment. Historically, RL has been successfully applied to inventory management, path planning, game playing, etc.\nOn the other hand, the past half decade has witnessed a breakthrough in deep learning applied to computer vision tasks, including image classification (Krizhevsky et al., 2012), segmentation (Long et al., 2015), object detection and localization (Girshick et al., 2014), and so on. In particular, researchers believe that deep Convolutional Neural Networks (ConvNets) can learn good features from raw image pixels, which is able to benefit higher-level tasks.\nEquipped with deep ConvNets, RL also shows impressive successes on those tasks involving image (-like) raw states, e.g., playing board game GO (Silver et al., 2016) and video game (Mnih et al., 2015; Wu & Tian, 2017). Recently, in the computer vision community there are also preliminary attempts of applying deep RL to traditional tasks, e.g., object localization (Caicedo & Lazebnik, 2015) and region proposal (Jie et al., 2016). There are also methods of visual tracking relying on RL (Choi et al., 2017a; Huang et al., 2017; Supancic & Ramanan, 2017; Yun et al., 2017). However, they are distinct from our work, as they formulate passive tracking with RL but have nothing to do with camera control. While our focus in this work is active tracking."
  }, {
    "heading": "3. Our Approach",
    "text": "In our approach, virtual tracking scenes are generated for both training and testing. To train the tracker, we employ\na state-of-the-art reinforcement learning algorithm, A3C (Mnih et al., 2016). For the sake of robust and effective training, we also propose data augmentation techniques and a customized reward function, which are elaborated later.\nAlthough various types of states are available, for a research purpose we let the state be only an RGB screen frame of the first-person perspective in this study. To be more specific, the tracker observes the raw visual state and takes one action from the action set A = {turn-left, turn-right, turn-left-andmove-forward, turn-right-and-move-forward, move-forward, no-op}. The action is processed by the environment, which returns to the agent the updated screen frame as well as the current reward."
  }, {
    "heading": "3.1. Tracking Scenarios",
    "text": "It is impossible to train the desired end-to-end active tracker in real-world scenarios. Thus, we adopt two types of virtual environments for simulated training.\nViZDoom. ViZDoom (Kempka et al., 2016; ViZ) is an RL research platform based on a 3D FPS video game called Doom. In ViZDoom, the game engine corresponds to the environment, while the video game player corresponds to the agent. The agent receives from the environment a state and a reward at each time step. In this study, we make customized ViZDoom maps (see Fig. 4) composed of an object (a monster) and background (ceiling, floor, and wall). The monster walks along a pre-specified path programmed by the ACS script (Kempka et al., 2016), and our goal is to train the agent, i.e., the tracker, to follow closely the object.\nUnreal Engine. Though convenient for research, ViZDoom does not provide realistic scenarios. To this end, we adopt Unreal Engine (UE) (unr) to construct nearly real-world environments. UE is a popular game engine and has a broad influence in the game industry. It provides realistic scenarios which can mimic real-world scenes (please see exemplar images in Fig. 5 and videos in our supplementary materials). We employ UnrealCV (Qiu et al., 2017), which provides convenient APIs, along with a wrapper (Zhong et al., 2017) compatible with OpenAI Gym (Brockman et al., 2016), for interactions between RL algorithms and the environments constructed based on UE."
  }, {
    "heading": "3.2. A3C Algorithm",
    "text": "Following (Mnih et al., 2016), we adopt a popular RL algorithm called Actor-Critic. At time step t, we denote by st the observed state, which corresponds to a raw RGB frame. The action set is denoted by A of size K = |A|. An action, at ∈ A, is drawn from a policy function distribution: at v π(·|st) ∈ RK , referred to as an Actor. The environment then returns a reward rt ∈ R according to a reward function rt = g(st), which will be characterized in Sec. 3.4. The updated state st+1 at next time step t+ 1 is subject to a certain but unknown state transition function st+1 = f(st, at), governed by the environment. In this way, we can observe a trace consisting of a sequence of tuplets τ = {. . . , (st, at, rt) , (st+1, at+1, rt+1) , . . .}. Meanwhile, we denote by V (st) ∈ R the expected accumulated reward in the future given state st (referred to as Critic).\nThe policy function π (·) and the value function V (·) are then jointly modeled by a neural network, as will be discussed in Sec. 3.3. Rewriting them as π(·|st; θ) and V (st; θ\n′) with parameters θ and θ′, respectively, we can learn θ and θ′ over the trace τ with simultaneous stochastic policy gradient and value function regression:\nθ ← θ + α ( Rt − V (st) ) ∇θ log π (at|st) + β∇θH ( π (·|st) ) ,\n(1)\nθ′ ← θ′ − α∇θ′ 1\n2\n( Rt − V (st) )2 , (2)\nwhereRt = ∑t+T−1 t′=t γ t′−trt′ is a discounted sum of future rewards up to T time steps with factor 0 < γ ≤ 1, α is the learning rate, H (·) is an entropy regularizer, and β is the regularizer factor.\nDuring training, several threads are launched, each maintaining an independent environment-agent interaction. However, the network parameters are shared across the threads and updated every T time steps asynchronously in a lock-free manner using Eq. (1) in each thread. This kind of manythread training is reported to be fast yet stable, leading to improved generalization (Mnih et al., 2016). Later in Sec. 3.5, we will introduce environment augmentation techniques to further improve the generalization ability."
  }, {
    "heading": "3.3. Network Architecture",
    "text": "The tracker is a ConvNet-LSTM neural network as shown in Fig. 2, where the architecture specification is given in the following table. The FC6 and FC1 correspond to the 6-action policy π (·|st) and the value V (st), respectively. The screen is resized to 84 × 84 × 3 RGB image as the network input.\nLayer# 1 2 3 4 5\nParameters C8×8-16S4 C4×4-32S2 FC256 LSTM256 FC6FC1"
  }, {
    "heading": "3.4. Reward Function",
    "text": "To perform active tracking, it is a natural intuition that the reward function should encourage the agent to closely follow the object. In this line of thought, firstly we define a twodimensional local coordinate system, denoted by S (see Fig. 3). The x-axis points from the agent’s left shoulder to right shoulder, and the y-axis is perpendicular to the x-axis and points to the agent’s front. The origin is where the agent is. System S is parallel to the floor. Secondly, we manage to obtain object’s local coordinate (x, y) and orientation a (in radius) with regard to system S.\nWith a slight abuse of notation, we can now write the reward function as\nr = A−\n(√ x2 + (y − d)2\nc + λ|a|\n) , (3)\nwhere A > 0, c > 0, d > 0, λ > 0 are tuning parameters. In plain English, Eq. (3) says that the maximum reward A is achieved when the object stands perfectly in front of the agent with a distance d and exhibits no rotation (see Fig. 3).\nIn Eq. (3) we have omitted the time step subscript t without loss of clarity. Also note that the reward function defined in this way does not explicitly depend on the raw visual state. Instead, it depends on certain internal states. Thanks to the APIs provided by virtual environments, we are able to access the interested internal states and develop the desired\nreward function."
  }, {
    "heading": "3.5. Environment Augmentation",
    "text": "To make the tracker generalize well, we propose simple yet effective techniques for environment augmentation during training.\nFor ViZDoom, recall the object’s local position and orientation (x, y, a) in system S described in Sec. 3.4. For a given environment (i.e., a ViZDoom map) with initial (x, y, a), we randomly perturb it N times by editing the map with the ACS script (Kempka et al., 2016), yielding a set of environments with varied initial positions and orientations {xi, yi, ai}Ni=1. We further allow flipping left-right the screen frame (and accordingly the left-right action). As a result, we obtain 2N environments out of one environment. See Fig. 3 for an illustration of several possible initial positions and orientations in the local system S. During the A3C training, we uniformly randomly sample one of the 2N environments at the beginning of every episode. As will be seen in Sec. 4.2, this technique significantly improves the generalization ability of the tracker.\nFor UE, we construct an environment with a character/target walking following a fixed path. To augment the environment, we randomly choose some background objects (e.g., tree or building) in the environment and make them invisible. At the same time, every episode starts from the position, where the agent fails at the last episode. This makes the environment and the starting point different from episode to episode, so the variations of the environment during training are augmented."
  }, {
    "heading": "4. Experimental Results",
    "text": "The settings are described in Sec. 4.1. The experimental results are reported for the virtual environments ViZDoom\n(Sec. 4.2) and UE (Sec. 4.3). Qualitative evaluation is performed for real-world scenarios taken from the VOT dataset (Sec. 4.4). To investigate what the tracker has learned, we conduct ablation analysis using a saliency visualization technique (Simonyan et al., 2013) in Sec. 4.5."
  }, {
    "heading": "4.1. Settings",
    "text": "Environment. A set of environments are produced for both training and testing. For ViZDoom, we adopt a training map as in Fig. 4, left column. This map is then augmented as described in Sec. 3.5 with N = 21, leading to 42 environments that we can sample from during training. For testing, we make other 9 maps, some of which are shown in Fig. 4, middle and right columns. In all maps, the path of the target is pre-specified, indicated by the blue lines. However, it is worth noting that the object does not strictly follow the planned path. Instead, it sometimes randomly moves in a “zig-zag” way during the course, which is a built-in game engine behavior. This poses an additional difficulty to the tracking problem.\nFor UE, we generate an environment named Square with random invisible background objects and a target named Stefani walking along a fixed path for training. For testing, we make another four environments named as Square1StefaniPath1 (S1SP1), Square1MalcomPath1 (S1MP1), Square1StefaniPath2 (S1SP2), and Square2MalcomPath2 (S2MP2). As shown in Fig. 5, Square1 and Square2 are two different maps, Stefani and Malcom are two characters/targets, and Path1 and Path2 are different paths. Note that, the training environment Square is generated by hiding some background objects in Square1.\nFor both ViZDoom and UE, we terminate an episdoe when either the accumulated reward drops below a threshold or the episode length reaches a maximum number. In our experiments, we let the reward threshold be -450 and the maximum length be 3000, respectively.\nMetric. Two metrics are employed for the experiments. Specifically, Accumulated Reward (AR) and Episode Length (EL) of each episode are calculated for quantitative evaluation. Note that, the immediate reward defined in Eq. (3) measures the goodness of tracking at some time step, so the metric AR is conceptually much like Precision in the conventional tracking literature. Also note that too small AR means a failure of tracking and leads to a termination of the current episode. As such, the metric EL roughly measures the duration of good tracking, which shares the same spirit of the metric Successfully Tracked Frames in conventional tracking applications. When letting A = 1.0 in Eq. (3), we have that the theoretically maximum AR and EL are both 3000 due to our episode termination criterion. In all the following experiments, 100 episodes are run to report the mean and standard deviation, unless specified otherwise.\nImplementation details. We include the implementation details in the supplementary material due to the space constraint."
  }, {
    "heading": "4.2. Active Tracking in The ViZDoom Environment",
    "text": "We firstly test the active tracker in a testing environment named Standard, showing the effectiveness of the proposed environment augmentation technique. The second part is contributed to the experiments in more challenging testing environments which vary from the Standard environment with regard to object appearance, background, path, and object distraction. Comparison with a set of traditional trackers is conducted in the last part.\nStandard Testing Environment. In Tab. 1, we report the results in an independent testing environment named Standard (see supplementary materials for its detailed description), where we compare two training protocols: with (called RandomizedEnv) or without (called SingleEnv) the augmentation technique as in Sec. 3.5. As can be seen, RandomizedEnv performs significantly better than SingleEnv.\nWe discover that the SingleEnv protocol quickly exhausts\n#1360 #1361 #1372 #1376 #1395 #1410\nFigure 6. Recovering tracking when the target disappears in the SharpTurn environment.\nthe training capability and obtains the best validation result at about 9 × 106 training iterations. On the contrary, the best validation result of RandomizedEnv protocol occurs at 48 × 106, showing that the capacity of the network is exploited better despite the longer training time. In the following experiments, we only report experimental results with the RandomizedEnv protocol.\nVarious Testing Environments. To evaluate the generalization ability of our active tracker, we test it in 8 more challenging environments as in Tab. 2. Comparing to the training environment, they present different target appearances, different backgrounds, more varying paths, and distracting targets. See supplementary materials for the detailed description.\nFrom the 4 categories in Tab. 2 we have findings below. 1) The tracker generalizes well in the case of target appearance changing (Zombie, Cacodemon). 2) The tracker is insensitive to background variations such as changing the ceiling and floor (FloorCeiling) or placing additional walls in the map (Corridor). 3) The tracker does not lose a target even when the target takes several sharp turns (SharpTurn). Note that in conventional tracking, the target is commonly assumed to move smoothly. We also observe that the tracker can recover tracking when it accidentally loses the target. As shown in Fig. 6, the target turns right suddenly and the tracker loses it (frame #1372). Although the target completely disappears in the image, the tracker takes a series of turn-right actions (frame #1376 to #1394). It rediscovers the target (frame #1410), and continues to track steadily afterwards. We believe that this capability attributes to the LSTM unit which takes into account historical states when producing current outputs. Our tracker performs well when the target walks counterclockwise (Counterclockwise), indicating that the tracker does not work by simply memorizing the turning pattern. 4) The tracker is insensitive to a distracting object (Noise1), even when the “bait” is very close to the path (Noise2).\nThe proposed tracker shows satisfactory generalization in various unseen environments. Readers are encouraged to\nwatch more result videos provided in our supplementary materials.\nComparison with Simulated Active Trackers. In a more extensive experiment we compare the proposed tracker with a few traditional trackers. These trackers are originally developed for passive tracking applications. Particularly, the MIL (Babenko et al., 2009), Meanshift (Comaniciu et al., 2000), KCF (Henriques et al., 2015), and Correlation (Danelljan et al., 2014) trackers are employed for comparison. We implement them by directly invoking the interface from OpenCV (Ope) (MIL, KCF and Meanshift trackers) and Dlib (Dli) (Correlation tracker).\nTo make the comparison feasible, we add to the passive tracker an additional PID-like module for the camera control, enabling it to interact with the environment (see Fig. 1, Right). In the first frame, a manual bounding box must be given to indicate the object to be tracked. For each subsequent frame, the passive tracker then predicts a bounding box, which is passed to the “Camera Control” module. Finally, the action is produced by “pulling back” the target to its position in a previous frame (see supplementary materials for the details of the implementation). For a fair comparison\nwith the proposed active tracker, we employ the same action set A as described in Sec. 3.\nArmed with this camera-control module, the performance of traditional trackers is compared with the active tracker in Standard, SharpTurn and Cacodemon. The results in Tab. 3 show that the end-to-end active tracker beats the simulated “active” trackers by a significant gap. We investigate the tracking process of these trackers and find that they lose the target soon. The Meanshift tracker works well when there is no camera shift between continuous frames, while in the active tracking scenario it loses the target soon. Both KCF and Correlation trackers seem not capable of handling such a large camera shift, so they do not work as well as the case in passive tracking. The MIL tracker works reasonably in the active case, while it easily drifts when the object turns suddenly.\nRecalling Fig. 6, another reason of our tracker beating the traditional trackers is that our tracker can quickly discover the target again in the case that it is missed. While the simulated active trackers can hardly recover from failure cases."
  }, {
    "heading": "4.3. Active Tracking in The UE Environment",
    "text": "We firstly compare models trained with randomized environment and single environment. Then we test our active tracker in four environments and also compare it against\ntraditional trackers.\nRandomizedEnv versus SingleEnv. Based on the Square environment, we train two models individually by the RandomizedEnv protocol (random number of invisible background objects and starting point) and SingleEnv protocol (fixed environment). They are tested in the S2MP2 environment, where the map, target, and the path are unseen during training. As shown in Tab. 4, similar results are obtained as those in Tab. 1. We believe that the improvement benefits from the environment randomness brought by the proposed environment augmentation techniques. In the following, we only report the results of RandomizedEnv protocol.\nVarious Testing Environments. To intensively investigate the generalization ability of the active tracker, we test it in four different environments and present the results in Tab. 5. We compare it with the simulated active trackers described in Sec. 4.2, as well as one based on the long-term TLD tracker (Kalal et al., 2012).\nAccording to the results in Tab. 5 we conduct the following analysis: 1) Comparison between S1SP1 and S1MP1 shows that the tracker generalizes well even when the model is trained with target Stefani, revealing that it does not overfit to a specialized appearance. 2) The active tracker performs well when changing the path (S1SP1 versus S1SP2), demonstrating that it does not act by memorizing specialized path. 3) When we change the map, target, and path at the same time (S2MP2), though the tracker could not seize the target as accurately as in previous environments (the AR value drops), it can still track objects robustly (comparable EL value as in previous environments), proving its superior generalization potential. 4) In most cases, the proposed tracker outperforms the simulated active tracker, or achieves compa-\nrable results if it is not the best. The results of the simulated active tracker also suggest that it is difficult to tune a unified camera-control module for them, even when a long term tracker is adopted (see the results of TLD). However, our work exactly sidesteps this issue by training an end-to-end active tracker."
  }, {
    "heading": "4.4. Active Tracking in Real-world Scenarios",
    "text": "To evaluate how the active tracker performs in real-world scenarios, we take the network trained in a UE environment and test it on a few video clips from the VOT dataset (Kristan et al., 2016). Obviously, we can by no means control the camera action for a recorded video. However, we can feed in the video frame sequentially and observe the output action predicted by the network, checking whether it is consistent with the actual situation.\nFig. 7 shows the output actions for two video clips named Woman and Sphere, respectively. The horizontal axis indicates the position of the target in the image, with a positive (negative) value meaning that a target in the right (left) part. The vertical axis indicates the size of the target, i.e., the area of the ground truth bounding box. Green and red dots indicate turn-left/turn-left-and-move-forward and turn-right/turn-right-and-move-forward actions, respectively. Yellow dots represent No-op action. As the figure\nshow, 1) When the target resides in the right (left) side, the tracker tends to turn right (left), trying to move the camera to “pull” the target to the center. 2) When the target size becomes bigger, which probably indicates that the tracker is too close to the target, the tracker outputs no-op actions more often, intending to stop and wait the target to move farther.\nWe believe that the qualitative evaluation shows evidence that the active tracker, learned from purely the virtual environment, is able to output correct actions for camera control in real-world scenarios. Due to the constraint of space, we include more results of the real-world scenarios in the supplementary materials."
  }, {
    "heading": "4.5. Action Saliency Map",
    "text": "We are curious about what the tracker has learned so that it leads to good performance. To this end, we follow the method in (Simonyan et al., 2013) to generate a saliency map of the input image with regard to a specific action. Making it more specific, an input frame si is fed into the tracker and forwarded to output the policy function. An action ai will be sampled subsequently. Then the gradient of ai with regard to si is propagated backwards to the input layer, and a saliency map is generated. This process calculates exactly which part of the original input image influences the corresponding action with the greatest magnitude.\nNote that the saliency map is image specific, i.e., for each input image a corresponding saliency map can be derived. Consequently, we can observe how the input images influence the tracker’s actions. Fig. 8 shows a few pairs of input image and corresponding saliency map. The saliency maps consistently show that the pixels corresponding to the object dominate the importance to actions of the tracker. It indicates that the tracker indeed learns how to find the target."
  }, {
    "heading": "5. Conclusion",
    "text": "We proposed an end-to-end active tracker via deep reinforcement learning. Unlike conventional passive trackers, the proposed tracker is trained in simulators, saving the efforts of human labeling or trail-and-errors in real-world. It shows good generalization to unseen environments. The tracking ability can potentially transfer to real-world scenarios."
  }, {
    "heading": "ACKNOWLEDGEMENT",
    "text": "We appreciate the anonymous ICML reviews that improve the quality of this paper. Thank Jia Xu for his helpful discussion. Fangwei Zhong and Yizhou Wang were supported in part by the following grants 973-2015CB351800, NSFC61625201, NSFC-61527804."
  }],
  "year": 2018,
  "references": [{
    "title": "Visual tracking with online multiple instance learning",
    "authors": ["Babenko", "Boris", "Yang", "Ming-Hsuan", "Belongie", "Serge"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2009
  }, {
    "title": "Staple: Complementary learners for realtime tracking",
    "authors": ["Bertinetto", "Luca", "Valmadre", "Jack", "Golodetz", "Stuart", "Miksik", "Ondrej", "Torr", "Philip HS"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Fully-convolutional siamese networks for object tracking",
    "authors": ["Bertinetto", "Luca", "Valmadre", "Jack", "Henriques", "Joao F", "Vedaldi", "Andrea", "Torr", "Philip HS"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "Visual object tracking using adaptive correlation filters",
    "authors": ["Bolme", "David S", "Beveridge", "J Ross", "Draper", "Bruce A", "Lui", "Yui Man"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2010
  }, {
    "title": "Active object localization with deep reinforcement learning",
    "authors": ["Caicedo", "Juan C", "Lazebnik", "Svetlana"],
    "venue": "In International Conference on Computer Vision, pp",
    "year": 2015
  }, {
    "title": "Visual tracking by reinforced decision making",
    "authors": ["Choi", "Janghoon", "Kwon", "Junseok", "Lee", "Kyoung Mu"],
    "venue": "arXiv preprint arXiv:1702.06291,",
    "year": 2017
  }, {
    "title": "Attentional correlation filter network for adaptive visual tracking",
    "authors": ["Choi", "Jongwon", "Jin Chang", "Hyung", "Yun", "Sangdoo", "Fischer", "Tobias", "Demiris", "Yiannis", "Young Choi", "Jin"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Realtime tracking of non-rigid objects using mean shift",
    "authors": ["Comaniciu", "Dorin", "Ramesh", "Visvanathan", "Meer", "Peter"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2000
  }, {
    "title": "Recurrently target-attending tracking",
    "authors": ["Cui", "Zhen", "Xiao", "Shengtao", "Feng", "Jiashi", "Yan", "Shuicheng"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Accurate scale estimation for robust visual tracking",
    "authors": ["Danelljan", "Martin", "Häger", "Gustav", "Khan", "Fahad", "Felsberg", "Michael"],
    "venue": "In British Machine Vision Conference,",
    "year": 2014
  }, {
    "title": "Eco: Efficient convolution operators for tracking",
    "authors": ["Danelljan", "Martin", "Bhat", "Goutam", "Shahbaz Khan", "Fahad", "Felsberg", "Michael"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Active motion detection and object tracking",
    "authors": ["Denzler", "Joachim", "Paulus", "Dietrich WR"],
    "venue": "In International Conference on Image Processing,",
    "year": 1994
  }, {
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
    "authors": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2014
  }, {
    "title": "Struck: Structured output tracking with kernels",
    "authors": ["Hare", "Sam", "Golodetz", "Stuart", "Saffari", "Amir", "Vineet", "Vibhav", "Cheng", "Ming-Ming", "Hicks", "Stephen L", "Torr", "Philip HS"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2016
  }, {
    "title": "High-speed tracking with kernelized correlation filters",
    "authors": ["Henriques", "João F", "Caseiro", "Rui", "Martins", "Pedro", "Batista", "Jorge"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2015
  }, {
    "title": "Single and multiple object tracking using log-euclidean riemannian subspace and block-division appearance model",
    "authors": ["Hu", "Weiming", "Li", "Xi", "Luo", "Wenhan", "Zhang", "Xiaoqin", "Maybank", "Stephen", "Zhongfei"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 2012
  }, {
    "title": "Learning policies for adaptive tracking with deep feature cascades",
    "authors": ["Huang", "Chen", "Lucey", "Simon", "Ramanan", "Deva"],
    "venue": "In International Conference on Computer Vision,",
    "year": 2017
  }, {
    "title": "Tree-structured reinforcement learning for sequential object localization",
    "authors": ["Jie", "Zequn", "Liang", "Xiaodan", "Feng", "Jiashi", "Jin", "Xiaojie", "Lu", "Wen", "Yan", "Shuicheng"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning",
    "authors": ["Kempka", "Michał", "Wydmuch", "Marek", "Runc", "Grzegorz", "Toczek", "Jakub", "Jaśkowski", "Wojciech"],
    "year": 2016
  }, {
    "title": "Detecting and tracking moving object using an active camera",
    "authors": ["Kim", "Kye Kyung", "Cho", "Soo Hyun", "Hae Jin", "Lee", "Jae Yeon"],
    "venue": "In International Conference on Advanced Communication Technology,",
    "year": 2005
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2012
  }, {
    "title": "Fully convolutional networks for semantic segmentation",
    "authors": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Hierarchical convolutional features for visual tracking",
    "authors": ["Ma", "Chao", "Huang", "Jia-Bin", "Yang", "Xiaokang", "MingHsuan"],
    "venue": "In International Conference on Computer Vision,",
    "year": 2015
  }, {
    "title": "Human-level control through deep reinforcement learning",
    "authors": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"],
    "venue": "Nature, 518(7540):529–533,",
    "year": 2015
  }, {
    "title": "Asynchronous methods for deep reinforcement learning",
    "authors": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Motion tracking with an active camera",
    "authors": ["Murray", "Don", "Basu", "Anup"],
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
    "year": 1994
  }, {
    "title": "Unrealcv: Virtual worlds for computer vision",
    "authors": ["Qiu", "Weichao", "Zhong", "Fangwei", "Zhang", "Yi", "Qiao", "Siyuan", "Xiao", "Zihao", "Kim", "Tae Soo", "Wang", "Yizhou Wang", "Yuille", "Alan"],
    "venue": "ACM Multimedia Open Source Software Competition,",
    "year": 2017
  }, {
    "title": "Incremental learning for robust visual tracking",
    "authors": ["Ross", "David A", "Lim", "Jongwoo", "Lin", "Ruei-Sung", "Yang", "MingHsuan"],
    "venue": "International Journal of Computer Vision,",
    "year": 2008
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"],
    "venue": "search. Nature,",
    "year": 2016
  }, {
    "title": "Deep inside convolutional networks: Visualising image classification models and saliency",
    "authors": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"],
    "venue": "maps. International Conference on Learning Representations,",
    "year": 2013
  }, {
    "title": "Tracking as online decision-making: Learning a policy from streaming videos with reinforcement learning",
    "authors": ["III Supancic", "James", "Ramanan", "Deva"],
    "venue": "In International Conference on Computer Vision,",
    "year": 2017
  }, {
    "title": "Introduction to Reinforcement Learning",
    "authors": ["Sutton", "Richard S", "Barto", "Andrew G"],
    "year": 1998
  }, {
    "title": "End-to-end representation learning for correlation filter based tracking",
    "authors": ["Valmadre", "Jack", "Bertinetto", "Luca", "Henriques", "Joao", "Vedaldi", "Andrea", "Torr", "Philip H. S"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Stct: Sequentially training convolutional networks for visual tracking",
    "authors": ["Wang", "Lijun", "Ouyang", "Wanli", "Xiaogang", "Lu", "Huchuan"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Learning a deep compact image representation for visual tracking",
    "authors": ["Wang", "Naiyan", "Yeung", "Dit-Yan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Online object tracking: A benchmark",
    "authors": ["Wu", "Yi", "Lim", "Jongwoo", "Yang", "Ming-Hsuan"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2013
  }, {
    "title": "Training agent for first-person shooter game with actor-critic curriculum learning",
    "authors": ["Wu", "Yuxin", "Tian", "Yuandong"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2017
  }, {
    "title": "Action-decision networks for visual tracking with deep reinforcement learning",
    "authors": ["Yun", "Sangdoo", "Choi", "Jongwon", "Yoo", "Youngjoon", "Kimin", "Young Choi", "Jin"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Gym-unrealcv: Realistic virtual worlds for visual reinforcement learning, 2017",
    "authors": ["Zhong", "Fangwei", "Qiu", "Weichao", "Yan", "Tingyun", "Alan", "Yuille", "Wang", "Yizhou"],
    "year": 2017
  }, {
    "title": "Beyond local search: Tracking objects everywhere with instance-specific proposals",
    "authors": ["Zhu", "Gao", "Porikli", "Fatih", "Li", "Hongdong"],
    "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }],
  "id": "SP:5a0485b6f4f80d613395139f4400bc7c3b511846",
  "authors": [{
    "name": "Wenhan Luo",
    "affiliations": []
  }, {
    "name": "Peng Sun",
    "affiliations": []
  }, {
    "name": "Fangwei Zhong",
    "affiliations": []
  }, {
    "name": "Wei Liu",
    "affiliations": []
  }, {
    "name": "Tong Zhang",
    "affiliations": []
  }, {
    "name": "Yizhou Wang",
    "affiliations": []
  }],
  "abstractText": "We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc.). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in realworld. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-toaction prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.",
  "title": "End-to-end Active Object Tracking via Reinforcement Learning"
}