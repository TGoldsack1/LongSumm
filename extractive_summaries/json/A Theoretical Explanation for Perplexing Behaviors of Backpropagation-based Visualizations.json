{
  "sections": [{
    "text": "proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis."
  }, {
    "heading": "1. Introduction",
    "text": "Driven by massive data and computational resources, modern convolutional neural networks (CNNs) and other network architectures have achieved many outstanding results, such as image recognition (Krizhevsky et al., 2012), neural machine translation (Sutskever et al., 2014), and playing Go games (Silver et al., 2016), etc. Despite their extensive applications, these neural networks are always considered as black boxes. Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions. The demand for explainable artificial intelligence (XAI) (Gunning, 2017) – human interpretable explanations of model decisions – has driven the development of visualization techniques, including image synthesis via activation\n1Department of Electrical and Computer Engineering, Rice University, Houston, USA. 2Department of Computer Science, Rice University, Houston, USA. 3Department of Neuroscience, Baylor College of Medicine, Houston, USA. Correspondence to: Weili Nie <wn8@rice.edu>, Ankit B. Patel <abp4@rice.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmaximization (Simonyan et al., 2013; Johnson et al., 2016; Nguyen et al., 2016) and backpropagation-based visualizations (Simonyan et al., 2013; Zeiler & Fergus, 2014; Springenberg et al., 2014; Shrikumar et al., 2017; Kindermans et al., 2017).\nThe basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space. The intensity changes of these pixels have the most significant impact on network decisions. Specifically, (Simonyan et al., 2013) visualizes the spatial support of a given class in a given image, i.e. saliency map, by using the true gradient which masks out negative entries of bottom data via the forward ReLU. Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult. (Zeiler & Fergus, 2014) visualize the reverse mapping from feature activities back to the input pixel space with the deconvolutional network (DeconvNet) method. The basic idea of DeconvNet is to mask out negative entries of the top gradients by resorting to the backward ReLU. (Springenberg et al., 2014) proposed the Guided Backpropagation (GBP) method which combines the above two methods: by considering both the forward and backward ReLUs, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations. More recently, DeepLift (Shrikumar et al., 2017) and PatternNet (Kindermans et al., 2017) have been proposed to further improve the visual quality of backpropagation-based methods.\nThis class of backpropagation-based visualizations, in particular GBP and DeconvNet, has attracted a lot of attention in both the deep learning community and other fields (Szegedy et al., 2013; Dosovitskiy & Brox, 2016; Selvaraju et al., 2016; Fong & Vedaldi, 2017; Kraus et al., 2016). Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored. Do the pretty visualizations actually tell us reliably about what the network is doing internally? Our experiments have confirmed previous observations (Mahendran & Vedaldi, 2016; Selvaraju et al., 2016; Samek et al., 2017) that saliency map is indeed very sensitive to the change of class labels, while GBP and DeconvNet, though their visualization results are much cleaner than saliency map, remain almost the same given different class labels. It seems that\nthe visual quality improvement of backpropagation-based methods is sacrificing the ability of highlighting important pixels to a specific output class. In this sense, GBP and DeconvNet may be unreliable in interpreting how deep neural networks make classification decisions.\nThe most commonly used explanation for these visualizations is to approximate the neural networks with a linear function (Simonyan et al., 2013; Kindermans et al., 2017), where the derivative of output with respect to input image is just the weight vector of the model. In such sense, the backpropagation-based methods can be regarded as visualizing the learned weights. But apparently the approximate linear model is too simplistic to reflect the highly nonlinear property of deep neural networks. For example, GBP and DeconvNet essentially apply the same algorithm as saliency map, but treat ReLU, the nonlinear activation, differently. The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model. Therefore, we need a more complex model, which should at least capture the impact of both forward ReLU and backward ReLU, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations.\nOur contributions. We provide a theoretical explanation for why GBP and DeconvNet generate more humaninterpretable but less class-sensitive visualizations than saliency map. Specifically, our analysis reveals that GBP and DeconvNet are essentially doing (partial) image recovery instead of highlighting class-relevant pixels or visualizing the learned weights, which means in principle they are unrelated to the decision-making of neural networks. We also find that it is the backward ReLU introduced by either GBP or DeconvNet, together with the local connections in CNNs that results in crisp visualizations. In particular, we explain how DeconvNet also relies on the max-pooling to recover the input. Finally, we do extensive experiments to support our theory and further reveal more detailed properties of these backpropagation-based visualizations1."
  }, {
    "heading": "2. Backpropagation-based Visualizations",
    "text": "In this section, we first give formal definitions of backpropagation-based visualizations: saliency map, DeconvNet and GBP, and then compare their empirical behaviors."
  }, {
    "heading": "2.1. Formal Definitions",
    "text": "The key difference of backpropagation-based methods is the way they propagate the output score back through the\n1Code is available at https://github.com/weilinie/BackpropVis\nReLU activations. As illustrated by Figure 1, we consider the i-th ReLU activation in the l-th layer with its input y (l) i and its output o (l) i and denote by σ(t) = max(t, 0) the ReLU activation. Also, denote by R (l) i the top gradient before activation, i.e., gradient of the output score with respect to o (l) i and denote by T (l) i the (modified) gradient after activation, i.e., gradient of the output score with respect to y (l) i . Then in the gradient calculations, the corresponding forward ReLU could be formally defined as a function\nσ (l) f,i(t) , I\n(\ny (l) i\n)\nt\nwhere I(·) is the indicator function and the corresponding backward ReLU could be formally defined as a function\nσ (l) b,i(t) , I\n(\nR (l) i\n)\nt\nTherefore, the formal definition of backpropagation-based methods for propagating the output score back through the i-th ReLU activation in the l-th layer is\nT (l) i =\n\n   \n   \nσ (l) f,i\n(\nR (l) i\n)\nfor saliency map\nσ (l) b,i\n(\nR (l) i\n)\nfor DeconvNet\nσ (l) f,i\n(\nσ (l) b,i\n(\nR (l) i\n))\nfor GBP\nwhich can be further uniformly formulated as\nT (l) i = h\n(\nR (l) i\n) ∂g (\ny (l) i\n)\n∂y (l) i\n(1)\nwhere the two functions h(·) and g(·) are defined as\nh(t) =\n{\nt for saliency map\nσ(t) for DeconvNet and GBP\ng(t) =\n{\nt for DeconvNet\nσ(t) for saliency map and GBP\n(2)"
  }, {
    "heading": "2.2. Empirical Observations",
    "text": "To be a good visualization method, a clean and visually human-interpretable result is very desirable. More importantly, it should also reveal how the neural networks make decisions. Based on this, we provide the empirical behaviors of the backpropagation-based visualizations for a pretrained VGG-16 net (Simonyan & Zisserman, 2014) in Figure 2. Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.\nFor the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out. For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose. This, together with more experiments, suggests that after introducing the backward ReLU, both DeconvNet and GBP modify the true gradient in a way that they create much cleaner results but their functionality as an indicator of important pixels to a specific class has disappeared. In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map."
  }, {
    "heading": "3. Theoretical Explanations",
    "text": "We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs. Besides, we also investigate their behaviors in well-trained CNNs."
  }, {
    "heading": "3.1. A Random Three-Layer CNN",
    "text": "Consider a three-layer CNN, consisting of an input layer and a convolutional hidden layer, followed by a ReLU activation function and a fully connected layer of which its output is called class logits. Formally, let x ∈ Rd be a normalized input image with dimension d and ‖x‖ = 1, and let W ∈ Rp×N be N convolutional filters where each column w(i) denotes the i-th filter with size p. Note that here we use vectors to represent images and filters for simplicity, and the analysis also works for the more practical two-dimensional case. Then, we let Y ∈ Rp×J be J image patches extracted from x, and each column y(j) with size p is generated by a linear function y(j) = Djx where Dj , [ 0p×(j−1)b Ip×p 0p×(d−(j−1)b−p) ] with b being the stride size2. For example, given a filter with size 3 and stride 1, the resulting j-th patch y(j) is made of the j-th to (j + 2)-th consecutive pixels. The weights in the fullyconnected layer can be represented by V ∈ RNJ×K with K being the number of output logits. Therefore, the k-th logit is represented by\nfk(x) =\nN ∑\ni=1\nJ ∑\nj=1\nVqij ,kσ(w (i)T y(j)) (3)\nwhere the index qij denotes the ((i− 1)J + j)-th entry in every column vector of weight matrix V .\nAssume every entry of V and W is sampled from an i.i.d. Gaussian distribution N (0, c2). The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN. Note that the norm of the final results will be in the range of [0, 1] as we apply the normalization during visualizations.\nLemma 1. The backpropagation-based visualizations for the k-th logit in a random three-layer CNN is formalized as\nsk(x) = 1\nZk\nJ ∑\nj=1\nDj T\nN ∑\ni=1\nh(Vqij ,k)w̃ (i,j) (4)\nwhere Zk is the normalization coefficient to ensure ‖sk(x)‖ ∈ [0, 1], h(·) is given by Eq. (2) and\nw̃(i,j) =\n{\nw(i) for DeconvNet w(i)I ( w(i)T y(j) ) for saliency map and GBP\n2Here we assume a VALID padding method implicitly, and other padding methods do not impact our analysis.\nProof. See Appendix A.\nNext, we can analyze the different behaviors of these backpropagation-based methods case by case."
  }, {
    "heading": "3.1.1. GUIDED BACKPROPAGATION",
    "text": "First, the behavior of GBP is given as follows.\nTheorem 1. In a random three-layer CNN, if the number of filters N is sufficiently large, GBP at the k-th logit can be approximated as\nsGBPk (x) ≈ x (5)\nProof. See Appendix B.\nThe above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label. However, according to the linear model explanation, backpropagation-based methods are visualizing learned weights, which should be random noise as they are all sampled from i.i.d Gaussians. Obviously, it is inconsistent with the actual behavior of GBP.\nAs the approximation in Eq. (5) builds on an assumption that the number of filters N is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery? From (Lugosi & Mendelson, 2017), we can set N = Õ( p ǫ2 ) such that with high probability ‖ 1 N ∑N i=1 w̃ (i,j) − E[w̃(i,j)]‖ < ǫ, where p denotes the filter size and Õ(·) hides some other factors. As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size p. As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image. For example, given a filter size 3× 3× 3, we need at most O(103) filters to achieve an estimation error ǫ less than 0.1. This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations."
  }, {
    "heading": "3.1.2. SALIENCY MAP AND DECONVNET",
    "text": "Here we show the behaviors of saliency map and DeconvNet in a random three-layer CNN are largely different from GBP.\nTheorem 2. In a random three-layer CNN, if the number of filters N is sufficiently large, saliency map and DeconvNet are approximated as Gaussian random variables satisfying\nsSalk (x), s Deconv k (x) ∼ N (0, I)\nProof. See Appendix C.\nThe above theorem shows that both saliency map and DeconvNet visualizations will yield random noise, conveying\nlittle information about the input image and class logits. For saliency map, it is easily understood since saliency map represents the true gradient of the class logit, which heavily depends on the weights. For DeconvNet, although its behavior appears similar to saliency map in this simplistic scenario, we will show later on that it behaves more similarly to GBP, in particular with the existence of max-pooling."
  }, {
    "heading": "3.2. Extensions to More Realistic Models",
    "text": "In this section, we extend our analysis of a simple random three-layer CNN to other more realistic cases, including the max-pooling, deeper nets and trained weights."
  }, {
    "heading": "3.2.1. CNNS WITH MAX-POOLING",
    "text": "If we add a max-pooling layer between the ReLU and the fully-connected layer, the k-th logit becomes\nfk(x) =\nN ∑\ni=1\nJ ∑\nj=1\nVq̃ij ,kδ(σ(w (i)T y(j)))\nwhere δ(·) denotes the max-pooling, which successively selects the maximum value in a fixed-size pooling window, and the new index q̃ij is the down-sampled version of qij . Then the backpropagation-based visualizations for the k-th logit can be formulated as\nsk(x) = 1\nZk\nJ ∑\nj=1\nDj T\nN ∑\ni=1\nh(δ′(oij)Vq̃ij ,k)w̃ (i,j) (6)\nwhere oij , σ(w (i)T y(j)) is the output of each ReLU activation and δ′(oij) denotes the derivative of δ(·) evaluated at oij , which is\nδ′(oij) =\n{\n1 if oij is chosen by max-pooling 0 otherwise\nSince oij ≥ 0 with equality holds for w (i)T y(j) ≤ 0, given a proper pooling window size, it is highly possible that oij is chosen by the max-pooling if and only if w(i)T y(j) > 0. It means with high probability, Eq. (6) is approximated as\nsk(x) ≈ 1\nZk\nJ ∑\nj=1\nDj T\nN ∑\ni=1\nh(Vq̃ij ,k)w̃ (i,j) I(w(i)T y(j))\n(7)\nFor saliency map and GBP, we know w̃(i,j)I(w(i)T y(j)) = w̃(i,j) and thus Eq. (7) is further reduced to Eq. (4), which means the behaviors of saliency map and GBP remain the same after introducing the max-pooling. However, with high probability, DeconvNet at the k-th logit becomes\nsDeconvk (x) ≈ 1\nZk\nJ ∑\nj=1\nDj T\nN ∑\ni=1\nσ(Vq̃ij ,k)w (i) I(w(i)T y(j))\nA Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations\nwhich is exactly the form of GBP in Eq. (4). Therefore, adding the max-pooling makes the DeconvNet behave like GBP – doing nothing but image recovery. This also explains and extends the previous intuitive claims in (Samek et al., 2017; Odena et al., 2016) that the image-specific information in DeconvNet comes from the max-pooling.\nNote that that the approximation from Eq. (6) to Eq. (7) in DeconvNet with the max-pooling is essentially different from the approximations used in GBP. For GBP, the approximate gap can be made arbitrarily small by increasing the hidden layer size N , leading to a perfect recovery of the input. However, for DeconvNet, given any pooling window size, there might always exist at least one of the following two contradictory cases: it is possible that aij is chosen by the max-pooling if w(i)T y(j) ≤ 0, and also possible that aij is not chosen if w\n(i)T y(j) > 0. This makes DeconvNet (with max-pooling), in theory, never recover input perfectly, which might explain why the unusual texture-like artifacts appear in the DeconvNet visualizations."
  }, {
    "heading": "3.2.2. DEEP CNNS",
    "text": "The analysis for a three-layer CNN can be generalized to the multi-layer (or deeper) case. For clarity, we formulate the k-th logit of an L-layer deep CNN in a matrix form:\nfk(x) = Γ (L)T k σ\n( Γ(L−1)T · · ·σ ( Γ(1)Tx ))\nwhere Γ(l) ∈ Rdl×dl+1 denotes either the convolutional or fully-connected operator matrix in the l-th layer and Γ (L) k is the k-th column of Γ (L). Denote by o(l) the output of ReLU activations in the l-th layer, i.e. o(l) = σ ( Γ(l)T o(l−1) )\n, ∀l ∈ {1, · · · , L−1} with o(0) , x. Then backpropagation-based visualizations at the k-th logit in an L-layer deep CNN can be formulated as\nsk(x) = 1\nZk\n∂õ(1)\n∂x · h(V̂\n(1) ·,k )\n(a) =\n1\nZk\nJ ∑\nj=1\nDj T\nN ∑\ni=1\nh(V̂ (1) qij ,k\n)w̃(i,j) (8)\nwith ∀l ∈ {1, · · · , L− 1},\nV̂ (l) ·,k =\n∂õ(l+1)\n∂o(l) · h\n(\n∂õ(l+2) ∂o(l+1) · · ·h\n(\n∂õ(L−1)\n∂o(L−2) h ( Γ (L) k )\n))\nwhere in (a) we rewrite sk(x) in an expanded form, õ (l) , g ( Γ(l)T o(l−1) )\n, w(i) is the i-th filter encoded in Γ(1) and N is the number of filters in the first convolutional layer. Also, h(·), g(·) and w̃(i,j) are defined in Eq. (2) and Lemma 1.\nFirst, the approximate property of V̂ (1) ·,k in the random deep CNN is given in the following proposition.\nw(i)\ny(j)\nProposition 1. For a random deep CNN where weights are i.i.d. Gaussians with zero mean, we can also approximate every entry of V̂ (1) ·,k as i.i.d. Gaussian with zero mean.\nProof. See Appendix D.\nBased on Proposition 1, we can see that the statistical properties of V̂ (1) qij ,k in Eq. (8) are approximately the same with those of Vqij ,k in Eq. (4), which means the analysis of backpropagation-based visualizations in a shallow threelayer CNN also applies to the deep CNN case. Therefore, the behaviors of these visualizations will barely change when increasing the depth of neural networks."
  }, {
    "heading": "3.2.3. CNNS WITH TRAINED WEIGHTS",
    "text": "The previous analysis for random CNNs does not apply to the trained case directly since the weights here may not be i.i.d. Gaussian distributed. For saliency map, which uses the true gradient, the trained weights are likely to impose a stronger bias towards some specific subset of the input pixels, and so they can highlight class-relevant pixels rather than producing random noise. For GBP and DeconvNet, the analysis is a little more involved.\nOn the one hand, the trained weights w(i) will only lie in a small subspace of the whole image patch space which will create some “dead zones”, as illustrated in Figure 3 (a). That\nis, all image patches lying in the “dead zone” will be filtered out by the forward ReLU. For example, it is well-known that the trained weights in the first convolutional layer are Gabor-like filters to detect the image patches containing edges (Yosinski et al., 2014; Zeiler & Fergus, 2014). That is, image patches without edges will probably be filtered out by the first convolutional layer. Also, the higher convolutional layers keep filtering out more image patches with certain patterns (e.g. Figure 9). See the supplementary material for a comparison between GBP and a linear edge detector.\nOn the other hand, as shown in Figure 3 (b) and (c), the histograms of weights connected to the respective one of any two different neurons in the first fully connected layer (called “fc1”) of the trained VGG-16 net are very similar to each other. Approximately, they form two very similar Gaussians with a small standard deviation, which means the (modified) gradients at any two different neurons in the layer “fc1” with respect to the input image are almost the same. Namely, ∂õ (fc1)\n∂x in Eq. (8) for GBP and DeconvNet\n(with max-pooling) satisfies\n∂õ (fc1) m\n∂x ≈ Fconv(x), ∀m ∈ {1, · · · ,M}\nwhere õ (fc1) m is the m-th entry of õ (fc1) and Fconv(·) : R d → R d denotes the (normalized) overall filtering effect of the convolutional layers and M is the number of neurons in the layer “fc1”. Thus, Eq. (8) for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as\nsk(x) = 1\nZk\n∂õ(fc1)\n∂x · h(V̂\n(fc1) ·,k )\n= 1\nZk\nM ∑\nm=1\n∂õ (fc1) m\n∂x · h(V̂\n(fc1) m,k )\n(a) ≈ Fconv(x)\n(9)\nwhere (a) follows from setting the normalization coefficient to be Zk = 1 ∑\nM m=1 h(V̂ (fc1) m,k\n) .\nIt shows that GBP and DeconvNet (with max-pooling) in a trained CNN are actually doing the partial image recovery, where the trained weights control which image patch could form an active path to the class logit. More importantly, this filtering process is not class sensitive (e.g. the edge detector). In the end, only these “active” image patches are combined in the first fully connected layer to form the final visualization results. As the right side of (9) does not depend on k, it illustrates why the GBP and DecovNet visualizations in the trained VGG are not class-sensitive."
  }, {
    "heading": "4. Experiments",
    "text": "To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully-\nconnected network (FCN) and a VGG-16 net. For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation 0.1. Unless stated otherwise, the input is the image “tabby” from the ImageNet dataset (Deng et al., 2009) with size 224×224×3. See the supplementary materials for more results on other images and other neural network such as ResNet (He et al., 2016). In the three-layer CNN, the filter size is 7× 7× 3, the number of filters is N = 256, and the stride is 2. In the three-layer FCN, the hidden layer size is set to Nh = 4096. By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit."
  }, {
    "heading": "4.1. Impact of Local Connections",
    "text": "Figure 4 shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively. We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section 3.1. In contrast, as local connections do not exist in the FCN and the input size (e.g. 224 × 224 × 3) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise. Particularly for GBP, the number of hidden neurons Nh = 4096 is still not large enough to recover the image.\nTo further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters N in the CNN and the number of hidden neurons Nh in the FCN, respectively, while keep other parameters fixed. The results are given in Figure 5. Note that in the FCN, we have downsampled the input image to be of size 64 × 64 × 3 due to computational limitations. We can see that as the number of filters N increases (resp. the hidden layer size Nh), the vi-\nsual quality of GBP in the CNN (resp. in the FCN) becomes better. Interestingly, even by setting Nh = 70000, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with N = 64. Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP."
  }, {
    "heading": "4.2. Impact of Max-Pooling and Network Depth",
    "text": "To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure 6 (top row). As compared with the visualizations in Figure 4 (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before. It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section 3.2.1.\nTo show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN. Figure 6 (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations. Though there are subtle visual differences between the top row and bottom row of Figure 6, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth. In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG , which is different from the trained VGG in Figure 2 where only those “active” image patches are preserved."
  }, {
    "heading": "4.3. Average l2 Distance Statistics",
    "text": "To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average l2 distance statistics as shown in Figure 7. Our results are obtained by first calculating the l2 distance of two visualization results given two different class logits for each input image and then taking an average of those l2 distances based on 10K images from the ImageNet test set. The process is repeated for all backpropagationbased methods in both random and trained cases. As we can see, the average l2 distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not. Interestingly, in the trained VGG-16 net, the average l2 distance of DeconvNet is slightly larger than that of GBP. It shows that the class insensitivity is exchanged for further improvement of visual quality."
  }, {
    "heading": "4.4. Adversarial Attack on VGG",
    "text": "Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery. The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image. In this experiment, we first generate an adversarial example “busby” via the fast gradient sign method (FGSM) (Goodfellow et al., 2014) by feeding the image “panda” into the pretrained VGG-16 net. Next, we apply the backpropagationbased visualizations to the original image “panda” and its adversary “busby” in the trained VGG-16 net. As shown in Figure 8, the saliency map visualization changes significantly whereas the GBP and DeconvNet visualizations remain almost unchanged after replacing “panda” by its adversary “busby”. Therefore, it further confirms that saliency map is class-sensitive in that it highlights important pixels in making classification decisions. However, GBP and DeconvNet are doing nothing but (partial) image recovery."
  }, {
    "heading": "4.5. VGG with Partly Trained Weights",
    "text": "There exist some differences for backpropagation-based visualizations, GBP and DeconvNet in particular, between the random and trained cases. We take GBP as an example here to investigate the contributions of different layers in the trained VGG-16 net to these visual differences.\nFirst, to isolate the impact of later layers, we load the trained weights up to a given layer and leave later layers randomly initialized. As shown in Figure 9 (top row), from “Conv11*” to “Conv5-1*” GBP keeps filtering out more image patches as the number of trained convolutional layers increases. However, from “Conv5-1*” to “FC3*” (i.e., the\nfully-trained case) GBP behaves almost the same, no matter weights in the dense layers are random or trained. Therefore, it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches. Also, it further confirms that GBP is class-insensitive. Furthermore, to reveal the impact of each layer, we load the trained weights for the whole VGG-16 net except for a given layer which is randomly initialized instead. The results are shown in Figure 9 (bottom row). We can see that the GBP visualization is blurry for “Conv1-1⋄”, clean with much background information for “Conv3-1⋄” and clean without background information for “Conv5-1⋄”, respectively. It means that the earlier convolutional layer has more important impact in the GBP visualization than the later convolutional layer."
  }, {
    "heading": "5. Conclusions",
    "text": "In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases. We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties. We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs, that is responsible for human-interpretable visualizations. We also explained how DeconvNet also relies on the max-pooling to recover the input. Our analysis was supported by extensive experiments. Finally, we hope our analysis can provide useful insights into developing better visualization methods for deep neural networks. A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer."
  }, {
    "heading": "Acknowledgements",
    "text": "Thanks to the anonymous reviewers for useful comments. WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003."
  }],
  "year": 2018,
  "references": [{
    "title": "Imagenet: A large-scale hierarchical image database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"],
    "venue": "In Computer Vision and Pattern Recognition,",
    "year": 2009
  }, {
    "title": "Inverting visual representations with convolutional networks",
    "authors": ["A. Dosovitskiy", "T. Brox"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Interpretable explanations of black boxes by meaningful perturbation",
    "authors": ["R.C. Fong", "A. Vedaldi"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2017
  }, {
    "title": "Explaining and harnessing adversarial examples",
    "authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"],
    "venue": "arXiv preprint arXiv:1412.6572,",
    "year": 2014
  }, {
    "title": "Explainable artificial intelligence (xai)",
    "authors": ["D. Gunning"],
    "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web,",
    "year": 2017
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2016
  }, {
    "title": "Perceptual losses for real-time style transfer and super-resolution",
    "authors": ["J. Johnson", "A. Alahi", "L. Fei-Fei"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "Patternnet and patternlrp–improving the interpretability of neural networks",
    "authors": ["Kindermans", "P.-J", "K.T. Schütt", "M. Alber", "K.R. Müller", "S. Dähne"],
    "venue": "arXiv preprint arXiv:1705.05598,",
    "year": 2017
  }, {
    "title": "Classifying and segmenting microscopy images with deep multiple instance",
    "authors": ["O.Z. Kraus", "J.L. Ba", "B.J. Frey"],
    "venue": "learning. Bioinformatics,",
    "year": 2016
  }, {
    "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "year": 2012
  }, {
    "title": "Sub-gaussian estimators of the mean of a random vector",
    "authors": ["G. Lugosi", "S. Mendelson"],
    "venue": "arXiv preprint arXiv:1702.00482,",
    "year": 2017
  }, {
    "title": "Salient deconvolutional networks",
    "authors": ["A. Mahendran", "A. Vedaldi"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2016
  }, {
    "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
    "authors": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Deconvolution and checkerboard artifacts. Distill, 2016",
    "authors": ["A. Odena", "V. Dumoulin", "C. Olah"],
    "year": 2016
  }, {
    "title": "Evaluating the visualization of what a deep neural network has learned",
    "authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "Müller", "K.-R"],
    "venue": "IEEE transactions on neural networks and learning systems,",
    "year": 2017
  }, {
    "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization",
    "authors": ["R.R. Selvaraju", "A. Das", "R. Vedantam", "M. Cogswell", "D. Parikh", "D. Batra"],
    "venue": "arXiv preprint arXiv:1610.02391,",
    "year": 2016
  }, {
    "title": "Learning important features through propagating activation differences",
    "authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"],
    "venue": "arXiv preprint arXiv:1704.02685,",
    "year": 2017
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"],
    "venue": "search. Nature,",
    "year": 2016
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
    "authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1312.6034,",
    "year": 2013
  }, {
    "title": "Striving for simplicity: The all convolutional net",
    "authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"],
    "venue": "arXiv preprint arXiv:1412.6806,",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks. In Advances in neural information processing",
    "authors": ["I. Sutskever", "O. Vinyals", "Q.V. Le"],
    "year": 2014
  }, {
    "title": "Intriguing properties of neural networks",
    "authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"],
    "venue": "arXiv preprint arXiv:1312.6199,",
    "year": 2013
  }, {
    "title": "How transferable are features in deep neural networks",
    "authors": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Visualizing and understanding convolutional networks",
    "authors": ["M.D. Zeiler", "R. Fergus"],
    "venue": "In European conference on computer vision,",
    "year": 2014
  }],
  "id": "SP:1df011fcb4c0ad43b5d1ce3b4d867ec1be114cd7",
  "authors": [{
    "name": "Weili Nie",
    "affiliations": []
  }, {
    "name": "Yang Zhang",
    "affiliations": []
  }, {
    "name": "Ankit B. Patel",
    "affiliations": []
  }],
  "abstractText": "Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.",
  "title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations"
}