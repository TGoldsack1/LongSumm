{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Over the last several years, the world has witnessed the emergence of data sets of an unprecedented scale across different scientific disciplines. This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data. In this paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.\nSeeding — the task of finding initial cluster centers — is critical to finding good clusterings for k-Means. In fact, the seeding step of the state of the art algorithm k-means++ (Arthur & Vassilvitskii, 2007) provides the\n1Department of Computer Science, ETH Zurich. Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntheoretical guarantee on the solution quality while the subsequent refinement using Lloyd’s algorithm (Lloyd, 1982) only guarantees that the quality does not deteriorate. While the k-means++ seeding step guarantees a solution that is O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through the data set. This makes it unsuitable for the massive data setting where the data set is distributed across machines and computation has to occur in parallel.\nAs a remedy, Bahmani et al. (2012) propose the k-means‖ algorithm which produces seedings for kMeans with a reduced number of sequential iterations. Whereas k-means++ only samples a single cluster center in each of k rounds, k-means‖ samples in expectation ` points in each of t iterations. Provided t is small enough, this makes k-means‖ suitable for a distributed setting as the number of synchronizations is reduced.\nOur contributions. We provide a novel analysis of k-means‖ that bounds the expected solution quality for any number of rounds t and any oversampling factor ` ≥ k, the two parameters that need to be chosen in practice. Our bound on the expected quantization error includes both a “traditional” multiplicative error term based on the optimal solution as well as a scale-invariant additive error term based on the variance of the data. The key insight is that this additive error term vanishes at a rate of ( k e` )t if t or ` is increased. This shows that k-means‖ provides provably good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon that k-means‖ works very well even for small t.\nWe further provide a hard instance on which k-means‖ provably incurs an additive error based on the variance of the data and for which an exclusively multiplicative error guarantee cannot be achieved. This implies that an additive error term such as the one in our analysis is in fact necessary if less than k − 1 rounds are employed."
  }, {
    "heading": "2. Background & related work",
    "text": "k-Means clustering. Let X denote a set of points in Rd. The k-Means clustering problem is to find a set C of k cluster centers in Rd that minimizes the quantization error\nφX (C) = ∑ x∈X d(x,C)2 = ∑ x∈X min q∈C ‖x− q‖22.\nAlgorithm 1 k-means++ seeding Require: weighted data set (X , w), number of clusters k\n1: C ← sample single x ∈ X with probability wx∑ x′∈X wx′ 2: for i = 2, . . . , k do 3: Sample x ∈ X with probability wx d(x,C)\n2∑ x′∈X wx′ d(x ′,C)2\n4: C ← C ∪ {x} 5: Return C\nWe denote the optimal quantization error by φOPT(X ) while the variance of the data is defined as Var(X ) = φX ({µ(X )}) where µ(X ) is the mean of X .\nk-means++ seeding. Given a data set X and any set of cluster centers C ⊂ X , the D2-sampling strategy selects a new center by sampling each point x ∈ X with probability\np(x) = d(x,C)2∑\nx′∈X d(x ′, C ′)2\n.\nThe seeding step of k-means++ (Arthur & Vassilvitskii, 2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k − 1 cluster centers using D2 sampling whereby C is always the set of previously sampled centers. Arthur & Vassilvitskii (2007) show that the solution quality φk-means++ of k-means++ seeding is bounded in expectation by\nE[φk-means++] ≤ 8 (log2 k + 2)φOPT(X ).\nThe computational complexity of k-means++ seeding is O(nkd) where n is the number of data points and d the dimensionality. Unfortunately, the iterations in k-means++ seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data. This makes the algorithm unsuitable for the distributed setting.\nk-means‖ seeding. As a remedy, Bahmani et al. (2012) propose the algorithm k-means‖ which aims to reduce the number of sequential iterations. The key component of k-means‖ is detailed in Algorithm 2 in what we call k-means‖ overseeding: First, a data point is sampled as the first cluster center uniformly at random. Then, in each of t sequential rounds, each data point x ∈ X is independently sampled with probability min ( 1, ` d(x,C) 2\nφX (C)\n) and\nadded to the set of sampled centers C at the end of the round. The parameter ` ≥ 1 is called the oversampling factor and determines the expected number of sampled points in each iteration.\nAt the end of Algorithm 2, one obtains an oversampled solution with t` cluster centers in expectation. The full k-means‖ seeding algorithm as detailed in Algorithm 3 reduces such a solution to k centers as follows: First, each of the centers in the oversampled solution is weighted by the number of data points which are closer to it than the\nAlgorithm 2 k-means‖ overseeding Require: data set X , # rounds t, oversampling factor `\n1: C ← sample a point uniformly at random from X 2: for i = 1, 2, . . . , t do 3: C ′ ← ∅ 4: for x ∈ X do 5: Add x to C ′ with probability min ( 1, ` d(x,C) 2\nφX (C) ) 6: C ← C ∪ C ′ 7: Return C\nAlgorithm 3 k-means‖ seeding Require: data set X , # rounds t, oversampling factor `\n1: B ← Result of Algorithm 2 applied to (X , t, `) 2: for c ∈ B do 3: Xc ← points x ∈ X whose closest center in B is c (ties broken arbitrarily but consistently) 4: wc ← |Xc| 5: C ← Result of Algorithm 1 applied to (B,w) 6: Return C\nother centers. Then, k-means++ seeding is run on the weighted oversampled solution to produce a set of k final centers. The total computational complexity of Algorithm 3 is O(nt`d) in expectation.\nThe key intuition behind k-means‖ is that, if we choose a large oversampling factor `, the number of rounds t can be small — certainly much smaller than k, preferably even constant. The step in lines 4 and 5 in Algorithm 2 can be distributed over several machines and after each round the set C can be synchronized. Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently run in a distributed setting.1\nOther related work. Celebi et al. (2013) provide an overview over different seeding methods for k-Means. D2sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and Arthur & Vassilvitskii (2007). This research direction has led to polynomial time approximation schemes based on D2-sampling (Jaiswal et al., 2014; 2015), constant factor approximations based on sampling more than k centers (Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Röglin, 2011). Recently, algorithms to approximate k-means++ seeding based on Markov Chain Monte Carlo have been proposed by Bachem et al. (2016b;a). Finally, k-means++ has been used to construct coresets — small data set summaries — for k-Means clustering (Lucic et al., 2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic et al., 2017).\n1A popular choice is the MLLib library of Apache Spark (Meng et al., 2016) which uses k-means‖ by default."
  }, {
    "heading": "3. Intuition and key results",
    "text": "In this section, we provide the intuition and the main results behind our novel analysis of k-means‖ and defer the formal statements and the formal proofs to Section 4."
  }, {
    "heading": "3.1. Solution quality of k-means‖",
    "text": "Solution quality of Algorithm 2. We first consider Algorithm 2 as it largely determines the final solution quality. Algorithm 3 with its use of k-means++ to obtain the final k cluster centers, only adds an additional O(log k) factor as shown in Theorem 1. Our key result is Lemma 4 (see Section 4) which guarantees that, for ` ≥ k, the expected error of solutions computed by Algorithm 2 is at most\nE[φX (C)] ≤ 2 ( k\ne`\n)t Var(X ) + 26φOPT(X ). (1)\nThe first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error φOPT(X ). It is scale-invariant since both the variance and the quantization error are scaled by λ2 if we scale the data setX by λ > 0. The second term is a “traditional” multiplicative error term based on the optimal quantization error.\nGiven a fixed oversampling factor `, the additive error term decreases exponentially if the number of rounds t is increased. Similarly, for a fixed number of rounds t, it decreases polynomially at a rate O ( 1 `t ) if the over sampling factor ` is increased. This result implies that even for a constant number of rounds one may obtain good clusterings by increasing the oversampling factor `. This explains the empirical observation that often even a low number of rounds t is sufficient and that increasing ` increases the solution quality (Bahmani et al., 2012). The practical implications of this result are non-trivial: Even for the choice of t = 5 and ` = 5k one retains at most 0.0004% of the variance as an additive error. Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error term (Bachem et al., 2017).\nComparison to previous result. Bahmani et al. (2012) show the following result: Let C be the set returned by Algorithm 2 with t rounds. For α = exp ( −(1− e−`/(2k)) ) ≈ e− `2k , Corollary 3 of Bahmani et al. (2012) bounds the expected quality of C by\nE[φX (C)] ≤ ( 1 + α\n2\n)t ψ + 16\n1− α φOPT(X ), (2)\nwhere ψ denotes the quantization error of X based on the first, uniformly sampled center in k-means‖. The key difference compared to our result is as follows: First, even as we increase `, the factor α is always non-negative. Hence, regardless of the choice of `, the additive ψ term is reduced\nby at most 12 per round. 2 This means that, given the analysis in Bahmani et al. (2012), one would always obtain a constant additive error for a constant number of rounds t, even as ` is increased.\nGuarantee for Algorithm 3. Our main result — Theorem 1 — bounds the expected quality of solutions produced by Algorithm 3. As in Bahmani et al. (2012), one loses another factor ofO(ln k) compared to (1) due to Algorithm 3. Theorem 1. Let k ∈ N, t ∈ N and ` ≥ k. Let X be a data set in Rd and C be the set returned by Algorithm 3. Then,\nE[φX (C)] ≤ O\n(( k\ne`\n)t ln k ) Var(X )+O(ln k)φOPT(X )."
  }, {
    "heading": "3.2. A hard instance for k-means‖",
    "text": "We consider the case t < k−1 which captures the scenario where k-means‖ is useful in practice as for t ≥ k one may simply use k-means++ instead.\nTheorem 2. For any β > 0, k ∈ N, t < k − 1 and ` ≥ 1, there exists a data set X of size 2(t+ 1) such that\nE[φX (C)] ≥ 1\n4 (4`t)\n−t Var(X ),\nwhere C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `. Furthermore,\nVar(X ) > 0, φOPT(X ) = 0 and n∆2 ≤ β\nwhere ∆ is the largest distance between any points in X .\nTheorem 2 shows that there exists a data set on which k-means‖ provably incurs a non-negligible error even if the optimal quantization error is zero. This implies that k-means‖ with t < k − 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets. We thus argue that an additive error bound such as the one in Theorem 1 is required. We note that the upper bound in (1) and the lower bound in Theorem 2 exhibit the same 1`t dependence on the oversampling factor ` for a given number of rounds t.\nFurthermore, Theorem 2 implies that, for general data sets, k-means‖ cannot achieve the multiplicative error of O(log k) in expectation as claimed by Bahmani et al. (2012).3 In particular, if the optimal quantization error is\n2Note that E[ψ] ≤ 2Var(X ) (Arthur & Vassilvitskii, 2007). 3To see this, let ψ = φX (c1) be the quantization error of the first sampled center in Algorithm 2 and choose β small enough such that the choice of t ∈ O(logψ) leads to t < k − 1. For X in Theorem 2, φOPT(X ) = 0 which implies that the desired multiplicative guarantee would require E[φX (C)] = 0. However, the non-negligible, additive error in Theorem 2 and Var(X ) > 0 implies that E[φX (C)] > 0.\nzero, then k-means‖ would need to return a solution with quantization error zero. While we are guaranteed to remove a constant fraction of the error in each round, the number of required iterations may be unbounded."
  }, {
    "heading": "4. Theoretical analysis",
    "text": "Proof of Theorem 1. The proof is divided into four steps: First, we relate k-means‖-style oversampling to k-means++-style D2-sampling in Lemmas 1 and 2. Second, we analyze a single iteration of Algorithm 2 in Lemma 3. Third, we bound the expected solution quality of Algorithm 2 in Lemma 4. Finally, we use this to bound the expected solution quality of Algorithm 3 in Theorem 1.\nLemma 1. Let A be a finite set and let f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V ) ≥ f(U) ≥ 0, for all V ⊆ U .\nLet P be a probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa ∈ [0, 1]. Let C be the set of elements a ∈ A for which the event Ea occurs.\nLet Q be the probability distribution on A where a single a ∈ A is sampled with probability qa/ ∑ a∈A qa.\nThen, with ∅ denoting the empty set, we have that\nEP [f(C)] ≤ EQ[f({a})] + e− ∑ a∈A qaf(∅).\nProof. To prove the claim, we first construct a series of sub-events of the events {Ea}a∈A and then use them to recursively bound EP [f(C)].\nLet m ∈ N. For each a ∈ A, let ia be an independent random variable drawn uniformly at random from {1, 2, . . . ,m}. For each a ∈ A and i = 1, 2, . . . ,m, let Fai be an independent event that occurs with probability\nP[Fai] = (\n1− qa m\n)i−1 .\nFor each a ∈ A and i = 1, 2, . . . ,m, denote by Eai the event that occurs if i = ia and both Ea and Fai occur. By design, all these events are independent and thus\nP[Eai] = P[Ea]P[Fai]P[ia = i] = qa m\n( 1− qa\nm\n)i−1 (3)\nfor each a ∈ A and i = 1, 2, . . . ,m. Furthermore, for any a, a′ ∈ A with a 6= a′ and any i, i′ ∈ {1, 2, . . . ,m}, the events Eai and Ea′i′ are independent.\nFor i = 1, 2, . . . ,m let Gi be the event that none of the events {Eai′}a∈A,i′≤i occur, i.e.,\nGi = ⋂ i′≤i ⋂ a∈A Eai′\nwhere A denotes the complement of A. For convenience, let G0 be the event that occurs with probability one.\nLet (a1, a2, . . . , a|A|) be any enumeration of A. For i = 1, 2, . . . ,m and j = 1, 2, . . . , |A|+1, define the event\nGi,j = Gi−1 ∩ ⋂\n0<j′<j\nEaj′ i.\nWe note that by definitionGi,1 = Gi−1 andGi,|A|+1 = Gi for i = 1, 2, . . . ,m.\nFor i = 1, 2, . . . ,m and j = 1, 2, . . . , |A|, we have E[f(C)|Gi,j ] =P [ Eaji | Gi,j ] E [ f(C) | Eaji ∩Gij ] + P [ Eaj′ i | Gij ] E[f(C) | Gi,j+1].\n(4)\nWe now bound the individual terms. The event Gi,j implies that the events {Eaji′}i′<i did not occur. Furthermore, Eaji is independent of the events {Eaj′ i′}i′=1,2,...,m for j′ 6= j. Hence, we have\nP [ Eaji | Gi,j ] = P [ Eaji | G0 ∩ ⋂ i′<i Eaji′ ]\n= P [ Eaji ] P [ G0 ∩ ⋂ i′<iEaji′\n] = P [ Eaji ] 1− P [⋃ i′<iEaji′\n] = P [ Eaji ] 1− ∑ i′<i P [ Eaji′ ] ,\n(5)\nwhere the last equality follows since the events {Eaji′}i′<i are disjoint. Using (3), we observe that ∑ i′<i P [ Eaji′ ] is a sum of a finite geometric series and we have∑ i′<i P [ Eaji′ ] = ∑ i′<i qa m ( 1− qa m\n)i′−1 = qa m 1− ( 1− qam )i−1 1− ( 1− qam\n) = 1− ( 1− qa\nm\n)i−1 .\nTogether with (3) and (5), this implies\nP [ Eaji | Gi,j ] = qa m\n( 1− qam )i−1( 1− qam\n)i−1 = qam. (6) The event Eaji implies that C contains aj . Hence, since f is monotonically decreasing, we have\nE [ f(C) | Eaji ∩Gij ] ≤ f({aj}).\nUsing (4) and (6), this implies\nE[f(C)|Gi,j ] ≤ qaj m f({aj})+\n( 1−\nqaj m\n) E[f(C) | Gi,j+1].\nApplying this result iteratively for j = 1, 2, . . . , |A| implies E[f(C)|Gi,1] = |A|∑ j=1 qaj m ∏ j′<j ( 1− qaj′ m ) f({aj}) +\n |A|∏ j=1 ( 1− qaj m )E[f(C) | Gi,|A|+1]. Note that 0 ≤ 1 − qam ≤ 1 for all a ∈ A and that f is non-negative. This implies that for i = 1, 2, . . . ,m\nE[f(C)|Gi,1] ≤ ∑ a∈A qa m f({a}) + c · E [ f(C)|Gi,|A|+1 ] where\nc = ∏ a∈A ( 1− qa m ) .\nSince Gi,1 = Gi−1 and Gi,|A|+1 = Gi, we have for i = 1, 2, . . . ,m\nE[f(C)|Gi−1] ≤ ∑ a∈A qa m f({a}) + c · E[f(C)|Gi].\nApplying this result iteratively, we obtain\nE[f(C)] ≤ ( m∑ i=1 ci−1 )∑ a∈A qa m f({a}) + cm · f(∅).\nSince 0 ≤ c ≤ 1, we have m∑ i=1 ci−1 ≤ ∞∑ i=1 ci−1 = 1 1− c .\nFor x ∈ [−1, 0] it holds that log(1 + x) ≤ x and hence\ncm = ∏ a∈A ( 1− qa m )m = exp ( m ∑ a∈A log ( 1− qa m ))\n≤ exp ( −m\n∑ a∈A qa m\n) = e− ∑ a∈A qa .\nThis implies that\nE[f(C)] ≤ 1 1− c ∑ a∈A qa m f({a})+e− ∑ a∈A qa ·f(∅). (7)\nWe show the main claim by contradiction. Assume that\nEP [f(C)] > EQ[f({a})] + e− ∑ a∈A qaf(∅).\nIf EQ[f({a})] = 0, the contradiction follows directly from (7). Otherwise, EQ[f({a})] > 0 implies that there exists an > 0 such that\nEP [f(C)] > (1 + )EQ[f({a})] + e− ∑ a∈A qaf(∅). (8)\nBy definition, we have\nc = ∏ a∈A ( 1− qa m ) = 1− ∑ a∈A qa m + o ( 1 m ) .\nThus, there exists a m ∈ N sufficiently large such that\nc = 1− ∑ a∈A qa m + o ( 1 m ) ≤ 1− 1 1 + ∑ a∈A qa m .\nTogether with (7), this implies\nE[f(C)] ≤ 1 + ∑ a∈A qa m ∑ a∈A qa m f({a}) + e− ∑ a∈A qa · f(∅)\n= (1 + )EQ[f({a})] + e− ∑ a∈A qaf(∅).\nwhich is a contradiction to (8) and thus proves the claim.\nLemma 2 extends Lemma 1 to k-means‖-style sampling probabilities of the form qa = min (1, `pa).\nLemma 2. Let ` ≥ 1. LetA be a finite set and let f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V ) ≥ f(U) ≥ 0, for all V ⊆ U . For each a ∈ A, let pa ≥ 0 and ∑ a∈A pa ≤ 1.\nLet P be the probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa = min (1, `pa). Let C be the set of elements a ∈ A for which the event Ea occurs.\nLet Q be the probability distribution on A where a single a ∈ A is sampled with probability pa/ ∑ a∈A pa.\nThen, with ∅ denoting the empty set, we have that\nEP [f(C)] ≤ 2EQ[f({a})] + e−` ∑ a∈A paf(∅).\nProof. LetA1 be the set of elements a ∈ A such that `pa ≤ 1 and A2 the set of elements a ∈ A such that `pa > 1. By definition, every element in A2 is sampled almost surely, i.e., A2 ⊆ C. This implies that almost surely\nf(C) = f (A2 ∪ (C ∩A1)) . (9)\nIf |A1|= 0, the result follows trivially since\nEP [f(C)] = f(A2) = EQ[f({a})].\nSimilarly, if |A2|= 0, the result follows directly from Lemma 1 with qa = `pa. For the remainder of the proof, we may thus assume that both A1 and A2 are non-empty.\nFor a ∈ A1, let qa = `pa and define the non-negative and monotonically decreasing function\ng(C) = f (A2 ∪ C) .\nLet p1 = ∑ a∈A1 pa and p2 = ∑ a∈A2 pa. Lemma 1 applied to A1, qa and g implies that\nEP [f(C)] = E[g(C)] ≤ ∑ a∈A1 pa p1 g({a}) + e−`p1g(∅).\n(10)\nLet d = ( 1− e−`p2 ) e−`p1\nand define α =\np2 p1 + p2 − p1 p1 + p2 d.\nBy design, α ≤ 1. Furthermore\n`p1 ≥ log `p1.\nSinceA2 is nonempty and pa ≥ 1` for all a ∈ A2, it follows that p2 ≥ 1` . This implies\ne`p1 ≥ `p1 ≥ p1 p2 .\nSince 0 ≤ ( 1− e−`p2 ) ≤ 1, we have\np2 ≥ p1e−`p1 ≥ p1 ( 1− e−`p2 ) e−`p1 = p1d.\nHence,\nα = p2 p1 + p2 − p1 p1 + p2\n( 1− e−`p2 ) e−`p1 ≥ 0.\nSince α ∈ [0, 1] and g({a}) ≤ g(∅) for any a ∈ A1, we may write (10), i.e., EP [f(C)] ≤ (1− α) ∑ a∈A1 pa p1 g({a}) + ( α+ e−`p1 ) g(∅).\n(11)\nBy definition, we have\n1− α = 1− p2 p1 + p2 + p1 p1 + p2 d = p1 p1 + p2 (1 + d).\nSince g({a}) ≤ f({a}), we thus have\n(1− α) ∑ a∈A1 pa p1 g({a}) ≤ (1 + d) ∑ a∈A1 pa p1 + p2 f({a}).\n(12)\nSimilarly, we have\nα+ e−`p1 = p2 p1 + p2 − p1 p1 + p2 d+ e−`p1\n= p2\np1 + p2 + d p2 p1 + p2 − d+ e−`p1\n= (1 + d) p2\np1 + p2 + e−`(p1+p2).\nSince g(∅) ≤ f(∅), it follows that( α+ e−`p1 ) g(∅) ≤ (1+d) p2\np1 + p2 g(∅)+e−`(p1+p2)f(∅).\n(13) Since g(∅) = f (A2) and thus g(∅) ≤ f({a}) for all a ∈ A2, we have\np2g(∅) = ∑ a∈A2 pag(∅) ≤ ∑ a∈A2 paf({a}). (14)\nCombining (11), (12), (13), and (14) leads to\nEP [f(C)] ≤ (1 + d)EQ[f({a})] + e−` ∑ a∈A paf(∅).\nSince p1 ≥ 0, we have 1 + d = 1 + ( 1− e−`p2 ) e−`p1 ≤ 2\nwhich proves the main claim.\nLemma 3 bounds the solution quality after each iteration of Algorithm 2 based on the solution before the iteration.\nLemma 3. Let k ∈ N and ` ≥ 1. Let X be a data set in Rd and denote by φOPT(X ) the optimal k-Means clustering cost. LetC denote the set of cluster centers at the beginning of an iteration in Algorithm 2 and C ′ the random set added in the iteration. Then, it holds that\nE[φX (C ∪ C ′)] ≤ ( k\ne`\n) φX (C) + 16φOPT(X ).\nProof. The proof relies on applying Lemma 2 to each cluster of the optimal solution. Let OPT denote any clustering achieving the minimal cost φOPT(X ) on X . We assign all the points x ∈ X to their closest cluster center in OPT with ties broken arbitrarily but consistently. For c ∈ OPT we denote by Xc the subset of X assigned to c. For each c ∈ OPT, let\nC ′c = C ′ ∩ Xc.\nBy definition, a ∈ Xc is included in C ′c with probability\nqa = min\n( 1,\n`d(a,C)2∑ a′∈X d(a ′, C)2\n) .\nFor each c ∈ OPT, we define the monotonically decreasing function fc : 2Xc → R≥0 to be\nfc(C ′ c) = φXc(C ∪ C ′c).\nFor each c ∈ OPT, Lemma 2 applied to Xc, C ′c and fc implies\nE[fc(C ′c)] ≤2 ∑ a∈Xc d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a})\n+ e −`\n∑ a∈Xc d(a,C)\n2∑ a′∈X d(a\n′,C)2 fc(∅).\n(15)\nSince fc({a}) = φXc(C ∪ {a}), the first term is equivalent to sampling a single element from Xc using D2 sampling. Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we have for all c ∈ OPT∑\na∈Xc\nd(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a}). ≤ 8φOPT(Xc). (16)\nFor each c ∈ OPT, we further have\ne −`\n∑ a∈Xc d(a,C)\n2∑ a′∈X d(a\n′,C)2 fc(∅) = e−`ucucφX (C).\nwhere\nuc =\n∑ a∈Xc d(a,C)\n2∑ a′∈X d(a ′, C)2 = φXc(C) φX (C) .\nWe have that\nlog `uc ≤ `uc − 1 ⇐⇒ `uc ≤ e`uc\ne\nwhich implies\ne−`ucucφX (C) ≤ 1\ne` φX (C). (17)\nCombining (15), (16) and (17), we obtain\nE[fc(C ′c)] ≤16φOPT(Xc) + 1\ne` φX (C). (18)\nSince E[φX (C ∪ C ′)] ≤ ∑ c∈OPT E[fc(C ′c)]\nand φX (OPT) = ∑ c∈OPT φXc(OPT),\nwe thus have E[φX (C ∪ C ′)] ≤ ( k\ne`\n) φX (C) + 16φOPT(X )\nwhich concludes the proof.\nAn iterated application of Lemma 3 allows us to bound the solution quality of Algorithm 2 in Lemma 4.\nLemma 4. Let k ∈ N, t ∈ N and ` ≥ k. Let X be a data set in Rd and C be the random set returned by Algorithm 2. Then,\nE[φX (C)] ≤ 2 ( k\ne`\n)t Var(X ) + 26φOPT(X ).\nProof. The algorithm starts with a uniformly sampled initial cluster center c1. We iteratively apply Lemma 3 for each of the t rounds to obtain\nE[φX (C)] ≤ ( k\ne`\n)t E[φX ({c1})] + 16stφOPT(X ) (19)\nwhere\nst = t∑ i=1 ( k e` )i−1 .\nFor ` ≥ k, we have 0 ≤ ke` ≤ 1/e and hence\nst ≤ t∑ i=1 1 ei−1 ≤ ∞∑ i=0 1 ei =\n1\n1− 1/e . (20)\nBy Lemma 3.2 of Arthur & Vassilvitskii (2007), we have that E[φX ({c1})] ≤ 2 Var(X ). Together with (19), (20) and 16/(1− 1/e) ≈ 25.31 < 26, this implies the required result.\nWith Lemma 4, we are further able to bound the solution quality of Algorithm 3 and prove Theorem 1.\nProof of Theorem 1. Let B be the set returned by Algorithm 2. For any x ∈ X , let bx denote its closest point in B with ties broken arbitrarily. By the triangle inequality and since (|a|+|b|)2 ≤ 2a2 + 2b2, for any x ∈ X\nd(x,C)2 ≤ 2 d(x, bx)2 + 2 d(bx, C)2\nand hence\nE[φX (C)] = ∑ x∈X d(x,C)2\n≤ 2 ∑ x∈X d(x, bx) 2 + 2 ∑ x∈X d(bx, C) 2\n= 2φX (B) + 2 ∑ x∈B wx d(x,C) 2.\n(21)\nLet OPTX be the optimal k-Means clustering solution on X and OPT(B,w) the optimal solution on the weighted set (B,w). By Theorem 1.1 of Arthur & Vassilvitskii (2007),\nk-means++ produces an α = 8(log2 k + 2) approximation to the optimal solution. This implies that∑\nx∈B wx d(x,C) 2 ≤ α ∑ x∈B wx d ( x,OPT(B,w) ) 2\n≤ α ∑ x∈B wx d(x,OPTX ) 2\n= α ∑ x∈X d(bx,OPTX ) 2.\n(22)\nBy the triangle inequality and since (|a|+|b|)2 ≤ 2a2+2b2, it holds for any x ∈ X that\nd(bx,OPTX ) 2 ≤ 2 d(x, bx)2 + 2 d(x,OPTX )2\nand hence∑ x∈X d(bx,OPTX ) 2 ≤ 2φX (B) + 2φOPT(X ). (23)\nCombining (21), (22) and (23), we obtain\nE[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT(X ).\nFinally, by Lemma 4, we have E[φX (C)] ≤(32 log2 k + 68) ( k\ne`\n)t Var(X )\n+ (432 log2 k + 916)φOPT(X ).\nProof of Theorem 2. For this proof, we explicitly construct a data set: Let β′ > 0 and consider points in onedimensional Euclidean space. For i = 1, 2, . . . , t, set\nxi =\n√ β′(4`t) 1−i − β′(4`t)−i\nas well as xt+1 = √ β′(4`t) −t .\nLet the data setX consist of the t+1 points {xi}t=1,2,...,t+1 as well as t + 1 points at the origin. Since t < k − 1, the optimal k-Means clustering solution consists of t + 2 points placed at each of the {xi}i=1,2,...t+1 and at 0. By design, this solution has a quantization error of zero and the variance is nonzero, i.e., φOPT(X ) = 0 and Var(X ) > 0 as claimed.\nChoose β′ = β2(t+1) . The maximal distance ∆ between any two points in X is bounded by ∆ = d(0, x1)2 ≤ β′. Since n = 2(t+ 1), this implies ψ ≤ n∆2 ≤ β as claimed.\nFor i = 1, 2, . . . , t, let Ci consist of 0 and all xj with j < i. By design, we have d(0, Ci)2 = 0 as well as d(xj , Ci)2 = 0 for j < i. For j ≥ i, we have d(xj , Ci)2 = d(xj , 0)2. For any i = 1, 2, . . . , t+ 1, we thus have∑\nj≥i\nd(xj , 0) 2 = β′(4`t) 1−i .\nConsider a single iteration of Algorithm 2 where C = Ci. In this case, all points in Xj with j < i are added to C ′ with probability zero and for j > i each point xj is added to C ′ with probability\nmin ( 1, `d(xj , 0) 2∑\nj′≥i d(xj′ , 0) 2\n) = `d(xj , 0) 2\nβ′(4`t) 1−i .\nBy the union bound, the probability that any of the points in ⋃ j>i{xj} are sampled is bounded by∑\nj>i\n`d(xj , 0) 2 β′(4`t) 1−i = 1 4t .\nThe point xi is not sampled with probability at most\n1−min ( 1, ` d(xi, 0) 2∑\nj′≥i d(xj′ , 0) 2\n) = 1−min ( 1, `− 1\n4t ) ≤ 1\n4t .\nBy the union bound, a single iteration of Algorithm 2 with C = Ci hence samples exactly the set C ′ = {xi} with probability at least ( 1− 12t ) .\nIn Algorithm 2, the first center is sampled uniformly at random from X . Since half of the elements in X are placed at 0, with probability at least 12 , the first center is at 0 or equivalently C = C1. With probability ( 1− 12t\n)t ≥ 12 , we then sample exactly the points x1, x2, . . . , xt in the t subsequent iterations. Hence, with probability at least 14 , the solution produced by Algorithm 2 consists of 0 and all xi except xt+1. Since xt+1 is closest to 0, this implies\nE[φX (C)] ≥ 1\n4 d(xt+1, 0)\n2 = 1\n4 β′(4`t) −t . (24)\nThe variance of X is bounded by a single point at 0, i.e., Var(X ) ≤ φX ({0}) = ∑ j≥1 d(xj , 0) 2 = β′.\nTogether with (24), we have that\nE[φX (C)] ≥ 1\n4 (4`t)\n−t Var(X ).\nThe same result extends to the output of Algorithm 3 as it always picks a subset of the output of Algorithm 2."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing."
  }],
  "year": 2017,
  "references": [{
    "title": "StreamKM++: A clustering algorithm for data streams",
    "authors": ["Ackermann", "Marcel R", "Märtens", "Marcus", "Raupach", "Christoph", "Swierkot", "Kamil", "Lammersen", "Christiane", "Sohler", "Christian"],
    "venue": "Journal of Experimental Algorithmics (JEA),",
    "year": 2012
  }, {
    "title": "Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization",
    "authors": ["Aggarwal", "Ankit", "Deshpande", "Amit", "Kannan", "Ravi"],
    "venue": "Algorithms and Techniques,",
    "year": 2009
  }, {
    "title": "Streaming k-means approximation",
    "authors": ["Ailon", "Nir", "Jaiswal", "Ragesh", "Monteleoni", "Claire"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2009
  }, {
    "title": "k-means++: The advantages of careful seeding",
    "authors": ["Arthur", "David", "Vassilvitskii", "Sergei"],
    "venue": "In Symposium on Discrete Algorithms (SODA),",
    "year": 2007
  }, {
    "title": "Coresets for nonparametric estimation - the case of DPmeans",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Krause", "Andreas"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Fast and provably good seedings for k-means",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "Hamed", "Krause", "Andreas"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Approximate k-means++ in sublinear time",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "S. Hamed", "Krause", "Andreas"],
    "venue": "In Conference on Artificial Intelligence (AAAI),",
    "year": 2016
  }, {
    "title": "Uniform deviation bounds for kmeans clustering",
    "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "S. Hamed", "Krause", "Andreas"],
    "venue": "In To appear in International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Scalable KMeans++",
    "authors": ["Bahmani", "Bahman", "Moseley", "Benjamin", "Vattani", "Andrea", "Kumar", "Ravi", "Vassilvitskii", "Sergei"],
    "venue": "Very Large Data Bases (VLDB),",
    "year": 2012
  }, {
    "title": "A bad instance for kmeans++",
    "authors": ["Brunsch", "Tobias", "Röglin", "Heiko"],
    "venue": "In International Conference on Theory and Applications of Models of Computation,",
    "year": 2011
  }, {
    "title": "A comparative study of efficient initialization methods for the k-means clustering algorithm",
    "authors": ["Celebi", "M Emre", "Kingravi", "Hassan A", "Vela", "Patricio A"],
    "venue": "Expert Systems with Applications,",
    "year": 2013
  }, {
    "title": "Bico: Birch meets coresets for k-means clustering",
    "authors": ["Fichtenberger", "Hendrik", "Gillé", "Marc", "Schmidt", "Melanie", "Schwiegelshohn", "Chris", "Sohler", "Christian"],
    "venue": "In European Symposium on Algorithms,",
    "year": 2013
  }, {
    "title": "A simple D2-sampling based PTAS for k-means and other clustering problems",
    "authors": ["Jaiswal", "Ragesh", "Kumar", "Amit", "Sen", "Sandeep"],
    "year": 2014
  }, {
    "title": "Improved analysis of D2-sampling based PTAS for k-means and other clustering problems",
    "authors": ["Jaiswal", "Ragesh", "Kumar", "Mehul", "Yadav", "Pulkit"],
    "venue": "Information Processing Letters,",
    "year": 2015
  }, {
    "title": "Least squares quantization in PCM",
    "authors": ["Lloyd", "Stuart"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1982
  }, {
    "title": "Strong coresets for hard and soft Bregman clustering with applications to exponential family mixtures",
    "authors": ["Lucic", "Mario", "Bachem", "Olivier", "Krause", "Andreas"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2016
  }, {
    "title": "Training mixture models at scale via coresets",
    "authors": ["Lucic", "Mario", "Faulkner", "Matthew", "Krause", "Andreas", "Feldman", "Dan"],
    "venue": "To appear in Journal of Machine Learning Research (JMLR),",
    "year": 2017
  }, {
    "title": "Mllib: Machine learning in Apache Spark",
    "authors": ["Meng", "Xiangrui", "Bradley", "Joseph", "B Yuvaz", "Sparks", "Evan", "Venkataraman", "Shivaram", "Liu", "Davies", "Freeman", "Jeremy", "D Tsai", "Amde", "Manish", "Owen", "Sean"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2016
  }, {
    "title": "The effectiveness of Lloyd-type methods for the k-means problem",
    "authors": ["Ostrovsky", "Rafail", "Rabani", "Yuval", "Schulman", "Leonard J", "Swamy", "Chaitanya"],
    "venue": "In Symposium on Foundations of Computer Science (FOCS),",
    "year": 2006
  }],
  "id": "SP:6821cf9d1b2e4e802787b5b509c768a3c0b983b4",
  "authors": [{
    "name": "Olivier Bachem",
    "affiliations": []
  }, {
    "name": "Mario Lucic",
    "affiliations": []
  }, {
    "name": "Andreas Krause",
    "affiliations": []
  }],
  "abstractText": "The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation. However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets. The standard remedy is to use the k-means‖ algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting. In this paper, we provide a novel analysis of the k-means‖ algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice. In particular, we show that k-means‖ provides provably good clusterings even for a small, constant number of iterations. This theoretical finding explains the common observation that k-means‖ performs extremely well in practice even if the number of rounds is low. We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k−1 rounds are employed.",
  "title": "Distributed and Provably Good Seedings for k-Means in Constant Rounds"
}