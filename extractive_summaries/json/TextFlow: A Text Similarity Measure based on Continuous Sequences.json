{
  "sections": [{
    "text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 763–772 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1071"
  }, {
    "heading": "1 Background",
    "text": "The number of pages required to print the content of the World Wide Web was estimated to 305 billion in a 2015 article1. While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace. A recent study shows that the average webpage weights 1,200 KB with plain text accounting for up to 16% of that size2.\nWhile efficient distribution of textual data and computations are the key to deal with the increas-\n1http://goo.gl/p9lt7V 2http://goo.gl/c41wpa\ning scale of textual search, similarity measures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information. These tasks are also often performed on dedicated document collections for domain-specific applications where text similarity measures can be directly applied.\nFinding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015). However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora.\nFor instance, Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words. The proposed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs. In a more general study, Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task. In their experiments the best performance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in (Li et al., 2006). This word order similarity is computed by constructing first two vectors representing the common words between two given sentences and using their respective positions in the sentences as term\n763\nweights. The similarity value is then obtained by subtracting the two vectors and taking the absolute value. While such representation takes into account the actual positions of the words, it does not allow detecting sub-sequence matches and takes into account missing words only by omission.\nMore generally, existing standalone (or traditional) text similarity measures rely on the intersections between token sets and/or text sizes and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein (Sankoff and Kruskal, 1983), Jaccard (Jain and Dubes, 1988) and Jaro (Jaro, 1989). The sequential nature of natural language is taken into account mostly through word n-grams and skipgrams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.\nIn this paper, we use intuitions from a common representation in DNA sequence alignment to design a new standalone similarity measure called TextFlow (XF). The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches. Our contributions can be detailed further as follows:\n• A novel standalone similarity measure which:\n– exploits the full sequence of words in the compared texts.\n– is asymmetric in a way that allows it to provide the best performance on different tasks (e.g., paraphrase detection, textual entailment and ranking).\n– when required, it can be trained with a small set of parameters controlling the impact of sub-sequence matching, position gaps and unmatched words.\n– provides consistent high performance across tasks and datasets compared to traditional similarity measures.\n• A neural network architecture to train TextFlow parameters for specific tasks.\n• An empirical study on both performance consistency and standard evaluation measures, performed with eight datasets from three different tasks.\n• A new evaluation measure, called CORE, used to better show the consistency of a system at high performance using both its rank average and rank variance when compared to competing systems over a set of datasets.\n2 The TextFlow Similarity\nXF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf. figure 1). We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf. figure 2). The area under the curve is considered to be the distance between the two text pairs which is then normalized with the matrix surface. For practical computation, we transform this first intuitive representation using the delta of positions as in figure 3. In this setting, the Y axis is the delta of positions of a word occurring in the two texts being compared. If the word does not occur in the target text, the delta is considered to be a maximum reference value (l in figure 2).\nThe semantics are: the bigger the area under curve is, the lower the similarity between the compared texts. XF values are real numbers in the [0,1] interval, with 1 indicating a perfect match, and 0 indicating that the compared texts do not have any common tokens. With this representation, we are able to take into account all matched words and sub-sequences at the same time. The exact value for the XF similarity between two texts X = {x1, x2, .., xn} and Y = {y1, y2, .., ym} is therefore computed as:\nXF (X,Y ) = 1− 1 nm\nn∑\ni=2\n1\nSi Ti,i−1(X,Y )\n− 1 nm\nn∑\ni=2\n1\nSi Ri,i−1(X,Y )\n(1)\nWith Ti,i−1(X,Y ) corresponding to the triangular area in the [i − 1, i] step (cf. figure 3) and Ri,i−1(X,Y ) corresponding to the rectangular component. They are expressed as:\nTi,i−1(X,Y ) = |∆P (xi, X, Y )−∆P (xi−1, X, Y )|\n2 (2)\nand:\nRi,i−1(X,Y ) = Min(∆P (xi, X, Y ),∆P (xi−1, X, Y )) (3)\nWith:\n• ∆P (xi, X, Y ) the minimum difference between xi positions in X and Y . xi position in X is multiplied by the factor |Y ||X| for normalization. If xi /∈ X ∩ Y , ∆P (xi, X, Y )\nis set to the same reference value equal to m, (i.e., the cost of a missing word is set by default to the length of the target text), and:\n• Si is the length of the longest matching sequence between X and Y including the word xi, if xi ∈ X ∩ Y , or 1 otherwise.\nXF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input textX . XF is an asymmetric similarity measure. Its asymmetric aspect has interesting semantic applications as we show in the example below (cf. figure 2). The minimum value of XF provided the best differentiation between positive and negative text pairs when looking for semantic equivalence (i.e., paraphrases), the maximum value was among the top three for the textual entailment example. We conduct this comparison at a larger scale in the evaluation section.\nWe add 3 parameters to XF in order to represent the importance that should be given to position deltas (Position factor α), missing words (sensitivity factor β), and sub-sequence matching (sequence factor γ), such that:\nXFα,β,γ(X,Y ) = 1− 1\nβnm\nn∑\ni=2\nα\nSγi T βi,i−1(X,Y )\n− 1 βnm\nn∑\ni=2\nα\nSγi Rβi,i−1(X,Y )\n(4)\nWith:\nT βi,i−1(X,Y ) = |∆βP (xi, X, Y )−∆βP (xi−1, X, Y )|\n2 (5)\nRβi,i−1(X,Y ) = Min(∆βP (xi, X, Y ),∆βP (xi−1, X, Y )) (6) and:\n• ∆βP (xi, X, Y ) = βm, if xi /∈ X ∩ Y\n• α < β: forces missing words to always cost more than matched words. • Sγi = {\n1ifSi = 1orxi /∈ X ∩ Y γ SiforSi > 1\nThe γ factor increases or decreases the impact of sub-sequence matching, α applies to individual token matches whether inside or outside a sequence, and β increases or decreases the impact of\nmissing tokens as well as the normalization quantity βnm in equation 4 to keep the similarity values in the [0,1] range."
  }, {
    "heading": "2.1 Parameter Training",
    "text": "By default XF has canonical parameters set to 1. However, when needed, α, β, and γ can be learned on training data for a specific task. We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value. To do so we compute, for each input text pair, the coefficients vector that would lead exactly to the XF value when multiplied by the vector< αβ , α βγ , 1 >. Figure 5) presents the training neural network considering several types of sequences (or translations) of the input text pairs (e.g., lemmas, words, synsets).\nWe use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf. figure 6)."
  }, {
    "heading": "3 Evaluation",
    "text": "Datasets. This evaluation was performed on 8 datasets from 3 different classification tasks: Tex-\ntual Entailment Recognition, Paraphrase Detection, and ranking relevance. The datasets are as follows:\n• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006). Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment. They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.\n• Guardian: an RTE dataset collected from 78,696 Guardian articles5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets. Positive examples were collected from the titles and first sentences. Negative examples were collected from the same source by selecting consecutive sentences and random sentences.\n• SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with\n5https://github.com/daoudclarke/ rte-experiment\n3 labels: entailment, neutral and contradiction (Bowman et al., 2015). We discarded the contradiction pairs as they do not necessarily represent dissimilar sentences and are therefore a random noise w.r.t. our similarity measure evaluation.\n• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.\n• Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow (Nakov et al., 2016). The dataset contains 3,169 training pairs and 700 test pairs. Three labels are considered: ”Perfect Match”, ”Relevant” or ”Irrelevant”. We combined the first two into the same positive category for our evaluation.\n• Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels.\nFeatures. After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token:\n• Word (plain text value)\n• Lemma\n• Part-Of-Speech (POS) tag\n• WordNet Synset6 OR Lemma\n• WordNet Synset OR Lemma for Nouns\n• WordNet Synset OR Lemma for Verbs\n• WordNet Synset OR Lemma for Nouns and Verbs.\nIn the last 4 types of sequences the lemma is used when there is no corresponding WordNet synset. In a first experiment we compare different aggregation functions on top of XF (minimum, maximum and average) in table 1. We used the LibLinear7 SVM classifier for this task.\nIn the second part of the evaluation, we use neural networks to compare the efficiency of XFc, XFt and other similarity measures with in the same setting. We use the neural net described in figure 5 for the trained versionXFt and the equivalent architecture presented in figure 6 for all other similarity measures. For canonical XFc we use by default the best aggregation for the task as observed in table 3.\n6https://wordnet.princeton.edu/ 7https://www.csie.ntu.edu.tw/˜cjlin/\nliblinear/\nSimilarity Measures. We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992). Implementation. XF was implemented in Java as an extension of the Simmetrics package, made available at this address9. The neural networks were implemented in Python with TensorFlow10. We also share the training sets used for both parameter training and evaluation. The evaluation was performed on a 4-core laptop with 32GB of RAM. The initial parameters for XFt were chosen with a random function. Evaluation Measures. We use the standard accuracy values and F1, precision and recall for the\n8https://github.com/Simmetrics/ simmetrics\n9https://github.com/ymrabet/TextFlow 10https://www.tensorflow.org/\npositive class (i.e., entailment, paraphrase, and ranking relevance). We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called Consistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}:\nCORE D,S,E (m) =\nMIN p∈S ( AVG d∈D (RS(Ed(p)) + Vd∈D(RS(Ed(p))) )\nAVG d∈D\n( RS(Ed(m)) ) + Vd∈D ( RS(Ed(m)) ) (7)\nWith RS(Ed(m)) the rank of m according to the evaluation measure E on dataset d w.r.t. competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets. The results in tables 2, 3, and 4 demonstrate the intuition. Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems S. The maximum value of CORE is 1 for the best performing system according to its rank. It also allows quantifying how consistently a system achieves high performance for the remaining systems.\nTextFlow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in (Achananuparp et al., 2008), with gaps of +1.0 in F1 and +6.1 accuracy on MSRP and +4.2 F1 and +2.7% accuracy on RTE 3."
  }, {
    "heading": "4 Discussion",
    "text": "4.1 Canonical Text Flow TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures. It also reached the best precision average with a gap of +1.8 to +6.3. On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3 (cf. table 3). On a rank level, XFc had the best consistent rank for\naccuracy, F1 and precision, and the second best for recall."
  }, {
    "heading": "4.2 Trained Text Flow",
    "text": "When compared to state-of-the-art measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8. XFt also obtained the second best F1 average with a -1.0 gap, but with clear inconsistencies according to the dataset. XFt obtained the best precision with a gap ranging from +0.8 to +7.1. XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%. Both its recall and F1 performance can be explained by the fact that the measure was trained to optimize accuracy, and not the F1 score for the positive class; which also suggests that the approach could be adapted to F1 optimization if needed."
  }, {
    "heading": "4.3 Synthesis",
    "text": "Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized. We argue that this consistency was made possible through the asymmetry of XF which allowed it to adapt to different kinds of similarities (i.e., equivalence/paraphrase, inference/entailment, and mutual distance/ranking). These results also show that the actual position difference is a relevant factor for text similarity. We explain it mainly by the natural flow of language where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are\nequivalent in meaning. The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version."
  }, {
    "heading": "4.4 Additional Experiments",
    "text": "In additional experiments, we compared TFc and TFt with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens. The results were significantly lower on all datasets (between 3 and 10 points loss in accuracy) for both the soa measures and TextFlow variants. This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of a new word inside a 3-words sequence leads to a tri-gram mismatch and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence. This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words. It must be noted here that not all languages grant the same level of importance to sequences and that additional multilingual tests have to be carried out.\nIn addition to binary classification output such as textual entailment and paraphrase recognition, text similarity measures can be evaluated more precisely when we consider the correlation of their values for ranking purposes.\nWe conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs. The datasets have 750 sentence pairs each, and are extracted from\nthe Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl11. When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correlation on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, noting that the Cosine similarity uses word frequencies when the evaluated version of TextFlow did not use word-level weights.\nIncluding word weights is one of the promising perspectives in line with this work as it could be done simply by making the deltas vary according to the weight/importance of the (un)matched word. Also, in such a setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. figure 3). We conducted a preliminary test using the inverse document frequency of the words as extracted from Wikipedia with Gensim12, which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them. Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances (Pedersen et al., 2004)."
  }, {
    "heading": "5 Conclusion",
    "text": "We presented a novel standalone similarity measure that takes into account continuous word sequences. An evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking. Among the potential extensions of this work are the inclusion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness. We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported in part by the Intramural Research Program of the NIH, National Library of Medicine.\n11goo.gl/NVnybD 12https://radimrehurek.com/gensim/"
  }],
  "year": 2017,
  "references": [{
    "title": "The evaluation of sentence similarity measures",
    "authors": ["Palakorn Achananuparp", "Xiaohua Hu", "Xiajiong Shen."],
    "venue": "Data warehousing and knowledge discovery, Springer, pages 305–316.",
    "year": 2008
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1508.05326 .",
    "year": 2015
  }, {
    "title": "The pascal recognising textual entailment challenge",
    "authors": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."],
    "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, Springer,",
    "year": 2006
  }, {
    "title": "Measures of the amount of ecologic association between species",
    "authors": ["Lee R Dice."],
    "venue": "Ecology 26(3):297– 302.",
    "year": 1945
  }, {
    "title": "Tolerating spelling errors during patient validation",
    "authors": ["Carol Friedman", "Robert Sideli."],
    "venue": "Computers and Biomedical Research 25(5):486–509.",
    "year": 1992
  }, {
    "title": "Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning",
    "authors": ["Vasileios Hatzivassiloglou", "Judith L Klavans", "Eleazar Eskin."],
    "venue": "Proceedings of the 1999 joint sigdat conference on empirical methods",
    "year": 1999
  }, {
    "title": "Algorithms for clustering data",
    "authors": ["Anil K Jain", "Richard C Dubes."],
    "venue": "Prentice-Hall, Inc.",
    "year": 1988
  }, {
    "title": "Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida",
    "authors": ["Matthew A Jaro."],
    "venue": "Journal of the American Statistical Association 84(406):414–420.",
    "year": 1989
  }, {
    "title": "Sentence similarity based on semantic nets and corpus statistics. IEEE transactions on knowledge and data engineering 18(8):1138–1150",
    "authors": ["Yuhua Li", "David McLean", "Zuhair A Bandar", "James D O’shea", "Keeley Crockett"],
    "year": 2006
  }, {
    "title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
    "authors": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"],
    "year": 2014
  }, {
    "title": "Bioinformatics: sequence and genome analysis",
    "authors": ["David W Mount."],
    "venue": "Cold Spring Harbor Laboratory Press.",
    "year": 2004
  }, {
    "title": "Semeval-2016 task 3: Community question answering",
    "authors": ["Preslav Nakov", "Lluı́s Màrquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "Jim Glass", "Bilal Randeree"],
    "year": 2016
  }, {
    "title": "Wordnet:: Similarity: measuring the relatedness of concepts",
    "authors": ["Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi."],
    "venue": "Demonstration papers at HLT-NAACL 2004. Association for Computational Linguistics, pages 38–41.",
    "year": 2004
  }, {
    "title": "Approximate string comparison and its effect on an advanced record linkage system",
    "authors": ["Edward H Porter", "William E Winkler"],
    "venue": "In Advanced record linkage system. US Bureau of the Census, Research Report. Citeseer",
    "year": 1997
  }, {
    "title": "A web-based kernel function for measuring the similarity of short text snippets",
    "authors": ["Mehran Sahami", "Timothy D Heilman."],
    "venue": "Proceedings of the 15th international conference on World Wide Web. AcM, pages 377–386.",
    "year": 2006
  }, {
    "title": "Time warps, string edits, and macromolecules: the theory and practice of sequence comparison",
    "authors": ["David Sankoff", "Joseph B Kruskal."],
    "venue": "Reading: Addison-Wesley Publication, 1983, edited by Sankoff, David; Kruskal, Joseph B. 1.",
    "year": 1983
  }, {
    "title": "Learning to rank short text pairs with convolutional deep neural networks",
    "authors": ["Aliaksei Severyn", "Alessandro Moschitti."],
    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,",
    "year": 2015
  }, {
    "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
    "authors": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "NIPS. volume 24, pages 801–809.",
    "year": 2011
  }, {
    "title": "Improving similarity measures for short segments of text",
    "authors": ["Wen-Tau Yih", "Christopher Meek."],
    "venue": "AAAI. volume 7, pages 1489–1494.",
    "year": 2007
  }, {
    "title": "Learning discriminative projections for text similarity measures",
    "authors": ["Wen-tau Yih", "Kristina Toutanova", "John C Platt", "Christopher Meek."],
    "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Com-",
    "year": 2011
  }],
  "id": "SP:0b9b987fdae057306835b9243c5b7cc54423bf28",
  "authors": [{
    "name": "Yassine Mrabet",
    "affiliations": []
  }, {
    "name": "Halil Kilicoglu",
    "affiliations": []
  }, {
    "name": "Dina Demner-Fushman",
    "affiliations": []
  }],
  "abstractText": "Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",
  "title": "TextFlow: A Text Similarity Measure based on Continuous Sequences"
}