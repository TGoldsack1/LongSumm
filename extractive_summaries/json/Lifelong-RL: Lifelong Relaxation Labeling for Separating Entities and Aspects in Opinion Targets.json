{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 225–235, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "A core problem of opinion mining or sentiment analysis is to identify each opinion/sentiment target and to classify the opinion/sentiment polarity on the target (Liu, 2012). For example, in a review sentence for a car, one wrote “Although the engine is slightly weak, this car is great.” The person is positive (opinion polarity) about the car (opinion target) as a whole, but slightly negative (opinion polarity) about the car’s engine (opinion target).\nPast research has proposed many techniques to extract opinion targets (we will just call them targets\nhereafter for simplicity) and also to classify sentiment polarities on the targets. However, a target can be an entity or an aspect (part or attribute) of an entity. “Engine” in the above sentence is just one aspect of the car, while “this car” refers to the whole car. Note that in (Liu, 2012), an entity is called a general aspect. For effective opinion mining, we need to classify whether a target is an entity or an aspect because they refer to very different things. One can be positive about the whole entity (car) but negative about some aspects of it (e.g., engine) and vice versa. This paper aims to perform the target classification task, which, to our knowledge, has not been attempted before. Although in supervised extraction one can annotate entities and aspects with separate labels in the training data to build a model to extract them separately, in this paper our goal is to help unsupervised target extraction methods to classify targets. Unsupervised target extraction methods are often preferred because they save the time-consuming data labeling or annotation step for each domain.\nProblem Statement: Given a set of opinion targets T = {t1, . . . , tn} extracted from an opinion corpus d, we want to classify each target ti ∈ T into one of the three classes, entity, aspect, or NIL, which are called class labels. NIL means that the target is neither an entity nor an aspect and is used because target extraction algorithms can make mistakes.\nThis paper does not propose a new target extraction algorithm. We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al., 2011), for extraction. We only focus on target classification after the targets have been extracted. Note that an entity here can be a named entity, a prod-\n225\nuct category, or an abstract product (e.g., “this machine” and “this product”). An named entity can be the name of a brand, a model, or a manufacturer. An aspect is a part or attribute of an entity, e.g., “battery” and “price” of the entity “camera”.\nSince our entities not just include the traditional named entities (e.g., “Microsoft” and “Google”) but also other expressions that refer to such entities, traditional named entity recognition algorithms are not sufficient. Pronouns such as “it,” “they,” etc., are not considered in this paper as co-reference resolution is out of the scope of this work.\nWe solve this problem in an unsupervised manner so that there is no need for labor-intensive manual labeling of the training data. One key observation of the problem is that although entities and aspects are different, they are closely related because aspects are parts or attributes of entities and they often have syntactic relationships in a sentence, e.g., “This phone’s screen is super.” Thus it is natural to solve the problem using a relational learning method. We employ the graph labeling algorithm, Relaxation Labeling (RL) (Hummel and Zucker, 1983), which performs unsupervised belief propagation on a graph. In our case, each target extracted from the given corpus d forms a graph node and each relation identified in d between two targets forms an edge. With some initial probability assignments, RL can assign each target node the most probable class label. Although some other graph labeling methods can be applied as well, the key issue here is that just using a propagation method in isolation is far from sufficient due to lack of information from the given corpus, which we detail in Section 5. We then employ Lifelong Machine Learning (LML) (Thrun, 1998; Chen and Liu, 2014b) to make a major improvement.\nLML works as follows: The learner has performed a number learning tasks in the past and has retained the knowledge gained so far. In the new/current task, it makes use of the past knowledge to help current learning and problem solving. Since RL is unsupervised, we can assume that the system has performed the same task on reviews of a large number of products/domains (or corpora). It has also saved all the graphs and classification results from those past domains in a Knowledge Base (KB). It then exploits this past knowledge to help classification in the current task/domain. We call this\ncombined approach of relaxation labeling and LML Lifelong-RL. The approach is effective because there is a significant amount of sharing of targets and target relations across domains.\nLML is different from the classic learning paradigm (supervised or unsupervised) because classic learning has no memory. It basically runs a learning algorithm on a given data in isolation without considering any past learned knowledge (Silver et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning.\nOur experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly."
  }, {
    "heading": "2 Related Work",
    "text": "Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opinion mining can be found in (Liu, 2012; Pang and Lee, 2008).\nOur work is related to transfer learning (Pan and Yang, 2010), which uses the source domain labeled data to help target domain learning, which has little or no labeled data. Our work is not just using a source domain to help a target domain. It is a continuous and cumulative learning process. Each new task can make use of the knowledge learned from all past tasks. Knowledge learned from the new task can also help improve learning of any past task. Transfer learning is not continuous, does not\naccumulate knowledge over time and cannot improve learning in the source domain. Our work is also related to multi-task learning (Caruana, 1997), which jointly optimizes a set of related learning tasks. Clearly, multi-task learning is different as we learn and save information which is more realistic when a large number of tasks are involved.\nOur work is most related to Lifelong Machine Learning (LML). Traditional LML focuses on supervised learning (Thrun, 1998; Ruvolo and Eaton, 2013; Chen et al., 2015). Recent work used LML in topic modeling (Chen and Liu, 2014a), which is unsupervised. Basically, they used topics generated from past domains to help current domain model inference. However, they are just for aspect extraction. So is the method in (Liu et al., 2016). They do not solve our problem. Their LML methods are also different from ours as we use a graph and results obtained in the past domains to augment the current task/domain graph to solve the problem."
  }, {
    "heading": "3 Lifelong-RL: The General Framework",
    "text": "In this section, we present the proposed general framework of lifelong relaxation labeling (LifelongRL). We first give an overview of the relaxation labeling algorithm, which forms the base. We then incorporate it with the LML capability. The next two sections detail how this general framework is applied to our proposed task of separating entities and aspects in opinion targets."
  }, {
    "heading": "3.1 Relaxation Labeling",
    "text": "Relaxation Labeling (RL) is an unsupervised graphbased label propagation algorithm that works iteratively. The graph consists of nodes and edges. Each edge represents a binary relationship between two nodes. Each node ti in the graph is associated with a multinomial distribution P (L(ti)) (L(ti) being the label of ti) on a label set Y . Each edge is associated with two conditional probability distributions P (L(ti)|L(tj)) and P (L(tj)|L(ti)), where P (L(ti)|L(tj)) represents how the label L(tj) influences the label L(ti) and vice versa. The neighbors Ne(ti) of a node ti are associated with a weight distribution w(tj |ti) with ∑ tj∈Ne(ti)w(tj |ti) = 1.\nGiven the initial values of these quantities as inputs, RL iteratively updates the label distribution\nof each node until convergence. Initially, we have P 0(L(ti)). Let ∆P r+1(L(ti)) be the change of P (L(ti)) at iteration r+ 1. Given P r(L(ti)) at iteration r, ∆P r+1(L(ti)) is computed by:\n∆P r+1(L(ti)) = ∑\ntj∈Ne(ti)(w(tj |ti) ·∑y∈Y (P (L(ti)|L(tj) = y)P r(L(tj) = y))) (1) Then, the updated label distribution for iteration\nr + 1, P r+1(L(ti)), is computed as follows:\nP r+1(L(ti)) = P r(L(ti))(1+∆P\nr+1(L(ti)))∑ y∈Y P r(L(ti)=y)(1+∆P r+1(L(ti)=y))\n(2)\nOnce RL ends, the final label of node ti is its highest probable label: L(ti) = argmax\ny∈Y (P (L(ti) = y)).\nNote that P (L(ti)|L(tj)) and w(tj |ti) are not updated in each RL iteration but only P (L(ti)) is. P (L(ti)|L(tj)), w(tj |ti) and P 0(L(ti)) are provided by the user or computed based on the application context. RL uses these values as input and iteratively updates P (L(ti)) based on Equations (1) and (2) until convergence. Next we discuss how to incorporate LML in RL."
  }, {
    "heading": "3.2 Lifelong Relaxation Labeling",
    "text": "For LML, it is assumed that at any time step, the system has worked on u past domain corpora D = {d1, . . . , du}. For each past domain corpus d ∈ D, the same Lifelong-RL algorithm was applied and its results were saved in the Knowledge Base (KB). Then the algorithm can borrow some useful prior/past knowledge in the KB to help RL in the new/current domain du+1. Once the results of the current domain are produced, they are also added to the KB for future use.\nWe now detail the specific types of information or knowledge that can be obtained from the past domains to help RL in the future, which should thus be stored in the KB.\n1. Prior edges: In many applications, the graph is not given. Instead, it has to be constructed based on the data from the new task/domain data du+1. However, due to the limited data in du+1, some edges between nodes that should be present are not extracted from the data. But such edges between the nodes may exist in\nsome past domains. Then, those edges and their associated probabilities can be borrowed.\n2. Prior labels: Some nodes in the current new domain may also exist in some past domains. Their labels in the past domains are very likely to be the same as those in the current domain. Then, those prior labels can give us a better idea about the initial label probability distributions of the nodes in the current domain du+1.\nTo leverage those edges and labels from the past domains, the system needs to ensure that they are likely to be correct and applicable to the current task domain. This is a challenge problem. In the next two sections, we detail how to ensure these to a large extent in our application context along with how to compute those initial probabilities."
  }, {
    "heading": "4 Initialization of Relaxation Labeling",
    "text": "We now discuss how the proposed Lifelong-RL general framework is applied to solve our problem. In our case, each node in the graph is an extracted target ti ∈ T , and each edge represents a binary relationship between two targets. T is the given set of all opinion targets extracted by an extraction algorithm from a review dataset/corpus d. The label set for each target is Y = {entity, aspect,NIL}. In this section, we describe how to use text clues in the corpus d to compute P (L(ti)|L(tj)), w(tj |ti) and P 0(L(ti)). In the next section, we present how these quantities are improved using prior knowledge from the past domains in the LML fashion."
  }, {
    "heading": "4.1 Text Clues for Initialization",
    "text": "We use two kinds of text clues, called type modifiers M(t) and relation modifiers MR to compute the initial label distribution P (L(ti)) and conditional label distribution P (L(ti)|L(tj)) respectively.\nType Modifier: This has two kinds MT = {mE ,mA}, where mE and mA represent entity modifier and aspect modifier respectively. For example, the word “this” as in “this camera is great” indicates that “camera” is probably an entity. Thus, “this” is a type modifier indicating M(camera) = mE . “These” is also a type modifier. Aspect modifier is implicitly assumed when the number of appearances of entity modifiers is less than or equal to a threshold (see Section 4.2).\nRelation Modifier: Given two targets, ti and tj , we use Mtj (ti) to denote the relation modifier that the label of target ti is influenced by the label of target tj . Relation modifiers are further divided into 3 kinds: MR = {mc,mA|E ,mE|A}.\nConjunction modifier mc: Conjoined items are usually of the same type. For example, in “price and service”, “and service” indicates a conjunction modifier for “price” and vice versa.\nEntity-aspect modifier mA|E : A possessive expression indicates an entity and an aspect relation. For example, in “the camera’s battery”, “camera” indicates an entity-aspect modifier for “battery”.\nAspect-entity modifier mE|A: Same as above except that “battery” indicates an aspect-entity modifier for “camera”.\nModifier Extraction: These modifiers are identified from the corpus d using three syntactic rules. “This” and “these” are used to extract type modifier M(t) = mE . CmE (t) is the occurrence count of that modifier on target t, which is used in determining the initial label distribution in Section 4.2.\nRelation modifiers are identified by dependency relations conj(ti, tj) and poss(ti, tj) using the Stanford Parser (Klein and Manning, 2003). Each occurrence of a relation rule contributes one count of Mtj (ti) for ti and one count of Mti(tj) for tj . We use Cmc,tj (ti), CmA|E ,tj (ti) and CmE|A,tj (ti) to denote the count of tj modifying ti with conjunction, entity-aspect and aspect-entity modifiers respectively. For example, “price and service” will contribute one count to Cmc,price(service) and one count to Cmc,service(price). Similarly, “camera’s battery” will contribute one count to CmA|E ,camera(battery) and one count to CmE|A,battery(camera)."
  }, {
    "heading": "4.2 Computing Initial Probabilities",
    "text": "The initial label probability distribution of target t is computed based on CmE (t), i.e.,\nP 0(L(t)) = { PmE (L(t)) if CmE (t) > α PmA(L(t)) if CmE (t) ≤ α\n(3) Here, we have two pre-defined distributions: PmE and PmA , which have a higher probability on entity and aspect respectively. The parameter α is a threshold indicating that if the entity modifier rarely occurs, the target is more likely to be an aspect. These\nvalues are set empirically (see Section 6). Let term q(Mtj (ti) = m) be the normalized weight on the count for each kind of relation modifier m ∈MR:\nq(Mtj (ti) = m) = Cm,tj (ti)\nCtj (ti) (4)\nwhere Ctj (ti) = ∑\nm∈MR Cm,tj (ti). The conditional label distribution P (L(ti)|L(tj)) of ti given the label of tj is the weighted sum over the three kinds of relation modifiers:\nP (L(ti)|L(tj)) = q(Mtj (ti) = mc) · Pmc(L(ti)|L(tj)) +q(Mtj (ti) = mA|E) · PmA|E (L(ti)|L(tj)) +q(Mtj (ti) = mE|A) · PmE|A(L(ti)|L(tj)) (5)\nwhere Pmc , PmA|E , and PmE|A are pre-defined conditional distributions. They are filled with values to model the label influence from neighbors and can be found in Section 6.\nFinally, target ti’s neighbor weight for target tj , i.e., w(tj |ti), is the ratio of the count of relation modifiers Ctj (ti) over the total of all ti’s neighbors:\nw(tj |ti) = Ctj (ti)∑\ntj′∈Ne(ti)Ctj′ (ti) (6)\nIf Ctj (ti) = 0, ti and tj has no edge between them."
  }, {
    "heading": "5 Using Past Knowledge in Lifelong-RL",
    "text": "Due to the fact that the review corpus du+1 in the current task domain may not be very large and that we use high quality syntactic rules to extract relations to build the graph to ensure precision, the number of relations extracted can be small and insufficient to produce a graph that is information rich with accurate initial probabilities. We thus apply LML to help using knowledge learned in the past. The proposed LML process in Lifelong-RL for our task is shown in Figure 1.\nOur prior knowledge includes type modifiers, relation modifiers and labels of targets obtained from past domains in D. Each record in the KB is stored as a 9-tuple: (d, ti, tj ,M d(ti),M d(tj), C d m,tj (ti), C d m,ti(tj), L d(ti), L d(tj)) where d ∈ D is a past domain; ti and tj are two targets; Md(ti), Md(tj) are their type\nmodifiers, Cdm,tj (ti) and C d m,ti(tj) are counts for relation modifiers; Ld(ti) and Ld(tj) are labels decided by RL. For example, the sentence “This camera’s battery is good” forms: (d, camera, battery,mE ,mA, CmE|A,battery(camera) = 1, CmA|E ,camera(battery) = 1, entity, aspect) . It means that in the past domain d, “camera” and “battery” are extracted targets. Since “camera” is followed by “this”, its type modifier is mE . Since “battery” is not identified by an entity modifier, it is mA. The pattern “camera’s battery” contributes one count for both relation modifiers CmE|A,battery(camera) and CmA|E ,camera(battery). RL has labeled “camera” as entity and “battery” as aspect in d.\nThe next two subsections present how to use the knowledge in the KB to improve the initial assignments for the label distributions, conditional label distributions and neighborhood weight distributions in order to achieve better final labeling/classification results for the current/new domain du+1."
  }, {
    "heading": "5.1 Exploiting Relation Modifiers in the KB",
    "text": "If two targets in the current domain corpus have no edge, we can check whether relation modifiers of the same two targets exist in some past domains. If so, we may be able to borrow them. But to ensure suitability, two consistency checks are performed.\nLabel Consistency Check: Since RL makes mistakes, we need to ensure that relation modifiers in a record in the KB are consistent with target labels in that past domain. For example, “camera’s battery” is confirmed by “camera” being labeled as entity and “battery” being labeled as aspect in a past domain d ∈ D. Without this consistency, the record may not be reliable and should be discarded from the KB.\nWe define an indicator variable Idm,tj (ti) to ensure that the record r’s relation modifier is consistent\nwith the labels of its two targets:\nIdmA|E ,tj (ti) =    1 if CdmA|E ,tj (ti) > 0 and Ld(ti) = aspect and Ld(tj) = entity\n0 otherwise\n(7)\nFor example, if “camera” is labeled as entity and “battery” is labeled as aspect in the past domain d, we have IdmA|E ,camera(battery) = 1 and IdmE|A,battery(camera) = 1.\nType Consistency Check: Here we ensure the type modifiers for two targets in the current domain du+1 are consistent with these type modifiers in the past domain d ∈ D. This is because an item can be an aspect in one domain but an entity in another. For example, if the current domain is “Cellphone”, borrowing the relation “camera’s battery” from domain “Camera” can introduce an error because “camera” is an aspect in domain “Cellphone”.\nSyntactic pattern “this” is a good indicator for this checking. In the “Cellphone” domain, “its camera” or “the camera” are often mentioned but not “this camera”. In the “Camera” domain, “this camera” is often mentioned. The type modifier of “camera” in “Cellphone” is mA, but in “Camera” it is mE .\nUpdating Probabilities in Current Domain du+1: Edges for RL are in the forms of conditional label distribution P (L(ti)|L(tj)) and neighborhood weight distribution w(tj |ti). We now discuss how to use the KB to estimate them more accurately.\nUpdating Conditional Label Distribution: Equation (5) tells that conditional label distribution P (L(ti)|L(tj)) is the weighted sum of relation modifiers’ label distributions Pmc , PmA|E , and PmE|A . These 3 label distributions are pre-defined and given in Table 2. They are not changed. Thus, we update conditional label distribution through updating the three relation modifiers’ weights q(Mtj (ti)) with the knowledge in the KB. Recall the three relation modifiers are MR = {mc,mA|E ,mE|A}.\nAfter consistency check, there can be multiple relation modifiers between two targets in similar past domains Ds ⊂ D. The number of domains supporting a relation modifier m ∈ MR can tell which kind of relation modifiers is common and likely to be correct. For example, given many past domains like “Laptop”, “Tablet”, “Cellphone”, etc., “camera\nand battery” appears more than “camera’s battery”, “camera” should be modified by “battery” more with mE|A rather than mc (likely to be an aspect).\nLet Cdu+1m,tj (ti) be the count that target ti modified by target tj on relation m in the current domain du+1 (not in KB). The count C(CL) is for updating the Conditional Label (CL) distributions considering the information in both the current domain du+1 and the KB. It is calculated as:\nC (CL) m,tj (ti) =\n{ C du+1 m,tj (ti) if C du+1 m,tj\n(ti) > 0∑ d∈Ds Idm,tj (ti) if ∑ m∈MR C du+1 m,tj (ti)) = 0\nThis equation says that if there is any relation modifier existing between the two targets in the new domain du+1, we do not borrow edges from the KB; Otherwise, the number of similar past domains supporting the relation modifier m is used. Recall that Idm,tj (ti) is the result calculated by Equation (7) after label consistency check.\nWe use count C(CL)m,tj (ti) to update q du+1(Mtj (ti)) using Equation (4) in Section 4.2. Then the conditional label distribution accommodating relation modifiers in the KB, P (LL1)(L(ti)|L(tj)), is calculated by Equation, (5) using qdu+1(Mtj (ti)). LL1 denotes Lifelong Learning 1.\nUpdating Neighbor Weight Distribution: Equation (6) says that w(tj |ti) is the importance of target ti’s neighbor tj to ti among all ti’s neighbors. When updating conditional label distribution using the KB, the number of domains can decide which kind of relation modifiersm is more common between the two targets ti and tj . But we cannot tell that neighbor tj is more important than another neighbor tj′ to ti.\nFor example, given the past domains such as “Laptop”, “Tablet”, “Cellphone”, etc., no matter how many domains believe “camera” is an aspect given “battery” is also an aspect, if the current domain is “All-in-one desktop computer”, we should not consider the strong influences from “battery” in the past domains. We should rely more on the weights of “camera”’s neighbors provided by “Allin-one desktop computer”. That means “mouse”, “keyboard”, “screen” etc., should have strong influences on “camera” than “battery” because most Allin-one desktops (e.g. iMac) do not have battery.\nWe introduce another indicator variable IDm,tj (ti) = ⋃ d∈Ds Idm,tj (ti), to indicate whether target tj modified ti on relation m in past similar domains Ds. It only considers the existence of a\nrelation modifier m among domains Ds. The count C(w)tj (ti) for updating the neighbor weight (w) distribution considers both the KB and the current domain du+1. It is as follows:\nC (w) tj (ti) =\n{ ∑ m∈MR C du+1 m,tj (ti) if ∑ m∈MR C du+1 m,tj\n(ti) > 0∑ m∈MR I Du m,tj (ti) if ∑ m∈MR C du+1 m,tj (ti) = 0\nThis equation tells that if there are relation modifiers existing between the two targets in the new domain du+1, we count the total times that tj modifies ti in the new domain; Otherwise, we count the total kinds of relation modifiers in MR if a relation modifier m ∈ MR existed in past domains. Let w(LL1)(tj |ti) be the neighbor weight distribution considering knowledge from the KB and du+1. It is calculated by Equation (6) using C(w)tj (ti).\nThe initial label distribution P du+1,0 is calculated by Equation (3) only using type modifiers found in the new domain du+1. We use Lifelong-RL-1 to denote the method that employs P (LL1)(L(ti)|L(tj)), w(LL1)(tj |ti) and P du+1,0 as inputs for RL."
  }, {
    "heading": "5.2 Exploiting Target Labels in the KB",
    "text": "Since we have target labels from past domains, we may have a better idea about the initial label probabilities of targets in the current domain du+1. For example, after labeling domains like “Cellphone”, “Laptop”, “Tablet,” and “E-reader”, we may have a good sense that “camera” is likely to be an aspect. To use such knowledge, we need to check if the type modifier of target t in the current domain matches those in past domains and only keep those domains that have such a matching type modifier.\nLet Ds ⊂ D be the past domains consistent with target t’s type modifier in the current domain du+1. Let CD s (L(t)) be the number of domains in Ds that target t is labeled as L(t). Let λ be the ratio that controls how much we trust knowledge from the KB. Then the initial label probability distribution P du+1,0 calculated by Equation (3) only using type modifier found in du+1 is replaced by :\nP (LL2),0(L(t)) = |D|×P du+1,0(L(t))+λCD\ns (L(t))\n|D|+λ|D| (8)\nSimilarly, let Ds ⊂ D be the past domains consistent with both targets ti’s and tj’s type modifiers in du+1. Let CD s (L(ti), L(tj)) be the number of domains inDs that ti and tj are labeled as L(ti) and\nL(tj) respectively. The conditional label probability distribution accommodating relation modifiers in the KB, P (LL1)(L(ti)|L(tj)), is further updated to P (LL2)(L(ti)|L(tj)) by exploiting the target labels in KB (LL2 denotes Lifelong Learning 2):\nP (LL2)(L(ti)|L(tj)) = |D|×P (LL1)(L(ti)|L(tj))+λCD s (L(ti),L(tj))\n|D|+λ|D| (9)\nFor example, given “this camera”, “battery” in the current domain, we are more likely to consider domains (e.g. “Film Camera”, “DSLR”, but not “Cellphone”) that have entity modifiers on “camera” and aspect modifiers on “battery”. Then we count the number of those domains that label “camera” as entity and “battery” as aspect: CD s (L(camera) = entity, L(battery) = aspect). Similarly, we count domains having other types of target labels on “camera” and “battery”. These counts form an updated conditional label distribution that estimates “camera” as an entity and “battery” as an aspect.\nNote that |D − Ds|, the number of past domains not consistent with targets’ type modifiers, is added to CD s (L(ti) = NIL) and CD s (L(ti) = NIL, L(tj)) for Equations (8) and (9) respectively to make the sum over L(ti) equal to 1. We use Lifelong-RL to denote this method which uses P (LL2),0(L(t)), P (LL2)(L(ti)|L(tj)) and w(LL1)(tj |ti) as input for RL."
  }, {
    "heading": "6 Experiments",
    "text": "We now evaluate the proposed method and compare with baselines. We use the DP method for target extraction (Qiu et al., 2011). This method uses dependency relations between opinion words and targets to extract targets using seed opinion words. Since our paper does not focus on extraction, interested readers can refer to (Qiu et al., 2011) for details."
  }, {
    "heading": "6.1 Experiment Settings",
    "text": "Evaluation Datasets: We use two sets of datasets. The first set consists of eight (8) annotated review datasets. We use each of them as the new domain data in LML to compute precision, recall, F1 scores. Five of them are from (Hu and Liu, 2004), and the remaining three are from (Liu et al., 2016). They have been used for target extraction, and thus have annotated targets, but no annotation on whether a\ntarget is an entity or aspect. We made this annotation, which is straightforward. We used two annotators to annotate the datasets. The Cohen’s kappa is 0.84. Through discussion, the annotators got complete agreement. Details of the datasets are listed in Table 1. Each cell is the number of distinct terms. These datasets are not very large but they are realistic because many products do not have a large number of reviews.\nThe second set consists of unlabeled review datasets from 100 diverse products or domains (Chen and Liu 2014). Each domain has 1000 reviews. They are treated as past domain data in LML since they are not annotated and thus cannot be used for computing evaluation measures.\nEvaluating Measures: We mainly use precision P , recall R, and F1-score F1 as evaluation measures. We take multiple occurrences of the same target as one count, and only evaluate entities and aspects. We will also give the accuracy results.\nCompared Methods: We compare the following methods, including our proposed method, LifelongRL.\nNER+TM: NER is Named Entity Recognition.\nWe can regard the extracted terms from a NER system as entities and the rest of the targets as aspects. However, a NER system cannot identify entities such as “this car” from “this car is great.” Its result is rather poor. But our type modifier (TM) does that, i.e., if an opinion target appears after “this” or “these” in at least two sentences, TM labels the target as an entity; otherwise an aspect. However, TM cannot extract named entities. Its result is also rather poor. We thus combine the two methods to give NER+TM as they complement each other very well. To make NER more powerful, we use two NER systems: Stanford-NER 1(Manning et al., 2014) and UIUC-NER2 (Ratinov and Roth, 2009). NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects.\nNER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT). For a new task, if any target word is in the list, it is treated as an entity; otherwise an aspect.\nRL: This is the base method described in Section 3. It performs relaxation labeling (RL) without the help of LML.\nLifelong-RL-1: This performs LML with RL but the current task only uses the relations in the KB from previous tasks (Section 5.1).\nLifelong-RL: This is our proposed final method. It improves Lifelong-RL-1 by further incorporating target labels in the KB from previous tasks (Section 5.2).\nParameter Settings: RL has 2 initial label distributions PmE and PmA and 3 conditional label distributions Pmc , PmE|A and PmA|E . Like other belief propagation algorithms, these probabilities need to be set empirically, as shown in Table 2. The parameter α is set to 1. Our LML method has one parameter λ for Lifelong-RL. We set it to 0.1."
  }, {
    "heading": "6.2 Results Analysis",
    "text": "Table 3 shows the test results of all systems in precision, recall and F1-score except NER+TM+DICT. NER+TM+DICT is not included due to space limitations and because it performed very poorly. The reason is that a target can be an entity in one domain\n1http://nlp.stanford.edu/software/CRF-NER.shtml 2https://cogcomp.cs.illinois.edu/page/software view/NETagger\nbut an aspect in another. Its average F1-score for entity is only 49.2, and for aspect is only 50.2.\nEntity Results Comparison: We observe from the table that although NER+TM combines NER and TM, its result for entities is still rather poor. We notice that phrases like “this price” causes low precision. Since it does not use many other relations and NER does not recognize many named entities that are written in lower case letters (e.g., “apple is good”), its recall is also low.\nRL has a higher precision as it considers relation modifiers. However, its recall is low because it lacks information in its graph, which causes RL to make many wrong decisions. Lifelong-RL-1 introduces relation modifiers in KB from past domains into the current task. Both precision and recall increase markedly.\nLifelong-RL improves Lifelong-RL-1 further by considering target labels of past domains. Their counts improve the initial label probability distributions and conditional label probability distributions. For example, “this price” may appear in some domains but “price”’s target label is mostly aspect. We consider their counts in initial label distributions and thus rectify the initial distribution of “price”. This makes “price” easier to be classified as aspect and thus improves the precision for entity.\nAspect Results Comparison: For aspects, the trend is the same but the improvements are not as dramatic as for entity. This is because the distribution of entity and aspect in the data is highly skewed. There are many more aspects than entities as we can see from the Table 1. When an entity term is wrongly classified as an aspect, it has much less impact on the aspect result than on the entity result.\nAccuracy Results Comparison: Table 4 gives the classification accuracy results considering all\nthree classes. We can see the similar trend. NER+TM+DICT’s average accuracy is only 45.89 and is not included in the table."
  }, {
    "heading": "7 Conclusion",
    "text": "This paper studied the problem of classifying opinion targets into entities and aspects. To the best of our knowledge, this problem has not been attempted in the unsupervised opinion target extraction setting. But this is an important problem because without separating or classifying them one will not know whether an opinion is about an entity as a whole or about a specific aspect of an entity. This paper proposed a novel method based on relaxation labeling and the paradigm of lifelong machine learning to solve the problem. Experimental results showed the effectiveness of the proposed method."
  }, {
    "heading": "Acknowledgments",
    "text": "This work was partially supported by National Science Foundation (NSF) grants IIS-1407927 and IIS1650900, and NCI grant R01CA192240. The content of the paper is solely the responsibility of the authors and does not necessarily represent the official views of the NSF or NCI."
  }],
  "year": 2016,
  "references": [{
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Machine learn-",
    "year": 1997
  }, {
    "title": "Fine granular aspect",
    "authors": ["Lei Fang", "Minlie Huang"],
    "year": 2012
  }, {
    "title": "Syntactic patterns versus word alignment: Extracting opinion targets from online reviews",
    "authors": ["Kang Liu", "Liheng Xu", "Jun Zhao."],
    "venue": "ACL (1), pages 1754– 1763.",
    "year": 2013
  }, {
    "title": "Improving opinion aspect extraction using semantic similarity and aspect associations",
    "authors": ["Qian Liu", "Bing Liu", "Yuanlin Zhang", "DooSoon Kim", "Zhiqiang Gao."],
    "venue": "AAAI.",
    "year": 2016
  }, {
    "title": "Sentiment analysis and opinion mining",
    "authors": ["Bing Liu."],
    "venue": "Synthesis lectures on human language technologies, 5(1):1–167.",
    "year": 2012
  }, {
    "title": "The Stanford CoreNLP natural language processing toolkit",
    "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."],
    "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55–60.",
    "year": 2014
  }, {
    "title": "Topic sentiment mixture: Modeling facets and opinions in weblogs",
    "authors": ["Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai."],
    "venue": "WWW ’07, pages 171–180.",
    "year": 2007
  }, {
    "title": "Open domain targeted sentiment",
    "authors": ["Margaret Mitchell", "Jacqui Aguilar", "Theresa Wilson", "Benjamin Van Durme."],
    "venue": "ACL ’13, pages 1643–1654.",
    "year": 2013
  }, {
    "title": "Aspect extraction through semi-supervised modeling",
    "authors": ["Arjun Mukherjee", "Bing Liu."],
    "venue": "ACL ’12, volume 1, pages 339–348.",
    "year": 2012
  }, {
    "title": "A survey on transfer learning",
    "authors": ["Sinno Jialin Pan", "Qiang Yang."],
    "venue": "Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.",
    "year": 2010
  }, {
    "title": "Opinion mining and sentiment analysis",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Foundations and trends in information retrieval, 2(1-2):1–135.",
    "year": 2008
  }, {
    "title": "Extracting product features and opinions from reviews",
    "authors": ["Ana-Maria Popescu", "Orena Etzioni."],
    "venue": "Natural language processing and text mining, pages 9–28. Springer.",
    "year": 2007
  }, {
    "title": "A rule-based approach to aspect extraction from product reviews",
    "authors": ["Soujanya Poria", "Erik Cambria", "Lun-Wei Ku", "Chen Gui", "Alexander Gelbukh."],
    "venue": "SocialNLP 2014, page 28.",
    "year": 2014
  }, {
    "title": "Opinion word expansion and target extraction through double propagation",
    "authors": ["Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen."],
    "venue": "Computational linguistics, 37(1):9–27.",
    "year": 2011
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["L. Ratinov", "D. Roth."],
    "venue": "CoNLL, 6.",
    "year": 2009
  }, {
    "title": "Active task selection for lifelong machine learning",
    "authors": ["Paul Ruvolo", "Eric Eaton."],
    "venue": "AAAI.",
    "year": 2013
  }, {
    "title": "Lifelong machine learning systems: Beyond learning algorithms",
    "authors": ["Daniel L Silver", "Qiang Yang", "Lianghao Li."],
    "venue": "AAAI Spring Symposium: Lifelong Machine Learning, pages 49–55.",
    "year": 2013
  }, {
    "title": "Lifelong learning algorithms",
    "authors": ["Sebastian Thrun."],
    "venue": "Learning to learn, pages 181–209. Springer.",
    "year": 1998
  }, {
    "title": "Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing",
    "authors": ["Bo Wang", "Houfeng Wang."],
    "venue": "IJCNLP ’08, pages 289–295.",
    "year": 2008
  }, {
    "title": "Latent aspect rating analysis on review text data: A rating regression approach",
    "authors": ["Hongning Wang", "Yue Lu", "Chengxiang Zhai."],
    "venue": "KDD ’10, pages 783– 792.",
    "year": 2010
  }, {
    "title": "Phrase dependency parsing for opinion mining",
    "authors": ["Yuanbin Wu", "Qi Zhang", "Xuanjing Huang", "Lide Wu."],
    "venue": "EMNLP ’09, pages 1533–1541.",
    "year": 2009
  }, {
    "title": "Extracting and ranking product features in opinion documents",
    "authors": ["Lei Zhang", "Bing Liu", "Suk Hwan Lim", "Eamonn O’Brien-Strain"],
    "venue": "In COLING ’10: Posters,",
    "year": 2010
  }, {
    "title": "Collective opinion target extraction in chinese microblogs",
    "authors": ["Xinjie Zhou", "Xiaojun Wan", "Jianguo Xiao."],
    "venue": "EMNLP, volume 13, pages 1840–1850.",
    "year": 2013
  }, {
    "title": "Movie review mining and summarization",
    "authors": ["Li Zhuang", "Feng Jing", "Xiao-Yan Zhu."],
    "venue": "CIKM ’06, pages 43–50.",
    "year": 2006
  }],
  "id": "SP:60e858403707aec0f677c79f5ae8681173803d25",
  "authors": [{
    "name": "Lei Shu",
    "affiliations": []
  }, {
    "name": "Bing Liu",
    "affiliations": []
  }, {
    "name": "Hu Xu",
    "affiliations": []
  }, {
    "name": "Annice Kim",
    "affiliations": []
  }],
  "abstractText": "It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.",
  "title": "Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets"
}