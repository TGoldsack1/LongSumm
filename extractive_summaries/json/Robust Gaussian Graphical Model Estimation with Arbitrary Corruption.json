{
  "sections": [{
    "text": "2 for each variable satisfies n\n2 . pn/plog d, where n is the sample size and d is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data."
  }, {
    "heading": "1 Introduction",
    "text": "Gaussian graphical models (GGMs) have attracted increasing attention in recent years, especially in the field of high-dimensional statistical learning. In Gaussian graphical models, a d-dimensional random vector X = (X\n1 , . . . , Xd)> follows a multivariate normal distribution Nd(0,⌃⇤). It corresponds to the vertex set V = {1, . . . , d} of an undirected graph G = (V,E), where the edge set E describes the conditional independence relationships between nodes X\n1 , . . . , Xd. It is well-known that the graph G is encoded by the sparsity pattern of the precision matrix ⇥ ⇤ = ⌃\n⇤ 1. More specifically, no edge connects Xi and Xj if and only if ⇥⇤ij = 0. Consequently, estimation of\n1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA. Correspondence to: Quanquan Gu <qg5w@virginia.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe precision matrix ⇥⇤ corresponds to parameter estimation, and specifying the non-zero set of ⇥⇤ corresponds to graphical model selection (Cox & Wermuth, 1996).\nIn the high-dimensional settings, where the number of variables d can exceed the number of observations n, a large body of literature has studied the problem of precision matrix estimation in Gaussian graphical models and their variants (Meinshausen & Bühlmann, 2006; Yuan & Lin, 2007; Friedman et al., 2008; Banerjee et al., 2008; Yuan, 2010; Cai et al., 2011; Wang et al., 2016; Xu & Gu, 2016; Xu et al., 2016; 2017). For instance, Meinshausen & Bühlmann (2006) developed a neighborhood pursuit approach for estimating conditional independence relationship separately for each node in the graph. This method estimates the precision matrix by solving a collection of sparse regression problems using Lasso in parallel. Yuan & Lin (2007); Friedman et al. (2008); Banerjee et al. (2008) proposed a `\n1 norm regularized Gaussian negative log-likelihood method, which called Graphical Lasso (GLasso), to directly estimate the precision matrix. More recently, Yuan (2010); Cai et al. (2011) proposed the graphical Dantzig selector and CLIME, respectively. Both of these methods can be solved by linear programming and have more favorable theoretical properties than GLasso.\nNote that most of the aforementioned methods rely on the assumption that the observations follow a Gaussian distribution. There also exists some work, such as Ravikumar et al. (2011), studied sub-Gaussian data under bounded higher order moments. However, in many real-word applications, the data can follow a heavy-tailed distribution, or may even be corrupted arbitrarily. In such cases, conventional methods yield inaccurate graph estimation even if there are only a few contaminated observations due to the lack of robustness. In order to address this issue, a large body of literature (Liu et al., 2012; Finegold & Drton, 2011; Hirose & Fujisawa, 2015; Sun & Li, 2012; Yang & Lozano, 2015; Balmand & Dalalyan, 2015; Öllerer & Croux, 2015; Loh & Tan, 2015; Chen et al., 2015; Tarr et al., 2016) has focused on providing more robust estimators for precision matrices in the past years. However, most of these estimators were established under some specific contamination models, thus they are not good at dealing with the situation when data are arbitrarily corrupted.\nIn this paper, we propose a robust estimator to estimate the precision matrix in high-dimensional GGMs with arbitrarily corrupted data. More specifically, we consider the situation that the corrupted data can appear in any coordinates of the observations. This includes situations that some observations are outliers or data follow some specific contamination models as special cases. The definition of the arbitrary corruption model will be presented in section 3. The key idea of our method is to use a robust covariance matrix estimator, which remains accurate provided a controlled number of arbitrarily corrupted coordinates. Our theory provides not only the spectral norm based estimation error of the proposed estimator, but also the model selection consistency guarantee. More importantly, we show that provided that the number of corrupted samples n\n2 for each variable satisfies n\n2 . pn/plog d, where n is the sample size and d is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. Beyond point estimation, we also propose a hypothesis testing procedure to assess the uncertainty of our robust estimator with corrupted observations, and construct the confidence interval for the point estimate. Thorough experiments on both synthetic data and real-world genomic data corroborate the effectiveness of our method.\nThe remainder of this paper is organized as follows: In Section 2, we discuss some more related work about the robust precision matrix estimation. Section 3 summarizes our proposed estimation method and testing procedure in general and also introduces some necessary backgrounds. Section 4 presents our main results including estimation error bound and inference property. Section 5 provides numerical results, for our method and a number of other methods, of some simulated datasets and a real example on gene expression data. Section 6 concludes with discussion.\nNotation Let A = [Aij ] 2 Rd⇥d be a d ⇥ d matrix and x = [x\n1 , . . . , xd]> 2 Rd be a d-dimensional vector. For 0 < q < 1, we define the `\n0 , `q and `1 vector norms as kxk\n0\n= Pd i=1 1{xi 6= 0}, kxkq =\n( Pd i=1 |xi|q) 1 q , kxk1 = max1id |xi|, where 1{·} represents the indicator function. We use the following notations for the matrix `q , `max, `1,1 and `F norms: kAkq = maxkxkq=1 kAxkq, kAk1,1 = maxij |Aij |, kAk1,1 = Pd\ni=1 Pd j=1 |Aij |, kAkF = ( P ij |Aij |2)1/2. We use A⇤j = (A1j , . . . , Adj)> to denote the j-th column vector of A and A⇤\\j to denote the submatrix of A with the jth column A⇤j removed. We also denote by max(A) and min\n(A) the largest and smallest eigenvalues of matrix A, respectively. Furthermore, for a matrix ⇥ and sets of tuples S, S\n1 , ⇥S1,S denotes the set of numbers (⇥jk)j2S1,k2S . We define the maximum degree of a graph or row cardinality as s = max\n1in |{j 2 V | ⇥⇤ij 6= 0}|, where\nV = {1, . . . , d} is the vertex set. Finally, for a sequence of random variables Xn, we write Xn\nd ! X , for some random variable X , if Xn converges in distribution to X ."
  }, {
    "heading": "2 Related Work",
    "text": "In recent years, some attempts have been made toward the robust estimation of high-dimensional GGMs under different corruption models. For example, to deal with heavy tailed distributions, Liu et al. (2012) developed a semiparametric approach called the nonparanormal SKEPTIC. Finegold & Drton (2011) proposed a penalized likelihood approach based on multivariate t-distributions. They also proposed an alternative t-model which requires the use of variational EM or Markov chain Monte Carlo algorithms. Hirose & Fujisawa (2015) introduced a robust estimation procedure for sparse precision matrices based on the penalized negative -likelihood function.\nIn order to address outliers, Sun & Li (2012) proposed a robust estimation of GGMs via a robustified likelihood function with `\n1 penalization. In particular, they first use coordinate descent to efficiently estimate the structure of the precision matrix. Then, based on the estimated structure, they re-estimate the parameters of the precision matrix using iterative proportional fitting algorithm to ensure the positive definiteness of their estimator. Yet their method does not have any theoretical guarantee. Yang & Lozano (2015) proposed a trimmed Graphical Lasso method. Specifically, by adding weights to different data points, they improved upon the original graphical Lasso such that it is more robust to outliers. However, they did not provide any model selection consistency guarantee. Balmand & Dalalyan (2015) also studied the problem of robustly estimating the covariance matrix when data are corrupted by outliers. In particular, they proposed to use a modified scaled lasso procedure for covariance matrix estimation and provided the theoretical guarantee of their method.\nAnother line of related work is Öllerer & Croux (2015); Loh & Tan (2015); Chen et al. (2015); Tarr et al. (2016), which studied the problem of robust precision matrix estimation in high dimensions under the ✏-contamination model. In particular, under the cell-wise contamination model, Tarr et al. (2016) evaluated the performance of the Glasso and CLIME estimators together with a U-statistic based robust covariance estimator for sparse precision matrix estimation. Under the same contamination model, Öllerer & Croux (2015) provided an analysis for the robustness of these estimators in terms of breakdown behavior. Later on, from the point of statistical consistency, Loh & Tan (2015) established the statistical error bounds for these estimators. However, these methods (Öllerer & Croux, 2015; Loh & Tan, 2015; Tarr et al., 2016) highly\ndepend on the specific cell-wise contamination structure on the data matrix. Recently, inspired by Tukey’s depth estimator (Tukey, 1975) for vector estimation, Chen et al. (2015) introduced the concept of matrix depth and proposed a robust covariance matrix estimator using empirical depth function. They showed that their proposed estimator can achieve minimax optimal statistical rate under Huber’s ✏-contamination model. However, it is computationally very expensive to compute the deepest depth of a matrix even in a moderate dimension, which makes such method infeasible in the high-dimensional regime.\nAll the aforementioned methods are limited to data with heavy tails and outliers. Therefore, they are not suitable to deal with data that are arbitrarily corrupted."
  }, {
    "heading": "3 Problem Setup and Estimation Method",
    "text": "In this section, we first introduce the setup of our problem, then we present our proposed estimation method and hypothesis testing procedure."
  }, {
    "heading": "3.1 Problem Setup",
    "text": "Let X = (X 1 , . . . , Xd)> be a d-dimensional multivariate Gaussian random vector with zero mean and covariance matrix ⌃⇤. It is associated with an undirected graph G = (V,E) with vertex set V = (1, . . . , d) corresponding to random variables and edge set E = {(j, k) | j 6= k,⇥⇤jk 6= 0} describing the connections of nodes, where ⇥ ⇤ = ⌃ ⇤ 1 is the precision matrix.\nSuppose we have n i.i.d. observations X 1 , . . . ,Xn, each of which is drawn from the multivariate Gaussian distribution Nd(0,⌃⇤). Let X = [X1, . . . ,Xn]> 2 Rn⇥d be the data matrix and there may exist arbitrary corruption of the data matrix X. More specifically, for each variable/column of data matrix X, we allow at most n\n2 coordinates to be arbitrarily corrupted, and we call this kind of corruption model as the arbitrary corruption model. Note that under the arbitrary corruption model, we do not require the corrupted entries lie in the same n\n2 rows. Clearly, a special case of the arbitrary corruption model is the outlier model where the corruption appears in n\n2 observations. Under the arbitrary corruption model, n\n2 is the upper bound on the number of corruptions for each variable, and under the outlier model, n\n2 is the upper bound on the number of outliers. Specifically, under the outlier model, the set of row indices {1, . . . , n} of the data matrix X is divided into two disjoint subsets A and O with |A| = n\n1 , |O| = n 2 , and n = n\n1 + n 2 . XA denotes samples drawn from the authentic distribution. XO denotes samples that are outliers. In general, there is no constraint on the type of corruptions in our setting except an upper bound on the number of corruptions, i.e., n\n2 . For example, these corruptions could be drawn from other distributions or even be deterministic."
  }, {
    "heading": "3.2 Estimation Method",
    "text": "Before we introduce our estimation method, we first introduce the truncated inner product which was proposed by Chen et al. (2013). The truncated inner product hu,vin2 is defined as follows: given two n-dimensional vectors u,v 2 Rn, and the truncation number n\n2 satisfying n 2\n n, we first compute the quantity qi = uivi, for i = 1, . . . , n. Then we sort {|qi|}ni=1 and select the smallest (n n\n2 ) ones. Let ⌦ be the set of selected indices with cardinality |⌦| = n n\n2 , then we have the truncated inner product as hu,vin2 = P i2⌦ qi.\nThe main idea of our estimation method is to use a robust covariance matrix estimator which can mitigate the impact of arbitrary corruptions. More specifically, given a data matrix X 2 Rn⇥d, which is arbitrarily corrupted, we obtain the robust covariance matrix estimator b⌃ through a truncation procedure that each element b⌃jk is calculated via truncated inner product hX⇤j ,X⇤kin2/n1. The motivation of this truncation procedure is that the corrupted coordinates with large magnitude may heavily affect the precision of our estimation results, and this simple truncation procedure can reduce such impact. Next, we introduce our robust estimator, which is based on the robust covariance matrix estimator and CLIME: b\n⇥ = argmin ⇥2Rd⇥d k⇥k 1,1 subject to kb⌃⇥ Ik1,1  ,\n(3.1)\nwhere b⌃ is the robust covariance matrix estimator obtained through truncation, > 0 is a constraint parameter. We refer to (3.1) as Robust CLIME (RCLIME). Note that here we do not consider the Glasso type estimator since it requires the stringent incoherence condition on the covariance matrix to guarantee the model selection consistency. Let ✓⇤i = ⇥⇤⇤i denote the i-th column of ⇥⇤. To estimate the precision matrix more efficiently, instead of solving (3.1), we can estimate each column of ⇥⇤ as follows:\nb✓ = argmin ✓2Rd k✓k 1 subject to kb⌃✓ eik1  ,\n(3.2)\nfor i = 1, . . . , d, and ei 2 Rd denotes a column vector that the i-th element is 1 and others are 0. Note that the combined solution b⇥1 = [b✓1\n1 , . . . , b✓1d] of (3.2) is equivalent to the solution of (3.1) (Cai et al., 2011). In addition, since b ⇥\n1 is not symmetric, we need the following symmetrization procedure to get our robust estimator\nb ⇥ = arginf ⇥2Sd++ k⇥ b⇥1k 1 , (3.3)\nwhere Sd ++ = {A 2 Rd⇥d | A = A>,A 0} denotes all d ⇥ d symmetric positive definite matrices. The symmetrization procedure in (3.3) can be solved by the projected gradient descent method, and in practice, we can use\nmany simple symmetrization methods, such as the method provided in Cai et al. (2011)."
  }, {
    "heading": "3.3 Hypothesis Test",
    "text": "Based on the proposed robust estimator (3.1), we are interested in testing whether there is an edge between node j and node k in GGMs (Jankova et al., 2015; Neykov et al., 2015; Gu et al., 2015; Xu et al., 2016). More specifically, we want to develop a procedure for the hypothesis test that H\n0\n: ⇥ ⇤ jk = 0 versus H1 : ⇥ ⇤ jk 6= 0. Let us assume that the k-th column of the precision matrix ⇥⇤ to be the vector ✓⇤k = (↵\n⇤, ⇤>)> where ↵⇤ is the j-th element of the vector ✓⇤k and\n⇤ 2 Rd 1 is the remaining (d 1)-dimensional vector. Thus it is equivalent to test the one dimensional component H\n0 : ↵⇤ = 0 versus the non-restricted alternative H\n1 : ↵⇤ 6= 0. In this case, ⇤ are nuisance parameters. To this end, we first introduce the following estimation equation projected (EEP) along the direction bw:\nbS(✓) = bw> b\n⌃✓ ek , (3.4)\nwhere b⌃ is the the robust covariance matrix estimator and b\nw is the solution of the optimization problem (3.2) with i = j. The motivation of projecting the estimation equation to a sparse direction (3.4) is to help us construct a test statistic which has a tractable limiting distribution in the high-dimensional regime. In high-dimensional settings, b ⌃ is not positive definite, we cannot solve the equation b ⌃\nb✓ ek = 0 by taking the inverse of b⌃ directly. Therefore, given the sparsity assumption on ✓⇤, the estimator in (3.2) can address such ill-posed problem for solving the estimation equation b⌃b✓ ek = 0 in high-dimensional settings. Furthermore, projecting the estimation equation to a certain direction (3.4) makes the limiting distribution of b✓ = (b↵, b >)> in (3.2) tractable. More specifically, if we choose the bw as the projection direction, then due to the fact that bw is a consistent estimator of w⇤ := ⇥⇤⇤j , the estimator b of the high-dimensional nuisance parameters in (3.2) is asymptotically ignorable along this direction. Therefore, we can solve the projected estimation equation bS(↵, b ) = 0 to get an debiased estimator of ↵⇤ as follows:\ne↵ = b↵ b w\n> ( b ⌃\nb✓ ek) b w > b ⌃⇤j , (3.5)\nwhere b✓ = (b↵, b >)> is the estimator of ⇥⇤⇤k, and bw is the estimator of ⇥⇤⇤j . Thus we define the following test statistic built upon the debiased estimator e↵\nbTn = p n 1 (e↵ ↵⇤)/b , (3.6)\nwhere n is the number of observations, n 2 is the upper bound on the number of corruptions, n\n1 = n n 2 , and b 2 = bwjb✓k + bwkb✓j , where bwj , b✓j denote the j-th elements of bw and b✓ respectively. Note that b 2 is a consistent estimator to 2 = w⇤j ✓⇤k + w ⇤ k✓ ⇤ j under the Gaussian\nassumption of the data, where w⇤j , ✓⇤k are the j-th and kth columns of w⇤ and ✓⇤ respectively. We will show in the next section that the proposed debiased estimator e↵ is consistent to ↵⇤, and the test statistic bTn is asymptotically normal p n 1\n(e↵ ↵⇤)/b d ! N(0, 1) under the null hypothesis. Therefore, our asymptotic level-↵ test is given by\nn =\n(\n0 (⌘ accept H 0 ) if | bTn|  C↵, 1 (⌘ reject H\n0 ) if | bTn| > C↵, (3.7)\nwhere C↵ = 1(1 ↵/2) is the (1 ↵/2)-quantile of the standard normal distribution N(0, 1). Furthermore, we can construct asymptotic level-↵ confidence intervals of ↵⇤ as e↵± 1(1 ↵/2)b / p n. Note that in practice, although we have no idea about the exact upper bound on the number of corruptions, i.e., n\n2 , we can use techniques such as crossvalidation to choose the best truncation number n\n2\n."
  }, {
    "heading": "4 Main Results",
    "text": "In this section, we present our main results and discuss connections with some related works. We start by stating some assumptions, which are required in our analysis. We impose an important eigenvalue condition on the population covariance matrix. Assumption 4.1. There exist a constant  > 0 such that\n0 < 1/  min (⌃ ⇤ )  max (⌃ ⇤ )   < 1.\nThis assumption can exclude singular or nearly singular covariance matrices, thus guarantee the uniqueness of ⇥⇤.\nIn this paper, we consider the precision matrix ⇥⇤ that belongs to a class of matrices U(s), i.e., U(s) =\n⌦ 2 Rd⇥d ⌦ 0, k⌦k 1  M,max 1id Pd j=1 1{⌦ij 6= 0}  s\n, where ⌦ 0 means ⌦ is positive definite and s corresponds to the row cardinality. Note that this sparse precision matrix class has been previously considered in Cai et al. (2011); Liu & Wang (2012); Zhao & Liu (2013). In addition, it immediately implies that k⇥⇤⇤jk1  k⇥⇤k\n1  M , where ⇥⇤⇤j is the jth column vector of ⇥⇤.\nNow, we are ready to provide our main results. The first one characterizes the performance of our robust estimator under the arbitrary corruption model. It shows that even if the upper bound on the number of corruptions n\n2 scales with p n, where n is the number of observations, our robust estimator can still recover the correct support. Note that our results are derived under the arbitrary corruption model. Since the outlier model is a special case of the arbitrary corruption model, our results can directly apply to the outlier case. Theorem 4.2. Under the arbitrary corruption model, suppose ⇥⇤ 2 U(s) and Assumption 4.1 is satisfied. In addition, assume the upper bound on the number of corruptions n\n2 satisfies n 2  a p n for some constant a 0. If\nn 4a2, and we choose the regularization parameter satisfying = CM2 p\nlog d/n + n 2 log d/n , then, with probability at least 1 C\n1 /d, the estimator b⇥ in (3.1) satisfies\nk b⇥ ⇥⇤k 2  C 2 M22 ✓ s\nr\nlog d\nn +\nn 2 s log d\nn\n◆\n. (4.1)\nFurthermore, if the nonzero entries of ⇥⇤ satisfy\nmin i 6=j,⇥⇤ij 6=0 |⇥⇤ij | C3M22\n✓\nr\nlog d\nn +\nn 2 log d\nn\n◆\n,\nthen the Robust CLIME can correctly identify nonzero entries of ⇥⇤. Remark 4.3. According to (4.1), the estimation error of our robust estimator consists of two terms. The first one O(s p\nlog d/n) corresponds to the estimation error without corruptions. The second extra term O(sn\n2 log d/n), which is linear in n\n2 , is due to the effect of arbitrary corruption. More specifically, if there is no corruption in our data, then the second term becomes zero since n\n2 = 0. Therefore, the estimation error of our method reduces to O(s p\nlog d/n), which matches the minimax optimal rate for sparse precision matrix estimation without corruptions in terms of spectral norm (Yuan, 2010; Cai et al., 2011; Ravikumar et al., 2011). In addition, (4.1) in Theorem 4.2 indicates that our robust estimator can correctly recover the support of ⇥⇤ even if the upper bound on the number of corruptions n\n2\nscales with p\nn/ log d, where n is the number of observations. In addition, under the outlier model, this estimation result is comparable to the result provided by Yang & Lozano (2015). In their study, they proved that the proposed estimator can successfully recover the true parameter provided that the upper bound of the number of outliers is O( p n). However, Yang & Lozano (2015) does not consider the case when the data is arbitrarily corrupted.\nFurthermore, if the upper bound on the number of corruptions n\n2 satisfies n 2 . pn/plog d, our robust estimator can achieve the same statistical rate as the standard estimator for Gaussian graphical models. This is summarized in the following corollary. Corollary 4.4. Under the same conditions of Theorem 4.2, if we further assume that the upper bound on the number of corruptions n\n2 satisfies n 2 . pn/plog d, then for the robust estimator b⇥ in (3.1), we have, with probability at least 1 C/d, that\nk b⇥ ⇥⇤k 2  C 1 M22s\nr\nlog d\nn .\nFurthermore, if the nonzero entries of ⇥⇤ satisfy\nmin i 6=j,⇥⇤ij 6=0 |⇥⇤ij | C2M22\np\nlog d/n,\nthen the Robust CLIME can correctly identify the nonzero entries of ⇥⇤.\nRemark 4.5. Compared with Theorem 4.2, Corollary 4.4 implies that under a slightly stricter condition on the upper bound of the number of corruptions n\n2\n= O( p n/ p log d), our robust estimator can successfully recover the true parameter ⇥⇤ with guaranteed estimation error O(s p\nlog d/n). Note that this error bound exactly recover the spectral norm error bound for the case without corruptions (Yuan, 2010; Cai et al., 2011; Ravikumar et al., 2011), which demonstrates the superiority of our estimator.\nNext, we present the asymptotic results of our proposed test statistics in (3.6), which verifies the effectiveness of our testing procedure. Note that we consider the case that the true observations are drawn from a Gaussian distribution.\nTheorem 4.6. Suppose Assumption 4.1 is satisfied andp n 1 sM24( p log d/n 1 + n 2 log d/n 1 ) 2\n= o(1), where n 1 = n n 2\n. If we choose regularization parameter satisfying = CM2( p\nlog d/n 1 +n 2 log d/n 1 ), then the test statistic in (3.6) is asymptotically normal\np n 1\n(e↵ ↵⇤) b d ! N(0, 1),\nwhere b 2 = bwjb✓k + bwkb✓j , and e↵ is defined in (3.5).\nRemark 4.7. Theorem 4.6 provides us an efficient test for the existence of an edge in GGMs, and gives us an efficient interval estimation of ↵⇤ = ⇥⇤ij . In addition, Theorem 4.6 implies that if the upper bound on the number of corruptions n\n2 satisfies n 2 . pn/plog d and the quantity M is a constant, then the assumptionp n 1 sM24( p log d/n 1 + n 2 log d/n 1 ) 2\n= o(1) reduces to s log d/ p n = o(1), which gives us the sparsity assumption that s = O( p n log d). This requirement on sparsity matches the best-known results for edge testing in GGMs (Liu et al., 2013; Ren et al., 2015). More importantly, Theorem 4.6 suggests that even when n\n2\n= O( p n/ p log d) out of n observations of each variable are arbitrarily corrupted, our testing procedure is still efficient."
  }, {
    "heading": "5 Experiments",
    "text": "In this section, we compare our robust estimator with some existing methods, including trimmed Graphical Lasso (tGLasso) (Yang & Lozano, 2015), t⇤-Lasso (tLasso) (Finegold & Drton, 2011), robust `\n1 penalized likelihood (RLL) (Sun & Li, 2012), nonparanormal SKEPTIC (Liu et al., 2012), and pairwise based covariance estimator (spearC) (Loh & Tan, 2015) on some synthetic datasets. Our comparisons focus on their performance in both graph recovery and parameter estimation. The implementation of tLasso and RLL is based on the code provided by authors. The implementation of other baseline algo-\nrithms is based on R package huge1. We conduct some simulations to investigate the performance of our proposed hypothesis testing procedure. Furthermore, we compare our method with GLasso on a gene expression data."
  }, {
    "heading": "5.1 Synthetic Data",
    "text": "In our numerical simulations, we consider the following two settings: (i) n = 100, d = 100; and (ii) n = 200, d = 400. We generate the true precision matrices based on two graph structures: cluster and band. More specifically, the precision matrices ⇥⇤ are generated by huge package, and the magnitude of correlations is the default value (0.3) in the huge generator. In order to incorporate corruptions, we generate our observations by the following procedure.\nFor the arbitrary corruption model, we first generate the n by d data matrix X from the Gaussian distribution Nd(0,⇥⇤ 1). Then, for each column of the data matrix, we let np coordinates be arbitrarily corrupted, where we consider the corruption rate p = 0.1 for small number of corruptions and p = 0.2 for large number of corruptions. In addition, each corrupted coordinate is generated by normal distributions N(µ, ) as follows:\nMA 1 : µ = 1, = 1, MA 2 : µ = 2, = 1. (5.1)\nFor the outlier model, we use the setup similar to Sun & Li (2012); Yang & Lozano (2015). Specifically, we generate each observation from the mixture model as follows:\nXi ⇠ (1 p)Nd(0,⇥⇤ 1) + p\n2\nNd(µ,⇥ 0 1 )\n+\np\n2\nNd( µ,⇥0 1) for i = 1, . . . , n,\nwhere we consider the corruption rate p = 0.1 for small number of corruptions and p = 0.2 for large number of corruptions. Furthermore, each outlier is generated by normal distributions Nd(µ,⇥0 1 ) as follows\nMO 1 : µ = (1, . . . , 1)>, ⇥0 = Id, (5.2)\nMO 2 : µ = (2, . . . , 2)>, ⇥0 = Id. (5.3)\nNote that under both corruption models, we set the corruption rate p 2 {0.1, 0.2}. In other words, we choose the number of corruptions to be 10% and 20% of all observations. This is due to the threshold of the number of corruptions n\n2\n= O( p n) suggested in our theorem.\nPoint Estimation: We choose tuning parameters of each method as follow. For tGLasso, we choose n\n2 /n from [0.5, 1], which is suggested by Yang & Lozano (2015). For RLL, we choose 2 {0.005, 0.01, 0.02}, which is suggested by Sun & Li (2012). And for Robust CLIME, we choose n\n2 around 15 (±5). Since the performance of t⇤1http://cran.r-project.org/web/packages/huge\nLasso is similar to t-Lasso, we just show the results of t⇤Lasso. All results we reported are their best performance based on these parameters.\nFirst, we use receiver operating characteristic (ROC) curves to compare the overall performance of our method with others in model selection over the full paths. For the arbitrary corruption model, the ROC curves on cluster graphs averaged over 50 simulations are shown in Figure 1. We can observe that under the arbitrary corruption model, as the number of corruptions increase, the advantage of our approach becomes more significant. For the outlier model, we also observe similar good performance of our method, especially for outliers with large magnitude. Due to space limit, the ROC curves for the outlier model can be found in the longer version of this paper. These results indicate that our method is very competitive in the graph recovery problem with arbitrary corruptions.\nThen, we evaluate the performance of our method and some existing approaches in parameter estimation. For model settings mentioned above, we choose the corruption rate p = 0.1 for the purpose of comparisons. We generate a dataset as the training sample, and an independent dataset from the same distribution as the test set. We set n 2 /n = 0.9 for tGLasso, = 0.01 for RLL, and n 2\n= np for Robust CLIME. We also choose the tuning parameter by grid search based on its performance on the training sample and evaluate those estimators on the test set. Here we use Spectral norm error k b⇥ ⇥⇤k\n2 and Frobenius norm error k b⇥ ⇥⇤kF to compare the performance of different methods in parameter estimation. Tables 1 and 2 summarize estimation error results in term of Spectral norm averaged over 50 simulations. These results demonstrate the advantage of our method in parameter estimation. Other comparison results in terms of Frobenius norm error are deferred to the longer version of this paper.\nHypothesis Test: We investigate the finite sample performance of our proposed hypothesis testing procedure through some simulation studies. We use the data generating process similar to Jankova et al. (2015); Neykov et al. (2015), and we consider the case that there are some corruptions in our data. More specifically, for the aforementioned two settings, we consider the band graph structure with band width 1 with the corresponding precision matrix ⇥⇤ generated by R package huge. The magnitude of correlations is the default value in the huge generator. In order to incorporate corruptions, we use the same approach described above to generate observations. Specifically, for the arbitrary corruption model, we generate samples through model MA\n2 in (5.1) with p = 0.1, 0.2. For the outlier model, we generate samples through model MO\n2\nin (5.2) with p = 0.1, 0.2.\nTo check the validity of the type I error of our test, we run\n500 simulations. The detail of our hypothesis testing procedure is described in Section 3.3. In the two different settings, we set n\n2 = 10 and n 2 = 20 respectively, and we choose the tuning parameters by cross-validations. Table 3 summarizes the empirical type I errors of our test in different settings. We can observe that the empirical type I\nerrors are close to the significance level. Figure 2 shows the Q-Q plots of our test statistic bTn in (3.6) based on 500 simulations. These plots corroborate the asymptotic normality of our test statistic. All these results demonstrate the advantage of our hypothesis testing procedure under the arbitrary corruption model."
  }, {
    "heading": "5.2 Gene Expression Data",
    "text": "In this subsection, we use the gene expression data of Arabidopsis thaliana, which was analyzed by Wille et al. (2004) and later on by Finegold & Drton (2011); Hirose & Fujisawa (2015), to illustrate the advantage of our method. This data set includes n = 118 observations with 39 gene expression levels. For this gene expression dataset, we preprocess it through R package limma3. Figure 6 in Appendix illustrates the histogram of some rescaled gene expression data. It shows that some rescaled gene expressions contain some expression levels with extreme large magnitude, which may be outliers. Therefore, we want to apply our method to construct a network among these genes. For Robust CLIME, we set n\n2 = 10 and adopt 5-fold crossvalidation to choose the tuning parameter .\nThe graph estimated by our method is given in Figure 3. The dotted arrows and the solid undirected edges correspond to the known metabolic pathway and the graph estimated by Robust CLIME, respectively. We can see that our approach identifies a similar graph to that obtained by previous analysis of Wille et al. (2004) but with fewer ”crosstalk” edges between two pathways. For example, our approach finds the important connection between AACT2 and\n3Available on http://bioconductor.org/packages/limma\nthe group MK, MPDC1, and FFPS2 in MAV pathway. And in MEP path way, it also identifies the connection among DXR, MCT, CMK and MECPS. Other methods such as GLasso tends to estimate more links between two pathways in order to identify these important relationships. These edges between two pathways provided by GLasso might be inaccurate relationships due to the lack of robustness. The graph recovered by GLasso and the graph established by Wille et al. (2004) can be found in the longer version of this paper."
  }, {
    "heading": "6 Conclusions and Future Work",
    "text": "In this paper, for the Gaussian graphical model estimation with arbitrary corruptions, we proposed a new estimator for high-dimensional precision matrices based on the robust covariance matrix estimator. We not only provide the estimation error bound of our robust estimator, but also propose a hypothesis testing procedure to assess the uncertainty of our robust estimator with corrupted observations, and construct the confidence interval for the point estimate. However, most of the robust high dimensional estimators as well as our proposed estimator are not invariant under the group action (Davies et al., 2005; Draisma et al., 2013), we will study this problem in our future work."
  }, {
    "heading": "Acknowledgment",
    "text": "We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."
  }],
  "year": 2017,
  "references": [{
    "title": "Convex programming approach to robust estimation of a multivariate gaussian model",
    "authors": ["Balmand", "Samuel", "Dalalyan", "Arnak"],
    "venue": "arXiv preprint arXiv:1512.04734,",
    "year": 2015
  }, {
    "title": "Model selection through sparse maximum likelihood estimation",
    "authors": ["O. Banerjee", "L.E. Ghaoui", "A. d’Aspremont"],
    "year": 2008
  }, {
    "title": "Robust covariance matrix estimation via matrix depth",
    "authors": ["Chen", "Mengjie", "Gao", "Chao", "Ren", "Zhao"],
    "venue": "arXiv preprint arXiv:1506.00691,",
    "year": 2015
  }, {
    "title": "Robust high dimensional sparse regression and matching pursuit",
    "authors": ["Chen", "Yudong", "Caramanis", "Constantine", "Mannor", "Shie"],
    "venue": "arXiv preprint arXiv:1301.2725,",
    "year": 2013
  }, {
    "title": "Multivariate dependencies: Models, analysis and interpretation, volume 67",
    "authors": ["Cox", "David Roxbee", "Wermuth", "Nanny"],
    "year": 1996
  }, {
    "title": "Breakdown and groups",
    "authors": ["Davies", "P Laurie", "Gather", "Ursula"],
    "venue": "The Annals of Statistics,",
    "year": 2005
  }, {
    "title": "Groups acting on gaussian graphical models",
    "authors": ["Draisma", "Jan", "Kuhnt", "Sonja", "Zwiernik", "Piotr"],
    "venue": "The Annals of Statistics,",
    "year": 2013
  }, {
    "title": "Robust graphical modeling of gene networks using classical and alternative t-distributions",
    "authors": ["Finegold", "Michael", "Drton", "Mathias"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2011
  }, {
    "title": "Sparse inverse covariance estimation with the graphical lasso",
    "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"],
    "year": 2008
  }, {
    "title": "Local and global inference for high dimensional gaussian copula graphical models",
    "authors": ["Gu", "Quanquan", "Cao", "Yuan", "Ning", "Yang", "Liu", "Han"],
    "venue": "arXiv preprint arXiv:1502.02347,",
    "year": 2015
  }, {
    "title": "Robust sparse gaussian graphical modeling",
    "authors": ["Hirose", "Kei", "Fujisawa", "Hironori"],
    "venue": "arXiv preprint arXiv:1508.05571,",
    "year": 2015
  }, {
    "title": "Confidence intervals for high-dimensional inverse covariance estimation",
    "authors": ["Jankova", "Jana", "van de Geer", "Sara"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2015
  }, {
    "title": "Tiger: A tuning-insensitive approach for optimally estimating gaussian graphical models",
    "authors": ["Liu", "Han", "Wang", "Lie"],
    "venue": "arXiv preprint arXiv:1209.2437,",
    "year": 2012
  }, {
    "title": "High-dimensional semiparametric gaussian copula graphical models",
    "authors": ["Liu", "Han", "Fang", "Yuan", "Ming", "Lafferty", "John", "Wasserman", "Larry"],
    "venue": "The Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Gaussian graphical model estimation with false discovery rate control",
    "authors": ["Liu", "Weidong"],
    "venue": "The Annals of Statistics,",
    "year": 2013
  }, {
    "title": "High-dimensional robust precision matrix estimation: Cellwise corruption under ✏-contamination",
    "authors": ["Loh", "Po-Ling", "Tan", "Xin Lu"],
    "venue": "arXiv preprint arXiv:1509.07229,",
    "year": 2015
  }, {
    "title": "High dimensional graphs and variable selection with the lasso",
    "authors": ["N. Meinshausen", "P. Bühlmann"],
    "year": 2006
  }, {
    "title": "A unified theory of confidence regions and testing for high dimensional estimating equations",
    "authors": ["Neykov", "Matey", "Ning", "Yang", "Liu", "Jun S", "Han"],
    "venue": "arXiv preprint arXiv:1510.08986,",
    "year": 2015
  }, {
    "title": "Robust highdimensional precision matrix estimation",
    "authors": ["Öllerer", "Viktoria", "Croux", "Christophe"],
    "venue": "In Modern Nonparametric, Robust and Multivariate Methods,",
    "year": 2015
  }, {
    "title": "Restricted eigenvalue properties for correlated gaussian designs",
    "authors": ["Raskutti", "Garvesh", "Wainwright", "Martin J", "Yu", "Bin"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Asymptotic normality and optimalities in estimation of large gaussian graphical models",
    "authors": ["Ren", "Zhao", "Sun", "Tingni", "Zhang", "Cun-Hui", "Zhou", "Harrison H"],
    "venue": "The Annals of Statistics,",
    "year": 2015
  }, {
    "title": "Robust gaussian graphical modeling via l1 penalization",
    "authors": ["Sun", "Hokeun", "Li", "Hongzhe"],
    "year": 2012
  }, {
    "title": "Robust estimation of precision matrices under cellwise contamination",
    "authors": ["Tarr", "Garth", "Müller", "Samuel", "Weber", "Neville C"],
    "venue": "Computational Statistics & Data Analysis,",
    "year": 2016
  }, {
    "title": "Mathematics and the picturing of data",
    "authors": ["Tukey", "John W"],
    "venue": "In Proceedings of the international congress of mathematicians,",
    "year": 1975
  }, {
    "title": "Introduction to the non-asymptotic analysis of random matrices",
    "authors": ["Vershynin", "Roman"],
    "venue": "arXiv preprint arXiv:1011.3027,",
    "year": 2010
  }, {
    "title": "Precision matrix estimation in high dimensional gaussian graphical models with faster rates",
    "authors": ["Wang", "Lingxiao", "Ren", "Xiang", "Gu", "Quanquan"],
    "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Semiparametric differential graph models",
    "authors": ["Xu", "Pan", "Gu", "Quanquan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Communicationefficient distributed estimation and inference for transelliptical graphical models",
    "authors": ["Xu", "Pan", "Tian", "Lu", "Gu", "Quanquan"],
    "venue": "arXiv preprint arXiv:1612.09297,",
    "year": 2016
  }, {
    "title": "Efficient algorithm for sparse tensor-variate gaussian graphical models via gradient descent",
    "authors": ["Xu", "Pan", "Zhang", "Tingting", "Gu", "Quanquan"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Robust gaussian graphical modeling with the trimmed graphical lasso",
    "authors": ["Yang", "Eunho", "Lozano", "Aurélie C"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "High dimensional inverse covariance matrix estimation via linear programming",
    "authors": ["M. Yuan"],
    "year": 2010
  }, {
    "title": "Model selection and estimation in the gaussian graphical model",
    "authors": ["M. Yuan", "Y. Lin"],
    "year": 2007
  }, {
    "title": "Sparse inverse covariance estimation with calibration",
    "authors": ["Zhao", "Tuo", "Liu", "Han"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2013
  }],
  "id": "SP:8885d8c54744aea171056b6db6ebfe98511f1d76",
  "authors": [{
    "name": "Lingxiao Wang",
    "affiliations": []
  }, {
    "name": "Quanquan Gu",
    "affiliations": []
  }],
  "abstractText": "We study the problem of estimating the highdimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a robust estimator for the sparse precision matrix in the highdimensional regime. At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In particular, we show that provided that the number of corrupted samples n",
  "title": "Robust Gaussian Graphical Model Estimation with Arbitrary Corruption"
}