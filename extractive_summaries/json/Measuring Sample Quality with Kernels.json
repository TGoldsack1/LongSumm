{
  "sections": [{
    "heading": "1. Introduction",
    "text": "When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations E\nP\n[h(Z)] = R p(x)h(x)dx under a target dis-\ntribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample aver-\n1Stanford University, Palo Alto, CA USA 2Microsoft Research New England, Cambridge, MA USA. Correspondence to: Jackson Gorham <jgorham@stanford.edu>, Lester Mackey <lmackey@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nages E Q\nn [h(X)] = 1 n\nP n\ni=1\nh(x i ). However, many exact MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.\nSince standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures – the Stein discrepancies – that measure how well E\nQ\nn\napproximates E P while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family – the graph Stein discrepancies – were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points.\nThis closed form represents a significant practical advantage, as no linear program solvers are necessary, and the computation of the discrepancy can be easily parallelized. However, as we will see in Section 3.2, not all kernel Stein discrepancies are suitable for our setting. In particular, in dimension d 3, the kernel Stein discrepancies previously recommended in the literature fail to detect when a sample is not converging to the target. To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions.\nAfter formally describing our goals for measuring sample quality in Section 2, we outline our strategy, based on Stein’s method, for constructing and analyzing practical quality measures at the start of Section 3. In Section 3.1, we define our family of closed-form quality measures – the kernel Stein discrepancies (KSDs) – and establish several\nappealing practical properties of these measures. We analyze the convergence properties of KSDs in Sections 3.2 and 3.3, showing that previously proposed KSDs fail to detect non-convergence and proposing practical convergencedetermining alternatives. Section 4 illustrates the value of convergence-determining kernel Stein discrepancies in a variety of applications, including hyperparameter selection, sampler selection, one-sample hypothesis testing, and sample quality improvement. Finally, in Section 5, we conclude with a discussion of related and future work.\nNotation We will use µ to denote a generic probability measure and ) to denote the weak convergence of a sequence of probability measures. We will use k·k\nr for r 2 [1,1] to represent the `r norm on Rd and occasionally refer to a generic norm k·k with associated dual norm kak⇤ , sup\nb2Rd,kbk=1 ha, bi for vectors a 2 Rd. We let e\nj be the j-th standard basis vector. For any function g : Rd ! Rd0 , we define M\n0 (g) , sup x2Rdkg(x)k\n2 , M\n1 (g) , sup x 6=ykg(x) g(y)k\n2 /kx yk 2 , and rg as the gradient with components (rg(x))\njk , r x\nk\ng j (x). We further let g 2 Cm indicate that g is m times continuously differentiable and g 2 Cm\n0 indicate that g 2 Cm and rlg is vanishing at infinity for all l 2 {0, . . . ,m}. We define C(m,m) (respectively, C(m,m)\nb\nand C(m,m) 0 ) to be the set of functions k : Rd ⇥ Rd ! R with (x, y) 7! rl\nx rl y\nk(x, y) continuous (respectively, continuous and uniformly bounded, continuous and vanishing at infinity) for all l 2 {0, . . . ,m}."
  }, {
    "heading": "2. Quality measures for samples",
    "text": "Consider a target distribution P with continuously differentiable (Lebesgue) density p supported on all of Rd. We assume that the score function b , r log p can be evaluated1 but that, for most functions of interest, direct integration under P is infeasible. We will therefore approximate integration under P using a weighted sample Q\nn =P n\ni=1\nq n (x i ) x\ni\nwith sample points x 1 , . . . , x n 2 Rd and q n\na probability mass function. We will make no assumptions about the origins of the sample points; they may be the output of a Markov chain or even deterministically generated.\nEach Q n offers an approximation E Q\nn [h(X)] =P n\ni=1\nq n (x i )h(x i ) for each intractable expectation E P\n[h(Z)], and our aim is to effectively compare the quality of the approximation offered by any two samples targeting P . In particular, we wish to produce a quality measure that (i) identifies when a sequence of samples is converging to the target, (ii) determines when a sequence of samples is not converging to the target, and (iii) is efficiently computable. Since our interest is in approx-\n1No knowledge of the normalizing constant is needed.\nimating expectations, we will consider discrepancies quantifying the maximum expectation error over a class of test functions H:\ndH(Qn, P ) , sup h2H |E P [h(Z)] E Q n [h(X)]|. (1)\nWhen H is large enough, for any sequence of probability measures (µ\nm\n) m 1, dH(µm, P ) ! 0 only if µm ) P . In this case, we call (1) an integral probability metric (IPM) (Müller, 1997). For example, when H = BLk·k2 , {h : Rd ! R |M\n0 (h) + M 1 (h)  1}, the IPM d BLk·k2 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution. Alternatively, when H = Wk·k2 , {h : Rd ! R |M1(h)  1} is the set of 1-Lipschitz functions, the IPM dWk·k in (1) is known as the Wasserstein metric.\nAn apparent practical problem with using the IPM dH as a sample quality measure is that E\nP [h(Z)] may not be computable for h 2 H. However, if H were chosen such that E P\n[h(Z)] = 0 for all h 2 H, then no explicit integration under P would be necessary. To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein’s method for characterizing distributional convergence."
  }, {
    "heading": "3. Stein’s method with kernels",
    "text": "Stein’s method (Stein, 1972) provides a three-step recipe for assessing convergence in distribution:\n1. Identify a Stein operator T that maps functions g : Rd ! Rd from a domain G to real-valued functions T g such that\nE P [(T g)(Z)] = 0 for all g 2 G. For any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as\nS(µ, T ,G) , sup g2G |E µ [(T g)(X)]| = dT G(µ, P ) (2)\nwhich, crucially, avoids explicit integration under P .\n2. Lower bound the Stein discrepancy by an IPM dH known to dominate weak convergence. This can be done once for a broad class of target distributions to ensure that µ\nm ) P whenever S(µ m , T ,G) ! 0 for a sequence of probability measures (µ\nm\n)\nm 1 (Desideratum (ii)).\n3. Provide an upper bound on the Stein discrepancy ensuring that S(µ\nm , T ,G) ! 0 under suitable convergence of µ\nm\nto P (Desideratum (i)).\nWhile Stein’s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality. The subsections to follow develop a specific, practical instantiation of the abstract Stein’s method recipe based on reproducing kernel Hilbert spaces. An empirical analysis of the Stein discrepancies recommended by our theory follows in Section 4."
  }, {
    "heading": "3.1. Selecting a Stein operator and a Stein set",
    "text": "A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017),\n(T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g0(x).\nInspired by the generator method of Barbour (1988; 1990) and Götze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator\n(T P g)(x) , 1 p(x) hr, p(x)g(x)i = hg(x), b(x)i+ hr, g(x)i for functions g : Rd ! Rd was independently developed, without connection to Stein’s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = r log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity.\nHereafter, we will let k : Rd ⇥Rd ! R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) K\nk\nof functions from Rd ! R. That is, K k\nis a Hilbert space of functions such that, for all x 2 Rd, k(x, ·) 2 K\nk and f(x) = hf, k(x, ·)iK\nk\nwhenever f 2 K k . We let k·kK k\nbe the norm induced from the inner product on K\nk\n.\nWith this definition, we define our kernel Stein set G k,k·k as the set of vector-valued functions g = (g 1 , . . . , g d\n) such that each component function g\nj belongs to K k\nand the vector of their norms kg\nj kK k\nbelongs to the k·k⇤ unit ball:2\nG k,k·k , {g = (g1, . . . , gd) | kvk⇤  1 for vj , kgjkK\nk }. The following result, proved in Section B, establishes that this is an acceptable domain for T\nP\n.\nProposition 1 (Zero mean test functions). If k 2 C(1,1) b and E P [kr log p(Z)k 2 ] < 1, then E P [(T P\ng)(Z)] = 0 for all g 2 G\nk,k·k.\n2Our analyses and algorithms support each gj belonging to a different RKHS Kk\nj\n, but we will not need that flexibility here.\nThe Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(µ, T\nP ,G k,k·k). When k·k = k·k\n2 , this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016). Our next result shows that, for any k·k, the KSD admits a closed-form solution. Proposition 2 (KSD closed form). Suppose k 2 C(1,1), and, for each j 2 {1, . . . d}, define the Stein kernel\nkj 0 (x, y) , 1 p(x)p(y) r x j r y j (p(x)k(x, y)p(y)) (3)\n= b j (x)b j (y)k(x, y) + b j (x)r y\nj\nk(x, y)\n+ b j (y)r x\nj\nk(x, y) +r x\nj\nr y\nj\nk(x, y).\nIf P d\nj=1\nE µ h kj 0 (X,X) 1/2 i < 1, then S(µ, T P ,G k,k·k) =\nkwk where w j , q E µ⇥µ[k j\n0 (X, ˜X)] with X, ˜X iid⇠ µ. The proof is found in Section C. Notably, when µ is the discrete measure Q\nn\n=\nP n\ni=1\nq n (x i ) x\ni , the KSD reduces to evaluating each kj\n0\nat pairs of support points as w j\n=qP n\ni,i 0 =1\nq n (x i )kj 0 (x i , x i 0 )q n (x i 0 ), a computation which\nis easily parallelized over sample pairs and coordinates j.\nOur Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k\n0 =P d\nj=1\nkj 0 to develop nonparametric control variates. Each term w\nj in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between µ and P measured with respect to the Stein kernel kj\n0 . In standard uses of MMD, an arbitrary kernel function is selected, and one must be able to compute expectations of the kernel function under P . Here, this requirement is satisfied automatically, since our induced kernels are chosen to have mean zero under P .\nFor clarity we will focus on the specific kernel Stein set choice G\nk , G k,k·k2 for the remainder of the paper, but our\nresults extend directly to KSDs based on any k·k, since all KSDs are equivalent in a strong sense: Proposition 3 (Kernel Stein set equivalence). Under the assumptions of Proposition 2, there are constants c\nd , c0 d > 0 depending only on d and k·k such that c\nd S(µ, T P ,G k,k·k)  S(µ, TP ,Gk,k·k2) \nc0 d S(µ, T P ,G k,k·k).\nThe short proof is found in Section D."
  }, {
    "heading": "3.2. Lower bounding the kernel Stein discrepancy",
    "text": "We next aim to establish conditions under which the KSD S(µ\nm , T P ,G k ) ! 0 only if µ m ) P (Desideratum (ii)). Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:\nDefinition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)). A distribution P is distantly dissipative if  0 , lim inf r!1 (r) > 0 for\n(r) = inf{ 2 hb(x) b(y),x yikx yk22 : kx yk2 = r}. (4)\nExamples of distributions in P include finite Gaussian mixtures with common covariance and all distributions strongly log-concave outside of a compact set, including Bayesian linear, logistic, and Huber regression posteriors with Gaussian priors (see Gorham et al., 2016, Section 4). Moreover, when d = 1, membership in P is sufficient to provide a lower bound on the KSD for most common kernels including the Gaussian, Matérn, and inverse multiquadric kernels. Theorem 5 (Univariate KSD detects non-convergence). Suppose that P 2 P and k(x, y) = (x y) for 2 C2 with a non-vanishing generalized Fourier transform. If d = 1, then S(µ\nm , T P ,G k ) ! 0 only if µ m ) P . The proof in Section E provides a lower bound on the KSD in terms of an IPM known to dominate weak convergence. However, our next theorem shows that in higher dimensions S(Q\nn , T P ,G k\n) can converge to 0 without the sequence (Q\nn\n) n 1 converging to any probability measure. This deficiency occurs even when the target is Gaussian. Theorem 6 (KSD fails with light kernel tails). Suppose k 2 C(1,1)\nb\nand define the kernel decay rate\n(r) , sup{max(|k(x, y)|, kr x k(x, y)k 2 ,\n|hr x ,r y k(x, y)i|) : kx yk 2 r}. If d 3, P = N (0, I\nd ), and (r) = o(r ↵) for ↵ , ( 1 2\n1\nd\n) 1, then S(Q n , T P ,G k ) ! 0 does not imply Q n ) P . Theorem 6 implies that KSDs based on the commonly used Gaussian kernel, Matérn kernel, and compactly supported kernels of Wendland (2004, Theorem 9.13) all fail to detect non-convergence when d 3. In addition, KSDs based on the inverse multiquadric kernel (k(x, y) = (c2 + kx yk2\n2\n) ) for < 1 fail to detect non-convergence for any d > 2 /( + 1). The proof in Section F shows that the violating sample sequences (Q\nn\n) n 1 are simple to construct, and we provide an empirical demonstration of this failure to detect non-convergence in Section 4.\nThe failure of the KSDs in Theorem 6 can be traced to their inability to enforce uniform tightness. A sequence of probability measures (µ\nm\n) m 1 is uniformly tight if for every ✏ > 0, there is a finite number R(✏) such that lim sup\nm\nµ m (kXk 2 > R(✏))  ✏. Uniform tightness implies that no mass in the sequence of probability measures escapes to infinity. When the kernel k decays more rapidly than the score function grows, the KSD ignores excess mass in the tails and hence can be driven to zero by a\nnon-tight sequence of increasingly diffuse probability measures. The following theorem demonstrates uniform tightness is the missing piece to ensure weak convergence. Theorem 7 (KSD detects tight non-convergence). Suppose that P 2 P and k(x, y) = (x y) for 2 C2 with a nonvanishing generalized Fourier transform. If (µ\nm\n) m 1 is uniformly tight, then S(µ\nm , T P ,G k ) ! 0 only if µ m ) P . Our proof in Section G explicitly lower bounds the KSD S(µ, T\nP ,G k\n) in terms of the bounded Lipschitz metric d BLk·k(µ, P ), which exactly metrizes weak convergence.\nIdeally, when a sequence of probability measures is not uniformly tight, the KSD would reflect this divergence in its reported value. To achieve this, we consider the inverse multiquadric (IMQ) kernel k(x, y) = (c2 + kx yk2\n2\n)\nfor some < 0 and c > 0. While KSDs based on IMQ kernels fail to determine convergence when < 1 (by Theorem 6), our next theorem shows that they automatically enforce tightness and detect non-convergence whenever 2 ( 1, 0). Theorem 8 (IMQ KSD detects non-convergence). Suppose P 2 P and k(x, y) = (c2 + kx yk2\n2\n) for c > 0 and 2 ( 1, 0). If S(µ\nm , T P ,G k ) ! 0, then µ m ) P . The proof in Section H provides a lower bound on the KSD in terms of the bounded Lipschitz metric d\nBLk·k(µ, P ). The success of the IMQ kernel over other common characteristic kernels can be attributed to its slow decay rate. When P 2 P and the IMQ exponent > 1, the function class T\nP G k\ncontains unbounded (coercive) functions. These functions ensure that the IMQ KSD S(µ\nm , T P ,G k )\ngoes to 0 only if (µ m ) m 1 is uniformly tight."
  }, {
    "heading": "3.3. Upper bounding the kernel Stein discrepancy",
    "text": "The usual goal in upper bounding the Stein discrepancy is to provide a rate of convergence to P for particular approximating sequences (µ\nm\n) 1 m=1\n. Because we aim to directly compute the KSD for arbitrary samples Q\nn , our chief purpose in this section is to ensure that the KSD S(µ\nm , T P ,G k ) will converge to zero when µ m\nis converging to P (Desideratum (i)). Proposition 9 (KSD detects convergence). If k 2 C(2,2)\nb\nand r log p is Lipschitz with E P [kr log p(Z)k2 2 ] < 1, then S(µ\nm , T P ,G k ) ! 0 whenever the Wasserstein distance dWk·k2 (µm, P ) ! 0. Proposition 9 applies to common kernels like the Gaussian, Matérn, and IMQ kernels, and its proof in Section I provides an explicit upper bound on the KSD in terms of the Wasserstein distance dWk·k2 . When Qn = 1 n P n i=1 x i for\nx i iid⇠ µ, (Liu et al., 2016, Thm. 4.1) further implies that S(Q\nn , T P ,G k ) ) S(µ, T P ,G k\n) at an O(n 1/2) rate under continuity and integrability assumptions on µ."
  }, {
    "heading": "4. Experiments",
    "text": "We next conduct an empirical evaluation of the KSD quality measures recommended by our theory, recording all timings on an Intel Xeon CPU E5-2650 v2 @ 2.60GHz. Throughout, we will refer to the KSD with IMQ base kernel k(x, y) = (c2 + kx yk2\n2\n) , exponent = 1 2 , and c = 1 as the IMQ KSD. Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.github.io/ SteinDiscrepancy.jl/."
  }, {
    "heading": "4.1. Comparing discrepancies",
    "text": "Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dWk·k2 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) / e 1 2kx+ e1k22 + e 1 2kx e1k22 and = 1.5 as our target P and generate a first sample point sequence i.i.d. from the target and a second sequence i.i.d. from one component of the mixture, N ( e\n1 , I d ). As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n 0.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample. This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy.\nThe middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P . Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.0.4 linear program solver for the graph Stein discrepancy. We find that the two methods have nearly identical runtimes when d = 1 but that the KSD is 10 to 1000 times faster when d = 4. In addition, the KSD is straightforwardly parallelized and does not require access to a linear program solver, making it an appealing practical choice for a quality measure.\nFinally, the right panel displays the optimal Stein func-\ntions, g j (y) = E Q n [ b j (X)k(X,y)+r x j k(X,y) ]\nS(Q n ,T P ,G k ) , recovered by the IMQ KSD when d = 1 and n = 103. The associated\ntest functions h(y) = (T P g)(y) = P d j=1 EQn [k j 0(X,y)]\nS(Q n ,T P ,G k ) are the mean-zero functions under P that best discriminate the target P and the sample Q\nn . As might be expected, the optimal test function for the single component sample features large magnitude values in the oversampled region far from the missing mode."
  }, {
    "heading": "4.2. The importance of kernel choice",
    "text": "Theorem 6 established that kernels with rapidly decaying tails yield KSDs that can be driven to zero by offtarget sample sequences. Our next experiment provides an empirical demonstration of this issue for a multivariate Gaussian target P = N (0, I\nd ) and KSDs based on the popular Gaussian (k(x, y) = e kx yk 2 2/2) and Matérn (k(x, y) = (1 + p 3kx yk\n2\n)e p 3kx yk2 ) radial kernels.\nFollowing the proof Theorem 6 in Section F, we construct an off-target sequence (Q\nn\n) n 1 that sends S(Qn, TP ,Gk) to 0 for these kernel choices whenever d 3. Specifically, for each n, we let Q\nn\n=\n1\nn\nP n\ni=1\nx\ni where, for all i and j, kx\ni k 2  2n1/d log n and kx i x j k 2\n2 log n. To select these sample points, we independently sample candidate points uniformly from the ball {x : kxk\n2  2n1/d log n}, accept any points not within 2 log n Euclidean distance of any previously accepted point, and terminate when n points have been accepted.\nFor various dimensions, Figure 2 displays the result of applying each KSD to the off-target sequence (Q\nn\n)\nn 1 and an “on-target” sequence of points sampled i.i.d. from P . For comparison, we also display the behavior of the IMQ KSD which provably controls tightness and dominates weak convergence for this target by Theorem 8. As predicted, the Gaussian and Matérn KSDs decay to 0 under the off-target sequence and decay more rapidly as the dimension d increases; the IMQ KSD remains bounded away from 0."
  }, {
    "heading": "4.3. Selecting sampler hyperparameters",
    "text": "The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) / ⇡(x) Q L\nl=1\n⇡(y l |x) for ⇡(·) a prior distribution on Rd and ⇡(y\nl |x) the likelihood of a datapoint y l\n. A standard slice sampler must evaluate the likelihood of all L datapoints to draw each new sample point x\ni . To reduce this cost, the approximate slice sampler introduces a tuning parameter ✏ which determines the number of datapoints that contribute to an approximation of the slice sampling step; an appropriate setting of this parameter is imperative for accurate inference. When ✏ is too small, relatively few sample points will be generated in a given amount of sampling time, yielding sample expectations with high Monte Carlo variance. When ✏ is too large, the large approximation error will produce biased samples that no longer resemble the target.\nTo assess the suitability of the KSD for tolerance parameter selection, we take as our target P the bimodal Gaussian mixture model posterior of (Welling & Teh, 2011). For an array of ✏ values, we generated 50 independent approximate slice sampling chains with batch size 5, each with a\nbudget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3. ESS, which does not detect Markov chain bias, is maximized at the largest hyperparameter evaluated (✏ = 10 1), while the KSD is minimized at an intermediate value (✏ = 10 2). The right panel of Figure 3 shows representative samples produced by several settings of ✏. The sample produced by the ESS-selected chain is significantly overdispersed, while the sample from ✏ = 0 has minimal coverage of the second mode due to\nits small sample size. The sample produced by the KSDselected chain best resembles the posterior target. Using 4 cores, the longest KSD computation with n = 103 sample points took 0.16s."
  }, {
    "heading": "4.4. Selecting samplers",
    "text": "Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (termed SGFS-f), a d⇥ d matrix must be inverted to draw each new sample point. Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point. Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution. Hence we will use the KSD – a quality measure that accounts for asymptotic bias – to evaluate and choose between these samplers.\nSpecifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1). The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 104 MNIST handwritten digit images. From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9. After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 ⇥ 104 points and d = 51 dimensions (including the intercept term). Figure 4 displays the IMQ KSD applied to the first n points in each sample. As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a\nHamiltonian Monte Carlo chain with 105 iterates. Both the KSD and the surrogate ground truth suggest that the moderate speed-up provided by SGFS-d (0.0017s per sample vs. 0.0019s for SGFS-f) is outweighed by the significant loss in inferential accuracy. However, the KSD assessment does not require access to an external trustworthy ground truth sample. The longest KSD computation took 400s using 16 cores."
  }, {
    "heading": "4.5. Beyond sample quality comparison",
    "text": "While our investigation of the KSD was motivated by the desire to develop practical, trustworthy tools for sample quality comparison, the kernels recommended by our theory can serve as drop-in replacements in other inferential tasks that make use of kernel Stein discrepancies."
  }, {
    "heading": "4.5.1. ONE-SAMPLE HYPOTHESIS TESTING",
    "text": "Chwialkowski et al. (2016) recently used the KSD S(Q\nn , T P ,G k\n) to develop a hypothesis test of whether a given sample from a Markov chain was drawn from a target distribution P (see also Liu et al., 2016). However, the authors noted that the KSD test with their default Gaussian base kernel k experienced a considerable loss of power as the dimension d increased. We recreate their experiment and show that this loss of power can be avoided by using our default IMQ kernel with = 1\n2 and c = 1. Following (Chwialkowski et al., 2016, Section 4) we draw z i iid⇠ N (0, I d ) and u i\niid⇠ Unif[0, 1] to generate a sample (x\ni\n)\nn\ni=1\nwith x i = z i + u i e 1 for n = 500 and various dimensions d. Using the authors’ code (modified to include an IMQ kernel), we compare the power of the Gaussian KSD test, the IMQ KSD test, and the standard normality test of Baringhaus & Henze (1988) (B&H) to discern whether the sample (x\ni\n)\n500\ni=1 came from the null distribution P = N (0, I\nd\n). The results, averaged over 400 simula-\ntions, are shown in Table 1. Notably, the IMQ KSD experiences no power degradation over this range of dimensions, thus improving on both the Gaussian KSD and the standard B&H normality tests."
  }, {
    "heading": "4.5.2. IMPROVING SAMPLE QUALITY",
    "text": "Liu & Lee (2016) recently used the KSD S(Q n , T P ,G k ) as a means of improving the quality of a sample. Specifically, given an initial sample Q\nn supported on x 1 , . . . , x n , they minimize S( ˜Q\nn , T P ,G k ) over all measures ˜Q n\nsupported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = T\nP G k\n. In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e 1hkx yk 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points. Using the authors’ code, we recreate the experiment from (Liu & Lee, 2016, Fig. 2b) and introduce a KSD objective with an IMQ kernel k(x, y) = (1 + 1\nh\nkx yk2 2 )\n1/2 with bandwidth selected in the same fashion. The starting sample is given by Q\nn\n=\n1\nn\nP n\ni=1\nx\ni for n = 100, various dimensions d, and each sample point drawn i.i.d. from P = N (0, I\nd ). For the initial sample and the optimized samples produced by each KSD, Figure 5 displays the mean squared error (MSE) 1\nd\nkE P [Z] E ˜\nQ\nn\n[X]k2 2\naveraged across 500 independently generated initial samples. Out of the box, the IMQ kernel produces better mean estimates than the standard Gaussian."
  }, {
    "heading": "5. Related and future work",
    "text": "The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested. For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.\nFor an approximation µ with continuously differentiable density r, Chwialkowski et al. (2016, Thm. 2.1) and Liu et al. (2016, Prop. 3.3) established that if k is C\n0 - universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E µ [k 0 (X,X) + kr log p(X) r(X) k2 2 ] < 1 for k 0 , Pd j=1 kj 0\n, then S(µ, T\nP ,G k\n) = 0 only if µ = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Matérn kernels are C\n0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains,\nwhere tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Schölkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence.\nWhile assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016).\nIn the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. A reader may also wonder for which distributions outside of P the KSD dominates weak convergence. The following theorem, proved in Section J, shows that no KSD with a C\n0 kernel dominates weak convergence when the target has a bounded score function. Theorem 10 (KSD fails for bounded scores). If r log p is bounded and k 2 C(1,1)\n0\n, then S(Q n , T P ,G k ) ! 0 does not imply Q\nn ) P . However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator T\nP\nwith diffusion Stein operators of the form (T g)(x) = 1\np(x) hr, p(x)(a(x) + c(x))g(x)i. An analogous construction should yield convergence-determining diffusion KSDs for P outside of P . Our results also extend to targets P supported on a convex subset X of Rd by choosing k to satisfy p(x)k(x, ·) ⌘ 0 for all x on the boundary of X ."
  }],
  "year": 2017,
  "references": [{
    "title": "Bayesian posterior sampling via stochastic gradient Fisher scoring",
    "authors": ["S. Ahn", "A. Korattikara", "M. Welling"],
    "venue": "In Proc. 29th ICML,",
    "year": 2012
  }, {
    "title": "Functional Analysis. Academic Press textbooks in mathematics",
    "authors": ["G. Bachman", "L. Narici"],
    "venue": "Dover Publications,",
    "year": 1966
  }, {
    "title": "Integration of radial functions",
    "authors": ["J. Baker"],
    "venue": "Mathematics Magazine,",
    "year": 1999
  }, {
    "title": "Stein’s method and Poisson process convergence",
    "authors": ["A.D. Barbour"],
    "venue": "J. Appl. Probab., (Special Vol. 25A):175–184,",
    "year": 1988
  }, {
    "title": "Stein’s method for diffusion approximations",
    "authors": ["A.D. Barbour"],
    "venue": "Probab. Theory Related Fields,",
    "year": 1990
  }, {
    "title": "A consistent test for multivariate normality based on the empirical characteristic function",
    "authors": ["L. Baringhaus", "N. Henze"],
    "venue": "Metrika,",
    "year": 1988
  }, {
    "title": "Julia: A fresh approach to numerical computing",
    "authors": ["J. Bezanson", "A. Edelman", "S. Karpinski", "V.B. Shah"],
    "venue": "arXiv preprint arXiv:1411.1607,",
    "year": 2014
  }, {
    "title": "Handbook of Markov chain Monte Carlo",
    "authors": ["S. Brooks", "A. Gelman", "G. Jones", "Meng", "X.-L"],
    "venue": "CRC press,",
    "year": 2011
  }, {
    "title": "Vector valued reproducing kernel hilbert spaces and universality",
    "authors": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanitá"],
    "venue": "Analysis and Applications,",
    "year": 2010
  }, {
    "title": "Nonnormal approximation by Stein’s method of exchangeable pairs with application to the Curie-Weiss model",
    "authors": ["S. Chatterjee", "Q. Shao"],
    "venue": "Ann. Appl. Probab.,",
    "year": 2011
  }, {
    "title": "Normal approximation by Stein’s method. Probability and its Applications",
    "authors": ["L. Chen", "L. Goldstein", "Q. Shao"],
    "year": 2011
  }, {
    "title": "A kernel test of goodness of fit",
    "authors": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"],
    "venue": "In Proc. 33rd ICML,",
    "year": 2016
  }, {
    "title": "Approximate slice sampling for Bayesian posterior inference",
    "authors": ["C. DuBois", "A. Korattikara", "M. Welling", "P. Smyth"],
    "venue": "In Proc. 17th AISTATS,",
    "year": 2014
  }, {
    "title": "Reflection couplings and contraction rates for diffusions",
    "authors": ["A. Eberle"],
    "venue": "Probab. Theory Related Fields, pp",
    "year": 2015
  }, {
    "title": "Output assessment for Monte Carlo simulations via the score statistic",
    "authors": ["Y. Fan", "S.P. Brooks", "A. Gelman"],
    "venue": "J. Comp. Graph. Stat.,",
    "year": 2006
  }, {
    "title": "Kernel measures of conditional dependence",
    "authors": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Schölkopf"],
    "venue": "In NIPS,",
    "year": 2007
  }, {
    "title": "Markov chain Monte Carlo maximum likelihood",
    "authors": ["C.J. Geyer"],
    "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface,",
    "year": 1991
  }, {
    "title": "Measuring sample quality with Stein’s method",
    "authors": ["J. Gorham", "L. Mackey"],
    "venue": "Adv. NIPS",
    "year": 2015
  }, {
    "title": "Measuring sample quality with diffusions",
    "authors": ["J. Gorham", "A. Duncan", "S. Vollmer", "L. Mackey"],
    "year": 2016
  }, {
    "title": "On the rate of convergence in the multivariate CLT",
    "authors": ["F. Götze"],
    "venue": "Ann. Probab.,",
    "year": 1991
  }, {
    "title": "A kernel two-sample test",
    "authors": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schölkopf", "A. Smola"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2012
  }, {
    "title": "The Plancherel formula, the Plancherel theorem, and the Fourier transform of orbital integrals",
    "authors": ["R. Herb", "P.J. Sally Jr."],
    "venue": "In Representation Theory and Mathematical Physics: Conference in Honor of Gregg Zuckerman’s 60th Birthday,",
    "year": 2009
  }, {
    "title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget",
    "authors": ["A. Korattikara", "Y. Chen", "M. Welling"],
    "venue": "In Proc. of 31st ICML,",
    "year": 2014
  }, {
    "title": "Stein’s method for comparison of univariate distributions",
    "authors": ["C. Ley", "G. Reinert", "Y. Swan"],
    "venue": "Probab. Surveys,",
    "year": 2017
  }, {
    "title": "Two methods for wild variational inference",
    "authors": ["Q. Liu", "Y. Feng"],
    "venue": "arXiv preprint arXiv:1612.00081,",
    "year": 2016
  }, {
    "title": "Variational Gradient Descent: A General Purpose",
    "authors": ["Q. Liu", "Wang", "D. Stein"],
    "venue": "Bayesian Inference Algorithm",
    "year": 2016
  }, {
    "title": "A kernelized Stein discrepancy for goodness-of-fit tests",
    "authors": ["Q. Liu", "J. Lee", "M. Jordan"],
    "venue": "In Proc. of 33rd ICML, volume 48 of ICML,",
    "year": 2016
  }, {
    "title": "Multivariate Stein factors for a class of strongly log-concave distributions",
    "authors": ["L. Mackey", "J. Gorham"],
    "venue": "Electron. Commun. Probab.,",
    "year": 2016
  }, {
    "title": "Integral probability metrics and their generating classes of functions",
    "authors": ["A. Müller"],
    "venue": "Ann. Appl. Probab.,",
    "year": 1997
  }, {
    "title": "Control functionals for QuasiMonte Carlo integration",
    "authors": ["C. Oates", "M. Girolami"],
    "year": 2015
  }, {
    "title": "Convergence rates for a class of estimators based on steins method",
    "authors": ["C. Oates", "J. Cockayne", "F. Briol", "M. Girolami"],
    "venue": "arXiv preprint arXiv:1603.03220,",
    "year": 2016
  }, {
    "title": "Control functionals for Monte Carlo integration",
    "authors": ["C.J. Oates", "M. Girolami", "N. Chopin"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. n/a–n/a,",
    "year": 2016
  }, {
    "title": "Operator variational inference",
    "authors": ["R. Ranganath", "D. Tran", "J. Altosaar", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions",
    "authors": ["C. Simon-Gabriel", "B. Schölkopf"],
    "venue": "arXiv preprint arXiv:1604.05251,",
    "year": 2016
  }, {
    "title": "On the optimal estimation of probability measures in weak and strong",
    "authors": ["B. Sriperumbudur"],
    "venue": "topologies. Bernoulli,",
    "year": 2016
  }, {
    "title": "Hilbert space embeddings and metrics on probability measures",
    "authors": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Schölkopf", "G. Lanckriet"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2010
  }, {
    "title": "A bound for the error in the normal approximation to the distribution of a sum of dependent random variables",
    "authors": ["C. Stein"],
    "venue": "In Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley,",
    "year": 1971
  }, {
    "title": "Use of exchangeable pairs in the analysis of simulations. In Stein’s method: expository lectures and applications, volume 46 of IMS Lecture Notes Monogr",
    "authors": ["C. Stein", "P. Diaconis", "S. Holmes", "G. Reinert"],
    "venue": "Ser., pp. 1–26. Inst. Math. Statist., Beachwood,",
    "year": 2004
  }, {
    "title": "Positive definite functions and generalizations, an historical survey",
    "authors": ["J. Stewart"],
    "venue": "Rocky Mountain J. Math.,",
    "year": 1976
  }, {
    "title": "Calculation of the Wasserstein distance between probability distributions on the line",
    "authors": ["S. Vallender"],
    "venue": "Theory Probab. Appl.,",
    "year": 1974
  }, {
    "title": "High-dimensional statistics: A non-asymptotic viewpoint. 2017",
    "authors": ["M. Wainwright"],
    "venue": "URL http: //www.stat.berkeley.edu/",
    "year": 2017
  }, {
    "title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning",
    "authors": ["D. Wang", "Q. Liu"],
    "year": 2016
  }, {
    "title": "Bayesian learning via stochastic gradient Langevin dynamics",
    "authors": ["M. Welling", "Y. Teh"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Scattered data approximation, volume 17",
    "authors": ["H. Wendland"],
    "venue": "Cambridge university press,",
    "year": 2004
  }],
  "id": "SP:7874688f0870193bdfd7ced1307c02e21dc5c3e6",
  "authors": [{
    "name": "Jackson Gorham",
    "affiliations": []
  }, {
    "name": "Lester Mackey",
    "affiliations": []
  }],
  "abstractText": "Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein’s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.",
  "title": "Measuring Sample Quality with Kernels"
}