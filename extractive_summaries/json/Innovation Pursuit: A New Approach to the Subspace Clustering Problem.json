{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Principal Component Analysis (PCA) is a popular and efficient procedure to approximate the data with a single low dimensional subspace (Lerman et al., 2015). Nonetheless, in numerous contemporary applications the data points may originate from multiple independent sources, in which case a union of subspaces can better model the data (Vidal, 2011). The problem of subspace clustering is concerned with learning these low-dimensional subspaces and clustering the data points to their respective subspaces, generally without prior knowledge about the number of subspaces and their dimensions, nor the membership of the data points\n1University of Central Florida, Orlando, Florida, USA. Correspondence to: Mostafa Rahmani <mostafa@knights.ucf.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nto these subspaces. Subspace clustering naturally arises in many machine learning and data analysis problems, including computer vision (e.g. motion segmentation (Vidal et al., 2008), face clustering (Ho et al., 2003)), image processing (Yang et al., 2008) and system identification (Vidal et al., 2003). Numerous approaches for subspace clustering have been studied in prior work, including statistical-based approaches (Yang et al., 2006; Rao et al., 2010), spectral clustering (Soltanolkotabi et al., 2012; Von Luxburg, 2007; Dyer et al., 2013; Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2013; Liu et al., 2013; Chen & Lerman, 2009), the algebraic-geometric approach (Vidal et al., 2005) and iterative methods (Bradley & Mangasarian, 2000). We refer the reader to (Vidal, 2011) for a comprehensive survey on the topic.\nThis paper aims to advance the state-of-the-art research on subspace clustering on several fronts. First, the proposed approach – termed iPursuit – rests on a novel geometrical idea whereby one subspace is identified at a time based on its novelty with respect to (w.r.t.) the other subspaces. Second, the proposed method is a provable and scalable subspace clustering algorithm – the computational complexity of iPursuit only scales linearly in the number of subspaces and quadratically in their dimensions (c.f. Section 2.5). In contrast to the spectral-clustering-based algorithms such as (Dyer et al., 2013; Elhamifar & Vidal, 2013; Liu et al., 2013), which need to solve an M22 -dimensional optimization problem to build the similarity matrix (where M2 is the number of data points), the proposed method requires solving few M2-dimensional linear optimization problems. This feature makes iPursuit remarkably faster than the state-of-the-art algorithms. Third, innovation pursuit in the data span enables superior performance when the subspaces have considerable intersections in comparison to the state-of-the-art subspace clustering algorithms."
  }, {
    "heading": "1.1. Notation and definitions",
    "text": "Given a matrix A, ‖A‖ denotes its spectral norm. For a vector a, ‖a‖ denotes its `2-norm and ‖a‖1 its `1-norm. Given matrices {Ai}ni=1 with equal number of rows, we use the union symbol ∪ to define the matrix\nn ∪ i=1 Ai :=\n[A1 A2 ... An] as the concatenation of the matrices {Ai}ni=1. For a matrix D, we overload the set member-\nship operator by using the notation d ∈ D to signify that d is a column of D. A collection of subspaces {Gi}ni=1 is said to be independent if dim ( n ⊕ i=1 Gi ) = ∑n i=1 dim(Gi), where ⊕ denotes the direct sum operator and dim(Gi) is the dimension of Gi. Given a vector a,\n∣∣a| is the vector of absolute values of the elements of a. For a real number a, sgn(a) denotes the sign of a. The complement of a set L is denoted Lc. For any positive integer n, the index set {1, . . . , n} is denoted [n].\nConsider two subspaces S1 and S2, such that S2 6⊆ S1 and S1 6⊆ S2. This means that each of the subspaces S1 and S2 carries some innovation w.r.t. the other. As such, corresponding to each subspace we define an innovation subspace capturing its novelty (innovation) w.r.t. the other subspaces, defined formally as follows.\nDefinition 1. Assume that V1 and V2 are two orthonormal bases for S1 and S2, respectively. We define the innovation subspace of S2 over S1, denoted I (S2 ⊥ S1), as the subspace spanned by ( I−V1VT1 ) V2. In other words, I (S2 ⊥ S1) is the complement of S1 in the subspace S1 ⊕ S2.\nSimilarly, we can also define I (S1 ⊥ S2) as the innovation subspace of S1 over S2. Fig. 1 illustrates a scenario in which the data lies in a union of a two-dimensional and a one-dimensional subspace. Note that the innovation subspace of S2 over S1 is orthogonal to S1 and is the complement of S1 in S1 ⊕ S2."
  }, {
    "heading": "2. Proposed Approach",
    "text": "In this section, the main geometrical idea underlying iPursuit is first presented. This idea is based on a non-convex `0-norm minimization problem searching for a direction of innovation in the span of the data. Then, we provide a convex relaxation to a linear optimization problem, whose solution is shown to yield the correct subspaces under mild sufficient conditions. Due to space limitations, the proofs of all the theoretical results are deferred to an extended version of this paper (Rahmani & Atia, 2015). It is assumed\nthat the given data matrix follows the following data model.\nData Model 1. The data matrix D ∈ RM1×M2 can be represented as D = [D1 ...DN ]T, where T is an arbitrary permutation matrix. The columns of Di ∈ RM1×ni lie in Si, where Si is an ri-dimensional linear subspace, for 1 ≤ i ≤ N , and, ∑N i=1 ni = M2. Define Vi as an orthonormal basis for Si. In addition, define D as the space spanned by the data, i.e., D =\nN ⊕ i=1 Si. Moreover,\nit is assumed that every subspace in the set of subspaces {Si}Ni=1 has an innovation over the other subspaces, to say that, for 1 ≤ i ≤ N , the subspace Si does not completely lie in\nN ⊕ k=1 k 6=i Sk . In addition, the columns of D are\nnormalized, i.e., each column has an `2-norm equal to one."
  }, {
    "heading": "2.1. iPursuit: Basic idea",
    "text": "iPursuit is a multi-step algorithm that identifies one subspace at a time. In each step, the data is clustered into two subspaces. One subspace is the identified subspace and the other one is the direct sum of the other subspaces. The data points of the identified subspace are removed and the algorithm is applied to the remaining data to find the next subspace. Accordingly, each step of the algorithm can be interpreted as a subspace clustering problem with two subspaces. Therefore, for ease of exposition we first investigate the two-subspace scenario then extend the result to multiple (more than two) subspaces. Thus, in this subsection, it is assumed that the data follows Data model 1 with N = 2.\nTo gain some intuition, we first consider a simple example before stating our main result. Suppose that S1 and S2 are not orthogonal and assume that n2 < n1. The non-orthogonality of S1 and S2 is not a requirement, but is merely used herein to simplify the exposition of the basic idea underlying the proposed approach. Define c∗ as the optimal point of the following optimization problem\nmin ĉ ‖ĉTD‖0 subject to ĉ ∈ D and ‖ĉ‖ = 1, (1)\nwhere ‖.‖0 is the `0-norm. The first constraint limits the search space to the span of the data, and the equality constraint ‖ĉ‖ = 1 is used to avoid the trivial ĉ = 0 solution. Assume that the columns of D1 and D2 are uniformly distributed in S1 and S2, respectively. Accordingly, with high probability (whp) the data is not aligned along any specific direction in S1 and S2.\nThe `0-norm minimization problem (1) searches for a nonzero vector inD that is orthogonal to the maximum number of data points. Since c∗ has to lie in D, we claim that the optimal point of (1) lies in I (S2 ⊥ S1) whp given the assumption that the number of data points in S1 is greater\nthan the number of data points in S2. To clarify, consider the following cases:\nI. If c∗ ∈ S1, then it would not be orthogonal to the majority of the data points in S1 given that the columns of D1 are uniformly distributed in S1. In addition, it cannot be orthogonal to most of the data points in S2 since S1 and S2 are not orthogonal. Since the optimal vector should be orthogonal to the maximum number of data points, it is highly likely that c∗ 6∈ S1. Similarly, it is highly unlikely that the optimal point lies in S2. II. If c∗ ∈ I (S1 ⊥ S2), then according to Definition 1, it is orthogonal to the data points in D2. However, since it was assumed that n2 < n1, the cost function of (1) can be decreased if c∗ lies in I (S2 ⊥ S1) (which is orthogonal to S1). III. If c∗ does not lie in any of the subspaces S1, S2, I (S2 ⊥ S1) and I (S2 ⊥ S1), then it is neither orthogonal to S1 nor to S2. Now, since the data points are distributed uniformly in the subspaces, we see that c∗ would not be orthogonal to the maximum number of data points.\nHence, it is highly likely that c∗ lies in I (S2 ⊥ S1). It follows that the columns of D corresponding to the nonzero elements of (c∗)TD lie in S2. The following lemma ensures that these columns span S2. Lemma 1. The columns of D corresponding to the nonzero elements of (c∗)TD span S2 if the following conditions are satisfied: (i) c∗ ∈ I (S2 ⊥ S1) , (ii) D2 cannot follow Data model 1 with N > 1, that is, the data points in D2 do not lie in the union of lower dimensional subspaces within S2 each with innovation w.r.t. the other subspaces.\nIt is important to note that the conditions of Lemma 1 are by no means restrictive. Specifically, if the requirement (ii) of Lemma 1 is not satisfied, then the problem can be viewed as a subspace clustering problem with more than two subspaces. In Section 2.4, we will investigate the clustering problem with more than two subspaces.\nRemark 1. At a high level, the innovation search problem (1) finds the most sparse vector in the row space of D. Interestingly, finding the most sparse vector in a linear subspace has bearing on, and has been effectively used in, other machine learning problems, including dictionary learning and spectral estimation (Qu et al., 2014)."
  }, {
    "heading": "2.2. Convex relaxation",
    "text": "The `1-norm is known to provide an efficient convex relaxation of the `0-norm. Thus, we relax the non-convex cost function and rewrite (1) as\nmin ĉ ‖ĉTD‖1 subject to ĉ ∈ D and ‖ĉ‖ = 1 . (2)\nSince the feasible set of (2) is non-convex, we further substitute the equality constraint with a linear constraint to\nconsider the following convex program\n(IP) min ĉ ‖ĉTD‖1 s. t. ĉ ∈ D and ĉTq = 1. (3)\n(IP) is the core program of iPursuit to find a direction of innovation. The vector q is a unit `2-norm vector that should not be orthogonal toD. In Section 2.6, we present a methodology to obtain a good choice for the vector q from the given data. However, our investigations have shown that iPursuit performs well generally even when q is chosen as a random vector in D."
  }, {
    "heading": "2.3. Segmentation of two subspaces: Performance guarantees",
    "text": "Suppose that D follows Data model 1 with N = 2, i.e., the data lies in a union of two subspaces. In order to show that the optimal point of (IP) yields correct clustering, it suffices to show that the optimal point lies in I (S2 ⊥ S1) given that condition (ii) of Lemma 1 is satisfied for D2 (or lies in I (S1 ⊥ S2) given that the condition is satisfied for D1). The following theorem provides sufficient conditions for the optimal point of (3) to lie in I (S2 ⊥ S1) provided that\ninf c∈I(S2⊥S1)\ncT q=1\n‖cTD‖1 < inf c∈I(S1⊥S2)\ncT q=1\n‖cTD‖1. (4)\nIf the inequality in (4) is reversed, then similar sufficient conditions can be established for the optimal point of (3) to lie in I (S1 ⊥ S2). Hence, assumption (4) does not lead to any loss of generality. The subspaces I (S2 ⊥ S1) and I (S1 ⊥ S2) are orthogonal to S1 and S2, respectively. Thus, (4) is equivalent to\ninf c∈I(S2⊥S1)\ncT q=1\n‖cTD2‖1 < inf c∈I(S1⊥S2)\ncT q=1\n‖cTD1‖1. (5)\nHenceforth, “innovation subspace” refers to I (S2 ⊥ S1) whenever the two-subspace scenario is considered and (5) is satisfied. Theorem 2 stated next provides sufficient conditions for the optimal point of (IP) in (3) to lie in I (S2 ⊥ S1). These conditions are characterized in terms of the optimal solution to an oracle optimization problem (OP), wherein the feasible set of (IP) is replaced by the innovation subspace. Define c2 as the optimal point of the following optimization problem\n(OP) min ĉ ‖ĉTD2‖1\nsubject to ĉ ∈ I (S2 ⊥ S1) and ĉTq = 1. (6)\nTheorem 2 establishes that c2 is the optimal point of (3). Before we state the theorem, we define the index set L0 := {i ∈ [n2] : cT2 di = 0,di ∈ D2}, with cardinality n0 = |L0| and a complement set Lc0, comprising the indices of the columns of D2 orthogonal to c2.\nTheorem 2. Suppose the data matrix D follows Data model 1 with N = 2. Also, assume that condition (5) and the requirement of Lemma 1 for D2 are satisfied (condition (ii) of Lemma 1). Let c2 be the optimal point of the oracle (OP) in (6) and define α =\n∑ di∈D2 i∈Lc0 sgn(cT2 di)di.\nAlso, let P2 denote an orthonormal basis for I (S2 ⊥ S1) and assume that q is a unit `2-norm vector in D that is not orthogonal to I (S2 ⊥ S1). If\n1 2 inf δ∈S1 ‖δ‖=1 ∑ di∈D1 ∣∣δTdi∣∣ > ‖VT1 V2‖(‖α‖+ n0) , and ‖qTP2‖ 2‖qTV1‖ ( inf δ∈S1 ‖δ‖=1 ∑ di∈D1\n∣∣δTdi∣∣) > ‖VT2 P2‖(‖α‖+ n0), (7)\nthen c2 ∈ I (S2 ⊥ S1) is the optimal point of (IP) in (3), and iPursuit clusters the data correctly.\nIn what follows, we provide a detailed discussion of the significance of the sufficient conditions (7) of Theorem 2, which reveal some interesting facts about the properties of iPursuit.\n1. Data distribution: The LHS of (7) is known as the permeance statistic, an efficient measure of how well the data points are distributed in a subspace (Lerman et al., 2015). As such, the sufficient conditions (7) imply that the distribution and the number of the data points within the subspaces are important performance factors for iPursuit. For a set of data points Di in a subspace Si, the permeance statistic is defined as P(Di,Si) = inf\nu∈Si ‖u‖=1\n∑ di∈Di ∣∣uTdi∣∣ . From this definition, we see that the permeance statistic is fairly small if a set of data points are aligned along a given direction (i.e. not well distribued). In addition, having n0 on the RHS reveals that the distribution of the data points within S2 also matters since c2 cannot be simultaneously orthogonal to a large number of columns of D2 if the data does not align along specific directions. Hence, according to (7), iPursuit yields correct clustering if the data is well distributed within the subspaces.\n2. The coherency of q with I (S2 ⊥ S1): For the optimal point of (IP) to lie in I (S2 ⊥ S1), the vector q should not be too coherent with S1. This can be seen by observing that if q has a small projection on I (S2 ⊥ S1) – in which case it would be more coherent with S1 – the Euclidean norm of any feasible point of (3) lying in I (S2 ⊥ S1) will have to be large to satisfy the equality constraint in (IP). Such points are less likely to be optimal in the sense of attaining the minimum of the objective function in (3). The aforementioned intuition is affirmed by the analysis. Indeed, the factor ‖qTP2‖/‖qTV1‖ in the sufficient conditions of Theorem 2 and Lemma 3 indicates that the coherency of q\nwith the innovation subspace is an important performance factor for iPursuit. The coherence property could have a more serious effect on the performance of the algorithm for non-independent subspaces, especially when the dimension of their intersection is significant. For instance, consider the scenario where the vector q is chosen randomly from D, and define y as the dimension of the intersection of S1 and S2. It follows that I (S2 ⊥ S1) has dimension r2 − y. Thus, E[‖q\nTP2‖] E[‖qTV1‖] = r2−y r1\n. Therefore, a randomly chosen vector q is likely to have a small projection on the innovation subspace when y is large. As such, in dealing with subspaces with significant intersection, it may not be favorable to choose the vector q at random. In Section 2.6 and section 2.7, we develop a simple technique to learn a good choice for q from the given data. It makes iPursuit remarkably powerful in dealing with subspaces with intersection as shown in the numerical results section. Remark 2. We conjecture that if the ratios {ni/ri}Ni=1 are sufficiently large, the optimal point of (2) always lies in an innovation subspace; our investigations have shown that the optimal point of (IP) always lies in an innovation subspace provided that q is sufficiently coherent with the innovation subspace regardless how large the intersections between the subspaces are. This is particularly compelling if the subspaces are too close, in which case spectralclustering-based methods can seldom construct a correct similarity matrix (which leads to large clustering errors). By contrast, in such cases iPursuit can yield accurate clustering provided that the constraint vector is sufficiently coherent with the innovation subspace. In Section 2.6, we present a method to identify a coherent constraint vector.\nNow, we demonstrate that the sufficient conditions (7) are not restrictive. The following lemma simplifies the conditions when the data points are randomly distributed in the subspaces. In this setting, we show that the conditions are naturally satisfied. Lemma 3. Assume that the distribution of the columns of D1 in S1 and D2 in S2 is uniformly random and consider the same setup of Theorem 2. If√\n2\nπ n1 r1 − 2 √ n1 − t1\n√ n1\nr1 − 1 > 2‖VT1 V2‖ ( t2 √ n2 − n0 + n0 ) ,\n‖qTP2‖ ‖qTV1‖\n(√ 2\nπ n1 r1 − 2 √ n1 − t1\n√ n1\nr1 − 1\n)\n> 2‖VT2 P2‖ ( t2 √ n2 − n0 + n0 ) ,\n(8)\nthen the optimal point of (3) lies in I (S2 ⊥ S1) with probability at least 1 − exp ( − r22 (t 2 2 − log(t22)− 1) ) −\nexp ( − t 2 1\n2\n) , for all t2 > 1 , t1 ≥ 0.\nWhen the data points are not aligned along any specific directions, c2 can only be simultaneously orthogonal to a small number of columns of D2. Thus, n0 will be much smaller than n2. The LHS of (8) has order n1 and the RHS has order √ n2+n0 (which is much smaller than n2). Therefore, the sufficient conditions are naturally satisfied when the data is well distributed within the subspaces."
  }, {
    "heading": "2.4. Clustering multiple subspaces",
    "text": "Suppose that D follows Data Model 1 with N = m where m > 2. Similar to (5), without loss of generality assume that\ninf c∈I ( Sm⊥\nm ⊕ k=1 k 6=m Sk ) cT qm=1 ‖cTDm‖1 < inf c∈I ( Sj⊥ m ⊕ k=1 k 6=j Sk ) cT qm=1 ‖cTDj‖1 (9)\nfor all 1 ≤ j ≤ m − 1 , where qm is a unit `2-norm in ⊕mk=1Sk. Given (9), we expect the optimal point of (IP) with q = qm to lie in the innovation subspace of Sm over m ⊕ k=1 k 6=m Sk, i.e., I ( Sm ⊥ m ⊕ k=1 k 6=m Sk ) , in which case Sm will be identified. Accordingly, in this step the clustering problem separates (Dm,Sm) and ( m−1 ∪ i=1 Di , m−1 ⊕ i=1 Si). Theorem 2 can be used to derive sufficient conditions for the optimal point to lie in I ( Sm ⊥\nm ⊕ k=1 k 6=m\nSk ) by substituting (D2,S2)\nwith (Dm,Sm) and (D1,S1) with ( m−1 ∪ i=1 Di , m−1 ⊕ i=1 Si). Consequently, we can also establish sufficient conditions for exact subspace segmentation. Due to space limitations, we defer the analysis to (Rahmani & Atia, 2015)."
  }, {
    "heading": "2.5. Complexity analysis",
    "text": "Define U as an orthonormal basis for D. Thus, the optimization problem (3) is equivalent to min a\n‖aTUTD‖1 subject to aTUTq = 1. Further, define f = UTq. This optimization problem can be efficiently solved using the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011). Due to space constraints, we defer the details of the iterative solver to (Rahmani & Atia, 2015). The complexity of the initialization step of the solver is O(r3) plus the complexity of obtaining U. Obtaining an appropriate U has O(r2M2) complexity by applying the clustering algorithm to a random subset of the rows of D (with the rank of sampled rows equal to r). In addition, the complexity of each iteration of the solver is O(rM2). Thus, the overall complexity is less than O((r3 + r2M2)N) since the number of data points remaining keeps decreasing over the iterations. In most cases, r M2, hence the overall complexity is roughly O(r2M2N).\nRemark 3. The proposed method brings about substantial speedups over existing algorithms due to the following: i) unlike existing multi-step algorithms (such as RANSAC) which have exponential complexity in the number and dimension of subspaces, the complexity of iPursuit is linear in the number of subspaces and quadratic in their dimension. In addition, while iPursuit has linear complexity in M2, spectral-clustering-based algorithms have complexity O(M22N) for their spectral clustering step plus the complexity of obtaining the similarity matrix; ii) more importantly, the solver of the proposed optimization problem has O(rM2) complexity per iteration, while the other operations – whose complexity are O(r2M2) and O(r3) – sit outside of the iterative solver. This feature makes the proposed method notably faster than most of the existing algorithms which solve high-dimensional optimization problems. For instance, solving the optimization problem of the SSC algorithm has roughly O(M32 + rM2) complexity per iteration (Elhamifar & Vidal, 2013). For instance, supposeM1 = 100, the data lies in a union of three 10-dimensional subspaces and ni = M2/3. Table 1 compares the running time of the algorithms. One can observe that iPursuit is remarkably faster. More running time comparisons are available in (Rahmani & Atia, 2015)."
  }, {
    "heading": "2.6. How to choose the vector q?",
    "text": "Our investigations have shown that iPursuit performs very well when the subspaces are independent or have small intersections even if q is chosen randomly. However, in the more challenging scenarios in which the dimensions of the intersections between the subspaces are significant, randomly choosing the vector q could be unfavorable since the dimension of the innovation subspace decreases as the dimension of the intersection increases. This motivates the methodology described next that aims to search for a “good” vector q. Consider the optimization problem\nmin q̂ ‖q̂TD‖2 subject to q̂ ∈ D and ‖q̂‖ = 1, (10)\nwhich searches for a non-zero vector in D with small projections on the columns of D. It is straightforward to show that the optimal point of (10) is the singular vector corresponding to the least non-zero singular value of D. When the subspaces are close to each other, it is not hard to see\nthat the least singular vector is highly coherent with the innovation subspace, thus can be a good candidate for the vector q. For subspaces with remarkable intersections, this choice of q brings about substantial improvement in performance compared to using a randomly generated q (cf. Section 3). Clearly, when the data is noisy, we utilize the least dominant singular vecor. In addition, when the singular values of the noisy data decay rapidly, it may be hard to accurately estimate the rank of D, which may lead to an unfavorable use of a singular vector corresponding to noise as the constraint vector. Alternatively, we can choose the data point closest to the least dominant singular vector as our vector q. This technique makes the proposed method robust to the presence of noise (cf. Section 2.7)."
  }, {
    "heading": "2.7. Noisy data",
    "text": "In the presence of additive noise, we model the data as De = D + E , where De is the given noisy data matrix, D is the clean data which follows Data model 1 and E represents the noise component. The rank of D is equal to r. Thus, the singular values of De can be divided into two subsets: the dominant singular values (the first r singular values) and the small singular values (or the singular values corresponding to the noise component). Estimating the number of dominant singular values is a fairly well-studied topic (Stoica & Selen, 2004).\nConsider the optimization problem (IP) using De, i.e.,\nmin ĉ ‖ĉTDe‖1 s.t. ĉ ∈ span(De) and ĉTq = 1. (11)\nClearly, the optimal point of (11) is very close to the subspace spanned by the singular vectors corresponding to the small singular values. Thus, if ce denotes the optimal solution of (11), then all the elements of cTe De will be fairly small and the subspaces cannot be distinguished. However, the span of the dominant singular vectors is approximately equal toD. Accordingly, we propose the following approximation to (IP),\nmin ĉ ‖ĉTDe‖1 s.t. ĉ ∈ span(Q) and ĉTq = 1 (12)\nwhere Q is an orthonormal basis for the span of the dominant singular vectors. The first constraint of (12) forces the optimal point to lie in span(Q), which serves as a good approximation to span(D). For instance, consider D = [D1 D2], where the columns of D1 ∈ R40×100 lie in a 5-dimensional subspace S1, and the columns of D2 ∈ R40×100 lie in another 5-dimensional subspace S2. Define ce and cr as the optimal points of (11) and (12), respectively. Fig. 2 shows |cTe De| and |cTr De| with the maximum element scaled to one. Clearly, cTr De can be used to correctly cluster the data. In addition, when D is low rank, the subspace constraint in (12) can filter out a remarkable portion of the noise component.\nWhen the data is noisy and the singular values of D decay rapidly, it may be hard to accurately estimate r. If the dimension is incorrectly estimated, Q may contain some singular vectors corresponding to the noise component, wherefore the optimal point of (12) could end up lying close to a noise singular vector. In the sequel, we present two effective techniques to effectively avoid this undesirable scenario.\n1. Using a data point as a constraint vector: A singular vector corresponding to the noise component is nearly orthogonal to the entire data, i.e., has small projection on all the data points. Thus, if the optimal vector is forced to have strong projection on a data point, it will be unlikely for the optimal direction to be close to a noise singular vector. Thus, we modify (12) as follows\nmin ĉ ‖ĉTDe‖1 s.t. ĉ ∈ span(Q) and ĉTdek = 1 , (13)\nwhere dek is the kth column of De. The modified constraint in (13) ensures that the optimal point is not orthogonal to dek. If dek lies in the subspace Si, the optimal point of (13) will lie in the innovation subspace corresponding to Si whp. In order to determine a good data point for the constraint vector, we leverage the principle presented in section 2.6. Specifically, we use the data point that is closest to the least dominant singular vector rather than the least dominant singular vector itself.\n2. Sparse representation of the optimal point: When D is low rank, i.e., r min(M1,M2), any direction in the span of the data – including the optimal direction sought by iPursuit – can be represented as a sparse combination of the data points. For such settings, we propose the alternative optimization problem\nmin a,z\n‖aTQTDe‖1 + γ‖z‖1\nsubject to a = QTDe z and aTQTdek = 1 , (14)\nwhere γ is a regularization parameter. Forcing a sparse representation in (14) for the optimal direction averts a solution that lies in close proximity with the small singular vectors, which are normally obtained through linear combinations of a large number of data points. This alternative\nformulation is particularly useful when the dimension of the data cannot be accurately estimated. When D is not a low rank matrix, we can set γ equal to zero. The table of Algorithm 1 details the proposed method for noisy data along with the used notation and definitions."
  }, {
    "heading": "2.7.1. ERROR PROPAGATION",
    "text": "If κ (or ci) and the threshold co in Algorithm 1 are chosen appropriately, the algorithm exhibits strong robustness in the presence of noise. Nonetheless, if the data is too noisy, an error incurred in one step of the algorithm may propagate and unfavorably affect the performance in subsequent steps. Two types of error could occur. The first type is that some data points are erroneously included in G1 or G2. An example is when Sm is the subspace to be identified in a given step of the algorithm (i.e., the optimal point of (13) lies in the innovation subspace corresponding to Sm), but few data points from the other subspaces are erroneously included in G1. The second type of error is that some of the data points remain unidentified. For instance, Sm is to be identified in a given iteration, yet not all the data points belonging to Sm are identified. In an extended version of this work (Rahmani & Atia, 2015), we discuss the two main sources of error and present some techniques to effectively neutralize their impact on subsequent iterations. In addition, a brief discussion about handling the presence of outliers is provided."
  }, {
    "heading": "2.8. Subspace clustering using direction search and spectral clustering",
    "text": "In (Rahmani & Atia, 2017), we showed that the direction search optimization problem (13) can be utilized to find a neighborhood set for the kth data point. We leveraged this feature to propose a new spectral-clusteringbased subspace segmentation algorithm, dubbed Direction search based Subspace Clustering (DSC) (Rahmani & Atia, 2017). We showed that DSC often outperforms the existing spectral-clustering-based methods particularly for hard scenarios involving high levels of noise and close subspaces, and notably improves the state-of-the-art result for the challenging problem of face segmentation using subspace clustering."
  }, {
    "heading": "3. Numerical Simulations",
    "text": ""
  }, {
    "heading": "3.1. The coherency parameter",
    "text": "In this experiment, we examine the impact of the coherency of q with the innovation subspace. Here, it is assumed that the data follows Data Model 1 with N = 2 and M1 = 50. The dimension of the subspaces is equal to 15 and the dimension of their intersection varies between 0 to 14. Each subspace contains 100 data points distributed uniformly at\nrandom within the subspace. Let cr = ‖qTP2‖/‖qTV1‖. Thus, cr captures the coherency of q with the innovation subspace. Define V̂1 and V̂2 as orthonormal bases for the identified subspaces. A trial is considered successful if\n‖(I−V1VT1 )V̂1‖F + ‖(I−V2VT2 )V̂2‖F ≤ 10−3 . (15)\nThe left plot of Fig. 3 shows the phase transition in the plane of cr and y, where y is the dimension of the intersection of the two subspaces. In this figure, white designates exact identification of the subspaces with probability almost equal to one. As shown, the probability of correct clustering increases if cr is increased. Remarkably, the left plot of Fig. 3 shows that when cr is sufficiently large, the algorithm yields exact segmentation even when y = 14."
  }, {
    "heading": "3.2. Clustering data in union of multiple subspaces",
    "text": "In this simulation, we consider the subspace clustering problem with 15 30-dimensional subspaces {Si}15i=1 and M1 = 500. Each subspace contains 90 data points and the distribution of the data within the subspaces is uniformly random. We compare the performance of the proposed approach to the state-of-the-art sparse subspace clustering (SSC) (Elhamifar & Vidal, 2013) algorithm and low rank representation (LRR) based clustering (Liu et al., 2013). The number of replicates used in spectral clustering for SSC and LRR is equal to 20. Define the clustering error as the ratio of misclassified points to the total number of data points. The right plot of Fig. 3 shows the clustering error versus the dimension of the intersection. The dimension of intersection varies between 1 and 29. Each point in the plot is obtained by averaging over 40 independent runs. iPursuit is shown to yield the best performance."
  }, {
    "heading": "3.3. Noisy data",
    "text": "In this section, we study the performance of the proposed approach, SSC, LRR, SCC (Chen & Lerman, 2009), TSC (Heckel & Bölcskei, 2013) and SSC-OMP (Dyer et al.,\nTable 2. CE (%) of algorithms on Hopkins155 dataset (Mean - Median).\nN SSC LRR iPursuit SSC-OMP TSC K-flats SCC\nN = 2 1.52 - 0 2.13 - 0 3.33 - 0.27 16.92 - 12.77 s 18.44 - 16.92 13.62 - 10.65 2.06 - 0 N = 3 4.40 - 1.56 s 4.03 - 1.43 6.91 - 2.44 27.96 - 30.98 28.58 - 29.67 14.07 - 14.18 6.37 - 0.21\nFigure 4. Performance of the algorithms versus the dimension of intersection for different noise levels.\n2013) with different noise levels, and varying dimensions of the intersection between the subspaces, which gives rise to both low rank and high rank data matrices. It is assumed that D follows Data model 1 with M1 = 100, M2 = 500, N = 6 and {ri}6i=1 = 15. The dimension of the intersection between the subspaces varies from 0 to 14. Thus, the rank of D ranges from 20 to 90. The Noisy data is modeled as De = D + E, with the elements of E sampled independently from a zero mean Gaussian distribution. Fig. 4 shows the performance of the different algorithms versus the dimension of the intersection for τ = ‖E‖F‖D‖F equal to 1/20, 1/10, 1/5 and 1/2. One can observe that even with τ = 1/5, iPursuit significantly outperforms the other algorithms. In addition, when the data is very noisy, i.e., τ = 1/2, it yields better performance when the dimension of the intersection is large. SSC, LRR, and SSC-OMP yield a better performance for lower dimension of intersection. This is explained by the fact that the rank of the data is high when the dimension of the intersection is low, and the subspace projection operation QTDe may not always filter out the additive noise effectively."
  }, {
    "heading": "3.4. Real data",
    "text": "We apply iPursuit to the problem of motion segmentation using the Hopkins155 (Tron & Vidal, 2007) dataset, which contains video sequences of 2 or 3 motions. In motion segmentation, each motion corresponds to one subspace.\nThus, the problem here is to cluster data lying in two or three subspaces. Table 2 shows the clustering error (in percentage) for iPursuit, SSC, LRR, TSC, SSC-OMP and Kflats. We use the results reported in (Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2013; Vidal, 2011; Park et al., 2014). For SSC-OMP and TSC, the number of parameters for motion segmentation are equal to 8 and 10. One can observe that iPursuit yields competitive results comparable to SSC, SCC, and LRR and outperforms TSC, SSC-OMP and K-flats.\nAlgorithm 1 Innovation pursuit (iPursuit) for noisy data Initialization Set κ, n̂ and N̂ as integers greater than 1, and set ci and co as positive real numbers less than 1.\nWhile The number of identified subspaces is less than N̂ or the number of the columns of De is less than n̂. 1. Obtaining the basis for the remaining Data: Construct Q as the orthonormal matrix formed by the dominant singular vectors of De. 2. Choosing the vector q: Set q = the column of De closest to the last column of Q. 3. Solve (14) and define c∗ = Qa∗, where a∗ is the optimal point\nof (14) and define h1 = ∣∣DTe c∗∣∣ max(\n∣∣DTe c∗∣∣) . 4. Finding a basis for the identified subspace: Construct G1 as the matrix consisting of the columns of De corresponding to the elements of h1 that are greater than ci. Alternatively, construct G1 using the columns of De corresponding to the κ largest elements of h1. Define F1 as an orthonormal basis for the dominant left singular vectors of G1."
  }, {
    "heading": "5. Finding a basis for the rest of the data:",
    "text": "Define the vector h2 whose entries are equal to the `2-norm of the columns of (I − F1FT1 )De. Scale h2 as h2 := h2/max(h2). Construct G2 as the columns of De corresponding to the elements of h2 greater than co. Define F2 as an orthonormal basis for the dominant left singular vectors of of G2."
  }, {
    "heading": "6. Find the data point belonging to the identified subspace:",
    "text": "Assign dei to the identified subspace if ‖FT1 dei‖ ≥ ‖FT2 dei‖. 7. Remove the data points belonging to the identified subspace: Update De by removing the columns corresponding to the identified subspace.\nEnd While\nAcknowledgment: This work was supported by NSF CAREER Award CCF-1552497 and NSF Grant CCF1320547."
  }],
  "year": 2017,
  "references": [{
    "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
    "authors": ["Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan"],
    "venue": "Foundations and Trends® in Machine Learning,",
    "year": 2011
  }, {
    "title": "Spectral curvature clustering (scc)",
    "authors": ["Chen", "Guangliang", "Lerman", "Gilad"],
    "venue": "International Journal of Computer Vision,",
    "year": 2009
  }, {
    "title": "Greedy feature selection for subspace clustering",
    "authors": ["Dyer", "Eva L", "Sankaranarayanan", "Aswin C", "Baraniuk", "Richard G"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Sparse subspace clustering: Algorithm, theory, and applications",
    "authors": ["Elhamifar", "Ehsan", "Vidal", "Rene"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2013
  }, {
    "title": "Robust subspace clustering via thresholding",
    "authors": ["Heckel", "Reinhard", "Bölcskei", "Helmut"],
    "venue": "arXiv preprint arXiv:1307.4891,",
    "year": 2013
  }, {
    "title": "Robust computation of linear models by convex relaxation",
    "authors": ["Lerman", "Gilad", "McCoy", "Michael B", "Tropp", "Joel A", "Zhang", "Teng"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2015
  }, {
    "title": "Robust recovery of subspace structures by low-rank representation",
    "authors": ["Liu", "Guangcan", "Lin", "Zhouchen", "Yan", "Shuicheng", "Sun", "Ju", "Yu", "Yong", "Ma", "Yi"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2013
  }, {
    "title": "Greedy subspace clustering",
    "authors": ["Park", "Dohyung", "Caramanis", "Constantine", "Sanghavi", "Sujay"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions",
    "authors": ["Qu", "Qing", "Sun", "Ju", "Wright", "John"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Innovation pursuit: A new approach to subspace clustering",
    "authors": ["Rahmani", "Mostafa", "Atia", "George"],
    "venue": "arXiv preprint arXiv:1512.00907,",
    "year": 2015
  }, {
    "title": "A direction search and spectral clustering based approach to subspace clustering",
    "authors": ["Rahmani", "Mostafa", "Atia", "George"],
    "venue": "arXiv preprint,",
    "year": 2017
  }, {
    "title": "Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories",
    "authors": ["Rao", "Shankar", "Tron", "Roberto", "Vidal", "Rene", "Ma", "Yi"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2010
  }, {
    "title": "A geometric analysis of subspace clustering with outliers",
    "authors": ["Soltanolkotabi", "Mahdi", "Candes", "Emmanuel J"],
    "venue": "The Annals of Statistics,",
    "year": 2012
  }, {
    "title": "Model-order selection: a review of information criterion rules",
    "authors": ["Stoica", "Petre", "Selen", "Yngve"],
    "venue": "Signal Processing Magazine, IEEE,",
    "year": 2004
  }, {
    "title": "A benchmark for the comparison of 3-d motion segmentation algorithms",
    "authors": ["Tron", "Roberto", "Vidal", "René"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp",
    "year": 2007
  }, {
    "title": "An algebraic geometric approach to the identification of a class of linear hybrid systems",
    "authors": ["Vidal", "René", "Soatto", "Stefano", "Ma", "Yi", "Sastry", "Shankar"],
    "venue": "In Proceedings of the 42nd IEEE Conference on Decision and Control (CDC),",
    "year": 2003
  }, {
    "title": "Generalized principal component analysis (GPCA)",
    "authors": ["Vidal", "Rene", "Ma", "Yi", "Sastry", "Shankar"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 1945
  }, {
    "title": "Multiframe motion segmentation with missing data using powerfactorization and GPCA",
    "authors": ["Vidal", "René", "Tron", "Roberto", "Hartley", "Richard"],
    "venue": "International Journal of Computer Vision,",
    "year": 2008
  }, {
    "title": "Subspace clustering",
    "authors": ["Vidal", "Rene"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2011
  }, {
    "title": "A tutorial on spectral clustering",
    "authors": ["Von Luxburg", "Ulrike"],
    "venue": "Statistics and computing,",
    "year": 2007
  }, {
    "title": "Robust statistical estimation and segmentation of multiple subspaces",
    "authors": ["Yang", "Allen Y", "Rao", "Shankar R", "Ma", "Yi"],
    "venue": "In Computer Vision and Pattern Recognition Workshop,",
    "year": 2006
  }, {
    "title": "Unsupervised segmentation of natural images via lossy data compression",
    "authors": ["Yang", "Allen Y", "Wright", "John", "Ma", "Yi", "Sastry", "S Shankar"],
    "venue": "Computer Vision and Image Understanding,",
    "year": 2008
  }],
  "id": "SP:622d7db1031ecbb4e674ec6061d31c3a7d301fbd",
  "authors": [{
    "name": "Mostafa Rahmani",
    "affiliations": []
  }, {
    "name": "George Atia",
    "affiliations": []
  }],
  "abstractText": "This paper presents a new scalable approach, termed Innovation Pursuit (iPursuit), to the problem of subspace clustering. iPursuit rests on a new geometrical idea whereby each subspace is identified based on its novelty with respect to the other subspaces. The subspaces are identified consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data. A detailed mathematical analysis is provided establishing sufficient conditions for the proposed approach to correctly cluster the data points. Moreover, the proposed direction search approach can be integrated with spectral clustering to yield a new variant of spectral-clustering-based algorithms. Remarkably, the proposed approach can provably yield exact clustering even when the subspaces have significant intersections. The numerical simulations demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms – more so for subspaces with significant intersections – along with substantial reductions in computational complexity.",
  "title": "Innovation Pursuit: A New Approach to the Subspace Clustering Problem"
}