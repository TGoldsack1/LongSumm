{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Bayesian Neural Networks (BNNs) are increasingly the defacto approach for modeling stochastic functions. By treating the weights in a neural network as random variables, and performing posterior inference on these weights, BNNs can avoid overfitting in the regime of small data, provide well-calibrated posterior uncertainty estimates, and model a large class of stochastic functions with heteroskedastic and multi-modal noise. These properties have resulted in BNNs being adopted in applications ranging from active learning (Hernández-Lobato & Adams, 2015; Gal et al., 2016a) and reinforcement learning (Blundell et al., 2015; Depeweg et al., 2017).\nWhile there have been many recent advances in training BNNs (Hernández-Lobato & Adams, 2015; Blundell et al., 2015; Rezende et al., 2014; Louizos & Welling, 2016; Hernandez-Lobato et al., 2016), model-selection in BNNs has received relatively less attention. Unfortunately, the\n1IBM research, Cambridge, MA, USA 2MIT-IBM Watson AI Lab 3Harvard University, Cambridge, MA, USA. Correspondence to: Soumya Ghosh <ghoshso@us.ibm.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nconsequences for a poor choice of architecture are severe: too few nodes, and the BNN will not be flexible enough to model the function of interest; too many nodes, and the BNN predictions will have large variance. We note that these Bayesian model selection concerns are subtlely different from overfitting and underfitting concerns that arise from maximum likelihood training: here, more expressive models (e.g. those with more nodes) require more data to concentrate the posterior. When there is insufficent data, the posterior uncertainty over the BNN weights will remain large, resulting in large variances in the BNN’s predictions. We illustrate this issue in Figure 1, where we see a BNN trained with too many parameters has higher variance around its predictions than one with fewer. Thus, the core concern of Bayesian model selection is to identify a model class expressive enough that it can explain the observed data set, but not so expressive that it can explain everything (Rasmussen & Ghahramani, 2001; Murray & Ghahramani, 2005).\nModel selection in BNNs is challenging because the number of nodes in a layer is a discrete quantity. Recently, (Ghosh & Doshi-Velez, 2017; Louizos et al., 2017) independently proposed performing model selection in Bayesian neural networks by placing Horseshoe pri-\nors (Carvalho et al., 2009) over the weights incident to each node in the network. This prior can be interpreted as a continuous relaxation of a spike-and-slab approach that would assign a discrete on-off variable to each node, allowing for computationally-efficient optimization via variational inference.\nIn this work, we expand upon this idea with several innovations and careful experiments. Via a combination of using regularized horseshoe priors for the node-specific weights and variational approximations that retain critical posterior structure, we both improve upon the statistical properties of the earlier works and provide improved generalization, especially for smaller data sets and in sample-limited settings such as reinforcement learning. We also present a new thresholding rule for pruning away nodes. Unlike previous work our rule does not require computing a point summary of the inferred posteriors. We compare the various model and inference combinations on a diverse set of regression and reinforcement learning tasks. We find that the proposed innovations consistently improve upon the compactness of the models learned without sacrificing predictive performance."
  }, {
    "heading": "2. Bayesian Neural Networks",
    "text": "A Bayesian neural network endows the parameters W of a neural network with distributions W ∼ p(W). When combined with inference algorithms that infer posterior distributions over weights, they are able to capture posterior as well as predictive uncertainties. For the following, consider a fully connected deep neural network with L−1 hidden layers, parameterized by a set of weight matrices W = {Wl}L1 , where Wl is of size RKl−1+1×Kl , and Kl is the number of units in layer l. The network maps an input x ∈ RD to a response f(W, x) by recursively applying the transformation h(WTl [z T l , 1]\nT ), where zl ∈ RKl×1 is the input into layer l, the initial input z0 is x, and h is a point-wise non-linearity such as the rectified-linear function, h(a) = max(0, a).\nGiven N observation response pairs D = {xn, yn}Nn=1 and p(W), we are interested in the posterior distribution p(W | D) ∝ ∏N n=1 p(yn | f(W, xn))p(W), and in using\nit for predicting responses to unseen data x∗, p(y∗ | x∗) =∫ p(y∗ | f(W, x∗))p(W | D)dW. The prior p(W) allows one to encode problem-specific beliefs as well as general properties about weights."
  }, {
    "heading": "3. Bayesian Neural Networks with Regularized Horseshoe Priors",
    "text": "Let wkl ∈ RKl−1+1×1 denote the set of all weights incident into unit k of hidden layer l. Ghosh & Doshi-Velez (2017); Louizos et al. (2017) introduce a prior such that each unit’s weight vector wkl is conditionally independent and follow\na group Horseshoe prior (Carvalho et al., 2009),\nwkl | τkl, υl ∼ N (0, (τ2klυ2l )I), τkl ∼ C+(0, b0), υl ∼ C+(0, bg). (1)\nHere, I is an identity matrix, a ∼ C+(0, b) is the HalfCauchy distribution with density p(a|b) = 2/πb(1 + (a2/b2)) for a > 0, τkl is a unit specific scale parameter, while the scale parameter υl is shared across the layer. This horseshoe prior exhibits Cauchy-like flat, heavy tails while maintaining an infinitely tall spike at zero. As a result, it allows sufficiently large unit weight vectors wkl to escape un-shrunk—by having a large scale parameter—while providing severe shrinkage to small weights. By forcing all weights incident on a unit to share scale parameters, we are able to induce sparsity at the unit level, turning off units that are unnecessary for explaining the data well. Intuitively, the shared layer wide scale υl pulls all units in layer l to zero, while the heavy tailed unit specific τkl scales allow some of the units to escape the shrinkage.\nRegularized Horseshoe Priors While the horseshoe prior has some good properties, when the amount of training data is limited, units with essentially no shrinkage can produce large weights can adversely affect generalization performance of HS-BNNs, with minor perturbations of the data leading to vastly different predictions. To deal with this issue, here we consider the regularized horseshoe prior (Piironen & Vehtari, 2017). Under this prior wkl is drawn from,\nwkl | τkl, υl, c ∼ N (0, (τ̃2klυ2l )I), τ̃2kl = c2τ2kl\nc2 + τ2klυ 2 l\n. (2)\nNote that for the weight node vectors that are strongly shrunk to zero, we will have tiny τ2klυ 2 l . When, τ 2 klυ 2 l ≪ c2, τ̃2kl → τ2klυ2l , recovering the original horseshoe prior. On the other hand, for the un-shrunk weights τ2klυ 2 l will be large, and when τ2klυ 2 l ≫ c2, τ̃2kl → c2. Thus, these weights under the regularized Horseshoe prior follow wkl ∼ N (0, c2I) and c acts as a weight decay hyperparameter. We place a Inv-Gamma(ca, cb) prior on c2. In the experimental section, we find that the regularized HSBNN does indeed improve generalization over HS-BNN. Below, we describe two essential parametrization considerations essential for using the regularized horseshoe in practice.\nHalf-Cauchy re-parameterization for variational learning. Instead of directly parameterizing the Half-Cauchy random variables in Equations 1 and 2, we use a convenient auxiliary variable parameterization (Wand et al., 2011) of the distribution, a ∼ C+(0, b) ⇐⇒ a2 | λ ∼ Inv-Gamma( 12 , 1 λ );λ ∼ Inv-Gamma( 1 2 , 1 b2 ), where v ∼ Inv-Gamma(a, b) is the Inverse Gamma distribution with\ndensity p(v) ∝ v−a−1exp{−b/v} for v > 0. This avoids the challenges posed by the direct approximation during variational learning — standard exponential family variational approximations struggle to capture the thick Cauchy tails, while a Cauchy approximating family leads to high variance gradients.\nSince the number of output units is fixed by the problem at hand, a sparsity inducing prior is not appropriate for the output layer. Instead, we place independent Gaussian priors, wkL ∼ N (0, κ2I) with vague hyper-priors κ ∼ C+(0, bκ = 5) on the output layer weights. The joint distribution of the regularized Horseshoe Bayesian neural network is then given by,\np(D, θ) = p(c | ca, cb)r(κ, ρκ | bκ) KL∏ k=1 N (wkL | 0, κI) L∏ l=1 r(υl, ϑl | bg) Kl∏ k=1 r(τkl, λkl | b0)N (wkl | 0, (τ̃2klυ2l )I) N∏ n=1 p(yn | f(W, xn)), (3)\nwhere p(yn|f(W, xn)) is the likelihood function and r(a, λ|b) = Inv-Gamma(a2| 12 , 1 λ )Inv-Gamma(λ| 1 2 , 1 b2 ), with θ = {W, T , κ, ρκ, c}, T = {{τkl}K,Lk=1,l=1, {υl}Ll=1, {λkl} K,L k=1,l=1, {ϑl}Ll=1}.\nNon-Centered Parameterization The regularized horseshoe (and the horseshoe) prior both exhibit strong correlations between the weights wkl and the scales τklυl. While their favorable sparsity inducing properties stem from this coupling, it also gives rise to coupled posteriors that exhibit pathological funnel shaped geometries (Betancourt & Girolami, 2015; Ingraham & Marks, 2016) that are difficult to reliably sample or approximate.\nAdopting non-centered parameterizations (Ingraham & Marks, 2016), helps alleviate the issue. Consider a reformulation of Equation 2,\nβkl ∼ N (0, I), wkl = τ̃klυlβkl, (4)\nwhere the distribution on the scales are left unchanged. Since the scales and weights are sampled from independent prior distributions and are marginally uncorrelated, such a parameterization is referred to as non-centered. The likelihood is now responsible for introducing the coupling between the two, when conditioning on observed data. Noncentered parameterizations are known to lead to simpler posterior geometries (Betancourt & Girolami, 2015). Empirically (Ghosh & Doshi-Velez, 2017) have shown that adopting a non-centered parameterization significantly improves the quality of the posterior approximation for BNNs with Horseshoe priors. Thus, we also adopt non-centered parameterizations for the regularized Horseshoe BNNs."
  }, {
    "heading": "4. Structured Variational Learning of Regularized Horseshoe BNNs",
    "text": "We approximate the intractable posterior p(θ | D) with a computationally convenient family. We exploit recently proposed stochastic extensions to scale to both large architectures and datasets, and use black-box variants to deal with non-conjugacy. We begin by selecting a tractable family of distributions q(θ | ϕ), with free variational parameters ϕ. Learning involves optimizing ϕ such that the Kullback-Liebler divergence between the approximation and the true posterior, KL(q(θ | ϕ)||p(θ | D)) is minimized. This is equivalent to maximizing the lower bound to the marginal likelihood (or evidence) p(D), p(D) ≥ L(ϕ) = Eqϕ [ln p(D, θ)] + H[q(θ | ϕ)]. The choice of the approximating family governs the quality of inference."
  }, {
    "heading": "4.1. Variational Approximation Choices",
    "text": "The more flexible the approximating family the better it approximates the true posterior. Below, we first describe a straight-forward fully-factored approximation and then a more sophisticated structured approximation that we demonstrate has better statistical properties.\nFully Factorized Approximations The simplest possibility is to use a fully factorized variational family,\nq(θ | ϕ) = ∏\na∈{c,κ,ρκ}\nq(a | ϕa) ∏ i,j,l\nq(βij,l | ϕβij,l)∏ k,l q(τkl | ϕτkl)q(λkl | ϕλkl) ∏ l q(υl | ϕυl)q(ϑl | ϕϑl). (5)\nRestricting the variational distribution for the non-centered weight βij,l between units i in layer l − 1 and j in layer l, q(βij,l | ϕβijl) to the Gaussian family N (βij,l | µij,l, σ2ij,l), and the non-negative scale parameters τ2kl and υ 2 l and the variance of the output layer weights to the log-Normal family, q(ln τ2kl | ϕτkl) = N (µτkl , σ2τkl), q(ln υ 2 l | ϕυl) = N (µυl , σ2υl), and q(ln κ 2 | ϕκ) = N (µκ, σ2κ), allows for the development of straightforward inference algorithms (Ghosh & Doshi-Velez, 2017; Louizos et al., 2017). It is not necessary to impose distributional constraints on the variational approximations of the auxiliary variables ϑl, λkl, or ρκ. Conditioned on the other variables the optimal variational family for these latent variables follow inverse Gamma distributions. We refer to this approximation as the factorized approximation.\nParameter-tied factorized approximation. The conditional variational distribution on wkl implied by Equations 5 and 7 is q(wkl | τkl, υl) = N (wkl | τklυlµkl, (τklυl)2Ψ), where Ψ is a diagonal matrix with elements populated by σ2ij,l and µkl consists of the corresponding variational means µij,l. The distributions of weights incident into a unit are thus coupled through τklυl while all weights\nin a layer are coupled through the layer wise scale υl. This view suggests that using a simpler approximating family q(βij,l | ϕβijl) = N (βij,l | µij,l, 1) results in an isotropic Gaussian approximation q(wkl | τkl, υl) = N (wkl | τklυlµkl, (τklυl)2I). Crucially, the scale parameters τklυl still allow for pruning of units when the scales approach zero. Moreover, by tying the variances of the noncentered weights together this approximation effectively halves the number of variational parameters and speeds up training (Ghosh & Doshi-Velez, 2017). We call this the tied-factorized approximation.\nStructured Variational Approximations Although computationally convenient, the factorized approximations fail to capture posterior correlations among the network weights, and more pertinently, between weights and scales.\nWe take a step towards a more structured variational approximation by using a layer-wise matrix variate Gaussian variational distribution for the non-centered weights and retaining the form of all the other factors from Equation 5. Let βl ∈ RKl−1+1×Kl denote the set of weights betweens layers l − 1 and l, then under this variational approximation we have q(βl | ϕβl) = MN (βl | Mβl , Uβl , Vβl), where Mβl ∈ RKl−1+1×Kl is the mean, Vβl ∈ RKl×Kl and Uβl ∈ RKl−1+1×Kl−1+1 capture the covariances among the columns and rows of βl, thereby modeling dependencies among the variational approximation to the weights in a layer. Louizos & Welling (2016) demonstrated that even when Uβl and Vβl are restricted to be diagonal, the matrix Gaussian approximation can lead to significant improvements over fully factorized approximations for vanilla BNNs. We call this the semi-structured1 approximation.\nThe horseshoe prior exhibits strong correlations between weights and their scales, which encourages strong posterior coupling between βkl and τkl. For effective shrinkage towards zero, it is important that the variational approximations are able to capture this strong dependence.\nTo do so, let Bl = [\nβl νTl\n] , νl = [ν1l, . . . , νKll] T , and\nνkl = lnτkl. Now using the variational approximation q(Bl | ϕBl) = MN (Bl | Ml, Ul, Vl), allows us to retain the coupling between weights incident into a unit and the corresponding unit specific scales, with appropriate parameterizations of Ul. In particular, we note that a diagonal Ul fails to capture the necessary correlations, and defeats the purpose of using a matrix Gaussian variational family to model the posterior of Bl. To retain computational efficiency while capturing dependencies among the rows of Bl we enforce a low-rank structure, Ul = Ψl + hlhTl , where Ψl ∈ RKl−1+2×Kl−1+2 is a diagonal matrix and\n1it captures correlations among weights but not between weights and scales\nhl ∈ RKl−1+2×1 is a column vector. We retain a diagonal structure for Vl ∈ RKl×Kl . We call this approximation the structured approximation. In the experimental section, we find that this structured approximation, indeed leads to stronger shrinkage towards zero in the recovered solutions. When combined with a pruning rule, it significantly compresses networks with excess capacity. Table 1 summarizes the variational approximations introduced in this section."
  }, {
    "heading": "4.2. Black Box Variational Inference",
    "text": "Irrespective of the variational family choice, the resulting evidence lower bound (ELBO),\nL(ϕ) = ∑ n E[ln p(yn | f(β, T , κ, xn))]+\nE[ln p(T , β, κ, ρκ | b0, bg, bκ)] +H[q(θ | ϕ)], (6)\nis challenging to evaluate. Here we have used β to denote the set of all non-centered weights in the network. The nonlinearities introduced by the neural network and the potential lack of conjugacy between the neural network parameterized likelihoods and the Horseshoe priors render the first expectation in Equation 6 intractable.\nRecent progress in black box variational inference (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2014; Titsias & Lázaro-gredilla, 2014) subverts this difficulty. These techniques compute noisy unbiased estimates of the gradient ∇ϕL̂(ϕ), by approximating the offending expectations with unbiased Monte-Carlo estimates and relying on either score function estimators (Williams, 1992; Ranganath et al., 2014) or reparameterization gradients (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lázaro-gredilla, 2014) to differentiate through the sampling process. With the unbiased gradients in hand, stochastic gradient ascent can be used to optimize the ELBO. In practice, reparameterization gradients exhibit significantly lower variances than their score function counterparts and are typically favored for differentiable models. The reparameterization gradients rely on the existence of a parameterization that separates the source of randomness from the parameters with respect to which the gradients are sought. For our Gaussian variational approximations, the well known non-centered parameterization, ζ ∼ N (µ, σ2) ⇔ ϵ ∼ N (0, 1), ζ = µ + σϵ, allows us\nto compute Monte-Carlo gradients,\n∇µ,σEqw [g(w)] ⇔ ∇µ,σEN (ϵ|0,1)[g(µ+ σϵ)]\n≈ 1 S ∑ s ∇µ,σg(µ+ σϵ(s)), (7)\nfor any differentiable function g and ϵ(s) ∼ N (0, 1). Furthermore, all practical implementations of variational Bayesian neural networks use a further re-parameterization to lower variance of the gradient estimator. They sample from the implied variational distribution over a layer’s preactivations instead of directly sampling the much higher dimensional weights (Kingma et al., 2015).\nVariational distribution on pre-activations The “local” re-parametrization is straightforward for all the approximations except the structured approximation. For that, observe that q(Bl | ϕBl) factorizes as q(βl | νl, ϕβl)q(νl | ϕνl). Moreover, conditioned on νl ∼ q(νl | ϕνl), βl follows another matrix Gaussian distribution. The conditional variational distribution is q(βl | νl, ϕβl) = MN (Mβl|νl , Uβl|νl , V ). It then follows that b = βTl a for an input a ∈ RKl−1+1×1 into layer l, is distributed as,\nb | a, νl, ϕβl ∼ N (b | µb,Σb), (8)\nwith µb = MTβl|νla, and Σb = (a TUβl|νla)V . Since, aTUβl|νla is scalar and V is diagonal, Σ is diagonal as well. For regularized HS-BNN, recall that the pre-activation of node k in layer l, is ukl = τ̃klυlb, and the corresponding variational posterior is,\nq(ukl | µukl , σ 2 ukl) = N (ukl | µukl , σ 2 ukl),\nµukl = τ̃ (s) kl υ (s) l µbk; σ 2 ukl = τ̃\n(s)2 kl υ (s) l 2 Σbk,k,\n(9)\nwhere τ (s)kl , υ (s) l , c (s) are samples from the corresponding log-Normal posteriors and τ̃ (s)kl is constructed as c(s) 2 τ (s) kl 2 /(c(s) 2 + τ (s) kl 2 υ (s) l 2 ).\nAlgorithm We now have a simple prescription for optimizing Equation 6. Recursively sampling the variational posterior of Equation 9 for each layer of the network, allows us to forward propagate information through the network. Using the reparameterizations (Equation 7), allows us to differentiate through the sampling process. We compute the necessary gradients through reverse mode automatic differentiation tools (Maclaurin et al., 2015). With the gradients in hand, we optimize L(ϕ) with respect to the variational weights ϕB , per-unit scales ϕτkl , per-layer scales ϕυl , and the variational scale for the output layer weights, ϕκ using Adam (Kingma & Ba, 2014). Conditioned on these, the optimal variational posteriors of the auxiliary variables ϑl, λkl, and ρκ follow Inverse Gamma distributions. Fixed point updates that maximize L(ϕ) with\nrespect to ϕϑl , ϕλkl , ϕρκ , holding the other variational parameters fixed are available. It can be shown that, q(λkl | ϕλkl) = Inv-Gamma(λkl | 1,E[ 1τkl ] + 1 b20 ). The distributions of the other auxiliary variables are analogous. By alternating between gradient and fixed point updates to maximize the ELBO in a coordinate ascent fashion we learn all variational parameters jointly (see Algorithm 1 of the supplement). Further details are available in the supplement.\nComputational Considerations The primary computational bottleneck for the structured approximation arises in computing the pre-activations in equation 8. While computing Σb in the factorized approximation involves a single inner product, in the structured case it requires the computation of the quadratic form aTUMβl|νla and a point wise multiplication with the elements of Vl. Owing to the diagonal plus rank-one structure of UMβl|νl , we only need two inner products, followed by a scalar squaring and addition to compute the quadratic form and Kl scalar multiplications for the point-wise multiplication with Vl. Thus the structured approximation is only marginally more expensive. Further, it uses only Kl + 2× (Kl−1 + 1) weight variance parameters per layer, instead of Kl × (Kl−1 + 1) parameters used by the factorized approximation. Not having to compute gradients and update these additional parameters further mitigates the performance difference."
  }, {
    "heading": "4.3. Pruning Rule",
    "text": "The Horseshoe and its regularized variant provide strong shrinkage towards zero for small wkl. However, the shrunk weights, although tiny, are never actually zero. A userdefined thresholding rule is required to prune away the shrunk weights. One could first summarize the inferred posterior distributions using a point estimate and then use the summary to define a thresholding rule (Louizos et al., 2017). We propose an alternate thresholding rule that obviates the need for a point summary. We prune away a unit, if p(τklυl < δ) > p0, where δ and p0 are user defined parameters, with τkl ∼ q(τkl | ϕτkl) and υl ∼ q(υl | ϕυl). Since, both τkl and υl are constrained to the log-Normal variational family, their product follows another log-Normal distribution, and implementing the thresholding rule simply amounts to computing the cumulative distribution function of the log-Normal distribution. To see why this rule is sensible, recall that for units which experience strong shrinkage the regularized Horseshoe tends to the Horseshoe. Under the Horseshoe prior, τklυl governs the (non-negative) scale of the weight node vector wkl. Therefore, under our thresholding rule, we prune away nodes whose posterior scales, place probability greater than p0 below a sufficiently small threshold δ. In our experiments, we set p0 = 0.9 and δ to either 10−3 or 10−5."
  }, {
    "heading": "5. Related Work",
    "text": "Bayesian neural networks have a long history. Early work can be traced back to (Buntine & Weigend, 1991; MacKay, 1992; Neal, 1993). These early approaches do not scale well to modern architectures or the large datasets required to learn them. Recent advances in stochastic MCMC methods (Li et al., 2016; Welling & Teh, 2011) and stochastic variational methods (Blundell et al., 2015; Rezende et al., 2014), black-box variational and alphadivergence minimization (Hernandez-Lobato et al., 2016; Ranganath et al., 2014), and probabilistic backpropagation (Hernández-Lobato & Adams, 2015) have reinvigorated interest in BNNs by allowing scalable inference.\nWork on learning structure in BNNs has received less attention. (Blundell et al., 2015) introduce a mixture-ofGaussians prior on the weights, with one mixture tightly concentrated around zero, thus approximating a spike and slab prior over weights. Others (Kingma et al., 2015; Gal & Ghahramani, 2016) have noticed connections between Dropout (Srivastava et al., 2014) and approximate variational inference. In particular, (Molchanov et al., 2017) show that the interpretation of Gaussian dropout as performing variational inference in a network with log uniform priors over weights leads to sparsity in weights. The goal of turning off edges is very different than the approach considered here, which performs model selection over the appropriate number of nodes. More closely related to us, are the recent works of (Ghosh & Doshi-Velez, 2017) and (Louizos et al., 2017). The authors consider group Horseshoe priors for unit pruning. We improve upon these works by using regularized Horseshoe priors that improve generalization, structured variational approximations that provide more accurate inferences, and by proposing a new thresholding rule to prune away units with small scales. Yet others (Neklyudov et al., 2017) have proposed pruning units via truncated log-normal priors over unit scales. However, they do not place priors over network weights and are unable to infer posterior weight uncertainty. In related but orthogonal research (Adams et al., 2010; Song et al., 2017) focused on the problem of structure learning in deep belief networks.\nThere is also a body of work on learning structure in nonBayesian neural networks. Early work (LeCun et al., 1990; Hassibi et al., 1993) pruned networks by analyzing secondorder derivatives of the objectives. More recently, (Wen et al., 2016) describe applications of structured sparsity not only for optimizing filters and layers but also computation time. Closer to our work in spirit, (Ochiai et al., 2016; Scardapane et al., 2017; Alvarez & Salzmann, 2016) and (Murray & Chiang, 2015) who use group sparsity to prune groups of weights—e.g. weights incident to a node. However, these approaches don’t model weight uncertainty and provide uniform shrinkage to all weights."
  }, {
    "heading": "6. Experiments",
    "text": "In this section, we present experiments that evaluate various aspects of the proposed regularized Horseshoe Bayesian neural network (reg-HS) and the structured variational approximation. In all experiments, we use a learning rate of 0.005, the global horseshoe scale bg = 10−5, a batch size of 128, ca = 2, and cb = 6. For the structured approximation, we also found that constraining Ψ, V , and h to unit-norms resulted in better predictive performance. Additional experimental details are in the supplement.\nRegularized Horseshoe Priors provide consistent benefits, especially on smaller data sets. We begin by comparing reg-HS against BNNs using the standard Horseshoe (HS) prior on a collection of diverse datasets from the UCI repository. We follow the protocol of (Hernández-Lobato & Adams, 2015) to compare the two models. To provide a controlled comparison, and to tease apart the effects of model versus inference enhancements we employ factorized variational approximations for either model. In figure 2, the UCI datasets are sorted from left to right, with the smallest on the left. We find that the regularized Horseshoe leads to consistent improvements in predictive performance. As expected, the gains are more prominent for the smaller datasets for which the regularization afforded by the regularized Horseshoe is crucial for avoiding overfitting. In the remainder, all reported experimental results use the reg-HS prior.\nStructured variational approximations provide greater shrinkage. Next, we evaluate the effect of utilizing structured variational approximations. In preliminary experiments, we found that of the approximations described in Section 4.1, the structured approximation outperformed the semi-structured variant while the factorized approximation provided better predictive performance than the tied approximation. In this section we only report results comparing models employing these two variational families.\nToy Data First, we explore the effects of structured and factorized variational approximations on predictive uncertainties. Following (Ghosh & Doshi-Velez, 2017) we consider a noisy regression problem: y = sin(x)+ϵ, ϵ ∼ N (0, 0.1), and explore the relationship between predictive uncertainty and model capacity. We compare a single layer 1000 unit BNN using a standard normal prior against BNNs with the regularized horseshoe prior utilizing factorized and structured variational approximations. Figures 1 and 3 show that while a BNN severely over-estimates the predictive uncertainty, models using the reg-HS priors by pruning away excess capacity, significantly improve the estimated uncertainty. Furthermore, we observe that the structured approximation best alleviates the under-fitting issues.\nControlled comparisons on UCI benchmarks We return to\nthe UCI benchmark to carefully vet the different variational approximations. We deviate from prior work, by using networks with significantly more capacity than previously considered for this benchmark. In particular, we use single layer networks with an order of magnitude more hidden units (500) than considered in previous work (50). This additional capacity is more than that needed to explain the UCI benchmark datasets well. With this experimental setup, we are able to evaluate how well the proposed methods perform at pruning away extra modeling capacity. For all but the ‘year‘ dataset, we report results from five trials each trained on a random 90/10 split of the data. For the large year dataset, we ran a single trial (details in the supplement). Figure 2 shows consistently stronger shrinkage.\nComparison against Factorized approximations. The factorized and structured variational approximations have similar predictive performance. However, the structured approximation consistently recovers solutions that exhibit\nmuch stronger shrinkage towards zero. Figure 2 demonstrates this effect on several UCI datasets, with more in the supplement. We have plotted 50 units with the smallest ||wkl||2 weight norms recovered by the factorized and structured approximations, from five random trials. Both approximations provide shrinkage towards zero, but the structured approximation has significantly stronger shrinkage. Further, the degree of shrinkage from the factorized approximation varies significantly between random initializations. In contrast, the structured approximation consistently provides strong shrinkage. We compare the shrinkages using ||E[wkl]||2 instead of applying the pruning rule from section 4.3 and comparing the resulting compression rates. This is because although the scales τklυl inferred by the factorized approximation provide a clear separation between signal and noise, they do not exhibit shrinkage toward zero. However, wkl = τklυlβkl does exhibit shrinkage and provides a fair comparison.\nComparison against competing methods. We compare the reg-HS model with structured variational approximation against the variational matrix Gaussian (VMG) approach of (Louizos & Welling, 2016), which has previously been shown to outperform other variational approaches to learning BNNs. We used the pruning rule with δ = 10−3 for all but the ‘year‘ dataset, for which we set δ = 10−5. Figure 2 demonstrates that structured reg-HS is competitive with VMG in terms of predictive performance. We either perform similarly or better than VMG on the majority of the datasets. More interestingly, structured reg-HS achieves competitive performance while pruning away excess capacity and achieving significant compression. We also fine-tuned the pruned model by updating the weight means while holding others fixed. However, this didn’t significantly affect predictive performance. Finally, we evaluate how reg-HS compares against VMG in the low data regime. For the three smallest UCI datasets we use ten percent of the data for training. In such limited data regimes (Figure 2) the shrinkage afforded by reg-HS leads to clear improvements in predictive performance over VMG.\nHS-BNNs improve reinforcement learning performance. So far, we have focused on using BNNs simply for prediction. One application area in which having good predictive uncertainty estimates is crucial is in model-based reinforcement learning scenarios (e.g. (Depeweg et al., 2017; Gal et al., 2016b; Killian et al., 2017)): here, it is essential not only to have an estimate of what state an agent may be in after taking a particular action, but also an accurate sense of all the states the agent may end up in. In the following, we apply our regularized HS-BNN with structured approximations to two domains: the 2D map of Killian et al. (2017) and acrobot Sutton & Barto (1998). For each domain, we focused on one instance dynamic setting. In each domain, we collected training samples by training\na DDQN (van Hasselt et al., 2016) online (updated every episode). The DDQN was trained with an epsilon-greedy policy that started at one and decayed to 0.15 with decay rate 0.99, for 500 episodes. This procedure ensured that we had a wide variety of samples that were still biased in coverage toward the optimal policy. To simulate resource constrained scenarios, we limited ourselves to 10% of DDQN training batches (346 samples for the 2D map and 822 training samples for acrobot). We considered two architectures, a single hidden layer network with 500 units, and a two layer network with 100 units per layer as the transition function for each domain. Then we simulated from each BNN to learn a DDQN policy (two layers of width 256, 512; learning rate 5e− 4) and tested this policy on the original simulator.\nAs in our prediction results, training a moderately-sized BNN with so few data results in severe underfitting, which in turn, adversely affects the quality of the policy that is learned. We see in table 2 that the better fitting of the structured reg-HS-BNN results in higher task performance, across domains and model architectures."
  }, {
    "heading": "7. Discussion and Conclusion",
    "text": "We demonstrated that the regularized horseshoe prior, combined with a structured variational distribution, is a computationally efficient tool for model selection in Bayesian neural networks. By retaining crucial posterior dependencies, the structured approximation provided, to our knowledge, state of the art shrinkage for BNNs while being competitive in predictive performance to existing approaches. We found, model re-parameterizations — decomposition of the Half-Cauchy priors into inverse gamma distributions and non-centered representations essential for avoiding poor local optima. There remain several interesting follow-on directions, including, modeling enhancements that use layer, node, or even weight specific weight decay c, or layer specific global shrinkage parameter bg to provide different levels of shrinkage to different parts of the BNN."
  }],
  "year": 2018,
  "references": [{
    "title": "Learning the structure of deep sparse graphical models",
    "authors": ["R.P. Adams", "H.M. Wallach", "Z. Ghahramani"],
    "venue": "In AISTATS,",
    "year": 2010
  }, {
    "title": "Learning the number of neurons in deep networks",
    "authors": ["J.M. Alvarez", "M. Salzmann"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Hamiltonian monte carlo for hierarchical models",
    "authors": ["M. Betancourt", "M. Girolami"],
    "venue": "Current trends in Bayesian methodology with applications,",
    "year": 2015
  }, {
    "title": "Weight uncertainty in neural networks",
    "authors": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "Handling sparsity via the horseshoe",
    "authors": ["C.M. Carvalho", "N.G. Polson", "J.G. Scott"],
    "venue": "In AISTATS,",
    "year": 2009
  }, {
    "title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks",
    "authors": ["S. Depeweg", "J.M. Hernández-Lobato", "F. Doshi-Velez", "S. Udluft"],
    "year": 2017
  }, {
    "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
    "authors": ["Y. Gal", "Z. Ghahramani"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Deep Bayesian active learning with image data",
    "authors": ["Y. Gal", "R. Islam", "Z. Ghahramani"],
    "venue": "In Bayesian Deep Learning workshop,",
    "year": 2016
  }, {
    "title": "Improving pilco with bayesian neural network dynamics models",
    "authors": ["Y. Gal", "R. McAllister", "C.E. Rasmussen"],
    "venue": "In Data-Efficient Machine Learning workshop,",
    "year": 2016
  }, {
    "title": "Model selection in bayesian neural networks via horseshoe priors",
    "authors": ["S. Ghosh", "F. Doshi-Velez"],
    "venue": "NIPS Workshop on Bayesian Deep Learning,",
    "year": 2017
  }, {
    "title": "Optimal brain surgeon and general network pruning",
    "authors": ["B. Hassibi", "D.G. Stork", "G.J. Wolff"],
    "venue": "In Neural Networks,",
    "year": 1993
  }, {
    "title": "Black-box alpha divergence minimization",
    "authors": ["J. Hernandez-Lobato", "Y. Li", "M. Rowland", "T. Bui", "D. Hernández-Lobato", "R. Turner"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Probabilistic backpropagation for scalable learning of bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R.P. Adams"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Bayesian sparsity for intractable distributions",
    "authors": ["J.B. Ingraham", "D.S. Marks"],
    "year": 2016
  }, {
    "title": "Robust and efficient transfer learning with hidden parameter markov decision processes",
    "authors": ["T.W. Killian", "S. Daulton", "F. Doshi-Velez", "G. Konidaris"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "year": 2014
  }, {
    "title": "Stochastic gradient VB and the variational auto-encoder",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Variational dropout and the local reparameterization trick",
    "authors": ["D.P. Kingma", "T. Salimans", "M. Welling"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Optimal brain damage",
    "authors": ["Y. LeCun", "J.S. Denker", "S.A. Solla"],
    "venue": "In NIPS, pp",
    "year": 1990
  }, {
    "title": "Preconditioned stochastic gradient langevin dynamics for deep neural networks. 2016",
    "authors": ["C. Li", "C. Chen", "D.E. Carlson", "L. Carin"],
    "year": 2016
  }, {
    "title": "Structured and efficient variational deep learning with matrix Gaussian posteriors",
    "authors": ["C. Louizos", "M. Welling"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Bayesian compression for deep learning",
    "authors": ["C. Louizos", "K. Ullrich", "M. Welling"],
    "year": 2017
  }, {
    "title": "A practical Bayesian framework for backpropagation networks",
    "authors": ["D.J. MacKay"],
    "venue": "Neural computation,",
    "year": 1992
  }, {
    "title": "Autograd: Effortless gradients in numpy",
    "authors": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"],
    "venue": "In ICML AutoML Workshop,",
    "year": 2015
  }, {
    "title": "Variational dropout sparsifies deep neural networks",
    "authors": ["D. Molchanov", "A. Ashukha", "D. Vetrov"],
    "year": 2017
  }, {
    "title": "A note on the evidence and bayesian occam’s razor",
    "authors": ["I. Murray", "Z. Ghahramani"],
    "year": 2005
  }, {
    "title": "Auto-sizing neural networks: With applications to n-gram language models",
    "authors": ["K. Murray", "D. Chiang"],
    "year": 2015
  }, {
    "title": "Bayesian learning via stochastic dynamics",
    "authors": ["R.M. Neal"],
    "venue": "In NIPS,",
    "year": 1993
  }, {
    "title": "Structured bayesian pruning via log-normal multiplicative noise",
    "authors": ["K. Neklyudov", "D. Molchanov", "A. Ashukha", "D.P. Vetrov"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Automatic node selection for deep neural networks using group lasso regularization",
    "authors": ["T. Ochiai", "S. Matsuda", "H. Watanabe", "S. Katagiri"],
    "year": 2016
  }, {
    "title": "On the hyperprior choice for the global shrinkage parameter in the horseshoe prior",
    "authors": ["J. Piironen", "A. Vehtari"],
    "year": 2017
  }, {
    "title": "Black box variational inference",
    "authors": ["R. Ranganath", "S. Gerrish", "D.M. Blei"],
    "venue": "In AISTATS, pp",
    "year": 2014
  }, {
    "title": "Occam’s razor",
    "authors": ["C.E. Rasmussen", "Z. Ghahramani"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2001
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "In Proceedings of The 31st International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Group sparse regularization for deep neural networks",
    "authors": ["S. Scardapane", "D. Comminiello", "A. Hussain", "A. Uncini"],
    "year": 2017
  }, {
    "title": "Scalable model selection for belief networks",
    "authors": ["Z. Song", "Y. Muraoka", "R. Fujimaki", "L. Carin"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"],
    "year": 1929
  }, {
    "title": "Reinforcement learning: An introduction, volume 1",
    "authors": ["R.S. Sutton", "A.G. Barto"],
    "venue": "MIT press Cambridge,",
    "year": 1998
  }, {
    "title": "Doubly stochastic variational Bayes for non-conjugate inference",
    "authors": ["M. Titsias", "M. Lázaro-gredilla"],
    "venue": "In ICML, pp. 1971–1979,",
    "year": 2014
  }, {
    "title": "Deep reinforcement learning with double q-learning",
    "authors": ["H. van Hasselt", "A. Guez", "D. Silver"],
    "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Mean field variational Bayes for elaborate distributions",
    "authors": ["M.P. Wand", "J.T. Ormerod", "S.A. Padoan", "R Fuhrwirth"],
    "venue": "Bayesian Analysis,",
    "year": 2011
  }, {
    "title": "Bayesian learning via stochastic gradient langevin dynamics",
    "authors": ["M. Welling", "Y.W. Teh"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning",
    "year": 2011
  }, {
    "title": "Learning structured sparsity in deep neural networks",
    "authors": ["W. Wen", "C. Wu", "Y. Wang", "Y. Chen", "H. Li"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["R.J. Williams"],
    "venue": "Machine learning,",
    "year": 1992
  }],
  "id": "SP:d57a82012f732863a8fdca17fc40c2e00bd2b14c",
  "authors": [{
    "name": "Soumya Ghosh",
    "affiliations": []
  }, {
    "name": "Jiayu Yao",
    "affiliations": []
  }, {
    "name": "Finale Doshi-Velez",
    "affiliations": []
  }],
  "abstractText": "Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection—even choosing the number of nodes—remains an open question. Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data. In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smallersample settings including reinforcement learning.",
  "title": "Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors"
}