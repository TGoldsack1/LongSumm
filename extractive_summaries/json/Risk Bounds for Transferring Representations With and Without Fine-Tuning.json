{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A widely used machine learning technique is the transfer of a representation learned from a source task, for which labeled data is abundant, to a target task, for which labeled data is scarce. This may be effective if the tasks approximately share an intermediate representation. For example:\n• features learned from an image of a human face to predict age may also be useful for predicting gender\n• word embeddings learned to predict word contexts may also be useful for part of speech tagging\n• features learned from financial data to predict loan default may also be useful for predicting insurance fraud.\nOften a representation is learned by a different organization that may have greater access to data, computational and human resources. Examples are the Google word2vec package (Mikolov et al., 2013), and downloadable pre-trained\n1The Australian National University and Data61, Canberra, ACT, Australia 2Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence to: Daniel McNamara <daniel.mcnamara@anu.edu.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nneural networks.1 Under this ‘representation-as-a-service’ model, a user may expect to access the representation itself, as well as information about its performance on the source task data on which it was trained. We aim to convert this into a guarantee of the usefulness of the representation on other tasks, which is known in advance without the effort or cost of testing the representation on the target task(s). Our analysis also covers the case where the source task is constructed from unlabeled data, as in neural network unsupervised pre-training.\nWe consider two approaches to transferring a representation learned from a source task to a target task, as shown in Figure 1. We may either treat the representation as fixed, or we may narrow the class of representations considered on the target task, which we refer to as fine-tuning. The fixed option may be attractive when very little labeled target task data is available and hence overfitting is a strong concern, while the advantage of fine-tuning is relatively greater hypothesis class expressiveness.\nLet X,Y and Z be sets known as the input, output and feature spaces respectively. Let F be a class of representations, where f : X → Z for f ∈ F . Let G be a class of specialized classifiers, where g : Z → Y for g ∈ G. Let the hypothesis class H := {h : ∃f ∈ F, g ∈ G such that h = g ◦ f}. Let hS , hT : X → Y be the labeling functions and PS , PT be the input distributions for source task S and target task T respectively. We consider the setting Y = {−1, 1}. Let the risk of a hypothesis h on S and T be RS(h) := Ex∼PS [hS(x) 6= h(x)] and RT (h) := Ex∼PT [hT (x) 6= h(x)] respectively. Let R̂S(h) and R̂T (h) be the corresponding empirical (i.e. training set) risks. We have mS labelled points for S and mT labelled points for T . Let dH be the VC dimension of H .\nThe remainder of the paper is structured as follows. In Section 2 we introduce related work. In Sections 3 and 4 we analyze the cases where the transferred representation is fixed and fine-tuned respectively. In Section 5 we apply the results and use them to motivate and test a practical approach to weight transfer in neural networks. We conclude in Section 6 and defer more involved proofs to Section 7.\n1See http://code.google.com/archive/p/ word2vec, http://caffe.berkeleyvision.org/ model_zoo and http://vlfeat.org/matconvnet/ pretrained for examples."
  }, {
    "heading": "2. Background",
    "text": "Empirical studies have shown the success of transferring representations between tasks (Donahue et al., 2014; Hoffman et al., 2014; Girshick et al., 2014; Socher et al., 2013; Bansal et al., 2014). Word embeddings learned on a source task have been shown (Qu et al., 2015) to perform better than unigram features on target tasks such as part of speech tagging, and comparably or better than embeddings finetuned on the target task. Yosinski et al. (2014) learned neural network weights using half of the ImageNet classes, and then learned the other classes with a neural network initialized with these weights, finding a benefit compared to random initialization only with target task fine-tuning. The transfer of representations, both with and without finetuning, is widely and successfully used.\nPrevious work on domain adaptation (Ben-David et al., 2010; Mansour et al., 2009; Germain et al., 2013) has considered learning a hypothesis h on S and re-using it on T , bounding RT (h) using RS(h) (measured with labeled source data) and some notion of similarity between PS and PT (measured with additional unlabeled target data). Such results motivate a joint optimization using labeled source and unlabeled target data (Ganin et al., 2016; Long et al., 2015) to learn separate mappings fS , fT : X → Z which make the induced distributions in the feature space Z similar, and a hypothesis g : Z → Y learned from the source labels which can be re-used on T . This approach assumes the tasks become the same if their input distributions can be aligned. We consider a relaxation where the tasks are more weakly related but some representation step can be transferred. We consider learning f : X → Z on S, re-using it on T , and then learning gT : Z → Y from a small amount of labeled target data. Given the widespread use of ‘downloadable’ representations, where f and gT are learned separately and there is no joint optimization over source and target data, this is a realistic setting.\nWork on lifelong learning relates the past performance of a representation over many tasks to its expected future performance. For a representation f ∈ F we construct G ◦ f := {g ◦ f : g ∈ G}. Suppose there is a distribution over tasks, known as an environment. Assume several tasks from this environment have been sampled, and that for each task some hypothesis in G ◦ f has been selected and its empirical risk evaluated. Previous work has provided bounds on the difference between the average empirical risk and the expected risk of the best hypothesis in G ◦ f for a new task drawn from the environment. Such bounds have been given by measuring the complexity of F and G using covering numbers (Baxter, 2000), a variant of the growth function (Galanti et al., 2016), and a distribution-dependent measure known as Gaussian complexity (Maurer et al., 2016). All of these bounds rely on\nknown past performance on a large number of tasks.2 In practice, however, representations such as neural network weights or word embeddings are often learned using only a single source task, which is the setting we consider."
  }, {
    "heading": "3. Representation Fixed by Source Task",
    "text": "Suppose labeled source data is abundant, labeled target data is scarce, and we believe the tasks share a representation. A natural approach to leveraging the source data is to learn ĝS ◦ f̂ ∈ H on S, from which we assume we may extract f̂ ∈ F ,3 then conduct empirical risk minimization over G ◦ f̂ := {g ◦ f̂ : g ∈ G} on T yielding ĝT ◦ f̂ . Theorem 1 upper-bounds RT (ĝT ◦ f̂) using four terms: a function ω measuring a transferrability property obtained analytically from the problem setting, the empirical risk R̂S(ĝS ◦ f̂), the generalization error of a hypothesis in H learned from mS samples, and the generalization error of a hypothesis in G learned from mT samples. The value of the theorem is that if ω(R) = O(R), R̂S(ĝS ◦ f̂) is a small constant, mS mT and dH dG,4 we improve on the VC dimension-based bound for learning T from scratch by avoiding the generalization error of a hypothesis in H learned from mT samples. Furthermore, we do not settle for bounding RT (ĝT ◦ f̂) in terms of R̂T (ĝT ◦ f̂), which may be large. The theorem can be used to select S given\n2Pentina & Lampert (2014) extend this analysis to stochastic hypotheses (i.e. distributions over deterministic hypotheses), where for each task we learn a posterior given a prior and training data. The quality of the prior affects the learner’s performance. The study proposes using source tasks to learn a ‘hyperposterior’, a distribution over priors which is sampled to give a prior for each task. Such a hyperposterior may focus the learner on a representation shared across tasks. The study gives a PAC-Bayes bound on the expected risk of using a hyperposterior to learn a new task drawn from the environment, in terms of the average empirical risk obtained using the hyperposterior to learn the source tasks.\n3This is not possible with knowledge of ĝS ◦ f̂ alone, but in the case of feedforward neural networks which we focus on, f̂ is known if the weights learned on S are known.\n4We have mS mT if labeled source task data is abundant while labeled target task data is scarce, and dH dG if we simplify target task learning by substantially reducing the hypothesis space to be searched.\nseveral options. While we refer to ω in a general form, we give an example in Section 3.1 and expect that others exist.5\nTheorem 1. Let ω : R → R be a non-decreasing function. Suppose PS , PT , hS , hT , f̂ , G have the property that ∀ĝS ∈ G, min\ng∈G RT (g ◦ f̂) ≤ ω(RS(ĝS ◦ f̂)). Let ĝT :=\narg min g∈G\nR̂T (g ◦ f̂). Then with probability at least 1 − δ\nover pairs of training sets for tasks S and T , RT (ĝT ◦ f̂) ≤ ω(R̂S(ĝS ◦ f̂) + 2\n√ 2dH log(2emS/dH)+2 log(8/δ)\nmS ) + 4 √\n2dG log(2emT /dG)+2 log(8/δ) mT .\nProof. Let g∗T := arg min g∈G RT (g ◦ f̂). With probability at least 1− δ,\nRT (ĝT ◦ f̂) ≤ R̂T (ĝT ◦ f̂) + 2 √\n2dG log(2emT /dG)+2 log(8/δ) mT\n≤ R̂T (g∗T ◦ f̂) + 2 √ 2dG log(2emT /dG)+2 log(8/δ) mT\n≤ RT (g∗T ◦ f̂) + 4 √ 2dG log(2emT /dG)+2 log(8/δ) mT\n≤ ω(RS(ĝS ◦ f̂)) + 4 √\n2dG log(2emT /dG)+2 log(8/δ) mT\n≤ ω(R̂S(ĝS ◦ f̂) + 2 √\n2dH log(2emS/dH)+2 log(8/δ) mS ) + 4 √\n2dG log(2emT /dG)+2 log(8/δ) mT .\nUsing m training points and a hypothesis class of VC dimension d, with probability at least 1 − δ, for all hypotheses h simultaneously, the riskR(h) and empirical risk R̂(h)\nsatisfy |R(h)−R̂(h)| ≤ 2 √\n2d log(2em/d)+2 log(4/δ) m (Mohri\net al., 2012). ForG this yields the first and third inequalities with probability at least 1 − δ2 . For H , because ω is nondecreasing, this yields the fifth inequality with probability at least 1 − δ2 . Applying the union bound achieves the desired result. The second inequality is by the definition of ĝT and the fourth inequality follows from our assumption."
  }, {
    "heading": "3.1. Neural Network Example with Fixed Representation",
    "text": "In Theorem 2, we give an example of the property required by Theorem 1 which is specific to a particular problem setting. We consider a neural network with a single hidden layer (see Figure 2). We propose transferring the lowerlevel weights (corresponding to f̂ ) learned on S, so that only the upper-level weights (corresponding to G) have to be learned on T . We want to show f̂ is also useful for T .\n5We define ω by relating RS(ĝS ◦ f̂) to min g∈G RT (g ◦ f̂), since we expect this may be feasible analytically as in our example in Section 3.1. However, because we only observe R̂S(ĝS ◦ f̂), in Theorem 1 we use this to bound RS(ĝS ◦ f̂) and then apply ω.\nTo do this, we assume that some lower-level weights perform well on both tasks, which is clearly a necessary condition for the specific f̂ we are transferring to perform well on both tasks. We also assume PS and PT have the relative rotation invariance property and that the upper-level weights have fixed magnitude. This is so that a point x for which f̂(x) contributes to the risk on T cannot be ‘hidden’ from the risk of using f̂ on S, either through low PS(x) or low magnitude upper-level weights. Hence RS(ĝS ◦ f̂) reliably indicates the usefulness of f̂ on T .\nLetX = Rn and Z = Rk. Let F be the function class such that f(x) = [a(w1 · x), . . . , a(wk · x)], where wi ∈ Rn for 1 ≤ i ≤ k, a : R → R is an odd function6 and · is the dot product. Let G be the function class such that g(z) = sign(v · z), where v ∈ {−1, 1}k. Suppose ∃f ∈ F, gS , gT ∈ G such that max[RS(gS ◦f), RT (gT ◦f)] ≤ . Let f̂(x) := [a(ŵ1 · x), . . . , a(ŵk · x)]. Given wi and ŵi, pick nonzero constants αi and βi such that ||wi|| = ||αiŵi − βiwi|| and wi · (αiŵi − βiwi) = 0. Let M be a 2k×nmatrix with rowsw1, α1ŵ1−β1w1, . . . , wk, αkŵk− βkwk. Suppose M is full rank.7 Suppose ∀x, x′ such that ||Mx|| = ||Mx′||, PT (x) ≤ cPS(x′) for some c ≥ 1, which we call relative rotation invariance and implies PS and PT have the same support. If M is an orthogonal matrix then ∀x, x′ such that ||x|| = ||x′||, PT (x) ≤ cPS(x′).8\nTheorem 2. Let ω(R) := cR + (1 + c). Then ∀ĝS ∈ G, min g∈G\nRT (g ◦ f̂) ≤ ω(RS(ĝS ◦ f̂)). 6i.e. a(−x) = −a(x). Examples are tanh, sign and identity. 7To see that this condition is necessary, consider the following example where M is not full rank. Let n = 4, k = 2, hS = sign(x1) and hT = sign(x2). For f(x) = [x1 + x2, x1 − x2], gS(z) = sign(z1 + z2) and gT (z) = sign(z1 − z2), we have RS(gS ◦ f) = RT (gT ◦ f) = 0. On S we learn f̂(x) = [x1 + x3, x1−x3] and ĝS(z) = sign(z1 +z2), so thatRS(ĝS ◦ f̂) = 0 but in general min\ng∈G RT (g ◦ f̂) > 0 since f̂ ignores x2.\n8For example, PS and PT are spherical Gaussians. For a zeromean multivariate Gaussian distribution this is achieved by the whitening transformation x → Λ−1/2UTx, where the columns of U and entries of the diagonal matrix Λ are the eigenvectors and eigenvalues of the distribution’s covariance matrix respectively."
  }, {
    "heading": "4. Representation Fine-Tuned on Target Task",
    "text": "Consider learning ĝS ◦ f̂ on S, and then using f̂ and RS(ĝS ◦ f̂) to find F̂ ⊆ F , as in Figure 1. Let h̃g◦f be a stochastic hypothesis (i.e. a distribution over H) associated with g ◦ f (e.g. g ◦ f is the mode of h̃g◦f ). We propose learning T with the hypothesis class H̃G◦F̂ := {h̃g◦f : f ∈ F̂ , g ∈ G} and the prior h̃ĝS◦f̂ . Learning T from scratch we assume that we would instead use H̃G◦F := {h̃g◦f : f ∈ F, g ∈ G} and some fixed prior h̃0 ∈ H̃G◦F . Let RT (h̃) := Ex∼PT ,h∼h̃[hT (x) 6= h(x)] and compute R̂T (h̃) on the training set distribution of T .\nIn Theorem 3 we show that if F̂ is ‘small enough’ so that all h̃ ∈ H̃G◦F̂ have a small KL divergence from h̃ĝS◦f̂ , we may apply a PAC-Bayes bound to the generalization error of hypotheses in H̃G◦F̂ involving four terms: a function ω measuring a transferrability property, the empirical risk R̂S(ĝS ◦ f̂), the generalization error of a hypothesis in H learned from mS points, and a weak dependence on mT . The value of the theorem is that if ω(R) = O(R), R̂S(ĝS◦f̂) is a small constant, andmS mT , we improve on the PAC-Bayes bound for H̃G◦F and h̃0.9 F̂ is useful if it is also ‘large enough’ in the sense that ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ . Here ω quantifies how large the F̂ we search on T must be in order to be ‘large enough’, in terms of RS(ĝS ◦ f̂). While in general such an F̂ and ω may not exist, we give an example in Section 4.1.\nTheorem 3. Let ω : R → R be non-decreasing. Suppose given f̂ ∈ F and RS(ĝS ◦ f̂) estimated from S, it is possible to construct F̂ with the property ∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)). Then with probability at least 1 − δ over pairs of training sets for tasks S and T , ∀h̃ ∈ H̃G◦F̂ , RT (h̃) ≤ R̂T (h̃) +√\nω(R̂S(ĝS◦f̂)+2 √ 2dH log(2emS/dH )+2 log(8/δ)\nmS )+log 2mT /δ\n2(mT−1) .\nProof. With probability at least 1− δ,\nRT (h̃) ≤ R̂T (h̃) + √ KL(h̃||h̃ĝS◦f̂ )+log 2mT /δ 2(mT−1)\n≤ R̂T (h̃) + √\nω(RS(ĝS◦f̂))+log 2mT /δ 2(mT−1) .\nThe first inequality holds with probability at least 1 − δ2 (Shalev-Shwartz & Ben-David, 2014). The second inequality holds by assumption. Furthermore, RS(ĝS ◦ f̂) ≤ R̂S(ĝS ◦ f̂) + 2 √ 2dH log(2emS/dH)+2 log(8/δ)\nmS with prob-\nability at least 1 − δ2 (Mohri et al., 2012) and ω is nondecreasing. The result follows from the union bound.\n9Using the restricted deterministic hypothesis class G ◦ F̂ := {h : ∃f ∈ F̂ , g ∈ G such that h = g ◦ f} and a VC dimensionbased bound may not improve on H , since possibly dG◦F̂ = dH ."
  }, {
    "heading": "4.1. Neural Network Example with Fine-Tuning",
    "text": "We transfer and fine-tune weights in a feedforward neural network with one hidden layer to instantiate the property required by Theorem 3. We learn a deterministic hypothesis of this type on S and obtain k estimated lowerlevel weight vectors ŵi. Learning T we now consider only lower-level weights near ŵi, corresponding to F̂ . On T we learn a stochastic hypothesis formed by taking a deterministic network and adding independent sources of spherical Gaussian noise to the lower-level weights and sign-flipping noise to the upper-level weights. The KL divergence between two of the stochastic hypotheses is expressed using the angles between their lower-level weights10 and a quantity computable from their upper-level weights.\nWe want to prove that we can construct such an F̂ to successfully learn T . To do this, we assume some lower-level weights wi perform well on both S and T . We make F̂ ‘small enough’ by only including lower-level weights with small angles to ŵi, and ‘large enough’ by using the risk observed using ŵi on S to provide an upper bound on the angle between each pair wi and ŵi. Our assumptions ensure that poor ŵi cannot be ‘hidden’ from the risk on S, either through low PS density in the region of disagreement between wi and ŵi, or through low magnitude higher-level weights. Hence we know that searching F̂ will include wi.\nLet X = Rn and Z = Rk, where k is odd. Let F be the function class such that f(x) = [sign(w1 · x), . . . , sign(wk · x)], where wi ∈ Rn for 1 ≤ i ≤ k. Let G be the function class such that g(z) = sign(v · z), where v ∈ {−1, 1}k. Let Bv be a distribution on {−1, 1}k such that for\nv′ ∼ Bv , Pr(v′) = k∏ j=1 p1(v ′ j=vj)(1 − p)1(v ′ j=−vj), where p ∈ [0.5, 1]. Let h̃g◦f := g′ ◦f ′ such that v′, w′1, . . . , w′k ∼\nBv k∏ i=1 N (wi, σ2I). Suppose ∃f ∈ F, gS , gT ∈ G such that max[RS(gS ◦ f), RT (h̃gT ◦f )] ≤ . Let f̂(x) := [sign(ŵ1 · x), . . . , sign(ŵk · x)], θ(wi, ŵi) be the angle between wi and ŵi, and assume ∀i, ||ŵi|| = 1. Define M as in Section 3.1. Let PS have the rotation invariance property ∀x, x′ such that ||Mx|| = ||Mx′||, PS(x) ≤ cPS(x′) for some c ≥ 1.\nTheorem 4. Given f̂ and RS(ĝS ◦ f̂) estimated from S, let θmax := π √ 2(k − 1)c(RS(ĝS ◦ f̂) + ) and F̂ := {f ∈ F : ∀i, ||wi|| = 1 ∧ |θ(wi, ŵi)| ≤ θmax}. Let ω(R) := kσ2 [1−cos θmax]+k[2p−1+(1−p) k] log2 p 1−p . Then ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ and ∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)).\n10Assuming that the lower-level weight vectors are of fixed magnitude, which is no loss of model expressiveness since we use the sign activation function at the hidden layer."
  }, {
    "heading": "5. Applications",
    "text": "We show the utility of the risk bounds, and present a novel technique and experiments motivated by our theorems."
  }, {
    "heading": "5.1. Using the Risk Bounds",
    "text": "The results described yield tighter bounds on risk when transferring representations from S, compared to learning T from scratch. Examples are shown in Figure 3.11\nWe set δ = 0.05. For the top part, we use the example from Section 3.1 and set n = 10, k = 5. Learning T from scratch with H , we use the bound from Mohri et al. (2012) used previously. The VC dimension of a network of |E| edges using the sign activation is O(|E| log |E|) (Shalev-Shwartz & Ben-David, 2014), where in our case |E| = nk+k. We use dH = |E| log |E| in the chart. Transferring a representation from S to T without fine-tuning, we consider the limit → 0, R̂S(ĝS ◦ f̂) → 0, mS → ∞, and hence ω(·) → 0 by Theorem 2. Furthermore, dG ≤ k since G is finite and hence dG ≤ log2 |G| (Shalev-Shwartz & Ben-David, 2014). We use the bound from Theorem 1.\nFor the bottom part, we use the example from Section 4.1 and set σ2 = 110 , k = 499, p = 2 3 . Learning T from scratch we use the stochastic hypothesis class {h̃g◦f : f ∈ F such that ∀i||wi|| = 1, g ∈ G} and a prior h̃0 where ∀i wi = 0 and v ∈ {−1, 1}k is arbitrary.12 Hence we have the bound KL(h̃||h̃0) ≤ 10k + k3 , which becomes tight for large k. We apply the PAC-Bayes bound (ShalevShwartz & Ben-David, 2014) used previously. Transferring a representation from S and fine-tuning on T , we consider the limit → 0, R̂S(ĝS ◦ f̂) → 0, mS → ∞. We have KL(h̃||h̃ĝS◦f̂ ) ≤ k 3 by Theorem 4. We use the bound from Theorem 3."
  }, {
    "heading": "5.2. Fine-Tuning through Regularization",
    "text": "We relax the hard constraint on F̂ from Section 4.1 by using a modified loss function, which we find performs better in practice. Let yi and ŷi be the label and prediction respectively for the ith training point. In a fully-connected feedforward neural network with l layers of weights, let W (j) be the jth weight matrix, Ŵ (j) be its estimate from S (excluding weights for bias units in both cases), and ||·||2 be the entry-wise 2 norm. A typical loss function (1) used for training is composed of the sum of training set log loss and L2 regularization on the weights.\n11Note that VC dimension risk bounds are known for being rather loose, while PAC-Bayesian bounds are tighter and hence yield non-trivial results in higher dimensions with fewer samples.\n12This class is as expressive as H̃G◦F but by setting ||wi|| = 1 the KL divergence of all hypotheses from any prior is bounded, allowing a fair comparison to H̃G◦F̂ . The choice of h̃0 minimizes worst case KL divergence to a hypothesis in the class.\nm∑\ni=1 [−yi log ŷi − (1− yi) log(1− ŷi)] + λ 2\nl∑\nj=1 (||W (j)||22)\n(1)\nWe replace the regularization penalty with (2).13\nl∑ j=1 [ λ1(j) 2 ||W (j) − Ŵ (j)||22 + λ2(j) 2 ||W (j)||22] (2) This penalizes estimates of W far from the representation learned on S. Since we expect the tasks to share a lowlevel representation (e.g. edge detectors for vision, word embeddings for text) but be distinct at higher levels (e.g. image components for vision, topics for text), we set λ1(·) to be a decreasing function, while λ2(·) controls standard L2 regularization. The technique is novel to our knowledge, although other approaches to transferring regularization between tasks exist (Evgeniou & Pontil, 2004; Raina et al., 2006; Argyriou et al., 2008; Ghifary et al., 2014)."
  }, {
    "heading": "5.3. Experiments",
    "text": "We experiment on basic image and text classification tasks.14 We show that learning algorithms motivated by our theoretical results can help to overcome a scarcity of labeled target task data. Note that we do not replicate the conditions specified in our theorems, nor do we attempt extensive tuning to achieve state-of-the-art performance.\n13Basing our approach on (1), we follow the convention that weights connected to bias units are excluded from the regularization penalty. However, the inclusion of these weights in the ||W (j) − Ŵ (j)|| term of (2) is a plausible variant.\n14The MNIST and 20 Newgroups datasets are available at http://yann.lecun.com/exdb/mnist and http:// qwone.com/˜jason/20Newsgroups respectively.\nWe randomly partition label classes into sets S+ and S−, where |S+| = |S−|.15 We construct T+ by randomly picking from S+ up to γ :=\n|S+∩T+| |S+| , then randomly picking\nfrom S− such that |T+| = |T−|. We let S be the task of distinguishing between S+ and S− and T be that of distinguishing T+ and T−. Constructing S+ and T+ as disjunctions of classes means that the class labels are a perfect representation shared between S and T .\nWe compare the accuracy on T of four options:\n• learn T from scratch (BASE)\n• transfer f̂ from S, fine-tune f and train g on T using (2) (FINE-TUNE f̂ )\n• transfer f̂ from S and fix, train g on T (FIX f̂ )16\n• transfer ĝS ◦ f̂ from S and fix (FIX ĝS ◦ f̂ ).17\nWe use λ1(1) = λ2(2) = λ := 1,18 λ1(2) = λ2(1) = 0, mT = 500 and the sigmoid activation function. For MNIST we use raw pixel intensities, a 784 × 50 × 1 network andmS = 50000. For NEWSGROUPS we use TF-IDF weighted counts of most frequent words, a 2000 × 50 × 1 network and mS = 15000. We use conjugate gradient optimization with 200 iterations.\nThe results are shown in Table 1.19 When the tasks are nonidentical, FINE-TUNE f̂ is mostly the strongest but performs better on MNIST. FIX f̂ outperforms BASE when γ ≥ 0.8 and hence the tasks are similar. While FIX f̂ outperforms FIX ĝS ◦ f̂ when the tasks are non-identical on MNIST, on NEWSGROUPS there is no evidence of benefit. When the tasks are identical, FIX ĝS ◦ f̂ is the strongest.\nIt appears that learning an MNIST digit requires a dense weight vector and so Ŵ (1) tends to encode single digits, which helps transferrability. However, it appears that since we may learn a newsgroup with a sparse weight vector, Ŵ (1) tends to encode disjunctions of newsgroups which somewhat reduces transferrability. When transferring representations does work, fine-tuning using the regularization penalty proposed in (2) improves performance.\n15For MNIST there are 10 label classes and for 20 Newgroups there are 20. In both cases the classes are approximately balanced. Note that we ignore the hierarchical structure of the 20 Newsgroups classes, which likely contributes to the lower accuracies reported for all methods for this dataset relative to MNIST.\n16i.e. logistic regression with L2 regularization and f̂ fixed. 17Used to isolate the benefit of transferring f̂ rather than ĝS ◦ f̂ . 18We explored tuning λ to lift the performance of BASE on MNIST, but found that the results did not materially improve. Potentially λ1(j) and λ2(j) in (2) could be tuned with cross validation on the target task.\n19For γ = 1, hS = hT . We do not consider γ < 0.5, since that is equivalent to 1−γ with the definitions of T+ and T− swapped."
  }, {
    "heading": "6. Conclusion",
    "text": "We developed sufficient conditions for the successful transfer of representations both with and without fine-tuning. This is a step towards a principled explanation of the empirical success achieved by such techniques. A promising direction for future work is generalizing the neural network architectures considered (e.g. using multiple hidden layers) and relaxing the distributional assumptions required. Furthermore, in the fine-tuning case it may be possible to upper bound the target task generalization error of hypotheses in G ◦ F̂ := {h : ∃f ∈ F̂ , g ∈ G such that h = g ◦ f} using another measure such as the Rademacher complexity of G ◦ F̂ , eliminating the need for stochastic hypotheses.\nWe proposed a novel form of regularization for neural network training motivated by our theoretical results, which penalizes divergence from source task weights and is stricter for lower-level weights. We validated this technique through applications to image and text classification. Future directions include experiments on more challenging tasks using deeper and more tailored network architectures (e.g. convolutional neural networks)."
  }, {
    "heading": "7. Additional Proofs",
    "text": "We provide complete proofs of Theorems 2 and 4. For brevity, we drop the explicit dependence of f , f̂ , hS and hT on x in our notation where the meaning is clear."
  }, {
    "heading": "7.1. Proof of Theorem 2",
    "text": "Proof. Let gS(z) := sign(vS · z), gT (z) := sign(vT · z), ĝS(z) := sign(v̂S · z), ĝT (z) := sign(d ∗ v̂S · z), where d := vS ∗vT ∈ {−1, 1}k and ∗ is the elementwise product. It is sufficient to showRT (ĝT ◦f̂) ≤ cRS(ĝS◦f̂)+ (1+c).\nRT (ĝT ◦ f̂)\n= Prx∼PT (hT d ∗ v̂S · f̂ ≤ 0)\n≤ Prx∼PT (hT d ∗ vS · f ≤ 0, d ∗ vS · fd ∗ v̂S · f̂ ≥ 0) + Prx∼PT (hT d ∗ vS · f ≥ 0, d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)\n≤ Prx∼PT (hT d ∗ vS · f ≤ 0)+ Prx∼PT (d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)\n≤ + Prx∼PT (d ∗ vS · fd ∗ v̂S · f̂ ≤ 0)\n≤ + cPrx∼PS (vS · fv̂S · f̂ ≤ 0)\n≤ + c[Prx∼PS (hS v̂S · f̂ ≤ 0, hSvS · f ≥ 0)+ Prx∼PS (hS v̂S · f̂ ≥ 0, hSvS · f ≤ 0)]\n≤ + c[Prx∼PS (hS v̂S · f̂ ≤ 0) +Prx∼PS (hSvS · f ≤ 0)]\n≤ cRS(ĝS ◦ f̂) + (1 + c).\nThe third and final inequalities are due to the shared representation assumption in the problem statement. The fourth inequality holds by Lemma 1. The remaining lines apply simple rules of probability.\nLemma 1. Suppose ∀x, x′ such that ||Mx|| = ||Mx′||, PT (x) ≤ cPS(x′). Let f, f̂ ∈ F , v, v̂, d ∈ {−1, 1}k. Then Prx∼PT (d∗v ·fd∗ v̂ · f̂ ≤ 0) ≤ cPrx∼PS (v · fv̂ · f̂ ≤ 0).\nProof. Suppose there is an invertible map Rn → Rn yielding x′ on input x, such that ∀x, ||Mx|| = ||Mx′|| and d ∗ v · f(x)d ∗ v̂ · f̂(x) = v · f(x′)v̂ · f̂(x′). Then the result follows since PT (x) ≤ cPS(x′) by assumption. Furthermore, if M is an orthogonal matrix, ||x|| = ||x′||.\nSuch a map is x′ := (MTM)−1MT d̃ ∗ (Mx), where d̃ := [d1, d1, . . . , dk, dk]. We have ∀i, wi · x′ = diwi · x and (αiŵi−βiwi)·x′ = di(αiŵi−βiwi)·x, and hence ŵi ·x′ = diŵi · x for αi, βi 6= 0. Therefore:\nd ∗ v · f(x)d ∗ v̂ · f̂(x)\n= v · d ∗ f(x)v̂ · d ∗ f̂(x)\n= v · f(x′)v̂ · d ∗ f̂(x)\n= v · f(x′)v̂ · f̂(x′).\nThe first equality is a property of the elementwise and dot products. For the second equality, a(wi·x′) = a(diwi·x) = dia(wi · x) since a is an odd function. Similarly, for the third equality a(ŵi · x′) = a(diŵi · x) = dia(ŵi · x)."
  }, {
    "heading": "7.2. Proof of Theorem 4",
    "text": "Proof of ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ . Recall that wi are the weight vectors for f and ŵi are those for f̂ . Observe that for any wi such that wi · ŵi < 0, we have −wi · ŵi > 0 and −visign(−wi · x) = visign(wi · x). Combining this with the assumption\nmin f∈F,gS ,gT∈G max[RS(gS ◦ f), RT (gT ◦ f)] ≤ , we conclude ∃f ∈ F, gS , gT ∈ G such that ∀i, wi · ŵi ≥ 0 and max[RS(gS ◦ f), RT (h̃gT ◦f )] ≤ .\nLet gS(z) := sign(vS · z) and ĝS(z) := sign(v̂S · z). Let P be a rotation invariant distribution for c = 1. To prove h̃gT ◦f ∈ H̃G◦F̂ , by the definition of H̃G◦F̂ it is sufficient to show ∀i, |θ(wi, ŵi)| ≤ π √ 2(k − 1)c(RS(ĝS ◦ f̂) + ).\nmax i |θ(wi,ŵi)|\nπ √ 2(k−1)\n≤ Prx∼P (vS · fvS · f̂ ≤ 0)\n≤ Prx∼P (vS · fv̂S · f̂ ≤ 0)\n≤ cPrx∼PS (vS · fv̂S · f̂ ≤ 0)\n≤ c[Prx∼PS (hSvS · f ≤ 0, hS v̂S · f̂ ≥ 0)+ Prx∼PS (hSvS · f ≥ 0, hS v̂S · f̂ ≤ 0)]\n≤ c[Prx∼PS (hSvS · f ≤ 0) + Prx∼PS (hS v̂S · f̂ ≤ 0)]\n≤ c[ +RS(ĝS ◦ f̂)].\nThe first inequality holds by Lemma 2. The second inequality holds by Lemma 3, using the fact ∀i, wi · ŵi ≥ 0. The third inequality uses the rotation invariance of PS . The following two lines use basic laws of probability. The final inequality uses the assumption RS(gS ◦ f) ≤ .\nProof of ∀h̃ ∈ H̃G◦F̂ ,KL(h̃||h̃ĝS◦f̂ ) ≤ ω(RS(ĝS ◦ f̂)). For any h̃g◦f ∈ H̃G◦F̂ , KL(h̃g◦f ||h̃ĝS◦f̂ ) = k∑ i=1 [KL(N (wi, σ2I)||N (ŵi, σ2I))] +KL(Bv||Bv̂S ).\nThe KL divergence of a product distribution is the sum of the KL divergences of its component distributions. We upper bound both terms and apply the definition of ω. k∑ i=1 KL(N (wi, σ2I)||N (ŵi, σ2I)) = 12σ2 k∑ i=1 ||wi − ŵi||2 = 12σ2 k∑ i=1 (||wi||2 + ||ŵi||2 − 2||wi||||ŵi|| cos |θ(wi, ŵi)|) = 1σ2 k∑ i=1 (1− cos |θ(wi, ŵi)|)\n≤ kσ2 [1− cos(π √ 2(k − 1)c(RS(ĝS ◦ f̂) + ))].\nThe first equality uses the KL divergence of Gaussian distributions. The second equality uses the law of cosines. The third equality is because ∀i, ||wi|| = ||ŵi|| = 1 by construction. The inequality follows by the definition of F̂ and the fact that 1− cos |θ| is non-decreasing for |θ| ∈ [0, π].\nKL(Bv||Bv̂S ) ≤ k∑ i=1 ( k i ) pi(1− p)k−i log2 pi(1−p)k−i (1−p)ipk−i\n= k[2p− 1 + (1− p)k] log2 p 1−p .\nThe first inequality uses the definition of Bv to express KL(Bv||Bv̂S ). The equality is a simplification.\nLemma 2. Suppose k is odd, v ∈ {−1, 1}k, f, f̂ ∈ F such that ∀i, wi · ŵi ≥ 0 and P is rotation invariant with c = 1. Then max i |θ(wi,ŵi)|\nπ √ 2(k−1) ≤ Prx∼P (v · fv · f̂ ≤ 0).\nProof. Let v−j := [v1, . . . , vj−1, vj+1, . . . , vk] and define f−j and f̂−j similarly. Let Pr(·) := Prx∼P (·).\nPr(v · fv · f̂ ≤ 0)\n≥ Pr(v · fv · f̂ < 0)\n≥ Pr(v−j · f−j = 0)Pr(v · fv · f̂ < 0|v−j · f−j = 0)\n= Pr(v−j · f−j = 0) Pr(vjfjv−j · f̂−j + fj f̂j < 0|v−j · f−j = 0)\n= Pr(v−j · f−j = 0) [Pr(vjfjv−j · f̂−j < −1, fj f̂j = 1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j < 1, fj f̂j = −1|v−j · f−j = 0)]\n≥ Pr(v−j · f−j = 0) [Pr(vjfjv−j · f̂−j < −1, fj f̂j = −1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j < 1, fj f̂j = −1|v−j · f−j = 0)]\n= Pr(v−j · f−j = 0) [Pr(vjfjv−j · f̂−j < −1, fj f̂j = −1|v−j · f−j = 0)+ Pr(vjfjv−j · f̂−j > −1, fj f̂j = −1|v−j · f−j = 0)]\n= Pr(v−j · f−j = 0)Pr(fj f̂j = −1|v−j · f−j = 0)\n= Pr(v−j · f−j = 0)Pr(fj f̂j = −1) = (k−1 k−1 2 ) ( 12 ) k−1 |θ(wj ,ŵj)| π\n≥ 2 k−1√\n2(k−1) ( 12 ) k−1 |θ(wj ,ŵj)| π\n≥ max i |θ(wi,ŵi)|\nπ √ 2(k−1) .\nThe third inequality follows since P is rotation invariant and wj · ŵj ≥ 0. The third and fifth equalities use rotation invariance. The final equality uses rotation invariance and the fact that k is odd. The fourth inequality is a standard lower bound for the central binomial coefficient. The other lines use basic simplifications and laws of probability.\nLemma 3. Suppose k is odd, v, v̂ ∈ {−1, 1}k, f, f̂ ∈ F such that ∀i, wi · ŵi ≥ 0 and P is rotation invariant with c = 1. Then Prx∼P (v · fv · f̂ ≤ 0) ≤ Prx∼P (v · fv̂ · f̂ ≤ 0). Proof. Let Pr(·) := Prx∼P (·) and E[·] := Ex∼P [·]. Let Pr(f̃) := Prx∼P ([f1(x)f̂1(x), . . . , fk(x)f̂k(x)] = f̃). Let d := v̂ ∗ v and ∆(x) := 1(v · f(x)v̂ · f̂(x) ≤ 0) − 1(v · f(x)v · f̂(x) ≤ 0). Assume v̂ 6= v (if v̂ = v then\nthe lemma clearly holds). Let a(f̃) := k∑ i=1 1(f̃i = 1) and let l := min i:di=−1 i. Let F̃ := {f̃ ∈ {−1, 1}k : a(f̃) > a(d ∗ f̃) ∨ (a(f̃) = a(d ∗ f̃) ∧ f̃l = 1)}.\nLet Φ(a) := 1 2k−1 bk/2c∑ b=0 b∑ j=da/2+b/2−k/4e ( a j )( k−a b−j ) . The\nterm b counts coordinates where vif̂i = sign(v · f), while j counts those where vifi = sign(v · f) and fi = f̂i.\nPr(v · fv̂ · f̂ ≤ 0)− Pr(v · fv · f̂ ≤ 0)\n= E[1(v · fv̂ · f̂ ≤ 0)]− E[1(v · fv · f̂ ≤ 0)]\n= E[∆] = ∑̃ f∈F̃ Pr(f̃)E[∆|f̃ ] + Pr(d ∗ f̃)E[∆|d ∗ f̃ ]\n= ∑̃ f∈F̃ [Pr(f̃)− Pr(d ∗ f̃)]E[∆|f̃ ]\n= ∑̃ f∈F̃ [Pr(f̃)− Pr(d ∗ f̃)]\n[Pr(v · fv · f̂ ≤ 0|d ∗ f̃)− Pr(v · fv · f̂ ≤ 0|f̃)] = ∑̃ f∈F̃ [Pr(f̃)− Pr(d ∗ f̃)][Φ(a(d ∗ f̃))− Φ(a(f̃))]\n≥ 0.\nThe second equality uses linearity of expectation. The third equality uses the law of total expectation and the definition of F̃ .\nThe fourth equality holds since E[∆|d ∗ f̃ ] = ∑ f∈{−1,1}k Pr(f |d ∗ f̃)E[∆|d ∗ f̃ , f ]\n= − ∑\nf∈{−1,1}k Pr(f |d ∗ f̃)E[∆|f̃ , f ]\n= − ∑\nf∈{−1,1}k Pr(f |f̃)E[∆|f̃ , f ] = −E[∆|f̃ ] due to the\nrotation invariance of P .\nThe fifth equality holds by expanding ∆, linearity of expectation, and a similar argument to the previous equality to show Pr(v · fv̂ · f̂ ≤ 0|f̃) = Pr(v · fv · f̂ ≤ 0|d ∗ f̃).\nThe sixth equality holds by the rotation invariance of P and the fact that k is odd.\nFor the final inequality, the right hand term is non-negative since a(f̃) ≥ a(d ∗ f̃) and Φ is non-increasing. The left hand term is also non-negative due to the rotation invariance assumption and the fact that ∀i, wi · ŵi ≥ 0."
  }, {
    "heading": "Acknowledgements",
    "text": "Daniel McNamara was a visitor at Carnegie Mellon University during the period of this research, supported by a Fulbright Postgraduate Scholarship.\nThis work was supported in part by NSF grants CCF1422910, CCF-1535967, IIS-1618714, and a Microsoft Research Faculty Fellowship.\nWe thank the anonymous reviewers for their useful comments."
  }],
  "year": 2017,
  "references": [{
    "title": "Convex multi-task feature learning",
    "authors": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"],
    "venue": "Machine Learning,",
    "year": 2008
  }, {
    "title": "Tailoring continuous word representations for dependency parsing",
    "authors": ["Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"],
    "venue": "In Association for Computational Linguistics,",
    "year": 2014
  }, {
    "title": "A model of inductive bias learning",
    "authors": ["Baxter", "Jonathan"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2000
  }, {
    "title": "A theory of learning from different domains",
    "authors": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Vaughan", "Jennifer Wortman"],
    "venue": "Machine Learning,",
    "year": 2010
  }, {
    "title": "DeCAF: a deep convolutional activation feature for generic visual recognition",
    "authors": ["Donahue", "Jeff", "Jia", "Yangqing", "Vinyals", "Oriol", "Hoffman", "Judy", "Zhang", "Ning", "Tzeng", "Eric", "Darrell", "Trevor"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Regularized multitask learning",
    "authors": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"],
    "venue": "In International Conference on Knowledge Discovery and Data Mining, pp",
    "year": 2004
  }, {
    "title": "A theoretical framework for deep transfer learning",
    "authors": ["Galanti", "Tomer", "Wolf", "Lior", "Hazan", "Tamir"],
    "venue": "Information and Inference,",
    "year": 2016
  }, {
    "title": "Domainadversarial training of neural networks",
    "authors": ["Ganin", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "François", "Marchand", "Mario", "Lempitsky", "Victor"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "A PAC-Bayesian approach for domain adaptation with specialization to linear classifiers",
    "authors": ["Germain", "Pascal", "Habrard", "Amaury", "Laviolette", "François", "Morvant", "Emilie"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Domain adaptive neural networks for object recognition",
    "authors": ["Ghifary", "Muhammad", "Kleijn", "W Bastiaan", "Zhang", "Mengjie"],
    "venue": "In Pacific Rim International Conference on Artificial Intelligence,",
    "year": 2014
  }, {
    "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
    "authors": ["Girshick", "Ross", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"],
    "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2014
  }, {
    "title": "LSDA: Large scale detection through adaptation",
    "authors": ["Hoffman", "Judy", "Guadarrama", "Sergio", "Tzeng", "Eric S", "Hu", "Ronghang", "Donahue", "Jeff", "Girshick", "Ross", "Darrell", "Trevor", "Saenko", "Kate"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Learning transferable features with deep adaptation networks",
    "authors": ["Long", "Mingsheng", "Cao", "Yue", "Wang", "Jianmin", "Jordan", "Michael I"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Domain adaptation: Learning bounds and algorithms",
    "authors": ["Mansour", "Yishay", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"],
    "venue": "In Conference on Learning Theory,",
    "year": 2009
  }, {
    "title": "The benefit of multitask representation learning",
    "authors": ["Maurer", "Andreas", "Pontil", "Massimiliano", "RomeraParedes", "Bernardino"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Foundations of Machine Learning",
    "authors": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"],
    "year": 2012
  }, {
    "title": "A PACBayesian bound for Lifelong Learning",
    "authors": ["Pentina", "Anastasia", "Lampert", "Christoph H"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Constructing informative priors using transfer learning",
    "authors": ["Raina", "Rajat", "Ng", "Andrew Y", "Koller", "Daphne"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2006
  }, {
    "title": "Understanding Machine Learning: From Theory to Algorithms",
    "authors": ["Shalev-Shwartz", "Shai", "Ben-David"],
    "year": 2014
  }, {
    "title": "Zero-shot learning through crossmodal transfer",
    "authors": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "How transferable are features in deep neural networks",
    "authors": ["Yosinski", "Jason", "Clune", "Jeff", "Bengio", "Yoshua", "Lipson", "Hod"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }],
  "id": "SP:b10c1c81abb61b348f31d103f10d24bf096833da",
  "authors": [{
    "name": "Daniel McNamara",
    "affiliations": []
  }, {
    "name": "Maria-Florina Balcan",
    "affiliations": []
  }],
  "abstractText": "A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight transfer, which we validate with experiments.",
  "title": "Risk Bounds for Transferring Representations With and Without Fine-Tuning"
}