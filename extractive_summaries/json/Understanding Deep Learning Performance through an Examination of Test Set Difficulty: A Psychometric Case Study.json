{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4711–4716 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n4711"
  }, {
    "heading": "1 Introduction",
    "text": "One method for interpreting deep neural networks (DNNs) is to examine model predictions for specific input examples, e.g. testing for shape bias as in Ritter et al. (2017). In the traditional classification task, the difficulty of the test set examples is not taken into account. The number of correctlylabeled examples is tallied up and reported. However, we hypothesize that it may be worthwhile to use difficulty when evaluating DNNs. For example, what does it mean if a trained model answers the more difficult examples correctly, but cannot correctly classify what are seemingly simple cases? Recent work has shown that for NLP tasks such as Natural Language Inference (NLI), models can achieve strong results by simply using the hypothesis of a premise-hypothesis pair and ignoring the premise entirely (Gururangan et al., 2016; Tsuchiya, 2018; Poliak et al., 2018).\nIn this work we consider understanding DNNs by looking at the difficulty of specific test set examples and comparing DNN performance under different training scenarios. Do DNN models learn examples of varying difficulty at different rates? If\na model does well on hard examples and poor on easy examples, then can we say that it has really learned anything? In contrast, if a model does well on easy items, because a dataset is all easy, have we really “solved” anything?\nTo model difficulty we use Item Response Theory (IRT) from psychometrics (Baker and Kim, 2004). IRT models characteristics such as difficulty and discrimination ability of specific examples (called “items”1) in order to estimate a latent ability trait of test-takers. Here we use IRT to model the difficulty of test items to determine how DNNs learn items of varying difficulty. IRT provides a well-studied methodology for modeling item difficulty as opposed to more heuristic-based difficulty estimates such as sentence length. IRT was previously used to build a new test set for the NLI task (Lalor et al., 2016) and show that model performance is dependent on test set difficulty. In this work we use IRT to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of SA.\nWe train three DNNs models with varying training set sizes to compare performance on two NLP tasks: NLI and Sentiment Analysis (SA). Our experiments show that a DNN model’s likelihood of classifying an item correctly is dependent on the item’s difficulty. In addition, as the models are trained with more data, the odds of answering easy examples correctly increases at a faster rate than the odds of answering a difficult example correctly. That is, performance starts to look more human, in the sense that humans learn easy items faster than they learn hard items.\nThat the DNNs are better at easy items than hard items seems intuitive but is a surprising and interesting result since the item difficulties are modeled from human data. There is no underlying reason\n1For the remainder of the paper we will refer to a single test set example as an “item” for consistency.\nthat the DNNs would find items that are easy for humans inherently easy. To our knowledge this is the first work to use a grounded measure of difficulty learned from human responses to understand DNN performance. Our contributions are as follows: (i) we use a well-studied methodology, IRT, to estimate item difficulty in two NLP tasks and show that this human-estimated difficulty is a useful predictor of DNN model performance, (ii) we show that as training size increases DNN performance trends towards expected human performance.2"
  }, {
    "heading": "2 Methods",
    "text": ""
  }, {
    "heading": "2.1 Estimating Item Difficulty",
    "text": "To model item difficulty we use the Three Parameter Logistic (3PL) model from IRT (Baker, 2001; Baker and Kim, 2004; Lalor et al., 2016). The 3PL model in IRT models an individual’s latent ability (θ) on a task as a function of three item characteristics: discrimination ability (a), difficulty (b), and guessing (c). For a particular item i, the probability that an individual j will answer item i correctly is a function of the individual’s ability and the three item characteristics:\npij(θj) = ci + 1− ci\n1 + e−ai(θj−bi) (1)\nwhere ai is the discrimination parameter (the value of the function slope at it’s steepest point), bi is the difficulty parameter (the value where pij(θj) = 0.5), and ci is the guessing parameter (the lower asymptote of the function). For a set of items I and a set of individuals J , the likelihood of each individual in J’s responses to the items in I is:\nL = J∏ j=1 I∏ i=1 pij(θj) yijqij(θj) (1−yij) (2)\nwhere qij(θj) = 1− pij(θj) and yij = 1 if individual j answered item i correctly and yij = 0 otherwise. Item parameters and individual ability are jointly estimated from a set of individuals’ response patterns using an Expectation-Maximization algorithm (Bock and Aitkin, 1981).\nIn this work we focus on the difficulty parameter bi, which represents the latent ability level at which an individual has a 50% chance of answering item\n2Code and data available at http://jplalor.github.io\ni correctly. Low values of bi are associated with easier items (since an individual with low ability has a 50% chance of answering correctly), and higher values of bi represent more difficult items."
  }, {
    "heading": "2.2 Data",
    "text": "To estimate item difficulties for NLI, we used the pre-trained IRT models of Lalor et al. (2016) and extracted the difficulty item parameters. The data consists of approximately 1000 human annotator responses from Amazon Mechanical Turk (AMT) for a selection of 180 premise-hypothesis pairs from the SNLI data set (Bowman et al., 2015). Each AMT worker (Turker) was shown the premisehypothesis pairs and was asked to indicate whether, if the premise was taken to be true, the hypothesis was (a) definitely true (entailment), (b) maybe true (neutral), or (c) definitely not true (contradiction).\nFor SA, we collected a new data set of labels for 134 examples randomly selected from the Stanford Sentiment Treebank (SSTB) (Socher et al., 2013), using a similar AMT setup as Lalor et al. (2016). For each randomly selected example, we had 1000 Turkers label the sentence as very negative, negative, neutral, positive, or very positive. We converted these responses to binary positive/negative labels and fit a new IRT 3PL model (§2.1) using the mirt R package (Chalmers et al., 2015). Very negative and negative labels were binned together, and neutral, positive, and very positive were binned together.\nTables 1 and 2 show examples of the items in our data sets, and the difficulty values estimated from the IRT models. The first example in Table 1 is a clear case of entailment, where if we assume that the premise is true, we can infer that the hypothesis is also true. The label of the second example in SNLI is contradiction, but in this case the result is not as clear. There are sports stadiums that offer lawn seating, and therefore this could potentially be a case of entailment (or neutral). Either way, one could argue that the second example here is more difficult than the first. Similarly, the first two examples of Table 2 are interesting. Both of these items are labeled as negative examples in the data set. The first example is clear, but the second one is more ambiguous. It could be considered a mild complement, since the author still endorses renting the movie. Therefore you could argue again that the second example is more difficult than the first. The learned difficulty parameters reflect this difference\nin difficulty in both cases. Inter-rater reliability scores for the collected annotations are showin in Table 3. Scores for the NLI annotations were calculated when the original dataset was collected and are reproduced here (Lalor et al., 2016). Human annotations for the SA annotations were converted to binary before calculating the agreement. We see that the agreement scores are in the range of 0.4 to 0.6 which is considered moderate agreement (Landis and Koch, 1977). With the large number of annotators it is to be expected that there is some disagreement in the labels. However this disagreement can be interpreted as varying difficulty of the items, which is what we expect when we fit the IRT models."
  }, {
    "heading": "2.3 Experiments",
    "text": "Our goal in this work is to understand how DNN performance on items of varying difficulty changes under different training scenarios. To test this, we trained three DNN models using subsets of the original SNLI and SSTB training data sets: (i) Long Short Term Memory Network (LSTM) (Bow-\nman et al., 2015), (ii) Convolutional Neural Network (CNN) (Kim, 2014), and (iii) Neural Semantic Encoder (NSE), a type of memory-augmented RNN (Munkhdalai and Yu, 2017).3 For each task (NLI and SA), we randomly sampled subsets of training data, from 100 examples up to and including the full training data sets.4 We trained each model on the training data subsets, using the original development sets for early stopping to prevent overfitting. The IRT data with difficulty estimates were used as test sets for the trained models.\nOnce the models were trained and had classified the IRT data sets, we fit logistic regression models to predict whether a DNN model would label an item correctly, using the training set size and item difficulty as the dependent parameters."
  }, {
    "heading": "3 Results",
    "text": "Figure 1 plots the contour plots of our learned regression models. The top row plots results for the NLI task, and the bottom row plots results for the SA task. From left to right in both rows, the plots show results for the LSTM, CNN, and NSE models. In each plot, the x-axis is the training set size, the y-axis is the item difficulty, and the contour lines represent the log-odds that the DNN model would classify an item correctly. As the plots show, item difficulty has a clear effect on classification. Easier items have higher odds of being classified correctly\n3Please refer to the appendix for model details. 4We sampled 100, 1000, 2000, 5000, 10000, 50000, 100000, 200000, and 500000 examples for NLI, and sampled 100, 1000, 5000, 10000, 50000, and 75000 examples for SA.\nacross all of the training set sizes. In addition, the slopes of the contour lines are steeper at lower levels of difficulty. This indicates that, moving left to right along the x-axis, a model’s odds of answering an easy item correctly increase more quickly than the odds of answering a harder item correctly.\nThe contour plots for the CNN and NSE models on the SA task (Figure 1, second row middle and right plots) show that the easier items have higher likelihood of being classified correctly, but the odds for the most difficult items decrease as training size increases. This suggests that these models are learning in such a way that improves performance on easy items but has a negative effect on hard items. This result is important for interpretability, as it could inform stakeholder decisions if they need to have difficult examples classified.\nThe idea that easy items should be easier than hard items is consistent with learning strategies in humans. For example, when teaching new concepts to students, easier concepts are presented first so that the students can learn patterns and core information before moving to more difficult concepts (Collins et al., 1988; Arroyo et al., 2010). As students do more examples, all questions get easier,\nbut easy questions get easier at a faster rate. Our result is also consistent with the key assumptions of curriculum learning (Bengio et al., 2009)."
  }, {
    "heading": "4 Related Work",
    "text": "Lalor et al. (2016) introduced the idea of applying IRT evaluation to NLP tasks. They built a set of scales using IRT for NLI and evaluated a single LSTM neural network to demonstrate the effectiveness of the evaluation, but did not evaluate other NLP models or tasks. Martı́nez-Plumed et al. (2016) consider IRT in the context of evaluating ML models, but they do not use a human population to calibrate the models, and obtain results that are difficult to interpret under IRT assumptions.\nThere has been work in the NLP community around modeling latent characteristics of data (Bruce and Wiebe, 1999) and annotators (Hovy et al., 2013), but none that apply the resulting metrics to interpret DNN models. Passonneau and Carpenter (2014) model the probability a label is correct with the probability of an annotator to label an item correctly according to the Dawid and Skene (1979) model, but do not consider difficulty or discriminatory ability of the data points.\nOne-shot learning is an attempt to build ML models that can generalize after being trained on one or a few examples of a class as opposed to a large training set (Lake et al., 2013). One-shot learning attempts to mimic human learning behaviors (i.e., generalization after being exposed to a small number of training examples) (Lake et al., 2016). Our work instead looks at comparisons to human performance, where any learning (on the part of models) has been completed beforehand. Our goal is to analyze DNN models and training set variations as they affect ability in the context of IRT."
  }, {
    "heading": "5 Discussion",
    "text": "In this work we have shown that DNN model performance is affected by item difficulty as well as training set size. This is the first work that has used a well-established method for estimating difficulty to analyze DNN model performance as opposed to heuristics. DNN models perform better on easy items, and as more data is introduced in training, easy items are learned more quickly than hard items. Learning easy examples faster than harder examples is what would be expected when examining human response patterns as they learn more about a subject. However this has not previously been shown to be true in DNN models.\nThat the results are consistent across NLI and SA shows that the methods can be applied to a number of NLP tasks. The SA results do show that the odds of labeling a difficult item correctly decrease with more training data 1. It could be the case that these difficult items in the SA task are more subjective than the easier items, for example a review that is fairly neutral and is split between positive and negative annotations. These cases would be more difficult for a model to label, and are worth examining in more detail. By identifying items such as these as difficult makes it easier to see where the model is going wrong and allows for research on better way to represent these cases.\nThis result has implications for how machine learning models are evaluated across tasks. The traditional assumption that the test data is drawn from the same distribution as the training data, makes it difficult to understand how a model will perform in settings where that assumption does not hold. However, if the difficulty of test set data is known, we can better understand what kind of examples a given model performs well on, and specific instances where a model underperforms (e.g. the\nmost difficult examples). In addition, researhers can build test sets that consist of a specific type of data (very easy, very hard, or a mix) to evaluate a trained model under specific assumptions to test generalization ability in a controlled way. This could allow for more confidence in model performance in more varied deployment settings, since there would be a set of tests a model would have to pass before being deployed.\nIt is important to note that the difficulty parameters were estimated from a human population, meaning that those items that are difficult for humans are in fact more difficult for the DNN models as well. This does not need to be the case given that DNNs learn very different patterns, etc. than humans. In fact there were exceptions in our results which shows that these models should be carefully examined using techniques like those described here. Future work can investigate why this is the case and how we can leverage this information to improve model performance and interpretability."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the AMT Turkers who completed our annotation task. This work was supported in part by the HSR&D award IIR 1I01HX001457 from the United States Department of Veterans Affairs (VA). We also acknowledge the support of LM012817 from the National Institutes of Health. This work was also supported in part by the Center for Intelligent Information Retrieval. The contents of this paper do not represent the views of CIIR, NIH, VA, or the United States Government."
  }],
  "year": 2018,
  "references": [{
    "title": "Effort-based tutoring: An empirical approach to intelligent tutoring",
    "authors": ["Ivon Arroyo", "Hasmik Mehranian", "Beverly P Woolf."],
    "venue": "Educational Data Mining 2010.",
    "year": 2010
  }, {
    "title": "The basics of item response theory",
    "authors": ["Frank B Baker."],
    "venue": "ERIC.",
    "year": 2001
  }, {
    "title": "Item Response Theory: Parameter Estimation Techniques, Second Edition",
    "authors": ["Frank B. Baker", "Seock-Ho Kim."],
    "venue": "CRC Press.",
    "year": 2004
  }, {
    "title": "Curriculum learning",
    "authors": ["Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM.",
    "year": 2009
  }, {
    "title": "Marginal maximum likelihood estimation of item parameters",
    "authors": ["R Darrell Bock", "Murray Aitkin"],
    "year": 1981
  }, {
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "D. Christopher Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2015
  }, {
    "title": "Recognizing subjectivity: A case study in manual tagging",
    "authors": ["Rebecca F. Bruce", "Janyce M. Wiebe."],
    "venue": "Nat. Lang. Eng., 5(2):187–205.",
    "year": 1999
  }, {
    "title": "Cognitive apprenticeship: Teaching the craft of reading, writing and mathematics",
    "authors": ["Allan Collins", "John Seely Brown", "Susan E Newman."],
    "venue": "Thinking: The Journal of Philosophy for Children, 8(1):2–10.",
    "year": 1988
  }, {
    "title": "Maximum likelihood estimation of observer error-rates using the em algorithm",
    "authors": ["A. Philip Dawid", "Allan M. Skene."],
    "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28.",
    "year": 1979
  }, {
    "title": "Annotation artifacts in natural language inference datg",
    "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R Bowman", "Noah A Smith."],
    "venue": "Proceedings of the 2018 Conference of the North American Chap-",
    "year": 2016
  }, {
    "title": "Long ShortTerm Memory",
    "authors": ["S Hochreiter", "J Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Learning whom to trust with mace",
    "authors": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."],
    "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2013
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751. Association for Computational Linguistics.",
    "year": 2014
  }, {
    "title": "One-shot learning by inverting a compositional causal process",
    "authors": ["Brenden M Lake", "Ruslan R Salakhutdinov", "Josh Tenenbaum."],
    "venue": "Advances in neural information processing systems, pages 2526–2534.",
    "year": 2013
  }, {
    "title": "Building machines that learn and think like people",
    "authors": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman."],
    "venue": "Behavioral and Brain Sciences, pages 1–101.",
    "year": 2016
  }, {
    "title": "Building an evaluation scale using item response theory",
    "authors": ["John P. Lalor", "Hao Wu", "Hong Yu."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 648–657. Association for Computational Linguis-",
    "year": 2016
  }, {
    "title": "The measurement of observer agreement for categorical data",
    "authors": ["J Richard Landis", "Gary G Koch."],
    "venue": "biometrics, pages 159–174.",
    "year": 1977
  }, {
    "title": "Making sense of item response theory in machine learning",
    "authors": ["Fernando Martı́nez-Plumed", "Ricardo B.C. Prudłncio", "Adolfo Martnez Us", "Jos Hernndez-Orallo"],
    "venue": "In ECAI,",
    "year": 2016
  }, {
    "title": "Neural semantic encoders",
    "authors": ["Tsendsuren Munkhdalai", "Hong Yu."],
    "venue": "EACL 2017.",
    "year": 2017
  }, {
    "title": "The benefits of a model of annotation",
    "authors": ["Rebecca J. Passonneau", "Bob Carpenter."],
    "venue": "Transactions of the Association of Computational Linguistics, 2:311–326.",
    "year": 2014
  }, {
    "title": "Hypothesis only baselines in natural language inference",
    "authors": ["Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme."],
    "venue": "Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (* SEM",
    "year": 2018
  }, {
    "title": "Cognitive psychology for deep neural networks: A shape bias case study",
    "authors": ["Samuel Ritter", "David GT Barrett", "Adam Santoro", "Matt M Botvinick."],
    "venue": "ICML.",
    "year": 2017
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "D. Christopher Manning", "Andrew Ng", "Christopher Potts."],
    "venue": "Proceedings of the 2013 Conference on",
    "year": 2013
  }, {
    "title": "Performance impact caused by hidden bias of training data for recognizing textual entailment",
    "authors": ["Masatoshi Tsuchiya."],
    "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris,",
    "year": 2018
  }],
  "id": "SP:26c3cb7d92a4e767cc9c96f964107f111479bfb8",
  "authors": [{
    "name": "John P. Lalor",
    "affiliations": []
  }, {
    "name": "Hao Wu",
    "affiliations": []
  }, {
    "name": "Tsendsuren Munkhdalai",
    "affiliations": []
  }, {
    "name": "Hong Yu",
    "affiliations": []
  }],
  "abstractText": "Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. We examine the impact of a test set question’s difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question’s difficulty. As DNNs are trained with more data, easy examples are learned more quickly than hard examples.",
  "title": "Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study"
}