{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In this paper, we analyze inference suboptimality: the mismatch between the true and approximate posterior. More specifically, we are interested in understanding what factors cause the gap between the marginal log-likelihood and the evidence lower bound (ELBO) in variational autoencoders (VAEs, Kingma & Welling (2014); Rezende et al. (2014)). We refer to this as the inference gap. Moreover, we break down the inference gap into two components: the approximation gap and the amortization gap. The approximation gap comes from the inability of the variational distribution family to exactly match the true posterior. The amortization gap refers to the difference caused by amortizing the variational parameters over the entire training set, instead of optimizing for each training example individually. We refer the reader to Table 1 for the definitions of the gaps and to\n1Department of Computer Science, University of Toronto, Toronto, Canada. Correspondence to: Chris Cremer <ccremer@cs.toronto.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nFig. 1 for a simple illustration of the gaps. In Fig. 1, L[q] refers to the ELBO evaluated using an amortized distribution q, as is typical of VAE training. In contrast, L[q⇤] is the ELBO evaluated using the optimal approximation within its variational family.\nThere has been significant work on improving variational inference in VAEs through the development of expressive approximate posteriors (Rezende & Mohamed, 2015; Kingma et al., 2016; Ranganath et al., 2016; Tomczak & Welling, 2016; 2017). These works have shown that with more expressive approximate posteriors, the model learns a better distribution over the data. Our study aims to gain a better understanding of the relationship between expressive approximations and improved generative models.\nOur experiments investigate how the choice of encoder, posterior approximation, decoder, and optimization affect the approximation and amortization gaps. We train VAE models in a number of settings on the MNIST (LeCun et al., 1998), Fashion-MNIST (Xiao et al., 2017), and CIFAR-10 (Krizhevsky & Hinton, 2009) datasets.\nOur contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in VAE inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
  }, {
    "heading": "2. Background",
    "text": ""
  }, {
    "heading": "2.1. Inference in Variational Autoencoders",
    "text": "Let x be the observed variable, z the latent variable, and p(x, z) be their joint distribution. Given a dataset X = {x1, x2, ..., xN}, we would like to maximize the marginal log-likelihood with respect to the model parameters ✓:\nlog p✓(X) = NX\ni=1\nlog p✓(xi) = NX\ni=1\nlog Z p✓(xi, zi)dzi.\nIn practice, the marginal log-likelihood is computationally intractable due to the integration over the latent variable z. Instead, VAEs introduce an inference network q (z|x) to approximate the true posterior p(z|x) and optimize the ELBO with respect to model parameters ✓ and inference network parameters (parameterization subscripts omitted for brevity): log p(x) = Eq(z|x)  log ✓ p(x, z)\nq(z|x)\n◆ + KL (q(z|x)||p(z|x))\n(1)\nEq(z|x)  log ✓ p(x, z)\nq(z|x)\n◆ = LVAE[q]. (2)\nFrom the above equation, we see that the ELBO is tight when q(z|x) = p(z|x). The choice of q(z|x) is often a factorized Gaussian distribution for its simplicity and efficiency. By utilizing the inference network (also referred to as encoder or recognition network), VAEs amortize inference over the entire dataset. Furthermore, the overall model is trained by stochastically optimizing the ELBO using the reparametrization trick (Kingma & Welling, 2014)."
  }, {
    "heading": "2.2. Expressive Approximate Posteriors",
    "text": "There are a number of strategies for increasing the expressiveness of approximate posteriors, going beyond the original factorized-Gaussian. We briefly summarize normalizing flows and auxiliary variables."
  }, {
    "heading": "2.2.1. NORMALIZING FLOWS",
    "text": "Normalizing flow (Rezende & Mohamed, 2015) is a change of variables procedure for constructing complex distributions by transforming probability densities through a series of invertible mappings. Specifically, if we transform\na random variable z0 with distribution q0(z), the resulting random variable zT = T (z0) has a distribution:\nqT (zT ) = q0(z0) det @zT @z0\n1\n. (3)\nBy successively applying these transformations, we can build arbitrarily complex distributions. Stacking these transformations remains tractable due to the determinant being decomposable: det(AB) = det(A)det(B). An important property of these transformations is that we can take expectations with respect to the transformed density qT (zT ) without explicitly knowing its formula due to the law of the unconscious statistician (LOTUS):\nEqT [h(zT )] = Eq0 [h(fT (fT 1(...f1(z0))))]. (4)\nUsing equations (3) and (4), the lower bound with the transformed approximation can be written as:\nEz0⇠q0(z|x)\n2\n64log\n0\nB@ p(x, zT )\nq0(z0|x) QT\nt=1 det @zt@zt 1 1\n1\nCA\n3\n75 .\n(5)\nThe main constraint on these transformations is that the determinant of their Jacobian needs to be easily computable."
  }, {
    "heading": "2.2.2. AUXILIARY VARIABLES",
    "text": "Deep generative models can be extended with auxiliary variables which leave the generative model unchanged but make the variational distribution more expressive. Just as hierarchical Bayesian models induce dependencies between data, hierarchical variational models can induce dependencies between latent variables. The addition of the auxiliary variable changes the lower bound to:\nEz,v⇠q(z,v|x)  log ✓ p(x, z)r(v|x, z)\nq(z, v|x)\n◆ (6)\n= Eq(z|x)  log ✓ p(x, z)\nq(z|x)\n◆ KL ⇣ q(v|z, x)kr(v|x, z) ⌘\n(7)\nwhere r(v|x, z) is called the reverse model. From Eqn. 7, we see that this bound is looser than the regular ELBO,\nhowever the extra flexibility provided by the auxiliary variable can result in a higher lower bound. This idea has been employed in works such as auxiliary deep generative models (ADGM, (Maaløe et al., 2016)), hierarchical variational models (HVM, (Ranganath et al., 2016)) and Hamiltonian variational inference (HVI, (Salimans et al., 2015))."
  }, {
    "heading": "3. Methods",
    "text": ""
  }, {
    "heading": "3.1. Approximation and Amortization Gaps",
    "text": "The inference gap G is the difference between the marginal log-likelihood log p(x) and a lower bound L[q]. Given the distribution in the family that maximizes the bound, q⇤(z|x) = argmaxq2Q L[q], the inference gap decomposes as the sum of approximation and amortization gaps:\nG = log p(x) L[q] = log p(x) L[q⇤]| {z } Approximation +L[q⇤] L[q]| {z } Amortization .\nFor VAEs, we can translate the gaps to KL divergences by rearranging Eqn. (1):\nGVAE = KL q⇤(z|x)||p(z|x)\n| {z }\nApproximation\n+ KL q(z|x)||p(z|x)\nKL q⇤(z|x)||p(z|x)\n| {z }\nAmortization\n.\n(8)"
  }, {
    "heading": "3.2. Flexible Approximate Posteriors",
    "text": "Our experiments involve expressive approximations which use flow transformations and auxiliary variables. The flow transformation that we employ is of the same type as the transformations of Real NVP (Dinh et al., 2017). We partition the latent variable z into two, z1 and z2, then perform the following transformations:\nz01 = z1 1(z2) + µ1(z2) (9) z02 = z2 2(z01) + µ2(z01) (10)\nwhere 1, 2, µ1, µ2 : Rn ! Rn are differentiable mappings parameterized by neural nets and takes the Hadamard or element-wise product. We partition the latent variable by simply indexing the elements of the first half and the second half. The determinant of the combined transformation’s Jacobian, det ⇣ @z0\n@z ⌘ , can be easily evaluated. See section 7.3 of the Supplementary material for a derivation. The lower bound of this approximation is the same as Eqn. (5). We refer to this approximation as qFlow.\nWe also experiment with an approximation that combines flow transformations and auxiliary variables. Let z 2 Rn be the variable of interest and v 2 Rn the auxiliary variable. The flow is the same as equations (9) and (10), where z1 is\nreplaced with z and z2 with v. We refer to this approximate distribution as qAF , where AF stands for auxiliary flow. We train this model by optimizing the following bound:\nEq0(z,v|x)\n2\n64log\n0\nB@ p(x, zT )r(vT |x, zT )\nqT (zT , vT |x) det\n⇣ @ztvt\n@zt 1vt 1\n⌘ 1\n1\nCA\n3\n75\n= L[qAF ]. (11)\nNote that this lower bound is looser as explained in Section 2.2.2. We refer readers to Section 7.0.2 in the Supplementary material for specific details of the flow configuration adopted in the experiments."
  }, {
    "heading": "3.3. Marginal Log-Likelihood Estimation and Evidence Lower Bounds",
    "text": "In this section, we describe the estimates we use to compute the bounds of the inference gaps: log p(x), L[q⇤], and L[q]. We use two bounds to estimate the marginal log-likelihood, log p(x): IWAE (Burda et al., 2016) and AIS (Neal, 2001).\nThe IWAE bound takes multiple importance weighted samples from the variational q distribution resulting in a tighter lower bound than the VAE bound. The IWAE bound is computed as:\nlog p(x) Ez1...zk⇠q(z|x)\n\" log 1\nk\nkX\ni=1\np(x, zi) q(zi|x)\n!# (12)\n= LIWAE[q].\nAs the number of importance samples approaches infinity, the bound approaches the marginal log-likelihood. It is often used as an evaluation metric for generative models (Burda et al., 2016; Kingma et al., 2016). AIS is potentially an even tighter lower bound. AIS weights samples from distributions which are sequentially annealed from an initial proposal distribution to the true posterior. See Section 7.4 in the Supplementary material for further details regarding AIS. To compute the AIS bound, we use 100 chains, each with 10000 intermediate distributions, where each transition consists of one HMC trajectory with 10 leapfrog steps. The initial distribution for AIS is the prior, so that it is encoderindependent.\nWe estimate the marginal log-likelihood by independently computing our tightest lower bounds then take the maximum of the two:\nlog p̂(x) = max(LAIS,LIWAE).\nThe L[q⇤] and L[q] bounds are the standard ELBOs, LVAE, from Eqn. (2), computed with either the amortized q or the optimal q⇤ (see below). When computing LVAE and LIWAE, we use 5000 samples."
  }, {
    "heading": "3.4. Local Optimization of the Approximate Distribution",
    "text": "To compute LVAE[q⇤], we optimize the parameters of the variational distribution for every datapoint. For the local optimization of qFFG, we initialize the mean and variance as the prior, i.e. N (0, I). We optimize the mean and variance using the Adam optimizer with a learning rate of 10 3. To determine convergence, after every 100 optimization steps, we compute the average of the previous 100 ELBO values and compare it to the best achieved average. If it does not improve for 10 consecutive iterations then the optimization is terminated. For qFlow and qAF , the same process is used to optimize all of its parameters. All neural nets for the flow were initialized with a variant of the Xavier initilization (Glorot & Bengio, 2010). We use 100 Monte Carlo samples to compute the ELBO to reduce variance."
  }, {
    "heading": "3.5. Validation of Bounds",
    "text": "The soundness of our empirical analysis depends on the reliability of the marginal log-likelihood estimator. For general importance sampling based estimators, the sample variance of the normalized importance weights can serve as an indicator of accuracy (Geweke, 1989; Neal, 2001). This quantitative measure, however, can also be unreliable, e.g. when the proposal misses an important mode of the target distribution (Neal, 2001).\nIn this work, we follow (Wu et al., 2017) to empirically validate our AIS estimates with Bidirectional Monte Carlo (BDMC, Grosse et al. (2015; 2016)). In addition to a lower bound provided by AIS, BDMC runs AIS chains backward from exact posterior samples to obtain an upper bound on the marginal log-likelihood. It should be noted that BDMC relies on the assumption that the distribution of the simulated data from the model roughly matches that of the real data. This is due to the backward chain initializes from exact posterior samples (Grosse et al., 2015).\nFor the MNIST and Fashion datasets, BDMC gives a gap within 0.1 nat for a linear schedule AIS with 104 intermediate distributions and 100 importance samples on 103 simulated datapoints. For 3-BIT CIFAR, the same AIS setting gives a gap within 1 nat with the sigmoidial annealing schedule (Grosse et al., 2015) on 100 simulated datapoints. Loosely speaking, this should give us confidence in how well our AIS lower bounds reflect the marginal loglikelihood computed on the real data."
  }, {
    "heading": "4. Related Work",
    "text": "Much of the earlier work on variational inference focused on optimizing the variational parameters locally for each datapoint, e.g. the original Stochastic Variational Inference scheme (SVI, Hoffman et al. (2013)). To scale inference to large datasets, most related works utilize inference networks to amortize the cost of inference over the entire dataset.\nOur work analyses the error that these inference networks introduce.\nMost relevant to our work is the recent work of Krishnan et al. (2017), which explicitly remarks on two sources of error in variational learning with inference networks, and proposes to optimize approximate inference locally from an initialization output by the inference network. They show improved training on high-dimensional, sparse data with the hybrid method, claiming that local optimization reduces the negative effects of random initialization in the inference network early on in training. Thus their work focuses on reducing the amortization gap early on in training. Similar to this idea, Hoffman (2017) proposes to perform approximate inference during model training with MCMC at an initialization given by a variational distribution. Our work provides a means of explaining these improvements in terms of the sources of inference suboptimality that they reduce."
  }, {
    "heading": "5. Experimental Results",
    "text": ""
  }, {
    "heading": "5.1. Intuition through Visualization",
    "text": "To begin, we would like to gain an intuitive visualization of the gaps presented in Section 3.1. To this end, we trained a VAE with a two-dimensional latent space on MNIST and in Fig. 2 we show contour plots of various distributions in the latent space. The first row contains contour plots of the true posteriors p(z|x) for four different training datapoints (columns). We have selected these four examples to highlight different inference phenomena. The amortized fully-factorized Gaussian (FFG) row refers to the output of the recognition net, in this case, a FFG approximation. Optimal FFG is the FFG that best fits the posterior of the datapoint. Optimal Flow is the optimal fit of a flexible distribution to the same posterior, where the flexible distribution we use is described in Section 3.2.\nPosterior A is an example of a distribution where a FFG can fit relatively well. Posterior B is an example of a posterior with dependence between dimensions, demonstrating the\nlimitation of having a factorized approximation. Posterior C highlights a shortcoming of performing amortization with a limited-capacity recognition network, where the amortized FFG shares little support with the true posterior. Posterior D is a bi-modal distribution which demonstrates the ability of the flexible approximation to fit to complex distributions, in contrast to the simple FFG approximation. These observations raise the following question: in more typical VAEs, is the amortization of inference the leading cause of the distribution mismatch, or is it the limited expressiveness of the approximation?"
  }, {
    "heading": "5.2. Amortization vs Approximation Gap",
    "text": "In this section, we compare how much the approximation and amortization gaps each contribute to the total inference gap. Table 2 are results of inference on the training set of MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized version of CIFAR-10, see Section 7.0.3 for details). For each dataset, we trained models with two different approximate posterior distributions: a fully-factorized Gaussian, qFFG, and the flexible distribution, qAF . Due to the computational cost of optimizing the local parameters for each datapoint, our evaluation is performed on a subset of 1000 datapoints for MNIST and Fashion-MNIST and a subset of 100 datapoints for 3-BIT CIFAR.\nFor MNIST, we see that the amortization and approximation gaps each account for nearly half of the inference gap. On the more difficult Fashion-MNIST dataset, the amortization gap is larger than the approximation gap. For CIFAR, we see that the amortization gap is much more significant compared to the approximation gap. Thus, for the three datasets and model architectures that we consider, the amortization gap is likely to be the more prominent cause of inference suboptimality, especially when the dataset becomes more challenging to model. This indicates that improvements in inference will likely be a result of reducing amortization error, rather than approximation errors.\nWith these results in mind, would simply increasing the capacity of the encoder improve the amortization gap? We\nexamined this by training the MNIST and Fashion-MNIST models from above but with larger encoders. See Section 7.0.2 for implementation details. Table 3 (left) are the results of this experiment. Comparing to Table 2, we see that, for both datasets and both variational distributions, using a larger encoder results in the inference gap decreasing and the decrease is mainly due to a reduction in the amortization gap."
  }, {
    "heading": "5.3. Influence of Flows on the Amortization Gap",
    "text": "The common reasoning for increasing the expressiveness of the approximate posterior is to minimize the difference between the true and approximate distributions, i.e. reduce the approximation gap. However, given that the expressive approximation is often accompanied by many additional parameters, we would like to know how much influence it has on the amortization error.\nTo investigate this, we trained a VAE on MNIST, discarded the encoder, then retrained encoders with different approximate distributions on the fixed decoder. We fixed the decoder so that the true posterior is constant for all the retrained encoders. The initial encoder was a two-layer MLP with a factorized Gaussian distribution. In order to emphasize a large amortization gap, the retrained encoders had no hidden layers (ie. just linear transformations). For the retraiend encoders, we tested three approximate distributions: fully factorized Gaussian (qFFG), auxiliary flow (qAV ), and Flow (qFlow). See Section 3.2 for the details of these distributions.\nThe inference gaps of the retrained encoders on the training set are shown in Table 4. As expected, we observe that the small encoder with qFFG has a very large amortization gap. However, when we use qAF or qFlow as the approximate distribution, we see the approximation gap decrease, but more importantly, there is a significant decrease in the amortization gap. This indicates that the parameters used for increasing the complexity of the approximation also play a large role in diminishing the amortization error.\nThese results are expected given that the parameterization of the Flow distribution can be interpreted as an instance of the RevNet (Gomez et al., 2017) which has demonstrated that Real-NVP transformations (Dinh et al., 2017) can model complex functions similar to typical MLPs. Thus the flow transformations we employ should also be expected to increase the expressiveness while also increasing the capacity of the encoder. The implication of this observation is that models which improve the flexibility of their variational approximation, and attribute their improved results to the increased expressiveness, may have actually been due to the reduction in amortization error."
  }, {
    "heading": "5.4. Influence of Approximate Posterior on True Posterior",
    "text": "To what extent does the posterior approximation affect the learned model? Turner & Sahani (2011) studied the biases in parameter learning induced by the variational approximation when learning via variational Expectation-Maximization. Similarly, we ask whether a factorized Gaussian approximation causes the true posterior to be more like a factorized Gaussian? Burda et al. (2016) visually demonstrate that when trained with an importance-weighted approximate posterior, the resulting true posterior is more complex than those trained with factorized Gaussian approximations. Just as it is hard to evaluate a generative model by visually inspecting samples, it is hard to judge how “Gaussian” the true posterior is by visual inspection. We can quantitatively determine\nhow close the posterior is to a fully-factorized Gaussian (FFG) by comparing the marginal log-likelihood estimate log p̂(x) and the Optimal FFG bound LVAE[q⇤FFG]. This is equivalent to estimating the KL divergence between the optimal Gaussian and the true posterior, KL (q⇤(z|x)||p(z|x)).\nIn Table 2 on MNIST, for the FFG trained model, KL (q⇤(z|x)||p(z|x)) is nearly the same for both q⇤FFG and q⇤AF . In contrast, on the model trained with qAF , KL (q⇤(z|x)||p(z|x)) is much larger for q⇤FFG than q⇤AF . This suggests that the true posterior of a FFG-trained model is closer to FFG than the true posterior of the Flow-trained model. The same observation can be made on the FashionMNIST dataset. This implies that the decoder can learn to have a true posterior that fits better to the approximation.\nThese observations justify our results of Section 5.2. which showed that the amortization error is often the main cause of inference suboptimality. One reason for this is that the generator accommodates the choice of approximation, thus reducing the approximation error.\nGiven that we have seen that the generator can accommodate the choice of approximation, our next question is whether a generator with more capacity increases its ability to fit to the approximation. To this end, we trained VAEs with decoders of different sizes and measured the approximation gaps on\nthe training set. Specifically, we trained decoders with 0, 2, and 4 hidden layers on MNIST. See Table 5 for the results. We see that as the capacity of the decoder increases, the approximation gap decreases. This result implies that the more flexible the generator is, the less flexible the approximate distribution needs to be to ensure accurate inference."
  }, {
    "heading": "5.5. Inference Generalization",
    "text": "How well does amortized inference generalize at test time? We address this question by visualizing the gaps on training and validation datapoints across the training epochs. In Fig. 3, the models are trained on 50000 binarized FashionMNIST datapoints and the gaps are computed on a subset of a 100 training and validation datapoints. The top and bottom boundaries of the blue region represent log p̂(x) and L[q⇤]. The bottom boundary of the orange region represents L[q]. In other words, the blue region is the approximation gap and the orange is the amortization gap.\nIn Fig. 3, the Standard model (top left) refers to a VAE of latent size 20 trained with a factorized Gaussian approximate posterior. In this case, the encoder and decoder both have two hidden layers each consisting of 200 hidden units. The Flow model (top right) augments the Standard model with a qFlow variational distribution. Larger Decoder and Larger Encoder models have factorized Gaussian distributions and increase the number of hidden layers to three and the number of units in each layer to 500.\nFirstly, we observe that for all models, the approximation gap on the training and validation sets are roughly equivalent. This indicates that the true posteriors of the held-out data are similar to that of the training data. Secondly, we note that for all models, the encoder overfits more than the decoder.\nThese observations resonate with the encoder overfitting findings by Wu et al. (2017).\nHow does increasing decoder capacity affect inference on held-out data? We know from Section 5.4 that increasing generator capacity results in a posterior that better fits the approximation making posterior inference easier. Furthermore, the Larger Decoder plot of Fig. 3 shows that increasing generator capacity causes the model to be more prone to overfitting. Thus, there is a tradeoff between ease of inference and decoder overfitting."
  }, {
    "heading": "5.5.1. ENCODER CAPACITY AND APPROXIMATION EXPRESSIVENESS",
    "text": "We have seen in Sections 5.2 and 5.3 that expressive approximations as well as increasing encoder capacity can lead to a reduction in the amortization gap. This leads us to the following question: when should we increase encoder capacity versus increasing the expressiveness of the approximation? We answer this question in terms of how well each model can generalize its efficient inference (recognition network and variational distribution) to held-out data.\nIn Fig. 3, we see that the Flow model and the Larger Encoder model achieve similar log p̂(x) on the validation set at the end of training. However, we see that the L[q] bound of the Larger Encoder model is significantly lower than the L[q] bound of the Flow model due to the encoder overfitting to the training data. Although they both model the data nearly equally well, the recognition net of the Larger Encoder model is no longer suitable to perform inference on the held-out data due to overfitting. Thus a potential rational for utilizing expressive approximations is that they improve generalization to held-out data in comparison to increasing the encoder capacity.\nWe highlight that, in many scenarios, efficient test time inference is not required and consequently, encoder overfitting is not an issue, since we can use non-efficient encoderindependent methods to estimate log p(x), such as AIS, IWAE with local optimization, or potentially retraining the encoder on the held-out data. In contrast, when efficient test time inference is required, encoder generalization is important and expressive approximations are likely advantageous."
  }, {
    "heading": "5.6. Annealing the Entropy",
    "text": "Typical warm-up (Bowman et al., 2015; Sønderby et al., 2016) refers to annealing the KL (q(z|x)||p(z)) term during training. This can also be interpreted as performing maximum likelihood estimation (MLE) early on during training. This optimization technique is known to help prevent the latent variable from degrading to the prior (Burda et al., 2016; Sønderby et al., 2016). We employ a similar annealing scheme during training by annealing the entropy of the\napproximate distribution:\nEz⇠q(z|x) [log p(x, z) log q(z|x)] ,\nwhere is annealed from 0 to 1 over training. This can be interpreted as maximum a posteriori (MAP) in the initial phase of training.\nWe find that warm-up techniques, such as annealing the entropy, are important for allowing the true posterior to be more complex. Table 3 (right) are results from a model trained without the entropy annealing schedule. Comparing these results to Table 2, we observe that the difference between LVAE[q⇤FFG] and LVAE[q⇤AF ] is significantly smaller without entropy annealing. This indicates that the true posterior is more Gaussian when entropy annealing is not used. This suggests that, in addition to preventing the latent variable from degrading to the prior, entropy annealing allows the true posterior to better utilize the flexibility of the expressive approximation."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we investigated how encoder capacity, approximation choice, decoder capacity, and model optimization influence inference suboptimality in terms of the approximation and amortization gaps. We discovered that the amortization gap can be a leading source to inference suboptimality and that the generator can reduce the approximation gap by learning a true posterior that fits to the choice of approximation. We showed that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation. We confirmed that increasing the capacity of the encoder reduces the amortization error. Additionally, we demonstrated that optimization techniques, such as entropy annealing, help the generative model to better utilize the flexibility of expressive variational distributions. Analyzing these gaps can be useful for guiding improvements in VAEs. Future work includes evaluating other types of expressive approximations, more complex likelihood functions, and datasets."
  }],
  "year": 2018,
  "references": [{
    "title": "Generating Sentences from a Continuous Space",
    "authors": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"],
    "year": 2015
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Clevert", "D.-A", "T. Unterthiner", "S. Hochreiter"],
    "venue": "arXiv preprint arXiv:1511.07289,",
    "year": 2015
  }, {
    "title": "Density estimation using Real NVP",
    "authors": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"],
    "year": 2017
  }, {
    "title": "Bayesian inference in econometric models using monte carlo integration",
    "authors": ["J. Geweke"],
    "venue": "Econometrica: Journal of the Econometric Society,",
    "year": 1989
  }, {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "authors": ["X. Glorot", "Y. Bengio"],
    "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "The reversible residual network: Backpropagation without storing activations",
    "authors": ["A.N. Gomez", "M. Ren", "R. Urtasun", "R.B. Grosse"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Sandwiching the marginal likelihood using bidirectional monte carlo",
    "authors": ["R. Grosse", "Z. Ghahramani", "R.P. Adams"],
    "venue": "arXiv preprint arXiv:1511.02543,",
    "year": 2015
  }, {
    "title": "Measuring the reliability of mcmc inference with bidirectional monte carlo",
    "authors": ["R.B. Grosse", "S. Ancha", "D.M. Roy"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Learning deep latent gaussian models with markov chain monte carlo",
    "authors": ["M.D. Hoffman"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Stochastic variational inference",
    "authors": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Nonequilibrium equality for free energy differences",
    "authors": ["C. Jarzynski"],
    "venue": "Physical Review Letters,",
    "year": 1997
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-Encoding Variational Bayes",
    "authors": ["D. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Improving Variational Inference with Inverse Autoregressive Flow",
    "authors": ["D. Kingma", "T. Salimans", "R. Jozefowicz", "X. Chen", "I. Sutskever", "M. Welling"],
    "year": 2016
  }, {
    "title": "On the challenges of learning with inference networks on sparse, high-dimensional data",
    "authors": ["R.G. Krishnan", "D. Liang", "M. Hoffman"],
    "year": 2017
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "venue": "University of Toronto,",
    "year": 2009
  }, {
    "title": "Classification using discriminative restricted boltzmann machines",
    "authors": ["H. Larochelle", "Y. Bengio"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Annealed importance sampling",
    "authors": ["R. Neal"],
    "venue": "Statistics and Computing,",
    "year": 2001
  }, {
    "title": "Variational Inference with Normalizing Flows",
    "authors": ["D. Rezende", "S. Mohamed"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
    "authors": ["D. Rezende", "S. Mohamed", "D. Wierstra"],
    "year": 2014
  }, {
    "title": "On the quantitative analysis of deep belief networks",
    "authors": ["R. Salakhutdinov", "I. Murray"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Markov chain monte carlo and variational inference: Bridging the gap",
    "authors": ["T. Salimans", "D. Kingma", "M. Welling"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Ladder variational autoencoders",
    "authors": ["C.K. Sønderby", "T. Raiko", "L. Maaløe", "S.K. Sønderby", "O. Winther"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Improving Variational Auto-Encoders using Householder Flow",
    "authors": ["J.M. Tomczak", "M. Welling"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Improving Variational Auto-Encoders using convex combination linear Inverse Autoregressive Flow",
    "authors": ["J.M. Tomczak", "M. Welling"],
    "venue": "ArXiv e-prints,",
    "year": 2017
  }, {
    "title": "Two problems with variational expectation maximisation for time-series models. inference and learning in dynamic models",
    "authors": ["R. Turner", "M. Sahani"],
    "year": 2011
  }, {
    "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
    "authors": ["Y. Wu", "Y. Burda", "R. Salakhutdinov", "R. Grosse"],
    "year": 2017
  }, {
    "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning",
    "authors": ["H. Xiao", "K. Rasul", "R. Vollgraf"],
    "year": 2017
  }],
  "id": "SP:f56ccbb3e38c53aa03540b6456d3cfa21e2d505b",
  "authors": [{
    "name": "Chris Cremer",
    "affiliations": []
  }, {
    "name": "Xuechen Li",
    "affiliations": []
  }, {
    "name": "David Duvenaud",
    "affiliations": []
  }],
  "abstractText": "Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.",
  "title": "Inference Suboptimality in Variational Autoencoders"
}