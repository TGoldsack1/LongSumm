{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 725–731 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n725\nMost of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1"
  }, {
    "heading": "1 Introduction",
    "text": "Text summarization is to produce a brief summary of the main ideas of the text. Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries. Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder.\n1The code is available at https://github.com/ lancopku/superAE\nThe contents on the social media are long, and contain many errors, which come from spelling mistakes, informal expressions, and grammatical mistakes (Baldwin et al., 2013). Large amount of errors in the contents cause great difficulties for text summarization. As for RNN-based Seq2Seq, it is difficult to compress a long sequence into an accurate representation (Li et al., 2015), because of the gradient vanishing and exploding problem.\nCompared with the source content, it is easier to encode the representations of the summaries, which are short and manually selected. Since the source content and the summary share the same points, it is possible to supervise the learning of the semantic representation of the source content with that of the summary.\nIn this paper, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. First, we train an autoencoder, which inputs and reconstructs the summaries, to obtain a better representation to generate the summaries. Then, we supervise the internal representation of Seq2Seq with that of autoencoder by minimizing the distance between two representations. Finally, we use adversarial learning to enhance the supervision. Following the previous work (Ma et al., 2017), We evaluate our proposed model on a Chinese social media dataset. Experimental results show that our model outperforms the state-of-theart baseline models. More specifically, our model outperforms the Seq2Seq baseline by the score of 7.1 ROUGE-1, 6.1 ROUGE-2, and 7.0 ROUGE-L."
  }, {
    "heading": "2 Proposed Model",
    "text": "We introduce our proposed model in detail in this section."
  }, {
    "heading": "2.1 Notation",
    "text": "Given a summarization dataset that consists of N data samples, the ith data sample (xi, yi) con-\ntains a source content xi = {x1, x2, ..., xM}, and a summary yi = {y1, y2, ..., yL}, while M is the number of the source words, and L is the number of the summary words. At the training stage, we train the model to generate the summary y given the source content x. At the test stage, the model decodes the predicted summary y′ given the source content x."
  }, {
    "heading": "2.2 Supervision with Autoencoder",
    "text": "Figure 1 shows the architecture of our model. At the training stage, the source content encoder compresses the input contents x into the internal representation zt with a Bi-LSTM encoder. At the same time, the summary encoder compresses the reference summary y into the representation zs. Then both zt and zs are fed into a LSTM decoder to generate the summary. Finally, the semantic representation of the source content is supervised by the summary.\nWe implement the supervision by minimizing the distance between the semantic representations zt and zs, and this term in the loss function can be written as:\nLS = λ\nNh d(zt, zs) (1)\nwhere d(zt, zs) is a function which measures the distance between zs and zt. λ is a tunable hyperparameter to balance the loss of the supervision and the other parts of the loss, and Nh is the number of the hidden unit to limit the magnitude of the distance function. We set λ = 0.3 based on the performance on the validation set. The distance between two representations can be written as:\nd(zt, zs) = ‖zt − zs‖2 (2)"
  }, {
    "heading": "2.3 Adversarial Learning",
    "text": "We further enhance the supervision with the adversarial learning approach. As shown in Eq. 1, we use a fixed hyper-parameter as a weight to measure the strength of the supervision of the autoencoder. However, in the case when the source content and summary have high relevance, the strength of the supervision should be higher, and when the source content and summary has low relevance, the strength should be lower. In order to determine the strength of supervision more dynamically, we introduce the adversarial learning. More specifically, we regard the representation of the autoencoder as the “gold” representation, and that of the sequence-to-sequence as the “fake” representation. A model is trained to discriminate between the gold and fake representations, which is called a discriminator. The discriminator tries to identify the two representations. On the contrary, the supervision, which minimizes the distance of the representations and makes them similar, tries to prevent the discriminator from making correct predictions. In this way, when the discriminator can distinguish the two representations (which means the source content and the summary has low relevance), the strength of supervision will be decreased, and when the discriminator fails to distinguish, the strength of supervision will be improved.\nIn implementation of the adversarial learning, the discriminator objective function can be written as:\nLD(θD) =− logPθD(y = 1|zt) − logPθD(y = 0|zs)\n(3)\nwhere PθD(y = 1|z) is the probability that the discriminator identifies the vector z as the “gold” representation, while PθD(y = 0|z) is the probability that the vector z is identified as the “fake” representation, and θD is the parameters of the discrim-\ninator. When minimizing the discriminator objective, we only train the parameters of the discriminator, while the rest of the parameters remains unchanged.\nThe supervision objective to be against the discriminator can be written as:\nLG(θE) =− logPθD(y = 0|zt) − logPθD(y = 1|zs)\n(4)\nWhen minimizing the supervision objective, we only update the parameters of the encoders."
  }, {
    "heading": "2.4 Loss Function and Training",
    "text": "There are several parts of the objective functions to optimize in our models. The first part is the cross entropy losses of the sequence-to-sequence and the autoencoder:\nLSeq2seq = − N∑ i=1 pSeq2seq(yi|zs) (5)\nLAE = − N∑ i=1 pAE(yi|zt) (6)\nThe second part is the L2 loss of the supervision, as written in Equation 1. The last part is the adversarial learning, which are Equation 3 and Equation 4. The sum of all these parts is the final loss function to optimize.\nWe use the Adam (Kingma and Ba, 2014) optimization method to train the model. For the hyper-parameters of Adam optimizer, we set the learning rate α = 0.001, two momentum parameters β1 = 0.9 and β2 = 0.999 respectively, and = 1 × 10−8. We clip the gradients (Pascanu et al., 2013) to the maximum norm of 10.0."
  }, {
    "heading": "3 Experiments",
    "text": "Following the previous work (Ma et al., 2017), we evaluate our model on a popular Chinese social media dataset. We first introduce the datasets, evaluation metrics, and experimental details. Then, we compare our model with several state-of-the-art systems."
  }, {
    "heading": "3.1 Dataset",
    "text": "Large Scale Chinese Social Media Text Summarization Dataset (LCSTS) is constructed by Hu et al. (2015). The dataset consists of more than 2,400,000 text-summary pairs, constructed from a famous Chinese social media website called Sina Weibo.2 It is split into three parts, with 2,400,591\n2http://weibo.com\npairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set."
  }, {
    "heading": "3.2 Evaluation Metric",
    "text": "Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results."
  }, {
    "heading": "3.3 Experimental Details",
    "text": "The vocabularies are extracted from the training sets, and the source contents and the summaries share the same vocabularies. In order to alleviate the risk of word segmentation mistakes, we split the Chinese sentences into characters. We prune the vocabulary size to 4,000, which covers most of the common characters.\nWe tune the hyper-parameters based on the ROUGE scores on the validation sets. We set the word embedding size and the hidden size to 512, and the number of LSTM layers is 2. The batch size is 64, and we do not use dropout (Srivastava et al., 2014) on this dataset. Following the previous work (Li et al., 2017), we implement the beam search, and set the beam size to 10."
  }, {
    "heading": "3.4 Baselines",
    "text": "We compare our model with the following stateof-the-art baselines.\n• RNN and RNN-cont are two sequence-tosequence baseline with GRU encoder and decoder, provided by Hu et al. (2015). The difference between them is that RNN-context has attention mechanism while RNN does not.\n• RNN-dist (Chen et al., 2016) is a distractionbased neural model, which the attention\nmechanism focuses on the different parts of the source content.\n• CopyNet (Gu et al., 2016) incorporates a copy mechanism to allow parts of the generated summary are copied from the source content.\n• SRB (Ma et al., 2017) is a sequence-tosequence based neural model with improving the semantic relevance between the input text and the output summary.\n• DRGD (Li et al., 2017) is a deep recurrent generative decoder model, combining the decoder with a variational autoencoder.\n• Seq2seq is our implementation of the sequence-to-sequence model with the attention mechanism, which has the same experimental setting as our model for fair comparison."
  }, {
    "heading": "3.5 Results",
    "text": "For the purpose of simplicity, we denote our supervision with autoencoder model as superAE. We report the ROUGE F1 score of our model and the baseline models on the test sets.\nTable 1 summarizes the results of our superAE model and several baselines. We first compare our model with Seq2Seq baseline. It shows that our\nsuperAE model has a large improvement over the Seq2Seq baseline by 7.1 ROUGE-1, 6.1 ROUGE2, and 7.0 ROUGE-L, which demonstrates the efficiency of our model. Moreover, we compare our model with the recent summarization systems, which have been evaluated on the same training set and the test sets as ours. Their results are directly reported in the referred articles. It shows that our superAE outperforms all of these models, with a relative gain of 2.2 ROUGE-1, 1.8 ROUGE-2, and 2.0 ROUGE-L. We also perform ablation study by removing the adversarial learning component, in order to show its contribution. It shows that the adversarial learning improves the performance of 1.5 ROUGE-1, 0.7 ROUGE-2, and 1.0 ROUGE-L.\nWe also give a summarization examples of our model. As shown in Table 3, the SeqSeq model captures the wrong meaning of the source content, and produces the summary that “China United Airlines exploded in the airport”. Our superAE model captures the correct points, so that the generated summary is close in meaning to the reference summary."
  }, {
    "heading": "3.6 Analysis of text representation",
    "text": "We want to analyze whether the internal text representation is improved by our superAE model. Since the text representation is abstractive and hard to evaluate, we translate the representation into a sentiment score with a sentiment classifier, and evaluate the quality of the representation by means of the sentiment accuracy.\nWe perform experiments on the Amazon Fine Foods Reviews Corpus (McAuley and Leskovec, 2013). The Amazon dataset contains users’ rating labels as well as the summary for the reviews, making it possible to train a classifier to predict the sentiment labels and a seq2seq model to generate summaries. First, we train the superAE model and\nthe seq2seq model with the text-summary pairs until convergence. Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders. The classifier is a simple feedforward neural network which maps the representation into the label distribution. Finally, we compute the accuracy of the predicted 2-class labels and 5-class labels.\nAs shown in Table 2, the seq2seq model achieves 80.7% and 65.1% accuracy of 2-class and 5-class, respectively. Our superAE model outperforms the baselines with a large margin of 8.1% and 6.6%."
  }, {
    "heading": "4 Related Work",
    "text": "Rush et al. (2015) first propose an abstractive based summarization model, which uses an attentive CNN encoder to compress texts and a neural network language model to generate summaries. Chopra et al. (2016) explore a recurrent structure for abstractive summarization. To deal with out-of-vocabulary problem, Nallapati et al. (2016)\npropose a generator-pointer model so that the decoder is able to generate words in source texts. Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents. See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism. Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets. Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs. Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries.\nOur work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014). Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks. Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation. Neural attention model is first proposed by Bahdanau et al. (2014)."
  }, {
    "heading": "5 Conclusion",
    "text": "We propose a novel model, in which the autoencoder is a supervisor of the sequence-to-sequence model, to learn a better internal representation for abstractive summarization. An adversarial learning approach is introduced to further improve the supervision of the autoencoder. Experimental results show that our model outperforms the sequence-to-sequence baseline by a large margin, and achieves the state-of-the-art performances on a Chinese social media dataset."
  }, {
    "heading": "Acknowledgements",
    "text": "Our work is supported by National Natural Science Foundation of China (No. 61433015, No. 61673028), National High Technology Research and Development Program of China (863 Program, No. 2015AA015404), and the National Thousand Young Talents Program. Xu Sun is the corresponding author of this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Distraction-based neural networks",
    "authors": ["Hui Jiang"],
    "year": 2016
  }, {
    "title": "On using very large tar",
    "authors": ["Yoshua Bengio"],
    "year": 2015
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "A hierarchical neural autoencoder for paragraphs and documents",
    "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
    "year": 2015
  }, {
    "title": "Deep recurrent generative decoder for abstractive text summarization",
    "authors": ["Piji Li", "Wai Lam", "Lidong Bing", "Zihao Wang."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen,",
    "year": 2017
  }, {
    "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
    "authors": ["Chin-Yew Lin", "Eduard H. Hovy."],
    "venue": "Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-",
    "year": 2003
  }, {
    "title": "Autoencoder for words",
    "authors": ["Cheng-Yuan Liou", "Wei-Chen Cheng", "Jiun-Wei Liou", "Daw-Ran Liou."],
    "venue": "Neurocomputing, 139:84–96.",
    "year": 2014
  }, {
    "title": "Modeling word perception using the elman network",
    "authors": ["Cheng-Yuan Liou", "Jau-Chi Huang", "Wen-Chie Yang."],
    "venue": "Neurocomputing, 71(16-18):3150– 3157.",
    "year": 2008
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 1412–",
    "year": 2015
  }, {
    "title": "Query and output: Generating words by querying distributed word representations for paraphrase generation",
    "authors": ["Shuming Ma", "Xu Sun", "Wei Li", "Sujian Li", "Wenjie Li", "Xuancheng Ren."],
    "venue": "NAACL 2018.",
    "year": 2018
  }, {
    "title": "Improving semantic relevance for sequence-to-sequence learning of chinese social media text summarization",
    "authors": ["Shuming Ma", "Xu Sun", "Jingjing Xu", "Houfeng Wang", "Wenjie Li", "Qi Su."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association",
    "year": 2017
  }, {
    "title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews",
    "authors": ["Julian John McAuley", "Jure Leskovec."],
    "venue": "22nd International World Wide Web Conference, WWW ’13, Rio de Janeiro, Brazil, May 13-17, 2013, pages 897–",
    "year": 2013
  }, {
    "title": "Abstractive text summarization using sequence-tosequence rnns and beyond",
    "authors": ["Ramesh Nallapati", "Bowen Zhou", "Cı́cero Nogueira dos Santos", "Çaglar Gülçehre", "Bing Xiang"],
    "venue": "In Proceedings of the 20th SIGNLL Conference on Computational Natural",
    "year": 2016
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."],
    "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 1310–1318.",
    "year": 2013
  }, {
    "title": "MEAD - A platform for multidocument multilingual text summarization",
    "authors": ["Winkel", "Zhu Zhang."],
    "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC 2004.",
    "year": 2004
  }, {
    "title": "A neural attention model for abstractive sentence summarization",
    "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal,",
    "year": 2015
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, pages 1073–1083.",
    "year": 2017
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15(1):1929–1958.",
    "year": 2014
  }, {
    "title": "meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting",
    "authors": ["Xu Sun", "Xuancheng Ren", "Shuming Ma", "Houfeng Wang."],
    "venue": "ICML 2017, pages 3299–3308.",
    "year": 2017
  }, {
    "title": "Label embedding network: Learning label representation for soft training of deep networks",
    "authors": ["Xu Sun", "Bingzhen Wei", "Xuancheng Ren", "Shuming Ma."],
    "venue": "CoRR, abs/1710.10393.",
    "year": 2017
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Neural headline generation on abstract meaning representation",
    "authors": ["Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP",
    "year": 2016
  }, {
    "title": "Automatic generation of story highlights",
    "authors": ["Kristian Woodsend", "Mirella Lapata."],
    "venue": "ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565– 574.",
    "year": 2010
  }, {
    "title": "Dpgan: Diversity-promoting generative adversarial network for generating informative and diversified text",
    "authors": ["Jingjing Xu", "Xu Sun", "Xuancheng Ren", "Junyang Lin", "Binzhen Wei", "Wei Li."],
    "venue": "CoRR, abs/1802.01345.",
    "year": 2018
  }, {
    "title": "Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach",
    "authors": ["Jingjing Xu", "Xu Sun", "Qi Zeng", "Xiaodong Zhang", "Xuancheng Ren", "Houfeng Wang", "Wenjie Li."],
    "venue": "ACL 2018.",
    "year": 2018
  }],
  "id": "SP:cd93be9221f74512f13bbe338be3c65c73d4e9fc",
  "authors": [{
    "name": "Shuming Ma",
    "affiliations": []
  }, {
    "name": "Xu Sun",
    "affiliations": []
  }, {
    "name": "Junyang Lin",
    "affiliations": []
  }, {
    "name": "Houfeng Wang",
    "affiliations": []
  }],
  "abstractText": "Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1",
  "title": "Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization"
}