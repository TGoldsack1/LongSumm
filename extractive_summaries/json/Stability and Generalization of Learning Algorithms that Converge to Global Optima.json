{
  "sections": [{
    "heading": "1. Introduction",
    "text": "The recent success of training complex models at state-ofthe-art accuracy in many common machine learning tasks has sparked significant interest and research in algorithmic machine learning. In practice, not only can these complex deep neural models yield zero training loss, they can also generalize surprisingly well (Zhang et al., 2016; Lin & Tegmark, 2016). Although there has been significant\n1Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Wisconsin, USA. Correspondence to: Zachary Charles <zcharles@wisc.edu>, Dimitris Papailiopoulos <dimitris@ece.wisc.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nrecent work in analyzing the generalization capacity of various learning algorithms, our theoretical understanding of their generalization properties falls far below what has been observed empirically.\nA useful proxy for analyzing the generalization performance of learning algorithms is that of stability. A training algorithm is stable if small changes in the training set result in small differences in the output predictions of the trained model. In their foundational work, Bousquet & Elisseeff (2002) establish that stability begets generalization.\nWhile there has been stability analysis for empirical risk minimizers (Bousquet & Elisseeff, 2002; Mukherjee et al., 2006), there are far fewer results for commonly used iterative learning algorithms. In a recent novel work, Hardt et al. (2016) establish stability bounds for SGD, and discuss algorithmic heuristics that provably increase the stability of SGD models. Unfortunately, showing non-trivial stability for more involved algorithms like SVRG (Johnson & Zhang, 2013) (even in the convex case), or SGD in more nuanced non-convex setups is not straightforward. While Hardt et al. (2016) provide an elegant analysis that shows stability of SGD for non-convex loss functions, the result requires very small step-sizes. The step-size is small enough that under standard smoothness assumptions (Ghadimi & Lan, 2013), approximate convergence may require an exponential number of steps (see Appendix section B.3). Generally, there seems to be a trade-off between convergence and stability of algorithms. In this work we show that under certain geometric assumptions on the loss function around global minima, we can actually leverage the convergence properties of an algorithm to prove that it is stable.\nThe goal of this work is to provide black-box and easy-touse stability results for a variety of learning algorithms in non-convex settings. We show that this is in some cases possible by decoupling the stability of global minima and their proximity to models trained by learning algorithms.\nOur Contributions: We establish that models trained by algorithms that converge to global minima are stable under the Polyak-Łojasiewicz (PL) and the quadratic growth (QG) conditions (Karimi et al., 2016). Informally, these conditions assert that the suboptimality of a model is upper\nbounded by the norm of its gradient and lower bounded by its distance to the closest global minimizer. These are weaker conditions than considered in previous work, but still match several known stability bounds. For example, in (Hardt et al., 2016) the authors require convexity or strong convexity. Gonen & Shalev-Shwartz (2017) prove the stability of ERMs for non-convex but locally strongly convex loss functions obeying strict saddle inequalities. By contrast, we develop comparable stability results for a large class of non-convex functions. Although (Hardt et al., 2016) establishes the stability of SGD for smooth non-convex objectives, the step size selection can be prohibitively small for convergence. Our bounds make no assumptions on the hyper-parameters of the algorithm.\nWe use our black-box results to directly compare the generalization performance of popular first-order methods in general learning setups. While direct proofs of stability may require novel algorithm-specific analysis, our results are derived from known convergence rates of popular algorithms. For strong convexity (a special case of the PL condition), we recover order-wise the stability bounds of Hardt et al. (2016), but for a larger family of optimization algorithms (e.g., SGD, GD, SVRG, etc). We show that many of these algorithms offer order-wise similar stability as saddle-point avoiding algorithms in non-convex problems where all local minima are global (Gonen & Shalev-Shwartz, 2017).\nWe give examples of some machine learning scenarios where the PL condition mentioned above holds true. Adapting techniques from (Hardt & Ma, 2016), we show that deep networks with linear activation functions are PL almost everywhere in the parameter space. Our theory allows us to derive results similar to those in (Kawaguchi, 2016) about local/global minimizers in linear neural networks and reformulate them in terms fo the PL condition. We also show that 1-layer neural networks with leaky ReLU activations satisfy the PL condition.\nFinally, we show that while SGD and GD have analogous stability in the convex setting, this breaks down in the nonconvex setting. We give an explicit example of a simple 1-layer neural network on which SGD is stable but GD is not. Such an example was theorized in (Hardt et al., 2016) (see Figure 10 in that paper); here we formalize the authors’ intuition. Our results offer yet another indication that models trained via SGD generalize better than those trained by full-batch GD.\nPrior Work: The idea of stability analysis has been around for more than 30 years since Devroye & Wagner (1979). Bousquet & Elisseeff (2002) defined several notions of algorithmic stability and used them to derive bounds on generalization error. Further work has focused on stability of randomized algorithms (Elisseeff et al., 2005) and the\ninterplay between uniform convergence and generalization (Shalev-Shwartz et al., 2010). Mukherjee et al. (2006) show that stability implies consistency of empirical risk minimization. Shalev-Shwartz et al. (2010) show that stability can also imply learnability in some problems.\nHardt et al. (2016) establish stability bounds for stochastic gradient descent (SGD) in the convex, strongly convex, and non-convex case. Work by Lin et al. (2016) shows that stability of SGD can be controlled by forms of regularization. In (Kuzborskij & Lampert, 2017), the authors give stability bounds for SGD that are data-dependent. These bounds are smaller than those in (Hardt et al., 2016), but require assumptions on the underlying data. Liu et al. give a related notion of uniform hypothesis stability and show that it implies guarantees on the generalization error (Liu et al., 2017).\nStability is closely related to the notion of differential privacy (Dwork, 2006). Roughly speaking, differential privacy ensures that the probability of observing any outcome from a statistical query changes if you modify any single dataset element. It was later shown that differentially private algorithms generalize well (Dwork et al., 2015; Nissim & Stemmer, 2015)."
  }, {
    "heading": "2. Preliminaries",
    "text": "We first introduce some notation we will use in this paper. For w 2 Rm, we will let kwk denote the 2-norm of w. For a matrix A 2 Rn⇥m, we will let min(A) denote its minimum singular value, and kAk\nF\ndenote its Frobenius norm.\nLet S = {z1, . . . , zn} be a set of training data, where zi iid⇠ D. For a model w and a loss function `, let `(w; z) be the error of w on the training example z. We define the expected risk of a model w by R[w] := E\nz⇠D `(w; z). Since, we do not have access to the underlying distribution D optimizing R[w] directly is not possible. Instead, we will measure the empirical risk of a model w on a set S, given by:\nR S\n[w] := 1\nn\nnX\ni=1\n`(w; z i ).\nThe generalization of our model is measured by the generalization gap, ✏gen(w) := |RS [w] R[w]|.\nFor our purposes, w will be the output of some (potentially randomized) learning algorithm A, trained on some data set S. We will denote this output by A(S). Let S0 = {z1, . . . , zi 1, z0\ni\n, z i+1, . . . , zn}, where z0\ni ⇠ D. We then have the following notion of uniform stability that was first introduced in (Bousquet & Elisseeff, 2002). Definition 1 (Uniform Stability). An algorithm A is uniformly ✏-stable if for all data sets S, S0 differing in at most one example, sup\nz\nEA ⇥ `(A(S); z) `(A(S0); z)]  ✏.\nThe expectation is taken with respect to the randomness of the algorithm A. Bousquet and Elisseeff establish that uniform stability implies small generalization gap (Bousquet & Elisseeff, 2002). Theorem 1. Suppose A is uniformly ✏-stable. Then |E S,A ⇥ R S [A(S)] R[A(S)] ⇤ |  ✏.\nIn practice, uniform stability may be too restrictive, since the bound above must hold for all z, irrespective of its marginal distribution. The following notion of stability, while weaker, is still enough to control the generalization gap. Given a data set S = {z1, . . . , zn} and i 2 {1, . . . , n}, we define Si as S\\z\ni . Definition 2 (Pointwise Hypothesis Stability, Bousquet & Elisseeff (2002)). A has pointwise hypothesis stability with respect to a loss function ` if 8i 2 {1, . . . , n}, E S [|`(A(S); z i ) `(A(Si); z i )|]  .\nPointwise hypothesis stability is a weaker notion than uniform stability, but can still be used to establish non-trivial generalization bounds. Theorem 2 (Elisseeff et al. (2005)). Suppose A has pointwise hypothesis stability with respect to ` where 0  `(w; z)  M . For any , with probability at least 1 ,\nR[A(S)]  R S [A(S)] + r M2 + 12Mn\n2n .\nWhile this result was initially proved only for non-random algorithms, (Elisseeff et al., 2005) later extended this type of argument to random algorithms using similar notions of stability.\nIn the following, we derive stability bounds for models trained on empirical risk functions satisfying the PL and QG conditions. To do so, we will assume that the functions in question are L-Lipschitz. Definition 3. A function f : ⌦ ! R is L-Lipschitz if for all x1, x2 2 ⌦, |f(x1) f(x2)|  Lkx1 x2k.\nIf f is assumed to be differentiable, this is equivalent to saying that for all x, krf(x)k  L.\nIn recent work, Karimi et al. (2016) used the PolyakŁojasiewicz condition to prove simplified nearly-optimal convergence rates for several first-order methods. Notably, there are some non-convex functions that satisfy the PL condition. The condition is defined below. Definition 4 (Polyak-Łojasiewicz). Fix a set X and let f⇤ = min\nx2X f(x). Then f satisfies the PolyakŁojasiewicz (PL) condition with parameter µ > 0 on X if for all x 2 X , 12krf(x)k 2 µ(f(x) f⇤).\nWe often refer to such functions as µ-PL. Note that for PL functions, every critical point is a global minimizer.\nWhile strong convexity implies PL, the reverse is not true. Moreover, PL functions are in general non-convex (e.g., invex functions). We also consider a strictly larger family of functions that satisfy the quadratic growth condition.\nDefinition 5 (Quadratic Growth). A function f satisfies the quadratic growth (QG) condition on X with parameter µ > 0 if for all x 2 X , f(x) f⇤ µ2 kx xpk 2, where x p denotes the euclidean projection of x onto the set of global minimizers of f in X (i.e., x\np is the closest point to x in X satisfying f(x\np\n) = f⇤).\nWe often refer to such functions as µ-QG. Both of these conditions have been considered in previous studies. The PL condition was first introduced by Polyak in (Lojasiewicz, 1963), who showed that under this assumption, gradient descent converges linearly. The QG condition has been considered under various guises (Bonnans & Ioffe, 1995; Ioffe, 1994) and can imply important properties about the geometry of critical points. For example, Anitescu (2000) showed that local minima of non-linear programs satisfying the QG condition are actually isolated stationary points. These kinds of geometric implications will allow us to derive stability results for large classes of algorithms.\nIn general, the PL condition retains many properties of strong convexity (such as the fact that under the PL assumption, all critical points are local minima) without requiring convexity. The QG condition further relaxes this, allowing for critical points that are not global minima, while still enforcing that locally, the function grows quadratically away from global minima. Figure 1 gives examples of µ-strongly convex, µ-PL, and µ-QG functions for µ = 2."
  }, {
    "heading": "3. Stability of Approximate Global Minima",
    "text": "In this section, we establish the stability of large classes of learning algorithms under the PL and QG conditions presented above. Our stability results are “black-box” in the sense that our bounds are decomposed as a sum of two terms: a term concerning the convergence of the algorithm to a global minimizer, and a term relevant to the geometry of the loss function around the global minima. Both terms are used to establish good generalization and provide some insights into the way that learning algorithms perform.\nFor an algorithm A, let w S denote its output on S. The empirical training error on a data set S is denoted f\nS (w) and defined as f\nS (w) = 1|S| P\nz2S `(w; z). We assume that `(·, w) is L-Lipschitz w.r.t. w for all z."
  }, {
    "heading": "3.1. Pointwise Hypothesis Stability for PL/QG Loss Functions",
    "text": "We will show that if f S satisfies the PL or QG condition, we will be able to quantify the stability of A. Although these conditions may at first seem unnatural, we show in Section 4 that they arise in a large number of machine learning settings, including in certain deep linear neural networks. Let Xmin denote the set of global minima of fS . Theorem 3. Assume that for all S and w 2 X , f\nS is PL with parameter µ. We assume that applying A to f\nS produces output w\nS that is converging to some global minimizer w⇤\nS . Then A has pointwise hypothesis stability with parameter ✏stab satisfying the following conditions.\nCase 1: If for all S, kw S w⇤ S k  ✏A then ✏stab  2L✏A + 2L 2\nµ(n 1) .\nCase 2: If for all S, |f S (w S ) f S (w⇤ S )|  ✏0A then ✏stab  2L q 2✏0A µ + 2L2 µ(n 1) .\nCase 3: If for all S, krf S (w S )k  ✏00A, then ✏stab  2L✏ 00 A\nµ\n+\n2L2\nµ(n 1) .\nSuppose our loss functions are PL and our algorithm A is an oracle that returns a global optimizer w⇤\nS . Then the terms ✏A, ✏0A, ✏ 00 A above are all equal to 0, leading to the following corollary.\nCorollary 4. Suppose f S is PL with parameter µ and let A(S) = argmin\nw2X fS(w). Then, A has pointwise hypothesis stability with ✏stab = 2L 2\nµ(n 1) .\nBousquet & Elisseeff (2002) considered the stability of empirical risk minimizers where the loss function satisfied strong convexity. Their work implies that for -strongly convex functions, the empirical risk minimizer has stability satisfying ✏stab  L 2\nn . Since -strongly convex implies - PL, Corollary 6 generalizes their result, with only a constant factor loss.\nRemark 1. Theorem 3 holds even if we only have information about A in expectation. For example, if we only know that EAkwS w⇤\nS k  ✏A, we still establish pointwise hypothesis stability (in expectation with respect to A), with the same constant as above. This allows us to apply our result to randomized algorithms such as SGD where we are interested in the convergence in expectation.\nA similar result to Theorem 3 can be derived for empirical risk functions that satisfy the QG condition and that are\nrealizable, that is, where the global minimum of f , denoted f⇤, is 0. Theorem 5. Suppose that for all S, f\nS is QG with parameter µ and f⇤ = 0. Suppose that applying A to f\nS produces output w\nS that is converging to some global minimizer w⇤ S . Assume that for all w and z, |`(w; z)|  c. Then A has pointwise hypothesis stability with parameter ✏stab satisfying the following conditions.\nCase 1: If for all S, kw S w⇤ S k  ✏A then ✏stab  2L✏A + 2L q c\nµn\n.\nCase 2: If for all S, |f S (w S ) f S (w⇤ S )|  ✏0A then ✏stab  2L q 2✏0A µ + 2L q c µn .\nRemark 2. Observe that unlike the case of PL empirical losses, QG empirical losses only allow for a O( 1p\nn ) convergence rate of stability. Moreover, similarly to our result for PL loss functions, the result of Theorem 5 holds even if we only have information about the convergence of A in expectation.\nFinally, we can obtain the following Corollary for empirical risk minimizers. Corollary 6. Let f\nS satisfy the QG inequality with parameter µ and let A(S) = argmin\nw2X fS(w). Then, A has pointwise hypothesis stability with ✏stab = 2L q c\nµn\n."
  }, {
    "heading": "3.2. Uniform Stability for PL/QG Loss Functions",
    "text": "Under a more restrictive setup, we can obtain similar bounds for uniform hypothesis stability, which is a stronger stability notion compared to its pointwise hypothesis variant. The usefulness of uniform stability compared to pointwise stability, is that it can lead to generalization bounds that concentrate exponentially faster (Bousquet & Elisseeff, 2002) with respect to the sample size n.\nAs before, given a data set S, we let denote w S be the model that A outputs. Let ⇡\nS (w) denote the closest optimal point of f\nS to w. We will denote ⇡ S (w S ) by w⇤ S . Let S, S0 be data sets differing in at most one entry. We will make the following technical assumption: Assumption 1. The empirical risk minimizers for f\nS and f S 0 , i.e., w⇤ S , w⇤ S 0 satisfy ⇡ S (w⇤ S 0) = w⇤ S , where ⇡ S\n(w) is the projection of w on the set of empirical risk minimizers of f\nS . Note that this is satisfied if for every data set S, there is a unique minimizer w⇤\nS\n.\nRemark 3. The above assumption is relatively strict, and in general does not apply to empirical losses with infinitely many global minima. To tackle the existence of infinitely many global minima, one could imagine designing A(S) to output a structured empirical risk minimizer, e.g., one such that if A is applied on S0, its projection on the optima of\nf S would always yield back A(S). This could be possible if A(S) corresponds to minimizing a regularized, or structured cost function whose set of optimizers only contained a small subset of the global minima of f\nS . Unfortunately, coming up with such a structured empirical risk minimizer for general non-convex losses seems far from straightforward, and serves as an interesting open problem.\nTheorem 7. Assume that for all S, f S satisfies the PL condition with parameter µ, and suppose that Assumption 1 holds. Then A has uniform stability with parameter ✏stab satisfying the following conditions.\nCase 1: If for all S, kw S w⇤ S k  ✏A then ✏stab  2L✏A + 2L 2\nµn\n.\nCase 2: If for all S, |f S (w S ) f S (w⇤ S )|  ✏0A then ✏stab  2L q 2✏0A µ + 2L2 µn .\nCase 3: If for all S, krf S (w S )k  ✏00A, then ✏stab  2L✏ 00 A\nµ\n+\n2L2\nµn\n.\nSince strong convexity is a special case of PL, this theorem implies that if we run enough iterations of a convergent algorithm A on a -strongly convex loss function, then we obtain uniform stability on the order of ✏stab = L2/ n. In particular, this theorem recovers the stability estimates for ERMs and SGD applied to strongly convex functions proved in (Bousquet & Elisseeff, 2002) and (Hardt et al., 2016), respectively.\nIn order to make this result more generally applicable, we would like to extend the theorem to a larger class of functions than just globally PL functions. If we assume boundedness of the loss function, then we can derive a similar result for globally QG functions. This leads us to the following theorem:\nTheorem 8. Assume that for all S, f S satisfies the QG condition with parameter µ, moreover let Assumption 1 hold. Suppose that for all z and w 2 X , `(w; z)  c. Then A is uniformly stable with parameter ✏stab satisfying:\nCase 1: If for all S, kw S w⇤ S k  ✏A then ✏stab  2L✏A + 2L q c\nµn\n.\nCase 2: If for all S, |f S (w S ) f⇤ S |  ✏0A then ✏stab  2L q 2✏0A µ + 2L q c µn .\nRemark 4. By analogous reasoning to that in Remark 1, both Theorem 7 and Theorem 8 hold if you only have information about the output of A in expectation."
  }, {
    "heading": "4. PL Loss Functions in Practice",
    "text": "Strongly Convex Composed with Piecewise-Linear Functions: As the bounds above show, the PL and QG\nconditions are sufficient for algorithmic stability and therefore imply good generalization. In this section, we show that the PL condition actually arises in some interesting machine learning setups, including least squares minimization, strongly convex functions composed with piecewise linear functions, and neural networks with linear activation functions. A first step towards a characterization of PL loss functions was proved by Karimi et al. (2016), which established that the composition of a strongly-convex function and a linear function results in a loss that satisfies the PL condition.\nWe wish to generalize this result to piecewise linear activation functions. Suppose that : R ! R is defined by (z) = c1z, for z > 0 and (z) = c2z, forz  0. Here c i\n> 0. For a vector z 2 Rn, we denote by (z) 2 Rn the vector whose ith component is (z\ni ). Note that this encompasses leaky-ReLU functions. Following similar techniques to those in (Karimi et al., 2016), we get the following result showing that the composition of strongly convex functions with piecewise-linear functions are PL. The proof can be found in Appendix A.6.\nTheorem 9. Let g be strongly-convex with parameter , a leaky ReLU activation function with slopes c1 and c2, and X a matrix with minimum singular value min(X). Let c = min{|c1|, |c2|}. Then f(w) = g( (Xw)) is PL almost everywhere with parameter µ = min(X)2c2.\nIn particular, 1-layer neural networks with a squared error loss and leaky ReLU activations satisfy the PL condition. More generally, this holds for any piecewise-linear activation function with slopes {c\ni }k i=1. As long as each slope\nis non-zero and X is full rank, the result above shows that the PL condition is satisfied. This result is closely related to that of (Brutzkus et al., 2017), which shows that SGD converges to global minimizers on such networks when the data is linearly separable.\nLinear Neural Networks: The results above only concern one layer neural networks. Given the prevalence of deep networks, we would like to say something about the associated loss function. As it turns out, we can prove that a PL inequality holds in large regions of the parameter space for deep linear networks. Such networks have recently become popular as objects of analysis due to the non-convexity of their landscape. In many cases, all critical points are global minimizers (Kawaguchi, 2016). We will show that a similar theorem can be derived through the lens of the PL condition.\nSay we are given a training set S = {z1, . . . , zn} where z i = (x i , y i ) for x i , y i\n2 Rd. Our neural network will have ` fully-connected non-input layers, each with d neurons and linear activation functions. We will parametrize the neural network model via W1, . . . ,W`, where each Wi 2 Rd⇥d. That is, the output at the first non-input layer is\nu1 = W1x and the output at layer k 2 is Akuk 1. Letting X,Y 2 Rd⇥N be the matrices with x\ni , y i as their columns (respectively), we can then write our loss function as\nf(W ) = 1\n2\nkW ` W ` 1 . . .W1X Y k2\nF\n.\nLet W = W ` W ` 1 . . .W1. The optimal value of W is W ⇤ = Y X+. Here, X+ = XT (XXT ) 1 is the pseudoinverse of X . We assume that X 2 Rd⇥N has rank d so that XXT is invertible. We will also make use of the following lemma which we prove in Appendix A.7.\nLemma 10. Let W 2 Rd⇥d be some weight matrix. Then for C = k(XXT ) 1Xk2\nF\n, we have\nCk(WX Y )XT k2 F kWX Y k2 F kY X+X Y k2 F .\nFor a matrix A, let min(A) denote the smallest singular value of A. For a given W1, . . . ,W`, let W = W\n` W ` 1 . . .W1. In Appendix A.8, we prove the following\nlemma.\nLemma 11. Suppose that the W i satisfy min(Wi) ⌧ > 0 for all i. Then,\nkrf(W1, . . . ,W`)k2 F `⌧2` 2k(WX Y )XT k2 F .\nCombining Lemmas 10 and 11, we derive the following interesting corollary about when critical points are global minimizers. This result is not directly related to the work above, but gives an easy way to understand the landscape of critical points of deep linear networks.\nTheorem 12. Let (W1, . . . ,W`) be a critical point such that each W\ni has full rank. Then (W1, . . . ,W`) is a global minimizer of f .\nThematically similar results have been derived previously for 1 layer networks in (Xie et al., 2017) and for deep neural networks in (Zhou & Feng, 2017). In (Kawaguchi, 2016), Kawaguchi derives a similar result to ours for deep linear neural networks, showing that every critical point is either a global minima or a saddle point. Our result, by contrast, implies that all full-rank critical points are global minima.\nLemmas 10 and 11 can also be combined to show that linear networks satisfy the PL condition in large regions of parameter space, as the following theorem states.\nTheorem 13. Suppose our weight matrices (W1, . . . ,W`) satisfy min(Wi) ⌧ for ⌧ > 0. Then f(W1, . . . ,W`) satisfies the following PL inequality:\n1\n2\nkrfk2 F\n`⌧ 2` 2\nk(XXT ) 1Xk2 F\n(f(W1, . . . ,W`) f⇤)."
  }, {
    "heading": "5. Stability of Some First-order Methods",
    "text": "We wish to apply our bounds from the previous section to popular convergent gradient-based methods. We consider SGD, GD, RCD, and SVRG. When we have L-Lipschitz, µ-PL loss functions f\nS and n training examples, Theorem 7 states that any learning algorithm A has uniform stability ✏stab satisfying ✏stab  O(L p ✏A/µ) +O(L2/µn).\nHere, ✏A refers to how quickly A converges to the optimal value of the loss function. Specifically, this holds if the algorithm produces a model w\nS satisfying |f S (w s ) f⇤ S |  O(✏A).\nThe convergence rates of SGD, GD, RCD, and SVRG have been studied extensively both for strongly convex functions (Bubeck et al., 2015; Johnson & Zhang, 2013; Nesterov, 2012; 2013) and PL functions (Karimi et al., 2016). These results are summarized in Table 1. When necessary to state the result, we assume a constant step-size of . Note that the same convergence rates apply to both -strongly convex and -PL functions.\nWe wish to perform enough iterations of our algorithm guarantee to guarantee the same stability as SGD in the strongly convex case. We need to determine how many iterations T we need to perform such that ✏A = O(L2/µn), as then Corollary 6 implies that our algorithm is uniformly stable with parameter ✏stab = O(L2/µn). The number of iterations needed are summarized in Table 2.\nIn these settings, the above algorithms all exhibit the same stability for these values of T , despite the potential nonconvexity. This is not the case for general non-convex functions. Several studies have observed that small-batch SGD offers superior generalization performance compared to large-batch SGD, or full-batch GD, when training deep neural networks (Keskar et al., 2016).\nUnfortunately our bounds are not nuanced enough to capture the difference in generalization performance between minibatch and large-batch SGD. In Section 6, we will make this observation formal. Although SGD and GD can be equally\nstable for non-convex problems satisfying the PL condition, there exist non-convex problems where full-batch GD is not stable and SGD is stable."
  }, {
    "heading": "6. The Instability of Gradient Descent",
    "text": "In (Hardt et al., 2016), the authors proved bounds on the uniform stability of SGD. They also noted that GD does not appear to be provably as stable for the non-convex case and sketched a situation in which this difference would appear. Due to the similarity of SGD and GD, one may expect similar uniform stability. While this is true in the convex setting (as we show in the Appendix, subsection B.1), this breaks down in the non-convex setting. Below we construct an explicit example where GD is not uniformly stable, but SGD is. This example formalizes the intuition given in (Hardt et al., 2016).\nFor x,w 2 Rm and y 2 R, we let `(w; (x, y)) = (hw, xi2+ hw, xi y)2. Intuitively, this is a generalized quadratic model where the predicted label ŷ for a given x is given by ŷ = hw, xi2 + hw, xi.\nFor almost every x, y, the function `(w; (x, y)) is nonconvex as it is a quartic polynomial in the weight vector w. We will use this function to construct data sets S, S0 that differ in only one entry, for which GD produces significantly different models. We consider this loss function for all z = (x, y) with kzk  C for C sufficiently large. When m = 1, the loss function simplifies to `(w; (x, y)) = (w2x2 + wx y)2.\nDefine ↵ := ( 1, 1), := ( 12 , 1). The graphs of `(w; z) at ↵ and are given in Figure 2. We also graph g(w), defined by g(w) = 12 (`(w;↵) + `(w; )).\nNote that the last function has two distinct basins of different heights. Taking the gradient, one can show that the rightmost function in Figure 2 has zero slope at ŵ ⇡ 0.598004. Comparing `(w;↵) and `(w; ), we see that the sign of their slopes agrees on ( 12 , 1 2 ) and on (1, 3 2 ). The slopes are of different sign in the interval [ 12 , 1]. We will use this to our\n2\n, 1)) (top-right), and g(w) = 1\n2\n[`(w; ( 1, 1)) +\n`(w; ( 1\n2\n, 1))] (bottom).\nadvantage in showing that gradient descent is not stable, while that SGD is.\nWe will construct points (x1, y1) and (x2, y2) such that `(w; (x1, y1)) and `(w; (x2, y2)) have positive and negative slope at ŵ. To do so, we will first construct an example with a slope of zero at ŵ. A straightforward computation shows that `(w; ( 12ŵ , 0)) has slope zero at ŵ. A graph of this loss function is given in Figure 3. Note that ŵ corresponds to the concave-down critical point in between the two global minima. Define z± = ⇣ 1 2(ŵ±✏) , 0 ⌘ . Straightforward calculations show that `(w; (z+, 0)) will have positive slope for w 2 (0, ŵ + ✏), while `(w; (z , 0)) will have negative slope for w 2 (ŵ ✏, 2(ŵ ✏)). In particular, their slopes have opposite signs in the interval (ŵ ✏, ŵ + ✏). Define S = {z1, . . . , zn 1, z }, S0 = {z1, . . . , zn 1, z+}, where zi = ↵ for 1  i  n 12 , zi = for n 1 2 < i  n 1.\nBy construction, we have\nfS(w) = n 1 n g(w) + `\n✓ w; ✓ 1\n2(ŵ + ✏) , 0\n◆◆ .\nfS0(w) = n 1 n g(w) + `\n✓ w; ✓ 1 2(ŵ ✏) , 0 ◆◆ .\nThen f S (w), f S 0 (w) will approximately have the shape of the bottom function in Figure 2 above. However, recall that d\ndw g(w) = 0 at w = ŵ. Therefore, there is some , with 0 < < ✏, such that for all w 2 (ŵ , ŵ + ), d\ndw\nf S (w) < 0 < d dw f S 0 (w).\nNow say that we initialize gradient descent with some stepsize > 0 in the interval (w , w + ). The above equation implies that the first step of gradient descent on f\nS\nwill produce a step moving to the left, but a step moving to the right for f\nS 0 . This will hold for all > 0. Moreover, the iterations for f\nS will continue to move to towards the left basin, while the iterations for f\nS 0 will continue to move to the right basin since `(w; (z+, 0)) has positive slope for w 2 (0, ŵ + ✏) while `(w; (z , 0)) has negative slope for w 2 (ŵ ✏, 2(ŵ ✏), and that g(w) has positive slope for w 2 ( 12 , ŵ) and negative slope for w 2 (ŵ, 3 2 ).\nAfter enough iterations of gradient descent on f S , f S 0 , we will obtain models w\nS and w S 0 that are close to the distinct local minima in the right-most graph in Figure 2. This will hold as long as is not too large. To ensure this does not happen, we restrict to  1.\nLet w1 < w2 denote the two local minima of g(w). For z⇤ = ( 12 , 1), plugging these values into `(w; z ⇤ ) shows that |`(w1; z⇤) `(w2; z⇤)| > 1. Since wS is close to w1 and w\nS 0 is close to w2, we get the following theorem. Theorem 14. For all n,m 1, 2 (0, 1], there are K 1, S, S0 ✓ Rm of size n with |S \\ S0| = n 1, and a nonzero measure A ✓ Rm such that if we perform at least K iterations of gradient descent with step-size on S and S0, starting in A, to get outputs A(S),A(S0), then there is a z⇤ such that |`(A(S); z⇤) `(A(S0); z⇤)| 12 .\nThis theorem establishes that there exist simple non-convex settings for which the uniform stability of gradient descent does not decrease with n. In light of the work in (Hardt et al., 2016), where the authors show that for very conservative step-sizes, SGD is stable on non-convex loss function, we might wonder whether SGD is stable in this setting. We show in Section B.2 that SGD is stable in this setting. For simplicity of analysis, we focus on the case where = 1. In this section we prove the following theorem. Theorem 15. Suppose that we initialize SGD in [ŵ ⌘, ŵ+ ⌘] with a step-size of = 1. Let A(S),A(S0) denote the output of SGD after k iteration for sufficiently large k. For kzk  2,EA[`(A(S); z) `(A(S0); z)]  O( 1\nn\n).\nThis is in contrast to gradient descent, which is unstable in this setting. While (Zhang et al., 2016) suggests that SGD is generally more stable in non-convex settings, proving that this holds remains an open problem."
  }, {
    "heading": "7. Conclusion",
    "text": "The success of machine learning algorithms in practice is often dictated by their ability to generalize. While recent work has developed great insight into the training error of machine learning algorithms, much less is understood about their generalization error. Most prior work has been algorithm-specific or has made strong assumptions on the loss function that may not hold in practice. By decomposing stability into convergence and the geometry of global minimizers, we are able to derive broader results. These easy-to-use stability results encompass a general class of non-convex functions, some of which appear in machine learning setups. Our bounds establish the stability for SGD, GD, SVRG, and RCD and match prior specialized results. Although our bounds are not nuanced enough to compare the generalization of mini-batch and large-batch SGD, we hope that the generality of our bounds serves as a step towards understanding the generalization performance of practical machine learning algorithms.\nThere are still many exciting open problems concerning the stability and generalization of machine learning and optimization algorithms. We give a few below.\nStability for non-convex loss functions: While our results establish the stability of learning algorithms in some nonconvex scenarios, it is unclear how to extend them directly to more general non-convex loss functions. Due to the wide variety of similar but not identical algorithms in machine learning, it would be particularly interesting to derive black-box results on the stability of learning algorithms for general non-convex loss functions, even when it concerns convergence to approximate local minima. In non-convex functions, local minima are not necessarily global. The question remains, among all local minima of a loss function, which one has the smallest generalization gap? The local minima with the smallest generalization error may not be a global minimizer. Even if we restrict to global minimizers, these may have different generalization errors. Is there a simple geometric characterization of their generalization error?\nGeneralization of SGD vs GD: We showed above that there are settings in which gradient descent is not uniformly stable, but SGD is. Empirically, SGD leads to small generalization error in neural networks (Zhang et al., 2016). Theoretically, it is unclear how widespread this phenomenon is. Does SGD actually lead to more generalizable models than gradient descent?"
  }],
  "year": 2018,
  "references": [{
    "title": "Degenerate nonlinear programming with a quadratic growth condition",
    "authors": ["M. Anitescu"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2000
  }, {
    "title": "Second-order sufficiency and quadratic growth for nonisolated minima",
    "authors": ["J.F. Bonnans", "A. Ioffe"],
    "venue": "Mathematics of Operations Research,",
    "year": 1995
  }, {
    "title": "Stability and generalization",
    "authors": ["O. Bousquet", "A. Elisseeff"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2002
  }, {
    "title": "Sgd learns over-parameterized networks that provably generalize on linearly separable data",
    "authors": ["A. Brutzkus", "A. Globerson", "E. Malach", "S. ShalevShwartz"],
    "venue": "arXiv preprint arXiv:1710.10174,",
    "year": 2017
  }, {
    "title": "Convex optimization: Algorithms and complexity",
    "authors": ["S Bubeck"],
    "venue": "Foundations and Trends® in Machine Learning,",
    "year": 2015
  }, {
    "title": "Distribution-free performance bounds for potential function rules",
    "authors": ["L. Devroye", "T. Wagner"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1979
  }, {
    "title": "Differential privacy. In 33rd International Colloquium on Automata, Languages and Programming, part II (ICALP",
    "authors": ["C. Dwork"],
    "year": 2006
  }, {
    "title": "Preserving statistical validity in adaptive data analysis",
    "authors": ["C. Dwork", "V. Feldman", "M. Hardt", "T. Pitassi", "O. Reingold", "A.L. Roth"],
    "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
    "year": 2015
  }, {
    "title": "Stability of randomized learning algorithms",
    "authors": ["A. Elisseeff", "T. Evgeniou", "M. Pontil"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2005
  }, {
    "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
    "authors": ["S. Ghadimi", "G. Lan"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2013
  }, {
    "title": "Fast rates for empirical risk minimization of strict saddle problems",
    "authors": ["A. Gonen", "S. Shalev-Shwartz"],
    "venue": "arXiv preprint,",
    "year": 2017
  }, {
    "title": "Identity matters in deep learning",
    "authors": ["M. Hardt", "T. Ma"],
    "venue": "CoRR, abs/1611.04231,",
    "year": 2016
  }, {
    "title": "Train faster, generalize better: Stability of stochastic gradient descent",
    "authors": ["M. Hardt", "B. Recht", "Y. Singer"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "On sensitivity analysis of nonlinear programs in banach spaces: the approach via composite unconstrained optimization",
    "authors": ["A. Ioffe"],
    "venue": "SIAM Journal on Optimization,",
    "year": 1994
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["R. Johnson", "T. Zhang"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition, pp. 795–811",
    "authors": ["H. Karimi", "J. Nutini", "M. Schmidt"],
    "year": 2016
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["K. Kawaguchi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
    "authors": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"],
    "venue": "arXiv preprint,",
    "year": 2016
  }, {
    "title": "Data-dependent stability of stochastic gradient descent",
    "authors": ["I. Kuzborskij", "C. Lampert"],
    "venue": "arXiv preprint,",
    "year": 2017
  }, {
    "title": "Why does deep and cheap learning work so well",
    "authors": ["H.W. Lin", "M. Tegmark"],
    "venue": "arXiv preprint,",
    "year": 2016
  }, {
    "title": "Generalization properties and implicit regularization for multiple passes sgm",
    "authors": ["J. Lin", "R. Camoriano", "L. Rosasco"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Algorithmic stability and hypothesis complexity",
    "authors": ["T. Liu", "G. Lugosi", "G. Neu", "D. Tao"],
    "venue": "arXiv preprint,",
    "year": 2017
  }, {
    "title": "A topological property of real analytic subsets",
    "authors": ["S. Lojasiewicz"],
    "venue": "Coll. du CNRS, Les équations aux dérivées partielles,",
    "year": 1963
  }, {
    "title": "Efficiency of coordinate descent methods on huge-scale optimization problems",
    "authors": ["Y. Nesterov"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2012
  }, {
    "title": "Introductory lectures on convex optimization: A basic course, volume 87",
    "authors": ["Y. Nesterov"],
    "venue": "Springer Science & Business Media,",
    "year": 2013
  }, {
    "title": "On the generalization properties of differential privacy",
    "authors": ["K. Nissim", "U. Stemmer"],
    "venue": "CoRR, abs/1504.05800,",
    "year": 2015
  }, {
    "title": "Learnability, stability and uniform convergence",
    "authors": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Diverse neural network learns true target functions",
    "authors": ["B. Xie", "Y. Liang", "L. Song"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "venue": "arXiv preprint,",
    "year": 2016
  }, {
    "title": "The landscape of deep learning algorithms",
    "authors": ["P. Zhou", "J. Feng"],
    "venue": "arXiv preprint,",
    "year": 2017
  }],
  "id": "SP:f194d76ca4f2ac3243fd84f4c3c8adbb353cfc7a",
  "authors": [{
    "name": "Zachary Charles",
    "affiliations": []
  }, {
    "name": "Dimitris Papailiopoulos",
    "affiliations": []
  }],
  "abstractText": "We establish novel generalization bounds for learning algorithms that converge to global minima. We derive black-box stability results that only depend on the convergence of a learning algorithm and the geometry around the minimizers of the empirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Łojasiewicz (PL) and the quadratic growth (QG) conditions, which we show arise for 1-layer neural networks with leaky ReLU activations and deep neural networks with linear activations. We use our results to establish the stability of first-order methods such as stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or improve state-of-the-art generalization bounds and can easily extend to similar optimization algorithms. Finally, although our results imply comparable stability for SGD and GD in the PL setting, we show that there exist simple quadratic models with multiple local minima where SGD is stable but GD is not.",
  "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima"
}