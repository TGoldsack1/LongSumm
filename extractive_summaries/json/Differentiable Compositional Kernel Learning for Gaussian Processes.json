{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Gaussian processes (GPs) are a powerful and widely used class of models due to their nonparametric nature, explicit representation of posterior uncertainty, and ability to flexibly model a variety of structures in data. However, patterns of generalization in GP depend heavily on the choice of kernel function (Rasmussen, 1999); different kernels can impose widely varying modeling assumptions, such as smoothness, linearity, or periodicity. Capturing appropriate kernel structures can be crucial for interpretability and extrapolation (Duvenaud et al., 2013; Wilson & Adams, 2013). Even for experts, choosing GP kernel structures remains a dark art.\nGPs’ strong dependence on kernel structures has motivated work on automatic kernel learning methods. Sometimes this\n1Department of Computer Science, University of Toronto, Toronto, ON, CA. 2Vector Institute. 3Uber Advanced Technologies Group, Toronto, ON, CA. Correspondence to: Shengyang Sun <ssy@cs.toronto.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ncan be done by imposing a specific kind of structure: e.g., Bach (2009); Duvenaud et al. (2011) learned kernel structures which were additive over subsets of the variables. A more expressive space of kernels is spectral mixtures (Wilson & Adams, 2013; Kom Samo & Roberts, 2015; Remes et al., 2017), which are based on spectral domain summations. For example, spectral mixture (SM) kernels (Wilson & Adams, 2013) approximate all stationary kernels using Gaussian mixture models in the spectral domain. Deep kernel learning (DKL) (Wilson et al., 2016) further boosted the expressiveness by transforming the inputs of spectral mixture base kernel with a deep neural network. However, the expressiveness of DKL still depends heavily on the kernel placed on the output layer.\nIn another line of work, Duvenaud et al. (2013) defined a context-free grammar of kernel structures based on the composition rules for kernels. Due to its compositionality, this grammar could express combinations of properties such as smoothness, linearity, or periodicity. They performed a greedy search over this grammar to find a kernel struture which matched the input data. Using the learned structures, they were able to produce sensible extrapolations and interpretable decompositions for time series datasets. Lloyd et al. (2014) extended this work to an Automatic Statistician which automatically generated natural language reports. All of these results depended crucially on the compositionality of the underlying space. The drawback was that discrete search over the kernel grammar is very expensive, often requiring hours of computation even for short time series.\nIn this paper, we propose the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The network’s first layer units represent primitive kernels, including those used by the Automatic Statistician. Subsequent layers are based on the composition rules for kernels, so that each intermediate unit is itself a valid kernel. The NKN can compactly approximate the kernel structures from the Automatic Statistician grammar, but is fully differentiable, so that the kernel structures can be learned with gradient-based optimization. To illustrate the flexibility of our approach, Figure 1 shows the result of fitting an NKN to model a 2-D function; it is able to extrapolate sensibly.\nWe analyze the NKN’s expressive power for various choices of primitive kernels. We show that the NKN can represent nonnegative polynomial functions of its primitive kernels, and from this demonstrate universality for the class of stationary kernels. Our universality result holds even if the width of the network is limited, analogously to Sutskever & Hinton (2008). Interestingly, we find that the network’s representations can be made significantly more compact by allowing its units to represent complex-valued kernels, and taking the real component only at the end.\nWe empirically analyze the NKN’s pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure. The NKN produces sensible extrapolations on both 1-D time series datasets and 2-D textures. It outperforms competing approaches on regression benchmarks. In the context of Bayesian optimization, it is able to optimize black-box functions more efficiently than generic smoothness kernels."
  }, {
    "heading": "2. Background",
    "text": ""
  }, {
    "heading": "2.1. Gaussian Process Regression",
    "text": "A Gaussian process (GP) defines a distribution p(f) over functions X ! R for some domain X . For any finite set {x1, ...,xn} ⇢ X , the function values f = (f(x1), f(x2), ..., f(xn)) have a multivariate Gaussian distribution. Gaussian processes are parameterized by a mean function µ(·) and a covariance function or kernel function k(·, ·). The marginal distribution of function values is given by\nf ⇠ N (µ,K XX ), (1) where K\nXX denotes the matrix of k(x i ,x j ) for all (i, j).\nAssume we are given a set of training input-output pairs, D = {(x\ni , y i )}n i=1 = (X,y), and each target yn is gener-\nated from the corresponding f(x n ) by adding independent Gaussian noise; i.e.,\ny n = f(x n ) + ✏ n , ✏ n\n⇠ N (0, 2) (2)\nAs the prior on f is a Gaussian process and the likelihood is Gaussian, the posterior on f is also Gaussian. We can use\nthis to make predictions p(y⇤|x⇤,D) in closed form:\np(y⇤|x⇤,D) = N (µ⇤, 2⇤) µ⇤ = K⇤X(KXX + 2 I) 1 y\n2⇤ = K⇤⇤ K⇤X(KXX + 2I) 1KX⇤ + 2 (3)\nHere we assume zero mean function for f . Most GP kernels have several hyperparameters ✓ which can be optimized jointly with to maximize the log marginal likelihood,\nL(✓) = ln p(y|0,K XX + 2I) (4)"
  }, {
    "heading": "2.2. Bochner’s Theorem",
    "text": "Gaussian Processes depend on specifying a kernel function k(x, x0), which acts as a similarity measure between inputs.\nDefinition 1. Let X be a set, and k be a conjugate symmetric function k : X ⇥ X ! C is a positive definite kernel if 8x\n1 , · · ·, x n 2 X and 8c 1 , · · ·, c n 2 C, nX\ni,j=1\nc\ni\nc\nj k(x i , x j ) 0, (5)\nwhere the bar denotes the complex conjugate. Bochner’s Theorem (Bochner, 1959) establishes a bijection between complex-valued stationary kernels and positive finite measures using Fourier transform, thus providing an approach to analyze stationary kernels in the spectral domain (Wilson & Adams, 2013; Kom Samo & Roberts, 2015).\nTheorem 1. (Bochner) A complex-valued function k on Rd is the covariance function of a weakly stationary mean square continuous complex-valued random process on Rd if and only if it can be represented as\nk(⌧ ) =\nZ\nRP exp(2⇡iw>⌧ ) (dw) (6)\nwhere is a positive and finite measure. If has a density S(w), then S is called the spectral density or power spectrum of k. S and k are Fourier duals."
  }, {
    "heading": "2.3. Automatic Statistician",
    "text": "For compositional kernel learning, the Automatic Statistician (Lloyd et al., 2014; Duvenaud et al., 2013) used a compositional space of kernels defined as sums and products of a small number of primitive kernels. The primitive kernels included:\n• radial basis functions, corresponding to smooth functions. RBF(x,x0) = 2 exp( kx x\n0k2 2l2 )\n• periodic. PER(x,x0) = 2 exp( 2 sin 2(⇡kx x0k/p)\nl\n2 )\n• linear kernel. LIN(x,x0) = 2x>x0\n• rational quadratic, corresponding to functions with multiple scale variations. RQ(x,x0) = 2(1+ kx x\n0k2 2↵l2 ) 1 ↵\n• white noise. WN(x,x0) = 2 x,x 0 • constant kernel. C(x,x0) = 2\nThe Automatic Statistician searches over the compositional space based on three search operators.\n1. Any subexpression S can be replaced with S + B, where B is any primitive kernel family. 2. Any subexpression S can be replaced with S ⇥ B, where B is any primitive kernel family. 3. Any primitive kernel B can be replaced with any other primitive kernel family B0.\nThe search procedure relies on a greedy search: at every stage, it searches over all subexpressions and all possible operators, then chooses the highest scoring combination. To score kernel families, it approximates the marginal likelihood using the Bayesian information criterion (Schwarz et al., 1978) after optimizing to find the maximumlikelihood kernel parameters."
  }, {
    "heading": "3. Neural Kernel Networks",
    "text": "In this section, we introduce the Neural Kernel Network (NKN), a neural net which computes compositional kernel structures and is end-to-end trainable with gradient-based optimization. The input to the network consists of two vectors x1,x2 2 Rd, and the output k(x1,x2) 2 R (or C) is the kernel value. Our NKN architecture is based on well-known composition rules for kernels: Lemma 2. For kernels k1, k2\n• For 1, 2 2 R+, 1k1 + 2k2 is a kernel.\n• The product k1k2 is a kernel.\nWe design the architecture such that every unit of the network computes a kernel, although some of those kernels may be complex-valued."
  }, {
    "heading": "3.1. Architecture",
    "text": "The first layer of the NKN consists of a set of primitive kernels. Subsequent layers alternate between linear combinations and products. Since the space of kernels is closed under both operations, each unit in the network represents a kernel. Linear combinations and products can be seen as OR-like and AND-like operations, respectively; this is a common pattern in neural net design (LeCun et al., 1989; Poon & Domingos, 2011). The full architecture is illustrated in Figure 2.\nPrimitive kernels. The first layer of the network consists of a set of primitive kernel families with simple functional forms. While any kernels can be used here, we use the RBF, PER, LIN, and RQ kernels from the Automatic Statistician (see Section 2.3) because these express important structural motifs for GPs. Each of these kernel families has an associated set of hyperparameters (such as lengthscales or variances), and instantiating the hyperparameters gives a kernel. These hyperparameters are treated as parameters (weights) in this layer of the network, and are optimized with the rest of the network. Note that it may be advantageous to have multiple copies of each primitive kernel so that they can be instantiated with different hyperparameters.\nLinear layers. The Linear layer closely resembles a fully connected layer in deep neural networks, with each layer h\nl\n= W\nl\nh l 1 representing a nonnegative linear combination of units in the previous layer (i.e. W\nl is a nonnegative matrix). In practice, we use the parameterization\nW\nl\n= log(1 + exp(A\nl )) to enforce the nonnegativity constraint. (Here, exp is applied elementwise.)\nThe Linear layer can be seen as a OR-like operation: two points are considered similar if either kernel has a high value, while the Linear layer further controls the balance using trainable weights.\nProduct layers. The Product layer introduces multiplication, in that each unit is the product of several units in the previous layer. This layer has a fixed connectivity pattern and no trainable parameters. While this fixed structure may appear restrictive, Section 3.3 shows that it does not restrict the expressiveness of the network.\nThe Product layer can be seen as an AND-like operation: two points are considered similar if both constituent kernels have large values.\nActivation functions. Analogously to ordinary neural nets, each layer may also include a nonlinear activation function, so that h\nl = f(z l ), where z l , the pre-activations, are the result of a linear combination or product. However, f must be selected with care in order to ensure closure of the kernels. Polynomials with positive coefficients, as well as the exponential function f(z) = ez , fulfill this requirement.\nComplex-valued kernels. Allowing units in NKN to represent complex-valued kernels as in Definition 1 and take the real component only at the end, can make the network’s representations significantly more compact. As complexvalued kernels also maintain closure under summation and multiplication (Yaglom, 2012), additional modifications are unnecessary. In practice, we can include exp(iµ>⌧ ) in our primitive kernels."
  }, {
    "heading": "3.2. Learning",
    "text": "Optimization. All trainable parameters can be grouped into two categories: (1) parameters of primitive kernels, e.g., lengthscale in an RBF kernel; (2) parameters of Linear layers. We jointly learn these parameters by maximizing the marginal likelihood L(✓). Since the NKN architecture is differentiable, we can jointly fit all parameters using gradient-based optimization.\nComputational Cost. NKN introduces small computational overhead. Suppose we have N data points and m connections in the NKN; the computational cost of the forward pass is O(N2m). Note that a moderately-sized NKN, as we used in our experiments1, has only tens of parameters, and the main computational bottleneck in training lies in inverting kernel matrix, which is an O(N3) operation; therefore, NKN incurs only small per-iteration overhead compared to ordinary GP training.\n1In our experiments, we found 1 or 2 modules work very well. But it might be advantageous to use more modules in other tasks."
  }, {
    "heading": "3.3. Universality",
    "text": "In this section, we analyze the expressive power of the NKN, and in particular its ability to approximate arbitrary stationary kernels. Our analysis provides insight into certain design decisions for the NKN: in particular, we show that the NKN can approximate some stationary kernels much more compactly if the units of the network are allowed to take complex values. Furthermore, we show that the fixed structure of the product layers does not limit what the network can represent. Definition 2. For kernels {k\nj }n j=1, a kernel k is positive-\nweighted polynomial (PWP) of these kernels if 9T 2 N and {w\nt , {p tj }n j=1|wi 2 R+, ptj 2 N}Tt=0, such that\nk(x, y) = TX\nt=1\nw t\nnY\nj=1\nk ptj\nj\n(7)\nholds for all x, y 2 R. Its degree is max t\nP n\nj=1 ptj .\nComposed of summation and multiplication, the NKN naturally forms a positive-weighted polynomial of primitive kernels. Although NKN adopts a fixed multiplication order in the Product layer, the following theorem shows that this fixed architecture doesn’t undermine NKN’s expressiveness (proof in Appendix D). Theorem 3. Given B primitive kernels,\n• An NKN with width 2B + 6 can represent any PWP of primitive kernels.\n• An NKN with width 2Bp+1 and p Linear-Product modules can represent any PWP with degree no more than 2 p.\nInterestingly, NKNs can sometimes approximate (realvalued) kernels more compactly if the hidden units are allowed to represent complex-valued kernels, and the real part is taken only at the end. In particular, we give an example of a spectral mixture kernel class which can be represented with an NKN with a single complex-valued primitive kernel, but whose real-valued NKN representation requires a primitive kernel for each mixture component (proof in Appendix E).\nExample 1. Define a d-dimensional spectral mixture kernel with n+1 components, k⇤(⌧ ) = n+1P t=1 n 2 2t cos(4 t 1 > ⌧ ). Then 9✏ > 0, such that 8{µ t }n t=1, and any PWP of {cos(µ> t ⌧ )}n t=1 denoted as ¯k,\nmax ⌧2Rd |¯k(⌧ ) k⇤(⌧ )| > ✏ (8)"
  }, {
    "heading": "In contrast, k⇤ can be represented as the real part of a PWP",
    "text": "of only one complex-valued primitive kernel ei1 > ⌧ ,\nk⇤(⌧ ) = <{ n+1X\nt=1\n✓ n\n2\n◆2t [ei1 > ⌧ ] 4t} (9)\nWe find that an NKN with small width can approximate any complex-valued stationary kernel, as shown in the following theorem (Proof in Appendix F).\nTheorem 4. For any d-dimensional complex-valued stationary kernel k⇤ and ✏ 2 R+, 9{\nj }d j=1, {µj}2dj=1, and an\nNKN ¯k with primitive kernels {exp( 2⇡2k⌧ j k2)}d j=1, {exp(iµ> j ⌧ )}2d j=1, and width no more than 6d+6, such that\nmax ⌧2Rd |¯k(⌧ ) k⇤(⌧ )| < ✏ (10)\nBeyond approximating stationary kernels, NKN can also capture non-stationary structure by incorporating nonstationary primitive kernels. In Appendix G, we prove that with the proper choice of primitive kernels, NKN can approximate a broad family of non-stationary kernels called generalized spectral kernels (Kom Samo & Roberts, 2015)."
  }, {
    "heading": "4. Related Work",
    "text": "Additive kernels (Duvenaud et al., 2011) are linear combinations of kernels over individual dimensions or groups of dimensions, and are a promising method to combat the curse of dimensionality. While additive kernels need an exponential number of multiplication terms in the input dimension, hierarchical kernel learning (HKL) (Bach, 2009) presents a similar kernel except selecting only a subset to get a polynomial number of terms. However, this subset selection imposes additional optimization difficulty.\nBased on Bochner’s theorem, there is another a line of work on designing kernels in the spectral domain, including sparse spectrum kernels (SS) (Lázaro-Gredilla et al., 2010); spectral mixture (SM) kernels (Wilson & Adams, 2013); generalized spectral kernels (GSK) (Kom Samo & Roberts, 2015) and generalized spectral mixture (GSM) kernels (Remes et al., 2017). Though these approaches often extrapolate sensibly, capturing complex covariance structure may require a large number of mixture components.\nThe Automatic Statistician (Duvenaud et al., 2013; Lloyd et al., 2014; Malkomes et al., 2016) used a compositional grammar of kernel structures to analyze datasets and provide natural language reports. In each stage, it considered all production rules and used the one that resulted in the largest log-likelihood improvement. Their model showed\ngood extrapolation for many time series tasks, attributed to the recovery of underlying structure. However, it relied on greedy discrete search over kernel and operator combinations, making it computational expensive, even for small time series datasets.\nThere have been several attempts (Hinton & Salakhutdinov, 2008; Wilson et al., 2016) to combine neural networks with Gaussian processes. Specifically, they used a fixed kernel structure on top of the hidden representation of a neural network. This is complementary to our work, which focuses on using neural networks to infer the kernel structure itself. Both approaches could potentially be combined.\nInstead of represeting kernel parametrically, Oliva et al. (2016) modeled random feature dimension with stick breaking prior and Tobar et al. (2015) generated functions as the convolution between a white noise process and a linear filter drawn from GP. These approaches offer much flexibility but also incur challenges in training."
  }, {
    "heading": "5. Experiments",
    "text": "We conducted a series of experiments to measure the NKN’s predictive ability in several settings: time series, regression benchmarks, and texture images. We focused in particular on extrapolation, since this is a strong test of whether it has uncovered the underlying structure. Furthermore, we tested the NKN on Bayesian Optimization, where model structure and calibrated uncertainty can each enable more efficient exploration. Code is available at git@github.com: ssydasheng/Neural-Kernel-Network.git"
  }, {
    "heading": "5.1. Time Series Extrapolation",
    "text": "We first conducted experiments time series datasets to study extrapolation performance. For all of these experiments, as well as the 2-d experiment in Figure 1, we used the same NKN architecture and training setup (Appendix J.1).\nWe validated the NKN on three time series datasets introduced by Duvenaud et al. (2013): airline passenger volume (Airline), Mauna Loa atmospheric CO2 concentration (Mauna), and solar irradiance (Solar). Our focus is on extrapolation, since this is a much better test than interpolation for whether the model has learned the underlying structure.\nWe compared the NKN with the Automatic Statistician (Duvenaud et al., 2013); both methods used RBF, RQ, PER and LIN as the primitive kernels. In addition, because many time series datasets appear to contain a combination of seasonal patterns, long-term trends, and medium-scale variability, we also considered a baseline consisting of sums of PER, LIN, RBF, and Constant kernels, with trainable weights and kernel parameters. We refer to this baseline as “heuristic”.\nThe results for Airline are shown in Figure 3, while the\nTable 1. Average test RMSE and log-likelihood for regression benchmarks with random splits.\nTEST RMSE TEST LOG-LIKELIHOOD\nDATASET BBB GP-RBF GP-SM4 GP-NKN BBB GP-RBF GP-SM4 GP-NKN BOSTON 3.171±0.149 2.753±0.137 2.979±0.162 2.506±0.150 -2.602±0.031 -2.434±0.069 -2.518±0.107 -2.394±0.080 CONCRETE 5.678±0.087 4.685±0.137 3.730±0.190 3.688±0.249 -3.149±0.018 -2.948±0.025 -2.662±0.053 -2.842±0.263 ENERGY 0.565±0.018 0.471±0.013 0.316±0.018 0.254±0.020 -1.500±0.006 -0.673±0.035 -0.320±0.089 -0.213±0.162 KIN8NM 0.080±0.001 0.068±0.001 0.061±0.000 0.067±0.001 1.111±0.007 1.287±0.007 1.387±0.006 1.291±0.006 NAVAL 0.000±0.000 0.000±0.000 0.000±0.000 0.000±0.000 6.143±0.032 9.557±0.001 9.923±0.000 9.916±0.000 POW. PLANT 4.023±0.036 3.014±0.068 2.781±0.071 2.675±0.074 -2.807±0.010 -2.518±0.020 -2.450±0.022 -2.406±0.023 WINE 0.643±0.012 0.597±0.013 0.579±0.012 0.523±0.011 -0.977±0.017 0.723±0.067 0.652±0.060 0.852±0.064 YACHT 1.174±0.086 0.447±0.083 0.436±0.070 0.305±0.060 -2.408±0.007 -0.714±0.449 -0.891±0.523 -0.116±0.270\nFigure 3. Extrapolation results of NKN on the Airline dataset. “Heuristic” denotes linear combination of RBF, PER, LIN, and Constant kernels. AS represents Automatic Statistician (Duvenaud et al., 2013). The red circles are the training points, and the curve after the blue dashed line is the extrapolation result. Shaded areas represent 1 standard deviation.\nresults for Mauna and Solar are shown in Figures 8 and 9 in the Appendix. All three models were able to capture the periodic and increasing patterns. However, the heuristic kernel failed to fit the data points well or capture the increasing amplitude, stemming from its lack of PER*LIN structure. In comparison, both AS and NKN fit the train-\ning points perfectly, and generated sensible extrapolations. However, the NKN was far faster to train because it avoided discrete search: for the Airline dataset, the NKN took only 201 seconds, compared with 6147 seconds for AS."
  }, {
    "heading": "5.2. Regression Benchmarks",
    "text": ""
  }, {
    "heading": "5.2.1. RANDOM TRAINING/TEST SPLITS",
    "text": "To evaluate the predictive performance of NKN, we first conducted experiments on regression benchmark datasets from the UCI collection (Asuncion & Newman, 2007). Following the settings in Hernández-Lobato & Adams (2015), the datasets were randomly split into training and testing sets, comprising 90% and 10% of the data respectively. This splitting process was repeated 10 times to reduce variability. We compared NKN to RBF and SM (Wilson & Adams, 2013) kernels, and the popular Bayesian neural network method Bayes-by-Backprop (BBB) (Blundell et al., 2015). For the SM kernel, we used 4 mixture components, so we denote it as SM-4. For all experiments, the NKN uses 6 primitive kernels including 2 RQ, 2 RBF, and 2 LIN. The following layers are organized as Linear8-Product4-Linear4-Product2-Linear1.2 We trained both the variance and d-dimensional lengthscales for all kernels. As a result, for d dimensional inputs, SM-4 has 8d+12 trainable parameters and NKN has 4d+ 85 parameters.\nAs shown in Table 1, BBB performed worse than the Gaussian processes methods on all datasets. On the other hand, NKN and SM-4 performed consistently better than the standard RBF kernel in terms of both RMSE and log-likelihoods. Moreover, the NKN outperformed the SM-4 kernel on all datasets other than Naval and Kin8nm."
  }, {
    "heading": "5.2.2. MEASURING EXTRAPOLATION WITH PCA SPLITS",
    "text": "Since the experiments just presented used random training/test splits, they can be thought of as measuring interpolation performance. We are also interested in measuring extrapolation performance, since this is a better measure of whether the model has captured the underlying structure. In\n2The number for each layer represents the output dimension.\norder to test this, we sorted the data points according to their projection onto the top principal component of the data. The top 1/15 and bottom 1/15 of the data were used as test data, and the remainder was used as training data.\nWe compared NKN with standard RBF and SM kernels using the same architectural settings as in the previous section. All models were trained for 20,000 iterations. To select the mixture number of SM kernels, we further subdivided the training set into a training and validation set, using the same PCA-splitting method as described above. For each dataset, we trained SM on the sub-training set using {1, 2, 3, 4} mixture components and selected the number based on validation error. (We considered up to 4 mixture components in order to roughly match the number of parameters for NKN.) Then we retrained the SM kernel using the combined training and validation sets. The resulting test RMSE and log-likelihood are shown in Table 2.\nAs seen in Table 2, all three kernels performed significantly worse compared with Table 1, consistent with the intuition that extrapolation is more difficult that interpolation. However, we can see NKN outperformed SM for most of the datasets. In particular, for small datasets (and hence more chance to overfit), NKN performed better than SM by a substantial margin, with the exception of the Energy dataset. This demonstrates the NKN was better able to capture the underlying structure, rather than overfitting the training points."
  }, {
    "heading": "5.3. Bayesian Optimization",
    "text": "Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) is a technique for optimization expensive black-box functions which repeatedly queries the function, fits a surrogate function to past queries, and maximizes an acquisition function to choose the next query. It’s important to model both the predictive mean (to query points that are likely to perform well) and the predictive variance (to query points that have high uncertainty). Typically, the surrogate functions are estimated using a GP with a simple kernel, such as Matern. But simple kernels lead to inefficient exploration due to the curse of dimensionality, leading various researchers to consider additive kernels (Kandasamy et al., 2015; Gardner et al., 2017; Wang et al., 2017). Since additivity is among the patterns the NKN can learn, we were interested in testing its performance on Bayesian optimization tasks with additive structure. We used Expectated Improvement (EI) to perform BO.\nFollowing the protocol in Kandasamy et al. (2015); Gardner et al. (2017); Wang et al. (2017), we evaluated the performance on three toy function benchmarks with additive structure,\nf(x) =\n|P |X\ni=1\nf i (x[P i ]) (11)\nThe d-dimensional Styblinski-Tang function and Michalewicz function have fully additive structure with independent dimensions. In our experiment, we set d = 10 and explored the function over domain [ 4, 4]d for Styblinski-Tang and [0,⇡]d for Michalewicz. We also experimented with a transformed Styblinski-Tang function, which applies Styblinski-Tang function on partitioned dimension groups.\nFor modelling additive functions with GP, the kernel can decompose as a summation between additive groups as well. k(x,x0) = P|P | i=1 ki(x[Pi],x 0 [P i\n]). We considered an oracle kernel, which was a linear combination of RBF kernels\ncorresponding to the true additive structure of the function. Both the kernel parameters and the combination coefficients were trained with maximum likelihood. We also tested the standard RBF kernel without additive structure. For the NKN, we used d RBF kernels over individual input dimensions as the primitive kernels. The following layers were arranged as Linear8-Product4-Linear4-Product2-Linear1. Note that, although the primitive kernels corresponded to seperate dimensions, NKN can represent additive structure through these linear combination and product operations. In all cases, we used Expected Improvement as the acquisition function.\nAs shown in Figure 4, for all three benchmarks, the oracle kernel not only converged faster than RBF kernel, but also found smaller function values by a large margin. In comparsion, we can see that although NKN converged slower than oracle in the beginning, it caught up with oracle eventually and reached the same function value. This suggests that the NKN is able to exploit additivity for Bayesian optimization."
  }, {
    "heading": "5.4. Texture Extrapolation",
    "text": "Based on Wilson et al. (2014), we evaluated the NKN on texture exploration, a test of the network’s ability to learn local correlations as well as complex quasi-periodic patterns. From the original 224 ⇥ 224 images, we removed a 60 ⇥ 80 region, as shown in Figure 5(a). From a regression perspective, this corresponds to 45376 training examples and 4800 test examples, where the inputs are 2-D pixel locations and the outputs are pixel intensities. To scale our algorithms to this setting, we used the approach of Wilson et al. (2014). In particular, we exploited the grid structure of texture images to represent the kernel matrix for the full image as a Kronecker product of kernel matrices along each dimension (Saatçi, 2012). Since some of the grid points are unobserved, we followed the algorithm in Wilson et al.\n(2014) complete the grid with imaginary observations, and placed infinite measurement noise on these observations.\nTo reconstruct the missing region, we used an NKN with 4 primitive kernels: LIN, RBF, RQ, and PER. As shown in Figure 5(c), our NKN was able to learn and extrapolate complex image patterns. As baselines, we tested RBF and PER kernels; those results are shown in Figure 5(d) and Figure 5(e). The RBF kernel was unable to extrapolate to the missing region, while the PER kernel was able to extrapolate beyond the training data since the image pattern is almost exactly periodic. We also tested the spectral mixture (SM) kernel, which has previously shown promising results in texture extrapolation. Even with 10 mixture components, its extrapolations were blurrier compared to those of the NKN. The second row shows extrapolations on an irregular paved pattern, which we believe is more difficult. The NKN still provided convincing extrapolation. By contrast, RBF and PER kernels were unable to capture enough information to reconstruct the missing region."
  }, {
    "heading": "6. Conclusion",
    "text": "We proposed the Neural Kernel Network (NKN), a differentiable architecture for compositional kernel learning. Since the architecture is based on the composition rules for kernels, the NKN can compactly approximate the kernel structures from the Automatic Statistician (AS) grammar. But because the architecture is differentiable, the kernel can be learned orders-of-magnitude faster than the AS using gradient-based optimization. We demonstrated the universality of the NKN for the class of stationary kernels, and showed that the network’s representations can be made significantly more compact using complex-valued kernels. Empirically, we found the NKN is capable of pattern discovery and extrapolation in both 1-D time series datasets and 2-D textures, and can find and exploit additive structure for Bayesian Optimization."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank David Duvenaud and Jeongseop Kim for their insightful comments and discussions on this project. SS was supported by a Connaught New Researcher Award and a Connaught Fellowship. GZ was supported by an NSERC Discovery Grant."
  }],
  "year": 2018,
  "references": [{
    "title": "Exploring large feature spaces with hierarchical multiple kernel learning",
    "authors": ["F.R. Bach"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Lectures on Fourier Integrals: With an Author’s Supplement on Monotonic Functions, Stieltjes Integrals and Harmonic Analysis; Translated from the Original German by Morris",
    "authors": ["S. Bochner"],
    "year": 1959
  }, {
    "title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
    "authors": ["E. Brochu", "V.M. Cora", "N. De Freitas"],
    "venue": "arXiv preprint arXiv:1012.2599,",
    "year": 2010
  }, {
    "title": "Information theory: coding theorems for discrete memoryless systems",
    "authors": ["I. Csiszar", "J. Körner"],
    "year": 2011
  }, {
    "title": "Structure discovery in nonparametric regression through compositional kernel search",
    "authors": ["D. Duvenaud", "J.R. Lloyd", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"],
    "venue": "arXiv preprint arXiv:1302.4922,",
    "year": 2013
  }, {
    "title": "Additive Gaussian processes",
    "authors": ["D.K. Duvenaud", "H. Nickisch", "C.E. Rasmussen"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2011
  }, {
    "title": "Discovering and exploiting additive structure for Bayesian optimization",
    "authors": ["J. Gardner", "C. Guo", "K. Weinberger", "R. Garnett", "R. Grosse"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Classes of kernels for machine learning: a statistics perspective",
    "authors": ["M.G. Genton"],
    "venue": "Journal of machine learning research,",
    "year": 2001
  }, {
    "title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R. Adams"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Using deep belief nets to learn covariance kernels for Gaussian processes",
    "authors": ["G.E. Hinton", "R.R. Salakhutdinov"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2008
  }, {
    "title": "A note on harmonizable and v-bounded processes",
    "authors": ["Y. Kakihara"],
    "venue": "Journal of Multivariate Analysis,",
    "year": 1985
  }, {
    "title": "High dimensional bayesian optimisation and bandits via additive models",
    "authors": ["K. Kandasamy", "J. Schneider", "B. Póczos"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Generalized spectral kernels",
    "authors": ["Kom Samo", "Y.-L", "S. Roberts"],
    "venue": "arXiv preprint arXiv:1506.02236,",
    "year": 2015
  }, {
    "title": "Sparse spectrum Gaussian process regression",
    "authors": ["M. Lázaro-Gredilla", "J. Quiñonero Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "Backpropagation applied to handwritten zip code recognition",
    "authors": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"],
    "venue": "Neural computation,",
    "year": 1989
  }, {
    "title": "Automatic construction and natural-language description of nonparametric regression models",
    "authors": ["J.R. Lloyd", "D.K. Duvenaud", "R.B. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"],
    "year": 2014
  }, {
    "title": "Bayesian optimization for automated model selection",
    "authors": ["G. Malkomes", "C. Schaff", "R. Garnett"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Bayesian nonparametric kernellearning",
    "authors": ["J.B. Oliva", "A. Dubey", "A.G. Wilson", "B. Póczos", "J. Schneider", "E.P. Xing"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Sum-product networks: A new deep architecture",
    "authors": ["H. Poon", "P. Domingos"],
    "venue": "In Computer Vision Workshops (ICCV Workshops),",
    "year": 2011
  }, {
    "title": "Evaluation of Gaussian processes and other methods for non-linear regression",
    "authors": ["C.E. Rasmussen"],
    "venue": "Citeseer,",
    "year": 1999
  }, {
    "title": "Non-stationary spectral kernels",
    "authors": ["S. Remes", "M. Heinonen", "S. Kaski"],
    "venue": "arXiv preprint arXiv:1705.08736,",
    "year": 2017
  }, {
    "title": "Scalable inference for structured Gaussian process models",
    "authors": ["Y. Saatçi"],
    "venue": "PhD thesis, Citeseer,",
    "year": 2012
  }, {
    "title": "Estimating the dimension of a model",
    "authors": ["G Schwarz"],
    "venue": "The annals of statistics,",
    "year": 1978
  }, {
    "title": "Practical bayesian optimization of machine learning algorithms",
    "authors": ["J. Snoek", "H. Larochelle", "R.P. Adams"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2012
  }, {
    "title": "Deep, narrow sigmoid belief networks are universal approximators",
    "authors": ["I. Sutskever", "G.E. Hinton"],
    "venue": "Neural computation,",
    "year": 2008
  }, {
    "title": "Variational learning of inducing variables in sparse Gaussian processes",
    "authors": ["M. Titsias"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2009
  }, {
    "title": "Learning stationary time series using gaussian processes with nonparametric kernels",
    "authors": ["F. Tobar", "T.D. Bui", "R.E. Turner"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Batched highdimensional bayesian optimization via structural kernel learning",
    "authors": ["Z. Wang", "C. Li", "S. Jegelka", "P. Kohli"],
    "venue": "arXiv preprint arXiv:1703.01973,",
    "year": 2017
  }, {
    "title": "Tauberian theorems",
    "authors": ["N. Wiener"],
    "venue": "Annals of mathematics,",
    "year": 1932
  }, {
    "title": "Gaussian process kernels for pattern discovery and extrapolation",
    "authors": ["A. Wilson", "R. Adams"],
    "venue": "In Proceedings of the 30th International Conference on Machine Learning",
    "year": 2013
  }, {
    "title": "Fast kernel learning for multidimensional pattern extrapolation",
    "authors": ["A.G. Wilson", "E. Gilboa", "A. Nehorai", "J.P. Cunningham"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Deep kernel learning",
    "authors": ["A.G. Wilson", "Z. Hu", "R. Salakhutdinov", "E.P. Xing"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Correlation theory of stationary and related random functions: Supplementary notes and references",
    "authors": ["A.M. Yaglom"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }],
  "id": "SP:6691429384bb94529ab126c3d384cd7ce7104dd1",
  "authors": [{
    "name": "Shengyang Sun",
    "affiliations": []
  }, {
    "name": "Guodong Zhang",
    "affiliations": []
  }, {
    "name": "Chaoqi Wang",
    "affiliations": []
  }, {
    "name": "Wenyuan Zeng",
    "affiliations": []
  }, {
    "name": "Jiaman Li",
    "affiliations": []
  }, {
    "name": "Roger Grosse",
    "affiliations": []
  }],
  "abstractText": "The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN’s architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradientbased optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate NKN’s pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.",
  "title": "Differentiable Compositional Kernel Learning for Gaussian Processes"
}