{
  "sections": [{
    "text": "data, i.e., a new family of probability distributions on permutations. Our model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure, in which deterministic comparisons between pairs of items are replaced by Bernoulli trials. The probability of producing a certain ranking as a result then essentially depends on the Bernoulli parameters, which can be interpreted as pairwise preferences. We show that our model can be written in closed form if insertion sort is used as sorting algorithm and can be characterized recursively if quick sort is used, and propose a maximum likelihood approach for parameter estimation. We also introduce a generalization of the model, in which the constraints on pairwise preferences are relaxed, and for which maximum likelihood estimation can be carried out based on a variation of the generalized iterative scaling algorithm. Experimentally, we show that the models perform very well in terms of goodness of fit, compared to existing models for ranking data."
  }, {
    "heading": "1. Introduction",
    "text": "The analysis of ranking data has a long tradition in statistics, and corresponding methods have been used in various fields of application, such as psychology and the social sciences (Marden, 1996). More recently, applications in information retrieval (Liu, 2009) and machine learning (F¨urnkranz & H¨ullermeier, 2010) have caused a renewed interest in the analysis of rankings and related statistical tools, such as probability distributions on rankings.\nIn contrast to probability distributions on the reals, the number of parametric distributions on rankings (permutations\n1\nHeinz Nixdorf Institute and Department of Computer Science,\nPaderborn University, Germany\n2\nYahoo Research, New York, USA.\nCorrespondence to: Adil El Mesaoudi-Paul <adil.paul@upb.de>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nof a fixed size) is rather limited. The most popular models are Mallows (Mallows, 1957) and Plackett-Luce (Plackett, 1975; Luce, 1959), and to a lesser extent Babington Smith (Babington-Smith, 1950). In this paper, we add another class of probability distributions to this repertoire.\nOur model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure (Biernacki & Jacques, 2013), that is, the idea that a ranking is produced as the result of a sorting process, in which comparisons are not deterministic but dependant on chance. More specifically, comparisons between pairs of items are modelled as Bernoulli trials, with the Bernoulli parameters representing pairwise preferences. While these preferences obey certain consistency constraints for our basic model, we also introduce a generalization for which these constraints are relaxed. For two sorting algorithms, insertion sort and quick sort, we show that the former model can be written in closed form, and that the latter has a recursive characterization.\nIn addition to proposing the models themselves, we address the problem of parameter estimation based on sample data. More specifically, we devise procedures for efficient maximum likelihood estimation. In an experimental study, we assess the performance of our models in terms of goodness of fit on a large number of real-world data sets.\nThe rest of the paper is organized as follows. In the next section, we introduce notation and recall the basic families of probability distributions on rankings. Our new model classes are introduced in Section 3, their instantiation for specific sorting algorithms is discussed in Section 4, and the problem of parameter estimation is addressed in Section 5. Experimental results are presented in Section 6, prior to concluding the paper in Section 7."
  }, {
    "heading": "2. Probability Distributions on Rankings",
    "text": "Consider a fixed set O = {o1, . . . , oK} of K choice alternatives (objects/options/items). We identify a ranking over O with a permutation ⇡ 2 SK , where SK denotes the collection of permutations on [K] = {1, . . . ,K}. Thus, each ⇡ is a mapping [K] ! [K], such that ⇡(k) denotes the position of the kth item ok in the associated ranking. With each ranking ⇡, we associate an ordering ⇡ 1, where ⇡ 1(j) is the index of the item on position j. To simplify notation, we\nshall denote by ⇡ both a ranking and the associated ordering, writing the former in brackets and the latter in parentheses. For example, ⇡ = [2, 3, 1], ⇡ = (3, 1, 2), as well as the function ⇡ defined by ⇡(1) = 2,⇡(2) = 1,⇡(3) = 1, all denote the ranking in which o3 is at the top, o1 in the middle, and o2 on the last position."
  }, {
    "heading": "2.1. Mallows Distribution and Extensions",
    "text": "The Mallows model (MM) (Mallows, 1957) belongs to the exponential family of distributions and is parametrized by a reference ranking ⌧ and a dispersion parameter :\nP⌧, (⇡) = 1\nC( ) exp\nD(⇡, ⌧) ,\nwhere D(⇡, ⌧) is the Kendall distance (the number of pairwise inversions between ⇡ and ⌧ ) and C( ) a normalization constant. Thus, Mallows is a distance-based model: the probability of a ranking ⇡ decreases with increasing distance from ⌧ , which is the mode of the distribution.\nThe generalized Mallows model (GMM) (Fligner & Verducci, 1986) is an extension of the MM model, which has K 1 dispersion parameters 1, . . . , K 1. Each of the latter affects one specific position in the ranking, thereby allowing permutations at the same distance from the reference ranking to have different probabilities. The probability of a ranking ⇡ according to the GMM model is given by\nP⌧, (⇡) = 1\nC( ) exp\n0 @ K 1X\nj=1\njVj(⇡)\n1 A ,\nwhere Vj(⇡) = P\ni>j\nq ⇡ 1(i) < ⇡ 1(j) y is the number\nof inversions for item oj in ⇡ with respect to the identity permutation 1 . As such, the GMM model uses an insertion procedure in its generative process, in which a ranking is generated by iteratively inserting elements according to the reference ranking into a list. The probability of inserting an element into a specific position is controlled by the inversion distance and the -parameters.\nMeek & Meila (2014) further extend the GMM to the recursive inversion model (RIM), which is able to capture a hierarchical structure on the items. Instead of inserting single items, complete subsequences are merged in a recursive manner, preserving the order within each subsequence. The model is specified by a binary recursive decomposition of the items represented by a structure ⌧ , and the number of inversions is controlled by a parameter ✓i associated with each merge operation. By representing a RIM as a binary tree, where the leaves correspond to the items and the internal vertices I to the parameters ✓i, the probability of a 1J·K maps true predictaes to 1 and false predicates to 0.\nranking ⌧(✓) becomes proportional to Y\ni2I exp\n✓ivi(⇡,⇡⌧ ) ,\nwhere vi(⇡,⇡⌧ ) is the number of inversions at vertex i of ⌧(✓) for the ranking ⇡."
  }, {
    "heading": "2.2. Plackett-Luce Distribution",
    "text": "The Plackett-Luce (PL) model (Plackett, 1975; Luce, 1959) is parametrized by a vector ✓ = (✓1, ✓2, . . . , ✓K) 2 RK+ . Each ✓i can be interpreted as the weight or “strength” of the option oi. The probability assigned by the PL model to a ranking represented by a permutation ⇡ 2 SK is given by\nP✓(⇡) = KY\ni=1\n✓⇡ 1(i) ✓⇡ 1(i) + ✓⇡ 1(i+1) + . . .+ ✓⇡ 1(K) (1)\nThe product on the right-hand side of (1) is the probability of producing the ranking ⇡ in a stagewise process: First, the item on the first position is selected, then the item on the second position, and so forth. In each step, the probability of an item to be chosen next is proportional to its weight. Consequently, items with a higher weight tend to occupy higher positions. In particular, the most probable ranking (i.e., the mode of the PL distribution) is simply obtained by sorting the items in decreasing order of their weight:\n⌧ = argmax ⇡2SK P✓(⇡) = argsort k2[K] {✓1, . . . , ✓K} (2)"
  }, {
    "heading": "2.3. Babington Smith Distribution",
    "text": "The Babington Smith (BS) model is defined as follows (Babington-Smith, 1950):\nP✓(⇡) = 1\nC(✓)\nY\n1i<jK p⇡ 1(i),⇡ 1(j) , (3)\nwhere pi,j is the probability to observe a preference oi oj in a direct comparison between oi and oj , and C(✓) is a normalization constant. Thus, the parametrization ✓ of the BS model consists of all pairwise probabilities pi,j = 1 pj,i, 1  i < j  K.\nThe BS distribution results from the following “trial and error” data-generating process: First, the order of each pair of objects oi and oj is determined independently at random (as a result of a Bernoulli trial, i.e., by flipping a coin with bias pi,j). Then, in case all pairwise comparisons form a consistent ranking, this ranking is adopted, otherwise the first step is repeated."
  }, {
    "heading": "2.4. Comparison",
    "text": "The previous models can naturally be distinguished in terms of their parametrization. The Mallows model is quite restricted and not very flexible. It has one degree of freedom\nto determine the location of the distribution (the reference ranking), and another parameter to determine the spread (comparable, for example, to the normal distribution on the reals). The PL model is more flexible (for example, see (Cheng et al., 2012) for a comparison of the expressivity of Mallows and PL), with a number of parameters that is linear in the number of items. BS has an even richter parametrization, the size of which grows quadratically with the number of items.\nFrom a preference modeling point of view, the parametrizations of PL and BS are both quite natural: PL specifies the strength of each option individually, whereas BS takes pairwise comparisons as a point of departure. Thus, while PL implies relatively strong consistency properties, such as strong stochastic transitivity, BS principally allows for preferential cycles."
  }, {
    "heading": "3. Ranking Distributions based on Sorting",
    "text": "PL and BS can both be interpreted in terms of an underlying data-generating process, in which a ranking is produced as the result of a specific stochastic process. However, especially in the case of BS, the “cognitive plausibility” of the process is questionable: It is difficult to imagine that a ranking of items is indeed produced by repeating the full set of stochastic pairwise comparisons, independently of each other, till reaching consistency (especially since most of such repetitions will be futile).\nAs an arguably more plausible assumption, one could imagine that a ranking is the result of a (noisy) sorting procedure. Indeed, when people produce a ranking, they often apply some kind of sorting process, in which items are compared only if necessary. This idea has recently been put forward by Biernacki and Jacques (Biernacki & Jacques, 2013), and provides the main point of departure for our contribution.\nA sorting algorithm puts objects stored in a list in a certain order, based on pairwise comparisons between these objects. Most often, the objects to be sorted are numbers, and the pairwise comparison is determined based on some binary relation, for example, the  relation for increasing and relation for decreasing order. Note that the list submitted as input to a (deterministic) sorting algorithm does not affect its output, but it does have an influence on its time complexity, and on the pairs of items that are compared. Therefore, the time complexity of sorting algorithms is often analyzed under the assumption of a uniform distribution over the possible inputs (average time complexity analysis)."
  }, {
    "heading": "3.1. Insertion Sort Rank Data Model",
    "text": "The model by Biernacki and Jacques (Biernacki & Jacques, 2013), called Insertion Sort Rank data (ISR) model, is specified by a reference ranking ⌧ and real parameter p, very\nmuch like the Mallows model. The former corresponds to the “correct” ranking, i.e., the mode of the distribution, and p 2 [0.5, 1] is the noise parameter that controls the peakedness of the distribution. More specifically, the following assumption is made: A sorting algorithm (insertion sort) is run on an initial ordering ⇡, and whenever two items oi and oj are compared, the “right” outcome (consistent with ⌧ ) is produced with probability p (hence the “wrong” outcome with probability 1 p).\nThe algorithm’s probability to terminate with a ranking obviously depends on the initial ordering ⇡, which is a latent variable of the model. To get rid of this influence, the initialization is “averaged out”, i.e., an expectation is taken over all initial rankings. Assuming a uniform distribution for ⇡, we thus obtain\nP( | ⌧, p) = 1 K!\nX\n⇡2SK P( |⇡, ⌧, p) . (4)\nThis model can also be written as follows:\nP( |P) = 1 C 0(P)\nX\n⇡2SK P( |⇡,P) ,\n=\n1\nC 0(P)\nX\n⇡2SK\nKY\ni=1\nY j 6=i pi,j d ,⇡i,j , (5)\nwhere P is a K ⇥K matrix P = [pi,j ]1i,jK with entries\npi,j = pi,j(⌧, p) =\n⇢ p if ⌧(oi) < ⌧(oj)\n1 p if ⌧(oi) > ⌧(oj) . (6)\nThat is, the matrix P is uniquely determined by ⌧ and p (and vice versa). Moreover, for rankings ,⇡ 2 SK ,\nD ,⇡ = ⇥ d ,⇡i,j ⇤ 1i,jK\nis a binary matrix with entries d ,⇡i,j = 1 if the sorting algorithm, given ⇡ as initial ordering and producing as output, has compared oi to oj with a win for oi, and to 0 otherwise. Finally,\nC 0(P) = X\n2SK\nX\n⇡2SK\nKY\ni=1\nY j 6=i pi,j d ,⇡i,j\nis the normalization constant.\nBiernacki and Jacques tackle the problem of estimating the parameters of the model, ⌧ and p, using the maximum likelihood principle. To this end, they adopt a latent variable interpretation of the model and propose an EM algorithm."
  }, {
    "heading": "3.2. The Conjunctive Noisy Sorting Model",
    "text": "Our model is a modification of (5), which looks very similar at first sight: Instead of averaging out the influence of the\ninitial ranking ⇡ in an additive way, by aggregating the probabilities P( |⇡, ⌧, p) with an arithmetic mean, we apply the product as an aggregation function:\nPA( | ⌧, p) / Y\n⇡2SK PA( |⇡, ⌧, p) ,\nwhere A is the underlying sorting algorithm. As for the latter, one may of course consider algorithms other than insertion sort. Indeed, any pairwise-comparison-based sorting algorithm can in principle be extended to a noisy sorting model by using stochastic pairwise comparisons (Braverman & Mossel, 2008; 2009). In Section 4, we will instantiate our model for two algorithms, insertion sort and quick sort.\nThere are different motivations for the above modification. First, as will be seen, the multiplicative variant has appealing mathematical properties and can be handled a bit more easily. Second, the model can also be motivated intuitively. The product is a conjunctive aggregation function (Grabisch et al., 2009), and combining probabilities in a conjunctive way is in agreement with standard (deterministic) sorting, where the “correct” output ordering is obtained regardless of the initial ordering ⇡, that is, as a result for all initial orderings ⇡. Therefore, we call our model the Conjunctive Noisy Sorting (CNS) model.\nRecall the definition of the matrix P with entries (6), which is in one-to-one relationship with the model parameters ⌧ and p. With this notation, the CNS model can also be written as follows:\nPA( |P) = 1\nC(P)\nY\n⇡2SK P( |⇡,P) ,\n=\n1\nC(P)\nY\n⇡2SK\nKY\ni=1\nY j 6=i pi,j d ,⇡i,j , (7)\nwhere C(P) = P 2SK Q ⇡2SK QK i=1 Q j 6=i pi,j d ,⇡i,j . To simplify this representation, we introduce\nD = ⇥ d i,j ⇤ 1i,jK , d i,j =\nX\n⇡2SK d ,⇡i,j .\nWith this notation, the model becomes\nPA( |P) = 1\nC(P)\nKY\ni=1\nY j 6=i pi,j d i,j , (8)\nwhere\nC(P) = X\n2SK\nKY\ni=1\nY j 6=i pi,j d i,j . (9)\nIn Section 4, explicit expressions for the exponents d i,j will be provided for two instantiations of the model (insertion sort and quick sort). Note that, since pi,j = p or pi,j = 1 p\nin (9), the normalization constant can be written as a power series in p:\nC(p) =\ne(K)X\nj=0\n↵(K)j · p j\nThe degree e(K) and the coefficients ↵(K)j are specific to K but can be precomputed."
  }, {
    "heading": "3.3. The Generalized Conjunctive Noisy Sorting Model",
    "text": "CNS is a relatively simple model, comparable to Mallows and ISR in terms of its parametrization. The distribution has a single mode at ⌧ , and all pairwise preferences are consistent with this reference. Here, we consider a more general model, which subsumes the CNS model as a special case, and in which these assumptions are relaxed. More specifically, like in the BS model, pairwise preferences pi,j are allowed to be defined independently for each pair of objects oi and oj , and are not assumed to obey any consistency conditions. Thus, we assume a noisy sorting procedure in which, whenever the comparison of objects oi and oj is required, a coin with success probability pi,j is flipped, and the outcome of this Bernoulli experiment determines the order of the two elements: oi is preferred to oj if the outcome is 1, and oj is preferred to oi otherwise. We furthermore assume that all pairwise comparisons are independent of each other, and that pi,j = 1 pj,i for all i, j 2 [K]. We summarize the probabilities pi,j in the matrix P 2 [0, 1]K⇥K , which constitutes the parametrization of the model, referred to as Generalized Conjunctive Noisy Sorting (GCNS) model. Together with a sorting algorithm A and an initial ordering ⇡, GCNS defines a distribution PA(· |⇡,P) over SK . Thus, for each ranking 2 SK , PA( |⇡,P) is the probability to end up with when applying A to the input ⇡, and comparing items oi and oj according to pi,j . Again, we eliminate the latent variable ⇡ via conjunctive aggregation:\nPA( |P) / Y\n⇡2SK PA( |⇡,P)\nTo obtain a more compact representation, we introduce binary matrices D ,⇡ = ⇥ d ,⇡i,j ⇤ 1i,jK for rankings ,⇡ 2 SK , where the entry d ,⇡i,j in D ,⇡ is set to 1 if the sorting algorithm A, given ⇡ as initial ordering and producing as output, has compared oi to oj with a win for oi, and to 0 otherwise, and the matrices\nD = ⇥ d i,j ⇤ 1i,jK , d i,j =\nX\n⇡2SK d ,⇡i,j . (10)\nWe shall consider only such sorting algorithms for which all these matrices are well-defined (which means that, given and ⇡, it is clear whether and how oi and oj have been compared); this includes insertion sort and quick sort, amongst\nothers. Using this notation, the GCNS model can be written as follows:\nPA( |P) = 1\nC(P)\nKY\ni=1\nY j 6=i p d i,j i,j , (11)\nwhere\nC(P) = X\n2SK\nKY\ni=1\nY j 6=i p d i,j i,j . (12)\nBased on (11), one can see that GCNS is a special case of the log-linear model over the symmetric group, because the log of the probabilities can be written as a linear function of the logarithm of the parameters. Note that the key quantity in the model is D , which we shall compute in a closed form when insertion sort is used as sorting algorithm, and characterize recursively when quick sort is used. Extreme probabilities pi,j 2 {0, 1} may cause problems in the case of inconsistencies, such as preferential cycles p1,2 = p2,3 = p3,1 = 1, which are not excluded in our general model. Applying a sorting algorithm A to some P 2 {0, 1}K⇥K , an initial ordering ⇡ will be turned into an ordering with probability 1, i.e., PA( |⇡,P) = 1 and PA( 0 |⇡,P) = 0 for all 0 6= . Then, unless the same is produced for all initial orderings ⇡, which is unlikely in the case of inconsistencies, the product Q ⇡ PA( |⇡,P) will vanish for all , which means that (11) is no longer well-defined. Therefore, we subsequently exclude extreme probabilities and assume 0 < pi,j < 1 for all i, j 2 [K]. Observation 1. Assuming that pi,j > 0 for all i, j 2 [K], the model (11) is well-defined in the sense that C(P) > 0; moreover, PA( |P) > 0 for all 2 SK ."
  }, {
    "heading": "3.4. Connection to BS",
    "text": "Our model has an interesting connection to BS. The latter is parametrized by the same probability matrix P, specifying probabilities pi,j = 1 pj,i for each pair of objects oi, oj . Moreover, with a normalizing constant C 00(P), it can be written as follows:\nP( |P) = 1 C 00(P)\nKY\ni=1\nY j 6=i p d i,j i,j ,\nwhere d i,j = 1 if (i) < (j) and = 0 otherwise. Comparing this expression with (11), it can be seen that BS has exactly the same structure as our model. The key difference concerns the values d i,j , which can be seen as weights specifying the importance of the comparison between oi and oj . In BS, d i,j 2 {0, 1} and d i,j + d j,i ⌘ 1, which means that each pair has the same importance. In our model, where a pair (oi, oj) can be more or less relevant when producing a ranking with a sorting algorithms A, more general (integer) values are possible.\nInterestingly, if the BS model is restricted such that pi,j = p if ⌧(i) < ⌧(j) and pi,j = 1 p if ⌧(i) > ⌧(j), for a fixed ⌧ 2 SK and probability p, it reduces to the Mallows model (Mallows, 1957). For exactly the same restriction, GCNS reduces to CNS. Roughly speaking, Mallows is to BS what CNS is to GCNS. Moreover, since GCNS can be seen as a “sorting variant” of BS, CNS can also be seen as a “sorting variant” of Mallows. This is another strong motivation of our model."
  }, {
    "heading": "4. Instantiations of the Ranking Model",
    "text": "To make the definition of our models complete, we make use of two sorting algorithms A: insertion sort, denoted by I, and quick sort, denoted by Q. For insertion sort algorithm, we show that (8) and (11) can be written in closed form, and for quick sort algorithm, we show that they can be characterized in a recursive way."
  }, {
    "heading": "4.1. Insertion Sort",
    "text": "In (stochastic) insertion sort, we start with an empty ordering, in which all K objects are inserted one by one, in the order determined by the initial ranking ⇡. In the lth iteration, we are given a partial ordering (oi(1), . . . , oi(l)) of l < K objects and insert another object o. To this end, o is first compared with oi(1), then with oi(2), and so forth. It is inserted as position j if oi(j) is the first item that looses its comparison with o; in case o is beaten by all l items, it is put on position l + 1.\nThus, in stochastic insertion sort, we produce an output ranking from an initial ranking ⇡ by comparing only a subset of all possible pairs of items. Note that the output of the noisy sorting procesure is a random ordering that depends on the success probabilities P = [pi,j ], and also on the initial ordering ⇡, i.e., the order in which items are inserted. The following example elaborates on this dependence. Example 1. Consider insertion sort with two different initial orderings ⇡ = (o1, o2, o3) and ⇡0 = (o3, o2, o1). Let the pairwise probabilities be p1,2 = 1/4, and p1,3 = p2,3 = 1/2. Now let us compute the probability of observing = (o1, o2, o3). Starting from ⇡, we first insert o1, then o2, and finally o3. Ending with is thus only possible if o1 has beaten o2 in the first comparison, o1 has also beaten o3, and o2 has beaten o3. Therefore, the probability of observing is proportional to p1,2p1,3p2,3 = 0.0625. Starting from ⇡0, is produced with fewer comparisons, and the same probability is proportional to p1,2p2,3 = 0.125. Now, we are going to focus on D = P\n⇡2SK D ,⇡ , where\nthe sum is elmentwise. The following observation allows us to compute D in a concise way. Lemma 1. Assume that id = (o1, . . . , oK). Then, for\ninsertion sort, the matrix D id is given by\nd idi,j =\n8 < :\nK! 2 if i < j K\nbi,j+2\n(K bi,j 2)! bi,j ! if j < i 0 otherwise ,\nwhere bi,j = i j 1. Furthermore, for any 2 SK , we have D = B D idB | , where B is the permutation matrix that corresponds to .\nProof. The first case is easy to verify, since insertion sort with an initial ordering ⇡ compares two objects only in case they are concordant (in the same order) in\nid and ⇡. The number of such orderings is\nK! 2 .\nThe second case is more involved, since one needs to calculate all initial orders ⇡ 2 SK in which a pair of objects, say oi and oj , are discordant and compared to each other in the course of the sorting procedure. Assume that insertion sort is run with ⇡ as initial order, and the output is id . It is easy to see that object oi and oj are not compared if there is a third object ok, which is between oj and oi with respect to\nid , and also between oj and oi in ⇡; Figure 1 illustrates such a configuration of objects. Therefore, the number of orderings for which oi and oj are compared is equal to\nK bi,j+2 (K bi,j 2)!, because oi and oj and the\nitems between them have to be ordered such that oi is the first, oj is the second, and all items between oj and oi with respect to\nid preceed them in ⇡. In addition, the items between oi and oj can be permuted arbitrarily in the initial order, which results in the term bi,j !.\nThe last claim can be verified based on the fact that the argument above holds for an arbitrary permutation of objects. This concludes the proof."
  }, {
    "heading": "4.2. Quick Sort",
    "text": "The quick sort algorithm is inherently random due to the random choice of the pivot item. We make use of a derandomized version by taking as pivot the item in the middle of the initial ordering (i.e., the item on position dK/2e for an ordering with K items).\nIn noisy quick sort, we start by picking a pivot element op from the elements {o1, . . . , oK} to be ordered. We then\nproceed with the partition operation, in which we construct two sub-orderings, one collecting the items that lost and the other one the items that won the pairwise comparison with the pivot element. The same process is repeated with each of the two sub-orderings (unless a sub-ordering reduces to a single item), eventually producing a complete ordering of the items. Again, we note that the output of the noisy sorting model based on quick sort depends on the pairwise probabilities P and the initial ordering ⇡. This dependence is illustrated in the following example. Example 2. Consider the quick sort algorithm with two different initial orderings ⇡ = (o1, o2, o3) and ⇡0 = (o2, o1, o3). Let the pairwise probabilities be given as in Example 1. It is easy to see that the probability of observing = (o1, o2, o3) starting from ⇡ is proportional to p1,2p2,3 = 0.125. When starting with ⇡0, this probability is proportional to p1,2p1,3p2,3 = 0.0625.\nThe following lemma gives a recursive expression of D for the case of quick sort as a sorting algorithm. Lemma 2. Assume that id = (o1, . . . , oK). Then, for quick sort, the matrix D id is given by\nd idij = (K 1)!\n2 4 iX\nk=1\nQ(k,K, i, j) + KX\nk=j\nQ(1, k, i, j)\n3 5 ,\nwhere\nQ(`, u, i, j) =\n8 >>< >>:\n0 if i < p = d i+j2 e < j 2 if p = i or p = j\nQ(1, p 1, i, j) if j < p Q(p+ 1, u, i, j) if i > p\nFurthermore, for any 2 SK , we have D = B D idB |\n, where B is the permutation matrix that corresponds to .\nProof. The first case of the recursion Q(`, u, i, j) follows from the fact that neither oi nor oj is chosen as pivot, in which case they will not be compared any more. In the second case, either oi or oj is chosen as pivot, in which case they will be compared. Otherwise, the recursion corresponds to the quick sort recursion."
  }, {
    "heading": "5. Parameter Estimation",
    "text": "In this section, we address the problem of parameter estimation, i.e., the question of how to fit our models to a given sample D = { 1, . . . , n} ⇢ SK using the principle of maximum likelihood (ML) estimation."
  }, {
    "heading": "5.1. The CNS Model",
    "text": "Given a set of observations { 1, . . . , n}, the ML estimation consists of solving the following constrained optimiza-\ntion problem:\nmax\nP⌧\nnX\n`=1\nKX\ni=1\nX j 6=i d `,⌧i,j log p ⌧ i,j n logC(P⌧ ) (13)\ns. t. p⌧i,j =\n⇢ p if ⌧(i) < ⌧(j)\n1 p if ⌧(i) > ⌧(j) 8i, j 2 [K], i 6= j\nRecall that P⌧ is equivalently represented by the reference order ⌧ and the probability p, i.e., the maximization in the above problem is over these two parameters.\nWe tackle the problem with simple hill-climbing search for ⌧ in the discrete space SK , initialized with the Borda ranking (i.e., sorting items according to their average rank in the data). The neighborhood of an ordering is defined as the set of all orderings that can be obtained by a swap of two adjacent items. For a fixed ⌧ , the optimization problem (13) reduces to a simple one-dimensional problem:\nmax\np\nnX\n`=1\n2 4 X\n⌧(i)<⌧(j)\nd `,⌧i,j log p+ X\n⌧(i)>⌧(j)\nd `,⌧i,j log(1 p)\n3 5\nn logC(P⌧ ) s. t. p 2 [0.5, 1]\n(14)\nThis problem is convex (the distribution belongs to the exponential family) and can be solved numerically, for example by means of the golden section method.\nIn each iteration of the algorithm, the best candidate solution (⌧, p) in the neighborhood of the current best solution is adopted, and the search stops if no improvement is possible anymore."
  }, {
    "heading": "5.2. The GCNS Model",
    "text": "The GCNS model (11) is parametrized by P. Here, the maximum likelihood (ML) principle cannot be applied directly, because the normalizing factor C(P) in (12) cannot be written in a closed form in terms of the model parameters. Therefore, we opt for using the generalized iterative scaling (GIS) procedure (Darroch & Ratcliff, 1972), an iterative method for estimating the probabilities in a log-linear model. Given a set of observations { 1, . . . , n}, ML estimation amounts to solving the following constrained optimization problem:\nmax\nP\nnX\n`=1\nKX\ni=1\nX j 6=i d `i,j log pi,j n logC(P)\ns. t. pi,j + pj,i = 1, 8i, j 2 [K], i 6= j .\n(15)\nLet #j denote the jth ranking according to some fixed ordering over SK (e.g. Lehmer code). With fj = #{i 2 [n] : i = #j}, the empirical frequencies corresponding\nto the probabilities of all possible permutations, the GIS procedure seeks to find a parameter estimate P0 for which\nK!X\n`=1\np0` d #` i,j =\nK!X\n`=1\nbp` d #` i,j (16)\nfor all i 6= j, where p0` = PA( #` |P0).\nObserve that the GIS procedure can be adapted to produce the parameters of the log-linear model instead of the probabilities pi,j (Malouf, 2002). In addition, we note that GIS requires the computation of a vector of length K!, a very costly operation that will be tackled based on a Monte Carlobased approximation technique. Further, based on Lemma 1 and 2, it is easy to see that the sum of exponents is constant for every ordering in case of both insertion and quick sort, that is PK i=1 P i 6=j d i,j = BK for all 2 SK . According to (Darroch & Ratcliff, 1972), P0 in (16) is the (unconstrained) ML estimate for P. In our case, however, the constraints pi,j + pj,i = 1 in (15) need to be taken into account. Therefore, we accompany each update step in the GIS procedure with a projection step, which ensures that the estimated parameters satisfy the constraints. One update step of the iterative procedure for the parameter estimation thus can be written as\np(n+1)i,j = ⇧ ⇣ p(n)i,j + (n) ⌘ ,\nwhere\n(n) = log\nPK! `=1 bpld #` i,j\nPK! `=1 p (n) l d #` i,j\n! 1 BK\n,\nand ⇧(x) denotes the least-squares projection of x = (xi,j , xj,i), given by\nargmin y2R2+ ||x y||2 s. t. yi,j + yj,i = 1 ,\n(17)\nwhich can be determined analytically."
  }, {
    "heading": "5.3. Sampling",
    "text": "The model (11) can be sampled by using MCMC based on the fact that one can compute the acceptance ratio as\nlog PA( |P) PA( 0|P) =\nKX\ni=1\nX j 6=i (d i,j d 0 i,j) log pi,j .\nThis allows us to make use of the Metropolis-Hastings (MH) algorithm. We use Mallows (Mallows, 1957) as proposal distribution. The pseudo-code of the sampling is given in Algorithm 1. The reference ranking of the Mallows model P(· | , ), denoted by , is always set to the current ranking i 1 (see line 5). In this case, it is easy to verify that the stationary distribution of the Markov chain is indeed\nPA( |P), because the Mallows model is symmetric in the sense that P( | , 0) = P( 0 | , ), and assigns positive probability to every ranking when > 0. Therefore, the detailed balance condition is satisfied, and the ergodicity of the chain is also ensured.\nAlgorithm 1 Metropolis-Hastings with Mallows proposal 1: procedure MH(T, ) 2: Select initial ordering 0 3: D = ; 4: for i = 1! T do 5: i ⇠ P(· | , i 1) . Proposal from Mallows 6: qi PK i=1 PK j 6=i(d i i,j d i 1 i,j ) log pi,j\n7: Accept i with probability min (1, exp(qi)) 8: D = D [ { i} 9: return D"
  }, {
    "heading": "6. Experiments",
    "text": "To investigate the performance of our new model and the effectiveness of parameter estimation, we conducted experiments on 213 real-world data sets from the PrefLib repository (http://www.preflib.org). These data sets originate from different domains, ranging from actual elections over movie rankings to competitor rankings from various sporting competitions. The number of items varies between 3 and 10 (details are summarized in the supplementary material).\nAll models are fit using maximum likelihood estimation, and Kullback-Leibler (KL) divergence between an empirical distribution and its estimation is used as a measure of the goodness of fit. In a first setting, we fit the models to the entire data, while in a second setting, we only fit to half of the data and determine divergence on the other half (averaging over 20 random splits).\nIn a first experiment, we compare ISR with our new variant CNS, with both insertion and quick sort as underlying sorting algorithms, using MM as an additional baseline. The ISR, CNS, and MM models are comparable in terms of their parametrization. A summary of the results in terms of win/tie/loss statistics is given in Table 1 (while the complete results can be found in the supplementary material). As can be seen, CNS shows a very strong performance, especially with insertion sort as a sorting algorithm.\nIn a second experiment, we compare CNS with its generalization GCNS, again with insertion and quick sort as underlying sorting algorithms in both models. The results in Table 1 clearly show that GCNS leads to better approximations. This is hardly surprising, given that GCNS has more parameters and therefore allows for fitting distributions in a more flexible way. Again, an instantiation with insertion sort seems to be preferable to the use of quick sort."
  }, {
    "heading": "7. Conclusion and Future Work",
    "text": "Adopting the idea of a data-generating process in the form of a noisy sorting procedure, we proposed a variant of a parametrized probability distribution on rankings as recently proposed by Biernacki and Jacques (Biernacki & Jacques, 2013), as well as a generalization that is more flexible and makes less stringent coherence assumptions. Our models have an intuitive interpretation, exhibit convenient mathematical properties, and seem to fit empirical data very well. For two sorting algorithms, insertion sort and quick sort, we developed parameter estimation techniques based on a closed-form expression of the likelihood function for the former, and a recursive characterization of it for the latter. Experimentally, insertion sort leads to better performance.\nIn future work, we plan to consider other sorting algorithms, such as merge sort and heap sort. Another direction worth to investigate is the analysis of algebraic properties of our models using tools from computational algebraic geometry (Geiger et al., 2006); such properties may simplify the handling of the model and help to further improve efficiency of parameter estimation. Last but not least, we are also interested in using the model for other machine learning problems, in which distributions on rankings are needed, such as learning to rank (Ailon et al., 2005; Ailon, 2008; Cao et al., 2007) and multi-armed bandits (Busa-Fekete & H¨ullermeier, 2014; Sz¨or´enyi et al., 2015)."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors gratefully acknowledge financial support by the Germany Research Foundation (DFG)."
  }],
  "year": 2018,
  "references": [{
    "title": "Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking",
    "authors": ["N. Ailon"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2008
  }, {
    "title": "Aggregating Inconsistent Information: Ranking and Clustering",
    "authors": ["N. Ailon", "M. Charikar", "A. Newman"],
    "venue": "In Proceedings of the Annual ACM Symposium on Theory of Computing (STOC),",
    "year": 2005
  }, {
    "title": "Discussion of Professor Ross’s Paper",
    "authors": ["B. Babington-Smith"],
    "venue": "Journal of the Royal Statistical Society B,",
    "year": 1950
  }, {
    "title": "A Generative Model for Rank Data Based on Insertion Sort Algorithm",
    "authors": ["C. Biernacki", "J. Jacques"],
    "venue": "Computational Statistics & Data Analysis,",
    "year": 2013
  }, {
    "title": "Noisy Sorting Without Resampling",
    "authors": ["M. Braverman", "E. Mossel"],
    "venue": "In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
    "year": 2008
  }, {
    "title": "A Survey of Preferencebased Online Learning with Bandit Algorithms",
    "authors": ["E. ullermeier"],
    "venue": "In Proceedings of Conference on Learning Theory (COLT),",
    "year": 2014
  }, {
    "title": "Learning to Rank: From Pairwise Approach to Listwise Approach",
    "authors": ["Z. Cao", "T. Qin", "Liu", "T.-Y", "Tsai", "M.-F", "H. Li"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2007
  }, {
    "title": "Label Ranking with Partial Abstention Based on Thresholded Probabilistic Models",
    "authors": ["E. ullermeier", "W. Waegeman", "V. Welker"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2012
  }, {
    "title": "Generalized Iterative Scaling for Log-Linear Models",
    "authors": ["J.N. Darroch", "D. Ratcliff"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1972
  }, {
    "title": "Distance Based Ranking Models",
    "authors": ["M.A. Fligner", "J.S. Verducci"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp",
    "year": 1986
  }, {
    "title": "Preference Learning: An Introduction",
    "authors": ["E. ullermeier"],
    "venue": "In Preference learning,",
    "year": 2010
  }, {
    "title": "Aggregation Functions",
    "authors": ["M. Grabisch", "Marichal", "J.-L", "R. Mesiar", "E. Pap"],
    "year": 2009
  }, {
    "title": "Learning to Rank for Information Retrieval",
    "authors": ["Liu", "T.-Y"],
    "venue": "Foundations and Trends in Information Retrieval,",
    "year": 2009
  }, {
    "title": "Individual Choice Behavior: A Theoretical Analysis",
    "authors": ["R.D. Luce"],
    "year": 1959
  }, {
    "title": "Non-null Ranking Models",
    "authors": ["C. Mallows"],
    "year": 1957
  }, {
    "title": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation",
    "authors": ["R. Malouf"],
    "venue": "In Proceedings of Conference on Natural Language Learning (COLING), pp",
    "year": 2002
  }, {
    "title": "Analyzing and Modeling Rank Data",
    "authors": ["J.I. Marden"],
    "venue": "CRC Press,",
    "year": 1996
  }, {
    "title": "Recursive Inversion Models for Permutations",
    "authors": ["C. Meek", "M. Meila"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2014
  }, {
    "title": "The Analysis of Permutations",
    "authors": ["R. Plackett"],
    "venue": "Applied Statistics,",
    "year": 1975
  }, {
    "title": "Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach",
    "authors": ["E. ullermeier"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }],
  "id": "SP:6af5644d958520c276b9ba00a63eb1cf54e41aae",
  "authors": [{
    "name": "Adil El Mesaoudi-Paul",
    "affiliations": []
  }, {
    "name": "Eyke Hüllermeier",
    "affiliations": []
  }, {
    "name": "Róbert Busa-Fekete",
    "affiliations": []
  }],
  "abstractText": "We propose a new statistical model for ranking data, i.e., a new family of probability distributions on permutations. Our model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure, in which deterministic comparisons between pairs of items are replaced by Bernoulli trials. The probability of producing a certain ranking as a result then essentially depends on the Bernoulli parameters, which can be interpreted as pairwise preferences. We show that our model can be written in closed form if insertion sort is used as sorting algorithm and can be characterized recursively if quick sort is used, and propose a maximum likelihood approach for parameter estimation. We also introduce a generalization of the model, in which the constraints on pairwise preferences are relaxed, and for which maximum likelihood estimation can be carried out based on a variation of the generalized iterative scaling algorithm. Experimentally, we show that the models perform very well in terms of goodness of fit, compared to existing models for ranking data.",
  "title": "Ranking Distributions based on Noisy Sorting"
}