{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Multivariate time series (MTS) analysis (Hamilton, 1994; Reinsel, 2003) has attracted a lot of attention in machine learning, signal processing, and other related areas, due to its impact and usefulness in many real world applications such as healthcare, climate, and financial forecasting. Statespace models such as Kalman filters (Kalman et al., 1960) and hidden Markov models (Rabiner, 1989) have been developed to model MTS and have shown promising results on prediction tasks such as forecasting and interpolation. However, in many applications, the MTS observations usually come from multiple sources and are often characterized\n*Equal contribution 1Department of Computer Science, University of Southern California, Los Angeles, California, United States. Correspondence to: Zhengping Che, Sanjay Purushotham, Guangyu Li, Bo Jiang, Yan Liu <{zche,spurusho,guangyul,boj,yanliu.cs}@usc.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nby various sampling rates. For example, in healthcare, vital signs such as heart rate are sampled frequently, while lab results such as pH are measured infrequently; in finance, the stock prices are sampled daily or even more frequently, while macro-economic data such as employment, GDP are sampled monthly or quarterly. Such time series observations with either regular or irregular sampling rates are termed as Multi-Rate Multivariate Time Series (MR-MTS) data. Modeling the MR-MTS using state-space models is challenging since MR-MTS naturally comes with multiple temporal dependencies and these dependencies may not have direct relationship to the sampling rates. That is, the long and short-term temporal dependencies may be associated with a few or all the time series data with different sampling rates. Capturing these temporal dependencies is important as they model the underlying data generation mechanism, and they impact the interpolation and forecasting tasks. Upsampling or downsampling MR-MTS to a single rate time series cannot address this challenge, since these simple techniques may artifically introduce or remove some naturally occurring dependencies present in MR-MTS. For example, forward/backward imputation will introduce long-term dependencies. Therefore, building models which can capture multiple temporal dependencies directly from the MR-MTS data is still an open problem in the time series analysis field.\nDeep learning models such as recurrent neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) have emerged as successful models for time series analysis (Graves et al., 2013; Mikolov et al., 2010) and sequence modeling applications (Socher et al., 2011; Xu et al., 2015). While deep discriminative models (Hermans & Schrauwen, 2013; Martens & Sutskever, 2011; Pascanu et al., 2013; Chung et al., 2016) have been shown to model complex non-linear temporal dependencies present in MTS, deep generative models (Gan et al., 2015; Rezende et al., 2014) have become more popular since they are intuitive, interpretable and are more powerful than their discriminative counterparts (Durbin & Koopman, 2012) and they capture the data generation process. Despite their success with single-rate time series data, the existing deep generative models are not suitable for modeling MRMTS as they are not designed to capture multiple temporal dependencies from different sampling rates.\nRecently, latent hierarchical structure learning based on deep learning models have led to remarkable ad-\nvances in capturing temporal dependencies from sequential data (El Hihi & Bengio, 1995; Chung et al., 2016; Koutnik et al., 2014). Motivated by these models, we propose a novel deep generative model termed as Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), which learns multiple temporal dependencies directly from MR-MTS by jointly modeling time series with different sampling rates. MRHDMM learns the latent hierarchical structures along with learnable switches and captures the data generation process of MR-MTS. It simultaneously learns a inference network and a generative model by leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. The data generation process of MR-HDMM can automatically infer the hierarchical structures directly from data, which is extremely helpful for downstream tasks such as interpolation and forecasting.\nIn summary, we develop a first-of-a-kind novel deep generative model called MR-HDMM to systematically capture the multiple temporal dependencies present in MR-MTS by using hierarchical latent structures and learnable switches. In addition, we also propose a new structured inference network for MR-HDMM. A comprehensive and systematic evaluation of the MR-HDMM model is conducted on two real-world datasets to demonstrate the state-of-the-art performance in forecasting and interpolation tasks. Finally, we interpret the learnt latent hierarchies from MR-HDMM to study the captured temporal dependencies."
  }, {
    "heading": "2. Related Work",
    "text": "State-space models such as Kalman filters (KF) (Kalman et al., 1960), and hidden Markov models (HMMs) (Rabiner, 1989) have been widely used in various time series applications such as speech recognition (Rabiner, 1989), atmospheric monitoring (Houtekamer & Mitchell, 2001), and robotic control (Negenborn, 2003). These approaches successfully model regularly sampled (i.e. sampled at the same frequency/rate) time series data, however, they cannot be directly used for MR-MTS as they cannot simultaneously capture the multiple temporal dependencies present in MR-MTS. To handle MR-MTS with state-space models, researchers have extended KF models and proposed multirate Kalman filters (MR-KF) (Armesto et al., 2008; Safari et al., 2014). MR-KF approaches either fuse the data with different sampling rates or fuse the estimates for KFs trained on each sampling rate. Many of these MR-KF approaches aim to improve the estimates for the highest sampled rate data and do not focus on capturing the multiple temporal dependencies present in MR-MTS. Moreover, the linear transition and emission functionality of the MR-KF models limits their usability on complex real-world data.\nRecently, researchers have resorted to deep learning models (Chung et al., 2016; Krishnan et al., 2015; Gan et al.,\n2015) to model the non-linear temporal dynamics of realworld and sequential data. Discriminative models such as hierarchical recurrent neural network (El Hihi & Bengio, 1995), hierarchical multiscale recurrent neural network (HM-RNN) (Chung et al., 2016), and phased long short-term memory (PLSTM) (Neil et al., 2016) have been proposed to capture temporal dependencies of sequential data. However, these discriminative models do not capture the underlying data generation process and therefore are not suited for forecasting and interpolation tasks. Deep generative models (Rezende et al., 2014; Krishnan et al., 2015; Gan et al., 2015) have been developed to model the data generation process of the complex time series data. Krishnan et al. (2015) proposed deep Kalman filter, a nonlinear state-space model, by marrying the ideas of deep neural networks with Kalman filters. Fraccaro et al. (2016) introduced stochastic recurrent neural network (SRNN) which glued a RNN with a state space model together to form a stochastic and sequential neural generative model. Even though these deep generative models are the state-of-the-art approaches to obtain the underlying data generation process, they are not designed to capture all the temporal dependencies of MR-MTS. None of the existing deep learning models or state-space models can be directly used for modeling MR-MTS. Thus, in this work, we develop a deep generative model which leverages the properties of the above discriminative and generative models, to model the data generation process of MR-MTS while also capturing the multiple temporal dependencies using a latent hierarchical structure."
  }, {
    "heading": "3. Our Model",
    "text": "In this section, we present our proposed Multi RateHierarchical Deep Markov Model (MR-HDMM). We first clarify the notations and definitions used in this paper.\nNotations Given a MR-MTS of L different sampling rates and length T , we use a vectorxlt ∈ RDl to represent the time series observations of lth rate at time t. Here l = 1, . . . , L, t = 1, . . . , T , and Dl is the dimension of time series with lth rate. The L sampling rates are in descending order, i.e., l = 1 and l = L refer to the highest and lowest sampling rates. To make the notations succinct, we use xl:l ′\nt:t′ to denote all observed time series of lth to l′th rates and from time t to t′. We use θ(.) and φ(.) to denote the parameter sets for generation model pθ and inference network qφ respectively. we use L layers of RNNs in the inference network to model MR-MTS of L different sampling rates. We use LHS , the number of hidden layers in both generation model and inference network, to control the depth of the learnt hierarchical structures. In the rest of this paper we take LHS = L for model simplicity, but in practice they are not tied. The latent states or variables are denoted by z, s and h. Their superscript and subscript respectively indicate the corresponding layer(s) and the time step(s) (e.g., z1:L1:T , s 2:L t , h l t).\nFigure 1 illustrates our MR-HDMM model which consists of the generation model and inference network. MR-HDMM captures the underlying data generation process by using the variational inference methods (Rezende et al., 2014; Kingma & Welling, 2013) and learns the latent hierarchical structures using learnable switches and auxiliary connections to adaptively encode the dependencies across the hierarchies and the timestamps. In particular, the switches use an update-and-reuse mechanism to control the updates of the latent states of a layer based on their previous states (i.e., utilizing temporal information) and the lower latent layers (i.e., utilizing the hierarchy). The switch triggers an update of the current states if it gets enough information from lower-level states, otherwise it reuses the previous states. Thus, the higher-level states act as summarized representations over the lower-level states and the switches help to propagate the temporal dependencies. The auxiliary connections (dashed lines in Figure 1(a)) between MR-MTS of different sampling rates and different latent layers help the model effectively capture the short-term and long-term temporal dependencies. Without the auxiliary connections, the higher-rate time series may mask the multi-scale dependencies present in the lower-rate time series data while propagating dependencies through bottom-up connections. Note that, the auxiliary connections are not related to the sampling rate of MR-MTS, and the sampling rate of higherrate variable need not be a multiple of sampling rate of the lower-rate variable. Due to the flexibility of auxiliary connections, our MR-HDMM can also handle irregularly sampled time series data or missing data. We can a) zero-out the missing data points in the inference network and remove the corresponding auxiliary connections in the generation model during training, and b) interpolate missing values by adding auxiliary connections in the well-trained model."
  }, {
    "heading": "3.1. Generation Model",
    "text": "Figure 1(a) shows the generation model of our MR-HDMM. The generation process of our MR-HDMM follows the transition and emission framework, which is obtained by applying deep recurrent neural networks to non-linear continuous state space models. The generation model is carefully designed to incorporate the switching mechanism and auxiliary connections in order to capture the multiple temporal dependencies present in MR-MTS.\nTransition We design the transition process of the latent state z to capture the hierarchical structure for multiple temporal dependencies with learnable binary switches s. For each non-bottom layer l > 1 and time step t ≥ 1, we use a binary switch state slt to control the updates of the corresponding latent states zlt, as shown in Figure 2. slt is obtained based on the values of the previous latent states zlt−1 and the lower layer latent states z l−1 t by a de-\nterministic mapping slt = I ( gθs(z l t−1, z l−1 t ) ≥ 0 ) . When the switch is on (i.e., update operation, slt = 1), z l t is updated based on zlt−1 and z l−1 t through a learnt transition distribution. We use a multivariate Gaussian distribution N ( µlt,Σ l t|zlt−1, zl−1t ; θz ) with mean and covariance given by (µlt,Σ l t) = gθz (z l t−1, z l−1 t ) as the transition distribution. When the switch is off (i.e., reuse operation, slt = 0), z l t will\nbe drawn from the same distribution as its previous states zlt−1, which is N ( µlt−1,Σ l t−1 ) . Note, unlike Chung et al. (2016), we do not copy the previous state since our latent states are stochastic. The latent states of the first layer (z11:T ) are always updated at each time step. In our model, gθs is parameterized by a multilayer perceptron (MLP), and gθz is parameterized by gated recurrent units (GRU) (Chung et al., 2014) to capture the temporal dependencies. With this update-or-reuse transition mechanism, higher latent layers tend to capture longer-term temporal dependencies through the bottom-up connections in the latent layers.\nEmission Multi-rate multivariate observation x needs to be generated from z in the emission process. In order to embed the multiple temporal dependencies in the generated MR-MTS, we introduce auxiliary connections (denoted by the dashed lines in Figure 1(a)) from the higher latent layers to the lower rate time series. That is, time series of lth rate at time t (i.e., xlt) is generated from all latent states up to lth layer z1:lt through emission distribution Π ( xlt|z1:lt ; θx ) . The choice of emission distribution Π is flexible and depends on the data type. Multinomial distribution is used for categorical data, and Gaussian distribution is used for continuous data. Since all the data in our tasks are continuous, we use Gaussian distribution where the mean µ(x) l\nt\nand covariance Σ(x) l\nt are determined by gθx(z 1:l t ), which\nis parameterized by an MLP.\nTo summarize, the overall generation process is described in Algorithm 1. The parameter set of generation model is θ = {θx, θz, θs}. Given this, the joint probability of MRMTS and the latent states/switches can be factorized by the following Equation (1).\npθ ( x1:L1:T ,z 1:L 1:T , s 2:L 1:T |z1:L0 ) =pθ ( x1:L1:T |z1:L1:T ) pθ ( z1:L1:T , s 2:L 1:T |z1:L0\n) =\nT∏ t=1 pθ ( x1:Lt |z1:Lt ) · T∏ t=1 pθ ( z1:Lt , s 2:L t |z1:Lt−1 ) =\nT∏ t=1 L∏ l=1 pθx ( xlt|z1:lt ) · T∏ t=1 pθz ( z1t |z1t−1 ) · T∏ t=1 L∏ l=2 pθs ( slt|zlt−1,zl−1t ) pθz ( zlt|zlt−1,zl−1t , slt ) (1)\nIn order to obtain the parameters of MR-HDMM, we need to maximize the log marginal likelihood of all MR-MTS data points, which is the summation of the log marginal likelihood L(θ) = log pθ ( x1:L1:T |z1:L0 ) of each MR-MTS data point x1:L1:T . The log marginal likelihood of one data point can be achieved by integrating out all possible z and s in Equation (1). Since s are deterministic binary variables, integrating them out can be done straightforwardly by taking their values in the likelihood. However, stochastic variable\nAlgorithm 1 Generation model of MR-HDMM 1: Initialize z1:L0 ∼ N (0, I) 2: for t = 1, . . . , T do 3: ( µ1t ,Σ 1 t ) = gθz (z 1 t−1)\n4: z1t ∼ N ( µ1t ,Σ 1 t ) {Transition of the first layer.} 5: for l = 2, · · · , L do 6: slt = I ( gθs(z l t−1,z l−1 t ) ≥ 0\n) 7: ( µlt,Σ l t ) = { gθz (z l t−1,z l−1 t ) if s l t = 1(\nµlt−1,Σ l t−1 )\notherwise. 8: zlt ∼ N ( µlt,Σ l t ) {Transition of other layers.}\n9: end for 10: for l = 1, · · · , L do 11: ( µ(x) l\nt,Σ (x)l t ) = gθx(z 1:l t )\n12: xlt ∼ N ( µ(x) l t,Σ (x)l t ) {Emission.} 13: end for 14: end for\nz cannot be analytically integrated out. Thus, we resort to the well-known variational principle (Jordan, 1998) and introduce our inference network below."
  }, {
    "heading": "3.2. Inference Network",
    "text": "We design our inference network to mimic the structure of the generative model. The goal is to obtain an objective which can be optimized easily and which can make the model parameter learning amenable. Instead of directly maximizing L(θ) w.r.t θ, we build an inference network with a tractable distribution qφ , and maximize the variational evidence lower bound (ELBO) F(θ, φ) ≤ L(θ) with respect to both θ and φ. Note, φ is the parameter set of the inference network which will is formally defined at the end of this section. The lower bound can be written as (please refer to the supplementary materials for full derivation):\nF(θ, φ) = Eqφ [ log pθ ( x1:L1:T |z1:L0:T )] −DKL ( qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0\n)∥∥∥pθ (z1:L1:T , s2:L1:T |z1:L0 )) (2)\nwhere the expectation of the first term is under qφ ( z1:L1:T |x1:L1:T , z1:L0 ) . To get a tight bound and an accurate estimate from our MR-HDMM, we need to properly design a new inference network as using the existing inference networks from SRNN (Fraccaro et al., 2016) or DMM (Krishnan et al., 2015) is not applicable for MR-MTS. In the following, we show how we design the inference network (Figure 1(b)) to obtain a good structured approximation to the posterior. First, we maintain the Markov properties of z in the inference network, which leads to the factorization:\nqφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0 ) = T∏ t=1 qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) (3)\nWe then leverage the hierarchical structure and inherit the switches from the generation model into the\nTable 1. Comparison of structured inference networks.\nInference network Implemented with RNN output Captured in hlt Variational approximation for zlt filtering forward RNN hforward xl1:t qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) smoothing backward RNN hbackward xlt:T qφ ( zlt|zlt−1,zl−1t , slt,x1:Lt:T\n) bi-direction bi-directional RNN [ hforward,hbackward ] xl1:T qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T\n) inference network. That is, the same gθs from the generation model is used in the inference network, i.e., qφ ( slt|zlt−1, zl−1t ,x1:L1:T ) = qφs ( slt|zlt−1, zl−1t ) =\npθs ( slt|zlt−1, zl−1t ) . Then, for each term in the righthand side of Equation (3) and for all t = 1, · · · , T , we have:\nqφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) =qφ ( z1t |z1t−1,x1:L1:T\n) · L∏ l=2 qφ ( slt|zlt−1,zl−1t ,x1:L1:T ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T\n) =qφ ( z1t |z1t−1,x1:L1:T\n) · L∏ l=2 pθs ( slt|zlt−1,zl−1t ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) (4) Thus, the inference network can be factorized by Equation (3) and (4). Note, we also can factorize generative model based on Equation (1). Given these, we further factorize the ELBO in Equation (2) as a summation of expectations of conditional log likelihood and KL divergence terms over time steps and hierarchical layers:\nF(θ, φ) = T∑ t=1 L∑ l=1 EQ∗(z1:lt ) log pθx ( xlt|z1:lt ) +\nT∑ t=1 EQ∗(z1t−1)DKL ( qφ ( z1t |x1:L1:T ,z1t−1 )∥∥∥pθ (z1t |z1t−1)) +\nT∑ t=1 L∑ l=2 EQ∗(z1t−1,zl−1t )\nDKL ( qφ ( zlt|x1:L1:T ,zlt−1,zl−1t )∥∥∥pθ (z1t |z1t−1,zl−1t )) (5) where Q∗ (·) denotes the marginal distribution of (·) from qφ . The details about the factorization and the marginalized distribution are provided in the supplementary materials.\nParameterization of inference network We parameterize the inference network and construct the variational approximation qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) used in Equation 5 by deep learning models. First, we use L RNNs to capture MR-MTS with L different sampling rates such that each rate is modeled by one RNN model separately. Second, to obtain lth latent states zlt of the inference network at time step t, we not only use the previous latent states zlt−1 and the lower layer latent states z l−1 t but also take the lth RNN output denoted by hlt as an input. Third, we reuse\nthe same latent state distribution and switch mechanism from the generation model to generate z of the inference network. To be more specific, zlt is drawn from a multivariate normal distribution, where the mean and covariance are reused from those of zlt−1 if s l t = 1 and l > 1, otherwise the mean and covariance are modeled by gated recurrent units (GRU) with input [ hlt, z l t−1, z l−1 t ] . The choice of the RNN models for hlt affects what and how the information at other time steps is considered in the approximation at time t, i.e. the form of qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) . Inspired by Krishnan et al. (2016), we construct the variational approximation in three settings (filtering, smoothing, bi-direction) for forecasting and interpolation tasks. In filtering setting, we only consider the information up to time t (i.e., x1:L1:t ) using forward RNNs. By doing this, we have hlt = hlt forward = RNN forward ( hlt−1 forward ,xlt ) , and thus\nqφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) = qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) . The filtering setting does not use future information, so it is suitable for forecasting task at future time step t′ > T . For interpolation tasks, we can use backward RNNs to utilize the information after time t (i.e., x1:Lt:T ) with h l t = h l t backward =\nRNN backward ( hlt+1 backward ,xlt ) , or bi-directional RNNs to uti-\nlize information across all time steps, which is x1:L1:T , at any time t with hlt = [ hlt forward ,hlt backward ] . These two models lead to smoothing and bidirection settings, respectively. We summarize the three inference networks in Table 1. We use φh and φz to denote the parameter sets related to h and z respectively and use φ = {φh, φz, φs = θs} to represent the parameter set of the inference network."
  }, {
    "heading": "3.3. Learning the Parameters",
    "text": "We jointly learn the parameters (θ, φ) of the generative model pθ and the inference network qφ by maximizing the ELBO in Equation (5). The main challenge in the optimization is obtaining the gradients of all the terms under the correct expectation i.e, EQ∗ . We use stochastic backpropagation (Kingma & Welling, 2013) for estimating all these gradients and train the model by stochastic gradient descent (SGD) approaches. We employ ancestral sampling techniques to obtain the samples z . That is, we draw all samples z in a sequential way from time 1 to T and from layer 1 to L. Given the samples from previous layer l − 1 or previous time t− 1, the new samples at time t and layer l will be distributed according to the marginal distribution Q∗. Notice\nAlgorithm 2 Learning MR-HDMM with stochastic backpropagation and SGD Require: X : a set of MR-MTS of L sampling rates; Initial (θ, φ)\n1: while not converged do 2: Choose a random minibatch of MR-MTS X ′ ⊂ X 3: for each sample x1:L1:T ∈ X ′ do 4: Compute h1:L1:T by inference network φh on input x 1:L 1:T 5: Sample ẑ1:L0 ∼ N (0, I) 6: for t = 1, · · · , T do 7: Estimate µ1t (φ) ,Σ1t\n(φ) by φz , and µ1t ,Σ1t by θz , given samples ẑ1t−1 and h 1 t\n8: Based onµ1t (φ) ,Σ1t (φ) ,µ1t ,Σ 1 t , compute the gradient of DKL ( qφ ( z1t |· )∥∥∥pθ (z1t |·)) 9: Sample ẑ1t ∼ N ( µ1t (φ) ,Σ1t (φ) )\n10: for l = 2, · · · , L do 11: Compute slt by θs from samples ẑlt−1 and ẑ l−1 t 12: Estimate µlt (φ) ,Σlt (φ) by φz , and µlt,Σlt by θz ,\ngiven samples ẑlt−1, ẑ l−1 t , s l t, and hlt\n13: Based on µlt (φ) ,Σlt (φ) ,µlt,Σ l t, compute the gradient of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·))\n14: Sample ẑlt ∼ N ( µlt (φ) ,Σlt (φ) ) 15: end for 16: Compute the gradient of log pθx ( xlt|ẑ1:lt\n) 17: end for 18: end for 19: Update (θ, φ) using all gradients 20: end while\nthat all terms of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·)) in Equation (5) are KL divergences between two multivariate Gaussian distributions, and pθx ( xlt|z1:lt ) is also a multivariate Gaussian distribution. Thus, all the required gradients can be estimated analytically from the samples drawn in our proposed way. Algorithm 2 shows the overall learning procedure."
  }, {
    "heading": "4. Experiments",
    "text": "We conducted experiments on two real-world datasets - the MIMIC-III healthcare dataset and the USHCN climate dataset - and answer the following questions: (a) How does our proposed model perform when compared to the existing state-of-the-art approaches? (b) To what extent, are the proposed learnable hierarchical latent structure and auxiliary connections useful to model the data generation process? (c) How do we interpret the hierarchy learned by the proposed model? In the remainder of this section, we will describe the datasets, methods, empirical results and interpretations to answer the above questions."
  }, {
    "heading": "4.1. Datasets and Experimental Design",
    "text": "MIMIC-III dataset MIMIC-III is a public de-identified dataset collected at Beth Israel Deaconess Medical Cen-\nter from 2001 to 2012 (Johnson et al., 2016). It contains over 58,000 hospital admission records of 38,645 adults and 7,875 neonates. For our experiments, we chose 10,709 adult admission records and extracted 62 temporal features from the first 72 hours. These features had one of the three sampling rates of 1 hour, 4 hours and 12 hours. To fill-in any missing entries in our dataset we used forward or linear imputation similar to Che et al. (2016). To ensure fair comparison, we only evaluate and compare all the models on the original time-series (i.e. non-imputed data). Our main tasks on the MIMIC-III dataset are forecasting on time series with all rates, and interpolation of the low-rate time series values.\nUSHCN climate dataset The U.S. Historical Climatology Network Monthly (USHCN) dataset (Menne et al., 2010) is publicly available and consists of daily meteorological data of 54 stations in California spanning from 1887 to 2009. It has five climate variables for each station: a) daily maximum temperature, b) daily minimum temperature, c) whether it was a snowy day or not, d) total daily precipitation, and e) daily snow precipitation. We preprocessed this dataset to extract daily climate data for 100 consecutive years starting from 1909. To get multi-rate time series data, we extract 208 features and split all features into 3 groups with sampling rates of 1 day, 5 days, and 10 days respectively. This public dataset has been carefully processed by National Oceanic and Atmospheric Administration (NOAA) to ensure quality control and it has no missing entries. Our tasks on this dataset are climate forecasting on all features and interpolation on 5-day and 10-day sampled data.\nTasks We use the proposed MR-HDMM on two prediction tasks: multi-rate time series forecasting and low-rate time series interpolation. Since both datasets have 3 different sampling rates, we use HSR/MSR/LSR to denote high/medium/low sampling rate respectively.\n• Forecasting: Predict the future multivariate time series based on its history. For MIMIC-III dataset, we predict the last 24 hrs time series based on the first (previous) 48 hours time series data. In USHCN dataset, we forecast the climate for the next 30 days based on the observations of the previous year. • Interpolation: Fill-in the low rate time series based on co-evolving higher rate time series data. For MIMIC-III dataset, we down-sampled 8 features from MSR to LSR and then performed interpolation task by up-sampling these 8 features back to MSR. For USHCN dataset, the interpolation task involved up-sampling the MSR and LSR features to HSR features, i.e. up-sample 5-day and 10-day data to 1-day. We demonstrate in-sample interpolation (i.e. interpolation within training dataset) and out-sample interpolation (i.e. interpolation in the testing dataset) on the MIMIC-III dataset and in-sample interpolation on the USHCN dataset.\nBaselines We compare MR-HDMM with several strong baselines in these two tasks. Additionally, to show the advantage of learnable hierarchical latent structure and auxiliary connections, we simplify MR-HDMM into two other models for comparison: (a) Multi-Rate Deep Markov Models (MR-DMM) which removes the hierarchical structure in latent space; (b) Hierarchical Deep Markov Models (HDMM) which drops the auxiliary connections between the lowerrate time series and higher level latent layers. MR-DMM and HDMM are discussed in the supplementary materials.\nFor forecasting tasks, we compare MR-HDMM with the following baseline models:\n• Single-rate: Kalman Filters (KF), Vector AutoRegression (VAR), Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), PhasedLSTM (PLSTM) (Neil et al., 2016), Deep Markov Models (DMM) (Krishnan et al., 2015) and Hierarchical Multiscale Recurrent Neural Networks (HM-RNN) (Chung et al., 2016). • Multi-rate: Multiple Kalman Filters (MKF) (Drolet et al.,\n2000), Multi-rate Kalman Filters (MR-KF) (Safari et al., 2014), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM).\nFor interpolation task, we compare MR-HDMM with the following baseline models:\n• Imputation methods: Mean imputation (Simple-Mean), Cubic Spline (CubicSpline) (De Boor et al., 1978), Multiple Imputations by Chained Equations (MICE) (White et al., 2011), MissForest (Stekhoven & Bühlmann, 2011), SoftImpute (Mazumder et al., 2010). • Deep learning models: Deep Markov Models (DMM), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM)."
  }, {
    "heading": "4.2. Evaluation and Implementation Details",
    "text": "We show the evaluation results of our MR-HDMM on the following: (a) Forecasting: we generate the next latent state using the learned transition distribution and then generate observations from these new latent states; (b) Interpolation: we use the mode of the approximated posterior in the generation model to generate the unseen data in low-rate time series. (c) Inference: we take multi-rate time series as the input to obtain the approximate posterior of latent states.\nFor generation model in MR-HDMM, we use multivariate Gaussian with diagonal covariance for both emission distribution and transition distribution. We parameterized the emission mapping gθx by a 3-layer MLP with ReLU activations, the transition mapping gθz by gated recurrent unit (GRU), and mapping gθs by a 3-layer MLP with ReLU activations on the hidden layers and linear activations on the output layer. For inference networks, we adopt filter-\ning setting for forecasting and bidirection setting for interpolation from Table 1 with 3-layer GRUs. To update θs, we replace the sign function with a sharp sigmoid function during training, and use the indicator function during validation. The single-rate baseline models cannot handle multi-rate data directly, and we up-sample all the lower rate data into higher rate data using linear interpolation. We use the stats-toolbox (Seabold & Perktold, 2010) in python for the VAR model implementation. We use pykalman (Duckworth, 2013) to implement all the KFbased models. The implementation details of the KF-based methods are discussed in the supplementary materials. For LSTM and PLSTM model, we use one layer with 100 neurons to model the time-series, and then apply a soft-max regressor on top of the last hidden state to do regression.\nTo ensure a fair comparison, we use roughly the same amount of parameters for all models. For experiments on USHCN dataset, train/valid/test sets were split as 70/10/20. For experiments on MIMIC-III, we used 5-fold cross validation (train on 3 folds, validate on another fold and test on the remaining fold) and report the average Mean Squared Error (MSE) of 5 runs for both forecasting and interpolation tasks. Note that, we train all the deep learning models with the Adam optimization method (Kingma & Ba, 2014) and use validation set to find the best weights, and report the results on the held-out test set. All the input variables are normalized to be of 0 mean and 1 standard deviation."
  }, {
    "heading": "4.3. Quantitative Results",
    "text": "Forecasting Table 2 and 3 respectively show the forecasting results on MIMIC-III and USHCN datasets in terms of MSE. Our proposed MR-HDMM outperforms all the competing multi-rate latent space models by at least 5% and beats the single-rate models by at least 15% on both datasets with all features. Our model also performs the best on single-rate HSR and MSR forecasting tasks, and performs well on the LSR forecasting task on MIMIC-III and USHCN datasets.\nInterpolation Table 4 shows the interpolation results on the two datasets. Since VAR and LSTM cannot be directly\nused for the interpolation task, we focus on evaluating generative models and imputation methods. From Table 4, we observe that our proposed model outperforms the baselines and the competing multi-rate latent space models by a large margin on all the interpolation tasks on these two datasets.\nInference We also compare the lower bound of loglikelihood of all generative models in Table 5. The higher\nlower bound value indicates a better fitted model given the training data. Our MR-HDMM model achieves the best performance on both datasets."
  }, {
    "heading": "4.4. Discussion",
    "text": "In all our experiments, MR-HDMM outperforms other generative models by a significant margin. Considering that all the deep generative models have the same amount of parameters, this improvement empirically demonstrates the effectiveness of our proposed learnable latent hierarchical structure and auxiliary connections. In Figure 3(a) and 3(b), we visualize the latent hierarchical structure of MR-HDMM learned from the first 48 hours of an admission in MIMICIII dataset and one-year climate observations in USHCN dataset. A color block indicates that the latent state zlt is updated from zlt−1 and z l−1 t (update), while the white block indicates zlt is generated from the same distribution of z l t−1 (reuse). As expected, the higher latent layers tend to update less frequently and capture the long-term temporal dependencies. To understand learned hierarchical structure more intuitively, we also show precipitation time series from USCHN dataset along with learned switches in Figure 3(b). We observe that the higher latent layer tends to update along with the precipitation, which is reasonable since precipitation makes significant changes to the underlying weather condition which is captured by the higher latent layer."
  }, {
    "heading": "5. Summary",
    "text": "We proposed the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM) - a novel deep generative model for forecasting and interpolation tasks on multi-rate multivariate time series (MR-MTS) data. MR-HDMM models the data generation process by learning a latent hierarchical structure using auxiliary connections and learnable switches to capture the temporal dependencies. Empirically we showed that our proposed model outperforms the existing single-rate and multi-rate models on healthcare and climate datasets."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported in part by NSF Research Grant IIS-1254206 and IIS-1539608, and MURI grant W911NF-11-1-0332. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government."
  }],
  "year": 2018,
  "references": [{
    "title": "On multi-rate fusion for non-linear sampled-data systems: Application to a 6d tracking system",
    "authors": ["L. Armesto", "J. Tornero", "M. Vincze"],
    "venue": "Robotics and Autonomous Systems,",
    "year": 2008
  }, {
    "title": "Recurrent neural networks for multivariate time series with missing values",
    "authors": ["Z. Che", "S. Purushotham", "K. Cho", "D. Sontag", "Y. Liu"],
    "venue": "arXiv preprint arXiv:1606.01865,",
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1412.3555,",
    "year": 2014
  }, {
    "title": "Hierarchical multiscale recurrent neural networks",
    "authors": ["J. Chung", "S. Ahn", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1609.01704,",
    "year": 2016
  }, {
    "title": "A practical guide to splines, volume 27",
    "authors": ["C. De Boor", "Mathématicien", "E.-U"],
    "year": 1978
  }, {
    "title": "Adaptable sensor fusion using multiple kalman filters",
    "authors": ["L. Drolet", "F. Michaud", "J. Côté"],
    "venue": "In Intelligent Robots and Systems,",
    "year": 2000
  }, {
    "title": "pykalman, an implementation of the kalman filter, kalman smoother, and em algorithm in python",
    "authors": ["D. Duckworth"],
    "venue": "https://pykalman.github.com,",
    "year": 2013
  }, {
    "title": "Time series analysis by state space methods, volume 38",
    "authors": ["J. Durbin", "S.J. Koopman"],
    "venue": "OUP Oxford,",
    "year": 2012
  }, {
    "title": "Hierarchical recurrent neural networks for long-term dependencies",
    "authors": ["S. El Hihi", "Y. Bengio"],
    "venue": "In NIPS,",
    "year": 1995
  }, {
    "title": "Sequential neural models with stochastic layers",
    "authors": ["M. Fraccaro", "S.K. Sønderby", "U. Paquet", "O. Winther"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Deep temporal sigmoid belief networks for sequence modeling",
    "authors": ["Z. Gan", "C. Li", "R. Henao", "D.E. Carlson", "L. Carin"],
    "venue": "In NIPS,",
    "year": 2015
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["A. Graves", "Mohamed", "A.-r", "G. Hinton"],
    "venue": "In ICASSP,",
    "year": 2013
  }, {
    "title": "Training and analysing deep recurrent neural networks. In Advances in neural information processing",
    "authors": ["M. Hermans", "B. Schrauwen"],
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural computation,",
    "year": 1997
  }, {
    "title": "A sequential ensemble kalman filter for atmospheric data assimilation",
    "authors": ["P.L. Houtekamer", "H.L. Mitchell"],
    "venue": "Monthly Weather Review,",
    "year": 2001
  }, {
    "title": "Mimic-iii, a freely accessible critical care database",
    "authors": ["A. Johnson", "T. Pollard", "L. Shen", "L. Lehman", "M. Feng", "M. Ghassemi", "B. Moody", "P. Szolovits", "L. Celi", "R. Mark"],
    "venue": "Scientific Data,",
    "year": 2016
  }, {
    "title": "Learning in graphical models, volume 89",
    "authors": ["M.I. Jordan"],
    "venue": "Springer Science & Business Media,",
    "year": 1998
  }, {
    "title": "A new approach to linear filtering and prediction problems",
    "authors": ["Kalman", "R. E"],
    "venue": "Journal of basic Engineering,",
    "year": 1960
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "A clockwork rnn",
    "authors": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Structured inference networks for nonlinear state space models",
    "authors": ["R.G. Krishnan", "U. Shalit", "D. Sontag"],
    "venue": "arXiv preprint arXiv:1609.09869,",
    "year": 2016
  }, {
    "title": "Learning recurrent neural networks with hessian-free optimization",
    "authors": ["J. Martens", "I. Sutskever"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Spectral regularization algorithms for learning large incomplete matrices",
    "authors": ["R. Mazumder", "T. Hastie", "R. Tibshirani"],
    "venue": "Journal of machine learning research,",
    "year": 2010
  }, {
    "title": "Long-term daily and monthly climate records from stations across the contiguous united",
    "authors": ["M. Menne", "C. Williams Jr.", "R. Vose"],
    "year": 2010
  }, {
    "title": "Recurrent neural network based language model",
    "authors": ["T. Mikolov", "M. Karafiát", "L. Burget", "J. Cernockỳ", "S. Khudanpur"],
    "venue": "In Interspeech,",
    "year": 2010
  }, {
    "title": "Robot localization and Kalman filters",
    "authors": ["R. Negenborn"],
    "venue": "PhD thesis, Utrecht University,",
    "year": 2003
  }, {
    "title": "Phased lstm: Accelerating recurrent network training for long or event-based sequences",
    "authors": ["D. Neil", "M. Pfeiffer", "Liu", "S.-C"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["R. Pascanu", "T. Mikolov", "Y. Bengio"],
    "venue": "ICML (3),",
    "year": 2013
  }, {
    "title": "A tutorial on hidden markov models and selected applications in speech recognition",
    "authors": ["L.R. Rabiner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1989
  }, {
    "title": "Elements of multivariate time series analysis",
    "authors": ["G.C. Reinsel"],
    "venue": "Springer Science & Business Media,",
    "year": 2003
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "arXiv preprint arXiv:1401.4082,",
    "year": 2014
  }, {
    "title": "Multirate multisensor data fusion for linear systems using kalman filters and a neural network",
    "authors": ["S. Safari", "F. Shabani", "D. Simon"],
    "venue": "Aerospace Science and Technology,",
    "year": 2014
  }, {
    "title": "Statsmodels: Econometric and statistical modeling with python",
    "authors": ["S. Seabold", "J. Perktold"],
    "venue": "In Proceedings of the 9th Python in Science Conference,",
    "year": 2010
  }, {
    "title": "Parsing natural scenes and natural language with recursive neural networks",
    "authors": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"],
    "venue": "In ICML,",
    "year": 2011
  }, {
    "title": "Missforest—nonparametric missing value imputation for mixed-type data",
    "authors": ["D.J. Stekhoven", "P. Bühlmann"],
    "year": 2011
  }, {
    "title": "Multiple imputation using chained equations: issues and guidance for practice",
    "authors": ["I.R. White", "P. Royston", "A.M. Wood"],
    "venue": "Statistics in medicine,",
    "year": 2011
  }, {
    "title": "Show, attend and tell: Neural image caption generation with visual attention",
    "authors": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"],
    "venue": "In ICML,",
    "year": 2015
  }],
  "id": "SP:bd82a5048345e6ab6a03a96aa985c49b7b6446f0",
  "authors": [{
    "name": "Zhengping Che",
    "affiliations": []
  }, {
    "name": "Sanjay Purushotham",
    "affiliations": []
  }, {
    "name": "Guangyu Li",
    "affiliations": []
  }, {
    "name": "Bo Jiang",
    "affiliations": []
  }, {
    "name": "Yan Liu",
    "affiliations": []
  }],
  "abstractText": "Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies. State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data. To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS. Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks. In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.",
  "title": "Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series"
}