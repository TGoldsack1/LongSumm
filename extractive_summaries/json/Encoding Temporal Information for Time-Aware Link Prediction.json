{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2350–2354, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics\nMost existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently."
  }, {
    "heading": "1 Introduction",
    "text": "Knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) and YAGO (Fabian et al., 2007) play a pivotal role in many NLP related applications. KBs consist of facts in the form of triplets (ei, r, ej), indicating that head entity ei and tail entity ej is linked by relation r. Although KBs are large, they are far from complete. Link prediction is to predict relations between entities based on existing triplets, which can alleviate the incompleteness of current KBs. Recently a promising approach for this task called knowledge base embedding (Nickel et al., 2011; Bordes et al., 2011; Socher et al., 2013) aims to embed entities and relations into a continuous vector space while preserving certain information of the KB graph. TransE (Bordes et al., 2013) is a typical model considering relation vector as trans-\nlating operations between head and tail vector, i.e., ei + r ≈ ej when (ei, r, ej) holds.\nMost existing KB embedding methods solely learn from time-unknown facts but ignore the useful temporal information in the KB. In fact, there are many temporal facts (or events) in the KB, e.g., (Obama, wasBornIn, Hawaii) happened at August 4, 1961. (Obama, presidentOf, USA) is true since 2009. Current KBs such as YAGO and Freebase store such temporal information either directly or indirectly. The happening time of time-sensitive facts may indicate special temporal order of facts and time-sensitive relations. For example, (Einstein, wasBornIn, Ulm) happened in 1879, (Einstein, wonPrize, Nobel Prize) happened in 1922, (Einstein, diedIn, Princeton) occurred in 1955. We can infer the temporal order of time-sensitive relations from all such kinds of facts: wasBornIn → wonPrize → diedIn. Traditional KB embedding models such as TransE often confuse relations such as wasBornIn and diedIn when predicting (person,?,location) because TransE learns only from time-unknown facts and cannot distinguish relations with similar semantic meaning. To make more accurate predictions, it is non-trivial for existing KB embedding methods to incorporate temporal order information.\nThis paper mainly focuses on incorporating the temporal order information and proposes a timeaware link prediction model. A new temporal dimension is added to fact triples, denoted as a quadruple: (ei, r, ej , tr), indicating the fact happened at time tr1. To make the embedding space compati-\n1tr is the absolute beginning time when the fact is true, e.g., ”1961-08-04” for (Obama, wasBornIn, Hawaii).\n2350\nble with the observed triple in the fact dimension, relation vectors behave as translations between entity vectors similarly as TransE models. To incorporate temporal order information between pair-wise temporal facts, we assume that prior time-sensitive relation vector can evolve into a subsequent timesensitive relation vector through a temporal transition. For example, we have two temporal facts sharing the same head entity: (ei, ri, ej , t1) and (ei, rj , ek, t2) and the temporal order constraint t1< t2, i.e., ri happens before rj , then we propose the assumption that prior relation ri after temporal transition should lie close to subsequent relation rj , i.e., riM ≈ rj , here matrix M captures the temporal order information between relations. In this way, both semantic and temporal information are embedded into a continuous vector space during learning. To the best of our knowledge, we are the first to consider such temporal information for KB embedding.\nWe evaluate our approach on public available datasets and our method outperforms state-of-the-art methods in the time-aware link prediction and triple classification tasks."
  }, {
    "heading": "2 Time-Aware KB Embedding",
    "text": "Traditional KB embedding methods encode only observed fact triples but neglect temporal constraints between time-sensitive entities and facts. To deal with this limitation, we introduce Time-Aware KB Embedding which constrains the task by incorporating temporal constraints.\nTo consider the happening time of facts, we formulate a temporal order constraint as an optimization problem based on a manifold regularization term. Specially, temporal order of relations in timesensitive facts should affect KB representation. If ri and rj share the same head entity ei, and ri occurs before rj , then prior relation’s vector ri could evolve into subsequent relation’s vector rj in the temporal dimension.\nTo encode the transition between time-sensitive relations, we define a transition matrix M ∈ Rn×n between pair-wise temporal ordering relation pair (ri, rj). Our optimization requires that positive temporal ordering relation pairs should have lower scores (energies) than negative pairs, so we define a\ntemporal order score function as\ng(ri, rj) = ‖riM− rj‖1, (1)\nwhich is expected to be a low score when the relation pair is in chronological order, and high otherwise.\nTo make the embedding space compatible with the observed triples, we make use of the triple set ∆ and follow the same strategy adopted in previous methods such as TransE.\nf(ei, r, ej) = ‖ei + r− ej‖1. (2)\nFor each candidate triple, it requires positive triples to have lower scores than negative triples.\nThe optimization is to minimize the joint score function,\nL= ∑\nx+∈∆\n[ ∑\nx−∈∆′ [γ1 + f(x\n+)− f(x−)]+\n+λ ∑\ny+∈Ωei ,y −∈Ω′ei\n[γ2 + g(y +)− g(y−)]+\n] (3)\nwhere x+ = (ei, ri, ej) ∈ ∆ is the positive triple (quad), x−=(e′i, ri, e ′ j)∈∆′ is corresponding the negative triple. y+ = (ri, rj)∈Ωei is the positive relation ordering pair with respect to (ei, ri, ej , tx). It’s defined as\nΩei = {(ri, rj)|(ei, ri, ej , tx)∈∆τ , (ei, rj , ek, ty)∈∆τ , tx< ty}, (4)\nwhere ri and rj share the same head entity ei, and y− = (rj , ri) ∈ Ω′ei are the corresponding negative relation order pairs by inverse. In experiments, we enforce constrains as ‖ei‖2 ≤ 1, ‖ri‖2 ≤ 1, ‖rj‖ ≤ 1 and ‖riM‖2 ≤ 1.\nThe first term in Equation (3) enforces the resultant embedding space compatible with all the observed triples, and the second term further requires the space to be temporally consistent and more accurate. Hyperparameter λ makes a trade-off between the two cases. Stochastic gradient descent (in minibatch mode) is adopted to solve the minimization problem."
  }, {
    "heading": "3 Experiments",
    "text": "We adopt the same evaluation metrics for timeaware KB embedding in two tasks: link prediction (Bordes et al., 2013) and triple classification (Socher et al., 2013)."
  }, {
    "heading": "3.1 Datasets",
    "text": "We create two temporal datasets from YAGO2 (Hoffart et al., 2013), consisting of time-sensitive facts. In YAGO2, MetaFacts contains all happening time for facts. DateFacts contains all creation time for entities. First, to make a pure time-sensitive dataset where all facts have time annotations, we selected the subset of entities that have at least 2 mentions in MetaFacts and DateFacts. This resulted in 15,914 triples (quadruples) which were randomly split with the ratio shown in Table 1. This dataset is denoted YG15k. Second, to make a mixed dataset, we created YG36k where 50% facts have time annotations and 50% do not. We will release the data upon request."
  }, {
    "heading": "3.2 Link Prediction",
    "text": "Link prediction is to complete the triple (h, r, t) when h, r or t is missing."
  }, {
    "heading": "3.2.1 Entity Prediction",
    "text": "Evaluation protocol. For each test triple with missing head or tail entity, various methods are used to compute the scores for all candidate entities and rank them in descending order. We use two metrics for our evaluation as in (Bordes et al., 2013): the mean of correct entity ranks (Mean Rank) and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the metrics are desirable but flawed when a corrupted triple exists in the KB. As a countermeasure, we filter out all these valid triples in the KB before ranking. We name the first evaluation set as Raw and the second as Filter. Baseline methods. For comparison, we select translating methods such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014b) and TransR (Lin et al., 2015b) as our baselines. We then use time-aware embedding based on these methods to obtain the corresponding time-aware embedding models. A model with time-aware embedding is denoted as ”tTransE” for example. Implementation details. For all methods, we create 100 mini-batches on each data set. The di-\nmension of the embedding n is set in the range of {20,50,100}, the margin γ1 and γ2 are set in the range {1,2,4,10}. The learning rate is set in the range {0.1, 0.01, 0.001}. The regularization hyperparameter λ is tuned in {10−1,10−2,10−3,10−4}. The best configuration is determined according to the mean rank in validation set. The optimal configurations are n=100,γ1=γ2=4,λ=10−2, learning rate is 0.001 and taking `1−norm. Results. Table 2 reports the results on the test set. From the results, we can see that time-aware embedding methods outperform all the baselines on all the data sets and with all the metrics. The improvements are usually quite significant. The Mean Rank drops by about 75%, and Hits@10 rises about 19% to 30%. This demonstrates the superiority and generality of our method. When dealing with sparse data YG15k, all the temporal information is utilized to model temporal associations and make the embeddings more accurate, so it obtains better improvement than mixing the time-unknown triples in YG36k."
  }, {
    "heading": "3.2.2 Relation Prediction",
    "text": "Relation prediction aims to predict relations given two entities. Evaluation results are shown in Table 3 on only YG15K due to limited space, where we report Hits@1 instead of Hits@10. Example prediction results for TransE and tTransE are compared in Table 4. For example, when testing (Billy Hughes,?,London,1862), it’s easy for TransE to mix relations wasBornIn and diedIn because they act similarly for a person and a place. But known that (Billy Hughes, isAffiliatedTo, National Labor Party) happened in 1916, and tTransE have learnt temporal order that wasBornIn→isAffiliatedTo→diedIn, so the regularization term |rbornT − raffiliated| is smaller than |rdiedT− raffiliated|, so correct answer wasBornIn ranks higher than diedIn."
  }, {
    "heading": "3.3 Triple Classification",
    "text": "Triple classification aims to judge whether an unseen triple is correct or not. Evaluation protocol. We follow the same evaluation protocol used in Socher et al. (2013). To create labeled data for classification, for each triple in the test and validation sets, we construct a corresponding negative triple by randomly corrupting the entities. To corrupt a position (head or tail), only entities that have appeared in that position are allowed. During triple classification, a triple is predicted as positive if the score is below a relation-specific threshold δr; otherwise as negative. We report averaged accuracy on the test sets. Implementation details. We use the same hyperparameter settings as in the link prediction task. The relation-specific threshold δr is determined by maximizing averaged accuracy on the validation sets. Results. Table 5 reports the results on the test sets. The results indicate that time-aware embedding outperforms all the baselines consistently. Temporal order information may help to distinguish positive and negative triples as different head entities may have different temporally associated relations. If the temporal order is the same with most facts, the regularization term helps it get lower energies and vice versa."
  }, {
    "heading": "4 Related Work",
    "text": "Many models have been proposed for KB embedding (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). External information is employed to improve KB embedding such as text (Riedel et al.,\n2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), and relation path (Lin et al., 2015a; Gu et al., 2015). However, these methods solely rely on triple facts but neglect temporal order constraints between facts. Temporal information such as relation ordering in text has been explored (Talukdar et al., 2012; Chambers et al., 2014; Bethard, 2013; Cassidy et al., 2014; Chambers et al., 2007; Chambers and Jurafsky, 2008). This paper proposes a time-aware embedding approach that employs temporal order constraints to improve KB embedding."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "In this paper, we propose a general time-aware KB embedding, which incorporates creation time of entities and imposes temporal order constraints on the geometric structure of the embedding space and enforce it to be temporally consistent and accurate. As future work: (1) We will incorporate the valid time of facts. (2) Some time-sensitive facts lack temporal information in YAGO2, we will mine such temporal information from texts."
  }, {
    "heading": "Acknowledgments",
    "text": "This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact author for this paper is Baobao Chang and Zhifang Sui."
  }],
  "year": 2016,
  "references": [{
    "title": "Cleartk-timeml: A minimalist",
    "authors": ["Steven Bethard"],
    "year": 2013
  }, {
    "title": "Freebase: a collabo",
    "authors": ["turge", "Jamie Taylor"],
    "year": 2008
  }, {
    "title": "Learning structured embed",
    "authors": ["Yoshua Bengio"],
    "year": 2011
  }, {
    "title": "Dense event ordering with",
    "authors": ["Steven Bethard"],
    "year": 2014
  }, {
    "title": "Traversing knowledge graphs in vector space",
    "authors": ["Kelvin Gu", "John Miller", "Percy Liang."],
    "venue": "arXiv preprint arXiv:1506.01094.",
    "year": 2015
  }, {
    "title": "Semantically smooth knowledge graph embedding",
    "authors": ["Shu Guo", "Quan Wang", "Bin Wang", "Lihong Wang", "Li Guo."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural",
    "year": 2015
  }, {
    "title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia",
    "authors": ["Johannes Hoffart", "Fabian M Suchanek", "Klaus Berberich", "Gerhard Weikum."],
    "venue": "Artificial Intelligence, 194:28–61.",
    "year": 2013
  }, {
    "title": "Modeling relation paths for representation learning of knowledge bases",
    "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun."],
    "venue": "arXiv preprint arXiv:1506.00379.",
    "year": 2015
  }, {
    "title": "Learning entity and relation embeddings for knowledge graph completion",
    "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"],
    "year": 2015
  }, {
    "title": "A three-way model for collective learning on multi-relational data",
    "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."],
    "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 809–816.",
    "year": 2011
  }, {
    "title": "Relation extraction with matrix factorization and universal schemas",
    "authors": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M Marlin"],
    "year": 2013
  }, {
    "title": "Reasoning with neural tensor networks for knowledge base completion",
    "authors": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."],
    "venue": "Advances in Neural Information Processing Systems, pages 926–934.",
    "year": 2013
  }, {
    "title": "Acquiring temporal constraints between relations",
    "authors": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell."],
    "venue": "CIKM.",
    "year": 2012
  }, {
    "title": "Knowledge graph and text jointly embedding",
    "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)., pages 1591–1601.",
    "year": 2014
  }, {
    "title": "Knowledge graph embedding by translating on hyperplanes",
    "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."],
    "venue": "Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, pages 1112–1119.",
    "year": 2014
  }, {
    "title": "Representation learning for measuring entity relatedness with rich information",
    "authors": ["Yu Zhao", "Zhiyuan Liu", "Maosong Sun."],
    "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 1412–1418. AAAI Press.",
    "year": 2015
  }],
  "id": "SP:fd458e7109e8ebac2f59d399981054957078c7a4",
  "authors": [{
    "name": "Tingsong Jiang",
    "affiliations": []
  }, {
    "name": "Tianyu Liu",
    "affiliations": []
  }, {
    "name": "Tao Ge",
    "affiliations": []
  }, {
    "name": "Lei Sha",
    "affiliations": []
  }, {
    "name": "Sujian Li",
    "affiliations": []
  }, {
    "name": "Baobao Chang",
    "affiliations": []
  }, {
    "name": "Zhifang Sui",
    "affiliations": []
  }],
  "abstractText": "Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently.",
  "title": "Encoding Temporal Information for Time-Aware Link Prediction"
}