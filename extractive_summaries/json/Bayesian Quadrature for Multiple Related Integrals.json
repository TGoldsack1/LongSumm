{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Probabilistic numerics (Hennig et al., 2015) proposes approaching problems of numerical analysis from the point of view of statistics. In particular, Bayesian probabilistic numerical methods approach this problem from a Bayesian point of view, and can provide posterior distributions on the solutions of numerical problems (e.g. in the case of this paper, the solution of some integral). These posterior distributions represent our epistemic uncertainty about these quantities of interest. In the case of quadrature rules, the uncertainty is due to the fact that we only have a finite number of\n*Equal contribution 1Department of Mathematics, Imperial College London 2Department of Statistics, University of Warwick 3The Alan Turing Institute for Data Science and AI. Correspondence to: François-Xavier Briol <f-x.briol@warwick.ac.uk>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nfunction evaluations and therefore are uncertaint about the value of the integral. The notion of Bayesian probabilistic numerical method was independently introduced by several authors (Larkin, 1972; Kadane & Wasilkowski, 1985; Diaconis, 1988; O’Hagan, 1992), but only recently formalised by (Cockayne et al., 2017).\nApart from the uncertainty quantification property described above, these methods have several other advantages over “classical” (i.e. non-Bayesian) numerical methods (although some of the classical and Bayesian methods coincide (Diaconis, 1988)). First of all, they allow the user to formulate all of its prior knowledge in the form of a prior, making all of the assumptions of the numerical scheme explicit. Second of all, they can allow for coherent propagation of numerical uncertainties through chains of computation; see (Cockayne et al., 2017; Oates et al., 2017a).\nHowever, one property which has not been studied so far is the possibility of jointly inferring several quantities of interest. In this paper, we study the problem of numerically integrating a sequence of functions f1, . . . , fD (which are correlated to one another) with respect to some probability measure Π, and hence propose to build a model for joint inference of ∫ f1dΠ, . . . , ∫ fDdΠ. Such a joint model allows for better finite-sample performance, and can also lead to more refined posterior distributions on each of the individual integrals.\nTo tackle this problem, we extend the well-known Bayesian quadrature (O’Hagan, 1991) algorithm and study the performance of the proposed methodology from a theoretical and experimental point of view. In particular, we provide asymptotic convergence results for the marginal posterior variance on each of the integrals, both in the case of a well specified and misspecified prior. We also demonstrate the performance of our algorithm on some toy problems from the engineering literature on multi-fidelity models, and on a challenging problem from the field of computer graphics."
  }, {
    "heading": "2. Methodology",
    "text": "Bayesian Quadrature Let (X ,B,Π) be a probability space and consider some function f : X → R where X ⊆ Rp, p ∈ N+. The classical problem of numerical\nintegration is concerned with approximating the integral:\nΠ[f ] := ∫ X f(x)Π(dx),\nwhere we assume ∫ X f\n2(x)Π(dx) < ∞. Under fairly general conditions on f , one can show that an optimal algorithm (in terms of worst-case integration error in some function space) takes the form of a quadrature (or cubature) rule Π̂[f ] = ∑N i=1 wif(xi) for some weights {wi}Ni=1 ∈ R and samples {xi}Ni=1 ∈ X (see (Bakhvalov, 1971)). These are also sometimes denoted in vectorised form as Π[f ] = w>f(X) where w = (w1, . . . , wN )>, X = (x1, . . . ,xN )\n> and f(X) = (f(x1), . . . , f(xN ))>. The notation Π̂[f ] is motivated by the fact that we can see this object as an exact integral with respect to a discrete measure Π̂ = ∑N i=1 wiδxi , where δxi denotes the Dirac delta measure taking value 1 at xi and 0 otherwise. Many popular numerical integration methods take this form, including Newton–Cotes rules, Gaussian quadrature, Monte Carlo methods and sparse grids.\nLet (Ω,F ,P) be another probability space. Bayesian quadrature (BQ), introduced by (O’Hagan, 1991), proposes to approach the problem of numerical integration by first formulating a prior stochastic model g : X × Ω → R for the integrand f (where ∀ω ∈ Ω, g(·, ω) represents a realisation of g). This prior model is then conditioned on the vector of observations f(X) to obtain a posterior model for f , which is then pushed forward through the integral operator to give a posterior on Π[f ].\nA popular choice of prior is a Gaussian Process (GP) GP(m, k) with m : X → R denoting the mean function (i.e. m(x) = Eω[g(x, ω)]), and c : X × X → R denoting the covariance function/kernel (i.e. c(x,x′) = Eω[(g(x, ω)−m(x))(g(x′, ω)−m(x′))]). Let us assume that m = 0 (this can be done without loss of generality since the domain can be re-parametrized to be centred at 0). After conditioning on X , we have a new Gaussian process gN with mean and covariance:\nmN (x) = c(x,X)c(X,X) −1f(X),\ncN (x,x ′) = c(x,x′)− c(x,X)c(X,X)−1c(X,x′),\nfor all x,x′ ∈ X . Here, c(X,X) is the Gram matrix with entries (c(X,X))ij = c(xi,xj) and c(x,X) = (c(x,x1), . . . , c(x,xN )) whilst c(X,x) = c(x,X)> . The push-forward of this posterior through the integral operator is a Gaussian distribution with mean and variance:\nE [Π[gN ]] = Π[c(·,X)]c(X,X)−1f(X), V [Π[gN ]] = ΠΠ̄ [c]−Π[c(·,X)]c(X,X)−1Π̄[c(X, ·)],\nwhere Π[c(·,X)] = (Π[c(·,x1)], . . . ,Π[c(·,xN )]). These expression can be obtained in closed-form if the kernel mean Π[c(·,x)] = ∫ X c(x ′,x)Π(dx′) (also called\nthe representer of integration) and initial error ΠΠ̄[c] =∫ X×X c(x,x\n′)Π(dx)Π(dx′) can be obtained in closed form (here Π̄ indicates that the integral is taken with respect to the second argument).\nThe choice of covariance function c can be used to encode prior beliefs about the function f , such as smoothness or periodicity, and is very important to obtain good performance in practice. A popular example is the family of Matérn kernels\ncα(x,x ′) = λ2\n21−α\nΓ(α)\n(√ 2α ‖x− x′‖22\nσ2 )α × Jα (√ 2α ‖x− x′‖22\nσ2\n) , (1)\nfor σ, λ > 0 where Jα is the Bessel function of the second kind and α > 0 gives the smoothness of the kernel. On X = Rp, this will give an RKHS normequivalent to the Sobolev space Wα2 (Rd)1. Examples of infinitely smooth kernels include the squared-exponential kernel c(x,x′) = exp(−‖x− x′‖22/σ2) where σ > 0, the multi-quadric kernel c(x,x′) = (−1)dβe(σ2+‖x−x′‖22)β for β, σ > 0, β 6∈ N and the inverse multi-quadric kernel c(x,x′) = (σ2 + ‖x− x′‖22)−β for β, σ > 0.\nIn practice, numerical inversion can be challenging since the Gram matrix tends to be nearly singular, and so one may wish to regularise the matrix using a Tikhonov penalty. The inverses above can also potentially render the computation of the BQ estimator computationally expensive (up to O(N3) cost in the most general settings), although this can be alleviated in specific cases (Karvonen & Särkkä, 2017b). Even if this is not the case, the additional cost can be worthwhile regardless since the method has been shown to attain fast convergence rates (Briol et al., 2015a;b; Kanagawa et al., 2016; 2017; Bach, 2017) when the target integrand and the kernel used are smooth.\nRecent research directions in BQ include efficient sampling algorithms (for the point set X) to improve the performance of the method (Rasmussen & Ghahramani, 2002; Huszar & Duvenaud, 2012; Gunter et al., 2014; Briol et al., 2015a; Karvonen & Särkkä, 2017a; Briol et al., 2017), asymptotic convergence results (Briol et al., 2015a;b; Kanagawa et al., 2016; Bach, 2017) and equivalence of BQ with known quadrature rules for certain choices of point sets and kernels (Sarkka et al., 2016; Karvonen & Särkkä, 2017a). Furthermore, there has also been a wide range of new applications, including to other numerical methods in optimization, linear algebra and functional approximation (Kersting & Hennig, 2016; Fitzsimons et al., 2017), inference in complex computer models\n1We say that two norms ‖ · ‖1 and ‖ · ‖2 on a vector space are norm-equivalent if and only if ∃C1, C2 > 0 such that C1‖ · ‖2 ≤ ‖ · ‖1 ≤ C2‖ · ‖2.\n(Oates et al., 2017c), and problems in econometrics (Oettershagen, 2017) and computer graphics (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b).\nAlthough other stochastic processes could of course be used (Cockayne et al., 2017), GPs are popular due to their conjugacy properties, and the terminology Bayesian quadrature usually refers to this case. Note that other names for BQ with GP priors include Gaussian-process quadrature (Sarkka et al., 2016) or kernel quadrature (Bach, 2017; Briol et al., 2017; Kanagawa et al., 2017). In fact, a well-known alternative view of the posterior mean provided by BQ is that of an optimally-weighted quadrature rule in a reproducing kernel Hilbert spaces (RKHS) in the classical worst-case setting (Ritter, 2000). Let Hk be an RKHS with inner product and norm denoted 〈·, ·〉k and ‖·‖k respectively; i.e. a Hilbert space with an associated symmetric and positive definite reproducing kernel k : X × X → R such that f(x) = 〈f, k(·,x)〉k (see (Berlinet & Thomas-Agnan, 2004) for a detailed study). Suppose that our integrand f ∈ Hk and that ∫ X k(x,x)Π(dx) <∞. In that case, using the Cauchy–Schwarz inequality, the integration error can be decomposed as:∣∣∣Π[f ]− Π̂[f ]∣∣∣ ≤ ‖f‖k ∥∥∥Π [k(·,x)]− Π̂ [k(·,x)]∥∥∥\nk .\nThe corresponding worst-case error over the unit ball of the spaceHk is given by:\ne ( Hk, Π̂,X ) = sup ‖f‖k≤1 ∣∣∣Π[f ]− Π̂[f ]∣∣∣ = ∥∥∥Π [k(·,x)]− Π̂ [k(·,x)]∥∥∥\nk = ( w>k(X,X)w − 2Π[k(·,X)]>w + ΠΠ̄[k] ) 1 2 .\nThis final expression can be minimised in closed form over w ∈ RN to show that the optimal quadrature rule has weights w = Π[k(·,X)]k(X,X)−1. This corresponds exactly to the weights for the BQ posterior mean if we take our prior on f to be a GP(0, k), whilst the worst-case error can be shown to correspond to the posterior variance squared. The BQ estimator with prior GP(0, c) is therefore optimal in the classical worst-case sense for the RKHSHc.\nMulti-output Bayesian Quadrature We now extend the set-up of our problem. Suppose we have a sequence of probability spaces (Xd,Bd,Πd) and functions fd : Xd → R for which we are interested in numerically computing integrals of the form Πd[fd] for d = 1, . . . , D. In many applications where we are faced with this type of problem, we also have prior knowledge about correlations between the individual fd. However, this information is often ignored and the integrals are approximated individually. This is not a principled approach from a Bayesian point of\nview since it means we are not conditioning on all available information. In this section, we extend the BQ algorithm to solve this problem by building a joint model of f1, . . . , fD in order to obtain a joint posterior on the integrals Π1[f1], . . . ,ΠD[fD].\nFor notational convenience, we will restrict ourselves to the case where all of the input domains are identical and denoted X , all of the probability measures are identical and denoted Π, and the input sets X = {Xd}Dd=1 consist of N points Xd = (xd,1, . . . ,xd,N ) per output function fd (note the setup can be made more general if necessary). We re-frame the integration problem as that of integrating some vector-valued function f : X → RD such that f(x) = (f1(x), . . . , fD(x))\n>; i.e. we want to estimate Π[f ] = (Π[f1], . . . ,Π[fD])\n>. In this multiple-integral setting, we can have generalised quadrature rules of the form:\nΠ̂[fd] = D∑ d′=1 N∑ i=1 (Wi)dd′fd′(xd′,i)\nwhere Wi ∈ RD×D are weight matrices and (Wi)dd′ gives the influence of the value of fd′ at xd′,i on the estimate of Π[fd]. The quadrature rule for f can be re-written in compact form as Π̂[f ] = W>f(X) for some weight matrix W ∈ RND×D (a concatenation of {Wi}Ni=1) and function-evaluations vector f(X) = (f1(x1,1), . . . , f1(x1,N ), . . . , fD(xD,1), . . . , fD(xD,N )) >.\nThese generalised quadrature rules encompass popular Monte Carlo methods such as control variates or functionals (Glasserman, 2004; Oates et al., 2017b), multilevel Monte Carlo (Giles, 2015) and multi-fidelity Monte Carlo (Peherstorfer et al., 2016b). However, it is important to point out that these methods can only deal with very specific relations between integrands, usually requiring ( ∫ X (fd(x)−fd′(x)) 2Π(dx)) 1 2 to be small for all pairs of integrands fd, fd′ . Our method will be able to make use of much more complex relations.\nWe propose to approach this problem using an extended version of BQ, where we impose a prior g : X × Ω→ RD which is a GP(0,C) on the extended space (this is often called a multi-output GP or co-kriging model (Alvarez et al., 2012)) where now C is matrix-valued and (C(x,x′))dd′ = Eω∼P[gd(x, ω)gd′(x′, ω)]. In this case, after conditioning on X , we have a GP gN with vectorvalued mean mN : X → RD and matrix-valued covariance CN : X × X → RD×D:\nmN (x) = C(x,X)C(X,X) −1f(X),\nCN (x,x ′) = C(x,x′)−C(x,X)C(X,X)−1C(X,x′),\nfor C(x,X) = (C(x,x1), . . . , C(x,xN )) ∈ RD×ND and Gram matrix C(X,X) ∈ RND×ND is:\nC(X,X) =  (C(X1,X1))1,1 . . . (C(X1,XD))1,D (C(X2,X1))2,1 ... (C(X2,XD))2,D\n... ... ... (C(XD,X1))D,1 . . . (C(XD,XD))D,D\n ,\nwhere C(Xd,Xd′)d,d′ is an N × N matrix. The posterior on the value of the integral vector Π[f ] can also be obtained whenever the kernel mean Π[C(·,x)] and initial error ΠΠ̄ [C] are available in closed form, which is potentially a restrictive condition. The authors of (Briol et al., 2015b) give a table of closed-form expressions of these quantities for popular kernels in the uni-output case, and we envision the same type of table being necessary for future extensions of multi-output BQ. Alternatively, (Oates et al., 2017b; 2016) proposed a kernel which is tailored to the target probability measure Π and which could also be extended to the multi-output case.\nProposition 1. Consider multi-output Bayesian Quadrature with a GP(0,C) prior on f = (f1, . . . , fD)>. The posterior distribution on Π[f ] is a D-dimensional Gaussian distribution with mean and covariance matrix:\nE [Π[gN ]] = Π[C(·,X)]C(X,X)−1f(X), V [Π[gN ]] = ΠΠ̄ [C]−Π[C(·,X)]C(X,X)−1Π̄[C(X, ·)].\nAll proofs can be found in Appendix B. In this case, we clearly end up with a generalised quadrature rule with weight matrix: W BQ = (Π [C(·,X)]C(X,X)−1)> ∈ RND×D. In general, the computational cost for computing the posterior mean and variance is now of orderO(N3D3). However, several choices of kernels can reduce this cost significantly, and it is also possible to obtain sparse GP approximations; see e.g. (Álvarez & Lawrence, 2011).\nThe choice of kernel C is of course once again of great importance since it encodes prior knowledge about each of the integrand and their correlation structure and should be made based on the application considered. We also remark that matrix valued kernels C can be described in term of some scalar-valued kernel r on the extended space X×{1, . . . , D} as (C(x,x′))dd′ = r((x, d), (x′, d′)). We now present two choices of covariance functions which are popular in the literature and will be used in this paper:\n• The separable kernel is of the form\nC(x,x′) = Bc(x,x′),\nwhereB ∈ RD×D is symmetric and positive definite, and c : X × X → R is a scalar-valued reproducing kernel. This treats the kernel as the product of two scalar-valued reproducing kernels, one defined on X\nand the other on {1, . . . , D}. A particular case of interest is the linear model of coregionalization (LMC) where the matrix is of the form (B)dd′ = ∑R i=1 a i da i d′ for some aid ∈ R. This type of kernel can lead to a lower computational cost of orderO(N3 +D3) when evaluating all fd on the same input set and using tensor product formulations (see Appendix C).\n• The process convolution kernel (Ver Hoef & Barry, 1998; Higdon, 2002; Alvarez et al., 2012) models the individual functions f1, . . . , fD as blurred transformations of R ∈ N+ underlying functions. It is given by:\n(C(x,x′))d,d′ = cd,d′(x,x ′) + cwd(x,x ′)δd,d′ ,\nwhere δdd′ = 1 if d = d′ and 0 else. Here there are two parts of the kernel, first cd,d′ : X × X → R defined as:\ncd,d′(x,x ′) = R∑ i=1 ∫ X Gid(x− z)×∫\nX Gid′(x ′ − z′)ci(z, z′)dz′dz,\nand cwd : X ×X → R representing covariance inherent to the dth function and Gid : X → R is a blurring kernel2 which is a continuous function either having compact support or being square integrable. Notice that taking Gid(x− z) = aidδ(x− z) (where δ(·) represents a Dirac function) gives back the LMC case.\nNote that it is also common to combine kernels, by summing them (i.e. C(x,x′) = ∑Q q=1Cq(x,x\n′)) in order to obtain more flexible models. The kernel means and initial error, as well as other details for implementation are provided in Appendix C."
  }, {
    "heading": "3. Theoretical results",
    "text": "In this section, we begin by exploring properties of multioutput BQ with GP(0,C) prior as an optimally-weighted quadrature algorithm in vector-valued RKHSHC .\nLet HK be a vector-valued RKHS with norm and inner product denoted ‖ · ‖K and 〈·, ·〉K respectively. These spaces were extensively studied in (Pedrick, 1957; Micchelli & Pontil, 2005; Carmeli et al., 2006; 2010; De Vito et al., 2013), and generalise the notion of RKHS to vectorvalued functions. In the multi-output case, there is also a one-to-one correspondance between the RKHS HK and the kernel K. Theorem 3.1 in (Micchelli & Pontil, 2005) shows that the minimizer of the variational problem:\nmin h∈HK\n{ ‖h‖2K : h : X → RD,h(xi) = f(xi) ∀xi ∈X } 2Note that the term “blurring kernel” does not mean the func-\ntion is a reproducing kernel.\ntakes the form of the multi-output posterior GP mean mN obtained after conditioning a GP(0,K) on some data set X . We can therefore extend a well-known result from the uni-output case to show that Π̂BQ[fd] is an optimally weighted quadrature rule for all fd in terms of their worstcase integration error, denoted:\ne(HC , Π̂,X, d) := sup ‖f‖C≤1 ∣∣∣Π[fd]− Π̂[fd]∣∣∣ . (2) Proposition 2 (Optimally weighted quadrature rule in HC). For a fixed point set X , denote by Π̂[f ] = W>f(X) any quadrature rule for the vector-valued function f = (f1, . . . , fD) and by Π̂BQ[f ] = W>BQf(X) the BQ rule with GP(0,C) prior. Then, ∀d = 1, . . . , D:\nWBQ = arg min W∈RND×D\ne(HC , Π̂,X, d).\nIn specific cases, it is also possible to characterise the rate of convergence of the worst-case error for each element fd. This is for example the case with the separable kernel introduced in Sec. 2, as will be demonstrated in the Theorem 1 below. First, we introduce some technical definitions which will be required for the statement of the theorem.\nWe say that a domain X ⊂ Rp satisfies an interior cone condition if there exists an angle θ ∈ (0, π2 ) and a radius r > 0 such that ∀x ∈ X , a unit vector ξ(x) exists such that the cone {x + λy : y ∈ Rp, ‖y‖2 = 1,y>ξ(x) ≥ cos θ, λ ∈ [0, r]} is a subset of X .\nFor a point setX , we call hX,X := supx∈X infxj∈X ‖x− xj‖2 the fill distance, qX := 12 minj 6=k ‖xj−xk‖2 the separation radius and ρX,X := hX,X /qX the mesh ratio. We will assume we evaluate all integrands on the same point setX which satisfies either of these assumptions:\n(A1) X consists of independently and identically distributed (IID) samples from some probability measure Π′ which admits a density π′ > 0 on X .\n(A2) X is a quasi-uniform grid on X ⊂ Rp (i.e. satisfies hX,X ≤ C1N− 1 p for some C1 > 0) and satisfies\nhX,X ≤ C2qX,X for some C2 > 0.\nExamples of point sets satisfying (A2) include uniform grid points in some hypercube.\nTheorem 1 (Convergence rate for BQ with separable kernel). Suppose we want to approximate Π[f ] for some f : X → RD and Π̂BQ[f ] is the multi-output BQ rule with the kernel C(x,x′) = Bc(x,x′) for some positive definite B ∈ RD×D and scalar-valued kernel c : X × X → R. Then, ∀d = 1, . . . , D, we have:\ne(HC , Π̂BQ,X, d) = O ( e(Hc, Π̂BQ,X) ) .\nIn particular, assume that X ⊂ Rp satisfies an interior cone condition with Lipschitz boundary3 and X satisfies assumption (A1) or (A2). Then, the following rates hold:\n• IfHc is norm-equivalent to an RKHS with Matérn kernel of smoothness α > p2 , we have ∀d = 1, . . . , D:\ne(HC , Π̂BQ,X, d) = O ( N− α p+ ) ,\nfor > 0 arbitrarily small.\n• If Hc is norm-equivalent to the RKHS with squaredexponential, multiquadric or inverse multiquadric kernel, we have ∀d = 1, . . . , D:\ne(HC , Π̂BQ,X, d) = O ( exp ( −C1N 1 p− )) ,\nfor someC1 > 0 and for some > 0 arbitrarily small.\nProposition 3 (Convergence rate for sum of kernels). Suppose that C(x,x′) = ∑Q q=1Cq(x,x ′). Then:\ne(HC , Π̂BQ,X, d) = arg max q∈{1,...,Q}\nO ( e(HCq , Π̂BQ,X, d) ) .\nWe clarify that the notation with is common in the numerical integration literature, and is used to hide powers of log n terms since these do not have a significant influence on the asymptotic convergence rate.\nIt is interesting to note that the rate of convergence for multi-output BQ is the same as that of uni-output BQ (Briol et al., 2015b). This can be explained intuitively by the fact that, when adding a new integrand, we can only gain by a constant factor since we always evaluate the functions at the same input points. In fact the proof of Thm. 1 provides an expression for this improvement factor (in terms of WCE) for any integrand fd, and this depends explicitly on its correlation with the other functions: | ∑D i,j=1(B\n−1)ijBidBjd|. From a practitioner’s viewpoint, this can clearly be used to balance the value of using several integrands with the additional computational cost incurred by using multi-output BQ.\nWe now give a result in the misspecified setting when the function f is assumed to be smoother than it is. In this case, it is still possible to recover the optimal convergence rate:\nTheorem 2 (Misspecified Convergence Result for Separable Kernel). Let cα be a kernel norm-equivalent to a Matérn kernel of smoothness α on some domain X with Lipschitz boundary and satisfying an interior cone condition. Consider the BQ rule Π̂BQ[f ] corresponding to a separable kernel Cα(x, x′) = Bcα(x, x′) with X satisfying\n3Formally defined in Appendix A for completeness.\nFigure 1. Multi-fidelity modelling: Plot of the Step function (top), Forrester function (bottom) for the low fidelity (left) and high fidelity (right). Each plot gives the true function (blue) and their unit-output (dashed, red), LMC-based multi-output (dashed, yellow) and PC-based multi-output (dotted purple) approximations.\n(A2), and suppose that f ∈ HCβ where p 2 ≤ β ≤ α. Then, ∀d = 1, . . . , D:∣∣∣Π[fd]− Π̂BQ[fd]∣∣∣ = O (N− βp+ ) , for some > 0.\nThis last theorem demonstrate that the method is rate adaptive as long as we choose a kernel which is too smooth. However, it also demonstrates a drawback of the separable kernels: if one of the integrands is rough but all other are smooth, then the worst-case error could potentially converge slowly for all of them.\nFinally, we note that studying the method in other information complexity settings than the worst-case would also be interesting. For example, it is trivial to show that the method above satisfies the definition of Bayesian probabilistic numerical method of (Cockayne et al., 2017) (Def. 2.5). Furthermore, optimality conditions for this method could also be obtained in a game-theoretic setting (in terms of a two-player mixed strategies game) by extending the theory on gamblets by (Owhadi & Scovel, 2017)."
  }, {
    "heading": "4. Applications",
    "text": "Multi-fidelity modelling Consider some function f high : X → R representing some complex engineering model of interest, which we would like to use for some task such as statistical inference or optimization. These models usually require the simulation of underlying physical systems, which can make each evaluation prohibitively expensive and will therefore limit N to the order of tens or hundreds. To tackle this issue, multi-fidelity modelling proposes to build cheap, but less accurate, alternatives f low1 , . . . , f low D−1 :\nModel BQ LMC-BQ PC-BQ\nStep (l) 0.02 (0.22) 0.02 (0.21) 0.02 (0.52) Step (h) 0.41 (0.03) 0.09 (0.09) 0.04 (0.15) For. (l) 0.08 (4.91) 0.08 (4.95) 0.07 (33.95) For. (h) 3.96 (3.98) 2.86 (27.01) 1.06 (63.80)\nX → R to f high, and use the cheaper models in order to accelerate computation for the task of interest. This can be done using surrogate models (e.g. support vector machines, GPs or neural networks), projection-based models (Krylov subspace or reduced basis methods) or models where the underlying physics is simplified; see (Peherstorfer et al., 2016a) for an overview.\nIn this section, we consider the problem of numerical integration in such a multi-fidelity setup. Note that two related methods for Monte Carlo estimation are the multi-fidelity Monte Carlo estimator (Peherstorfer et al., 2016a) and the multilevel Monte Carlo of (Giles, 2015), both of which are based on control variate identities.\nWe approach this problem with multi-output BQ on the vector-valued function f = (f high, f low1 , . . . , f low D−1)\n>. Note that multi-output Gaussian processes were already proposed for multi-fidelity modelling in (Perdikaris et al., 2016; Parussini et al., 2017), and we extend their methodologies to the task of numerical integration. We consider two toy problems from this literature (Raissi & Karniadakis, 2016) to highlight some of the advantages and disadvantages of our methodology\n1. A step function on X = [0, 2]:\nf low1 (x) = { 0, x ≤ 1 1, x > 1 f high(x) = { −1, x ≤ 1 2, x > 1\n2. The Forrester function with Jump on X = [0, 1]:\nf low1 (x) =\n{ (3x−1)2 sin(12x−4)\n4 + 10(x− 1), x ≤ 1 2\n3 + (3x−1) 2 sin(12x−4)\n4 + 10(x− 1), x > 1 2\nf high(x) =\n{ 2f low(x)− 20(x− 1), x ≤ 12\n4 + 2f low(x)− 20(x− 1), x > 12\nThe functions and conditioned GPs are given in Fig. 1, whilst the uni-output and multi-output BQ estimates for integration of these functions against a uniform measure Π\nare given in the table in Fig. 2. In both cases, 20 equidistant points are used, with point number 4, 10, 11, 14 and 17 used to evaluate the high fidelity model and the others used for the low fidelity model. The choice of kernel hyperparameters is made by maximising the marginal likelihood (often called empirical Bayes). Further details, and an additional test function can be found in Appendix D.2.\nNote that both of these problems are challenging for several reasons. Firstly, due to their discontinuity, the integrands are not in the RKHS HC corresponding to the kernel C used in multi-output BQ. In particular, the problems are misspecified in the sense that the true function is not in the support of the prior. It is therefore difficult to interpret the posterior distribution on Π[f ], and we end up with credible intervals which are too wide. This is for example illustrated in the values of the posterior variance for the high-fidelity Forrester function. Secondly, in each case, the high and low-fidelity models are defined on different scales and so require tuning of several kernel hyper-parameters. This can of course make it challenging for multi-output BQ since the number of function evaluations N is small and empirical Bayes will tend to be inefficient in those cases.\nHowever, despite these two issues, it is interesting to note that both of the multi-output BQ methods manage to significantly outperform uni-output BQ in terms of point estimate, as the sharing of information allows the multi-output models to better represent the main trends in the functions. Furthermore, the multi-output BQ does not suffer from the issues of overconfident posterior credible intervals present in uni-output BQ; contrast for example the posterior variances for the high-fidelity step function.\nGlobal illumination In this section, we apply multioutput BQ to a challenging numerical integration problem from the field of computer graphics, known as global illumination. BQ was previously applied to this problem in several papers (Brouillat et al., 2009; Marques et al., 2013; Briol et al., 2015b), but we propose to extend these results using multi-output BQ.\nGlobal illumination is a problem which occurs when trying to obtain realistic representation of light interactions for the design of virtual environments (e.g. a video game). One model of the amount of light coming from an object towards the camera (representing the current viewpoint on this environment) is given by the following equation:\nL0(ω0) = Le(ω0) + ∫ S2 Li(ωi)ρ(ωi, ω0)[ωi · n]+dΠ(ωi).\nwhere [x]+ = max(0, x). The function L0 : S2 → R evaluated at ω0 is called the outgoing radiance in direction ω0 (the angle of the outgoing light from the object normal n), Le(ω0) : S2 → R is the amount of light emitted by\nthe object, and Li : S2 → R evaluated at ωi is the amount of light reflected by the object (which originated from an angle ωi from the object’s normal n). Here, S2 = {x = (x1, x2, x3) ∈ R3 : ‖x‖2 = 1} and ρ(ωi, ω0) : S2 × S2 → R is called the bidirectional reflectance distribution and represents the proportion of light being reflected.\nWe follow (Briol et al., 2015b) and consider the problem as Π[hω0 ] = ∫ S2 h\nω0(ωi)Π(dωi) where Π is the uniform measure on S2, and hω0(ωi) = Li(ωi)ρ(ωi, ω0)[ωi · ω0]+ is a function which can be evaluated by making a call to an environment map (which we consider to be a black box). One scenario which is common in these type of problems is to look at an object from different angles ω0, with the camera moving. In this case, it is reasonable to assume that the different integrands hω0 will be very similar when the difference in the angle ω0 is small, and it is therefore natural to consider jointly estimating their integrals. In the experiments we consider five integrands fi = hω i 0 for i = 1, . . . , 5 where ω10 , . . . , ω 5 0 are on a great circle of the sphere at intervals determined by an angle of 0.005π.\nWe therefore consider two-output and five-output BQ with independent and identically distributed (Monte Carlo) samples X from the uniform measure Π. We propose to use a separable kernel with scalar-valued RKHS Hc being a Sobolev space of smoothness 32 over S\n2 and has kernel c(x,x′) = 83 − ‖x − x\n′‖22. For the matrix B representing the covariance between outputs, we propose to make this covariance proportional to the difference in angle at which the camera looks at the object. In particular we choose (B)ij = exp(ω i 0 · ω j 0 − 1) for simplicity, but this could be generalised to include a lengthscale and amplitude hyperparameter to be learnt together with the hyperparameters of the scalar-valued kernel c.\nThe GP means for the one-output and five-output cases are given in Fig. 3, and we can clearly notice a significant improvement in approximation accuracy with the larger number of outputs. Results for integration error are given in Fig. 4. As noticed, the integration error (for a fixed number of evaluationsN of each integrand) is significantly reduced by increasing the number of outputs D. The individual posterior variances for this problem (see Appendix D.3 Fig. 10) are also smaller, reflecting the fact that our uncertainty is reduced due to use of observations from other integrands.\nIn fact, a small extension of Thm. 1 (combined with the rate for the scalar-valued kernel in (Briol et al., 2015b)) allows us to obtain an asymptotic convergence rate for the posterior variance on each integral Π[fd]: Corollary 1. LetX be the sphere S2 andX be IID uniform points onX . AssumeC is a separable kernel with c defined above. Then e(HC , Π̂BQ,X, d) = OP ( N− 3 4 ) .\nThe same rate with improved rate constant was observed\nin (Briol et al., 2015b) when using QMC point sets, and similar gains could be obtained in this multi-output case.\nWe note that there a significant potential further gains for the use of multi-output BQ in this setting. Similar integration problems need to be computed for three colors in every pixel of an image, and for every image in a video. This is challenging computationally and limits the use of Monte Carlo methods to a few dozen points. Designing specific matrix-valued kernels could provide enormous gains since we end up with thousands of correlated integrands. Furthermore, the weights only depend on the choice of kernel and not on function values, so that all of the weights could be pre-computed off-line to be later used in real-time."
  }, {
    "heading": "5. Conclusion",
    "text": "We have proposed an extension of Bayesian Quadrature to the case where we are interested in numerically computing the integral of several functions which are related. In particular we have proposed a new algorithm based on jointly modelling the integrands with a Gaussian prior. Then, we provided a theoretical study of the rate of convergence for the case where the kernel is separable and illustrated the potential of our methodology on applications in multi-fidelity\nmodelling and computer graphics. Our main contribution however, has been to highlight the natural extension of Bayesian probabilistic numerical methods to the joint estimation of the solution of several numerical problems (in this case, numerical integration problems).\nThere are several possible extensions of multi-output BQ which we reserve for future work. One important question remaining is that of the choice of sampling distribution. In the uni-output case, it is well known that obtaining an optimal sampling distribution with respect to the Vn[Π[f ]] is intractable in most cases. (Briol et al., 2017) proposed an algorithm to approach such a distribution, and (Kanagawa et al., 2017) provided conditions on the point sets to guarantee fast convergence. In the multi-output case, the problem is even more complex due to the interaction between the different integration problems. However, the literature on the design of experiments for co-kriging/multi-output GPs may be of interest, and the use of more advanced sampling distributions will certainly provide significant gains."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors are grateful to Alessandro Barp, Aretha Teckentrup, Chris Oates and Motonobu Kanagawa for helpful discussions. FXB was supported by the EPSRC grants [EP/L016710/1, EP/R018413/1, EP/N510129/1]. MG was supported by the EPSRC grants [EP/J016934/3, EP/K034154/1, EP/P020720/1, EP/R018413/1, EP/N510129/1], an EPSRC Established Career Fellowship, the EU grant [EU/259348] and the\nLloyds Register Foundation Programme on Data-Centric Engineering. The authors would like to thank the Isaac Newton Institute for Mathematical Sciences for support and hospitality during the programme on “Uncertainty Quantification for Complex Systems: Theory and Methodologies”. This work was supported by EPSRC grant no [EP/K032208/1]. Finally, this material was also based upon work partially supported by the National Science Foundation under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute."
  }],
  "references": [{
    "title": "Computationally Efficient Convolved Multiple Output Gaussian Processes",
    "authors": ["M. Álvarez", "N. Lawrence"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Kernels for Vector-Valued Functions: A Review",
    "authors": ["M. Alvarez", "L. Rosasco", "N. Lawrence"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "On the Equivalence between Quadrature Rules and Random Features",
    "authors": ["F. Bach"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2017
  }, {
    "title": "On the optimality of linear methods for operator approximation in convex classes of functions",
    "authors": ["N.S. Bakhvalov"],
    "venue": "USSR Computational Mathematics and Mathematical Physics,",
    "year": 1971
  }, {
    "title": "Reproducing Kernel Hilbert Spaces in Probability and Statistics",
    "authors": ["A. Berlinet", "C. Thomas-Agnan"],
    "year": 2004
  }, {
    "title": "Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees",
    "authors": ["Briol", "F.-X", "C.J. Oates", "M. Girolami", "M.A. Osborne"],
    "venue": "In Advances In Neural Information Processing Systems",
    "year": 2015
  }, {
    "title": "Probabilistic Integration: A Role in Statistical Computation? arXiv:1512.00933, to appear in ”Statistical Science”, 2015b",
    "authors": ["Briol", "F.-X", "C.J. Oates", "M. Girolami", "M.A. Osborne", "D. Sejdinovic"],
    "year": 2015
  }, {
    "title": "On the Sampling Problem for Kernel Quadrature",
    "authors": ["Briol", "F.-X", "C.J. Oates", "J. Cockayne", "M. Girolami"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning, PMLR 70,",
    "year": 2017
  }, {
    "title": "A Bayesian Monte Carlo Approach to global illumination",
    "authors": ["J. Brouillat", "C. Bouville", "B. Loos", "C. Hansen", "K. Bouatouch"],
    "venue": "Computer Graphics Forum,",
    "year": 2009
  }, {
    "title": "Vector Valued Reproducing Kernel Hilbert Spaces of Integrable Functions and Mercer Theorem",
    "authors": ["C. Carmeli", "E. De Vito", "A. Toigo"],
    "venue": "Analysis and Applications,",
    "year": 2006
  }, {
    "title": "Vector valued reproducing kernel Hilbert spaces and universality",
    "authors": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanita"],
    "venue": "Analysis and Applications,",
    "year": 2010
  }, {
    "title": "An extension of Mercer theorem to matrix-valued measurable kernels",
    "authors": ["E. De Vito", "V. Umanità", "S. Villa"],
    "venue": "Applied and Computational Harmonic Analysis,",
    "year": 2013
  }, {
    "title": "Bayesian Numerical Analysis",
    "authors": ["P. Diaconis"],
    "venue": "Statistical Decision Theory and Related Topics IV, pp",
    "year": 1988
  }, {
    "title": "Bayesian Inference of Log Determinants",
    "authors": ["J. Fitzsimons", "K. Cutajar", "M. Osborne", "S. Roberts", "M. Filippone"],
    "venue": "Uncertainty in Artificial Intelligence,",
    "year": 2017
  }, {
    "title": "Multilevel Monte Carlo methods",
    "authors": ["M.B. Giles"],
    "venue": "Acta Numerica,",
    "year": 2015
  }, {
    "title": "Sampling for inference in probabilistic models with fast Bayesian quadrature",
    "authors": ["T. Gunter", "R. Garnett", "M. Osborne", "P. Hennig", "S. Roberts"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Probabilistic Numerics and Uncertainty in Computations",
    "authors": ["P. Hennig", "M.A. Osborne", "M. Girolami"],
    "venue": "Journal of the Royal Society A,",
    "year": 2015
  }, {
    "title": "Space and space-time modeling using process convolutions. Quantitative methods for current environmental issues",
    "authors": ["D.M. Higdon"],
    "year": 2002
  }, {
    "title": "Optimally-Weighted Herding is Bayesian Quadrature",
    "authors": ["F. Huszar", "D. Duvenaud"],
    "venue": "In Uncertainty in Artificial Intelligence,",
    "year": 2012
  }, {
    "title": "Average case epsilon-complexity in computer science: A Bayesian view",
    "authors": ["J.B. Kadane", "G.W. Wasilkowski"],
    "venue": "In Bayesian Statistics 2, Proceedings of the Second Valencia International Meeting,",
    "year": 1985
  }, {
    "title": "Convergence guarantees for kernel-based quadrature rules in misspecified settings",
    "authors": ["M. Kanagawa", "B. Sriperumbudur", "K. Fukumizu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Convergence Analysis of Deterministic KernelBased Quadrature Rules in Misspecified Settings",
    "authors": ["M. Kanagawa", "B.K. Sriperumbudur", "K. Fukumizu"],
    "year": 2017
  }, {
    "title": "Classical quadrature rules via Gaussian processes",
    "authors": ["T. Karvonen", "S. Särkkä"],
    "venue": "IEEE International Workshop on Machine Learning for Signal Processing,",
    "year": 2017
  }, {
    "title": "Fully symmetric kernel quadrature",
    "authors": ["T. Karvonen", "S. Särkkä"],
    "year": 2017
  }, {
    "title": "Active Uncertainty Calibration in Bayesian ODE Solvers",
    "authors": ["H. Kersting", "P. Hennig"],
    "venue": "In Uncertainty in Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Gaussian measure in Hilbert space and applications in numerical analysis",
    "authors": ["F.M. Larkin"],
    "venue": "Rocky Mountain Journal of Mathematics,",
    "year": 1972
  }, {
    "title": "A spherical Gaussian framework for Bayesian Monte Carlo rendering of glossy surfaces",
    "authors": ["R. Marques", "C. Bouville", "M. Ribardiere", "P. Santos", "K. Bouatouch"],
    "venue": "IEEE Transactions on Visualization and Computer Graphics,",
    "year": 2013
  }, {
    "title": "On learning vector-valued functions",
    "authors": ["C.A. Micchelli", "M. Pontil"],
    "venue": "Neural Computation,",
    "year": 2005
  }, {
    "title": "Sobolev error estimates and a Bernstein inequality for scattered data interpolation via radial basis functions",
    "authors": ["F.J. Narcowich", "J.D. Ward", "H. Wendland"],
    "venue": "Constructive Approximation,",
    "year": 2006
  }, {
    "title": "Convergence Rates for a Class of Estimators",
    "authors": ["C.J. Oates", "J. Cockayne", "Briol", "F.-X", "M. Girolami"],
    "venue": "Based on Stein’s Identity",
    "year": 2016
  }, {
    "title": "Bayesian Probabilistic Numerical Methods for Industrial Process Monitoring",
    "authors": ["C.J. Oates", "J. Cockayne", "R.G. Aykroyd"],
    "year": 2017
  }, {
    "title": "Control functionals for Monte Carlo integration",
    "authors": ["C.J. Oates", "M. Girolami", "N. Chopin"],
    "venue": "Journal of the Royal Statistical Society B: Statistical Methodology,",
    "year": 2017
  }, {
    "title": "Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models",
    "authors": ["C.J. Oates", "S. Niederer", "A. Lee", "Briol", "F.-X", "M. Girolami"],
    "venue": "Advances in Neural Information Processing,",
    "year": 2017
  }, {
    "title": "Construction of optimal cubature algorithms with applications to econometrics and uncertainty quantification",
    "authors": ["J. Oettershagen"],
    "venue": "PhD thesis, Rheinischen FriedrichWilhelms-Universität Bonn,",
    "year": 2017
  }, {
    "title": "Bayes-Hermite quadrature",
    "authors": ["A. O’Hagan"],
    "venue": "Journal of Statistical Planning and Inference,",
    "year": 1991
  }, {
    "title": "Some Bayesian numerical analysis",
    "authors": ["A. O’Hagan"],
    "venue": "Bayesian Statistics,",
    "year": 1992
  }, {
    "title": "Universal Scalable Robust Solvers from Computational Information Games and fast eigenspace adapted Multiresolution Analysis",
    "authors": ["H. Owhadi", "C. Scovel"],
    "year": 2017
  }, {
    "title": "Multi-fidelity Gaussian process regression for prediction of random fields",
    "authors": ["L. Parussini", "D. Venturi", "P. Perdikaris", "G.E. Karniadakis"],
    "venue": "Journal of Computational Physics,",
    "year": 2017
  }, {
    "title": "Theory of reproducing kernels for Hilbert spaces of vector valued functions",
    "authors": ["G. Pedrick"],
    "venue": "PhD thesis, University of Kansas,",
    "year": 1957
  }, {
    "title": "Survey of Multifidelity Methods in Uncertainty Propagation, Inference, and Optimization",
    "authors": ["B. Peherstorfer", "K. Willcox", "M. Gunzburger"],
    "venue": "ACDL Technical Report TR16-1,",
    "year": 2016
  }, {
    "title": "Optimal model management for multifidelity monte carlo estimation",
    "authors": ["B. Peherstorfer", "K. Willcox", "M. Gunzburger"],
    "venue": "SIAM Journal of Scientific Computing,",
    "year": 2016
  }, {
    "title": "Nonlinear information fusion algorithms for robust multi-fidelity modeling",
    "authors": ["P. Perdikaris", "M. Raissi", "A. Damianou", "N. Lawrence", "G. Karniadakis"],
    "venue": "Proceedings of the Royal Society A: Mathematical, Physical, and Engineering Sciences,",
    "year": 2016
  }, {
    "title": "Bayesian Monte Carlo",
    "authors": ["C. Rasmussen", "Z. Ghahramani"],
    "venue": "In Advances in Neural Information Processing Systems, pp",
    "year": 2002
  }, {
    "title": "Average-case analysis of numerical problems",
    "authors": ["K. Ritter"],
    "year": 2000
  }, {
    "title": "On the relation between Gaussian process quadratures and sigma-point methods",
    "authors": ["S. Sarkka", "J. Hartikainen", "L. Svensson", "F. Sandblom"],
    "venue": "Journal of Advances in Information Fusion,",
    "year": 2016
  }, {
    "title": "An Allen-Cahn equation with continuation, 2010. URL http://www.chebfun.org/ examples/ode-nonlin/AllenCahn.html",
    "authors": ["N. Trefethen"],
    "year": 2010
  }, {
    "title": "Constructing and fitting models for cokriging and multivariable spatial prediction",
    "authors": ["J.M. Ver Hoef", "R.P. Barry"],
    "venue": "Journal of Statistical Planning and Inference,",
    "year": 1998
  }],
  "id": "SP:b0718acb79ab7a8fef371cdfcb595c820afc156f",
  "authors": [{
    "name": "Xiaoyue Xi",
    "affiliations": []
  }, {
    "name": "François-Xavier Briol",
    "affiliations": []
  }, {
    "name": "Mark Girolami",
    "affiliations": []
  }],
  "abstractText": "Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods. This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency. We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions. We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.",
  "title": "Bayesian Quadrature for Multiple Related Integrals"
}