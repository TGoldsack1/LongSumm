{
  "sections": [{
    "heading": "1. Introduction",
    "text": "K-means clustering is a classical clustering problems and has been studied for several decades. The goal of K-Means clustering is to find a set of k cluster centers for a dataset such that the sum of squared distances of each point to its closest cluster center is minimized. While it is known that k-means clustering is an NP hard optimization problem even for k = 2 (Dasgupta, 2008), in practice a local search heuristic due to Lloyd (Lloyd, 1982) is widely used for solving K-means clustering problem. Lloyd’s iterative\n1Department of Electrical Engineering & Computer Science, Wichita State University, KS, USA. Correspondence to: Kaushik Sinha <kaushik.sinha@wichita.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nalgorithm begins with k arbitrary “cluster centers”, and in each iteration, each point is assigned to the nearest cluster center, and each cluster center is recomputed as the center of mass of all points assigned to it. These last two steps are repeated until the process stabilizes. Lloyd’s algorithm for k-means clustering is known to be one of the top ten data mining tools of the last fifty years (Wu, 2008).\nK-means clustering is typically performed on a data matrix A ∈ Rn×d, consisting of n data points each having d attributes/features and per iteration computational cost of Lloyd’s algorithm is O(nkd). In recent years there has been a series of work towards reducing this computational cost and speeding up k-means clustering computation. Most of these works can broadly be classified into three categories. In the first category, K-means clustering algorithm is accelerated by avoiding unnecessary distance calculations by applying various forms of triangular inequality and by keeping track of lower and upper bounds for distances between points and cluster centers (Elkan, 2003; Hamerly, 2010; Drake & Hamerly, 2012; Ding et al., 2015; Newling & Fleuret, 2016; Bottesch et al., 2016). All these algorithms ensure the exact same clustering result that would have been obtained had one applied Lloyd’s heuristic from the same set of initial cluster centers without applying any distance inequality bounds. In the second category, various dimension reduction techniques are applied to data matrix A to reduce data dimensionality from d to d′ (d′ d), where d′ is independent of n and d, so that optimal k-means clustering solution of dimensionality reduced dataset A′ ∈ Rn×d′ ensures an approximately optimal k-means clustering objective function of A. Most prominent among these is the random projection based dimensionality reduction technique that reduces data dimensionality from d to Ω(k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al., 2010; 2015; Cohen et al., 2015) and also from d to O(log(k)/ 2) resulting in (9 + ) approximation of the optimal k-means objective function (Cohen et al., 2015). Recently, (Liu et al., 2017) demonstrated the the random projection step can be performed by multiplying with a sparse matrix that yields the same (1 + ) approximation guarantee. Additionally, random feature selection method reduces data dimensionality from d to Ω(k log k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al.,\n2015; Cohen et al., 2015). In the third category, a smaller subset of n data points called coresets, are constructed so that optimal weighted k-means clustering objective function performed on this coreset is (1 + ) approximation of the optimal k-means objective function performed on the original dataset (Feldman & Langberg, 2011; Feldman et al., 2013). In k-means clustering using Lloyd’s heuristic, a major computational bottleneck arises from Euclidean distance computation between each data point and k cluster centers in every iteration. For a data point a ∈ Rd and a cluster center µ ∈ Rd, the Euclidean distance can be represented as, ‖a − µ‖2 = ‖a‖2 + ‖µ‖2 − 2a>µ. Note that ‖a‖2 needs be computed for each data point only once over all iterations of Lloyd’s heuristics (and can be done off-line), ‖µ‖2 needs be computed once for each cluster center in each iteration, while the dot product needs to be computed for every possible data point, cluster center pair in every single iteration. In fact, the dot product between a cluster center µ and all n data points can be computed by a simple matrix-vector multiplication: Aµ. If the data matrix A is significantly sparse (i.e., number of non-zero entries is reasonable small) the above matrix vector multiplication can be performed reasonably fast. A key question that is not addressed in the literature is, To what extent the data matrix A can be made sparse without significantly affecting optimal k-means clustering objective? In this paper we show that under mild conditions, we can randomly sparsify data matrix A to obtain a sparse data matrix Ã such that optimal k-means clustering solution of Ã yields an approximately optimal k-means clustering objective of A with high probability. Note that such a sparsification scheme can be extremely useful in practice. If the original data matrix is reasonably dense, then such sparsification results in fast matrix-vector multiplication, thereby speeding up k-means clustering. However, it may seem at first that for many real world high dimensional datasets that are very sparse to begin with, such as text datasets represented in “bag of word” format, such sparsification scheme may not be useful. But, note that instead of directly working with high dimensional data, typically a random projection step is often applied first to reduce data dimensionality since it is known that optimal k-means clustering solution of this randomly projected dataset results in approximately optimal k-means clustering objective of the original high dimensional dataset (Boutsidis et al., 2010; 2015; Cohen et al., 2015; Liu et al., 2017). Unfortunately, such a random projection step results in a dense projected data matrix. Interestingly, our sparsification method can now be applied on this projected dense data matrix to reap further computational benefit in addition to the computational benefit already achieved by random projection step (see Figure 1).\nTo quantify the approximation factor as well as the level of sparsity of our proposed method, we use ideas from\n(Achlioptas & Mcsherry, 2007) which establishes that random matrix sparsification approximately preserves low rank matrix structure with high probability and also ideas from (Cohen et al., 2015) which establishes to what extent an approximately optimal low rank matrix serves as a projection cost preserving sketch. To the best of our knowledge, this is the first result that quantifies how random matrix sparsification affects k-means clustering. In particular, we make the following contributions in this paper.\n• We show that for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈ Rn×d, such that, Ã contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and optimal k-means clustering solution of Ã results in (1 + ) approximation of optimal k-means objective of A with high probability.\n• We show that for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈ Rn×d, such that, Ã contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and any approximately optimal k-means clustering solution of Ã, having (1 + ) approximation of optimal k-means objective of Ã, results in (1 +O( )) approximation of optimal k-means objective of A with high probability.\n• We present experimental results on three real world datasets to demonstrate effect of our proposed random sparsification scheme on k-means clustering solution.\nThe rest of the paper is organized as follows. In section 2 we present k-means clustering problem in matrix notation and introduce uniform and non-uniform sampling strategies for random matrix sparsification. We propose an algorithm for k-means clustering using random matrix sparsification in section 3 and present its analysis in section 4. Empirical evaluations are presented in section 5. Finally, we conclude and point out a few open questions in section 6."
  }, {
    "heading": "2. Preliminaries",
    "text": ""
  }, {
    "heading": "2.1. Notation and linear algebra basics",
    "text": "We use bold lower case letters to denote vectors and bold upper case letters to denote matrices. For any n and d, consider a matrix A ∈ Rn×d with rank r = rank(A). Using singular value decomposition A can be written as A = UΣV>, where U ∈ Rn×r contains r left singular vectors u1,u2, . . . ,ur ∈ Rn, V contains r right singular vectors v1,v2, . . . ,vr ∈ Rd, and Σ ∈ Rr×r is a positive diagonal matrix containing the singular values of A : σ1(A) ≥ σ2(A) ≥ · · · ≥ σr(A). A can also be written as A = ∑r i=1 σi(A)uiv > i . For any k ≤ r,\nAk = ∑k i=1 σi(A)uiv > i is the best rank k approximation to A for any unitarily invariant norm, including Frobenious\nand spectral norm (Mirsky, 1960). Note that A = Ak + Ar−k where Ar−k = ∑r i=k+1 σi(A)uiv > i . Therefore, Ar−k = A−Ak. Square Frobenious norm of A is given by ‖A‖2F = ∑ i,jA(i, j) 2 = trace(AA>) = ∑ i σ 2 i (A). The spectral norm of A is given by ‖A‖2 = σ1(A). Ak satisfies ‖A −Ak‖F = minB,rank(B)=k ‖A − B‖F and ‖A−Ak‖2 = minB,rank(B)=k ‖A−B‖2."
  }, {
    "heading": "2.2. K-means clustering",
    "text": "The objective of k-means clustering is to partition n data points in Rd, {a1, . . . ,an}, into k non-overlapping clusters C = {C1, . . . , Ck} such that points that are close to each other belong to the same cluster and points that are far from each other belong to to different clusters. Let µi be the centroid of cluster Ci and for any data point ai, let C(ai) be the index of the cluster to which ai is assigned to. The goal of k-means clustering is to minimize the objective function\nk∑ i=1 ∑ aj∈Ci ‖aj −µi‖22 = n∑ j=1 ‖aj −µC(aj)‖ 2 2 (1)\nLet A ∈ Rn×d be a data matrix containing the n data points {a1, . . . ,an} as rows and for any clustering C, let XC ∈ Rn×k be the cluster indicator matrix, with XC(i, j) = 1/ √ |Cj | if ai is assigned to Cj and XC(i, j) = 0 otherwise. The k-means objective function given in equation 1 can now be represented in the matrix notation as,\n‖A−XCX>CA‖2F = n∑ j=1 ‖aj −µC(aj)‖ 2 2 (2)\nBy construction, the columns of XC have disjoint support and are orthonormal vectors and XCX>C is an orthogonal projection matrix of rank k. Let S be the set of all possible rank k cluster projection matrices of the form XCX>C . The objective of k-means clustering is to find an optimal clustering of A that minimizes the objective function in equation\n2, that is, to find XCopt such that,\nXCopt = argmin XCX>C ∈S\n‖A−XCX>CA‖2F\nAs mentioned earlier, finding XCopt is an NP-hard problem. Any cluster indicator matrix Xγ is called an γapproximation for the k-means clustering problem (γ ≥ 1) for data matrix A if it satisfies,\n‖A−XγX>γA‖2F ≤ γ min XCX>C ∈S ‖A−XCX>CA‖2F\n= γ‖A−XCoptX>CoptA‖ 2 F"
  }, {
    "heading": "2.3. Random matrix sparsification",
    "text": "Given a data matrix A ∈ Rn×d, the basic idea of random matrix sparsification is to randomly sparsify the entries of A to get a matrix Ã ∈ Rn×d such that Ã contains fewer nonzero entries compared to A. Such a sparse matrix Ã speeds up matrix-vector multiplication by decreasing the number of arithmetic operations. Let us write Ã = A+N, where N ∈ Rn×d. A fundamental result of random matrix theory is that, as long as N is a random matrix whose entries are zero mean, independent random variables with bounded variance, no low dimensional subspace accommodates N well, i.e., ‖Nm‖2 and ‖Nm‖F are small for small m. In fact, optimal rank m approximation to Ã approximates A nearly as well as Am as long as ‖Am‖ ‖Nm‖ (for both Frobenious and spectral norm) and the quantity ‖Nm‖ bounds the influence that N may exert on the optimal rank m approximation to Ã. Next we describe a simple random uniform sampling scheme as well as a simple non-uniform sampling scheme for generating sparse Ã that were proposed in (Achlioptas & Mcsherry, 2007) along with bounds on ‖Nm‖. In the following, we set b to be b = max(i,j) |A(i, j)|."
  }, {
    "heading": "2.3.1. UNIFORM SAMPLING SCHEME",
    "text": "In random matrix sparsificaion using uniform sampling scheme, p fraction of entries of A are set to zero to obtain a sparse Ã. In particular,\nÃ(i, j) = { A(i, j)/p with probability p 0 otherwise\n(3)\nIt was shown in (Achlioptas & Mcsherry, 2007) that as long as p is bounded from below, with high probability, ‖Nm‖ is bounded as shown below (a simplified version of a result from (Achlioptas & Mcsherry, 2007)).\nTheorem 1. [Theorem 2 of (Achlioptas & Mcsherry, 2007)] For p ≥ (8 log n)4/n, let Ã be the random sparse matrix obtained by applying uniform sampling scheme (equation 3). Then with probability at least (1 − 1/n19 log3 n), for any m ≤ min{n, d}, N satisfies ‖Nm‖2 ≤ 4b √ n/p and\n‖Nm‖F ≤ 4b √ mn/p."
  }, {
    "heading": "2.3.2. NON-UNIFORM SAMPLING SCHEME",
    "text": "Random sparsification using uniform sampling can be improved by retaining entries with probability that depends on their magnitude. For any p > 0 define τij = p(A(i, j)/b)2\nand let pij = max { τij , √ τij × (8 log n)4/n } . Then a\nsparse Ã can be obtained from A using the following nonuniform sampling scheme.\nÃ(i, j) = { A(i, j)/pij with probability pij 0 otherwise (4)\nSuch non-uniform sampling scheme yields greater sparsification when entry magnitudes vary, without increasing error bound of Theorem 1.\nTheorem 2. [Theorem 3 of (Achlioptas & Mcsherry, 2007)] Let Ã be the random sparse matrix obtained by applying non-uniform sampling scheme (equation 4). Then with probability at least (1− 1/n19 log3 n), for any m ≤ min{n, d}, N satisfies ‖Nm‖2 ≤ 4b √ n/p and ‖Nm‖F ≤ 4b √ mn/p."
  }, {
    "heading": "In addition, expected number of non-zero entries in Ã is at",
    "text": "most p(‖A‖F /b)2 + d(8 log n)4.\nFor any s > 0, setting p = s(b/‖A‖F )2 in the above Theorem ensures that expected number of non-zero entries in Ã is at most s+ d(8 log n)4."
  }, {
    "heading": "3. An algorithm for k-means clustering using random matrix sparsification",
    "text": "While the goal of k-means clustering is to well approximate each row of A with its cluster center, as can be seen from equation 1, an equivalent formulation in equation 2 shows that the problem actually amounts to finding an optimal rank\nAlgorithm 1 K-means clustering using random sparsification Input : Data matrix A ∈ Rn×d, number of clusters k, a positive scalar p and a γ-approximation k-means algorithm. Output : Cluster indicator matrix Xγ̃ determining a k partition of the rows of A.\n1: Compute Ã using non-uniform sampling scheme (equation 4). 2: Run the γ-approximation k-means algorithm on Ã to obtain Xγ̃ . 3: Return Xγ̃ .\nk subspace for approximating the columns of A. Moreover, the choice of subspace is constrained because it must be spanned by the columns of a cluster indicator matrix. The random sampling schemes presented in the previous section yields a sparse Ã whose optimal rank m approximation Ãm approximates Am reasonably well for small m. For appropriate choice of m, if such Am approximates optimal rank k subspace for approximating the columns of A well, then a reasonable strategy for k-means clustering that will reduce number of arithmetic operations is to perform kmeans clustering on Ã, instead of A, and hope that optimal k-means clustering solution of Ã will be close to optimal kmeans clustering solution of A. We propose such a strategy in Algorithm 1. In the next section we present an analysis of this algorithm and prove that an optimal k-means clustering solution of Ã indeed results in an approximately optimal k-means objective of A."
  }, {
    "heading": "4. Analysis of algorithm",
    "text": "In this section we present an analysis of Algorithm 1. For all our results we have assumed that n ≥ d. The main intuition for the technical part of the proof is that even though A and Ã look very different because of the enforced sparse structure, if their appropriate low-rank structures are similar, that is enough to argue that optimal k-means solution of Ã is close to optimal k-means solution of A. We use the notion of projection cost preserving sketch1 as a useful mathematical object for our proof. If B is a rank k projection-cost preserving sketch for A with error 1, then it implies (can be easily shown) that optimal k-means solution of B is (1 + 1) optimal k-means solution of A (Cohen et al., 2015). It turns out that for appropriate choice of m = m(k, 1), the best rankm approximation of A, namely Am, constructed by the m largest SVD structure form a rank k projection-cost preserving sketch for A with error 1.\n1B ∈ Rn×d ′\nis a rank k projection-cost preserving sketch of A ∈ Rn×d, with error 0 ≤ ≤ 1 if, for all rank k orthogonal projection matrices P ∈ Rn×n it holds that (1− )‖A−PA‖2F ≤ ‖B − PB‖2F + c ≤ (1 + )‖A − PA‖2F for some fixed nonnegative constant c that may depend on A and B but is independent of P.\nSimilarly, Ãm is a rank k projection-cost preserving sketch for Ã. We show that optimal k-means solution of Ã is close to optimal k-mean solution of A in two steps.\n1. First, we show the reverse direction of the implication of rank k projection-cost preserving sketch also holds. In particular, we show that an optimal k-means solution of Ã is also (1 +O( 1)) optimal for Ãm.\n2. Then we show that Ãm is also rank k projection-cost preserving sketch for A with a different error 2. (this is where we quantify amount of sparsity to k-means error)\nCombining these two facts and properly choosing 1 and 2, we conclude that optimal k-means solution of Ã is (1 + ) optimal k-means solution of A. We lay out the necessary details in the following subsections."
  }, {
    "heading": "4.1. Relation between clustering objective functions of",
    "text": "Ã and Ãm\nWe first show that close to optimal cluster indicator matrix obtained by solving k-means clustering problem on Ã yields a close to optimal k-means objective of Ãm.\nLemma 1. For any 0 < ≤ 1/2, let m = dk/ e. For any Ã ∈ Rn×d with rank r ≥ dk/ e + k, let Ãm be its best rank m approximation. For any set S of rank k cluster projection matrices, let P̃∗ = argminP∈S ‖Ã − PÃ‖2F and P̃∗m = argminP∈S ‖Ãm−PÃm‖2F . For any γ ≥ 1, if ‖Ã− P̂Ã‖2F ≤ γ‖Ã− P̃∗Ã‖2F , then, ‖Ãm− P̂Ãm‖2F ≤ γ‖Ãm − P̃∗mÃm‖2F + (γ − 1)‖Ã − Ãm‖2F + ‖P̂(Ã − Ãm)‖2F . In particular, the following holds.\n(i) If γ = 1, then, ‖Ãm − P̃∗Ãm‖2F ≤ (1 + 2 )‖Ãm − P̃∗mÃm‖2. (ii) If γ = 1 + 1, for any 0 < 1 < 1 satisfying 1 ∑r i=m+1 σ 2 i (Ã) ≤ ∑m+k i=m+1 σ 2 i (Ã), then, ‖Ãm − P̂Ãm‖2F ≤ (1 + 1 + 4 )‖Ãm − P̃∗mÃm‖2.\nProof of the above lemma, which follows from repeated applications of linear algebra basics, choice of m, definition of P̂ and optimality of P̃∗, is long and technical and is differed to the supplementary material for better readability. Note that on the left hand side of the first inequality above (for γ = 1), we have used P̃∗ instead of P̂ since P̂ and P̃∗ are identical for γ = 1."
  }, {
    "heading": "4.2. Relation between clustering objective functions of Ãm and A",
    "text": "Now we show that optimal cluster indicator matrix obtained by solving k-means clustering problem on Ãm results in close to optimal k-means objective of A. Let P∗ =\nargminP∈S ‖A − PA‖2F and P̃∗m = argminP∈S ‖Ãm − PÃm‖2F . Our goal is to show that ‖A−P̃∗mA‖2F is close to ‖A−P∗A‖2F . In fact, we prove a stronger result showing that Ãm is a rank k projection cost preserving sketch2 of A. We do this in multiple steps. First we show that for small k, Ãm is approximately best rank m subspace of A. Next, we show that such an Ãm is a rank k projection cost preserving sketch for A, which in turn ensures the required guarantee.\nWe start with the following lemma which is a consequence of Theorem 2. Lemma 2. Fix any m ≥ 1 and let 0 < 2 < 1/ √ m. Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = 16nb 2\n22‖A‖2F . Then Ã contains O ( n 22 + d(log n)4 ) non-zero entries in expectation and with probability at least (1− 1/n19 log3 n),\n‖A− Ãm‖F ≤ ‖A−Am‖F + 3 √ 2m 1/4‖A‖F\nWe tailor the above result to show that under mild conditions Ãm approximates Am reasonably well.\nLemma 3. Fix any 3, where 0 < 3 < 1. Let rank of A be ρ. Fix any k that satisfies ∑k/ 3 i=1 σ 2 i (A) ≤ 12 ∑ρ i=1 σ 2 i (A), and let m = dk/ 3e. Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n93‖A‖2F\n) . Then Ã contains\nO ( nk 93 + d(log n)4 ) non-zero entries in expectation and with probability at least (1− 1/n19 log3 n),\n‖A− Ãm‖F ≤ (1 + 23)‖A−Am‖F\nNext, we use a result from (Cohen et al., 2015) to show that if Ãm is close to best rank approximation of A, then Ãm is rank k projection cost preserving sketch for A.\nTheorem 3. [Theorem 9 of (Cohen et al., 2015)] Let m = dk/ 3e. For any A ∈ Rn×d, 0 ≤ 4 ≤ 1 and any B ∈ Rn×d with rank(B) = m satisfying ‖A − B‖2F ≤ (1 + 24)‖A−Am‖2F , the sketch B is a projection cost preserving sketch for A. Specifically, for all rank k orthogonal projections P,\n(1− 2 4)‖A−PA‖2F ≤ ‖B−PB‖2F + c ≤ (1 + 2 3 + 5 4)‖A−PA‖2F\nwhere c is a non-negative scalar.\nFrom Lemma 3 we see that with high probability, ‖A − Ãm‖2F ≤ (1 + 2 23 + 43)‖A −Am‖2F = (1 + 3 23)‖A −\n2If B is a rank k projection cost preserving sketch of A, then optimal k-means clustering solution of B results in approximately optimal k-means clustering objective of A (Cohen et al., 2015).\nAm‖2F . Setting 4 = √\n3 3 and B = Ãm, it follows from Theorem 3 that Ãm is a rank k projection cost preserving sketch of A."
  }, {
    "heading": "4.3. Main result",
    "text": "We combine these results from previous two subsections to present the main result of this paper. Theorem 4. Fix any , where 0 < < 1/4. Let rank of A be ρ. For any k that satisfies ∑k/ i=1 σ 2 i (A) ≤ 1 2 ∑ρ i=1 σ 2 i (A), and let m = dk/ e. Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n9‖A‖2F\n) . For any\nset S of rank k cluster projection matrices, let P∗ = argminP∈S ‖A−PA‖2F , P̃∗ = argminP∈S ‖Ã−PÃ‖2F and P̃∗m = argminP∈S ‖Ãm − PÃm‖2F . For any γ ≥ 1, if ‖Ã − P̂Ã‖2F ≤ γ‖Ã − P̃∗Ã‖2F , then Ã contains O ( nk 9 + d(log n) 4 )\nnon-zero entries in expectation and with probability at least (1 − 1/n19 log3 n) the following holds,\n(i) If γ = 1, then ‖A−P̂A‖2F ≤ (1+2 )(1+11 ) 1−4 ‖A−P ∗A‖2\n(ii) If γ = 1 + 1, for any 0 < 1 < 1 satisfying 1 ∑r i=m+1 σ 2 i (Ã) ≤ ∑m+k i=m+1 σ 2 i (Ã), then ‖A − P̂A‖2F ≤ (1+ 1+4 )(1+11 ) 1−4 ‖A−P ∗A‖2\nProof. Set 3 = . Then from Lemma 3 we get, ‖A − Ãm‖2F ≤ (1 + 3 2)‖A −Am‖2F . Now setting B = Ãm and 4 = √ 3 in Theorem 3, for any rank k orthogonal projection P we get, (1− 2 √\n3 )‖A−PA‖2F ≤ ‖Ãm −PÃm‖2F + c ≤ (1 + 2 + 5 √ 3 )‖A−PA‖2F\nor simplifying,\n(1− 4 )‖A−PA‖2F ≤ ‖Ãm −PÃm‖2F + c ≤ (1 + 11 )‖A−PA‖2F (5)\nLet γ1 = (1 + 2 ) if γ = 1 and γ1 = (1 + 1 + 4 ) if γ ≥ 1. Then from lemma 1 we get ‖Ãm − P̂Ãm‖2F ≤ γ1‖Ãm − P̃∗mÃm‖2. Using this result and repeated application of Equation 5 we get,\n‖A− P̂A‖2F\n≤ 1 1− 4\n{ ‖Ãm − P̂Ãm‖2F + c } ≤ 1\n1− 4\n{ γ1‖Ãm − P̃∗mÃm‖2F + c } ≤ 1\n1− 4\n{ γ1‖Ãm −P∗Ãm‖2F + c } ≤ 1\n1− 4 { γ1 [ (1 + 11 )‖A−P∗A‖2F − c ] + c }\n≤ γ1(1 + 11 ) 1− 4 ‖A−P∗A‖2F\nSubstituting appropriate value of γ1 yields the result.\nThe above result (Theorem 4) is obtained by stitching together many intermediate results. To make sure that everything works at the end, we have different ranges for in Lemma 1 and Theorem 4.\nA simple consequence of the above theorem is the following result which ensures (1 + ′) approximation for any 0 < ′ < 1.\nCorollary 1. Fix any ′, where 0 < ′ < 1. Let rank of A be ρ. Let m = O(k/ ′) and fix any k that satisfies∑m i=1 σ 2 i (A) ≤ 12 ∑ρ i=1 σ 2 i (A). Let Ã be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n( ′)9‖A‖2F\n) . For any set S of rank\nk cluster projection matrices, let P∗ = argminP∈S ‖A− PA‖2F and P̃∗ = argminP∈S ‖Ã − PÃ‖2F . For any 1 ≤ γ ≤ 2 satisfying (γ − 1) ∑r i=m+1 σ\n2 i (Ã) ≤∑m+k\ni=m+1 σ 2 i (Ã), if ‖Ã− P̂Ã‖2F ≤ γ‖Ã− P̃∗Ã‖2F , then Ã contains O (\nnk ( ′)9 + d(log n)\n4 )\nnon-zero entries in ex-\npectation and with probability at least (1−1/n19 log3 n) the following holds,\n‖A− P̂A‖2F ≤ γ(1 + ′)‖A−P∗A‖2"
  }, {
    "heading": "5. Empirical evaluations",
    "text": "In this section we present empirical evaluation of our proposed algorithm on three real world datasets: USPS, RCV1 and TDT2. The USPS dataset (Hull, 1994) contains 9298 handwritten digit images, where each 16 × 16 image is represented by a feature vector of length 256. We seek to find k = 10 clusters, one for each of the ten digits. The RCV1 dataset (Lewis et al., 2004) is an archive of over 800, 000 manually categorized news articles recently made available by Reuters. We use a smaller subset of this dataset available from LIBSVM webpage (LIB) containing 15, 564 news articles from 53 categories. Each such news article is represented by a feature vector of length 47, 236. We seek to find k = 53 clusters, one for each news article category. The TDT2 dataset (Cieri et al., 1999) consists of 11201 text documents which are classified into 96 semantic categories. We use a smaller subset of this dataset available from Deng Cai’s webpage3 where those documents appearing in two or more categories are removed, and only the largest 30 categories are kept, resulting in 9, 394 documents in total. Each such document is represented by a feature vector of length 36, 771. We seek to find k = 30 clusters, one for each news article category. For USPS dataset, all (100%) data matrix entries are non-zero. However, for TDT2 dataset, only 0.35% entries of the 9394 × 36771 data matrix are\n3http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html\nnon-zero, while for RCV1 dataset, only 0.14% entries of the 15564 × 47236 data matrix are non-zero. Since these two later datasets are already very sparse we reduce data dimensionality to 1000 using random projection in both cases by multiplying original data matrices with a random projection matrix (of appropriate size) whose entries are standard i.i.d. normals4. After this random projection step, resulting projected data matrices become dense matrices, each containing 100% non-zero entries. As we will demonstrate next, for these two dense projected matrices our proposed sparsification method finds k-means clustering solution without severely affecting cluster quality.\nTo apply Lloyd’s heuristic for k-means clustering we use Matlab’s kmeans function which, by default, uses kmeans++ algorithm (Arthur & Vassilvitskii, 2007) for cluster center initialization. We repeat this clustering 30 times, each time initializing cluster center using k-means++ and selecting the final clustering as the one with lowest k-means objective. We demonstrate the effect of random sparsification obtained by uniform and non-uniform sampling on k-means clustering by reporting the following quantities, (a) the ratio h1(q) = ‖A−XqX>q A‖2F /‖A−XX>A‖2F , (b) cluster quality h2(q), measured by normalized mutual information (with respect to ground truth cluster labels of A) of a sparse data matrix Ã whose q fracation of entries are non-zero, and (c) normalized objective function h3(q) = ‖A − XqX>q A‖2F /‖A‖2F , as we vary q. In the above description, Xq is the cluster indicator matrix obtained by running k-means on sparse data matrix Ã whose q fraction of entries are non-zero and X is the cluster indicator matrix obtained by running k-means on A.\nFor uniform sampling, p simply indicates that p fraction\n4It has been shown (Cohen et al., 2015) that such dimensionality reduction introduces (1 + ) relative error to optimal k-means objective. We chose projected dimension to be 1000 since increasing it further did not increase normalized mutual information significantly.\nof entries of Ã are non-zero (in expectation, when A is dense matrix). For non-uniform sampling, number of nonzero entries in Ã can only be guaranteed by Theorem 2, which typically holds for large n. In our empirical evaluation we use a slightly different strategy for non-uniform sampling than what is presented in section 2.3.2. However, this modified strategy, in principle, is still similar to what is presented in section 2.3.2. For any fixed value of p, note that τij = p(A(i, j)/b)\n2. Now, instead of using pij in terms of τij as given in section 2.3.2, we use,\npij = { τij if τij ≥ p× f√ τij × p× f otherwise\nwhere, f > 1, is to be chosen later. Therefore, for τij < p×f , pij = √ τij × p× f = p×(|A(i, j)|/b)× √ f . The basic idea is still same as before, i.e., when τij is small, instead of setting pij ∝ (A(i, j))2, we set pij ∝ |A(i, j)|. Now consider the case when τij < p× f , for all i, j. The expected number of non-zero entries is ∑ i,j p × √ f × (|A(i, j)|/b) = pnd× √ f×Avg(|A(i, j)|/b). Therefore, if we choose5 f = 1/(Avg(|A(i, j)|/b))2, expected number of non-zero entries in Ã is pnd. In general, when the condition τij < p× f does not hold for all i, j, the expected fraction will be even less since pij < p, for τij ≥ p× f . In our experimental setting, we set f = 1/(Avg(|A(i, j)|/b))2. This ensures for any p, non-uniform sampling results in at most p fraction of non-zero entries in Ã. In our experiments, we vary p from 0.01 to 1.0 in steps of 0.01 and for each value of p, the number of of non-zero entries in Ã obtained due to non-uniform sampling is denoted by q and is plotted in Figure 2 for all three datasets. Observe that for all values of p, q = p for uniform sampling and q ≤ p for non-uniform sampling.\nNext, in Figure 3 we show how random sparsification affects k-means clustering quality with increasing q. As can be seen from Figure 3, with increasing q, h1(q) and h3(q) decrease, while h2(q) increases as one would expect. In fact, h1(q) decreases quickly towards its optimal value 1 and corresponding h2(q) value quickly increases towards optimal k-means normalized mutual information of A. The normalized k-means objective h3(q) also shows steady decrease with increasing q. As can be seen from Figure 3, for all three datasets, non-uniform sampling yields better k-means clustering performance compared to uniform sampling. This makes perfect sense since non-uniform sampling, unlike uniform sampling, enforces sparsity by retaining entries with probability that depends on their magnitude. In fact, for TDT2 and RCV1 datasets, non-uniform sampling results in significant improvement in k-means clustering performance compared to uniform sampling.\n5Avg(|A(i, j)|) represents average over all entries |A(i, j)|.\nUSPS\nTDT2\nRCV1"
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper we proposed a simple algorithm for k-means clustering using random matrix sparsification and presented its analysis. We proved that under mild condition, for any ∈ (0, 1), a dense data matrix A ∈ Rn×d can be randomly sparsified to yield a data matrix Ã ∈ Rn×d containing O(nk/ 9 + d log4 n) non-zero entries in expectation, such that an (1+ )-approximate k-mean clustering solution of Ã results in (1 +O( ))-approximate clustering solution of A with high probability. Empirical results on three real world datasets demonstrated that k-means clustering solution of Ã was indeed very close to k-means clustering solution of A. Moreover, sparsification obtained by non-uniform sampling resulted in better cluster quality compared to uniform sampling. Empirical results also seem to suggest that the O(1/ 9) dependence on the number of non-zero entries in Ã is possibly a bit loose. We conclude this paper with two possible open questions: (a) Is it possible to analytically provide a better estimate (by improving dependence on 1/ ) of the number of non-zero entries in the sparse matrix Ã that ensures (1 + ) approximation guarantee? and, (b) Using different proof technique, is it possible to show that γ-approximate clustering solution of Ã will result in γ(1 + )-approximate solution of A as shown in Corollary 1, even for γ > 2? In other words, is the restriction on γ\nin Corollary 1 a limitation of the proof technique or does it indicate computational hardness of the problem?"
  }],
  "year": 2018,
  "references": [{
    "title": "Fast computation of low-rank matrix approximations",
    "authors": ["D. Achlioptas", "F. Mcsherry"],
    "venue": "J. ACM,",
    "year": 2007
  }, {
    "title": "K-means++: The advantages of careful seeding",
    "authors": ["D. Arthur", "S. Vassilvitskii"],
    "venue": "In 18th Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2007
  }, {
    "title": "Speeding up kmeans by approximating euclidean distances via block vectors",
    "authors": ["T. Bottesch", "T. Buhler", "M. Kachele"],
    "venue": "In 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Random projections for k-means clustering",
    "authors": ["C. Boutsidis", "A. Zouzias", "P. Drineas"],
    "venue": "In 24th Annual Conference on Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Randomized dimensionality reduction for k-means clustering",
    "authors": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "The TDT-2 text and speech corpus",
    "authors": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"],
    "venue": "In DARPA Broadcast News Workshop,",
    "year": 1999
  }, {
    "title": "Dimensionality reduction for k-means clustering and low rank approximation",
    "authors": ["M.B. Cohen", "S. Elder", "C. Musco", "M. Persu"],
    "venue": "In 47th Annual Symposium on Theory of Computing,",
    "year": 2015
  }, {
    "title": "The Hardness of K-means Clustering",
    "authors": ["S. Dasgupta"],
    "venue": "Technical report (University of California, San Diego. Department of Computer Science and Engineering). Department of Computer Science and Engineering,",
    "year": 2008
  }, {
    "title": "Yinyanh k-means: A drop-in replacement of the classic k-means with consistent speedup",
    "authors": ["Y. Ding", "Y. Zhao", "X. Shen", "M. Musuvathi", "T. Mytkowicz"],
    "venue": "In 32nd International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Accelerated k-means with adaptive distance bounds",
    "authors": ["J. Drake", "G. Hamerly"],
    "venue": "In 5th NIPS Workshop on Optimization for Machine Learning,",
    "year": 2012
  }, {
    "title": "Using triangle inequality to accelerate k-means",
    "authors": ["C. Elkan"],
    "venue": "In 20th International Conference on Machine Learning,",
    "year": 2003
  }, {
    "title": "A unified framework for approximating and clustering data",
    "authors": ["D. Feldman", "M. Langberg"],
    "venue": "In 43rd Annual ACM Symposium on Theory of Computing,",
    "year": 2011
  }, {
    "title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering",
    "authors": ["D. Feldman", "M. Schmidt", "C. Sohler"],
    "venue": "In 24th Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2013
  }, {
    "title": "Making k-means even faster",
    "authors": ["G. Hamerly"],
    "venue": "In SIAM International Conference on Data Mining,",
    "year": 2010
  }, {
    "title": "A database for hand written text recognition research",
    "authors": ["J.J. Hull"],
    "venue": "IEEE Transactions on pattern Analysis and machine Intelligence,",
    "year": 1994
  }, {
    "title": "RCV1: A new benchmark collection for text categorization research",
    "authors": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2004
  }, {
    "title": "Sparse embedded kmeans clustering",
    "authors": ["W. Liu", "X. Shen", "I.W. Tsang"],
    "venue": "In 31st Annual Conference on Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Least square quantization in PCM",
    "authors": ["S. Lloyd"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1982
  }, {
    "title": "Symmetric gauge functions and unitarily invariant norms",
    "authors": ["L. Mirsky"],
    "venue": "The Quarterly Journal of Mathematics,",
    "year": 1960
  }, {
    "title": "Fast k-means with accurate bounds",
    "authors": ["J. Newling", "F. Fleuret"],
    "venue": "In 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Top 10 algorithms in data mining. Knowledge and Information Syaytems",
    "authors": ["X. Wu"],
    "year": 2008
  }],
  "id": "SP:a85bb2f8fe74b6ab7f4c8a749997c5887503f51e",
  "authors": [{
    "name": "Kaushik Sinha",
    "affiliations": []
  }],
  "abstractText": "K-means clustering algorithm using Lloyd’s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high computational cost resulting from pairwise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd’s heuristic. Main contributing factor of this computational bottle neck is a matrix-vector multiplication step, where the matrix contains all the data points and the vector is a cluster center. In this paper we show that we can randomly sparsify the original data matrix resulting in a sparse data matrix which can significantly speed up the above mentioned matrix vector multiplication step without significantly affecting cluster quality. In particular, we show that optimal k-means clustering solution of the sparse data matrix, obtained by applying random matrix sparsification, results in an approximately optimal k-means clustering objective of the original data matrix. Our empirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our proposed sparsification method can indeed achieve satisfactory clustering performance.",
  "title": "K-means clustering using random matrix sparsification"
}