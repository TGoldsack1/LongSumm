{
  "sections": [{
    "text": "Proceedings of NAACL-HLT 2018, pages 200–207 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Knowledge Bases (KBs) are widely used for representing information in a structured format. Such KBs, including Wikidata (Vrandečić and Krötzsch, 2014), Google Knowledge Vault (Dong et al., 2014), and YAGO (Suchanek et al., 2007), often store information as facts in the form of triples, consisting of two entities and a relation between them. KBs have many applications in fields such as machine translation, information retrieval and question answering (Ferrucci, 2012).\nWhen considering a KB’s suitability for a task, primary considerations are the number of facts it contains (Färber et al., 2015), and the precision of those facts. One metric which is often overlooked is completeness. This can be defined as the proportion of facts about an entity that are present in the KB as compared to an ideal KB which has every\nfact that can be known about that entity. For example, previous research (Suchanek et al., 2011; Min et al., 2013) has shown that between 69% and 99% of entities in popular KBs lack at least one relation that other entities in the same class have. As of 2016, Wikidata knows the father of only 2% of all people in the KB (Galárraga et al., 2017). Google found that 71% of people in Freebase have no known place of birth, and 75% have no known nationality (Dong et al., 2014).\nPrevious work has focused on a general concept of completeness, where all KB entities are expected to be fully complete, independent of how the KB is used (Motro, 1989; Razniewski et al., 2016; Zaveri et al., 2013). This is a problem because different use cases of a KB may have different completeness requirements. For this work, we were interested in determining a KB’s completeness with respect to its query usage, which we term Demand-Weighted Completeness. For example, a relation used 100 times per day is more important than one only used twice per day."
  }, {
    "heading": "1.1 Problem specification",
    "text": "We define our task as follows: ‘Given an entity E in a KB, and query usage data of the KB, predict the distribution of relations that E must have in order for 95% of queries about E to be answered successfully.’"
  }, {
    "heading": "1.2 Motivation",
    "text": "Demand-Weighted Completeness allows us to predict both important missing relations for existing entities, and relations required for unseen entities. As a result we can target acquisition of sources to fill important KB gaps.\nIt is possible to be entirely reactive when ad-\n200\ndressing gaps in KB data. Failing queries can be examined and missing fields marked for investigation. However, this approach assumes that:\n1. the same KB entity will be accessed again in future, making the data acquisition useful. This is far from guaranteed.\n2. the KB already contains all entities needed. While this may hold for some use cases, the most useful KB’s today grow and change to reflect a changing world.\nBoth assumptions become unnecessary with an abstract representation of entities, allowing generalization to predict usage. The appropriateness of the abstract representation can be measured by how well the model distinguishes different entity types, and how well the model predicts actual usage for a set of entities, either known or unknown.\nFurther, the Demand-Weighted Completeness of a KB with respect to a specific task can be used as a metric for system performance at that task. By identifying gaps in the KB, it allows targeting of specific improvements to achieve the greatest increase in completeness.\nOur work is the first to consider KB completeness using the distribution of observed KB queries as a signal. This paper details a learning-based approach that predicts the required relation distributions for both seen and unseen class signatures (Section 3), and shows that a neural network model can generalize relation distributions efficiently and accurately compared to a baseline frequency-based approach (Section 6)."
  }, {
    "heading": "2 Related work",
    "text": "Previous work has studied the completeness of the individual properties or database tables over which queries are executed (Razniewski and Nutt, 2011; Razniewski et al., 2015). This approach is suitable for KBs or use cases where individual tables, and individual rows in those tables, are all of equal importance to the KB, or are queried separately.\nCompleteness of KBs has also been measured based on the cardinality of properties. Galárraga et al. (2017) and Mirza et al. (2016) estimated cardinality for several relations with respect to individual entities, yielding targeted completeness information for specific entities. This approach depends on the availability of relevant free text, and uses handcrafted regular expressions to extract the\ninformation, which can be noisy and doesn’t scale to large numbers of relations.\nThe potential for metrics around completeness and dynamicity of a KB are explored in Zaveri et al. (2013), focusing on the task-independent idea of completeness, and the temporal currency, volatility and timeliness of the KB contents. While their concept of timeliness has some similarities to demand-weighted completeness in its taskspecific ’data currency’, we focus more on how the demand varies over time, and how the completeness of the KB varies with respect to that change in demand."
  }, {
    "heading": "3 Representing Entities",
    "text": ""
  }, {
    "heading": "3.1 Class Distributions",
    "text": "The data for a single entity does not generalize on its own. In order to generalize from observed usage information to unseen entities and unseen usage, and smooth out outliers, we need to combine data from similar entities. Such combination requires a shared entity representation, allowing combination of similar entities while preventing their confusion with dissimilar entities.\nFor this work, an entity may be a member of multiple classes (or types). We aggregate usage across multiple entities by abstracting to their classes. Membership of a class can be considered as a binary attribute for an entity, with the entity’s membership of all the classes considered in the analysis forming a class signature.\nFor example, the entity barackObama is a person, politician, democrat, and writer, among other classes. He is not a republican. Considering these five classes as our class space, the class signature for barackObama would look like Figure 1.\nDefining an entity by its classes has precedent in previous work (Galárraga et al., 2017; Razniewski et al., 2016). It allows consideration of entities and\nclass combinations not yet seen in the KB (though not entirely new classes)."
  }, {
    "heading": "3.2 Relation Distributions",
    "text": "KB queries can be considered as graph traversals, stepping through multiple edges of the knowledge graph to determine the result of multi-clause query. For example, the query:\ny : hasPresident(USA, x) ∧ hasSpouse(y, x) (1)\ndetermines the spouse of the president of the United States by composing two clauses, as shown in Figure 2.\nThe demand-weighted importance of a relation R for an entity E is defined as the number of query clauses about E which contain R, as a fraction of the total number of clauses about E. For example, Equation 1 contains two clauses. As the first clause queries for the hasPresident relation of the USA entity, we attribute this occurrence of hasPresident to the USA entity. Aggregating the clauses for an entity gives a total entity usage of the form seen in Figure 3.\nSince the distribution of relation usage is dominated by a few high-value relations (see Figure 6), we only consider relations required to satisfy 95% of queries."
  }, {
    "heading": "3.3 Predicting Relations from Classes",
    "text": "Combining the two representation methods above, we aim to predict the relation distribution for a\ngiven entity (as in Figure 4) using the class membership for the entity (as in Figure 1). This provides the expected usage profile of an entity, potentially before it has seen any usage."
  }, {
    "heading": "4 Data and Models",
    "text": ""
  }, {
    "heading": "4.1 Our knowledge base",
    "text": "We make use of a proprietary KB (Tunstall-Pedoe, 2010) constructed over several years, combining a hand-curated ontology with publicly available data from Wikipedia, Freebase, DBPedia, and other sources. However, the task can be applied to any KB with usage data, relations and classes. We use a subset of our KB for this analysis due to the limitation of model size as a function of the number of classes (input features) and the number of relations (output features).\nOur usage data is generated by our Natural Language Understanding system, which produces KB queries from text utterances. Though it is difficult to remove all biases and errors from the system when operated at industrial scale, we use a hybrid system of curated rules and statistical methods to reduce such problems to a minimum. Such errors should not impact the way we evaluate different models for their ability to model the data itself."
  }, {
    "heading": "4.2 Datasets",
    "text": "To create a class signature, we first determine the binary class membership vector for every entity in the usage dataset. We then group entities by class signature, so entities with identical class membership are grouped together.\nFor each class signature, we generate the relation distribution from the usage data of the entities with that signature. In our case, this usage data is a random subset of query traffic against the KB taken from a specific period of time. The more usage a class signature has, the more fine-grained the\ndistribution of relations becomes. The data is divided into 10 cross-validation folds to ensure that no class signature appears in both the validation and training sets.\nWe generate 3 different sizes of dataset for experimentation (see Table 1), to see how dataset size influences the models."
  }, {
    "heading": "4.3 Relation prediction models",
    "text": ""
  }, {
    "heading": "4.3.1 Baseline - Frequency-Based",
    "text": "In this approach, we compute the relation distribution for each individual class by summing the usage data for all entities of that class (see Section 3). This gives a combined raw relation usage as seen in Figure 5.\nFor every class in the training set we store this raw relation distribution. At test time, we compute the predicted relation distribution for a class signature as the normalized sum of the raw distributions of all its classes. However, these single-class distributions do not capture the influence of class co-occurrence, where the presence of two classes together may have a stronger influence on the importance of a relation than each class on their own. Additionally, storing distributions for each class signature does not scale, and does not generalize to unseen class combinations."
  }, {
    "heading": "4.3.2 Learning-Based Approaches",
    "text": "To investigate the impact of class co-occurrence, we use two different learning models to predict the relation distribution for a given set of input classes. The vector of classes comprising the class signature is used as input to the learned models.\nLinear regression. Using the normalized relation distribution for each class signature, we\ntrained a least-squares linear regression model to predict the relation distribution from a binary vector of classes. This model has (n×m) parameters, where n is the number of input classes and m is the number of relations. We implemented our linear regression model using Scikit-learn toolkit (Pedregosa et al., 2011).\nNeural network. We trained a feed-forward neural network using the binary class vector as the input layer, with a low-dimensional (h) hidden layer (with rectified linear unit as activation) followed by a softmax output layer of the size of the relation set. This model has h(n+m) parameters, which depending on the value of h is significantly smaller than the linear regression model. The objective function used for training was KullbackLiebler Divergence. We chose Keras (Chollet, 2015) to implement the neural network model. The model had a single 10-node Rectified Linear Unit hidden layer, with a softmax over the output."
  }, {
    "heading": "5 Evaluation",
    "text": "We compare the predicted relation distributions to those observed for the test examples in two ways:\nWeighted Jaccard Index. We modified the Jaccard index (Jaccard, 1912) to include a weighting term, which weights every relation with the mean weight in the predicted and observed distribution (see Figure 6). This rewards a correctly predicted relation without focusing on the proportion predicted for that relation, and is sufficient to define a set of important relations for a class sig-\nnature. This is given by:\nJ =\n∑ i W (Ri)×Ri ∈ (P ∩O) ∑ i W (Ri)×Ri ∈ (P ∪O)\n(2)\nwhere P is the predicted distribution, O is the observed distribution, W (Ri) is the mean weight of relation Ri in P and O. We also calculate false negatives (observed but not predicted) and false positives (predicted but not observed), by modifying the second term in the numerator of Equation 2 to give P\\O and O\\P , rather than P ∩O.\nIntersection. We compute the intersection of the two distributions (see Figure 6). This is a more strict comparison between the distributions which penalizes differences in weight for individual relations. This is given by:\nI = ∑\ni\nmin(P (Ri), O(Ri)) (3)"
  }, {
    "heading": "5.1 Usage Weighted Evaluation",
    "text": "We also evaluated the models using the Weighted Jaccard index and Intersection methods, but weighting by usage counts for each signature. This metric rewards the models more for correctly predicting relation distributions for common class signatures in the usage data. While unweighted analysis is useful to examine how the model covers the breadth of the problem space, weighted evaluation more closely reflects the model’s utility for real usage data."
  }, {
    "heading": "5.2 Temporal Prediction",
    "text": "Additionally, we evaluated the models on their ability to predict future usage. With an unchanging usage pattern, evaluation against future usage would be equivalent to cross-validation (assuming the same signature distribution in the folds). However, in many real world cases, usage of a KB varies over time, seasonally or as a result of changing user requirements.\nTherefore we also evaluated a neural model against future usage data to measure how elapsed time affected model performance. The datasets T1, T2, and T3 each contain 3 datasets (of similar size to D1small, D2medium, and D1large), and were created using usage data from time periods with a fixed offset, t. The base set was created at time t0, T1 at time t0 + t, T2 at time t0 + 2t, and T3 at time t0+3t. A time interval was chosen that reflected the known variability of the usage data,\nsuch that we would expect the usage to not be the same."
  }, {
    "heading": "6 Results",
    "text": ""
  }, {
    "heading": "6.1 Cross-Validation",
    "text": "10-fold cross-validation results are shown in Table 2. The neural network model performs best, outperforming the baseline model by 6-8 percentage points. The regression model performs worst, trailing the baseline model by 4-8 percentage points."
  }, {
    "heading": "6.1.1 Baseline",
    "text": "The baseline model shows little improvement with increasing amounts of data - the results from D1small to D3large (3x more data points) only improve by just over 1 percentage point. This suggests that this model is unable to generalise from the data, which is expected from the lack of class co-occurrence information in the model. Interestingly, the baseline model shows an increase in false negatives on the larger datasets, implying the lack of generalisation is more problematic for more fine-grained relation distributions."
  }, {
    "heading": "6.1.2 Linear Regression",
    "text": "The linear regression model gives a much lower Jaccard measure than the baseline model. This is likely due to the number of parameters in the model relative to the number of examples. For D1small, the model has approximately 6m parameters, with 12k training examples, making this an\nunder-determined system. For D3large the number of parameters rises to 20m, with 37k training examples, maintaining the poor example:parameter ratio. From this we might expect the performance of the model to be invariant with the amount of data.\nHowever, the larger datasets also have higher resolution relation distributions, as they are aggregated from more individual examples. This has the effect of reducing the impact of outliers in the data, giving improved predictions when the model generalises. We do indeed see that the linear regression model improves notably with larger datasets, closing the gap to the baseline model from 8 percentage points to 4."
  }, {
    "heading": "6.1.3 Neural Network",
    "text": "The neural network model shows much better performance than either of the other two methods. The Jaccard score is consistently 6-8% above the regression model, with far fewer false negatives and smaller numbers of false positives. This is likely to be due to the smaller number of parameters of the neural model versus the linear regression model. For D3large, the 10-node hidden layer model amounts to 115k parameters with 37k training examples, a far better ratio (though still not ideal) than for the linear regression model."
  }, {
    "heading": "6.1.4 Weighted Evaluation",
    "text": "We include in Table 3 the results using the weighted evaluation scheme described in Section 5.1. This gives more usage-focused evaluation, emphasizing the non-uniform usage of different class signatures. The D3large neural model achieves 85% precision with a weighted evaluation. With the low rate of false negatives, this indicates that a similar model could be used to predict the necessary relations for KB usage."
  }, {
    "heading": "6.2 Intersection",
    "text": "Table 4 gives measurements of the intersection metric. These show a similar trend to the Jaccard scores, with lower absolute values from the stricter evaluation metric. Although the Jaccard measure shows correct relation set prediction with a precision of 0.700, predicting the proportions for those relations accurately remains a difficult problem. The best value we achieved was 0.398."
  }, {
    "heading": "6.3 Unweighted Temporal Prediction",
    "text": "In addition to evaluating models on their ability to predict the behaviour of unseen class signatures, we also evaluated the neural model on its ability to predict future usage behaviour. The results of this experiment are given in Table 5.\nWe observe a very slight downward trend in the precision of the model using all three base datasets (D1small - D3large), with a steeper (but still slight) downward trend for the larger datasets. This suggests that a model trained on usage data from one period of time will have significant predictive power on future datasets."
  }, {
    "heading": "7 Measuring Completeness of a KB",
    "text": "Once we have a suitable model of the expected relation distributions for class combinations, we use the model to predict the expected relation distribution for specific entities in our KB. We then compare the predicted relation distribution to the observed relations for each specific entity. The completeness of an entity is given by the sum of the relation proportions for the predicted relations the entity has in the KB.\nAny gaps for an entity represent relations that, if added to the KB, would have a quantifiable positive impact on the performance of the KB. By focussing on the most important entities according to our usage, we can target fact addition to have the greatest impact to the usage the KB receives.\nBy aggregating the completeness values for a set of entities, we may estimate the completeness of subsets of the KB. This aggregation is weighted by the frequency with which the entity appears in the usage data, giving a usage-weighted measure of the subset’s completeness. These subsets can represent individual topics, individual classes of entity, or overall information about the KB as a whole.\nFor example, using the best neural model above on an unrepresentative subset of our KB, we evaluate the completeness of that subset at 58.3%. This not only implies that we are missing a substantial amount of necessary information for these entities with respect to the usage data chosen, but permits targeting of source acquisition to improve the entity completness in aggregate. For example, if we are missing a large number of hasBirthdate facts for people, we might locate a source that has that information. We can quantify the benefit of that effort in terms of improved usage performance."
  }, {
    "heading": "8 Conclusions and Future Work",
    "text": "We have introduced the notion of DemandWeighted Completeness as a way of determining a KB’s suitability by employing usage data. We have demonstrated a method to predict the distribution of relations needed in a KB for entities of a given class signature, and have compared three different models for predicting these distributions. Further, we have described a method to measure the completeness of a KB using these distributions.\nFor future work we would like to try complex neural network architectures, regularisation, and semantic embeddings or other abstracted relations to enhance the signatures. We would also like to investigate Good-Turing frequency estimation (Good, 1953)."
  }],
  "year": 2018,
  "references": [{
    "title": "Keras",
    "authors": ["François Chollet."],
    "venue": "https://github. com/fchollet/keras.",
    "year": 2015
  }, {
    "title": "Knowledge Vault: A web-scale approach to probabilistic knowledge fusion",
    "authors": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang."],
    "venue": "Proceedings of the 20th ACM",
    "year": 2014
  }, {
    "title": "A comparative survey of DBpedia, Freebase, OpenCyc, Wikidata, and YAGO",
    "authors": ["Michael Färber", "Basil Ell", "Carsten Menne", "Achim Rettinger."],
    "venue": "Semantic Web Journal, July .",
    "year": 2015
  }, {
    "title": "Introduction to ”this is watson",
    "authors": ["D.A. Ferrucci."],
    "venue": "IBM J. Res. Dev. 56(3):235–249. https://doi. org/10.1147/JRD.2012.2184356.",
    "year": 2012
  }, {
    "title": "Predicting completeness in knowledge bases",
    "authors": ["Luis Galárraga", "Simon Razniewski", "Antoine Amarilli", "Fabian M. Suchanek."],
    "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, New York, NY,",
    "year": 2017
  }, {
    "title": "The population frequencies of species and the estimation of population parameters",
    "authors": ["I.J. Good."],
    "venue": "Biometrika 40(3-4):237. https://doi. org/10.1093/biomet/40.3-4.237.",
    "year": 1953
  }, {
    "title": "The distribution of the flora in the alpine zone.1",
    "authors": ["Paul Jaccard"],
    "venue": "New Phytologist 11(2):37–50",
    "year": 1912
  }, {
    "title": "Distant supervision for relation extraction with an incomplete knowledge base",
    "authors": ["Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek."],
    "venue": "HLT-NAACL. pages 777–782.",
    "year": 2013
  }, {
    "title": "Expanding Wikidata’s parenthood information by 178%, or how to mine relation cardinalities",
    "authors": ["Paramita Mirza", "Simon Razniewski", "Werner Nutt."],
    "venue": "ISWC 2016 Posters & Demonstrations Trac. CEUR-WS. org.",
    "year": 2016
  }, {
    "title": "Integrity = validity + completeness",
    "authors": ["Amihai Motro."],
    "venue": "ACM Trans. Database Syst. 14(4):480– 502. https://doi.org/10.1145/76902. 76904.",
    "year": 1989
  }, {
    "title": "Scikit-learn: Machine learning",
    "authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"],
    "year": 2011
  }, {
    "title": "Identifying the extent of completeness of query answers over partially complete databases",
    "authors": ["Simon Razniewski", "Flip Korn", "Werner Nutt", "Divesh Srivastava."],
    "venue": "Proceedings of the 2015 ACM SIGMOD International Conference on Management",
    "year": 2015
  }, {
    "title": "Completeness of queries over incomplete databases",
    "authors": ["Simon Razniewski", "Werner Nutt."],
    "venue": "VLDB 4:749–760.",
    "year": 2011
  }, {
    "title": "But what do we actually know",
    "authors": ["Simon Razniewski", "Fabian M Suchanek", "Werner Nutt."],
    "venue": "Proceedings of AKBC pages 40–44.",
    "year": 2016
  }, {
    "title": "Watermarking for ontologies",
    "authors": ["Fabian Suchanek", "David Gross-Amblard", "Serge Abiteboul."],
    "venue": "The semantic web–ISWC 2011 pages 697–713.",
    "year": 2011
  }, {
    "title": "YAGO: A core of semantic knowledge",
    "authors": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."],
    "venue": "Proceedings of the 16th International Conference on World Wide Web. ACM, New York, NY, USA, WWW ’07, pages 697–706. https:",
    "year": 2007
  }, {
    "title": "True Knowledge: Open-domain question answering using structured knowledge and inference",
    "authors": ["William Tunstall-Pedoe."],
    "venue": "AI Magazine 31(3):80–",
    "year": 2010
  }, {
    "title": "Wikidata: A free collaborative knowledgebase",
    "authors": ["Denny Vrandečić", "Markus Krötzsch."],
    "venue": "Commun. ACM 57(10):78–85. https://doi.org/",
    "year": 2014
  }, {
    "title": "Quality assessment for linked open data: A survey",
    "authors": ["Amrapali Zaveri", "Anisa Rula", "Andrea Maurino", "Ricardo Pietrobon", "Jens Lehmann", "Sören Auer"],
    "year": 2013
  }],
  "id": "SP:50b282f47c5207456ebd4d5fd00fea7a09cd32f2",
  "authors": [{
    "name": "Andrew Hopkinson",
    "affiliations": []
  }, {
    "name": "Amit Gurdasani",
    "affiliations": []
  }, {
    "name": "Arpit Mittal",
    "affiliations": []
  }],
  "abstractText": "In this paper we introduce the notion of Demand-Weighted Completeness, allowing estimation of the completeness of a knowledge base with respect to how it is used. Defining an entity by its classes, we employ usage data to predict the distribution over relations for that entity. For example, instances of person in a knowledge base may require a birth date, name and nationality to be considered complete. These predicted relation distributions enable detection of important gaps in the knowledge base, and define the required facts for unseen entities. Such characterisation of the knowledge base can also quantify how usage and completeness change over time. We demonstrate a method to measure DemandWeighted Completeness, and show that a simple neural network model performs well at this prediction task.",
  "title": "Demand-Weighted Completeness Prediction for a Knowledge Base"
}