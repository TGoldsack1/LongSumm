{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 125–136 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n125"
  }, {
    "heading": "1 Introduction",
    "text": "Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includ-\nWe release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue\ning recent work on introduction of large textualdialogue datasets (e.g., Lowe et al. (2015); Serban et al. (2016)) and end-to-end neural network based models (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b).\nCurrent dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored (Eckert et al., 1997; Singh et al., 2000; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrkšić et al., 2016), as well as work on gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017). However, the visual context in these tasks is lim-\nited to one static image. Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answers).\nSeveral situations of real-world dialogue among humans involve more ‘dynamic’ visual context, i.e., video-style information of the world moving around us (both spatially and temporally). Further, several human conversations involve more than two speakers, with changing roles. In order to develop such dynamically-visual multimodal dialogue models, we introduce a new ‘manyspeaker, video-context chat’ testbed, along with a new dataset and models for the same. Our dataset is based on live-broadcast soccer (FIFA18) game videos from the ‘Twitch.tv’ live video streaming platform, along with the spontaneous, many-speaker live chats about the game. This challenging testbed allows us to develop dialogue models where the generated response is required to be relevant to the temporal and spatial events in the live video, as well as be relevant to the chat history (with potential impact towards videogrounded applications such as personal assistants, intelligent tutors, and human-robot collaboration).\nWe also present several strong discriminative and generative baselines that learn to retrieve and generate bimodal-relevant responses. We first present a triple-encoder discriminative model to encode the video, chat history, and response, and then classify the relevance label of the response. We then improve over this model via tridirectional attention flow (TriDAF). For the generative models, we model bidirectional attention flow between the video and textual chat context encoders, which then decodes the response. We evaluate these models via retrieval ranking-recall, phrasematching metrics, as well as human evaluation studies. We also present dataset analysis as well as model ablations and attention visualizations to understand the contribution of the video vs. chat modalities and the model components."
  }, {
    "heading": "2 Related Work",
    "text": "Early dialogue systems had components of natural language (NL) understanding unit, dialogue manager, and NL generation unit (Bates, 1995). Statistical learning methods were used for automatic feature extraction (Dowding et al., 1993; Mikolov et al., 2013), dialogue managers incorporated reward-driven reinforcement learning (Young et al., 2013; Shah et al., 2016), and the\ngeneration units have been extended with seq2seq neural network models (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016).\nIn addition to the focus on textual dialogue context, using multimodal context brings more potential for having real-world grounded conversations. For example, spoken dialogue systems have been widely explored (Singh et al., 2000; Gurevych and Strube, 2004; Georgila et al., 2006; Eckert et al., 1997; Young, 2000; Janin et al., 2003; De Mori, 2007; Wen et al., 2015; Su et al., 2016; Mrkšić et al., 2016; Hori et al., 2016; Celikyilmaz et al., 2015, 2017), as well as gesture and haptics based dialogue (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008). Additionally, dialogue systems for digital personal assistants are also well explored (Myers et al., 2007; Sarikaya et al., 2016; Damacharla et al., 2018). In the visual modality direction, some important recent attempts have been made to use static image based context in dialogue systems (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017), who proposed the ‘visual dialog’ task, where the human can ask questions on a static image, and an agent interacts by answering these questions based on the previous chat context and the image’s visual features. Also, Celikyilmaz et al. (2014) used visual display information for on-screen item resolution in utterances for improving personal digital assistants.\nIn contrast, we propose to employ dynamic video-based information as visual context knowledge in dialogue models, so as to move towards video-grounded intelligent assistant applications. In the video+language direction, previous work has looked at video captioning (Venugopalan et al., 2015) as well as Q&A and fill-inthe-blank tasks on videos (Tapaswi et al., 2016; Jang et al., 2017; Maharaj et al., 2017) and interactive 3D environments (Das et al., 2018; Yan et al., 2018; Gordon et al., 2017; Anderson et al., 2017). There has also been early related work on generating sportscast commentaries from simulation (RoboCup) soccer videos represented as non-visual state information (Chen and Mooney, 2008). Also, Liu et al. (2016a) presented some initial ideas on robots learning grounded task representations by watching and interacting with humans performing the task (i.e., by converting human demonstration videos to Causal And-Or graphs). On the other hand, we propose a new video-chat dataset where the\ndialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW) (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992). In the live video stream direction, Fu et al. (2017) and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and Barbieri et al. (2017) presented emotes and troll prediction."
  }, {
    "heading": "3 Twitch-FIFA Dataset",
    "text": ""
  }, {
    "heading": "3.1 Dataset Collection and Processing",
    "text": "For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA18) games along with the users’ live chat conversations about the game. This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based dialogue data. An example is shown in Fig. 1 (and an original screenshot example in Fig. 2), where the users perform a complex ‘manyspeaker’, ‘multimodal’ dialogue. Overall, we collected 49 FIFA-18 game videos along with their users’ chat, and divided them into 33 videos for training, 8 videos for validation, and 8 videos for testing. Each such video is several hours long, providing a good amount of data (Table 2).\nTo extract triples (instances) of video context, chat context, and response from this data, we divide these videos based on the fixed time frames instead of fixed number of utterances in order to maintain conversation topic clusters (because of the sparse nature of chat utterances count over the time). First, we use 20-sec context windows to extract the video clips and users utterances in\nthis time frame, and use it as our video and chat contexts, resp. Next, the chat utterances in the immediately-following 10-sec window (response window) that do not overlap with the next instance’s context window are considered as potential responses.1 Hence, there are only two instances (triples) in a 60-sec long video, i.e., 20-sec video+chat context window and 10-sec response window, and there is no overlap between the instances. Now, out of these potential responses, to only allow the response that has at least some good coherence and relevance with the chat context’s topic, we choose the first (earliest) response that has high similarity with some other utterance in this response window (using 0.5 BLEU-4 threshold, based on manual inspection).2 Human Quality Evaluation of Data Filtering Process: To evaluate the quality of the responses that result from our filtering process described above, we performed an anonymous (randomly shuffled w/o identity) human comparison between the response selected by our filtering process vs. the first response from the response window without any filtering, based on relevance w.r.t. video and chat context. Table 1 presents the results on 100 sample size, showing that humans in a blindtest found 90% (34+56) of our filtered responses as valid responses, verifying that our response selection procedure is reasonable. Furthermore, out of these 90% valid responses, we found that 55% are chat-only relevant, 11% are video-only relevant, and 24% are both video+chat relevant.\nIn order to make the above procedure safe and to make the dataset more challenging, we also discourage frequent responses (top-20 most-frequent\n1We use non-overlapping windows because: (1) the utterances are non-uniformly distributed in time and hence if we have a shifting window, sometimes a particular data instance/chunk becomes very sparse and contains almost zero utterances; (2) we do not want overlap between response of one window with the context of the next window, so as to avoid the encoder already having seen the response (as part of context) that the decoder needs to generate for the other window.\n2Based on intuition that if multiple speakers are saying the same response in that 10-second window, then this response should be more meaningful/relevant w.r.t. chat context.\ngeneric utterances) unless no other response satisfies the similarity condition, hence suppressing the frequent responses.3 If we couldn’t find any utterance based on the multi-response matching procedure described above, then we just consider the first utterance in the 10-second window as the response.4 We also make sure that the chat context window has at least 4 utterances, otherwise we exclude that context window and also the corresponding response window from the dataset. After all this processing, our final resulting dataset contains 10, 510 samples in training, 2, 153 samples in validation, and 2, 780 samples in test.5"
  }, {
    "heading": "3.2 Dataset Analysis",
    "text": "Dataset Statistics Table 2 presents the full statistics on train, validation, and test sets of our Twitch-FIFA dataset, after the filtering process described in Sec. 3.1. As shown, the average chat context length in the dataset is around 68 words, and the average response length is 6.3 words. Chat Context Size Fig. 3 presents the study of number of utterances in the chat context vs. the number of such training samples. As we limit the minimum number of utterances to 4, chat context with less than 4 utterances is not present in the dataset. From the Fig. 3, it is clear that as the number of utterances in the chat context increases, the number of such training samples decrease. Frequent Words Fig. 4 presents the top-20 frequent words (excluding stop words) and their corresponding frequency in our Twitch-FIFA dataset. Most of these frequent words are related to soccer vocabulary. Also, some of these frequent words are twitch emotes (e.g. ‘kappa’, ‘inceptionlove’).\n3Note that this filtering suppresses the performance of simple frequent-response baseline described in Sec. 4.1.\n4Other preprocessing steps include: omit the utterances in the response window which refer to a speaker name out of the current chat context; remove non-representative utterances, e.g., those with hyperlinks; replace (anonymize) all the user identities mentioned in the utterances with a common tag (i.e., anonymizing due to similar intuitions from the Q&A community (Hermann et al., 2015)).\n5Note that this is substantially larger than or comparable to most current video captioning datasets. We plan to further extend our dataset based on diverse games and video types."
  }, {
    "heading": "4 Models",
    "text": "Let v = {v1, v2, .., vm} be the video context frames, u = {u1, u2, .., un} be the textual chat (utterance) context tokens, and r = {r1, r2, .., rk} be response tokens generated (or retrieved)."
  }, {
    "heading": "4.1 Baselines",
    "text": "Our simple non-trained baselines are MostFrequent-Response (re-rank the candidate responses based on their frequency in the training set), Chat-Response-Cosine (re-rank the candidate responses based on their similarity score w.r.t. the chat context), and Nearest-Neighbor (find the Kbest similar chat contexts in the training set, take their corresponding responses, and then re-rank the candidate responses based on mean similarity score w.r.t. this K-best response set). For trained baselines, we use logistic regression and Naive Bayes methods. We use the final state of a Twitch-trained RNN Language Model to represent the chat context and response. Please see supplementary for full details."
  }, {
    "heading": "4.2 Discriminative Models",
    "text": ""
  }, {
    "heading": "4.2.1 Triple Encoder",
    "text": "For our simpler discriminative model, we use a ‘triple encoder’ to encode the video context, chat context, and response (see Fig. 5), as an extension of the dual encoder model in Lowe et al. (2015). The task here is to predict the given train-\ning triple (v, u, r) as positive or negative. Let hvf , huf , and h r f be the final state information of the video, chat, and response LSTM-RNN (bidirectional) encoders respectively; then the probability of a positive training triple is defined as follows:\np(v, u, r; θ) = σ([hvf ;h u f ] TWhrf + b) (1)\nwhere W and b are trainable parameters. Here, W can be viewed as a similarity matrix which will bring the context [hvf ;h u f ] into the same space as the response hrf , and get a suitable similarity score. For optimizing our discriminative model, we use max-margin loss function similar to Mao et al. (2016) and Yu et al. (2017). Given a positive training triple (v, u, r), let the corresponding negative training triples be (v′, u, r), (v, u′, r), and (v, u, r′), i.e., one modality is wrong at a time in each of these three (see Sec. 5 for the negative example selection). The max-margin loss is:\nL(θ) = ∑ [max(0,M + log p(v′, u, r)− log p(v, u, r))\n+ max(0,M + log p(v, u′, r)− log p(v, u, r)) + max(0,M + log p(v, u, r′)− log p(v, u, r))]\n(2)\nwhere the summation is over all the training triples in the dataset. M is a tunable margin hyperparameter between positive and negative training triples."
  }, {
    "heading": "4.2.2 Tridirectional Attention Flow (TriDAF)",
    "text": "Our tridirectional attention flow model learns stronger joint spaces between the three modalities in a mutual-information way. We use bidirectional attention flow mechanisms (Seo et al., 2017) between the video and chat contexts, between the video context and the response, as well as between the chat context and the response, hence enabling attention flow across all three modalities, as shown in Fig. 6. We name this model Tridirectional Attention Flow or TriDAF. We will next discuss the bidirectional attention flow mechanism between video and chat contexts, but the same formulation holds true for bidirectional attention between video context and response, and between chat context and response. Given the video context hidden\nstate hvi and chat context hidden state h u j at time steps i and j respectively, the bidirectional attention mechanism is based on the similarity score:\nS (v,u) i,j = w T S(v,u) [hvi ;h u j ;h v i huj ] (3)\nwhere S(v,u)i,j is a scalar, wS(v,u) is a trainable parameter, and denote element-wise multiplication. The attention distribution from chat context to video context is defined as αi: = softmax(Si:), hence the chat-to-video context vector cv←ui = ∑ j αi,jh u j . Similarly, the attention distribution from video context to chat context is defined as βj: = softmax(S:j), hence the videoto-chat context vector cu←vj = ∑ i βj,ih v i .\nWe then compute similar bidirectional attention flow mechanisms between the video context and response, and between the chat context and response. Then, we concatenate each hidden state and its corresponding context vector from other two modalities, e.g., ĥvi = [h v i ; c v←u i ; c v←r i ] for the ith timestep of the video context. Finally, we add self-attention mechanism (Lin et al., 2017) across the concatenated hidden states of each of the three modules.6 If ĥvi is the final concatenated vector of the video context at time step i, then the selfattention weights αs for this video context are the softmax of es:\nesi = V v a tanh(W v a ĥ v i + b v a) (4)\nwhere V va , W v a , and b v a are trainable self-attention parameters. The final representation vector of the full video context after self-attention is ĉv =∑\ni α s i ĥ v i . Similarly, the final representation vectors of the chat context and the response are ĉu and ĉr, respectively. Finally, the probability that 6In our preliminary experiments, we found that adding self-attention is 0.92% better in recall@1 and faster than passing the hidden states through another layer of RNN, as done in Seo et al. (2017).\nthe given training triple (v, u, r) is positive is:\np(v, u, r; θ) = σ([ĉv; ĉu]TWĉr + b) (5)\nAgain, here also we use max-margin loss (Eqn. 2)."
  }, {
    "heading": "4.3 Generative Models",
    "text": ""
  }, {
    "heading": "4.3.1 Seq2seq with Attention",
    "text": "Our simpler generative model is a sequence-tosequence model with bilinear attention mechanism (similar to Luong et al. (2015)). We have two encoders, one for encoding the video context and another for encoding the chat context, as shown in Fig. 7. We combine the final state information from both encoders and give it as initial state to the response generation decoder. The two encoders and the decoder are all two-layer LSTMRNNs. Let hvi and h u j be the hidden states of video and chat encoders at time step i and j respectively. At each time step t of the decoder with hidden state hrt , the decoder attends to parts of video and chat encoders and uses the combined information to generate the next token. Let αt and βt be the attention weight distributions for video and chat encoders respectively with video context vector cvt = ∑ i αt,ih v i and chat context vector\ncut = ∑ j βt,jh u j . The attention distribution for video encoder is defined as (and the same holds for chat encoder):\net,i = h r t TW va h v i ; αt = softmax(et) (6)\nwhere W va is a trainable parameter. Next, we concatenate the attention-based context information (cvt and c u t ) and decoder hidden state (h r t ), and do a non-linear transformation to get the final hidden state ĥrt as follows:\nĥrt = tanh(Wc[c v t ; c u t ;h r t ]) (7)\nwhere Wc is again a trainable parameter. Finally, we project the final hidden state information to vocabulary size and give it as input to a softmax layer to get the vocabulary distribution p(rt|r1:t−1, v, u; θ). During training, we minimize the cross-entropy loss defined as follows:\nLXE(θ) = − ∑∑\nt\nlog p(rt|r1:t−1, v, u; θ) (8)\nwhere the final summation is over all the training triples in the dataset.\nFurther, to train a stronger generative model with negative training examples (which teaches\nthe model to give higher generative decoder probability to the positive response as compared to all the negative ones), we use a max-margin loss (similar to Eqn. 2 in Sec. 4.2.1): LMM(θ) = ∑ [max(0,M + log p(r|v′, u)− log p(r|v, u))\n+ max(0,M + log p(r|v, u′)− log p(r|v, u)) + max(0,M + log p(r′|v, u)− log p(r|v, u))]\n(9)\nwhere the summation is over all the training triples in the dataset. Overall, the final joint loss function is a weighted combination of cross-entropy loss and max-margin loss: L(θ) = LXE(θ) + λLMM(θ), where λ is a tunable hyperparameter."
  }, {
    "heading": "4.3.2 Bidirectional Attention Flow (BiDAF)",
    "text": "The stronger version of our generative model extends the two-encoder-attention-decoder model above to add bidirectional attention flow (BiDAF) mechanism (Seo et al., 2017) between video and chat encoders, as shown in Fig. 7. Given the hidden states hvi and h u j of video and chat encoders at time step i and j, the final hidden states after the BiDAF are ĥvi = [h v i ; c v←u i ] and ĥ u j = [h u i ; c u←v j ] (similar to as described in Sec. 4.2.2), respectively. Now, the decoder attends over these final hidden states, and the rest of the decoder process is similar to Sec 4.3.1 above, including the weighted joint cross-entropy and max-margin loss."
  }, {
    "heading": "5 Experimental Setup",
    "text": "Evaluation We first evaluate both our discriminative and generative models using retrieval-based recall@k scores, which is a concrete metric for such dialogue generation tasks (Lowe et al., 2015). For our discriminative models, we simply rerank the given responses (in a candidate list of size 10, based on 9 negative examples; more details below)\nin the order of the probability score each response gets from the model. If the positive response is within the top-k list, then the recall@k score is 1, otherwise 0, following previous Ubuntu-dialogue work (Lowe et al., 2015). For the generative models, we follow a similar approach, but the reranking score for a candidate response is based on the log probability score given by the generative models’ decoder for that response, following the setup of previous visual-dialog work (Das et al., 2017). In our experiments, we use recall@1, recall@2, and recall@5 scores. For completeness, we also report the phrase-matching metric scores: METEOR (Denkowski and Lavie, 2014) and ROUGE (Lin, 2004) for our generative models. We also present human evaluation.\nTraining Details For negative samples, during training, for every positive triple (video, chat, response) in the training set, we sample 3 random negative triples. For validation/test, we sample 9 random negative responses elsewhere from the validation/test set. Also, the negative samples don’t come from the video corresponding to the positive response. More details of negative samples and other training details (e.g., dimension/vocab sizes, visual feature details, validationbased hyperparamater tuning and model selection), are discussed in the supplementary."
  }, {
    "heading": "6 Results and Analysis",
    "text": ""
  }, {
    "heading": "6.1 Human Evaluation of Dataset",
    "text": "First, the overall human quality evaluation of our dataset (shown in Table 1) demonstrates that it\ncontains 90% responses relevant to video and/or chat context. Next, we also do a blind human study on the recall-based setup (on a set of 100 samples from the validation set), where we anonymize the positive response by randomly mixing it with 9 tricky negative responses in the retrieval list, and ask the user to select the most relevant response for the given video and/or chat context. We found that human performance on this task is around 55% recall@1, demonstrating that this 10-way-discriminative recall-based task setup is reasonably challenging for humans,7 but also that there is a lot of scope for future model improvements because the chance baseline is only 10% and the best-performing model so far (see Sec. 6.3) achieves only 22% recall@1 (on dev set), and hence there is a large 33% gap."
  }, {
    "heading": "6.2 Baseline Results",
    "text": "Table 3 displays all our primary results. We first discuss results of our simple non-trained and trained baselines (see Sec. 4.1). The ‘MostFrequent-Response’ baseline, which just ranks the 10-sized response retrieval list based on their frequency in the training data, gets only around 10% recall@1.8 Our other non-trained baselines: ‘Chat-Response-Cosine’ and ‘Nearest Neighbor’, which ranks the candidate responses based on (Twitch-trained RNN encoder’s vector) cosine similarity with chat-context and K-best training contexts’ response vectors, respectively, achieves slightly better scores. We also show that our simple trained baselines (logistic regression and nearest neighbor) also achieve relatively low scores, indicating that a simple, shallow model will not work on this challenging dataset."
  }, {
    "heading": "6.3 Discriminative Model Results",
    "text": "Next, we present the recall@k retrieval performance of our various discriminative models in Ta-\n7This relatively low human recall@1 performance is because this is a challenging, 10-way-discriminative evaluation, i.e., the choice comes w.r.t. 9 tricky negative examples along with just 1 positive example (hence chance-baseline is only 10%). Note that these negative examples are an artifact of specifically recall-based evaluation only, and will not affect the more important real-world task of response generation (for which our dataset’s response quality is 90%, as shown in Table 1). Moreover, our dataset filtering (see Sec. 3.1) also ‘suppresses’ simple baselines and makes the task even harder.\n8Note that the performance of this baseline is worse than the random choice baseline (recall@1:10%, recall@2:20%, recall@5:50%) because our dataset filtering process already suppresses frequent responses (see Sec. 3.1), in order to provide a challenging dataset for the community.\nble 3: dual encoder (chat context only), dual encoder (video context only), triple encoder, and TriDAF model with self-attention. Our dual encoder models are significantly better than random choice and all our simple baselines above, and further show that they have complementary information because using both of them together (in ‘Triple Encoder’) improves the overall performance of the model. Finally, we show that our novel TriDAF model with self-attention performs significantly better than the triple encoder model.9"
  }, {
    "heading": "6.4 Generative Model Results",
    "text": "Next, we evaluate the performance of our generative models with both retrieval-based recall@k scores and phrase matching-based metrics as discussed in Sec. 5 (as well as human evaluation). We first discuss the retrieval-based recall@k results in Table 3. Starting with a simple sequenceto-sequence attention model with video only, chat only, and both video and chat encoders, the recall@k scores are better than all the simple baselines. Moreover, using both video+chat context is again better than using only one context modality. Finally, we show that the addition of the bidirectional attention flow mechanism improves the performance in all recall@k scores.10 Note that generative model scores are lower than the discriminative models on retrieval recall@k metric, which is expected (see discussion in previous visual dialogue work (Das et al., 2017)), because discriminative models can tune to the biases in the response candidate options, but generative models are more useful for real-world tasks such as\n9Statistical significance of p < 0.01 for recall@1, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.\n10Stat. signif. p < 0.05 for recall@1 w.r.t. Seq2seq+Atten (video+chat); p < 0.01 w.r.t. chat- and video-only models.\ngeneration of novel responses word-by-word from scratch in Siri/Alexa/Cortana style applications (whereas discriminative models can only rank the pre-given list of responses).\nWe also evaluate our generative models with phrase-level matching metrics: METEOR and ROUGE-L, as shown in Table 4. Again, our BiDAF model is stat. significantly better than nonBiDAF model on both METEOR (p < 0.01) and ROUGE-L (p < 0.02) metrics. Since dialogue systems can have several diverse, non-overlapping valid responses, we consider a multi-reference setup where all the utterances in the 10-sec response window are treated as valid responses.11"
  }, {
    "heading": "6.5 Human Evaluation of Models",
    "text": "Finally, we also perform human evaluation to compare our top two generative models, i.e., the video+chat seq2seq with attention and its extension with BiDAF (Sec. 4.3), based on a 100-sized sample. We take the generated response from both these models, and randomly shuffle these pairs to anonymize model identity. We then ask two annotators (for 50 task instances each) to score the responses of these two models based on relevance. Note that the human evaluators were familiar with Twitch FIFA-18 video games and also the Twitch’s unique set of chat mannerisms and emotes. As shown in Table 5, our BiDAF based generative model performs better than the non-BiDAF one, which is already quite a strong video+chat encoder model with attention."
  }, {
    "heading": "7 Ablations and Analysis",
    "text": ""
  }, {
    "heading": "7.1 Negative Training Pairs",
    "text": "We also compare the effect of different negative training triples that we discussed in Sec. 5. Table 6 shows the comparison between one negative\n11Liu et al. (2016b) discussed that BLEU and most phrase matching metrics are not good for evaluating dialogue systems. Also, generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference (Lowe et al., 2015; Liu et al., 2016b; Li et al., 2016). We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation.\ntraining triple (with just a negative response) vs. three negative training triples (one with negative video context, one with negative chat context, and another with negative response), showing that using the 3-negative examples setup is substantially better."
  }, {
    "heading": "7.2 Discriminative Loss Functions",
    "text": "Table 7 shows the performance comparison between the classification loss and max-margin loss on our TriDAF with self-attention discriminative model (Sec. 4.2.2). We observe that max-margin loss performs better than the classification loss, which is intuitive because max-margin loss tries to differentiate between positive and negative training example triples."
  }, {
    "heading": "7.3 Generative Loss Functions",
    "text": "For our best generative model (BiDAF), Table 8 shows that using a joint loss of cross-entropy and max-margin is better than just using only cross-entropy loss optimization (Sec. 4.3.1). Maxmargin loss provides knowledge about the negative samples for the generative model, hence improves the retrieval-based recall@k scores."
  }, {
    "heading": "7.4 Attention Visualization and Examples",
    "text": "Finally, we show some interesting output examples from both our discriminative and generative models as shown in Fig. 8. Additionally, Fig. 9\nvisualizes that our models can learn some correct attention alignments from the generated output response word to the appropriate (goal-related) video frames as well as chat context words."
  }, {
    "heading": "8 Conclusion",
    "text": "We presented a new game-chat based videocontext, many-speaker dialogue task and dataset. We also presented several baselines and state-ofthe-art discriminative and generative models on this task. We hope that this testbed will be a good starting point to encourage future work on the challenging video-context dialogue paradigm. In future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset. We also plan to explore advanced video features such as activity recognition, person identification, etc."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the reviewers for their helpful comments. This work was supported by DARPA YFA17-D17AP00022, ARO-YIP Award W911NF-18-1-0336, Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency."
  }],
  "year": 2018,
  "references": [{
    "title": "Visionand-language navigation: Interpreting visually",
    "authors": ["Peter Anderson", "Qi Wu", "Damien Teney", "Jake Bruce", "Mark Johnson", "Niko Sünderhauf", "Ian Reid", "Stephen Gould", "Anton van den Hengel"],
    "year": 2017
  }, {
    "title": "Towards the understanding of gaming audiences by modeling twitch emotes",
    "authors": ["Francesco Barbieri", "Luis Espinosa-Anke", "Miguel Ballesteros", "Horacio Saggion"],
    "venue": "In Third Workshop on Noisy Usergenerated Text (W-NUT",
    "year": 2017
  }, {
    "title": "Models of natural language understanding",
    "authors": ["Madeleine Bates."],
    "venue": "Proceedings of the National Academy of Sciences, 92(22):9977–9982.",
    "year": 1995
  }, {
    "title": "Embodied conversation: integrating face and gesture into automatic spoken dialogue systems",
    "authors": ["Justine Cassell"],
    "year": 1999
  }, {
    "title": "Deep learning for spoken and text dialog systems",
    "authors": ["Asli Celikyilmaz", "Li Deng", "Dilek Hakkani-Tur."],
    "venue": "Deep Learning in Natural Language Processing (eds. Li Deng and Yang Liu).",
    "year": 2017
  }, {
    "title": "Resolving referring expressions in conversational dialogs for natural user interfaces",
    "authors": ["Asli Celikyilmaz", "Zhaleh Feizollahi", "Dilek HakkaniTur", "Ruhi Sarikaya."],
    "venue": "EMNLP, pages 2094–2104.",
    "year": 2014
  }, {
    "title": "A universal model for flexible item selection in conversational dialogs",
    "authors": ["Asli Celikyilmaz", "Zhaleh Feizollahi", "Dilek HakkaniTur", "Ruhi Sarikaya."],
    "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on, pages 361–367.",
    "year": 2015
  }, {
    "title": "Learning to sportscast: a test of grounded language acquisition",
    "authors": ["David L Chen", "Raymond J Mooney."],
    "venue": "Proceedings of the 25th international conference on Machine learning, pages 128–135. ACM.",
    "year": 2008
  }, {
    "title": "Effects of voice-based synthetic assistant on performance of emergency care provider",
    "authors": ["Praveen Damacharla", "Parashar Dhakal", "Sebastian Stumbo", "Ahmad Y Javaid", "Subhashini Ganapathy", "David A Malek", "Douglas C Hodge", "Vijay Devabhaktuni"],
    "year": 2018
  }, {
    "title": "Embodied question answering",
    "authors": ["Abhishek Das", "Samyak Datta", "Georgia Gkioxari", "Stefan Lee", "Devi Parikh", "Dhruv Batra."],
    "venue": "CVPR.",
    "year": 2018
  }, {
    "title": "Visual dialog",
    "authors": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José MF Moura", "Devi Parikh", "Dhruv Batra."],
    "venue": "CVPR.",
    "year": 2017
  }, {
    "title": "Spoken language understanding: a survey",
    "authors": ["Renato De Mori."],
    "venue": "Automatic Speech Recognition & Understanding, 2007. ASRU. IEEE Workshop on, pages 365–376. IEEE.",
    "year": 2007
  }, {
    "title": "Meteor universal: Language specific translation evaluation for any target language",
    "authors": ["Michael Denkowski", "Alon Lavie."],
    "venue": "Proceedings of the ninth workshop on statistical machine translation, pages 376–380.",
    "year": 2014
  }, {
    "title": "Gemini: A natural language system for spoken-language understanding",
    "authors": ["John Dowding", "Jean Mark Gawron", "Doug Appelt", "John Bear", "Lynn Cherny", "Robert Moore", "Douglas Moran."],
    "venue": "ACL, pages 54–61.",
    "year": 1993
  }, {
    "title": "User modeling for spoken dialogue system evaluation",
    "authors": ["Wieland Eckert", "Esther Levin", "Roberto Pieraccini."],
    "venue": "Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on, pages 80–87. IEEE.",
    "year": 1997
  }, {
    "title": "An introduction to the bootstrap",
    "authors": ["Bradley Efron", "Robert J Tibshirani."],
    "venue": "CRC press.",
    "year": 1994
  }, {
    "title": "The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue",
    "authors": ["Mary Ellen Foster", "Ellen Gurman Bard", "Markus Guhe", "Robin L Hill", "Jon Oberlander", "Alois Knoll."],
    "venue": "Proceedings of the 3rd ACM/IEEE in-",
    "year": 2008
  }, {
    "title": "Video highlight prediction using audience chat reactions",
    "authors": ["Cheng-Yang Fu", "Joon Lee", "Mohit Bansal", "Alexander C Berg."],
    "venue": "EMNLP.",
    "year": 2017
  }, {
    "title": "User simulation for spoken dialogue systems: Learning and evaluation",
    "authors": ["Kallirroi Georgila", "James Henderson", "Oliver Lemon."],
    "venue": "Ninth International Conference on Spoken Language Processing.",
    "year": 2006
  }, {
    "title": "Iqa: Visual question answering in interactive environments",
    "authors": ["Daniel Gordon", "Aniruddha Kembhavi", "Mohammad Rastegari", "Joseph Redmon", "Dieter Fox", "Ali Farhadi."],
    "venue": "arXiv preprint arXiv:1712.03316.",
    "year": 2017
  }, {
    "title": "Semantic similarity applied to spoken dialogue summarization",
    "authors": ["Iryna Gurevych", "Michael Strube."],
    "venue": "Proceedings of the 20th international conference on Computational Linguistics, page 764. Association for Computational Linguistics.",
    "year": 2004
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "NIPS, pages 1693–1701.",
    "year": 2015
  }, {
    "title": "Dialog state tracking with attention-based sequenceto-sequence learning",
    "authors": ["Takaaki Hori", "Hai Wang", "Chiori Hori", "Shinji Watanabe", "Bret Harsham", "Jonathan Le Roux", "John R Hershey", "Yusuke Koji", "Yi Jing", "Zhaocheng Zhu"],
    "venue": "Spoken Language Technol-",
    "year": 2016
  }, {
    "title": "Cobot in lambdamoo: A social statistics agent",
    "authors": ["Charles Lee Isbell", "Michael Kearns", "Dave Kormann", "Satinder Singh", "Peter Stone."],
    "venue": "AAAI/IAAI, pages 36–41.",
    "year": 2000
  }, {
    "title": "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
    "authors": ["Yunseok Jang", "Yale Song", "Youngjae Yu", "Youngjin Kim", "Gunhee Kim."],
    "venue": "CVPR, pages 2680–8.",
    "year": 2017
  }, {
    "title": "The icsi meeting corpus",
    "authors": ["Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke"],
    "venue": "In Acoustics, Speech, and Signal Processing,",
    "year": 2003
  }, {
    "title": "Match: An architecture for multimodal dialogue systems",
    "authors": ["Michael Johnston", "Srinivas Bangalore", "Gunaranjan Vasireddy", "Amanda Stent", "Patrick Ehlen", "Marilyn Walker", "Steve Whittaker", "Preetam Maloor."],
    "venue": "Proceedings of the 40th Annual Meet-",
    "year": 2002
  }, {
    "title": "A persona-based neural conversation model",
    "authors": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios P Spithourakis", "Jianfeng Gao", "Bill Dolan."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "ROUGE: A package for automatic evaluation of summaries",
    "authors": ["Chin-Yew Lin."],
    "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain.",
    "year": 2004
  }, {
    "title": "A structured self-attentive sentence embedding",
    "authors": ["Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Task learning through visual demonstration and situated dialogue",
    "authors": ["Changsong Liu", "Joyce Y Chai", "Nishant Shukla", "Song-Chun Zhu."],
    "venue": "AAAI Workshop: Symbiotic Cognitive Systems.",
    "year": 2016
  }, {
    "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
    "authors": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
    "authors": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."],
    "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue.",
    "year": 2015
  }, {
    "title": "Lstm based conversation models",
    "authors": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."],
    "venue": "arXiv preprint arXiv:1603.09457.",
    "year": 2016
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering",
    "authors": ["Tegan Maharaj", "Nicolas Ballas", "Anna Rohrbach", "Aaron Courville", "Christopher Pal."],
    "venue": "CVPR.",
    "year": 2017
  }, {
    "title": "Generation and comprehension of unambiguous object descriptions",
    "authors": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan L Yuille", "Kevin Murphy."],
    "venue": "CVPR, pages 11–20.",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Imagegrounded conversations: Multimodal context for natural question and response generation",
    "authors": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P Spithourakis", "Lucy Vanderwende."],
    "venue": "arXiv",
    "year": 2017
  }, {
    "title": "Neural belief tracker: Data-driven dialogue state tracking",
    "authors": ["Nikola Mrkšić", "Diarmuid O Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young."],
    "venue": "arXiv preprint arXiv:1606.03777.",
    "year": 2016
  }, {
    "title": "An intelligent personal assistant for task and time management",
    "authors": ["Karen Myers", "Pauline Berry", "Jim Blythe", "Ken Conley", "Melinda Gervasio", "Deborah L McGuinness", "David Morley", "Avi Pfeffer", "Martha Pollack", "Milind Tambe."],
    "venue": "AI Magazine, 28(2):47.",
    "year": 2007
  }, {
    "title": "Computer-intensive methods for testing hypotheses",
    "authors": ["Eric W Noreen."],
    "venue": "Wiley New York.",
    "year": 1989
  }, {
    "title": "Video highlights detection and summarization with lag-calibration based on concept-emotion mapping of crowdsourced time-sync comments",
    "authors": ["Qing Ping", "Chaomei Chen."],
    "venue": "EMNLP Workshop on New Frontiers in Summarization.",
    "year": 2017
  }, {
    "title": "Natural language generation in dialog systems",
    "authors": ["Owen Rambow", "Srinivas Bangalore", "Marilyn Walker."],
    "venue": "Proceedings of the first international conference on Human language technology research, pages 1–4. Association for Computational",
    "year": 2001
  }, {
    "title": "A corpus collection and annotation framework for learning multimodal clarification strategies",
    "authors": ["Verena Rieser", "Ivana Kruijff-Korbayová", "Oliver Lemon."],
    "venue": "6th SIGdial Workshop on DISCOURSE and DIALOGUE.",
    "year": 2005
  }, {
    "title": "Learning effective multimodal dialogue strategies from wizardof-oz data: Bootstrapping and evaluation",
    "authors": ["Verena Rieser", "Oliver Lemon."],
    "venue": "ACL, pages 638–646.",
    "year": 2008
  }, {
    "title": "Data-driven response generation in social media",
    "authors": ["Alan Ritter", "Colin Cherry", "William B Dolan."],
    "venue": "EMNLP, pages 583–593. Association for Computational Linguistics.",
    "year": 2011
  }, {
    "title": "An overview of end-to-end language understanding and dialog",
    "authors": ["Ruhi Sarikaya", "Paul A Crook", "Alex Marin", "Minwoo Jeong", "Jean-Philippe Robichaud", "Asli Celikyilmaz", "Young-Bum Kim", "Alexandre Rochette", "Omar Zia Khan", "Xiaohu Liu"],
    "year": 2016
  }, {
    "title": "Taking cscw seriously",
    "authors": ["Kjeld Schmidt", "Liam Bannon."],
    "venue": "Computer Supported Cooperative Work (CSCW), 1(1-2):7–40.",
    "year": 1992
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "ICLR.",
    "year": 2017
  }, {
    "title": "Multiresolution recurrent neural networks: An application to dialogue response generation",
    "authors": ["Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron C Courville."],
    "venue": "AAAI, pages 3288–3294.",
    "year": 2017
  }, {
    "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C Courville", "Joelle Pineau."],
    "venue": "AAAI, pages 3776–3784.",
    "year": 2016
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "AAAI, pages 3295–3301.",
    "year": 2017
  }, {
    "title": "Interactive reinforcement learning for taskoriented dialogue management",
    "authors": ["Pararth Shah", "Dilek Hakkani-Tür", "Larry Heck."],
    "venue": "NIPS Deep Learning for Action and Interaction Workshop.",
    "year": 2016
  }, {
    "title": "Reinforcement learning for spoken dialogue systems",
    "authors": ["Satinder P Singh", "Michael J Kearns", "Diane J Litman", "Marilyn A Walker."],
    "venue": "Advances in Neural Information Processing Systems, pages 956– 962.",
    "year": 2000
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."],
    "venue": "arXiv preprint",
    "year": 2015
  }, {
    "title": "On-line active reward learning for policy optimisation in spoken dialogue systems",
    "authors": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina RojasBarahona", "Stefan Ultes", "David Vandyke", "TsungHsien Wen", "Steve Young."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Movieqa: Understanding stories in movies through question-answering",
    "authors": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."],
    "venue": "CVPR, pages 4631– 4640.",
    "year": 2016
  }, {
    "title": "Sequence to sequence-video to text",
    "authors": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."],
    "venue": "CVPR, pages 4534–4542.",
    "year": 2015
  }, {
    "title": "A neural conversational model",
    "authors": ["Oriol Vinyals", "Quoc Le."],
    "venue": "Proceedings of ICML Deep Learning Workshop.",
    "year": 2015
  }, {
    "title": "Guesswhat?! visual object discovery through multi-modal dialogue",
    "authors": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron Courville."],
    "venue": "CVPR.",
    "year": 2017
  }, {
    "title": "Advances in automatic meeting record creation and access",
    "authors": ["Alex Waibel", "Michael Bett", "Florian Metze", "Klaus Ries", "Thomas Schaaf", "Tanja Schultz", "Hagen Soltau", "Hua Yu", "Klaus Zechner."],
    "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceed-",
    "year": 2001
  }, {
    "title": "Elizaa computer program for the study of natural language communication between man and machine",
    "authors": ["Joseph Weizenbaum."],
    "venue": "Communications of the ACM, 9(1):36–45.",
    "year": 1966
  }, {
    "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems",
    "authors": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "PeiHao Su", "David Vandyke", "Steve Young."],
    "venue": "arXiv preprint arXiv:1508.01745.",
    "year": 2015
  }, {
    "title": "CHALET: Cornell house agent learning environment",
    "authors": ["Claudia Yan", "Dipendra Misra", "Andrew Bennnett", "Aaron Walsman", "Yonatan Bisk", "Yoav Artzi."],
    "venue": "arXiv preprint arXiv:1801.07357.",
    "year": 2018
  }, {
    "title": "Pomdp-based statistical spoken dialog systems: A review",
    "authors": ["Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D Williams."],
    "venue": "Proceedings of the IEEE, 101(5):1160–1179.",
    "year": 2013
  }, {
    "title": "Probabilistic methods in spoken– dialogue systems",
    "authors": ["Steve J Young."],
    "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 358(1769):1389–1402.",
    "year": 2000
  }, {
    "title": "A joint speaker-listener-reinforcer model for referring expressions",
    "authors": ["Licheng Yu", "Hao Tan", "Mohit Bansal", "Tamara L Berg."],
    "venue": "CVPR.",
    "year": 2017
  }],
  "id": "SP:667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23",
  "authors": [{
    "name": "Ramakanth Pasunuru",
    "affiliations": []
  }, {
    "name": "Mohit Bansal",
    "affiliations": []
  }],
  "abstractText": "Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on livebroadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrasematching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.",
  "title": "Game-Based Video-Context Dialogue"
}