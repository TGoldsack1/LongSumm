{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1243"
  }, {
    "heading": "1 Introduction",
    "text": "Neural architectures have taken the field of machine translation by storm and are in the process of replacing phrase-based systems. Based on the encoder-decoder framework (Sutskever et al., 2014) increasingly complex neural systems are being developed at the moment. These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions (Gehring et al., 2017) or stacked self-attention layers (Vaswani et al., 2017). These architectural changes have led to great performance improvements over classical RNN-based neural translation systems (Bahdanau et al., 2014).\n∗Code and a workflow that reproduces the experiments are available at https://github.com/philschulz/ stochastic-decoder.\n†Work done prior to joining Amazon.\nSurprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures. A notable exception is the work of Zhang et al. (2016) who introduce a sentence-level latent Gaussian variable. In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of Bahdanau et al. (2014). Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.). Neural machine translation (NMT) models are incapable of capturing this variation, however. This is because their likelihood function incorporates the statistical assumption that there is one (and only one) output1 for a given source sentence, i.e.,\nP (yn1 |xm1 ) = n∏\ni=1\nP (yi|xm1 , y<i) . (1)\nOur proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data. The noise sources are modelled as Gaussian random variables. The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data.\n• A thorough discussions of issues encountered when training this model. In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically.\n1Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence. The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.\n• An empirical demonstration of the improvements achievable with the proposed model."
  }, {
    "heading": "2 Neural Machine Translation",
    "text": "The NMT system upon which we base our experiments is based on the work of Bahdanau et al. (2014). The likelihood of the model is given in Equation (1). We briefly describe its architecture. Let xm1 = (x1, . . . , xm) be the source sentence and yn1 the target sentence. Let RNN (·) be any function computed by a recurrent neural network (we use a bi-LSTM for the encoder and an LSTM for the decoder). We call the decoder state at the ith target position ti; 1 ≤ i ≤ n. The computation performed by the baseline system is summarised below.[\nh1, . . . , hm ] = RNN (xm1 ) (2a)\nt̃i = RNN (ti−1, yi−1) (2b)\neij = v ⊤ a tanh ( Wa[t̃i, hj ] ⊤ + ba ) (2c) αij = exp (eij)∑m j=1 exp (eij) (2d)\nci = m∑ j=1 αijhj (2e) ti = Wt[t̃i, ci] ⊤ + bt (2f)\nϕi = softmax(Woti + bo) (2g)\nThe parameters {Wa,Wt,Wo, ba, bt, bo, va} ⊆ θ are learned during training. The model is trained usingmaximum likelihood estimation. Thismeans that we employ a cross-entropy loss whose input is the probability vector returned by the softmax."
  }, {
    "heading": "3 Stochastic Decoder",
    "text": "This section introduces our stochastic decoder model for capturing word-level variation in translation data."
  }, {
    "heading": "3.1 Motivation",
    "text": "Imagine an idealised translator whose translations are always perfectly accurate and fluent. If an MT systemwas providedwith training data from such a translator, it would still encounter variation in that data. After all, there are several perfectly accurate and fluent translations for each source sentence. These can be highly different in both their lexical as well as their syntactic realisations.\nIn practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing onemachinegenerated translation against several human translations (see e.g. Papineni et al., 2002).\nPrior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is thus called for—and our results confirm this intuition.\nAn example of variation comes from free word order and agreement phenomena in morphologically rich languages. An English sentence with rigid word order may be translated into several orderings in German. However, all orderings need to respect the agreement relationship between the main verb and the subject (indicated by underlining) as well as the dative case of the direct object (dashes) and the accusative of the indirect object (dots). The agreement requirements are fixed and independent of word order.\n1. I can’t imagine you naked. (a) Ich kann mir . . . .dich nicht nackt vorstellen. (b) Ich kann . . . . .dich mir nicht nackt vorstellen. (c) . . . . .Dich kann ichmir nicht nackt vorstellen.\nStochastically encoding the word order variation allows the model to learn the same agreement phenomenon from different translation variants as it does not need to encode the word order and agreement relationships jointly in the decoder state.\nFurther examples of VP and NP variation from an actual translation corpus are shown in Figure 1.\nWe aim to address these word-level variation phenomena with a stochastic decoder model."
  }, {
    "heading": "3.2 Model formulation",
    "text": "The model contains a latent Gaussian variable for each target position. This variable depends on the previous latent states and the decoder state. Through the use of recurrent networks, the conditioning context does not need to be restricted and the likelihood factorises exactly.\nP (yn1 |xm1 ) = ∫\ndzn0 p(z0|xm1 )× n∏\ni=1\np(zi|z<i, y<i, xm1 )P (yi|zi1, y<i, xm1 ) (3)\nAs can be seen from Equation (3), the model also contains a 0th latent variable that is meant to initialise the chain of latent variables based solely on the source sentence. Contrast this with the model of Zhang et al. (2016) which uses only that 0th variable. A graphical representation of the stochastic decoder model is given in Figure 2a. Its generative story is as follows\nZ0|xm1 ∼ N (µ0, σ20) (4a) Zi|z<i, y<i, xm1 ∼ N (µi, σ2i ) (4b) Yi|zi0, y<i, xm1 ∼ Cat(ϕi) (4c)\nwhere i = 1, . . . , n and both the Gaussian and the Categorical parameters are predicted by neural network architectures whose inputs vary per time step. This probabilistic formulation can be implemented with a multitude of different architectures. We present ours in the next section."
  }, {
    "heading": "3.3 Neural Architecture",
    "text": "Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs). We use a reparametrisation of the Gaussian variables (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and Lázaro-Gredilla, 2014) to enable backpropagation inside a stochastic computation graph (Schulman et al., 2015). In order to sample ddimensional Gaussian variable z ∈ Rd with mean µ and variance σ2, we first sample from a standard Gaussian distribution and then transform the sample,\nz = µ+ σ ⊙ ϵ ϵ ∼ N (0, I) . (5)\nHere µ, σ ∈ Rd and ⊙ denotes element-wise multiplication (also known as Hadamard product). See the supplement for details on the Gaussian reparametrisation. We use neural networks with one hidden layer with a tanh activation to compute the mean and standard deviation of each Gaussian distribution. A softplus transformation is applied to the output of the standard deviation’s network to ensure positivity. Let us denote the functions that these networks compute by f . For the initial latent state z0 we compute the mean and standard deviation as\nµ0 = fµ0 (hm) σ0 = fσ0 (hm) . (6)\nThe parameters of all other latent distributions are computed by functions fµ and fσ whose inputs vary per target position.\nµi = fµ (ti−1, zi−1) σi = fσ (ti−1, zi−1) (7)\nUsing these values, each latent variable is sampled according to Equation (5). The sampled latent variables are then used to modify the update of the decoder hidden state (Equation (2b)) as follows:\nt̃i = RNN (ti−1, yi−1, zi) (8)\nThe remaining computations stay unchanged. Notice that the latent values are used directly in updating the decoder state. This makes the decoder state a function of a random variable and thus the decoder state is itself random. Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic."
  }, {
    "heading": "4 Inference and Training",
    "text": "We use variational inference (see e.g. Blei et al., 2017) to train the model. In variational inference, we employ a variational distribution q(z) that approximates the true posterior p(z|x) over the latent variables. The distribution q(z) has its own set of parameters λ that is disjoint from the set of model parameters θ. It is used to maximise the evidence lower bound (ELBO) which is a lower bound on the marginal likelihood p(x). The ELBO is maximised with respect to both the model parameters θ and the variational parameters λ. Most NLP models that use DGMs only use one latent variable (e.g. Bowman et al., 2016). Models\nthat use several variables usually employ a mean field approximation under which all latent variables are independent. This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017). For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables,\nq(zn0 ) = q(z0) n∏ i=1 q(zi|z<i) . (9)\nOur stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another. The ELBO thus consists of nested positional ELBOs,\nELBO0 + Eq(z0)[ELBO1 +Eq(z1)[ELBO2 + . . .]] ,\n(10)\nwhere for a given target position i the ELBO is\nELBOi = Eq(zi) [log p(yi|x m 1 , y<i, z<i, zi)]\n−KL (q(zi) || p(zi|xm1 , y<i, z<i)) . (11)\nThe first term is often called reconstruction or likelihood term whereas the second term is called the KL term. Since the KL term is a function of two Gaussian distributions, and the Gaussian is an exponential family, we can compute it analytically (Michalowicz et al., 2014), without the need for sampling. This is very similar to the hierarchical latent variable model of Rezende et al. (2014). Following common practice in DGM research, we employ a neural network to compute the variational distributions. To discriminate it from the\ngenerative model, we call this neural net the inference model. At training time both the source and target sentence are observed. We exploit this by endowing our inference model with a “lookahead” mechanism. Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be processed by the generative decoder. This allows the latent distribution to not only encode information about the currently modelled word but also about the target words that follow it. The conditioning of the inference network is illustrated graphically in Figure 2b. The inference network produces additional representations of the target sentence. One representation encodes the target sentence bidirectionally (12a), in analogy to the source sentence encoding. The second representation is built by encoding the target sentence in reverse (12b). This reverse encoding can be used to provide information about future context to the decoder. We use the symbols b and r for the bidirectional and reverse target encodings, respectively. In our experiments, we again use LSTMs to compute these encodings.[\nb1, . . . , bn ] = RNN (yn1 ) (12a)[\nr1, . . . , rn ] = RNN (yn1 ) (12b)\nIn analogy to the generativemodel (Section 3.3), the inference network uses single hidden layer networks to compute the mean and standard deviations of the latent variable distributions. We denote these functions g and again employ different functions for the initial latent state and all other latent states.\nµ0 = gµ0 (hm, bn) (13a) σ0 = gσ0 (hm, bn) (13b) µi = gµ (ti−1, zi−1, ri, yi) (13c) σi = gσ (ti−1, zi−1, ri, yi) (13d)\nAs before, we use Equation (5) to sample from the variational distribution. During training, all samples are obtained from the inference network. Only at test time do we sample from the generator. Notice that since the inference network conditions on representations produced by the generator network, a naïve application of backpropagation would update parts of the generator network with gradients computed for\nthe inference network. We prevent this by blocking gradient flow from the inference net into the generator."
  }, {
    "heading": "4.1 Analysis of the Training Procedure",
    "text": "The training procedure as outlined above does not work well empirically. This is because our model uses a strong generator. By this we mean that the generation model (that is the baseline NMT model) is a very good density model in and by itself and does not need to rely on latent information to achieve acceptable likelihood values during training. DGMs with strong generators have a tendency to not make use of latent information (Bowman et al., 2016). This problem went initially unnoticed because early DGMs (Kingma and Welling, 2014; Rezende et al., 2014) used weak generators2, i.e. models that made very strong independence assumptions and were not able to capture contextual information without making use of the information encoded by the latent variable. WhyDGMswould ignore the latent information can be understood by considering the KL-term of the ELBO. In order for the latent variable to be informative about the observed data, we need them to have high mutual information I(Z;Y ).\nI(Z;Y ) = Ep(z,y) [ log p(Z, Y )\np(Z)p(Y )\n] (14)\nObserve that we can rewrite the mutual information as an expected KL divergence by applying the definition of conditional probability.\nI(Z;Y ) = Ep(y) [KL (p(Z|Y ) || p(Z))] (15)\nSince we cannot compute the posterior p(z|y) exactly, we approximate it with the variational distribution q(z|y) (the joint is approximated by q(z|y)p(y) where the latter factor is the data distribution). To the extent that the variational distribution recovers the true posterior, the mutual information can be computed this way. In fact, if we take the learned prior p(z) to be an approximation of themarginal ∫ q(z|y)p(y)dy it can easily be shown that the thus computed KL term is an upper bound on mutual information (Alemi et al., 2017). The trouble is that the ELBO (Equation (11)) can be trivially maximised by setting the KL-term to 0 and maximising only the reconstruction term.\n2The term weak generator has first been coined by Alemi et al. (2017).\nThis is especially likely at the beginning of training when the variational approximation does not yet encode much useful information. We can only hope to learn a useful variational distribution if a) the variational approximation is allowed to move away from the prior and b) the resulting increase in the reconstruction term is higher than the increase in the KL-term (i.e. the ELBO increases overall). Several schemes have been proposed to enable better learning of the variational distribution (Bowman et al., 2016; Kingma et al., 2016; Alemi et al., 2017). Here we use KL scaling and increase the scale gradually until the original objective is recovered. This has the following effect: during the initial learning stage, the KL-term barely contributes to the objective and thus the updates to the variational parameters are driven by the signal from the reconstruction term and hardly restricted by the prior. Once the scale factor approaches 1 the variational distribution will be highly informative to the generator (assuming sufficiently slow increase of the scale factor). The KL-term can now be minimised by matching the prior to the variational distribution. Notice that up to this point, the prior has hardly been updated. Thus moving the variational approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes. This is in stark contrast to Bowman et al. (2016) whose prior was a fixed standard normal distribution. Although they used KL scaling, the KL term could only be decreased by moving the variational approximation back to the fixed prior. This problem disappears in our model where priors are learned. Moving the prior towards the variational approximation has another desirable effect. The prior can now learn to emulate the variational “lookahead” mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens). At test time we can thus hope to have learned latent variable distributions that encode information not only about the output at the current position but about future outputs as well."
  }, {
    "heading": "5 Experiments",
    "text": "We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations. We trained models to\ntranslate from English into Arabic, Czech, French and German. The number of sentences for each language after preprocessing is shown in Table 1. The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings. As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4. Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2. We report baselines with and without dropout (Srivastava et al., 2014). For dropout a retention probability of 0.5 was used. As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT). Our implementation differs from theirs in three aspects. First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the inference network (Zhang et al. (2016) use the average of all states). Second, the latent variable is smaller in size than the one used by (Zhang et al., 2016).5 This was done to make their model and the stochastic decoder proposed here as similar as possible. Finally, their implementation was based on groundhog whereas ours builds on Sockeye. Our stochastic decoder model (SDEC) is also built on top of the basic Sockeyemodel. It adds the components described in Sections 3 and 4. Recall that the functions that compute the means and standard deviations are implemented by neural nets with a single hidden layer with tanh activation. The width of that layer is twice the size of the latent variable. In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1). The scale started from 0 and was increased by 1/20,000 after each mini-batch. Thus, at iteration t the scale is min(t/20,000, 1). All models use 1028 units for the LSTM hid3https://github.com/google/sentencepiece 4https://github.com/awslabs/sockeye 5We did, however, find that increasing the latent variable size actually hurt performance in our implementation.\nden state (or 512 for each direction in the bidirectional LSTMs) and 256 for the attention mechansim. Training is done with Adam (Kingma and Ba, 2015). In decoding we use a beam of size 5 and output the most likely word at each position. We deterministically set all latent variables to their mean values during decoding. Monte Carlo decoding (Gal, 2016) is difficult to apply to our setting as it would require sampling entire translations.\nResults We show the BLEU scores for all models that we tested on the IWSLT data set in Table 2. The stochastic decoder dominates the Sockeye baseline across all 4 languages, and outperforms SENT on most languages. Except on German, there is a trend towards smaller latent variable sizes being more helpful. This is in line with findings by Chung et al. (2015) and Fraccaro et al. (2016) who also used relatively small latent variables. This observation also implies that our model does not improve simply because it has more parameters than the baseline. That the margin between the SDEC and SENT models is not large was to be expected for two reasons. First, Chung et al. (2015) and Fraccaro et al. (2016) have shown that stochastic RNNs lead to enormous improvements in modelling continuous sequences but only modest increases in performance for discrete sequences (such as natural language). Second, translation performance is measured in BLEU score. We observed that SDEC often reached better ELBO values than SENT indicating a better model fit. How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future research.\nQualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation. To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled instead of fixed. Contrary to the BLEU-based evaluation, beam search was not used in this evaluation in order to avoid interaction between different latent variable samples. See Figure 3 for examples of syntactic and lexical variation. It is important to note that we do not sample from the categorical output distribution. For each target position we pick the most likely word. A non-stochastic NMT system would always yield the same translation in\nthis scenario. Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system. This supports our initial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address. Like the model of Bowman et al. (2016), SENT presumably tends to ignore the latent variable."
  }, {
    "heading": "6 Related Work",
    "text": "The stochastic decoder is strongly influenced by previous work on stochastic RNNs. The first such proposal was made by Bayer and Osendorfer (2015) who introduced i.i.d. Gaussian latent variables at each output position. Since their model neglects any sequential dependence of the noise sources, it underperformed on several sequence modeling tasks. Chung et al. (2015) made the latent variables depend on previous information by feeding the previous decoder state into the latent variable sampler. Their inference model did not make use of future elements in the sequence. Using a “look-ahead” mechanism in the inference net was proposed by Fraccaro et al. (2016) who had a separate stochastic and deterministic RNN layer which both influence the output. Since the stochastic layer in their model depends on the deterministic layer but not vice versa, they could first run the deterministic layer at inference time and then condition the inference net’s encoding of the future on the thus obtained features. Like us, they used KL scaling during training. More recently, Goyal et al. (2017) proposed an auxiliary loss that has the inference net predict future feature representations. This approach yields state-of-the-art results but is still in need of a theoretical justification. Within translation, Zhang et al. (2016) were the first to incorporate Gaussian variables into an NMT model. Their approach only uses one sentence-level latent variable (corresponding to our z0) and can thus not deal with word-level variation directly. Concurrently to our work, Su et al. (2018) have also proposed a recurrent latent variable model for NMT. Their approach differs from ours in that they do not use a 0th latent variable nor a look-ahead mechanism during inference time. Furthermore, their underlying recurrent model is a GRU. In the wider field of NLP, deep generative mod-\nels have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016)."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "Wehave presented a recurrent decoder formachine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora. Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation. The proposed model consistently outperforms strong baselines\non several language pairs. As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promising.\n• Latent factor models: our model only contains one source of variation per word. A latent factor model such as DARN (Gregor et al., 2014) would consider several sources simultaneously. This would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena. • Richer prior and variational distributions: The diagonal Gaussian is likely too simple a\ndistribution to appropriately model the variation in our data. Richer distributions computed by normalising flows (Rezende and Mohamed, 2015; Kingma et al., 2016) will likely improve our model. • Extension to other architectures: Introducing latent variables into non-autoregressive translation models such as the transformer (Vaswani et al., 2017) should increase their translation ability further."
  }, {
    "heading": "8 Acknowledgements",
    "text": "Philip Schulz and Wilker Aziz were supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002. Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105)."
  }],
  "year": 2018,
  "references": [{
    "title": "An information theoretic analysis of deep latent variable models",
    "authors": ["Alexander Alemi", "Ben Poole", "Ian Fischer", "Joshua V. Dillon", "Rif A. Saurous", "Kevin Murphy."],
    "venue": "arxiv preprint .",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "ICLR.",
    "year": 2014
  }, {
    "title": "Learning stochastic recurrent networks",
    "authors": ["Justin Bayer", "Christian Osendorfer."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Variational inference: A review for statisticians",
    "authors": ["David M. Blei", "Alp Kucukelbir", "Jon D. McAuliffe."],
    "venue": "Journal of the American Statistical Association 112(518):859–877.",
    "year": 2017
  }, {
    "title": "Generating sentences from a continuous space",
    "authors": ["Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal Józefowicz", "Samy Bengio."],
    "venue": "CoNLL 2016. pages 10–21.",
    "year": 2016
  }, {
    "title": "A recurrent latent variable model for sequential data",
    "authors": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio."],
    "venue": "NIPS 28, pages 2980–2988.",
    "year": 2015
  }, {
    "title": "Sequential neural models with stochastic layers",
    "authors": ["Marco Fraccaro", "Søren Kaae Sø nderby", "Ulrich Paquet", "Ole Winther."],
    "venue": "NIPS 29, pages 2199– 2207.",
    "year": 2016
  }, {
    "title": "Uncertainty in Deep Learning",
    "authors": ["Yarin Gal."],
    "venue": "Ph.D. thesis, University of Cambridge.",
    "year": 2016
  }, {
    "title": "Convolutional sequence to sequence learning",
    "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin."],
    "venue": "ICML. pages 1243–1252.",
    "year": 2017
  }, {
    "title": "Z-forcing: Training stochastic recurrent networks",
    "authors": ["Anirudh Goyal", "Alessandro Sordoni", "Marc-Alexandre Côté", "Nan Ke", "Yoshua Bengio."],
    "venue": "NIPS 30, pages 6716–6726.",
    "year": 2017
  }, {
    "title": "Deep autoregressive networks",
    "authors": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra."],
    "venue": "ICML. Bejing, China, pages 1242–1250.",
    "year": 2014
  }, {
    "title": "Sockeye: A Toolkit for Neural Machine Translation",
    "authors": ["Felix Hieber", "Tobias Domhan", "Michael Denkowski", "David Vilar", "Artem Sokolov", "Ann Clifton", "Matt Post."],
    "venue": "ArXiv e-prints .",
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "ICLR.",
    "year": 2015
  }, {
    "title": "Improved variational inference with inverse autoregressive flow",
    "authors": ["Diederik P Kingma", "Tim Salimans", "Rafal Jozefowicz", "Xi Chen", "Ilya Sutskever", "Max Welling."],
    "venue": "NIPS 29, pages 4743–4751.",
    "year": 2016
  }, {
    "title": "Autoencoding variational Bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "ICLR.",
    "year": 2014
  }, {
    "title": "Language as a latent variable: Discrete generative models for sentence compression",
    "authors": ["Yishu Miao", "Phil Blunsom."],
    "venue": "EMNLP. pages 319–328.",
    "year": 2016
  }, {
    "title": "Neural variational inference for text processing",
    "authors": ["Yishu Miao", "Lei Yu", "Phil Blunsom."],
    "venue": "ICML. New York, New York, USA, pages 1727–1736.",
    "year": 2016
  }, {
    "title": "Handbook of Differential Entropy",
    "authors": ["Joseph Victor Michalowicz", "Jonathan M. Nichols", "Frank Bucholtz."],
    "venue": "CRC Press.",
    "year": 2014
  }, {
    "title": "BLEU: A method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "ACL. pages 311–318.",
    "year": 2002
  }, {
    "title": "Variational inference with normalizing flows",
    "authors": ["Danilo Rezende", "Shakir Mohamed."],
    "venue": "ICML. volume 37, pages 1530–1538.",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."],
    "venue": "ICML.",
    "year": 2014
  }, {
    "title": "Gradient estimation using stochastic computation graphs",
    "authors": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel."],
    "venue": "NIPS 28, pages 3528–3536.",
    "year": 2015
  }, {
    "title": "A hybrid convolutional variational autoencoder for text generation",
    "authors": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth."],
    "venue": "EMNLP. pages 627–637.",
    "year": 2017
  }, {
    "title": "Learning structured output representation using deep conditional generative models",
    "authors": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan."],
    "venue": "NIPS 28, pages 3483–3491.",
    "year": 2015
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research 15:1929–1958.",
    "year": 2014
  }, {
    "title": "Variational recurrent neural machine translation",
    "authors": ["Jinsong Su", "Shan Wu", "Deyi Xiong", "Yaojie Ly", "Xianpei Han", "Biao Zhang."],
    "venue": "AAAI .",
    "year": 2018
  }, {
    "title": "Sequence to sequence learningwith neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "NIPS 27, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Doubly stochastic Variational Bayes for nonconjugate inference",
    "authors": ["Michalis Titsias", "Miguel Lázaro-Gredilla."],
    "venue": "ICML. pages 1971–1979.",
    "year": 2014
  }, {
    "title": "Attention is all you need",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin."],
    "venue": "NIPS 30, pages 6000–6010.",
    "year": 2017
  }, {
    "title": "Latent intention dialogue models",
    "authors": ["Tsung-Hsien Wen", "Yishu Miao", "Phil Blunsom", "Steve Young."],
    "venue": "ICML. pages 3732–3741.",
    "year": 2017
  }, {
    "title": "Variational neural machine translation",
    "authors": ["Biao Zhang", "Deyi Xiong", "jinsong su", "Hong Duan", "Min Zhang"],
    "venue": "In EMNLP",
    "year": 2016
  }, {
    "title": "Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction",
    "authors": ["Chunting Zhou andGrahamNeubig."],
    "venue": "ACL. pages 310– 320.",
    "year": 2017
  }],
  "id": "SP:305514c2c04c089fc6f382cc987d881c7913b40d",
  "authors": [{
    "name": "Philip Schulz",
    "affiliations": []
  }],
  "abstractText": "The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an indepth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.",
  "title": "A Stochastic Decoder for Neural Machine Translation"
}