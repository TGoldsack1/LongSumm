{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 395–400 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n395"
  }, {
    "heading": "1 Introduction",
    "text": "Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015). This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012). Consider the following text from a Spanish news article:\n“Tormenta de nieve afecta a 100 millones de personas en EEUU. Unos 100 millones de personas enfrentaban el sábado nuevas dificultades tras la enorme tormenta de nieve de hace dı́as en la costa este de Estados Unidos.”\nThe mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent. A coreference model trained on English data is unlikely to coreference these two\nmentions in Spanish since these mentions did not appear in English data and a regular English style abbreviation of “Estados Unidos” will be “EU” instead of “EEUU”. But in the bilingual EnglishSpanish word embedding space, the word embedding of “EEUU” sits close to the word embedding of “US” and the sum of word embeddings of “Estados Unidos” sit close to the sum of word embeddings of “United States”. Therefore, a coreference model trained using English-Spanish bilingual word embeddings on English data has the potential to make the correct coreference decision between “EEUU” and “Estados Unidos” without ever encountering these mentions in training data.\nThe contributions of this paper are two-fold. Firstly, we propose an entity-centric neural crosslingual coreference model. This model, when trained on English and tested on Chinese and Spanish from the TAC 2015 Trilingual Entity Discovery and Linking (EDL) Task (Ji et al., 2015), achieves competitive results to models trained directly on Chinese and Spanish respectively. Secondly, a pipeline consisting of this coreference model and an Entity Linking (henceforth EL) model can achieve superior linking accuracy than the official top ranking system in 2015 on Chinese and Spanish test sets, without using any supervision in Chinese or Spanish.\nAlthough most of the active coreference research is on solving the problem of noun phrase coreference resolution in the Ontonotes data set, invigorated by the 2011 and 2012 CoNLL shared task (Pradhan et al., 2011, 2012), there are many important applications/end tasks where the mentions of interest are not noun phrases. Consider the sentence,\n“(U.S. president Barack Obama who started ((his) political career) in (Illinois)), was born in (Hawaii).”\nThe bracketing represents the Ontonotes style\nnoun phrases and underlines represent the phrases that should be linked to Wikipedia by an EL system. Note that mentions like “U.S.” and “Barack Obama” do not align with any noun phrase. Therefore, in this work, we focus on coreference on mentions that arise in our end task of entity linking and conduct experiments on TAC TriLingual 2015 data sets consisting of English, Chinese and Spanish."
  }, {
    "heading": "2 Coreference Model",
    "text": "Each mention has a mention type (m type) of either name or nominal and an entity type (e type) of Person (PER) / Location (LOC) / GPE / facility (FAC) / organization (ORG) (following standard TAC (Ji et al., 2015) notations).\nThe objective of our model is to compute a function that can decide whether two partially constructed entities should be coreferenced or not. We gradually merge the mentions in the given document to form entities. Mentions are considered in the order of names and then nominals and within each group, mentions are arranged in the order they appear in the document. Suppose, the sorted order of mentions are m1, . . ., mN1 , mN1+1, . . . , mN1+N2 where N1 and N2 are respectively the number of the named and nominal mentions. A singleton entity is created from each mention. Let the order of entities be e1, . . . , eN1 , eN1+1, . . . , eN1+N2 . We merge the named entities with other named entities, then nominal entities with named entities in the same sentence and finally we merge nominal entities across sentences as follows: Step 1: For each named entity ei (1 ≤ i ≤ N1), antecedents are all entities ej (1 ≤ j ≤ i − 1) such that ej and ei have same e type. Training examples are triplets of the form (ei, ej , yij). If ei and ej are coreferent (meaning, yij=1), they are merged. Step 2: For each nominal entity ei (N1 + 1 ≤ i ≤ N1 + N2), we consider antecedents ej such that ei and ej have the same e type and ej has some mention that appears in the same sentence as some mention in ei. Training examples are generated and entities are merged as in the previous step. Step 3: This is similar to previous step, except ei and ej have no sentence restriction. Features: For each training triplet (e1, e2, y12), the network takes the entity pair (e1, e2) as input and tries to predict y12 as output. Since each entity\nrepresents a set of mentions, the entity-pair embedding is obtained from the embeddings of mention pairs generated from the cross product of the entity pair. Let M(e1, e2) be the set {(mi,mj) | (mi,mj)∈ e1 × e2} . For each (mi,mj) ∈ M(e1, e2), a feature vector φmi,mj is computed. Then, every feature in φmi,mj is embedded as a vector in the real space. Let vmi,mj dentote the concatenation of embeddings of all features in φmi,mj . Embeddings of all features except the words are learned in the training process. Word embeddings are pre-trained. vmi,mj includes the following language independent features: String match: whether mi is a substring or exact match of mj and vice versa (e.g. mi = “Barack Obama” and mj = “Obama”) Distance: word distance and sentence distance between mi and mj discretized into bins m type: concatenation of m types for mi and mj e type: concatenation of e types for mi and mj Acronym: whether mi is an acronym of mj or vice versa (e.g. mi = “United States” and mj = “US”) First name mismatch: whether mi and mj belong to e type of PERSON with the same last name but different first name (e.g. mi=“Barack Obama” and mj = “Michelle Obama”) Speaker detection: whether mi and mj both occur in the context of words indicating speech e.g. “say”, “said” In addition, vmi,mj includes the average of the word embeddings of mi and average of the word embeddings of mj ."
  }, {
    "heading": "2.1 Network Architecture",
    "text": "The network architecture from the input to the output is shown in figure 1. Embedding Layer: For each training triplet (e1, e2, y), a sequence of vectors vmi,mj (for each ((mi,mj) ∈ M(e1, e2))) is given as input to the network. Relu Layer: vrmi,mj = max(0,W\n(1)vmi,mj ) Attention Layer: To generate the entity-pair embedding, we need to combine the embeddings of mention pairs generated from the entity-pair. Consider two entities e1 = (President1, Obama)} and e2 = {(President2, Clinton)}. Here the superscripts are used to indicate two different mentions with the same surface form. Since the named mention pair (Obama, Clinton) has no string overlap, e1 and e2 should not be coreferenced even though the\nnominal mention pair (President1, President2) has full string overlap. So, while combining the embeddings for the mention pairs, mention pairs with m type (name, name) should get higher weight than mention pairs with m type (nominal, nominal). The entity pair embedding is the weighted sum of the mention-pair embeddings. We introduce 4 parameters aname,name, aname,nominal, anominal,nominal and anominal,name as weights for mention pair embeddings with m types of (name, name), (name, nominal), (nominal, nominal) and (nominal, name) respectively. The entity pair embedding is computed as follows:\nvae1,e2 =∑ (mi,mj)∈M(e1,e2) am type(mi),m type(mj) N vrmi,mj\nHere N is a normalizing constant given by:\nN = √ ∑ (mi,mj)∈M(e1,e2) a2m type(mi),m type(mj)\nThis layer represents attention over the mention pair embeddings where attention weights are based on the m types of the mention pairs. Sigmoid Layer: vse1,e2 = σ(W\n(2)vae1,e2) Output Layer:\nP (y12 = 1|e1, e2) = 1\n1 + e−w s.vse1,e2\nThe training objective is to maximize L. L = ∏ d∈D ∏ (e1,e2,y12)∈Sd P (y12|e1, e2;W (1),W (2), a, ws) (1) Here D is the corpus and Sd is the training triplets generated from document d.\nDecoding proceeds similarly to training algorithm, except at each of the three steps, for each entity ei, the highest scoring antecdent ej is selected and if the score is above a threshold, ei and ej are merged."
  }, {
    "heading": "3 A Zero-shot Entity Linking model",
    "text": "We use our recently proposed cross-lingual EL model, described in (Sil et al., 2018), where our target is to perform “zero shot learning” (Socher et al., 2013; Palatucci et al., 2009). We train an EL model on English and use it to decode on any other language, provided that we have access to multi-lingual embeddings from English and the target language. We briefly describe our techniques here and direct the interested readers to the paper. The EL model computes several similarity/coherence scores S in a “feature abstraction layer” which computes several measures of similarity between the context of the mention m in the query document and the context of the candidate link’s Wikipedia page which are fed to a\nfeed-forward neural layer which acts as a binary classifier to predict the correct link for m. Specifically, the feature abstraction layer computes cosine similarities (Sil and Florian, 2016) between the representations of the source query document and the target Wikipedia pages over various granularities. These representations are computed by performing CNNs and LSTMs over the context of the entities. Then these similarities are fed into a Multi-perspective Binning layer which maps each similarity into a higher dimensional vector. We also train fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks.\nThe model achieves state-of-the-art (SOTA) results on English benchmark EL datasets and also performs surprisingly well on Spanish and Chinese. However, although the EL model is “zeroshot”, the within-document coreference resolution in the system is a language-dependent SOTA coreference system that has won multiple TACKBP (Ji et al., 2015; Sil et al., 2015) evaluations but is trained on the target language. Hence, our aim is to apply our proposed coreference model to the EL system to perform an extrinsic evaluation of our proposed algorithm."
  }, {
    "heading": "4 Experiments",
    "text": "We evaluate cross-lingual transfer of coreference models on the TAC 2015 Tri-Lingual EL datasets. It contains mentions annotated with their grounded Freebase 1 links (if such links exist) or corpus-wide clustering information for 3 languages: English (henceforth, En), Chinese (henceforth, Zh) and Spanish (henceforth, Es). Table 1 shows the size of the training and test sets for the three languages. The documents come from two genres of newswire and discussion forums. The mentions in this dataset are either named entities or nominals that belong to five types: PER, ORG, GPE, LOC and FAC. Hyperparameters: Every feature is embedded in a 50 dimensional space except the words which reside in a 300 dimensional space. The Relu and Sigmoid layers have 100 and 500 neurons respectively. We use SGD for optimization with an initial learning rate of 0.05 which is linearly reduced to\n1TAC uses BaseKB, which is a snapshot of Freebase. SIL18 links entities to Wikipedia and in-turn links them to BaseKB.\n0.0001. Our mini batch size is 32 and we train for 50 epochs and keep the best model based on dev set. Coreference Results: For each language, we follow the official train-test splits made in the TAC 2015 competition. Except, a small portion of the training set is held out as development set for tuning the models. All experimental results on all languages reported in this paper were obtained on the official test sets. We used the official CoNLL 2012 evaluation script and report MUC, B3 and CEAF scores and their average (CONLL score). See Pradhan et al. (2011, 2012).\nTo test the competitiveness of our model with other SOTA models, we train the publicly available system of Clark and Manning (2016) (henceforth, C&M16) on the TAC 15 En training set and test on the TAC 15 En test set. The C&M16 system normally outputs both noun phrase mentions and their coreference and is trained on Ontonotes. To ensure a fair comparison, we changed the configuration of the system to accept gold mention boundaries both during training and testing. Since the system was unable to deal with partially overlapping mentions, we excluded such mentions in the evaluation. Table 2 shows that our model outperforms C&M16 by 8 points.\nFor cross-lingual experiments, we build monolingual embeddings for En, Zh and Es using the widely used CBOW word2vec model (Mikolov et al., 2013a). Recently Canonical Correlation Analysis (CCA) (Faruqui and Dyer, 2014), MultiCCA (Ammar et al., 2016) and Weighted Regression (Mikolov et al., 2013b) have been proposed for building the multi-lingual embedding space from monolingual embedding. In our prelimi-\nnary experiments, the technique of Mikolov et al. (2013b) performed the best and so we used it to project the embeddings of Zh and Es onto En.\nIn Table 3, “En Model” refers to the model that was trained on the En training set of TAC 15 using multi-lingual embeddings and tested on the Es and Zh testing set of TAC 15. “Es Model” refers to the model trained on Es training set of TAC 15 using Es embeddings. “Zh Model” refers to the model trained on the Zh training set of TAC 15 using Zh embeddings. The En model performs 0.5 point below the Es model on the Es test set. On the Zh test set, the En model performs only 0.3 point below the Zh model. Hence, we show that without using any target language training data, the En model with multi-lingual embeddings gives comparable results to models trained on the target language. EL Results: We replace the in-document coreference system (trained on the target language) of SIL18 with our En model to investigate the performance of our proposed algorithm on an extrinsic task. Table 4 shows the EL results on Es and Zh test sets respectively. “EL - Coref” refers to the case where the first step of coreference is not used and EL is used to link the mentions directly to Freebase. “EL + En Coref” refers to the case where the neural english coreference model is first used on Zh or Es data followed by the EL model. The former is 3 points below the latter on Es and 2.6 points below Zh, implying coreference is a vital task for EL. Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively. Finally, we show the SOTA results on these two data sets recently reported by SIL18. Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as\nthe test set .Without using any in-language training data, our results are competitive to their results (1.2% below on Es and 0.5% below on Zh)."
  }, {
    "heading": "5 Related Work",
    "text": "Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Björkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules. Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding. However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language. In contrast, our model performs cross-lingual coreference. There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016). In this work, we show that such promise exists for coreference also.\nThe tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013). However, the recent SOTA was obtained using pipeline models of coreference and EL (Sil et al., 2018). Compared to a joint model, pipeline models are easier to implement, improve and adapt to a new domain."
  }, {
    "heading": "6 Conclusion",
    "text": "The proposed cross-lingual coreference model was found to be empirically strong in both intrinsic and extrinsic evaluations in the context of an entity linking task."
  }],
  "year": 2018,
  "references": [{
    "title": "Massively multilingual word embeddings",
    "authors": ["Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith."],
    "venue": "arXiv preprint arXiv:1602.01925.",
    "year": 2016
  }, {
    "title": "Understanding the value of features for coreference resolution",
    "authors": ["Eric Bengtson", "Dan Roth."],
    "venue": "EMNLP.",
    "year": 2008
  }, {
    "title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features",
    "authors": ["Anders Björkelund", "Jonas Kuhn."],
    "venue": "ACL.",
    "year": 2014
  }, {
    "title": "Entitycentric coreference resolution with model stacking",
    "authors": ["Kevin Clark", "Christopher D Manning."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Improving coreference resolution by learning entitylevel distributed representations",
    "authors": ["Kevin Clark", "Christopher D Manning."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Decentralized entity-level modeling for coreference resolution",
    "authors": ["Greg Durrett", "David Leo Wright Hall", "Dan Klein."],
    "venue": "ACL.",
    "year": 2013
  }, {
    "title": "A joint model for entity analysis: Coreference, typing, and linking",
    "authors": ["Greg Durrett", "Dan Klein."],
    "venue": "Transactions of the Association for Computational Linguistics, 2.",
    "year": 2014
  }, {
    "title": "Improving vector space word representations using multilingual correlation",
    "authors": ["Manaal Faruqui", "Chris Dyer."],
    "venue": "EACL.",
    "year": 2014
  }, {
    "title": "Latent structure perceptron with feature induction for unrestricted coreference resolution",
    "authors": ["Eraldo Rezende Fernandes", "Cı́cero Nogueira Dos Santos", "Ruy Luiz Milidiú"],
    "year": 2012
  }, {
    "title": "Joint coreference resolution and named-entity linking with multi-pass sieves",
    "authors": ["Hannaneh Hajishirzi", "Leila Zilles", "Daniel S Weld", "Luke Zettlemoyer."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Overview of tac-kbp2015 tri-lingual entity discovery and linking",
    "authors": ["Heng Ji", "Joel Nothman", "Ben Hachey", "Radu Florian."],
    "venue": "TAC.",
    "year": 2015
  }, {
    "title": "Overview of tac-kbp2014 entity discovery and linking tasks. In TAC",
    "authors": ["Heng Ji", "Joel Nothman", "Ben Hachey"],
    "year": 2014
  }, {
    "title": "End-to-end neural coreference resolution",
    "authors": ["Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer."],
    "venue": "arXiv preprint arXiv:1707.07045.",
    "year": 2017
  }, {
    "title": "Latent structures for coreference resolution",
    "authors": ["Sebastian Martschat", "Michael Strube."],
    "venue": "Transactions of the Association for Computational Linguistics, 3:405–418.",
    "year": 2015
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1309.4168.",
    "year": 2013
  }, {
    "title": "Improving machine learning approaches to coreference resolution",
    "authors": ["Vincent Ng", "Claire Cardie."],
    "venue": "ACL.",
    "year": 2002
  }, {
    "title": "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
    "authors": ["Jian Ni", "Georgiana Dinu", "Radu Florian."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Zero-shot learning with semantic output codes",
    "authors": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell."],
    "venue": "NIPS.",
    "year": 2009
  }, {
    "title": "Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
    "authors": ["Sameer Pradhan", "Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang."],
    "venue": "EMNLPCoNLL.",
    "year": 2012
  }, {
    "title": "Conll-2011 shared task: Modeling unrestricted coreference in ontonotes",
    "authors": ["Sameer Pradhan", "Lance Ramshaw", "Mitchell Marcus", "Martha Palmer", "Ralph Weischedel", "Nianwen Xue."],
    "venue": "CoNLL.",
    "year": 2011
  }, {
    "title": "A multipass sieve for coreference resolution",
    "authors": ["Karthik Raghunathan", "Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher Manning."],
    "venue": "EMNLP.",
    "year": 2010
  }, {
    "title": "Supervised models for coreference resolution",
    "authors": ["Altaf Rahman", "Vincent Ng."],
    "venue": "EMNLP.",
    "year": 2009
  }, {
    "title": "The ibm systems for trilingual entity discovery and linking at tac 2015",
    "authors": ["Avirup Sil", "Georgiana Dinu", "Radu Florian."],
    "venue": "TAC.",
    "year": 2015
  }, {
    "title": "One for all: Towards language independent named entity linking",
    "authors": ["Avirup Sil", "Radu Florian."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Neural cross-lingual entity linking",
    "authors": ["Avirup Sil", "Gourab Kundu", "Radu Florian", "Wael Hamza."],
    "venue": "AAAI.",
    "year": 2018
  }, {
    "title": "Zero-shot learning through cross-modal transfer",
    "authors": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng."],
    "venue": "NIPS.",
    "year": 2013
  }, {
    "title": "Cross-lingual wikification using multilingual embeddings",
    "authors": ["Chen-Tse Tsai", "Dan Roth."],
    "venue": "HLTNAACL.",
    "year": 2016
  }, {
    "title": "Cross-lingual models of word embeddings: An empirical comparison",
    "authors": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Learning global features for coreference resolution",
    "authors": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber."],
    "venue": "NAACL.",
    "year": 2016
  }, {
    "title": "Learning anaphoricity and antecedent ranking features for coreference resolution",
    "authors": ["Sam Joshua Wiseman", "Alexander Matthew Rush", "Stuart Merrill Shieber", "Jason Weston."],
    "venue": "ACL.",
    "year": 2015
  }],
  "id": "SP:92bb675be982e0d1f8f392cb8671a79aa5baf31b",
  "authors": [{
    "name": "Gourab Kundu",
    "affiliations": []
  }, {
    "name": "Wael Hamza",
    "affiliations": []
  }],
  "abstractText": "We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and languageindependent features. We perform both intrinsic and extrinsic evaluations of our model. In the intrinsic evaluation, we show that our model, when trained on English and tested on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Spanish respectively. In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish.",
  "title": "Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking"
}