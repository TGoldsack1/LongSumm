{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368‚Äì2378 Copenhagen, Denmark, September 7‚Äì11, 2017. c¬©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios.\nFor example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question‚Äìanswer pairs. When the system generates a candidate semantic parse during training, the quality needs to be indirectly measured by\ncomparing the derived answers from the knowledge base and the provided labeled answers.\nThis setting of implicit supervision increases the difficulty of learning a neural model, not only because the signals are vague and noisy, but also delayed. For instance, among different semantic parses that result in the same answers, typically only few of them correctly represent the meaning of the question. Moreover, the correctness of answers corresponding to a parse can only be evaluated through an external oracle (e.g., executing the query on the knowledge base) after the parse is fully constructed. Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present.\nIn this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals. By casting structured-output learning as a search problem, the key insight in MMRN is the\n1Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available.\n2368\nspecial mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of the current action. On the other hand, the implicit supervision can be viewed as a source of delayed rewards, where the reward of the actions can only be revealed later. We unify these two types of reward signals by using a maximum margin update, inspired by structured SVM (Joachims et al., 2009).\nThe effectiveness of MMRN is demonstrated on three NLP tasks: named entity recognition, entity linking and semantic parsing. MMRN outperforms the current best results on CoNLL-2003 named entity recognition dataset (Tjong Kim Sang and De Meulder, 2003), reaching 91.4% F1, in the close setting where no gazetteer is allowed. It also performs comparably to the existing state-of-theart systems on entity linking. Models for these two tasks are trained using explicit supervision. For semantic parsing, where only implicit supervision signals are provided, MMRN is able to learn from delayed rewards, improving the entity linking component and the overall semantic parsing framework jointly, and outperforms the best published system by 1.4% absolute on the WebQSP dataset (Yih et al., 2016).\nIn the rest of the paper, we survey the most related work in Sec. 2 and give an in-depth discussion on comparing MMRN and other learning frameworks in Sec. 7. We start the description of our method from the search formulation and the state‚Äìaction spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper."
  }, {
    "heading": "2 Related Work",
    "text": "Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP). Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009). Later, the learning to search framework is pro-\nposed (DaumeÃÅ and Marcu, 2005; DaumeÃÅ et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015).\nLatent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quattoni et al., 2007). The main difference compared to our framework is the existence of the external environment when learning from implicit signals.\nUpadhyay et al. (2016) first proposed the idea of learning from implicit supervision, and is the most related paper to our work. Compared to their linear algorithm, our framework is more principled and general as we integrate the concept of margin in our method. Furthermore, we also extend the framework using neural models."
  }, {
    "heading": "3 Search-based Inference",
    "text": "In our framework, predicting the best structured output, inference, is formulated as a state/action search problem. Our search space can be described as follows. The initial state, s0, is the starting point of the search process. We define Œ≥(s) as the set of all feasible actions that can be taken at s, and denote s‚Ä≤ = œÑ(s, a) as the transition function, where s‚Ä≤ is the new state after taking action a from s. A path h is a sequence of state‚Äìaction pairs, starting with the initial state: h = {(s0, a0), . . . , (sk, ak)}, where si = œÑ(si‚àí1, ai‚àí1), ‚àÄi = 1, . . . , k. We denote h ; sÃÇ, if sÃÇ = œÑ(sk, ak), the final state which the path h leads to. A path essentially is a partial or complete structured prediction. For each input x, we define H(x) to be the set of all possible paths for the input. We also define E(x) = {h | h ‚àà H(x),h ; sÃÇ, Œ≥(sÃÇ) = ‚àÖ}, which is all possible paths that lead to terminal states.\nGiven a state s and an action a, the scoring function fŒ∏(s, a) measures the quality of an immediate action with respect to the current state, where Œ∏ is the model parameters. The score of a path h is defined as the sum of the scores for state-action pairs in h: fŒ∏(h) = ‚àëk i=0 fŒ∏(si, ai). During test time, inference is to find the best path in E(x): arg maxh‚ààE(x) fŒ∏(h;x). In practice, inference is often approximated by beam search when no efficient algorithm exists.\nIn the remaining of this section, we describe the states and actions in the targeted tasks in this work: named entity recognition, entity linking and semantic parsing. The the model and learning algorithm will be discussed in Sec. 4 and Sec. 5."
  }, {
    "heading": "3.1 Named entity recognition",
    "text": "The task of named entity recognition (NER) is to identify entity mentions in a sentence, as well as to assign their types, such as Person or Location. Following the conventional setting, we treat it as a sequence labeling problem using the standard BIOES encoding. For instance, a ‚ÄúB-LOC‚Äù tag on a word means that the word is the beginning of a multi-word location entity.\nGiven a sentence as input, the states represent the tags assigned to the words. Starting from the initial state, s0, where no tag has been assigned, the search process explores the sequence tagging from the left-to-right order. For each word, the actions are the legitimate tags that can be assigned to it, which depend on previous actions. For example, if the ‚ÄúS-PER‚Äù tag (‚ÄúS‚Äù means a single word entity) has been assigned to the previous word, then an action of labeling the current word with either ‚ÄúI-PER‚Äù or ‚ÄúE-PER‚Äù cannot can be taken. The search reaches a terminal state when all words in the sentence have been tagged."
  }, {
    "heading": "3.2 Entity linking",
    "text": "The problem of entity linking (EL) is similar to NER, but instead of tagging the mention using a small set of generic entity types, the goal here is to ground the mention to a specific entity, stored in a knowledge base or described by a Wikipedia page. For example, consider the sentence ‚Äúnfl news: draft results for giants‚Äù and assume that the mention candidates ‚Äúnfl‚Äù and ‚Äúgiants‚Äù are given. A state reflects how we have assigned the entity labels to these candidates. Following the same leftto-right order and starting from the empty assignment s0, the first action to take is to assign the entity label to the first candidate ‚Äúnfl‚Äù. A legitimate action set can be all the entities that have been associated with this mention in the training set (e.g., ‚ÄúNational Football League‚Äù or ‚ÄúNational Fertilizers Limited‚Äù). Once the action is completed, the transition function will bring the focus to the next mention candidate (i.e., ‚Äúgiants‚Äù). The search reaches a terminal state when all the candidate mentions in the sentence have been linked."
  }, {
    "heading": "3.3 Semantic parsing",
    "text": "Our third targeted task is semantic parsing (SP), which is a task of mapping a text utterance to a formal meaning representation. In this paper, we focus on a specific type of semantic parsing problem that maps a natural language question to a structured query, which is executed on a knowledge base to retrieve the answer to the original question.\nFigure 2 shows the semantic parses of an example question ‚Äúwho played meg in season 1 of family guy‚Äù, assuming the knowledge base is Freebase (Bollacker et al., 2008). An entity linking component plays an important role by mapping ‚Äúmeg‚Äù to MegGriffin and ‚Äúseason 1 of family guy‚Äù to FamilyGuySeason1. Predicates like cast, actor and character are also from the knowledge base that define the relationships between these entities and the answer. Together the semantic parse in Œª-calculus is shown in the top of Figure 2. Equivalently, the semantic parse can be represented as a query graph (Figure 2 bottom), which is used in the STAGG system (Yih et al., 2015). The nodes are either grounded entities or variables, where x is the answer entity. The edges denote the relationship between two entities.\nRegardless of the choice of the formal language, the process of constructing the semantic parse is typically formulated as a search problem. A state is essentially a partial or complete semantic parse, and an action is to extend the current semantic parse by adding a new relation or constraint.\nDifferent from previous systems which treat entity linking as a static component, our search space consists of the search space of both entity linking and semantic parsing. That is, the search space is the union of the search space of entity linking described in Section 3.2 and the search space of the semantic parses, which we describe below. Integrating search spaces allows the model to use implicit signals to update both the semantic parsing\nand the entity linking systems. To the best of our knowledge, this is the first work that jointly learns the entity linking and semantic parsing systems.\nOur search space is defined as follows. Starting from the initial state s0, the model first explores the entity linking search space. Once the entity linking assignment are assigned (e.g. FamilyGuySeason1 in Figure 2.) The second phase is then to determine the main relationship between the topic entity and the answer (e.g., the cast-actor chain between FamilyGuySeason1 and x). Constraints (e.g., the character is MegGriffin) that describe the additional properties that the answer needs to have are added last. In this case, any state that is a legitimate semantic parse (consisting of one topic entity and one main relationship, as well as zero or more constraints) can lead to a terminal state."
  }, {
    "heading": "4 Maximum Margin Reward Networks",
    "text": "In this section, we introduce the learning framework of MMRN, which includes two main components: reward and max-margin loss. The former is a mechanism for using implicit and explicit supervision signals in a unified way; the latter formally defines the learning objective."
  }, {
    "heading": "4.1 Reward",
    "text": "The key insight of MMRN is that different types of supervision signals can be represented using the appropriate design of the reward function. A reward function is defined over a state‚Äìaction pair R(s, a), representing the true quality of taking action a in the state s. The reward for a path can be formally defined as: R(h) = ‚àëk i=0R(si, ai). Intuitively, when the annotated action sequences (explicit supervision signals) exist, the model only needs to learn to imitate the annotated sequence. For instance, when learning NER in the fully supervised setting, the equivalent way of using Hamming distance is to define the reward R(s, a) to be 1 if a matches the annotated sequence at the current state, and 0 otherwise.\nIn the setting where only implicit supervision is available, the reward function can still be designed to capture the signals. For instance, when only the question‚Äìanswer pairs exist for learning the semantic parser, the reward can be defined by comparing the answers derived from a candidate parse and the labeled answers. More formally, assume that s = œÑ(s‚Ä≤, a) is the state after applying\naction a to state s‚Ä≤. Let Y (s) be the set of predicted answers generated from state s, and Y (s) = {} when s is not a legitimate semantic parse. The reward function R(s‚Ä≤, a) can be defined by comparing Y (s) and the labeled answers, A, to the input question. While a set similarity function like the Jaccard coefficient can be used as the reward function, we chose the F1 score in this work as it was used as the evaluation metric in previous work (Berant et al., 2013). Figure 3 shows an example of this reward function."
  }, {
    "heading": "4.2 Max-Margin Loss & Learning Algorithm",
    "text": "The MMRN learning algorithm can be viewed as an extension of M3N (Taskar et al., 2004) and Structured SVM (Joachims et al., 2009; Yu and Joachims, 2009). The learning algorithm takes three steps, where the first two involve two different search procedures. The final step is to update the models with respect to the inference results.\nFinding the best path The first search step is to find the best path h‚àó by solving the following optimization problem:\nh‚àó = arg max h‚ààE(x) R(h; y) + fŒ∏(h). (1)\nThe first term defines the path that has the highest reward. Because it is possible that several paths share the same reward, the second term leverages the current model and serves as the tie-breaker, where is a hyper-parameter that is set to a small positive number in our experiments.\nWhen explicit supervision is available, solving Eq. (1) is trivial ‚Äì the search simply returns the annotated sequence. In the case of implicit supervision, where true rewards are only revealed for complete action sequences, the search problem becomes difficult as the rewards of early state‚Äìaction\npairs are zeros. In this situation, the search algorithm uses the model score fŒ∏ to guide the search. One possible design is to use beam search for the optimization problem, where the search procedure follows the current model in the early stage (given thatR(h) = 0). After generating several complete action sequences, the true reward function is then used to find h‚àó. The tie-breaker also picks the best sequence when there are multiple sequences that lead to the same reward. Note that h‚àó can change between iterations because of the tie-breaker.\nFinding the most violated path Once h‚àó is found, it is used as our reference path. We would like to update the model so that the scoring function fŒ∏ will behave similarly to the reward R. More formally, we aim to update the model parameters Œ∏ to satisfy the following constraint.\nfŒ∏(h‚àó)‚àí fŒ∏(h) ‚â• R(h‚àó)‚àíR(h),‚àÄh.\nThe constraint implies that the ‚Äúbest‚Äù action sequence should rank higher than any other sequence by a margin computed from rewards as R(h‚àó) ‚àí R(h). The degree of violation of this constraint, with respect to h, is thus (R(h‚àó)‚àíR(h)) ‚àí (fŒ∏(h‚àó)‚àí fŒ∏(h)) = fŒ∏(h) ‚àí R(h)‚àí fŒ∏(h‚àó) +R(h‚àó). The max-margin loss is defined accordingly:\nL(h,h‚àó) = max(fŒ∏(h)‚àíR(h)‚àífŒ∏(h‚àó)+R(h‚àó), 0)\nL(h,h‚àó) is our optimization goal, where we want to update the model by fixing the biggest violation. Note that the associated constraint is only violated when L(h,h‚àó) is positive. To find the path h in this step that maximizes the violation is equivalent to maximizing fŒ∏(h) ‚àí R(h), given that the rest of the terms are constant with respect to h.\nWhen there exist only explicit supervision signals, our objective function reduces to the one for optimizing structured SVM without regularization. For implicit signals, we find h‚àó approximately before we optimize the margin loss. In this case, the search is not exact as the reward signals are delayed. Nevertheless, we found the margin loss worked well empirically, as it kept decreasing in general until being stable.\nAlgorithm 1 summarizes the learning procedure of MMRN. Search is used in both Line 2 and 3. In Line 4, the algorithm performs a gradient update to modify all the model parameters.\nAlgorithm 1 Maximum Margin Reward Networks 1: for a random labeled data (x, y) do 2: h‚àó ‚Üê arg max\nh‚ààE(x) R(h; y) + fŒ∏(h)\n3: hÃÇ‚Üê arg max h‚ààE(x) fŒ∏(h)‚àíR(h; y) 4: update Œ∏ by minimizing L(hÃÇ,h‚àó) 5: end for"
  }, {
    "heading": "4.3 Practical Considerations",
    "text": "Although the learning algorithm of MMRN is simple and general, the quality of the learned model is dictated by the effectiveness of the search procedure. Increasing the beam size generally helps improve the model, but also slows down the training, and has a limited effect when dealing with a large search space. Domain-specific heuristics for pruning search space should thus be used when available. For instance, in the task of semantic parsing, when the reward of a legitimate semantic parse is 0, it implies that none of the derived answers is included in the labeled set of answers. When all the possible follow-up actions can only make the semantic parse stricter (e.g., adding constraints), and result in a subset of the current derived answers, it is clear that the rewards of all these new states are 0 as well. Paths from this state can thus be pruned.\nAnother strategy for improving search quality is to use approximated reward in the early stage of search. Very often the true rewards at this stage are 0, and are not useful to guide the search to find the best path. The approximated reward function can be thought of as estimating whether there exists a high-reward state that is reachable from the current state. The effectiveness of this strategy has been demonstrated successfully by several recent efforts (Mnih et al., 2013; Krishnamurthy et al., 2015; Silver et al., 2016; Narasimhan et al., 2016)."
  }, {
    "heading": "5 Neural Architectures",
    "text": "While the learning algorithm of MMRN described in Sec. 4 is general, the exact model design is taskdependent. In this section, we describe in detail the neural network architectures of the three targeted tasks, named entity recognition, entity linking and semantic parsing."
  }, {
    "heading": "5.1 Named Entity Recognition",
    "text": "Recall that NER is formulated as a sequence labeling problem, and each action is to label a word with a tag using the BIOES encoding (cf. Sec. 3.1).\nThe model of the action scoring function fŒ∏(s, a) is depicted in Figure 4, which is basically the dot product of the action embedding and state embedding. The action embedding is initialized randomly for each action, but can be fine-tuned during training (i.e. back-propagate the error through the network and update the word/entity type embeddings). The state embedding is the concatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings proposed by Limsopatham and Collier (2016)."
  }, {
    "heading": "5.2 Entity Linking",
    "text": "An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings. All these embeddings, except the feature vectors, are fine-tuned during training.\nSome unique properties of our entity linking model are worth noticing. First, we add mention context embeddings from a bidirectional LSTM module as additional input. While using LSTMs is a common practice for sequence labeling, it is not usually used for short-text entity linking. For each mention, we only extract the output from the bi-LSTM module at the start and end tokens of the mention, and concatenate them as the mention context embeddings. Second, we construct entity embeddings using the average of its Freebase (Bollacker et al., 2008) type embeddings2,\n2We use only the 358 most frequent Freebase entity types.\nAvg.{‚Ä¶ Statistic features\nInput ùë•\nTwo hidden layers\n=\nAverage of entity type embeddings\nfùúÉ(ùë†, ùëé)\nState ùë† determines the mention index ùëö Action ùëé determines the entity index\nMention ùëö\nFigure 5: The action scoring model for EL.\ninitialized using pre-trained embeddings. Adding these two types of embeddings has shown to improve the performance in our experiments."
  }, {
    "heading": "5.3 Semantic Parsing",
    "text": "Our semantic parsing model follows the STAGG system (Yih et al., 2015), which uses a stagewise search procedure to expand the candidate semantic parses gradually (cf. Sec. 3.3). Compared to the original system, we make two notable changes. First, we use a two-layer feed-forward neural network to replace the original linear ranker that scores the candidate semantic parses. Second, instead of using a separately trained entity linking system, we incorporate our entity linking networks described in Sec. 5.2 as part of the semantic parsing model. The training process will thus fine tune the entity linking component to improve the semantic parsing system."
  }, {
    "heading": "6 Experiments",
    "text": "It is important to have a general machine learning model working for both implicit and explicit supervision signals. We valid our learning framework when the explicit supervision signals are presented, as well as demonstrate the support of the scenario where supervision signals are mixed.\nSpecifically, in this section, we report the experimental results of MMRN on named entity recognition and entity linking, both using explicit supervision, and on semantic parsing, using implicit supervision. In all our experiments, we tuned hyperparameters on the development set (each task respectively), and then re-trained the models on the combination of the training and development set."
  }, {
    "heading": "6.1 Named entity recognition",
    "text": "We use the CoNLL-2003 shared task data for the NER experiments, where the standard evaluation\nmetric is the F1 score. The pre-trained word embeddings are 100-dimension GloVe vectors trained on 6 billion tokens (Pennington et al., 2014)3. The search procedure is conducted using beam search, and the reward function is simply the number of correct tag assignments to the words.\nThe results are shown in Table 1, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN."
  }, {
    "heading": "6.2 Entity linking",
    "text": "For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang\n3Available at http://nlp.stanford.edu/projects/glove/ 4NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html\nand Chang, 2014; Yang and Chang, 2015; Yang et al., 2016). We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output. An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with the gold mention boundary. Interested readers can refer to (Carmel et al., 2014) for more detail.\nWe initialize the word embeddings from pretrained GloVe vectors trained on the twitter corpus, and type embeddings from the pre-trained skip-gram model (Mikolov et al., 2013)5. Sizes of both word embeddings are set to 200. Inference is done using a dynamic programming algorithm.\nResults of entity linking experiments are presented in Table 2, which are compared with those of S-MART (Yang and Chang, 2015)6 and NTEL (Yang et al., 2016)7, two state-of-the-art entity linking systems for short texts. Our MMRN-EL is comparable to the best system. We also conducted two ablation studies by removing the entity type vectors (MMRN-EL - Entity), and by removing the LSTM vectors (MMRN-EL - LSTM). Both show significant performance drops, which validates the importance of these two additional input vectors."
  }, {
    "heading": "6.3 Semantic parsing",
    "text": "For semantic parsing, we use the dataset WebQSP8 (Yih et al., 2016) in our experiments. This dataset is a clean and enhanced version of the widely used WebQuestions dataset (Berant et al., 2013), which consists of pairs of questions and answers found in Freebase. Compared to WebQuestions, WebQSP excludes questions with ambiguous intent, and provides verified answers and full semantic parses to the remaining 4,737 questions.\nWe follow the implicit supervision setting in (Yih et al., 2016), using 3, 098 question‚Äìanswer pairs for training, and 1, 639 for testing. A subset of 620 pairs from the training set is used for hyperparameter tuning. Because there can be multiple answers to a question, the quality of a semantic parser is measured using the averaged F1 score of the predicted answers.\n5Available at https://code.google.com/archive/p/word2vec/ 6The winning system of the NEEL challenge. 7To have a fair comparison, we compare to the results of\nNTEL which do not use pretrained user embedding. 8Available at http://aka.ms/WebQSP\nWe experiment with two configurations of incorporating the entity linking component. MMRNPIPELINE trains an MMRN-EL model using the entity linking labels in WebQSP separately. Given a question, the entities in it are first predicted, and used as input to the semantic parsing system. In contrast, MMRN-JOINT incorporates the MMRN-EL model in the whole framework. During this joint training process, 15 entity link results are sampled according to the current MMRN-EL model, and passed to the downstream networks. In both cases, we use the previous entity linking model trained on the NEEL dataset to initialize the parameters. As discussed in Sec. 4.1, in this implicit supervision setting, we directly set the (delayed) reward function to be the F1 score, which can be obtained by comparing the annotated answers with predicted answers.\nTable 3 summarizes the results of the MMRNbased semantic parsing systems and other strong baselines. The SP column reports the averaged F1 scores. Compared to the pipeline approach (MMRN-PIPELINE), the joint learning framework (MMRN-JOINT) improves significantly, reaching 68.1% F1. To compare different learning methods, we also apply REINFORCE (Williams, 1992), a popular policy gradient algorithm, to train our joint model using the same setting and reward function.9 MMRN-JOINT outperforms REINFORCE and its variant, REINFORECE+, which re-normalizes the probabilities of the sampled candidate sequences. Its result is also better than the state-of-the-art STAGG system. Note that we use the same architectures and initialization procedures for MMRN-PIPELINE/JOINT and REINFORCE/REINFORCE+. Therefore, the superior performance of MMRN-JOINT shows that the joint learning plays a crucial role in addition to the choices of architecture. Comparing to STAGG, note that Yih et al. (2016) did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predictions of their entity linking system for re-ranking parses. Our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance.\nOur joint model also improves the entity linking prediction on the questions in WebQSP using the implicit signals (the EL columns in Ta-\n9The REINFORCE algorithm uses warm initialization‚Äî the entity linking parameters are initialized using the model trained on the NEEL dataset.\nble 3). The F1 score of MMRN-JOINT on entity linking is 2.4 points higher than the baseline MMRNPIPELINE. Note that the entity linking results of MMRN-PIPELINE (line 1) are exactly the results of the entity linking component MMRN-EL. The result is also better than REINFORCE, and comparable to REINFORCE+.\nRecently Liang et al. (2016) proposed Neural Symbolic Machine (NSM) and reported the best result of 69.0 F1 score on the WebQSP dataset using the weak supervision settings.10 The NSM architecture for semantic parsing is significantly different from the architecture used in (Yih et al., 2016) and the one used in this paper. In contrast, MMRN is a general learning framework that allows joint training on existing models (i.e. entity linking and semantic parsing modules). This allows MMRN to use the labels of semantic parsing task as implicit supervision signals for the entity linking module. It would be interesting to apply MMRN on the newly proposed architectures as well."
  }, {
    "heading": "7 Discussion",
    "text": "We discuss several issues that are highly related to MMRN in this section.\nLearning to Search There are two main differences between MMRN and search-based algorithms, such as SEARN (DaumeÃÅ et al., 2009) and DAGGER (Ross et al., 2011). First, both SEARN and DAGGER focus on imitation learning, assuming explicit supervision signals exist. They use a two-step model learning approach:\n10The paper is published after the submission of this paper.\n(1) create cost-sensitive examples by listing state‚Äì action pairs and their corresponding (estimated) losses; (2) apply cost-aware training algorithms. In contrast, MMRN directly updates the parameters using back-propagation based on search results of each example. Second, SEARN mixes the optimal and current policies during learning, while MMRN performs search twice and simply pushes the current policy towards the optimal one. Recently, Chang et al. (2015) extend this line of work and discuss different roll-in and roll-out strategies during training for structured contextual bandit settings. As MMRN uses two search procedures, there is no need to mix different search policies.\nReinforcement Learning In many reinforcement learning scenarios, the search space is not fully controllable by the agent. For example, a chess playing agent cannot control the move made by its opponent, and has to commit a single move and wait for the opponent. Note that the agent can still think ahead and build a search tree, but only one move can be made in the end. In contrast, in scenarios like semantic parsing, the whole search space is controlled by the agent itself. Therefore, from the initial state, we can explore several search paths and get their real rewards. This may explain why MMRN can be more efficient than REINFORCE, as MMRN can use the reward signals of multiple paths more effectively. In addition, MMRN is not a probabilistic model, so it does not need to handle normalization issues, which often causes large variance in estimating the gradient direction when optimizing the expected reward.\nSemantic Parsing MMRN can be applied for many semantic parsing tasks. One key step is to design the right approximated reward for a given task to guide the beam search to nd the reference parses in MMRN, given that the actual reward is often very sparse. In our companion paper, (Iyyer et al., 2017), we used a simple form of approximated reward to get feedback as early as possible during search. In other words, the semantic parse will be executed as soon as the parse is executable (even if the parse is still not completed) during search. The execution results will be used to calculate the Jaccard coefficient with respect to the labeled answers as the approximated rewards. The use of approximated reward has been proven to be effective in (Iyyer et al., 2017).\nAn important research direction for semantic\nparsing is to reduce the supervision cost. In (Yih et al., 2016), the authors demonstrated that labeling semantic parses is possible and often more effective with a sophisticated labeling interface. However, collecting answers may still be easier or faster for certain problems or annotators. This suggests that we could allow the annotators to choose to label semantic parses or answers in order to minimize the supervision cost. MMRN would be an ideal learning algorithm for this scenario."
  }, {
    "heading": "8 Conclusion",
    "text": "This paper proposes Maximum Margin Reward Networks, a structured learning framework that can learn from both explicit and implicit supervision signals. In the future, we plan to apply Maximum Margin Reward Networks on other structured learning tasks. Improving MMRN for dealing with large search space is an important future direction as well."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their insightful comments. The first author is partly sponsored by DARPA under agreement number FA8750-13-2-0008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government."
  }],
  "year": 2017,
  "references": [{
    "title": "Weakly supervised learning of semantic parsers for mapping instructions to actions",
    "authors": ["Yoav Artzi", "Luke Zettlemoyer."],
    "venue": "TACL.",
    "year": 2013
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "ICDM.",
    "year": 2008
  }, {
    "title": "Making sense of microposts:(# microposts2014) named entity extraction & linking challenge",
    "authors": ["Amparo E Cano", "Giuseppe Rizzo", "Andrea Varga", "Matthew Rowe", "Milan Stankovic", "Aba-Sah Dadzie."],
    "venue": "CEUR Workshop.",
    "year": 2014
  }, {
    "title": "ERD‚Äô14: entity recognition and disambiguation challenge",
    "authors": ["David Carmel", "Ming-Wei Chang", "Evgeniy Gabrilovich", "Bo-June Paul Hsu", "Kuansan Wang."],
    "venue": "ACM SIGIR Forum.",
    "year": 2014
  }, {
    "title": "Learning to search better than your teacher",
    "authors": ["Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum√© III", "John Langford."],
    "venue": "ICML.",
    "year": 2015
  }, {
    "title": "Named entity recognition with bidirectional LSTM-CNNs",
    "authors": ["Jason PC Chiu", "Eric Nichols."],
    "venue": "arXiv preprint arXiv:1511.08308.",
    "year": 2015
  }, {
    "title": "Driving semantic parsing from the world‚Äôs response",
    "authors": ["James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth."],
    "venue": "CoNLL.",
    "year": 2010
  }, {
    "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "SIGDAT.",
    "year": 2002
  }, {
    "title": "Incremental parsing with the perceptron algorithm",
    "authors": ["Michael Collins", "Brian Roark."],
    "venue": "ACL.",
    "year": 2004
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "L√©on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "JMLR.",
    "year": 2011
  }, {
    "title": "Search-based structured prediction",
    "authors": ["Hal Daum√©", "John Langford", "Daniel Marcu."],
    "venue": "Machine learning.",
    "year": 2009
  }, {
    "title": "Learning as search optimization: approximate large margin methods for structured prediction",
    "authors": ["Hal Daum√©", "Daniel Marcu."],
    "venue": "ICML.",
    "year": 2005
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Transitionbased dependency parsing with stack long shortterm memory",
    "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Entity linking on microblogs with spatial and temporal signals",
    "authors": ["Yuan Fang", "Ming-Wei Chang."],
    "venue": "TACL.",
    "year": 2014
  }, {
    "title": "Microblog entity linking by leveraging extra posts",
    "authors": ["Yuhang Guo", "Bing Qin", "Ting Liu", "Sheng Li."],
    "venue": "EMNLP.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "J√ºrgen Schmidhuber."],
    "venue": "Neural computation.",
    "year": 1997
  }, {
    "title": "Bidirectional LSTM-CRF models for sequence tagging",
    "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."],
    "venue": "arXiv preprint arXiv:1508.01991.",
    "year": 2015
  }, {
    "title": "Search-based neural structured learning for sequential question answering",
    "authors": ["Mohit Iyyer", "Wen-tau Yih", "Ming-Wei Chang."],
    "venue": "ACL.",
    "year": 2017
  }, {
    "title": "Cutting-plane training of structural svms",
    "authors": ["Thorsten Joachims", "Thomas Finley", "ChunNam John Yu."],
    "venue": "Machine Learning.",
    "year": 2009
  }, {
    "title": "Learning to search better than your teacher",
    "authors": ["Akshay Krishnamurthy", "CMU EDU", "Hal Daum√© III", "UMD EDU."],
    "venue": "arXiv preprint arXiv:1502.02206.",
    "year": 2015
  }, {
    "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
    "authors": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."],
    "venue": "ICML.",
    "year": 2001
  }, {
    "title": "Neural architectures for named entity recognition",
    "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."],
    "venue": "arXiv preprint arXiv:1603.01360.",
    "year": 2016
  }, {
    "title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
    "authors": ["Chen Liang", "Jonathan Berant", "Quoc Le", "Kenneth D Forbus", "Ni Lao."],
    "venue": "arXiv preprint arXiv:1611.00020.",
    "year": 2016
  }, {
    "title": "Bidirectional LSTM for named entity recognition in twitter messages",
    "authors": ["Nut Limsopatham", "Nigel Collier."],
    "venue": "WNUT.",
    "year": 2016
  }, {
    "title": "Finding function in form: Compositional character models for open vocabulary word representation",
    "authors": ["Wang Ling", "Tiago Luƒ±ÃÅs", "Luƒ±ÃÅs Marujo", "Ram√≥n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"],
    "venue": "In EMNLP",
    "year": 2015
  }, {
    "title": "End-to-end sequence labeling via bi-directional LSTM-CNNSCRF",
    "authors": ["Xuezhe Ma", "Eduard Hovy."],
    "venue": "arXiv preprint arXiv:1603.01354.",
    "year": 2016
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "NIPS.",
    "year": 2013
  }, {
    "title": "Playing atari with deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."],
    "venue": "arXiv preprint arXiv:1312.5602.",
    "year": 2013
  }, {
    "title": "Improving information extraction by acquiring external evidence with reinforcement learning",
    "authors": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2014
  }, {
    "title": "Hidden conditional random fields",
    "authors": ["Ariadna Quattoni", "Sybor Wang", "Louis-Philippe Morency", "Morency Collins", "Trevor Darrell."],
    "venue": "PAMI.",
    "year": 2007
  }, {
    "title": "Design challenges and misconceptions in named entity recognition",
    "authors": ["Lev Ratinov", "Dan Roth."],
    "venue": "CoNLL.",
    "year": 2009
  }, {
    "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
    "authors": ["St√©phane Ross", "Geoffrey J Gordon", "Drew Bagnell."],
    "venue": "AISTATS.",
    "year": 2011
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"],
    "year": 2016
  }, {
    "title": "Max-margin markov networks",
    "authors": ["Ben Taskar", "Carlos Guestrin", "Daphne Koller."],
    "venue": "NIPS.",
    "year": 2004
  }, {
    "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
    "authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder."],
    "venue": "NAACL.",
    "year": 2003
  }, {
    "title": "Learning from explicit and implicit supervision jointly for algebra word problems",
    "authors": ["Shyam Upadhyay", "Ming-Wei Chang", "Kai-Wei Chang", "Wen-tau Yih."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "≈Åukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "NIPS.",
    "year": 2015
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J Williams."],
    "venue": "Machine learning.",
    "year": 1992
  }, {
    "title": "S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking",
    "authors": ["Yi Yang", "Ming-Wei Chang."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "Toward socially-infused information extraction: Embedding authors, mentions, and entities",
    "authors": ["Yi Yang", "Ming-Wei Chang", "Jacob Eisenstein."],
    "venue": "EMNLP.",
    "year": 2016
  }, {
    "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base",
    "authors": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."],
    "venue": "ACL.",
    "year": 2015
  }, {
    "title": "The value of semantic parse labeling for knowledge base question answering",
    "authors": ["Wen-tau Yih", "Matthew Richardson", "Christopher Meek", "Ming-Wei Chang", "Jina Suh."],
    "venue": "ACL.",
    "year": 2016
  }, {
    "title": "Learning structural SVMs with latent variables",
    "authors": ["Chun-Nam John Yu", "Thorsten Joachims."],
    "venue": "ICML.",
    "year": 2009
  }],
  "id": "SP:be0b922ec9625a5908032bde6ae47fa6c4216a38",
  "authors": [{
    "name": "Haoruo Peng",
    "affiliations": []
  }, {
    "name": "Ming-Wei Chang",
    "affiliations": []
  }, {
    "name": "Wen-tau Yih",
    "affiliations": []
  }, {
    "name": "Lacey Chabert",
    "affiliations": []
  }],
  "abstractText": "Neural networks have achieved state-ofthe-art performance on several structuredoutput prediction tasks, trained in a fully supervised fashion. However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of neural networks. In this work, we propose Maximum Margin Reward Networks, a neural networkbased framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure). On named entity recognition and semantic parsing, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.",
  "title": "Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision"
}