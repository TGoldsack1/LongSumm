{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 621–626 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n621"
  }, {
    "heading": "1 Introduction",
    "text": "Learning the representations of languages is a fundamental problem in natural language processing and most existing methods exploit the hypothesis that words occurring in similar contexts tend to have similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information. Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015).\nFor low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision. Artetxe et al. (2017) successfully learn bilingual word embeddings with\nonly a parallel vocabulary of aligned digits. Zhang et al. (2017) utilize adversarial training to obtain cross-lingual word embeddings without any parallel data. However, their performance is still significantly worse than supervised methods. By combining the merits of several previous works, Conneau et al. (2018) introduce a model that reaches and even outperforms supervised state-of-the-art methods with no parallel data.\nIn recent years, generative models have become more and more powerful. Both Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (VAEs) (Kingma and Welling, 2014) are prominent ones. In this work, we borrow the ideas from both GANs and VAEs to tackle the problem of bilingual lexicon induction. The basic idea is to learn latent variables that could capture semantic meaning of words, which would be helpful for bilingual lexicon induction. We also utilize adversarial training for our model and require no form of supervision. We evaluate our approach on several language pairs and experimental results demonstrate that our model could achieve promising performance. We further combine our model with several helpful techniques and show our model could perform competitively and even superiorly compared with several state-of-the-art methods."
  }, {
    "heading": "2 Related Work",
    "text": ""
  }, {
    "heading": "2.1 Bilingual Lexicon Induction",
    "text": "Extracting bilingual lexica has been studied by researchers for a long time. Mikolov et al. (2013) first observe there is isomorphic structure among word embeddings trained separately on monolingual corpora and they learn the linear transformation between languages. Zhang et al. (2016b) improve the method by constraining the transformation matrix to be orthogonal. Xing et al. (2015) incorporate length normalization during training and\nmaximize the cosine similarity instead. They point out that adding an orthogonality constraint can improve performance and has a closed-form solution, which was referred to as Procrustes approach in Smith et al. (2017). Canonical correlation analysis has also been used to map both languages to a shared vector space (Faruqui and Dyer, 2014; Lu et al., 2015).\nTo reduce the need for supervision signals, Artetxe et al. (2017) use identical digits and numbers to form an initial seed dictionary and then iteratively refine their results until convergence. Zhang et al. (2017) apply adversarial training to align monolingual word vector spaces with no supervision. Conneau et al. (2018) improve the model by combining adversarial training and Procrustes approach, and their unsupervised approach could reach and even outperform state-of-the-art supervised approaches. In this work, we make further improvements and enhance the model proposed in (Conneau et al., 2018) with latent variable model and iterative training procedure."
  }, {
    "heading": "2.2 Generative Models",
    "text": "VAEs (Kingma and Welling, 2014) represent one of the most successful deep generative models. Standard VAEs assume observed variables are generated from latent variables and the latent variables are sampled from a simple Gaussian distribution. Typically, VAEs utilize an neural inference model to approximate the intractable posterior, and optimize model parameters jointly with\na reparameterized variational lower bound. VAEs have been successfully applied in several natural language processing tasks before (Zhang et al., 2016a; Bowman et al., 2016).\nGANs (Goodfellow et al., 2014) are another framework for estimating generative models via an adversarial process and have attracted huge attention. The basic strategy is to train a generative model and a discriminative model simultaneously via an adversarial process. Adversarial training technique for matching distribution has proven to be powerful in a variety of tasks (Bowman et al., 2016). Adversarial Autoencoder (Makhzani et al., 2015) is a probabilistic autoencoder that uses the GANs to perform variational inference. By combining a VAE with a GAN, Larsen et al. (2016) use learned feature representations in the GAN discriminator as the basis for the VAE reconstruction objective. GANs have been applied in machine translation before (Yang et al., 2018; Lample et al., 2018)."
  }, {
    "heading": "3 Proposed Approach",
    "text": "In this section, we first briefly introduce VAEs, and then we illustrate the details and training techniques of our proposed model."
  }, {
    "heading": "3.1 Variational Autoencoder",
    "text": "Variational Autoencoders (VAEs) are deep generative model which are capable of learning complex density models for data via latent variables. Given a nonlinear generative model pθ(x|z) with input x ∈ RD and associated latent variable z ∈ RL drawn from a prior distribution p0(z), the goal of VAEs is to use a recognition model, qφ(z|x) to approximate the posterior distribution of the latent variables by maximizing the following variational lower bound\nLθ,φ = Eqφ(z|x)[log pθ(x|z)]−KL(qφ(z|x)||p0(z)), (1) where KL refers to Kullback-Leibler divergence."
  }, {
    "heading": "3.2 Our Model",
    "text": "Basically, our model assumes that the source word embedding {xn} and the target word embedding {yn} could be drawn from a same latent variable space {zn}, where {zn} is capable of capturing semantic meaning of words.\nIn contrast to the standard VAE prior that assumes each latent embedding zn to be drawn from the same latent Gaussian, our model just requires\nthe distribution of latent variables for source and target word embeddings to be equal. To achieve such a goal, we utilize adversarial training to guide the two latent distributions to match with each other.\nAs in adversarial training, we have networks φs and φt for both source and target space, striving to map words into the same latent space, while the discriminator D is a binary classifier which tries to distinguish between the two languages. We also have reconstruction networks θs and θt as in VAEs.\nThe objective function for the discriminator D could be formulated as\nLD = Ezy∼qφt (z|y)[logD(zy)]\n+ Ezx∼qφs (z|x)[log(1−D(zx))]. (2)\nFor the source side, the objective is to minimize\nLφs,θs = Ezx∼qφs (z|x)[log pθs(x|zx)] − Ezx∼qφs (z|x)[logD(zx)].\n(3)\nHere we define qφs(z|x) = N (µs(x),Σs(x)), where µs(x) = Wµsx and Σs(x) = exp(Wσsx); Wµs and Wσs are learned parameters. We also define the mean of pθs(x|z) to be WTµsz. The objective function and structure for φt are similar.\nThe basic framework of our model is shown in Figure 1. As we could see from the figure, our model tries to map the source and target word embedding into the same latent space which could capture the semantic meaning of words.\nTheoretical analysis has revealed that adversarial training tries to minimize the Jensen-Shannon (JS) divergence between the real and fake distribution. Therefore, one can view our model as replace KL divergence in Equation 1 with JS divergence and change the Gaussian prior to the target distribution."
  }, {
    "heading": "3.3 Training Strategy",
    "text": "Our model has two generators φs and φt, and we have found that training them jointly would be extremely unstable. In this paper, we propose an iterative method to train our models. Basically, we first initialize Wµt to be identity matrix and train φs and θs on the source side. After convergence, we freeze Wµs , and then train φt and θt in the target side. The pseudo-code for this process is shown in Algorithm 1. It should be noted that there is no variance once completing training.\nAlgorithm 1 Training Strategy 1: Wµt = I 2: for i = 1, · · · , niter do 3: while φs and θs have not converged do 4: Update discriminator D 5: Update φs and θs 6: end while 7: while φt and θt have not converged do 8: Update discriminator D 9: Update φt and θt 10: end while 11: end for"
  }, {
    "heading": "4 Experiment",
    "text": "Our experiments could be divided into two parts. In the first part, we conduct experiments on smallscale datasets and our main baseline is Zhang et al. (2017). In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) on largescale datasets."
  }, {
    "heading": "4.1 Small-scale Datasets",
    "text": "In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder (Zhang et al., 2017). For justice, we use the same model selection strategy with Zhang et al. (2017), i.e. we choose the model whose sum of reconstruction loss and classification accuracy is the least. The source and target word embeddings would be first mapped into the latent space. For each source word embedding x, it would be first transformed into zx. The the its k nearest target embeddings would be retrieved and be compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-1 accuracy."
  }, {
    "heading": "4.1.1 Experiments on Chinese-English Dataset",
    "text": "For this set of experiments, we use the same data as Zhang et al. (2017). The statistics of the final training data is given in Table 1. We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexicon for evaluation.\nThe baseline models are MonoGiza system (Dou et al., 2015), translation matrix (TM) (Mikolov et al., 2013), isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach (Zhang et al., 2017).\nTable 2 summarizes the performance of baseline models and our approach. The results of baseline models are cited from Zhang et al. (2017). As we can see from the table, our model could achieve superior performance compared with other baseline models. Table 3 lists some word translation examples given by our model."
  }, {
    "heading": "4.1.2 Experiments on Other Language Pairs Datasets",
    "text": "We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with Zhang et al. (2017). and the statistics are shown in Table 1. The ground truth bilingual lexica for Spanish-English and Italian-\nEnglish are obtained from Multilingual Unsupervised and Supervised Embeddings (MUSE) 1.\nThe experimental results are shown in Table 4. Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset. Our model is able to outperform baseline model in this setting."
  }, {
    "heading": "4.2 Large-scale Datasets",
    "text": "In this section, we integrate our method with Conneau et al. (2018), whose method improves Zhang et al. (2017) by more sophiscated refinement procedure and validation criterion. We replace their first step, namely the adversarial training step, with our model. Basically, we first map the source and target embeddings into the latent space using our algorithm, and then fine-tune the identity mapping in the latent space with the closed-form Procrustes solution. We use their similarity measure, namely cross-domain similarity local scaling (CSLS), to produce reliable matching pairs and validation criterion for unsupervised model selection.\n1https://github.com/facebookresearch/MUSE\nWe conduct experiments on English-Spanish, English-Russian and English-Chinese datasets, which are the same as Conneau et al. (2018). The results are shown in Table 5. As seen, our model could consistently achieve better performance compared with adversarial training. After refinement, our model could further achieve competitive and even superior results compared with state-of-the-art unsupervised methods. This further demonstrates the capacity of our model."
  }, {
    "heading": "5 Conclusion",
    "text": "Based on the assumption that word vectors in different languages could be drawn from a same latent variable space, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. Experimental results on several language pairs have demonstrated the effectiveness and universality of our model. We hope our method could be beneficial to other areas such as unsupervised machine translation (Lample et al., 2018).\nFuture directions include validate our model on more realistic scenarios (Dinu et al., 2015) as well as combine our algorithms with more sophiscated adversarial networks (Arjovsky et al., 2017; Gulrajani et al., 2017)."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank all the anonymous reviewers for their insightful comments."
  }],
  "year": 2018,
  "references": [{
    "title": "Wasserstein generative adversarial networks",
    "authors": ["Martin Arjovsky", "Soumith Chintala", "Léon Bottou."],
    "venue": "International Conference on Machine Learning (ICML).",
    "year": 2017
  }, {
    "title": "Learning bilingual word embeddings with (almost) no bilingual data",
    "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."],
    "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "Enriching word vectors with subword information",
    "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL).",
    "year": 2017
  }, {
    "title": "Generating sentences from a continuous space",
    "authors": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio."],
    "venue": "SIGNLL Conference on Computational Natural Language Learning (CoNLL).",
    "year": 2016
  }, {
    "title": "Word translation without parallel data",
    "authors": ["Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou"],
    "venue": "In International Conference on Learning Representations (ICLR)",
    "year": 2018
  }, {
    "title": "Improving zero-shot learning by mitigating the hubness problem",
    "authors": ["Georgiana Dinu", "Angeliki Lazaridou", "Marco Baroni."],
    "venue": "International Conference on Learning Representations (ICLR), Workshop Track.",
    "year": 2015
  }, {
    "title": "Unifying bayesian inference and vector space models for improved decipherment",
    "authors": ["Qing Dou", "Ashish Vaswani", "Kevin Knight", "Chris Dyer."],
    "venue": "International Joint Conference on Natural Language Processing (IJCNLP).",
    "year": 2015
  }, {
    "title": "Improving vector space word representations using multilingual correlation",
    "authors": ["Manaal Faruqui", "Chris Dyer."],
    "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2014
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Conference on Neural Information Processing Systems (NIPS).",
    "year": 2014
  }, {
    "title": "Improved training of wasserstein gans",
    "authors": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron C Courville."],
    "venue": "Advances in Neural Information Processing Systems (NIPS).",
    "year": 2017
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["Diederik P Kingma", "Max Welling."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2014
  }, {
    "title": "Unsupervised machine translation using monolingual corpora",
    "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc’Aurelio Ranzato"],
    "venue": "only. International Conference on Learning Representations (ICLR)",
    "year": 2018
  }, {
    "title": "Autoencoding beyond pixels using a learned similarity metric",
    "authors": ["Anders Boesen Lindbo Larsen", "Søren Kaae Sønderby", "Hugo Larochelle", "Ole Winther."],
    "venue": "International Conference on Machine Learning (ICML).",
    "year": 2016
  }, {
    "title": "Deep multilingual correlation for improved word embeddings",
    "authors": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."],
    "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "year": 2015
  }, {
    "title": "Adversarial autoencoders",
    "authors": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow", "Brendan Frey."],
    "venue": "arXiv preprint arXiv:1511.05644.",
    "year": 2015
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1309.4168.",
    "year": 2013
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
    "authors": ["Samuel L Smith", "David HP Turban", "Steven Hamblin", "Nils Y Hammerla."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Normalized word embedding and orthogonal transform for bilingual word translation",
    "authors": ["Chao Xing", "Dong Wang", "Chao Liu", "Yiye Lin."],
    "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
    "year": 2015
  }, {
    "title": "Improving neural machine translation with conditional sequence generative adversarial nets",
    "authors": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu."],
    "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "year": 2018
  }, {
    "title": "Variational neural machine translation",
    "authors": ["Biao Zhang", "Deyi Xiong", "Hong Duan", "Min Zhang"],
    "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    "year": 2016
  }, {
    "title": "Adversarial training for unsupervised bilingual lexicon induction",
    "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "Ten pairs to tag– multilingual pos tagging via coarse mapping between embeddings",
    "authors": ["Yuan Zhang", "David Gaddy", "Regina Barzilay", "Tommi Jaakkola."],
    "venue": "Annual Conference of the North American Chapter of the Association for",
    "year": 2016
  }],
  "id": "SP:11187ff3008163b673aa34ba4eafdb0ceb6db6b1",
  "authors": [{
    "name": "Zi-Yi Dou",
    "affiliations": []
  }, {
    "name": "Zhi-Hao Zhou",
    "affiliations": []
  }, {
    "name": "Shujian Huang",
    "affiliations": []
  }],
  "abstractText": "Bilingual lexicon extraction has been studied for decades and most previous methods have relied on parallel corpora or bilingual dictionaries. Recent studies have shown that it is possible to build a bilingual dictionary by aligning monolingual word embedding spaces in an unsupervised way. With the recent advances in generative models, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models.",
  "title": "Unsupervised Bilingual Lexicon Induction via Latent Variable Models"
}