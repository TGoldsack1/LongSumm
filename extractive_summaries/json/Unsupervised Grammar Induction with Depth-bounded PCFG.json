{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Grammar acquisition or grammar induction (Carroll and Charniak, 1992) has been of interest to linguists and cognitive scientists for decades. This task is interesting because a well-performing acquisition model can serve as a good baseline for examining factors of grounding (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010), or as a piece of evidence (Clark, 2001; Zuidema, 2003) about the Distributional Hypothesis (Harris, 1954) against the poverty of the stimulus (Chomsky, 1965). Unfortunately, previous attempts at inducing unbounded context-free grammars (Johnson et al., 2007; Liang\net al., 2009) converged to weak modes of a very multimodal distribution of grammars. There has been recent interest in applying cognitively- or empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). Ponvert et al. (2011) and Shain et al. (2016) in particular report benefits for depth bounds on grammar acquisition using hierarchical sequence models, but either without the capacity to learn full grammar rules (e.g. that a noun phrase may consist of a noun phrase followed by a prepositional phrase), or with a very large parameter space that may offset the gains of depth-bounding. This work extends the depth-bounding approach to directly induce probabilistic context-free grammars,1 which have a smaller parameter space than hierarchical sequence models, and therefore arguably make better use of the space reductions of depthbounding. This approach employs a procedure for deriving a sequence model from a PCFG (van Schijndel et al., 2013), developed in the context of a supervised learning model, and adapts it to an unsupervised setting.\nResults for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, as shown in a noun phrase discovery task, something which has not been demonstrated by other acquisition models.\n1https://github.com/lifengjin/db-pcfg\n211\nTransactions of the Association for Computational Linguistics, vol. 6, pp. 211–224, 2018. Action Editor: Xavier Carreras. Submission batch: 9/2017; Revision batch: 12/2017; Published 4/2018.\nc©2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license."
  }, {
    "heading": "2 Related work",
    "text": "This paper describes a Bayesian Dirichlet model of depth-bounded probabilistic context-free grammar (PCFG) induction. Bayesian Dirichlet models have been applied to the related area of latent variable PCFG induction (Johnson et al., 2007; Liang et al., 2009), in which subtypes of categories like noun phrases and verb phrases are induced on a given tree structure. The model described in this paper is given only words and not only induces categories for constituents but also tree structures.\nThere are a wide variety of approaches to grammar induction outside the Bayesian modeling paradigm. The CCL system (Seginer, 2007a) uses deterministic scoring systems to generate bracketed output of raw text. UPPARSE (Ponvert et al., 2011) uses a cascade of HMM chunkers to produce syntactic structures. BMMM+DMV (Christodoulopoulos et al., 2012) combines an unsupervised partof-speech (POS) tagger BMMM and an unsupervised dependency grammar inducer DMV (Klein and Manning, 2004). The BMMM+DMV system alternates between phases of inducing POS tags and inducing dependency structures. A large amount work (Klein and Manning, 2002; Klein and Manning, 2004; Bod, 2006; Berg-kirkpatrick et al., 2010; Gillenwater et al., 2011; Headden et al., 2009; Bisk and Hockenmaier, 2013; Scicluna and de la Higuera, 2014; Jiang et al., 2016; Han et al., 2017) has been on grammar induction with input annotated with POS tags, mostly for dependency grammar induction. Although POS tags can also be induced, this separate induction has been criticized (Pate and Johnson, 2016) for missing an opportunity to leverage information learned in grammar induction to estimate POS tags. Moreover, most of these models explore a search space that includes syntactic analyses that may be extensively center embedded and therefore are unlikely to be produced by human speakers. Unlike most of these approaches, the model described in this paper uses cognitively motivated bounds on the depth of human recursive processing to constrain its search of possible trees for input sentences.\nSome previous work uses depth bounds in the form of sequence models (Ponvert et al., 2011; Shain et al., 2016), but these either do not produce\ncomplete phrase structure grammars (Ponvert et al., 2011) or do so at the expense of large parameter sets (Shain et al., 2016). Other work implements depth bounds on left-corner configurations of dependency grammars (Noji and Johnson, 2016), but the use of a dependency grammar makes the system impractical for addressing questions of how category types such as noun phrases may be learned. Unlike these, the model described in this paper induces a PCFG directly and then bounds it with a model-to-model transform, which yields a smaller space of learnable parameters and directly models the acquisition of category types as labels.\nSome induction models learn semantic grammars from text annotated with semantic predicates (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2012). There is evidence humans use semantic bootstrapping during grammar acquisition (Naigles, 1990), but these models typically rely on a set of pre-defined universals, such as combinators (Steedman, 2000), which simplify the induction task. In order to help address the question of whether such universals are indeed necessary for grammar induction, the model described in this paper does not assume any strong universals except independently motivated limits on working memory."
  }, {
    "heading": "3 Background",
    "text": "Like Noji and Johnson (2016) and Shain et al. (2016), the model described in this paper defines bounding depth in terms of memory elements required in a left-corner parse. A left-corner parser (Rosenkrantz and Lewis, 1970; JohnsonLaird, 1983; Abney and Johnson, 1991; Resnik, 1992) uses a stack of memory elements to store derivation fragments during incremental processing. Each derivation fragment represents a disjoint connected component of phrase structure a/b consisting of a top sign a lacking a bottom sign b yet to come. For example, Figure 1 shows the derivation fragments in a traversal of a phrase structure tree for the sentence The cart the horse the man bought pulled broke. Immediately before processing the word man, the traversal has recognized three fragments of tree structure: two from category NP to category RC (covering the cart and the horse) and one from category NP to category N (cover-\ning the). Derivation fragments at every time step are numbered top-down by depth d to a maximum depth of D. A left-corner parser requires more derivation fragments — and thus more memory — to process center-embedded constructions than to process left- or right-embedded constructions, consistent with observations that center embedding is more difficult for humans to process (Chomsky and Miller, 1963; Miller and Isard, 1964). Grammar acquisition models (Noji and Johnson, 2016; Shain et al., 2016) then restrict this memory to some low bound, e.g. two derivation fragments.\nFor sequences of observed word tokens wt for time steps t ∈ {1..T }, sequence models like Ponvert et al. (2011) and Shain et al. (2016) hypothesize sequences of hidden states qt. Models like Shain et al. (2016) implement bounded grammar rules as depth bounds on a hierarchical sequence model implementation of a left-corner parser, using random variables within each hidden state qt for:\n1. preterminal labels pt and labels of top and bottom signs, adt and b d t , of derivation fragments\nat each depth level d (which correspond to left and right children in tree structure), and\n2. Boolean variables for decisions to ‘fork out’ ft and ‘join in’ jt derivation fragments (in\nJohnson-Laird (1983) terms, to shift with or without match and to predict with or without match).\nProbabilities from these distributions are then multiplied together to define a transition model M over hidden states:\nM[qt−1,qt] = P(qt | qt−1) (1a) def = P( ft pt jt a1..Dt b 1..D t | qt−1) (1b)\n= P( ft | qt−1) ·P(pt | qt−1 ft) ·P( jt | qt−1 ft pt) ·P(a1..Dt | qt−1 ft pt jt) ·P(b1..Dt | qt−1 ft pt jt a1..Dt ). (1c)\nFor example, just after the word horse is recognized in Figure 1, the parser store contains two derivation fragments yielding the cart and the horse, both with top category NP and bottom category RC. The parser then decides to fork out the next word the based on the bottom category RC of the last derivation fragment on the store. Then the parser generates a preterminal category D for this word based on this fork decision and the bottom category of the last derivation fragment on the store. Then the parser decides not to join the resulting D directly to the RC above it, based on these fork and preterminal decisions and the bottom category of the store. Finally the parser generates NP and N as the top and bottom categories of a new derivation fragment yielding just the new word the based on all these previous decisions, resulting in the store state shown in the figure.\nThe model over the fork decision (shift with or without match) is defined in terms of a depthspecific sub-model θF,d̄, where ⊥ is an empty derivation fragment and d̄ is the depth of the deepest nonempty derivation fragment at time step t − 1:\nP( ft | qt−1)def= PθF,d̄ ( ft | bd̄t−1); d̄ =maxd {b d t−1,⊥} (2)\nThe model over the preterminal category label is then conditioned on this fork decision. When there is no fork, the preterminal category label is deterministically linked to the category label of the bottom sign of the deepest derivation fragment at the previous time step (using ~φ as a deterministic indicator function, equal to one when φ is true and zero\notherwise). When there is a fork, the preterminal category label is defined in terms of a depth-specific sub-model θP,d̄: 2\nP(pt | qt−1 ft)def=  ~pt=bd̄t−1 if ft = 0 PθP,d̄ (pt | bd̄t−1) if ft = 1. (3)\nThe model over the join decision (predict with or without match) is also defined in terms of a depthspecific sub-model θJ,d̄ with parameters depending on the outcome of the fork decision:3\nP( jt | qt−1 ft pt)def= \nPθJ,d̄ ( jt | bd̄−1t−1 ad̄t−1) if ft=0 PθJ,d̄+1( jt | bd̄t−1 pt) if ft=1.\n(4)\nDecisions about the top categories of derivation fragments a1..Dt (which correspond to left siblings in tree structures) are decomposed into fork- and joinspecific cases. When there is a join, the top category of the deepest derivation fragment deterministically depends on the corresponding value at the previous time step. When there is no join, the top category is defined in terms of a depth-specific sub-model:4\nPθA(a 1..D t | qt−1 ft pt jt)def= φd̄−2 · ~ad̄−1t =ad̄−1t−1 · ψd̄+0 if ft, jt =0, 1 φd̄−1 · PθA,d̄ (ad̄t | bd̄−1t−1 ad̄t−1) · ψd̄+1 if ft, jt =0, 0 φd̄−1 · ~ad̄t =ad̄t−1 · ψd̄+1 if ft, jt =1, 1 φd̄−0 · PθA,d̄+1(ad̄+1t | bd̄t−1 pt) · ψd̄+2 if ft, jt =1, 0.\n(5)\nDecisions about the bottom categories b1..Dt (which correspond to right children in tree structures) also depend on the outcome of the fork and join variables, but are defined in terms of a side- and depth-specific sub-model in every case:5 PθB(b 1..D t | qt−1 ft pt jt a1..Dt )def= φd̄−2 · PθB,R,d̄−1(bd̄−1t | bd̄−1t−1 ad̄t−1) · ψd̄+0 if ft, jt =0, 1 φd̄−1 · PθB,L,d̄ (bd̄t | ad̄t ad̄t−1) · ψd̄+1 if ft, jt =0, 0 φd̄−1 · PθB,R,d̄ (bd̄t | bd̄t−1 pt) · ψd̄+1 if ft, jt =1, 1 φd̄−0 · PθB,L,d̄+1(bd̄+1t | ad̄+1t pt) · ψd̄+2 if ft, jt =1, 0.\n(6) 2 Here, again, d̄ =maxd{bdt−1,⊥}. 3 Again, d̄ =maxd{bdt−1,⊥}. 4 Here φd̄ = ~a1..d̄t = a 1..d̄ t−1 , ψd̄ = ~a d̄+1..D t = ⊥ , and again,\nd̄ =maxd{bdt−1,⊥}. 5 Here φd̄ = ~b1..d̄t = b 1..d̄ t−1 , ψd̄ = ~b d̄+1..D t = ⊥ , and again, d̄ =maxd{bdt−1,⊥}.\nIn a sequence model inducer like Shain et al. (2016), these depth-specific models are assumed to be independent of each other and fit with a Gibbs sampler, backward sampling hidden variable sequences from forward distributions using this compiled transition model M (Carter and Kohn, 1996), then counting individual sub-model outcomes from sampled hidden variable sequences, then resampling each sub-model using these counts with Dirichlet priors over a, b, and p models and Beta priors over f and j models, then re-compiling these resampled models into a new M.\nHowever, note that with K category labels this model contains DK2 + 3DK3 separate parameters for preterminal categories and top and bottom categories of derivation fragments at every depth level, each of which can be independently learned by the Gibbs sampler. Although this allows the hierarchical sequence model to learn grammars that are more expressive than PCFGs, the search space is several times larger than the K3 space of PCFG nonterminal expansions. The model described in this paper instead induces a PCFG and derives sequence model distributions from the PCFG, which has fewer parameters, and thus strictly reduces the search space of the model."
  }, {
    "heading": "4 The DB-PCFG Model",
    "text": "The depth-bounded probabilistic context-free grammar (DB-PCFG) model described in this paper directly induces a PCFG and then deterministically derives the parameters of a probabilistic left-corner parser from this single source. This derivation is based on an existing derivation of probabilistic left-corner parser models from PCFGs (van Schijndel et al., 2013), which was developed in a supervised parsing model, adapted here to run more efficiently within a larger unsupervised grammar induction model.6\nA PCFG can be defined in Chomsky normal form as a matrix G of binary rule probabilities with one row for each of K parent symbols c and one column for each of K2+W combinations of left and\n6 More specifically, the derivation differs from that of van Schijndel et al. (2013) in that it removes terminal symbols from conditional dependencies of models over fork and join decisions and top and bottom category labels, substantially reducing the size of the derived model that must be run during induction.\nright child symbols a and b, which can be pairs of nonterminals or observed words from vocabulary W followed by null symbols ⊥:7\nG = ∑\na,b,c\nP(c→ a b | c) δc (δa ⊗ δb)>. (7)\nA depth-bounded grammar is a set of side- and depth-specific distributions:\nGD = {Gs,d | s ∈ {L,R}, d ∈ {1..D}}. (8)\nThe posterior probability of a depth-bounded model GD given a corpus (sequence) of words w1..T is proportional to the product of a likelihood and a prior:\nP(GD | w1..T ) ∝ P(w1..T | GD) · P(GD). (9)\nThe likelihood is defined as a marginal over bounded PCFG trees τ of the probability of that tree given the grammar times the product of the probability of the word at each time step or token index t given this tree:8\nP(w1..T | GD) = ∑\nτ\nP(τ | GD) · ∏\nt\nP(wt | τ). (10)\nThe probability of each tree is defined to be the product of the probabilities of each of its branches:9\nP(τ | GD) = ∏\nτη∈τ PGD(τη → τη0 τη1 | τη). (11)\n7 This definition assumes a Kronecker delta function δi, defined as a vector with value one at index i and zeros everywhere else, and a Kronecker product M ⊗ N over matrices M and N, which tiles copies of N weighted by values in M as follows:\nM ⊗ N =  M[1,1] N M[1,2] N · · · M[2,1] N M[2,2] N · · ·\n... ...\n. . .\n . (1’)\nThe Kronecker product specializes to vectors as single-column matrices, generating vectors that contain the products of all combinations of elements in the operand vectors.\n8 This notation assumes the observed data w1..T is a single long sequence of words, and the hidden variable τ is a single large but depth-bounded tree structure (e.g. a right-branching discourse structure). Since the implementation is incremental, segmentation decisions may indeed be treated as hidden variables in τ, but the experiments described in Section 5 are run on sentence-segmented input.\n9 Here, η is a node address, with left child η0 and right child η1, or with right child equal to ⊥ if unary.\nThe probability P(GD) is itself an integral over the product of a deterministic transform φ from an unbounded grammar to a bounded grammar P(GD | G) = ~GD = φ(G) and a prior over unbounded grammars P(G):\nP(GD) = ∫ P(GD | G) · P(G) · dG. (12)\nDistributions P(G) for each nonterminal symbol (rows) within this unbounded grammar can then be sampled from a Dirichlet distribution with a symmetric parameter β:\nG ∼ Dirichlet(β), (13)\nwhich then yields a corresponding transformed sample in P(GD) for corresponding nonterminals. Note that this model is different than that of Shain et al. (2016), who induce a hierarchical HMM directly.\nA depth-specific grammar GD is (deterministically) derived from G via transform φwith probabilities for expansions constrained to and renormalized over only those outcomes that yield terminals within a particular depth bound D. This depth-bounded grammar is then used to derive left-corner expectations (anticipated counts of categories appearing as left descendants of other categories), and ultimately the parameters of the depth-bounded leftcorner parser defined in Section 3. Counts for G are then obtained from sampled hidden state sequences, and rows of G are then directly sampled from the posterior updated by these counts."
  }, {
    "heading": "4.1 Depth-bounded grammar",
    "text": "In order to ensure the bounded version of G is a consistent probability model, it must be renormalized in transform φ to assign a probability of zero to any derivation that exceeds its depth bound D. For example, if D = 2, then it is not possible to expand a left sibling at depth 2 to anything other than a lexical item, so the probability of any non-lexical expansion must be removed from the depth-bounded model, and the probabilities of all remaining outcomes must be renormalized to a new total without this probability. Following van Schijndel et al. (2013), this can be done by iteratively defining a side- and depthspecific containment likelihood h(i)s,d for left- or rightside siblings s ∈ {L,R} at depth d ∈ {1..D} at each it-\neration i ∈ {1..I},10 as a vector with one row for each nonterminal or terminal symbol (or null symbol ⊥) in G, containing the probability of each symbol generating a complete yield within depth d as an s-side sibling:\nh(0)s,d = 0 (14a)\nh(i)L,d =  G (1 ⊗ δ⊥ + h(i−1)L,d ⊗ h(i−1)R,d ) if d ≤ D + 1 0 if d > D + 1\n(14b)\nh(i)R,d =  δT if d = 0 G (1 ⊗ δ⊥ + h(i−1)L,d+1 ⊗ h(i−1)R,d ) if 0 < d ≤ D 0 if d > D.\n(14c)\nwhere ‘T’ is a top-level category label at depth zero. A depth-bounded grammar Gs,d can then be defined to be the original grammar G reweighted and renormalized by this containment likelihood:11\nGL,d = G diag(1 ⊗ δ⊥ + h(I)L,d ⊗ h(I)R,d)\nh(I)L,d (15a)\nGR,d = G diag(1 ⊗ δ⊥ + h(I)L,d+1 ⊗ h(I)R,d)\nh(I)R,d . (15b)\nThis renormalization ensures the depth-bounded model is consistent. Moreover, this distinction between a learned unbounded grammar G and a derived bounded grammar Gs,d which is used to derive a parsing model may be regarded as an instance of Chomsky’s (1965) distinction between linguistic competence and performance.\nThe side- and depth-specific grammar can then be used to define expected counts of categories occurring as left descendants (or ‘left corners’) of right-\n10 Experiments described in this article use I = 20 following observations of convergence at this point in supervised parsing.\n11 where diag(v) is a diagonalization of a vector v:\ndiag(v) =  v[1] 0 · · · 0 v[2] ... . . .  . (2’)\nsibling ancestors:\nE(1)d = GR,d (diag(1) ⊗ 1) (16a) E(i)d = E (i−1) d GL,d (diag(1) ⊗ 1) (16b)\nE+d = ∑I i=1 E (i) d . (16c)\nThis left-corner expectation will be used to estimate the marginalized probability over all grammar rule expansions between derivation fragments, which must traverse an unknown number of left children of some right-sibling ancestor."
  }, {
    "heading": "4.2 Depth-bounded parsing",
    "text": "Again following van Schijndel et al. (2013), the fork and join decision, and the preterminal, top and bottom category label sub-models described in Section 3 can now be defined in terms of these sideand depth-specific grammars Gs,d and depth-specific left-corner expectations E+d .\nFirst, probabilities for no-fork and yes-fork outcomes below some bottom sign of category b at depth d are defined as the normalized probabilities, respectively, of any lexical expansion of a right sibling b at depth d, and of any lexical expansion following any number of left child expansions from b at depth d:\nPθF,d (0 | b) = δb >GR,d (1 ⊗ δ⊥)\nδb >(GR,d + E+d GL,d) (1 ⊗ δ⊥)\n(17a)\nPθF,d (1 | b) = δb >E+d GL,d (1 ⊗ δ⊥)\nδb >(GR,d + E+d GL,d) (1 ⊗ δ⊥)\n. (17b)\nThe probability of a preterminal p given a bottom category b is simply a normalized left-corner expected count of p under b:\nPθP,d (p | b)def= δb > E+d δp\nδb > E+d 1\n. (18)\nYes-join and no-join probabilities below bottom sign b and above top sign a at depth d are then defined similarly to fork probabilities, as the normalized probabilities, respectively, of an expansion to left child a of a right sibling b at depth d, and of an expansion to left child a following any number of\nleft child expansions from b at depth d:\nPθJ,d (1 | b a) = δb >GR,d (δa ⊗ 1)\nδb >(GR,d + E+d GL,d) (δa ⊗ 1)\n(19a)\nPθJ,d (0 | b a) = δb >E+d GL,d (δa ⊗ 1)\nδb >(GR,d + E+d GL,d) (δa ⊗ 1)\n.\n(19b)\nThe distribution over category labels for top signs a above some top sign of category c and below a bottom sign of category b at depth d is defined as the normalized distribution over category labels following a chain of left children expanding from b which then expands to have a left child of category c:\nPθA,d (a | b c) = δb >E+d diag(δa) GL,d (δc ⊗ 1)\nδb >E+d diag(1) GL,d (δc ⊗ 1)\n. (20)\nThe distribution over category labels for bottom signs b below some sign a and sibling of top sign c is then defined as the normalized distribution over right children of grammar rules expanding from a to c followed by b:\nPθB,s,d (b | a c) = δa >Gs,d (δc ⊗ δb)\nδa >Gs,d (δc ⊗ 1) . (21)\nFinally, a lexical observation model L is defined as a matrix of unary rule probabilities with one row for each combination of store state and preterminal symbol and one column for each observation symbol: L = 1 ⊗G (diag(1) ⊗ δ⊥). (22)"
  }, {
    "heading": "4.3 Gibbs sampling",
    "text": "Grammar induction in this model then follows a forward-filtering backward-sampling algorithm (Carter and Kohn, 1996). This algorithm first computes a forward distribution vt over hidden states at each time step t from an initial value ⊥:\nv0> = δ⊥> (23a) vt> = vt−1>M diag(L δwt ). (23b)\nThe algorithm then samples hidden states backward from a multinomial distribution given the previously sampled state qt+1 at time step t+1 (assuming input parameters to the multinomial function are normalized):\nqt ∼ Multinom( diag(vt) M diag(L δwt+1) δqt+1 ). (24)\nGrammar rule applications C are then counted from these sampled sequences:12\nC = ∑\nt  δbd̄−1t−1 (δad̄t−1 ⊗ δbd̄−1t ) > if ft, jt = 0, 1 δad̄t (δad̄t−1 ⊗ δbd̄t ) > if ft, jt = 0, 0 δbd̄t−1 (δpt ⊗ δbd̄t ) > if ft, jt = 1, 1\nδad̄+1t (δpt ⊗ δbd̄+1t ) > if ft, jt = 1, 0\n+ ∑\nt\nδpt (δwt ⊗ δ⊥)>, (25)\nand a new grammar G is sampled from a Dirichlet distribution with counts C and a symmetric hyperparameter β as parameters:\nG ∼ Dirichlet( C + β ). (26)\nThis grammar is then used to define transition and lexical models M and L as defined in Sections 3 through 4.2 to complete the cycle."
  }, {
    "heading": "4.4 Model hyper-parameters and priors",
    "text": "There are three hyper-parameters in the model. K is the number of non-terminal categories in the grammar G, D is the maximum depth, and β is the parameter for the symmetric Dirichlet prior over multinomial distributions in the grammar G.\nAs seen from the previous subsection, the prior is over all possible rules in an unbounded PCFG grammar. Because the number of non-terminal categories of the unbounded PCFG grammar is given as a hyper-parameter, the number of rules in the grammar is always known. It is possible to use nonparametric priors over the number of non-terminal categories, however due to the need to dynamically mitigate the computational complexity of filtering and sampling using arbitrarily large category sets, this is left for future work."
  }, {
    "heading": "5 Evaluation",
    "text": "The DB-PCFG model described in Section 4 is evaluated first on synthetic data to determine whether it can reliably learn a recursive grammar from data with a known optimum solution, and to determine the hyper-parameter value for β for doing so. Two experiments on natural data are then carried out. First, the model is run on natural data from the Adam\n12 Again, d̄ =maxd{adt−1,⊥}.\nand Eve parts of the CHILDES corpus (Macwhinney, 1992) to compare with other grammar induction systems on a human-like acquisition task. Then data from the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) is used for further comparison in a domain for which competing systems are optimized. The competing systems include UPPARSE (Ponvert et al., 2011)13, CCL (Seginer, 2007a)14, BMMM+DMV with undirected dependency features (Christodoulopoulos et al., 2012)15 and UHHMM (Shain et al., 2016).16\nFor the natural language datasets, the variously parametrized DB-PCFG systems17 are first validated on a development set, and the optimal system is then run until convergence with the chosen hyperparameters on the test set. In development experiments, the log-likelihood of the dataset plateaus usually after 500 iterations. The system is therefore run at least 500 iterations in all test set experiments, with one iteration being a full cycle of Gibbs sampling. The system is then checked to see whether the loglikelihood has plateaued, and halted if it has.\nThe DB-PCFG model assigns trees sampled from conditional posteriors to all sentences in a dataset in every iteration as part of the inference. The system is further allowed to run at least 250 iterations after convergence and proposed parses are chosen from the iteration with the greatest log-likelihood after convergence. However, once the system reaches convergence, the evaluation scores of parses from different iterations post-convergence appear to differ very little."
  }, {
    "heading": "5.1 Synthetic data",
    "text": "Following Liang et al. (2009) and Scicluna and de la Higuera (2014), an initial set of experiments on synthetic data are used to investigate basic properties of the model—in particular:\n13https://github.com/eponvert/upparse 14https://github.com/DrDub/cclparser 15BMMM:https://github.com/christos-c/bmmm\nDMV:https://code.google.com/archive/p/ pr-toolkit/\n16https://github.com/tmills/uhhmm/tree/\ncoling16 17The most complex configuration that would run on available GPUs was D=2,K=15. Analysis of full WSJ (Schuler et al., 2010) shows 47.38% of sentences require depth 2, 38.32% require depth 3 and 6.26% require depth 4.\na) X1\nb) X1\nc) X2\nd) X2\n1. whether the model is balanced or biased in favor of left- or right-branching solutions,\n2. whether the model is able to posit recursive structure in appropriate places, and\n3. what hyper-parameters enable the model to find optimal modes more quickly.\nThe risk of bias in branching structure is important because it might unfairly inflate induction results on languages like English, which are heavily right branching. In order to assess its bias, the model is evaluated on two synthetic datasets, each consisting of 200 sentences. The first dataset is a leftbranching corpus, which consists of 100 sentences of the form a b and 100 sentences of the form a b b, with optimal tree structures as shown in Figure 2 (a) and (b). The second dataset is a right-branching corpus, which consists of 100 sentences of the form a b and 100 sentences of the form a a b, with optimal tree structures as shown in Figure 2 (c) and (d). Results show both structures (and both corresponding grammars) are learnable by the model, and result in approximately the same log likelihood. These synthetic datasets are also used to tune the β hyperparameter of the model (as defined in Section 4) to enable it to find optimal modes more quickly. The resulting β setting of 0.2 is then used in induction on the CHILDES and Penn Treebank corpora.\nAfter validating that the model is not biased, the model is also evaluated on a synthetic centerembedding corpus consisting of 50 sentences each of the form a b c; a b b c; a b a b c; and a b b a b b c, which has optimal tree structures as shown in Figure 3.18 Note that the (b) and (d) trees have depth 2\n18 Here, in order to more closely resemble natural language input, tokens a, b, and c are randomly chosen uniformly from {a1, . . . , a50}, {b1, . . . , b50} and {c1, . . . , c50}, respectively.\nbecause they each have a complex sub-tree spanning a b and a b b embedded in the center of the yield of the root. Results show the model is capable of learning depth 2 (recursive) grammars.\nFinally, as a gauge of the complexity of this task, results of the model described in this paper are compared with those of other grammar induction models on the center-embedding dataset. In this experiment, all models are assigned hyper-parameters matching the optimal solution. The DB-PCFG is run with K=5 and D=2 and β=0.2 for all priors, the BMMM+DMV (Christodoulopoulos et al., 2012) is run with 3 preterminal categories, and the UHHMM model is run with 2 active states, 4 awaited states and 3 parts of speech.19 Table 1 shows the PARSEVAL scores for parsed trees using the learned grammar from each unsupervised system. Only the DBPCFG model is able to recognize the correct tree structures and the correct category labels on this dataset, showing the task is indeed a robust challenge. This suggests that hyper-parameters optimized on this dataset may be portable to natural data.\n19It is not possible to use just 2 awaited states, which is the gold setting, since the UHHMM system errors out when the number of categories is small."
  }, {
    "heading": "5.2 Child-directed speech corpus",
    "text": "After setting the β hyperparameter on synthetic datasets, the DB-PCFG model is evaluated on 14,251 sentences of transcribed child-directed speech from the Eve section of the Brown corpus of CHILDES (Macwhinney, 1992). Hyperparameters D and K are set to optimize performance on the Adam section of the Brown Corpus of CHILDES, which is about twice as long as Eve. Following previous work, these experiments leave all punctuation in the input for learning, then remove it in all evaluations on development and test data.\nModel performance is evaluated against Penn Treebank style annotations of both Adam and Eve corpora (Pearl and Sprouse, 2013). Table 2 shows the PARSEVAL scores of the DB-PCFG system with different hyperparameters on the Adam corpus for development.The simplest configuration, D1K15 (depth 1 only with 15 non-terminal categories), obtains the best score, so this setting is applied to the test corpus, Eve. Results of the D=1,K=15 DB-PCFG model on Eve are then compared against those of other grammar induction systems which use only raw text as input on the same corpus. Following Shain et al. (2016) the BMMM+DMV system is run for 10 iterations with 45 categories and its output is converted from dependency graphs to constituent\ntrees (Collins et al., 1999). The UHHMM system is run on the Eve corpus using settings in Shain et al. (2016), which also includes a post-process option to flatten trees (reported here as UHHMM-F).\nTable 3 shows the PARSEVAL scores for all the competing systems on the Eve dataset. The rightbranching baseline is still the most accurate in terms of PARSEVAL scores, presumably because of the highly right-branching structure of child-directed speech in English. The DB-PCFG system with only one memory depth and 15 non-terminal categories achieves the best performance in terms of F1 score and recall among all the competing systems, significantly outperforming other systems (p < 0.0001, permutation test).20\nThe Eve corpus has about 5,000 sentences with more than one depth level, therefore one might expect a depth-two model to perform better than a depth-one model, but this is not true if only PARSEVAL scores are considered. This issue will be revisited in the following section with the noun phrase discovery task."
  }, {
    "heading": "5.3 NP discovery on child-directed speech",
    "text": "When humans acquire grammar, they do not only learn tree structures, they also learn category types: noun phrases, verb phrases, prepositional phrases, and where each type can and cannot occur.\n20Resulting scores are better when applying Shain et al. (2016) flattening to output binary-branching trees. For the D=1, K=15 model, precision and F1 can be raised to 70.31% and 74.33%. However, since the flattening is a heuristic which may not apply in all cases, these scores are not considered to be comparable results.\nSome of these category types — in particular, noun phrases — are fairly universal across languages, and may be useful in downstream tasks such as (unsupervised) named entity recognition. The DB-PCFG and other models that can be made to produce category types are therefore evaluated on a noun phrase discovery task.\nTwo metrics are used for this evaluation. First, the evaluation counts all constituents proposed by the candidate systems, and calculates recall against the gold annotation of noun phrases. This metric is not affected by which branching paradigm the system is using and reveals more about the systems’ performances. This metric differs from that used by Ponvert et al. (2011) in that this metric takes NPs at all levels in gold annotation into account, not just base NPs.21\nThe second metric, for systems that produce category labels, calculates F1 scores of induced categories that can be mapped to noun phrases. The first 4,000 sentences are used as the development set for learning mappings from induced category labels to phrase types. The evaluation calculates precision, recall and F1 of all spans of proposed categories against the gold annotations of noun phrases in the development set, and aggregates the categories ranked by their precision scores so that the F1 score of the aggregated category is the highest on the development set. The evaluation then calculates the F1 score of this aggregated category on the remainder of the dataset, excluding this development set.\n21Ponvert et al. (2011) define base NPs as NPs with no NP descendants, a restriction motivated by their particular task (chunking).\nThe UHHMM system is the only competing system that is natively able to produce labels for proposed constituents. BMMM+DMV does not produce constituents with labels by default, but can be evaluated using this metric by converting dependency graphs into constituent trees, then labeling each constituent with the part-of-speech tag of the head. For CCL and UPPARSE, the NP agg F1 scores are not reported because they do not produce labeled constituents.\nTable 4 shows the scores for all systems on the Eve dataset and four runs of the DB-PCFG system on these two evaluation metrics. Surprisingly the D=2, K=15 model which has the lowest PARSEVAL scores is most accurate at discovering noun phrases. It has the highest scores on both evaluation metrics. The best model in terms of PARSEVAL scores, the D=1, K=15 DB-PCFG model, performs poorly among the DB-PCFG models, despite the fact that its NP recall is higher than the competing systems. The low score of NP agg F1 of DB-PCFG at D1K15 shows a diffusion of induced syntactic categories when the model is trying to find a balance among labeling and branching decisions. The UPPARSE system, which is proposed as a base NP chunker, is relatively poor at NP recall by this definition.\nThe right-branching baseline does not perform well in terms of NP recall. This is mainly because noun phrases are often left children of some other constituent and the right branching model is unable to incorporate them into the syntactic structures of whole sentences. Therefore although the rightbranching model is the best model in terms of PARSEVAL scores, it is not helpful in terms of finding\nnoun phrases."
  }, {
    "heading": "5.4 Penn Treebank",
    "text": "To further facilitate direct comparison to previous work, we run experiments on sentences from the Penn Treebank (Marcus et al., 1993). The first experiment uses the sentences from Wall Street Journal part of the Penn Treebank with at most 20 words (WSJ20). The first half of the WSJ20 dataset is used as a development set (WSJ20dev) and the second half is used as a test set (WSJ20test). We also extract sentences in WSJ20test with at most 10 words from the proposed parses from all systems and report results on them (WSJ10test). WSJ20dev is used for finding the optimal hyperparameters for both DBPCFG and BMMM-DMV systems.22\nTable 5 shows the PARSEVAL scores of all systems. The right-branching baseline is relatively weak on these two datasets, mainly because formal writing is more complex and uses more non-rightbranching structures (e.g., subjects with modifiers or parentheticals) than child-directed speech. For WSJ10test, both the DB-PCFG system and CCL are able to outperform the right branching baseline. The F1 difference between the best-performing previouswork system, CCL, and DB-PCFG is highly significant. For WSJ20test, again both CCL and DB-\n22Although UHHMM also needs tuning, in practice we find that this system is too inefficient to be tuned on a development set, and it requires too many resources when the hyperparameters become larger than used in previous work. We believe that further increasing the hyperparameters of UHHMM may lead to performance increase, but the released version is not scalable to larger values of these settings. We also do not report UHHMM on WSJ20test for the same scalabilty reason. The results of WSJ10test of UHHMM is induced with all WSJ10 sentences.\nPCFG are above the right-branching baseline. The difference between the F scores of CCL and DBPCFG is very small compared to WSJ10, however it is also significant.\nIt is possible that the DB-PCFG is being penalized for inducing fully binarized parse trees. The accuracy of the DB-PCFG model is dominated by recall rather than precision, whereas CCL and other systems are more balanced. This is an important distinction if it is assumed that phrase structure is binary (Kayne, 1981; Larson, 1988), in which case precision merely scores non-linguistic decisions about whether to suppress annotation of nonmaximal projections. However, since other systems are not optimized for recall, it would not be fair to use only recall as a comparison metric in this study.\nFinally, Table 6 shows the published results of different systems on WSJ. The CCL results come from Seginer (2007b), where the CCL system is trained with all sentences from WSJ, and evaluated on sentences with 40 words or fewer from WSJ (WSJ40) and WSJ10. The UPPARSE results come from Ponvert et al. (2011), where the UPPARSE system is trained using 00-21 sections of WSJ, and evaluated on section 23 and the WSJ10 subset of section 23. The DB-PCFG system uses hyperparameters optimized on the WSJ20dev set, and is evaluated on WSJ40 and WSJ10, both excluding WSJ20dev. The results are not directly comparable, but the results from the DB-PCFG system is competitive with the other systems, and numerically have the best recall scores."
  }, {
    "heading": "6 Conclusion",
    "text": "This paper describes a Bayesian Dirichlet model of depth-bounded PCFG induction. Unlike earlier work this model implements depth bounds directly\non PCFGs by derivation, reducing the search space of possible trees for input words without exploding the search space of parameters with multiple sideand depth-specific copies of each rule. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.\nIn addition to its practical merits, this model may offer some theoretical insight for linguists and other cognitive scientists. First, the model does not assume any universals except independently motivated limits on working memory, which may help address the question of whether universals are indeed necessary for grammar induction. Second, the distinction this model draws between its learned unbounded grammar G and its derived bounded grammar GD seems to align with Chomsky’s (1965) distinction between competence and performance, and has the potential to offer some formal guidance to linguistic inquiry about both kinds of models."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors would like to thank Cory Shain and William Bryce for their valuable input. We would like also to thank the Action Editor Xavier Carreras and the anonymous reviewers for insightful comments. Computations for this project were partly run on the Ohio Supercomputer Center (1987). This research was funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022. The content of the information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred."
  }],
  "year": 2018,
  "references": [{
    "title": "Memory requirements and local ambiguities of parsing strategies",
    "authors": ["Steven P. Abney", "Mark Johnson."],
    "venue": "J. Psycholinguistic Research, 20(3):233–250.",
    "year": 1991
  }, {
    "title": "Painless unsupervised learning with features",
    "authors": ["Taylor Berg-kirkpatrick", "Alexandre Bouchard-Côté", "John DeNero", "Dan Klein."],
    "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
    "year": 2010
  }, {
    "title": "An HDP Model for Inducing Combinatory Categorial Grammars",
    "authors": ["Yonatan Bisk", "Julia Hockenmaier."],
    "venue": "Transactions Of The Association For Computational Linguistics, pages 75–88.",
    "year": 2013
  }, {
    "title": "Unsupervised parsing with U-DOP",
    "authors": ["Rens Bod."],
    "venue": "Proceedings of the Conference on Computational Natural Language Learning, pages 85–92.",
    "year": 2006
  }, {
    "title": "Two experiments on learning probabilistic dependency grammars from corpora",
    "authors": ["Glenn Carroll", "Eugene Charniak."],
    "venue": "Working Notes of the Workshop on Statistically-Based NLP Techniques, (March):1–13.",
    "year": 1992
  }, {
    "title": "Markov chain Monte Carlo in conditionally Gaussian state space models",
    "authors": ["C.K. Carter", "R. Kohn."],
    "venue": "Biometrika, 83(3):589–601.",
    "year": 1996
  }, {
    "title": "Introduction to the formal analysis of natural languages",
    "authors": ["Noam Chomsky", "George A. Miller."],
    "venue": "Handbook of Mathematical Psychology, pages 269– 321. Wiley, New York, NY.",
    "year": 1963
  }, {
    "title": "Aspects of the Theory of Syntax",
    "authors": ["Noam Chomsky."],
    "venue": "MIT Press, Cambridge, MA.",
    "year": 1965
  }, {
    "title": "Turning the pipeline into a loop: iterated unsupervised dependency parsing and POS induction",
    "authors": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of the Annual Conference of the North American Chapter of the Associa-",
    "year": 2012
  }, {
    "title": "Unsupervised induction of stochastic context-free grammars using distributional clustering",
    "authors": ["Alexander Clark."],
    "venue": "Proceedings of the Workshop on Computational Natural Language Learning, volume 7, pages 1–8.",
    "year": 2001
  }, {
    "title": "A Statistical Parser for Czech",
    "authors": ["Michael Collins", "Lance Ramshaw", "Jan Hajič", "Christoph Tillmann."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 505–512.",
    "year": 1999
  }, {
    "title": "Posterior sparsity in unsupervised dependency parsing",
    "authors": ["Jennifer Gillenwater", "Kuzman Ganchev", "João Graça", "Fernando Pereira", "Ben Taskar."],
    "venue": "Journal of Machine Learning Research, 12:455–490.",
    "year": 2011
  }, {
    "title": "Dependency grammar induction with neural lexicalization and big training data",
    "authors": ["Wenjuan Han", "Yong Jiang", "Kewei Tu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1684–1689.",
    "year": 2017
  }, {
    "title": "Distributional structure",
    "authors": ["Zellig Harris."],
    "venue": "Jerry A. Fodor and Jerrold J. Katz, editors, The Structure of Language: Readings in the Philosophy of Language, volume 10, pages 33–49. Prentice-Hall.",
    "year": 1954
  }, {
    "title": "Improving unsupervised dependency parsing with richer contexts and smoothing",
    "authors": ["III William P. Headden", "Mark Johnson", "David McClosky."],
    "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2009
  }, {
    "title": "Unsupervised neural dependency parsing",
    "authors": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, number 61503248, pages 763– 771.",
    "year": 2016
  }, {
    "title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models",
    "authors": ["Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater."],
    "venue": "Advances in Neural Information Processing Systems, volume 19, page 641.",
    "year": 2007
  }, {
    "title": "Mental models: Towards a cognitive science of language, inference, and consciousness",
    "authors": ["Philip N. Johnson-Laird."],
    "venue": "Harvard University Press, Cambridge, MA, USA.",
    "year": 1983
  }, {
    "title": "Unambiguous Paths",
    "authors": ["Richard Kayne."],
    "venue": "R. May and J. Koster, editors, Levels of Syntactic Representation, pages 143–183. Foris Publishers.",
    "year": 1981
  }, {
    "title": "A generative constituent-context model for improved grammar induction",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 128–135.",
    "year": 2002
  }, {
    "title": "Corpusbased induction of syntactic structure: Models of dependency and constituency",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics, volume 1, pages 478–485.",
    "year": 2004
  }, {
    "title": "Inducing probabilistic CCG grammars from logical form with higherorder unification",
    "authors": ["Tom Kwiatkowski", "Luke S. Zettlemoyer", "Sharon Goldwater", "Mark Steedman."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
    "year": 2010
  }, {
    "title": "A probabilistic model of syntactic and semantic acquisition",
    "authors": ["Tom Kwiatkowski", "Sharon Goldwater", "Luke S. Zettlemoyer", "Mark Steedman"],
    "year": 2012
  }, {
    "title": "On the double object construction",
    "authors": ["Richard K. Larson."],
    "venue": "Linguistic Inquiry, 19(3):335–391.",
    "year": 1988
  }, {
    "title": "Probabilistic Grammars and Hierarchical Dirichlet Processes",
    "authors": ["Percy Liang", "Michael I. Jordan", "Dan Klein."],
    "venue": "The Handbook of Applied Bayesian Analysis.",
    "year": 2009
  }, {
    "title": "The CHILDES Project: Tools for Analyzing Talk",
    "authors": ["Brian Macwhinney."],
    "venue": "Lawrence Elrbaum Associates, Mahwah, NJ, third edition.",
    "year": 1992
  }, {
    "title": "Building a large annotated corpus of English: The Penn Treebank",
    "authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."],
    "venue": "Computational Linguistics, 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Free recall of self-embedded English sentences",
    "authors": ["George A. Miller", "Stephen Isard."],
    "venue": "Information and Control, 7:292–303.",
    "year": 1964
  }, {
    "title": "Children use syntax to learn verb meanings",
    "authors": ["Letitia R. Naigles."],
    "venue": "The Journal of Child Language, 17:357–374.",
    "year": 1990
  }, {
    "title": "Using Leftcorner Parsing to Encode Universal Structural Constraints in Grammar Induction",
    "authors": ["Hiroshi Noji", "Mark Johnson."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 33–43.",
    "year": 2016
  }, {
    "title": "Grammar induction from ( lots of ) words alone",
    "authors": ["John K. Pate", "Mark Johnson."],
    "venue": "Proceedings of the International Conference on Computational Linguistics, pages 23–32.",
    "year": 2016
  }, {
    "title": "Syntactic islands and learning biases: Combining experimental syntax and computational modeling to investigate the language acquisition problem",
    "authors": ["Lisa Pearl", "Jon Sprouse."],
    "venue": "Language Acquisition, 20(1):23– 68, 1.",
    "year": 2013
  }, {
    "title": "Simple unsupervised grammar induction from raw text with cascaded finite state models",
    "authors": ["Elias Ponvert", "Jason Baldridge", "Katrin Erk."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 1077–1086.",
    "year": 2011
  }, {
    "title": "Left-corner parsing and psychological plausibility",
    "authors": ["Philip Resnik."],
    "venue": "Proceedings of the International Conference on Computational Linguistics, pages 191– 197.",
    "year": 1992
  }, {
    "title": "Deterministic left corner parser",
    "authors": ["Stanley J. Rosenkrantz", "Philip M. Lewis", "II."],
    "venue": "IEEE Conference Record of the 11th Annual Symposium on Switching and Automata, pages 139–152.",
    "year": 1970
  }, {
    "title": "Broad-coverage parsing using human-Like memory constraints",
    "authors": ["William Schuler", "Samir AbdelRahman", "Tim Miller", "Lane Schwartz."],
    "venue": "Computational Linguistics, 36(1):1–30.",
    "year": 2010
  }, {
    "title": "PCFG induction for unsupervised parsing and language modelling",
    "authors": ["James Scicluna", "Colin de la Higuera."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1353–1362.",
    "year": 2014
  }, {
    "title": "Fast Unsupervised Incremental Parsing",
    "authors": ["Yoav Seginer."],
    "venue": "Proceedings of the Annual Meeting of the Association of Computational Linguistics, pages 384– 391.",
    "year": 2007
  }, {
    "title": "Learning Syntactic Structure",
    "authors": ["Yoav Seginer."],
    "venue": "Ph.D. thesis, University of Amsterdam.",
    "year": 2007
  }, {
    "title": "Memorybounded left-corner unsupervised grammar induction on child-directed input",
    "authors": ["Cory Shain", "William Bryce", "Lifeng Jin", "Victoria Krakovna", "Finale Doshi-Velez", "Timothy Miller", "William Schuler", "Lane Schwartz."],
    "venue": "Proceedings of the In-",
    "year": 2016
  }, {
    "title": "The Syntactic Process",
    "authors": ["Mark Steedman."],
    "venue": "MIT Press/Bradford Books, Cambridge, MA.",
    "year": 2000
  }, {
    "title": "A Model of language processing as hierarchic sequential prediction",
    "authors": ["Marten van Schijndel", "Andy Exley", "William Schuler."],
    "venue": "Topics in Cognitive Science, 5(3):522–540.",
    "year": 2013
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the Proceedings of the Conference Annual Conference on Uncertainty in Artificial Intel-",
    "year": 2005
  }, {
    "title": "How the poverty of the stimulus solves the poverty of the stimulus",
    "authors": ["Willem Zuidema."],
    "venue": "Advances in Neural Information Processing Systems, volume 15, page 51.",
    "year": 2003
  }],
  "id": "SP:d4177bda2652150b02058d07c5e6c25466cdadd0",
  "authors": [{
    "name": "Lifeng Jin",
    "affiliations": []
  }, {
    "name": "Finale Doshi-Velez",
    "affiliations": []
  }, {
    "name": "Timothy Miller",
    "affiliations": []
  }, {
    "name": "William Schuler",
    "affiliations": []
  }],
  "abstractText": "There has been recent interest in applying cognitivelyor empirically-motivated bounds on recursion depth to limit the search space of grammar induction models (Ponvert et al., 2011; Noji and Johnson, 2016; Shain et al., 2016). This work extends this depthbounding approach to probabilistic contextfree grammar induction (DB-PCFG), which has a smaller parameter space than hierarchical sequence models, and therefore more fully exploits the space reductions of depthbounding. Results for this model on grammar acquisition from transcribed child-directed speech and newswire text exceed or are competitive with those of other models when evaluated on parse accuracy. Moreover, grammars acquired from this model demonstrate a consistent use of category labels, something which has not been demonstrated by other acquisition models.",
  "title": "Unsupervised Grammar Induction with Depth-bounded PCFG"
}