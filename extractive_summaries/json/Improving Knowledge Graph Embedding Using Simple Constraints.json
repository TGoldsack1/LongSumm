{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 110–121 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n110"
  }, {
    "heading": "1 Introduction",
    "text": "The past decade has witnessed great achievements in building web-scale knowledge graphs (KGs), e.g., Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), and Google’s Knowledge\n∗Corresponding author: Quan Wang.\nVault (Dong et al., 2014). A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015; Hoffmann et al., 2011; Yang and Mitchell, 2017).\nRecently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011, 2013; Jenatton et al., 2012; Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013; Bordes et al., 2014; Wang et al., 2014; Lin et al., 2015b; Xiao et al., 2016; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014; Zhong et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Guo et al., 2015; Luo et al., 2015b; Xie et al., 2016a,b; Xiao et al., 2017). See (Wang et al., 2017) for a thorough review.\nThis paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode regularities of logical entailment between relations into their\ndistributed representations, which might be advantageous to downstream tasks like link prediction and relation extraction (Rocktäschel et al., 2015; Guo et al., 2016). These constraints impose prior beliefs upon the structure of the embedding space, and will help us to learn more predictive embeddings, without significantly increasing the space or time complexity.\nOur work has some similarities to those which integrate logical background knowledge into KG embedding (Rocktäschel et al., 2015; Wang et al., 2015; Guo et al., 2016, 2018). Most of such works, however, need grounding of first-order logic rules. The grounding process could be time and space inefficient especially for complicated rules. To avoid grounding, Demeester et al. (2016) tried to model rules using only relation representations. But their work creates vector representations for entity pairs rather than individual entities, and hence fails to handle unpaired entities. Moreover, it can only incorporate strict, hard rules which usually require extensive manual effort to create. Minervini et al. (2017b) proposed adversarial training which can integrate first-order logic rules without grounding. But their work, again, focuses on strict, hard rules. Minervini et al. (2017a) tried to handle uncertainty of rules. But their work assigns to different rules a same confidence level, and considers only equivalence and inversion of relations, which might not always be available in a given KG.\nOur approach differs from the aforementioned works in that: (i) it imposes constraints directly on entity and relation representations without grounding, and can easily scale up to large KGs; (ii) the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no manual effort and applicable to almost all KGs; (iii) it learns an individual representation for each entity, and can successfully make predictions between unpaired entities.\nWe evaluate our approach on publicly available KGs of WordNet, Freebase, and DBpedia as well. Experimental results indicate that our approach is simple yet surprisingly effective, achieving significant and consistent improvements over competitive baselines, but without negative impacts on efficiency or scalability. The non-negativity and approximate entailment constraints indeed improve model interpretability, resulting in a substantially increased structuring of the embedding space.\nThe remainder of this paper is organized as follows. We first review related work in Section 2, and then detail our approach in Section 3. Experiments and results are reported in Section 4, followed by concluding remarks in Section 5."
  }, {
    "heading": "2 Related Work",
    "text": "Recent years have seen growing interest in learning distributed representations for entities and relations in KGs, a.k.a. KG embedding. Early works on this topic devised very simple models to learn such distributed representations, solely on the basis of triples observed in a given KG, e.g., TransE which takes relations as translating operations between head and tail entities (Bordes et al., 2013), and RESCAL which models triples through bilinear operations over entity and relation representations (Nickel et al., 2011). Later attempts roughly fell into two groups: (i) those which tried to design more complicated triple scoring models, e.g., the TransE extensions (Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015), the RESCAL extensions (Yang et al., 2015; Nickel et al., 2016b; Trouillon et al., 2016; Liu et al., 2017), and the (deep) neural network models (Socher et al., 2013; Bordes et al., 2014; Shi and Weninger, 2017; Schlichtkrull et al., 2017; Dettmers et al., 2018); (ii) those which tried to integrate extra information beyond triples, e.g., entity types (Guo et al., 2015; Xie et al., 2016b), relation paths (Neelakantan et al., 2015; Lin et al., 2015a), and textual descriptions (Xie et al., 2016a; Xiao et al., 2017). Please refer to (Nickel et al., 2016a; Wang et al., 2017) for a thorough review of these techniques. In this paper, we show the potential of using very simple constraints (i.e., nonnegativity constraints and approximate entailment constraints) to improve KG embedding, without significantly increasing the model complexity.\nA line of research related to ours is KG embedding with logical background knowledge incorporated (Rocktäschel et al., 2015; Wang et al., 2015; Guo et al., 2016, 2018). But most of such works require grounding of first-order logic rules, which is time and space inefficient especially for complicated rules. To avoid grounding, Demeester et al. (2016) proposed lifted rule injection, and Minervini et al. (2017b) investigated adversarial training. Both works, however, can only handle strict, hard rules which usually require extensive effort to create. Minervini et al. (2017a) tried to handle uncertainty of background knowledge. But their work\nconsiders only equivalence and inversion between relations, which might not always be available in a given KG. Our approach, in contrast, imposes constraints directly on entity and relation representations without grounding. And the constraints used are quite universal, requiring no manual effort and applicable to almost all KGs.\nNon-negativity has long been a subject studied in various research fields. Previous studies reveal that non-negativity could naturally induce sparsity and, in most cases, better interpretability (Lee and Seung, 1999). In many NLP-related tasks, nonnegativity constraints are introduced to learn more interpretable word representations, which capture the notion of semantic composition (Murphy et al., 2012; Luo et al., 2015a; Fyshe et al., 2015). In this paper, we investigate the ability of non-negativity constraints to learn more accurate KG embeddings with good interpretability."
  }, {
    "heading": "3 Our Approach",
    "text": "This section presents our approach. We first introduce a basic embedding technique to model triples in a given KG (§ 3.1). Then we discuss the nonnegativity constraints over entity representations (§ 3.2) and the approximate entailment constraints over relation representations (§ 3.3). And finally we present the overall model (§ 3.4)."
  }, {
    "heading": "3.1 A Basic Embedding Model",
    "text": "We choose ComplEx (Trouillon et al., 2016) as our basic embedding model, since it is simple and efficient, achieving state-of-the-art predictive performance. Specifically, suppose we are given a KG containing a set of triples O = {(ei, rk, ej)}, with each triple composed of two entities ei, ej ∈ E and their relation rk ∈ R. Here E is the set of entities and R the set of relations. ComplEx then represents each entity e ∈ E as a complex-valued vector e∈ Cd, and each relation r ∈ R a complex-valued vector r ∈ Cd, where d is the dimensionality of the embedding space. Each x ∈ Cd consists of a real vector component Re(x) and an imaginary vector component Im(x), i.e., x = Re(x) + iIm(x). For any given triple (ei, rk, ej) ∈ E ×R× E , a multilinear dot product is used to score that triple, i.e.,\nφ(ei, rk, ej) , Re(〈ei, rk, ēj〉) , Re( ∑\n` [ei]`[rk]`[ēj ]`), (1)\nwhere ei, rk, ej ∈ Cd are the vectorial representations associated with ei, rk, ej , respectively; ēj is\nthe conjugate of ej ; [·]` is the `-th entry of a vector; and Re(·) means taking the real part of a complex value. Triples with higher φ(·, ·, ·) scores are more likely to be true. Owing to the asymmetry of this scoring function, i.e., φ(ei, rk, ej) 6= φ(ej , rk, ei), ComplEx can effectively handle asymmetric relations (Trouillon et al., 2016)."
  }, {
    "heading": "3.2 Non-negativity of Entity Representations",
    "text": "On top of the basic ComplEx model, we further require entities to have non-negative (and bounded) vectorial representations. In fact, these distributed representations can be taken as feature vectors for entities, with latent semantics encoded in different dimensions. In ComplEx, as well as most (if not all) previous approaches, there is no limitation on the range of such feature values, which means that both positive and negative properties of an entity can be encoded in its representation. However, as pointed out by Murphy et al. (2012), it would be uneconomical to store all negative properties of an entity or a concept. For instance, to describe cats (a concept), people usually use positive properties such as cats are mammals, cats eat fishes, and cats have four legs, but hardly ever negative properties like cats are not vehicles, cats do not have wheels, or cats are not used for communication.\nBased on such intuition, this paper proposes to impose non-negativity constraints on entity representations, by using which only positive properties will be stored in these representations. To better compare different entities on the same scale, we further require entity representations to stay within the hypercube of [0, 1]d, as approximately Boolean embeddings (Kruszewski et al., 2015), i.e.,\n0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E , (2)\nwhere e ∈ Cd is the representation for entity e ∈ E , with its real and imaginary components denoted by Re(e), Im(e) ∈ Rd; 0 and 1 are d-dimensional vectors with all their entries being 0 or 1; and≥,≤ ,= denote the entry-wise comparisons throughout the paper whenever applicable. As shown by Lee and Seung (1999), non-negativity, in most cases, will further induce sparsity and interpretability."
  }, {
    "heading": "3.3 Approximate Entailment for Relations",
    "text": "Besides the non-negativity constraints over entity representations, we also study approximate entailment constraints over relation representations. By approximate entailment, we mean an ordered pair\nof relations that the former approximately entails the latter, e.g., BornInCountry and Nationality, stating that a person born in a country is very likely, but not necessarily, to have a nationality of that country. Each such relation pair is associated with a weight to indicate the confidence level of entailment. A larger weight stands for a higher level of confidence. We denote by rp\nλ−→ rq the approximate entailment between relations rp and rq, with confidence level λ. This kind of entailment can be derived automatically from a KG by modern rule mining systems (Galárraga et al., 2015). Let T denote the set of all such approximate entailments derived beforehand.\nBefore diving into approximate entailment, we first explore the modeling of strict entailment, i.e., entailment with infinite confidence level λ = +∞. The strict entailment rp → rq states that if relation rp holds then relation rq must also hold. This entailment can be roughly modelled by requiring\nφ(ei, rp, ej) ≤ φ(ei, rq, ej), ∀ei, ej ∈ E , (3)\nwhere φ(·, ·, ·) is the score for a triple predicted by the embedding model, defined by Eq. (1). Eq. (3) can be interpreted as follows: for any two entities ei and ej , if (ei, rp, ej) is a true fact with a high score φ(ei, rp, ej), then the triple (ei, rq, ej) with an even higher score should also be predicted as a true fact by the embedding model. Note that given the non-negativity constraints defined by Eq. (2), a sufficient condition for Eq. (3) to hold, is to further impose\nRe(rp) ≤ Re(rq), Im(rp) = Im(rq), (4)\nwhere rp and rq are the complex-valued representations for rp and rq respectively, with the real and imaginary components denoted by Re(·), Im(·) ∈ Rd. That means, when the constraints of Eq. (4) (along with those of Eq. (2)) are satisfied, the requirement of Eq. (3) (or in other words rp → rq) will always hold. We provide a proof of sufficiency as supplementary material.\nNext we examine the modeling of approximate entailment. To this end, we further introduce the confidence level λ and allow slackness in Eq. (4), which yields\nλ ( Re(rp)− Re(rq) ) ≤ α, (5)\nλ ( Im(rp)− Im(rq) )2 ≤ β. (6) Here α,β ≥ 0 are slack variables, and (·)2 means an entry-wise operation. Entailments with higher\nconfidence levels show less tolerance for violating the constraints. When λ = +∞, Eqs. (5) – (6) degenerate to Eq. (4). The above analysis indicates that our approach can model entailment simply by imposing constraints over relation representations, without traversing all possible (ei, ej) entity pairs (i.e., grounding). In addition, different confidence levels are encoded in the constraints, making our approach moderately tolerant of uncertainty."
  }, {
    "heading": "3.4 The Overall Model",
    "text": "Finally, we combine together the basic embedding model of ComplEx, the non-negativity constraints on entity representations, and the approximate entailment constraints over relation representations. The overall model is presented as follows:\nmin Θ,{α,β} ∑ D+∪D− log ( 1 + exp(−yijkφ(ei, rk, ej)) ) + µ ∑ T 1>(α + β) + η‖Θ‖22,\ns.t. λ ( Re(rp)− Re(rq) ) ≤ α,\nλ ( Im(rp)− Im(rq) )2 ≤ β, α,β ≥ 0, ∀rp\nλ−→ rq ∈ T , 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E . (7)\nHere, Θ , {e : e ∈ E} ∪ {r : r ∈ R} is the set of all entity and relation representations;D+ andD− are the sets of positive and negative training triples respectively; a positive triple is directly observed in the KG, i.e., (ei, rk, ej) ∈ O; a negative triple can be generated by randomly corrupting the head or the tail entity of a positive triple, i.e., (e′i, rk, ej) or (ei, rk, e′j); yijk = ±1 is the label (positive or negative) of triple (ei, rk, ej). In this optimization, the first term of the objective function is a typical logistic loss, which enforces triples to have scores close to their labels. The second term is the sum of slack variables in the approximate entailment constraints, with a penalty coefficient µ ≥ 0. The motivation is, although we allow slackness in those constraints we hope the total slackness to be small, so that the constraints can be better satisfied. The last term is L2 regularization to avoid over-fitting, and η ≥ 0 is the regularization coefficient.\nTo solve this optimization problem, the approximate entailment constraints (as well as the corresponding slack variables) are converted into penalty terms and added to the objective function, while the non-negativity constraints remain as they are. As such, the optimization problem of Eq. (7) can\nbe rewritten as:\nmin Θ ∑ D+∪D− log ( 1 + exp(−yijkφ(ei, rk, ej)) ) + µ ∑ T λ1> [ Re(rp)−Re(rq) ] +\n+ µ ∑ T λ1> ( Im(rp)−Im(rq) )2 + η‖Θ‖22,\ns.t. 0 ≤ Re(e), Im(e) ≤ 1, ∀e ∈ E , (8)\nwhere [x]+ = max(0,x) with max(·, ·) being an entry-wise operation. The equivalence between Eq. (7) and Eq. (8) is shown in the supplementary material. We use SGD in mini-batch mode as our optimizer, with AdaGrad (Duchi et al., 2011) to tune the learning rate. After each gradient descent step, we project (by truncation) real and imaginary components of entity representations into the hypercube of [0, 1]d, to satisfy the non-negativity constraints.\nWhile favouring a better structuring of the embedding space, imposing the additional constraints will not substantially increase model complexity. Our approach has a space complexity of O(nd + md), which is the same as that of ComplEx. Here, n is the number of entities, m the number of relations, and O(nd+md) to store a d-dimensional complex-valued vector for each entity and each relation. The time complexity (per iteration) of our approach isO(sd+td+n̄d), where s is the average number of triples in a mini-batch, n̄ the average number of entities in a mini-batch, and t the total number of approximate entailments in T . O(sd) is to handle triples in a mini-batch, O(td) penalty terms introduced by the approximate entailments, and O(n̄d) further the non-negativity constraints on entity representations. Usually there are much fewer entailments than triples, i.e., t s, and also n̄ ≤ 2s.1 So the time complexity of our approach is on a par withO(sd), i.e., the time complexity of ComplEx."
  }, {
    "heading": "4 Experiments and Results",
    "text": "This section presents our experiments and results. We first introduce the datasets used in our experiments (§ 4.1). Then we empirically evaluate our approach in the link prediction task (§ 4.2). After that, we conduct extensive analysis on both entity representations (§ 4.3) and relation representations (§ 4.4) to show the interpretability of our model.\n1There will be at most 2s entities contained in s triples.\nCode and data used in the experiments are available at https://github.com/iieir-km/ ComplEx-NNE_AER."
  }, {
    "heading": "4.1 Datasets",
    "text": "The first two datasets we used are WN18 and FB15K, released by Bordes et al. (2013).2 WN18 is a subset of WordNet containing 18 relations and 40,943 entities, and FB15K a subset of Freebase containing 1,345 relations and 14,951 entities. We create our third dataset from the mapping-based objects of core DBpedia.3 We eliminate relations not included within the DBpedia ontology such as HomePage and Logo, and discard entities appearing less than 20 times. The final dataset, referred to as DB100K, is composed of 470 relations and 99,604 entities. Triples on each datasets are further divided into training, validation, and test sets, used for model training, hyperparameter tuning, and evaluation respectively. We follow the original split for WN18 and FB15K, and draw a split of 597,572/ 50,000/50,000 triples for DB100K.\nWe further use AMIE+ (Galárraga et al., 2015)4 to extract approximate entailments automatically from the training set of each dataset. As suggested by Guo et al. (2018), we consider entailments with PCA confidence higher than 0.8.5 As such, we extract 17 approximate entailments from WN18, 535 from FB15K, and 56 from DB100K. Table 1 gives some examples of these approximate entailments, along with their confidence levels. Table 2 further summarizes the statistics of the datasets."
  }, {
    "heading": "4.2 Link Prediction",
    "text": "We first evaluate our approach in the link prediction task, which aims to predict a triple (ei, rk, ej) with ei or ej missing, i.e., predict ei given (rk, ej) or predict ej given (ei, rk).\nEvaluation Protocol: We follow the protocol introduced by Bordes et al. (2013). For each test triple (ei, rk, ej), we replace its head entity ei with every entity e′i ∈ E , and calculate a score for the corrupted triple (e′i, rk, ej), e.g., φ(e ′ i, rk, ej) defined by Eq. (1). Then we sort these scores in de2https://everest.hds.utc.fr/doku.php? id=en:smemlj12 3http://downloads.dbpedia.org/2016-10/ core/ 4https://www.mpi-inf.mpg.de/departmen ts/databases-and-information-systems/res earch/yago-naga/amie/\n5PCA confidence is the confidence under the partial completeness assumption. See (Galárraga et al., 2015) for details.\nscending order, and get the rank of the correct entity ei. During ranking, we remove corrupted triples that already exist in either the training, validation, or test set, i.e., the filtered setting as described in (Bordes et al., 2013). This whole procedure is repeated while replacing the tail entity ej . We report on the test set the mean reciprocal rank (MRR) and the proportion of correct entities ranked in the top n (HITS@N), with n = 1, 3, 10.\nComparison Settings: We compare the performance of our approach against a variety of KG embedding models developed in recent years. These models can be categorized into three groups:\n• Simple embedding models that utilize triples alone without integrating extra information, including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), and ANALOGY (Liu et al., 2017). Our approach is developed on the basis of ComplEx.\n• Other extensions of ComplEx that integrate logical background knowledge in addition to triples, including RUGE (Guo et al., 2018) and ComplExR (Minervini et al., 2017a). The former requires grounding of first-order logic rules. The latter is restricted to relation equiv-\nalence and inversion, and assigns an identical confidence level to all different rules.\n• Latest developments or implementations that achieve current state-of-the-art performance reported on the benchmarks of WN18 and FB15K, including R-GCN (Schlichtkrull et al., 2017), ConvE (Dettmers et al., 2018), and Single DistMult (Kadlec et al., 2017).6 The first two are built based on neural network architectures, which are, by nature, more complicated than the simple models. The last one is a re-implementation of DistMult, generating 1000 to 2000 negative training examples per positive one, which leads to better performance but requires significantly longer training time.\nWe further evaluate our approach in two different settings: (i) ComplEx-NNE that imposes only the Non-Negativity constraints on Entity representations, i.e., optimization Eq. (8) with µ = 0; and (ii) ComplEx-NNE+AER that further imposes the Approximate Entailment constraints over Relation representations besides those non-negativity ones, i.e., optimization Eq. (8) with µ > 0.\nImplementation Details: We compare our approach against all the three groups of baselines on the benchmarks of WN18 and FB15K. We directly report their original results on these two datasets to avoid re-implementation bias. On DB100K, the newly created dataset, we take the first two groups of baselines, i.e., those simple embedding models and ComplEx extensions with logical background knowledge incorporated. We do not use the third group of baselines due to efficiency and complexity issues. We use the code provided by Trouillon et al. (2016)7 for TransE, DistMult, and ComplEx, and the code released by their authors for ANALOGY8 and RUGE9. We re-implement HolE and ComplExR so that all the baselines (as well as our approach) share the same optimization mode, i.e., SGD with AdaGrad and gradient normalization, to facilitate a fair comparison.10 We follow Trouillon et al. (2016) to adopt a ranking loss for TransE and a logistic loss for all the other methods.\n6We do not consider Ensemble DistMult (Dettmers et al., 2018) which combines several different models together, to facilitate a fair comparison.\n7https://github.com/ttrouill/complex 8https://github.com/quark0/ANALOGY 9https://github.com/iieir-km/RUGE\n10An exception here is that ANALOGY uses asynchronous SGD with AdaGrad (Liu et al., 2017).\nAmong those baselines, RUGE and ComplExR require additional logical background knowledge. RUGE makes use of soft rules, which are extracted by AMIE+ from the training sets. As suggested by Guo et al. (2018), length-1 and length-2 rules with PCA confidence higher than 0.8 are utilized. Note that our approach also makes use of AMIE+ rules with PCA confidence higher than 0.8. But it only considers entailments between a pair of relations, i.e., length-1 rules. ComplExR takes into account equivalence and inversion between relations. We derive such axioms directly from our approximate entailments. If rp λ1−→ rq and rq λ2−→ rp with λ1, λ2 > 0.8, we think relations rp and rq are equivalent. And similarly, if r−1p λ1−→ rq and r−1q λ2−→ rp with\nλ1, λ2 > 0.8, we consider rp as an inverse of rq. For all the methods, we create 100 mini-batches on each dataset, and conduct a grid search to find hyperparameters that maximize MRR on the validation set, with at most 1000 iterations over the training set. Specifically, we tune the embedding size d ∈ {100, 150, 200}, the L2 regularization coefficient η ∈ {0.001, 0.003, 0.01, 0.03, 0.1}, the ratio of negative over positive training examples α ∈ {2, 10}, and the initial learning rate γ ∈ {0.01, 0.05, 0.1, 0.5, 1.0}. For TransE, we tune the margin of the ranking loss δ ∈ {0.1, 0.2, 0.5, 1, 2, 5, 10}. Other hyperparameters of ANALOGY and RUGE are set or tuned according to the default settings suggested by their authors (Liu et al., 2017; Guo et al., 2018). After getting the best ComplEx model, we tune the relation constraint penalty of our approach ComplEx-NNE+AER (µ in Eq. (8)) in the range of {10−5, 10−4, · · · , 104, 105}, with all its other hyperparameters fixed to their optimal configurations. We then directly set µ = 0 to get the optimal ComplEx-NNE model. The weight of soft constraints in ComplExR is tuned in the same range as µ. The optimal configurations for our approach are: d = 200, η = 0.03, α = 10, γ = 1.0, µ = 10 on WN18; d = 200, η=0.01, α=10, γ = 0.5, µ = 10−3 on FB15K; and d = 150, η = 0.03, α = 10, γ = 0.1, µ = 10−5 on DB100K.\nExperimental Results: Table 3 presents the results on the test sets of WN18 and FB15K, where the results for the baselines are taken directly from\nprevious literature. Table 4 further provides the results on the test set of DB100K, with all the methods tuned and tested in (almost) the same setting. On all the datasets, we test statistical significance of the improvements achieved by ComplEx-NNE/ ComplEx-NNE+AER over ComplEx, by using a paired t-test. The reciprocal rank or HITS@N value with n = 1, 3, 10 for each test triple is used as paired data. The symbol “∗” indicates a significance level of p < 0.05.\nThe results demonstrate that imposing the nonnegativity and approximate entailment constraints indeed improves KG embedding. ComplEx-NNE and ComplEx-NNE+AER perform better than (or at least equally well as) ComplEx in almost all the metrics on all the three datasets, and most of the improvements are statistically significant (except those on WN18). More interestingly, just by introducing these simple constraints, ComplEx-NNE+ AER can beat very strong baselines, including the best performing basic models like ANALOGY, those previous extensions of ComplEx like RUGE or ComplExR, and even the complicated developments or implementations like ConvE or Single DistMult. This demonstrates the superiority of our approach."
  }, {
    "heading": "4.3 Analysis on Entity Representations",
    "text": "This section inspects how the structure of the entity embedding space changes when the constraints are imposed. We first provide the visualization of entity representations on DB100K. On this dataset each entity is associated with a single type label.11 We pick 4 types reptile, wine region, species, and programming language, and randomly select 30 entities from each type. Figure 1 visualizes the representations of these entities learned by ComplEx and ComplEx-NNE+AER (real components only), with the optimal configurations determined by link prediction (see § 4.2 for details, applicable to all analysis hereafter). During the visualization, we normalize the real component of each entity by [x̃]`= [x]`−min(x) max(x)−min(x) , where min(x) or max(x) is the minimum or maximum entry of x respectively. We observe that after imposing the non-negativity constraints, ComplEx-NNE+AER indeed obtains compact and interpretable representations for entities. Each entity is represented by only a relatively small number of “active” dimensions. And entities\n11http://downloads.dbpedia.org/2016-10/ core-i18n/en/instance_types_wkd_uris_en. ttl.bz2\nwith the same type tend to activate the same set of dimensions, while entities with different types often get clearly different dimensions activated.\nThen we investigate the semantic purity of these dimensions. Specifically, we collect the representations of all the entities on DB100K (real components only). For each dimension of these representations, top K percent of entities with the highest activation values on this dimension are picked. We can calculate the entropy of the type distribution of the entities selected. This entropy reflects diversity of entity types, or in other words, semantic purity. If all the K percent of entities have the same type, we will get the lowest entropy of zero (the highest semantic purity). On the contrary, if each of them has a distinct type, we will get the highest entropy (the lowest semantic purity). Figure 2 shows the average entropy over all dimensions of entity representations (real components only) learned by ComplEx, ComplEx-NNE, and ComplEx-NNE+\nAER, as K varies. We can see that after imposing the non-negativity constraints, ComplEx-NNE and ComplEx-NNE+AER can learn entity representations with latent dimensions of consistently higher semantic purity. We have conducted the same analyses on imaginary components of entity representations, and observed similar phenomena. The results are given as supplementary material."
  }, {
    "heading": "4.4 Analysis on Relation Representations",
    "text": "This section further provides a visual inspection of the relation embedding space when the constraints are imposed. To this end, we group relation pairs involved in the DB100K entailment constraints into 3 classes: equivalence, inversion, and others.12 We choose 2 pairs of relations from each class, and visualize these relation representations learned by ComplEx-NNE+AER in Figure 3, where for each relation we randomly pick 5 dimensions from both its real and imaginary components. By imposing the approximate entailment constraints, these relation representations can encode logical regularities quite well. Pairs of relations from the first class (equivalence) tend to have identical representations rp ≈ rq, those from the second class (inversion) complex conjugate representations rp ≈ r̄q; and the others representations that Re(rp) ≤ Re(rq) and Im(rp) ≈ Im(rq).\n12Equivalence and inversion are detected using heuristics introduced in § 4.2 (implementation details). See the supplementary material for detailed properties of these three classes."
  }, {
    "heading": "5 Conclusion",
    "text": "This paper investigates the potential of using very simple constraints to improve KG embedding. Two types of constraints have been studied: (i) the non-negativity constraints to learn compact, interpretable entity representations, and (ii) the approximate entailment constraints to further encode logical regularities into relation representations. Such constraints impose prior beliefs upon the structure of the embedding space, and will not significantly increase the space or time complexity. Experimental results on benchmark KGs demonstrate that our method is simple yet surprisingly effective, showing significant and consistent improvements over strong baselines. The constraints indeed improve model interpretability, yielding a substantially increased structuring of the embedding space."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank all the anonymous reviewers for their insightful and valuable suggestions, which help to improve the quality of this paper. This work is supported by the National Key Research and Development Program of China (No. 2016QY03D0503) and the Fundamental Theory and Cutting Edge Technology Research Program of the Institute of Information Engineering, Chinese Academy of Sciences (No. Y7Z0261101)."
  }],
  "year": 2018,
  "references": [{
    "title": "Freebase: A collaboratively created graph database for structuring human knowledge",
    "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."],
    "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
    "year": 2008
  }, {
    "title": "A semantic matching energy function for learning with multi-relational data",
    "authors": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."],
    "venue": "Machine Learning 94(2):233–259.",
    "year": 2014
  }, {
    "title": "Translating embeddings for modeling multirelational data",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcı́aDurán", "Jason Weston", "Oksana Yakhnenko"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Learning structured embeddings of knowledge bases",
    "authors": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio."],
    "venue": "Proceedings of the 25th AAAI Conference on Artificial Intelligence. pages 301–306.",
    "year": 2011
  }, {
    "title": "Typed tensor decompo",
    "authors": ["Christopher Meek"],
    "year": 2014
  }, {
    "title": "Lifted rule injection for relation",
    "authors": ["Riedel"],
    "year": 2016
  }, {
    "title": "Discovery and Data Mining",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer"],
    "year": 2011
  }, {
    "title": "Semantically smooth knowledge",
    "authors": ["Li Guo"],
    "year": 2015
  }, {
    "title": "Jointly embedding knowledge graphs",
    "authors": ["Li Guo"],
    "year": 2016
  }, {
    "title": "Knowledge graph embedding with",
    "authors": ["Li Guo"],
    "year": 2018
  }, {
    "title": "Knowledge-based weak supervision for information extraction of overlapping relations",
    "authors": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for",
    "year": 2011
  }, {
    "title": "A latent factor model for highly multi-relational data",
    "authors": ["Rodolphe Jenatton", "Nicolas L. Roux", "Antoine Bordes", "Guillaume R. Obozinski."],
    "venue": "Advances in Neural Information Processing Systems. pages 3167–3175.",
    "year": 2012
  }, {
    "title": "Knowledge graph embedding via dynamic mapping matrix",
    "authors": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "Knowledge base completion: Baselines strike back",
    "authors": ["Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst."],
    "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP. pages 69–74.",
    "year": 2017
  }, {
    "title": "Deriving Boolean structures from distributional vectors",
    "authors": ["German Kruszewski", "Denis Paperno", "Marco Baroni."],
    "venue": "Transactions of the Association for Computational Linguistics 3:375–388.",
    "year": 2015
  }, {
    "title": "Learning the parts of objects by non-negative matrix factorization",
    "authors": ["Daniel D. Lee", "H. Sebastian Seung."],
    "venue": "Nature 401:788–791.",
    "year": 1999
  }, {
    "title": "DBpedia: A largescale, multilingual knowledge base",
    "authors": ["Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "Sören Auer"],
    "year": 2015
  }, {
    "title": "Modeling relation paths for representation learning of knowledge bases",
    "authors": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-",
    "year": 2015
  }, {
    "title": "Learning entity and relation embeddings for knowledge graph completion",
    "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."],
    "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence. pages 2181–2187.",
    "year": 2015
  }, {
    "title": "Analogical inference for multi-relational embeddings",
    "authors": ["Hanxiao Liu", "Yuexin Wu", "Yiming Yang."],
    "venue": "Proceedings of the 34th International Conference on Machine Learning. pages 2168–2178.",
    "year": 2017
  }, {
    "title": "Online learning of interpretable word embeddings",
    "authors": ["Hongyin Luo", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1687–1692.",
    "year": 2015
  }, {
    "title": "Context-dependent knowledge graph embedding",
    "authors": ["Yuanfei Luo", "Quan Wang", "Bin Wang", "Li Guo."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1656–1661.",
    "year": 2015
  }, {
    "title": "Regularizing knowledge graph embeddings via equivalence and inversion axioms",
    "authors": ["Pasquale Minervini", "Luca Costabello", "Emir Muñoz", "Vı́t Nováček", "Pierre-Yves Vandenbussche"],
    "venue": "In Joint European Conference on Machine Learning and Knowledge",
    "year": 2017
  }, {
    "title": "Adversarial sets for regularising neural link predictors",
    "authors": ["Pasquale Minervini", "Thomas Demeester", "Tim Rocktäschel", "Sebastian Riedel."],
    "venue": "Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence.",
    "year": 2017
  }, {
    "title": "Learning effective and interpretable semantic models using non-negative sparse embedding",
    "authors": ["Brian Murphy", "Partha Talukdar", "Tom Mitchell."],
    "venue": "Proceedings of COLING 2012. pages 1933–1950.",
    "year": 2012
  }, {
    "title": "Compositional vector space models for knowledge base completion",
    "authors": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International",
    "year": 2015
  }, {
    "title": "A review of relational machine learning for knowledge graphs",
    "authors": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich."],
    "venue": "Proceedings of the IEEE 104(1):11–33.",
    "year": 2016
  }, {
    "title": "Holographic embeddings of knowledge graphs",
    "authors": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio."],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence. pages 1955–1961.",
    "year": 2016
  }, {
    "title": "A three-way model for collective learning on multi-relational data",
    "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning. pages 809–816.",
    "year": 2011
  }, {
    "title": "Injecting logical background knowledge into embeddings for relation extraction",
    "authors": ["Tim Rocktäschel", "Sameer Singh", "Sebastian Riedel."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational",
    "year": 2015
  }, {
    "title": "Modeling relational data with graph convolutional networks",
    "authors": ["Michael Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling."],
    "venue": "arXiv:1703.06103 .",
    "year": 2017
  }, {
    "title": "ProjE: Embedding projection for knowledge graph completion",
    "authors": ["Baoxu Shi", "Tim Weninger."],
    "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence. pages 1236–1242.",
    "year": 2017
  }, {
    "title": "Reasoning with neural tensor networks for knowledge base completion",
    "authors": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."],
    "venue": "Advances in Neural Information Processing Systems. pages 926–934.",
    "year": 2013
  }, {
    "title": "Complex embeddings for simple link prediction",
    "authors": ["Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Eric Gaussier", "Guillaume Bouchard."],
    "venue": "Proceedings of the 33rd International Conference on Machine Learning. pages 2071–2080.",
    "year": 2016
  }, {
    "title": "Knowledge graph embedding: A survey of approaches and applications",
    "authors": ["Quan Wang", "Zhendong Mao", "Bin Wang", "Li Guo."],
    "venue": "IEEE Transactions on Knowledge and Data Engineering 29(12):2724– 2743.",
    "year": 2017
  }, {
    "title": "Knowledge base completion using embeddings and rules",
    "authors": ["Quan Wang", "Bin Wang", "Li Guo."],
    "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence. pages 1859–1865.",
    "year": 2015
  }, {
    "title": "Knowledge graph embedding by translating on hyperplanes",
    "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."],
    "venue": "Proceedings of the 28th AAAI Conference on Artificial Intelligence. pages 1112–1119.",
    "year": 2014
  }, {
    "title": "Learning to identify the best contexts for knowledge-based WSD",
    "authors": ["Evgenia Wasserman-Pritsker", "William W. Cohen", "Einat Minkov."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1662–1667.",
    "year": 2015
  }, {
    "title": "TransG: A generative model for knowledge graph embedding",
    "authors": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 2316–2325.",
    "year": 2016
  }, {
    "title": "SSP: Semantic space projection for knowledge graph embedding with text descriptions",
    "authors": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu."],
    "venue": "Proceedings of the 31st AAAI Conference on Artificial Intelligence. pages 3104–3110.",
    "year": 2017
  }, {
    "title": "Representation learning of knowledge graphs with entity descriptions",
    "authors": ["Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun."],
    "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence. pages 2659–2665.",
    "year": 2016
  }, {
    "title": "Representation learning of knowledge graphs with hierarchical types",
    "authors": ["Ruobing Xie", "Zhiyuan Liu", "Maosong Sun."],
    "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence. pages 2965–2971.",
    "year": 2016
  }, {
    "title": "Leveraging knowledge bases in LSTMs for improving machine reading",
    "authors": ["Bishan Yang", "Tom Mitchell."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pages 1436–1446.",
    "year": 2017
  }, {
    "title": "Embedding entities and relations for learning and inference in knowledge bases",
    "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Aligning knowledge and text embeddings by entity descriptions",
    "authors": ["Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 267–272.",
    "year": 2015
  }],
  "id": "SP:75cefc96745eb0cb6be53556ba129f2bf2a7d898",
  "authors": [{
    "name": "Boyang Ding",
    "affiliations": []
  }, {
    "name": "Quan Wang",
    "affiliations": []
  }, {
    "name": "Bin Wang",
    "affiliations": []
  }, {
    "name": "Li Guo",
    "affiliations": []
  }],
  "abstractText": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/i ieir-km/ComplEx-NNE_AER.",
  "title": "Improving Knowledge Graph Embedding Using Simple Constraints"
}