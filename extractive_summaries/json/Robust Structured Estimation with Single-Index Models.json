{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In machine learning and statistics, a linear model of the form y = ⟨θ∗,x⟩+ϵ is widely used to find the relationship between feature and response, which has gained overwhelming popularity for a very long time. Here y ∈ R and x ∈ Rp is the pair of observed response and feature/measurement vector, ϵ is a zero-mean noise, and θ∗ ∈ Rp is the unknown parameter to be estimated. The simplicity of linear model leads to its great interpretability and computational efficiency, which are often favored in practical applications. On theoretical side, even in high-dimensional regime where sample size is smaller than the problem dimension p, strong statistical guarantees have been established under mild assumptions for various estimators, such as Lasso (Tibshirani, 1996) and Dantzig selector (Candes & Tao, 2007). Despite its attractive merits, one main drawback of linear models is the stringent assumption of linear relationship between x and y, which may fail to hold in com-\n1Department of Computer Science & Engineering, University of Minnesota-Twin Cities, Minnesota, USA. Correspondence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nplicated scenarios. To introduce more flexibility, one option is to consider the general single-index models (SIMs) (Ichimura, 1993; Horowitz & Hardle, 1996),\nE[y|x] = f∗(⟨θ∗,x⟩) , (1)\nwhere f∗ : R 7→ R is an unknown univariate transfer function (a.k.a. link function). This class of models enjoys rich modeling power in the sense that it encompasses several useful models as special cases, which are briefly described below:\n• One-bit Compressed Sensing: In one-bit compressed sensing (1-bit CS) (Boufounos & Baraniuk, 2008; Plan & Vershynin, 2013), the response y is restricted to be binary, i.e., y ∈ {+1,−1}, and the range of transfer function f∗ is [−1, 1]. Given the measurement vector x, one can generate y from the Bernoulli model,\ny + 1\n2 ∼ Ber\n( f∗(⟨θ∗,x⟩) + 1\n2\n) . (2)\nIn the noiseless case, f∗(z) = sign(z) and y always reflects the true sign of ⟨θ∗,x⟩, while y can be incorrect for other f∗ whose shape determines the noise level in some way. • Generalized Linear Models: In generalized linear models (GLMs) (McCullagh, 1984), the transfer function is assumed to be monotonically increasing and conditional distribution of y|x belongs to exponential family. Different choices of f∗ give rise to different members in GLMs. If f∗ is identity function f∗(z) = z, one has the simple linear models, while the sigmoid function f∗(z) = 11+e−z results in the logistic model for binary classification. In this work, however, we have no access to exact f∗ other than knowing it is monotonic. • Noise in Monotone Transfer: Instead of having the general expectation form of y as GLMs, one could directly introduce the noise inside monotone transfer f̃ to model the randomness of y (Plan et al., 2016),\ny = f̃ (⟨θ∗,x⟩+ ϵ) . (3)\nIn this setting, the transfer function f̃ is slightly different from the f∗ in (1), which are related by f∗(z) = Eϵ[f̃(z + ϵ)|z].\nA key advantage of SIM is its robustness. First, allowing unknown f∗ prevents the mis-specification of transfer function, which could otherwise lead to a poor estimate of θ∗. Secondly, the model in (1) makes minimal assumption on the distribution of y, thus being able to tolerate potentially heavy-tailed noise.\nIn order to estimate θ∗, we are given n measurements of (x, y) ∈ Rp × R, denoted by {(xi, yi)}ni=1. In this work, we focus on the n < p regime. In such high-dimensional setting, the recovery of θ∗ is quite challenging as the problem is ill-posed even when f∗ is given. Over the last decade, substantial progress has been made to address the challenge by exploiting the apriori structure of parameter θ∗, like sparsity (Tibshirani, 1996). For simple linear models or GLMs with known transfer, extensive studies have shown that sparse θ∗ can be consistently estimated under mild assumptions, with much lower sample complexity than p (Candes & Tao, 2007; Wainwright, 2009; Bickel et al., 2009; Kakade et al., 2010; Negahban et al., 2012; Yang et al., 2016). Recently the notion of structure has been suitably generalized beyond the unstructured sparsity (Bach et al., 2012), and Gaussian width (Gordon, 1985) has emerged as a useful measure to characterize the structural complexity which further determines the recovery guarantee of θ∗ (Chandrasekaran et al., 2012; Rao et al., 2012; Oymak et al., 2013; Amelunxen et al., 2014; Banerjee et al., 2014; Chatterjee et al., 2014; Vershynin, 2015; Tropp, 2015; Chen & Banerjee, 2016).\nIn the absence of exact f∗, though 1-bit CS and related variants were well-studied in recent years (Boufounos & Baraniuk, 2008; Jacques et al., 2013; Plan & Vershynin, 2013; Gopi et al., 2013; Zhang et al., 2014; Chen & Banerjee, 2015a; Zhu & Gu, 2015; Yi et al., 2015; Slawski & Li, 2015; Li, 2016; Slawski & Li, 2016), the exploration of general SIMs or the cases with monotone transfers is relatively limited, especially in the high-dimensional regime. Kalai & Sastry (2009) and Kakade et al. (2011) investigated the low-dimensional SIMs with monotone transfers, and they proposed perceptron-type algorithms to estimate both f∗ and θ∗, with provable guarantees on prediction error. In high dimension, general SIMs were studied by Alquier & Biau (2013) and Radchenko (2015), in which only unstructured sparsity of θ∗ is considered. The algorithm developed in (Alquier & Biau, 2013) relies on reversible jump MCMC, which could be slow. In Radchenko (2015), a path fitting algorithm is designed to recover f∗ and θ∗, but only asymptotic guarantees are provided. Ganti et al. (2015) considered the high-dimensional setting with monotone transfer, and their iterative algorithm is based on non-convex optimization, for which it is hard to establish the convergence. Besides, the prediction error bound they derived is also weak (in the sense that it\nis even worse than the initialization of the algorithm). Recently Oymak & Soltanolkotabi (2016) proposed a constrained least-squares method to estimate θ∗, with recovery error characterized by Gaussian width and related quantities. Though their analysis considered the general structure of θ∗, it only holds for noiseless setting where y = f(⟨θ∗,x⟩). General structure of θ∗ was also explored in Vershynin (2015) and Plan et al. (2016). Other types of statistical guarantees for high-dimensional SIMs is also available, such as support recovery of θ∗ in Neykov et al. (2016). It is worth noting that all the aforementioned statistical analyses rely on sub-Gaussian noise or the transfer function being bounded or Lipschitz, which indicates that none of the results can immediately hold for heavy-tailed noise (or without Lipschitzness and boundedness).\nIn this paper, we focus on the parameter estimation of θ∗ instead of the prediction of y given new x. In particular, we propose two families of generalized estimators, constrained and regularized, for model (1) under Gaussian measurement. The parameter θ∗ is assumed to possess certain lowcomplexity structure, which can be either captured by a constraint θ∗ ∈ K or a norm regularization term ∥θ∗∥. Our general approach is inspired by U -statistics and the advances in 1-bit CS, and subsumes several existing 1-bit CS algorithms as special cases. Similar to those algorithms, our estimator is simple and often admits closed-form solutions. Regarding the recovery analysis, there are two appealing aspects. First our results work for general structure, with error bound characterized by Gaussian width and some other easy-to-compute geometric measures. Instantiating our results with specific structure of θ∗ recovers previously established error bounds for 1-bit CS (Zhang et al., 2014; Chen & Banerjee, 2015a), which are sharper than those yielded by the general analysis in Plan & Vershynin (2013). Second, our analysis works with limited assumptions on the condition distribution of y. In particular, our estimator is robust to heavy-tailed noise and permit unbounded transfer functions f∗ as well as non-Lipschitz ones. At the heart of our analysis is the generic chaining method (Talagrand, 2014), an advanced tool in probability theory, which has been successfully applied to sparse recovery (Koltchinskii, 2011) and dimensionality reduction (Dirksen, 2016), etc. Another key ingredient in our proof is a Hoeffding-type concentration inequality for U -statistics (Lee, 1990) with sub-Gaussian tails, which is less known yet generalizes the popular one for bounded U -statistics (Hoeffding, 1963). Apart from 1-bit CS, we particularly investigate the model (3), for which the generalized estimator is specialized in a novel way. The resulting estimator better leverages the monotonicity of the transfer function, which is also demonstrated through experiments. For the ease of exposition, whenever we say “monotone”, it means “monotonically increasing” by default. Throughout the\npaper, we will use c, C,C ′, C0, C1 and so on to denote absolute constants, which may differ from context to context. Detailed proofs are deferred to the supplementary material due to page limit.\nThe rest of the paper is organized as follows. In Section 2, we introduce our estimators for SIMs along with their recovery guarantees. We also provide a few examples in 1-bit CS for illustration. Section 3 is focused on model (3), for which we instantiate the general results in a new way. Other structures of θ∗ beyond unstructured sparsity are also discussed. Section 4 provides the proof of our main results and the related lemmas. In Section 5, we complement our theoretical developments with some experiment results. The final section is dedicated to conclusions."
  }, {
    "heading": "2. Generalized Estimation for Structured Parameter",
    "text": ""
  }, {
    "heading": "2.1. Assumptions and Preliminaries",
    "text": "For the sake of identifiability, we assume w.l.o.g. that ∥θ∗∥2 = 1 throughout the paper. At the first glimpse of model (1), we may realize that it is difficult to recover θ∗ due to unknown f∗. In contrast, when f∗ is given, the recovery guarantees of θ∗ can be established under mild assumptions of x and y, such as boundedness or subGaussianity. If we know certain properties of the transfer function like the monotonicity introduced in GLMs and (3), the structure of f∗ is largely restricted, and it is tempting to expect that similar results will continue to hold. Unfortunately, we first have the following claim, which indicates that without other constraints on f∗ beyond strict monotonicity, θ∗ cannot be consistently estimated under general sub-Gaussian (or bounded) measurement, even in the noiseless setting of (3).\nClaim 1 Suppose that each element xi of x is sampled i.i.d. from Rademacher distribution, i.e., P(xi = 1) = P(xi = −1) = 0.5. Under model (3) with noise ϵ = 0, there exists a θ̄ ∈ Sp−1 together with a monotone f̄ , such that supp(θ̄) = supp(θ∗) and yi = f̄(⟨θ̄,xi⟩) for data {(xi, yi)}ni=1 with arbitrarily large sample size n, while ∥θ̄ − θ∗∥2 > δ for some constant δ.\nNow that consistent estimation of θ∗ is not possible for general sub-Gaussian measurement, it might be reasonable to focus on certain special cases. For this work, we assume that x is standard Gaussian N (0, I). For SIM (1), we additionally assume that the distribution of y depends on x only through the value of ⟨θ∗,x⟩, i.e., the distribution of y|x is fixed if ⟨θ∗,x⟩ is given (no matter what the exact x is). This assumption is quite minimal, and it turns out that the examples we provide in Section 1 all satisfy it (if noise ϵ is independent of x in (3)). The same assumption is used\nin Plan et al. (2016) as well.\nUnder the assumptions above, given m i.i.d. observations (x1, y1), . . . , (xm, ym), we define\nu ((x1, y1), . . . , (xm, ym)) = m∑ i=1 qi (y1, . . . , ym) · xi ,\n(4) where all qi : Rm 7→ R are bounded functions with |qi| ≤ 1, which are chosen along with m based on the properties of the transfer function. In Section 2.4 and 3.1, we will see examples for their choices. The vector u ∈ Rp is critical due to the key observation below.\nLemma 1 Suppose the distribution of y in model (1) depends on x through ⟨θ∗,x⟩ and we define accordingly\nbi (z1, . . . , zm;θ ∗) = (5)\nE [qi (y1, . . . , ym) |⟨θ∗,x1⟩ = z1, . . . , ⟨θ∗,xm⟩ = zm] .\nWith x being standard Gaussian N (0, I), u defined in (4) satisfies\nE [u ((x1, y1), . . . , (xm, ym))] = βθ∗ , (6)\nwhere β = ∑m i=1 E[bi (g1, . . . , gm;θ∗) · gi], and g1, . . . , gm are i.i.d. standard Gaussian.\nNote that Lemma 1 is true for all choices of qi, and the proof is given in the supplement. This lemma presents an insight towards the design of our estimator, that is, the direction of θ∗ can be approximated if we have a good sense about Eu. As we will see in the sequel, the scalar β plays a key role in the estimation error bound, which can give us clues to the choice of qi. We can assume w.l.o.g. that β ≥ 0 since we can flip the sign of each qi.\nThe recovery analysis is built on the notion of Gaussian width (Gordon, 1985), which is defined for any A ⊆ Rp as w(A) = E[supv∈A⟨g,v⟩], where g is a standard Gaussian random vector. Roughly speaking, w(A) measures the scaled width of set A averaged over each direction."
  }, {
    "heading": "2.2. Generalized Estimator",
    "text": "Inspired by Lemma 1, we define the vector û for the observed data {(xi, yi)}ni=1,\nû = (n−m)!\nn!\n∑ 1≤i1,...,im≤n i1 ̸=... ̸=im u ((xi1 , yi1), . . . , (xim , yim)) ,\n(7) which is an unbiased estimator of Eu, meaning that Eû = Eu = βθ∗. When m = 2, we essentially have\nû = 1 n(n− 1) ∑\n1≤i,j≤n i ̸=j\nu ((xi, yi), (xj , yj)) (8)\nIn fact, û can be treated as a vector version of U -statistics with order m. Given û, a naive way to estimate θ∗ is to simply normalize û, i.e., θ̂ = û/∥û∥2. In highdimensional setting, θ∗ is often structured, but the naive estimator fails to take such information into account, which would lead to large error. To incorporate the prior knowledge on θ∗, we design two types of estimator, the constrained one and the regularized one.\nConstrained Estimator: If we assume that θ∗ belongs to some structured set K ⊆ Sp−1, then the estimation of θ∗ is carried out via the constrained optimization\nθ̂ = argmin θ∈Rp\n− ⟨û,θ⟩ s.t. θ ∈ K . (9)\nHere the set K can be non-convex, as long as the optimization can be solved globally. Since the objective function is very simple, we can often end up with a global minimizer. Similar estimator has been used in Plan et al. (2016), but they only focused on specific û.\nRegularized Estimator: If we assume that the structure of θ∗ can be captured by certain norm ∥ · ∥, we may alternatively use the regularized estimator to find θ∗,\nθ̂ = argmin θ∈Rp\n− ⟨û,θ⟩+ λ∥θ∥ s.t. ∥θ∥2 ≤ 1 . (10)\nThe optimization is convex, thus the global minimum is always attained. Previously this estimator was used in 1-bit CS scenario with L1 norm (Zhang et al., 2014)."
  }, {
    "heading": "2.3. Recovery Analysis",
    "text": "Regarding the constrained estimator, the recovery of θ∗ relies on the geometry of θ̂, which is described by\nAK(θ∗) = cone { v ∣∣∣ v = θ̂ − θ∗, θ̂ ∈ K} ∩ Sp−1\n(11) The set AK(θ∗) essentially contains all possible directions that error θ̂ − θ∗ could lie in. The following theorem characterizes the error of θ̂.\nTheorem 1 Suppose that the optimization (9) can be solved to global minimum. Then the following error bound holds for the minimizer θ̂ with probability at least 1 − C ′′ exp ( −w2 (AK(θ∗)) ) ,\n∥∥∥θ̂ − θ∗∥∥∥ 2 ≤ Cκm 3 2 β · w(AK(θ ∗)) + C ′√ n , (12)\nwhere κ is the sub-Gaussian norm of a standard Gaussian random variable, and C, C ′, C ′′ are all absolute constant.\nRemark: Note that estimator is consistent as long as β ̸= 0. The error bound inversely depends on the scale of β,\nwhich implies that we should construct suitable qi such that β is large according to its definition in Lemma 1. The choice of qi further depends on the assumed property of f∗. Though dependency on m may prevent us from using higher-order u, m is typically small in practice and can be treated as constant.\nFor regularized estimator, we can similarly establish the recovery guarantee in terms of Gaussian width.\nTheorem 2 Define the following set for any ρ > 1, Aρ (θ∗) = cone { v ∣∣∣ ∥v + θ∗∥ ≤ ∥θ∗∥+ ∥v∥\nρ\n} ∩ Sp−1\nIf we set λ = ρ ∥û− βθ∗∥∗ = O(ρm3/2w(B∥·∥)/ √ n) and it satisfies λ < ∥û∥∗, then with probability at least 1− C ′ exp ( −w2 ( B∥·∥\n)) , θ̂ in (10) satisfies∥∥∥θ̂ − θ∗∥∥∥\n2 ≤ C(1 + ρ)κm\n3 2 β · Ψ(Aρ(θ∗)) · w\n( B∥·∥ ) √ n ,\n(13) where Ψ(Aρ(θ∗)) = supv∈Aρ(θ∗) ∥v∥ and B∥·∥ = {v | ∥v∥ ≤ 1} is the unit ball of norm ∥ · ∥.\nRemark: The geometry of the regularized estimator is slightly different from the constrained one. Instead of having AK(θ∗), here the set Aρ(θ∗) depends on the choice of the regularization parameter λ. The same phenomenon also appears in the (Banerjee et al., 2014). The geometric measure Ψ(Aρ(θ∗)) is called restricted norm compatibility, which is non-random. For many interesting cases, it is easy to calculate (Negahban et al., 2012; Chen & Banerjee, 2015b)."
  }, {
    "heading": "2.4. Application to 1-bit CS",
    "text": "For 1-bit CS problem (2), the u defined in (4) can be chosen with m = 1 and qi = yi, ending up with\nu ((x, y)) = yx and û = 1\nn n∑ i=1 yixi . (14)\nBy such choice of u, the β defined in Lemma 1 is simply β = E[f∗(g)g] with g being standard Gaussian random vector. Under reasonably mild noise, y is likely to take the sign of the linear measurement, which means that f∗(g) should be close to 1 (or -1) if g is positive (or negative). Thus we expect f∗(g)g to be positive most of time and β to be large. Given the choice of u, we can specialize our generalized constrained/regularized estimator to obtain previous results. If θ∗ is assumed to be s-sparse, for constrained estimator, we can choose a straightforward K = {θ | ∥θ∥0 ≤ s}∩Sp−1, which results in the k-support norm estimator (Chen & Banerjee, 2015a),\nθ̂ks = argmin θ∈Rp − ⟨û,θ⟩ s.t. ∥θ∥0 ≤ s, ∥θ∥2 = 1 (15)\nThough K is non-convex, the global minimizer can actually be obtained in closed form,\nθ̂ksj =\n{ ûj / ∥|û|↓1:s∥2 , if |ûj | is in |û| ↓ 1:s\n0 , otherwise (16)\nwhere |û|↓ is the absolute-value counterpart of û with entries sorted in descending order, and the subscript takes the top s entries. Similarly if the regularized estimator is instantiated with L1 norm ∥ · ∥1, we obtain the so-called passive algorithm introduced in Zhang et al. (2014),\nθ̂ps = argmin θ∈Rp − ⟨û,θ⟩+ λ∥θ∥1 s.t. ∥θ∥2 ≤ 1 , (17)\nwhose solution is given by θ̂ps = S (û, λ) /∥S (û, λ) ∥2, where S(·, ·) is the elementwise soft-thresholding operator, Si(û, λ) = max{sign(ûi)(|ûi|−λ), 0}. Based on Theorem 1 and 2, we can easily obtain the error bound for both ksupport norm estimator and passive algorithm.\nCorollary 1 Assume that {(xi, yi)}ni=1 follow 1-bit CS model in (2) and û is given as (14). For any s-sparse θ∗, with high probability, θ̂ produced by both (15) and (17) (i.e., θ̂ks and θ̂ps) satisfy\n∥∥∥θ̂ − θ∗∥∥∥ 2 ≤ O (√ s log p n ) (18)\nThe proof is included in the supplementary material. The above result was shown by Slawski & Li (2015) and Zhang et al. (2014), but their analyses do not consider the general structure. Compared with O( 4 √ s log p/n) yielded by the general result in Plan & Vershynin (2013), our bound is much sharper."
  }, {
    "heading": "3. A New Estimator for Monotone Transfer",
    "text": "In this section, we specifically study model (3). Here we further assume that f̃ is strictly increasing. What is worth mentioning is that the estimator we develop here can be applied to GLMs as well. To avoid the confusion with u and û defined previously, we instead use new notations h and ĥ respectively in this section."
  }, {
    "heading": "3.1. Estimator with Second-Order ĥ",
    "text": "To motivate the design of h, it is helpful to rewrite model (3) by applying the inverse of f̃ on both sides,\nf̃−1(y) = ⟨θ∗,x⟩+ ϵ . (19)\nNote that the new formulation resembles the linear model except that we have no access to the value of f̃−1(y). Instead, all we know about r = [f̃−1(y1), . . . , f̃−1(yn)]T ∈ Rn is that it preserves the ordering of y = [y1, . . . , yn]T .\nPut in another way, r needs to satisfy the constraint that ri > rj iff. yi > yj and ri < rj iff. yi < yj . To move one step further, it is equivalent to sign(yi − yj) = sign(ri−rj) = sign(⟨θ∗,xi−xj⟩+ϵi−ϵj) based on model assumption. Hence the information contained in sample {(xi, yi)}ni=1 can be interpreted from the perspective of 1- bit CS, where sign(yi − yj) reflects the perturbed sign of linear measurement ⟨θ∗,xi − xj⟩. Inspired by the u for 1-bit CS, we may choose m = 2 and define h, ĥ as\nh ((x1, y1), (x2, y2)) = sign(y1 − y2) · (x1 − x2) , (20)\nĥ = 1 n(n− 1) ∑\n1≤i,j≤n i ̸=j\nh ((xi, yi), (xj , yj)) , (21)\nGiven the definition of ĥ, Lemma 1 directly implies the following corollary.\nCorollary 2 Suppose that (x1, y2) and (x2, y2) are generated by model (3), where x1,x2 follow Gaussian distribution N (0, I), and the noise ϵ1, ϵ2 are independent of x1,x2 and identically (but arbitrarily) distributed. Then the expectation of h ((x1, y1), (x2, y2)) satisfies\nE [h ((x1, y1), (x2, y2))] = √ 2β′θ∗ , (22)\nwhere β′ = Eg∼N (0,1) [ sign ( g + (ϵ1 − ϵ2)/ √ 2 ) · g ] .\nRemark: The scalar √ 2β′ serves as the role of β in Lemma 1, and β′ is always guaranteed to be strictly positive regardless how the noise is distributed, which keeps θ∗ distinguishable all the time. To see this, let ξ = (ϵ1− ϵ2)/ √ 2. Note that ξ is symmetric, thus εξ has the same distribution as ξ, where ε is a Rademacher random variable. Therefore\nβ′ = E [sign (g + ξ) · g] = Eg,ξEε [sign (g + εξ) · g] = EξEg [\nsign (g − ξ) + sign (g + ξ) 2\n· g ]\nSince g(g − ξ) + g(g + ξ) = 2g2 ≥ 0, it follows that sign(g(g− ξ))+ sign(g(g+ ξ)) = (sign(g− ξ)+ sign(g+ ξ)) · sign(g) ≥ 0, thus (sign(g − ξ) + sign(g + ξ)) · g is always nonnegative. Find a large enough M > 0 such that P(|ξ| ≤ M) = 0.5 > 0, and we have\nβ′ = E [sign (g + ξ) · g] ≥ EξEg [|g| · I{|g| > |ξ|}]\n≥ 0.5Eg [|g| · I{|g| > M}] = M\n2 · P(|g| > M) > 0 .\nIn the ideal noiseless case, β′ achieve its maximum, β′max = E[sign(g)g] = E[|g|] = √ 2/π. In the worst case, if ϵ1 and ϵ2 are heavy-tailed and dominate g, then β′ ≈ E [ sign ( (ϵ1 − ϵ2)/ √ 2 ) · g ] ≈ 0.\nNow we can instantiate the generalized estimator based on ĥ. For example, if θ∗ is s-sparse, we estimate it by\nθ̂ = argmin θ∈Rp\n− ⟨ĥ,θ⟩ s.t. ∥θ∥0 ≤ s, ∥θ∥2 = 1 (23)\nwhich enjoys O (√ s log p/n )\nerror rate as shown in Corollary 1. The regularized estimator can also be obtained with the same ĥ according to (17). The bottleneck of computing θ̂ lies in the calculation of ĥ. A simple proposition below enables us to get ĥ in a fast manner.\nProposition 1 Given {(xi, yi)}ni=1, let π↓ be the permutation of {1, . . . , n} such that yπ↓1 > yπ↓2 > . . . > yπ↓n . Then we have\nĥ = 2\nn(n− 1) n∑ i=1 (n+ 1− 2i) · xπ↓i (24)\nRemark: Based on the proposition above, ĥ can be efficiently computed in O(np+ n log n) time, i.e., O(n log n) time for sorting y and O(np) time for the weighted sum of all xi. This is a significant improvement compared with the the naive calculation using (21), which takes O(n2p) time."
  }, {
    "heading": "3.2. Beyond Unstructured Sparsity",
    "text": "So far we have illustrated the Gaussian width based error bounds, viz (12) and (13), only through unstructured sparsity of θ∗. Here we provide two more examples, nonoverlapping group sparsity and fused sparsity.\nNon-Overlapping Group Sparsity: Suppose the coordinates of θ∗ has been partitioned into K predefined disjoint groups G1, . . . ,GK ⊆ {1, 2, . . . , p}, out of which only k groups are non-zero. If we use the regularized estimator with L2,1 norm ∥θ∥2,1 = ∑K i=1 ∥θGi∥2, the optimal solution can be similarly obtained as (17), with elementwise soft-thresholding replaced by the groupwise one. The related geometric measures that appears in (13) can be found in Banerjee et al. (2014), which are given by\nΨ(Aρ(θ∗)) ≤ O( √ k) (25)\nw ( B∥·∥2,1 ) ≤ O( √ logK + √ G) (26)\nFused Sparsity: θ∗ is said to be s-fused-sparse if the cardinality of the set F(θ∗) = {1 ≤ i < p | θ∗i ̸= θ∗i+1} is smaller than s. If we resort to the constrained estimator (9) with K = {θ | |F(θ)| ≤ s, ∥θ∥2 = 1}, the associated optimization can be solved by dynamic programming (Bellman, 1961). The proposition below upper bounds the corresponding Gaussian width w(AK(θ∗)) in (12).\nProposition 2 For s-fused-sparse θ∗, the Gaussian width of set AK(θ∗) with K = {θ | |F(θ)| ≤ s, ∥θ∥2 = 1} satisfies\nw(AK(θ∗)) ≤ O( √ s log p) (27)\nThe proof can be found in (Slawski & Li, 2016), and we provide a different one in supplementary material."
  }, {
    "heading": "4. Lemmas and Proof Sketch of Theorem 1",
    "text": "Here we first present the important technical lemmas that will be used in the proof of Theorem 1. The first one is the Hoeffding-type inequality for sub-Gaussian U -statistics. In the literature, most of the studies are centered around bounded U -statistics, for which the celebrated concentration is established by Hoeffding (1963). Yet it is not easy to locate the counterpart for sub-Gaussian case. Therefore we provide the following result and attach a proof in the supplementary material.\nLemma 2 (Concentration for sub-Gaussian U -statistics) Define the U -statistic\nUn,m(h) = (n−m)!\nn!\n∑ 1≤i1,...,im≤n i1 ̸=i2 ̸=... ̸=im h (zi1 , . . . , zim) (28)\nwith order m and kernel h : Rd×m 7→ R based on n independent copies of random vector z ∈ Rd, denoted by z1, · · · , zn. If h(·, . . . , ·) is sub-Gaussian with ∥h∥ψ2 ≤ κ, then the following inequality holds for Un,m(h) with any δ > 0,\nP (|Un,m(h)− EUn,m(h)| > δ) ≤ 2 exp ( −C ⌊ n m ⌋ · δ 2 κ2 ) , (29) in which C is an absolute constant.\nAs mentioned earlier in Section 1, generic chaining is the key tool that our analysis relies on. Specifically we utilize Theorem 2.2.27 from (Talagrand, 2014).\nLemma 3 (Generic chaining concentration) Given metric space (T , s), if an associated stochastic process {Zt}t∈T has sub-Gaussian incremental, i.e., satisfies the condition\nP (|Zu − Zv| ≥ δ) ≤ C exp ( − C ′δ2\ns2(u,v)\n) , ∀ u,v ∈ T ,\n(30) then the following inequality holds\nP (\nsup u,v∈T\n|Zu − Zv| ≥ C1 (γ2(T , s) + δ · diam (T , s)) )\n≤ C2 exp ( −δ2 ) , (31)\nwhere C,C ′, C1 and C2 are all absolute constants.\nThe definition of the above γ2-functional γ2(·, ·) is complicated, and is not of great importance. We refer interested\nreader to the books, Talagrand (2005; 2014). Loosely speaking, γ2(T , s) can be thought of as a measure for the size of set T under metric s. What really matters is the following relationship between γ2-functional and Gaussian width. (see Theorem 2.4.1 in Talagrand (2014))\nLemma 4 (Majorizing measures theorem) For any set T ⊆ Rp, the γ2-functional w.r.t. L2-metric and Gaussian width satisfy the following inequality with an absolute constant C0,\nγ2 (T , ∥ · ∥2) ≤ C0 · w(T ) (32)\nEquipped with these lemmas, we are ready to present the proof sketch of Theorem 1. A complete proof is deferred to the supplementary material.\nProof Sketch of Theorem 1: We use the shorthand notation AK for the set AK(θ∗). As θ̂ attains the global minimum of (9), we have\n⟨θ̂ − θ∗, û⟩ ≥ 0 ⇐⇒ ⟨ θ̂ − θ∗, û\nβ − θ∗ + θ∗\n⟩ ≥ 0\n=⇒ ⟨θ̂,θ∗⟩ ≥ 1− ∥θ̂ − θ∗∥2 · sup v∈AK∪{0}\n⟨ v, û\nβ − θ∗ ⟩ In order to bound the supremum above, we use the result from generic chaining. We define the stochastic process {Zv = ⟨v, û/β − θ∗⟩}v∈AK∪{0}. First, we need to check the process has sub-Gaussian incremental. For simplicity, we denote u ((xi1 , yi1), . . . , (xim , yim)) by ui1,...,im . By the definitions and properties of sub-Gaussian norm (Vershynin, 2012), it is not difficult to show that ∥⟨ui1,...,im ,v −w⟩∥ψ2 ≤ κm · ∥v −w∥2 for any v,w ∈ AK ∪ {0}. By Lemma 2, we have\nP (|Zv − Zw| > δ) ≤ 2 exp ( −C ′ · nβ 2δ2\nm3κ2 · ∥v −w∥22\n) .\nTherefore we can conclude that {Zv} has sub-Gaussian incremental w.r.t. the metric s(v,w) , κm 32 · ∥v − w∥2/β √ n. Now applying Lemma 3 to {Zv} with a bit calculation, we can obtain\nP (\nsup v∈AK∪{0}\n|Zv| ≥ C1κm\n3 2\nβ √ n\n· ( γ2 (AK ∪ {0}, ∥ · ∥2)\n+ 2δ )) ≤ C2 exp ( −δ2 ) Using γ2 (AK ∪ {0}, ∥ · ∥2) ≤ C0 ·w (AK ∪ {0}) implied by Lemma 4 and taking δ = w (AK ∪ {0}), we get\nsup v∈AK∪{0}\n⟨ v, û\nβ − θ∗\n⟩ ≤ C3κm 3 2\nβ · w (AK) + C4√ n\nwith probability at least 1 − C2 exp ( −w2 (AK) ) . The inequality also uses the fact that w (AK ∪ {0}) ≤ w (AK) +\nC4, which is a result of Lemma 2 in Maurer et al. (2014) (See Lemma A in supplementary material). Lastly we turn to the quantity ∥θ̂ − θ∗∥2,\n∥θ̂ − θ∗∥2 ≤ √ 2− 2⟨θ̂,θ∗⟩ ≤ 2C3κm 3 2\nβ · w (AK) + C4√ n .\nWe finish the proof by letting C = 2C3, C ′ = C4 and C ′′ = C2."
  }, {
    "heading": "5. Experiment",
    "text": "In the experiment, we focus on model (3) with sparse θ∗. The problem dimension is fixed as p = 1000, and the sparsity of θ∗ is set to 10. Essentially we generate our data (x, y) from\ny = f̃ (⟨θ∗,x⟩+ ϵ) ,\nwhere x ∼ N (0, I) and ϵ ∼ N (0, σ2). σ ranges from 0.3 to 1.5. We choose three monotonically increasing f̃ , f̃(z) = 1/(1 + exp(−z)) (which is bounded and Lipschitz), f̃(z) = z3 (which is unbounded and non-Lipschitz), and f̃(z) = log(1 + exp(z)) (which is unbounded but Lipschitz). The sample size n varies from 200 to 1000. We use the estimator (23) in Section 3. The baselines we compare with is the SILO and iSILO algorithm introduced in (Ganti et al., 2015). SILO does not quite take the monotonicity in account. In fact, it is the special case of our generalized constrained estimator which uses the same choice of u as 1-bit CS. The original SILO use the constraint set {θ | ∥θ∥1 ≤ √ s, ∥θ∥2 ≤ 1}, which is computationally less efficient and statistically no better than K = {θ | ∥θ∥0 ≤ s} ∩ Sp−1 (Zhang et al., 2014; Chen & Banerjee, 2015a). Hence we also use K in SILO for a fair comparison. iSILO relies on a specific implementation of isotonic regression which explicitly restricts the Lipschitz constant of f̃ to be one. To fit iSILO into our setting, we remove the Lipschitzness constraint and perform the standard isotonic regression. Since the convergence is not guaranteed for the iterative procedure of iSILO, the number of its iterations is fixed to 100. The best tuning parameter of iSILO is obtained by grid search.\nThe experiment results are shown in Figure 1. Overall the iSILO algorithm works well under small noise, while our estimator has better performance when the variance of noise increases. To better demonstrate the robustness of our estimator to heavy-tailed noise, instead of Gaussian noise, we sample ϵ from the Student’s t distribution with degrees of freedom equal to 3. We repeat the experiments for f̃(z) = z3, and obtain the plots in Figure 2. We can see that the error of our estimator consistently decreases for all choice of σ as n increases. For SILO and iSILO, the errors are relatively large, and unable to shrink for large σ even when more data are provided."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper, we study the parameter estimation for the high-dimensional single-index models. We propose two classes of robust estimators, which generalize previous works in two aspects. First we allow the diverse structure (e.g., binary, monotone and etc.) of the transfer function, which can help us customize the estimators. Secondly the structure of the true parameter can be general, either encoded by a constraint or a norm. With limited assumption on the noise, we can show that the estimation error can be bounded by simple geometric measures under Gaussian\nmeasurement, which subsumes the existing results for specific settings. The experiment results also validate our theoretical analyses."
  }, {
    "heading": "Acknowledgements",
    "text": "We thank Sreangsu Acharyya for helpful discussions related to the paper. The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo."
  }],
  "year": 2017,
  "references": [{
    "title": "Sparse single-index model",
    "authors": ["P. Alquier", "G. Biau"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Living on the edge: Phase transitions in convex programs with random data",
    "authors": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"],
    "venue": "Inform. Inference,",
    "year": 2014
  }, {
    "title": "Estimation with norm regularization",
    "authors": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2014
  }, {
    "title": "On the approximation of curves by line segments using dynamic programming",
    "authors": ["R. Bellman"],
    "venue": "Communications of the ACM,",
    "year": 1961
  }, {
    "title": "Simultaneous analysis of Lasso and Dantzig selector",
    "authors": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"],
    "venue": "The Annals of Statistics,",
    "year": 2009
  }, {
    "title": "1-bit compressive sensing",
    "authors": ["Boufounos", "P. T", "R.G. Baraniuk"],
    "venue": "In Information Sciences and Systems,",
    "year": 2008
  }, {
    "title": "The Dantzig selector: statistical estimation when p is much larger than n",
    "authors": ["E. Candes", "T. Tao"],
    "venue": "The Annals of Statistics,",
    "year": 2007
  }, {
    "title": "The convex geometry of linear inverse problems",
    "authors": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2012
  }, {
    "title": "Generalized dantzig selector: Application to the k-support norm",
    "authors": ["S. Chatterjee", "S. Chen", "A. Banerjee"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2014
  }, {
    "title": "One-bit compressed sensing with the k-support norm",
    "authors": ["S. Chen", "A. Banerjee"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2015
  }, {
    "title": "Structured estimation with atomic norms: General bounds and applications",
    "authors": ["S. Chen", "A. Banerjee"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Structured matrix recovery via the generalized dantzig selector",
    "authors": ["S. Chen", "A. Banerjee"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Dimensionality reduction with subgaussian matrices: A unified theory",
    "authors": ["S. Dirksen"],
    "venue": "Foundations of Computational Mathematics,",
    "year": 2016
  }, {
    "title": "Learning single index models in high dimensions",
    "authors": ["R. Ganti", "N. Rao", "Willett", "R. M", "R. Nowak"],
    "venue": "arXiv preprint arXiv:1506.08910,",
    "year": 2015
  }, {
    "title": "One-bit compressed sensing: Provable support and vector recovery",
    "authors": ["S. Gopi", "P. Netrapalli", "P. Jain", "A. Nori"],
    "venue": "In Proceedings of The 30th International Conference on Machine Learning,",
    "year": 2013
  }, {
    "title": "Some inequalities for gaussian processes and applications",
    "authors": ["Y. Gordon"],
    "venue": "Israel Journal of Mathematics,",
    "year": 1985
  }, {
    "title": "Probability inequalities for sums of bounded random variables",
    "authors": ["W. Hoeffding"],
    "venue": "Journal of the American statistical association,",
    "year": 1963
  }, {
    "title": "Direct semiparametric estimation of single-index models with discrete covariates",
    "authors": ["J.L. Horowitz", "W. Hardle"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1996
  }, {
    "title": "Semiparametric least squares (sls) and weighted sls estimation of single-index models",
    "authors": ["H. Ichimura"],
    "venue": "Journal of Econometrics,",
    "year": 1993
  }, {
    "title": "Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors",
    "authors": ["L. Jacques", "Laska", "J. N", "Boufounos", "P. T", "R.G. Baraniuk"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2013
  }, {
    "title": "Learning exponential families in high-dimensions: Strong convexity and sparsity",
    "authors": ["S. Kakade", "O. Shamir", "K. Sindharan", "A. Tewari"],
    "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Efficient learning of generalized linear and single index models with isotonic regression",
    "authors": ["Kakade", "S. M", "V. Kanade", "O. Shamir", "A. Kalai"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2011
  }, {
    "title": "The isotron algorithm: Highdimensional isotonic regression",
    "authors": ["Kalai", "A. T", "R. Sastry"],
    "venue": "In COLT,",
    "year": 2009
  }, {
    "title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Lecture Notes in Mathematics",
    "authors": ["V. Koltchinskii"],
    "year": 2011
  }, {
    "title": "One scan 1-bit compressed sensing",
    "authors": ["P. Li"],
    "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning",
    "authors": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"],
    "venue": "In Conference on Learning Theory (COLT),",
    "year": 2014
  }, {
    "title": "Generalized linear models",
    "authors": ["P. McCullagh"],
    "venue": "European Journal of Operational Research,",
    "year": 1984
  }, {
    "title": "A unified framework for the analysis of regularized M -estimators",
    "authors": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"],
    "venue": "Statistical Science,",
    "year": 2012
  }, {
    "title": "L1-regularized least squares for support recovery of high dimensional single index models with gaussian designs",
    "authors": ["M. Neykov", "J.S. Liu", "T. Cai"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2016
  }, {
    "title": "Fast and reliable parameter estimation from nonlinear observations",
    "authors": ["S. Oymak", "M. Soltanolkotabi"],
    "venue": "arXiv preprint arXiv:1610.07108,",
    "year": 2016
  }, {
    "title": "The squared-error of generalized lasso: A precise analysis",
    "authors": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"],
    "venue": "In Communication, Control, and Computing (Allerton),",
    "year": 2013
  }, {
    "title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
    "authors": ["Y. Plan", "R. Vershynin"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2013
  }, {
    "title": "Highdimensional estimation with geometric constraints",
    "authors": ["Y. Plan", "R. Vershynin", "E. Yudovina"],
    "venue": "Information and Inference,",
    "year": 2016
  }, {
    "title": "High dimensional single index models",
    "authors": ["P. Radchenko"],
    "venue": "Journal of Multivariate Analysis,",
    "year": 2015
  }, {
    "title": "Universal Measurement Bounds for Structured Sparse Signal Recovery",
    "authors": ["N. Rao", "B. Recht", "R. Nowak"],
    "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2012
  }, {
    "title": "b-bit marginal regression",
    "authors": ["M. Slawski", "P. Li"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Linear signal recovery from bbit-quantized linear measurements: precise analysis of the trade-off between bit depth and number of measurements",
    "authors": ["M. Slawski", "P. Li"],
    "venue": "arXiv preprint arXiv:1607.02649,",
    "year": 2016
  }, {
    "title": "The Generic Chaining",
    "authors": ["M. Talagrand"],
    "year": 2005
  }, {
    "title": "Upper and Lower Bounds for Stochastic Processes",
    "authors": ["M. Talagrand"],
    "year": 2014
  }, {
    "title": "Regression shrinkage and selection via the Lasso",
    "authors": ["R. Tibshirani"],
    "venue": "Journal of the Royal Statistical Society, Series B,",
    "year": 1996
  }, {
    "title": "Convex recovery of a structured signal from independent random linear measurements",
    "authors": ["J.A. Tropp"],
    "venue": "In Sampling Theory, a Renaissance",
    "year": 2015
  }, {
    "title": "Introduction to the non-asymptotic analysis of random matrices",
    "authors": ["R. Vershynin"],
    "venue": "Compressed Sensing,",
    "year": 2012
  }, {
    "title": "Estimation in High Dimensions: A Geometric Perspective, pp. 3–66",
    "authors": ["R. Vershynin"],
    "venue": "Springer International Publishing,",
    "year": 2015
  }, {
    "title": "Sharp thresholds for noisy and highdimensional recovery of sparsity using l1-constrained quadratic programming(Lasso)",
    "authors": ["M.J. Wainwright"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2009
  }, {
    "title": "Sparse nonlinear regression: Parameter estimation under nonconvexity",
    "authors": ["Z. Yang", "Z. Wang", "H. Liu", "Y.C. Eldar", "T. Zhang"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Optimal linear estimation under unknown nonlinear transform",
    "authors": ["X. Yi", "Z. Wang", "C. Caramanis", "H. Liu"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Efficient algorithms for robust one-bit compressive sensing",
    "authors": ["L. Zhang", "J. Yi", "R. Jin"],
    "venue": "In Proceedings of the 31st International Conference on Machine Learning",
    "year": 2014
  }, {
    "title": "Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing",
    "authors": ["R. Zhu", "Q. Gu"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
    "year": 2015
  }],
  "id": "SP:3516222cdab2e0d19bb0477af53815e0cbe1e116",
  "authors": [{
    "name": "Sheng Chen",
    "affiliations": []
  }, {
    "name": "Arindam Banerjee",
    "affiliations": []
  }],
  "abstractText": "In this paper, we investigate general single-index models (SIMs) in high dimensions. Based on U -statistics, we propose two types of robust estimators for the recovery of model parameters, which can be viewed as generalizations of several existing algorithms for one-bit compressed sensing (1-bit CS). With minimal assumption on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying parameter. Moreover, the proposed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity. Experimental results are provided to support our theoretical analyses.",
  "title": "Robust Structured Estimation with Single-Index Models"
}