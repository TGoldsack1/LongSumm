{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656–661 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n656"
  }, {
    "heading": "1 Introduction",
    "text": "Neural Machine Translation (NMT) has shown remarkable progress in recent years. However, it requires large amounts of bilingual data to learn a translation model with reasonable quality (Koehn and Knowles, 2017). This requirement can be compensated by leveraging curated monolingual linguistic resources in a multi-task learning framework. Essentially, learned knowledge from auxiliary linguistic tasks serves as inductive bias for the translation task to lead to better generalizations.\nMulti-Task Learning (MTL) is an effective approach for leveraging commonalities of related\ntasks to improve performance. Various recent works have attempted to improve NMT by scaffolding translation task on a single auxiliary task (Domhan and Hieber, 2017; Zhang and Zong, 2016; Dalvi et al., 2017). Recently, (Niehues and Cho, 2017) have made use of several linguistic tasks to improve NMT. Their method shares components of the SEQ2SEQ model among the tasks, e.g. encoder, decoder or the attention mechanism. However, this approach has two limitations: (i) it fully shares the components, and (ii) the shared component(s) are shared among all of the tasks. The first limitation can be addressed using deep stacked layers in encoder/decoder, and sharing the layers partially (Zaremoodi and Haffari, 2018). The second limitation causes this MTL approach to suffer from task interference or inability to leverages commonalities among a subset of tasks. Recently, (Ruder et al., 2017) tried to address this issue; however, their method is restrictive for SEQ2SEQ scenarios and does not consider the input at each time step to modulate parameter sharing.\nIn this paper, we address the task interference problem by learning how to dynamically control the amount of sharing among all tasks. We extended the recurrent units with multiple blocks along with a routing network to dynamically control sharing of blocks conditioning on the task at hand, the input, and model state. Empirical results on two low-resource translation scenarios, English to Farsi and Vietnamese, show the effectiveness of the proposed model by achieving +1 BLEU score improvement compared to strong baselines."
  }, {
    "heading": "2 SEQ2SEQ MTL Using Recurrent Unit with Adaptive Routed Blocks",
    "text": "Our MTL is based on the sequential encoderdecoder architecture with the attention mecha-\nnism (Luong et al., 2015b; Bahdanau et al., 2014). The encoder/decoder consist of recurrent units to read/generate a sentence sequentially. Sharing the parameters of the recurrent units among different tasks is indeed sharing the knowledge for controlling the information flow in the hidden states. Sharing these parameters among all tasks may, however, lead to task interference or inability to leverages commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple blocks, each of which processing its own information flow through the time. The state of the recurrent unit at each time step is composed of the states of these blocks. The recurrent unit is equipped with a routing mechanism to softly direct the input at each time step to these blocks (see Fig 1). Each block mimics an expert in handling different kinds of information, coordinated by the router. In MTL, the tasks can use different subsets of these shared experts.\n(Rosenbaum et al., 2018) uses a routing network for adaptive selection of non-linear functions for MTL. However, it is for fixed-size inputs based on a feed-forward architecture, and is not applicable to SEQ2SEQ scenarios such as MT. (Shazeer et al., 2017) uses Mixture-of-Experts (feed-forward sub-networks) between stacked layers of recurrent units, to adaptively gate state information vertically. This is in contrast to our approach where the horizontal information flow is adaptively modulated, as we would like to minimise the task interference in MTL.\nAssuming there are n blocks in a recurrent unit, we share n− 1 blocks among the tasks, and let the last one to be task-specific1. Task-specific block receives the input of the unit directly while shared blocks are fed with modulated input by the routing network. The state of the unit at each time-step would be the aggregation of blocks’ states."
  }, {
    "heading": "2.1 Routing Mechanism",
    "text": "At each time step, the routing network is responsible to softly forward the input to the shared blocks conditioning on the input xt, and the previous hidden state of the unit ht−1 as follows:\nst = tanh(Wx · xt +Wh · ht−1 + bs), τt = softmax(Wτ · st + bτ ),\nwhere W ’s and b’s are the parameters. Then, the i-th shared block is fed with the input of the\n1multiple recurrent units can be stacked on top of each other to consist a multi-layer component\nunit modulated by the corresponding output of the routing network x̃(i)t = τt[i]xt where τt[i] is the scalar output of the routing network for the i-th block.\nThe hidden state of the unit is the concatenation of the hidden state of the shared and taskspecific parts ht = [h (shared) t ;h (task) t ]. The state of task-specific part is the state of the corresponding block h(task)t = h (n+1) t , and the state of the shared part is the sum of states of shared blocks weighted by the outputs of the routing network h (shared) t = ∑n i=1 τt[i]h (i) t ."
  }, {
    "heading": "2.2 Block Architecture",
    "text": "Each block is responsible to control its own flow of information via a standard gating mechanism. Our recurrent units are agnostic to the internal architecture of the blocks; we use the gated-recurrent unit (Cho et al., 2014) in this paper. For the i-th block the corresponding equations are as follows:\nz (i) t = σ(W (i) z x̃ (i) t +U (i) z h (i) t−1 + b (i) z ), r (i) t = σ(W (i) r x̃ (i) t +U (i) r h (i) t−1 + b (i) r ), h̃ (i) t = tanh(W (i) h x̃ (i) t +U (i) h h (i) t−1 + b (i) h ), h (i) t = z (i) t h (i) t−1 + (1− z (i) t ) h̃ (i) t ."
  }, {
    "heading": "2.3 Training Objective and Schedule.",
    "text": "The rest of the model is similar to attentional SEQ2SEQ model (Luong et al., 2015b) which computes the conditional probability of the target sequence given the source Pθ(y|x) =∏ j Pθ(yj |y<jx). For the case of training M + 1 SEQ2SEQ transduction tasks, each of which is associated with a training set Dm := {(xi,yi)}Nmi=1, the parameters of MTL architecture Θmtl =\n{Θm}Mm=0 are learned by maximizing the following objective: Lmtl(Θmtl) := M∑ m=0 γm |Dm| ∑ (x,y)∈Dm logPΘm(y|x)\nwhere |Dm| is the size of the training set for themth task, and γm is responsible to balance the influence of tasks in the training objective. We explored different values in preliminary experiments, and found that for our training schedule γ = 1 for all tasks results in the best performance. Generally, γ is useful when the dataset sizes for auxiliary tasks are imbalanced (our training schedule handles the main task).\nVariants of stochastic gradient descent (SGD) can be used to optimize the objective function. In our training schedule, we randomly select a mini-batch from the main task (translation) and another mini-batch from a randomly selected auxiliary task to make the next SGD update. Selecting a mini-batch from the main task in each SGD update ensures that its training signals are not washed out by auxiliary tasks."
  }, {
    "heading": "3 Experiments",
    "text": ""
  }, {
    "heading": "3.1 Bilingual Corpora",
    "text": "We use two language-pairs, translating from English to Farsi and Vietnamese. We have chosen them to analyze the effect of multi-task learning on languages with different underlying linguistic structures2. We apply BPE (Sennrich et al., 2016) on the union of source and target vocabularies for English-Vietnamese, and separate vocabularies for English-Farsi as the alphabets are disjoined (30K BPE operations). Further details about the corpora and their pre-processing is as follows:\n• The English-Farsi corpus has ∼105K sentence pairs. It is assembled from English-Farsi parallel subtitles from the TED corpus (Tiedemann, 2012), accompanied by all the parallel news text in LDC2016E93 Farsi Representative Language Pack from the Linguistic Data Consortium. The corpus has been normalized using the Hazm toolkit3. We have removed sentences with more than 80 tokens in either side (before applying BPE). 3k and 4k sentence pairs were held out for the purpose of validation and test.\n2English and Vietnamese are SVO, and Farsi is SOV. 3www.sobhe.ir/hazm\n• The English-Vietnamese has ∼133K training pairs. It is the preprocessed version of the IWSLT 2015 translation task provided by (Luong and Manning, 2015). It consists of subtitles and their corresponding translations of a collection of public speeches from TED and TEDX talks. The “tst2012” and “tst2013” parts are used as validation and test sets, respectively. We have removed sentence pairs which had more than 300 tokens after applying BPE on either sides."
  }, {
    "heading": "3.2 Auxiliary Tasks",
    "text": "We have chosen the following auxiliary tasks to leverage the syntactic and semantic knowledge to improve NMT:\nNamed-Entity Recognition (NER). It is expected that learning to recognize named-entities help the model to learn translation pattern by masking out named-entites. We have used the NER data comes from the CONLL shared task.4 Sentences in this dataset come from a collection of newswire articles from the Reuters Corpus. These sentences are annotated with four types of named entities: persons, locations, organizations and names of miscellaneous entities.\nSyntactic Parsing. By learning the phrase structure of the input sentence, the model would be able to learn better re-ordering. Specially, in the case of language pairs with high level of syntactic divergence (e.g. English-Farsi). We have used Penn Tree Bank parsing data with the standard split for training, development, and test (Marcus et al., 1993). We cast syntactic parsing to a SEQ2SEQ transduction task by linearizing constituency trees (Vinyals et al., 2015).\nSemantic Parsing. Learning semantic parsing helps the model to abstract away the meaning from the surface in order to convey it in the target translation. For this task, we have used the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10)5. This corpus contains natural language sentences from newswire, weblogs, web discussion forums and broadcast conversations. We cast this task to a SEQ2SEQ transduction task by linearizing the AMR graphs (Konstas et al., 2017).\n4https://www.clips.uantwerpen.be/conll2003/ner 5https://catalog.ldc.upenn.edu/LDC2017T10"
  }, {
    "heading": "3.3 Models and Baselines",
    "text": "We have implemented the proposed MTL architecture along with the baselines in C++ using DyNet (Neubig et al., 2017) on top of Mantis (Cohn et al., 2016) which is an implementation of the attentional SEQ2SEQ NMT model. For our MTL architecture, we used the proposed recurrent unit with 3 blocks in encoder and decoder. For the fair comparison in terms the of number of parameters, we used 3 stacked layers in both encoder and decoder components for the baselines. We compare against the following baselines:\n• Baseline 1: The vanilla SEQ2SEQ model (Luong et al., 2015a) without any auxiliary task.\n• Baseline 2: The MTL architecture proposed in (Niehues and Cho, 2017) which fully shares parameters in components. We have used their best performing architecture with our training schedule. We have extended their work with deep stacked layers for the sake of comparison.\n• Baseline 3: The MTL architecture proposed in (Zaremoodi and Haffari, 2018) which uses deep stacked layers in the components and shares the parameters of the top two/one stacked layers among encoders/decoders of all tasks6.\nFor the proposed MTL, we use recurrent units with 400 hidden dimensions for each block. The encoders and decoders of the baselines use GRU units with 400 hidden dimensions. The attention component has 400 dimensions. We use Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 0.003 for all the tasks. Learning\n6In preliminary experiments, we have tried different sharing scenarios and this one led to the best results.\nrates are halved on the decrease in the performance on the dev set of corresponding task. Mini-batch size is set to 32, and dropout rate is 0.5. All models are trained for 50 epochs and the best models are saved based on the perplexity on the dev set of the translation task.\nFor each task, we add special tokens to the beginning of source sequence (similar to (Johnson et al., 2017)) to indicate which task the sequence pair comes from.\nWe used greedy decoding to generate translation. In order to measure translation quality, we use BLEU7 (Papineni et al., 2002) and TER (Snover et al., 2006) scores."
  }, {
    "heading": "3.4 Results and analysis",
    "text": "Table 1 reports the results for the baselines and our proposed method on the two aforementioned translation tasks. As expected, the performance of MTL models are better than the baseline 1 (only MT task). As seen, partial parameter sharing is more effective than fully parameter sharing. Furthermore, our proposed architecture with adaptive\n7Using “multi-bleu.perl” script from Moses (Koehn et al., 2007).\nsharing performs better than the other MTL methods on all tasks, and achieve +1 BLEU score improvements on the test sets. The improvements in the translation quality of NMT models trained by our MTL method may be attributed to less interference with multiple auxiliary tasks.\nFigure 2 shows the average percentage of block usage for each task in an MTL model with 3 shared blocks, on the English-Farsi test set. We have aggregated the output of the routing network for the blocks in the encoder recurrent units over all the input tokens. Then, it is normalized by dividing on the total number of input tokens. Based on Figure 2, the first and third blocks are more specialized (based on their usage) for the translation and NER tasks, respectively. The second block is mostly used by the semantic and syntactic parsing tasks, so specialized for them. This confirms our model leverages commonalities among subsets of tasks by dedicating common blocks to them to reduce task interference."
  }, {
    "heading": "4 Conclusions",
    "text": "We have presented an effective MTL approach to improve NMT for low-resource languages, by leveraging curated linguistic resources on the source side. We address the task interference issue in previous MTL models by extending the recurrent units with multiple blocks along with a trainable routing network. Our experimental results on low-resource English to Farsi and Vietnamese datasets, show +1 BLEU score improvements compared to strong baselines."
  }, {
    "heading": "Acknowledgments",
    "text": "The research reported here was initiated at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. We are very grateful to the workshop members for the insightful discussions and data pre-processing. This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www.massive.org.au), and by the Australian Research Council through DP160102686. The first author was partly supported by CSIRO’s Data61. We would like to thank the anonymous reviewers for their constructive feedback."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1409.0473.",
    "year": 2014
  }, {
    "title": "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2014
  }, {
    "title": "Incorporating structural alignment biases into an attentional neural translation model",
    "authors": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."],
    "venue": "Proceedings of the 2016 Conference of the North",
    "year": 2016
  }, {
    "title": "Understanding and improving morphological learning in the neural machine translation decoder",
    "authors": ["Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Yonatan Belinkov", "Stephan Vogel."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natu-",
    "year": 2017
  }, {
    "title": "Using targetside monolingual data for neural machine translation through multi-task learning",
    "authors": ["Tobias Domhan", "Felix Hieber."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1501–1506.",
    "year": 2017
  }, {
    "title": "Google’s multilingual neural machine translation system: Enabling zero-shot translation",
    "authors": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Viégas", "Martin Wattenberg", "Greg Corrado"],
    "year": 2017
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik Kingma", "Jimmy Ba."],
    "venue": "arXiv preprint arXiv:1412.6980.",
    "year": 2014
  }, {
    "title": "Six challenges for neural machine translation",
    "authors": ["Philipp Koehn", "Rebecca Knowles."],
    "venue": "Proceedings of the First Workshop on Neural Machine Translation, pages 28–39.",
    "year": 2017
  }, {
    "title": "Neural amr: Sequence-to-sequence models for parsing and generation",
    "authors": ["Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "Stanford neural machine translation systems for spoken language domain",
    "authors": ["Minh-Thang Luong", "Christopher D. Manning."],
    "venue": "International Workshop on Spoken Language Translation.",
    "year": 2015
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for",
    "year": 2015
  }, {
    "title": "Effective Approaches to Attentionbased Neural Machine Translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
    "year": 2015
  }, {
    "title": "Building a large annotated corpus of english: The penn treebank",
    "authors": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Comput. Linguist., 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"],
    "year": 2017
  }, {
    "title": "Exploiting linguistic resources for neural machine translation using multi-task learning",
    "authors": ["Jan Niehues", "Eunah Cho."],
    "venue": "Proceedings of the Second Conference on Machine Translation, pages 80–89.",
    "year": 2017
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318.",
    "year": 2002
  }, {
    "title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning",
    "authors": ["Clemens Rosenbaum", "Tim Klinger", "Matthew Riemer."],
    "venue": "International Conference on Learning Representations.",
    "year": 2018
  }, {
    "title": "Sluice networks: Learning what to share between loosely related tasks",
    "authors": ["Sebastian Ruder", "Joachim Bingel", "Isabelle Augenstein", "Anders Søgaard."],
    "venue": "CoRR, abs/1705.08142.",
    "year": 2017
  }, {
    "title": "Neural Machine Translation of Rare Words with Subword Units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725.",
    "year": 2016
  }, {
    "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
    "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean."],
    "venue": "arXiv preprint arXiv:1701.06538.",
    "year": 2017
  }, {
    "title": "A study of translation edit rate with targeted human annotation",
    "authors": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."],
    "venue": "Proceedings of association for machine translation in the Americas.",
    "year": 2006
  }, {
    "title": "Parallel data, tools and interfaces in opus",
    "authors": ["Jörg Tiedemann."],
    "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 2214–2218.",
    "year": 2012
  }, {
    "title": "Grammar as a foreign language",
    "authors": ["Oriol Vinyals", "Ł ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."],
    "venue": "Advances in Neural Information Processing Systems 28, pages 2773– 2781.",
    "year": 2015
  }, {
    "title": "Neural machine translation for bilingually scarce scenarios: A deep multi-task learning approach",
    "authors": ["Poorya Zaremoodi", "Gholamreza Haffari."],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
    "year": 2018
  }, {
    "title": "Exploiting source-side monolingual data in neural machine translation",
    "authors": ["Jiajun Zhang", "Chengqing Zong."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545.",
    "year": 2016
  }],
  "id": "SP:3d9195f94592ef5db25ad6ca8ee54e49349dbf6a",
  "authors": [{
    "name": "Poorya Zaremoodi",
    "affiliations": []
  }, {
    "name": "Wray Buntine",
    "affiliations": []
  }, {
    "name": "Gholamreza Haffari",
    "affiliations": []
  }],
  "abstractText": "Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is MultiTask Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the SEQ2SEQ transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference, and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.",
  "title": "Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation"
}