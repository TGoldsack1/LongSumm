{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Audio synthesis is important for a large range of applications including text-to-speech (TTS) systems and music generation. Audio generation algorithms, know as vocoders in TTS and synthesizers in music, respond to higher-level control signals to create fine-grained audio waveforms. Synthesizers have a long history of being hand-designed instruments, accepting control signals such as ‘pitch’, ‘velocity’, and filter parameters to shape the tone, timbre, and dynamics of a sound (Pinch et al., 2009). In spite of their limitations, or perhaps because of them, synthesizers have had a profound effect on the course of music and culture in the past half century (Punk, 2014).\n*Equal contribution 1Google Brain 2DeepMind. Correspondence to: Jesse Engel <jesseengel@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nIn this paper, we outline a data-driven approach to audio synthesis. Rather than specifying a specific arrangement of oscillators or an algorithm for sample playback, such as in FM Synthesis or Granular Synthesis (Chowning, 1973; Xenakis, 1971), we show that it is possible to generate new types of expressive and realistic instrument sounds with a neural network model. Further, we show that this model can learn a semantically meaningful hidden representation that can be used as a high-level control signal for manipulating tone, timbre, and dynamics during playback.\nExplicitly, our two contributions to advance the state of generative audio modeling are:\n• A WaveNet-style autoencoder that learns temporal hidden codes to effectively capture longer term structure without external conditioning.\n• NSynth: a large-scale dataset for exploring neural audio synthesis of musical notes.\nThe primary motivation for our novel autoencoder structure follows from the recent advances in autoregressive models like WaveNet (van den Oord et al., 2016a) and SampleRNN (Mehri et al., 2016). They have proven to be effective at modeling short and medium scale (∼500ms) signals, but rely on external conditioning for longer-term dependencies. Our autoencoder removes the need for that external conditioning. It consists of a WaveNet-like encoder that infers hidden embeddings distributed in time and a WaveNet decoder that uses those embeddings to effectively reconstruct the original audio. This structure allows the size of an embedding to scale with the size of the input and encode over much longer time scales.\nRecent breakthroughs in generative modeling of images (Kingma & Welling, 2013; Goodfellow et al., 2014; van den Oord et al., 2016b) have been predicated on the availability of high-quality and large-scale datasets such as MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009). While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative\nevaluation, such as with the use of the Inception score (Salimans et al., 2016).\nWe recognized the need for an audio dataset that was as approachable as those in the image domain. Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; BertinMahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al., 2009).\nInspired by the large, high-quality image datasets, NSynth is an order of magnitude larger than comparable public datasets (Humphrey, 2016). It consists of ∼300k foursecond annotated notes sampled at 16kHz from ∼1k harmonic musical instruments.\nAfter introducing the models and describing the dataset, we evaluate the performance of the WaveNet autoencoder over a baseline convolutional autoencoder model trained on spectrograms. We examine the tasks of reconstruction and interpolation, and analyze the learned space of embeddings. For qualitative evaluation, we include supplemental audio files for all examples mentioned in this paper. Despite our best efforts to convey analysis in plots, listening to the samples is essential to understanding this paper and we strongly encourage the reader to listen along as they read."
  }, {
    "heading": "2. Models",
    "text": ""
  }, {
    "heading": "2.1. WaveNet Autoencoder",
    "text": "WaveNet (van den Oord et al., 2016a) is a powerful generative approach to probabilistic modeling of raw audio. In this section we describe our novel WaveNet autoencoder structure. The primary motivation for this approach is to attain consistent long-term structure without external conditioning. A secondary motivation is to use the learned encodings for applications such as meaningful audio interpolation.\nRecalling the original WaveNet architecture described in (van den Oord et al., 2016a), at each step a stack of dilated convolutions predicts the next sample of audio from a fixed-size input of prior sample values. The joint probability of the audio x is factorized as a product of conditional probabilities:\np(x) = N∏ i=1 p(xi|x1, ..., xN−1)\nUnconditional generation from this model manifests as “babbling” due to the lack of longer term structure (see Supplemental for an audio example). However, (van den Oord et al., 2016a) showed in the context of speech that long-range structure can be enforced by conditioning on temporally aligned linguistic features.\nOur autoencoder removes the need for that external con-\nditioning. It works by taking raw audio waveform as input from which the encoder produces an embedding Z = f(x). Next, we causally shift the same input and feed it into the decoder, which reproduces the input waveform. The joint probablity is now:\np(x) = N∏ i=1 p(xi|x1, ..., xN−1, f(x))\nWe could parameterize Z as a latent variable p(Z|x) that we would have to marginalize over (Gulrajani et al., 2016), but in practice we have found this to be less effective. As discussed in (Chen et al., 2016), this may be due to the decoder being so powerful that it can ignore the latent variables unless they encode a much larger context that’s otherwise inaccessible.\nNote that the decoder could completely ignore the deterministic encoding and degenerate to a standard unconditioned WaveNet. However, because the encoding is a strong signal for the supervised output, the model learns to utilize it.\nDuring inference, the decoder autoregressively generates a single output sample at a time conditioned on an embedding and a starting palette of zeros. The embedding can be inferred deterministically from audio or drawn from other points in the embedding space, e.g. through interpolation or analogy (White, 2016).\nFigure 1b depicts the model architecture in more detail. The temporal encoder model is a 30-layer nonlinear residual network of dilated convolutions followed by 1x1 convolutions. Each convolution has 128 channels and precedes a ReLU nonlinearity. The output feed into another 1x1 convolution before downsampling with average pooling to get the encoding Z. We call it a ‘temporal encoding’ because the result is a sequence of hidden codes with separate dimensions for time and channel. The time resolution depends on the stride of the pooling. We tune the stride, keeping total size of the embedding constant (∼32x compression). In the trade-off between temporal resolution and embedding expressivity, we find a sweet spot at a stride of 512 (32ms) with 16 dimensions per timestep, yielding a 125x16 embedding for each NSynth note. We additionally explore models that condition on global attributes by utilizing a one-hot pitch embedding.\nThe WaveNet decoder model is similar to that presented in (van den Oord et al., 2016a). We condition it by biasing every layer with a different linear projection of the temporal embeddings. Since the decoder does not downsample anywhere in the network, we upsample the temporal encodings\nto the original audio rate with nearest neighbor interpolation. As in the original design, we quantize our input audio using 8-bit mu-law encoding and predict each output step with a softmax over the resulting 256 values.\nThis WaveNet autoencoder is a deep and expressive network, but has the trade-off of being limited in temporal context to the chunk-size of the training audio. While this is sufficient for consistently encoding the identity of a sound and interpolating among many sounds, achieving larger context would be better and is an area of ongoing research."
  }, {
    "heading": "2.2. Baseline: Spectral Autoencoder",
    "text": "As a point of comparison, we set out to create a straightforward yet strong baseline for the our neural audio synthesis experiments. Inspired by image models (Vincent et al., 2010), we explore convolutional autoencoder structures with a bottleneck that forces the model to find a compressed representation for an entire note. Figure 1a shows a block diagram of our baseline architecture. The convolutional encoder and decoder are each 10 layers deep with 2x2 strides and 4x4 kernels. Every layer is followed by a leaky-ReLU (0.1) nonlinearity and batch normalization (Ioffe & Szegedy, 2015). The number of channels grows from 128 to 1024 before a linear fully-connected layer creates a single 19841 dimensional hidden vector (Z) to match that of the WaveNet autoencoder.\nGiven the simplicity of the architecture, we examined a range of input representations. Using the raw waveform as input with a mean-squared error (MSE) cost proved difficult to train and highlighted the inadequacy of the independent Gaussian assumption. Spectral representations such as the real and imaginary components of the Fast Fourier Transform (FFT) fared better, but suffered from low perceptual quality despite achieving low MSE cost. We found that training on the log magnitude of the power spectra, peak normalized to be between 0 and 1, correlated better with perceptual distortion.\nWe also explored several representations of phase, including instantaneous frequency and circular normal cost functions (see Supplemental), but in each case independently estimating phase and magnitude led to poor sample quality due to phase errors. We find a large improvement by estimating only the magnitude and using a well established iterative technique to reconstruct the phase (Griffin & Lim, 1984). To get the best results, we used a large FFT size (1024) relative to the hop size (256) and ran the algorithm for 1000 iterations. As a final heuristic, we weighted the MSE loss, starting at 10 for 0Hz and decreasing linearly to\n1This size was aligned with a WaveNet autoencoder that had a pooling stride of 1024 and a 62x32 embedding.\n1 at 4000Hz and above. At the expense of some precision in timbre, this created more phase coherence for the fundamentals of notes, where errors in the linear spectrum lead to a larger relative error in frequency."
  }, {
    "heading": "2.3. Training",
    "text": "We train all models with stochastic gradient descent with an Adam optimizer (Kingma & Ba, 2014). The baseline models commonly use a learning rate of 1e-4, while the WaveNet models use a schedule, starting at 2e-4 and descending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k, and 240k respectively. The baseline models train asynchronously for 1800k iterations with a batch size of 8. The WaveNet models train synchronously for 250k iterations with a batch size of 32."
  }, {
    "heading": "3. The NSynth Dataset",
    "text": "To evaluate our WaveNet autoencoder model, we wanted an audio dataset that let us explore the learned embeddings. Musical notes are an ideal setting for this study as we hypothesize that the embeddings will capture structure such as pitch, dynamics, and timbre. While several smaller datasets currently exist (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, highquality data, motivating the development of a new dataset."
  }, {
    "heading": "3.1. A Dataset of Musical Notes",
    "text": "NSynth consists of 306 043 musical notes, each with a unique pitch, timbre, and envelope. For 1006 instruments from commercial sample libraries, we generated four second, monophonic 16kHz audio snippets, referred to as notes, by ranging over every pitch of a standard MIDI piano (21-108) as well as five different velocities2 (25, 50, 75, 100, 127). The note was held for the first three seconds and allowed to decay for the final second. Some instruments are not capable of producing all 88 pitches in this range, resulting in an average of 65.4 pitches per instrument. Furthermore, the commercial sample packs occasionally contain duplicate sounds across multiple velocities, leaving an average of 4.75 unique velocities per pitch."
  }, {
    "heading": "3.2. Annotations",
    "text": "We also annotated each of the notes with three additional pieces of information based on a combination of human evaluation and heuristic algorithms:\n• Source: The method of sound production for the note’s instrument. This can be one of ‘acoustic’ or\n2MIDI velocity is similar to volume control and they have a direct relationship. For physical intuition, higher velocity corresponds to pressing a piano key harder.\n‘electronic’ for instruments that were recorded from acoustic or electronic instruments, respectively, or ‘synthetic’ for synthesized instruments.\n• Family: The high-level family of which the note’s instrument is a member. Each instrument is a member of exactly one family. See Supplemental for the complete list.\n• Qualities: Sonic qualities of the note. See Supplemental for the complete list of classes and their cooccurrences. Each note is annotated with zero or more qualities."
  }, {
    "heading": "3.3. Availability",
    "text": "The full NSynth dataset will be made publicly available in a serialized format after publication. is available for download at http://download.magenta.tensorflow.org/hans as TFRecord files split into training and holdout sets. Each note is represented by a serialized TensorFlow Example protocol buffer containing the note and annotations. Details of the format can be found in the README."
  }, {
    "heading": "4. Evaluation",
    "text": "We evaluate and analyze our models on the tasks of note reconstruction, instrument interpolation, and pitch interpolation.\nAudio is notoriously hard to represent visually. Magnitude spectrograms capture many aspects of a signal for analytics, but two spectrograms that appear very similar to the eye can correspond to audio that sound drastically different due to phase differences. We have included supplemental audio examples of every plot and encourage the reader to listen along as they read.\nThat said, in our analysis we present examples as plots of the constant-q transform (CQT) (Brown, 1991; Schörkhuber & Klapuri, 2010), which is useful because it is shift invariant to changes in the fundamental frequency. In this way, the structure and envelope of the overtone series (higher harmonics) determines the dynamics and timbre of a note, regardless of its base frequency. However, due to the logarithmic binning of frequencies, transient noise-like impulses appear as rainbow “pyramidal spikes” rather than straight broadband lines. We display CQTs with a pitch range of 24-96 (C2-C8), hop size of 256, 40 bins per octave, and a filter scale of 0.8.\nAs phase plays such an essential part in sample quality, we have attempted to show both magnitude and phase on the same plot. The intensity of lines is proportional to the log magnitude of the power spectrum while the color\nis given by the derivative of the unrolled phase (‘instantaneous frequency’) (Boashash, 1992). We display the derivative of the phase because it creates a solid continuous line for a harmonic of a consistent frequency. We can understand this because if the instantaneous frequency of a harmonic (fharm) and an FFT bin (fbin) are not exactly equal, each timestep will introduce a constant phase shift, ∆φ = (fbin − fharm) hopsizesamplerate ."
  }, {
    "heading": "4.1. Reconstruction",
    "text": "Figure 2 displays CQT spectrograms for notes from 3 different instruments in the holdout set, where the original note spectrograms are on the first column and the model reconstruction spectrograms are on the second and third columns. Each note has a similar structure with some noise on onset, a fundamental frequency with a series of harmonics, and a decay. For all the WaveNet models, there is a slight built-in distortion due to the compression of the mulaw encoding. It is a minor effect for many samples, but is more pronounced for lower frequencies. Using different representations without this distortion is an ongoing area of research.\nWhile each spectrogram matches the general contour of the original note, we can hear a pronounced difference in sample quality that we can ascribe to certain features. For the Glockenspiel, we can see that the WaveNet autoencoder reproduces the magnitude and phase of the fundamental (solid blue stripe, (A)), and also the noise on attack (vertical rainbow spike (B)). There is a slight error in the fun-\ndamental as it starts a little high and quickly descends to the correct pitch (C). In contrast, the baseline has a more percussive, multitonal sound, similar to a bell or gong. The fundamental is still present, but so are other frequencies, and the phases estimated from the Griffin-Lim procedure are noisy as indicated by the blurred horizontal rainbow texture (D).\nThe electric piano has a more clearly defined harmonic series (the horizontal rainbow solid lines, (E)) and a noise on the beginning and end of the note (vertical rainbow spikes, (F)). Listening to the sound, we hear that it is slightly distorted, which promotes these upper harmonics. Both the WaveNet autoencoder and the baseline produce spectrograms with similar shapes to the original, but with different types of phase artifacts. The WaveNet model has sufficient phase structure to model the distortion, but has a slight wavering of the instantaneous frequency of some harmonics, as seen in the color change in harmonic stripes (G). In contrast, the baseline lacks the structure in phase to maintain the punchy character of the original note, and produces a duller sound that is slightly out of tune. This is represented in the less brightly colored harmonics due to phase noise (H).\nThe flugelhorn displays perhaps the starkest difference between the two models. The sound combines rich harmonics (many lines), non-tonal wind and lip noise (background color), and vibrato - oscillation of pitch that results in a corresponding rainbow of color in all of the harmonics. While the WaveNet autoencoder does not replicate the ex-\nact trace of the vibrato (I), it creates a very similar spectrogram with oscillations in the instantaneous frequency at all levels synced across the harmonics (J). This results in a rich and natural sounding reconstruction with all three aspects of the original sound. The baseline, by comparison, is unable to model such structure. It creates a more or less correct harmonic series, but the phase has lots of random perturbations. Visually this shows up as colors which are faded and speckled with rainbow noise (K), which contrasts with the bright colors of the original and WaveNet examples. Acoustically, this manifests as an unappealing buzzing sound laid over an inexpressive and consistent series of harmonics. The WaveNet model also produces a few inaudible discontinuities visually evidenced by the vertical rainbow spikes (L)."
  }, {
    "heading": "4.1.1. QUANTITATIVE COMPARISON",
    "text": "Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Supplemental). The network configuration is the same as the baseline encoder and testing is done on reconstructions of a randomly chosen subset of 4096 examples from the held-out set.\nThe results in Table 1 confirm our qualititive observation that the WaveNet reconstructions are of superior quality. The classifier is ∼70% more successful at extracting pitch from the reconstructed WaveNet samples than the baseline\nand several points higher for predicting quality information, giving an accuracy roughly equal to the original audio."
  }, {
    "heading": "4.2. Interpolation in Timbre and Dynamics",
    "text": "Given the limited factors of variation in the dataset, we know that a successful embedding space (Z) should span the range of timbre and dynamics in its reconstructions. In Figure 3, we show reconstructions from linear interpolations (0.5:0.5) in the Z space among three different instruments and additionally compare these to interpolations in the original audio space. The latter are simple superpositions of the individual instruments’ spectrograms. This is perceptually equivalent to the two instruments being played at the same time.\nIn contrast, we find that the generative models fuse aspects of the instruments. As we saw in Section 4.1, the WaveNet autoencoder models the data much more realistically than the baseline, so it is no surprise that it also learns a manifold of codes that yield more perceptually interesting re-\nconstructions.\nFor example, in the interpolated note between the bass and flute (Figure 3, column 2), we can hear and see that both the baseline and WaveNet models blend the harmonic structure of the two instruments while imposing the amplitude envelope of the bass note onto the upper harmonics of the flute note. However, the WaveNet model goes beyond this to create a dynamic mixing of the overtones in time, even jumping to a higher harmonic at the end of the note (A). This sound captures expressive aspects of the timbre and dynamics of both the bass and flute, but is distinctly separate from either original note. This contrasts with the interpolation in audio space, where the dynamics and timbre of the two notes is independent. The baseline model also introduces phase distortion similar to those in the reconstructions of the bass and flute.\nWe see this phenomenon again in the interpolation between flute and organ (Figure 3, column 4). Both models also seem to create new harmonic structure, rather than just overlay the original harmonics. The WaveNet model adds additional harmonics as well as a sub-harmonic to the original flute note, all while preserving phase relationships (B). The resulting sound has the breathiness of a flute, with the upper frequency modulation of an organ. By contrast, the lack of phase structure in the baseline leads to a new harmonic yet dull sound lacking a unique character.\nThe WaveNet model additionally has a tendency to exaggerate amplitude modulation behavior, while the baseline suppresses it. If we examine the original organ sound (Figure 3, column 5), we can see a subtle modulation signified by the blue harmonics periodically fading to black (C). The baseline model misses this behavior completely as it is washed out. Conversely, the WaveNet model amplifies the behavior, adding in new harmonics not present in the original note and modulating all the harmonics. This is seen in the figure by four vertical black stripes that align with the four modulations of the original signal (D)."
  }, {
    "heading": "4.3. Entanglement of Pitch and Timbre",
    "text": "By conditioning on pitch during training, we hypothesize that we should be able to generate multiple pitches from a single Z vector that preserve the identity of timbre and dynamics. Our initial attempts were unsuccessful, as it seems our models had learned to ignore the conditioning variable. We investigate this further with classification and correlation studies."
  }, {
    "heading": "4.3.1. PITCH CLASSIFICATION FROM Z",
    "text": "One way to study the entanglement of pitch andZ is to consider the pitch classification accuracy from embeddings. If training with pitch conditioning disentangles the represen-\ntation of pitch and timbre, then we would expect a linear pitch classifier trained on the embeddings to drop in accuracy. To test this, we train a series of baseline autoencoder models with different embedding sizes, both with and without pitch conditioning. For each model, we then train a logistic regression pitch classifier on its embeddings and test on a random sample of 4096 held-out embeddings.\nThe first two rows of Table 2 demonstrate that the baseline and WaveNet models decrease in classification accuracy by 13-30% when adding pitch conditioning during training. This is indicative a reduced presence of pitch information in the latent code and thus a decoupling of pitch and timbre information. Further, as the total embedding size decreases below 512, the accuracy drop becomes much more pronounced, reaching a 75% relative decrease. This is likely due to the greater expressivity of larger embeddings, where there is less to be gained from utilizing the pitch conditioning. However, as the embedding size decreases, so too does reconstruction quality. This is more pronounced for the WaveNet models, which have farther to fall in terms of sample quality.\nAs a proof of principle, we find that for a baseline model with an embedding size of 128, we are able to successfully balance reconstruction quality and response to conditioning. Figure 4 demonstrates two octaves of a C major chord created from a single embedding of an electric piano note, but conditioned on different pitches. The resulting harmonic structure of the original note is only partially preserved across the range. As we shift the pitch upwards, a sub-harmonic emerges (A) such that the pitch +12 note is similar to the original except that the harmonics of the octave are accentuated in amplitude. This aligns with our pitch classification results, where we find that pitches are most commonly confused with those one octave away (see Supplemental). These errors can account for as much as 20% absolute classification error."
  }, {
    "heading": "4.3.2. Z CORRELATION ACROSS PITCH",
    "text": "We can gain further insight into the relationship between timbre and pitch by examining the correlation of WaveNet embeddings among pitches for a given instrument. Figure 5 shows correlations for several instruments across their entire 88 note range at velocity 127. We see that each instrument has a unique partitioning into two or more registers over which notes of different pitches have similar embeddings. Even the average over all instruments shows a broad distinction between high and low registers. On reflection, this is unsurprising as the timbre and dynamics of an instrument can vary dramatically across its range."
  }, {
    "heading": "5. Conclusion and Future Directions",
    "text": "In this paper, we have introduced a WaveNet autoencoder model that captures long term structure without the need for external conditioning and demonstrated its effectiveness on the new NSynth dataset for generative modeling of audio.\nThe WaveNet autoencoder that we describe is a powerful representation for which there remain multiple avenues of exploration. It builds upon the fine-grained local understanding of the original WaveNet work and provides access to a useful hidden space. However, due to memory constraints, it is unable to fully capture global context. Overcoming this limitation is an important open problem.\nNSynth was inspired by image recognition datasets that have been core to recent progress in deep learning. We encourage the broader community to use NSynth as a benchmark and entry point into audio machine learning. We also view NSynth as a building block for future datasets and envision a high-quality multi-note dataset for tasks like generation and transcription that involve learning complex language-like dependencies."
  }, {
    "heading": "Acknowledgments",
    "text": "A huge thanks to Hans Bernhard with the dataset, Colin Raffel for crucial conversations, and Sageev Oore for thoughtful analysis."
  }],
  "year": 2017,
  "references": [{
    "title": "The million song dataset",
    "authors": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel PW", "Whitman", "Brian", "Lamere", "Paul"],
    "venue": "In ISMIR,",
    "year": 2011
  }, {
    "title": "Estimating and interpreting the instantaneous frequency of a signal",
    "authors": ["Boashash", "Boualem"],
    "venue": "i. fundamentals. Proceedings of the IEEE,",
    "year": 1992
  }, {
    "title": "Calculation of a constant q spectral transform",
    "authors": ["Brown", "Judith C"],
    "venue": "The Journal of the Acoustical Society of America,",
    "year": 1991
  }, {
    "title": "Variational lossy autoencoder",
    "authors": ["Dhariwal", "Prafulla", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"],
    "venue": "CoRR, abs/1611.02731,",
    "year": 2016
  }, {
    "title": "The synthesis of complex audio spectra by means of frequency modulation",
    "authors": ["Chowning", "John M"],
    "venue": "Journal of the audio engineering society,",
    "year": 1973
  }, {
    "title": "ImageNet: A Large-Scale Hierarchical Image Database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"],
    "venue": "In CVPR09,",
    "year": 2009
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2014
  }, {
    "title": "Rwc music database: Music genre database and musical instrument sound database",
    "authors": ["Goto", "Masataka", "Hashiguchi", "Hiroki", "Nishimura", "Takuichi", "Oka", "Ryuichi"],
    "year": 2003
  }, {
    "title": "Signal estimation from modified short-time fourier transform",
    "authors": ["Griffin", "Daniel", "Lim", "Jae"],
    "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,",
    "year": 1984
  }, {
    "title": "Pixelvae: A latent variable model for natural images",
    "authors": ["Gulrajani", "Ishaan", "Kumar", "Kundan", "Ahmed", "Faruk", "Taiga", "Adrien Ali", "Visin", "Francesco", "Vázquez", "David", "Courville", "Aaron C"],
    "venue": "CoRR, abs/1611.05013,",
    "year": 2016
  }, {
    "title": "Minst, a collection of musical sound datasets, 2016",
    "authors": ["Humphrey", "Eric J"],
    "venue": "URL https://github.com/ ejhumphrey/minst-dataset/",
    "year": 2016
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate",
    "authors": ["Ioffe", "Sergey", "Szegedy", "Christian"],
    "venue": "shift. CoRR,",
    "year": 2015
  }, {
    "title": "The blizzard challenge",
    "authors": ["King", "Simon", "Clark", "Robert AJ", "Mayo", "Catherine", "Karaiskos", "Vasilis"],
    "year": 2008
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"],
    "venue": "CoRR, abs/1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["Kingma", "Diederik P", "Welling", "Max"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"],
    "year": 2009
  }, {
    "title": "The mnist database of handwritten digits",
    "authors": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"],
    "year": 1998
  }, {
    "title": "Sound texture synthesis via filter statistics",
    "authors": ["McDermott", "Josh H", "Oxenham", "Andrew J", "Simoncelli", "Eero P"],
    "venue": "In Applications of Signal Processing to Audio and Acoustics,",
    "year": 2009
  }, {
    "title": "Samplernn: An unconditional end-to-end neural audio generation model",
    "authors": ["Mehri", "Soroush", "Kumar", "Kundan", "Gulrajani", "Ishaan", "Rithesh", "Jain", "Shubham", "Sotelo", "Jose", "Courville", "Aaron C", "Bengio", "Yoshua"],
    "venue": "CoRR, abs/1612.07837,",
    "year": 2016
  }, {
    "title": "Reading digits in natural images with unsupervised feature learning",
    "authors": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"],
    "venue": "In NIPS workshop on deep learning and unsupervised feature learning,",
    "year": 2011
  }, {
    "title": "Analog days: The invention and impact of the Moog synthesizer",
    "authors": ["Pinch", "Trevor J", "Trocco", "Frank", "TJ"],
    "year": 2009
  }, {
    "title": "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching",
    "authors": ["Raffel", "Colin"],
    "venue": "PhD thesis, COLUMBIA UNIVERSITY,",
    "year": 2016
  }, {
    "title": "Improved techniques for training gans",
    "authors": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"],
    "venue": "CoRR, abs/1606.03498,",
    "year": 2016
  }, {
    "title": "Musical audio synthesis using autoencoding neural nets",
    "authors": ["Sarroff", "Andy M", "Casey", "Michael A"],
    "venue": "In ICMC,",
    "year": 2014
  }, {
    "title": "Constant-q transform toolbox for music processing",
    "authors": ["Schörkhuber", "Christian", "Klapuri", "Anssi"],
    "venue": "Sound and Music Computing Conference,",
    "year": 2010
  }, {
    "title": "A note on the evaluation of generative models",
    "authors": ["Theis", "Lucas", "Oord", "Aäron van den", "Bethge", "Matthias"],
    "venue": "arXiv preprint arXiv:1511.01844,",
    "year": 2015
  }, {
    "title": "Learning features of music from scratch",
    "authors": ["Thickstun", "John", "Harchaoui", "Zaid", "Kakade", "Sham"],
    "venue": "In preprint,",
    "year": 2016
  }, {
    "title": "Wavenet: A generative model for raw audio",
    "authors": ["van den Oord", "Aäron", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew W", "Kavukcuoglu", "Koray"],
    "venue": "CoRR, abs/1609.03499,",
    "year": 2016
  }, {
    "title": "Sampling generative networks: Notes on a few effective techniques",
    "authors": ["White", "Tom"],
    "venue": "CoRR, abs/1609.04468,",
    "year": 2016
  }],
  "id": "SP:97c01b6cef7d7d88ec7eda488bfdc46fd601e76a",
  "authors": [{
    "name": "Jesse Engel",
    "affiliations": []
  }, {
    "name": "Cinjon Resnick",
    "affiliations": []
  }, {
    "name": "Adam Roberts",
    "affiliations": []
  }, {
    "name": "Sander Dieleman",
    "affiliations": []
  }, {
    "name": "Mohammad Norouzi",
    "affiliations": []
  }, {
    "name": "Douglas Eck",
    "affiliations": []
  }, {
    "name": "Karen Simonyan",
    "affiliations": []
  }],
  "abstractText": "Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",
  "title": "Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders"
}