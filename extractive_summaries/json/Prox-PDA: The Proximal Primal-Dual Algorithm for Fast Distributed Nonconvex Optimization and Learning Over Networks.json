{
  "sections": [{
    "heading": "1. Introduction",
    "text": "We consider the following optimization problem\nmin z∈RM g(z) := N∑ i=1 fi(z), (1)\nwhere each fi, i ∈ {1, · · · , N} := [N ] is a nonconvex cost function, and we assume that it is smooth and has Lipschitz continuous gradient.\nSuch a finite sum problem plays a central role in machine learning and signal/information processing (Cevher et al., 2014; Hong et al., 2016). In particular, in the class of empirical risk minimization (ERM) problem, z represents the feature vectors to be learned, and each fi can represent: 1) a mini-batch of (possibly nonconvex) loss functions modeling data fidelity (Antoniadis et al., 2009); 2) nonconvex activation functions of neural networks (Allen-Zhu & Hazan,\n1Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA, USA 2College of Information Science and Electronic Engineering, Zhejiang University, China. Correspondence to: Mingyi Hong <mingyi@iastate.edu>, Davood Hajinezhad <dhaji@iastate.edu>, Ming-Min Zhao <zmmblack@zju.edu.cn>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n2016); 3) nonconvex utility functions used in applications such as resource allocation (Bjornson & Jorswieck, 2013). Recently, a number of works in machine learning community have been focused on designing fast algorithms for solving problem (1) in centralized setting; e.g., SAG (Defazio et al., 2014), SAGA (Schmidt et al., 2013), and SVRG (Johnson & Zhang, 2013) for convex problems, and (Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Hajinezhad et al., 2016b; Rahimpour et al., 2016) for nonconvex problems.\nIn this work, we are interested in designing algorithms that solve problem (1) in a distributed manner. In particular, we focus on the scenario where each fi (or equivalently, each subset of data points in the ERM problem) is available locally at a given computing node i ∈ [N ], and the nodes are connected via a network. Clearly, such distributed optimization and learning scenario is important for machine learning, because in contemporary applications such as document topic modeling and/or social network data analysis, oftentimes data corporas are stored in geographically distributed locations without any central controller managing the entire network of nodes; see (Forero et al., 2010; Yan et al., 2013; Rahmani & Atia, 2015; Aybat & Hamedani, 2016).\nRelated Works. Distributed convex optimization and learning has been thoroughly investigated in the literature. In (Nedic & Ozdaglar, 2009b), the authors propose a distributed subgradient algorithm (DSG), which allows the agents to jointly optimize problem (1). Subsequently, many variants of DSG have been proposed, either with special assumptions on the underlying graph, or having additional structures of the problem; see, e.g., (Lobel & Ozdaglar, 2011; Lobel et al., 2011; Nedic & Olshevsky, 2015). The rate of convergence for DSG is O(log(r)/ √ r) under certain diminishing stepsize rules. Recently, a number of algorithms such as the exact first-order algorithm (EXTRA) (Shi et al., 2014) and DLM (Ling et al., 2015) have been proposed, which use constant stepsize and achieve faster O(1/r) rate for convex problems. Recent works that apply distributed optimization algorithms to machine learning applications include (Scardapane et al., 2016; Aybat & Hamedani, 2016; Scardapane & Lorenzo, 2016).\nOn the other hand, there has been little work for dis-\ntributed optimization and learning when the objective function involves nonconvex problems. A dual subgradient method has been proposed in (Zhu & Martinez, 2010), which relaxes the exact consensus constraint. In (Bianchi & Jakubowicz, 2013) a stochastic projection algorithm using diminishing stepsizes has been proposed. An ADMM based algorithm has been presented in (Hong et al., 2014; Hajinezhad & Hong, 2015) for a special type of problem called global consensus, where all distributed nodes are directly connected to a central controller. Utilizing certain convexification decomposition technique the authors of (Lorenzo & Scutari, 2016) designed an algorithm named NEXT, which converges to the set of stationary solutions when using diminishing stepsizes. To the best of our knowledge, no multi agent distributed algorithm is able to guarantee global sublinear convergence rate for problem (1).\nOur Contributions. In this work, we propose a proximal primal-dual algorithm (Prox-PDA) for problem (1) over an undirected connected network. We show that Prox-PDA converges to the set of stationary solutions of problem (1) (satisfying the first-order optimality condition) in a globally sublinear manner. We also show that Prox-PDA can be extended in several directions to improve its practical performance. To the best of our knowledge, this is the first algorithm that is capable of achieving global sublinear convergence rate for distributed non-convex optimization.\nFurther, our work reveals an interesting connection between the primal-dual based algorithm Prox-PDA and the primal-only fast distributed algorithms such as EXTRA (Shi et al., 2014). Such new insight of the connection between primal-dual and primal-only algorithms could be of independent interest for the optimization community. Finally, we generalize the theory for Prox-PDA based algorithms to a challenging distributed matrix factorization problem."
  }, {
    "heading": "2. System Model",
    "text": "Define a graph G := {V, E}, where V and E are the node and edge sets; Let |V| = N and |E| = E. Each node v ∈ V represents an agent in the network, and each edge eij = (i, j) ∈ E indicates that node i and j are neighbors; see Fig.1(Left). Assume that each node i can only communicate with its immediate neighbors, defined as Ni := {j | (i, j) ∈ V}, with |Ni| = di. The distributed version of problem (1) is given as below\nmin xi∈RM f(x) := N∑ i=1 fi(xi), s.t. xi = xj , ∀ (i, j) ∈ E . (2)\nClearly the above problem is equivalent to (1) as long as G is connected. For notational simplicity, define x := {xi} ∈ RNM×1, and Q := N ×M .\nTo proceed, let us introduce a few useful quantities related to graph G.\n• The incidence matrix Ã ∈ RE×N is a matrix with entires Ã(k, i) = 1 and Ã(k, j) = −1 if k = (i, j) ∈ E with j > i, and all the rest of the entries being zero. For example, for the network in Fig.1 (Left); the incidence matrix is given in Fig.1 (Right). Define the extended incidence matrix as\nA := Ã⊗ IM ∈ REM×Q, (3)\nwhere ⊗ denotes the Kronecker product.\n• The Degree matrix D̃ ∈ RN×N is given by D̃ := diag[d1, · · · , dN ]; Let D := D̃ ⊗ IM ∈ RQ×Q.\n• The signed and the signless Laplacian matrices (denoted as L− and L+ respectively), are given below\nL− := A>A ∈ RQ×Q, L+ := 2D −A>A ∈ RQ×Q. (4)\nUsing the above notations, one can verify that problem (2) can be written in the following compact form\nmin x∈RQ f(x), s.t. Ax = 0. (5)"
  }, {
    "heading": "3. The Prox-PDA Algorithm",
    "text": "The proposed algorithm builds upon the classical augmented Lagrangian (AL) method (Bertsekas, 1982; Powell, 1969). Let us define the AL function for (5) as\nLβ(x, µ) = f(x) + 〈µ,Ax〉+ β\n2 ‖Ax‖2, (6)\nwhere µ ∈ RQ is the dual variable; β > 0 is a penalty parameter. Let B ∈ RQ×Q be some arbitrary matrix to be determined shortly. Then the proposed algorithm is given in the table below (Algorithm 1). In Prox-PDA, the primal iteration (7a) minimizes the augmented Lagrangian plus a proximal term β2 ‖x − x\nr‖2BTB . We emphasize that the proximal term is critical in both the algorithm implementation and the analysis. It is used to ensure the following key properties: (1). The primal problem is strongly convex; (2). The primal problem is decomposable over different network nodes, hence distributedly implementable. To see the first point, suppose BTB is chosen such that ATA + BTB IQ, and that f(x) has Lipschitz gradient. Then by a result in (Zlobec, 2005)[Theorem 2.1], we know\nAlgorithm 1 The Prox-PDA Algorithm\n1: At iteration 0, initialize µ0 = 0 and x0 ∈ RQ. 2: At each iteration r + 1, update variables by:\nxr+1 = arg min x∈RQ f(x) + 〈µr, Ax〉+ β 2 ‖Ax‖2\n+ β\n2 ‖x− xr‖2BTB ; (7a)\nµr+1 = µr + βAxr+1. (7b)\nthat there exists β > 0 large enough such that the objective function of (7a) is strongly convex. To see the second point, Let B := |A|, where the absolute value is taken for each component of A. It can be verified that BTB = L+, and step (7a) becomes\nxr+1 = argmin x N∑ i=1 fi(xi) + 〈µr, Ax〉\n+ β 2 xTL−x+ β 2 (x− xr)TL+(x− xr)\n= argmin x N∑ i=1 fi(xi) + 〈µr, Ax〉+ βxTDx− βxTL+xr.\nClearly this problem is separable over the nodes, therefore it can be solved completely distributedly."
  }, {
    "heading": "4. The Convergence Analysis",
    "text": "In this section we provide convergence analysis for Algorithm 1. The key in the analysis is the construction of a novel potential function, which decreases at every iteration of the algorithm. In particular, the constructed potential function is a conic combination of the AL function and the size of the violation of the consensus constraint, therefore it measures the progress of both the primal and dual updates.\nWe first state our main assumptions below.\n[A1.] The function f(x) is differentiable and has Lipschitz continuous gradient, i.e.,\n‖∇f(x)−∇f(x)‖ ≤ L‖x− y‖, ∀ x, y ∈ RQ.\nFurther assume that ATA+BTB IQ.\n[A2.] There exists a constant δ > 0 such that\n∃ f > −∞, s.t. f(x) + δ 2 ‖Ax‖2 ≥ f, ∀ x ∈ RQ.\nWithout loss of generality we will assume that f = 0. Below we provide a few nonconvex smooth functions that satisfy our assumptions, all of which are commonly used as activation functions for neural networks.\n• The sigmoid function sigmoid(x) = 11+e−x . • The arctan and tanh function. • The logit function logit(x) = e x\nex+1 ."
  }, {
    "heading": "4.1. The Analysis Steps",
    "text": "Below we provide the analysis of Prox-PDA. First we provide a bound on the size of the constraint violation using a quantity related to the primal iterates. Let σmin denotes the smallest non-zero eigenvalue of ATA, and we define wr := (xr+1−xr)−(xr−xr−1) for notational simplicity. We have the following result. Lemma 1 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Prox-PDA\n1 β ‖µr+1 − µr‖2\n≤ 2L 2\nβσmin ∥∥xr − xr+1∥∥2 + 2β σmin ‖BTBwr‖2. (8)\nThen we bound the descent of the AL function. Lemma 2 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Algorithm 1\nLβ(x r+1, µr+1)− Lβ(xr, µr) ≤ 2β‖BTB‖ σmin ‖wr‖2BTB\n− ( β − L\n2 − 2L\n2\nβσmin\n) ‖xr+1 − xr‖2. (9)\nA key observation from Lemma 2 is that no matter how large β is, the rhs of (9) cannot be made negative. This observation suggests that in contrast to (Hong et al., 2014; Hajinezhad et al., 2016a) the augmented Lagrangian alone cannot serve as the potential function for Prox-PDA. In search for an appropriate potential function, we need a new object that is decreasing in the order of β ‖wr‖2BTB . The following lemma shows that the descent of the sum of the constraint violation and the proximal term has the desired property.\nLemma 3 Suppose Assumption [A1] is satisfied. Then the following is true\nβ\n2\n( ‖Axr+1‖2 + ‖xr+1 − xr‖2BTB ) ≤ L‖xr+1 − xr‖2 + β\n2\n( ‖Axr‖2 + ‖xr − xr−1‖2BTB ) − β\n2\n( ‖wr‖2BTB + ‖A(x r+1 − xr)‖2 ) . (10)\nIt is interesting to observe that the new object, β/2 ( ‖Axr+1‖2 + ‖xr+1 − xr‖2BTB ) , increases in L‖xr+1 − xr‖2 and decreases in β/2‖wr‖2BTB , while the AL behaves in an opposite manner (cf. Lemma 2). More importantly, in our new object, the constant in front of ‖xr+1 − xr‖2 is independent of β. Although neither of these two objects decreases by itself, quite surprisingly, a proper conic combination of these two objects decreases at every iteration of Prox-PDA. To precisely state the claim, let us define the potential function for Algorithm 1 as\nPc,β(x r+1, xr, µr+1) := Lβ(x r+1, µr+1)\n+ cβ\n2\n( ‖Axr+1‖2 + ‖xr+1 − xr‖2BTB ) , (11)\nwhere c > 0 is some constant to be determined later. We have the following result.\nLemma 4 Suppose the assumptions made in Lemmas 1 – 3 are satisfied. Then we have the following\nPc,β(x r+1, xr, µr+1) ≤ Pc,β(xr, xr−1, µr) − ( β − L\n2 − 2L\n2\nβσmin − cL\n) ‖xr+1 − xr‖2\n− ( cβ\n2 − 2β‖B TB‖F σmin\n) ‖wr‖2BTB . (12)\nFrom the above analysis, it is easy to see that as long as c and β are sufficiently large, the potential function decreases at each iteration of Prox-PDA. Below we derive the precise bounds for c and β. First, a sufficient condition for c is given below (note, that δ > 0 is defined in Assumption [A2])\nc ≥ max { δ\nL , 4‖BTB‖F σmin\n} . (13)\nSecond, for any given c, we need β to satisfy β−L2 − 2L2\nβσmin − cL > 0, which implies the following\nβ > L\n2 2c+ 1 +√(2c+ 1)2 + 16L2 σmin  . (14) We conclude that if both (13) and (14) are satisfied, then the potential function Pc,β(xr+1, xr, µr+1) decreases at every iteration.\nOur next step shows that by using the particular choices of c and β in (13) and (14), the constructed potential function is lower bounded.\nLemma 5 Suppose [A1] - [A2] are satisfied, and (c, β) are chosen according to (13) and (14). Then the following statement holds true\n∃ P > −∞ s.t. Pc,β(xr+1, xr, µr+1) ≥ P , ∀ r > 0. Now we are ready to present the main result of this section. To this end, define Q(xr+1, µr) as the optimality gap of problem (5), given by\nQ(xr+1, µr) := ‖∇xLβ(xr+1, µr)‖2 + ‖Axr+1‖2. (15)\nIt is easy to see that Q(xr+1, µr) → 0 implies that any limit point (x∗, µ∗), if it exists, is a KKT point of (5) that satisfies the following conditions\n0 = ∇f(x∗) +ATµ∗, Ax∗ = 0. (16)\nIn the following we show that the gap Q(·) not only decreases to zero, but does so in a sublinear manner.\nTheorem 1 Suppose Assumption A and the conditions (13) and (14) are satisfied. Then we have • Eventual Consensus:\nlim r→∞ µr+1 − µr → 0, lim r→∞ Axr → 0.\n• Convergence to Stationary Points: Every limit point of the iterates {xr, µr} generated by Algorithm 1 converges to a KKT point of problem (5). Further, Q(xr+1, µr)→ 0. • Sublinear Convergence Rate: For any given ϕ > 0, let us define T to be the first time that the optimality gap reaches below ϕ, i.e.,\nT := arg min r Q(xr+1, µr) ≤ ϕ.\nThen for some ν > 0, we have ϕ ≤ νT−1 . That is, the optimality gap Q(xr+1, µr) converges sublinearly."
  }, {
    "heading": "5. Variants of Prox-PDA",
    "text": "In this section, we discuss two important extensions of the Prox-PDA, one allows the x-problem (7a) to be solved inexactly, while the second allows the use of increasing penalty parameter β. In many practical applications, exactly minimizing the augmented Lagrangian may not be easy. Therefore, we propose the proximal gradient primaldual algorithm (Prox-GPDA), whose main steps are given below\nxr+1 = arg min x∈RQ 〈∇f(xr), x− xr〉+ 〈µr, Ax〉\n+ β 2 ‖Ax‖2 + β 2 ‖x− xr‖2BTB ; (17) µr+1 = µr + βAxr+1. (18)\nThe analysis of this algorithm follows similar steps as that for Prox-PDA. For detailed discussion see (Hong, 2016).\nOur second variant do not require to explicitly compute the bound for β given in (14). Indeed, the bounds on β derived in the previous sections are the worst case bounds, and algorithms that use stepsizes that strictly satisfy such bounds may be slow at the beginning. In practice, one may prefer to start with a small penalty parameter and gradually increase it. We denote this algorithm by Prox-PDA-IP, whose main steps are given below\nxr+1 = arg min x∈RQ f(x) + 〈µr, Ax〉\n+ βr+1 2 ‖Ax‖2 + β r+1 2 ‖x− xr‖2BTB ; (19)\nµr+1 = µr + βr+1Axr+1. (20)\nNote that one can also replace f(x) in (19) by 〈∇f(xr), x− xr〉 to obtain a similar variant for Prox-GPDA denoted by Prox-GPDA-IP. Throughout this section we will still as-\nsume that Assumption A holds true. Further, we will assume that βr satisfies the following conditions\n1\nβr → 0, ∞∑ r=1 1 βr =∞, βr+1 ≥ βr,\nmax r\n(βr+1 − βr) < κ, for some finite κ > 0. (21)\nAlso without loss of generality we will assume that\nBTB 0, and ‖BTB‖F > 1. (22)\nNote that this is always possible, by adding an identity matrix to BTB if necessary.\nThe analysis for Prox-PDA-IP is long and technical, therefore we refer the readers to (Hong, 2016). The key step is to construct a new potential function, given below\nPβr+1,c(x r+1, xr, µr+1) = Lβr+1(x r+1, µr+1)\n+ cβr+1βr 2 ‖Axr+1‖2 + cβ r+1βr 2 ‖xr − xr+1‖2BTB .\nThe insight here is that in order to achieve the desired descent, in the potential function the coefficients for ‖xr+1− xr‖2BTB and ‖Ax\nr+1‖2 should be proportional to O ( (βr)2 ) . We have the following theorem regarding to the convergence of Prox-PDA-IP.\nTheorem 2 Suppose Assumption A and (21) are satisfied. Suppose that B is selected such that (22) holds true. Then the following hold for Prox-PDA-IP\n• Eventual Consensus: lim r→∞ µr+1 − µr → 0, lim r→∞ Axr → 0.\n• Convergence to KKT Points: Every limit point of the iterates {xr, µr} generated by Prox-PDA-IP converges to a KKT point of problem (5). Further, Q(xr+1, µr)→ 0."
  }, {
    "heading": "6. Connections and Discussions",
    "text": "In this section we present an interesting observation which establishes links between the so-called EXTRA algorithm (Shi et al., 2014) (developed for distributed, but convex optimization) and the Prox-GPDA.\nSpecifically, the optimality condition of the x-update step (17) is given by\n∇f(xr) +AT (µr + βAxr+1) + β(BTB(xr+1 − xr)) = 0.\nUtilizing the fact that ATA = L−, BTB = L+ and L+ + L− = 2D, we have\n∇f(xr) +ATµr + 2βDxr+1 − βL+xr = 0.\nSubtracting the same equation evaluated at the previous iteration, we obtain\n∇f(xr)−∇f(xr−1) + βL−xr + 2βD(xr+1 − xr) − βL+(xr − xr−1) = 0,\nwhere we have used the fact that AT (µr − µr−1) = βATAxr = βL−xr. Rearranging terms, we have\nxr+1 = xr − 1 2β D−1\n( ∇f(xr)−∇f(xr−1) ) + 1\n2 D−1(L+ − L−)xr − 1 2 D−1L+xr−1\n= xr − 1 2β D−1\n( ∇f(xr)−∇f(xr−1) ) +Wxr\n− 1 2 (I +W )xr−1, (23)\nwhere in the last equality we have defined the weight matrix W := 12D\n−1(L+ − L−), which is a row stochastic matrix.\nIteration (23) has the same form as the EXTRA algorithm given in (Shi et al., 2014), therefore we can conclude that EXTRA is a special case of Prox-GPDA. Moreover, by appealing to our analysis in Section 5, it readily follows that EXTRA works for the nonconvex distributed optimization problem as well.\nWe remark that each node i can distributedly implement iteration (23) by performing the following\nxr+1i = x r i −\n1\n2βdi\n( ∇fi(xri )−∇fi(xr−1i ) ) + ∑\nj∈N (i)\n1 di xrj − 1 2  ∑ j∈N (i) 1 di xr−1j + x r−1 i  . (24) Clearly, at iteration r + 1, besides the local gradient information, node i only needs the aggregated information from its neighbors, ∑ j∈N (i) x r j . Therefore the algorithm is distributedly implementable."
  }, {
    "heading": "7. Distributed Matrix Factorization",
    "text": "In this section we study a variant of the Prox-PDA/ProxPDA-IP for the following distributed matrix factorization problem (Ling et al., 2012)\nmin X,Y\n1 2 ‖XY − Z‖2F + η‖X‖2F + h(Y ) (25)\n= N∑ i=1 1 2 ‖Xyi − zi‖2 + γ‖X‖2F + hi(yi),\ns.t. ‖yi‖2 ≤ τ, ∀ i\nwhere X ∈ RM×K , Y ∈ RK×N ; for each i, yi ∈ RK consists of one column of Y ; Z ∈ RM×N is some known matrix; h(Y ) := ∑N i=1 hi(yi) is some convex but possibly nonsmooth penalization term; η > 0 is some given constant; for notation simplicity we have defined γ := 1/Nη.\nIt is easy to extend the above formulation to the case where Y and Z both have NP columns, and each yi and zi consists of P columns of Y and Z respectively.\nWe assume that h(Y ) is lower bounded over dom (h). One application of problem (25) is the distributed sparse dictionary learning problem where X is the dictionary to be learned, each zi is a training data sample, and each yi is the sparse coefficient corresponding to the particular training sample zi. The constraint ‖yi‖2 ≤ τ simply says that the size of the coefficient must be bounded.\nConsider a distributed scenario where N agents form a graph {V, E}, each having a column of Y . We reformulate problem (25) as\nmin {Xi},{yi} N∑ i=1 ( 1 2 ‖Xiyi − zi‖2 + hi(yi) + γ‖Xi‖2F ) s.t. ‖yi‖2 ≤ τ, ∀ i Xi = Xj , ∀ (i, j) ∈ E .\nLet us stack all the variables Xi, and define X := [X1;X2; · · · ;XN ] ∈ RNM×K . Define the block signed incidence matrix as A = Ã ⊗ IM ∈ REM×NM , where A is the standard graph incidence matrix. Define the block signless incidence matrix B ∈ REM×NM similarly. If the graph is connected, then the condition AX = 0 implies network-wide consensus. We formulate the distributed matrix factorization problem as\nmin {Xi},{yi} f(X, Y ) + h(Y )\n:= N∑ i=1 ( 1 2 ‖Xiyi − zi‖2 + γ‖Xi‖2F + hi(yi) ) s.t. ‖yi‖2 ≤ τ, ∀ i AX = 0. (26)\nClearly the above problem does not satisfy Assumption A, because the objective function is not smooth, and neither ∇Xf(X, Y ) nor ∇Y f(X, Y ) is Lipschitz continuous. The latter fact poses significant difficulty in algorithm development and analysis. Define the block-signed/signless Laplacians as\nL− = ATA, L+ = BTB. (27)\nThe AL function for the above problem is given by\nLβ(X, Y,Ω) = N∑ i=1 ( 1 2 ‖Xiyi − zi‖2 + γ‖Xi‖2F + hi(yi) ) + 〈Ω,AX〉+ β\n2 〈AX,AX〉, (28)\nwhere Ω := {Ωe} ∈ REM×K is the matrix of the dual variable, with Ωe ∈ RM×K being the dual variable for the consensus constraint on link e, i.e, Xi = Xj , e = (i, j).\nLet us generalize Algorithm 1 for distributed matrix factorization given in Algorithm 2. In Algorithm 2 we have introduced a sequence {θri ≥ 0} which measures the size\nAlgorithm 2 Prox-PDA for Distr. Matrix Factorization\n1: At iteration 0, initialize Ω0 = 0, and X0, y0 2: At each iteration r + 1, update variables by:\nθri = ‖Xri yri − zi‖2, ∀ i; (29a)\nyr+1i = arg min ‖yi‖2≤τ\n1 2 ‖Xri yi − zi‖2 + hi(yi)\n+ θri 2 ‖yi − yri ‖2, ∀ i; (29b)\nXr+1 = argmin X f(X, Y r+1) + 〈Ωr,AX〉 (29c)\n+ β 2 〈AX,AX〉+ β 2 〈B(X −Xr),B(X −Xr)〉;\nΩr+1 = Ωr + βAXr+1. (29d)\nof the local factorization error. We note that including the proximal term θ r i\n2 ‖yi − y r i ‖2 is the key to achieve conver-\ngence for Algorithm 2.\nLet us comment on the distributed implementation of the algorithm. First note that the y subproblem (29b) is naturally distributed to each node, that is, only local information is needed to perform the update. Second, the X subproblem (29c) can also be decomposed into N subproblems, one for each node. To be more precise, let us examine the terms in (29c) one by one. First, the term f(X, Y r+1) =∑N i=1 ( 1 2‖Xiy r+1 i − zi‖2 + hi(y r+1 i ) + γ‖Xi‖2F ) , hence it is decomposable. Second, the term 〈Ωr,AX〉 can be expressed as\n〈Ωr,AX〉 = N∑ i=1 ∑ e∈U(i) 〈Ωre, Xi〉 − ∑ e∈H(i) 〈Ωre, Xi〉\nwhere the sets U(i) and H(i) are defined as U(i) := {e | e = (i, j) ∈ E , i ≥ j}, H(i) := {e | e = (i, j) ∈ E , j ≥ i}.\nSimilarly, we have 〈BXr,BX〉 = N∑ i=1\n〈 Xi, diX r i + ∑ j∈N(i) Xrj 〉 β 2 (〈AX,AX〉+ 〈BX,BX〉)\n= β〈DX,X〉 = β N∑ i=1 di‖Xi‖2F ,\nwhere D := D̃ ⊗ IM ∈ RNM×NM with D̃ being the degree matrix. It is easy to see that the X subproblem (29c) is separable over the distributed agents. Finally, one can verify that the Ω update step (29d) can be implemented by each edge e ∈ E as follows\nΩr+1e = Ω r e + β ( Xr+1i −X r+1 j ) , e = (i, j), i ≥ j.\nTo show convergence rate of the algorithm, we need the following definition Q(Xr+1, Y r+1,Ωr) := β‖AXr+1‖2 + ‖[Zr+11 ;Z r+1 2 ]‖ 2,\nwhere we have defined Zr+11 := ∇XLβ(X r+1, Y r+1,Ωr); Zr+12 := Y r+1\n− proxh+ι(Y) [ Y r+1 −∇Y ( Lβ(X r+1, Y r+1,Ωr)− h(Y ) )] .\nIn the above expression, we have used Y :=⋃ i { ‖yi‖2 ≤ τ } to denote the feasible set of Y , and used ι(Y) to denote the indicator function of such set. Similarly as in Section 4, we can show that Q(Xr+1, Y r+1,Ωr) → 0 implies that every limit point of (Xr+1, Y r+1,Ωr) is a KKT point of problem (26).\nNext we present the main convergence analysis for Algorithm 2. The proof is long and technical, therefore we refer the readers to (Hong, 2016).\nTheorem 3 Consider using Algorithm 2 to solve the distributed matrix factorization problem (26). Suppose that h(Y ) is lower bounded over dom h(x), and that the penalty parameter β, together with two positive constants c and d, satisfies the following conditions\nβ + 2γ 2 − 8(τ\n2 + 4γ2)\nβσmin − cd 2 > 0,\n1 2 − 8 σminβ − c d > 0, 1 2 − 8τ σminβ − cτ d > 0,\ncβ 2 − 2β‖B TB‖ σmin > 0.\n(30)\nThen in the limit, consensus will be achieved, i.e.,\nlim r→∞\n‖Xri −Xrj ‖ = 0, ∀ (i, j) ∈ E .\nFurther, the sequences {Xr+1} and {Ωr+1} are both bounded, and every limit point generated by Algorithm 2 is a KKT point of problem (25).\nAdditionally, Algorithm 2 converges sublinearly. Specifically, for any given ϕ > 0, define T to be the first time that the optimality gap reaches below ϕ, i.e.,\nT := argmin r\nQ(Xr+1, Y r+1,Ωr) ≤ ϕ.\nThen for some constant ν > 0 we have ϕ ≤ νT−1 . We can see that it is always possible to find the tuple {β, c, d > 0} that satisfies (30): c can be solely determined by the last inequality; for fixed c, the constant d needs to be chosen large enough such that 1/2 − cd > 0 and 1/2 − cτd > 0 are satisfied. After c and d are fixed, one can always choose β large enough to satisfy the first three conditions. In practice, we typically prefer to choose β as small as possible to improve the convergence speed. Therefore empirically one can start with (for some small ν > 0): c = 4‖B\nTB‖ σmin + ν, d = max{4, 2cτ}, and then\ngradually increase d to find an appropriate β that satisfies the first three conditions.\nWe remark that Algorithm 2 can be extended to the case with increasing penalty. Due to the space limitation we omit the details here."
  }, {
    "heading": "8. Numerical Results",
    "text": "In this section, we demonstrate the performance of the proposed algorithms. All experiments are performed in Matlab (2016b) on a desktop with an Intel Core(TM) i5-4690 CPU (3.50 GHz) and 8GB RAM running Windows 7."
  }, {
    "heading": "8.1. Distributed Binary Classification",
    "text": "In this subsection, we study the problem of binary classification using nonconvex regularizers in the mini-bach setup i.e. each node stores b (batch size) data points, and each component function is given by\nfi(xi) = 1\nNb [ b∑ j=1 log(1 + exp(−yijxTi vij)) + M∑ k=1 λαx2i,k 1 + αx2i,k ]\nwhere vij ∈ RM and yij ∈ {1,−1} are the feature vector and the label for the jth date point in ith agent (Antoniadis et al., 2009). We use the parameter settings of λ = 0.001, α = 1 and M = 10. We randomly generated 100, 000 data points and distribute them into N = 20 nodes (i.e. b = 5000). We use the optimality gap (opt-gap) and constraint violation (con-vio), displayed below, to measure the quality of the solution generated by different algorithms\nopt-gap := ∥∥∥∥ N∑ i=1 ∇fi(zi) ∥∥∥∥2 + ‖Ax‖2, con-vio = ‖Ax‖2.\nWe compare the the Prox-GPDA, and Prox-GPDA-IP with the distributed subgradient (DSG) method (Nedic & Ozdaglar, 2009a;b) (which is only known to work for convex cases) and the Push-sum algorithm (Tatarenko & Touri, 2015). The performance of all three algorithms in terms of the consensus error and the optimality gap (averaged over 30 problem instances) are presented in Fig. 2. The penalty parameter for Prox-GPDA is chosen such that satisfy (14), and βr for Prox-GPDA-IP is set as 0.05 log(r), the stepsizes of the DSG algorithm and the Push-sum algorithm are chosen as 1/0.05 log(r) and 1/r, respectively. Note that these parameters are tuned for each algorithm to achieve the best results. All Algorithms will stop after 1000 iterations. It can be observed that the Prox-GPDA with constant stepsize outperforms other algorithms. The Push-sum algorithm does not seem to converge within 1000 iterations.\nTo see more results, we compare different algorithms with different number of agents in the network (N ). The problem and the algorithms setup are set as the previous case. We measure the optimality gap as well as the constraint violation and the results are respectively reported in Table 1 and Table 2. In the tables Alg1, Alg2, Alg3, Alg4\ndenote Prox-GPDA, Prox-GPDA-IP, DGS, and Push-sum algorithms respectively. As seen, the performance of the proposed algorithms (Alg1, Alg2) are significantly better than DGS and Push-Sum."
  }, {
    "heading": "8.2. Distributed Matrix Factorization",
    "text": "In this section we consider the distributed matrix factorization problem (25). The training data is constructed\nby randomly extracting 300 overlapping patches from the 512 × 512 image of barbara.png, each with size 16 × 16 pixels. Each of the extracted patch is vectorized, resulting a training data set Z of size 256 × 300. We consider a network of N = 10 agents, and the columns of Z are evenly distributed among the agents (each having P = 30 columns). We compare Prox-PDA-IP with the EXTRAAO algorithm proposed in (H.-T. Wai & Scaglione, 2015). Note that the EXTRA-AO is also designed for a similar distributed matrix factorization problem and it works well in practice. However, it does not have formal convergence proof. We initialize both algorithms with X being the 2D discrete cosine transform (DCT) matrix. We set γ = 0.05, τ = 105 and β = 0.001r, and the results are averaged over 30 problem instances. The stepsizes of the EXTRAAO is set as αAO = 0.03 and βAO = 0.002. In Fig. 3, we compare the performance of the proposed Prox-PDAIP and the EXTRA-AO. It can be observed that our proposed algorithm converges faster than the EXTRA-AO. We have observed that the EXTRA-AO does have reasonably good practical performance, however it lacks formal convergence proof."
  }, {
    "heading": "Acknowledgment",
    "text": "The authors supported by National Natural Science Foundation (Grant No. CCF-1526078)."
  }],
  "year": 2017,
  "references": [{
    "title": "Variance Reduction for Faster NonConvex Optimization",
    "authors": ["Z. Allen-Zhu", "E. Hazan"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Penalized likelihood regression for generalized linear models with non-quadratic penalties",
    "authors": ["A. Antoniadis", "I. Gijbels", "M. Nikolova"],
    "venue": "Annals of the Institute of Statistical Mathematics,",
    "year": 2009
  }, {
    "title": "A primal-dual method for conic constrained distributed optimization problems",
    "authors": ["Aybat", "N-S", "Hamedani", "E-Y"],
    "venue": "Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Constrained Optimization and Lagrange Multiplier Method",
    "authors": ["D.P. Bertsekas"],
    "year": 1982
  }, {
    "title": "Convergence of a multi-agent projected stochastic gradient algorithm for non-convex optimization",
    "authors": ["P. Bianchi", "J. Jakubowicz"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2013
  }, {
    "title": "Optimal resource allocation in coordinated multi-cell systems",
    "authors": ["E. Bjornson", "E. Jorswieck"],
    "venue": "Foundations and Trends in Communications and Information Theory,",
    "year": 2013
  }, {
    "title": "Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics",
    "authors": ["V. Cevher", "S. Becker", "M. Schmidt"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2014
  }, {
    "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
    "authors": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"],
    "venue": "In The Proceeding of NIPS,",
    "year": 2014
  }, {
    "title": "Consensusbased distributed support vector machines",
    "authors": ["P.A. Forero", "A. Cano", "G.B. Giannakis"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2010
  }, {
    "title": "A consensus-based decentralized algorithm for non-convex optimization with application to dictionary learning",
    "authors": ["H.-T. Wai", "T.-H. Chang", "A. Scaglione"],
    "venue": "In the Proceedings of the IEEE ICASSP,",
    "year": 2015
  }, {
    "title": "Nonconvex alternating direction method of multipliers for distributed sparse principal component analysis",
    "authors": ["D. Hajinezhad", "M. Hong"],
    "venue": "In IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE,",
    "year": 2015
  }, {
    "title": "Nonnegative matrix factorization using admm: Algorithm and convergence analysis",
    "authors": ["D. Hajinezhad", "Chang", "T-H", "X. Wang", "Q. Shi", "M. Hong"],
    "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2016
  }, {
    "title": "Nestt: A nonconvex primal-dual splitting method for distributed and stochastic optimization",
    "authors": ["D. Hajinezhad", "M. Hong", "T. Zhao", "Z. Wang"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Decomposing linearly constrained nonconvex problems by a proximal primal dual approach: Algorithms, convergence, and applications",
    "authors": ["M. Hong"],
    "venue": "arXiv preprint arXiv:1604.00543,",
    "year": 2016
  }, {
    "title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
    "authors": ["M. Hong", "Luo", "Z.-Q", "M. Razaviyayn"],
    "year": 2014
  }, {
    "title": "A unified algorithmic framework for block-structured optimization involving big data",
    "authors": ["M. Hong", "M. Razaviyayn", "Luo", "Z.-Q", "Pang", "J.-S"],
    "venue": "IEEE Signal Processing Magazine,",
    "year": 2016
  }, {
    "title": "Accelerating stochastic gradient descent using predictive variance reduction",
    "authors": ["R. Johnson", "T. Zhang"],
    "venue": "In the Proceedings of the Neural Information Processing (NIPS)",
    "year": 2013
  }, {
    "title": "Decentralized low-rank matrix completion",
    "authors": ["Q. Ling", "Y. Xu", "W. Yin", "Z. Wen"],
    "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2012
  }, {
    "title": "DLM: Decentralized linearized alternating direction method of multipliers",
    "authors": ["Q. Ling", "W. Shi", "G. Wu", "A. Ribeiro"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2015
  }, {
    "title": "Distributed subgradient methods for convex optimization over random networks",
    "authors": ["I. Lobel", "A. Ozdaglar"],
    "venue": "Automatic Control, IEEE Transactions on,",
    "year": 2011
  }, {
    "title": "Distributed multi-agent optimization with state-dependent communication",
    "authors": ["I. Lobel", "A. Ozdaglar", "D. Feijer"],
    "venue": "Mathematical Programming,",
    "year": 2011
  }, {
    "title": "Next: In-network nonconvex optimization",
    "authors": ["Lorenzo", "P. Di", "G. Scutari"],
    "venue": "IEEE Transactions on Signal and Information Processing over Networks,",
    "year": 2016
  }, {
    "title": "Distributed optimization over timevarying directed graphs",
    "authors": ["A. Nedic", "A. Olshevsky"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2015
  }, {
    "title": "Cooperative distributed multi-agent optimization. In Convex Optimization in Signal Processing and Communications",
    "authors": ["A. Nedic", "A. Ozdaglar"],
    "year": 2009
  }, {
    "title": "Distributed subgradient methods for multi-agent optimization",
    "authors": ["A. Nedic", "A. Ozdaglar"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2009
  }, {
    "title": "An efficient method for nonlinear constraints in minimization problems",
    "authors": ["M.M.D. Powell"],
    "year": 1969
  }, {
    "title": "Distributed object recognition in smart camera networks",
    "authors": ["Rahimpour", "Alireza", "Taalimi", "Ali", "Luo", "Jiajia", "Qi", "Hairong"],
    "venue": "In IEEE International Conference on Image Processing,",
    "year": 2016
  }, {
    "title": "A decentralized approach to robust subspace recovery",
    "authors": ["M. Rahmani", "G. Atia"],
    "venue": "In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
    "year": 2015
  }, {
    "title": "Fast incremental method for nonconvex optimization",
    "authors": ["S. Reddi", "S. Sra", "B. Poczos", "A. Smola"],
    "venue": "arXiv preprint arXiv:1603.06159,",
    "year": 2016
  }, {
    "title": "A framework for parallel and distributed training of neural networks",
    "authors": ["S. Scardapane", "Lorenzo", "P. Di"],
    "venue": "arXiv preprint arXiv:1610.07448,",
    "year": 2016
  }, {
    "title": "Distributed semi-supervised support vector machines",
    "authors": ["S. Scardapane", "R. Fierimonte", "Lorenzo", "P. Di", "M. Panella", "A. Uncini"],
    "venue": "Neural Networks,",
    "year": 2016
  }, {
    "title": "Minimizing finite sums with the stochastic average gradient",
    "authors": ["M. Schmidt", "Roux", "N. Le", "F. Bach"],
    "year": 2013
  }, {
    "title": "Extra: An exact first-order algorithm for decentralized consensus optimization",
    "authors": ["W. Shi", "Q. Ling", "G. Wu", "W. Yin"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2014
  }, {
    "title": "Non-convex distributed optimization",
    "authors": ["T. Tatarenko", "B. Touri"],
    "venue": "arXiv Preprint:",
    "year": 2015
  }, {
    "title": "Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties",
    "authors": ["F. Yan", "S. Sundaram", "S.V.N. Vishwanathan", "Y. Qi"],
    "venue": "IEEE Transactions on Knowledge and Data Engineering,",
    "year": 2013
  }, {
    "title": "An approximate dual subgradient algorithm for multi-agent non-convex optimization",
    "authors": ["M. Zhu", "S. Martinez"],
    "venue": "IEEE Conference on Decision and Control (CDC),",
    "year": 2010
  }, {
    "title": "On the liu-floudas convexification of smooth programs",
    "authors": ["S. Zlobec"],
    "venue": "Journal of Global Optimization,",
    "year": 2005
  }],
  "id": "SP:7797ec742b3d17d96a1e60c77d20eaafe10b0f9c",
  "authors": [{
    "name": "Mingyi Hong",
    "affiliations": []
  }, {
    "name": "Davood Hajinezhad",
    "affiliations": []
  }, {
    "name": "Ming-Min Zhao",
    "affiliations": []
  }],
  "abstractText": "In this paper we consider nonconvex optimization and learning over a network of distributed nodes. We develop a Proximal Primal-Dual Algorithm (Prox-PDA), which enables the network nodes to distributedly and collectively compute the set of first-order stationary solutions in a global sublinear manner [with a rate of O(1/r), where r is the iteration counter]. To the best of our knowledge, this is the first algorithm that enables distributed nonconvex optimization with global sublinear rate guarantees. Our numerical experiments also demonstrate the effectiveness of the proposed algorithm.",
  "title": "Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks"
}