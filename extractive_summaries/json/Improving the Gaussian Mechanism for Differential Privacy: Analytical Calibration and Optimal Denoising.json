{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Output perturbation is a cornerstone of mechanism design in differential privacy (DP). Well-known mechanisms in this class are the Laplace and Gaussian mechanisms (Dwork et al., 2006; Dwork & Roth, 2014). More complex mechanisms are often obtained by composing multiple applications of these basic output perturbation mechanisms. For example, the Laplace mechanism is the basic building block of the sparse vector mechanism (Dwork et al., 2009), and\n1Amazon Research, Cambridge, UK 2Amazon Web Services, Palo Alto, USA 3University of California, Santa Barbara, USA. Correspondence to: Borja Balle <pigem@amazon.co.uk>, YuXiang Wang <yuxiangw@amazon.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nthe Gaussian mechanism is the building block of private empirical risk minimization algorithms based on stochastic gradient descent (Bassily et al., 2014). Analysing the privacy of such complex mechanisms turns out to be a delicate and error-prone task (Lyu et al., 2017). In particular, obtaining tight privacy analyses leading to optimal utility is one of the main challenges in the design of advanced DP mechanisms. An alternative to tight a-priori analyses is to equip complex mechanisms with algorithmic noise calibration and accounting methods. These methods use numerical computations to, e.g. calibrate perturbations and compute cumulative privacy losses at run time, without relying on hand-crafted worst-case bounds. For example, recent works have proposed methods to account for the privacy loss under compositions occurring in complex mechanisms (Rogers et al., 2016; Abadi et al., 2016).\nIn this work we revisit the Gaussian mechanism and develop two ideas to improve the utility of output perturbation DP mechanisms based on Gaussian noise. The first improvement is an algorithmic noise calibration strategy that uses numerical evaluations of the Gaussian cumulative density function (CDF) to obtain the optimal variance to achieve DP using Gaussian perturbation. The analysis and the resulting algorithm are provided in Section 3. In order to motivate the need for a numerical approach to calibrate the noise of a DP Gaussian perturbation mechanism, we start with an analysis of the main limitations of the classical Gaussian mechanism in Section 2. A numerical evaluation provided in Section 5.1 showcases the advantages of our optimal calibration procedure.\nThe second improvement equips the Gaussian perturbation mechanism with a post-processing step which denoises the output using adaptive estimation techniques from the statistics literature. Since DP is preserved by post-processing and the distribution of the perturbation added to the desired outcome is known, this allows a mechanism to achieve the desired privacy guarantee while increasing the accuracy of the released value. The relevant denoising estimators and their utility guarantees are discussed in Section 4. Results presented in this section are not new: they are the product of a century’s worth of research in statistical estimation. Our contribution is to compile relevant results scattered\nthroughout the literature in a single place and showcase their practical impact in synthetic (Section 5.2) and real (Section 5.3) datasets, thus providing useful pointers and guidelines for practitioners."
  }, {
    "heading": "2. Limitations of the Classical Gaussian Mechanism",
    "text": "Let X be an input space equipped with a symmetric neighbouring relation x ' x′. Let ε ≥ 0 and δ ∈ [0, 1] be two privacy parameters. A Y-valued randomized algorithm M : X → Y is (ε, δ)-DP (Dwork et al., 2006) if for every pair of neighbouring inputs x ' x′ and every possible (measurable) output set E ⊆ Y the following inequality holds:\nP[M(x) ∈ E] ≤ eεP[M(x′) ∈ E] + δ . (1)\nThe definition of DP captures the intuition that a computation on private data will not reveal sensitive information about individuals in a dataset if removing or replacing an individual in the dataset has a negligible effect in the output distribution.\nIn this paper we focus on the family of so-called output perturbation DP mechanisms. An output perturbation mechanism M for a deterministic vector-valued computation f : X → Rd is obtained by computing the function f on the input data x and then adding random noise sampled from a random variable Z to the output. The amount of noise required to ensure the mechanism M(x) = f(x) + Z satisfies a given privacy guarantee typically depends on how sensitive the function f is to changes in the input and the specific distribution chosen for Z. The Gaussian mechanism gives a way to calibrate a zero mean isotropic Gaussian perturbation Z ∼ N (0, σ2I) to the global L2 sensitivity ∆ = supx'x′ ‖f(x)− f(x′)‖ of f as follows. Theorem 1 (Classical Gaussian Mechanism). For any ε, δ ∈ (0, 1), the Gaussian output perturbation mechanism with σ = ∆ √ 2 log(1.25/δ)/ε is (ε, δ)-DP.\nA natural question one can ask about this result is whether this value of σ provides the minimal amount of noise required to obtain (ε, δ)-DP with Gaussian perturbations. Another natural question is what happens in the case ε ≥ 1. This section addresses both these questions. First we show that the value of σ given in Theorem 1 is suboptimal in the high privacy regime ε → 0. Then we show that this problem is in fact inherent to the usual proof strategy used to analyze the Gaussian mechanism. We conclude the section by showing that for large values of ε the standard deviation of a Gaussian perturbation that provides (ε, δ)-DP must scale like Ω(1/ √ ε). This implies that the scaling Θ(1/ε) provided by the classical Gaussian mechanism in the range ε ∈ (0, 1) cannot be extended beyond any bounded interval."
  }, {
    "heading": "2.1. Limitations in the High Privacy Regime",
    "text": "To illustrate the sub-optimality of the classical Gaussian mechanism in the regime ε → 0 we start by showing it is possible to achieve (0, δ)-DP using Gaussian perturbations. This clearly falls outside the capabilities of the classical Gaussian mechanism, since the standard deviation σ = Θ(1/ε) provided by Theorem 1 grows to infinity as ε→ 0. Theorem 2. A Gaussian output perturbation mechanism with σ = ∆/2δ is (0, δ)-DP1.\nPrevious analyses of the Gaussian mechanism are based on a simple sufficient condition for DP in terms of the privacy loss random variable (Dwork & Roth, 2014). The next section explains why the usual analysis of the Gaussian mechanism cannot yield tight bounds for the regime ε → 0. This shows that our example is not a corner case, but a fundamental limitation of trying to establish (ε, δ)-DP through said sufficient condition."
  }, {
    "heading": "2.2. Limitations of Privacy Loss Analyses",
    "text": "Given a vector-valued mechanism M let pM(x)(y) denote the density of the random variable Y = M(x). The privacy loss function of M on a pair of neighbouring inputs x ' x′ is defined as\n`M,x,x′(y) = log\n( pM(x)(y)\npM(x′)(y)\n) .\nThe privacy loss random variable LM,x,x′ = `M,x,x′(Y ) is the transformation of the output random variable Y = M(x) by the function `M,x,x′ . For the particular case of a Gaussian mechanism M(x) = f(x) + Z with Z ∼ N (0, σ2I) it is well-known that the privacy loss random variable is also Gaussian (Dwork & Rothblum, 2016).\nLemma 3. The privacy loss LM,x,x′ of a Gaussian output perturbation mechanism follows a distribution N (η, 2η) with η = D2/2σ2, where D = ‖f(x)− f(x′)‖.\nThe privacy analysis of the classical Gaussian mechanism relies on the following sufficient condition: a mechanism M is (ε, δ)-DP if the privacy loss LM,x,x′ satisfies\n∀x ' x′ : P[LM,x,x′ ≥ ε] ≤ δ . (2)\nSince Lemma 3 shows the privacy loss LM,x,x′ of the Gaussian mechanism is a Gaussian random variable with mean ‖f(x)− f(x′)‖2/2σ2, we have P[LM,x,x′ > 0] ≥ 1/2 for any pair of datasets with f(x) 6= f(x′). This observation shows that in general it is not possible to use this sufficient condition for (ε, δ)-DP to prove that the Gaussian mechanism achieves (0, δ)-DP for any δ < 1/2. In other words,\n1Proofs for all results given in the paper are presented in Appendix A.\nthe sufficient condition is not necessary in the regime ε→ 0. We conclude that an alternative analysis is required in order to improve the dependence on ε in the Gaussian mechanism."
  }, {
    "heading": "2.3. Limitations in the Low Privacy Regime",
    "text": "The last question we address in this section is whether the order of magnitude σ = Θ(1/ε) given by Theorem 1 for ε ≤ 1 can be extended to privacy parameters of the form ε > 1. We show this is not the case by providing the following lower bound.\nTheorem 4. Let f : X → Rd have global L2 sensitivity ∆. Suppose ε > 0 and 0 < δ < 1/2 − e−3ε/ √ 4πε. If the mechanism M(x) = f(x) + Z with Z ∼ N (0, σ2I) is (ε, δ)-DP, then σ ≥ ∆/ √ 2ε.\nNote that as ε → ∞ the upper bound on δ in Theorem 4 converges to 1/2. Thus, as ε increases the range of δ’s requiring noise of the order Ω(1/ √ ε) increases to include all parameters of practical interest. This shows that the rate σ = Θ(1/ε) provided by the classical Gaussian mechanism cannot be extended beyond the interval ε ∈ (0, 1). Note this provides an interesting contrast with the Laplace mechanism, which can achieve ε-DP with standard deviation Θ(1/ε) in the low privacy regime."
  }, {
    "heading": "3. The Analytic Gaussian Mechanism",
    "text": "The limitations of the classical Gaussian mechanism described in the previous section suggest there is room for improvement in the calibration of the variance of a Gaussian perturbation to the corresponding global L2 sensitivity. Here we present a method for optimal noise calibration for Gaussian perturbations that we call analytic Gaussian mechanism. To do so we must address the two sources of slack in the classical analysis: the sufficient condition (2) used to reduce the analysis to finding an upper bound for P[N (η, 2η) > ε], and the use of a Gaussian tail approximation to obtain such upper bound. We address the first source of slack by showing that the sufficient condition in terms of the privacy loss random variable comes from a relaxation of a necessary and sufficient condition involving two privacy loss random variables. When specialized to the Gaussian mechanism, this condition involves probabilities about Gaussian random variables, which instead of approximating by a tail bound we represent explicitly in terms of the CDF of the standard univariate Gaussian distribution:\nΦ(t) = P[N (0, 1) ≤ t] = 1√ 2π\n∫ t\n−∞ e−y 2/2dy .\nUsing this point of view, we introduce a calibration strategy for Gaussian perturbations that requires solving a simple optimization problem involving Φ(t). We discuss how to solve this optimization at the end of this section.\nThe first step in our analysis is to provide a necessary and sufficient condition for differential privacy in terms of privacy loss random variables. This is captured by the following result.\nTheorem 5. A mechanism M : X→ Y is (ε, δ)-DP if and only if the following holds for every x ' x′:\nP[LM,x,x′ ≥ ε]− eεP[LM,x′,x ≤ −ε] ≤ δ . (3)\nNote that Theorem 5 immediately implies the sufficient condition given in (2) through the inequality\nP[LM,x,x′ ≥ ε]− eεP[LM,x′,x ≤ −ε] ≤ P[LM,x,x′ ≥ ε] .\nNow we can use Lemma 3 to specialize (3) for a Gaussian output perturbation mechanism. The relevant computations are packaged in the following result, where we express the probabilities in (3) in terms of the Gaussian CDF Φ.\nLemma 6. Suppose M(x) = f(x) + Z is a Gaussian output perturbation mechanism with Z ∼ N (0, σ2I). For any x ' x′ let D = ‖f(x) − f(x′)‖. Then the following hold for any ε ≥ 0:\nP[LM,x,x′ ≥ ε] = Φ ( D\n2σ − εσ D\n) , (4)\nP[LM,x′,x ≤ −ε] = Φ ( −D\n2σ − εσ D\n) . (5)\nThis result specializes the left hand side of (3) in terms of the distance D = ‖f(x)− f(x′)‖ between the output means on a pair of neighbouring datasets. To complete the derivation of our analytic Gaussian mechanism we need to ensure that (3) is satisfied for every pair x ' x′. The next lemma shows that this reduces to plugging the global L2 sensitivity ∆ in the place of D in (4) and (5).\nLemma 7. For any ε ≥ 0, the function h : R≥0 → R defined as follows is monotonically increasing:\nh(η) = P[N (η, 2η) ≥ ε]− eεP[N (η, 2η) ≤ −ε] .\nNow we are ready to state our main result, whose proof follows directly from Theorem 5, Lemma 7, and equations (4) and (5).\nTheorem 8 (Analytic Gaussian Mechanism). Let f : X→ Rd be a function with globalL2 sensitivity ∆. For any ε ≥ 0 and δ ∈ [0, 1], the Gaussian output perturbation mechanism M(x) = f(x) +Z with Z ∼ N (0, σ2I) is (ε, δ)-DP if and only if\nΦ\n( ∆\n2σ − εσ ∆\n) − eεΦ ( − ∆\n2σ − εσ ∆\n) ≤ δ . (6)\nThis result shows that in order to obtain an (ε, δ)-DP Gaussian output perturbation mechanism for a function f with\nAlgorithm 1: Analytic Gaussian Mechanism Public Inputs: f , ∆, ε, δ Private Inputs: x Let δ0 = Φ(0)− eεΦ(− √ 2ε) if δ ≥ δ0 then Define B+ε (v) = Φ( √ εv)− eεΦ(− √ ε(v + 2))\nCompute v∗ = sup{v ∈ R≥0 : B+ε (v) ≤ δ} Let α = √ 1 + v∗/2− √ v∗/2\nelse Define B−ε (u) = Φ(− √ εu)− eεΦ(− √ ε(u+ 2))\nCompute u∗ = inf{u ∈ R≥0 : B−ε (u) ≤ δ} Let α = √ 1 + u∗/2 + √ u∗/2\nLet σ = α∆/ √\n2ε Return f(x) +N (0, σ2I)\nglobal L2 sensitivity ∆ it is enough to find a noise variance σ2 satisfying (6). One could now use upper and lower bounds for the tail of the Gaussian CDF to derive an analytic expression for a parameter σ satisfying this constraint. However, this again leads to a suboptimal result due to the slack in these tail bounds in the non-asymptotic regime. Instead, we propose to find σ using a numerical algorithm by leveraging the fact that the Gaussian CDF can be written as Φ(t) = (1 + erf(t/ √ 2))/2, where erf is the standard error function. Efficient implementations of this function to very high accuracies are provided by most statistical and numerical software packages. However, this strategy requires some care in order to avoid numerical stability issues around the point where the expression ∆/2σ− εσ/∆ in (6) changes sign. Thus, we further massage the left hand side (6) we obtain the implementation of the analytic Gaussian mechanism given in Algorithm 1. The correctness of this implementation is provided by the following result.\nTheorem 9. Let f be a function with global L2 sensitivity ∆. For any ε > 0 and δ ∈ (0, 1), the mechanism described in Algorithm 1 is (ε, δ)-DP.\nGiven a numerical oracle for computing Φ(t) based on the error function it is relatively straightforward to implement a solver for finding the values v∗ and u∗ needed in Algorithm 1. For example, using the fact that B+ε (v) is monotonically increasing we see that computing v∗ is a root finding problem for which one can use Newton’s method since the derivative of Φ(t) can be computed in closed form using Leibniz’s rule. In practice we find that a simple scheme based on binary search initiated from an interval obtained by finding the smallest k ∈ N such that B+ε (2k) > δ provides a very efficient and robust way to find v∗ up to arbitrary accuracies (the same applies to u∗)."
  }, {
    "heading": "4. Optimal Denoising",
    "text": "Can we improve the performance of analytical Gaussian mechanism even further? The answer is “yes” and “no”. We can’t because Algorithm 1 is already the exact calibration of the Gaussian noise level to the given privacy budget. But if we consider the problem of designing the best differentially private procedure M(x) that approximates f(x), then there could still be room for improvement.\nIn this section, we consider a specific class of mechanisms that denoise the output of a Gaussian mechanism. Let ŷ ∼ N (f(x), σ2I), we are interested in designing a postprocessing function g such that ỹ = g(ŷ) is closer to f(x) than ŷ. This class of mechanisms are of particular interest for differential privacy because (1) since differential privacy is preserved by post-processing, releasing a function ỹ = g(ŷ) of a differentially private output is again differentially private; (2) since information about f and the distribution of the noise are publicly known, this information can be leveraged to design denoising functions.\nThis is a statistical estimation problem, where f(x) is the underlying parameter and ŷ is the data. Since in this case we are adding the noise ourselves, it is possible to use the classical statistical theory on Gaussian models as is because the Gaussian assumption is now true by construction. This is however an unusual estimation problem where all we observe is a single data point. Since ŷ is the maximum likelihood estimator, if there is no additional information about f(x), we cannot hope to improve the estimation error uniformly over all f(x) ∈ Rd. But there is still something we can do when we consider either of the following assumptions: (A.1) x is drawn from some underlying distribution, thus inducing some distribution on f(x); or, (A.2) ‖f(x)‖p ≤ B for some p,B > 0, where ‖ · ‖p is the Lpnorm (or pseudo-norm when p < 1).\nOptimal Bayesian denoising. Assumption A.1 translates the problem of optimal denoising into a Bayesian estimation problem, where the underlying parameter f(x) has a prior distribution, and the task is to find an estimator that attains the Bayes risk — the minimum of the average estimation error integrated over a prior π, defined as\nR(π) = min g:Rd→Rd\nE [ E[‖g(ŷ)− f(x)‖2|f(x)] ] .\nFor square loss, the Bayes estimator is simply the posterior mean estimator, as the following theorem shows:\nTheorem 10. Let x ∼ π and assume the induced distribution of f(x) is square integrable. Then the Bayes estimator ỹBayes is given by\nỹBayes = argmin g:Rd→Rd\nE [ ‖g(ŷ)− f(x)‖2 ] = E[f(x)|ŷ] .\nThe proof can be found in any standard statistics textbook (see, e.g., Lehmann & Casella, 2006). One may ask what the corresponding MSE is and how much it improves over the version without post-processing. The answer depends on the prior and the amount of noise added for differential privacy. When f(x) ∼ N (0, w2I), the posterior mean estimator can be written analytically into ỹBayes = (w\n2/(w2 + σ2))ŷ, and the corresponding Bayes risk is E[‖ỹBayes − f(x)‖2] = dw2σ2/(σ2 + w2). In other word, we get a factor of w2/(w2 + σ2) improvement over simply using ŷ.\nIn general, there is no analytical form for the posterior mean, but if we can evaluate the density of f(x) or sample from the distribution of x, then we can obtain an arbitrarily good approximation of ỹBayes using Markov Chain Monte Carlo techniques.\nOptimal frequentist denoising. Assumption A.2 spells out a minimax estimation problem, where the underlying parameter f(x) is assumed to be within a set S ⊂ Rd. In particular, we are interested in finding ỹminimax that attains the minimax risk\nR(S) = min g:Rd→Rd max f(x)∈S\nE [ ‖g(ŷ)− f(x)‖2 ] ,\non Lp balls S = B(p,B) = {y ∈ Rd | ‖y‖p ≤ B} of radius B.\nA complete characterization of this minimax risk (up to a constant) is given by Birgé & Massart (2001, Proposition 5), who show that in the non-trivial region2 of the signal to noise ratio B/σ, the ball S = B(p,B) satisfies\nR(S) = Θ ( Bpσ2−p ( 1 + log ( dσp\nBp\n))1−p/2) (7)\nfor 0 < p < 2 and when p ≥ 2, Donoho et al. (1990) show that\nR(S) = Θ\n( B2σ2\nσ2 +B2/d\n) .\nDeriving exact minimax estimators is challenging and most analyses assume certain asymptotic regimes (see the case for p = 2 by Bickel et al. (1981)). Nonetheless, some techniques have been shown to match R(B(p,B)) up to a small constant factor in the finite sample regime (see, e.g., Donoho et al., 1990; Donoho & Johnstone, 1994). This means that we can often improve the square error from dσ2 toR(B(p,B)) when we have the additional information that f(x) is in some Lp ball. This could be especially helpful in the high-dimensional case for p < 2. For instance if p = 1 and B = σ, then we obtain a risk σB √ 1 + log(dσ/B),\n2When √\nlog d ≤ B/σ ≤ cpd1/p for a constant cp that depends only on p.\nwhich improves exponentially in d over the dσ2 risk of ŷ. More practically, if f(x) is a sparse histogram with s nonzero elements, then taking p → 0 will result in an error bound on the order of sσ2(1 + log(d)), which is linear in the sparsity s rather than the dimension d.\nAdaptive estimation. What if we do not know the prior parameter w2, or a right choice of B and p? Can we still come up with estimators that take advantage of these structures? It turns out that this is the problem of designing adaptive estimators which sits at the heart of statistical research. An adaptive estimator in our case, is one that does not need to know w2 or a pair of B and p, yet behave nearly as well as Bayes estimator that knows w2 or the minimax estimator that knows B and p for each parameter regime.\nWe first give an example of an adaptive Bayes estimator that does not require us to specify a prior, yet can perform almost as well as the optimal Bayes estimator for all isotropic Gaussian prior simultaneously.\nTheorem 11 (James-Stein estimator and its adaptivity). When d ≥ 3, substituting w2 in ỹBayes with its maximum likelihood estimate under\nf(x) ∼ N (0, w2I) , ŷ|f(x) ∼ N (f(x), σ2I)\nproduces the James-Stein estimator\nỹJS =\n( 1− (d− 2)σ 2 ‖ŷ‖2 ) ŷ.\nMoreover, it has an MSE\nE [ ‖ỹJS − f(x)‖2 ] = dσ2 ( 1− (d− 2) 2\nd2 σ2 w2 + σ2\n) .\nThe James-Stein estimator has the property that it always improves the MLE ŷ when d ≥ 3 (Stein, 1956) and it always achieves a risk that is within a d2/(d − 2)2 multiplicative factor of the Bayes risk of ỹBayes for any w2.\nWe now move on to describe a method that is adaptive to B and p in minimax estimation. Quite remarkably, Donoho (1995) shows that choosing λ = σ √ 2 log d in the softthresholding estimator\nỹTH = sign(ŷ) max{0, |ŷ| − λ} (8)\nyields a nearly optimal estimator for every Lp ball.\nTheorem 12 (The adaptivity of soft-thresholding, Theorem 4.2 of (Donoho, 1995)). Let S = B(p,B) for some p,B > 0. The soft-thresholding estimator with λ = σ √ 2 log d obeys that\nsup f(x)∈S\nE [ ‖ỹTH − f(x)‖2 ] ≤ (2 log d+1)(σ2+2.22R(S)) .\nThe result implies that the soft-thresholding estimator is nearly optimal for all balls up to a multiplicative factor of 4.44 log(d).\nThanks to the fact that we know the parameter σ exactly, both ỹJS and ỹTH are now completely free of tuning parameters. Yet, they can achieve remarkable adaptivity that covers a large class of model assumptions and function classes. A relatively small price to pay for such adaptivity is that we might lose a constant (or a log(d)) factor. Whether such adaptivity is worth will vary on a case-by-case basis.\nTake the problem of private releasing a histogram of n items in d bins. Theorem 12 and Equation (7) with p ≤ 1 imply that the soft-thresholding estimator obeys\nE [ ‖ỹTH − f(x)‖2 ] = Õ ( min{sσ2, n1/kσ2−1/k} ) .\nwhere s denotes the number of nonzero elements in f(x) and k is the largest power-law exponent greater than 1 such that order statistics f(x)(d−i+1) ≤ ni−k for all i = 1, ..., d and Õ hides logarithmic factors in d, dσ/n. The fact that s ≤ d implies that the soft-thresholding estimator improves over the naive private release for all d, n, s and the n1/k factor suggests that we can take advantage of an unknown power law distribution even if the histogram is not really sparse. This makes our technique effective in the many data mining problems where power law distributions occur naturally (Faloutsos et al., 1999).\nRelated work. Denoising as a post-processing step in the context of differentially privacy is not a new idea. Notably, Barak et al. (2007); Hay et al. (2009) show that a postprocessing step enforcing consistency of contingency table releases and graph degree sequences leads to more accurate estimations. Williams & McSherry (2010) sets up the general statistical (Bayesian) inference problem of DP releases by integrating auxiliary information (a prior). Karwa et al. (2016) exploits knowledge of the noise distribution use to achieve DP in the inference procedure of a network model and shows that it helps to preserve asymptotic efficiency. Nikolov et al. (2013) demonstrates that projecting linear regression solutions to a known `1-ball improves the estimation error from O(poly(d)) to O(polylog(d)) when the underlying ground truth is sparse. Bernstein et al. (2017) uses Expectation–Maximization to denoise the parameters of a class of graphical models starting from noisy empirical moments obtained using the Laplace mechanism.\nIn all the above references there is some prior knowledge (constraint sets, sparsity or Bayesian prior) that is exploited to improve the utility of DP releases. To the best of our knowledge, we are the first to consider “adaptive estimation” and demonstrate how classical techniques can be helpful even without such prior knowledge. These estimators are not new; they have been known in the statistics literature for\ndecades. Our purpose is to compile facts that are relevant to the practice of DP and initiate a systematic study of how these ideas affect the utility of DP mechanisms, which we complement with the experimental evaluation presented in the next section."
  }, {
    "heading": "5. Numerical Experiments",
    "text": "This section provides an experimental evaluation of the improvements in utility provided by optimal calibration and adaptive denoising. First we numerically compare the variance of the analytic Gaussian mechanism and the classical mechanism for a variety of privacy parameters. Then we evaluate the contributions of denoising and analytic calibration against a series of baselines for the task of private mean estimation using synthetic data. We also evaluate several denoising strategies on the task of releasing heat maps based on the New York City taxi dataset under differential privacy. Further experiments are presented in Appendix B, including an evaluation of denoising strategies for the task of private histogram release."
  }, {
    "heading": "5.1. Analytic Gaussian Mechanism",
    "text": "We implemented Algorithm 1 in Python3 and ran experiments to compare the variance of the perturbation obtained with the analytic Gaussian mechanism versus the variance required by the classical Gaussian mechanism. In all our experiments the values of v∗ and u∗ were solved up to an accuracy of 10−12 using binary search and the implementation of the erf function provided by SciPy (Jones et al., 2001).\nThe results are presented in the two leftmost panels in Figure 1. The plots show that as ε→ 0 the optimally calibrated perturbation outperforms the classical mechanism by several orders of magnitude. Furthermore, we see that even for values of ε close to 1 our mechanism reduces the variance by a factor of 1.4 or more, with higher improvements for larger values of δ."
  }, {
    "heading": "5.2. Denoising for Mean Estimation",
    "text": "Our next experiment evaluates the utility of denoising combined with the analytical Gaussian mechanism for the task of private mean estimation. The input to the mechanism is a dataset x = (x1, . . . , xn) containing n vectors xi ∈ Rd and the underlying deterministic functionality is f(x) = (1/n) ∑n i=1 xi. This relatively simple task is a classic example from the family of linear queries which are frequently considered in the differential privacy literature. We compare the accuracy of several mechanisms M for releasing a private version of f(x) in terms of the Euclidean\n3See https://github.com/BorjaBalle/ analytic-gaussian-mechanism.\ndistance ‖M(x)− f(x)‖2. In particular, we test the analytical Gaussian mechanism with either James-Stein denoising cf. Theorem 11 (aGM-JS) or optimal thresholding denoising cf. Theorem 12 (aGM-TH), as well as several baselines including: the classical Gaussian mechanism (cGM), the analytical Gaussian mechanism without denoising (aGM), and the Laplace mechanism (Lap) using the same ε parameter as the Gaussian mechanisms.\nTo provide a thorough comparison we explore of the different parameters of the problem on the final utility. The key parameters of the problem are the dimension d and the DP parameters ε and δ. The dimension affects the utility through the bounds provided in Theorem 11 and Theorem 12. The DP parameters affect the utility through the variance σ2 of the mechanism, which is also affected by the sample size n via the global sensitivity. Thus, we can characterize the effect of σ2 by keeping n fixed and changing the DP parameters. In our experiments we consider a fixed sample size n = 500 and privacy parameter δ = 10−4 while trying several values for ε.\nThe other parameter that affects the utility is the “size” of f(x), controlled either through the variance w2 or the norm ball S. Since the denoising estimators we use are adaptive to these parameters and do not need to know them in advance, we sample the dataset x repeatedly to obtain a diversity of values for f(x). Each dataset x is sampled as follows: first sample a center x0 ∼ N (0, I) and then build x = (x1, . . . , xn) with xi = x0 + ξi, where each ξi is i.i.d. with independent coordinates sampled uniformly from the interval [−1/2, 1/2]. Thus, in each dataset the points xi all lie in an L∞-ball of radius 1, leading to a global L2 sensitivity ∆2 = √ d/n and a global L1 sensitivity ∆1 = d/n. These are used to calibrate the Gaussian and Laplace perturbations, respectively.\nThe results are presented in two rightmost panels of Figure 1. Each point in every plot is the result of averaging the error\nover 100 repetitions with different datasets. The first plot uses ε = 0.01 and shows how denoised methods improve the accuracy over all the other methods, sometimes by orders of magnitude. The second plot shows that for this problem the James-Stein estimator provides better accuracy in the high-dimensional setting."
  }, {
    "heading": "5.3. New York City Taxi Heat Maps",
    "text": "In this section, we apply our method to New York City taxi data. The dataset is a collection of time-stamped pick-ups and drop-offs of taxi drivers and we are interested in sharing a density map of such pick-ups and drop-offs in Manhattan at a specific time of a specific day under differential privacy.\nThis is a problem of significant practical interest. Ever since the NYC Taxi & Limousine Commission released this dataset, there has been multiple independent reports concerning the security and privacy risks this dataset poses for taxi drivers and their passengers (see, e.g., Pandurangan; Douriez et al., 2016). The techniques presented in this paper allow us to provably prevent individuals (on both the per-trip level and per-cab level) in the dataset from being identified, while remarkably, permitting the release of rich information about the data with fine-grained spatial and temporal resolution.\nSpecifically, we apply the analytical Gaussian mechanism to release the number of picks-ups and drop-offs at every traffic junction in Manhattan. There are a total of 3,784 such traffic junctions and they are connected by 7,070 sections of roads. We will treat them as nodes and edges on a graph. In the post-processing phase, we apply graph smoothing techniques to reveal the underlying signal despite the noise due to aGM. Specifically, we compare the JS-estimator and the soft-thresholding estimator we described in Section 4, as well as the same soft-thresholding estimator applied to the coefficients of a graph wavelet transform due to Sharpnack et al. (2013). The basis transformation is important be-\ncause the data might be sparser in the transformed domain. For reference, we also include the state-of-the-art graph smoothing techniques called graph trend filtering (Wang et al., 2016), which has one additional tuning parameter but has been shown to perform significantly better than wavelet smoothing in practice.\nOur experiments provide cab-level differential privacy by assuming that every driver does a maximum of 5 trips within an hour so that we have a global L2-sensitivity of ∆ = 5. This is a conservative but reasonable estimate and can be enforced by preprocessing the data. Data within each hour is gathered and distributed to each traffic junction using a kernel density estimator; further details are documented in Doraiswamy et al. (2014).\nWe present some qualitative comparisons in Figure 2, where we visualize the privately released heat map with and without post-processing. Relatively speaking, trend filtering performs better than wavelet smoothing, but both approaches significantly improves the RMSE over the DP release without post-processing. The results in Appendix B provide quantitative results by comparing the mean square error of cGM, aGM as well as the aforementioned denoising techniques for data corresponding to different time intervals."
  }, {
    "heading": "6. Conclusion and Discussion",
    "text": "In this paper, we embark on a journey of pushing the utility limit of Gaussian mechanism for (ε, δ)-differential privacy. We propose a novel method to obtain the optimal calibration\nof Gaussian perturbations required to attain a given DP guarantee. We also review decades of research in statistical estimation theory and show that combining these techniques with differential privacy one obtains powerful adaptivity that denoises differentially private outputs nearly optimally without additional hyperparameters. On synthetic data and on the New York City Taxi dataset we illustrate a significant gain in estimation error and fine-grained spatial-temporal resolution.\nThere are a number of theoretical problems of interest for future work. First, on the problem of differentially private estimation. Our post-processing approach effectively restricts our choice of algorithms to the composition of privacy release and post-processing. While we now know that we are optimal in both components, it is unclear whether we lose anything relative to the best differentially private algorithms. Secondly, the analytical calibration proposed in this paper is optimal for achieving (ε, δ)-DP with Gaussian noise. But when building complex mechanisms we are stuck in the dilemma of choosing between (a) using the aGM with the advanced composition (Kairouz et al., 2015); or, (b) using Rényi DP (Mironov, 2017) or zCDP (Bun & Steinke, 2016) for tighter composition and calculate the (ε, δ) from moment bounds. While (a) is tighter in the calculation the privacy parameters of each intermediate value, (b) is tighter in the composition but cannot take advantage of aGM. It would be interesting if we could get the best of both worlds."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank Doraiswamy et al. (2014) for sharing their preprocessed NYC taxi dataset, the anonymous reviewers for helpful comments that led to improvements of the paper and Stephen E. Fienberg for discussions that inspired the authors to think about optimal post-processing."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep learning with differential privacy",
    "authors": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"],
    "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,",
    "year": 2016
  }, {
    "title": "Private empirical risk minimization: Efficient algorithms and tight error bounds",
    "authors": ["R. Bassily", "A. Smith", "A. Thakurta"],
    "venue": "In Foundations of Computer Science (FOCS),",
    "year": 2014
  }, {
    "title": "Differentially private learning of undirected graphical models using collective graphical models",
    "authors": ["G. Bernstein", "R. McKenna", "T. Sun", "D. Sheldon", "M. Hay", "G. Miklau"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2017
  }, {
    "title": "Minimax estimation of the mean of a normal distribution when the parameter space is restricted",
    "authors": ["P Bickel"],
    "venue": "The Annals of Statistics,",
    "year": 1981
  }, {
    "title": "Gaussian model selection",
    "authors": ["L. Birgé", "P. Massart"],
    "venue": "Journal of the European Mathematical Society,",
    "year": 2001
  }, {
    "title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds",
    "authors": ["M. Bun", "T. Steinke"],
    "venue": "In Theory of Cryptography Conference,",
    "year": 2016
  }, {
    "title": "De-noising by soft-thresholding",
    "authors": ["D.L. Donoho"],
    "venue": "IEEE transactions on information theory,",
    "year": 1995
  }, {
    "title": "Minimax risk over pballs for p-error",
    "authors": ["D.L. Donoho", "I.M. Johnstone"],
    "venue": "Probability Theory and Related Fields,",
    "year": 1994
  }, {
    "title": "Minimax risk over hyperrectangles, and implications",
    "authors": ["D.L. Donoho", "R.C. Liu", "B. MacGibbon"],
    "venue": "The Annals of Statistics,",
    "year": 1990
  }, {
    "title": "Using topological analysis to support eventguided exploration in urban data",
    "authors": ["H. Doraiswamy", "N. Ferreira", "T. Damoulas", "J. Freire", "C.T. Silva"],
    "venue": "IEEE transactions on visualization and computer graphics,",
    "year": 2014
  }, {
    "title": "Anonymizing nyc taxi data: Does it matter",
    "authors": ["M. Douriez", "H. Doraiswamy", "J. Freire", "C.T. Silva"],
    "venue": "In Data Science and Advanced Analytics (DSAA),",
    "year": 2016
  }, {
    "title": "The algorithmic foundations of differential privacy",
    "authors": ["C. Dwork", "A. Roth"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Concentrated differential privacy",
    "authors": ["C. Dwork", "G.N. Rothblum"],
    "venue": "arXiv preprint arXiv:1603.01887,",
    "year": 2016
  }, {
    "title": "Calibrating noise to sensitivity in private data analysis",
    "authors": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"],
    "venue": "In Theory of cryptography,",
    "year": 2006
  }, {
    "title": "On the complexity of differentially private data release: efficient algorithms and hardness results",
    "authors": ["C. Dwork", "M. Naor", "O. Reingold", "G.N. Rothblum", "S. Vadhan"],
    "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,",
    "year": 2009
  }, {
    "title": "On power-law relationships of the internet topology",
    "authors": ["M. Faloutsos", "P. Faloutsos", "C. Faloutsos"],
    "venue": "In ACM SIGCOMM computer communication review,",
    "year": 1999
  }, {
    "title": "Values of mills’ ratio of area to bounding ordinate and of the normal probability integral for large values of the argument",
    "authors": ["R.D. Gordon"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1941
  }, {
    "title": "Accurate estimation of the degree distribution of private networks",
    "authors": ["M. Hay", "C. Li", "G. Miklau", "D. Jensen"],
    "venue": "In Data Mining,",
    "year": 2009
  }, {
    "title": "SciPy: Open source scientific tools for Python, 2001. URL http: //www.scipy.org",
    "authors": ["E. Jones", "T. Oliphant", "P Peterson"],
    "year": 2001
  }, {
    "title": "The composition theorem for differential privacy",
    "authors": ["P. Kairouz", "S. Oh", "P. Viswanath"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Inference using noisy degrees: Differentially private β-model and synthetic graphs",
    "authors": ["V. Karwa", "A Slavković"],
    "venue": "The Annals of Statistics,",
    "year": 2016
  }, {
    "title": "Theory of point estimation",
    "authors": ["E.L. Lehmann", "G. Casella"],
    "venue": "Springer Science & Business Media,",
    "year": 2006
  }, {
    "title": "Understanding the sparse vector technique for differential privacy",
    "authors": ["M. Lyu", "D. Su", "N. Li"],
    "venue": "Proceedings of the VLDB Endowment,",
    "year": 2017
  }, {
    "title": "Renyi differential privacy",
    "authors": ["I. Mironov"],
    "venue": "In Computer Security Foundations Symposium (CSF),",
    "year": 2017
  }, {
    "title": "The geometry of differential privacy: the sparse and approximate cases",
    "authors": ["A. Nikolov", "K. Talwar", "L. Zhang"],
    "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
    "year": 2013
  }, {
    "title": "Privacy odometers and filters: Pay-as-you-go composition",
    "authors": ["R.M. Rogers", "A. Roth", "J. Ullman", "S. Vadhan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Detecting activations over graphs using spanning tree wavelet bases",
    "authors": ["J. Sharpnack", "A. Singh", "A. Krishnamurthy"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2013
  }, {
    "title": "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution",
    "authors": ["C. Stein"],
    "venue": "In Proceedings of the Third Berkeley symposium on mathematical statistics and probability,",
    "year": 1956
  }, {
    "title": "Dirichlet draws are sparse with high probability",
    "authors": ["M. Telgarsky"],
    "venue": "CoRR, abs/1301.4917,",
    "year": 2013
  }, {
    "title": "Probabilistic inference and differential privacy",
    "authors": ["O. Williams", "F. McSherry"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }],
  "id": "SP:20d0fc672d8e1aac6c4b9833e9ae858d2e6c5686",
  "authors": [{
    "name": "Borja Balle",
    "affiliations": []
  }, {
    "name": "Yu-Xiang Wang",
    "affiliations": []
  }],
  "abstractText": "The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime (ε → 0) and it cannot be extended to the low privacy regime (ε → ∞). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the highdimensional regime.",
  "title": "Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising"
}