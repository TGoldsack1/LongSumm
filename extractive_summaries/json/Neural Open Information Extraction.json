{
  "sections": [{
    "text": "Keywords: Open Information Extraction Word embeddings RNN GRU LSTM"
  }, {
    "heading": "1 Introduction",
    "text": "Natural Language Processing (NLP) techniques that facilitates the process of fetching important information from large data are highly demanded. With the ongoing development in the field of NLP, OIE gained a massive amount of attention in the past years. It is the process of extracting a relation tuple from a text corpus in the form of <Entity1> <Relation> <Entity 2> as seen in Table 1.\nOIE plays a fundamental role in turning massive, unstructured text corpora into factual information, it can be used as a foundation to many NLP tasks, including, Information Extraction, Question Answering and Summarization.\nPreviously, OIE paradigms either utilized automatically assembled training data or hand-crafted heuristics. Nonetheless, after deep learning techniques paved their way in various NLP tasks researchers aimed their focus towards neural networks.\nRNN is a robust class of artificial neural networks, contrary to Feed-Forward networks, RRNs can loop among nodes, thus it’s capable of apprehending temporal behavior. This results in permitting information to persist in them, by selecting which information to keep and which to forget by taking into consideration the current input and the previous data it received.\n© Springer Nature Switzerland AG 2019 E. Métais et al. (Eds.): NLDB 2019, LNCS 11608, pp. 359–367, 2019. https://doi.org/10.1007/978-3-030-23281-8_31\nIn this paper we present an OIE model that employs RNNs to extract relation triples. Recently, RNNs proved their importance by achieving notable performance in various NLP tasks such as translation [1] and speech recognition [2], they are heavily applied in Google Home [3] and Amazon’s Alexa [4]. The features that make RNNs a good fit for NLP applications are notable [5]. For instance, they take into consideration the order of the words, in addition, GPU can be utilized to carry out RNN computation therefore they perform well on large datasets. Also, RNNs can handle arbitrary input and output lengths. Furthermore, we demonstrate that contextual embedding enhances the overall performance of OIE task compared to non-contextual word embedding techniques.\nThe remainder of this paper is structured as follows; Sect. 2 reviews the existing OIE state-of-art models, while Sect. 3 presents the proposed OIE model, followed by the results and evaluation in Sect. 4. Finally, Sect. 5 concludes the paper and discusses future work."
  }, {
    "heading": "2 Related Work",
    "text": "In this section we review existing OIE state-of-art architectures, a complete picture can be found in [6]. OIE can be categorized into two broad categories, approaches that requires automatically machine learning classifiers and approaches that utilizes handcrafted rules [7]. Newly, deep learning techniques started paving their way towards OIE systems.\n2.1 Machine Learning Classifiers\nIn 2007, Banko et al. [8] introduced TextRunner, the first OIE system is a fully implemented, highly adaptable, self-supervised system that relies on shallow syntactic analysis. It makes use of a domain-independent technique on a text corpus in order to extract relation tuples. TextRunner extracts all possible relation tuples by making a single pass over the corpus using Conditional Random Field classifier, tuples that are classified as trustworthy are reserved by the extractor.\nWikipedia-based Open Extractor (WOE) system [9], introduced by Wu and Weld, that operate in two modes: WOEPos and WOEParse. The WOEPos system employs a CRF extractor trained with shallow syntactic features, in contrast to WOEParse that makes use of a rich dictionary of dependency path patterns. Heuristically matching\nWikipedia info box values with corresponding text for automatic assembly of training examples is the primary idea behind WOE herby enhancing TextRunner’s performance."
  }, {
    "heading": "2.2 Hand-Crafted Rules",
    "text": "REVERB proposed by Fader et al. [10]. REVERB relies on the process of relation phrases that meet syntactic and lexical constraints, afterwards it extracts noun phrase argument pairs for each relation phrase. Logistic regression classifier is latter used to assign a confidence score for each extracted tuple. Subsequently, Etzioni et al. [11] presented the second generation of OIE, R2A2 by combing REVERB with an argument identifier - ARGLEARNER - to enrich argument extraction for the relation phrases.\nDel Corro and Gemulla proposed ClausIE [12], a clause-based OIE system that expoilts the linguistic knowledge of the grammar of the English language to locate clauses in an input corpus. It determines the dependency parse of the input sentence to realize its syntactical structure. Then, the algorithm acquires a set of coherent derived clauses based on the dependency parse and small domain-independent lexica and generate one or more propositions for each clause. ClausIE fundamentally vary from the aforementioned OIE systems in the way that it doesn’t utilize any training data in contrast to REVERB [10] and TextRunner [8]."
  }, {
    "heading": "2.3 Neural Approaches",
    "text": "A neural OIE paradigm was proposed by Cui et al. [13] that employs a Recurrent Neural Network (RNN) encoder-decoder framework. The encoder-decoder infrastructure is a method for text generation and has already been utilized in other NLP tasks successfully as illustrated in [13] The encoder inputs a variable length sequence and outputs a compressed representation vector, which is then passed to the decoder, resulting in the output sequence produced by the decoder. Both the encoder and decoder use a 3-layer Long Short-Term Memory (LSTM) [14]. Training data is obtained from high confidence binary extractions from state-of-the-art OIE system. Thus, the extraction of high-quality tuples.\nIn addition to the work of Cui et al., Stanovsky et al. [15] developed a Bidirectional LSTM transducer to extract OIE tuples, proving that supervised learning can have a strong impact on OIE performance. By extending the work made on deep semantic role labeling to extract OIE tuples authors of [15] were able to achieve notable results. Moreover, their work emphasis that research on Question Answering-Semantic Role Labeling paradigms can greatly benefit future OIE models."
  }, {
    "heading": "3 Proposed Model",
    "text": "Our proposed model is built on the work of Stanovsky et al. [15] by treating OIE task as a sequencing labeling problem resulting in the extraction of multiple, overlapping tuples for each sentence.\nThe proposed neural network framework takes a fixed length vector of an embedded sentence as an input. In addition, predicates are the building blocks of any language, they denote strong actions which are considered extremely effective in extracting relations of interest. Thus, following the work of [15], we assume that the predicate in each sentence represents the relation that’s associated with the tuple, therefore the predicate is sent to the network as a feature vector along with the Part of Speech (POS) tag of the sentence using NLTK [16]."
  }, {
    "heading": "3.1 Contextual Embedding",
    "text": "ELMo (Embedding from Language Models) [17] is a deep contextualized word representation that models both: complex syntactic and semantic features of a word and the way in which these words’ uses differ throughout linguistic. The key idea behind ELMo is contextual embedding, thus the representation of each word differs according to its neighboring words. The generated word vectors are acquired from the functions of the internal states of a deep bidirectional language model, which is pre-trained on a large dataset. We integrated ELMo embedding in our OIE model, results proved that contextual embedding yield to a better performance. The aforementioned neural OIE methods utilized either GloVe [18] or Word2Vec [19], both are non-contextual word embeddings. Comparative results are demonstrated in the subsequent section."
  }, {
    "heading": "3.2 GRU Model Architecture",
    "text": "RNNs are hard to train due the vanishing and the exploding gradient descent problems during the back-propagating process [20]. Efforts were made to overcome this complication, hence LSTMs and GRUs were developed. They both successfully dealt with the difficultly of training RNNs. Indeed, LSTM and GRU are considered very effective models for learning very long contexts. The way they are used in [21] allows to train on long word-contexts.\nGRUs are comparatively new and employs fewer number of parameters than LSTMs which eventually entails that GRUs are both lighter and faster to train than LSTM. GRU merges LSTM’s Input and Forget gate in the Update gate. In Addition, it merges the cell state and the hidden state which lowers the complexity of the model.\nContrary to LSTM, GRU has 2 gates instead of 3:\n• Reset Gate: that decides how to integrate the previous memory with the current input. • Update Gate: that determines the amount that it should keep from the prior memory.\nFor GRU, the hidden state Ht is computed as [22]:\nZt ¼ rðXtUzþHt 1WzÞ ð1Þ\nRt ¼ rðXtUr þHt 1WrÞ ð2Þ\nht ¼ tanhðXtUh þðRt Ht 1ÞWhÞ ð3Þ\nHt ¼ ð1 ZtÞ ht 1þZt ht ð4Þ\nWhere Z and R denotes the update gate and the reset gate respectively. X represents the input vector, while U and W represent parameter vectors.\nOur proposed OIE architecture is shown in Fig. 1. In our OIE model we implemented a 2-Layer Bidirectional GRUs. The default application of RNNs is to assess information in a single direction. However, it has been shown that modelling information in a bidirectional technique results in better performance [21, 23]. A Bidirectional GRU was employed to encapsulate forward and backward lexical semantics of each word in a given sentence. A bidirectional network can be generated in 2 different approaches; either by having 2 RNN operating in opposing directions or within the internal architecture of the RNN itself, in our model we employed the latter approach.\nAfter encoding the 3 inputs using ELMo -the word, the POS tag of each word and the predicate as shown in Eq. (5)- they are all concatenated and passed as single feature vector to the Bidirectional GRU. Subsequently, the Bidirectional GRU outputs a tensor that’s passed to 3-layer Time Distributed Dense layer which is later passed to the SoftMax layer for label prediction.\nFeatureVector ¼ ELMo Wordð Þ ELMo POSð Þ ELMo Predicateð Þ ð5Þ\nEventually, SoftMax layer assigns a probability of each word belonging to a certain label. We used BIO tags (Begin – Intermediate – Outside) [24] that’s demonstrates the location of each word in the sentence, and each label is later assigned accordingly as shown in the last layer in Fig. 1. A sentence might include more than one entity, each sentence may output more than one tuple as the example in Table 1; however, our\nmodel captures binary relations. If a sentence contains no relation between the words only the predicate is assigned as “P-B” and label “O” is allocated to the remaining the words in the sentence."
  }, {
    "heading": "3.3 Hyperparameters Settings",
    "text": "Our neural OIE architecture was implemented using Keras framework [25] with TensorFlow backend [26]. Our model was trained on 10 epochs with the dropout rate set to 0.1 for regularization to avoid over-fitting. The data is divided into 100 batches. Moreover, we use early stopping to terminate training when the performance stops improving. Each Bidirectional GRU has 128 units, which is the same number of the hidden units in the subsequent 3 Time Distributed Dense layers. The activation function used in the 3 Time Distributed Dense layers is Rectified Linear Unit (ReLU) [27]. Adam optimizer [28] was employed to train our model."
  }, {
    "heading": "4 Results and Evaluation",
    "text": "The performance of the proposed OIE model was tested on two different datasets. Three experiments were carried out to measure and compare the performance of the proposed Bidirectional GRU-based OIE approach using contextual embedding."
  }, {
    "heading": "4.1 Dataset",
    "text": "The dataset we obtained for our model is further divided into two sets: Newswire corpus and Wikipedia News Corpus (WikiNews) [29]. Our dataset is split into a training set to train the model, development set to validate the model and a test set that is used to calculate the performance of our OIE proposed architecture. The number of sentences and number of tuples in each dataset can be found in Table 2. We tried to test our model using the dataset introduced by [15] that is automatically generated from a Question Answering dataset, but we couldn’t obtain it."
  }, {
    "heading": "4.2 Experimental Results and Analysis",
    "text": "Three evaluation metrics were used to measure the performance of our model: Recall (R), Precision (P) and F-measure (F). All the aforementioned measures were expressed as percentages throughout the experiments. With the F-measure being the breakthrough performance measure. Detailed results of the experiments can be found in Table 3.\nExperiment 1 In the first experiment we compare the results of employing ELMo embeddings against GloVe embeddings. As demonstrated in Table 3, when a Bidirectional GRU network is employed using ELMo instead of GloVe it yields to an increase in the F-Measure from 56.1% to 58.7% on WikiNews dataset and from 50.4% to 52.1% on Newswire dataset. An increase in the F-Measure by 3% can also be observed when a Bidirectional LSTM model that uses contextual embeddings is employed in contrast to non-contextual embedding. Hence, contextual embeddings have a notable effect on the performance of OIE task.\nExperiment 2 Subsequently, in the second experiment We compare our OIE model (BiGRU (ELMo)) against the model proposed by Stanovsky et al. [15] (BiLSTM (GloVe)). Table 3 shows the effect of utilizing contextual word embedding in a Bidirectional GRU network on extracting relation triples. The proposed model achieved an F-Measure of 52.1% compared to 43.0% achieved by [15] on Newswire dataset. Results on WikiNews dataset followed the same trend, our model increased the F-measure by 11.6%. It is observed that the proposed OIE system outperforms the model proposed by [15].\nExperiment 3 In the final experiment, we illustrate the effect of implementing a Bidirectional GRU instead of single direction GRU network. As we previously mentioned in Sect. 3.2, unidirectional networks can only have access to past information, thus output is based on what the network have previously learned, unlike bidirectional networks that can capture both, past and future information. This elaborates the massive decrease in the F-Measure by of the GRU network by 8.6% and by 13.3% on Newswire and WikiNews respectively, compared to our proposed Bidirectional GRU model.\nIt is note-worthy that we tested the effect of building a Hierarchical Attention Network (HAN) [30] over a RNN. HAN employs stacked RNN on word-level to capture the informative words in a sentence, it then combines the representation of those vital words to produce a sentence vector [30]. However, the OIE model underperformed using HAN."
  }, {
    "heading": "5 Conclusion and Future Work",
    "text": "The Bidirectional GRU-based OIE model with contextual word embeddings presented here delivers higher performance than existing state-of-the-art algorithm. The impact that contextual embedding had over our OIE architecture is notable in our experiments. In addition, Bidirectional GRU enhanced the performance with less complexity when compared to Bidirectional LSTM.\nWe believe that there is still a room for development in the field OIE. OIE can’t be regarded as a solved NLP task. For instance, little work has been done in extracting Nary relations, the main focus has been directed towards the extraction of binary relations, omitting the importance of higher order relations. The presented work can be further extended to extract N-ary relation. In the future, we would like to test our model on a larger dataset and would like to test the adaptability of the model on other languages. Finally, this approach can be employed in other NLP tasks such as question answering and summarization."
  }],
  "year": 2019,
  "references": [{
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["K Cho"],
    "venue": "arXiv preprint arXiv:1406.1078",
    "year": 2014
  }, {
    "title": "Speech recognition with deep recurrent neural networks",
    "authors": ["A. Graves", "A. Mohamed", "G. Hinton"],
    "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE",
    "year": 2013
  }, {
    "title": "Acoustic modeling for Google Home",
    "authors": ["B Li"],
    "venue": "Interspeech",
    "year": 2017
  }, {
    "title": "Comparative study of CNN and RNN for natural language processing",
    "authors": ["W Yin"],
    "venue": "arXiv preprint arXiv:1702.01923",
    "year": 2017
  }, {
    "title": "Uncovering algorithmic approaches in open information extraction: a literature review",
    "authors": ["I. Sarhan", "M. Spruit"],
    "venue": "30th Benelux Conference on Artificial Intelligence. Springer CSAI/JADS",
    "year": 2018
  }, {
    "title": "An over view of open information extraction (invited talk)",
    "authors": ["P. Gamallo"],
    "venue": "OASIcs-Open Access Series in Informatics, vol. 38. Schloss Dagstuhl Leibniz Zentrum fuer Informatik",
    "year": 2014
  }, {
    "title": "Open information extraction from the web",
    "authors": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"],
    "venue": "IJCAI, vol. 7, pp. 2670–2676",
    "year": 2007
  }, {
    "title": "Open information extraction using Wikipedia",
    "authors": ["F. Wu", "D.S. Weld"],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics",
    "year": 2010
  }, {
    "title": "Identifying relations for open information extraction",
    "authors": ["A. Fader", "S. Soderland", "O. Etzioni"],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",
    "year": 2011
  }, {
    "title": "Open information extraction: the second generation",
    "authors": ["O. Etzioni", "A. Fader", "J. Christensen", "S. Soderland", "M. Mausam"],
    "venue": "IJCAI, vol. 11, pp. 3–10",
    "year": 2011
  }, {
    "title": "ClausIE: clause-based open information extraction",
    "authors": ["L. Del Corro", "R. Gemulla"],
    "venue": "Proceedings of the 22nd International Conference on WWW, pp. 355–366. ACM",
    "year": 2013
  }, {
    "title": "Neural open information extraction",
    "authors": ["L. Cui", "F. Wei", "M. Zhou"],
    "venue": "arXiv:1805.04270",
    "year": 2018
  }, {
    "title": "Long short-term memory",
    "authors": ["S. Hochreiter", "J. Schmidhuber"],
    "venue": "Neural Comput. 9(8), 1735–1780",
    "year": 1997
  }, {
    "title": "Supervised open information extraction",
    "authors": ["G Stanovsky"],
    "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), vol. 1",
    "year": 2018
  }, {
    "title": "NLTK: the natural language toolkit",
    "authors": ["E. Loper", "S. Bird"],
    "venue": "arXiv preprint cs/0205028",
    "year": 2002
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["Peters", "M.E"],
    "venue": "arXiv preprint arXiv:1802. 05365",
    "year": 2018
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C. Manning"],
    "venue": "2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    "year": 2014
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"],
    "venue": "Advances in Neural Information Processing Systems, pp. 3111–3119",
    "year": 2013
  }, {
    "title": "On the difficulty of training recurrent neural networks",
    "authors": ["R. Pascanu", "T. Mikolov", "Y. Bengio"],
    "venue": "International Conference on Machine Learning",
    "year": 2013
  }, {
    "title": "A step beyond local observations with a dialog aware bidirectional GRU network for Spoken Language Understanding",
    "authors": ["V. Vukotic", "C. Raymond", "G. Gravier"],
    "venue": "Interspeech",
    "year": 2016
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"],
    "venue": "EMNLP",
    "year": 2014
  }, {
    "title": "Ask the GRU: multi-task learning for deep text recommendations",
    "authors": ["T. Bansal", "D. Belanger", "A. McCallum"],
    "venue": "Proceedings of the 10th ACM Conference on Recommender Systems. ACM",
    "year": 2016
  }, {
    "title": "Text chunking using transformation-based learning",
    "authors": ["L.A. Ramshaw", "M.P. Marcus"],
    "venue": "Armstrong, S., Church, K., Isabelle, P., Manzi, S., Tzoukermann, E., Yarowsky, D. (eds.) Natural Language Processing Using Very Large Corpora. Text, Speech and Language Technology, vol. 11, pp. 157–176. Springer, Dordrecht",
    "year": 1999
  }, {
    "title": "Tensorflow: a system for large-scale machine learning",
    "authors": ["M Abadi"],
    "venue": "12th Symposium on Operating Systems Design and Implementation (OSDI 2016)",
    "year": 2016
  }, {
    "title": "Rectified linear units improve restricted boltzmann machines",
    "authors": ["V. Nair", "G.E. Hinton"],
    "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10)",
    "year": 2010
  }, {
    "title": "Adam: a method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv: 1412.6980",
    "year": 2014
  }, {
    "title": "Creating a large benchmark for open information extraction",
    "authors": ["G. Stanovsky", "I. Dagan"],
    "venue": "Proceedings of the 2016 Conference on EMNLP",
    "year": 2016
  }, {
    "title": "Hierarchical attention networks for document classification",
    "authors": ["Z Yang"],
    "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    "year": 2016
  }],
  "id": "SP:d1e21861c174a5a70739bed121980ce86a2cf465",
  "authors": [{
    "name": "Injy Sarhan",
    "affiliations": []
  }, {
    "name": "Marco R. Spruit",
    "affiliations": []
  }],
  "abstractText": "Open Information Extraction (OIE) is a challenging task of extracting relation tuples from an unstructured corpus. While several OIE algorithms have been developed in the past decade, only few employ deep learning techniques. In this paper, a novel OIE neural model that leverages Recurrent Neural Networks (RNN) using Gated Recurrent Units (GRUs) is presented. Moreover, we integrate the innovative contextual word embeddings into our OIE model, which further enhances the performance. The results demonstrate that our proposed neural OIE model outperforms the existing stateof-art on two datasets.",
  "title": "Contextualized Word Embeddings in a Neural Open Information Extraction Model"
}