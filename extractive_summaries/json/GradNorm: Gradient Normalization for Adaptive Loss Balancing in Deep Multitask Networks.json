{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks. However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems\n1Magic Leap, Inc. Correspondence to: Zhao Chen <zchen@magicleap.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nsuch as smartphones, wearable devices, and robots/drones. Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass. Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result. In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance.\nIn general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks. Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features (Long & Wang, 2015) or optimizing multitask network architectures (Misra et al., 2016), etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients. A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes. We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function.\nIn practice, the multitask loss function is often assumed to be linear in the single task losses Li, L = ∑ i wiLi, where the sum runs over all T tasks. In our case, we propose an adaptive method, and so wi can vary at each training step t: wi = wi(t). This linear form of the loss function is convenient for implementing gradient balancing, as wi very directly and linearly couples to the backpropagated gradient magnitudes from each task. The challenge is then to find the best value for each wi at each training step t that balances the contribution of each task for optimal model training. To optimize the weights wi(t) for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small. The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight wi(t) should decrease relative to other task weights wj(t)|j 6=i to allow other tasks more influence on\ntraining. Our algorithm is similar to batch normalization (Ioffe & Szegedy, 2015) with two main differences: (1) we normalize across tasks instead of across data batches, and (2) we use rate balancing as a desired objective to inform our normalization. We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting.\nOur main contributions to multitask learning are as follows:\n1. An efficient algorithm for multitask loss balancing which directly tunes gradient magnitudes.\n2. A method which matches or surpasses the performance of very expensive exhaustive grid search procedures, but which only requires tuning a single hyperparameter.\n3. A demonstration that direct gradient interaction provides a powerful way of controlling multitask learning."
  }, {
    "heading": "2. Related Work",
    "text": "Multitask learning was introduced well before the advent of deep learning (Caruana, 1998; Bakker & Heskes, 2003), but the robust learned features within deep networks and their excellent single-task performance have spurned renewed interest. Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Collobert & Weston, 2008; Hashimoto et al., 2016; Søgaard & Goldberg, 2016) to speech synthesis (Seltzer & Droppo, 2013; Wu et al., 2015), from very domain-specific applications such as traffic prediction (Huang et al., 2014) to very general cross-domain work (Bilen & Vedaldi, 2017). Multitask learning has also been explored in the context of curriculum learning (Graves et al., 2017), where subsets of tasks are subsequently trained based on local rewards; we here explore the opposite approach, where tasks are jointly trained based on global rewards such as total loss decrease.\nMultitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding. Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus, 2015; Teichmann et al., 2016) to much larger subsets as in UberNet (Kokkinos, 2016). Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al., 2017) or YOLO-9000 for object detection (Redmon & Farhadi, 2016). Particularly of note is the rich and significant body of work on finding explicit ways to exploit task relationships within a multitask model. Clustering methods have shown success beyond deep models (Jacob et al., 2009; Kang et al., 2011), while constructs such as deep relationship networks (Long & Wang, 2015) and cross-stich networks (Misra et al., 2016)\ngive deep networks the capacity to search for meaningful relationships between tasks and to learn which features to share between them. Work in (Warde-Farley et al., 2014) and (Lu et al., 2016) use groupings amongst labels to search through possible architectures for learning. Perhaps the most relevant to the current work, (Kendall et al., 2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task."
  }, {
    "heading": "3. The GradNorm Algorithm",
    "text": ""
  }, {
    "heading": "3.1. Definitions and Preliminaries",
    "text": "For a multitask loss function L(t) = ∑ wi(t)Li(t), we aim to learn the functions wi(t) with the following goals: (1) to place gradient norms for different tasks on a common scale through which we can reason about their relative magnitudes, and (2) to dynamically adjust gradient norms so different tasks train at similar rates. To this end, we first define the relevant quantities, first with respect to the gradients we will be manipulating.\n• W : The subset of the full network weights W ⊂ W where we actually apply GradNorm. W is generally chosen as the last shared layer of weights to save on compute costs1.\n• G(i)W (t) = ||∇Wwi(t)Li(t)||2: the L2 norm of the gradient of the weighted single-task loss wi(t)Li(t) with respect to the chosen weights W .\n• GW (t) = Etask[G(i)W (t)]: the average gradient norm across all tasks at training time t.\nWe also define various training rates for each task i:\n• L̃i(t) = Li(t)/Li(0): the loss ratio for task i at time t. L̃i(t) is a measure of the inverse training rate of task i (i.e. lower values of L̃i(t) correspond to a faster training rate for task i)2.\n• ri(t) = L̃i(t)/Etask[L̃i(t)]: the relative inverse training rate of task i.\nWith the above definitions in place, we now complete our description of the GradNorm algorithm."
  }, {
    "heading": "3.2. Balancing Gradients with GradNorm",
    "text": "As stated in Section 3.1, GradNorm should establish a common scale for gradient magnitudes, and also should balance\n1In our experiments this choice of W causes GradNorm to increase training time by only ∼ 5% on NYUv2.\n2Networks in this paper all had stable initializations and Li(0) could be used directly. When Li(0) is sharply dependent on initialization, we can use a theoretical initial loss instead. E.g. for Li the CE loss across C classes, we can use Li(0) = log(C).\ntraining rates of different tasks. The common scale for gradients is most naturally the average gradient norm, GW (t), which establishes a baseline at each timestep t by which we can determine relative gradient sizes. The relative inverse training rate of task i, ri(t), can be used to rate balance our gradients. Concretely, the higher the value of ri(t), the higher the gradient magnitudes should be for task i in order to encourage the task to train more quickly. Therefore, our desired gradient norm for each task i is simply:\nG (i) W (t) 7→ GW (t)× [ri(t)] α, (1)\nwhere α is an additional hyperparameter. α sets the strength of the restoring force which pulls tasks back to a common training rate. In cases where tasks are very different in their complexity, leading to dramatically different learning dynamics between tasks, a higher value of α should be used to enforce stronger training rate balancing. When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of α is appropriate. Note that α = 0 will always try to pin the norms of backpropagated gradients from each task to be equal at W . See Section 5.4 for more details on the effects of tuning α.\nEquation 1 gives a target for each task i’s gradient norms, and we update our loss weights wi(t) to move gradient\nnorms towards this target for each task. GradNorm is then implemented as an L1 loss function Lgrad between the actual and target gradient norms at each timestep for each task, summed over all tasks:\nLgrad(t;wi(t)) = ∑ i ∣∣∣∣G(i)W (t)−GW (t)× [ri(t)]α∣∣∣∣ 1 (2)\nwhere the summation runs through all T tasks. When differentiating this loss Lgrad, we treat the target gradient norm GW (t)× [ri(t)]α as a fixed constant to prevent loss weights wi(t) from spuriously drifting towards zero. Lgrad is then differentiated only with respect to the wi, as the wi(t) directly control gradient magnitudes per task. The computed gradients ∇wiLgrad are then applied via standard update rules to update each wi (as shown in Figure 1). The full GradNorm algorithm is summarized in Algorithm 1. Note that after every update step, we also renormalize the weights wi(t) so that ∑ i wi(t) = T in order to decouple gradient normalization from the global learning rate."
  }, {
    "heading": "4. A Toy Example",
    "text": "To illustrate GradNorm on a simple, interpretable system, we construct a common scenario for multitask networks: training tasks which have similar loss functions but different loss scales. In such situations, if we naı̈vely pick wi(t) = 1\nAlgorithm 1 Training with GradNorm Initialize wi(0) = 1 ∀i Initialize network weightsW Pick value for α > 0 and pick the weightsW (usually the\nfinal layer of weights which are shared between tasks) for t = 0 to max train steps do\nInput batch xi to compute Li(t) ∀i and L(t) = ∑ i wi(t)Li(t) [standard forward pass] Compute G(i)W (t) and ri(t) ∀i Compute GW (t) by averaging the G (i) W (t)\nCompute Lgrad = ∑ i|G (i) W (t)−GW (t)× [ri(t)]α|1 Compute GradNorm gradients∇wiLgrad, keeping targets GW (t)× [ri(t)]α constant Compute standard gradients∇WL(t) Update wi(t) 7→ wi(t+ 1) using ∇wiLgrad UpdateW(t) 7→ W(t+ 1) using∇WL(t) [standard\nbackward pass] Renormalize wi(t+ 1) so that ∑ i wi(t+ 1) = T\nend for\nfor all loss weights wi(t), the network training will be dominated by tasks with larger loss scales that backpropagate larger gradients. We will demonstrate that GradNorm overcomes this issue.\nConsider T regression tasks trained using standard squared loss onto the functions\nfi(x) = σi tanh((B + i)x), (3)\nwhere tanh(·) acts element-wise. Inputs are dimension 250 and outputs dimension 100, while B and i are constant matrices with their elements generated IID from N (0, 10) and N (0, 3.5), respectively. Each task therefore shares information in B but also contains task-specific information i. The σi are the key parameters within this problem; they are fixed scalars which set the scales of the outputs fi. A higher scale for fi induces a higher expected value of squared loss for that task. Such tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients. This scenario generally leads to suboptimal training dynamics when the higher σi tasks dominate the training across all tasks.\nTo train our toy models, we use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk. A final affine transformation layer gives T final predictions (corresponding to T different tasks). To ensure valid analysis, we only compare models initialized to the same random values and fed data generated from the same fixed random seed. The asymmetry α is set low to 0.12 for these experiments, as the output functions fi are all of the same functional form and thus we expect the asymmetry between tasks to be minimal.\nIn these toy problems, we measure the task-normalized testtime loss to judge test-time performance, which is the sum of the test loss ratios for each task, ∑ i Li(t)/Li(0). We do this because a simple sum of losses is an inadequate performance metric for multitask networks when different loss scales exist; higher loss scale tasks will factor disproportionately highly in the loss. There unfortunately exists no general single scalar which gives a meaningful measure of multitask performance in all scenarios, but our toy problem was specifically designed with tasks which are statistically identical except for their loss scales σi. There is therefore a clear measure of overall network performance, which is the sum of losses normalized by each task’s variance σ2i - equivalent (up to a scaling factor) to the sum of loss ratios.\nFor T = 2, we choose the values (σ0, σ1) = (1.0, 100.0) and show the results of training in the top panels of Figure 2. If we train with equal weightswi = 1, task 1 suppresses task 0 from learning due to task 1’s higher loss scale. However, gradient normalization increases w0(t) to counteract the larger gradients coming from T1, and the improved task balance results in better test-time performance.\nThe possible benefits of gradient normalization become even clearer when the number of tasks increases. For T = 10, we sample the σi from a wide normal distribution and plot the results in the bottom panels of Figure 2. GradNorm significantly improves test time performance over naı̈vely weighting each task the same. Similarly to the T = 2 case, for T = 10 the wi(t) grow larger for smaller σi tasks.\nFor both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by (Kendall et al., 2017). Uncertainty weighting, which enforces that wi(t) ∼ 1/Li(t), tends to grow the weights wi(t) too large and too quickly as the loss for each task drops. Although such networks train quickly at the onset, the training soon deteriorates. This issue is largely caused by the fact that uncertainty weighting allows wi(t) to change without constraint (compared to GradNorm which ensures∑ wi(t) = T always), which pushes the global learning rate up rapidly as the network trains.\nThe traces for each wi(t) during a single GradNorm run are observed to be stable and convergent. In Section 5.3 we will see how the time-averaged weightsEt[wi(t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure."
  }, {
    "heading": "5. Application to a Large Real-World Dataset",
    "text": "We use two variants of NYUv2 (Nathan Silberman & Fergus, 2012) as our main datasets. Please refer to the Supplementary Materials for additional results on a 9-task facial landmark dataset found in (Zhang et al., 2014). The standard NYUv2 dataset carries depth, surface normals, and semantic\nsegmentation labels (clustered into 13 distinct classes) for a variety of indoor scenes in different room types (bathrooms, living rooms, studies, etc.). NYUv2 is relatively small (795 training, 654 test images), but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm across various tasks.\nWe augment the standard NYUv2 depth dataset with flips and additional frames from each video, resulting in 90,000 images complete with pixel-wise depth, surface normals, and room keypoint labels (segmentation labels are, unfortunately, not available for these additional frames). Keypoint labels are professionally annotated by humans, while surface normals are generated algorithmically. The full dataset is then split by scene for a 90/10 train/test split. See Figure 6 for examples. We will generally refer to these two datasets as NYUv2+seg and NYUv2+kpts, respectively.\nAll inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels. We use these resolutions following (Lee et al., 2017), which represents the state-of-the-art in room keypoint prediction and from which we also derive our VGG-style model architecture. These resolutions also allow us to keep models relatively slim while not compromising semantic complexity in the ground truth output maps."
  }, {
    "heading": "5.1. Model and General Training Characteristics",
    "text": "We try two different models: (1) a SegNet (Badrinarayanan et al., 2015; Lee et al., 2017) network with a symmetric VGG16 (Simonyan & Zisserman, 2014) encoder/decoder,\nand (2) an FCN (Long et al., 2015) network with a modified ResNet-50 (He et al., 2016) encoder and shallow ResNet decoder. The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCN learns all upsampling filters. The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet: stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers. Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet. All model parameters are shared amongst all tasks until the final layer. Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate GradNorm’s robustness to the choice of base architecture.\nWe use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals. As in (Lee et al., 2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss. Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use ri(t) for each task i as a direct proxy for each task’s relative inverse training rate.\nAll runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference. All NYUv2 runs begin with a learning rate of 2e5. NYUv2+kpts runs last 80000 steps with a learning rate\ndecay of 0.2 every 25000 steps. NYUv2+seg runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps. Updating wi(t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting ((Kendall et al., 2017)) baseline. All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen. We implement GradNorm using TensorFlow v1.2.1."
  }, {
    "heading": "5.2. Main Results on NYUv2",
    "text": "In Table 1 we display the performance of GradNorm on the NYUv2+seg dataset. We see that GradNorm α = 1.5 improves the performance of all three tasks with respect to the equal-weights baseline (where wi(t) = 1 for all t,i), and either surpasses or matches (within statistical noise) the best performance of single networks for each task. The GradNorm Static network uses static weights derived from a GradNorm network by calculating the time-averaged weights Et[wi(t)] for each task during a GradNorm training run, and retraining a network with weights fixed to those values. GradNorm thus can also be used to extract good values for static weights. We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.\nTo show how GradNorm can perform in the presence of a larger dataset, we also perform extensive experiments on the NYUv2+kpts dataset, which is augmented to a factor of 50x more data. The results are shown in Table 2. As with the NYUv2+seg runs, GradNorm networks outperform other multitask methods, and either matches (within noise) or surpasses the performance of single-task networks.\nFigure 3 shows test and training loss curves for GradNorm (α = 1.5) and baselines on the larger NYUv2+kpts dataset for our VGG SegNet models. GradNorm improves test-time depth error by ∼ 5%, despite converging to a much higher training loss. GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry α = 1.5), and ultimately suppresses the depth weight wdepth(t) to lower than 0.10 (see Section 5.4 for more details). The same\ntrend exists for keypoint regression, and is a clear signal of network regularization. In contrast, uncertainty weighting (Kendall et al., 2017) always moves test and training error in the same direction, and thus is not a good regularizer. Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results."
  }, {
    "heading": "5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass",
    "text": "For our VGG SegNet, we train 100 networks from scratch with random task weights on NYUv2+kpts. Weights are sampled from a uniform distribution and renormalized to sum to T = 3. For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm\nα = 1.5 VGG SegNet network at the same 15000 steps. The results are shown in Figure 4.\nEven after 100 networks trained, grid search still falls short of our GradNorm network. Even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights Et[wi(t)]. At an L2 distance of ∼ 3, grid search networks on average have almost double the errors per task compared to our GradNorm network. GradNorm has therefore found the optimal grid search weights in one single training run."
  }, {
    "heading": "5.4. Effects of tuning the asymmetry α",
    "text": "The only hyperparameter in our algorithm is the asymmetry α. The optimal value of α for NYUv2 lies near α = 1.5, while in the highly symmetric toy example in Section 4 we used α = 0.12. This observation reinforces our characterization of α as an asymmetry parameter.\nTuning α leads to performance gains, but we found that for NYUv2, almost any value of 0 < α < 3 will improve network performance over an equal weights baseline (see Supplementary for details). Figure 5 shows that higher values of α tend to push the weights wi(t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth). Remarkably, at α = 1.75 (not shown) wdepth(t) is suppressed to below 0.02 at no detriment to network performance on the depth task."
  }, {
    "heading": "5.5. Qualitative Results",
    "text": "Figure 6 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the NYUv2+seg and NYUv2+kpts datasets. Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network. Some\nimprovements are incremental, but GradNorm produces superior visual results in tasks for which there are significant quantitative improvements in Tables 1 and 2."
  }, {
    "heading": "6. Conclusions",
    "text": "We introduced GradNorm, an efficient algorithm for tuning loss weights in a multi-task learning setting based on balancing the training rates of different tasks. We demonstrated on both synthetic and real datasets that GradNorm improves multitask test-time performance in a variety of scenarios, and can accommodate various levels of asymmetry amongst the different tasks through the hyperparameter α. Our empirical results indicate that GradNorm offers su-\nperior performance over state-of-the-art multitask adaptive weighting methods and can match or surpass the performance of exhaustive grid search while being significantly less time-intensive.\nLooking ahead, algorithms such as GradNorm may have applications beyond multitask learning. We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance. We thus believe that our work not only provides a robust new algorithm for multitask learning, but also reinforces the powerful idea that gradient tuning is fundamental for training large, effective models on complex tasks."
  }],
  "year": 2018,
  "references": [{
    "title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
    "authors": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"],
    "venue": "arXiv preprint arXiv:1511.00561,",
    "year": 2015
  }, {
    "title": "Task clustering and gating for bayesian multitask learning",
    "authors": ["B. Bakker", "T. Heskes"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2003
  }, {
    "title": "Universal representations: The missing link between faces, text, planktons, and cat breeds",
    "authors": ["H. Bilen", "A. Vedaldi"],
    "venue": "arXiv preprint arXiv:1701.07275,",
    "year": 2017
  }, {
    "title": "Multitask learning. In Learning to learn, pp. 95–133",
    "authors": ["R. Caruana"],
    "year": 1998
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["R. Collobert", "J. Weston"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
    "authors": ["D. Eigen", "R. Fergus"],
    "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp",
    "year": 2015
  }, {
    "title": "Automated curriculum learning for neural networks",
    "authors": ["A. Graves", "M.G. Bellemare", "J. Menick", "R. Munos", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1704.03003,",
    "year": 2017
  }, {
    "title": "A joint many-task model: Growing a neural network for multiple nlp tasks",
    "authors": ["K. Hashimoto", "C. Xiong", "Y. Tsuruoka", "R. Socher"],
    "venue": "arXiv preprint arXiv:1611.01587,",
    "year": 2016
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
    "year": 2016
  }, {
    "title": "Deep architecture for traffic flow prediction: deep belief networks with multitask learning",
    "authors": ["W. Huang", "G. Song", "H. Hong", "K. Xie"],
    "venue": "IEEE Transactions on Intelligent Transportation Systems,",
    "year": 2014
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Clustered multi-task learning: A convex formulation",
    "authors": ["L. Jacob", "Vert", "J.-p", "F.R. Bach"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Learning with whom to share in multi-task feature learning",
    "authors": ["Z. Kang", "K. Grauman", "F. Sha"],
    "venue": "In Proceedings of the 28th International Conference on Machine Learning",
    "year": 2011
  }, {
    "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
    "authors": ["A. Kendall", "Y. Gal", "R. Cipolla"],
    "venue": "arXiv preprint arXiv:1705.07115,",
    "year": 2017
  }, {
    "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
    "authors": ["I. Kokkinos"],
    "venue": "arXiv preprint arXiv:1609.02132,",
    "year": 2016
  }, {
    "title": "Roomnet: End-to-end room layout estimation",
    "authors": ["Lee", "C.-Y", "V. Badrinarayanan", "T. Malisiewicz", "A. Rabinovich"],
    "venue": "arXiv preprint arXiv:1703.06241,",
    "year": 2017
  }, {
    "title": "Fully convolutional networks for semantic segmentation",
    "authors": ["J. Long", "E. Shelhamer", "T. Darrell"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2015
  }, {
    "title": "Learning multiple tasks with deep relationship networks",
    "authors": ["M. Long", "J. Wang"],
    "venue": "arXiv preprint arXiv:1506.02117,",
    "year": 2015
  }, {
    "title": "Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification",
    "authors": ["Y. Lu", "A. Kumar", "S. Zhai", "Y. Cheng", "T. Javidi", "R. Feris"],
    "venue": "arXiv preprint arXiv:1611.05377,",
    "year": 2016
  }, {
    "title": "Crossstitch networks for multi-task learning",
    "authors": ["I. Misra", "A. Shrivastava", "A. Gupta", "M. Hebert"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Indoor segmentation and support inference from rgbd images",
    "authors": ["Nathan Silberman", "P.K. Derek Hoiem", "R. Fergus"],
    "venue": "In ECCV,",
    "year": 2012
  }, {
    "title": "Yolo9000: better, faster, stronger",
    "authors": ["J. Redmon", "A. Farhadi"],
    "venue": "arXiv preprint arXiv:1612.08242,",
    "year": 2016
  }, {
    "title": "Multi-task learning in deep neural networks for improved phoneme recognition",
    "authors": ["M.L. Seltzer", "J. Droppo"],
    "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2013
  }, {
    "title": "Very deep convolutional networks for large-scale image recognition",
    "authors": ["K. Simonyan", "A. Zisserman"],
    "venue": "arXiv preprint arXiv:1409.1556,",
    "year": 2014
  }, {
    "title": "Deep multi-task learning with low level tasks supervised at lower layers",
    "authors": ["A. Søgaard", "Y. Goldberg"],
    "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,",
    "year": 2016
  }, {
    "title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
    "authors": ["M. Teichmann", "M. Weber", "M. Zoellner", "R. Cipolla", "R. Urtasun"],
    "venue": "arXiv preprint arXiv:1612.07695,",
    "year": 2016
  }, {
    "title": "Selfinformed neural network structure learning",
    "authors": ["D. Warde-Farley", "A. Rabinovich", "D. Anguelov"],
    "venue": "arXiv preprint arXiv:1412.6563,",
    "year": 2014
  }, {
    "title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis",
    "authors": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"],
    "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
    "year": 2015
  }, {
    "title": "Facial landmark detection by deep multi-task learning",
    "authors": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"],
    "venue": "In European Conference on Computer Vision,",
    "year": 2014
  }],
  "id": "SP:f3f87f444afc4158ea959da3032309bcb4385135",
  "authors": [{
    "name": "Zhao Chen",
    "affiliations": []
  }, {
    "name": "Vijay Badrinarayanan",
    "affiliations": []
  }, {
    "name": "Chen-Yu Lee",
    "affiliations": []
  }, {
    "name": "Andrew Rabinovich",
    "affiliations": []
  }],
  "abstractText": "Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",
  "title": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks"
}