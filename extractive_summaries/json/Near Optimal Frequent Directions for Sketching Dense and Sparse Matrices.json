{
  "sections": [{
    "heading": "1. Introduction",
    "text": "For large-scale matrix computations, exact algorithms are often too slow, so a large body of works focus on designing fast randomized approximation algorithms. To speedup the computation, matrix sketching is a commonly used technique, e.g. (Sarlos, 2006; Clarkson & Woodruff, 2013; Avron et al., 2013; Chierichetti et al., 2017). In real-world applications, the data often arrives in a streaming fashion and it is often impractical or impossible to store the entire data set in the main memory.\nIn this paper, we study online streaming algorithms for maintaining matrix sketches with small covariance errors. In the streaming model, the rows of the input matrix arrive one at a time; the algorithm is only allowed to make one pass over the stream with severely limited working space, which is required to maintain a sketch continuously. This problem has\n1School of Data Science, Fudan University, China. Correspondence to: Zengfeng Huang <huangzf@fudan.edu.cn>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nreceived lots of attention recently (Liberty, 2013; Ghashami & Phillips, 2014; Woodruff, 2014; Ghashami et al., 2016; Wei et al., 2016).\nThe popular Frequent Directions algorithms (Liberty, 2013; Ghashami et al., 2016) achieve optimal tradeoff between space usage and approximation error (Woodruff, 2014), which have found lots of applications in online learning, e.g., (Boutsidis et al., 2015; Karnin & Liberty, 2015; Leng et al., 2015; Huang & Kasiviswanathan, 2015; Luo et al., 2016; Calandriello et al., 2017), and other problems (Song et al., 2015; Ye et al., 2016; Kim et al., 2016). However, it is unclear whether their running times can be improved; one might hope to get linear (in sparsity) time algorithms, which is possible for many matrix problems, e.g. (Clarkson & Woodruff, 2013). This paper is motivated by the following question:\n• Is there an input sparsity time Frequent Directions, which achieves the same optimal space-error tradeoff ?"
  }, {
    "heading": "1.1. Problem definitions",
    "text": "Given a matrix A ∈ Rn×d, we want to compute a much smaller matrix B ∈ R`×d, which has low covariance error, i.e., ‖ATA−BTB‖2. Definition 1 (Covariance Sketch). For any 0 < α < 1, and integer 0 ≤ k ≤ rank(A), we will call B an (α, k)-covsketch of A, if the covariance error1\n‖ATA−BTB‖2 ≤ α‖A− [A]k‖2F . (1)\nHere ‖ · ‖2 and ‖ · ‖F are the spectral norm and Frobenius norm of matrices; [A]k is the best rank-k approximation to A. We will use πkB(A) to denote the projection of A on the top-k singular vectors of B, i.e. πkB(A) = AV V\nT , where the columns of V are the top-k right singular vectors of B. Definition 2 (Projection error). The projection error of B with respect to A is defined as ‖A− πkB(A)‖2F .\nNote πkB(A) is a rank-k matrix, thereby the projection error is at least ‖A−[A]k‖2F . It is proved in (Ghashami & Phillips, 2014) that one can obtain relative projection error from small covariance error. We include a proof of the next lemma in the supplementary material.\n1for k = 0, we define [A]0 = 0\nLemma 1 (covariance error to projection error (Ghashami & Phillips, 2014) (modified)).\n‖A− πkB(A)‖2F ≤ ‖A− [A]k‖2F + 2k · ‖ATA−BTB‖2.\nTherefore, any ( ε2k , k)-cov-sketch B has projection error\n‖A− πkB(A)‖2F ≤ (1 + ε)‖A− [A]k‖2F . (2)\nWe will often refer to such sketches as (ε, k)-proj-sketches.\nModern data matrices are often large and sparse. So we will always assume n and d are very large, typically d n, and nnz(A) nd, where nnz(A) in the number of nonzero entries in A. Moreover, we assume that each entry of A is representable by O(log(nd)) bits. To simplify the analysis, we assume the entries of A are integers of magnitude at most poly(nd); the general case can be reduced to this, see e.g. (Boutsidis et al., 2016)."
  }, {
    "heading": "1.2. Previous results",
    "text": "In the row-wise update streaming model, Liberty’s Frequent Direction (FD) algorithm (Liberty, 2013), with an improved analysis in (Ghashami & Phillips, 2014), maintains an (α, k)-cov-sketch B ∈ R`×d at any time, where ` = O(k+α−1). The algorithm uses O(d`) space and runs inO(nd`) time. For sparse matrices, the running time of FD is improved to O(nnz(A)` log d+ nnz(A) log n+ n`2) by Ghashami et al. (Ghashami et al., 2016). Set α = ε/2k (or ` = O(k/ε)), and by Lemma 1, B is a (ε, k)-projsketch. Now, B contains O(k/ε) rows, the space and the running time become O(dk/ε) and O(nnz(A)kε−1 · log d+ nnz(A) log n+nk2ε−2) respectively. It was shown by Woodruff (Woodruff, 2014) that the space used by FD is optimal for both covariance error and projection error. A natural question is if the running time can be improved. In particular,\n• Is there an input sparsity time algorithm, i.e., in time O(nnz(A) + (n + d) · poly(kα−1)), which achieves the same guarantee as FD?"
  }, {
    "heading": "1.3. Our contributions",
    "text": "This paper almost settles the above question. Our main contributions are summarized as follows.\n1. We show that o(nnz(A)k) time is likely very difficult to achieve, as it will imply a breakthrough in fast matrix multiplication. In particular, we prove that computing an (O(1), k)-cov-sketch B ∈ RO(k)×d of A is as hard as left multiplying A by an arbitrary matrix C ∈ Rk×n.\n2. We give a new space-optimal streaming algorithm with O(ndk) + Õ(dα−3) running time to compute (α, k)-covsketches for dense matrices, which improves the original\nFD algorithm for small α. The running time is optimal up to lower order terms, provided matrix multiplication cannot be improved significantly.\n3. We then give a new space-optimal streaming algorithm withO(nnz(A)k+nnz(A) log n)+Õ(nk3+dα−3) running time to compute (α, k)-cov-sketches for sparse matrices. We separate the dependence of 1/α from nnz(A), which improves the results of (Ghashami et al., 2016) for small α. In particular, computing an (ε, k)-proj-sketch, our algorithm only needs O(nnz(A)k) time (ignoring lower order terms) as opposed to O(nnz(A)kε−1 · log d) in (Ghashami et al., 2016) (see Table 1). Even when α is small, our algorithm improves a log d factor. Moreover, for k = Ω(log n), the running time of our algorithm matches the lower bound."
  }, {
    "heading": "1.4. Other related works",
    "text": "The problem of computing (α, k)-cov-sketches was also studied in the sliding window streaming model (Wei et al., 2016) and distributed models (Ghashami et al., 2014; Huang et al., 2017). A closely related problem, namely approximate PCA, was studied in (Kannan et al., 2014; Liang et al., 2014; Boutsidis et al., 2016; Zhang et al., 2015). (Clarkson & Woodruff, 2009) studied other streaming numerical linear algebra problems."
  }, {
    "heading": "1.5. Matrix preliminaries and notations",
    "text": "We always use n for the number rows, and d for the dimension of each row. For a d-dimensional vector x, ‖x‖ is the `2 norm of x. We use xi to denote the ith entry of x, and Diag(x) ∈ Rd×d is a diagonal matrix such that the ith diagonal entry is xi. Let A ∈ Rn×d with n > d, we use Ai to denote the ith row of A, and ai,j for the (i, j)-th entry of A. nnz(A) is the number of non-zero entries in A, and rows(A) is the number of rows in A. We write the (reduced) singular value decomposition of A as (U,Σ, V ) = SVD(A). The computation time of standard SVD algorithms is O(nd2). We use ‖A‖2 or ‖A‖ to denote the spectral norm of A, which is the largest singular value of A, and ‖A‖F for the Frobenius Norm, which is √∑ i,j a 2 i,j . For k ≤ rank(A), we use [A]k to denote the best rank k approximation of A. We define [A]0 = 0. [A;B] is the matrix formed by concatenating the rows of A and B. We use Õ() to hide polylog(ndk) factors."
  }, {
    "heading": "1.6. Tools",
    "text": "Frequent Directions. We will use the Frequent Directions (FD) algorithm by Liberty (Liberty, 2013), denoted as FD(A,α, k), and the main result is summarized in the following theorem.\nTheorem 1 ((Liberty, 2013)). Given A ∈ Rn×d, in one pass, FD(A,α, k) processes A in O(nd(k + α−1)) time\nand O(d(k + α−1)) space. It maintains a matrix B ∈ RO(k+α−1)×d such that ‖ATA−BTB‖2 ≤ α‖A−[A]k‖2F .\nRow sampling. We provide a result about row sampling, which is analogous to a result from (Drineas et al., 2006). The difference is that they use iid sampling, i.e., each row of B is an iid sample from the rows ofA. On the other hand, we use Bernoulli sampling, i.e., sample each Ai independently with some probability qi, and B is the set of sampled rows. Bernoulli sampling can easily be combined with FD in the streaming model. The proof is essentially the same as that for iid sampling, which can be found in the supplementary material. Theorem 2. For any A ∈ Rn×d and F > 0, we sample each row Ai with probability pi ≥ ‖Ai‖ 2\nα2F ; if it is sampled, scale it by 1/ √ pi. Let B be the (rescaled) sampled rows, then w.p. 0.99, ‖ATA − BTB‖2 ≤ 10α √ F‖A‖F , and ‖B‖F ≤ 10‖A‖F . The expected number of rows sampled is O(‖A‖ 2 F\nα2F ).\nInput-sparsity time lower rank approximation algorithm. There are nnz(A) (omitting lower order terms) time algorithms (Clarkson & Woodruff, 2013), which output a matrix Z consists of k orthonormal rows such that ‖A − ZTZA‖2F ≤ (1 + ε)‖A − [A]k‖2F . For our application, we only need a constant approximation, and only require Z to contain O(k) rows. For this purpose, we give a simplified algorithm with slightly better running time than directly applying the results from (Clarkson & Woodruff, 2013). We require high success probability, which can be achieved using similar techniques as in (Boutsidis et al., 2016). The proof of the following theorem can be found in the supplementary file. Theorem 3 (weak low rank approximation). For any integers `, d, given A ∈ R`×d, there is an algorithm that uses O(nnz(A) log(1/δ)) + Õ(`k3) time and O(`(k2 + log 1δ )) space, and outputs a matrix Z ∈ RO(k)×` with orthonormal rows such that with probability 1 − δ, ‖A − ZTZA‖2F ≤ O(1)‖A− [A]k‖2F ."
  }, {
    "heading": "2. Time lower bound",
    "text": "In this section, we provide a conditional lower bound for our problem based on the idea of (Musco & Woodruff,\n2017). We prove that the existence of algorithms which compute an (O(1), k)-cov-sketch in time o(nnz(A)k) implies a breakthrough in matrix multiplication, which is likely very difficult. In fact, the lower bound holds even for offline algorithms without constraints on working space.\nTheorem 4. Assume there is an algorithm A , which, given any A ∈ Rn×d with polynomially bounded integer entries, returns B ∈ RO(k)×d in time o(nnz(A)k) such that\n‖ATA−BTB‖2 ≤ ∆‖A− [A]k‖2F ,\nfor some constant error parameter ∆. Then there is an o(nnz(M)k) + O(dk2) time algorithm for multiplying arbitrary polynomially bounded integer matrices MT ∈ R(d−k)×n, C ∈ Rn×k.\nProof. For any matrices M ∈ Rn×(d−k) and C ∈ Rn×k with integer entries in [−U,U ], let A ∈ Rn×d be the matrix which is a concatenation of the columns of M and wC, i.e., A = [M,wC] . Here w is large number will be determined later. We have ‖A− [A]k‖2F ≤ ‖M‖2F ≤ ndU2 and\nATA =\n[ MTM wMTC\nwCTM w2CTC\n] .\nWe assume A is an algorithm with running time T , which can compute a sketch matrix B ∈ RO(k)×d of A such that\n‖ATA−BTB‖2 ≤ ∆‖A− [A]k‖2F ≤ ∆U2nd,\nfor some constant error parameter ∆.\nThe spectral norm of a matrix N is the largest singular value, which can be equivalently defined as ‖N‖2 = maxx,y:‖x‖=‖y‖=1 x\nTNy, therebyNi,j = eTi Nej ≤ ‖N‖2 for all i, j. It follows that (ATA − BTB)i,j ≤ ∆U2nd, meaning the corresponding block of BTB is an entry-wise approximation to wMTC within additive error ∆U2nd.\nNow if w is a large integer, say w = d3∆U2nde, we can recover MTC from BTB exactly by rounding the numbers in BTB to their nearest integers (as MTC is an integer matrix). Note BTB can be computed in time O(dk2) given B and the rounding can be done in O(dk), so using A, the exact integer matrix multiplication MTC can be computed in time O(T + dk2). Therefore, we have proved that if T = o(nnz(A)k) = o(nnz(M)k + dk2), then MTC can\nbe computed in time o(nnz(M)k) +O(dk2), which will be a breakthrough in fast matrix multiplication. We remark that all the integers in our reduction are at most poly(nd) in magnitude, as long as U = poly(nd), so our reduction works for anyM,C with polynomially bounded entries."
  }, {
    "heading": "3. Algorithm for dense matrices",
    "text": "Theorem 5 (FFDdense(A,α, k)). Given a matrix A ∈ Rn×d, 0 < α < 1 and 0 ≤ k ≤ d, FFDdense(A,α, k) processes A in one pass using O(ndk) + Õ(dα−3) time and O(dk + dα−1)) space, and maintains a matrix B. With probability 0.99, it holds that ‖ATA − BTB‖2 ≤ α‖A− [A]k‖2F .\nOverview of the algorithm. To speed up FD, we will use the idea of adaptive random sampling. Let us first review the standard FD algorithm. Given an integer parameter ` ≤ d, the algorithm always maintains a matrix B with at most 2` rows at any time. When a new row v arrives, it processes the row using FDShrink(B, v, `) (Algorithm 3). In this procedure, we first append a after B; if B has no more than 2` rows we do nothing, and otherwise we do a DenseShrink operation (Algorithm 1) on B, which halves the number of rows in B (after removing zero rows). It was proved in (Liberty, 2013) and (Ghashami & Phillips, 2014) that for ` = k + α−1, we have\n‖ATA−BTB‖2 ≤ α‖A− [A]k‖2F .\nSince each SVD computation in DenseShrink takes O(d`2) time, and there are totally n/` SVD computations (SVD is applied every ` rows), the running time is O(nd`) = O(nd(k + α−1)). Our goal is to separate nd from α−1 in the running time.\nAlgorithm 1 DenseShrink Input: B ∈ R2`×d.\n1: Compute [U,Σ, V ] = SVD(B), and σ = Σ`,`. 2: Σ̂ = √ max(Σ2 − σ2I`, 0) I ReLu(x) = max(x, 0) 3: return B = Σ̂V T\nAlgorithm 2 DenseShrinkR Input: B ∈ R2`×d.\n1: Compute [U,Σ, V ] = SVD(B), and σ = Σ`,`. 2: Σ̂ = √ max(Σ2 − σ2I`, 0)\n3: Σ̄ = √\nΣ2 − Σ̂2 I Σ2 = Σ̄2 + Σ̂2 4: return B = Σ̂V T , Σ̄ and V T\nTo achieve this, we first compute a coarse approximation using FD by invoking B = FD(A, k, 12k ), which takes\nAlgorithm 3 FDShrink Input: B ∈ R`′×d, v ∈ Rd, and an integer `\nI it always holds that `′ < 2`. 1: B = [B;v] 2: If `′ + 1 = 2`, then B = DenseShrink(B, `). 3: return B\nO(ndk) time. The key idea here is that in each DenseShrink operation, after shrinking B, we also return the residual; we call this modified shrinking operation DenseShrinkR (see Algorithm 2). Let C be the matrix which is the concatenation of all residuals return from DenseShrinkR. We will showATA = BTB+CTC and ‖C‖2F ≤ ‖A−[A]k‖2F . We then refine the answer by computing an approximation to C. Since the norm of C is small, random sampling suffices. To save space, the sampled rows will be fed to a standard FD algorithm. See Algorithm 4 for detailed description of the algorithm.\nAlgorithm 4 FFDdense Input: A ∈ Rn×d, 0 < α < 1, and integer k ≤ d.\n1: F = 0, ` = 3k, Q = empty 2: for i = 1 to n do 3: Append Ai after B 4: if rows(B) = 2` then 5: [B,Σ, V ] = DenseShrinkR(B) I Here ` = 3k, thus B = FD(A, 12k , k) 6: F = F + ‖Σ‖2F ,\nI ### Next: subsample C := ΣV , and then compressed the sampled rows using standard FD\n7: for j = 1 to 2` do 8: pj = Σ2j α2F\n9: Sample Cj with probability pj . 10: if Cj is sampled then 11: Set v = Cj√pj . 12: Q = FDShrink([Q : v], 1α ) I Invoking FD with k = 0 13: end if 14: end for 15: end if 16: end for 17: return [B;Q]\nCorrectness. We note that, at the end of Algorithm 4, B = FD(A, 12k , k), so ‖A\nTA−BTB‖2 ≤ ‖A− [A]k‖2F /2k, or equivalently\nmax x:‖x‖=1\n| ‖Ax‖2 − ‖Bx‖2 | ≤ ‖A− [A]k‖2F /2k. (3)\nLet Σ(i), V (i), and B(i) be the value of Σ, V , and B respectively returned by ith DenseShrinkR operation (line 5). Let\nC(i) = Σ(i)V (i). We use B′(i) to denote the value of B right before the ith DenseShrinkR operation (or the input of the ith DenseShrinkR operation) . From Algorithm 2, we have that\nB′(i)TB′(i) = B(i)TB(i) + V (i)TΣ(i)2V (i)\n= B(i)TB(i) + C(i)TC(i).\nLet A(i) be the rows of A arrived between the (i − 1)th and the ith DenseShrinkR operation, which means B′(i) = [B(i−1);A(i)], and thus\nB′(i)TB′(i) = B(i−1)TB(i−1) +A(i)TA(i).\nCombined with the previous equality, we get\nA(i)TA(i) +B(i−1)TB(i−1) −B(i)TB(i) = C(i)TC(i).\nLet t be the total number of iterations. We define B(0) = 0, and C = [C(1); · · · ;C(t)]. Summing the above equality over i = 1, · · · , t, we have\nCTC = ∑ i C(i)TC(i)\n= ∑ i ( A(i)TA(i) +B(i−1)TB(i−1) −B(i)TB(i) ) = ATA−BTB.\nIt follows that\n‖C‖2F = trace(CTC) = trace(ATA)− trace(BTB) = ‖A‖2F − ‖B‖2F .\nNow we bound ‖A‖2F − ‖B‖2F using similar ideas as in (Ghashami & Phillips, 2014). Let wj be the jth singular vector of A, we have ‖C‖2F = ‖A‖2F − ‖B‖2F\n= k∑ j=1 ‖Awj‖2 + d∑ j=k+1 ‖Awj‖2 − ‖B‖2F ≤ k∑ j=1 ‖Awj‖2 + ‖A− [A]k‖2F − k∑ j=1 ‖Bwj‖2\nbecause k∑ j=1 ‖Bwj‖2 ≤ ‖B‖2F\n≤ ‖A− [A]k‖2F + k · ‖A− [A]k‖2F /2k by Eq (3) = 1.5‖A− [A]k‖2F . (4)\nIn the algorithm, each row of C is sampled with probability ‖Cj‖2 α2F , where F is the current squared F-norm of C. Let Cs be the sampled rows. Given Eq (4), we can prove the following using Theorem 2\n‖CTC − CTs Cs‖2 ≤ α‖A− [A]k‖2F , and\n‖Cs‖2F = O(1) · ‖A− [A]k‖2F .\nAt the end of the algorithm, Q = FD(Cs, α, 0), then\n‖CTs Cs −QTQ‖2 ≤ α‖Cs‖2F ≤ O(α) · ‖A− [A]k‖2F .\nApplying triangle inequality, we have ‖CTC −QTQ‖2 ≤ O(α) · ‖A− [A]k‖2F , and thus\n‖ATA−BTB −QTQ‖2 = ‖CTC −QTQ‖2 ≤ O(α) · ‖A− [A]k‖2F ,\nwhich proves the correctness.\nSpace and running time. The space is dominated by maintainingB = FD(A, 1/2k, k) andQ = FD(Cs, α, 0), which is O(dk + d/α) in total.\nThe running time of computing B is O(ndk), and the running time for Q is O(rows(Cs)d/α). To bound rows(Cs), we divide the stream into epochs, where F roughly doubles in each epoch. This means the total number of epochs is bounded by O(log(nd)), since we assume each real number in the input can be represented by O(log(nd)) bits2. Applying Theorem 2 on the submatrix in each epoch, it is easy to check the expected number of rows sampled in each epoch is O(1/α2), so rows(Cs) = O( log(nd) α2 ). Thus the total running time is O(ndk) + Õ(dα−3). We remark that the residual return by DenseShrinkR is in the form of C = ΣV T , where Σ is diagonal and V has orthonormal columns. Therefore, the row norms of C are simply the diagonals of Σ."
  }, {
    "heading": "4. Algorithm for sparse matrices",
    "text": ""
  }, {
    "heading": "4.1. Overview of our algorithm",
    "text": "Our approach is quite different from (Ghashami et al., 2016). Their main idea is to use fast approximate SVD (Musco & Musco, 2015) in the original FD, which leads to suboptimal time. Our approach is summarized as follows.\n1. Decompose ATA = A′TA′ +RTR, such that A′ contains small number of rows and ‖A′−[A′]k‖2F = O(1)· ‖A− [A]k‖2F . Moreover, ‖R‖2F = O(1) ·‖A− [A]k‖2F .\n2. Compute a sketch B of A′ using fast FD algorithm for dense matrices (Theorem 5), which satisfies that ‖A′TA′ − BTB‖2 ≤ α‖A′ − [A′]k‖2F ≤ α‖A − [A]k‖2F .\n3. Compute a sketch matrix C of R such that ‖RTR − CTC‖2 ≤ α‖R‖2F ≤ O(α) · ‖A− [A]k‖2F , which can be done via random sampling (Theorem 2) combined with FD.\n2A rigorous analysis on this will be more subtle; see discussions in the proof of Lemma 6 below.\n4. The final sketch is S = [B;C].\nNote that S = [B;C] approximate [A′;R] in the sense that\n‖A′TA′ +RTR−BTB − CTC‖2 ≤ ‖A′TA′ −BTB‖2 + ‖RTR− CTC‖2 ≤ O(α) · ‖A− [A]k‖2F .\nFrom step (1), we have ATA = A′TA′ + RTR, and thus [B;C] is a good approximation of A. Next we briefly describe how to implement this in one pass and small space.\nTo achieve (1), we use the following new idea. Let Z ∈ RO(k)×d be an orthonormal matrix satisfying ‖A − ZTZA‖2F ≤ O(1)‖A − [A]k‖2F . Let A′ = ZA and R = (I − ZTZ)A. It is easy to check that A′ and R satisfy the requirement of (1). In the streaming model, we divide A into blocks, each of which contains roughly dk non-zero entries, and thus there are at most t = nnz(A)dk blocks. We use the above idea for each of the blocks and concatenate the results together. More precisely, for each block A(i) ∈ R`i×d, we use an input-sparsity time algorithm (Theorem 3) to compute a matrix Z(i) ∈ RO(k)×`i such that\n‖A(i) − Z(i)TZ(i)A(i)‖2F ≤ 4‖A(i) − [A(i)]k‖2F .\nLet A′(i) = Z(i)A(i), and R(i) = (I − Z(i)TZ(i))A(i). We then set A′ = [A′(1); · · · ;A′(t)] and R = [R(1); · · · ;R(t)], and prove that A′ and R satisfy the requirement of (1), where A′ only has t×O(k) = O( nnz(A)d ) rows (since each block of A′ has O(k) rows). Here we do not compute R explicitly, as we will sample a subset of the rows from R. Note that the running time of this step is dominated by computing Z(i)A(i), which is O(nnz(A(i))k), and thus O(nnz(A)k) in total.\nTo compute B of step (2), we may use the standard FD(A′, α, k) (Theorem 1). Since A′ has at most O( nnz(A)d ) rows, B can be computed in O(nnz(A)(k + α−1)) time. However, it still has an nnz(A)α−1 term. So we apply our faster FD algorithm for dense matrices (Theorem 5) on A′, which only takes O(nnz(A)k) + Õ(dα−3) time.\nIn order to compute a sketch C of R in step (3), we first subsample the rows of R using streaming Bernoulli sampling. One difficulty is that R could be dense, and it may take nd time to compute the norms of the rows. Fortunately, constant approximations of the norms are good enough, and thus we can use Johnson-Lindenstrauss (JL) (Johnson & Lindenstrauss, 1984) transform to reduce the dimensionality of R from d to O(log n). Let Φ ∈ RO(logn)×d be a JL transform, then RΦT can be computed in time O(nnz(A) log n+nk log n). Now we only need to compute the norms of the rows in RΦT , which are constant approximations to the row norms inR (by JL Lemma). LetQ be the\nsampled rows, with rows(Q) = Õ(1/α2), and each row of Q can be computed in time O(kd) as Z(i)A(i) has already been computed in step (1). We finally use FD(Q,α, 0) to compute a sketch matrix C of Q in time Õ(dα−3).\nIn all, the running time is roughly O(nnz(A)(k + log n)) + Õ(dα−3 + dkα−2)."
  }, {
    "heading": "4.2. Our algorithm",
    "text": "Algorithm 5 FFDsparse Input: A ∈ Rn×d, α ∈ (0, 1), and integers k ≤ d.\n1: F = η, F ′ = 0, B = 0, Q = 0 I η will be determined in Lemma 6 2: Divide the rows of A into continuous blocks A(1), · · · , A(t): we will put new rows into the current block until: a) the number of non-zero entries exceeds dk, or b) the number of rows is dk log(nd) . When either a) or b) happens, we start a new block. Note that the total number of blocks t ≤ nnz(A)dk + nk log(nd) d . 3: for i = 1 to t do 4: Use Theorem 3 to compute Z(i) such that ‖A(i) −\nZ(i)TZ(i)A(i)‖2F ≤ O(1)‖A(i) − [A(i)]k‖2F with probability 1− 1/n2. Let `i = rows(A(i)).\n5: Compute A′(i) = ZA(i). 6: Compute W = (I − Z(i)TZ(i))A(i)Φ, where Φ ∈\nRd×O(logn) is a dense JL transform (each entry is an iid Gaussian). Compute all the rows norms of W , and let w ∈ R`i be the vector of these norms.\n7: F ′ = F ′ + ‖w‖2, and if F ′ ≥ 2F , F = F ′. I F = O(1) · ∑ i(I − Z(i)TZ(i))A(i) by JL 8: Let p ∈ R`i such that pj = w 2 i\nα2F for j = 1, · · · , `i. Let x ∈ R`i be a random vector with iid entries. For each j, xj = 1/pj w.p. pj , and xj = 0 w.p. 1− pj .\n9: Let R(i) = (I − Z(i)TZ(i))A(i) and Q(i) = Diag(x) ·R(i) (no need to compute R(i) explicitly). 10: B = FFDdense([B;A′(i)], α, k). I Sketching A′ = [A′(1); · · · ;A′(t)] using Theorem 5 11: C = FD([C;Q(i)], α, 0). I Sketching Q = [Q(1); · · · ;Q(t)] using FD. 12: end for 13: return [B;C]\nTheorem 6 (FFDsparse). Given any matrix A ∈ Rn×d, 0 < α < 1 and 0 ≤ k ≤ d, FFDsparse(A,α, k) (Algorithm 5) maintains a matrix S in a streaming fashion, such that\n‖ATA− STS‖2 ≤ α‖A− [A]k‖2F .\nThe algorithm uses O(d(k + α−1)) space and runs in O(nnz(A)k + nnz(A) log n) + Õ(nk3 + d · poly(kα−1)).\nBy Lemma 1, we also have the following result.\nTheorem 7. Given any matrix A ∈ Rn×d, 0 < ε < 1 and 0 < k ≤ d, there is a streaming algorithm which maintains a strong (ε, k)-proj-sketch S ∈ RO(k/ε)×d. The algorithm uses O(dk/ε) space and runs in O(nnz(A)k + nnz(A) log n) + Õ(nk3 + d · poly(kε−1))."
  }, {
    "heading": "4.3. Proof of Theorem 6",
    "text": "Proof of Theorem 6. The detail of our fast algorithm for sparse matrix is described in Algorithm 5.\nWe let A′ = [A′(1); · · · ;A′(t)], R = [R(1); · · · ;R(t)], and Q = [Q(1); · · · ;Q(t)]. We use w(i) to denote the vector w in ith iteration. We need some technical lemmas.\nLemma 2. With probability at least 1 − 1/n, (1) ‖A′ − [A′]k‖F ≤ ‖A− [A]k‖F ; (2) ‖R‖2F ≤ O(1) ·‖A− [A]k‖2F .\nProof. We divide A and A′ into blocks as defined in Algorithm 5, i.e. A = [A(1); · · · ;A(t)] and A′ = [A′(1); · · · ;A(t)]. For each i, we have A′(i) = Z(i)A(i) for some matrix Z(i) with O(k) orthonormal rows such that, with probability at least 1− 1/n2,\n‖A(i)−Z(i)TZ(i)A(i)‖2F ≤ O(1) ·‖A(i)− [A(i)]k‖2F . (5)\nBy union bound, with probability at least 1− 1/n, eqn (5) holds for all i simultaneously. Let P be the projection matrix onto the subspace spanned by the top-k right singular vectors of A. So we have\n‖A′ − [A′]k‖2F ≤ ‖A′ −A′P‖2F\n= t∑ i=1 ‖A′(i) −A′(i)P‖2F P has rank k\n= t∑ i=1 ‖Z(i)A(i) − Z(i)A(i)P‖2F\n≤ t∑ i=1 ‖A(i) −A(i)P‖2F Z(i) is a orthogonal = ‖A−AP‖2F = ‖A− [A]k‖2F , by definition of P\nwhich proves (1).\nAs defined in Algorithm 5, R(i) = (I − Z(i)TZ(i))A(i), where Z(i) satisfies (5). Therefore,\n‖R‖2F = t∑ i=1 ‖(I − Z(i)TZ(i))A(i)‖2F\n≤ O(1) · t∑ i=1 ‖A(i) − [A(i)]k‖2F = O(1) · ‖A− [A]k‖2F\nwhich proves (2).\nLemma 3. ATA = RTR+A′TA′; with probability 1−1/n it holds that ‖ATA−A′TA′‖F ≤ O(1) · ‖A− [A]k‖2F .\nProof. To prove the first part, we only need to prove A(i)TA(i) = R(i)TR(i) +A′(i)TA′(i) holds for all i. Recall that A′(i) = Z(i)A(i). For each i, we have\nR(i)TR(i) = A(i)T (I − Z(i)TZ(i)) · (I − Z(i)TZ(i))A(i)\n= A(i)T (I − Z(i)TZ(i))A(i)\n= A(i)TA(i) −A′(i)TA′(i).\nThis proves the first part, from which, we also get\n‖ATA−A′TA′‖F = ‖RTR‖F ≤ ‖R‖2F ,\nwhere the inequality is from the submultiplicative of matrix norms. Then the second part follows from Lemma 2.\nLemma 4. If the entries of A are integers bounded in magnitude by poly(nd) and rank(A) ≥ 1.1k, then ‖A − [A]k‖2F ≥ 1/poly(nd).\nProof. The lemma directly follows from a result of (Clarkson & Woodruff, 2009), and here we use the restated version from (Boutsidis et al., 2016).\nLemma 5 (Lemma 37 of (Boutsidis et al., 2016)). If an n× d matrix A has integer entries bounded in magnitude by γ, and has rank ρ, then the k-th largest singular value of A satisfies\nσk ≥ (ndγ2)−k/2(ρ−k).\nLemma 6. We set η = poly−1(nd), then with probability at least 0.99, it holds that\n‖Q‖2F = O(‖R‖2F ), ‖RTR−QTQ‖2 = O(α)·‖A−[A]k‖2F ,\nand rows(Q) = O(log(nd)/α2).\nProof. Let us first assume rank(A) ≥ 1.1k. Each row Ri ofR is sampled with probability Θ(‖Ri‖ 2\nα2F ) (by JL property), with F initialized to be η. We have η ≤ ‖A− [A]k‖2F (by Lemma 4). When ‖R‖2F ≥ η, the probability is at least Ω( ‖Ri‖ 2\nα2‖R‖2F ) (since F will be a constant approximation of ‖R‖2F by JL), so the first two parts directly follow from Theorem 2 and Lemma 2. Otherwise if ‖R‖2F ≤ η, then the probability is Ω(‖Ri‖ 2\nα2η ) = Ω( ‖Ri‖2\nα2‖A−[A]k‖2F ) the first two\nparts follow by Theorem 2.\nTo bound the number of rows sampled, we divide the stream into epochs, where F roughly doubles in each epoch. So the total number of epochs is bounded by O(log ‖R‖Fη ), as the final value of F is at most O(‖R‖2F ). As we assume each\nentry of A is integer bounded in magnitude by poly(nd), which implies the number of epochs is\nO(log ‖R‖2F η\n) ≤ O ( log ( ‖A‖2F · poly(nd) )) = O(log(nd)).\nThe number of rows sampled in each epoch is at most O(1/α2): let a1, · · · , at be the rows of R in the epoch, and thus ∑ j ‖aj‖2 ≤ O(F ) (otherwise the epoch ends); each row aj is sampled with probability Θ( ‖aj‖2 α2F ), which implies the total number of rows sampled is O(1/α2). This proves the third part.\nThe case rank(A) ≤ 1.1k is easier: we can set the rank parameter k a little larger (say k′ = 2k) in our algorithm so that R is always 0 by Lemma 2, and thus rows(Q) = 0. In this case, our algorithm is essentially exact.\nCorrectness. By union bound, with probability 0.9 all the above lemmas hold simultaneously, and we will assume this happens. Since C = FD(Q,α, 0), by Theorem 1 and Lemma 6, we have\n‖QTQ− CTC‖2 ≤ α‖Q‖2F ≤ O(α) · ‖R‖2F . (6)\nSince B is a matrix such that B = FFDdense(A′, α, k), by Theorem 5, we have\n‖A′TA′ −BTB‖2 ≤ α‖A′ − [A′]k‖2F ≤ α‖A− [A]k‖2F , (7)\nwhere the last inequality is from Lemma 2. Let S = [B;C],\n‖ATA− STS‖2 = ‖ATA−BTB − CTC‖2 ≤ ‖ATA−A′TA′ − CTC‖2 + ‖A′TA′ −BTB‖2 ≤ ‖ATA−A′TA′ − CTC‖2 + α‖A− [A]k‖2F by (7) = ‖RTR− CTC‖2 + α‖A− [A]k‖2F ≤ ‖RTR−QTQ‖2 + ‖QTQ− CTC‖2 + α‖A− [A]k‖2F\ntriangle inequality\n≤ O(α) · ‖A− [A]k‖2F +O(α) · ‖R‖2F + α‖A− [A]k‖2F by (6) and Lemma 6\n≤ O(α) · ‖A− [A]k‖2F , by Lemma 2\nwhich proves the error bound after adjusting α by a constant.\nSpace and running time. For space, we need a buffer to store a new block ofA, the size of which is at most dk+d, as the nnz(A(i)) is at most dk+ d. When applying Theorem 3, we set δ = 1/n2, and the input matrix has at most dk log(nd) rows, so we need O(dk) space to compute and store Z. A′(i) is of dimension O(k)× d, which needs O(dk) space to compute and store. To naively compute W = (I − ZTZ)A(i)Φ, we need O(dk + d log n) space. However,\nobserve that we do not have to compute W explicitly, we only need to know its row norms, i.e. the vector w. To save space, we compute one column of W at a time, i.e., generate columns of Φ one by one, and update w iteratively, and thus the extra space used is O(d). From Theorem 5, the space used by FFDdense(A′, α, k) is O(d(k + α−1)). Note that, in line 11 of Algorithm 5, the rows of Q(i) can be computed one by one, and thus compute C = FD(Q,α, 0) uses O(d/α) space by theorem 1. So the total space usage is bounded by O(d(k + α−1)).\nLet `i be the number of rows in ith block A(i). The time to compute Z using Theorem 3 is O(nnz(A(i) log n) + Õ(`ik\n3). Hence the total time used on this step is∑ i O(nnz(A(i) log n) + Õ(`ik 3) =\nO(nnz(A) log n) + Õ(nk3).\nThe step to compute the matrix multiplicationA′(i) = ZA(i) takes O(nnz(A(i))k) time, since Z has O(k) rows. So the total time spent on this step is O(nnz(A)k). By the definition of blocks, there are at most O( nnz(A)dk + nk log(nd) d ) blocks. After left multiplying Z, each block contributes O(k) rows to A′, and thus the total number of rows in A′ is at most O( nnz(A)d + nk2 log(nd)\nd ). Computing B by invoking B = FFDdense(A′, α, k) needs O(rows(A′)dk) + Õ(d/α3) = O(nnz(A)k) + Õ(nk3 + d/α3) time. Finally, each row of Q can be computed in time O(dk) given A′ (which has been computed in line 5). Invoking C = FD(Q,α, 0) needs Õ(d/α3 + dk/α2) since rows(Q) = Õ(1/α2) by Lemma 6. The total time is thus O(nnz(A)k + nnz(A) log n) + Õ(nk3 + dα−3 + dkα−2)."
  }, {
    "heading": "5. Conclusion",
    "text": "In this paper, we study covariance sketches for matrices in the streaming model. We provide new space-optimal algorithms with improved running time. We also prove that our running times cannot be significantly improved unless the state-of-the-art matrix multiplication algorithms can. Thus, we almost settle the time complexity of this problem."
  }, {
    "heading": "Acknowledgments",
    "text": "This work is supported by Shanghai Science and Technology Commission (Grant No. 17JC1420200) and Shanghai Sailing Program (Grant No. 18YF1401200)."
  }],
  "year": 2018,
  "references": [{
    "title": "Sketching structured matrices for faster nonlinear regression",
    "authors": ["Avron", "Haim", "Sindhwani", "Vikas", "Woodruff", "David"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2013
  }, {
    "title": "Online principal components analysis",
    "authors": ["Boutsidis", "Christos", "Garber", "Dan", "Karnin", "Zohar", "Liberty", "Edo"],
    "venue": "In SODA. SIAM,",
    "year": 2015
  }, {
    "title": "Optimal principal component analysis in distributed and streaming models",
    "authors": ["Boutsidis", "Christos", "D Woodruff", "Zhong", "Peilin"],
    "year": 2016
  }, {
    "title": "Efficient second-order online kernel learning with adaptive embedding",
    "authors": ["Calandriello", "Daniele", "Lazaric", "Alessandro", "Valko", "Michal"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Algorithms for `p low rank approximation",
    "authors": ["Chierichetti", "Flavio", "Gollapudi", "Sreenivas", "Kumar", "Ravi", "Lattanzi", "Silvio", "Panigrahy", "Rina", "Woodruff", "David"],
    "year": 2017
  }, {
    "title": "Numerical linear algebra in the streaming model",
    "authors": ["Clarkson", "Kenneth L", "Woodruff", "David P"],
    "venue": "In STOC,",
    "year": 2009
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["Clarkson", "Kenneth L", "Woodruff", "David P"],
    "venue": "In STOC,",
    "year": 2013
  }, {
    "title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication",
    "authors": ["Drineas", "Petros", "Kannan", "Ravi", "Mahoney", "Michael W"],
    "venue": "SIAM Journal on Computing,",
    "year": 2006
  }, {
    "title": "Relative errors for deterministic low-rank matrix approximations",
    "authors": ["Ghashami", "Mina", "Phillips", "Jeff M"],
    "venue": "In SODA,",
    "year": 2014
  }, {
    "title": "Continuous matrix approximation on distributed data",
    "authors": ["Ghashami", "Mina", "Phillips", "Jeff M", "Li", "Feifei"],
    "venue": "Proceedings of the VLDB Endowment,",
    "year": 2014
  }, {
    "title": "Efficient frequent directions algorithm for sparse matrices",
    "authors": ["Ghashami", "Mina", "Liberty", "Edo", "Phillips", "Jeff M"],
    "year": 2016
  }, {
    "title": "Streaming anomaly detection using randomized matrix sketching",
    "authors": ["Huang", "Hao", "Kasiviswanathan", "Shiva Prasad"],
    "venue": "Proceedings of the VLDB Endowment,",
    "year": 2015
  }, {
    "title": "Efficient matrix sketching over distributed data",
    "authors": ["Huang", "Zengfeng", "Lin", "Xuemin", "Zhang", "Wenjie", "Ying"],
    "venue": "In Proceedings of PODS,",
    "year": 2017
  }, {
    "title": "Extensions of lipschitz mappings into a hilbert space",
    "authors": ["Johnson", "William B", "Lindenstrauss", "Joram"],
    "venue": "Contemporary mathematics,",
    "year": 1984
  }, {
    "title": "Principal component analysis and higher correlations for distributed data",
    "authors": ["Kannan", "Ravi", "Vempala", "Santosh", "Woodruff", "David"],
    "venue": "In Proceedings of The 27th Conference on Learning Theory, pp",
    "year": 2014
  }, {
    "title": "Online pca with spectral bounds",
    "authors": ["Karnin", "Zohar", "Liberty", "Edo"],
    "venue": "In Conference on Learning Theory,",
    "year": 2015
  }, {
    "title": "Scalable semi-supervised query classification using matrix sketching",
    "authors": ["Kim", "Young-Bum", "Stratos", "Karl", "Sarikaya", "Ruhi"],
    "venue": "In ACL,",
    "year": 2016
  }, {
    "title": "Online sketching hashing",
    "authors": ["Leng", "Cong", "Wu", "Jiaxiang", "Cheng", "Jian", "Bai", "Xiao", "Lu", "Hanqing"],
    "venue": "In IEEE CVPR, pp",
    "year": 2015
  }, {
    "title": "Improved distributed principal component analysis",
    "authors": ["Liang", "Yingyu", "Balcan", "Maria-Florina F", "Kanchanapally", "Vandana", "Woodruff", "David"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Simple and deterministic matrix sketching",
    "authors": ["Liberty", "Edo"],
    "venue": "In KDD, pp. 581–588",
    "year": 2013
  }, {
    "title": "Efficient second order online learning by sketching",
    "authors": ["Luo", "Haipeng", "Agarwal", "Alekh", "Cesa-Bianchi", "Nicolo", "Langford", "John"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition",
    "authors": ["Musco", "Cameron", "Christopher"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Is input sparsity time possible for kernel low-rank approximation",
    "authors": ["Musco", "Cameron", "Woodruff", "David"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Improved approximation algorithms for large matrices via random projections",
    "authors": ["Sarlos", "Tamas"],
    "venue": "In FOCS,",
    "year": 2006
  }, {
    "title": "Incremental matrix factorization via feature space re-learning for recommender system",
    "authors": ["Song", "Qiang", "Cheng", "Jian", "Lu", "Hanqing"],
    "venue": "In RecSys,",
    "year": 2015
  }, {
    "title": "Matrix sketching over sliding windows",
    "authors": ["Wei", "Zhewei", "Liu", "Xuancheng", "Li", "Feifei", "Shang", "Shuo", "Du", "Xiaoyong", "Wen", "Ji-Rong"],
    "year": 2016
  }, {
    "title": "Low rank approximation lower bounds in row-update streams",
    "authors": ["Woodruff", "David"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Frequent direction algorithms for approximate matrix multiplication with applications in cca",
    "authors": ["Ye", "Qiaomin", "Luo", "Zhang", "Zhihua"],
    "venue": "In AAAI,",
    "year": 2016
  }],
  "id": "SP:b0d988adcf72b1ccebbe4f1bf32ff30e80902466",
  "authors": [{
    "name": "Zengfeng Huang",
    "affiliations": []
  }, {
    "name": "∈ R`×d",
    "affiliations": []
  }],
  "abstractText": "Given a large matrix A ∈ Rn×d, we consider the problem of computing a sketch matrix B ∈ R`×d which is significantly smaller than but still well approximates A. We are interested in minimizing the covariance error ‖AA−BB‖2. We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space. The popular Frequent Directions algorithm of (Liberty, 2013) and its variants achieve optimal space-error tradeoff. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the time complexity of this problem. In particular, we provide new space-optimal algorithms with faster running times. Moreover, we also show that the running times of our algorithms are near-optimal unless the state-of-the-art running time of matrix multiplication can be improved significantly.",
  "title": "Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices"
}