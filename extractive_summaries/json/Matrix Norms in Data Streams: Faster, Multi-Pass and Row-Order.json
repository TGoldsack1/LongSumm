{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Modern datasets, from text documents and images to social graphs, are often represented as a large matrix A ∈ Rm×n. In many application domains, including database queries, data mining, network transactions and sensor networks (see e.g. (Liberty, 2013; Wei et al., 2016; Huang & Kasiviswanathan, 2015) for recent examples), the input matrix A is presented to the algorithm as a data stream, i.e., a sequence of items/updates that can take several forms. In the entry-wise (or insertion-only) model, each item specifies (i, j, Aij) and provides the value of one entry, in arbitrary order (and the unspecified entries are set to 0). The roworder model is similar, except that the items follow the nat-\n*Equal contribution 1Johns Hopkins University 2ETH Zurich 3Weizmann Institute of Science 4Nanyang Technological University 5Carnegie Mellon University 6Princeton University. Correspondence to: Lin F. Yang <lin.yang@princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nural order (sorted with i as the primary key, and j as the secondary one). In the turnstile model, each stream item has the form (i, j, δ) and represents an update Aij ← Aij + δ for δ ∈ R (after initializing A to the all-zeros matrix). These models capture different access patterns, but all three can represent sparse matrices quite efficiently, because zero entries are implicit. As usual, the key parameters of an algorithm in the data-stream model are its memory (also referred to as storage/space requirements) and its running time (per update and to report its output). Many properties of a matrix are directly related to its spectral characteristics, i.e., its singular values. For example, the number of non-zero singular values is just the matrix rank, which determines the degrees of freedom of a corresponding linear system; the maximum and minimum singular values of a matrix determine its condition number, which in turn determines the hardness of many problems, such as optimization problems; the leading singular values of a matrix determine how well a matrix can be represented by the principal components; and so forth. It is generally hard to compute directly the singular values of a matrix, especially in the streaming model, but luckily, the Schatten norms of the matrix can often be used as surrogates for its spectrum, see e.g. (Zhang et al., 2015; Kong & Valiant, 2016; Di Napoli et al., 2016; Khetan & Oh, 2017). Formally, the Schatten p-norm of a matrix A ∈ Rm×n is defined, for every p ≥ 1, as\n‖A‖Sp := (∑ j≥1 σ p j )1/p ,\nwhere σ1 ≥ · · · ≥ σmin(m,n) are the singular values of A. This definition naturally extends to all 0 < p < 1 although then it is not a norm, and also to p = 0,∞ by taking the limit. This is a very important family of matrix norms, and includes as special cases the well-known trace/nuclear norm ||A||∗ = ∑ j≥1 σj = ‖A‖S1 , the\nFrobenius norm ||A||F = (∑ j≥1 σ 2 j )1/2 = ‖A‖S2 , and the spectral/operator norm ‖A‖op = σ1(A) = ||A||S∞ . We study algorithms that approximate the Schatten p-norm of a matrix A presented in a data stream. While this problem has attracted significant attention lately (Andoni & Nguyên, 2013; Li et al., 2014; Li & Woodruff, 2016a;b; 2017), our results address three new aspects. First, we design faster and more space-efficient multi-pass algorithms. Second, we consider the row-order model, which is a com-\nmon access pattern for matrix data (see, e.g. (Liberty, 2013)). Third, we design algorithms with faster update time and/or query time. The above three aspects were not considered previously for matrix norms, and our work opens the door for further diversification of prevailing models (and thereby of current algorithms). In particular, our results can be applicable to classical scenarios, e.g., where data is stored on disk (or any media where a linear scan is much faster than random access), and potentially lead to performance improvements in other such domains. In the next few subsections, we present our contributions in more detail."
  }, {
    "heading": "1.1. New Estimator for PSD Matrices (or Even p)",
    "text": "Our first results rely on a new method for estimating the Schatten p-norm ||A||Sp of a positive semidefinite matrix (PSD) matrix A ∈ Rn×n for integer p ≥ 2. This method yields two new streaming algorithms in the turnstile model, which require, respectively, one pass and dp/2e passes over the input. Both algorithms are at least as good as the previous ones in all three standard performance measures of storage, update time, and query time; and each algorithm offers significant improvements in two out of these three. Our one-pass algorithm achieves update time O(1) compared with the previous poly(n), and query time O(nω(1−p/2)), where ω ≤ 2.373 is the matrix multiplication exponent (Le Gall, 2014), compared with the previous np−2. And our multi-pass algorithm requires storage that is sublinear in n, compared with O(n) previously. We note that if p is even, then the above results extend to arbitrary A ∈ Rm×n (and not only PSD) by a standard argument. A detailed comparison of the bounds is given in Table 1, and the results themselves appear in Section 3. Throughout the paper, a matrix is called sparse if it has at most O(1) non-zero entries per row and per column. We write Õ(f) as a shorthand for O(f · logO(1) f), and write Oa(f) to indicate that the constant in O-notation depends on some parameter a.\nTechniques Our technical innovation is an unbiased estimator of tr(Ap) for a symmetric (and not only PSD) matrix A ∈ Rn×n. To see why this is useful, denote the eigenvalues of A by λ1 ≥ · · · ≥ λn, and observe that if A is PSD (or alternatively if p is even), then tr(Ap) = ∑ i λ p i =∑\ni σi(A) p = ||A||pSp . Our estimator has the form\nX := tr(G1AG T 2 G2AG T 3 · · ·GpAGT1 ), (1)\nwhere Gi ∈ Rt×n are certain random matrices. This estimator X can be computed from the p bilinear sketches {GiAGTi+1}i∈[p] by straightforward matrix multiplication, where Gp+1 := G1 by convention. And if, say, t = O(n1−2/p), then each bilinear sketch has dimension O(t2) = O(n2−4/p). These determine the streaming algorithm’s storage requirement and query time, and, if the matrices\n{Gi}i∈[p] have sparse columns, the updates will be fast. The main difficulty is to bound the estimator’s variance, which highly depends on the choice of the matrices {Gi}i∈[p]. The basics of this technique can be seen in the case p = 4, if the Gi’s satisfy the following definition. Definition 1.1. A random matrix S ∈ Rt×n is called an ( , δ, d)-Johnson-Lindenstrauss Transformation (JLT) if for every V ⊆ Rn of cardinality |V | ≤ d it holds that\nPr [ ∀x ∈ V, ‖Sx‖22 ∈ (1± )‖x‖22 ] ≥ 1− δ.\nAn ( , δ, d)-JLT can be constructed with t = O( −2 log(d/δ)) rows, which is optimal (see (Kane et al., 2011) or (Jayram & Woodruff, 2013)). While using independent N(0, 1/t) Gaussians entries works, there is a construction with only O( −1 log(1/δ)) non-zero entries per column (Kane & Nelson, 2014). The case p = 4 has a particularly short and simple analysis, whenever G1 and G2 are independent ( , δ, n)-JLT matrices, which we can achieve with t = O( −2 log n). The first idea is to “peel off” Gi from both sides, using that for any PSD matrix M , with high probability tr(GiMGTi ) ∈ (1 ± ) tr(M) (see Lemma 3.2 for a precise statement). A second idea is to use the identity tr(BC) = tr(CB) to rewrite tr(AATGT2 G2AA T ) = tr(G2AA TAATGT2 ). Now using the first idea once again, we are likely to arrive at an approximation to tr(AAT AAT ) = ||A||S4 . The full details are given in Section 3.1. The sketching method extends from p = 4 to any integer p ≥ 2, but the simple analysis above breaks (because for p > 4 the “inside” matrix M is no longer PSD) and thus our analysis is much more involved. We first analyze Gi’s with independent Gaussian entries, by a careful expansion of the fourth moment of X , which exploits certain cancellations occurring (only) for Gaussians. We then consider Gi’s that are sampled from a particular sparse JLT due to (Thorup & Zhang, 2004), and employ a symmetrizationand-decoupling argument to compare the variance of X in this case with that of Gaussian Gi’s. We make two technical remarks. First, proving E[X] = tr(Ap) is straightforward. Indeed, by the second idea above, we can rewrite X = tr(G1AG T 2 G2AG T 3 · · ·GpAGT1 ) as X = tr(GT1 G1AG T 2 G2A · · · GTpGpA). Now using E[GTi Gi] = I together with linearity of trace and of expectation, we obtain that E[X] = tr(Ap). Second, after setting t = O(n1−2/p) (independent of ), our bound on the variance is O(E[X]2), which we can decrease in a standard way, taking O(1/ 2) repetitions. See Sections 3.2 and 3.4 for details. The multi-pass streaming algorithm is implemented slightly differently, in that G1 ∈ R1×n, i.e., has only one row. The other matrices G2, . . . , Gp ∈ Rt×n are as before, although we now set t = O(n1−1/(p−1)). Our es-\ntimator X can be computed in dp/2e passes with space only 2t as follows. In the first pass, compute vectors XL ← G1AGT2 ∈ R1×t and XR ← GTpAG1 ∈ Rt×1, and then on the i-th pass update XL ← XLGTi AGi+1 and XR ← Gp−i+1AGTp−i+2XR. Notice that the computation in each pass is linear in A. For even p, after completing p/2 passes, compute and output X ′ = XLXR ∈ R (and similarly for odd p). This X ′ is similar to the estimator X described above, except for the new dimensions of the Gi’s. See Sections 3.3 and 3.4. This multi-pass algorithm offers a very significant space savings over the one-pass algorithm. It is also a bit surprising because its space is getting close to the corresponding vector norm, namely, the `p-norm on Rn, for which the optimal space for O(p) passes is Õ(n1−2/p) bits. In fact, for the vector norm, O(p) passes do not significantly reduce the storage needed compared with one pass, which stands in sharp contrast to Schatten p-norms. As mentioned before, if p is even then the algorithms extends to arbitrary A ∈ Rm×n by a standard argument."
  }, {
    "heading": "1.2. Lower Bound for PSD Matrices",
    "text": "Recent work (Li & Woodruff, 2016a) has improved the storage lower bound for estimating Schatten p-norms for non-integer values of p, by showing that (1 + )- approximation (in the one-pass entry-wise model) requires storage n1−g( ), for some function g( ) → 0 as → 0, even for a sparse matrix. This contrasts with our algorithms for PSD matrices (from Section 1.1), where the exponent is independent of and bounded away from 1. However, the hard distribution used by (Li & Woodruff, 2016a) is not over PSD matrices, leaving open the possibility that PSD matrices admit algorithms that use storage O(nc) for c < 1 independent of . We close this gap in Section 4, by adapting the lower bound of (Li & Woodruff, 2016a) to PSD matrices, to show, for every non-integer p > 0, a storage lower bound of Ω(n1−g\n′( )) for some function g′( ) → 0 as → 0 (again, in the one-pass entry-wise model and even for a sparse matrix). A key feature of our lower bounds for PSD matrices is that they hold in the model in which each entry of the matrix occurs exactly once in the stream. This models applications where the matrix resides in external memory and\nis being streamed through main memory; in such a model multiple updates to an entry may not appear. While it is possible to obtain lower bounds for PSD matrices by embedding the multiplayer SET-DISJOINTNESS lower bound (Bar-Yossef et al., 2002) for vectors onto the diagonal of a matrix, to apply such lower bounds the diagonal entries need to be incremented repeatedly, that is, one such diagonal entry needs to be updated nΩ(1) times. In contrast, in our lower bounds each matrix entry occurs exactly once in the stream, i.e., there are no updates to entries."
  }, {
    "heading": "1.3. Results for Row-Order Model",
    "text": "For sparse matrices, estimating Schatten p-norms in the row-order model can be reduced to estimating Schatten (p/2)-norms in the turnstile model. Consider estimating ‖A‖pSp for some sparse matrixA. The algorithm first forms ATA = ∑ iA T i Ai “on the fly”, by reading each row Ai and immediately generating a stream of updates that corresponds to the non-zero entries in ATi Ai, and then it can just estimate the Schatten (p/2)-norm of that stream, because ‖ATA‖p/2Sp/2 = ‖A‖ p Sp\n. Observe that each row Ai has only O(1) non-zero entries, hence also ATi Ai has only O(1) non-zero entries, and the algorithm only needs O(1) space to generate the updates to ATA. Moreover, since A is sparse, also ATA is sparse. It was shown in (Li & Woodruff, 2016a) how to estimate the Schatten p-norm, for an even integer p, using Õp, (n1−2/p) bits of space, even in the turnstile model. For p ∈ 4Z, the above yields an algorithm in the row-order model that uses Õp, (n1−4/p) bits of space for sparse matrices. In Sections C and D, we study the problem in the row-order model for all p > 0. When p is not an even integer, we prove that (1 + )-approximating the Schatten p-norm in the one-pass entry-wise model requires Ω (n1−g( )) bits of space where g( ) → 0 as → 0. This bound holds even for sparse matrices, in which case it is almost tight. When p ≥ 4 is an even integer, we prove a lower bound of Ωp(n\n1−4/p) bits of space, matching up to logarithmic factors the algorithm from above for p ∈ 4Z. For the remaining case p ≡ 2 (mod 4), we present an algorithm using Õp, (n\n1−4/(p+2)) space, leaving a slight polynomial gap from the lower bound of Ωp(n1−4/p)."
  }, {
    "heading": "1.4. Previous Work",
    "text": "The aforementioned algorithm of (Li et al., 2014) uses a single sketching matrix G, for example, if A is PSD, then their sketch is S = GAGT , where G ∈ Rt×n is a Gaussian matrix. Its estimate for ||A||Sp is produced by summing over all “cycles” Si1,i2Si2,i3 · · ·Sip,i1 , where i1, . . . , ip ∈ [t] are distinct. Our sketch improves upon theirs in both update time and query time. The only other streaming algorithm for Schatten p-norms that we are aware of is that of (Li & Woodruff, 2016a) (Theorem 7), which uses space O(n1− 2 p poly(1 , log n)) but works only for matrices that have O(1)-entries per row and per column. One possible approach to improve the update time would be to replace the Gaussian matrices in (Li et al., 2014) with a distribution over matrices that admit a fast multiplication algorithm. The analysis done in (Li et al., 2014) relies on the Gaussian entries (rotational invariance, in particular), so the replacement matrix should preserve the distribution of the sketch. Kapralov, Potluru, and Woodruff (Kapralov et al., 2016) present just such a distribution on matrices G̃, where the multiplication G̃A can be computed quickly and G̃A is close to GA in total variation distance. Unfortunately, under the distribution of (Kapralov et al., 2016), or any other with a similar guarantee on total variation distance, each coordinate update to A results in a dense rankone update to the sketch, which means that the update time is not improved. Several strong lower bounds are known for approximating Schatten p-norms and other matrix functions, both for the dimension of a sketch and for storage requirement (bits). Li, Nguyen and Woodruff (Li et al., 2014) prove that for 0 ≤ p < 2 every linear sketch that can approximate rank and Schatten p-norm must have dimension Ω( √ n) and every bilinear sketch must have dimension Ω(n1− ). Li and Woodruff (Li & Woodruff, 2016b) show that every linear sketch for Schatten p-norms, p ≥ 2, requires dimension Ω(n2−4/p). In (Li & Woodruff, 2016a), they prove space complexity lower bounds that hold even when the input matrix is sparse. Specifically, they show that one-pass streaming algorithms which (1± )-approximate various functions of the singular values, including Schatten p-norms when p\nis not an even integer, require Ω(n1−g( )) bits of space for some function g( )→ 0 as → 0. Additional space lower bounds, e.g., for p ∈ [1, 2), can be deduced from a general statement of (Andoni et al., 2015), see Table 1 of (Li & Woodruff, 2016a)."
  }, {
    "heading": "2. Notation and Preliminaries",
    "text": "The space bounds of sketching algorithms in the turnstile model are stated in terms of sketch dimension (number of entries). The number of bits required can be larger by a log nM factor, where M is the absolute ratio of the largest element in the matrix to the smallest. We call a matrix a Gaussian matrix if its entries are independent N(0, 1) random variables. A matrix G of dimension t × n is a column-normalized Gaussian matrix if G = G′/ √ t, where G′ is a Gaussian matrix. Now-standard techniques such as Nisan’s pseudorandom generator or k-wise independence can be used to derandomize Gaussian matrices for use in sketching algorithms. Column-normalized Gaussian matrices serve as JLTs. In particular, there exists a constant c such that if G be a t× n column-normalized Gaussian matrix with t ≥ c 2 log d δ , then G is a ( , δ, d)-JLT (Indyk & Motwani, 1998)."
  }, {
    "heading": "3. New Estimator for PSD Matrices (and Integer p)",
    "text": "The main result in this section is a new one-pass streaming algorithm for estimating the Schatten p-norm, for integer p ≥ 2. When p is odd, it additionally requires that the input matrix is PSD. The first version of this algorithm, described in Section 3.2, has the same storage requirement of Õp(n2−4/p/ 2) bits as the previous algorithm of (Li et al., 2014) that uses cycle sums, but has a simpler analysis and faster query time1, which is roughly matrix multiplication time, nω , instead of np. Moreover, it is based on a new method that leads to a dp/2e-pass algorithm with storage requirement Õp(n1−1/(p−1)/ 2) bits, as described\n1In (Kong & Valiant, 2016), Kong and Valiant independently improve the algorithm in (Li et al., 2014) to the same runtime as Theorem 3.3 in this paper by considering only “increasing cycles”.\nin Section 3.3. Previously, the algorithm in Theorem 6.1 of (Woodruff, 2014) has the same number of passes but larger storage requirement O(n/ 2).2 Finally, we improve the update time, as described in Section 3.4, by employing the sketching matrices Gi that are certain sparse matrices instead of Gaussians. We start in Section 3.1 with the case p = 4, which is based on the same sketch but is significantly easier to analyse.\n3.1. Schatten 4-Norm using JLT matrices Theorem 3.1. Let G1, G2 ∈ Rt×n be independent ( , δn , 1)-JLT matrices. Then for every A ∈ R n×m,\nPr [\ntr(G1AA TGT2 G2AA TGT1 ) ∈ (1±2 )2||A||4S4 ] = 1−2δ.\nThus, one can find a (1± )-approximation to the Schatten4 norm of a general matrixA ∈ Rn×m using a linear sketch of dimension O( −2n log n).\nBefore proving the theorem, we remark that if each column of Gi has only s non-zero entries, it is easy to see that the update time of this linear sketch is O(s), assuming any entry of G1 and G2 can be accessed in O(1) time (in a streaming algorithm, the entries are usually computed from a small random seed in polylog(n) time). The query time is dominated by multiplying a matrix of size t×n with one of size n× t, and thus takes O(tω ·n/t) = Õ(nω/ 2(ω−1)) time. Now we prove Theorem 3.1, for which we need the following lemma.\nLemma 3.2. Let G ∈ Rt×n be an ( , δ/n, 1)-JLT matrix. Then for every PSD matrix A ∈ Rn×n,\nPr [ tr(GAGT ) ∈ (1± ) tr(A) ] ≥ 1− δ.\nProof. By the Spectral Theorem, A = UΛUT , where Λ is a diagonal matrix and U is an orthonormal matrix. Then G′ = GU is still an ( , δ/n, 1)-JLT. Thus\ntr(GAGT ) = tr(G′ΛG′T ) = tr( √ ΛG′TG′ √ Λ)\n= n∑ i=1 λie T i G ′TG′ei = n∑ i=1 λi||G′ei||22.\nBy the JLT guarantee and a union bound, with probability at least 1−δ, for all i ∈ [n] we have ||G′ei||22 ∈ [1− , 1+ ], in which case tr(GAGT ) ∈ (1± ) tr(A).\nof Theorem 3.1. Apply Lemma 3.2 to the PSD matrix AATAAT , to get that with probability at least 1 − δ (over the choice of G2),\ntr(G2AA TAATGT2 ) ∈ (1± 2 ) tr(AATAAT )\n= (1± 2 )||A||S4 , 2We note that also in Theorem 6.1 of (Woodruff, 2014) it is required that p is even or that the input matrix is PSD, but this is erroneously omitted.\nwhere the left-hand side is equal to tr(AATGT2 G2AA T ), by the identity tr(MMT ) = tr(MTM). Now suppose (by conditioning) that G2 is already fixed, and apply the same lemma to the PSD matrixAATGT2 G2AA\nT , to get that with probability at least 1− δ (over the choice of G1),\ntr(G1AA TGT2 G2AA TGT1 ) ∈ (1±2 ) tr(AATGT2 G2AAT ).\nThe proof follows by a union bound. The linear sketch of A consists of the two matrices G1A and G2A, which suffices to estimate ||A||4S4 as above with δ = 1/8. This sketch is linear and its dimension is 2tn, where we can use say Gaussians to obtain t = O( −2 log n).\n3.2. Schatten p-norm Using Gaussians We now design a sketch for Schatten-p norm that uses column-normalized Gaussian matrices. We will later extend and refine it to improve the per-update processing time.\nTheorem 3.3. For every 0 < < 1/2 and integer p ≥ 2, there is an algorithm that outputs a (1 ± )- approximation to the Schatten-p norm of a PSD matrix A ∈ Rn×n using a randomized linear sketch of dimension s = Op( −2n2−4/p). The update time (for each entry in A) is O(s) and the query time (for computing the estimate) is O( −2n(1−2/p)ω), where ω < 2.373 is the matrix multiplication constant."
  }, {
    "heading": "If p is even, the above algorithm extends to a general matrix",
    "text": "A ∈ Rn×m. The first part of the theorem (for PSD matrices) follows directly from Proposition 3.4 below. The proposition is applicable to all symmetric matrices, but ‖A‖pSp = tr(A\np) only for PSD matrices or even p. The linear sketch stores GiAGTi+1 for i = 1, . . . , p, where by convention Gp+1 = G1, repeated independently in parallel Op(1/ 2) times. Thus, the sketch has dimension Op( −2t2). The estimator is obtained by computing the Op(1/ 2) independent copies of X and reporting their average. To analyze its accuracy, notice that a PSD matrix A satisfies E[X] = tr(Ap) = ||A||pp. Then setting t = n1−2/p gives Var(X) ≤ Op(||A||Sp)2p and averaging multiple independent copies of X reduces the variance. The second part (for general matrices), follows by using the same sketch for the symmetric matrix B = ( 0 A AT 0 ) , because the nonzero singular values of B are those of A repeated twice and ||B||pSp = 2||A|| p Sp\n= 2 tr(Ap), where the last equality uses the assumption that p is even. Because the correctness of the algorithm comes from bounding the variance of X , it is enough that the entries in each Gaussian matrix are four-wise independent, which is crucial for applications with limited storage like streaming.\nProposition 3.4. For integer p ≥ 2 and t ≥ 1, let\nG1, . . . , Gp be independent t × n column-normalized Gaussian matrices. Then for every symmetric matrix A ∈ Rn×n, the estimator X = tr ( G1AG T 2 G2A . . .G T p GpAG T 1 ) satisfies\nE[X] = tr(Ap) and Var(X) =\nOp 1+b p2 c+1∑ z=2 ( n1− 2 p t )z + p∑ z=2 ( n1− 2 z t )z ||A||2pSp . The full proof of this proposition is postponed to Section E. We outline the general idea here. It is standard that a Gaussian matrix is rotational invariant, i.e., G and GU are identically distributed for any orthogonal matrix U . Thus, by the Spectral Theorem, instead of considering symmetric matrix A = UΛUT , we can consider only its diagonalization Λ. The proof of this proposition proceeds first by expanding X in terms of inner products of columns of the matrix G, i.e., X = ∑ i1,i2,...,ip∈[n] λi1λi2 . . . λip · 〈g (1) i1 , g (1) i2 〉· 〈g(2)i2 , g (2) i3 〉 . . . 〈g(p)ip , g (p) i1 〉, where λi is the i-th eigenvalue of A and g(j)ij is the ij-th column of Gj . We then expand E(X2). The non-zero terms in E(X2) are composed by only those terms of even powers in every eigenvalue. Computing the expectation of each term is straightforward because the entries of G are independent Gaussian random variables, but the crux of the proof is in bounding the sum of the terms. We introduce a collection of diagrams that aid in enumerating the terms according to their structure and computing the sum."
  }, {
    "heading": "3.3. Multi-Pass Algorithm",
    "text": "The proof of Proposition 3.4 relies on the matricesGi being Gaussians in two places. First, we assume that the matrix A is diagonal, and in general we need to consider GiU instead of Gi. Second, the columns of these matrices have small variance/moments, as described in (7)-(8). We now generalize the proof to relax these requirements (e.g., to 4-wise independence) and obtain a multi-pass algorithm.\nLemma 3.5. For integers p ≥ 2 and 1 ≤ t′ ≤ t, let G1 ∈ Rt′×n and G2, . . . , Gp ∈ Rt×n be independent columnnormalized Gaussian matrices with 4-wise independent entries. Then for every symmetric matrix A ∈ Rn×n, the estimator X = tr ( G1AG T 2 G2A . . .G T pGpAG T 1 ) satisfies\nE[X] = tr(Ap) and Var(X) =\nOp\n( 1 + bp/2c∑ z=2 nz−1−2(z−1)/p t′tz−1 + p∑ z=2 nz−2 t′tz−1 ) ‖A‖2pSp .\nThe proof of this lemma is postponed to Section F. It is a direct corollary of the proof of Proposition 3.4, except that t′, the size of the first sketch matrix, is emphasized.\nWe can now use the above sketch to approximate the Schatten p-norm using Õ(n1−1/(p−1)) bits of space with dp/2e passes over the input.\nTheorem 3.6. Let p ≥ 2 be an even integer. There is a dp/2e-pass streaming algorithm, that on input matrix A ∈ Rn×m with n ≥ m given as a stream, outputs an estimate X such that with probability at least 0.9, X ∈ (1± )||A||pSpand uses Op(n\n1−1/(p−1)/ 2) words of space. The above extends to all integers p ≥ 2 if A is PSD. The full proof is presented in Appendix A. We here sketch the proof. We take G1 ∈ R1×n and G2, G3, . . . , Gp ∈ Rt×n as independent column normalized Gaussian matrix, where t = O(n1−1/(p−1)). We then show an algorithm that computes in dp/2e-pass the estimator X = G1AG T 2 G2 . . . GpAG T 1 and uses space at most t. As shown in Lemma 3.5, X = tr(X) is a unbiased estimator for Schatten p-norm with constant variance. By repeating the algorithm O(1/ 2) times in parallel, we reach the desired accuracy."
  }, {
    "heading": "3.4. Faster Update Time",
    "text": "Since Gaussian matrices are dense, a change to one coordinate of the input matrix A may lead to a change of every entry in the sketch. This means long update times for a streaming algorithm based on the sketch. In this section we extend our result for Gaussian sketching matrices to a distribution over {−1, 0, 1} valued matrices with only one non-zero entry per column. The new sketch can be used to improve the update time of algorithms in the last two sections.\nDefinition 3.7 (Sparse ZD-sketch). Let Dt,n be the distribution over matrices G := ZD ∈ Rt×n, where Z = (z1, z2, . . . , zn) ∈ Rt×n and D = diag(d1, d2, . . . , dn) are generated as follows. Let h : [n] → [t] be a 4-wise independent hash function, and set Zi,j = 1{i=h(j)}, i.e., in each zj only the h(j)-th coordinate is set to 1, and all other coordinates are 0. The diagonal entries of D are four-wise independent uniform {−1, 1} random variables, and D is independent from Z.\nNotice that each column of G has a single non-zero entry, which is actually a random sign, and the n columns are four-wise independent. This random matrix G is similar to the sketching matrix used in (Thorup & Zhang, 2004) to speed up the update time when estimating the second frequency moment of a vector in Rn. Also note that the ZDsketch is a version of sparse JL matrices (see e.g., (Kane & Nelson, 2014; Dasgupta et al., 2010)). In this paper we do not aim at optimizing the sparsity as we focus on approximating Schatten norms. It is fairly easy to show that ZD-sketch works for approximating Schatten p-norm of matrices with all entries nonnegative. The proof is presented in Section G. We now show that the conclusion of Theorem 3.3 and Theorem 3.6\nstill hold if we replace the Gaussian matrices in the sketch with independent samples from the sparse ZD-sketch. A major difficulty that arises in replacing the Gaussian matrix with the sparse ZD-sketch is the latter’s lack of rotational invariance. To prove Theorem 3.3 we were able to expand X2 in terms of the eigenvalues of A and compute the expectation term-by-term, but this is not possible for the sparse ZD-sketch. For example, let G be a Gaussian matrix. For any orthogonal matrix U , the matrix GU is again a Gaussian matrix with an identical distribution to G. This does not hold for sparse ZD-sketch. As a consequence, in the expansion of E(X2) in the proof of Proposition 3.4, the non-zero terms would also include those monomials of odd powers of λi(A). For example, for the Schatten 3-norm, one cannot bound ∑ i1,i2,...i6∈[n] ∏6 j=1 λij by O(‖A‖6S3). But this term appears in the expansion of E(X2) of the Schatten 3-norm estimator if using the sparse ZD-sketch matrices. To resolve this problem, we use a technique similar to the proof of the Hanson-Wright Inequality in (Rudelson & Vershynin, 2013) to bound the variance of X . The proof is composed of three major steps. The first step is to decouple the dependent summands by injecting independence. The second step is to replace the independent random vectors with fully independent Gaussian vectors while preserving the variance. We can then apply our techniques for Gaussians to bound the variance of the final random variable. The case p = 1 is useful to illustrate the technique, even though Schatten 1-norm approximation can be easily accomplished in other ways. LetG ∈ Rt×n be the sparse JLT matrix and let A ∈ Rn×n be PSD. The sketch is GAGT and\ntr(GAGT )− tr(A) = ∑ i 6=j ai,j〈gi, gj〉. (2)\nSince i 6= j, gi and gj are independent. However the summands are subtly dependent. We first decouple the summand by choosing δi ∼Bernoulli(1/2), and write 〈gi, gj〉 = 4E(δi(1 − δj)〈gi, gj〉). Let V = {i : δi =\n1}, then ∑ i 6=j ai,j〈gi, gj〉 = 4Eδ ∑ i∈V,j∈V̄ ai,j〈gi, gj〉. Thus conditioning on δ and {gj : j ∈ V̄ }, the set {〈gi, ∑ j∈V̄ ai,jgj〉 : i ∈ V } is a set of independent random variables. We can match these random variables with Gaussian random variables of the same variance, and thus replace gi with independent Gaussian vectors. The same process can be repeated for gj : j ∈ V̄ , and replace every vector gi : i ∈ [n] by independent Gaussian vectors. This lets us apply similar techniques as used in the proof of Proposition 3.4 to bound the variance of the resulting random variable, and thus bound the variance of the original random variable tr(GAGT )− tr(A). The analogue of (2) for the case of our general estimator, X − tr(Ap), is much more complicated than the p = 1 case. We observe that these terms can be grouped as a sum of products of consecutive walks, i.e., ai1,i2ai2,i3 . . . aiz,jz+1〈g (z+1) jz+1 , g (z+1) iz+1\n〉 for some z. Notice that 〈g(z\n′) j′ , g (z′) j′ 〉 = 1 for any j′ and z′. For each walk, we\ncan apply a similar idea to replace the gi vectors with independent Gaussian vectors. Again, we apply similar techniques as used in the proof of Proposition 3.4 to bound the variance of each group. As a result, when replacing the Gaussian matrices by sparse JLT matrices, Lemma 3.5 still holds. Using the sparse ZD-sketch, we are able to achieve the same space bound and query time as in Theorem 3.3 and Theorem 3.6. But our update time is improved to O(1/ 2). We present the full statement of our theorem below. The full proof can be found in (Braverman et al., 2016).\nTheorem 3.8. For every 0 < < 1/2 and integer p ≥ 2, there is a randomized one-pass streaming algorithm A with space requirement O(n2−4/p/ 2), that given as input a PSD matrix A ∈ Rn×n, outputs with high probability a (1 + )-approximation of ||A||pSp . The algorithm processes an update in time O(1/ 2), and computes the output (after the updates) in time O(n(1−2/p)ω)/ 2, where ω < 3 is the matrix multiplication constant.\nThere is similarly a randomized dp/2e-pass streaming algorithm B with space requirement O(n1−1/(p−1)/ 2), update time in a pass O(1/ 2), and output time O(n(1−2/p)/ 2). For even p ≥ 2, both algorithms extend to general input"
  }, {
    "heading": "A ∈ Rn×m with m ≤ n.",
    "text": ""
  }, {
    "heading": "4. Lower Bound For PSD Matrices",
    "text": "In this section we show the lower bounds for sketching Schatten-norms for PSD matrices. This lower bounds suggest that our upper bound is nearly tight. The proof is presented in Section B.\nTheorem 4.1. Suppose that p > 0 andX ∈ Rn×n is a PSD matrix given in the entry-wise streaming model.\n(a) When p ∈ Z, there is c = c(p) > 0 such that every one-pass streaming algorithm that (1+c)-approximates ‖X‖Sp with probability 2/3 must use Ωp(n1−2/p) bits of space for even p, and Ωp(n1−2/(p−1)) bits of space for odd p.\n(b) When p 6∈ Z, for every integer t ≥ 2, there is c = c(p, t) > 0 such that every one-pass streaming algorithm that (1 + c)-approximates ‖X‖Sp with probability 2/3 must use Ωp,t(n1−1/t) bits of space.\nWe remark that all lower bounds in Theorem 4.1 even hold for sparse matrices, since the hard instances are sparse. The lower bounds for non-integers p and even integers p are strengthenings of the same lower bounds in (Li & Woodruff, 2016a), and are almost tight and tight up to polylogarithmic factors, respectively."
  }, {
    "heading": "5. Experiments",
    "text": "In this section we show numerical experiments that illustrate the performance of our Schatten-norm estimator described in Section 3. We consider two sets of synthetic inputs, which roughly represent the extreme cases for all inputs. One is a matrix drawn from a standard Gaussian distribution, i.e., A = GG>, where each entry of G ∈ Rn×n is an independent N (0, 1) random variable. The other is a matrix drawn from a Bernoulli distribution, i.e., A = BB>, where each entry of B ∈ Rn×n is an independent B(0.5) random variable. We chose n = 200 in both cases. We construct our estimator using the native pseudo-random generator in matlab as our hash function. We measure the error of our estimator when varying the hidden constant in our choice of k (recall that our sketching matrix is of size k × n for k = O(n1−2/p)) and varying p. These results are presented in Figure 1. We then compared the update time of our sparse estimator with the estimators described in (Li & Woodruff, 2016a) and (Kong & Valiant, 2016) that are based on a dense Gaussian distribution. The result is shown in Figure 2. Our estimators are comparably or a little less accurate than theirs but are two orders of\nmagnitude faster in terms of update time. In Figure 2, we used our ZD sketch from Definition 3.7. Since each update only updates a single entry to the matrix, the update time is almost 0. On the other hand, the dense Gaussian sketch requires at least Θ(n2−4/p) operations."
  }, {
    "heading": "Acknowledgment",
    "text": "This material is based upon work supported in part by the National Science Foundation under Grants No. 1447639, 1650041, 1652257 and CCF-1815840, the ONR Award N00014-18-1-2364, the Israel Science Foundation grant #897/13 and by a Minerva Foundation grant."
  }],
  "year": 2018,
  "references": [{
    "title": "Eigenvalues of a matrix in the streaming model",
    "authors": ["Andoni", "Alexandr", "Nguyên", "Huy"],
    "venue": "In 24th Annual ACMSIAM Symposium on Discrete Algorithms,",
    "year": 2013
  }, {
    "title": "Sketching and embedding are equivalent for norms",
    "authors": ["Andoni", "Alexandr", "Krauthgamer", "Robert", "Razenshteyn", "Ilya"],
    "venue": "Annual ACM Symposium on Theory of Computing,",
    "year": 2015
  }, {
    "title": "Special Functions",
    "authors": ["Andrews", "George E", "Askey", "Richard", "Roy", "Ranjan"],
    "year": 1999
  }, {
    "title": "An information statistics approach to data stream and communication complexity",
    "authors": ["Bar-Yossef", "Ziv", "Jayram", "Thathachar S", "Kumar", "Ravi", "D. Sivakumar"],
    "venue": "In 43rd Annual IEEE Symposium on Foundations of Computer Science,",
    "year": 2002
  }, {
    "title": "Matrix norms in data streams: Faster, multi-pass and roworder",
    "authors": ["Braverman", "Vladimir", "Chestnut", "Stephen R", "Krauthgamer", "Robert", "Li", "Yi", "Woodruff", "David P", "Yang", "Lin F"],
    "venue": "[cs.DS],",
    "year": 2016
  }, {
    "title": "A sparse johnson: Lindenstrauss transform",
    "authors": ["Dasgupta", "Anirban", "Kumar", "Ravi", "Sarlós", "Tamás"],
    "venue": "In Proceedings of the forty-second ACM symposium on Theory of computing,",
    "year": 2010
  }, {
    "title": "Efficient estimation of eigenvalue counts in an interval",
    "authors": ["Di Napoli", "Edoardo", "Polizzi", "Eric", "Saad", "Yousef"],
    "venue": "Numerical Linear Algebra with Applications,",
    "year": 2016
  }, {
    "title": "Weighted random sampling with a reservoir",
    "authors": ["Efraimidis", "Pavlos S", "Spirakis", "Paul G"],
    "venue": "Information Processing Letters,",
    "year": 2006
  }, {
    "title": "Approximate nearest neighbors: towards removing the curse of dimensionality",
    "authors": ["Indyk", "Piotr", "Motwani", "Rajeev"],
    "venue": "In 30th Annual ACM Symposium on Theory of Computing,",
    "year": 1998
  }, {
    "title": "Optimal bounds for Johnson-Lindenstrauss transforms and streaming problems with subconstant error",
    "authors": ["Jayram", "Thathachar S", "Woodruff", "David P"],
    "venue": "ACM Transactions on Algorithms,",
    "year": 2013
  }, {
    "title": "Almost optimal explicit Johnson-Lindenstrauss families. In Approximation, Randomization, and Combinatorial Optimization",
    "authors": ["Kane", "Daniel", "Meka", "Raghu", "Nelson", "Jelani"],
    "venue": "Algorithms and Techniques,",
    "year": 2011
  }, {
    "title": "Sparser JohnsonLindenstrauss transforms",
    "authors": ["Kane", "Daniel M", "Nelson", "Jelani"],
    "venue": "Journal of the ACM,",
    "year": 2014
  }, {
    "title": "How to fake multiply by a gaussian matrix",
    "authors": ["Kapralov", "Michael", "Potluru", "Vamsi", "Woodruff", "David"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Matrix norm estimation from a few entries",
    "authors": ["Khetan", "Ashish", "Oh", "Sewoong"],
    "year": 2017
  }, {
    "title": "Sketching cuts in graphs and hypergraphs",
    "authors": ["Kogan", "Dmitry", "Krauthgamer", "Robert"],
    "venue": "In Conference on Innovations in Theoretical Computer Science,",
    "year": 2015
  }, {
    "title": "Spectrum estimation from samples",
    "authors": ["Kong", "Weihao", "Valiant", "Gregory"],
    "venue": "arXiv preprint arXiv:1602.00061,",
    "year": 2016
  }, {
    "title": "Powers of tensors and fast matrix multiplication",
    "authors": ["Le Gall", "François"],
    "venue": "In 39th International Symposium on Symbolic and Algebraic Computation,",
    "year": 2014
  }, {
    "title": "On approximating functions of the singular values in a stream",
    "authors": ["Li", "Yi", "Woodruff", "David P"],
    "venue": "In 48th Annual ACM Symposium on Theory of Computing,",
    "year": 2016
  }, {
    "title": "Embeddings of Schatten Norms with Applications to Data Streams",
    "authors": ["Li", "Yi", "Woodruff", "David P"],
    "venue": "In 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017),",
    "year": 2017
  }, {
    "title": "On sketching matrix norms and the top singular vector",
    "authors": ["Li", "Yi", "Nguyen", "Huy L", "Woodruff", "David P"],
    "venue": "In 25th Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2014
  }, {
    "title": "Simple and deterministic matrix sketching",
    "authors": ["Liberty", "Edo"],
    "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 581–588",
    "year": 2013
  }, {
    "title": "Hanson-Wright inequality and sub-gaussian concentration",
    "authors": ["Rudelson", "Mark", "Vershynin", "Roman"],
    "venue": "Electron. Commun. Probab,",
    "year": 2013
  }, {
    "title": "Tabulation based 4universal hashing with applications to second moment estimation",
    "authors": ["Thorup", "Mikkel", "Zhang", "Yin"],
    "venue": "In SODA,",
    "year": 2004
  }, {
    "title": "The streaming complexity of cycle counting, sorting by reversals, and other problems",
    "authors": ["Verbin", "Elad", "Yu", "Wei"],
    "venue": "In Proceedings of the 22nd ACM-SIAM SODA, pp",
    "year": 2011
  }, {
    "title": "Matrix sketching over sliding windows",
    "authors": ["Wei", "Zhewei", "Liu", "Xuancheng", "Li", "Feifei", "Shang", "Shuo", "Du", "Xiaoyong", "Wen", "Ji-Rong"],
    "venue": "In Proceedings of the 2016 International Conference on Management of Data,",
    "year": 2016
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["Woodruff", "David P"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Distributed estimation of generalized matrix rank: Efficient algorithms and lower bounds",
    "authors": ["Zhang", "Yuchen", "Wainwright", "Martin", "Jordan", "Michael"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }],
  "id": "SP:5e03ac37491313ebdadfe4adbbd32d2ed8ef5855",
  "authors": [{
    "name": "Vladimir Braverman",
    "affiliations": []
  }, {
    "name": "Stephen Chestnut",
    "affiliations": []
  }, {
    "name": "Robert Krauthgamer",
    "affiliations": []
  }, {
    "name": "Yi Li",
    "affiliations": []
  }, {
    "name": "David Woodruff",
    "affiliations": []
  }, {
    "name": "Lin Yang",
    "affiliations": []
  }],
  "abstractText": "Given the prevalence of large scale linear algebra problems in machine learning, recently there has been considerable effort in characterizing which functions can be approximated efficiently of a matrix in the data stream model. We study a number of aspects of estimating matrix norms – an important class of matrix functions – in a stream that have not previously been considered: (1) multi-pass algorithms, (2) algorithms that see the underlying matrix one row at a time, and (3) time-efficient algorithms. Our multi-pass and row-order algorithms use less memory than what is provably required in the single-pass and entrywise-update models, and thus give separations between these models (in terms of memory). Moreover, all of our algorithms are considerably faster than previous ones. We also prove a number of lower bounds, and obtain for instance, a near-complete characterization of the memory required of row-order algorithms for estimating Schatten p-norms of sparse matrices. We complement our results with numerical experiments.",
  "title": "Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order"
}