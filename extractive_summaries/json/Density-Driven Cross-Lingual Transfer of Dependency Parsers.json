{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 328–338, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "In recent years there has been a great deal of interest in dependency parsing models for natural languages. Supervised learning methods have been shown to produce highly accurate dependencyparsing models; unfortunately, these methods rely on human-annotated data, which is expensive to obtain, leading to a significant barrier to the development of dependency parsers for new languages. Recent work has considered unsupervised methods (e.g. (Klein and Manning, 2004; Headden III et al., 2009; Gillenwater et al., 2011; Mareček and Straka, 2013; Spitkovsky et al., 2013; Le and Zuidema, 2015; Grave and Elhadad, 2015)), or methods that transfer linguistic structures across languages (e.g. (Cohen et al., 2011; McDonald et al., 2011; Ma and Xia, 2014; Tiedemann, 2015;\n∗Currently on leave at Google Inc. New York.\nGuo et al., 2015; Zhang and Barzilay, 2015; Xiao and Guo, 2015)), in an effort to reduce or eliminate the need for annotated training examples. Unfortunately the accuracy of these methods generally lags quite substantially behind the performance of fully supervised approaches.\nThis paper describes novel methods for the transfer of syntactic information between languages. As in previous work (Hwa et al., 2005; Ganchev et al., 2009; McDonald et al., 2011; Ma and Xia, 2014), our goal is to induce a dependency parser in a target language of interest without any direct supervision (i.e., a treebank) in the target language: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source languages. We can then use alignments induced using tools such as GIZA++ (Och and Ney, 2000), to transfer dependencies from the source language(s) to the target language (example projections are shown in Figure 1). A target language parser is then trained on the projected dependencies.\nOur contributions are as follows:\n• We demonstrate the utility of dense projected structures when training the target-language parser. In the most extreme case, a “dense” structure is a sentence in the target language where the projected dependencies form a fully projective tree that includes all words in the sentence (we will refer to these structures as “full” trees). In more relaxed definitions, we might include sentences where at least some proportion (e.g., 80%) of the words participate as a modifier in some dependency, or where long sequences (e.g., 7 words or more) of words all participate as modifiers in some dependency. We give empirical evidence that dense structures give particularly high accuracy for their projected dependencies.\n328\n• We describe a training algorithm that builds on the definitions of dense structures. The algorithm initially trains the model on full trees, then iteratively introduces increasingly relaxed definitions of density. The algorithm makes use of a training method that can leverage partial (incomplete) dependency structures, and also makes use of confidence scores from a perceptron-trained model.\nIn spite of the simplicity of our approach, our experiments demonstrate significant improvements in accuracy over previous work. In experiments on transfer from a single source language (English) to a single target language (German, French, Spanish, Italian, Portuguese, and Swedish), our average dependency accuracy is 78.89%. When using multiple source languages, average accuracy is improved to 82.18%. This is a 5.51% absolute improvement over the previous best results reported on this data set, 76.67% for the approach of (Ma and Xia, 2014). To give another perspective, our accuracy is close to that of the fully supervised approach of (McDonald et al., 2005), which gives 84.29% accuracy on this data. To the best of our knowledge these are the highest accuracy parsing results for an approach that makes no use of treebank data for the language of interest."
  }, {
    "heading": "2 Related Work",
    "text": "A number of researchers have considered the problem of projecting linguistic annotations from the source to the target language in a parallel corpus (Yarowsky et al., 2001; Hwa et al., 2005;\nGanchev et al., 2009; Spreyer and Kuhn, 2009; McDonald et al., 2011; Ma and Xia, 2014). The projected annotations are then used to train a model in the target language. This prior work involves various innovations such as the use of posterior regularization (Ganchev et al., 2009), the use of entropy regularization and parallel guidance (Ma and Xia, 2014), the use of a simple method to transfer delexicalized parsers across languages (McDonald et al., 2011), and a method for training on partial annotations that are projected from source to target language (Spreyer and Kuhn, 2009). There is also recent work on treebank translation via a machine translation system (Tiedemann et al., 2014; Tiedemann, 2015). The work of (McDonald et al., 2011) and (Ma and Xia, 2014) is most relevant to our own work, for two reasons: first, these papers consider dependency parsing, and as in our work use the latest version of the Google universal treebank for evaluation;1 second, these papers represent the state of the art in accuracy. The results in (Ma and Xia, 2014) dominate the accuracies for all other papers discussed in this related work section: they report an average accuracy of 76.67% on the languages German, Italian, Spanish, French, Swedish and Portuguese; this evaluation includes all sentence lengths.\nOther work on unsupervised parsing has considered various methods that transfer information from source to target languages, where parsers are available in the source languages, but without the use of parallel corpora (Cohen et al., 2011; Dur-\n1The original paper of (McDonald et al., 2011) does not use the Google universal treebank, however (Ma and Xia, 2014) reimplemented the model and report results on the Google universal treebank.\nrett et al., 2012; Naseem et al., 2012; Täckström et al., 2013; Duong et al., 2015; Zhang and Barzilay, 2015). These results are somewhat below the performance of (Ma and Xia, 2014).2"
  }, {
    "heading": "3 Our Approach",
    "text": "This section describes our approach, giving definitions of parallel data and of dense projected structures; describing preliminary exploratory experiments on transfer from German to English; describing the iterative training algorithm used in our work; and finally describing a generalization of the method to transfer from multiple languages."
  }, {
    "heading": "3.1 Parallel Data Definitions",
    "text": "We assume that we have parallel data in two languages. The source language, for which we have a supervised parser, is assumed to be English. The target language, for which our goal is to learn a parser, will be referred to as the “foreign” language. We describe the generalization to more than two languages in §3.5.\nWe use the following notation. Our parallel data is a set of examples (e(k), f (k)) for k = 1 . . . n, where each e(k) is an English sentence, and each f (k) is a foreign sentence. Each e(k) = e (k) 1 . . . e (k) sk where e (k) i is a word, and sk is the length of k’th source sentence. Similarly, f (k) = f\n(k) 1 . . . f (k) tk where f (k)j is a word, and tk is the length of k’th foreign sentence.\nA dependency is a four-tuple (l, k, h,m) where l ∈ {e, f} is the language, k is the sentence number, h is the head index, m is the modifier index. Note that if l = e then we have 0 ≤ h ≤ sk and 1 ≤ m ≤ sk, conversely if l = f then 0 ≤ h ≤ tk and 1 ≤ m ≤ tk. We use h = 0 when h is the root of the sentence.\nFor any k ∈ {1 . . . n}, j ∈ {0 . . . tk}, Ak,j is an integer specifying which word in e(k)1 . . . e (k) sk , word f (k)j is aligned to. It is NULL if f (k) j is not aligned to anything. We have Ak,0 = 0 for all k: that is, the root in one language is always aligned to the root in the other language.\nIn our experiments we use intersected alignments from GIZA++ (Och and Ney, 2000) to provide the Ak,j values.\n2With one exception: on Spanish, using the CoNLL definition of dependencies. The good results from (Ma and Xia, 2014) on the universal dependencies for Spanish may show that the result on the CONLL data is an anomaly, perhaps due to the annotation scheme in Spanish being different from other languages."
  }, {
    "heading": "3.2 Projected Dependencies",
    "text": "We now describe various sets of projected dependencies. We use D to denote the set of all dependencies in the source language: these dependencies are the result of parsing the English side of the translation data using a supervised parser. Each dependency (l, k, h,m) ∈ D is a four-tuple as described above, with l = e. We will use P to denote the set of all projected dependencies from the source to target language. The set P is constructed from D and the alignment variables Ak,j as follows:\nP = {(l, k, h,m) : l = f ∧ (e, k, Ak,h, Ak,m) ∈ D}\nWe say the k’th sentence receives a full parse under the dependencies P if the dependencies (f, k, h,m) for k form a projective tree over the entire sentence: that is, each word has exactly one head, the root symbol is the head of the entire structure, and the resulting structure is a projective tree. We use T100 ⊆ {1 . . . n} to denote the set of all sentences that receive a full parse under P . We then define the following set,\nP100 = {(l, k, h,m) ∈ P : k ∈ T100} We say the k’th sentence receives a dense parse under the dependencies P if the dependencies of the form (f, k, h,m) for k form a projective tree over at least 80% of the words in the sentence. We use T80 ⊆ {1 . . . n} to denote the set of all sentences that receive a dense parse under P . We then define the following set,\nP80 = {(l, k, h,m) ∈ P : k ∈ T80} We say the k’th sentence receives a span-s parse where s is an integer if there is a sequence of at least s consecutive words in the target language that are all seen as a modifier in the set P . We use Ss to refer to the set of all sentences with a span-s parse. We define the sets\nP≥7 = {(l, k, h,m) ∈ P : k ∈ S7} P≥5 = {(l, k, h,m) ∈ P : k ∈ S5} P≥1 = {(l, k, h,m) ∈ P : k ∈ S1}\nFinally, we also create datasets that only include projected dependencies that are consistent with respect to part-of-speech (POS) tags for the head and\nmodifier words in source and target data. We assume a function POS(k, j, i) which returns TRUE if the POS tags for words f (k)j and e (k) i are consistent. The definition of POS-consistent projected dependencies is then as follows:\nP̄ = {(l, k, h,m) ∈ P : POS(k, h,Ak,h) ∧ POS(k,m,Ak,m)}\nWe experiment with two definitions for the POS function. The first imposes a hard constraint, that the POS tags in the two languages must be identical. The second imposes a soft constraint, that the two POS tags must fall into the same equivalance class: the equivalence classes used are listed in §4.1.\nGiven this definition of P̄ , we can create sets P̄100, P̄80, P̄≥7, P̄≥5, and P̄≥1, using analogous definitions to those given above."
  }, {
    "heading": "3.3 Preliminary Experiments with Transfer from English to German",
    "text": "Throughout the experiments in this paper, we used German as the target language for development of our approach. Table 1 shows some preliminary results on transferring dependencies from English to German. We can estimate the accuracy of dependency subsets such as P100, P80, P≥7 and so on by comparing these dependencies to the dependencies from a supervised German parser on the same data. That is, we use a supervised parser to provide gold standard annotations. The full set of dependencies P give 74.0% accuracy under this measure; results for P100 are considerably higher in accuracy, ranging from 83.0% to 90.1% depending on how POS constraints are used.\nAs a second evaluation method, we can test the accuracy of a model trained on the P100 data. The benefit of the soft-matching POS definition is clear. The hard match definition harms performance, presumably because it reduces the number of sentences used to train the model.\nThroughout the rest of this paper, we use the soft POS constraints in all projection algorithms.3"
  }, {
    "heading": "3.4 The Training Procedure",
    "text": "We now describe the training procedure used in our experiments. We use a perceptron-trained shift-reduce parser, similar to that of (Zhang and Nivre, 2011). We assume that the parser is able\n3The hard constraint is also used by Ma and Xia (2014).\nto operate in a “constrained” mode, where it returns the highest scoring parse that is consistent with a given subset of dependencies. This can be achieved via zero-cost dynamic oracles (Goldberg and Nivre, 2013).\nWe assume the following definitions:\n• TRAIN(D) is a function that takes a set of dependency structures D as input, and returns a model θ as its output. The dependency structures are assumed to be full trees: that is, they correspond to fully projected trees with the root symbol as their root.\n• CDECODE(P, θ) is a function that takes a set of partial dependency structures P , and a model θ as input, and as output returns a set of full trees D. It achieves this by constrained decoding of the sentences inP under the model θ, where for each sentence we use beam search to search for the highest scoring projective full tree that is consistent with the dependencies in P . • TOP(D, θ) takes as input a set of full trees D, and a model θ. It returns the top m highest scoring trees in D (in our experiments we usedm = 200, 000), where the score for each tree is the perceptron-based score normalized by the sentence length. Thus we return the\n200,000 trees that the perceptron is most confident on.4\nFigure 2 shows the learning algorithm. It generates a sequence of parsing models, θ1 . . . θ4. In the first stage of learning, the model is initialized by training on P100. The method then uses this model to fill in the missing dependencies on P80 ∪ P≥7 using the CDECODE method; this data is added to P100 and the model is retrained. The method is iterated, at each point adding in additional partial structures (note that P≥7 ⊆ P≥5 ⊆ P≥1, hence at each stage we expand the set of training data that is parsed using CDECODE)."
  }, {
    "heading": "3.5 Generalization to Multiple Languages",
    "text": "We now consider the generalization to learning from multiple languages. We again assume that the task is to learn a parser in a single target language, for example German. We assume that we now have multiple source languages. For example, in our experiments with German as the target, we used English, French, Spanish, Portuguese, Swedish, and Italian as source languages. We assume that we have fully supervised parsers for all source languages. We will consider two methods for combining information from the different languages:\nMethod 1: Concatenation In this approach, we form sets P , P100, P80, P≥7 etc. from each of the languages separately, and then concatenate5 the data to give new definitions of P , P100,P80, P≥7 etc. Method 2: Voting In this case, we assume that each target language sentence is aligned to a source language sentence in each of the source languages. This is the case, for example, in the\n4In cases where |D| < m, the entire set D is returned. 5That is, dependency structures projected from different\nlanguages are taken to be entirely separate from each other.\nEuroparl data, where we have translations of the same material into multiple languages. We can then create the set P of projected dependencies using a voting scheme. For any word (k, j) seen in the target language, each source language will identify a headword (this headword may be NULL if there is no alignment giving a dependency). We simply take the most frequent headword chosen by the languages. After creating the set P , we can create subsets such as P100, P80, P≥7 in exactly the same way as before.\nOnce the various projected dependency training sets have been created, we train the dependency parsing model using the algorithm given in §3.4."
  }, {
    "heading": "4 Experiments",
    "text": "We now describe experiments using our approach. We first describe data and tools used in the experiments, and then describe results."
  }, {
    "heading": "4.1 Data and Tools",
    "text": "Data We use the EuroParl data (Koehn, 2005) as our parallel data and the Google universal treebank (v2; standard data) (McDonald et al., 2013) as our evaluation data, and as our training data for the supervised source-language parsers. We use seven languages that are present in both Europarl and the Google universal treebank: English (used only as the source language), and German, Spanish, French, Italian, Portuguese and Swedish.\nWord Alignments We use Giza++6 (Och and Ney, 2000) to induce word alignments. Sentences with length greater than 100 and single-word sentences are removed from the parallel data. We follow common practice in training Giza++ for both translation directions, and taking the intersection of the two sets as our final alignment. Giza++ de-\n6http://www.statmt.org/moses/giza/ GIZA++.html\nen→trgt concat→trgt voting→trgt\nfault alignment model is used in all of our experiments.\nThe Parsing Model For all parsing experiments we use the Yara parser7 (Rasooli and Tetreault, 2015), a reimplementation of the k-beam arc-eager parser of Zhang and Nivre (2011). We use a beam size of 64, and Brown clustering features8 (Brown et al., 1992; Liang, 2005). The parser gives performance close to the state of the art: for example on section 23 of the Penn WSJ treebank (Marcus et al., 1993), it achieves 93.32% accuracy, compared to 92.9% accuracy for the parser of (Zhang and Nivre, 2011).\nPOS Consistency As mentioned in §3.2, we define a soft POS consistency constraint to prune some projected dependencies. A source/target language word pair satisifies this constraint if one of the following conditions hold: 1) the POS tags for the two words are identical; 2) the word forms for the two words are identical (this occurs frequently for numbers, for example); 3) both tags are in one of the following equivalence classes: {ADV ↔ ADJ} {ADV ↔ PRT} {ADJ ↔ PRON} {DET ↔ NUM} {DET ↔ PRON} {DET ↔ NOUN} {PRON↔NOUN} {NUM↔X} {X↔ .}. These rules were developed primarily on German, with some additional validation on Spanish. These rules required a small amount of human engineering, but we view this as relatively negligible.\nParameter Tuning We used German as a target language in the development of our approach, and in setting hyper-parameters. The parser is\n7https://github.com/yahoo/YaraParser 8https://github.com/percyliang/\nbrown-cluster\ntrained using the averaged structured perceptron algorithm (Collins, 2002) with max-violation updates (Huang et al., 2012). The number of iterations over the training data is 5 when training model θ1 in any setting, and 2, 1 and 4 when training models θ2, θ3, θ4 respectively. These values are chosen by observing the performance on German. We use θ4 as the final output from the training process: this is found to be optimal in English to German projections."
  }, {
    "heading": "4.2 Results",
    "text": "This section gives results of our approach for the single source, multi-source (concatenation) and multi-source (voting) methods. Following previous work (Ma and Xia, 2014) we use goldstandard part-of-speech (POS) tags on test data. We also provide results with automatic POS tags.\nResults with a Single Source Language The first set of results are with a single source language; we use English as the source in all of these experiments. Table 2 shows the accuracy of parameters θ1 . . . θ4 for transfer into German, Spanish, French, Italian, Portuguese, and Swedish. Even the lowest performing model, θ1, which is trained only on full trees, has a performance of 75.88%, close to the 76.15% accuracy for the method of (Ma and Xia, 2014). There are clear gains as we move from θ1 to θ4, on all languages. The average accuracy for θ4 is 78.89%.\nResults with Multiple Source Languages, using Concatenation Table 2 shows results using multiple source languages, using the concatenation method. In these experiments for a given target language we use all other languages in our\ndata as source languages. The performance of θ1 improves from an average of 75.88% for a single source language, to 79.76% for multiple languages. The performance of θ4 gives an additional improvement to 81.23%.\nResults with Multiple Source Languages, using Voting The final set of results in Table 2 are for multiple languages using the voting strategy. There are further improvements: model θ1 has average accuracy of 80.95%, and model θ4 has average accuracy of 82.18%.\nResults with Automatic POS Tags We use our final θ4 models to parse the treebank with automatic tags provided by the same POS tagger used for tagging the parallel data. Table 3 shows the results for the transfer methods and the supervised parsing models of (McDonald et al., 2011) and (Rasooli and Tetreault, 2015). The first-order supervised method of (McDonald et al., 2005) gives only a 1.7% average absolute improvement in ac-\ncuracy over the voting method. For one language (Swedish), our method actually gives improved accuracy over the 1st order parser.\nComparison to Previous Results Table 4 gives a comparison of the accuracy on the six languages, using the single source and multiple source methods, to previous work. As shown in the table, our model outperforms all models: among them, the results of (McDonald et al., 2011) and (Ma and Xia, 2014) are directly comparable to us because they use the same training and evaluation data. The recent work of (Xiao and Guo, 2015) uses the same parallel data but evaluates on CoNLL treebanks but their results are lower than Ma and Xia (2014). The recent work of (Guo et al., 2015) evaluates on the same data as ours but uses different parallel corpora. They only reported on three languages (German: 60.35, Spanish: 71.90 and French: 72.93) which are all far bellow our results. The work of (Grave and Elhadad, 2015) is the state-of-the-art fully unsupervised model with\nminimal linguistic prior knowledge. The model of (Zhang and Barzilay, 2015) does not use any parallel data but uses linguistic information across languages. Their semi-supervised model selectively samples 50 annotated sentences but our model outperforms their model.\nCompared to the results of (McDonald et al., 2011) and (Ma and Xia, 2014) which are directly comparable, there are clear improvements across all languages; the highest accuracy, 82.18%, is a 5.51% absolute improvement over the average accuracy for (Ma and Xia, 2014)."
  }, {
    "heading": "5 Analysis",
    "text": "We conclude with some analysis of the accuracy of the projected dependencies for the different languages, for different definitions (P100, P80 etc.), and for different projection methods. Table 5 gives a summary of statistics for the various languages. Recall that German is used as the development language in our experiments; the other languages can be considered to be test languages. In all cases the accuracy reported is the percentage match to a supervised parser used to parse the same data.\nThere are some clear trends. The accuracy of the P100 datasets is high, with an average accuracy of 84.7% for the single source method, 88.3% for the concatenation method, and 89.0% for the voting method. The voting method not only increases accuracy over the single source method, but also increases the number of sentences (from an average 17k to 77k) and the average number of dependencies per sentence (from 6.8 to 10.4).\nThe accuracy of the P80 ∪ P≥7 datasets is slightly lower, with around 83-87% accuracy for the single source, concatenation and voting methods. The voting method gives a significant increase in the number of sentences—from an av-\nerage of 140k to 243k. The average sentence length for this data is around 28 words, considerably longer than the P100 data; the addition of longer sentences is very likely beneficial to the model. For the voting method the average number of dependencies is 13.7, giving an average density of 50% on these sentences.\nThe accuracy for the different languages, in particular for the voting data, is surprisingly uniform, with a range of 85.8-91.4% for the P100 data, and 81.3-87.4% for the P80 ∪ P≥7 data. The number of sentences for each language, the average length of those sentences, and average number of dependencies per sentence is also quite uniform, with the exception of German, which is a clear outlier. German has fewer sentences, and fewer dependencies per sentence: this may account for it having the lowest accuracy for our models. Future work should investigate why this is the case: one hypothesis is that German has quite different word order from the other languages (it is V2, and verb final), which may lead to a degradation in the quality of the alignments from GIZA++, or in the projection process.\nFinally, figure 3 shows some randomly selected examples from the P100 data for Spanish, giving a qualitative feel for the data obtained using the voting method."
  }, {
    "heading": "6 Conclusions",
    "text": "We have described a density-driven method for the induction of dependency parsers using parallel data and source-language parsers. The key ideas are a series of increasingly relaxed definitions of density, together with an iterative training procedure that makes use of these definitions. The method gives a significant gain over previous methods, with dependency accuracies approach-\ning the level of fully supervised methods. Future work should consider application of the method to a broader set of languages, and application of the method to transfer of information other than dependency structures."
  }, {
    "heading": "Acknowledgement",
    "text": "We thank Avner May and anonymous reviewers for their useful comments. Mohammad Sadegh Rasooli was supported by a grant from Bloomberg’s Knowledge Engineering team."
  }],
  "year": 2015,
  "references": [{
    "title": "Class-based n-gram models of natural language",
    "authors": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."],
    "venue": "Computational linguistics, 18(4):467–479.",
    "year": 1992
  }, {
    "title": "Unsupervised structure prediction with nonparallel multilingual guidance",
    "authors": ["Shay B. Cohen", "Dipanjan Das", "Noah A. Smith."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, Edinburgh,",
    "year": 2011
  }, {
    "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
    "authors": ["Michael Collins."],
    "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Associ-",
    "year": 2002
  }, {
    "title": "Cross-lingual transfer for unsupervised dependency parsing without parallel data",
    "authors": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."],
    "venue": "In",
    "year": 2015
  }, {
    "title": "Syntactic transfer using a bilingual lexicon",
    "authors": ["Greg Durrett", "Adam Pauls", "Dan Klein."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11,",
    "year": 2012
  }, {
    "title": "Dependency grammar induction via bitext projection constraints",
    "authors": ["Kuzman Ganchev", "Jennifer Gillenwater", "Ben Taskar."],
    "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
    "year": 2009
  }, {
    "title": "Posterior sparsity in unsupervised dependency parsing",
    "authors": ["Jennifer Gillenwater", "Kuzman Ganchev", "João Graça", "Fernando Pereira", "Ben Taskar."],
    "venue": "The Journal of Machine Learning Research, 12:455– 490.",
    "year": 2011
  }, {
    "title": "Training deterministic parsers with non-deterministic oracles",
    "authors": ["Yoav Goldberg", "Joakim Nivre."],
    "venue": "TACL, 1:403–414.",
    "year": 2013
  }, {
    "title": "A convex and feature-rich discriminative approach to dependency grammar induction",
    "authors": ["Edouard Grave", "Noémie Elhadad."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
    "year": 2015
  }, {
    "title": "Cross-lingual dependency parsing based on distributed representations",
    "authors": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."],
    "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
    "year": 2015
  }, {
    "title": "Improving unsupervised dependency parsing with richer contexts and smoothing",
    "authors": ["William P. Headden III", "Mark Johnson", "David McClosky."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American",
    "year": 2009
  }, {
    "title": "Structured perceptron with inexact search",
    "authors": ["Liang Huang", "Suphan Fayong", "Yang Guo."],
    "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
    "year": 2012
  }, {
    "title": "Bootstrapping parsers via syntactic projection across parallel texts",
    "authors": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."],
    "venue": "Natural language engineering, 11(03):311–325.",
    "year": 2005
  }, {
    "title": "Corpus-based induction of syntactic structure: Models of dependency and constituency",
    "authors": ["Dan Klein", "Christopher D. Manning."],
    "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg,",
    "year": 2004
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "MT summit, volume 5, pages 79–86.",
    "year": 2005
  }, {
    "title": "Unsupervised dependency parsing: Let’s use supervised parsers",
    "authors": ["Phong Le", "Willem Zuidema."],
    "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
    "year": 2015
  }, {
    "title": "Semi-supervised learning for natural language",
    "authors": ["Percy Liang."],
    "venue": "Ph.D. thesis, Massachusetts Institute of Technology.",
    "year": 2005
  }, {
    "title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization",
    "authors": ["Xuezhe Ma", "Fei Xia."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1:",
    "year": 2014
  }, {
    "title": "Building a large annotated corpus of English: The Penn treebank",
    "authors": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."],
    "venue": "Computational linguistics, 19(2):313–330.",
    "year": 1993
  }, {
    "title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing",
    "authors": ["David Mareček", "Milan Straka."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
    "year": 2013
  }, {
    "title": "Non-projective dependency parsing using spanning tree algorithms",
    "authors": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Hajič."],
    "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-",
    "year": 2005
  }, {
    "title": "Multi-source transfer of delexicalized dependency parsers",
    "authors": ["Ryan McDonald", "Slav Petrov", "Keith Hall."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, Edinburgh, Scotland, UK., July.",
    "year": 2011
  }, {
    "title": "Selective sharing for multilingual dependency parsing",
    "authors": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629–637. Asso-",
    "year": 2012
  }, {
    "title": "Giza++: Training of statistical translation models",
    "authors": ["Franz Josef Och", "Hermann Ney"],
    "year": 2000
  }, {
    "title": "Yara parser: A fast and accurate dependency parser",
    "authors": ["Mohammad Sadegh Rasooli", "Joel Tetreault."],
    "venue": "arXiv preprint arXiv:1503.06733.",
    "year": 2015
  }, {
    "title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction",
    "authors": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-",
    "year": 2013
  }, {
    "title": "Data-driven dependency parsing of new languages using incomplete and noisy training data",
    "authors": ["Kathrin Spreyer", "Jonas Kuhn."],
    "venue": "Proceedings of 337",
    "year": 2009
  }, {
    "title": "Target language adaptation of discriminative transfer parsers",
    "authors": ["Oscar Täckström", "Ryan McDonald", "Joakim Nivre."],
    "venue": "Transactions for ACL.",
    "year": 2013
  }, {
    "title": "Treebank translation for cross-lingual parser induction",
    "authors": ["Jörg Tiedemann", "Željko Agić", "Joakim Nivre."],
    "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 130–140, Ann Arbor, Michigan, June. Asso-",
    "year": 2014
  }, {
    "title": "Improving the cross-lingual projection of syntactic dependencies",
    "authors": ["Jörg Tiedemann."],
    "venue": "Nordic Conference of Computational Linguistics NODALIDA 2015, pages 191–199.",
    "year": 2015
  }, {
    "title": "Annotation projection-based representation learning for crosslingual dependency parsing",
    "authors": ["Min Xiao", "Yuhong Guo."],
    "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 73–82, Beijing, China,",
    "year": 2015
  }, {
    "title": "Inducing multilingual text analysis tools via robust projection across aligned corpora",
    "authors": ["David Yarowsky", "Grace Ngai", "Richard Wicentowski."],
    "venue": "Proceedings of the First International Conference on Human Language Technology Research,",
    "year": 2001
  }, {
    "title": "Hierarchical low-rank tensors for multilingual transfer parsing",
    "authors": ["Yuan Zhang", "Regina Barzilay."],
    "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal, September.",
    "year": 2015
  }, {
    "title": "Transition-based dependency parsing with rich non-local features",
    "authors": ["Yue Zhang", "Joakim Nivre."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Ore-",
    "year": 2011
  }],
  "id": "SP:32d3e513cbf33f43e91a7145055409091ac31fad",
  "authors": [{
    "name": "Mohammad Sadegh Rasooli",
    "affiliations": []
  }, {
    "name": "Michael Collins",
    "affiliations": []
  }],
  "abstractText": "We present a novel method for the crosslingual transfer of dependency parsers. Our goal is to induce a dependency parser in a target language of interest without any direct supervision: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source language(s). Our key contributions are to show the utility of dense projected structures when training the target language parser, and to introduce a novel learning algorithm that makes use of dense structures. Results on several languages show an absolute improvement of 5.51% in average dependency accuracy over the state-of-the-art method of (Ma and Xia, 2014). Our average dependency accuracy of 82.18% compares favourably to the accuracy of fully supervised methods.",
  "title": "Density-Driven Cross-Lingual Transfer of Dependency Parsers"
}