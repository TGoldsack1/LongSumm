{
  "sections": [{
    "text": "√ T .\nIt is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings. In a COLT 2017 open problem (Agarwal et al., 2017), Agarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit problem. In the present paper, we resolve this open problem by presenting a new strategy based on augmenting the policy space.1"
  }, {
    "heading": "1 Introduction",
    "text": "The contextual bandit problem is an influential extension of the classical multi-armed bandit. It can be described as follows. Let K be the number of actions, E a set of experts (or “policies”), T the time horizon, and denote ∆K = {x ∈ [0, 1]K : ∑K i=1 x(i) = 1}. At each time step t = 1, . . . , T ,\n• The player receives from each expert e ∈ E an “advice” ξet ∈ ∆K .\n• Using advices and previous feedbacks, the player selects a probability distribution pt ∈ ∆K .\n*Equal contribution 1Microsoft Research AI 2Princeton University. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Sébastien Bubeck <sebubeck@microsoft.com>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n1The full version of this paper can be found at https:// arxiv.org/abs/1802.03386. The work was done when Yuanzhi Li was a summer intern at Microsoft Research in 2017.\n• The adversary selects a loss function `t : [K]→ [0, 1]. • The player plays an action at ∈ [K] at random from pt\n(and independently of the past).\n• The player’s suffered loss is `t(at) ∈ [0, 1], which is also the only feedback the player receives about the loss function `t.\nThe player’s performance at the end of the T rounds is measured through the regret with respect to the best expert:\nRT def = max\ne∈E { E [ T∑ t=1 `t(at)− 〈ξet , `t〉 ]}\n= max e∈E { E [ T∑ t=1 〈pt − ξet , `t〉 ]} . (1.1)\nA landmark result by Auer et al. (2002) is that a regret of order O( √ TK log(|E|)) is achievable in this setting. The general intuition captured by regret bounds is that the player’s performance is equal to the best expert’s performance up to a term of lower order. However the aforementioned bound might fail to capture this intuition if T L∗T def = mine∈E E ∑T t=1〈ξet , `t〉. It is thus natural to ask whether one could obtain a stronger guarantee where T is essentially replaced by L∗T . This question was posed as a COLT 2017 open problem (Agarwal et al., 2017). Such bounds are called first-order regret bounds, and they are known to be possible with full information (Auer et al., 2002), as well as in the multi-armed bandit setting (Allenberg et al., 2006) (see also (Foster et al., 2016) for a different proof) and the semi-bandit framework (Neu, 2015; Lykouris et al., 2017). Our main contribution is a new algorithm for contextual bandit, which we call MYGA (see Section 2), and for which we prove the following first-order regret bound, thus resolving the open problem. Theorem 1.1. For any loss sequence such that mine∈E E ∑T t=1〈ξet , `t〉 ≤ L∗ one has that MYGA\nwith γ = Θ(η) and η = Θ ( min { 1 K , √ log(|E|+T ) KL∗ }) satisfies\nRT ≤ O (√ K log(|E|+ T )L∗ +K log(|E|+ T ) ) ."
  }, {
    "heading": "2 Algorithm Description",
    "text": "In this section we describe the MYGA algorithm."
  }, {
    "heading": "2.1 Truncation",
    "text": "We introduce a truncation operator T ks that takes as input an index k ∈ [K] and a threshold s ∈ [0, 12 ]. Then, treating the first k arms as “majority arms” and the last K − k arms as “minority arms,” T ks redistributes “multiplicatively” the probability mass of all minority arms below threshold s to the majority arms.\nDefinition 2.1. For k ∈ [K] and s ∈ (0, 12 ], the truncation operator T ks : ∆K → ∆K is defined as follows. Given any q ∈ ∆K , then we set T ks q(i) = 0, i > k and q(i) ≤ s; q(i), i > k and q(i) > s; q(i) · ( 1 + ∑ j:j>k∧ q(j)≤s q(j)∑\nj≤k q(j)\n) , i ≤ k.\nEquivalently one can define T ks q(i) for the majority arms i ≤ k with the following implicit formula:\nT ks q(i) = q(i)∑ j≤k q(j) ∑ j≤k T ks q(j) . (2.1)\nTo see this it suffices to note that the amount of mass in the majority arms is given by∑ j≤k T ks q(j) = 1− ∑ j>k T ks q(j) = 1− ∑ j:j>k∧ q(j)>s q(j)\n= ∑ j≤k q(j) + ∑ j:j>k∧ q(j)≤s q(j) .\nIf K = 2, then T 1s q simply adds q(2) into q(1) if q(2) ≤ s. For an example with K = 11, see Figure 1."
  }, {
    "heading": "2.2 Informal description",
    "text": "MYGA is parameterized by two parameters: a classical learning rate η > 0, and a thresholding parameter γ ∈ 12T N = { 12T , 2 2T , 3 2T , . . . }. Also let S = (γ, 1/2] ∩ 1 2T N = (γ, 1/2] ∩ { 12T , 2 2T , 3 2T , . . . } At a high level, a key feature of MYGA is to introduce a set of auxiliary experts, one for each s ∈ S. More precisely, in each round t, after receiving expert advices {ξet }e∈E ,\nMYGA calculates a distribution ξst ∈ ∆K for each s ∈ S. Then, MYGA uses the standard exponential weight updates on E′ = E ∪ S with learning rate η > 0, to calculate a weight functionwt ∈ RE∪S+ —see (2.3). Then, it computes • ζt ∈ ∆K , the weighted average of expert advices in E:\nζt = 1∑\ne∈E wt(e) ∑ e∈E wt(e) · ξet .\n• qt ∈ ∆K , the weighted average of expert advices in E′:\nqt = 1\n‖wt‖1 ∑ e∈E′ wt(e) · ξet .\nUsing these information, MYGA calculates the probability distribution pt ∈ ∆K from which the arm is played at round t.\nLet us now explain how pt and ξst , s ∈ S are defined. First we remark that in the contextual bandit setting, the arm index has no real meaning since in each round t we can permute the arms by some πt : [K] → [K] and permute the expert’s advices and the loss vector by the same πt. For this reason, throughout this paper, we shall assume\n∀t ∈ [T ] : ζt(1) ≥ ζt(2) ≥ · · · ζt(K) . Let us define the “pivot” index kt = min{i ∈ [K] :∑ j≤i ζt(j) ≥ 1/2}. Then, in order to perform truncation, MYGA views the first kt arms as “majority arms” and the last K − kt arms as “minority arms” of the current round t. At a high level we will have:\n• the distribution to play from is pt = T ktγ qt. • each auxiliary expert s ∈ S is defined by ξst = T kts qt.\nWe now give a more precise description in Algorithm 1."
  }, {
    "heading": "3 Preliminaries",
    "text": "Definition 3.1. For analysis purpose, let us define the truncated loss ¯̀t(i) def = `t(i)1{pt(i) > 0}, so that\nEat [ 〈˜̀t, pt〉] = 〈¯̀t, pt〉 = 〈`t, pt〉 .\nWe next derive two lemmas that will prove useful to isolate\nAlgorithm 1 MYGA (Make the minoritY Great Again)\nInput: learning rate η > 0, threshold parameter γ ∈ 12T N 1: S ← (γ, 1/2] ∩ 12T N and w1 ← (1, . . . , 1) ∈ R E∪S\n2: for t = 1 to T do 3: receive advices ξet ∈ ∆K from each expert e ∈ E 4: weighted average ζt ← ∑ e∈E wt(e)ξ e t∑\ne∈E wt(e) ∈ ∆K\n5: assume ζt(1) ≥ ζt(2) ≥ · · · ζt(K) wlog. by permuting the arms 6: kt ← min{i ∈ [K] : ∑ j≤i ζt(j) ≥ 1/2} the first kt arms are majority arms 7: find qt ∈ ∆K such that qt can be found in time O(K|S|) = O(KT ), see Lemma 6.1\nqt = 1∑ e∈E wt(e)+ ∑ s∈S wt(s) (∑ e∈E wt(e)ξ e t + ∑ s∈S wt(s)T kts qt ) . (2.2)\n8: ξst ← T kts qt for every s ∈ S and pt ← T ktγ qt 9: draw an arm at ∈ [K] from probability distribution pt and receive feedback `t(at)\n10: compute loss estimator ˜̀t ∈ RK+ as ˜̀t(i) = `t(i)pt(i)1i=at 11: update the exponential weights for any e ∈ E ∪ S:\nwt+1(e) = exp ( − η ∑t r=1〈ξer , ˜̀r〉) . (2.3)\n12: end for\nthe properties of the truncation operator T ks that are needed to obtain a first-order regret bound. Lemma 3.2. Let γ ∈ [0, 1] and assume that for all i ∈ [K], (1− cKγ)pt(i) ≤ qt(i) for some universal constant c > 0, and that pt(i) 6= 0⇒ pt(i) ≥ qt(i). Then one has\n(1− cKγ)LT −L∗T ≤ log(|E′|) η + η 2 E T∑ t=1 ‖¯̀t‖22 . (3.1)\nProof. Using 〈pt, `t〉 = 〈pt, ¯̀t〉, 〈−ξet , `t〉 ≤ 〈−ξet , ¯̀t〉, and (1− cKγ)pt(i) ≤ qt(i), we have\n(1− cKγ)LT − L∗T ≤ max e∈E′ E T∑ t=1 〈(1− cKγ)pt − ξet , ¯̀t〉\n≤ max e∈E′ E T∑ t=1 〈qt − ξet , ¯̀t〉 .\nThe rest of the proof follows from standard argument to bound the regret of Exp4, see e.g., (Bubeck & CesaBianchi, 2012, Theorem 4.2) (with the minor modification that the assumption on pt implies that ˜̀t(i) ≤ `t(i)qt(i)1{i = at}).\nThe next lemma is straightforward. Lemma 3.3. In addition to the assumptions in Lemma 3.2, assume that there exists some numerical constants c′, c′′ ≥ 0 such that\nγ E T∑ t=1 ‖¯̀t‖22 ≤ 2 c′ (η + γ) K LT + 2 c′′ log(|E′|) η .\n(3.2)\nThen one has( 1− cKγ − ( η + η2\nγ\n) c′K) ) (LT − L∗T )\n≤ ( 1\nη + c′′ γ\n) log(|E′|) + ( cKγ + ( η + η2\nγ\n) c′K ) L∗T .\nWe now see that it suffices to show that MYGA satisfies the assumptions of Lemma 3.2 and Lemma 3.3 for γ ' η, and η ' min { 1 K , √ log(|E′|) KL∗T } (assume that L∗T is\nknown), in which case one obtains a bound of order√ K log(|E′|)L∗T +K log(|E′|).\nIn fact the assumption of Lemma 3.2 will be easily verified, and the real difficulty will be to prove (3.2). We observe that the standard trick of thresholding the arms with probability below γ would yield (3.2) with the right hand side replaced by LT , and in turn this leads to a regret of order (L∗T ) 2/3. Our goal is to improve over this naive argument.\n4 Proof of the 2-Armed Case The goal of this section is to explain how our MYGA algorithm arises naturally. To focus on the main ideas we restrict to the case K = 2. The complete formal proof of Theorem 1.1 is given in Section 5.\nRecall we have assumed without loss of generality that ζt(1) ≥ ζt(2) for each round t ∈ [T ]. This implies kt = 1 because ζt(1) ≥ 12 . In this simple case, for s ∈ [0, 1/2], we abbreviate our truncation operator T kts as Ts, and it acts as\nfollows. Given q ∈ ∆2 if q(2) ≤ s we have Tsq = (1, 0); and if q(2) > s we have Tsq = q.\nIn particular, we have qt(1) ≥ qt(2) and pt(1) ≥ pt(2) for all t ∈ [T ]. We refer to arm 1 as the majority arm and arm 2 as the minority arm. We denote M = E ∑T t=1 ¯̀ t(1) as\nthe loss of the majority arm and m = E ∑T t=1 ¯̀ t(2) as the loss of the minority arm. Since `t ∈ [0, 1]K and K = 2, we have E ∑T t=1 ‖¯̀t‖22 ≤ E ∑T t=1 ¯̀ t(1) + ¯̀t(2) = M +m . (4.1) Observe also that one always has LT ≥ 12M (indeed pt(1) ≥ qt(1) ≥ 1/2), and thus the whole game to prove (3.2) is to upper bound the minority’s loss m."
  }, {
    "heading": "4.1 When the minority suffers small loss",
    "text": "Assume that m ≤ (c′ − 1)M for some constant c′ > 0. Then, because M ≤ 2LT , one can directly obtain (3.2) from (4.1) with c′′ = 0. In words, when the minority arm has a total loss comparable to the majority arm, simply playing from ζt would satisfy a first-order regret bound.\nOur main idea is to somehow enforce this relation m .M between the minority and majority losses, by “truncating” probabilities appropriately. Indeed, recall that if after some truncation we have pt(2) = 0, then it satisfies ¯̀t(2) = 0 so the minority loss m can be improved."
  }, {
    "heading": "4.2 Make the minority great again",
    "text": "Our key new insight is captured by the following lemma which is proved using an integral averaging argument.\nDefinition 4.1. For each s ≥ γ, let Lst def = E ∑T t=1〈Tsqt, `t〉 be the expected loss if the truncated strategy Tsqt ∈ ∆K is played at each round. Lemma 4.2. As long as m−M > 0,\n∃s ∈ (γ, 1/2] : m−M ≤ LT − L s T\nγ .\nIn words, ifm is large, then smust be a much better threshold compared to γ, that is LT − LsT is large.\nProof of Lemma 4.2. For any s ≥ γ, define the function f(s) def = E ∑T t=1 1{qt(2) ≤ s}(¯̀t(1)− ¯̀t(2)) . Let us pick s ∈ [γ, 1/2] to minimize f(s), and breaking ties by choosing the smaller value of s. We make several observations:\n• f(γ) ≥ 0 because for any t with qt(2) ≤ γ we must have ¯̀t(2) = 0.\n• f(1/2) = M −m < 0. • s > γ because f(s) ≤ f(1/2) < 0.\nLet us define the points s0 def = γ and\n{s1 < . . . < sm} def = (γ, s] ∩ {q1(2), . . . , qT (2)}.\nNote that the tie-breaking rule for the choice of s ensures sm = s (if sm < s then it must satisfy f(sm) = f(s) giving a contradiction). Using the identity T∑ t=1 〈Tsqt − qt, ¯̀t〉 = 1{qt(2) ≤ s}qt(2)(¯̀t(1)− ¯̀t(2)) , (4.2) we calculate that\nLT − LsT\n= E T∑ t=1 〈Tγqt − Tsqt, `t〉 = E T∑ t=1 〈Tγqt − Tsqt, ¯̀t〉\n= E T∑ t=1 (1{qt(2) ≤ γ} − 1{qt(2) ≤ s})\n× qt(2)(¯̀t(1)− ¯̀t(2))\n= E T∑ t=1 m∑ i=1 −si1{qt(2) = si}(¯̀t(1)− ¯̀t(2))\n= m∑ i=1 si(f(si−1)− f(si))\n= m−1∑ i=1 (si+1 − si)f(si) + s1f(s0)− smf(sm) .\nSince f(s0) ≥ 0, f(si) ≥ f(s) and s = sm, we conclude that\nLT − LsT ≥ (sm − s1)f(sm)− smf(sm) = −s1f(sm) ≥ γ(m−M) .\nGiven Lemma 4.2, a very intuitive strategy start to emerge. Suppose we can somehow get an upper bound of the form\nLT − LsT ≤ O ( log(|E′|) η + η(m+M) + γLT ) . (4.3) Then, putting this into Lemma 4.2 and using M ≤ 2LT , we have for any γ ≥ 2η,\nγm ≤ O ( log(|E′|) η + γLT ) .\nIn words, the minority arm also suffers from a small loss (and thus is great again!) Putting this into (4.1), we immediately get (3.2) as desired and finish the proof of Theorem 1.1 in the case K = 2.\nThus, we are left with showing (4.3). The main idea is to add the truncated strategy Tsqt as an additional auxiliary expert. If we can achieve this, then (4.3) can be obtained from the regret formula in Lemma 3.2."
  }, {
    "heading": "4.3 Expanding the set of experts",
    "text": "Assume for a moment that we somehow expand the set of experts into E′ ⊃ E so that: ∀s ∈ (γ, 1/2],∃e ∈ E′ such that for all t ∈ [T ], ξet = Tsqt . (4.4) Then clearly (4.3) would be satisfied using Lemma 3.2, (4.1) and L∗T ≤ LsT (the loss of an expert should be no\nbetter than the loss of the best expert L∗T ).\nThere are two issues with condition (4.4): first, it selfreferential, in the sense that it assumes {ξet }e∈E′ satisfies a certain form depending on qt while qt is defined via {ξet }e∈E′ (recall (2.2)); and second, it potentially requires to have an infinite number of experts (one for each s ∈ (γ, 1/2]). Let us first deal with the second issue via discretization.\nLemma 4.3. In the same setting as Lemma 4.2, there exists s ∈ S def= (γ, 1/2] ∩ 12T N such that\nm−M ≤ 1 + LT − L s T\nγ .\nProof. For x ∈ R let x be the smallest element in [x,+∞) ∩ 12T N. For any s ∈ S we can rewrite (4.2) as (note that x ≤ s⇔ x ≤ s) 〈Tsqt− qt, ¯̀t〉 = 1{qt(2) ≤ s}qt(2)(¯̀t(1)− ¯̀t(2)) + εt,s , where |εt,s| ≤ 1/2T . Using the same proof of Lemma 4.2, and redefining\nf(s) def = E ∑T t=1 1{qt(2) ≤ s}(¯̀t(1)− ¯̀t(2)) .\nwe get that there exists s1, . . . , sm ∈ S def = (γ, 12 ] ∩ 1 2T N and ε ∈ [−1, 1] such that\nLT − LsT = ε+ m∑ i=1 si(f(si−1)− f(si)) .\nThe rest of the proof now follows from the same proof of Lemma 4.2, except that we minimize f(s) over s ∈ S instead of s ∈ [γ, 12 ].\nThus, instead of (4.4), we only need to require\n∀s ∈ S, ∃e ∈ E′ such that for all t ∈ [T ], ξet = Tsqt . (4.5)\nWe now resolve the self-referentiality of (4.5) by defining simultaneously qt and ξet , e ∈ S as follows. Consider the map Ft : [0, 1/2]→ [0, 1/2] defined by:\nFt(x) = 1∑ e∈E wt(e) + ∑ s∈S wt(s)\n× (∑ e∈E wt(e)ξ e t (2) + ∑ s∈S wt(s)x1{x > s} ) .\nIt suffices to find a fixed point x = Ft(x): indeed, setting qt def = (1− x, x) and\nξst (2) def = x1{x > s} = Tsqt for s ∈ S, we have both (4.5) holds and qt = 1‖wt‖1 ∑ e∈E′ wt(e) · ξet is the correct weighted average of expert advices in E′ = E ∪ S Finally, Ft has a fixed point since it is a nondecreasing function from a closed interval to itself. It is also not hard to find such a point algorithmically.\nThis concludes the (slightly informal) proof forK = 2. We give the complete proof for arbitrary K in the next section."
  }, {
    "heading": "5 Proof of Theorem 1.1",
    "text": "In this section, we assume qt ∈ ∆K satisfies (2.2) and we defer the constructive proof of finding qt to Section 6. Recall the arm index has no real meaning so without loss of generality we have permuted the arms so that\nζt(1) ≥ ζt(2) ≤ . . . ≥ ζt(K) for each t = 1, 2, . . . , T . We refer to {1, 2, . . . , kt} the set of majority arms and {kt+1, . . . ,K} the set of minority arms at round t.2 We let M def = ∑T t=1 E ∑ i≤kt ¯̀ t(i) and m def = ∑T t=1 E ∑ i>kt ¯̀ t(i) respectively be the total loss of the majority and minority arms. We again have\nE ∑T t=1 ‖¯̀t‖22 ≤ E ∑T t=1 ∑ i∈[K] ¯̀ t(i) = M +m . (5.1)\nThus, the whole game to prove (3.2) is to upper bound M and m."
  }, {
    "heading": "5.1 Useful properties",
    "text": "We state a few properties about qt and its truncations.\nLemma 5.1. In each round t = 1, 2, . . . , T , if qt satisfies (2.2), then for every s ∈ S and i ≤ kt:\nξst (i) = ζt(i)∑ j≤k ζt(j)\n· ( 1− ∑ j>k ξst (j) )\nProof. Let i ≤ kt and s ∈ S. By (2.1) and since ξst = T kts qt one has\nξst (i) = qt(i)∑ j≤k qt(j) ∑ j≤k ξst (j) .\nMoreover qt is a mixture of ζt and truncated versions of ζt so similarly using (2.1) one has\nqt(i) = ζt(i)∑ j≤k ζt(j) ∑ j≤k qt(j) .\nPutting the two above displays together concludes the proof.\nLemma 5.2. In each round t = 1, 2, . . . , T , if qt satisfies (2.2), then\n• for every i > kt it satisfies qt(i) ≤ ζt(i), and • for every i ≤ kt it satisfies qt(i) ≥ ζt(i) ≥ 12K .\nProof. For sake of notation we drop the index t in this proof. Recall q = ∑ e∈E∪S w(e) ‖w‖1 · ξ e.\n• For every minority arm i > k, every s ∈ S, we have ξs(i) = ( T ks q ) (i) ≤ q(i) according to Definition 2.1.\n2We stress that in the K-arm setting, although kt is the minimum index such that ζt(1) + · · ·+ ζt(kt) ≥ 12 , it may not be the minimum index so that qt(1) + · · ·+ qt(kt) ≥ 12 .\nTherefore, we must have q(i) = ∑ e∈E∪S w(e) ‖w‖1 ·\nξe(i) ≤ ∑ e∈E w(e)ξ\ne(i)∑ e∈E w(e) = ζ(i).\n• For every majority arm i ≤ k, we have (using Lemma 5.1)\nξe(i) = ζ(i)∑ j≤k ζ(j) · (1− ∑ j>k ξs(j))\n≥ ζ(i)∑ j≤k ζ(j) · (1− ∑ j>k ζ(j)) = ζ(i)\nFrom the definition of k = min{i ∈ [K] : ∑ j≤i ζ(j) ≥ 1 2}, we can also conclude ζ(i) ≥ ζ(k) ≥ 12K . This is because 1 2 ≤ ∑ j>k ζ(j) ≤ Kζ(k).\nThe next lemma shows that setting pt = T ktγ qt satisfies the assumption of Lemma 3.2.\nLemma 5.3. If qt satisfies (2.2), γ ∈ (0, 12 ] and pt = T ktγ qt, then for every arm i ∈ [K]: (1−2Kγ)pt(i) ≤ qt(i) and pt(i) 6= 0⇒ pt(i) ≥ qt(i) .\nProof. For sake of notation we drop the index t in this proof.\nBy Definition 2.1 and Lemma 5.2, we have for every i ∈ [K]:\np(i) ≤ q(i) ( 1 + ∑ j:j>k∧ q(j)≤γ q(j)∑\nj≤k q(j) ) ≤ q(i) ( 1 +\n∑ j:q(j)≤γ q(j)∑ j≤k ζ(j) ) ≤ q(i)(1 + 2Kγ) .\nThe other statement follows because whenever p(i) 6= 0, Definition 2.1 says it must satisfy p(i) ≥ q(i)."
  }, {
    "heading": "5.2 Bounding m and M",
    "text": "We first upper bound M and then upper bound m.\nLemma 5.4. If qt satisfies (2.2), then M ≤ 2KLT .\nProof. Using Lemma 5.2 we have qt(i) ≥ 12K for any i ≤ kt. Also, pt(i) ≥ qt(i) for every i satisfying ¯̀t(i) > 0 (owing to Definition 3.1 and Lemma 5.3). Therefore,\nM = T∑ t=1 E ∑ i≤kt ¯̀ t(i) ≤ 2K T∑ t=1 E ∑ i≤kt qt(i) · ¯̀t(i)\n≤ 2K T∑ t=1 E ∑ i≤kt pt(i) · ¯̀t(i) ≤ 2K T∑ t=1 E〈pt, ¯̀t〉\n= 2K T∑ t=1 E〈pt, `t〉 = 2KLT .\nLemma 5.5. Suppose qt satisfies (2.2), and denote by Lst def = E ∑T t=1〈T kts qt, `t〉 = E ∑T t=1〈ξst , `t〉 the total\nexpected loss of qt truncated to s. Then, as long as m− 2KLT > 0,\n∃s ∈ (γ, 1/2] ∩ 1 2T N : m− 2KLT ≤ 1 + LT − LsT γ .\nProof. The proof is a careful generalization of the proof of Lemma 4.3 (which in turn is just a discretization of the proof of Lemma 4.2). Recall the notation x for the smallest element in [x,+∞) ∩ 12T N, and observe that for s ∈ 1 2T N, x ≤ s⇔ x ≤ s. Denote by\n`majt def = ∑ i≤kt qt(i)∑ j≤kt qt(j) ¯̀ t(i) .\nthe weighted loss of the majority arms at round t. We have∑T t=1 ` maj t ≤ 2LT because ∑ j≤kt qt(j) ≥ ∑ j≤kt ζt(j) ≥ 1 2 and qt(i) ≤ pt(i) whenever ¯̀t(i) > 0 (owing to Definition 3.1 and Lemma 5.3).\nNow, for any s ≥ γ, define the function\nf(s) def = E ∑T t=1 ∑ i>kt 1{qt(i) ≤ s}(`majt − ¯̀t(i)) .\nLet us pick s ∈ [γ, 1/2] ∩ 12T N to minimize f(s), and breaking ties by choosing the smaller value of s. We make several observations:\n• f(γ) ≥ 0 because for any t and i > kt with qt(i) ≤ γ we must have pt(i) = (T ktγ qt)(i) = 0 and thus ¯̀t(i) = 0 by the definition of ¯̀t in Definition 3.1.\n• f(1/2) = ∑T t=1(K − kt)` maj t −m ≤ 2KLT −m < 0.\n• s > γ because f(s) ≤ f(1/2) < 0.\nLet us define the points s0 def = γ and\n{s1 < . . . < sm} def = (γ, s] ∩ ⋃ i∈[K] {q1(i), . . . , qT (i)}.\nNote that the tie-breaking rule for the choice of s ensures sm = s (if sm < s then it must satisfy f(sm) = f(s) giving a contradiction).\nObserve that by definition of the truncation operator, one has\n〈T kts qt − qt, ¯̀t〉 = ∑ i>kt 1{qt(i) ≤ s}qt(i)(`majt − ¯̀t(i))\nIn fact, after rounding, one can rewrite the above for some εs,t ∈ [− 12T , 1 2T ] as\n〈T kts qt−qt, ¯̀t〉 = εs,t+ ∑ i>kt 1{qt(i) ≤ s}qt(i)(`majt −¯̀t(i))\nThen, for some ε ∈ [−1, 1], one has\nLT − LsT = E T∑ t=1 〈T ktγ qt − T kts qt, `t〉\n= E T∑ t=1 〈T ktγ qt − T kts qt, ¯̀t〉\n= ε+ E T∑ t=1 ∑ i>kt (1{qt(i) ≤ γ} − 1{qt(i) ≤ s})qt(i)(`majt − ¯̀t(i))\n= ε+ E m∑ j=1 T∑ t=1 ∑ i>kt −sj1{qt(i) = sj}(`majt − ¯̀t(i))\n= ε+ m∑ j=1 sj(f(sj−1)− f(sj))\n= ε+ m−1∑ j=1 (sj+1 − sj)f(sj) + s1f(s0)− smf(sm) .\nSince f(s0) = f(γ) ≥ 0, f(si) ≥ f(s) and s = sm, we conclude that\nLT − LsT ≥ ε+ (sm − s1)f(sm)− smf(sm) = ε− s1f(sm) ≥ γ(m− 2KLT ) ."
  }, {
    "heading": "5.3 Putting all together",
    "text": "Finally, using Lemma 3.2 (which applies thanks to Lemma 5.3), (5.1) and L∗T ≤ LsT (the loss of an expert is no better than the loss of the best expert L∗T ), we have\nLT − LsT ≤ O ( log(|E′|) η + η(m+M) + γKLT ) . (5.2) Putting this into Lemma 5.5 and then using M ≤ 2KLT from Lemma 5.4, we have for any γ ≥ 2η,\nγ(m+M) ≤ O ( log(|E′|) η + γKLT ) .\nPutting this into (5.1), we immediately get (3.2) as desired. This finishes the proof of Theorem 1.1. It only remains to ensure that qt verifying (2.2) indeed exists. We provide an algorithm for this in Section 6."
  }, {
    "heading": "6 Algorithmic Process to Find qt",
    "text": "In this section, we answer the question of how to algorithmically find qt satisfying the implicitly definition (2.2). We recall (2.2):\nqt = 1∑ e∈E wt(e) + ∑ s∈S wt(s)\n× (∑\ne∈E wt(e)ξ e t + ∑ s∈S wt(s)T kts qt ) . (2.2)\nWe show the following general lemma: Lemma 6.1. Given k ∈ [K], a finite subset S ⊂ [ 0, 12 ] , ζ ∈ ∆K with ζ(1) ≥ · · · ≥ ζ(K), and W ∈ ∆1+|S|, Algorithm 2 finds some q ∈ ∆K such that\nq = W (1)ζ + ∑ s∈SW (s)T ks q .\nFurthermore, Algorithm 2 runs in time O(K · |S|).\nWe observe that by setting k = kt, ζ = ζt = ∑ e∈E wt(e)·ξ e t∑\ne∈E wt(e) , W (1) =\n∑ e∈E wt(e)\n‖wt‖1\nand ∀s ∈ S : W (s) = wt(s)‖wt‖1 in Lemma 6.1, we immediately obtain a vector q ∈ ∆K that we can use as qt.\nIntuition for Lemma 6.1. We only search for q that is monotonically non-increasing for minority arms. This implies T ks q is also non-increasing for minority arms. In symbols: q(k + 1) ≥ · · · ≥ q(K) and\n(T ks q)(k + 1) ≥ · · · ≥ (T ks q)(K) . Due to such monotonicity, when computing T ks q for each s ∈ S, there must exist some index πs ∈ {k + 1, k + 2, . . . ,K + 1} such that the entry q(i) gets zeroed out for all i ≥ πs or in symbols, (T ks q)(i) = 0 for all i ≥ πs. Now, the main idea of Algorithm 2 is to search for such non-increasing function π : S → [K+1]. It initializes itself with πs = k + 1 for all s ∈ S, and then tries to increase π coordinate by coordinate.\nFor each choice of π, Algorithm 2 computes a candidate distribution qπ ∈ ∆K which satisfies\nqπ = W (1)ζ + ∑ s∈S W (s)us (6.1)\nwhere each us is qπ but truncated so that its probabilities after πs are redistributed to the first k arms, or in symbols,\nus(i) =  0, i ≥ πs; qπ(i), πs > i > k; qπ(i) · ( 1 + ∑ j:j≥πs qπ(j)∑ j≤k qπ(j) ) , i ≤ k.\nOne can verify that the distribution qπ ∈ ∆K defined in Line 3 of Algorithm 2 is an explicit solution to (6.1). Unfortunately, each us may not satisfy T ks qπ = us. In particular, there may exist\nsome s ∈ S and i > k such that qπ(i) > s but us(i) = 0. This means, we may have truncated too much for expert s in defining us, and we must increase πs.\nPerhaps not very surprisingly, if each iteration we only increase one πs by exactly 1, then we never overshoot and there exists a moment when q = qπ exactly satisfies\nq = W (1)ζ + ∑ s∈SW (s)T ks q .\nWe now give a formal proof of Lemma 6.1."
  }, {
    "heading": "6.1 Proof details",
    "text": "Claim 6.2. We claim some properties about Algorithm 2 (a) The process finishes after at most K · |S| iterations. (b) We always have qπ(k + 1) ≥ · · · ≥ qπ(K). (c) As π changes, for each minority arm i > k, qπ(i) never\ndecreases.\n(d) When the while loop ends, for each i > k and s ∈ S, we have qπ(i) > s⇐⇒ πs > i.\nThe proof of Claim 6.2 can be found in the full version.\nProof of Lemma 6.1. Suppose in the end of Algorithm 2 we obtain q = qπ for some π : S → [K+1]. Let ξs = T ks q\nAlgorithm 2 Input: k ∈ [K], a finite set S ⊆ [ 0, 12 ] , ζ ∈ ∆K with ζ(1) ≥ · · · ≥ ζ(K), and W ∈ ∆1+|S|\nOutput: q ∈ ∆K such that q = W (1)ζ + ∑ s∈SW (s)T ks q.\n1: initialize π : S → [K + 1] as πs = k + 1; will ensure πs ∈ {k + 1, k + 2, . . . ,K + 1} 2: while true do\n3: qπ(i)←  W (1) 1− ∑ s∈S∧πs>iW (s)\n· ζ(i), if i > k; ζ(i)∑ j≤k ζ(j) · (1− ∑ j>k qπ(j)), if i ≤ k.\nqπ ∈ ∆K\n4: Pick any s ∈ S with πs ≤ K such that qπ(πs) > s. 5: if s is not found then break 6: else πs ← πs + 1. 7: end while 8: return qπ .\nfor each s ∈ S and q′ = W (1)ζ + ∑ s∈SW (s)T ks q. We need to show q = q′. For every minority arm i > k:\nq′(i) ¬ = W (1) · ζ(i) + ∑ s∈S W (s) · ξs(i)\n = W (1) · ζ(i) + ( ∑ s∈S∧q(i)>s W (s) ) · q(i)\n® = W (1) · ζ(i) + ( ∑ s∈S∧πs>i W (s) ) · q(i) ¯= q(i) .\nAbove, equality ¬ is by the definition of q′, equality  is by the definition of ξs = T ks q, equality ® follows from Claim 6.2.d, and equality ¯ is by definition of q(i) = qπ(i) = W (1) 1− ∑ s∈S∧πs>iW (s) · ζ(i). For every majority arm i ≤ k, q′(i)\nζ(i)\n¬ = W (1) · ζ(i)ζ(i) + ∑ s∈SW (s) · ξs(i) ζ(i)\n = W (1) + ∑ s∈SW (s) · ∑ j≤k ξ s(j)∑ j≤k ζ(j)\n(6.2)\nwhere equality ¬ is by the definition of q′ and equality  is because for every i ≤ k it satisfies ξ\ns(i) q(i) =\n∑ j≤k ξ\ns(j)∑ j≤k q(j)\n(using definition of ξs = T ks q) and for every i ≤ k it satisfies ζ(i)q(i) = ∑ j≤k ζ(j)∑ j≤k q(j)\n(using definition of q = qπ Line 3 of Algorithm 2).\nNow, the right hand side of (6.2) is independent of i. Therefore, we can write q′(i) = C1 · ζ(i) for each i ≤ k with some constant C1 > 0. Our definition of q = qπ (see Line 3 of Algorithm 2) ensures that we can also write q(i) = C2 · ζ(i) for each i ≤ k with some constant C2 > 0. Therefore, since for every i > k we have already shown q′(i) = q(i), it must satisfy C1 = C2 and therefore q′(i) = q(i) for all i ∈ [K]. After proving q′ = q, we only need to argue about the running time.\nIf Algorithm 2 is implemented naively, then the total running time is O((K · |S|)2) because there are at most K · |S|\niterations (see Claim 6.2.a) and in each iteration we can compute qπ in time O(K · |S|). In fact it is rather easy to find implicit update rules to make each iteration of Algorithm 2 run in O(1) time. We give some hints below.\nIndeed, if in an iteration some πs is changed from i to i+ 1 (recalling i > k), then we can update qπ(i) in O(1) time. For each j > k where j 6= i, we have qπ(j) is unchanged. The values of qπ(j) for j ≤ k all need to be changed, but they are only changed altogether by the same multiplicative factor (which can again be calculated in O(1) time).\nFinally, to search for s ∈ S with πs ≤ K and qπ(πs) > s, we do not need to go through all s ∈ S. Instead, for each i > k, we maintain “the smallest si ∈ S so that qπ(i) > si.” Then, whenever πsi ≤ i, that means we can pick s = si because qπ(πs) = qπ(πsi) ≥ qπ(i) > si = s. For such reason, one can maintain a first-in-first-out list to store all values of i where qπ(i) > si. In each iteration of Algorithm 2 we simply pick the first element in list and perform the update. This changes exactly one qπ(j) for j > k, and thus may additionally insert one element to list. Therefore, in each iteration we only needO(1) time to find some πs to increase."
  }],
  "year": 2018,
  "references": [{
    "title": "Open problem: First-order regret bounds for contextual bandits",
    "authors": ["A. Agarwal", "A. Krishnamurthy", "J. Langford", "H. Luo", "R.E. Schapire"],
    "venue": "Proceedings of the 2017 Conference on Learning Theory,",
    "year": 2017
  }, {
    "title": "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring",
    "authors": ["C. Allenberg", "P. Auer", "L. Györfi", "G. Ottucsák"],
    "venue": "In Proceedings of the 17th International Conference on Algorithmic Learning Theory (ALT),",
    "year": 2006
  }, {
    "title": "The non-stochastic multi-armed bandit problem",
    "authors": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"],
    "venue": "SIAM Journal on Computing,",
    "year": 2002
  }, {
    "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
    "authors": ["S. Bubeck", "N. Cesa-Bianchi"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2012
  }, {
    "title": "Learning in games: Robustness of fast convergence",
    "authors": ["D.J. Foster", "Z. Li", "T. Lykouris", "K. Sridharan", "E. Tardos"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Small-loss bounds for online learning with partial information",
    "authors": ["T. Lykouris", "K. Sridharan", "E. Tardos"],
    "venue": "arXiv preprint arXiv:1711.03639,",
    "year": 2017
  }, {
    "title": "First-order regret bounds for combinatorial semibandits",
    "authors": ["G. Neu"],
    "venue": "In Proceedings of the 2015 Conference on Learning Theory (COLT),",
    "year": 2015
  }],
  "id": "SP:6077662d29c768ce81353b197ac8c31453b8a297",
  "authors": [{
    "name": "Zeyuan Allen-Zhu",
    "affiliations": []
  }, {
    "name": "Sébastien Bubeck",
    "affiliations": []
  }, {
    "name": "Yuanzhi Li",
    "affiliations": []
  }],
  "abstractText": "Regret bounds in online learning compare the player’s performance to L∗, the optimal performance in hindsight with a fixed strategy. Typically such bounds scale with the square root of the time horizon T . The more refined concept of first-order regret bound replaces this with a scaling √ L∗, which may be much smaller than √ T . It is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings. In a COLT 2017 open problem (Agarwal et al., 2017), Agarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit problem. In the present paper, we resolve this open problem by presenting a new strategy based on augmenting the policy space.1",
  "title": "Make the Minority Great Again:  First-Order Regret Bound for Contextual Bandits"
}