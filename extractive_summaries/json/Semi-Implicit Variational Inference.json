{
  "sections": [{
    "text": "We extend the existing framework of semiimplicit variational inference (SIVI) and introduce doubly semi-implicit variational inference (DSIVI), a way to perform variational inference and learning when both the approximate posterior and the prior distribution are semi-implicit. In other words, DSIVI performs inference in models where the prior and the posterior can be expressed as an intractable infinite mixture of some analytic density with a highly flexible implicit mixing distribution. We provide a sandwich bound on the evidence lower bound (ELBO) objective that can be made arbitrarily tight. Unlike discriminator-based and kernel-based approaches to implicit variational inference, DSIVI optimizes a proper lower bound on ELBO that is asymptotically exact. We evaluate DSIVI on a set of problems that benefit from implicit priors. In particular, we show that DSIVI gives rise to a simple modification of VampPrior, the current state-of-theart prior for variational autoencoders, which improves its performance."
  }, {
    "heading": "1 INTRODUCTION",
    "text": "Bayesian inference is an important tool in machine learning. It provides a principled way to reason about uncertainty in parameters or hidden representations. In recent years, there has been great progress in scalable Bayesian methods, which made it possible to perform approximate inference for large-scale datasets and deep learning models.\nProceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by the author(s).\nOne of such methods is variational inference (VI) [3], which is an optimization-based approach. Given a probabilistic model p(x, z) = p(x | z)p(z), where x are observed data and z are latent variables, VI seeks to maximize the evidence lower bound (ELBO).\nL(φ) = Eqφ(z)[log p(x | z)]−KL(qφ(z) ‖ p(z)), (1)\nwhere qφ(z) approximates the intractable true posterior distribution p(z |x). The parametric approximation family for qφ is chosen in such a way, that we can efficiently estimate L(φ) and its gradients w.r.t. φ.\nSuch approximations to the true posterior are often too simplistic. There exists a variety of ways to extend the variational family to mitigate this. They can be divided roughly into two main groups: those that require the probability density function of the approximate posterior to be analytically tractable (which we will call explicit models) [30, 12, 9, 5, 36, 38, 6, 27, 20] and those that do not (implicit models) [11, 22, 37, 17, 21, 31, 40]. For latter, we only assume that it is possible to sample from such distributions, whereas the density may be inaccessible.\nNot only approximate posteriors but also priors in such models are often chosen to be very simple to make computations tractable. This can lead to overregularization and poor hidden representations in generative models such as variational autoencoders (VAE, [15]) [10, 35, 1]. In Bayesian neural networks, a standard normal prior is the default choice, but together with the mean field posterior, it can lead to overpruning and consequently underfitting [39]. To overcome such problem in practice, one usually scales the KL divergence term in the expression for ELBO or truncates the variances of the approximate posterior [24, 19, 18].\nAnother way to overcome this problem is to consider more complicated prior distributions, e.g. implicit priors. For example, hierarchical priors usually impose an implicit marginal prior when hyperparameters are integrated out. To perform inference in such models, one often resorts to joint inference over both param-\neters and hyperparameters, even though we are only interested in the marginal posterior over parameters of the model. Another example of implicit prior distributions is the optimal prior for variational autoencoders. It can be shown that the aggregated posterior distribution is the optimal prior for VAE [10], and it can be regarded as an implicit distribution. The VampPrior model [35] approximates this implicit prior using an explicit discrete mixture of Gaussian posteriors. However, this model can be further improved if we consider an arbitrary trainable semi-implicit prior.\nIn this paper, we extend the recently proposed framework of semi-implicit variational inference (SIVI) [40] and consider priors and posteriors that are defined as semi-implicit distributions. By “semi-implicit” we mean distributions that do not have a tractable PDF (i.e. implicit), but that can be represented as a mixture of some analytically tractable density with a flexible mixing distribution, either explicit or implicit.\nOur contributions can be summarized as follows. Firstly, we prove that the SIVI objective is actually a lower bound on the true ELBO, which allows us to sandwich the ELBO between an upper bound and a lower bound which are both asymptotically exact. Secondly, we propose doubly semi-implicit variational inference (DSIVI), a general-purpose framework for variational inference and variational learning in the case when both the posterior and the prior are semiimplicit. We construct a SIVI-inspired asymptotically exact lower bound on the ELBO for this case, and use the variational representation of the KL divergence to obtain the upper bound. Finally, we consider a wide range of applications where semi-implicit distributions naturally arise, and show how the use of DSIVI in these settings is beneficial."
  }, {
    "heading": "2 PRELIMINARIES",
    "text": "Consider a probabilistic model defined by its joint distribution p(x, z) = p(x | z)p(z), where variables x are observed, and z are the latent variables. Variational inference is a family of methods that approximate the intractable posterior distribution p(z |x) with a tractable parametric distribution qφ(z). To do so, VI methods maximize the evidence lower bound (ELBO):\nlog p(x) ≥ L(φ) = Eqφ(z) log p(x | z)p(z) qφ(z) → max φ .\n(2) The maximum of the evidence lower bound corresponds to the minimum of the KL-divergence KL(qφ(z) ‖ p(z |x)) between the variational distribution qφ(z) and the exact posterior p(z |x). In the more general variational learning setting, the prior distribution may also be a parametric distribution pθ(z) [11].\nIn this case, one would optimize the ELBO w.r.t. both the variational parameters φ and the prior parameters θ, thus performing approximate maximization of the marginal likelihood p(x | θ).\nThe common way to estimate the gradient of this objective is to use the reparameterization trick [15]. The reparameterization trick recasts the sampling from the parametric distribution qφ(z) as the sampling of nonparametric noise ε ∼ p(ε), followed by a deterministic parametric transformation z = f(ε, φ). Still, such gradient estimator requires log-densities of both the prior distribution p(z) and the approximate posterior qφ(z) in closed form. Several methods have been proposed to overcome this limitation [27, 21, 31]. However, such methods usually provide a biased estimate of the evidence lower bound with no practical way of estimating the introduced bias.\nThe reparameterizable distributions with no closedform densities are usually referred to as implicit distributions. In this paper we consider the so-called semiimplicit distributions that are defined as an implicit mixture of explicit conditional distributions:\nqφ(z) = ∫ qφ(z |ψ)qφ(ψ) dψ. (3)\nHere, the conditional distribution qφ(z |ψ) is explicit. However, when its condition ψ follows an implicit distribution qφ(ψ), the resulting marginal distribution qφ(z) is implicit. We will refer to qφ(ψ) as the mixing distribution, and to ψ as the mixing variables.\nNote that we may easily sample from semi-implicit distributions: in order to sample z from qφ(z), we need to first sample the mixing variable ψ ∼ qφ(ψ), and then sample z from the conditional qφ(z |ψ). Further in the text, we will assume this sampling scheme when using expectations Ez∼qφ(z) over semi-implicit distributions. Also note that an arbitrary implicit distribution can be represented in a semi-implicit form: qφ(z) = ∫ δ(z − z′)qφ(z′) dz′."
  }, {
    "heading": "3 RELATED WORK",
    "text": "There are several approaches to inference and learning in models with implicit distributions.\nOne approach is commonly referred to as hierarchical variational inference or auxiliary variable models. It allows for inference with implicit approximate posteriors qφ(z) that can be represented as a marginal distribution of an explicit joint distribution qφ(z) =∫ qφ(z, ψ) dψ. The ELBO is then bounded from below using a reverse variational model rω(ψ | z) ≈ qφ(ψ | z) [27, 29, 19]. This method does not allow for implicit prior distributions, requires access to the explicit joint\ndensity qφ(z, ψ) and has no way to estimate the increased inference gap, introduced by the imperfect reverse model. However, recently proposed deep weight prior [2] provides a new lower bound, suitable for learning hierarchical priors in a similar fashion.\nAnother family of models uses an optimal discriminator to estimate the ratio of implicit densities r(z) = qφ(z) pθ(z) [21, 22, 11]. This is the most general approach to inference and learning with implicit distributions, but it also optimizes a biased surrogate ELBO, and the induced bias cannot be estimated. Also, different authors report that the performance of this approach is poor if the dimensionality of the implicit densities is high [32, 37]. This is the only approach that allows to perform variational learning (learning the parameters θ of the prior distribution pθ(z)). However, it is nontrivial and requires differentiation through a series of SGD updates. This approach has not been validated in practice yet and has only been proposed as a theoretical concept [11]. On the contrary, DSIVI provides a lower bound that can be directly optimized w.r.t. both the variational parameters φ and the prior parameters θ, naturally enabling variational learning.\nKernel implicit variational inference (KIVI) [31] is another approach that uses kernelized ridge regression to approximate the density ratio. It is reported to be more stable than the discriminator-based approaches, as the proposed density ratio estimator can be computed in closed form. Still, this procedure introduces a bias that is not addressed. Also, KIVI relies on adaptive contrast that does not allow for implicit prior distributions [21, 31].\nThere are also alternative formulations of variational inference that are based on different divergences. One example is operator variational inference [26] that uses the Langevin-Stein operator to design a new variational objective. Although it allows for arbitrary implicit posterior approximations, the prior distribution has to be explicit."
  }, {
    "heading": "4 DOUBLY SEMI-IMPLICIT",
    "text": "VARIATIONAL INFERENCE\nIn this section, we will describe semi-implicit variational inference, study its properties, and then extend it for the case of semi-implicit prior distributions."
  }, {
    "heading": "4.1 Semi-Implicit Variational Inference",
    "text": "Semi-implicit variational inference [40] considers models with an explicit joint distribution p(x, z) and a semi-implicit approximate posterior qφ(z), as defined in Eq. (3). The basic idea of semi-implicit variational\ninference is to approximate the semi-implicit approximate posterior with a finite mixture:\nqφ(z) = ∫ qφ(z |ψ)qφ(ψ) dψ ≈\n≈ 1 K K∑ k=1 qφ(z |ψk), ψk ∼ qφ(ψ). (4)\nSIVI provides an upper bound LqK ≥ L q\nK+1 ≥ L, and a surrogate objective LqK that both converge to ELBO as K goes to infinity (Lq∞ = L q ∞ = L):\nLqK = Eqφ(z) log p(x | z)p(z)− (5)\n− Eψ0..K∼qφ(ψ)Ez∼qφ(z |ψ0) log 1\nK K∑ k=1 qφ(z |ψk),\nLqK = Eqφ(z) log p(x | z)p(z)− (6)\n− Eψ0..K∼qφ(ψ)Ez∼qφ(z |ψ0) log 1\nK + 1 K∑ k=0 qφ(z |ψk).\nThe surrogate objective LqK is then used for optimization."
  }, {
    "heading": "4.2 SIVI Lower Bound",
    "text": "Although it was shown that Lq0 is a lower bound for ELBO, it has not been clear whether this holds for arbitrary K, and whether maximizing LqK leads to a correct procedure. Here, we show that LqK is indeed a lower bound on ELBO L. Theorem 1. Consider L and LqK defined as in Eq. (2) and (6). Then LqK converges to L from below as K → ∞, satisfying LqK ≤ L q K+1 ≤ L, and\nLqK = Eψ0..K∼qφ(ψ)EqKφ (z |ψ0..K) log p(x | z)p(z) qKφ (z |ψ0..K) , (7)\nwhere qKφ (z |ψ0..K) = 1\nK + 1 K∑ k=0 qφ(z |ψk). (8)\nThe proof can be found in Appendix A.\nIt can be seen from Eq. (7) that the surrogate objective LqK proposed by [40] is actually the ELBO for a finite mixture approximation qKφ (z |ψ0, . . . , ψK), that is averaged over all such mixtures (averaged over samples of ψ0, . . . , ψK ∼ qφ(ψ))."
  }, {
    "heading": "4.3 Semi-Implicit Priors",
    "text": "Inspired by the derivation of the SIVI upper bound, we can derive the lower bound LpK for the case of semiimplicit prior distributions. Right now, for simplicity,\nassume an explicit approximate posterior qφ(z), and a semi-implicit prior pθ(z) = ∫ pθ(z|ζ)pθ(ζ) dζ\nLpK = Eqφ(z) log p(x | z)−\n− Eζ1..K∼pθ(ζ)Eqφ(z) log qφ(z)\n1 K ∑K k=1 pθ(z | ζk) , (9)\nLpK ≤ L p K+1 ≤ L p ∞ = L. (10)\nThis bound has the same properties: it is nondecreasing in K and is asymptotically exact. To see why LpK ≤ L, one just needs to apply the Jensen’s inequality for the logarithm:\nEζ1..K∼pθ(ζ)Eqφ(z) log 1\nK K∑ k=1 pθ(z | ζk) ≤\n≤ Eqφ(z) logEζ1..K∼pθ(ζ) 1\nK K∑ k=1 pθ(z | ζk) =\n= Eqφ(z) log pθ(z).\n(11)\nTo show that this bound is non-decreasing in K, one can refer to the proof of proposition 3 in the SIVI paper [40, Appendix A].\nNote that it is no longer possible to use the same trick to obtain the upper bound. Still, we can obtain an upper bound using the variational representation of the KL-divergence [25]:\nKL(qφ(z) ‖ pθ(z)) =\n= 1 + sup g:dom z→R\n{ Eqφ(z)g(z)− Epθ(z)e g(z) } ≥\n≥ 1 + sup η\n{ Eqφ(z)g(z, η)− Epθ(z)e g(z,η) } , (12)\nLpη = Eqφ(z) log p(x | z)−\n− 1− Eqφ(z)g(z, η) + Epθ(z)e g(z,η).\n(13)\nHere we substitute the maximization over all functions with a single parametric function. In order to obtain a tighter bound, we can minimize this bound w.r.t. the parameters η of function g(z, η).\nNote that in order to find the optimal value for η, one does not need to estimate the entropy term or the likelihood term of the objective:\nη∗ = arg min η\n[ −Eqφ(z)g(z, η) + Epθ(z)e g(z,η) ] . (14)\nThis allows us to obtain a lower bound on the KLdivergence between two arbitrary (semi-)implicit distributions, and, consequently, results in an upper bound on the ELBO."
  }, {
    "heading": "4.4 Final Objective",
    "text": "We can combine the bounds for the semi-implicit posterior and the semi-implicit prior to obtain the final lower bound\nLq,p K1,K2 = Eqφ(z) log p(x | z)−\n− Eψ0..K1∼qφ(ψ)Eqφ(z |ψ0) log 1\nK1 + 1 K1∑ k=0 qφ(z |ψk)+\n+ Eζ1..K2∼pθ(ζ)Eqφ(z) log 1\nK2 K2∑ k=1 pθ(z | ζk), (15)\nand the upper bound\nLq,pη =Eqφ(z) log p(x | z)−\n− 1− Eqφ(z)g(z, η) + Epθ(z)e g(z,η).\n(16)\nThe lower bound Lq,p K1,K2\nis non-decreasing in both K1 and K2, and is asymptotically exact:\nLq,p K1,K2 ≤ Lq,p K1+1,K2 , Lq,p K1,K2 ≤ Lq,p K1,K2+1 , (17)\nlim K1,K2→∞ Lq,p K1,K2 = L. (18)\nWe use the lower bound for optimization, whereas the upper bound may be used to estimate the gap between the lower bound and the true ELBO. The final algorithm for DSIVI is presented in Algorithm 1. Unless stated otherwise, we use 1 MC sample to estimate the gradients of the lower bound (see Algorithm 1 for more details). In the case where the prior distribution is explicit, one may resort to the upper bound LqK , proposed in SIVI [40]."
  }, {
    "heading": "5 APPLICATIONS",
    "text": "In this section we describe several settings that can benefit from semi-implicit prior distributions."
  }, {
    "heading": "5.1 VAE with Semi-Implicit Priors",
    "text": "The default choice of the prior distribution p(z) for the VAE model is the standard Gaussian distribution. However, such choice is known to over-regularize the model [35, 8].\nIt can be shown that the so-called aggregated posterior distribution is the optimal prior distribution for a VAE in terms of the value of ELBO [10, 35]:\np∗(z) = 1\nN N∑ n=1 qφ(z |xn), (19)\nwhere the summation is over all training samples xn, n = 1, . . . , N . However, this extreme case leads to\nAlgorithm 1 Doubly semi-implicit VI (and learning) Require: SI posterior qφ(z) = ∫ qφ(z |ψ)qφ(ψ) dψ\nRequire: SI prior pθ(z) = ∫ pθ(z | ζ)pθ(ζ) dζ Require: explicit log-likelihood log p(x | z) Variational inference (find φ) and learning (find θ) for t← 1 to T do\nψ0, . . . , ψK1 ∼ qφ(ψ) . Reparameterization ζ1, . . . , ζK2 ∼ pθ(ζ) . Reparameterization z ∼ qφ(z |ψ0) . Reparameterization Estimate LLH ' log p(x | z) LE ← − log 1K1+1 ∑K1 k=0 qφ(z |ψk)\nLCE ← − log 1K2 ∑K2 k=1 pθ(z | ζk) L̂ q,p\nK1,K2 ← LLH + LE − LCE\nUse ∇φL̂ q,p\nK1,K2 to update φ\nif Variational learning then Use ∇θL̂ q,p\nK1,K2 to update θ\nend if end for Upper bound for t← 1 to T do\nz ∼ qφ(z) . Reparameterization z′ ∼ pθ(z) . Reparameterization L← −g(z, η) + eg(z′,η) Use −∇ηL to update η\nend for Estimate Lq,p\nK1,K2 and Lq,pη using Eq. (15) and (16)\nreturn φ, θ, η,Lq,p K1,K2 ,Lq,pη\noverfitting [10, 35], and is highly computationally inefficient. A possible middle ground is to consider the variational mixture of posteriors prior distribution (the VampPrior) [35]:\npV amp(z) = 1\nK K∑ k=1 qφ(z |uk). (20)\nThe VampPrior is defined as a mixture of K variational posteriors qφ(z |uk) for a set of inducing points {uk}Kk=1. These inducing points may be learnable (an ordinary VampPrior) or fixed at a random subset of the training data (VampPrior-data). The VampPrior battles over-regularization by considering a flexible empirical prior distribution, being a mixture of fullyfactorized Gaussians, and by coupling the parameters of the prior distribution and the variational posteriors.\nThere are two ways to improve this technique by using DSIVI. We can regard the aggregated posterior p∗(z) as a semi-implicit distribution:\np∗(z) = 1\nN N∑ n=1 qφ(z|xn) = ∫ qφ(z|x)pdata(x)dx. (21)\nNext, we can use it as a semi-implicit prior and exploit the lower bound, presented in Section 4.3:\nLpK = 1\nN N∑ n=1 Eqφ(z | xn) [ log p(xn | z) qφ(z |xn) +\n+Eu1..K∼pdata(x) log 1\nK K∑ k=1 qφ(z |uk)\n] .\n(22)\nNote that the only difference from the training objective of VampPrior-data is that the inducing points uk are not fixed, but are resampled at each estimation of the lower bound. As we show in the experiments, such reformulation of VampPrior-data drastically improves its test log-likelihood.\nWe can also consider an arbitrary semi-implicit prior distribution:\npSIθ (z) = ∫ pθ(z | ζ)pθ(ζ) dζ. (23)\nFor example, we consider a fully-factorized Gaussian conditional prior pθ(z | ζ) = N (z | ζ,diag(σ2)) with mean ζ and trainable variances σ2j . The implicit generator pθ(ζ) can be parameterized by an arbitrary neural network with weights θ that transforms a standard Gaussian noise ε to mixing parameters ζ. As we show in the experiments, such semi-implicit posterior outperforms VampPrior even though it does not couple the parameters of the prior and the variational posteriors.\nWe can also apply the importance-weighted lower bound [4] similarly to the importance weighted SIVAE [40], and obtain IW-DSIVAE, a lower bound on the IWAE objective for a variational autoencoder with a semi-implicit prior and a semi-implicit posterior. The exact expression for this lower bound is presented in Appendix B."
  }, {
    "heading": "5.2 Variational Inference with Hierarchical Priors",
    "text": "A lot of probabilistic models use hierarchical prior distributions: instead of a non-parametric prior p(w) they use a parametric conditional prior p(w |α) with hyperparameters α, and a hyperprior over these parameters p(α). A discriminative model with such hierarchical prior may be defined as follows [23, 33, 34, 7, 31, 18]:\np(t, w, α |x) = p(t |x,w)p(w |α)p(α). (24)\nA common way to perform inference in such models is to approximate the joint posterior qφ(w,α) ≈ p(w,α |Xtr, Ttr) given the training data (Xtr, Ttr) [7, 31, 18]. Then the marginal approximate posterior\nqφ(w) = ∫ qφ(w,α) dα is used to approximate the predictive distribution on unseen data p(t |x,Xtr, Ttr):\np(t |x,Xtr, Ttr) = ∫ p(t |x,w)p(w |Xtr, Ttr) dw =\n= ∫ p(t |x,w) ∫ p(w,α |Xtr, Ttr) dα dw ≈\n≈ ∫ p(t |x,w) ∫ qφ(w,α)dα dw =\n= ∫ p(t |x,w)qφ(w) dw.\n(25) The inference is performed by maximization of the following variational lower bound:\nLjoint(φ) = Eqφ(w,α) log p(t |x,w)p(w |α)p(α)\nqφ(w,α) . (26)\nWe actually are not interested in the joint posterior qφ(w,α), and we only need it to obtain the marginal posterior qφ(w). In this case we can reformulate the problem as variational inference with a semi-implicit prior p(w) = ∫ p(w |α)p(α) dα and a semi-implicit\nposterior qφ(w) = ∫ qφ(w |α)qφ(α) dα:\nLmarginal(φ) = Eqφ(w) log p(t |x,w)p(w)\nqφ(w) . (27)\nThen it can be shown that optimization of the second objective results in a better fit of the marginal posterior:\nTheorem 2. Let φj and φm maximize Ljoint and Lmarginal correspondingly. Then\nKL(qφm(w) ‖ p(w |Xtr, Ttr)) ≤ KL( qφj (w) ‖ p(w |Xtr, Ttr)). (28)\nThe proof can be found in Appendix C.\nIt means that if the likelihood function does not depend on the hyperparameters α, it is beneficial to consider the semi-implicit formulation instead of the joint formulation of variational inference even if the approximation family stays exactly the same. In the experiments, we show that the proposed DSIVI procedure matches the performance of direct optimization of Lmarginal, whereas joint VI performs much worse."
  }, {
    "heading": "6 EXPERIMENTS",
    "text": ""
  }, {
    "heading": "6.1 Variational Inference with Hierarchical Priors",
    "text": "We consider a Bayesian neural network with a fullyfactorized hierarchical prior distribution with a Gaussian conditional p(wij |αij) = N (wij | 0, α−1ij ) and a\nGamma hyperprior over the inverse variances p(αij) = Gamma(αij | 0.5, 2). Such hierarchical prior induces a fully-factorized Student’s t-distribution with one degree of freedom as the marginal prior p(wij) = t(wij | ν = 1). Note that in this case, we can estimate the marginal evidence lower bound directly. We consider a fully-connected neural network with two hidden layers of 300 and 100 neurons on the MNIST dataset [16]. We train all methods with the same hyperparameters: we use batch size 200, use Adam optimizer [13] with default parameters, starting with learning rate 10−3, and train for 200 epochs, using linear learning rate decay.\nWe consider three different ways to perform inference in this model, the marginal inference, the joint inference, and DSIVI, as described in Section 5.2. For joint inference, we consider a fully-factorized joint approximate posterior qφ(w,α) = qφ(w)qφ(α), with qφ(w) being a fully-factorized Gaussian, and qφ(α) being a fully-factorized Log-Normal distribution. Such joint approximate posterior induces a fully-factorized Gaussian marginal posterior qφ(w). Therefore, we use a fully-factorized Gaussian posterior for the marginal inference and DSIVI. Note that in this case, only the prior distribution is semi-implicit. All models have been trained with the local reparameterization trick [14].\nWe perform inference using these three different variational objectives, and then compare the true evidence lower bound Lmarginal on the training set. As the marginal variational approximation is the same in all four cases, the training ELBO can act as a proxy metric for the KL-divergence between the marginal approximate posterior and the true marginal posterior. The results are presented in Figure 1. DSIVI with as low as K = 10 samples during training exactly matches the performance of the true marginal variational inference, whereas other approximations fall far behind. All three methods achieve 97.7−98.0% test set accuracy, and the test log-likelihood is approximately the same for all methods, ranging from −830 to −855. However, the difference in the marginal ELBO is high. The final values of the ELBO, its decomposition into train log-likelihood and the KL term, and the test loglikelihood are presented in Table 2 in Appendix C."
  }, {
    "heading": "6.2 Comparison to Alternatives",
    "text": "We compare DSIVI to other methods for implicit VI on a toy problem of approximating a centered standard Student’s t-distribution p(z) with 1 degrees of freedom with a Laplace distribution qφ(z) by representing them as scale mixtures of Gaussians. Namely, we represent p(z) = ∫ N (z | 0, α−1)Gamma(α | 0.5, 2) dα,\nand qφ(z) = ∫ N (z |µ, τ)Exp(τ |λ) dτ . We train all\nmethods by minimizing the corresponding approximations to the KL-divergence KL(qφ(z) ‖ p(z)) w.r.t. the parameters µ and λ of the approximation qφ(z).\nAs baselines, we use prior methods for implicit VI: Adversarial Variational Bayes (AVB) [21], which is a discriminator-based method, and Kernel Implicit Variational Inference (KIVI) [31]. For AVB we fix architecture of the “discriminator” neural network to have 2 hidden layers with 3 and 4 hidden units with LeakyReLU (α = 0.2) activation, and for KIVI we use fixed λ = 0.001 with varying number of samples. For AVB we tried different numbers of training samples and optimization steps to optimize the discriminator at each step of optimizing over φ. We used Adam optimizer with learning rate 10−2 and one MC sample to estimate gradients w.r.t. φ.\nWe report the KL-divergence KL(qφ(z) ‖ p(z)), estimated using 10000 MC samples averaged over 10 runs. The results are presented in Figure 2. DSIVI converges faster, is more stable, and only has one hyperparameter, the number of samples K in the DSIVI objective."
  }, {
    "heading": "6.3 Sequential Approximation",
    "text": "We illustrate the expressiveness of DSIVI with implicit prior and posterior distributions on the following toy problem. Consider an explicit distribution p(z). We would like to learn a semi-implicit distribution qφ1(z) = Eqφ1 (ψ)[qφ1(z |ψ)] to match p(z). During the first step, we apply DSIVI to tune the parameters φ1 so as to minimize KL(qφ1(z) ‖ p(z)). Then, we take the trained semi-implicit qφ1(z) as a new target for z and tune φ2 minimizing KL(qφ2(z) ‖ qφ1(z)). After we repeat the iterative process k times, qφk(z) obtained through minimization of KL(qφk(z) ‖ qφk−1(z)) should still match p(z).\nIn our experiments, we follow [40] and model qφi(ψ) by a multi-layer perceptron (MLP) with layer widths [30,60,30] with ReLU activations and a tendimensional standard normal noise as its input. We also fix all conditionals qφi(z |ψ) = N (z |ψ, σ2I), σ2 = 0.1. We choose p(z) to be either a one-dimensional mixture of Gaussians or a two-dimensional “banana” distribution. In Figure 3 we plot values of KL(qφi(z) ‖ p(z)), i = 1, . . . , 9 for different values of K1 = K2 = K (see Algorithm 1) when p(z) is a onedimensional mixture of Gaussians (see Appendix D for a detailed description and additional plots). In Figure 4 we plot the approximate PDF of qφk(z) after 9 steps for different values of K. As we can see, even though both “prior” and “posterior” distributions are semi-implicit, the algorithm can still accurately learn the original target distribution after several iterations."
  }, {
    "heading": "6.4 VAE with Semi-Implicit Optimal Prior",
    "text": "We follow the same experimental setup and use the same hyperparameters, as suggested for VampPrior [35]. We consider two architectures, the VAE and the HVAE (hierarchical VAE, [35]), applied to the MNIST dataset with dynamic binarization [28]. In both cases, all distributions (except the prior) have been modeled by fully-factorized neural networks with two hidden layers of 300 hidden units each. We used 40-dimensional latent vectors z (40-dimensional z1 and z2 for HVAE) and Bernoulli likelihood with dynamic binarization for the MNIST dataset. As suggested in the VampPrior paper, we used 500 pseudo-inputs for VampPrior-based models in all cases (higher num-\nber of pseudo-inputs led to overfitting). To measure the performance of all models, we bound the test log-likelihood with the IWAE objective [4] with 5000 samples for the VampPrior-based methods, and estimate the corresponding IW-DSIVAE lower bound with K = 20000 for the DSIVI-based methods (see Appendix B for more details).\nWe consider two formulations, described in Section 5.1: DSIVI-agg stands for the semi-implicit formulation of the aggregated posterior (21), and DSIVI-prior stands for a general semi-implicit prior (23). For the DSIVIprior we have used a fully-factorized Gaussian conditional p(z | ζ) = N (z | ζ,diag(σ2)), where the mixing parameters ζ are the output of a fully-connected neural network with two hidden layers with 300 and 600 hidden units respectively, applied to a 300-dimensional standard Gaussian noise . The first and second hid-\nden layers were followed by ReLU non-linearities, and no non-linearities were applied to obtain ζ. We did not use warm-up [35] with DSIVI-prior.\nThe results are presented in Table 1. DSIVI-agg is a simple modification of VampPrior-data that significantly improves the test log-likelihood, and even outperforms the VampPrior with trained inducing inputs. DSIVI-prior outperforms VampPrior even without warm-up and without coupling the parameters of the prior and the variational posteriors."
  }, {
    "heading": "7 CONCLUSION",
    "text": "We have presented DSIVI, a general-purpose framework that allows to perform variational inference and variational learning when both the approximate posterior distribution and the prior distribution are semiimplicit. DSIVI provides an asymptotically exact lower bound on the ELBO, and also an upper bound that can be made arbitrarily tight. It allows us to estimate the ELBO in any model with semi-implicit distributions, which was not the case for other methods. We have shown the effectiveness of DSIVI applied to a range of problems, e.g. models with hierarchical priors and variational autoencoders with semi-implicit empirical priors. In particular, we show how DSIVIbased treatment improves the performance of VampPrior, the current state-of-the-art prior distribution for VAE."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "We would like to thank Arsenii Ashukha and Kirill Neklyudov for valuable discussions. Dmitry Molchanov and Valery Kharitonov were supported by Samsung Research, Samsung Electronics. Results on applications of semi-implicit models, shown in Section 5, have been obtained by Dmitry Vetrov and supported by the Russian Science Foundation grant no.17-7120072."
  }],
  "year": 2019,
  "references": [{
    "title": "Fixing a broken elbo",
    "authors": ["A. Alemi", "B. Poole", "I. Fischer", "J. Dillon", "R.A. Saurous", "K. Murphy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2018
  }, {
    "title": "The deep weight prior",
    "authors": ["A. Atanov", "A. Ashukha", "K. Struminsky", "D. Vetrov", "M. Welling"],
    "venue": "In International Conference on Learning Representations,",
    "year": 2019
  }, {
    "title": "Variational inference: A review for statisticians",
    "authors": ["D.M. Blei", "A. Kucukelbir", "J.D. McAuliffe"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2017
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"],
    "venue": "arXiv preprint arXiv:1509.00519,",
    "year": 2015
  }, {
    "title": "Linear response methods for accurate covariance estimates from mean field variational bayes",
    "authors": ["R.J. Giordano", "T. Broderick", "M.I. Jordan"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Variational gaussian copula inference",
    "authors": ["S. Han", "X. Liao", "D. Dunson", "L. Carin"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2016
  }, {
    "title": "Probabilistic backpropagation for scalable learning of bayesian neural networks",
    "authors": ["J.M. Hernández-Lobato", "R. Adams"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "β-vae: Learning basic visual concepts with a constrained variational framework",
    "authors": ["I. Higgins", "L. Matthey", "A. Pal", "C. Burgess", "X. Glorot", "M. Botvinick", "S. Mohamed", "A. Lerchner"],
    "year": 2016
  }, {
    "title": "Structured stochastic variational inference",
    "authors": ["M.D. Hoffman", "D.M. Blei"],
    "venue": "In Artificial Intelligence and Statistics,",
    "year": 2015
  }, {
    "title": "Elbo surgery: yet another way to carve up the variational evidence lower bound",
    "authors": ["M.D. Hoffman", "M.J. Johnson"],
    "venue": "In Workshop in Advances in Approximate Bayesian Inference,",
    "year": 2016
  }, {
    "title": "Variational inference using implicit distributions",
    "authors": ["F. Huszár"],
    "venue": "arXiv preprint arXiv:1702.08235,",
    "year": 2017
  }, {
    "title": "Improving the mean field approximation via the use of mixture distributions",
    "authors": ["T.S. Jaakkola", "M.I. Jordan"],
    "venue": "In Learning in graphical models,",
    "year": 1998
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D.P. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Variational dropout and the local reparameterization trick",
    "authors": ["D.P. Kingma", "T. Salimans", "M. Welling"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Autoencoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "arXiv preprint arXiv:1312.6114,",
    "year": 2013
  }, {
    "title": "Gradient-based learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Gradient estimators for implicit models",
    "authors": ["Y. Li", "R.E. Turner"],
    "venue": "arXiv preprint arXiv:1705.07107,",
    "year": 2017
  }, {
    "title": "Bayesian compression for deep learning",
    "authors": ["C. Louizos", "K. Ullrich", "M. Welling"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Multiplicative normalizing flows for variational bayesian neural networks",
    "authors": ["C. Louizos", "M. Welling"],
    "venue": "arXiv preprint arXiv:1703.01961,",
    "year": 2017
  }, {
    "title": "Auxiliary deep generative models",
    "authors": ["L. Maaløe", "C.K. Sønderby", "S.K. Sønderby", "O. Winther"],
    "venue": "Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. ICML, 2017",
    "authors": ["L. Mescheder", "S. Nowozin", "A. Geiger"],
    "year": 2017
  }, {
    "title": "Learning in implicit generative models",
    "authors": ["S. Mohamed", "B. Lakshminarayanan"],
    "venue": "arXiv preprint arXiv:1610.03483,",
    "year": 2016
  }, {
    "title": "Bayesian learning for neural networks, volume",
    "authors": ["R.M. Neal"],
    "year": 1995
  }, {
    "title": "Structured bayesian pruning via lognormal multiplicative noise",
    "authors": ["K. Neklyudov", "D. Molchanov", "A. Ashukha", "D.P. Vetrov"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
    "authors": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2010
  }, {
    "title": "Operator variational inference",
    "authors": ["R. Ranganath", "D. Tran", "J. Altosaar", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Hierarchical variational models",
    "authors": ["R. Ranganath", "D. Tran", "D. Blei"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "On the quantitative analysis of deep belief networks",
    "authors": ["R. Salakhutdinov", "I. Murray"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "Markov chain monte carlo and variational inference: Bridging the gap",
    "authors": ["T. Salimans", "D. Kingma", "M. Welling"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Exploiting tractable substructures in intractable networks",
    "authors": ["L.K. Saul", "M.I. Jordan"],
    "venue": "In Advances in neural information processing systems,",
    "year": 1996
  }, {
    "title": "Kernel implicit variational inference",
    "authors": ["J. Shi", "S. Sun", "J. Zhu"],
    "venue": "arXiv preprint arXiv:1705.10119,",
    "year": 2017
  }, {
    "title": "Density ratio estimation in machine learning",
    "authors": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"],
    "year": 2012
  }, {
    "title": "Sparse Bayesian Learning and the Relevance",
    "authors": ["M. Tipping"],
    "venue": "Vector Machine",
    "year": 2000
  }, {
    "title": "Doubly stochastic variational bayes for non-conjugate inference",
    "authors": ["M. Titsias", "M. Lázaro-Gredilla"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Vae with a vampprior",
    "authors": ["J.M. Tomczak", "M. Welling"],
    "venue": "arXiv preprint arXiv:1705.07120,",
    "year": 2017
  }, {
    "title": "Copula variational inference",
    "authors": ["D. Tran", "D. Blei", "E.M. Airoldi"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Hierarchical implicit models and likelihood-free variational inference",
    "authors": ["D. Tran", "R. Ranganath", "D. Blei"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "The variational gaussian process",
    "authors": ["D. Tran", "R. Ranganath", "D.M. Blei"],
    "year": 2016
  }, {
    "title": "Overpruning in variational bayesian neural networks",
    "authors": ["B. Trippe", "R. Turner"],
    "venue": "arXiv preprint arXiv:1801.06230,",
    "year": 2018
  }, {
    "title": "Semi-implicit variational inference",
    "authors": ["M. Yin", "M. Zhou"],
    "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
    "year": 2018
  }],
  "id": "SP:78f6d3a5539a6513061e46789e7c1277a625fb4e",
  "authors": [{
    "name": "Dmitry Molchanov",
    "affiliations": []
  }, {
    "name": "Valery Kharitonov",
    "affiliations": []
  }, {
    "name": "Artem Sobolev",
    "affiliations": []
  }, {
    "name": "Dmitry Vetrov",
    "affiliations": []
  }],
  "abstractText": "We extend the existing framework of semiimplicit variational inference (SIVI) and introduce doubly semi-implicit variational inference (DSIVI), a way to perform variational inference and learning when both the approximate posterior and the prior distribution are semi-implicit. In other words, DSIVI performs inference in models where the prior and the posterior can be expressed as an intractable infinite mixture of some analytic density with a highly flexible implicit mixing distribution. We provide a sandwich bound on the evidence lower bound (ELBO) objective that can be made arbitrarily tight. Unlike discriminator-based and kernel-based approaches to implicit variational inference, DSIVI optimizes a proper lower bound on ELBO that is asymptotically exact. We evaluate DSIVI on a set of problems that benefit from implicit priors. In particular, we show that DSIVI gives rise to a simple modification of VampPrior, the current state-of-theart prior for variational autoencoders, which improves its performance.",
  "title": "Doubly Semi-Implicit Variational Inference"
}