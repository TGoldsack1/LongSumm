{
  "sections": [{
    "text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1391–1400, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics."
  }, {
    "heading": "1 Introduction",
    "text": "Hierarchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.\nIn order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information\n∗Corresponding author\nfrom parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.\nUnfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT?\nLearning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of their phrasal substitutions. We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals. In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.\nThe big challenge here is that thousands of tar-\n1391\nget phrasal substitutions will be generated for one single nonterminal during decoding. Computing vector representations for all these phrases will be very time-consuming. We therefore introduce two different methods to handle it. In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network. These projected vectors are used as approximations to real target representations to compute semantic similarities. In the second method, we decode sentences in two passes. The first pass collects target phrase candidates from n-best translations of sentences generated by the baseline. The second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals.\nOur contributions are two-fold. First, we learn semantic representations for nonterminals from their phrasal substitutions with two different methods. This is the first time, to the best of our knowledge, to induce semantic representations for nonterminals from unlabeled data in the context of SMT. Second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above. We incorporate both source-/target-side semantic nonterminal refinement model and their combination based on learned nonterminal representations into translation system. Experiment results show that our method can achieve an improvement of 1.16 BLEU points over the baseline system on NIST MT evaluation test sets.\nThe rest of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 presents our approach of learning semantic vectors for nonterminals, followed by Section 4 describing the details of our semantic nonterminal refinement model. Section 5 introduces the integration of the proposed model into SMT. Experiment results are reported in Section 6. Finally, we conclude our work in Section 7."
  }, {
    "heading": "2 Related Work",
    "text": "A variety of approaches have been explored for nonterminal refinement in hierarchical phrasebased translation. These approaches can be categorized into two groups: 1) augmenting the nonterminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules. The former\nonly allows substitution operations with matched labels. The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.\nAmong approaches which directly refine the single label to more fine-grained labels, syntactic and semantic knowledge are explored in various ways. The syntactically augmented translation model (SAMT) proposed by Zollmann and Venugopal (2006) uses syntactic categories extracted from target-side parse trees to augment nonterminals in hierarchical rules. Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data.\nOn the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la-\ntent syntactic distributions for each nonterminal. They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.\nThe difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al., 2010; Socher et al., 2010)."
  }, {
    "heading": "3 Learning Semantic Representations for Nonterminals",
    "text": "In our framework, semantic representations for nonterminal Xs are automatically induced from word-aligned parallel corpus. In this section, we detail the essential component of our approach, i.e., how to learn semantic vectors for nonterminals and how to project source semantic vectors onto target language semantic space. Before discussing nonterminal representations, we briefly introduce vector representations for words and phrases."
  }, {
    "heading": "3.1 Prerequisite: Learning Words and Phrases Representations",
    "text": "We employ a neural method, specifically the continuous bag-of-words model (Mikolov et al., 2013a) to learn high-quality vector representations for words. Once we complete the training of the continuous bag-of-words model, word embeddings form an embedding matrix M ∈ Rd×|V |, where d is a pre-determined embedding dimensionality and each word w in the vocabulary V corresponds to a vector ~v ∈ Rd. Given the embedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M .\nWe further feed these learned word embeddings\nto recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations. In traditional RAE (shown in Figure 1), given two input children representation vectors ~c1 ∈ Rd and ~c2 ∈ Rd , their parent representation ~p can be calculated as follows:\n~p = f (1)(W (1)[~c1; ~c2] + b(1)) (1)\nwhere [~c1; ~c2] ∈ R2d is the concatenation of vectors of two children, W (1) ∈ Rd×2d is a weight matrix, b(1) ∈ Rd is a bias term, and f (1) is an element-wise activation function such as tanh. The above output representation ~p can be used as a child vector to construct the representation for a larger subphrase. This process is repeated until a binary tree covering the whole input phrase is generated.\nIn order to evaluate how well the parent vector represents its children, we can reconstruct the children in a reconstruction layer:\n[~c1 ′ ; ~c2 ′ ] = f (2)(W (2)~p+ b(2)) (2)\nwhere ~c1 ′ and ~c2 ′\nare the reconstructed children, W (2) is a weight matrix for reconstruction, b(2) is a bias term for reconstruction, and f (2) is an element-wise activation function.\nFor each node in the generated binary tree, we compute Euclidean distance between the original input vectors and the reconstructed vectors to measure the reconstruction error:\nErec([~c1; ~c2]) = 1 2 ‖[~c1; ~c2]− [~c1′ ; ~c2′ ]‖2 (3)\nBy minimizing the total reconstruction error over all nonterminal nodes, we can learn parameters of RAE.\nSocher et al. (2011) propose a greedy unsupervised RAE as an extension to the above traditional RAE. The main difference is that in the unsupervised RAE there is no tree structure which is given for traditional RAE. It can learn both representations and tree structures of phrases or sentences. In this work, we adopt the unsupervised RAE to learn vector representations for phrases."
  }, {
    "heading": "3.2 Inducing Nonterminal Representations from Phrase Representations",
    "text": "As we extract hierarchical rules from phrases by replacing subphrases with nonterminal symbols, a nonterminal X is generalized from a number of\nsubphrases. We believe that these subphrases determine syntactic and semantic properties of the nonterminal X . We therefore enrich each nonterminalX with a semantic vector induced from vector representations of phrases that are replaced by the nonterminal during rule extraction.\nFor an SCFG rule, we can learn semantic vectors for nonterminals on both the source and target side. Due to the space limitation, we introduce the procedure of learning nonterminal vectors on the source side. Semantic vectors on the target side can be learned analogically.\nFor each source-side nonterminal X of a hierarchical rule, we collect all source subphrases replaced by X in a source subphrase set P = {p1, p2, · · · , pm}. We also count the number of times of these phrases being replaced by nonterminal X on training data during rule extraction. We collect these numbers in a count set C = {c1, c2, · · · , cm}. Based on the phrase set P , count set C and learned phrase vector representations in P , we can compute a semantic vector ~vx for nonterminal X in each SCFG rule.\nWe propose two general approaches to obtain semantic vectors for nonterminals: a weighted mean value method and a minimum distance method. Given phrase vector representations ~Pr = {~p1, ~p2, . . . , ~pm} , we calculate the semantic vector for a nonterminal generalized from these phrases as follows.\nWeighted mean value method (MV) computes semantic vector ~vx as:\n~vx = ∑m\ni=1 ci · ~pi∑m i=1 ci\n(4)\nMinimum distance method (MD) finds a point in semantic space to minimize the sum of Eu-\nclidean distances of vectors in ~Pr to this point. Formally,\n~vx = argmin ~vx m∑ i=1 √√√√ d∑ j=1 (pij − vxj)2 (5)\nWe use the stochastic gradient descent algorithm to find the minimal distance and the point ~vx. The component vxj can be updated by vxj ← vxj + λ ∂f∂vxj where f is ∑m i=1 √∑d j=1(pij − vxj)2 and λ is the learning rate. Similar to the center of gravity, the semantic vector ~vx learned by this method acts as a semantic centroid for all vectors of phrases that are substituted by X . Nonterminals in different hierarchical translation rules will have different semantic centroids. These centroids will help translation model capture semantic diversity to a certain degree."
  }, {
    "heading": "3.3 Mapping Source-Side Representations onto Target-Side Semantic Space",
    "text": "As we discussed in Section 1, directly learning vector representations for target phrases is very costly in practice. Inspired by Mikolov et al. (2013b), we adopt vector projection to alleviate this problem. Different from mapping representations from the source side to the target side by learning a linear matrix on word alignments (Mikolov et al., 2013b), we project source multiword phrase representations onto the target semantic space in a nonlinear manner as we believe that nonlinear relations between languages are more reasonable. Specifically, we use a neural network to achieve this goal. Our neural network is a multilayer feed-forward neural network with one hidden layer. The functional form can be written in the following equation:\n~p = tanh(W (4)(tanh(W (3) ~src) + b(3)) + b(4)) (6) where ~src is the input vector which is learned in the source semantic space, W (3) denotes the weight matrix for connections between input and hidden neurons and W (4) denotes the weight matrix for links between hidden neurons and output, b(3) and b(4) are bias terms. To train the neural network, we optimize the following objective:"
  }, {
    "heading": "J = argmin",
    "text": "W (3),W (4) 1 N N∑ i=1 ‖ ~trgi − ~pi‖2 +R(θ) (7)\nwhere N is the number of training examples, ~trgi is the target vector representation for the ith example learned by RAE and ~pi is the output of the neural network for the source vector representation ~srci of ith example. R(θ) is the regularizer on parameters:\nR(θ) = λL 2 ‖W‖2 (8)\nwhere W denotes parameters for parameter matrices W (3), W (4) and bias terms b(3) , b(4)."
  }, {
    "heading": "4 Semantic Nonterminal Refinement Model",
    "text": "In this section, we describe our semantic nonterminal refinement model on the basis of induced real-valued semantic vectors for nonterminals."
  }, {
    "heading": "4.1 Nonterminal Representations in Hierarchical Rules",
    "text": "We incorporate learned semantic representations of nonterminals into hierarchical rules. In particular, ordinary hierarchical rules take the following form:\nX → 〈aXsb, cXtd〉 (9)\nwhere a/b, c/d are strings of terminals on the source and target side, s and t are placeholders denoting the nonterminal X on the source or target side, Xs and Xt are aligned to each other.\nRepresentations for nonterminals can be on either the source or target side. They are attached to hierarchical rules as follows:\nX → 〈aXsb, cXtd, ~vxs, ~vxt〉 (10)\nwhere ~vx. is the source- or target-side semantic representation for nonterminal. In this way, we keep original translation rules intact and decorate nonterminals with their semantic representations."
  }, {
    "heading": "4.2 The Model",
    "text": "The proposed semantic nonterminal refinement model estimates the semantic similarity between a phrase p and nonterminal X . The phrase p and nonterminal X will have a high similarity score in the representation space if they are semantically similar. The higher semantic similarity scores are, the more compatible nonterminals are with corresponding phrases.\nThere is another nonterminal S in glue rules, which are formalized as follows:\nS → 〈S1X2, S1X2〉 (11)\nS → 〈X1, X1〉 (12) This nonterminal S is different from X . We therefore treat it as a special case in the computation of semantic similarity.\nIn this work, we explore two approaches to compute similarity: one based on cosine similarity and the other based on Euclidean distance.\nGiven a phrase vector representation ~p and nonterminalX semantic vector ~vx, Cosine Similarity (CS) is computed as:\ncos(~p, ~vx) = ~p · ~vx ‖~p‖‖ ~vx‖ (13)\nWe set α for the Cosine Similarity between the glue rule and its corresponding phrase as follows:\nSeSim = { cos(~p, ~vx) hierarchical rules\nα glue rules (14)\nAs for Euclidean Distance (ED), it is computed according to the following formula:\ndist(~p, ~vx) = √√√√ d∑ i=1 (pi − vxi)2 (15)\nand similarly we set β for glue rules:\nSeSim = { dist(~p, ~vx) hierarchical rules\nβ glue rules (16)"
  }, {
    "heading": "5 Decoding",
    "text": "We incorporate the proposed model as a new feature into the hierarchical phrase-based translation system. Specifically, two features are added into the baseline system:\n1. Source-side semantic similarity between source phrases and nonterminals\n2. Target-side semantic similarity between target phrases and nonterminals\nWe compute source- and target-side similarities based on representations of nonterminals and phrasal substitutions for each applied rule, and sum up these similarities to calculate the total score of a derivation on the two features.\nThe integration of the source-side semantic nonterminal refinement model into the decoder is trivial. For the target-side model, however, we have to consider the efficiency issue as we mentioned in Section 1. We introduce two different methods to integrate the target-side model into the decoder: 1) projection and 2) two-pass decoding. In the first integration method, a mapping neural network is trained to map source phrase representations onto the target semantic space as described in Section 3.3. The projection can be linear if we remove the hidden layer in the projection neural network. This is similar to the mapping matrix learned by Mikolov et al. (2013b). We calculate semantic similarities between projected representations of phrases and those of nonterminals. In the two-pass decoding, we collect target phrase candidates from 100-best translations for each source sentence generated by the baseline in the first pass and learn vector representations for these target phrase candidates. Then in the second pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector representations. If a target phrase appears in the collected set, the target-side semantic nonterminal refinement model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; otherwise the model will give a penalty. This is because this phrase is not a desirable phrase as it is not used in 100-best translations.\nThe weights of these two features are tuned by the Minimum Error Rate Training (MERT)(Och, 2003), together with weights of other sub-models on a development set. Figure 2 shows the architecture of SMT system with the proposed semantic nonterminal refinement model."
  }, {
    "heading": "6 Experiment",
    "text": "In this section, we conducted a series of experiments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions:\n1. Which approach is better for learning nonterminal representations, weighted mean value or minimum distance?\n2. Can the target-side semantic nonterminal refinement model improve translation quality? And which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding?\n3. Does the combination of source and target semantic nonterminal refinement models provide further improvement?"
  }, {
    "heading": "6.1 Setup",
    "text": "Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words from LDC data1. We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set.\nWe ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU3 as our evaluation metric. In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments.\nWe used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continuous bag-of-words model with a context window of size 5. The monolingual corpus, which was used to pre-train word embeddings, is extracted from\n1The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News).\n2http://www.speech.sri.com/projects/srilm/download.html 3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 4https://code.google.com/p/word2vec/\nthe above parallel corpus in SMT. To train vector representations for multi-word phrases, we randomly selected 1M bilingual sentences 5 as training set and used the unsupervised greedy RAE following (Socher et al., 2011). We used a learning rate of 10−3 for our minimum distance method that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal.\nFor projection neural network in Section 3.3, we set 300 units for the hidden layer and dimensionality of 30 for both input and output vectors. Learning rate was set to 10−3 and the regularization coefficient λL was set to 10−3. To construct the training set for the projection neural network, we selected phrase pairs from our rule table and used their representations on the source and target side as training examples. We randomly selected 5M examples as training set, 10k examples as development set and 10k examples as test set. The multi-layer projection neural network was trained with the back-propagation and stochastic gradient descent algorithm with a mini-batch size of 5k.\nOur baseline system is an in-house hierarchical phrase-based system (Chiang, 2007). The features used in the baseline system includes a 4-gram language model trained on the Xinhua section of the English Gigaword corpus, a 3-gram language model trained on the target part of the bilingual training data, bidirectional translation probabilities, bidirectional lexical weights, a word count, a phrase count and a glue rule count.\nIn order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (SynMis) which was used by Huang et al. (2013) and integrated it into hierarchical phrase-based system. Syn-Mis model decorates each nonterminal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with corresponding source spans. In order to obtain head POS tags for Syn-Mis model, we used the Stanford dependency parser 6 (Chang et al., 2009) to parse Chinese sentences in our training corpus and NIST development/test sets.\n5We choose bilingual sentences because we want to obtain bilingual training examples to train our projection neural network as described in Section 3.3.\n6http://nlp.stanford.edu/software/lex-parser.shtml"
  }, {
    "heading": "6.2 Different Approaches to Learn Vector Representations for Nonterminals",
    "text": "Our first group of experiments were carried out to investigate which approach is more appropriate to learn semantic vectors for nonterminals. We only used the source-side semantic nonterminal refinement model in these experiments. In order to validate the effectiveness of the proposed approaches for learning nonterminal semantic vectors, we combined the minimum distance method (MD) with the Euclidean Distance (ED) because both of them are distance-based, and combined the weighted mean value method (MV) with the Cosine Similarity model (CS) as they belong to vector-based approaches. We chose α = 1.0, 0, -1.0 and β = 0, 0.5, 1.0 for glue rules to study the impact of these parameters. We compared our model with the baseline and Syn-Mis model.\nResults are shown in Table 1. From Table 1, we observe that the proposed two approaches are able to achieve significant improvements over the baseline. (MV + CS) and (MD + ED) achieve up to an absolute improvement of 1.09 and 0.81 (when α = 0 and β = 0.5) BLEU points respectively over the baseline on the development test set MT06. And the approach (MV + CS) with α = 0 outperforms Syn-Mis by 0.4 BLEU points on MT06 without using any syntactic information. The approach (MV + CS) achieves better performance and it is more efficient than (MD + ED) where the computation of semantic centroids is time-consuming. Therefore, we adopt the approach (MV + CS) with α = 0 to learn semantic vectors for nonterminals and compute semantic similarities in the following experiments."
  }, {
    "heading": "6.3 Effect of the Target Semantic Nonterminal Refinement Models",
    "text": "In the second set of experiments, we further validate the effectiveness of semantic nonterminal vectors learned on the target side. In these experiments, learning vector representations and computing semantic similarities were performed on the target language semantic space. We also compared the two integration methods discussed in Section 5 for the target-side model. With regard to the projection method, we further compared the linear projection (the projection neural network without hidden layer) with the nonlinear projection (with hidden layer). Experiment results are shown in Table 2.\nFrom Table 2, we can see that\n• Two-pass decoding achieves the highest BLEU scores, which are higher than those of the baseline by 0.75 and 0.66 BLEU points on MT06 and MT08 respectively. The reason may be that noisy translation candidates are filtered out in the first pass. This finding is consistent with many other multiple-pass systems in natural language processing, e.g., two-pass parsing (Zettlemoyer and Collins, 2007).\n• Nonlinear projection achieves an improvement of 0.62 BLEU points over the baseline on MT06. It outperforms linear projection method on both sets. These empirical results support our assumption that nonlinear relations between languages are more reasonable than linear relations.\n• The results prove that the target-side semantic nonterminal refinement model is also able\nto improve the baseline system, although the gain is less than that of the source-side counterpart."
  }, {
    "heading": "6.4 Combination of the Source and Target Models",
    "text": "Finally, we integrated both the source- and targetside semantic nonterminal refinement models into the baseline system. In this experiment, we adopted nonlinear projection to obtain target semantic vector representations for target phrases. These two models collectively achieve a gain of up to 1.16 BLEU points over the baseline and 0.41 BLEU points over Syn-Mis model on average, which is shown in Table 3."
  }, {
    "heading": "7 Conclusion",
    "text": "We have presented a framework to refine nonterminal X in hierarchical translation rules with semantic representations. The semantic vectors are derived from vector representations of phrasal substitutions, which are automatically learned using an unsupervised RAE. As the semantic nonterminal refinement model is capable of selecting more semantically similar translation rules, it achieves statistically significant improvements over the baseline on Chinese-to-English translation. Experiment results have shown that\n• Using (MV + CS) approach to learn semantic representations for nonterminals can achieve better performance than (MD + ED) in terms of BLEU scores.\n• Target-side semantic nonterminal refinement model is able to substantially improve translation quality over the baseline. Two-pass de-\ncoding method is superior to the projection method.\n• The simultaneous incorporation of the source- and target-side models can achieve further improvements over a single-side model.\nFor the future work, we are interested in learning bilingual representations (Lauly et al., 2014; Gouws et al., 2014) for nonterminals. We also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals."
  }, {
    "heading": "Acknowledgment",
    "text": "The work was sponsored by the National Natural Science Foundation of China (Grants No. 61403269, 61432013 and 61333018) and Natural Science Foundation of Jiangsu Province (Grant No. BK20140355). We would like to thank three anonymous reviewers for their insightful comments."
  }],
  "year": 2015,
  "references": [{
    "title": "Ccg contextual labels in hierarchical phrase-based smt",
    "authors": ["Hala Almaghout", "Jie Jiang", "Andy Way."],
    "venue": "Proceedings of the 15th Annual Conference of the European Association for Machine Translation.",
    "year": 2011
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"],
    "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual",
    "year": 2014
  }, {
    "title": "Soft dependency matching for hierarchical phrase-based machine translation",
    "authors": ["Hailong Cao", "Dongdong Zhang", "Ming Zhou", "Tiejun Zhao."],
    "venue": "COLING.",
    "year": 2014
  }, {
    "title": "Discriminative reordering with chinese grammatical relations features",
    "authors": ["Pi-Chuan Chang", "Huihsin Tseng", "Dan Jurafsky", "Christopher D Manning."],
    "venue": "Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages",
    "year": 2009
  }, {
    "title": "Hierarchical phrase-based translation",
    "authors": ["David Chiang."],
    "venue": "computational linguistics, 33(2):201–228.",
    "year": 2007
  }, {
    "title": "Learning to translate with source and target syntax",
    "authors": ["David Chiang."],
    "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
    "authors": ["Jonathan H Clark", "Chris Dyer", "Alon Lavie", "Noah A Smith."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
    "year": 2011
  }, {
    "title": "Utilizing target-side semantic role labels to assist hierarchical phrase-based machine translation",
    "authors": ["Qin Gao", "Stephan Vogel."],
    "venue": "Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 107–115.",
    "year": 2011
  }, {
    "title": "Bilbowa: Fast bilingual distributed representations without word alignments",
    "authors": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."],
    "venue": "arXiv preprint arXiv:1410.2455.",
    "year": 2014
  }, {
    "title": "Improving syntax-augmented machine translation by coarsening the label set",
    "authors": ["Greg Hanneman", "Alon Lavie."],
    "venue": "HLT-NAACL, pages 288–297.",
    "year": 2013
  }, {
    "title": "Improved translation with source syntax labels",
    "authors": ["Hieu Hoang", "Philipp Koehn."],
    "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 409–417. Association for Computational Linguistics.",
    "year": 2010
  }, {
    "title": "Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions",
    "authors": ["Zhongqiang Huang", "Martin Čmejrek", "Bowen Zhou."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language",
    "year": 2010
  }, {
    "title": "Factored soft source syntactic constraints for hierarchical machine translation",
    "authors": ["Zhongqiang Huang", "Jacob Devlin", "Rabih Zbib."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556–566.",
    "year": 2013
  }, {
    "title": "Statistical phrase-based translation",
    "authors": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-",
    "year": 2003
  }, {
    "title": "An autoencoder approach to learning bilingual word representations",
    "authors": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha."],
    "venue": "Advances in Neural Information Processing Systems, pages 1853–",
    "year": 2014
  }, {
    "title": "Head-driven hierarchical phrasebased translation",
    "authors": ["Junhui Li", "Zhaopeng Tu", "Guodong Zhou", "Josef van Genabith."],
    "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 33–37.",
    "year": 2012
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "Exploiting similarities among languages for machine translation",
    "authors": ["Tomas Mikolov", "Quoc V Le", "Ilya Sutskever."],
    "venue": "arXiv preprint arXiv:1309.4168.",
    "year": 2013
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."],
    "venue": "Advances in Neural Information Processing Systems, pages 3111–3119.",
    "year": 2013
  }, {
    "title": "Vector-based models of semantic composition",
    "authors": ["Jeff Mitchell", "Mirella Lapata."],
    "venue": "Proceedings of the 46st Annual Meeting on Association for Computational, pages 236–244.",
    "year": 2008
  }, {
    "title": "Learning hierarchical translation structure with linguistic annotations",
    "authors": ["Markos Mylonakis", "Khalil Sima’an"],
    "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
    "year": 2011
  }, {
    "title": "Minimum error rate training in statistical machine translation",
    "authors": ["Franz Josef Och."],
    "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.",
    "year": 2003
  }, {
    "title": "Effective use of linguistic and contextual information for statistical machine translation",
    "authors": ["Libin Shen", "Jinxi Xu", "Bing Zhang", "Spyros Matsoukas", "Ralph Weischedel."],
    "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language",
    "year": 2009
  }, {
    "title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
    "authors": ["Richard Socher", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning",
    "year": 2010
  }, {
    "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-",
    "year": 2011
  }, {
    "title": "Srilm-an extensible language modeling toolkit",
    "authors": ["Andreas Stolcke"],
    "venue": "INTERSPEECH.",
    "year": 2002
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for",
    "year": 2010
  }, {
    "title": "Preference grammars: Softening syntactic constraints to improve statistical machine translation",
    "authors": ["Ashish Venugopal", "Andreas Zollmann", "Noah A Smith", "Stephan Vogel."],
    "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference",
    "year": 2009
  }, {
    "title": "Using categorial grammar to label translation rules",
    "authors": ["Jonathan Weese", "Chris Callison-Burch", "Adam Lopez."],
    "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 222–231. Association for Computational Lin-",
    "year": 2012
  }, {
    "title": "Online learning of relaxed ccg grammars for parsing to logical form",
    "authors": ["Luke S Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
    "year": 2007
  }, {
    "title": "Syntax augmented machine translation via chart parsing",
    "authors": ["Andreas Zollmann", "Ashish Venugopal."],
    "venue": "Proceedings of the Workshop on Statistical Machine Translation, pages 138–141. Association for Computational Linguistics.",
    "year": 2006
  }, {
    "title": "A word-class approach to labeling pscfg rules for machine translation",
    "authors": ["Andreas Zollmann", "Stephan Vogel."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,",
    "year": 2011
  }],
  "id": "SP:0c8cc142ae61a0c76b722b216b92e29c777f4490",
  "authors": [{
    "name": "Xing Wang",
    "affiliations": []
  }, {
    "name": "Deyi Xiong",
    "affiliations": []
  }, {
    "name": "Min Zhang",
    "affiliations": []
  }],
  "abstractText": "In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufficient information for phrasal substitution. In this paper we propose a framework to refine nonterminals in hierarchical translation rules with real-valued semantic representations. The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolingual corpus. Based on the learned semantic vectors, we build a semantic nonterminal refinement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules. Experiment results on ChineseEnglish translation show that the proposed model significantly improves translation quality on NIST test sets.",
  "title": "Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation"
}