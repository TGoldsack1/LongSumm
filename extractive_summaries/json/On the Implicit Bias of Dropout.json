{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Modern machine learning systems based on deep neural networks are usually over-parameterized, i.e. the number of parameters in the model is much larger than the size of the training data, which makes these systems prone to overfitting. Several explicit regularization strategies have been used in practice to help these systems generalize, including `1 and `2 regularization of the parameters (Nowlan and Hinton, 1992). Recently, (Neyshabur et al., 2015) showed that a variety of such norm-based regularizers can provide sizeindependent capacity control, suggesting that the network size is not a good measure of complexity in such settings. Such a view had been previously motivated in the context of matrix factorization (Srebro et al., 2005), where it is preferable to have many factors of limited overall influence rather than a few important ones.\nBesides explicit regularization techniques, practitioners have used a spectrum of algorithmic approaches to improve the generalization ability of over-parametrized models. This includes early stopping of back propagation (Caruana et al., 2001), batch normalization (Ioffe and Szegedy, 2015) and\n1Department of Computer Science, Johns Hopkins University, Baltimore, USA 2Department of Biomedical Engineering, Johns Hopkins University, Baltimore, USA. Correspondence to: Raman Arora <arora@cs.jhu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndropout (Srivastava et al., 2014). In particular, dropout, which is the focus of this paper, randomly drops hidden nodes along with their connections at training time. Dropout was introduced by Srivastava et al. (2014) as a way of breaking up co-adaptation among neurons, drawing insights from the success of the sexual reproduction model in the evolution of advanced organisms. While dropout has enjoyed tremendous success in training deep neural networks, the theoretical understanding of how dropout (and other algorithmic heuristics) provide regularization in deep learning remains somewhat limited.\nWe argue that a prerequisite for understanding implicit regularization due to various algorithmic heuristics in deep learning, including dropout, is to analyze their behavior in simpler models. Therefore, in this paper, we consider the following learning problem. Let x ∈ Rd2 represent an input feature vector with some unknown distribution D such that Ex∼D[xx>] = I. The output label vector y ∈ Rd1 is given as y = Mx for some M ∈ Rd1×d2 . We consider the hypothesis class represented by a single hidden-layer linear network parametrized as hU,V(x) = UV>x, where V ∈ Rd2×r and U ∈ Rd1×r are the weight matrices in the first and the second layers, respectively. The goal of learning is to find weight matrices U,V that minimize the expected loss `(U,V) := Ex∼D[‖y−hU,V(x)‖2] = Ex∼D[‖y−UV>x‖2].\nA natural learning algorithm to consider is back-propagation with dropout, which can be seen as an instance of stochastic gradient descent on the following objective:\nf(U,V) :=Ebi∼Ber(θ),x∼D [∥∥∥∥y− 1θU diag(b)V>x ∥∥∥∥2 ] , (1)\nwhere the expectation is w.r.t. the underlying distribution on data as well as randomization due to dropout (each hidden unit is dropped independently with probability 1− θ). This procedure, which we simply refer to as dropout in this paper, is given in Algorithm 1.\nIt is easy to check (see Lemma A.1 in the supplementary) that the objective in equation (1) can be written as\nf(U,V) = `(U,V) + λ r∑ i=1 ‖ui‖2‖vi‖2, (2)\nwhere λ = 1−θθ is the regularization parameter, and ui and vi represent the ith columns of U and V, respectively. Note\nthat while the goal was to minimize the expected squared loss, using dropout with gradient descent amounts to finding a minimum of the objective in equation (2); we argue that the additional term in the objective serves as a regularizer, R(U,V) := λ ∑r i=1 ‖ui‖2‖vi‖2, and is an explicit instantiation of the implicit bias of dropout. Furthermore, we note that this regularizer is closely related to path regularization which is given as the square-root of the sum over all paths, from input to output, of the product of the squared weights along the path (Neyshabur et al., 2015). Formally, for a single layer network, path regularization is given as\nψ2(U,V) =  r∑ i=1 d1∑ j=1 d2∑ k=1 u2jiv 2 ki  12 . (3) Interestingly, the dropout regularizer is equal to the square of the path regularizer, i.e. R(U,V) = λψ22(U,V). While this observation is rather immediate, it has profound implications owing to the fact that path regularization provides size-independent capacity control in deep learning, thereby supporting empirical evidence that dropout finds good solutions in over-parametrized settings.\nIn this paper, we focus on studying the optimization landscape of the objective in equation (2) for a single hiddenlayer linear network with dropout and the special case of an autoencoder with tied weights. Furthermore, we are interested in characterizing the solutions to which dropout (i.e. Algorithm 1) converges. We make the following progress toward addressing these questions.\n1. We formally characterize the implicit bias of dropout. We show that, when minimizing the expected loss `(U,V) with dropout, any global minimum (Ũ, Ṽ) satisfies ψ2(Ũ, Ṽ) = min{ψ2(U,V) s.t. UV> = ŨṼ\n>}. More importantly, for auto-encoders with tied weights, we show that all local minima inherit this property.\n2. Despite the non-convex nature of the problem, we completely characterize the global optima by giving necessary and sufficient conditions for optimality.\n3. We describe the optimization landscape of the dropout problem. In particular, we show that for a sufficiently small dropout rate, all local minima of the objective in equation (2) are global and all saddle points are non-degenerate. This allows Algorithm 1 to efficiently escape saddle points and converge to a global optimum.\nThe rest of the paper is organized as follows. In Section 2, we study dropout for single hidden-layer linear auto-encoder networks with weights tied between the first and the second layers. This gives us the tools to study the dropout problem in a more general setting of single hidden-layer linear\nAlgorithm 1 Dropout with Stochastic Gradient Descent\ninput Data {(xt, yt)} T−1 t=0 , dropout rate 1−θ, learning rate η\n1: Initialize U0,V0 2: for t = 0, 1, . . . , T − 1 do 3: sample bt element-wise from Bernoulli(θ) 4: Update the weights\nUt+1←Ut−η ( 1\nθ Ut diag(bt)V>t xt−yt\n) x>t Vt diag(bt)\nVt+1←Vt−ηxt ( 1\nθ x>t Vt diag(bt)U > t −y>t\n) Ut diag(bt)\n5: end for output UT ,VT\nnetworks in Section 3. In Section 4, we characterize the optimization landscape of the objective in (2), show that it satisfies the strict saddle property, and that there are no spurious local minima. We specialize our results to matrix factorization in Section 5, and in Section 6, we discuss preliminary experiments to support our theoretical results."
  }, {
    "heading": "1.1. Notation",
    "text": "We denote matrices, vectors, scalar variables and sets by Roman capital letters, Roman small letters, small letters and script letters respectively (e.g. X, x, x, and X ). For any integer d, we represent the set {1, . . . , d} by [d]. For any integer i, ei denotes the i-th standard basis. For any integer d, 1d ∈ Rd is the vector of all ones, ‖x‖ represents the `2-norm of vector x, and ‖X‖, ‖X‖F , ‖X‖∗ and λi(X) represent the spectral norm, the Frobenius norm, the nuclear norm and the i-th largest singular value of matrix X, respectively. 〈·, ·〉 represents the standard inner product, for vectors or matrices, where 〈X,X′〉 = Tr(X>X′). For a matrix X ∈ Rd1×d2 , diag(X) ∈ Rmin{d1,d2} returns its diagonal elements. Similarly, for a vector x ∈ Rd, diag(x) ∈ Rd×d is a diagonal matrix with x on its diagonal. For any scalar x, we define (x)+ = max{x, 0}, and for a matrix X, (X)+ is the elementwise application of (·)+ to X. For a matrix X with a compact singular value decomposition X = UΣV>, and for any scalar α ≥ 0, we define the singular-value shrinkagethresholding operator as Sα(X) := U(Σ− αI)+V>."
  }, {
    "heading": "2. Linear autoencoders with tied weights",
    "text": "We begin with a simpler hypothesis family of single hiddenlayer linear auto-encoders with weights tied such that U = V. Studying the problem in this setting helps our intuition about the implicit bias that dropout induces on weight matrices U. This analysis will be extended to the more general setting of single hidden-layer linear networks in the next section.\nRecall that the goal here is to find an autoencoder network represented by a weight matrix U ∈ Rd2×r that solves:\nmin U∈Rd2×r `(U,U) + λ r∑ i=1 ‖ui‖4, (4)\nwhere ui is the ith column of U. Note that the loss function `(U,U) is invariant under rotations, i.e., for any orthogonal transformation Q ∈ Rd×d,Q>Q = QQ> = Id, it holds that\n`(U,U)=Ex∼D[‖y− UQQ>U>x‖2]=`(UQ,UQ),\nso that applying a rotation matrix to a candidate solution U does not change the value of the loss function. However, the regularizer is not rotation-invariant and clearly depends on the choice of Q. Therefore, in order to solve Problem (4), we need to find a rotation matrix that minimizes the value of the regularizer for a given weight matrix.\nTo that end, let us denote the squared column norms of the weight matrix U by nu = (‖u1‖2, . . . , ‖ur‖2) and let 1r ∈ Rr be the vector of all ones. Then, for any U,\nR(U,U) = λ r∑ i=1 ‖ui‖4 = λ r ‖1r‖2‖nu‖2\n≥ λ r 〈1r, nu〉2 = λ r ( r∑ i=1 ‖ui‖2 )2 = λ r ‖U‖4F ,\nwhere the inequality follows from Cauchy-Schwartz inequality. Hence, the regularizer is lower bounded by λr ‖U‖ 4 F , with equality if and only if nu is parallel to 1r, i.e. when all the columns of U have equal norms. Since the loss function is rotation invariant, one can always decrease the value of the overall objective by rotating U such that UQ has a smaller regularizer. A natural question to ask, therefore, is if there always exists a rotation matrix Q such that the matrix UQ has equal column norms. In order to formally address this question, we introduce the following definition. Definition 2.1 (Equalized weight matrix, equalized autoencoder, equalizer). A weight matrix U is said to be equalized if all its columns have equal norms. An autoencoder with tied weights is said to be equalized if the norm of the incoming weight vector is equal across all hidden nodes in the network. An orthogonal transformation Q is said to be an equalizer of U (equivalently, of the corresponding autoencoder) if UQ is equalized.\nNext, we show that any matrix U can be equalized. Theorem 2.2. Any weight matrix U ∈ Rd×r (equivalently, the corresponding autoencoder network hU,U) can be equalized. Furthermore, there exists a polynomial time algorithm (Algorithm 2) that returns an equalizer for a given matrix.\nThe key insight here is that if GU := U>U is the Gram matrix associated with the weight matrix U, then hU,U is equalized by Q if and only if all diagonal elements of Q>GUQ\nAlgorithm 2 EQZ(U) equalizer of an auto-encoder hU,U input U ∈ Rd×r\n1: G← U>U 2: Q← Ir 3: for i = 1 to r do 4: [V,Λ]←eig(G) {G=VΛV> eigendecomposition} 5: w = 1√ r−i+1 ∑r−i+1 i=1 vi 6: Qi ← [w w⊥] {w⊥ ∈ R(r−i+1)×(r−i) orthonormal basis for the Null space of w} 7: G← Q>i GQi {Making first diagonal element zero} 8: G← G(2 : end, 2 : end) {First principal submatrix}\n9: Q← Q [\nIi−1 0 0 Qi ] 10: end for output Q {such that UQ is equalized}\nare equal. More importantly, if GU = VΛV> is an eigendecomposition of GU, then for w = 1√r ∑r i=1 vi, it holds that w>GUw = Tr GUr ; Proof of Theorem 2.2 uses this property to recursively equalize all diagonal elements of GU.\nFinally, we argue that the implicit bias induced by dropout is closely related to the notion of equalized network introduced above. In particular, our main result of the section states that the dropout enforces any globally optimal network to be equalized. Formally, we show the following. Theorem 2.3. If U is a global optimum of Problem 4, then U is equalized. Furthermore, it holds that\nR(U) = λ\nr ‖U‖4F .\nTheorem 2.3 characterizes the effect of regularization induced by dropout in learning autoencoders with tied weights. It states that for any globally optimal network, the columns of the corresponding weight matrix have equal norms. In other words, dropout tends to give equal weights to all hidden nodes – it shows that dropout implicitly biases the optimal networks towards having hidden nodes with limited overall influence rather than a few important ones.\nWhile Theorem 2.3 makes explicit the bias of dropout and gives a necessary condition for global optimality in terms of the weight matrix U∗, it does not characterize the bias induced in terms of the network (i.e. in terms of U∗U>∗ ). The following theorem completes the characterization by describing globally optimal autoencoder networks. Since the goal is to understand the implicit bias of dropout, we specify the global optimum in terms of the true concept, M. Theorem 2.4. For any j ∈ [r], let κj := 1j ∑j i=1 λi(M). Furthermore, define ρ := max{j ∈ [r] : λj(M) > λjκjr+λj }. Then, if U∗ is a global optimum of Problem 4, it satisfies that U∗U>∗ = S λρκρ\nr+λρ\n(M).\nRemark 2.5. In light of Theorem 2.3, the proof of Theorem 2.4 entails solving the following optimization problem\nmin U∈Rd×r\n`(U,U) + λ\nr ‖U‖4F , (5)\ninstead of Problem 4. This follows since the loss function `(U,U) is invariant under rotations, hence a weight matrix U cannot be optimal if there exists a rotation matrix Q such that R(UQ,UQ) < R(U,U). Now, while the objective in Problem 5 is a lower bound on the objective in Problem 4, by Theorem 2.2, we know that any weight matrix can be equalized. Thus, it follows that the minimum of the two problems coincide. Although Problem 5 is still non-convex, it is easier to study owing to a simpler form of the regularizer. Figure 1 shows how optimization landscape changes with different dropout rates for a single hidden layer linear autoencoder with one dimensional input and output and with a hidden layer of width two."
  }, {
    "heading": "3. Single hidden-layer linear networks",
    "text": "Next, we consider the more general setting of a shallow linear network with a single hidden layer. Recall, that the\ngoal is to find weight matrices U,V that solve\nmin U∈Rd1×r,V∈Rd2×r `(U,V) + λ r∑ i=1 ‖ui‖2‖vi‖2. (6)\nAs in the previous section, we note that the loss function is rotation invariant, i.e. `(UQ,VQ) = `(U,V) for any rotation matrix Q, however the regularizer is not invariant to rotations. Furthermore, it is easy to verify that both the loss function and the regularizer are invariant under rescaling of the incoming and outgoing weights to hidden neurons. Remark 3.1 (Rescaling invariance). The objective function in Problem (2) is invariant under rescaling of weight matrices, i.e. invariant to transformations of the form Ū = UD, V̄ = VD−1, where D is a diagonal matrix with positive entries. This follows since ŪV̄> = UDD−>V> = UV>, so that `(Ū, V̄) = `(U,V), and also R(Ū, V̄) = R(U,V) since r∑ i=1 ‖ūi‖2‖v̄i‖2 = r∑ i=1 ‖diui‖2‖ 1 di vi‖2 = r∑ i=1 ‖ui‖2‖vi‖2.\nAs a result of rescaling invariance, f(Ū, V̄) = f(U,V). Now, following similar arguments as in the previous section,\nwe define nu,v = (‖u1‖‖v1‖, . . . , ‖ur‖‖vr‖), and note that\nR(U,V) = λ r∑ i=1 ‖ui‖2‖vi‖2 = λ r ‖1r‖2‖nu,v‖2\n≥ λ r 〈1r, nu,v〉2 = λ r ( r∑ i=1 ‖ui‖‖vi‖ )2 ,\nwhere the inequality is due to Cauchy-Schwartz, and the lower bound is achieved if and only if nu,v is a scalar multiple of 1r, i.e. iff ‖ui‖‖vi‖ = ‖u1‖‖v1‖ for all i = 1, . . . , r. This observation motivates the following definition. Definition 3.2 (Jointly equalized weight matrices, equalized linear networks). A pair of weight matrices (U,V) ∈ Rd1×r×Rd2×r is said to be jointly equalized if ‖ui‖‖vi‖ = ‖u1‖‖v1‖ for all i ∈ [r]. A single hidden-layer linear network is said to be equalized if the product of the norms of the incoming and outgoing weights are equal for all hidden nodes. Equivalently, a single hidden-layer network parametrized by weight matrices U,V, is equalized if U,V are jointly equalized. An orthogonal transformation Q ∈ Rr×r is an equalizer of a single hidden-layer network hU,V parametrized by weight matrices U,V, if hUQ,VQ is equalized. The network hU,V (the pair(U,V)) then are said to be jointly equalizable by Q.\nNote that Theorem 2.2 only guarantees the existence of an equalizer for an autoencoder with tied weights. It does not inform us regarding the existence of a rotation matrix that jointly equalizes a general network parameterized by a pair of weight matrices (U,V); in fact, it is not true in general that any pair (U,V) is jointly equalizable. Indeed, the general case requires a more careful treatment. It turns out that while a given pair of matrices (U,V) may not be jointly equalizable there exists a pair (Ũ, Ṽ) that is jointly equalizable and implements the same network function, i.e. hŨ,Ṽ = hU,V. Formally, we state the following result. Theorem 3.3. For any given pair of weight matrices (U,V) ∈ Rd1×r×Rd2×r, there exists another pair (Ũ, Ṽ) ∈ Rd1×r × Rd2×r and a rotation matrix Q ∈ Rr×r such that hŨ,Ṽ = hU,V and hŨ,Ṽ is jointly equalizable by Q. Furthermore, for Ū := ŨQ and V̄ := ṼQ it holds that ‖ūi‖2 = ‖v̄i‖2 = 1r‖UV >‖∗ for i = 1, . . . , r.\nTheorem 3.3 implies that for any network hU,V there exists an equalized network hŪ,V̄ such that hŪ,V̄ = hU,V. Hence, it is always possible to reduce the objective by equalizing the network, and a network hU,V is globally optimal only if it is equalized. Theorem 3.4. If (U,V) is a global optimum of Problem 6, then U,V are jointly equalized. Furthermore, it holds that\nR(U,V) = λ\nr ( r∑ i=1 ‖ui‖‖vi‖ )2 = λ r ‖UV>‖2∗\nRemark 3.5. As in the case of autoencoders with tied weights in Section 2, a complete characterization of the implicit bias of dropout is given by considering the global optimality in terms of the network, i.e. in terms of the product of the weight matrices UV>. Not surprisingly, even in the case of single hidden-layer networks, dropout promotes sparsity, i.e. favors low-rank weight matrices.\nTheorem 3.6. For any j ∈ [r], let κj := 1j ∑j i=1 λi(M). Furthermore, define ρ := max{j ∈ [r] : λj(M) > λjκjr+λj }. Then, if (U∗,V∗) is a global optimum of Problem 6, it satisfies that U∗V>∗ = S λρκρ\nr+λρ\n(M)."
  }, {
    "heading": "4. Geometry of the Optimization Problem",
    "text": "While the focus in Section 2 and Section 3 was on understanding the implicit bias of dropout in terms of the global optima of the resulting regularized learning problem, here we focus on computational aspects of dropout as an optimization procedure. Since dropout is a first-order method (see Algorithm 1) and the landscape of Problem 4 is highly non-convex, we can perhaps only hope to find a local minimum, that too provided if the problem has no degenerate saddle points (Lee et al., 2016; Ge et al., 2015). Therefore, in this section, we pose the following questions: What is the implicit bias of dropout in terms of local minima? Do local minima share anything with global minima structurally or in terms of the objective? Can dropout find a local optimum?\nFor the sake of simplicity of analysis, we focus on the case of autoencoders with tied weight as in Section 2. We show in Section 4.1 that (a) local minima of Problem 4 inherit the same implicit bias as the global optima, i.e. all local minima are equalized. Then, in Section 4.2, we show that for sufficiently small regularization parameter, (b) there are no spurious local minima, i.e. all local minima are global, and (c) all saddle points are non-degenerate (see Definition 4.2)."
  }, {
    "heading": "4.1. Implicit bias in local optima",
    "text": "We begin by recalling that the loss `(U,U) is rotation invariant, i.e. `(UQ,UQ) = `(U,U) for any rotation matrix Q. Now, if the weight matrix U were not equalized, then there exist indices i, j ∈ [r] such that ‖ui‖ > ‖uj‖. We show that it is easy to design a rotation matrix (equal to identity everywhere expect for columns i and j) that moves mass from ui to uj such that the difference in the norms of the corresponding columns of UQ decreases strictly while leaving the norms of other columns invariant. In other words, this rotation strictly reduces the regularizer and hence the objective. Formally, this implies the following result.\nLemma 4.1. All local optima of Problem 4 are equalized, i.e. if U is a local optimum, then ‖ui‖ = ‖uj‖ ∀i, j ∈ [r].\nLemma 4.1 unveils a fundamental property of dropout. As\nsoon as we perform dropout in the hidden layer – no matter how small the dropout rate – all local minima become equalized."
  }, {
    "heading": "4.2. Landscape properties",
    "text": "Next, we characterize the solutions to which dropout (i.e. Algorithm 1) converges. We do so by understanding the optimization landscape of Problem 4. Central to our analysis, is the following notion of strict saddle property. Definition 4.2 (Strict saddle point/property). Let f : U → R be a twice differentiable function and let U ∈ U be a critical point of f . Then, U is a strict saddle point of f if the Hessian of f at U has at least one negative eigenvalue, i.e. λmin(∇2f(U)) < 0. Furthermore, f satisfies strict saddle property if all saddle points of f are strict saddle.\nStrict saddle property ensures that for any critical point U that is not a local optimum, the Hessian has a significant negative eigenvalue which allows first order methods such as gradient descent (GD) and stochastic gradient descent (SGD) to escape saddle points and converge to a local minimum (Lee et al., 2016; Ge et al., 2015). Following this idea, there has been a flurry of works on studying the landscape of different machine learning problems, including low rank matrix recovery (Bhojanapalli et al., 2016), generalized phase retrieval problem (Sun et al., 2016), matrix completion (Ge et al., 2016), deep linear networks (Kawaguchi, 2016), matrix sensing and robust PCA (Ge et al., 2017) and tensor decomposition (Ge et al., 2015), making a case for global optimality of first order methods.\nFor the special case of no regularization (i.e. λ = 0; equivalently, no dropout), Problem 4 reduces to standard squared loss minimization which has been shown to have no spurious local minima and satisfy strict saddle property (see, e.g. (Baldi and Hornik, 1989; Jin et al., 2017)). However, the regularizer induced by dropout can potentially introduce new spurious local minima as well as degenerate saddle points. Our next result establishes that that is not the case, at least when the dropout rate is sufficiently small. Theorem 4.3. For regularization parameter λ < rλr(M)∑r i=1 λi(M)−rλr(M)\n, (a) all local minima of Problem 4 are global, and (b) all saddle points are strict saddle points.\nA couple of remarks are in order. First, Theorem 4.3 guarantees that any critical point U that is not a global optimum is a strict saddle point, i.e. ∇2f(U,U) has a negative eigenvalue. This property allows first order methods, such as dropout given in Algorithm 1, to escape such saddle points. Second, note that the guarantees in Theorem 4.3 hold when the regularization parameter λ is sufficiently small. Assumptions of this kind are common in the literature (see, for example (Ge et al., 2017)). While this is a sufficient condition for the result in Theorem 4.3, it is not clear if it is necessary."
  }, {
    "heading": "5. Matrix Factorization with Dropout",
    "text": "The optimization problem associated with learning a shallow network, i.e. Problem 6, is closely related to the optimization problem for matrix factorization. Recall that in matrix factorization, given a matrix M ∈ Rd1×d2 , one seeks to find factors U,V that minimize `(U,V) = ‖M − UV>‖2F . Matrix factorization has recently been studied with dropout by Zhai and Zhang (2015); He et al. (2016) and Cavazza et al. (2018) where at each iteration of gradient descent on the loss function, the columns of factors U,V are dropped independently and with equal probability. Following Cavazza et al. (2018), we can write the resulting problem as\nmin U∈Rd1×r,V∈Rd2×r ‖M− UV>‖2F + λ r∑ i=1 ‖ui‖2‖vi‖2, (7)\nwhich is identical to Problem 6. However, there are two key distinctions. First, we are interested in stochastic optimization problem whereas the matrix factorization problem is typically posed for a given matrix. Second, for the learning problem that we consider here, it is unreasonable to assume access to the true model (i.e. matrix M). Nonetheless, many of the insights we develop here as well as the technical results and algorithmic contributions apply to matrix factorization. Therefore, the goal in this section is to bring to bear the results in Sections 2, 3 and 4 to matrix factorization.\nWe note that Theorem 3.6 and Theorem 3.3, both of which hold for matrix factorization, imply that there is a polynomial time algorithm to solve the matrix factorization problem. In order to find a global optimum of Problem 7, we first compute the optimal M̄ = ŨṼ > using shrinkagethresholding operation (see Theorem 3.6). A global optimum (Ū, V̄) is then obtained by joint equalization of (Ũ, Ṽ) (see Theorem 3.3) using Algorithm 2. The whole procedure is described in Algorithm 3. Few remarks are in order.\nRemark 5.1 (Computational cost of Algorithm 3). It is easy to check that computing ρ, M̄, Ũ and Ṽ requires computing a rank-r SVD of M, which costs O(d2r), where\nAlgorithm 3 Polynomial time solver for Problem 7\ninput Matrix M ∈ Rd2×d1 to be factorized, size of factorization r, regularization parameter λ\n1: ρ← max{j ∈ [r] : λj(M) > λjκjr+λj }, where κj = 1j ∑j i=1 λi(M) for j ∈ [r]. 2: M̄← S λρκρ r+λρ (M) 3: (U,Σ,V)← svd(M̄) 4: Ũ← UΣ 12 , Ṽ← VΣ 12 5: Q← EQZ(Ũ) {Algorithm 2} 6: Ū← ŨQ, V̄← ṼQ\noutput Ū, V̄ {global optimum of Problem 7}\nd = max{d1, d2}. Algorithm 2 entails computing GU = U>U, which costs O(r2d) and the cost of each iterate of Algorithm 2 is dominated by computing the eigendecomposition which is O(r3). Overall, the computational cost of Algorithm 3 is O(d2r + dr2 + r4).\nRemark 5.2 (Universal Equalizer). While Algorithm 2 is efficient (only linear in the dimension) for any rank r, there is a more effective equalization procedure when r is a power of 2. In this case, we can give a universal equalizer which works simultaneously for all matrices in Rd×r. Let U ∈ Rd×r, r = 2k, k ∈ N and let U = WΣV> be its full SVD. The matrix Ũ = UQ is equalized, where Q = VZk and\nZk :=  1 k = 1 2 −k+1 2 [ Zk−1 Zk−1 −Zk−1 Zk−1 ] k > 1 .\nFinally, we note that Problem 7 is an instance of regularized matrix factorization which has recently received considerable attention in the machine learning literature (Ge et al., 2016; 2017; Haeffele and Vidal, 2017). These works show that the saddle points of a class of regularized matrix factorization problems have certain “nice” properties (i.e. escape directions characterized by negative curvature around saddle points) which allow variants of first-order methods such as perturbed gradient descent (Ge et al., 2015; Jin et al., 2017) to converge to a local optimum. Distinct from that line of research, we completely characterize the set of global optima of Problem 7, and provide a polynomial time algorithm to find a global optimum.\nThe work most similar to the matrix factorization problem we consider in this section is that of Cavazza et al. (2018), with respect to which we make several important contributions: (I) Cavazza et al. (2018) characterize optimal solu-\ntions only in terms of the product of the factors, and not in terms of the factors themselves, whereas we provide globally optimal solutions in terms of the factors; (II) Cavazza et al. (2018) require the rank r of the desired factorization to be variable and above some threshold, whereas we consider fixed rank-r factorization for any r; (III) Cavazza et al. (2018) can only find low rank solutions using an adaptive dropout rate, which is not how dropout is used in practice, whereas we consider any fixed dropout rate; and (IV) we give an efficient poly time algorithm to find optimal factors."
  }, {
    "heading": "6. Empirical Results",
    "text": "Dropout is a popular algorithmic technique used for avoiding overfitting when training large deep neural networks. The goal of this section is not to attest to the already wellestablished success of dropout. Instead, the purpose of this section is to simply confirm the theoretical results we showed in the previous section, as a proof of concept.\nWe begin with a toy example in order to visually illustrate the optimization landscape. We use dropout to learn a simple linear auto-encoder with one-dimensional input and output (i.e. a network represented by a scalar M = 2) and a single hidden layer of width r = 2. The input features are sampled for a standard normal distribution. Figure 2 shows the optimization landscape along with the contours of the level sets, and a trace of iterates of dropout (Algorithm 1). The initial iterates and global optima (given by Theorem 2.4) are shown by red and green dots, respectively. Since at any global optimum the weights are equalized, the optimal weight vector in this case is parallel to the vector (±1,±1). We see that dropout converges to a global minimum.\nFor a second illustrative experiment, we use Algorithm 1 to train a shallow linear network, where the input x ∈ R80"
  }, {
    "heading": "100 101 102 103 104 105",
    "text": "is distributed according to the standard Normal distribution. The output y ∈ R120 is generated as y = Mx, where M ∈ R120×80 is drawn randomly by uniformly sampling the right and left singular subspaces and with a spectrum decaying exponentially. Figure 3 illustrates the behavior of Algorithm 1 for different values of the regularization parameter (λ ∈ {0.1, 0.5, 1}), and for different sizes of factors (r ∈ {20, 80}). The curve in blue shows the objective value for the iterates of dropout, and the line in red shows the optimal value of the objective (i.e. objective for a global optimum found using Theorem 3.6). All plots are averaged over 50 runs of Algorithm 1 (averaged over different random initializations, random realizations of Bernoulli dropout, as well as random draws of training examples).\nTo verify that the solution found by dropout actually has equalized factors, we consider the following measure. At each iteration, we compute the “importance scores”, α(i)t = ‖uti‖‖vti‖, i ∈ [r], where uti and vti are the i-th columns of Ut and Vt, respectively. The rightmost panel of Figure 3 shows the variance of α(i)t ’s, over the hidden nodes i ∈ [r], at each iterate t. Note that a high variance in αt corresponds to large variation in the values of ‖uti‖‖vti‖. When the variance is equal to zero, all importance scores are equal, thus the factors are equalized. We see that iterations of Algorithm 1 decrease this measure monotonically, and the larger the value of λ, the faster the weights become equalized."
  }, {
    "heading": "7. Discussion",
    "text": "There has been much effort in recent years to understand the theoretical underpinnings of dropout (see Baldi and Sad-\nowski (2013); Gal and Ghahramani (2016); Wager et al. (2013); Helmbold and Long (2015)). In this paper, we study the implicit bias of dropout in shallow linear networks. We show that dropout prefers solutions with minimal path regularization which yield strong capacity control guarantees in deep learning. Despite being a non-convex optimization problem, we are able to fully characterize the global optima of the dropout objective. Our analysis shows that dropout favors low-rank weight matrices that are equalized. This theoretical finding confirms that dropout as a procedure uniformly allocates weights to different subnetworks, which is akin to preventing co-adaptation.\nWe characterize the optimization landscape of learning autoencoders with dropout. We first show that the local optima inherit the same implicit bias as global optimal, i.e. all local optima are equalized. Then, we show that for sufficiently small dropout rates, there are no spurious local minima in the landscape, and all saddle points are non-degenerate. These properties suggest that dropout – as an optimization procedure – can efficiently converge to a globally optimal solution specified by our theorems.\nUnderstanding dropout in shallow linear networks is a prerequisite for understanding dropout in deep learning. We see natural extensions of our results in two directions: 1) shallow networks with non-linear activation function such as rectified linear units (ReLU) which have been shown to enable faster training (Glorot et al., 2011) and are better understood in terms of the family of functions represented by ReLU-nets (Arora et al., 2018), and 2) exploring the global optimality in deeper networks, even for linear activations."
  }, {
    "heading": "Acknowledgements",
    "text": "This research was supported in part by NSF BIGDATA grant IIS-1546482 and NSF grant IIS-1618485."
  }],
  "year": 2018,
  "references": [{
    "title": "On matrices of trace zero",
    "authors": ["Abraham Adrian Albert", "Benjamin Muckenhoupt"],
    "venue": "The Michigan Mathematical Journal,",
    "year": 1957
  }, {
    "title": "Understanding deep neural networks with rectified linear units",
    "authors": ["Raman Arora", "Amitabh Basu", "Poorya Mianjy", "Anirbit Mukherjee"],
    "venue": "In International Conference on Learning Representations (ICLR),",
    "year": 2018
  }, {
    "title": "Neural networks and principal component analysis: Learning from examples without local minima",
    "authors": ["Pierre Baldi", "Kurt Hornik"],
    "venue": "Neural networks,",
    "year": 1989
  }, {
    "title": "Understanding dropout",
    "authors": ["Pierre Baldi", "Peter J Sadowski"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Global optimality of local search for low rank matrix recovery",
    "authors": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nati Srebro"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
    "authors": ["Rich Caruana", "Steve Lawrence", "C Lee Giles"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2001
  }, {
    "title": "Dropout as a low-rank regularizer for matrix factorization",
    "authors": ["Jacopo Cavazza", "Benjamin D. Haeffele", "Connor Lane", "Pietro Morerio", "Vittorio Murino", "Rene Vidal"],
    "venue": "Int. Conf. on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2018
  }, {
    "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
    "authors": ["Yarin Gal", "Zoubin Ghahramani"],
    "venue": "In Int. Conf. Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Escaping from saddle pointsonline stochastic gradient for tensor decomposition",
    "authors": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"],
    "venue": "In Conf. Learning Theory (COLT),",
    "year": 2015
  }, {
    "title": "Matrix completion has no spurious local minimum",
    "authors": ["Rong Ge", "Jason D Lee", "Tengyu Ma"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "No spurious local minima in nonconvex low rank problems: A unified geometric analysis",
    "authors": ["Rong Ge", "Chi Jin", "Yi Zheng"],
    "venue": "arXiv preprint arXiv:1704.00708,",
    "year": 2017
  }, {
    "title": "Deep sparse rectifier neural networks",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"],
    "venue": "In Proc. Intl. Conf. on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2011
  }, {
    "title": "Structured low-rank matrix factorization: Global optimality, algorithms, and applications",
    "authors": ["Benjamin D Haeffele", "Rene Vidal"],
    "venue": "arXiv preprint arXiv:1708.07850,",
    "year": 2017
  }, {
    "title": "Dropout non-negative matrix factorization for independent feature learning",
    "authors": ["Zhicheng He", "Jie Liu", "Caihua Liu", "Yuan Wang", "Airu Yin", "Yalou Huang"],
    "venue": "In Int. Conf. on Computer Proc. of Oriental Languages",
    "year": 2016
  }, {
    "title": "On the inductive bias of dropout",
    "authors": ["David P Helmbold", "Philip M Long"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2015
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["Sergey Ioffe", "Christian Szegedy"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "How to escape saddle points efficiently",
    "authors": ["Chi Jin", "Rong Ge", "Praneeth Netrapalli", "Sham M Kakade", "Michael I Jordan"],
    "venue": "arXiv preprint arXiv:1703.00887,",
    "year": 2017
  }, {
    "title": "Deep learning without poor local minima",
    "authors": ["Kenji Kawaguchi"],
    "venue": "In Adv in Neural Information Proc. Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Gradient descent converges to minimizers",
    "authors": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"],
    "venue": "arXiv preprint arXiv:1602.04915,",
    "year": 2016
  }, {
    "title": "Norm-based capacity control in neural networks",
    "authors": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"],
    "venue": "In Conf. on Learning Theory (COLT),",
    "year": 2015
  }, {
    "title": "Simplifying neural networks by soft weight-sharing",
    "authors": ["Steven J Nowlan", "Geoffrey E Hinton"],
    "venue": "Neural computation,",
    "year": 1992
  }, {
    "title": "Maximum-margin matrix factorization",
    "authors": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2005
  }, {
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"],
    "venue": "Journal of Machine Learning Research (JMLR),",
    "year": 2014
  }, {
    "title": "A geometric analysis of phase retrieval",
    "authors": ["Ju Sun", "Qing Qu", "John Wright"],
    "venue": "In IEEE International Symposium on Information Theory (ISIT),",
    "year": 2016
  }, {
    "title": "Dropout training as adaptive regularization",
    "authors": ["Stefan Wager", "Sida Wang", "Percy S Liang"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Dropout training of matrix factorization and autoencoder for link prediction in sparse graphs",
    "authors": ["Shuangfei Zhai", "Zhongfei Zhang"],
    "venue": "In Proc. of SIAM International Conference on Data Mining (ICDM),",
    "year": 2015
  }],
  "id": "SP:c867c438b4845afe0b2890710f7fba945a2c5620",
  "authors": [{
    "name": "Poorya Mianjy",
    "affiliations": []
  }, {
    "name": "Raman Arora",
    "affiliations": []
  }, {
    "name": "Rene Vidal",
    "affiliations": []
  }],
  "abstractText": "Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.",
  "title": "On the Implicit Bias of Dropout"
}