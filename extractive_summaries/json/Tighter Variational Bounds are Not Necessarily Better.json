{
  "sections": [{
    "heading": "1 Introduction",
    "text": "Variational bounds provide tractable and state-of-the-art objectives for training deep generative models (Kingma & Welling, 2014; Rezende et al., 2014). Typically taking the form of a lower bound on the intractable model evidence, they provide surrogate targets that are more amenable to optimization. In general, this optimization requires the generation of approximate posterior samples during the model training and so a number of methods simultaneously learn an inference network alongside the target generative network. As well as assisting the training process, this inference network is often also of direct interest itself. For example, variational bounds are often used to train auto-encoders (Bourlard\n1Department of Statistics, University of Oxford 2Department of Engineering, University of Oxford 3Department of Computer Science, University of British Columbia. Correspondence to: Tom Rainforth <rainforth@stats.ox.ac.uk>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\n& Kamp, 1988; Hinton & Zemel, 1994; Gregor et al., 2016; Chen et al., 2017), for which the inference network forms the encoder. Variational bounds are also used in amortized and traditional Bayesian inference contexts (Hoffman et al., 2013; Ranganath et al., 2014; Paige & Wood, 2016; Le et al., 2017), for which the generative model is fixed and the inference network is the primary target for the training. The performance of variational approaches depends upon the choice of evidence lower bound (ELBO) and the formulation of the inference network, with the two often intricately linked to one another; if the inference network formulation is not sufficiently expressive, this can have a knock-on effect on the generative network (Burda et al., 2016). In choosing the ELBO, it is often implicitly assumed that using tighter ELBOs is universally beneficial, at least whenever this does not in turn lead to higher variance gradient estimates. In this work we question this implicit assumption by demonstrating that, although using a tighter ELBO is typically beneficial to gradient updates of the generative network, it can be detrimental to updates of the inference network. Remarkably, we find that it is possible to simultaneously tighten the bound, reduce the variance of the gradient updates, and arbitrarily deteriorate the training of the inference network. Specifically, we present theoretical and empirical evidence that increasing the number of importance sampling particles, K, to tighten the bound in the importance-weighted autoencoder (IWAE) (Burda et al., 2016), degrades the signal-tonoise ratio (SNR) of the gradient estimates for the inference network, inevitably deteriorating the overall learning process. In short, this behavior manifests because even though increasing K decreases the standard deviation of the gradient estimates, it decreases the magnitude of the true gradient faster, such that the relative variance increases. Our results suggest that it may be best to use distinct objectives for learning the generative and inference networks, or that when using the same target, it should take into account the needs of both networks. Namely, while tighter bounds are typically better for training the generative network, looser bounds are often preferable for training the inference network. Based on these insights, we introduce three new algorithms: the partially importance-weighted auto-encoder (PIWAE), the multiply importance-weighted auto-encoder (MIWAE), and the combination importance-\nweighted auto-encoder (CIWAE). Each of these include IWAE as a special case and are based on the same set of importance weights, but use these weights in different ways to ensure a higher SNR for the inference network. We demonstrate that our new algorithms can produce inference networks more closely representing the true posterior than IWAE, while matching the training of the generative network, or potentially even improving it in the case of PIWAE. Even when treating the IWAE objective itself as the measure of performance, all our algorithms are able to demonstrate clear improvements over IWAE."
  }, {
    "heading": "2 Background and Notation",
    "text": "Let x be an X -valued random variable defined via a process involving an unobserved Z-valued random variable z with joint density pθ(x, z). Direct maximum likelihood estimation of θ is generally intractable if pθ(x, z) is a deep generative model due to the marginalization of z. A common strategy is to instead optimize a variational lower bound on log pθ(x), defined via an auxiliary inference model qφ(z|x):\nELBOVAE(θ, φ, x) := ∫ qφ(z|x) log pθ(x, z)\nqφ(z|x) dz\n= log pθ(x)−KL(qφ(z|x)||pθ(z|x)). (1) Typically, qφ is parameterized by a neural network, for which the approach is known as the variational auto-encoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014). Optimization is performed with stochastic gradient ascent (SGA) using unbiased estimates of ∇θ,φ ELBOVAE(θ, φ, x). If qφ is reparameterizable, then given a reparameterized sample z ∼ qφ(z|x), the gradients ∇θ,φ(log pθ(x, z) − log qφ(z|x)) can be used for the optimization. The VAE objective places a harsh penalty on mismatch between qφ(z|x) and pθ(z|x); optimizing jointly in θ, φ can confound improvements in log pθ(x) with reductions in the KL (Turner & Sahani, 2011). Thus, research has looked to develop bounds that separate the tightness of the bound from the expressiveness of the class of qφ. For example, the IWAE objectives (Burda et al., 2016), which we denote as ELBOIS(θ, φ, x), are a family of bounds defined by\nQIS(z1:K |x) := ∏K\nk=1 qφ(zk|x),\nẐIS(z1:K , x) := 1\nK ∑K k=1 pθ(x, zk) qφ(zk|x) , (2)\nELBOIS(θ, φ, x) := ∫ QIS(z1:K |x) log ẐIS(z1:K , x) dz1:K\n≤ log pθ(x). The IWAE objectives generalize the VAE objective (K = 1 corresponds to the VAE) and the bounds become strictly tighter asK increases (Burda et al., 2016). When the family of qφ contains the true posteriors, the global optimum parameters {θ∗, φ∗} are independent ofK, see e.g. (Le et al., 2018). Nonetheless, except for the most trivial models, it is not usually the case that qφ contains the true posteriors, and Burda et al. (2016) provide strong empirical evidence that\nsetting K > 1 leads to significant empirical gains over the VAE in terms of learning the generative model. Optimizing tighter bounds is usually empirically associated with better models pθ in terms of marginal likelihood on held out data. Other related approaches extend this to sequential Monte Carlo (SMC) (Maddison et al., 2017; Le et al., 2018; Naesseth et al., 2018) or change the lower bound that is optimized to reduce the bias (Li & Turner, 2016; Bamler et al., 2017). A second, unrelated, approach is to tighten the bound by improving the expressiveness of qφ (Salimans et al., 2015; Tran et al., 2015; Rezende & Mohamed, 2015; Kingma et al., 2016; Maaløe et al., 2016; Ranganath et al., 2016). In this work, we focus on the former, algorithmic, approaches to tightening bounds."
  }, {
    "heading": "3 Assessing the Signal-to-Noise Ratio of the Gradient Estimators",
    "text": "Because it is not feasible to analytically optimize any ELBO in complex models, the effectiveness of any particular choice of ELBO is linked to our ability to numerically solve the resulting optimization problem. This motivates us to examine the effect K has on the variance and magnitude of the gradient estimates of IWAE for the two networks. More generally, we study IWAE gradient estimators constructed as the average of M estimates, each built from K independent particles. We present a result characterizing the asymptotic signal-to-noise ratio in M and K. For the standard case of M = 1, our result shows that the signal-to-noise ratio of the reparameterization gradients of the inference network for the IWAE decreases with rate O(1/ √ K).\nAs estimating the ELBO requires a Monte Carlo estimation of an expectation over z, we have two sample sizes to tune for the estimate: the number of samples M used for Monte Carlo estimation of the ELBO and the number of importance samples K used in the bound construction. Here M does not change the true value of∇θ,φ ELBO, only our variance in estimating it, while changing K changes the ELBO itself, with larger K leading to tighter bounds (Burda et al., 2016). Presuming that reparameterization is possible, we can express our gradient estimate in the general form\n∆M,K := 1\nM ∑M m=1 ∇θ,φ log 1 K ∑K k=1 wm,k, (3)\nwhere wm,k = pθ(zm,k, x)\nqφ(zm,k|x) and zm,k\ni.i.d.∼ qφ(zm,k|x).\nThus, for a fixed budget of T = MK samples, we have a family of estimators with the cases K = 1 and M = 1 corresponding respectively to the VAE and IWAE objectives. We will use ∆M,K (θ) to refer to gradient estimates with respect to θ and ∆M,K (φ) for those with respect to φ. Variance is not always a good barometer for the effectiveness of a gradient estimation scheme; estimators with small expected values need proportionally smaller variances to be estimated accurately. In the case of IWAE, when changes in K\nsimultaneously affect both the variance and expected value, the quality of the estimator for learning can actually worsen as the variance decreases. To see why, consider the marginal likelihood estimates Ẑm,K = ∑K k=1 wm,k. Because these become exact (and thus independent of the proposal) as K →∞, it must be the case that limK→∞∆M,K(φ) = 0. Thus asK becomes large, the expected value of the gradient must decrease along with its variance, such that the variance relative to the problem scaling need not actually improve. To investigate this formally, we introduce the signal-tonoise-ratio (SNR), defining it to be the absolute value of the expected estimate scaled by its standard deviation:\nSNRM,K(θ) = |E [∆M,K(θ)] /σ [∆M,K(θ)]| (4) where σ[·] denotes the standard deviation of a random variable. The SNR is defined separately on each dimension of the parameter vector and similarly for SNRM,K(φ). It provides a measure of the relative accuracy of the gradient estimates. Though a high SNR does not always indicate a good SGA scheme (as the target objective itself might be poorly chosen), a low SNR is always problematic as it indicates that the gradient estimates are dominated by noise: if SNR → 0 then the estimates become completely random. We are now ready to state our main theoretical result: SNRM,K(θ) = O( √ MK) and SNRM,K(φ) = O( √ M/K).\nTheorem 1. Assume that when M = K = 1, the expected gradients; the variances of the gradients; and the first four moments of w1,1,∇θw1,1, and∇φw1,1 are all finite and the variances are also non-zero. Then the signal-to-noise ratios of the gradient estimates converge at the following rates\nSNRM,K(θ) = (5)\n√ M ∣∣∣∣∣∣∣∣ √ K ∇θZ − 12Z√K∇θ ( Var[w1,1] Z2 ) +O ( 1 K3/2 )√ E [ w21,1 (∇θ logw1,1 −∇θ logZ)2 ] +O ( 1 K ) ∣∣∣∣∣∣∣∣\nSNRM,K(φ) = √ M ∣∣∣∣∣∣ ∇φVar [w1,1] +O ( 1 K ) 2Z √ K σ [∇φw1,1] +O ( 1√ K ) ∣∣∣∣∣∣ (6)\nwhere Z := pθ(x) is the true marginal likelihood.\nProof. We give an intuitive demonstration of the result here and provide a formal proof in Appendix A. The effect of M on the SNR follows from using the law of large numbers on the random variable∇θ,φ log Ẑm,K . Namely, the overall expectation is independent of M and the variance reduces at a rate O(1/M). The effect of K is more complicated but is perhaps most easily seen by noting that (Burda et al., 2016)\n∇θ,φ log Ẑm,K = K∑ k=1 wm,k∑K `=1 wm,k ∇θ,φ log ( wm,k ) ,\nsuch that ∇θ,φ log Ẑm,K can be interpreted as a selfnormalized importance sampling estimate. We can, there-\nfore, invoke the known result (see e.g. Hesterberg (1988)) that the bias of a self-normalized importance sampler converges at a rate O(1/K) and the standard deviation at a rate O(1/ √ K). We thus see that the SNR converges at a rate\nO((1/K)/(1/ √ K)) = O(1/ √ K) if the asymptotic gradi-\nent is 0 and O((1)/(1/ √ K)) = O( √ K) otherwise, giving the convergence rates in the φ and θ cases respectively. The implication of these rates is that increasing M is monotonically beneficial to the SNR for both θ and φ, but that increasing K is beneficial to the former and detrimental to the latter. We emphasize that this means the SNR for the IWAE inference network gets worse as we increase K: this is not just an opportunity cost from the fact that we could have increased M instead, increasing the total number of samples used in the estimator actually worsens the SNR!"
  }, {
    "heading": "3.1 Asymptotic Direction",
    "text": "An important point of note is that the dependence of the true inference network gradients becomes independent of K as K becomes large. Namely, because we have as an intermediary result from deriving the SNRs that\nE [∆M,K(φ)] = − ∇φVar [w1,1]\n2KZ2 +O\n( 1\nK2\n) , (7)\nwe see that expected gradient points in the direction of −∇φVar [w1,1] as K → ∞. This direction is rather interesting: it implies that as K → ∞, the optimal φ is that which minimizes the variance of the weights. This is well known to be the optimal importance sampling distribution in terms of estimating the marginal likelihood (Owen, 2013). Given that the role of the inference network during training is to estimate the marginal likelihood, this is thus arguably exactly what we want to optimize for. As such, this result, which complements those of (Cremer et al., 2017), suggests that increasing K provides a preferable target in terms of the direction of the true inference network gradients. We thus see that there is a trade-off with the fact that increasing K also diminishes the SNR, reducing the estimates to pure noise if K is set too high. In the absence of other factors, there may thus be a “sweet-spot” for setting K."
  }, {
    "heading": "3.2 Multiple Data Points",
    "text": "Typically when training deep generative models, one does not optimize a single ELBO but instead its average over multiple data points, i.e.\nJ (θ, φ) := 1 N ∑N n=1 ELBOIS(θ, φ, x (n)). (8)\nOur results extend to this setting because the z are drawn independently for each x(n), so\nE [ 1\nN ∑N n=1 ∆ (n) M,K ] = 1 N ∑N n=1 E [ ∆ (n) M,K ] , (9)\nVar [ 1\nN ∑N n=1 ∆ (n) M,K ] = 1 N2 ∑N n=1 Var [ ∆ (n) M,K ] . (10)\nWe thus also see that if we are using mini-batches such that N is a chosen parameter and the x(n) are drawn from the\nempirical data distribution, then the SNRs of ∆̄N,M,K := 1 N ∑N n=1 ∆ (n) M,K scales as √ N , i.e. SNRN,M,K(θ) =\nO( √ NMK) and SNRN,M,K(φ) = O( √ NM/K). Therefore increasing N has the same ubiquitous benefit as increasing M . In the rest of the paper, we will implicitly be considering the SNRs for ∆̄N,M,K , but will omit the dependency on N to simplify the notation."
  }, {
    "heading": "4 Empirical Confirmation",
    "text": "Our convergence results hold exactly in relation to M (and N ) but are only asymptotic in K due to the higher order terms. Therefore their applicability should be viewed with a healthy degree of skepticism in the small K regime. With this in mind, we now present empirical support for our theoretical results and test how well they hold in the small K regime using a simple Gaussian model, for which we can analytically calculate the ground truth. Consider a family of generative models with RD–valued latent variables z and observed variables x:\nz ∼ N (z;µ, I), x|z ∼ N (x; z, I), (11) which is parameterized by θ := µ. Let the inference network be parameterized by φ = (A, b), A ∈ RD×D, b ∈ RD where qφ(z|x) = N (z;Ax + b, 23I). Given a dataset (x(n))Nn=1, we can analytically calculate the optimum of our target J (θ, φ) as explained in Appendix B, giving θ∗ := µ∗ = 1N ∑N n=1 x\n(n) and φ∗ := (A∗, b∗), where A∗ = I/2 and b∗ = µ∗/2. Though this will not be the case in general, for this particular problem, the optimal proposal is independent of K. However, the expected gradients for the inference network still change with K. To conduct our investigation, we randomly generated a synthetic dataset from the model with D = 20 dimensions, N = 1024 data points, and a true model parameter value µtrue that was itself randomly generated from a unit Gaussian, i.e. µtrue ∼ N (µtrue; 0, I). We then considered the gradient at a random point in the parameter space close to optimum (we also consider a point far from the optimum in Appendix C.3). Namely each dimension of each parameter was randomly offset from its optimum value using a zero-mean Gaussian with standard deviation 0.01. We then\ncalculated empirical estimates of the ELBO gradients for IWAE, where M = 1 is held fixed and we increase K, and for VAE, where K = 1 is held fixed and we increase M . In all cases we calculated 104 such estimates and used these samples to provide empirical estimates for, amongst other things, the mean and standard deviation of the estimator, and thereby an empirical estimate for the SNR. We start by examining the qualitative behavior of the different gradient estimators as K increases as shown in Figure 1. This shows histograms of the IWAE gradient estimators for a single parameter of the inference network (left) and generative network (right). We first see in Figure 1a that as K increases, both the magnitude and the standard deviation of the estimator decrease for the inference network, with the former decreasing faster. This matches the qualitative behavior of our theoretical result, with the SNR ratio diminishing as K increases. In particular, the probability that the gradient is positive or negative becomes roughly equal for larger values of K, meaning the optimizer is equally likely to increase as decrease the inference network parameters at the next iteration. By contrast, for the generative network, IWAE converges towards a non-zero gradient, such that, even though the SNR initially decreases with K, it then rises again, with a very clear gradient signal for K = 1000. To provide a more rigorous analysis, we next directly examine the convergence of the SNR. Figure 2 shows the convergence of the estimators with increasing M and K. The observed rates for the inference network (Figure 2a) correspond to our theoretical results, with the suggested rates observed all the way back to K = M = 1. As expected, we see that as M increases, so does SNRM,K(b), but as K increases, SNRM,K(b) reduces. In Figure 2b, we see that the theoretical convergence for SNRM,K(µ) is again observed exactly for variations in M , but a more unusual behavior is seen for variations in K, where the SNR initially decreases before starting to increase again for large enough K, eventually exhibiting behavior consistent with the theoretical result for large enough K. The driving factor for this is that here E[∆M,∞(µ)] has a smaller magnitude than (and opposite sign to) E[∆M,1(µ)]\n(see Figure 1b). If we think of the estimators for all values of K as biased estimates for E[∆M,∞(µ)], we see from our theoretical results that this bias decreases faster than the standard deviation. Consequently, while the magnitude of this bias remains large compared to E[∆M,∞(µ)], it is the predominant component in the true gradient and we see similar SNR behavior as in the inference network. Note that this does not mean that the estimates are getting worse for the generative network. As we increase K our bound is getting tighter and our estimates closer to the true gradient for the target that we actually want to optimize ∇µ logZ. See Appendix C.2 for more details. As we previously discussed, it is also the case that increasing K could be beneficial for the inference network even if it reduces the SNR by improving the direction of the expected gradient. However, as we will now show, the SNR is, for this problem, the dominant effect for the inference network."
  }, {
    "heading": "4.1 Directional Signal-to-Noise Ratio",
    "text": "As a reassurance that our chosen definition of the SNR is appropriate for the problem at hand and to examine the effect of multiple dimensions explicitly, we now also consider an alternative definition of the SNR that is similar (though distinct) to that used in (Roberts & Tedrake, 2009). We refer to this as the “directional” SNR (DSNR). At a high-level, we define the DSNR by splitting each gradient estimate into two component vectors, one parallel to the true gradient and one perpendicular, then taking the expectation of ratio of their magnitudes. More precisely, we define u = E [∆M,K ] /‖E [∆M,K ]‖2 as being the true normalized gradient direction and then the DSNR as\nDSNRM,K = E [ ‖∆‖‖2 ‖∆⊥‖2 ] where (12)\n∆‖ = ( ∆TM,Ku ) u and ∆⊥ = ∆M,K −∆‖.\nThe DSNR thus provides a measure of the expected proportion of the gradient that will point in the true direction. For perfect estimates of the gradients, then DSNR → ∞, but unlike the SNR, arbitrarily bad estimates do not have\nDSNR = 0 because even random vectors will have a component of their gradient in the true direction. The convergence of the DSNR is shown in Figure 3, for which the true normalized gradient u has been estimated empirically, noting that this varies with K. We see a similar qualitative behavior to the SNR, with the gradients of IWAE for the inference network degrading to having the same directional accuracy as drawing a random vector. Interestingly, the DSNR seems to be following the same asymptotic convergence behavior as the SNR for both networks in M (as shown by the dashed lines), even though we have no theoretical result to suggest this should occur. As our theoretical results suggest that the direction of the true gradients correspond to targeting an improved objective asK increases, we now examine whether this or the changes in the SNR is the dominant effect. To this end, we repeat our calculations for the DSNR but take u as the target direction of the gradient for K = 1000. This provides a measure of how varying M and K affects the quality of the gradient directions as biased estimators for E [∆1,1000] /‖E [∆1,1000]‖2. As shown in Figure 4, increasing K is still detrimental for the inference network by this metric, even though it brings the expected gradient estimate closer to the target gradient. By contrast, increasing K is now monotonically beneficial for the generative network. Increasing M leads to initial improvements for the inference network before plateauing due to the bias of the estimator. For the generative network, increasing M has little impact, with the bias being the dominant factor throughout. Though this metric is not an absolute measure of performance of the SGA scheme, e.g. because high bias may be more detrimental than high variance, it is nonetheless a powerful result in suggesting that increasing K can be detrimental to learning the inference network."
  }, {
    "heading": "5 New Estimators",
    "text": "Based on our theoretical results, we now introduce three new algorithms that address the issue of diminishing SNR for the inference network. Our first, MIWAE, is exactly equivalent to the general formulation given in (3), the distinction from\nprevious approaches coming from the fact that it takes both M > 1 and K > 1. The motivation for this is that because our inference network SNR increases as O( √ M/K), we should be able to mitigate the issues increasing K has on the SNR by also increasing M . For fairness, we will keep our overall budget T = MK fixed, but we will show that given this budget, the optimal value for M is often not 1. In practice, we expect that it will often be beneficial to increase the mini-batch sizeN rather thanM for MIWAE; as we showed in Section 3.2 this has the same effect on the SNR. Nonetheless, MIWAE forms an interesting reference method for testing our theoretical results and, as we will show, it can offer improvements over IWAE for a given N . Our second algorithm, CIWAE uses a convex combination of the IWAE and VAE bounds, namely\nELBOCIWAE = β ELBOVAE +(1− β) ELBOIWAE (13) where β ∈ [0, 1] is a combination parameter. It is trivial to see that ELBOCIWAE is a lower bound on the log marginal that is tighter than the VAE bound but looser than the IWAE bound. We then employ the following estimator ∆CK,β = (14)\n∇θ,φ ( β 1\nK K∑ k=1\nlogwk + (1− β) log ( 1\nK K∑ k=1 wk )) where we use the same wk for both terms. The motivation for CIWAE is that, if we set β to a relatively small value, the objective will behave mostly like IWAE, except when\nthe expected IWAE gradient becomes very small. When this happens, the VAE component should “take-over” and alleviate SNR issues: the asymptotic SNR of ∆CK,β for φ is O( √ MK) because the VAE component has non-zero expectation in the limit K →∞. Our results suggest that what is good for the generative network, in terms of setting K, is often detrimental for the inference network. It is therefore natural to question whether it is sensible to always use the same target for both the inference and generative networks. Motivated by this, our third method, PIWAE, uses the IWAE target when training the generative network, but the MIWAE target for training the inference network. We thus have\n∆CK,β(θ) = ∇θ log 1\nK ∑K k=1 wk (15a)\n∆CM,K,β(φ) = 1\nM ∑M m=1 ∇φ log 1 L ∑L `=1 wm,` (15b)\nwhere we will generally set K = ML so that the same weights can be used for both gradients."
  }, {
    "heading": "5.1 Experiments",
    "text": "We now use our new estimators to train deep generative models for the MNIST digits dataset (LeCun et al., 1998). For this, we duplicated the architecture and training schedule outlined in Burda et al. (2016). In particular, all networks were trained and evaluated using their stochastic binarization. For all methods we set a budget of T = 64 weights in the target estimate for each datapoint in the minibatch.\nTable 1: Mean final test set performance ± standard deviation over 4 runs. Numbers in brackets in indicate (M,K). The best result is shown in red, while bold results are not statistically significant to best result at the 5% level of a Welch’s t-test.\nMetric IWAE PIWAE (4, 16) PIWAE (8, 8) MIWAE (4, 16) MIWAE (8, 8) CIWAE β = 0.05 CIWAE β = 0.5 VAE IWAE-64 –86.11± 0.10 –85.68± 0.06 –85.74± 0.07 –85.60± 0.07 –85.69± 0.04 –85.91± 0.11 –86.08± 0.08 –86.69± 0.08 log p̂(x) –84.52± 0.02 –84.40± 0.17 –84.46± 0.06 –84.56± 0.05 –84.97± 0.10 –84.57± 0.09 –85.24± 0.08 –86.21± 0.19 −KL(Q||P ) –1.59± 0.10 –1.27± 0.18 –1.28± 0.09 –1.04± 0.08 –0.72± 0.11 –1.34± 0.14 –0.84± 0.11 –0.47± 0.20\nTo assess different aspects of the training performance, we consider three different metrics: ELBOIWAE with K = 64, ELBOIWAE with K = 5000, and the latter of these minus the former. All reported metrics are evaluated on the test data. The motivation for the ELBOIWAE with K = 64 metric, denoted as IWAE-64, is that this is the target used for training the IWAE and so if another method does better on this metric than the IWAE, this is a clear indicator that SNR issues of the IWAE estimator have degraded its performance. In fact, this would demonstrate that, from a practical perspective, using the IWAE estimator is sub-optimal, even if our explicit aim is to optimize the IWAE bound. The second metric, ELBOIWAE with K = 5000, denoted log p̂(x), is used as a surrogate for estimating the log marginal likelihood and thus provides an indicator for fidelity of the learned generative model. The third metric is an estimator for the divergence implicitly targeted by the IWAE. Namely, as shown by Le et al. (2018), the ELBOIWAE can be interpreted as\nELBOIWAE = log pθ(x)−KL(Qφ(z|x)||Pθ(z|x)) (16)\nwhere Qφ(z|x) := ∏K\nk=1 qφ(zk|x), and (17)\nPθ(z|x) := 1\nK ∑K k=1 ∏K `=1 qφ(z`|x) qφ(zk|x) pθ(zk|x). (18)\nThus we can estimate KL(Qφ(z|x)||Pθ(z|x)) using log p̂(x) − IWAE-64, to provide a metric for divergence between the inference network and the proposal network. We use this instead of KL(qφ(z|x)||pθ(z|x)) because the latter can be deceptive metric for the inference network fidelity. For example, it tends to prefer qφ(z|x) that cover only one of the posterior modes, rather than encompassing all of them. As we showed in Section 3.1, the implied target of the true gradients for the inference network improves as K increases and so KL(Qφ(z|x)||Pθ(z|x)) should be a more reliable metric of inference network performance. Figure 5 shows the convergence of these metrics for each algorithm. Here we have considered the middle value for each of the parameters, namely K = M = 8 for PIWAE and MIWAE, and β = 0.5 for CIWAE. We see that PIWAE and MIWAE both comfortably outperformed, and CIWAE\nslightly outperformed, IWAE in terms of IWAE-64 metric, despite IWAE being directly trained on this target. In terms of log p̂(x), PIWAE gave the best performance, followed by IWAE. For the KL, we see that the VAE performed best followed by MIWAE, with IWAE performing the worst. We note here that the KL is not an exact measure of the inference network performance as it also depends on the generative model. As such, the apparent superior performance of the VAE may be because it produces a simpler model, as per the observations of Burda et al. (2016), which in turn is easier to learn an inference network for. Critically though, PIWAE improves this metric whilst also improving generative network performance, such that this reasoning no longer applies. Similar behavior is observed for MIWAE and CIWAE for different parameter settings (see Appendix D). We next considered tuning the parameters for each of our algorithms as shown in Figure 6, for which we look at the final metric values after training. Table 1 further summarizes the performance for certain selected parameter settings. For MIWAE we see that as we increase M , the log p̂(x) metric gets worse, while the KL gets better. The IWAE-64 metric initially increases withM , before reducing again fromM = 16 to M = 64, suggesting that intermediate values for M (i.e. M 6= 1, K 6= 1) give a better trade-off. For PIWAE, similar behavior to MIWAE is seen for the IWAE64 and KL metrics. However, unlike for MIWAE, we see that log p̂(x) initially increases with M , such that PIWAE provides uniform improvement over IWAE for the M = 2, 4, 8, and 16 cases. CIWAE exhibits similar behavior in increasing β as increasing M for MIWAE, but there appears to be a larger degree of noise in the evaluations, while the optimal value of β, though non-zero, seems to be closer to IWAE than for the other algorithms. As an additional measure of the performance of the inference network that is distinct to any of the training targets, we also considered the effective sample size (ESS) (Owen, 2013) for the fully trained networks, defined as\nESS = ( ∑K k=1 wk) 2/ ∑K k=1 w 2 k. (19)\nThe ESS is a measure of how many unweighted samples would be equivalent to the weighted sample set. A low ESS indicates that the inference network is struggling to perform effective inference for the generative network. The results, given in Figure 7, show that the ESSs for CIWAE, MIWAE, and the VAE were all significantly larger than for IWAE and PIWAE, with IWAE giving a particularly poor ESS. Our final experiment looks at the SNR values for the inference networks during training. Here we took a number of different neural network gradient weights at different layers of the network and calculated empirical estimates for their SNRs at various points during the training. We then averaged these estimates over the different network weights, the results of which are given in Figure 8. This clearly shows the low SNR exhibited by the IWAE inference network, suggesting that our results from the simple Gaussian experiments carry over to the more complex neural network domain."
  }, {
    "heading": "6 Conclusions",
    "text": "We have provided theoretical and empirical evidence that algorithmic approaches to increasing the tightness of the ELBO independently to the expressiveness of the inference network can be detrimental to learning by reducing the signal-to-noise ratio of the inference network gradients. Experiments on a simple latent variable model confirmed our theoretical findings. We then exploited these insights to introduce three estimators, PIWAE, MIWAE, and CIWAE and showed that each can deliver improvements over IWAE, even when the metric used for this assessment is the IWAE target itself. In particular, each was able to deliver improvement in the training of the inference network, without any reduction in the quality of the learned generative network. Whereas MIWAE and CIWAE mostly allow for balancing the requirements of the inference and generative networks, PIWAE appears to be able to offer simultaneous improvements to both, with the improved training of the inference network having a knock-on effect on the generative network. Key to achieving this is, is its use of separate targets for the two networks, opening up interesting avenues for future work."
  }, {
    "heading": "Acknowledgments",
    "text": "TR and YWT are supported in part by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007–2013) / ERC grant agreement no. 617071. TAL is supported by a Google studentship, project code DF6700. MI is supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. CJM is funded by a DeepMind Scholarship. FW is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668."
  }],
  "year": 2018,
  "references": [{
    "title": "Perturbative black box variational inference",
    "authors": ["R. Bamler", "C. Zhang", "M. Opper", "S. Mandt"],
    "venue": "arXiv preprint arXiv:1709.07433,",
    "year": 2017
  }, {
    "title": "Auto-association by multilayer perceptrons and singular value decomposition",
    "authors": ["H. Bourlard", "Y. Kamp"],
    "venue": "Biological cybernetics,",
    "year": 1988
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"],
    "venue": "In ICLR,",
    "year": 2016
  }, {
    "title": "Variational lossy autoencoder",
    "authors": ["X. Chen", "D.P. Kingma", "T. Salimans", "Y. Duan", "P. Dhariwal", "J. Schulman", "I. Sutskever", "P. Abbeel"],
    "venue": "In ICLR,",
    "year": 2017
  }, {
    "title": "Reinterpreting importance-weighted autoencoders",
    "authors": ["C. Cremer", "Q. Morris", "D. Duvenaud"],
    "venue": "arXiv preprint arXiv:1704.02916,",
    "year": 2017
  }, {
    "title": "Mcmc design-based non-parametric regression for rare event. application to nested risk computations",
    "authors": ["G. Fort", "E. Gobet", "E. Moulines"],
    "venue": "Monte Carlo Methods and Applications,",
    "year": 2017
  }, {
    "title": "Towards conceptual compression",
    "authors": ["K. Gregor", "F. Besse", "D.J. Rezende", "I. Danihelka", "D. Wierstra"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Advances in importance sampling",
    "authors": ["T.C. Hesterberg"],
    "venue": "PhD thesis, Stanford University,",
    "year": 1988
  }, {
    "title": "Autoencoders, minimum description length and helmholtz free energy",
    "authors": ["G.E. Hinton", "R.S. Zemel"],
    "venue": "In NIPS,",
    "year": 1994
  }, {
    "title": "Stochastic variational inference",
    "authors": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational Bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "venue": "In ICLR,",
    "year": 2014
  }, {
    "title": "Improving variational inference with inverse autoregressive flow",
    "authors": ["D.P. Kingma", "T. Salimans", "M. Welling"],
    "venue": "arXiv preprint arXiv:1606.04934,",
    "year": 2016
  }, {
    "title": "Autoencoding sequential Monte Carlo",
    "authors": ["T.A. Le", "M. Igl", "T. Rainforth", "T. Jin", "F. Wood"],
    "venue": "In ICLR,",
    "year": 2018
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Rényi divergence variational inference",
    "authors": ["Y. Li", "R.E. Turner"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Auxiliary deep generative models",
    "authors": ["L. Maaløe", "C.K. Sønderby", "S.K. Sønderby", "O. Winther"],
    "venue": "arXiv preprint arXiv:1602.05473,",
    "year": 2016
  }, {
    "title": "Filtering variational objectives",
    "authors": ["C.J. Maddison", "D. Lawson", "G. Tucker", "N. Heess", "M. Norouzi", "A. Mnih", "A. Doucet", "Y.W. Teh"],
    "venue": "arXiv preprint arXiv:1705.09279,",
    "year": 2017
  }, {
    "title": "Variational sequential Monte Carlo",
    "authors": ["C.A. Naesseth", "S.W. Linderman", "R. Ranganath", "D.M. Blei"],
    "year": 2018
  }, {
    "title": "Inference networks for sequential Monte Carlo in graphical models",
    "authors": ["B. Paige", "F. Wood"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Automating Inference, Learning, and Design using Probabilistic Programming",
    "authors": ["T. Rainforth"],
    "venue": "PhD thesis,",
    "year": 2017
  }, {
    "title": "On nesting Monte Carlo estimators",
    "authors": ["T. Rainforth", "R. Cornish", "H. Yang", "A. Warrington", "F. Wood"],
    "venue": "In ICML,",
    "year": 2018
  }, {
    "title": "Black box variational inference",
    "authors": ["R. Ranganath", "S. Gerrish", "D. Blei"],
    "venue": "In AISTATS,",
    "year": 2014
  }, {
    "title": "Hierarchical variational models",
    "authors": ["R. Ranganath", "D. Tran", "D. Blei"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Variational inference with normalizing flows",
    "authors": ["D. Rezende", "S. Mohamed"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Signal-to-noise ratio analysis of policy gradient algorithms",
    "authors": ["J.W. Roberts", "R. Tedrake"],
    "venue": "In NIPS,",
    "year": 2009
  }, {
    "title": "Markov chain Monte Carlo and variational inference: Bridging the gap",
    "authors": ["T. Salimans", "D. Kingma", "M. Welling"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "The variational Gaussian process",
    "authors": ["D. Tran", "R. Ranganath", "D.M. Blei"],
    "venue": "arXiv preprint arXiv:1511.06499,",
    "year": 2015
  }, {
    "title": "Two problems with variational expectation maximisation for time-series models",
    "authors": ["R.E. Turner", "M. Sahani"],
    "venue": "Bayesian Time series models,",
    "year": 2011
  }],
  "id": "SP:b9532aff9d2630de3b99008b8f1a143f03f284cd",
  "authors": [{
    "name": "Tom Rainforth",
    "affiliations": []
  }, {
    "name": "Adam R. Kosiorek",
    "affiliations": []
  }, {
    "name": "Tuan Anh Le",
    "affiliations": []
  }, {
    "name": "Chris J. Maddison",
    "affiliations": []
  }, {
    "name": "Maximilian Igl",
    "affiliations": []
  }, {
    "name": "Frank Wood",
    "affiliations": []
  }, {
    "name": "Yee Whye Teh",
    "affiliations": []
  }],
  "abstractText": "We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.",
  "title": "Tighter Variational Bounds are Not Necessarily Better"
}