{
  "sections": [{
    "text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 118–127, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014).\nUsually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect\n∗ Corresponding author.\nto a supervised training criterion. However, most pre-training methods are based on unsupervised objectives (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013), which is effective to improve the final performance, but it does not directly optimize the desired task.\nMulti-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performance relative to learning each task independently. Inspired by the success of multi-task learning (Caruana, 1997), several neural network based models (Collobert and Weston, 2008; Liu et al., 2015b) are proposed for NLP tasks, which utilized multi-task learning to jointly learn several tasks with the aim of mutual benefit. The characteristic of these multi-task architectures is they share some lower layers to determine common features. After the shared layers, the remaining layers are split into multiple specific tasks.\nIn this paper, we propose two deep architectures of sharing information among several tasks in multitask learning framework. All the related tasks are integrated into a single system which is trained jointly. More specifically, inspired by Neural Turing Machine (NTM) (Graves et al., 2014) and memory network (Sukhbaatar et al., 2015), we equip taskspecific long short-term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with an external shared memory. The external memory has capability to store long term information and knowledge shared by several related tasks. Different with NTM, we use a deep fusion strategy to integrate the information from the external memory into taskspecific LSTM, in which a fusion gate controls the\n118\ninformation flowing flexibly and enables the model to selectively utilize the shared information.\nWe demonstrate the effectiveness of our architectures on two groups of text classification tasks. Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.\nOur contributions are of three-folds:\n• We proposed a generic multi-task framework, in which different tasks can share information by an external memory and communicate by a reading/writing mechanism. Two proposed models are complementary to prior multi-task neural networks.\n• Different with Neural Turing Machine and memory network, we introduce a deep fusion mechanism between internal and external memories, which helps the LSTM units keep them interacting closely without being conflated.\n• As a by-product, the fusion gate enables us to better understand how the external shared memory helps specific task."
  }, {
    "heading": "2 Neural Memory Models for Specific Task",
    "text": "In this section, we briefly describe LSTM model, and then propose an external memory enhanced LSTM with deep fusion."
  }, {
    "heading": "2.1 Long Short-term Memory",
    "text": "Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies. LSTM maintains an internal memory cell that updates and exposes its content only when deemed necessary.\nArchitecturally speaking, the memory state and output state are explicitly separated by activation gates (Wang and Cho, 2015). However, the limitation of LSTM is that it lacks a mechanism to index its memory while writing and reading (Danihelka et al., 2016).\nWhile there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz\net al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\nThe LSTM is precisely specified as follows.\n \nc̃t ot it ft\n  =  \ntanh σ σ σ\n  ( Wp [ xt\nht−1\n] + bp ) , (1)\nct = c̃t it + ct−1 ft, (2) ht = ot tanh (ct) , (3)\nwhere xt ∈ Rm is the input at the current time step; W ∈ R4h×(d+m) and bp ∈ R4h are parameters of affine transformation; σ denotes the logistic sigmoid function and denotes elementwise multiplication.\nThe update of each LSTM unit can be written precisely as follows:\n(ht, ct) = LSTM(ht−1, ct−1,xt, θp). (4)\nHere, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. (1-3), and θp represents all the parameters of LSTM."
  }, {
    "heading": "2.2 Memory Enhanced LSTM",
    "text": "LSTM has an internal memory to keep useful information for specific task, some of which may be beneficial to other tasks. However, it is non-trivial to share information stored in internal memory.\nRecently, there are some works to augment LSTM with an external memory, such as neural Turing machine (Graves et al., 2014) and memory network (Sukhbaatar et al., 2015), called memory enhanced LSTM (ME-LSTM). These models enhance the low-capacity internal memory to have a capability of modelling long pieces of text (Andrychowicz and Kurach, 2016).\nInspired by these models, we introduce an external memory to share information among several tasks. To better control shared information and understand how it is utilized from external memory, we propose a deep fusion strategy for ME-LSTM.\nAs shown in Figure 1, ME-LSTM consists the original LSTM and an external memory which is maintained by reading and writing operations. The LSTM not only interacts with the input and output information but accesses the external memory using selective read and write operations.\nThe external memory and corresponding operations will be discussed in detail below.\nExternal Memory The form of external memory is defined as a matrix M ∈ RK×M , where K is the number of memory segments, and M is the size of each segment. Besides, K and M are generally instance-independent and pre-defined as hyperparameters.\nAt each step t, LSTM emits output ht and three key vectors kt, et and at simultaneously. kt, et and at can be computed as\n  kt et at   =   tanh σ tanh   (Wmht + bm) (5)\nwhere Wm and bm are parameters of affine transformation.\nReading The read operation is to read information rt ∈ RM from memory Mt−1.\nrt = αtMt−1, (6)\nwhere rt denotes the reading vector and αt ∈ RK represents a distribution over the set of segments of memory Mt−1, which controls the amount of information to be read from and written to the memory.\nEach scalar αt,k in attention distribution αt can be obtained as:\nαt,k = softmax(g(Mt−1,k,kt−1)) (7)\nwhere Mt−1,k represents the k-th row memory vector, and kt−1 is a key vector emitted by LSTM.\nHere g(x,y) (x ∈ RM ,y ∈ RM ) is a align function for which we consider two different alternatives:\ng(x,y) =\n{ vT tanh(Wa[x;y])\ncosine(x, y) (8)\nwhere v ∈ RM is a parameter vector. In our current implementation, the similarity measure is cosine similarity.\nWriting The memory can be written by two operations: erase and add.\nMt = Mt−1(1− αteTt ) + αtaTt , (9)\nwhere et,at ∈ RM represent erase and add vectors respectively.\nTo facilitate the following statements, we re-write the writing equation as:\nMt = fwrite(Mt−1, αt,ht). (10)\nDeep Fusion between External and Internal Memories After we obtain the information from external memory, we need a strategy to comprehensively utilize information from both external and internal memory.\nTo better control signals flowing from external memory, inspired by (Wang and Cho, 2015), we propose a deep fusion strategy to keep internal and external memories interacting closely without being conflated.\nIn detail, the state ht of LSTM at step t depends on both the read vector rt from external memory, and internal memory ct, which is computed by\nht = ot tanh(ct + gt (Wfrt)), (11)\nwhere Wf is parameter matrix, and gt is a fusion gate to select information from external memory, which is computed by\ngt = σ(Wrrt + Wcct), (12)\nwhere Wr and Wc are parameter matrices. Finally, the update of external memory enhanced LSTM unit can be written precisely as\n(ht,Mt, ct) = ME-LSTM(ht−1,\nMt−1, ct−1,xt, θp, θq), (13)\nwhere θp represents all the parameters of LSTM internal structure and θq represents all the parameters to maintain the external memory.\nOverall, the external memory enables ME-LSTM to have larger capability to store more information, thereby increasing the ability of ME-LSTM. The read and write operations allow ME-LSTM to capture complex sentence patterns."
  }, {
    "heading": "3 Deep Architectures with Shared Memory for Multi-task Learning",
    "text": "Most existing neural network methods are based on supervised training objectives on a single task (Collobert et al., 2011; Socher et al., 2013; Kalchbrenner et al., 2014). These methods often suffer from the limited amounts of training data. To deal with this problem, these models often involve an unsupervised pre-training phase. This unsupervised pre-training is effective to improve the final performance, but it does not directly optimize the desired\ntask. Motivated by the success of multi-task learning (Caruana, 1997), we propose two deep architectures with shared external memory to leverage supervised data from many related tasks. Deep neural model is well suited for multi-task learning since the features learned from a task may be useful for other tasks. Figure 2 gives an illustration of our proposed architectures.\nARC-I: Global Shared Memory In ARC-I, the input is modelled by a task-specific LSTM and external shared memory. More formally, given an input text x, the task-specific output h(m)t of taskm at step t is defined as\n(h (m) t ,M (s) t , c (m) t ) = ME-LSTM(h (m) t−1,\nM (s) t−1, c (m) t−1,xt, θ (m) p , θ (s) q ), (14)\nwhere xt represents word embeddings of word xt; the superscript s represents the parameters are shared across different tasks; the superscript m represents that the parameters or variables are taskspecific for task m.\nHere all tasks share single global memory M(s), meaning that all tasks can read information from it and have the duty to write their shared or taskspecific information into the memory.\nM (s) t = fwrite(M (s) t−1, α (s) t ,h (m) t ) (15)\nAfter calculating the task-specific representation of text h(m)T for task m, we can predict the probability distribution over classes.\nARC-II: Local-Global Hybrid Memory In ARC-I, all tasks share a global memory, but can also record task-specific information besides shared information. To address this, we allocate each task a local task-specific external memory, which can further write shared information to a global memory for all tasks.\nMore generally, for task m, we assign each taskspecific LSTM with a local memory M(m), followed by a global memory M(s), which is shared across different tasks.\nThe read and write operations of the local and global memory are defined as\nr (m) t = α (m) t M (m) t , (16)\nM (m) t = fwrite(M (m) t−1, α (m) t ,h (m) t ), (17)\nr (s) t = α (s) t−1M (s) t−1, (18)\nM (s) t = fwrite(M (s) t−1, α (s) t , r (m) t ), (19)\nwhere the superscript s represents the parameters are shared across different tasks; the superscript m represents that the parameters or variables are taskspecific for task m.\nIn ARC-II, the local memories enhance the capacity of memorizing, while global memory enables the information flowing from different tasks to interact sufficiently."
  }, {
    "heading": "4 Training",
    "text": "The task-specific representation h(m), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific output layers.\nŷ(m) = softmax(W(m)h(m) + b(m)), (20)\nwhere ŷ(m) is prediction probabilities for task m. Given M related tasks, our global cost function is the linear combination of cost function for all tasks.\nφ =\nM∑\nm=1\nλmL(ŷ (m), y(m)) (21)\nwhere λm is the weights for each task m respectively.\nComputational Cost Compared with vanilla LSTM, our proposed two models do not cause much extra computational cost while converge faster. In our experiment, the most complicated ARC-II, costs 2 times as long as vanilla LSTM."
  }, {
    "heading": "5 Experiment",
    "text": "In this section, we investigate the empirical performances of our proposed architectures on two multitask datasets. Each dataset contains several related tasks."
  }, {
    "heading": "5.1 Datasets",
    "text": "The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1.\nMovie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews.\n• SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes.\nIt is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of sub-\njective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000\nmovie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences.\n1http://nlp.stanford.edu/sentiment. 2http://ai.stanford.edu/˜amaas/data/\nsentiment/\nProduct Reviews This dataset3, constructed by Blitzer et al. (2007), contains Amazon product reviews from four different domains: Books, DVDs, Electronics and Kitchen appliances. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 20% and 10% respectively."
  }, {
    "heading": "5.2 Competitor Methods for Multi-task Learning",
    "text": "The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as strong competitor methods .\n• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.\n3https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/\n• MT-DNN: The model is proposed by Liu et al. (2015b) with bag-of-words input and multilayer perceptrons, in which a hidden layer is shared."
  }, {
    "heading": "5.3 Hyperparameters and Training",
    "text": "The networks are trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule (Duchi et al., 2011).\nThe word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16.\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], l2 regularization [0.0, 5E−5, 1E−5]. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters are set as Table 2."
  }, {
    "heading": "5.4 Multi-task Learning of Movie Reviews",
    "text": "We first compare our proposed models with the baseline system for single task classification. Table 3 shows the classification accuracies on the movie reviews dataset. The row of “Single Task” shows the results of LSTM and ME-LSTM for each individual task. With the help of multi-task learning, the performances of these four tasks are improved by 1.8% (ARC-I) and 2.9% (ARC-II) on average relative to LSTM. We can find that the architecture of local-global hybrid external memory has better performances. The reason is that the global memory in ARC-I could store some task-specific information besides shared information, which maybe noisy to other tasks. Moreover, both of our proposed models outperform MT-CNN and MT-DNN, which indicates the effectiveness of our proposed shared mechanism. To give an intuitive evaluation of these results, we also list the following state-of-the-art neural models. With the help of utilizing the shared information of several related tasks, our results outperform most of state-of-the-art models. Although Tree-LSTM outperforms our method on SST-1, it needs an external parser to get the sentence topological structure. It is worth noticing that our models are generic and compatible with the other LSTM based models. For example, we can easily extend our models to incorporate the Tree-LSTM model."
  }, {
    "heading": "5.5 Multi-task Learning of Product Reviews",
    "text": "Table 4 shows the classification accuracies on the tasks of product reviews. The row of “Single Task” shows the results of the baseline for each individual task. With the help of global shared memory (ARC-I), the performances of these four tasks are improved by an average of 2.9%(2.6%) compared with LSTM(ME-LSTM). ARC-II achieves best performances on three sub-tasks, and its average improvement is 3.7%(3.5%). Compared with MT-CNN and MT-DNN, our models achieve a better performance. We think the reason is that our models can not only share lexical information but share complicated patterns of sentences by reading/writing operations of external memory. Furthermore, these results on product reviews are consistent with that on movie reviews, which shows our architectures are robust."
  }, {
    "heading": "5.6 Case Study",
    "text": "To get an intuitive understanding of what is happening when we use shared memory to predict the class of text, we design an experiment to compare and analyze the difference between our models and vanilla LSTM, thereby demonstrating the effectiveness of our proposed architectures.\nWe sample two sentences from the SST-2 validation dataset, and the changes of the predicted sentiment score at different time steps are shown in Figure 3, which are obtained by vanilla LSTM and ARC-I respectively. Additionally, both models are bidirectional for better visualization. To get more insights into how the shared external memory influences the specific task, we plot and observe the evolving activation of fusion gates through time, which controls signals flowing from a shared external memory to task-specific output, to understand the behaviour of neurons.\nFor the sentence “It is a cookie-cutter movie, a cut-and-paste job.”, which has a negative sentiment, while the standard LSTM gives a wrong prediction due to not understanding the informative words “cookie-cutter” and “cut-and-paste”.\nIn contrast, our model makes a correct prediction and the reason can be inferred from the activation of fusion gates. As shown in Figure 3-(c), we can see clearly the neurons are activated much when they take input as “cookie-cutter” and “cut-and-paste”, which indicates much information in shared memory has be passed into LSTM, therefore enabling the model to give a correct prediction.\nAnother case “If you were not nearly moved to tears by a couple of scenes , you ’ve got ice water in your veins”, a subjunctive clause introduced by “if ”, has a positive sentiment.\nAs shown in Figure 3-(b,d), vanilla LSTM failed to capture the implicit meaning behind the sentence, while our model is sensitive to the pattern “If ... were not ...” and has an accurate understanding of the sentence, which indicates the shared memory mechanism can not only enrich the meaning of certain words, but teach some information of sentence structure to specific task."
  }, {
    "heading": "6 Related Work",
    "text": "Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015b; Liu et al., 2016b). In most of these models, the lower layers are shared across all tasks, while top layers are task-specific.\nCollobert and Weston (2008) used a shared representation for input words and solved different traditional NLP tasks within one framework. However, only one lookup table is shared, and the other lookup tables and layers are task-specific.\nLiu et al. (2015b) developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which loses the information of word order.\nMore recently, several multi-task encoder-\ndecoder networks were also proposed for neural machine translation (Dong et al., 2015; Luong et al., 2015; Firat et al., 2016), which can make use of cross-lingual information.\nUnlike these works, in this paper we design two neural architectures with shared memory for multitask learning, which can store useful information across the tasks. Our architectures are relatively loosely coupled, and therefore more flexible to expand. With the help of shared memory, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "In this paper, we introduce two deep architectures for multi-task learning. The difference with the previous models is the mechanisms of sharing information among several tasks. We design an external\nmemory to store the knowledge shared by several related tasks. Experimental results show that our models can improve the performances of several related tasks by exploring common features.\nIn addition, we also propose a deep fusion strategy to integrate the information from the external memory into task-specific LSTM with a fusion gate.\nIn future work, we would like to investigate the other sharing mechanisms of neural network based multi-task learning."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Program of China (No. 2015AA015408)."
  }],
  "year": 2016,
  "references": [{
    "title": "Learning efficient algorithms with hierarchical attentive memory",
    "authors": ["Marcin Andrychowicz", "Karol Kurach."],
    "venue": "arXiv preprint arXiv:1602.03218.",
    "year": 2016
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["D. Bahdanau", "K. Cho", "Y. Bengio."],
    "venue": "ArXiv e-prints, September.",
    "year": 2014
  }, {
    "title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
    "authors": ["John Blitzer", "Mark Dredze", "Fernando Pereira"],
    "venue": "In ACL,",
    "year": 2007
  }, {
    "title": "Multitask learning",
    "authors": ["Rich Caruana."],
    "venue": "Machine learning, 28(1):41–75.",
    "year": 1997
  }, {
    "title": "A fast and accurate dependency parser using neural networks",
    "authors": ["Danqi Chen", "Christopher D Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.",
    "year": 2014
  }, {
    "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
    "authors": ["Ronan Collobert", "Jason Weston."],
    "venue": "Proceedings of ICML.",
    "year": 2008
  }, {
    "title": "Natural language processing (almost) from scratch",
    "authors": ["Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."],
    "venue": "The Journal of Machine Learning Research, 12:2493– 2537.",
    "year": 2011
  }, {
    "title": "Associative long short-term memory",
    "authors": ["Ivo Danihelka", "Greg Wayne", "Benigno Uria", "Nal Kalchbrenner", "Alex Graves."],
    "venue": "CoRR, abs/1602.03032.",
    "year": 2016
  }, {
    "title": "Modelling, visualising and summarising documents with a single convolutional neural network",
    "authors": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas."],
    "venue": "arXiv preprint arXiv:1406.3830.",
    "year": 2014
  }, {
    "title": "Multi-task learning for multiple language translation",
    "authors": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."],
    "venue": "Proceedings of the ACL.",
    "year": 2015
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "The Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Finding structure in time",
    "authors": ["Jeffrey L Elman."],
    "venue": "Cognitive science, 14(2):179–211.",
    "year": 1990
  }, {
    "title": "Multi-way, multilingual neural machine translation with a shared attention mechanism",
    "authors": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv preprint arXiv:1601.01073.",
    "year": 2016
  }, {
    "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520.",
    "year": 2011
  }, {
    "title": "Neural turing machines",
    "authors": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."],
    "venue": "arXiv preprint arXiv:1410.5401.",
    "year": 2014
  }, {
    "title": "Generating sequences with recurrent neural networks",
    "authors": ["Alex Graves."],
    "venue": "arXiv preprint arXiv:1308.0850.",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural computation, 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Convolutional neural network architectures for matching natural language sentences",
    "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."],
    "venue": "Advances in Neural Information Processing Systems.",
    "year": 2014
  }, {
    "title": "An empirical exploration of recurrent network architectures",
    "authors": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "Proceedings of The 32nd International Conference on Machine Learning.",
    "year": 2015
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of ACL.",
    "year": 2014
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "arXiv preprint arXiv:1408.5882.",
    "year": 2014
  }, {
    "title": "Multi-timescale long shortterm memory neural network for modelling sentences and documents",
    "authors": ["PengFei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang."],
    "venue": "Proceedings of the Conference on EMNLP.",
    "year": 2015
  }, {
    "title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval",
    "authors": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang."],
    "venue": "NAACL.",
    "year": 2015
  }, {
    "title": "Deep fusion LSTMs for text semantic matching",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Jifan Chen", "Xuanjing Huang."],
    "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Recurrent neural network for text classification with multi-task learning",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of International Joint Conference on Artificial Intelligence.",
    "year": 2016
  }, {
    "title": "Multi-task sequence to sequence learning",
    "authors": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."],
    "venue": "arXiv preprint arXiv:1511.06114.",
    "year": 2015
  }, {
    "title": "Learning word vectors for sentiment analysis",
    "authors": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of the ACL, pages 142–150.",
    "year": 2011
  }, {
    "title": "Efficient estimation of word representations in vector space",
    "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "arXiv preprint arXiv:1301.3781.",
    "year": 2013
  }, {
    "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
    "authors": ["Bo Pang", "Lillian Lee."],
    "venue": "Proceedings of ACL.",
    "year": 2004
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."],
    "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532–1543.",
    "year": 2014
  }, {
    "title": "Semisupervised recursive autoencoders for predicting sentiment distributions",
    "authors": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."],
    "venue": "Proceedings of EMNLP.",
    "year": 2011
  }, {
    "title": "Semantic compositionality through recursive matrix-vector spaces",
    "authors": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."],
    "venue": "Proceedings of EMNLP, pages 1201–1211.",
    "year": 2012
  }, {
    "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
    "authors": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."],
    "venue": "Proceedings of EMNLP.",
    "year": 2013
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Improved semantic representations from tree-structured long short-term memory networks",
    "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."],
    "venue": "arXiv preprint arXiv:1503.00075.",
    "year": 2015
  }, {
    "title": "Word representations: a simple and general method for semi-supervised learning",
    "authors": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."],
    "venue": "Proceedings of ACL.",
    "year": 2010
  }, {
    "title": "Largercontext language modelling",
    "authors": ["Tian Wang", "Kyunghyun Cho."],
    "venue": "arXiv preprint arXiv:1511.03729. 127",
    "year": 2015
  }],
  "id": "SP:fd39ea962bb30839966b83685c7312bd3810b87e",
  "authors": [{
    "name": "Pengfei Liu",
    "affiliations": []
  }, {
    "name": "Xipeng Qiu",
    "affiliations": []
  }, {
    "name": "Xuanjing Huang",
    "affiliations": []
  }],
  "abstractText": "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.",
  "title": "Deep Multi-Task Learning with Shared Memory for Text Classification"
}