{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1861–1870 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n1861"
  }, {
    "heading": "1 Introduction",
    "text": "Text representation is a crucial problem in most natural language processing (NLP) and information retrieval (IR) tasks. In monolingual IR, early research works mostly use vector space models for query-document semantic matching (Salton et al., 1975), which suffer from the problem of synonymy and polysemy. In order to bridge the lexical gaps, latent semantic models such as latent semantic analysis (LSA) (Deerwester et al., 1990) have been proposed to abstract away from surface text\nforms to approximate semantics. More recently, text representation learned with neural networks is attracting increasing attention of the IR community (Mitra and Craswell, 2017) and positive results have been reported on various evaluation data sets (Fan et al., 2018).\nCompared to the prosperity in monolingual IR, there have been less advancements in CLIR where documents are written in a language different from that of queries. In addition to document ranking, CLIR models need to cross the language barriers, which makes the task intuitively more difficult than monolingual IR. Traditional approaches reduce CLIR to its monolingual counterpart via performing some way of translation on queries or/and documents. The typical translation process is performed with either off-the-shelf machine translation (MT) systems or multilingual dictionaries (Nie, 2010). However, MT based approaches are far from being a suitable solution for solving CLIR problems (refer to detailed analysis in (Zhou et al., 2012)). Dictionary-based approaches suffer from the same problem of lexical gaps as in the monolingual case (Gupta et al., 2017). An efficient cross-language representation is in need for CLIR, which is expected to be able to cross both the language and lexical gaps.\nThe most intuitive idea one can have so as to represent text in cross-language settings is to extend those models in monolingual environment. For instance, we note studies such as the extension of LSA in (Littman et al., 1998), the extension of principle component analysis (PCA) in (Platt et al., 2010), the extension of autoencoder model in (Chandar et al., 2014), and the extension of word2vec (Mikolov et al., 2013) in (Vulić and Moens, 2015). These approaches construct crosslanguage and semantic-rich representation of text, which can be applied to CLIR directly. However, all the models listed here aim to learn general text\nrepresentation where the objective is to capture term proximity rather than relevance that is essential for retrieval task (Zamani and Croft, 2017). A recent work (Gupta et al., 2017) tries to learn taskspecific representation for CLIR. However, their model only captures ranking signals in monolingual settings, which does not necessarily generalize well in CLIR.\nIn this paper, we propose to learn task-specific text representation for CLIR via a novel adversarial learning framework. Following the convention in generative adversarial networks (GAN) (Goodfellow et al., 2014), our representation learning model is realized as an interplay between two processes, an embedding generator (G) and an adversarial discriminator (D), conducted as a minmax game. With the GAN framework, we design three constraints to direct the representation learning process. CLIR is essentially a ranking problem and we develop a matching constraint to make sure that documents can be ranked in the right order given a query in another language. The matching constraint considers both cross-language and monolingual pairwise ranking signals, which is superior to previous studies (e.g., (Gupta et al., 2017)) only considering monolingual matching signals. Meanwhile, a translation constraint is imposed on the latent representation to bridge the language gaps. These two constraints direct the encoding networks to generate a language-invariant and task-specific representation in the embedding space. Lastly, an adversarial constraint is proposed to force both language and source invariant to be reached more efficiently and effectively. Through the joint exploitation of these constraints in an adversarial manner, the embedding space being optimal for CLIR will then result through the convergence of this process. Comprehensive CLIR experiments reveal that our model is superior to state-of-the-art continuous space models and approaches the machine translation and monolingual baselines."
  }, {
    "heading": "2 Related work",
    "text": "Text representation has been a long-standing research question in IR. Classic methods such as vector space model are not able to deal with lexical gaps between queries and documents, resulting in inferior retrieval performance. Latent semantic approaches such as LSA (Deerwester et al., 1990) and latent dirichlet allocation (LDA) (Blei et al.,\n2003) abstract away from surface text forms to alleviate sparsity and approximate semantics. More recently, learning based approaches with neural networks have gained great success in NLP (Baroni et al., 2014) and started to attract increasing interests of the IR community. In terms of word level embedding, word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) are two models that have been cited frequently in recent literature. These two models provide semantic-rich representations to bridge lexical gaps between queries and documents, which have been used broadly in neural IR studies (Ganguly et al., 2015; Zheng and Callan, 2015; Zamani and Croft, 2016).\nThe above studies deal with monolingual text representation, which are related to the crosslanguage models presented below. As for CLIR, typical approaches reduce CLIR to its monolingual counterparts via performing some way of translation. Machine translation systems such as Google translator1 have been widely used to translate queries or documents, which serve as a default and convenient translation option in CLIR. It is however far from being a suitable solution for solving CLIR problems (a detailed analysis can be found in (Zhou et al., 2012)). An alternative solution is to rely on multilingual dictionaries to perform lexicon-level translation, which is mostly in combination with either language modeling strategy (Kraaij et al., 2003) or query structuring framework (Pirkola, 1998). However, dictionary-based methods still suffer from the lexical gap problem which reduces their performance in CLIR.\nIn fact, researchers have extended the models in monolingual settings and developed continuous space models for cross-language tasks to capture rich semantics. These cross-language extensions can be applied to CLIR directly. For instance, Littman et al. (1998) extend LSA to its cross-language version CL-LSA by concatenating document-term matrix of parallel data which acts as dual-language documents to be learned by LSA. Such a methodology leads to a dual-language semantic space in which terms from both languages are represented. Vinokourov et al. (2002) use parallel data to find most likely correlations between projected vectors based on canonical component analysis technique. The OPCA model (Platt et al., 2010) starts with the basic model\n1https://translate.google.com\nPCA that is then made discriminative by encouraging comparable document pairs to have similar vector representation. Compared to CL-LSA, OPCA avoids the use of artificial concatenated documents. More recently, neural models have been employed to learn cross-language representations. For instance, autoencoder is extended to a bilingual version BAE in (Chandar et al., 2014) which learns vectorial word representations from aligned sentences. Yih et al. (2011) develop S2Net to learn a projection matrix to map the corresponding term vectors into a latent space where similar documents are close. S2Net is implemented with Siamese neural network framework. Vulić and Moens (2015) first merge two documents from the aligned document pair in a comparable corpus and then train word2vec on the pseudo-bilingual document to obtain cross-language embeddings. The above approaches learn general text representation that captures term proximity rather than relevance which is important for retrieval task (Zamani and Croft, 2017). A recent work (Gupta et al., 2017) tries to learn task-specific embeddings for CLIR. However, it learns ranking signals by preserving pairwise ranking in monolingual settings prior to a transfer learning process to another language, which does not necessarily generalize well in CLIR.\nOne can find from above analysis that, most existing approaches, either based on neural networks or not, learn general embeddings irrelevant to CLIR. We argue that task-specific embeddings are superior, a fact that is inspired by monolingual IR studies and that will actually be validated by CLIR experiments in this paper. To this end, we will learn cross-language and task-specific embeddings for CLIR via a novel text representation model based on adversarial learning (Goodfellow et al., 2014)."
  }, {
    "heading": "3 Representation learning framework",
    "text": "We will present in this section a neural representation learning framework for CLIR. As discussed before, the framework is realized based on adversarial learning as an interplay between the generator process and the discriminator process. We will develop three constraints, namely a matching constraint, a translation constraint and an adversarial constraint, to direct the learning of cross-language and target-specific text embeddings. For ease of presentation, let us assume in CLIR we have a\nsource language query qs and a target language document dt. The translation of qs in the target language is qt. The learning framework is illustrated in figure 1, which consists of an adversarial network NNadv, three dimension adaptation networks NNdim and three encoding networks respectively for qt, dt and qs."
  }, {
    "heading": "3.1 Text representation networks",
    "text": "There have been various approaches one can use to encode sentences/documents into dense vectors. For instance, models based on convolutional neural networks (Kalchbrenner et al., 2014) and models based on recurrent neural networks (Liu et al., 2016) have been popular choices.\nIn order to map queries and documents into the embedding space, we make use of recurrent neural network with the long short-term memory (LSTM) architecture that can deal with vanishing and exploring gradient problems (Hochreiter and Schmidhuber, 1997). We present here derivation details of LSTM for clarification sake. The LSTM framework consists of several gates to control the cell state in the network. Firstly, a forget gate f (a\nsigmoid layer) functions according to:\nf τ = σ(Wf · [hτ−1, xτ ] + bf )\nThen, an input gate i (a sigmoid layer) and a tanh layer work together as follows:\niτ = σ(Wi · [hτ−1, xτ ] + bi)\nC̃τ = tanh(Wc · [hτ−1, xτ ] + bc)\nWith the forget gate f , the input gate i and the new value C̃, one can update the cell state C as:\nCτ = f τ ∗ Cτ−1 + iτ ∗ C̃τ\nLastly, an output gate o (a sigmoid layer) outputs:\noτ = σ(Wo · [hτ−1, xτ ] + bo) hτ = oτ ∗ tanh(Cτ )\nIn above equations, xτ is the input at time step τ . hτ and hτ−1 denote the hidden states at time steps τ and τ − 1. All W and b are parameters. For brevity, we can write the update process as:\nhτ = LSTM(hτ−1, xτ )\nGiven a text sequence x = (x1, x2, . . . , xl), typical methods take the output hl of LSTM at the last time step l as the concentrated representation of the whole sequence x (Sutskever et al., 2014). Since queries in IR tasks tend to be short and noisy, we make use of Bidirectional LSTM with pooling (Tan et al., 2015) to obtain a more effective text representation from all the hidden states h1:l. The sequence x is fed from left to right into LSTMa and from right to left into LSTMb. The new hidden state hτab at time step τ is obtained by concatenating the hidden states of LSTMa and LSTMb at their respective time step τ . Since max-pooling has been proven to be efficient in similar tasks (Tan et al., 2015), the latent representation zx of x can be formulated as:\nzx = NNdim(MaxPooling(h 1:l ab ))\nwhere x can be qs, qt or dt. NNdim is designed to adapt the output dimension and to allow further flexibility for representation learning."
  }, {
    "heading": "3.2 Matching constraint and Translation constraint",
    "text": "Document ranking is the central problem in both monolingual IR and CLIR tasks. CLIR differs itself from its monolingual counterpart in that the\nlanguage gap needs to be crossed prior to the retrieval process. Since the choice of translation strategies (query, document or both) affects the design of other components in our model, we will discuss the translation constraint in section 3.2.1 prior to matching constraints in sections 3.2.2 and 3.2.3."
  }, {
    "heading": "3.2.1 Translation constraint",
    "text": "The translation constraint is developed to minimize the differences between a pair of parallel texts, which serves as a basic requirement in the translation scenario. Such a constraint directs the learning of language-invariant text representation for CLIR. We follow the arguments in previous studies (Vilares et al., 2016) and choose to translate queries in our model, since it is computationally expensive to translate large-scale document collections in practice. In this paper, we directly employ Google translator to translate queries, which is a popular choice for machine translation that leads to state-of-the-art translation performance. The translation constraint is then imposed on the embedding vectors zqs and zqt of the queries qs and qt. The translation lossLtra on a set QP of query pairs can be defined with the squared L2 norm, which is:\nLtra = ∑\n(qs,qt)∈QP\n‖zqs − zqt‖22"
  }, {
    "heading": "3.2.2 Cross-language matching constraint",
    "text": "The matching constraint captures essential characteristics of cross-language ranking. Following the practice in learning to rank (Liu, 2009), we model document ranking in the pairwise style where the relevance information is in the form of preferences between pairs of documents with respect to individual queries. In the model for CLIR, since we have matching signals from both monolingual text pairs and cross-language text pairs, the model can benefit from complementary knowledge from two resources. The monolingual pairwise matching constraint will be introduced in section 3.2.3.\nSimilar to neural models in monolingual settings (Huang et al., 2013), the cross-language pairwise matching constraint is placed on top of the embedding vectors of source language query and target language documents. In figure 1, let us assume xqs has a relevant document xdt+ and an irrelevant document xdt− according to annotated text pairs. In training, the positive sample xdt+\nfor xqs can be chosen as the most relevant texts according to annotation, and the negative sample xdt− is picked randomly from the data collection. The cross-language matching constraint encourages the hidden representation of xdt+ to be near to the hidden representation of xqs in the semanticrich embedding space. Meanwhile, it asks the hidden representation of xdt− to be far from that of xqs . We follow typical neural IR models and make use of cosine as the distance measure of hidden vectors. The probability that dt+ is ranked higher than dt− given qs can be derived as:\nP̂ (qs) = σ[βc · (cos(zqs , zdt+)− cos(zqs , zdt−))]\nwhere σ is the sigmoid function with a hyperparameter βc controlling its shape. The crosslanguage matching loss Lmatc on cross-language triplet set QDc can be defined with cross-entropy loss as:\nLmatc = ∑\n(qs,dt+,dt−)∈QDc\nCE[P (qs), P̂ (qs)]\nwhere CE denotes the cross-entropy operator between two distributions and P (qs) is the actual counterpart of P̂ (qs) estimated from annotation with a strategy similar to that in (Dehghani et al., 2017)."
  }, {
    "heading": "3.2.3 Monolingual matching constraint",
    "text": "The monolingual matching constraint Lmatm can be built in a way similar to that of Lmatc. Lmatm is imposed on a set QDm of monolingual triplet (qt, dt+, dt−) as:\nLmatm = ∑\n(qt,dt+,dt−)∈QDm\nCE[P (qt), P̃ (qt)]\nwhere P (qt) is the actual counterpart of P̃ (qt) estimated from annotation. P̃ (qt) denotes the probability that dt+ is ranked higher than dt− given qt. It can be computed with the sigmoid function as:\nP̃ (qt) = σ[βm · (cos(zqt , zdt+)− cos(zqt , zdt−))]\nwhere βm is a hyper-parameter."
  }, {
    "heading": "3.2.4 Embedding generator constraint",
    "text": "Since our model is implemented with adversarial framework, we propose to model the representation generator G, which embodies the process of language-invariant and task-specific embedding of queries and documents into a latent\nsubspace, under a combination of three constraints introduced above. The translation constraint aims to guarantee language invariant when translating queries. The cross-language matching constraint explicitly captures cross-language ranking signals from cross-language text pairs. The monolingual matching constraint takes monolingual ranking into account so as to complement the crosslanguage ranking signals.\nCombing the three constraints above, we obtain a comprehensive constraint that should be obeyed by the embedding generator process. With the regularization term Lreg equaling to the sum of Frobenius norms of all weight matrices in the text embedding phase, we can write the embedding generator constraint LG as:\nLG(θG) = γ1 ·Ltra+γ2 ·Lmatc+γ3 ·Lmatm+Lreg\nwhere θG denotes the set of parameters in the generator networks, and γ1, γ2, γ3 are hyperparameters."
  }, {
    "heading": "3.3 Adversarial constraint",
    "text": "We will introduce the adversarial constraint in this part. GAN (Goodfellow et al., 2014) simultaneously trains a generative model G and a disriminative model D in a competing way. G generates samples from a source of noisew that satisfies w ∼ Pn(w) and tries to capture the real data distribution Pr. D learns to distinguish between the generated samples from G and the true data sampled from Pr (in practice, from training data). The training procedure for G is to try its best to fool D. Let us assume that G generates samples satisfying the distribution Pg that is implicitly decided by G(w). The GAN value function V (G,D) on which D and G play the minmax game can be written as:\nmin G max D V (D,G) =Ex∼Pr [logD(x)] (1)\n+ Ex∼Pg [log(1−D(x))]\nTheoretical analysis has indicated that playing the minmax game as above amounts to minimizing the Jensen-Shannon divergence between Pg and Pr.\nWe follow the general idea of GAN and develop an adversarial component on top of the embedding space in figure 1. We note that GAN has been used in representation learning in a similar way as in (Bousmalis et al., 2016; Liu et al., 2017). In our model in figure 1, the adversarial component NNadv acts as the discriminator D\nwhich tries its best to detect whether the embedding vector z is encoded from xqt , xdt or xqs . In this paper, NNadv is implemented as a neural network with a softmax output layer. The output of NNadv then corresponds to a probability distribution vector over the input sources. Let us denote the ground truth label of the current input z to NNadv as lz which indicates the source that z is encoded from. We can adjust equation 1 to our settings and obtain the adversarial loss Ladv on a query set Qt and a document set Dt in the target language, as well as a query set Qs in the source language, which can be written as:\nLadv = min G max D ∑ x∈Qt,Dt,Qs logNNadv(zx) ◦ lzx\nwhere ◦ is the inner product operator."
  }, {
    "heading": "3.4 Training procedure",
    "text": "Following the training convention of GAN (Goodfellow et al., 2014), the process of learning the language-invariant and task-specific text representation for CLIR should be conducted by jointly minimizing the generator constraint LG and the adversarial loss Ladv, which leads us to the combined objective function L as:\nL = LG + Ladv\nAccording to the rule of playing the minmax game in GAN, G tries its best to maximize the probability that D makes a mistake and D tries its best to distinguish between real data and generated data (in our case, various input sources). The theoretical requirement behind GAN that D is maintained near its optimal solution as long as G changes slowly enough motivates us to update the discriminator part k steps per update of the generator part in the iterative optimization process. Based on these discussions, the minmax optimization process can be derived as:\n1. Optimize D when fixing G through: θ̂D = argmaxθD L(θ̂G, θD)\n2. Optimize G when fixing D through: θ̂G = argminθG L(θG, θ̂D)\nThe optimization can be implemented with mini-batch gradient ascent (for θD) and descent (for θG)."
  }, {
    "heading": "4 Experiments and results",
    "text": "In this section, we conduct CLIR experiments so as to compare our text representation model with several other models."
  }, {
    "heading": "4.1 Data sets",
    "text": ""
  }, {
    "heading": "4.1.1 CLIR evaluation sets",
    "text": "To perform CLIR experiments, we rely on broadly used data sets released in the bilingual tasks of the cross-language evaluation forum (CLEF) 2. We choose to use the data from the year 2000 to 2004. Table 1 lists the characteristics of the data set, which include number of documents (Nd), number of distinct words (Nw), the average document length (DLavg) and the number of queries (Nq) in each task. We use source language queries in French (Fr), German (De) and Italian (It) to retrieve target language documents in English (En). Queries from year 2000 to 2002 are combined to a single task in table 1 since they have the same target set."
  }, {
    "heading": "4.1.2 Training set",
    "text": "In order to train the representation learning model, we need to construct a data set consisting of annotated text pairs. We combine AOL queries (Pass et al., 2006) and a set of news titles downloaded from the news sites3 to constitute training query set of diversity. Following the previous work (Gupta et al., 2017), we sample a balanced subset (1M) from such query set and use these queries to retrieve the data collection with BM25. For each training query, we take the top retrieved texts as positive samples, and the negative samples are selected randomly from the data collection. In addition to the pseudo-labeled text pairs of low quality, we combine the LETOR4.0 dataset (Qin and Liu, 2013) that is developed for evaluating learning to rank models. The LETOR4.0 dataset consists of relevance judgments of higher quality compared to\n2http://www.clef-initiative.eu 3We fetch 2.8M web pages from several news websites such as ChinaDaily (www.chinadaily.com.cn) and XinhuaNews (www.xinhuanet.com).\npseudo-labeled data. The two data resources can complement each other in the training process.\nIn our experiments, the pseudo-labeled data is used to train the whole model and the LETOR dataset is employed to fine tune the parameters relevant to the source queries and target documents which are more important for the cross-language retrieval task."
  }, {
    "heading": "4.2 Experimental settings",
    "text": ""
  }, {
    "heading": "4.2.1 Experimental setup",
    "text": "The terms are initialized as the 512d word2vec vectors trained on Wikipedia dump corpus4. The term embeddings are fed into the LSTM model of which the hidden unit number is chosen from {64, 128, 256, 512}. The adversarial network NNadv is as a three-layer feed-forward network with softmax on top of the last layer. NNdim is implemented as a feed-forward network with layer dimension chosen from {32, 64, 128, 256} and hidden layer number chosen from {1, 2}. The values of hyper-parameters γ1, γ2 and γ3 are chosen from {0.01, 0.1, 1, 10, 100}. The learning rate is selected from {10−1, 10−2, 10−3, 10−4, 10−5}. Those hyper-parameters are tuned on the validation set which is 20% of the training queries randomly selected.\nFor evaluation, we present results in terms of mean average precision (MAP). Statistically significant differences between various models are determined using the paired t-test with p < 0.05."
  }, {
    "heading": "4.2.2 Baseline approaches",
    "text": "We make use of three categories of baselines for CLIR experiments.\n1. Monolingual run (MON): a baseline with target language queries that are strictly parallel to source language queries.\n2. Machine translation (MT): a baseline with target-language queries translated by machine translation system from sourcelanguage queries.\n3. Cross-language text representation models: baselines that rely on continuous space models for cross-language text representation. We make use here of S2Net (Yih et al., 2011), BAE (Chandar et al., 2014), and XCNN (Gupta et al., 2017) for the CLIR task.\n4https://dumps.wikimedia.org"
  }, {
    "heading": "4.3 Results and analysis",
    "text": ""
  }, {
    "heading": "4.3.1 Comparisons to state-of-the-art",
    "text": "Table 2 lists the experimental results on CLEF dataset for our model (the column OURS) and all baseline models. There are three data collections and three language pairs, amounting to nine cross-language retrieval tasks. Except the strong baselines MON and MT, our model shows the best overall performance among all CLIR strategies. Indeed, our model outperforms all continuous space baselines (i.e., S2Net, BAE and XCNN) with statistical significance in almost all cases. Our model decreases slightly from the strong MT baseline in most retrieval tasks with only one degradation being significant on 03(De-En). Furthermore, one can find that our model approaches the monolingual baseline very much in all retrieval tasks with all MAP ratios around or over 90%. In our experiments, we have not performed comparisons to CL-LSA (Littman et al., 1998) and its variant OPCA (Platt et al., 2010), because they have been consistently outperformed by other CLIR strategies with a large margin (Schauble and Sheridan, 1997; Nie, 2010; Vulić et al., 2011).\nAmong all continuous space baselines, the most recent model XCNN shows the best performance. XCNN always outperforms linear projection methods S2Net with significance. It also significantly outperforms the non-linear model BAE in all cases. This is coincident with previous conclusions in (Gupta et al., 2017) due to the fact that XCNN learns target-specific representation for CLIR but the other models do not. Our model also tries to learn task-specific representation for CLIR, which significantly outperforms XCNN in most cases according to the results in table 2. The reasons might be that (1) our method is modeled in a more effective adversarial learning framework. (2) we explicitly capture crosslanguage ranking signals in embedding generator in addition to monolingual ranking signals used in XCNN. (3) our model can jointly capture the translation knowledge and document ranking knowledge in a unified framework."
  }, {
    "heading": "4.3.2 Variant of our model",
    "text": "Our model can be customized easily by altering the constraints to direct the representation learning process. Since the specificity of our model comes from the adversarial learning framework that has never been investigated in CLIR, we re-\nmove the constraint Ladv from the original model M and obtain the variantMadv. In this case,Madv can be optimized with standard mini-batch gradient descent approach, without playing the minmax game. We redo above CLIR experiments with the same settings as above and obtain the retrieval results of Madv in table 3.\nFrom the results one can find that when removing the adversarial component from the original model, Madv decreases from the original model M in all retrieval tasks. The differences that are significant appear in 5 out of 9 retrieval tasks. The results demonstrate that learning generator and discriminator in a competing style within the adversarial learning framework leads to representation of higher quality, which eventually supports efficient CLIR. If we compare the variant Madv with the XCNN model in table 2, we find that Madv still performs better than XCNN in most\ncases. Such a comparison implicitly indicates that the joint exploitation of monolingual matching constraint, cross-language matching constraint and translation constraint in a single model is more efficient than using them separately as in the XCNN model."
  }, {
    "heading": "5 Conclusions",
    "text": "In this paper, we propose a novel text representation approach for CLIR based on the adversarial learning framework. The learning framework is implemented as an interplay between an embedding generator process and an adversarial discriminator process, which leads to an optimal representation that is both language invariant and domain specific. The embedding generator is learned such that it explicitly considers both cross-language and monolingual pairwise ranking signals. In this way, it can ensure that the learned embeddings benefit from both sources and are directly optimized for CLIR. To the best of our knowledge, it is the first time adversarial learning has been applied to CLIR. Experiments on various language pairs in CLEF data collection show that our model is significantly better than other latent semantic models for CLIR. Indeed, our model approaches the performance of machine translation and monolingual baselines."
  }, {
    "heading": "Acknowledgments",
    "text": "We thank the anonymous reviewers for their valuable comments. This work was supported by the Fundamental Research Funds for Central Universities of CCNU (No. CCNU15A05062)."
  }],
  "year": 2018,
  "references": [{
    "title": "Don’t count, predict! a systematic comparison of context-counting vs",
    "authors": ["Marco Baroni", "Georgiana Dinu", "Germán Kruszewski."],
    "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for",
    "year": 2014
  }, {
    "title": "Latent dirichlet allocation",
    "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."],
    "venue": "Journal of Machine Learning Research, 3:993–1022.",
    "year": 2003
  }, {
    "title": "Domain separation networks",
    "authors": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."],
    "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS, pages 343–",
    "year": 2016
  }, {
    "title": "An autoencoder approach to learning bilingual word representations",
    "authors": ["A P Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha."],
    "venue": "Proceedings of the 27th International Conference",
    "year": 2014
  }, {
    "title": "Indexing by latent semantic analysis",
    "authors": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman."],
    "venue": "Journal of the American Society for Information Science, 41(6):391–407.",
    "year": 1990
  }, {
    "title": "Neural ranking models with weak supervision",
    "authors": ["Mostafa Dehghani", "Hamed Zamani", "Aliaksei Severyn", "Jaap Kamps", "W. Bruce Croft."],
    "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information",
    "year": 2017
  }, {
    "title": "Modeling diverse relevance patterns in ad-hoc retrieval",
    "authors": ["Yixing Fan", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Chengxiang Zhai", "Xueqi Cheng."],
    "venue": "Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Infor-",
    "year": 2018
  }, {
    "title": "Word embedding based generalized language model for information retrieval",
    "authors": ["Debasis Ganguly", "Dwaipayan Roy", "Mandar Mitra", "Gareth J.F. Jones."],
    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Develop-",
    "year": 2015
  }, {
    "title": "Generative adversarial nets",
    "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."],
    "venue": "Proceedings of the 27th International Conference on Neural Information Processing",
    "year": 2014
  }, {
    "title": "Continuous space models for clir",
    "authors": ["Parth Gupta", "Rafael E. Banchs", "Paolo Rosso."],
    "venue": "Information Processing & Management, 53(2):359 – 370.",
    "year": 2017
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Computation, 9(8):1735–1780.",
    "year": 1997
  }, {
    "title": "Learning deep structured semantic models for web search using clickthrough data",
    "authors": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."],
    "venue": "Proceedings of the 22nd ACM International Conference on Information and",
    "year": 2013
  }, {
    "title": "A convolutional neural network for modelling sentences",
    "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."],
    "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL, pages 655–665.",
    "year": 2014
  }, {
    "title": "Embedding web-based statistical translation models in cross-language information retrieval",
    "authors": ["Wessel Kraaij", "Jian-Yun Nie", "Michel Simard."],
    "venue": "Computational Linguistics, 29(3):381–419.",
    "year": 2003
  }, {
    "title": "Automatic cross-language information retrieval using latent semantic indexing",
    "authors": ["Michael L. Littman", "Susan T. Dumais", "Thomas K. Landauer."],
    "venue": "Cross-Language Information Retrieval, pages 51– 62, Boston, MA. Springer.",
    "year": 1998
  }, {
    "title": "Deep fusion lstms for text semantic matching",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Jifan Chen", "Xuanjing Huang."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL, pages 1034–1043.",
    "year": 2016
  }, {
    "title": "Adversarial multi-task learning for text classification",
    "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL, pages 1–10.",
    "year": 2017
  }, {
    "title": "Learning to rank for information retrieval",
    "authors": ["Tie-Yan Liu."],
    "venue": "Foundations and Trends in Information Retrieval, 3(3):225–331.",
    "year": 2009
  }, {
    "title": "Distributed representations of words and phrases and their compositionality",
    "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."],
    "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems,",
    "year": 2013
  }, {
    "title": "Neural models for information retrieval",
    "authors": ["Bhaskar Mitra", "Nick Craswell."],
    "venue": "CoRR, abs/1705.01509.",
    "year": 2017
  }, {
    "title": "Cross-language information retrieval",
    "authors": ["Jian-Yun Nie."],
    "venue": "Synthesis Lectures on Human Language Technologies, 3(1):1–125.",
    "year": 2010
  }, {
    "title": "A picture of search",
    "authors": ["Greg Pass", "Abdur Chowdhury", "Cayley Torgeson."],
    "venue": "Proceedings of the 1st International Conference on Scalable Information Systems, InfoScale.",
    "year": 2006
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1532–1543.",
    "year": 2014
  }, {
    "title": "The effects of query structure and dictionary setups in dictionary-based cross-language information retrieval",
    "authors": ["Ari Pirkola."],
    "venue": "Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
    "year": 1998
  }, {
    "title": "Translingual document representations from discriminative projections",
    "authors": ["John C. Platt", "Kristina Toutanova", "Wen-tau Yih."],
    "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 251–261.",
    "year": 2010
  }, {
    "title": "A vector space model for automatic indexing",
    "authors": ["G. Salton", "A. Wong", "C.S. Yang."],
    "venue": "Communications of the ACM, 18(11):613–620.",
    "year": 1975
  }, {
    "title": "Crosslanguage information retrieval (clir) track overview",
    "authors": ["Peter Schauble", "Paraic Sheridan."],
    "venue": "Proceedings of the sixth Text REtrieval Conference, TREC.",
    "year": 1997
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Lstmbased deep learning models for non-factoid answer selection",
    "authors": ["Ming Tan", "Bing Xiang", "Bowen Zhou."],
    "venue": "CoRR, abs/1511.04108.",
    "year": 2015
  }, {
    "title": "On the feasibility of character n-grams pseudo-translation for cross-language information retrieval tasks",
    "authors": ["Jesus Vilares", "Manuel Vilares", "Miguel A. Alonso", "Michael P. Oakes."],
    "venue": "Computer Speech and Language, 36(C):136–164.",
    "year": 2016
  }, {
    "title": "Inferring a semantic representation of text via cross-language correlation analysis",
    "authors": ["Alexei Vinokourov", "John Shawe-Taylor", "Nello Cristianini."],
    "venue": "Proceedings of the 15th International Conference on Neural Information Processing Systems, NIPS,",
    "year": 2002
  }, {
    "title": "Cross-language information retrieval with latent topic models trained on a comparable corpus",
    "authors": ["Ivan Vulić", "Wim De Smet", "Marie-Francine Moens."],
    "venue": "Proceedings of the 7th Asia Conference on Information Retrieval Technology, AIRS, pages 37–48.",
    "year": 2011
  }, {
    "title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings",
    "authors": ["Ivan Vulić", "Marie-Francine Moens."],
    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information",
    "year": 2015
  }, {
    "title": "Learning discriminative projections for text similarity measures",
    "authors": ["Wen-tau Yih", "Kristina Toutanova", "John C. Platt", "Christopher Meek."],
    "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL, pages 247–",
    "year": 2011
  }, {
    "title": "Embedding-based query language models",
    "authors": ["Hamed Zamani", "W. Bruce Croft."],
    "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval, ICTIR, pages 147–156.",
    "year": 2016
  }, {
    "title": "Relevancebased word embedding",
    "authors": ["Hamed Zamani", "W. Bruce Croft."],
    "venue": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR, pages 505–514.",
    "year": 2017
  }, {
    "title": "Learning to reweight terms with distributed representations",
    "authors": ["Guoqing Zheng", "Jamie Callan."],
    "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR, pages 575–584.",
    "year": 2015
  }, {
    "title": "Translation techniques in cross-language information retrieval",
    "authors": ["Dong Zhou", "Mark Truran", "Tim Brailsford", "Vincent Wade", "Helen Ashman."],
    "venue": "ACM Computing Surveys, 45(1):1:1–1:44.",
    "year": 2012
  }],
  "id": "SP:9485df66c8abb600ef57ed8911f78403bc86e0e7",
  "authors": [{
    "name": "Bo Li",
    "affiliations": []
  }, {
    "name": "Ping Cheng",
    "affiliations": []
  }],
  "abstractText": "The existing studies in cross-language information retrieval (CLIR) mostly rely on general text representation models (e.g., vector space model or latent semantic analysis). These models are not optimized for the target retrieval task. In this paper, we follow the success of neural representation in natural language processing (NLP) and develop a novel text representation model based on adversarial learning, which seeks a task-specific embedding space for CLIR. Adversarial learning is implemented as an interplay between the generator process and the discriminator process. In order to adapt adversarial learning to CLIR, we design three constraints to direct representation learning, which are (1) a matching constraint capturing essential characteristics of cross-language ranking, (2) a translation constraint bridging language gaps, and (3) an adversarial constraint forcing both language and source invariant to be reached more efficiently and effectively. Through the joint exploitation of these constraints in an adversarial manner, the underlying cross-language semantics relevant to retrieval tasks are better preserved in the embedding space. Standard CLIR experiments show that our model significantly outperforms state-of-the-art continuous space models and approaches the strong machine translation and monolingual baselines.",
  "title": "Learning Neural Representation for CLIR with Adversarial Framework"
}