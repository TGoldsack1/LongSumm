{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2333–2343 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n2333"
  }, {
    "heading": "1 Introduction",
    "text": "A good negotiator needs to decide on the strategy for achieving a certain goal (e.g., proposing $6000) and the realization of that strategy via generation of natural language (e.g., “I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”).\nMost past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuayáhuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy\nand language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016).\nTo alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language generator. Our framework consists of three components shown in Figure 1: First, the parser identifies keywords and entities to map each utterance to a coarse dialogue act capturing the highlevel strategic move. Then, the dialogue manager chooses a responding dialogue act based on a sequence-to-sequence model over coarse dialogue acts learned from parsed training dialogues. Finally, the generator produces an utterance given the dialogue act and the utterance history.\nOur framework follows that of traditional goaloriented dialogue systems (Young et al., 2013), with one important difference: coarse dialogue acts are not intended to and cannot capture the full meaning of an utterance. As negotiation dialogues are fairly open-ended, the generator needs to depend on the full utterance history. For example, consider the first turn in Figure 1. We cannot generate a response given only the dialogue act inform; we must also look at the previous question. However, we still optimize the dialogue manager in the coarse dialogue act space using supervised learning, reinforcement learning, or domain-\nspecific knowledge. Existing human-human negotiation datasets are grounded in closed-domain games with a fixed set of objects such as Settlers of Catan (lumber, coal, brick, wheat, and sheep) (Afantenos et al., 2012; Asher et al., 2016) or item division (book, hat, and ball) (DeVault et al., 2015; Lewis et al., 2017). These objects lack the richness of the real world. To study human negotiation in more open-ended settings that involve real goods, we scraped postings of items for sale from craigslist.org as our negotiation scenario. By hiring workers on Amazon Mechanical Turk (AMT) to play the role of buyers and sellers, we collected a new dataset (CRAIGSLISTBARGAIN) of negotiation dialogues.1 Compared to existing datasets, our more realistic scenario invites richer negotiation behavior involving open-ended aspects such as cheap talk or side offers.\nWe evaluate two families of systems modeling coarse dialogue acts and words respectively, which are optimized by supervised learning, reinforcement learning, or domain knowledge. Each system is evaluated on our new CRAIGSLISTBARGAIN dataset and the DEALORNODEAL dataset of Lewis et al. (2017) by asking AMT workers to chat with the system in an A/B testing setting. We focus on two metrics: task-specific scores (e.g., utility) and human-likeness. We show that reinforcement learning on coarse dialogue acts avoids\n1 Available at https://stanfordnlp.github. io/cocoa.\ndegenerate solutions, which was a problem in Li et al. (2016); Lewis et al. (2017). Our modular model maintains reasonable human-like behavior while still optimizes the objective. Furthermore, we find that models trained over coarse dialogue acts are stronger negotiators (even with only supervised learning) and produce more diverse utterances than models trained over words. Finally, the interpretability of coarse dialogue acts allows system developers to combine the learned dialogue policy with hand-coded rules, thus imposing stronger control over the desired strategy."
  }, {
    "heading": "2 Craigslist Negotiation Dataset",
    "text": "Previous negotiation datasets were collected in the context of games. For example, Asher et al. (2016) collected chat logs from online Settlers of Catan. Lewis et al. (2017) asked two people to divide a set of hats, books, and balls. While such games are convenient for grounding and evaluation, it restricts the dialogue domain and the richness of the language. Most utterances are direct offers such as “has anyone got wood for me?” and “I want the ball.”, whereas real-world negotiation would involve more information gathering and persuasion.\nTo encourage more open-ended, realistic negotiation, we propose the CRAIGSLISTBARGAIN task. Two agents are assigned the role of a buyer and a seller; they are asked to negotiate the price of an item for sale on Craigslist given a description and photos. As with the real platform, the listing price is shown to both agents. We addition-\nally suggest a private price to the buyer as a target. Agents chat freely in alternating turns. Either agent can enter an offer price at any time, which can be accepted or rejected by the partner. Agents also have the option to quit, in which case the task is completed with no agreement.\nTo generate the negotiation scenarios, we scraped postings on sfbay.craigslist.org from the 6 most popular categories (housing, furniture, cars, bikes, phones, and electronics). Each posting produces three scenarios with the buyer’s target prices at 0.5x, 0.7x and 0.9x of the listing price. Statistics of the scenarios are shown in Table 2.\nWe collected 6682 human-human dialogues on AMT using the interface shown in Appendix A\nFigure 2. The dataset statistics in Table 3 show that CRAIGSLISTBARGAIN has longer dialogues and more diverse utterances compared to prior datasets. Furthermore, workers were encouraged to embellish the item and negotiate side offers such as free delivery or pick-up. This highly relatable scenario leads to richer dialogues such as the one shown in Table 1. We also observed various persuasion techniques listed in Table 4 such as embellishment, side offers, and appeals to sympathy."
  }, {
    "heading": "3 Approach",
    "text": ""
  }, {
    "heading": "3.1 Motivation",
    "text": "While end-to-end neural models have made promising progress in dialogue systems (Wen et al., 2017a; Dhingra et al., 2017), we find they\nstruggle to simultaneously learn the strategy and the rich utterances necessary to succeed in the CRAIGSLISTBARGAIN domain, e.g., Table 8(a) shows a typical dialogue between a human and a sequence-to-sequence-based bot, where the bot easily agrees. We wish to now separate negotiation strategy and language generation. Suppose the buyer says: “All right. Well I think 275 is a little high for a 10 year old TV. Can you lower the price some? How about 150?” We can capture the highest-order bit with a coarse dialogue act propose(price=150). Then, to generate the seller’s response, the agent can first focus on this coarse\ndialogue act rather than having to ingest the freeform text all at once. Once a counter price is decided, the rest is open-ended justification for the proposed price, e.g., emphasizing the quality of the TV despite its age.\nMotivated by these observations, we now describe a modular framework that extracts coarse dialogue acts from utterances, learns to optimize strategy in the dialogue act space, and uses retrieval to fill in the open-ended parts conditioned on the full dialogue history."
  }, {
    "heading": "3.2 Overview",
    "text": "Our goal is to build a dialogue agent that takes the dialogue history, i.e. a sequence of utterances x1, . . . , xt 1 along with the dialogue scenario c (e.g., item description), and produces a distribution over the responding utterance xt.\nFor each utterance xt (e.g., “I am willing to pay $15”), we define a coarse dialogue act zt (e.g., propose(price=15)); the coarse dialogue act serves as a logical skeleton which does not attempt to capture the full semantics of the utterance. Following the strategy of traditional goal-oriented dialogue systems (Young et al., 2013), we broadly define our model in terms of the following three modules:\n1. A parser that (deterministically) maps an input utterance xt 1 into a coarse dialogue act zt 1 given the dialogue history x<t and z<t, as well as the scenario c.\n2. A manager that predicts the responding dialogue act zt given past coarse dialogue acts z<t and the scenario c.\n3. A generator that turns the coarse dialogue act zt to a natural language response xt given the full dialogue history x<t.\nBecause coarse dialogue acts do not capture the full semantics, the parser and the generator maintains full access to the dialogue history. The main\nrestriction is the manager examining the dialogue acts, which we show will reduce the risk of degeneracy during reinforcement learning Section 4.4. We now describe each module in detail (Figure 1)."
  }, {
    "heading": "3.3 Parser",
    "text": "Our framework is centered around the coarse dialogue act z, which consists of an intent and a set of arguments. For example, “I am willing to pay $15” is mapped to propose(price=15). The fact that our coarse dialogue acts do not intend to capture the full semantics of a sentence allows us to use a simple rule-based parser. It detects the intent and its arguments by regular expression matching and a few if-then rules. Our parser starts by detecting entities (e.g., prices, objects) and matching keyword patterns (e.g., “go lower”). These signals are checked against an ordered list of rules, where we choose the first matched intent in the case of multiple matches. An unknown act is output if no rule is triggered. The list of intent parsing rules used are shown in Table 5. Please refer to Appendix B for argument parsing based on entity detection."
  }, {
    "heading": "3.4 Manager",
    "text": "The dialogue manager decides what action zt the dialogue agent should take at each time step t given the sequence of past coarse dialogue acts z<t and the scenario c. Below, we describe three ways to learn the dialogue manager with increasing controllability: modeling human behavior in the training corpus (supervised learning), explicitly optimizing a reward function (reinforcement learning), and injecting hand-coded rules (hybrid policy).\nSupervised learning. Given a parsed training corpus, each training example is a sequence of coarse dialogue acts over one dialogue, z1, . . . , zT . We learn the transition probabilities\np✓(zt | z<t, c) by maximizing the likelihood of the training data.\nWe use a standard sequence-to-sequence model with attention. Each coarse dialogue act is represented as a sequence of tokens, i.e. an intent followed by each of its arguments, e.g., “o↵er 150”. During the agent’s listening turn, an LSTM encodes the received coarse dialogue act; during its speaking turn, another LSTM decodes the tokens in the coarse dialogue act. The hidden states are carried over the entire dialogue to provide full history.\nThe vocabulary of coarse dialogue acts is much smaller than the word vocabulary. For example, our implementation includes fewer than 10 intents and argument values are normalized and binned (see Section 4.2).\nReinforcement learning. Supervised learning aims to mimic the average human behavior, but sometimes we want to directly optimize for a particular dialogue goal. In reinforcement learning, we define a reward R(z1:T ) on the entire sequence of coarse dialogue acts. Specifically, we experiment with three reward functions:\n• Utility is the objective of a self-interested agent. For CRAIGSLISTBARGAIN, we set the utility function to be a linear function of the final price, such that the buyer has a utility of\n1 at their target price, the seller has a utility of 1 at the listing price, and both agents have a utility of zero at the midpoint of the listing price and the buyer’s target price, making it a zero-sum game. For DEALORNODEAL, utility is the total value of objects given to the agent.\n• Fairness aims to achieve equal outcome for both agents, i.e. the difference between two agents’ utilities.\n• Length is the number of utterances in a dialogue, thus encourages agents to chat as long as possible.\nThe reward is 1 if no agreement is reached. We use policy gradient (Williams, 1992) for optimization. Given a sampled trajectory z1:T and the final reward r, let ai be the i-th generated token (i.e. “action” taken by the policy) along the trajectory. We update the parameters ✓ by\n✓ ✓ ⌘ X\ni\nr✓ log p✓(ai | a<i, c)(r b) (1)\nwhere ⌘ is the learning rate and b is a baseline estimated by the average return so far for variance reduction.\nHybrid policy. Given the interpretable coarse dialogue acts, a simple option is to write a rulebased manager with domain knowledge, e.g., if zt 1 = greet, then zt = greet. We combine these rules with a learned manager to fine-tune the dialogue policy. Specifically, the dialogue manager predicts the intent from a learned sequence model but fills in the arguments (e.g., price) using rules. For example, given a predicted intent propose, we can set the price to be the average of the buyer’s and seller’s current proposals (a split-thedifference strategy)."
  }, {
    "heading": "3.5 Generator",
    "text": "We use retrieval-based generation to condition on both the coarse dialogue act and the dialogue history. Each candidate in our database for retrieval is a tuple of an utterance xt and its dialogue context xt 1, represented by both templates and coarse dialogue acts. i.e. (d(xt 1), zt 1, d(xt), zt), where d is the template extractor. Specifically, given a parsed training set, each utterance is converted to a template by delexicalizing arguments in its coarse dialogue act. For example, “How about $150?”\nbecomes “How about [price]?”, where [price] is a placeholder to be filled in at generation time.\nAt test time, given zt from the dialogue manager, the generator first retrieves candidates with the same intent as zt and zt 1. Next, candidates are ranked by similarity between their context templates and the current dialogue context. Specifically, we represent the context d(xt 1) as a TF-IDF weighted bag-of-words vector and similarity is computed by a dot product of two context vectors. To encourage diversity, the generator samples an utterance from the top K candidates according to the distribution given by a trigram language model estimated on the training data."
  }, {
    "heading": "4 Experiments",
    "text": ""
  }, {
    "heading": "4.1 Tasks",
    "text": "We test our approach on two negotiation tasks. CRAIGSLISTBARGAIN (Section 2) asks a buyer and a seller to negotiate the price of an item for sale given its Craigslist post. DEALORNODEAL (Lewis et al., 2017) asks two agents to divide a set of items given their private utility functions."
  }, {
    "heading": "4.2 Models",
    "text": "We compare two families of models: end-to-end neural models that directly map the input dialogue context to a sequence of output words, and our modular models that use coarse dialogue acts as the intermediate representation.\nWe start by training the word-based model and the act-based model with supervised learning (SL).\n• SL(word): a sequence-to-sequence model with attention over previous utterances and the scenario, both embedded as a continuous Bag-of-Words;\n• SL(act): our model described in Section 3 with a rule-based parser, a learned neural dialogue manager, and a retrieval-based generator.\nTo handle the large range of argument values (prices) in CRAIGSLISTBARGAIN for act-based models, we normalize the prices such that an agent’s target price is 1 and the bottomline price is 0. For the buyer, the target is given and the bottomline is the listing price. For the seller, the target is the listing price and the bottomline is set to 0.7x of the listing price. The prices are then\nbinned according to their approximate values with two digits after the decimal point.\nNext, given the pretrained SL models, we fine-tune them with the three reward functions (Section 3.4), producing RLutility, RLfairness, and RLlength.\nIn addition, we compare with the hybrid model, SL(act)+rule. It predicts the next intent using a trigram language model learned over intent sequences in the training data, and fills in the arguments with hand-coded rules. For CRAIGSLISTBARGAIN, the only argument is the price. The agent always splits the difference when making counter proposals, rejects an offer if it is worse than its bottomline and accepts otherwise. For DEALORNODEAL, the agent maintains an estimate of the partner’s private utility function. In case of disagreement, it gives up the item with the lowest value of (own utility partner utility) and takes an item of estimated zero utility to the partner. The agent agrees whenever a proposal is better than the last one or its predefined target. A high-level comparison of all models is shown in Table 6."
  }, {
    "heading": "4.3 Training Details",
    "text": "CRAIGSLISTBARGAIN For SL(word), we use a sequence-to-sequence model with attention over 3 previous utterances and the negotiation scenario (embedded as a continuous Bag-of-Words). For both SL(word) and SL(act), we use 300- dimensional word vectors initialized by pretrained GloVe word vectors (Pennington et al., 2014), and a two-layer LSTM with 300 hidden units for both the encoder and the decoder. Parameters are initialized by sampling from a uniform distribution between -0.1 and 0.1. For optimization, we use AdaGrad (Duchi et al., 2010) with a learning rate of 0.01 and a mini-batch size of 128. We train the model for 20 epochs and choose the model with the lowest validation loss.\nFor RL, we first fit a partner model using supervised learning (e.g., SL(word)), then run RL\noptimization objective. For each group of RL models, the column of the optimization objective is highlighted. For human-likeness, scores that are better than others in the same group with statistical significance (p < 0.05 given by paired t-tests) are in bold. Overall, with SL, all models are human-like, however, act-based models better matches human statistics across all metrics; with RL, word-based models becomes degenerate, whereas act-based models optimize the reward while maintaining human-likeness.\nagainst it. One agent is updated by policy gradient and the partner model is fixed during training. We use a learning rate of 0.001 and train for 5000 episodes (dialogues). The model with the highest reward on the validation set is chosen.\nDEALORNODEAL For act-based models, we use the same parameterization as CRAIGSLISTBARGAIN. For word-based models, we use the implementation from Lewis et al. (2017).2 Note that for fair comparison, we did not apply SL interleaving during RL training and rollouts during inference."
  }, {
    "heading": "4.4 Human Evaluation",
    "text": "We evaluated each system on two metrics: taskspecific scores (e.g., utility) and human-likeness. The scores tell us how well the system is playing the game, and human-likeness tells us whether the bot deviates from human behavior, presumably due to over-optimization.\nWe put up all 9 systems online and hired workers from AMT to chat with the bots. Each worker was randomly paired with one of the bots or another worker, so as to compare the bots with human performance under the same conditions. At\n2https://github.com/facebookresearch/ end-to-end-negotiator\nthe end of a chat, workers were asked the question “Do you think your partner demonstrated reasonable human behavior?”. They provided answers on a Likert scale from 1 (not at all) to 5 (definitely). Table 7 shows the human evaluation results on CRAIGSLISTBARGAIN and DEALORNODEAL respectively. We also show example human-bot dialogues in Table 8 and Appendix C.\nSL(act) learns more human-like behavior. We first compare performance of SL models over words and coarse dialogue acts. Both SL(word) and SL(act) achieved similar scores on humanlikeness (no statistically significant difference). However, SL(word) better matched human statistics such as dialogue length and utility. For instance, SL(word) tended to produce short, generic utterances as shown in Table 8(a); they also agreed on a deal more quickly because utterances such as “deal” and “I can do that” are frequent in negotiation dialogues. This behavior is reflected by the shorter dialogue length and lower utility of SL(word) models.\nRL(word) leads to degeneracy. On CRAIGSLISTBARGAIN, all RL(word) models clearly have low scores on human-likeness in Table 7. They merely learned to repeat a few sentences: The three most frequent\nsentences of RLutility(word), RLfairness(word), and RLlength(word) account for 81.6%, 100% and 100% of all utterances. For example, RLutility(word) almost always opened with “i can pick it up”, then offer its target price. RLlength(word) repeated generic sentences until the partner submitted a price. While they scored high on the reward being optimized, the conversations are unnatural.\nOn DEALORNODEAL, we have observed similar patterns. A general strategy learned by RL(word) was to pick an offer depending on its objective, then repeat the same utterance over and over again (e.g., “i need the ball.”), resulting in low human-likeness scores. One exception is RLfairness(word), since most of its offers were reasonable and agreed on immediately (it has the shorted dialogue length), the conversations are natural.\nRL(act) optimizes different negotiation goals while being human-like. On both tasks, RL(act) models optimized their rewards while maintaining reasonable human-likeness scores. We now show that different models demonstrated different negotiation behavior. Two main strategies learned by RLlength(act) were to ask questions and to postpone offer submission. On CRAIGSLISTBARGAIN, when acting as a buyer, 42.4% of its utterances were questions, compared to 30.2% for other models. On both tasks, it tended to wait for the partner to submit an offer (even after a deal was agreed on), compared to RLmargin(act) which almost always submitted offers first. For RLfairness(act), it aimed to agree on a price in the middle of the listing price and the buyer’s target price for CRAIGSLISTBARGAIN. Since the buyer’s target was hidden, when the agent was the seller, it tended to wait for the buyer to propose prices first. Similary, on DEALORN-\nODEAL it waited to hear the parter’s offer and sometimes changed its offer afterwards, whereas the other models often insisted on one offer.\nOn both tasks, RLutility(act) learned to insist on its offer and refuse to budge. This ended up frustrating many people, which is why it has a low agreement rate. The problem is that our human model is simply a SL model trained on humanhuman dialogues, which may not accurately reflects real human behavior during human-bot chat. For example, the SL model often agrees after a few turns of insistence on a proposal, whereas humans get annoyed if the partner is not willing to make compromises at all. However, by injecting domain knowledge to SL(act)+rule, e.g., making a small compromise is better than stubbornly being fixed on a single price, we were able to achieve high utility and human-likeness on both CRAIGSLISTBARGAIN and DEALORNODEAL."
  }, {
    "heading": "5 Related Work and Discussion",
    "text": "Recent work has explored the space between goal-oriented dialogue and open-domain chit-chat through collaborative or competitive language games, such as collecting cards in a maze (Potts, 2012), finding a mutual friend (He et al., 2017), or splitting a set of items (DeVault et al., 2015; Lewis et al., 2017). Our CRAIGSLISTBARGAIN dialogue falls in this category, but exhibits richer and more diverse language than prior datasets. Our dataset calls for systems that can handle both strategic decision-making and open-ended text generation.\nTraditional goal-oriented dialogue systems build a pipeline of modules (Young et al., 2013; Williams et al., 2016). Due to the laborious dialogue state design and annotation, recent work has been exploring ways to replace these modules with neural networks and end-to-end training while still having a logical backbone (Wen et al., 2017a; Bordes and Weston, 2017; He et al., 2017). Our work is closely related to the Hybrid Code Network (Williams et al., 2017), but the key difference is that Williams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diver-\nsity, it does not provide controllability over the dialogue strategy.\nOur work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining. Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012).\nTo conclude, we have introduced CRAIGSLISTBARGAIN, a rich dataset of human-human negotiation dialogues. We have also presented a modular approach based on coarse dialogue acts that models a rough strategic backbone as well allowing for open-ended generation. We hope this work will spur more research in hybrid approaches that can work in open-ended, goal-oriented settings.\nAcknowledgments. This work is supported by DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462. We thank members of the Stanford NLP group for insightful discussion and the anonymous reviewers for constructive feedback.\nReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform: https: //worksheets.codalab.org/worksheets/ 0x453913e76b65495d8b9730d41c7e0a0c/."
  }],
  "year": 2018,
  "references": [{
    "title": "Modelling strategic conversation: Model, annotation design and corpus",
    "authors": ["S. Afantenos", "N. Asher", "F. Benamara", "A. Cadilhac", "C. Dégremont", "P. Denis", "M. Guhe", "S. Keizer", "A. Lascarides", "O. Lemon"],
    "venue": "In Proceedings of SemDial",
    "year": 2012
  }, {
    "title": "Discourse structure and dialogue acts in multiparty dialogue: the STAC corpus",
    "authors": ["N. Asher", "J. Hunter", "M. Morey", "F. Benamara", "S. Afantenos."],
    "venue": "Language Resources and Evaluation Conference (LREC).",
    "year": 2016
  }, {
    "title": "Learning end-to-end goal-oriented dialog",
    "authors": ["A. Bordes", "J. Weston."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2017
  }, {
    "title": "Negotiation Games: Applying Game Theory to Bargaining and Arbitration",
    "authors": ["S.J. Brams."],
    "venue": "Psychology Press.",
    "year": 2003
  }, {
    "title": "Latent variable dialogue models and their diversity",
    "authors": ["K. Cao", "S. Clark."],
    "venue": "European Association for Computational Linguistics (EACL).",
    "year": 2017
  }, {
    "title": "Emergent communication through negotiation",
    "authors": ["K. Cao", "A. Lazaridou", "M. Lanctot", "J.Z. Leibo", "K. Tuyls", "S. Clark."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2018
  }, {
    "title": "Strategic dialogue management via deep reinforcement learning",
    "authors": ["H. Cuayáhuitl", "S. Keizer", "O. Lemon."],
    "venue": "Advances in Neural Information Processing Systems (NIPS).",
    "year": 2015
  }, {
    "title": "Toward natural turn-taking in a virtual human negotiation agent",
    "authors": ["D. DeVault", "J. Mell", "J. Gratch."],
    "venue": "Association for the Advancement of Artificial Intelligence (AAAI).",
    "year": 2015
  }, {
    "title": "End-to-end reinforcement learning of dialogue agents for information access",
    "authors": ["B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y. Chen", "F. Ahmed", "L. Deng."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["J. Duchi", "E. Hazan", "Y. Singer."],
    "venue": "Conference on Learning Theory (COLT).",
    "year": 2010
  }, {
    "title": "Learning noncooperative dialogue behaviours",
    "authors": ["I. Efstathiou", "O. Lemon."],
    "venue": "Special Interest Group on Discourse and Dialogue (SIGDIAL).",
    "year": 2014
  }, {
    "title": "A cultural decision-making model for negotiation based on inverse reinforcement learning",
    "authors": ["N. Elnaz", "G. Kallirroi", "T. David."],
    "venue": "The Annual Meeting of the Cognitive Science Society.",
    "year": 2012
  }, {
    "title": "Learning mixed initiative dialog strategies by using reinforcement learning on both conversants",
    "authors": ["M.S. English", "P.A. Heeman."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2005
  }, {
    "title": "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings",
    "authors": ["H. He", "A. Balakrishnan", "M. Eric", "P. Liang."],
    "venue": "Association for Computational Linguistics (ACL), pages 1766–1776.",
    "year": 2017
  }, {
    "title": "Reinforcement learning in multi-party trading dialog",
    "authors": ["T. Hiraoka", "K. Georgila", "E. Nouri", "D. Traum."],
    "venue": "Special Interest Group on Discourse and Dialogue (SIGDIAL).",
    "year": 2015
  }, {
    "title": "Evaluating persuasion strategies and deep reinforcement learning methods for negotiation dialogue agents",
    "authors": ["S. Keizer", "M. Guhe", "H. Cuayahuitl", "I. Efstathiou", "K. Engelbrecht", "M. Dobre", "A. Lascarides", "O. Lemon."],
    "venue": "European Association for",
    "year": 2017
  }, {
    "title": "Deal or no deal? end-to-end learning for negotiation dialogues",
    "authors": ["M. Lewis", "D. Yarats", "Y.N. Dauphin", "D. Parikh", "D. Batra."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2017
  }, {
    "title": "Deep reinforcement learning for dialogue generation",
    "authors": ["J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky", "M. Galley", "J. Gao."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2016
  }, {
    "title": "Adversarial learning for neural dialogue generation",
    "authors": ["J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky."],
    "venue": "arXiv preprint arXiv:1701.06547.",
    "year": 2017
  }, {
    "title": "Training end-to-end dialogue systems with the ubuntu dialogue corpus",
    "authors": ["R.T. Lowe", "N. Pow", "I. Serban", "L. Charlin", "C. Liu", "J. Pineau."],
    "venue": "Dialogue and Discourse, 8.",
    "year": 2017
  }, {
    "title": "GloVe: Global vectors for word representation",
    "authors": ["J. Pennington", "R. Socher", "C.D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Goal-driven answers in the Cards dialogue corpus",
    "authors": ["C. Potts."],
    "venue": "Proceedings of the 30th West Coast Conference on Formal Linguistics, pages 1–20.",
    "year": 2012
  }, {
    "title": "A neural network approach to context-sensitive generation of conversational responses",
    "authors": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan."],
    "venue": "North American Association for Computational Linguis-",
    "year": 2015
  }, {
    "title": "Reinforcement learning of cooperative persuasive dialogue policies using framing",
    "authors": ["H. Takuya", "N. Graham", "S. Sakriani", "T. Tomoki", "N. Satoshi."],
    "venue": "International Conference on Computational Linguistics (COLING).",
    "year": 2014
  }, {
    "title": "Multi-party, multi-issue, multistrategy negotiation for multi-modal virtual agents",
    "authors": ["D. Traum", "S.C. Marsella", "J. Gratch", "J. Lee", "A. Hartholt."],
    "venue": "International Workshop on Intelligent Virtual Agents, pages 117–130.",
    "year": 2008
  }, {
    "title": "A network-based end-to-end trainable task-oriented dialogue system",
    "authors": ["T. Wen", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "P. Su", "S. Ultes", "D. Vandyke", "S. Young."],
    "venue": "European Association for Computational Linguistics (EACL), pages 438–449.",
    "year": 2017
  }, {
    "title": "Latent intention dialogue models",
    "authors": ["T. Wen", "Y. Miao", "P. Blunsom", "S. Young."],
    "venue": "International Conference on Machine Learning (ICML).",
    "year": 2017
  }, {
    "title": "Hybrid code networks: Practical and efficient end-toend dialog control with supervised and reinforcement learning",
    "authors": ["J.D. Williams", "K. Asadi", "G. Zweig."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2017
  }, {
    "title": "The dialog state tracking challenge series: A review",
    "authors": ["J.D. Williams", "A. Raux", "M. Henderson."],
    "venue": "Dialogue and Discourse, 7.",
    "year": 2016
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["R.J. Williams."],
    "venue": "Machine learning, 8(3):229–256.",
    "year": 1992
  }, {
    "title": "POMDP-based statistical spoken dialog systems: A review",
    "authors": ["S. Young", "M. Gašić", "B. Thomson", "J.D. Williams."],
    "venue": "Proceedings of the IEEE, 5, pages 1160–1179.",
    "year": 2013
  }, {
    "title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
    "authors": ["T. Zhao", "R. Zhao", "M. Eskenazi."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2017
  }],
  "id": "SP:d9cdd620de0d00800ec28760476267e5dfa76c2e",
  "authors": [{
    "name": "He He",
    "affiliations": []
  }, {
    "name": "Derek Chen",
    "affiliations": []
  }, {
    "name": "Anusha Balakrishnan",
    "affiliations": []
  }, {
    "name": "Percy Liang",
    "affiliations": []
  }],
  "abstractText": "We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $50) and the execution of that strategy (e.g., generating “The bike is brand new. Selling for just $50!”). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.",
  "title": "Decoupling Strategy and Generation in Negotiation Dialogues"
}