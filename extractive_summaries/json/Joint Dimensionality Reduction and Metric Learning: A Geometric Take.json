{
  "sections": [{
    "heading": "1. Introduction",
    "text": "“To make it tractable for the distance metric learning algorithms we perform dimensionality reduction by PCA to a 100 dimensional subspace” (Koestinger et al., 2012). “Like most of the metric learning methods we first center the dataset and reduce the dimensionality to a n-dimensional space by PCA” (Bohné et al., 2014). “ITML and LDML are intractable when using 600 PCA dimensions” (Guillaumin et al., 2009).\nThese quotations, extracted from the metric learning literature, give rise to a simple question: Is PCA, or, more generally, dimensionality reduction, a must to make metric learning work on high-dimensional data, such as that in computer vision problems?\nTo quantify this, in the top portion of Table 1, we provide the area under the ROC curve of state-of-the-art metric learning techniques applied to the ASLAN dataset (KliperGross et al., 2012) using various PCA dimensions (denoted\n1Data61, CSIRO, Canberra, Australia 2Australian National University, Canberra, Australia 3CVLab, EPFL, Switzerland. Correspondence to: Mehrtash Harandi <mehrtash.harandi@anu.edu.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nby p). These results suggest that state-of-the-art methods either scale poorly with the dimensionality of the input and thus require PCA to remain tractable (e.g., LDML), or require PCA to achieve an accuracy comparable to the other baselines (e.g., KISSME).\nIn essence, this observation indicates that dimensionality reduction is beneficial to (i) reduce the computational burden of the algorithms; and (ii) extract the relevant information from the original noisy data. However, it also raises an additional question: Is PCA, or any other specific dimensionality reduction technique, really the best method for the problem at hand? In other words, Shouldn’t we rather learn the low-dimensional representation and the metric jointly?\nMotivated by these questions, in this paper, we introduce a unified formulation for dimensionality reduction and metric learning. As suggested by our results on the ASLAN dataset in the bottom row of Table 1, our method outperforms the state-of-the-art metric learning techniques. Furthermore, despite the fact that we directly use highdimensional features as input, our method has comparable runtimes to that of the fastest algorithms working on PCAbased low-dimensional representations.\nIn the context of Mahalanobis metric learning, several methods have proposed to allow the metric M to have low rank, thus inherently performing dimensionality reduction. Most methods, however, achieve this implicitly, by letting M be positive semi-definite (Weinberger & Saul, 2009; Davis et al., 2007), which, as opposed to explicit dimensionality reduction, does not reduce the computational cost of these algorithms. As a consequence, they still need to rely on PCA as a pre-processing step in practice. While (Lu et al., 2014) explicitly decomposes M = LLT , it enforces orthogonality constraints on L to disambiguate the solutions, thus effectively only performing dimensionality reduction, and not metric learning. By contrast, our approach lets us learn a complete Mahalanobis metric jointly with a low-dimensional projection.\nAt the heart of our joint dimensionality reduction and metric learning formulation lie notions of Riemannian geometry and quotient spaces. More specifically, we model the projection to a low-dimensional space as a point on a Stiefel manifold, and the metric in this space as a Symmetric Pos-\nitive Definite (SPD) matrix. We then show that our search space reduces to a quotient of the product space of the Stiefel and SPD manifolds with the orthogonal group. By building upon recent advances in optimization on Riemannian matrix manifolds (Absil et al., 2009), we therefore develop a mathematical framework that effectively and efficiently lets us find a solution in this space. Furthermore, we show that our formulation can be kernelized. This not only lets us handle non-linearity in the data, but also makes our approach applicable to non-vectorial input data, such as linear subspaces (Harandi et al., 2014), which have proven beneficial for many recognition tasks.\nWe demonstrate the benefits of our joint dimensionality reduction and metric learning approach over existing metric learning schemes on several tasks, including action similarity matching, face verification and person re-identification."
  }, {
    "heading": "2. Mathematical Background",
    "text": "In this work, as most metric learning algorithms, we are interested in learning a Mahalanobis distance defined below.\nDefinition 1 (The Mahalanobis distance). The Mahalanobis distance between x and x̃ in Rn is defined as\nd2M (x, x̃) = ‖x− x̃‖2M = (x− x̃)TM(x− x̃) . (1)\nTo have a valid metric, the Mahalanobis matrix M must be positive definite.\nAs will be shown in Section 3, our approach to learning a Mahalanobis metric can be formulated as a non-convex optimization problem on a Riemannian manifold. This type of problems can be expressed with the general form\nminimize f(z)\ns.t. z ∈M , (2)\nwhere M is a Riemannian manifold, i.e., informally, a smooth surface that locally resembles a Euclidean space.\nWhile Riemannian manifolds can often be explicitly encoded in terms of constraints on z, the recent advances in Riemannian optimization techniques (Absil et al., 2009)\nhave shown the benefits of truly exploiting the geometry of the manifold over standard constrained optimization. As a consequence, these techniques have become increasingly popular in diverse application domains (Mishra et al., 2014; Harandi et al., 2017; Cunningham & Ghahramani, 2015). A detailed discussion of Riemannian optimization goes beyond the scope of this paper, and we refer the interested reader to (Absil et al., 2009).\nAs will be discussed in details in Section 3, we formulate metric learning in the quotient space of the product space of two Riemannian manifolds with the orthogonal group. The two Riemannian manifolds at the heart of this formulation are the Stiefel manifold and the manifold of Symmetric Positive Definite (SPD) matrices defined below.\nDefinition 2 (The Stiefel Manifold). The set of (n × p)dimensional matrices, p ≤ n, with orthonormal columns endowed with the Frobenius inner product1 forms a compact Riemannian manifold called the Stiefel manifold St(p, n) (Boothby, 2003).\nSt(p, n) , {W ∈ Rn×p : W TW = Ip} . (3)\nDefinition 3 (The SPD Manifold). The set of (p × p) dimensional real, SPD matrices endowed with the Affine Invariant Riemannian Metric (AIRM) (Pennec et al., 2006) forms the SPD manifold Sp++.\nSp++ , {M ∈ Rp×p : vTMv > 0, ∀v ∈ Rp − {0p}} . (4)\nThe dimensionality of St(p, n) and Sp++ are np− 12p(p+1) and p(p+ 1)/2, respectively."
  }, {
    "heading": "3. Our Approach",
    "text": "Our goal, as that of many other metric learning algorithms, is to learn a Mahalanobis distance between the input measurements. Ideally, this distance should reflect the class\n1Note that the literature is divided between this choice and another form of Riemannian metric. See (Edelman et al., 1998) for details.\nlabels of the samples. Furthermore, motivated by our analysis of existing methods, which all benefit from a PCA preprocessing step, we also seek to reduce the dimensionality of the data. However, in contrast to existing methods, we propose to learn the lower-dimensional representation and the Mahalanobis distance in that space jointly.\nMore specifically, we want to learn a projection W : Rn → Rp and a Mahalanobis matrix M ∈ Sp++, such that the induced distance in Rp is more discriminative. To this end, let X = {(xi, x̃i, yi)}mi=1 be a set of triplets, where xi, x̃i ∈ Rn are the feature vectors of two training samples, and the label yi ∈ {0, 1} determines whether xi and x̃i are similar (yi = 1) or not (yi = 0). The Mahalanobis distance between xi and x̃i in the low-dimensional space can thus be written as\nd2M ,W (xi, x̃i) = (W Txi −W T x̃i)TM(W Txi −W T x̃i)\n= (x− x̃i)TWMW T (xi − x̃i) . (5)\nTo learn a latent space whose Mahalanobis distance reflects class similarity, we make use of the logistic loss. More precisely, for each pair of samples (xi, x̃i) sharing the same label, i.e., yi = 1, we define the loss\n`(xi, x̃i|yi = 1) = log(1 + pi) , (6)\nwith pi = exp ( β(x− x̃)TWMW T (x− x̃) ) , β > 0. (7)\nConversely, for a pair of samples (xj , x̃j) whose labels differ, i.e., yj = 0, we define the loss\n`(xj , x̃j |yj = 0) = log(1 + p−1j ) . (8)\nIntuitively, the loss of Eq. 6 is minimized when d2M ,W (xi, x̃i) → 0, whereas the loss of Eq. 8 is minimized when d2M ,W (xj , x̃j)→∞.\nThe losses for all training triplets can be grouped into a cost function of the form\nL(W ,M |X) , ∑\ni|yi=1\nlog(1 + pi)\n+ ∑\ni|yi=0\nlog(1 + p−1i ) + λr(M ,M0), (9)\nwhich further encodes a regularizer on M . This regularizer, r : Sp++ × S p ++ → R+, allows us to exploit prior knowledge on the Mahalanobis matrix, encoded by a reference matrix M0. Following common practice (Davis et al., 2007; Hoffman et al., 2014), we make use of the asymmetric Burg divergence, which yields\nr(M ,M0) = Tr(MM −1 0 )− log det(MM −1 0 )− p .\n(10)\nIn our experiments, since typically no strong prior is available, we simply use M0 = Ip, i.e., the identity matrix. Joint dimensionality reduction and metric learning can then be achieved by minimizing the cost function of Eq. 9 w.r.t. W and M . To avoid degeneracies, and following common practice in dimensionality reduction, we constrain W to be a matrix with orthonormal columns. That is,\nW TW = Ip. (11)\nWith this constraint, W is in fact a point on the Stiefel manifold St(p, n). Since both M and W lie on Riemannian manifolds, albeit different ones, we propose to make use of Riemannian optimization to solve our problem, as described below."
  }, {
    "heading": "3.1. Manifold-based Optimization",
    "text": "To determine W and M , we need to solve the optimization problem\nmin W ,M\nL(W ,M |X)\ns.t. W TW = Ip, M 0 . (12)\nJointly minimizing with respect to W and M can be achieved by making use of the product space of the Stiefel and SPD manifolds,Mp = St(p, n)×Sp++. Both St(p, n) and Sp++ are smooth homogeneous spaces and their product preserves smoothness and differentiability (Absil et al., 2009). Thus, Mp can be given a Riemannian structure. However, in our case, a closer look at L(W ,M |X) reveals that\nL(W ,M |X) = L(WR,RTMR|X) ,∀R ∈ Op , (13)\nwhere Op is the orthogonal group. This implies that π :Mp×Op →Mp : ( (W ,M ) ,R ) → ( WR,RTMR ) (14) is a right group action onMp. The theorem below establishes an important property about the action ofOp onMp, which will prove crucial to our develop our approach. Theorem 1. The setM , ( St(p, n) × Sp++ ) \\O(p) with the equivalence relation\n[ ( W ,M ) ] ∼ {( WR,RTMR ) ; ∀R ∈ O(p) } (15)\nand Riemannian metric\ng(W ,M)((ξW , ξM ) , (ςW , ςM )) = 2 Tr(ξ T W ςW ) (16)\n+ Tr(M−1ξMM −1ςM )\nforms a Riemannian quotient manifold.\nProof. See the supplementary material.\nThe search space of our problem therefore truly is this Riemannian manifoldM. To be able to perform Riemannian optimization onM, below, we derive the required entities.\nThe Geometry ofM\nThe general theory of quotient manifolds (Lee, 2003; Absil et al., 2009) tells us that the equivalence relation splits the tangent space ofMp at Ω = (W ,M) into two complementary parts: the horizontal spaceHΩMp and the vertical space VΩMp. These two spaces are such that\ngp(hΩ,vΩ) = 0, ∀hΩ ∈ HΩMp and ∀vΩ ∈ VΩMp , (17) where gp is the Riemannian metric of the product manifold Mp. The vertical space VΩMp has the property that projecting any of its vectors to Mp via the exponential map yields a point in the equivalence class of Ω. Therefore, the tangent space of M can be identified with the horizontal space, i.e., T[Ω]M , HΩMp.\nA tangent vector ξ↑Ω ∈ T[Ω]M can be obtained from a tangent vector ξΩ ∈ TΩMp by projection. It can be shown that the horizontal space at Mp 3 (W ,M) = (U [Ip, 0p,n−p]\nT ,M) with U ∈ On is the set (details in the supplementary material){(\nU\n[ V M−1 −M−1V\nB\n] ,V )} ,\nwith V ∈ Sym(p), B ∈ R(n−p)×p. Furthermore, we have the following theorem to obtain the tangent vectors inM. Theorem 2 (Projecting on the Horizontal Space). For (ξW , ξM ) ∈ T(W ,M)Mp, the horizontal vector (i.e., the associated tangent vector in T[(W ,M)]M) is identified as(\nξW −WΘ, ξM −MΘ + ΘM ) , (18)\nwith Θ the solution of the following Sylvester equation:\nΘM2 + M2Θ = M ( ξTWW −W T ξW +\nM−1ξM − ξMM−1 ) M . (19)\nProof. See the supplementary material.\nTo perform Newton-type optimization onM, we also need the form of the retraction R[(W ,M)] : T[(W ,M)]M→M, which follows from the retraction onMp. In particular, we suggest the following retraction:\nR[(W ,M)](ξW , ξM ) , ( uf(W + ξW ), M1/2 expm(M−1/2ξMM −1/2)M1/2 ) . (20)\nHere uf(A) = A(ATA)−1/2, which yields an orthogonal matrix and expm(·) denotes the matrix exponential. Altogether, this provides us with the tools required to perform Riemannian optimization to solve our problem. The only missing mathematical entity is the Euclidean gradient of\nour loss function w.r.t. W and M , which we provide in the supplementary material.\nIn our experiments, we employed Conjugate Gradient descent on M to solve (12). In particular, we implemented the operations required for our manifold within the manopt Riemannian optimization toolbox (Boumal et al., 2014). The code is available at https://sites.google. com/site/mehrtashharandi/.\nIn Fig. 1, we illustrate the typical convergence behavior of our algorithm using the ASLAN dataset (Kliper-Gross et al., 2012). In our experiments, we have observed that the algorithm converges quite fast (typically in less than 25 iterations), thus making it scalable to learning large metrics."
  }, {
    "heading": "3.2. Computational Complexity",
    "text": "The complexity of each iteration of our algorithm to solve (12) depends on the computational cost of the following major steps:\n• Objective function evaluation. Computing L(W ,M |X) takes O(mnp+mp2 + p3 + np2).\n• Euclidean gradient evaluation: Computing ∇W takes O(mn2 + pn2 + np2), and computing ∇M takes O(mn2 + pn2 + np2 + p3). Note that some computations are common to both ∇W and ∇M . Hence the total flops for this step is less than the addition of the Stiefel and SPD parts.\n• Projecting (∇W ,∇M ) to the tangent space of Mp takes O(2p2(n+ p)).\n• Projecting a tangent vector in Mp costs O(2np2) to form the Sylvester equation and O(p3) to solve it using the Bartels - Stewart algorithm (Bartels & Stewart, 1972).\n• Retraction: For the Stiefel part, the retraction τSt takes O(4np2 + 11p3). For the SPD part, τSPD takes O(3p3).\nThese steps are either linear or quadratic in n. Therefore, and as evidenced by our experiments, our approach can effectively and efficiently handle high-dimensional input features without any PCA pre-processing.\nRemark 1. The number of unknowns determined by our algorithm corresponds to the dimensionality of Mp, that is, np− 12p(p+ 1) + 1 2p(p+ 1)− 1 2p(p− 1) = 1 2p(2n− p + 1). By contrast, the metric learning techniques that utilize PCA as a pre-processing step only determine 12p(p+ 1) unknowns, which is typically much smaller. As such, our method can potentially better leverage large amounts of training triplets. In our Big Data era, we believe this to be an important strength of our approach."
  }, {
    "heading": "3.3. Discussion",
    "text": "An SPD matrix M ∈ Sp++ can be decomposed as UDU T with U ∈ Op and D a diagonal matrix with positive elements. As such, the term WMW T appearing in our loss L(W ,M |X) can be written as\nWMW T = WUDUTW T = V DV T ,\nwith St(p, n) 3 V = WU . Thus, our optimization problem can be expressed by a loss L(V ,D|X) with a search space defined as St(p, n) × Rp+. Theoretically, this representation has the same expressive power as our formulation if we ignore the invariance of V DV T to permutations. However, in the context of fixed-rank matrix factorization (see (Mishra et al., 2014), Section 3.2), it has been shown that, for a parametrization of the form WMV T , where W ,V ∈ St(p, n), modeling M as an SPD matrix is typically more effective than as a diagonal matrix with positive elements. The argument there is that it “gives more flexibility to optimization algorithms” (Mishra et al., 2014).\nOne can also factorize WMW T as LLT with L ∈ Rn×p. This factorization, though being widely used, is not invariant to the action of Op, meaning that replacing L → LR,R ∈ Op will not change the loss. Such an invariance hinders gradient descent algorithms, as shown for example in (Journée et al., 2010; Mishra et al., 2014). In Section 6, we empirically show that this is indeed the case for the problem of interest here, i.e., metric learning.\nIn (Journée et al., 2010), the invariance induced by the action of Op in a factorization of the form LLT is taken into account. In particular, the authors make use of a quotient geometry to overcome the undesirable effects of the invariance in gradient descent optimization. There is a subtle, yet important difference between our formulation and that of (Journée et al., 2010): Our approach can benefit from a factorization with redundancy, which is effective in practice. Furthermore, note that the geometry developed in our paper can also handle the case where a Mahalanobis metric is searched for (i.e., without recasting the problem as a fac-\ntorization problem), which is the case in techniques such as (Globerson & Roweis, 2005; Koestinger et al., 2012; Zadeh et al., 2016).\nBefore concluding this part, we contrast the aforementioned factorization for the experiment reported in Table 1. To this end, we replace the term WMW T in our loss with\n1. LLT , L ∈ Rn×p, and optimize using Euclidean geometry. We call this solution Euc-LLT .\n2. LLT , L ∈ Rn×p, and optimize using the geometry developed in (Journée et al., 2010). We call this solution Rim-LLT .\n3. V DV T , V ∈ St(p, n) and D a diagonal and positive matrix. We optimize using the geometry of the product manifold St(p, n) × Rp+. We call this solution RimV DV T .\nFollowing the experiment shown in Table 1, we evaluate the AUC for various dimensionalities using the aforementioned geometries. The results are provided in Table 2. First, we note that the general practice, i.e., using Euclidean geometry, is significantly outperformed by its Riemannian counterparts. The quotient geometry developed in (Journée et al., 2010) performs on par with our approach for low dimensionalities (e.g., p = 25). However, for larger dimensionalities, our technique yields more accurate solutions, suggesting that the redundancy in the formulation plays an important role. The importance of the redundancy can also be noticed by comparing Rim-V DV T against our solution. In terms of computation time, the diagonal form, i.e., Rim-V DV T yields only slightly faster runtimes. In the particular case of ASLAN, the training time for p = 1000 was reduced to 150s."
  }, {
    "heading": "4. Kernelizing the Solution",
    "text": "We now show how our approach can handle nonlinearity in the data, as well as generalize to non-vectorial input data, such as linear subspaces, which have proven effective for video recognition (Turaga et al., 2011; Harandi et al., 2014; Jayasumana et al., 2015). Following common practice when converting a linear algorithm to a nonlinear one (e.g., from PCA to kernel PCA), we make use of a mapping of the input data to a Reproducing Kernel Hilbert Space (RKHS). As shown below, the resulting algorithm then only depends on kernel values (i.e., it does not explicitly depend on the mapping to RKHS). Since much progress has recently been made in developing positive def-\ninite kernels for non-vectorial data (Harandi et al., 2014; Jayasumana et al., 2015; Vishwanathan et al., 2010), this makes our approach applicable to a much broader variety of input types.\nSpecifically, let φ : X → H be a mapping from the input spaceX to an RKHSHwith corresponding kernel function k(xi,xj) = 〈φ(xi), φ(xj)〉. Following the same formalism as before, we can define a cost function of the form\nLH(W ,M |X) , ∑\ni,yi=1\nlog(1 + p̃i) (21)\n+ ∑\ni,yi=0\nlog(1 + p̃−1i ) + λr(M ,M0),\nwith p̃i = exp ( βd2H(xi, x̃i) ) and d2H(x, x̃) = ( φ(xi) −\nφ(xj) )T WMW T ( φ(xi)− φ(xj) ) , with M ∈ Sp++ and W ∈ St(p,dim(H)). Note that for universal kernel functions, such as the Gaussian kernel, dim(H) → ∞. We therefore need a formulation where only the kernel function appears, and not φ explicitly. To this end, we exploit the representer theorem (Schölkopf et al., 2001), which states that the mapping W lies in the span of the training data, and can thus be expressed as W = Φ(D)A. Here, Φ(D) = (φ(d1), · · · , φ(dl)) ∈ Rdim(H)×l is a matrix that stacks the representation of the l training samples in the feature space. In this formalism, the orthogonality constraint on W can be written as\nW TW = AT Φ(D)T Φ(D)A = ATK(D,D)A = Ip ,\nwhere K(D,D) ∈ Sl++ is the kernel matrix with elements [K(D,D)]i,j = k(di,dj). Let us define St(p, l) 3 B = K(D,D)1/2A, such that the orthogonality constraint becomes BTB = Ip. This lets us write\nd2H(x, x̃) = ( φ(x)− φ(x̃) )T WMW T ( φ(x)− φ(x̃) ) = ( k(x,D)− k(x̃,D) )T K(D,D)− 1 2BMBT\n×K(D,D)− 12 ( k(x,D)− k(x̃,D) ) , (22)\nwhere Rl 3 k(x,D) = ( k(x,d1), · · · , k(x,dl) )T . Thus, the cost defined in Eq. 21 can be rewritten as a function of B, i.e., LH(B,M |X), which, by taking d2H(x, x̃) from Eq. 22, only depends on kernel values. Since this cost function has essentially the same form as the one derived in Section 3, and the variables M and B lie on the same types of manifold as those of Section 3, we can use the same optimization strategy as before."
  }, {
    "heading": "5. Related Work",
    "text": "Metric learning is a well-studied problem whose origins can be traced back to the early eighties (e.g., (Short & Fukunaga, 1981)). Here, we focus on the prime representatives that will be used as baselines in our experiments.\nFor a more thorough study, we refer the reader to the recent book by (Bellet et al., 2015).\nThe idea of Neighborhood Component Analysis (NCA) (Goldberger et al., 2004) is to optimize the error of a stochastic nearest neighbor classifier in the space induced by the Mahalanobis metric. The InformationTheoretic Metric Learning (ITML) algorithm, proposed by (Davis et al., 2007), learns a Mahalanobis metric by exploiting a notion of margin between pairs of samples. More precisely, the algorithm searches for a Mahalanobis matrix satisfying two types of constraints: (i) an upper bound u on the distance between pairs of samples from the same class, i.e., in our formalism, d2M (xi, x̃i) ≤ u, ∀i | yi = 1; (ii) a lower bound l on the distance between pairs of dissimilar samples, i.e., d2M (xi, x̃i) ≥ l, ∀i | yi = 0.\nThe Large Margin Nearest Neighbors (LMNN) of (Weinberger & Saul, 2009) introduces the notion of local margins for metric learning. In LMNN, learning the Mahalanobis metric is expressed as a convex optimization problem that encourages the k nearest neighbors of any training instance xi to belong to the same class as xi, while keeping away instances of other classes.\nLogistic Discriminant based Metric Learning (LDML) (Guillaumin et al., 2009) relies on a Mahalanobis distance-based sigmoid function to encode the likelihood that two samples belong to the same class. The metric is then learned by maximizing the likelihood of the sample pairs (xi, x̃i) that truly belong to the same class, i.e., yi = 1, while minimizing that of the sample pairs that do not, i.e., yi = 0.\nWhile effective, all the above-mentioned techniques rely on PCA as a pre-processing step to remain tractable. By contrast, the efficient “Keep It Simple and Straightforward Metric” (KISSME) algorithm of (Koestinger et al., 2012) focuses on addressing large-scale problems. KISSME assumes that the similar and dissimilar pairs are generated from two independent Gaussian distributions. Computing the Mahalanobis metric then translates to maximizing a log-likelihood, which can be achieved in closedform. As illustrated in Table 1, however, this algorithm requires PCA pre-processing to achieve accuracies comparable to the ones produced by the other algorithms. In the spirit of KISSME, Geometric Mean Metric Learning (GMML) (Zadeh et al., 2016) relies on the geodesic connecting two covariance matrices to identify the Mahalanobis metric.\nWhile effective and quite efficient, the above-mentioned techniques usually rely on PCA as a pre-processing step to reduce the dimensionality of the data. As evidenced by our experiments, this pre-processing step is sub-optimal.\nFigure 2: Examples from the ASLAN dataset."
  }, {
    "heading": "6. Experimental Evaluation",
    "text": "We now evaluate our algorithms (DRML and kDRML) and compare them with the representative baseline metric learning methods discussed above, i.e., NCA (Goldberger et al., 2004) LMNN (Weinberger & Saul, 2009), ITML (Davis et al., 2007), LDML (Guillaumin et al., 2009), KISSME (Koestinger et al., 2012) and GMML (Zadeh et al., 2016), as well as with datasetspecific baselines mentioned below. Our experiments consist of two parts. First, we make use of benchmark datasets where the data can be represented in vector (Euclidean) form, and thus both DRML and kDRML are applicable. Second, we consider manifold-valued data where only kDRML applies.\nIn all our experiments, we followed the so-called restricted protocol. That is, the only information accessible to the algorithms is the similarity/dissimilarity labels of pairs of samples; the class labels of the samples are unknown. For all the methods, we report the results obtained with the best subspace dimension. Note that this means that not all methods use the same subspace dimension. However, it makes the comparison more fair, since it truly shows the full potential of the algorithms."
  }, {
    "heading": "6.1. Experiments with Euclidean Data",
    "text": "ACTION SIMILARITY MATCHING.\nAs a first experiment, we considered the task of action similarity recognition using the ASLAN dataset (Kliper-Gross et al., 2012). The ASLAN dataset contains 3,697 human action clips collected from YouTube, spanning over 432 unique action categories (see Fig. 2). The sample distribution across the categories is highly uneven, with 116 classes possessing only one video clip. The benchmark protocol focuses on action similarity (same/not-same), rather than action classification, and testing is performed on previously-unseen actions.\nThe dataset comes with 10 predefined splits of the data, where each split consists of 5,400 training and 600 testing pairs of action videos. The ASLAN dataset also provides three different types of descriptors: Histogram of Oriented\nGradients (HoG), Histogram of Optical Flow (HoF), and a composition of both (referred to as HnF). The videos are represented by spatiotemporal bags of features (Laptev et al., 2008) with a codebook of size 5,000. For kDRML, we used an RBF Gaussian kernel whose bandwidth was set using Jaakkola’s heuristic (Jaakkola et al., 1999).\nIn Table 3, we report the classification accuracy and the Area Under the ROC Curve (AUC) of our algorithms and of the baselines. Here, we also include the results of the benchmark (Kliper-Gross et al., 2012), which provides us with a direct comparison of previously published results. Note that DRML and kDRML outperform all the other algorithms. In general, kDRML performs better than DRML.\nTo further evidence the benefits of jointly learning the low-dimensional projection and the metric, we performed the following experiment, using the HoG features. We fixed the matrix W to the subspace obtained by PCA, and learned the metric using our loss function. This resulted in a drop in accuracy of roughly 1%, i.e., a CRR of 57.4%. This confirms our intuition that we can achieve better than PCA by jointly learning the subspace and the metric. Remark 2. In (Kliper-Gross et al., 2012), it was shown that other metrics (e.g., the cosine similarity) could outperform the Euclidean distance (used here as a baseline). In principle, our framework can also be used to learn cosine similarities by generalizing the inner product 〈a, b〉 as aTWMW T b. Doing so, however, goes beyond the scope of this paper.\nPERSON RE-IDENTIFICATION.\nFor the task of person re-identification, we used the iLIDS dataset (Zheng et al., 2009). The dataset consists of 476 images of 119 pedestrians and was captured in an airport. The number of images for each person varies from 2 to 8. The dataset contains severe occlusions caused by people and baggage.\nIn our experiments, we adopted the single-shot protocol. That is, the dataset was randomly divided into two subsets, training and test, with 59 and 60 exclusive individuals, respectively. The random splitting was repeated 10 times. In each partition, one image from each individual\nin the test set was randomly selected as the reference image and the rest of the images were used as query images. This process was repeated 20 times. We used the features provided by the authors of (Xiong et al., 2014). These features describe each image using 16-bin histograms from the RGB, YUV and HSV color channels, as well as texture histograms based on Local Binary Patterns (Ojala et al., 2002) extracted from 6 non-overlapping horizontal bands. For the kernel-based solutions, i.e., kDRML and kLFDA, we used the Chi-square kernel.\nWe report performance in terms of the Cumulative Match Characteristic (CMC) curves for different rank values r indicating that we search for a correct match among the r nearest neighbors. Table 4 compares our results with those of the baseline metric learning algorithms, as well as with kernel Local Fisher Discriminant Analysis (kLFDA) (Xiong et al., 2014), which represents the stateof-the-art on this dataset. Our kDRML method achieves the highest scores for all ranks. Note that kLFDA requires the subject identities during training, while the other methods, including ours, don’t. Despite this, kDRML outperforms the state-of-the-art results of kLFDA-Chi2."
  }, {
    "heading": "6.2. Experiments with Manifold-Valued Data",
    "text": "To illustrate the fact that our algorithm generalizes to nonvectorial input data, we utilized the Youtube Faces (YTF) dataset (Wolf et al., 2011) and represented each video as a point on a Grassmann manifold. The YTF dataset contains 3,425 videos of 1,595 subjects collected from the YouTube website. These videos depict large variations in pose, illumination and expression. To evaluate the performance of the algorithms, we followed the protocol suggested in (Wolf et al., 2011). Specifically, we used the 5,000 video pairs officially provided with the dataset, which are equally divided into 10 folds. Each fold contains 250 ‘same’ and 250 ‘not-same’ pairs. We used the provided LBP features\nand modeled each video by a subspace of dimensionality 10 as described in (Wolf et al., 2011). As a result, each video was modeled as a point on the Grassmann manifold G(10, 1770), where 1,770 is the dimensionality of the LBP features. We used the projection kernel defined as\nkproj(Si,Sj) = ‖STi Sj‖2F .\nWhile kDRML directly uses a kernel function, some baselines (e.g., KISSME), do not. To still be able to report results for these baselines, we utilized kernel PCA, instead of PCA, to create their inputs.\nTable 5 summarizes the performance of the metric learning techniques and the baseline (Wolf et al., 2011) using the same input, i.e., subspaces of dimensionality 10. Here again, kDRML comfortably outperforms the other methods for all the error metrics. For example, the gap in accuracy between kDRML and its closest competitor, i.e., KISSME, is more than 4%. Remark 3. Note that, as shown in (Feragen et al., 2015), an RBF kernel of the form exp(−σd2g(·, ·)) with dg being the geodesic distance on the Grassmann manifold is not a positive definite kernel. The projection kernel, however, has been shown to be positive definite (Hamm & Lee, 2008), which, ultimately, is all we require to make our algorithm applicable to manifold-valued data. Furthermore, this kernel has proven effective in a variety of applications (Hamm & Lee, 2008; Harandi et al., 2017)."
  }, {
    "heading": "7. Conclusions and Future Work",
    "text": "In this paper, we have argued against treating dimensionality reduction as a pre-processing step to metric learning. We have therefore introduced a framework that learns a low-dimensional representation and a Mahalanobis metric in this space in a unified manner. We have shown that the resulting framework could be cast an optimization problem on the quotient space of the product space of two Riemannian manifolds with the orthogonal group. Our experiments have evidenced the benefits of our unified approach over state-of-the-art metric learning algorithms that rely on PCA as a pre-processing step. In the future, we plan to study the use of other cost functions within our unified framework, especially formulations based on the concept of large margin."
  }],
  "year": 2017,
  "references": [{
    "title": "Optimization algorithms on matrix manifolds",
    "authors": ["Absil", "P-A", "Mahony", "Robert", "Sepulchre", "Rodolphe"],
    "year": 2009
  }, {
    "title": "Solution of the matrix equation ax+ xb=",
    "authors": ["Bartels", "Richard H", "Stewart", "GW"],
    "venue": "c. Communications of the ACM,",
    "year": 1972
  }, {
    "title": "Large margin local metric learning",
    "authors": ["Bohné", "Julien", "Ying", "Yiming", "Gentric", "Stéphane", "Pontil", "Massimiliano"],
    "venue": "In Proc. European Conference on Computer Vision (ECCV),",
    "year": 2014
  }, {
    "title": "An introduction to differentiable manifolds and Riemannian geometry, volume 120",
    "authors": ["Boothby", "William Munger"],
    "venue": "Gulf Professional Publishing,",
    "year": 2003
  }, {
    "title": "Manopt, a Matlab toolbox for optimization on manifolds",
    "authors": ["N. Boumal", "B. Mishra", "Absil", "P.-A", "R. Sepulchre"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2014
  }, {
    "title": "Linear dimensionality reduction: Survey, insights, and generalizations",
    "authors": ["Cunningham", "John P", "Ghahramani", "Zoubin"],
    "year": 2015
  }, {
    "title": "Information-theoretic metric learning",
    "authors": ["Davis", "Jason V", "Kulis", "Brian", "Jain", "Prateek", "Sra", "Suvrit", "Dhillon", "Inderjit S"],
    "venue": "In Proc. Int. Conference on Machine Learning (ICML),",
    "year": 2007
  }, {
    "title": "The geometry of algorithms with orthogonality constraints",
    "authors": ["Edelman", "Alan", "Arias", "Tomás A", "Smith", "Steven T"],
    "venue": "SIAM journal on Matrix Analysis and Applications,",
    "year": 1998
  }, {
    "title": "Geodesic exponential kernels: When curvature and linearity conflict",
    "authors": ["Feragen", "Aasa", "Lauze", "Francois", "Hauberg", "Soren"],
    "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2015
  }, {
    "title": "Metric learning by collapsing classes",
    "authors": ["Globerson", "Amir", "Roweis", "Sam"],
    "venue": "In Proc. Advances in Neural Information Processing Systems (NIPS),",
    "year": 2005
  }, {
    "title": "Neighbourhood components analysis",
    "authors": ["Goldberger", "Jacob", "Roweis", "Sam", "Hinton", "Geoff", "Salakhutdinov", "Ruslan"],
    "venue": "In Proc. Advances in Neural Information Processing Systems (NIPS),",
    "year": 2004
  }, {
    "title": "Is that you? metric learning approaches for face identification",
    "authors": ["Guillaumin", "Matthieu", "Verbeek", "Jakob", "Schmid", "Cordelia"],
    "venue": "In Proc. Int. Conference on Computer Vision (ICCV),",
    "year": 2009
  }, {
    "title": "Grassmann discriminant analysis: a unifying view on subspace-based learning",
    "authors": ["Hamm", "Jihun", "Lee", "Daniel D"],
    "venue": "In Proc. Int. Conference on Machine Learning (ICML),",
    "year": 2008
  }, {
    "title": "Expanding the family of Grassmannian kernels: An embedding perspective",
    "authors": ["Harandi", "Mehrtash", "Salzmann", "Mathieu", "Jayasumana", "Sadeep", "Hartley", "Richard", "Li", "Hongdong"],
    "venue": "In Proc. European Conference on Computer Vision (ECCV),",
    "year": 2014
  }, {
    "title": "Dimensionality reduction on SPD manifolds: The emergence of geometry-aware methods",
    "authors": ["Harandi", "Mehrtash", "Salzmann", "Mathieu", "Hartley", "Richard"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2017
  }, {
    "title": "Asymmetric and category invariant feature transformations for domain adaptation",
    "authors": ["Hoffman", "Judy", "Rodner", "Erik", "Donahue", "Jeff", "Kulis", "Brian", "Saenko", "Kate"],
    "venue": "Int. Journal of Computer Vision,",
    "year": 2014
  }, {
    "title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
    "authors": ["Huang", "Gary B", "Ramesh", "Manu", "Berg", "Tamara", "Learned-Miller", "Erik"],
    "venue": "Technical report,",
    "year": 2007
  }, {
    "title": "Using the Fisher kernel method to detect remote protein homologies",
    "authors": ["Jaakkola", "Tommi", "Diekhans", "Mark", "Haussler", "David"],
    "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,",
    "year": 1999
  }, {
    "title": "Kernel methods on Riemannian manifolds with Gaussian RBF kernels",
    "authors": ["S. Jayasumana", "R. Hartley", "M. Salzmann", "H. Li", "M. Harandi"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2015
  }, {
    "title": "Low-rank optimization on the cone of positive semidefinite matrices",
    "authors": ["Journée", "Michel", "Bach", "Francis", "Absil", "P-A", "Sepulchre", "Rodolphe"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2010
  }, {
    "title": "The action similarity labeling challenge",
    "authors": ["Kliper-Gross", "Orit", "Hassner", "Tal", "Wolf", "Lior"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2012
  }, {
    "title": "Large scale metric learning from equivalence constraints",
    "authors": ["Koestinger", "Martin", "Hirzer", "Wohlhart", "Paul", "Roth", "Peter M", "Bischof", "Horst"],
    "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2012
  }, {
    "title": "Learning realistic human actions from movies",
    "authors": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"],
    "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp",
    "year": 2008
  }, {
    "title": "Neighborhood repulsed metric learning for kinship verification",
    "authors": ["Lu", "Jiwen", "Zhou", "Xiuzhuang", "Tan", "Yap-Pen", "Shang", "Yuanyuan", "Jie"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2014
  }, {
    "title": "Fixed-rank matrix factorizations and Riemannian lowrank optimization",
    "authors": ["B Mishra", "G Meyer", "S Bonnabel", "R. Sepulchre"],
    "venue": "Computational Statistics,",
    "year": 2014
  }, {
    "title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns",
    "authors": ["Ojala", "Timo", "Pietikäinen", "Matti", "Mäenpää", "Topi"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2002
  }, {
    "title": "A Riemannian framework for tensor computing",
    "authors": ["Pennec", "Xavier", "Fillard", "Pierre", "Ayache", "Nicholas"],
    "venue": "Int. Journal of Computer Vision,",
    "year": 2006
  }, {
    "title": "A generalized representer theorem",
    "authors": ["Schölkopf", "Bernhard", "Herbrich", "Ralf", "Smola", "Alex J"],
    "venue": "In Computational learning theory,",
    "year": 2001
  }, {
    "title": "The optimal distance measure for nearest neighbor classification",
    "authors": ["Short", "Robert D", "Fukunaga", "Keinosuke"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 1981
  }, {
    "title": "Statistical computations on Grassmann and Stiefel manifolds for image and video-based recognition",
    "authors": ["P. Turaga", "A. Veeraraghavan", "A. Srivastava", "R. Chellappa"],
    "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,",
    "year": 2011
  }, {
    "title": "Distance metric learning for large margin nearest neighbor classification",
    "authors": ["Weinberger", "Kilian Q", "Saul", "Lawrence K"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2009
  }, {
    "title": "Face recognition in unconstrained videos with matched background similarity",
    "authors": ["Wolf", "Lior", "Hassner", "Tal", "Maoz", "Itay"],
    "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
    "year": 2011
  }, {
    "title": "Person re-identification using kernel-based metric learning methods",
    "authors": ["Xiong", "Fei", "Gou", "Mengran", "Camps", "Octavia", "Sznaier", "Mario"],
    "venue": "In Proc. European Conference on Computer Vision (ECCV),",
    "year": 2014
  }, {
    "title": "Geometric mean metric learning",
    "authors": ["Zadeh", "Pourya", "Hosseini", "Reshad", "Sra", "Suvrit"],
    "venue": "In Proc. Int. Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Associating groups of people",
    "authors": ["Zheng", "Wei-Shi", "Gong", "Shaogang", "Xiang", "Tao"],
    "venue": "In BMVC,",
    "year": 2009
  }],
  "id": "SP:e7f6bfb9bb591eb1404ae13f0fa13ad4a3179150",
  "authors": [{
    "name": "Mehrtash Harandi",
    "affiliations": []
  }, {
    "name": "Mathieu Salzmann",
    "affiliations": []
  }, {
    "name": "Richard Hartley",
    "affiliations": []
  }],
  "abstractText": "To be tractable and robust to data noise, existing metric learning algorithms commonly rely on PCA as a pre-processing step. How can we know, however, that PCA, or any other specific dimensionality reduction technique, is the method of choice for the problem at hand? The answer is simple: We cannot! To address this issue, in this paper, we develop a Riemannian framework to jointly learn a mapping performing dimensionality reduction and a metric in the induced space. Our experiments evidence that, while we directly work on high-dimensional features, our approach yields competitive runtimes with and higher accuracy than state-of-the-art metric learning algorithms.",
  "title": "Joint Dimensionality Reduction and Metric Learning: A Geometric Take"
}