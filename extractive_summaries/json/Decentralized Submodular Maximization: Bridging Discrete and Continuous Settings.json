{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In recent years, we have reached unprecedented data volumes that are high dimensional and sit over (clouds of) networked machines. As a result, decentralized collection of these data sets along with accompanying distributed op-\n1Laboratory for Information and Decision Systems, Massachusetts Institute of Technology 2Department of Electrical and Systems Engineering, University of Pennsylvania 3Department of Electrical Engineering and Computer Science, Yale University. Correspondence to: Aryan Mokhtari <aryanm@mit.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ntimization methods are not only desirable but very often necessary (Boyd et al., 2011).\nThe focus of this paper is on decentralized optimization, the goal of which is to maximize/minimize a global objective function –distributed over a network of computing units– through local computation and communications among nodes. A canonical example in machine learning is fitting models using M-estimators where given a set of data points the parameters of the model are estimated through an empirical risk minimization (Vapnik, 1998). Here, the global objective function is defined as an average of local loss functions associated with each data point. Such local loss functions can be convex (e.g., logistic regression, SVM, etc) or non-convex (e.g., non-linear square loss, robust regression, mixture of Gaussians, deep neural nets, etc) (Mei et al., 2016). Due to the sheer volume of data points, these optimization tasks cannot be fulfilled on a single computing cluster node. Instead, we need to opt for decentralized solutions that can efficiently exploit dispersed (and often distant) computational resources linked through a tightly connected network. Furthermore, local computations should be light so that they can be done on single machines. In particular, when the data is high dimensional, extra care should be given to any optimization procedure that relies on projections over the feasibility domain.\nIn addition to large scale machine learning applications, decentralized optimization is a method of choice in many other domains such as Internet of Things (IoT) (Abu-Elkheir et al., 2013), remote sensing (Ma et al., 2015), multi-robot systems (Tanner & Kumar, 2005), and sensor networks (Rabbat & Nowak, 2004). In such scenarios, individual entities can communicate over a network and interact with the environment by exchanging the data generated through sensing. At the same time they can react to events and trigger actions to control the physical world. These applications highlight another important aspect of decentralized optimization where private data is collected by different sensing units (Yang et al., 2017). Here again, we aim to optimize a global objective function while avoiding to share the private data among computing units. Thus, by design, one cannot solve such private optimization problems in a centralized manner and should rely on decentralized solutions where local private\ncomputation is done where the data is collected.\nContinuous submodular functions, a broad subclass of nonconvex functions with diminishing returns property, have recently received considerable attention (Bach, 2015; Bian et al., 2017). Due to their interesting structures that allow strong approximation guarantees (Mokhtari et al., 2018a; Bian et al., 2017), they have found various applications, including the design of online experiments (Chen et al., 2018), budget and resource allocations (Eghbali & Fazel, 2016; Staib & Jegelka, 2017), and learning assignments (Golovin et al., 2014). However, all the existing work suffer from centralized computing. Given that many information gathering, data summarization, and non-parametric learning problems are inherently related to large-scale submodular maximization, the demand for a fully decentralized solution is immediate. In this paper, we develop the first decentralized framework for both continuous and discrete submodular functions. Our contributions are as follows:\n• Continuous submodular maximization: For any global objective function that is monotone and continuous DR-submodular and subject to any downclosed and bounded convex body, we develop Decentralized Continuous Greedy, a decentralized and projection-free algorithm that achieves the tight (1 1/e ✏) approximation guarantee in O(1/✏2) rounds of local communication.\n• Discrete submodular maximization: For any global objective function that is monotone and submodular and subject to any matroid constraint, we develop a discrete variant of Decentralized Continuous Greedy that achieves the tight (1 1/e ✏) approximation ratio in O(1/✏3) rounds of communication.\nAll proofs are provided in the supplementary material."
  }, {
    "heading": "2. Related Work",
    "text": "Decentralized optimization is a challenging problem as nodes only have access to separate components of the global objective function, while they aim to collectively reach the global optimum point. Indeed, one naive approach to tackle this problem is to broadcast local objective functions to all the nodes in the network and then solve the problem locally. However, this scheme requires high communication overhead and disregards the privacy associated with the data of each node. An alternative approach is the master-slave setting (Bekkerman et al., 2011; Shamir et al., 2014; Zhang & Lin, 2015) where at each iteration, nodes use their local data to compute the information needed by the master node. Once the master node receives all the local information, it updates its decision and broadcasts the decision to all the nodes. Although this scheme protects the privacy of nodes it is not robust to machine failures and is prone to high overall\ncommunication time. In decentralized methods, these issues are overcame by removing the master node and considering each node as an independent unit that is allowed to exchange information with its neighbors.\nConvex decentralized consensus optimization is a relatively mature area with a myriad of primal and dual algorithms (Bertsekas & Tsitsiklis, 1989). Among primal methods, decentralized (sub)gradient descent is perhaps the most well known algorithm which is a mix of local gradient descent and successive averaging (Nedic & Ozdaglar, 2009; Yuan et al., 2016). It also can be interpreted as a penalty method that encourages agreement among neighboring nodes. This latter interpretation has been exploited to solve the penalized objective function using accelerated gradient descent (Jakovetić et al., 2014; Qu & Li, 2017), Newton’s method (Mokhtari et al., 2017; Bajovic et al., 2017), or quasi-Newton algorithms (Eisen et al., 2017). The methods that operate in the dual domain consider a constraint that enforces equality between nodes’ variables and solve the problem by ascending on the dual function to find optimal Lagrange multipliers. A short list of dual methods are the alternating directions method of multipliers (ADMM) (Boyd et al., 2011), dual ascent algorithm (Rabbat et al., 2005), and augmented Lagrangian methods (Jakovetic et al., 2015; Chatzipanagiotis & Zavlanos, 2015). Recently, there have been many attempts to extend the tools in decentralized consensus optimization to the case that the objective function is non-convex (Di Lorenzo & Scutari, 2016; Sun et al., 2016; Hajinezhad et al., 2016; Tatarenko & Touri, 2017). However, such works are mainly concerned with reaching a stationary point and naturally cannot provide any optimality guarantee.\nIn this paper, our focus is to provide the first decentralized algorithms for both discrete and continuous submodular functions. It is known that the centralized greedy approach of (Nemhauser et al., 1978), and its many variants (Feige et al., 2011; Buchbinder et al., 2015; 2014; Feldman et al., 2017; Mirzasoleiman et al., 2016), reach tight approximation guarantees in various scenarios. As such methods are sequential in nature, they do not scale to massive datasets. To partially resolve this issue, MapReduce style methods, with a master-slave architecture, have been proposed (Mirzasoleiman et al., 2013; Kumar et al., 2015; da Ponte Barbosa et al., 2015; Mirrokni & Zadimoghaddam, 2015; Qu et al., 2015).\nOne can extend the notion of diminishing returns to continuous domains (Wolsey, 1982; Bach, 2015). Even though continuous submodular functions are not generally convex (nor concave) Hassani et al. (2017) showed that in the monotone setting and subject to a general bounded convex body constraint, stochastic gradient methods can achieve a 1/2 approximation guarantee. The approximation guarantee can be tightened to (1 1/e) by using Frank-Wolfe (Bian et al.,\n2017) or stochastic Frank-Wolfe (Mokhtari et al., 2018a)."
  }, {
    "heading": "3. Notation and Background",
    "text": "In this section, we review the notation that we use throughout the paper. We then give the precise definition of submodularity in discrete and continuous domains.\nNotation. Lowercase boldface v denotes a vector and uppercase boldface W a matrix. The i-th element of v is written as vi and the element on the i-th row and j-th column of W is denoted by wi,j . We use kvk to denote the Euclidean norm of vector v and kWk to denote the spectral norm of matrix W. The null space of matrix W is denoted by null(W). The inner product of vectors x,y is indicated by hx,yi, and the transpose of a vector v or matrix W are denoted by v† and W†, respectively. The vector 1n 2 Rn is the vector of all ones with n components, and the vector 0p 2 Rp is the vector of all zeros with p components. Submodulary. A set function f : 2V ! R+, defined on the ground set V , is called submodular if for all A,B ✓ V , we have f(A)+f(B) f(A\\B)+f(A[B). We often need to maximize submodular functions subject to a down-closed set family I . In particular, we say I ⇢ 2V is a matroid if 1) for any A ⇢ B ⇢ V , if B 2 I, then A 2 I and 2) for any A,B 2 I if |A| < |B|, then there is an element e 2 B such that A [ {e} 2 I. The notion of submodularity goes beyond the discrete domain (Wolsey, 1982; Vondrák, 2007; Bach, 2015). Consider a continuous function F : X ! R+ where the set X ✓ Rp is of the form X =Qpi=1 Xi and each Xi is a compact subset of R+. We call the continuous function F submodular if for all x,y 2 X we have\nF (x) + F (y) F (x _ y) + F (x ^ y), (1) where x_y := max(x,y) (component-wise) and x^y := min(x,y) (component-wise). In this paper, our focus is on differentiable continuous submodular functions with two additional properties: monotonicity and diminishing returns. Formally, a submodular function F is monotone if\nx  y =) F (x)  F (y), (2) for all x,y 2 X . Note that x  y in (2) means that xi  yi for all i = 1, . . . , p. Furthermore, a differentiable submodular function F is called DR-submodular (i.e., shows diminishing returns) if the gradients are antitone, namely, for all x,y 2 X we have\nx  y =) rF (x) rF (y). (3) When the function F is twice differentiable, submodularity implies that all cross-second-derivatives are non-positive (Bach, 2015), and DR-submodularity implies that all secondderivatives are non-positive (Bian et al., 2017) In this work,\nwe consider the maximization of continuous submodular functions subject to down-closed convex bodies C ⇢ Rp+ defined as follows. For any two vectors x,y 2 Rp+, where x  y, down-closedness means that if y 2 C, then so is x 2 C. Note that for a down-closed set we have 0p 2 C."
  }, {
    "heading": "4. Decentralized Submodular Maximization",
    "text": "In this section, we state the problem of decentralized submodular maximization in continuous and discrete settings.\nContinuous Case. We consider a set of n computing machines/sensors that communicate over a graph to maximize a global objective function. Each machine can be viewed as a node i 2 N , {1, · · · , n}. We further assume that the possible communication links among nodes are given by a bidirectional connected communication graph G = (N , E) where each node can only communicate with its neighbors in G. We formally use Ni to denote node i’s neighbors. In our setting, we assume that each node i 2 N has access to a local function Fi : X ! R+. The nodes cooperate in order to maximize the aggregate monotone and continuous DR-submodular function F : X ! R+ subject to a down-closed convex body C ⇢ X ⇢ Rp+, i.e.,\nmax x2C F (x) = max x2C\n1\nn\nnX\ni=1\nFi(x). (4)\nThe goal is to design a message passing algorithm to solve (4) such that: (i) at each iteration t, the nodes send their messages (and share their information) to their neighbors in G, and (ii) as t grows, all the nodes reach to a point x 2 C that provides a (near-) optimal solution for (4).\nDiscrete Case. Let us now consider the discrete counterpart of problem (4). In this setting, each node i 2 N has access to a local set function fi : 2V ! R+. The nodes cooperate in maximizing the aggregate monotone submodular function f : 2V ! R+ subject to a matroid constraint I, i.e.\nmax S2I f(S) = max S2I\n1\nn\nnX\ni=1\nfi(S). (5)\nNote that even in the centralized case, and under reasonable complexity-theoretic assumptions, the best approximation guarantee we can achieve for Problems (4) and (5) is (1 1/e) (Feige, 1998). In the following, we show that it is possible to achieve the same approximation guarantee in a decentralized setting."
  }, {
    "heading": "5. Decentralized Continuous Greedy Method",
    "text": "In this section, we introduce the Decentralized Continuous Greedy (DCG) algorithm for solving Problem (4). Recall that in a decentralized setting, the nodes\nhave to cooperate (i.e., send messages to their neighbors) in order to solve the global optimization problem. We will explain how such messages are designed and communicated in DCG. Each node i in the network keeps track of two local variables xi,di 2 Rp which are iteratively updated at each round t using the information gathered from the neighboring nodes. The vector xti is the local decision variable of node i at step t whose value we expect to eventually converge to the (1 1/e) fraction of the optimal solution of Problem (4). The vector dti is the estimate of the gradient of the global objective function that node i keeps at step t.\nTo properly incorporate the received information from their neighbors, nodes should assign nonnegative weights to their neighbors. Define wij 0 to be the weight that node i assigns to node j. These weights indicate the effect of (variable or gradient) information nodes received from their neighbors in order to update their local (variable or gradient) information. Indeed, the weights wij must fulfill some requirements (later described in Assumption 1), but they are design parameters of DCG and can be properly chosen by the nodes prior to the implementation of the algorithm.\nThe first step at each round t of DCG is updating the local gradient approximation vectors dti using local and neighboring gradient information. In particular, node i computes its vector dti according to the update rule\nd t i = (1 ↵)\nX\nj2Ni[{i}\nwijd t 1 j + ↵rFi(xti), (6)\nwhere ↵ 2 [0, 1] is an averaging coefficient. Note that the sum\nP j2Ni[{i} wijd t 1 j in (6) is a weighted average of\nnode i’s vector dt 1i and its neighbors d t 1 j , evaluated at step t 1. Hence, node i computes the vector dti by evaluating a weighted average of its current local gradient rFi(xti) and the local and neighboring gradient information at step t 1, i.e.,Pj2Ni[{i} wijdt 1j . Since the vector dti is evaluated by aggregating gradient information from neighboring nodes, it is reasonable to expect that dti becomes a proper approximation for the global objective function gradient (1/n) Pn k=1 rfk(x) as time progresses. Note that to implement the update in (6) nodes should exchange their local vectors dti with their neighbors.\nUsing the gradient approximation vector dti, each node i evaluates its local ascent direction vti by solving\nv t i = argmax v2C hdti,vi. (7)\nThe update in (7) is also known as conditional gradient update. Ideally, in a conditional gradient method, we should choose the feasible direction v 2 C that maximizes the inner product by the full gradient vector 1n Pn k=1 rFk(xti). However, since in the decentralized setting the exact gradient 1n Pn k=1 rFk(xti) is not available at the i-th node, we\nAlgorithm 1 DCG at node i Require: Stepsize ↵ and weights wij for j 2 Ni [ {i}\n1: Initialize local vectors as x0i = d0i = 0p 2: Initialize neighbor’s vectors as x0j = d0j = 0p if j2Ni 3: for t = 1, 2, . . . , T do 4: Compute dti = (1 ↵) X\nj2Ni[{i}\nwijd t 1 j + ↵rFi(xti);\n5: Exchange dti with neighboring nodes j 2 Ni 6: Evaluate vti = argmaxv2C hdti,vi; 7: Update the variable xt+1i = X\nj2Ni[{i}\nwijx t j + 1\nT v\nt i ;\n8: Exchange xt+1i with neighboring nodes j 2 Ni 9: end for\nreplace it by its current approximation dti and hence we obtain the update rule (7).\nAfter computing the local ascent directions vti , the nodes update their local variables xti by averaging their local and neighboring iterates and ascend in the direction vti with stepsize 1/T where T is the total number of iterations, i.e.,\nx t+1 i =\nX\nj2Ni[{i}\nwijx t j + 1\nT v\nt i . (8)\nThe update rule (8) ensures that the neighboring iterates are not far from each other via the averaging termP\nj2Ni[{i} wijx t j , while the iterates approach the optimal maximizer of the global objective function by ascending in the conditional gradient direction vti . The update in (8) requires a round of local communication among neighbors to exchange their local variables xti. The steps of the DCG method are summarized in Algorithm 1.\nIndeed, the weights wij that nodes assign to each other cannot be arbitrary. In the following, we formalize the conditions that they should satisfy (Yuan et al., 2016).\nAssumption 1 The weights that nodes assign to each other are nonegative, i.e., wij 0 for all i, j 2 N , and if node j is not a neighbor of node i then the corresponding weight is zero, i.e., wij = 0 if j /2 Ni. Further, the weight matrix W 2 Rn⇥n with entries wij satisfies W † = W, W1n = 1n, null(I W) = span(1n). (9)\nThe first condition in (9) ensures that the weights are symmetric, i.e., wij = wji. The second condition guarantees the weights that each node assigns to itself and its neighbors sum up to 1, i.e., Pn j=1 wij = 1 for all i. Note that the condition W1n = 1n implies that I W is rank deficient. Hence, the last condition in (9) ensures that the rank of I W is exactly n 1. Indeed, it is possible to optimally\ndesign the weight matrix W to accelerate the averaging process as discussed in (Boyd et al., 2004), but this is not the focus of this paper. We emphasize that W is not a problem parameter, and we design it prior to running DCG.\nNotice that the stepsize 1/T and the conditions in Assumption 1 on the weights wij are needed to ensure that the local variables xti are in the feasible set C, as stated in the following proposition.\nProposition 1 Consider the DCG method outlined in Algorithm 1. If Assumption 1 holds and nodes start from x\n0 i = 0p 2 C, then the local iterates xti are always in the\nfeasible set C, i.e., xti 2 C for all i 2 N and t = 1, . . . , T .\nLet us now explain how DCG relates to and innovates beyond the exisiting work in submodular maximization as well as decentralized convex optimization. Note that in order to solve Problem (4) in a centralized fashion (i.e., when every node has access to all the local functions) we can use the continuous greedy algorithm (Vondrák, 2008), a variant of the conditional gradient method. However, in decentralized settings, nodes have only access to their local gradients, and therefore, continuous greedy is not implementable. Similar to the decentralized convex optimization, we can address this issue via local information aggregation. Our proposed DCG method incorporates the idea of choosing the ascent direction according to a conditional gradient update as is done in the continuous greedy algorithm (i.e., the update rule (7)), while it aggregates the global objective function information through local communications with neighboring nodes (i.e., the update rule (8)). Unlike traditional consensus optimization methods that require exchanging nodes’ local variables only (Nedic & Ozdaglar, 2009; Nedic et al., 2010), DCG also requires exchanging local gradient vectors to achieve a (1 1/e) fraction of the optimal solution at each node (i.e., the update rule (6)). This major difference is due to the fact that in conditional gradient methods, unlike proximal gradient algorithms, the local gradients can not be used instead of the global gradient. In other words, in the update rule (7), we can not use the local gradients rFi(xti) in lieu of dti. Indeed, there are settings for which such a replacement provides arbitrarily bad solutions. We formally characterize the convergence of DCG in Theorem 1."
  }, {
    "heading": "5.1. Extension to the Discrete Setting",
    "text": "In this section we show how DCG can be used for maximizing a decentralized submodular set function f , namely Problem (5), through its continuous relaxation. Formally, in lieu of solving Problem (5), we can form the following decentralized continuous optimization problem\nmax x2C\n1\nn\nnX\ni=1\nFi(x), (10)\nAlgorithm 2 Discrete DCG at node i Require: ↵, 2 [0, 1] and weights wij for j 2 Ni [ {i}\n1: Initialize local vectors as x0i = d0i = g0i = 0 2: Initialize neighbor’s vectors as x0j = d0j = 0 if j 2 Ni 3: for t = 1, 2, . . . , T do 4: Compute gti = (1 )gt 1i + r ˜Fi(xti); 5: Compute dti = (1 ↵) X\nj2Ni[{i}\nwijd t 1 j + ↵g t i ;\n6: Exchange dti with neighboring nodes j 2 Ni 7: Evaluate vti = argmaxv2C hdti,vi; 8: Update the variable xt+1i = X\nj2Ni[{i}\nwijx t j + 1\nT v\nt i ;\n9: Exchange xt+1i with neighboring nodes j 2 Ni; 10: end for 11: Apply proper rounding to obtain a solution for (5);\nwhere Fi is the multilinear extension of fi defined as\nFi(x) = X\nS⇢V fi(S)\nY i2S xi Y j /2S (1 xj), (11)\nand the down-closed convex set C = conv{1I : I 2 I} is the matroid polytope. Note that the discrete and continuous optimization formulations lead to the same optimal value (Calinescu et al., 2011).\nBased on the expression in (11), computing the full gradient rFi at each node i will require an exponential computation in terms of |V |, since the number of summands in (11) is 2\n|V |. As a result, in the discrete setting, we will slightly modify the DCG algorithm and work with unbiased estimates of the gradient that can be computed in time O(|V |) (see Appendix 9.7 for one such estimator). More precisely, in the discrete setting, each node i 2 N updates three local variables xti,dti,gti 2 R|V |. The variables xti,dti play the same role as in DCG and are updated using the messages received from the neighboring nodes. The variable gti at node i is defined to approximate the local gradient rFi(xti). Consider the vector r ˜Fi(xti) as an unbiased estimator of the local gradient rFi(xti) at time t, and define the vector g\nt i as the outcome of the recursion\ng t i = (1 )gt 1i + r ˜Fi(xti), (12)\nwhere 2 [0, 1] is the averaging parameter. We initialize all vectors as g0i = 0 2 R|V |. It was shown recently (Mokhtari et al., 2018a;b) that the averaging technique in (12) reduces the noise of the gradient approximations. Therefore, the sequence of gti approaches the true local gradient rFi(xti) as time progresses.\nThe steps of the Decentralized Continuous Greedy for the discrete setting is summarized in Algo-\nrithm 2. Note that the major difference between the Discrete DCG method (Algorithm 2) and the continuous DCG method (Algorithm 1) is in Step 5 in which the exact local gradient rFi(xti) is replaced by the stochastic approximation gti which only requires access to the computationally cheap unbiased gradient estimator r ˜Fi(xti). The communication complexity of both the discrete and continuous versions of DCG are the same at each round. However, since we are using unbiased estimations of the local gradients rFi(xi), the Discrete DCG takes more rounds to converge to a near-optimal solution compared to continuous DCG. We characterize the convergence of Discrete DCG in Theorem 2. Further, the implementation of Discrete DCG requires rounding the continuous solution to obtain a discrete solution for the original problem without any loss in terms of the objective function value. The provably lossless rounding schemes include the pipage rounding (Calinescu et al., 2011) and contention resolution (Chekuri et al., 2014)."
  }, {
    "heading": "6. Convergence Analysis",
    "text": "In this section, we study the convergence properties of DCG in both continuous and discrete settings. In this regard, we assume that the following conditions hold.\nAssumption 2 Euclidean distance of the elements in the set C are uniformly bounded, i.e., for all x,y 2 C we have\nkx yk  D. (13) Assumption 3 The local objective functions Fi(x) are monotone and DR-submodular. Further, their gradients are L-Lipschitz continuous over the set X , i.e., for all x,y 2 X\nkrFi(x) rFi(y)k  Lkx yk. (14) Assumption 4 The norm of gradients krFi(x)k are bounded over the convex set C, i.e., for all x 2 C, i 2 N ,\nkrFi(x)k  G. (15)\nThe condition in Assumption 2 guarantees that the diameter of the convex set C is bounded. Assumption 3 is needed to ensure that the local objective functions Fi are smooth. Finally, the condition in Assumption 4 enforces the gradients norm to be bounded over the convex set C. All these assumptions are customary and necessary in the analysis of decentralized algorithms. For more details, please check Section VII-B in Jakovetić et al. (2014).\nWe proceed to derive a constant factor approximation for DCG. Our main result is stated in Theorem 1. However, to better illustrate the main result, we first need to provide several definitions and technical lemmas. Let us begin by defining the average variables ¯xt as\n¯ x t = 1\nn\nnX\ni=1\nx t i. (16)\nIn the following lemma, we establish an upper bound on the variation in the sequence of average variables {¯xt}. Lemma 1 Consider the proposed DCG algorithm defined in Algorithm 1. Further, recall the definition of ¯xt in (16). If Assumptions 1 and 2 hold, then the difference between two consecutive average vectors is upper bounded by\nk¯xt+1 ¯xtk  D T . (17)\nRecall that at every node i, the messages are mixed using the coefficients wij , i.e., the i-th row of the matrix W. It is thus not hard to see that the spectral properties of W (e.g. the spectral gap) play an important role in the the speed of achieving consensus in decentralized methods.\nDefinition 1 Consider the eigenvalues of W which can be sorted in a nonincreasing order as 1 = 1(W) 2(W) · · · n(W) > 1. Define as the second largest magnitude of the eigenvalues of W, i.e.,\n:= max{| 2(W)|, | n(W)|}. (18)\nAs we will see, a mixing matrix W with smaller has a larger spectral gap 1 which yields faster convergence (Boyd et al., 2004; Duchi et al., 2012). In the following lemma, we derive an upper bound on the sum of the distances between the local iterates xti and their average ¯xt, where the bound is a function of the graph spectral gap 1 , size of the network n, and the total number of iterations T .\nLemma 2 Consider the proposed DCG algorithm defined in Algorithm 1. Further, recall the definition of ¯xt in (16). If Assumptions 1 and 2 hold, then for all t  T we have\nnX\ni=1\nx t i ¯xt\n2 !1/2  p nD\nT (1 ) . (19)\nLet us now define ¯dt as the average of local gradient approximations dti at step t, i.e., ¯dt = 1 n Pn i=1 d t i. We will show in the following that the vectors dti also become uniformly close to ¯dt.\nLemma 3 Consider the proposed DCG algorithm defined in Algorithm 1. If Assumptions 1 and 3 hold, then\nnX\ni=1\nkdti ¯dtk2 !1/2  ↵ p nG\n1 (1 ↵) . (20)\nLemma 3 guarantees that the individual local gradient approximation vectors dti are close to the average vector ¯dt if the parameter ↵ is small. To show that the gradient vectors dti, generated by DCG, approximate the gradient of the\nglobal objective function, we further need to show that the average vector ¯dt approaches the global objective function gradient rF . We prove this claim in the following lemma. Lemma 4 Consider the proposed DCG algorithm defined in Algorithm 1. If Assumptions 1-4 hold, then\n¯ d t 1 n\nnX\ni=1\nrFi(¯xt)\n (1 ↵)tG+ ✓ (1 ↵)LD\n↵T +\nLD T (1 ) ◆ . (21)\nBy combining Lemmas 3 and 4 and setting ↵ = 1/ p T we can conclude that the local gradient approximation vector d\nt i of each node i is within O(1/\np T ) distance of the global\nobjective gradient rF (¯xt) evaluated at ¯xt. We use this observation in the following theorem to show that the sequence of iterates generated by DCG achieves the tight (1 1/e) approximation ratio of the optimum global solution.\nTheorem 1 Consider the proposed DCG method outlined in Algorithm 1. Further, consider x⇤ as the global maximizer of Problem (4). If Assumptions 1-4 hold and we set ↵ = 1/ p T , for all nodes j 2 N , the local variable xTj obtained after T iterations satisfies\nF (xTj ) (1 e 1)F (x⇤) LD2 +GD T 1/2 GD\nT 1/2(1 ) LD 2\n2T GD + LD\n2\nT (1 ) . (22)\nTheorem 1 shows that the sequence of the local variables x\nt j , generated by DCG, is able to achieve the optimal approximation ratio (1 1/e), while the error term vanishes at a sublinear rate of O(1/T 1/2), i.e.,\nF (xTj ) (1 1/e)F (x⇤) O ✓\n1 (1 )T 1/2 ◆ , (23)\nwhich implies that the iterate of each node reaches an objective value larger than (1 1/e ✏)OPT after O(1/✏2) rounds of communication. It is worth mentioning that the result in Theorem 1 is consistent with classical results in decentralized optimization that the error term vanishes faster for the graphs with larger spectral gap 1 . We proceed to study the convergence properties of Discrete DCG in Algorithm 2. To do so, we first assume that the variance of the stochastic gradients r ˜Fi(x) used in Discrete DCG is bounded. We justify this assumption in Remark 1.\nAssumption 5 The variance of the unbiased estimators r ˜F (x) is bounded above by 2 over the convex set C, i.e., for any i 2 N and any vector x 2 C we can write\nE h kr ˜Fi(x) rFi(x)k2 i  2, (24)\nwhere the expectation is with respect to the randomness of the unbiased estimator.\nIn the following theorem, we show that Discrete DCG achieves a (1 1/e) approximation ration for Problem (5).\nTheorem 2 Consider our proposed Discrete DCG algorithm outlined in Algorithm 2. Recall the definition of the multilinear extension function Fi in (11). If Assumptions 1-5 hold and we set ↵ = T 1/2 and = T 2/3, then for all nodes j 2 N the local variables xTj obtained after running Discrete DCG for T iterations satisfy\nE ⇥ F (xTj ) ⇤ (1 e 1)F (x⇤) O ✓\n1 (1 )T 1/3 ◆ , (25)\nwhere x⇤ is the global maximizer of Problem (10).\nTheorem 2 states that the sequence of iterates generated by Discrete DCG achieves the tight (1 1/e ✏) approximation guarantee for Problem (10) after O(1/✏3) iterations. Remark 1 For any submodular set function h : 2V ! R with associated multilinear extension H , it can be shown that its Lipschitz constant L and the gradient norm G are both bounded above by mf\np|V |, where mf is the maximum marginal value of f , i.e., mf = maxi2V f({i}) (see, Hassani et al. (2017)). Similarly, it can be shown that for the unbiased estimator in Appendix 9.7 we have  mf p|V |."
  }, {
    "heading": "7. Numerical Experiments",
    "text": "We will consider a discrete setting for our experiments and use Algorithm 2 to find a decentralized solution. The main objective is to demonstrate how consensus is reached and how the global objective increases depending on the topology of the network and the parameters of the algorithm.\nFor our experiments, we have used the MovieLens data set. It consists of 1 million ratings (from 1 to 5) by M = 6000 users for p = 4000 movies. We consider a network of n = 100 nodes. The data has been distributed equally between the nodes of the network, i.e., the set of users has been partitioned into 100 equally-sized sets and each node in the network has access to only one chunk (partition) of the data. The global task is to find a set of k movies that are most satisfactory to all the users (the precise formulation will appear shortly). However, as each of the nodes in the network has access to the data of a small portion of the users, the nodes have to cooperate to fulfill the global task.\nWe consider a well motivated objective function for the experiments. Let r`,j denote the rating of user ` for movie j (if such a rating does not exist in the data we assign r`,j to 0). We associate to each user ` a “facility location” objective function g`(S) = maxj2S r`,j , where S is any subset of\nthe movies (i.e. the ground set V is the set of the movies). Such a function shows how much user ` will be “satisfied” by a subset S of the movies. Recall that each node i in the network has access to the data of a (small) subset of users which we denote by Ui. The objective function associated with node i is given by fi(S) = P `2Ui g`(S). With such a choice of the local functions, our global task is hence to solve problem (5) when the matroid I is the k-uniform matroid (a.k.a. the k-cardinality constraint).\nWe consider three different choices for the underlying communication graph between the 100 nodes: A line graph (which looks like a simple path from node 1 to node 100), an Erdos-Renyi random graph (with average degree 5), and a complete graph. The matrix W is chosen as follows (based on each of the three graphs). If (i, j) is and edge of the graph, we let wi,j = 1/(1+max(di, dj)). If (i, j) is not an edge and i, j are distinct integers, we have wi,j = 0. Finally we let wi,i = 1 P j2N wi,j . It is not hard to show that the above choice for W satisfies Assumption 1.\nFigure 1 shows how consensus is reached w.r.t each of the three underlying networks. To measure consensus, we plot the (logarithm of) distance-to-average value 1n Pn i=1 ||xTi\n¯ x T || as a function of the total number of iterations T averaged over many trials (see (16) for the definition of ¯xT ). It is easy to see that the distance to average is small if and only if all the local decisions xTi are close to the average decision ¯xT . As expected, it takes much less time to reach consensus when the underlying graph is fully connected (i.e. complete graph). For the line graph, the convergence is very slow as this graph has the least degree of connectivity.\nFigure 2 depicts the obtained objective value of Discrete DCG (Algorithm 2) for the three networks considered above. More precisely, we plot the value 1n Pn i=1 f(x T i ) obtained\nat the end of Algorithm 2 as a function of the cardinality constraint k. We also compare these values with the value obtained by the centralized greedy algorithm (i.e. the centralized solution). A few comments are in order. The performance of Algorithm 2 is close to the centralized solution when the underlying graph is the Erdos-Renyi (with average degree 5) graph or the complete graphs. This is because for both such graphs consensus is achieved from the early stages of the algorithm. By increasing T , we see that the performance becomes closer to the centralized solution. However, when the underlying graph is the line graph, then consensus will not be achieved unless the number of iterations is significantly increased. Consequently, for small number of iterations (e.g. T  1000) the performance of the algorithm will not be close to the centralized solution."
  }, {
    "heading": "8. Conclusion",
    "text": "In this paper, we proposed the first fully decentralized optimization method for maximizing discrete and continuous submodular functions. We developed Decentralized Continuous Greedy (DCG) that achieves a (1 1/e ✏) approximation guarantee with O(1/✏2) and (1/✏3) local rounds of communication in the continuous and discrete settings, respectively."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was done while A. Mokhtari was visiting the Simons Institute for the Theory of Computing, and his work was partially supported by the DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF grant #CCF-1740425. The work of A. Karbasi was supported by DARPA Young Faculty Award (D16AP00046) and AFOSR YIP (FA9550-18-1-0160)."
  }],
  "year": 2018,
  "references": [{
    "title": "Data management for the internet of things: Design primitives and solution",
    "authors": ["Abu-Elkheir", "Mervat", "Hayajneh", "Mohammad", "Ali", "Najah Abu"],
    "year": 2013
  }, {
    "title": "Submodular functions: from discrete to continuous domains",
    "authors": ["F. Bach"],
    "venue": "arXiv preprint arXiv:1511.00394,",
    "year": 2015
  }, {
    "title": "Newton-like method with diagonal correction for distributed optimization",
    "authors": ["Bajovic", "Dragana", "Jakovetic", "Dusan", "Krejic", "Natasa", "Jerinkic", "Natasa Krklec"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2017
  }, {
    "title": "Scaling up machine learning: Parallel and distributed approaches",
    "authors": ["Bekkerman", "Ron", "Bilenko", "Mikhail", "Langford", "John"],
    "year": 2011
  }, {
    "title": "Parallel and distributed computation: numerical methods, volume 23",
    "authors": ["Bertsekas", "Dimitri P", "Tsitsiklis", "John N"],
    "venue": "Prentice hall Englewood Cliffs, NJ,",
    "year": 1989
  }, {
    "title": "Guaranteed nonconvex optimization: Submodular maximization over continuous domains",
    "authors": ["Bian", "Andrew An", "Mirzasoleiman", "Baharan", "Buhmann", "Joachim M", "Krause", "Andreas"],
    "year": 2017
  }, {
    "title": "Fastest mixing markov chain on a graph",
    "authors": ["Boyd", "Stephen", "Diaconis", "Persi", "Xiao", "Lin"],
    "venue": "SIAM review,",
    "year": 2004
  }, {
    "title": "Submodular maximization with cardinality constraints",
    "authors": ["Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy"],
    "venue": "SODA",
    "year": 2014
  }, {
    "title": "A tight linear time (1/2)-approximation for unconstrained submodular maximization",
    "authors": ["Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy"],
    "venue": "SIAM Journal on Computing,",
    "year": 2015
  }, {
    "title": "Maximizing a monotone submodular function subject to a matroid constraint",
    "authors": ["Calinescu", "Gruia", "Chekuri", "Chandra", "Pál", "Martin", "Vondrák", "Jan"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "On the convergence rate of a distributed augmented Lagrangian optimization algorithm",
    "authors": ["Chatzipanagiotis", "Nikolaos", "Zavlanos", "Michael M"],
    "venue": "In (ACC),",
    "year": 2015
  }, {
    "title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes",
    "authors": ["Chekuri", "Chandra", "Vondrák", "Jan", "Zenklusen", "Rico"],
    "venue": "SIAM J. Comput.,",
    "year": 2014
  }, {
    "title": "Online continuous submodular maximization",
    "authors": ["Chen", "Lin", "Hassani", "Hamed", "Karbasi", "Amin"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "The power of randomization: Distributed submodular maximization on massive datasets",
    "authors": ["da Ponte Barbosa", "Rafael", "Ene", "Alina", "Nguyen", "Huy L", "Ward", "Justin"],
    "venue": "In ICML,",
    "year": 2015
  }, {
    "title": "Next: In-network nonconvex optimization",
    "authors": ["Di Lorenzo", "Paolo", "Scutari", "Gesualdo"],
    "venue": "IEEE Trans. on Signal and Information Process. over Networks,",
    "year": 2016
  }, {
    "title": "Dual averaging for distributed optimization: Convergence analysis and network scaling",
    "authors": ["Duchi", "John C", "Agarwal", "Alekh", "Wainwright", "Martin J"],
    "venue": "IEEE Transactions on Automatic control,",
    "year": 2012
  }, {
    "title": "Designing smoothing functions for improved worst-case competitive ratio in online optimization",
    "authors": ["Eghbali", "Reza", "Fazel", "Maryam"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "Decentralized quasi-Newton methods",
    "authors": ["Eisen", "Mark", "Mokhtari", "Aryan", "Ribeiro", "Alejandro"],
    "venue": "IEEE Transactions on Signal Processing,",
    "year": 2017
  }, {
    "title": "A threshold of ln n for approximating set cover",
    "authors": ["Feige", "Uriel"],
    "venue": "Journal of the ACM (JACM),",
    "year": 1998
  }, {
    "title": "Maximizing non-monotone submodular functions",
    "authors": ["Feige", "Uriel", "Mirrokni", "Vahab S", "Vondrak", "Jan"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "Greed is good: Near-optimal submodular maximization via greedy optimization",
    "authors": ["Feldman", "Moran", "Harshaw", "Christopher", "Karbasi", "Amin"],
    "venue": "arXiv preprint arXiv:1704.01652,",
    "year": 2017
  }, {
    "title": "Online submodular maximization under a matroid constraint with application to learning assignments",
    "authors": ["Golovin", "Daniel", "Krause", "Andreas", "Streeter", "Matthew"],
    "venue": "arXiv preprint arXiv:1407.1082,",
    "year": 2014
  }, {
    "title": "NESTT: A nonconvex primal-dual splitting method for distributed and stochastic optimization",
    "authors": ["Hajinezhad", "Davood", "Hong", "Mingyi", "Zhao", "Tuo", "Wang", "Zhaoran"],
    "year": 2016
  }, {
    "title": "Gradient methods for submodular maximization",
    "authors": ["Hassani", "S. Hamed", "Soltanolkotabi", "Mahdi", "Karbasi", "Amin"],
    "venue": "In NIPS,",
    "year": 2017
  }, {
    "title": "Fast distributed gradient methods",
    "authors": ["Jakovetić", "Dušan", "Xavier", "Joao", "Moura", "José MF"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2014
  }, {
    "title": "Linear convergence rate of a class of distributed augmented Lagrangian algorithms",
    "authors": ["Jakovetic", "Dusan", "Moura", "Jose MF", "Xavier", "Joao"],
    "venue": "Automatic Control, IEEE Transactions on,",
    "year": 2015
  }, {
    "title": "Fast greedy algorithms in mapreduce and streaming",
    "authors": ["Kumar", "Ravi", "Moseley", "Benjamin", "Vassilvitskii", "Sergei", "Vattani", "Andrea"],
    "venue": "TOPC,",
    "year": 2015
  }, {
    "title": "Remote sensing big data computing: Challenges and opportunities",
    "authors": ["Ma", "Yan", "Wu", "Haiping", "Wang", "Lizhe", "Huang", "Bormin", "Ranjan", "Rajiv", "Zomaya", "Albert", "Jie", "Wei"],
    "venue": "Future Generation Computer Systems,",
    "year": 2015
  }, {
    "title": "The landscape of empirical risk for non-convex losses",
    "authors": ["Mei", "Song", "Bai", "Yu", "Montanari", "Andrea"],
    "venue": "arXiv preprint arXiv:1607.06534,",
    "year": 2016
  }, {
    "title": "Randomized composable core-sets for distributed submodular maximization",
    "authors": ["Mirrokni", "Vahab S", "Zadimoghaddam", "Morteza"],
    "venue": "STOC",
    "year": 2015
  }, {
    "title": "Distributed submodular maximization: Identifying representative elements in massive data",
    "authors": ["Mirzasoleiman", "Baharan", "Karbasi", "Amin", "Sarkar", "Rik", "Krause", "Andreas"],
    "venue": "In NIPS,",
    "year": 2013
  }, {
    "title": "Fast constrained submodular maximization: Personalized data summarization",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin"],
    "venue": "In ICML 2016,",
    "year": 2016
  }, {
    "title": "Network Newton distributed optimization methods",
    "authors": ["Mokhtari", "Aryan", "Ling", "Qing", "Ribeiro", "Alejandro"],
    "venue": "IEEE Trans. on Signal Process.,",
    "year": 2017
  }, {
    "title": "Conditional gradient method for stochastic submodular maximization: Closing the gap",
    "authors": ["Mokhtari", "Aryan", "Hassani", "Hamed", "Karbasi", "Amin"],
    "venue": "In AISTATS,",
    "year": 2018
  }, {
    "title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
    "authors": ["Mokhtari", "Aryan", "Hassani", "Hamed", "Karbasi", "Amin"],
    "venue": "arXiv preprint arXiv:1804.09554,",
    "year": 2018
  }, {
    "title": "Distributed subgradient methods for multi-agent optimization",
    "authors": ["Nedic", "Angelia", "Ozdaglar", "Asuman"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2009
  }, {
    "title": "Constrained consensus and optimization in multi-agent networks",
    "authors": ["Nedic", "Angelia", "Ozdaglar", "Asuman", "Parrilo", "Pablo A"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2010
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions–I",
    "authors": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "Accelerated distributed Nesterov gradient descent",
    "authors": ["Qu", "Guannan", "Li", "Na"],
    "venue": "arXiv preprint arXiv:1705.07176,",
    "year": 2017
  }, {
    "title": "Distributed greedy algorithm for satellite assignment problem with submodular utility function",
    "authors": ["Qu", "Guannan", "Brown", "Dave", "Li", "Na"],
    "venue": "IFAC-PapersOnLine,",
    "year": 2015
  }, {
    "title": "Distributed optimization in sensor networks",
    "authors": ["Rabbat", "Michael", "Nowak", "Robert"],
    "venue": "In Proceedings of the 3rd international symposium on Information processing in sensor networks,",
    "year": 2004
  }, {
    "title": "Generalized consensus computation in networked systems with erasure links",
    "authors": ["Rabbat", "Michael G", "Nowak", "Robert D"],
    "venue": "In SPAWC,",
    "year": 2005
  }, {
    "title": "Communication-efficient distributed optimization using an approximate Newton-type method",
    "authors": ["Shamir", "Ohad", "Srebro", "Nathan", "Zhang", "Tong"],
    "venue": "ICML",
    "year": 2014
  }, {
    "title": "Robust budget allocation via continuous submodular functions",
    "authors": ["Staib", "Matthew", "Jegelka", "Stefanie"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2017
  }, {
    "title": "Distributed nonconvex multiagent optimization over timevarying networks",
    "authors": ["Sun", "Ying", "Scutari", "Gesualdo", "Palomar", "Daniel"],
    "venue": "In Signals, Systems and Computers,",
    "year": 2016
  }, {
    "title": "Towards decentralization of multi-robot navigation functions",
    "authors": ["Tanner", "Herbert G", "Kumar", "Amit"],
    "venue": "ICRA",
    "year": 2005
  }, {
    "title": "Non-convex distributed optimization",
    "authors": ["Tatarenko", "Tatiana", "Touri", "Behrouz"],
    "venue": "IEEE Transactions on Automatic Control,",
    "year": 2017
  }, {
    "title": "Statistical learning theory",
    "authors": ["Vapnik", "Vladimir"],
    "venue": "ISBN",
    "year": 1998
  }, {
    "title": "Submodularity in combinatorial optimization",
    "authors": ["Vondrák", "Jan"],
    "year": 2007
  }, {
    "title": "Optimal approximation for the submodular welfare problem in the value oracle model",
    "authors": ["Vondrák", "Jan"],
    "venue": "In STOC, pp",
    "year": 2008
  }, {
    "title": "An analysis of the greedy algorithm for the submodular set covering problem",
    "authors": ["Wolsey", "Laurence A"],
    "year": 1982
  }, {
    "title": "A survey on security and privacy issues in internet-of-things",
    "authors": ["Yang", "Yuchen", "Wu", "Longfei", "Yin", "Guisheng", "Li", "Lijie", "Zhao", "Hongbin"],
    "venue": "IEEE Internet of Things Journal,",
    "year": 2017
  }, {
    "title": "On the convergence of decentralized gradient descent",
    "authors": ["Yuan", "Kun", "Ling", "Qing", "Yin", "Wotao"],
    "venue": "SIAM Journal on Optimization,",
    "year": 2016
  }, {
    "title": "DiSCO: Distributed optimization for self-concordant empirical loss",
    "authors": ["Zhang", "Yuchen", "Lin", "Xiao"],
    "venue": "ICML",
    "year": 2015
  }],
  "id": "SP:72aaa94823f0aaf1f13da184a45c34c63efc1af9",
  "authors": [{
    "name": "Aryan Mokhtari",
    "affiliations": []
  }, {
    "name": "Hamed Hassani",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight (1 1/e) approximation factor of the optimum global solution using only local computation and communication. We also provide strong convergence bounds as a function of network size and spectral characteristics of the underlying topology. Interestingly, DCG readily provides a simple recipe for decentralized discrete submodular maximization through the means of continuous relaxations. Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develop a consensus algorithm that also achieves the tight (1 1/e) approximation guarantee of the global discrete solution once a proper rounding scheme is applied.",
  "title": "Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings"
}