{
  "sections": [{
    "heading": "1. Introduction",
    "text": "A set function f : 2V → R is said to be submodular if for all sets S ⊆ T ⊆ V and every element v ∈ V we have f(S ∪ {v}) − f(S) ≥ f(T ∪ {v}) − f(T ). That is, the marginal contribution of any element v to the value of the function f(S) diminishes as the input set S increases. The theory of submodular maximization unifies and generalizes diverse problems in combinatorial optimization, including the Max-Cover, Max-Cut, and Facility Location problems. In turn, this theory has recently found numerous applications to problems in machine learning, data science, and artificial intelligence. A few such applications include exemplar-based clustering (Krause & Gomes, 2010), feature selection for classification (Krause & Guestrin, 2005), document and corpus summarization (Lin & Bilmes, 2011;\n1Yale University, New Haven, CT, USA 2Princeton University, Princeton, NJ, USA 3ETH Zurich, Zurich, Switzerland. Correspondence to: Marko Mitrovic <marko.mitrovic@yale.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nKirchhoff & Bilmes, 2014; Sipos et al., 2012), crowd teaching (Singla et al., 2014), and influence maximization in social networks (Kempe et al., 2003).\nSome of the most compelling use cases for these applications concern sensitive data about individuals (Mirzasoleiman et al., 2016a;b). As a running example, let us consider the specific problem of determining which of a collection of features (e.g. age, height, weight, etc.) are most relevant to a binary classification task (e.g. predicting whether an individual is likely to have diabetes). In this problem, a sensitive training set takes the form D = {(xi, yi)}ni=1 where each individual i’s data consists of features xi,1, . . . , xi,m together with a class label yi. The goal is to identify a small subset S ⊆ [m] of features which can then be used to build a good classifier for y. Many techniques exist for feature selection, including one based on maximizing a submodular function which captures the mutual information between a subset of features and the class label of interest (Krause & Guestrin, 2005). However, for both legal (e.g. compliance with HIPAA regulations) and ethical reasons, it is important that the selection of relevant features does not compromise the privacy of any individual who has contributed to the training data set. Unfortunately, the theory of submodular maximization does not in itself accommodate such privacy concerns.\nTo this end, we propose a systematic study of differentially private submodular maximization to enable these applications based on submodular maximization, while provably guaranteeing individual-level privacy. The notion of differential privacy (Dwork et al., 2006) offers a strong protection of individual-level privacy. Nevertheless, differential privacy has been shown to permit useful data analysis and machine learning tasks. In a nutshell, the definition formalizes a guarantee that no individual’s data should have too significant an effect on the outcome of a computation. We provide the formal definition in Section 2. Such a privacy guarantee is obtained through the introduction of random noise, so private submodular maximization is conceptually related to the problem of submodular maximization in the presence of noise (Cheraghchi, 2012; Hassidim & Singer, 2016).\nIn this work, we study the following problem under various conditions on the submodular objective function f (monotone vs. non-monotone), and various choices of the con-\nstraint C (cardinality, matroid, or p-extendible system). Problem 1. Given a sensitive dataset D associated to a submodular function fD : 2V → R: Find a subset S ∈ C ⊂ 2V that approximately maximizes fD(S) in a manner that guarantees differential privacy with respect to the input dataset D.\nAn important special case of this problem was studied in prior work of Gupta et al. (2010). They considered the “combinatorial public projects” problem (Papadimitriou et al., 2008), where given a dataset D = (x1, . . . , xn), the function fD takes the particular form fD(S) = 1 n ∑n i=1 fxi(S) for monotone submodular functions fxi : 2 V → [0, 1], and is to be maximized subject to a cardinality constraint |S| ≤ k. We call functions of this form decomposable. They presented a simple greedy algorithm, which will be central to our work, together with a tailored analysis which achieves strong accuracy guarantees in this special case.\nHowever, there are many cases of Problem 1 which do not fall into the combinatorial public projects framework. For some problems, including feature selection via mutual information, the submodular function fD of interest depends on the datasetD in ways much more complicated than averaging functions associated to each individual. The focus of our work is on understanding Problem 1 in circumstances which capture a broader class of useful applications and constraints in machine learning. We summarize our specific contributions in Section 1.2."
  }, {
    "heading": "1.1. The greedy paradigm",
    "text": "Even without concern for privacy, the problem of submodular maximization poses computational challenges. In particular, exact submodular maximization subject to a cardinality constraint is NP-hard. One of the principal approaches to designing efficient approximation algorithms is to use a greedy strategy (Nemhauser et al., 1978). Consider the problem of maximizing a set function f(S) subject to the cardinality constraint |S| ≤ k. In each of rounds i = 1, . . . , k, the basic greedy algorithm constructs Si from Si−1 by adding the element vi ∈ (V \\ Si−1) which maximizes the marginal gain f(Si−1 ∪ {vi}) − f(Si−1). Nemhauser et al. (1978) famously showed that this algorithm yields a (1−1/e)-approximation to the optimal value of f(S) whenever f is a monotone submodular function.\nIn the combinatorial public projects setting, Gupta et al. (2010) showed how to make the greedy algorithm compatible with differential privacy by randomizing the procedure for selecting each vi. This selection procedure is specified by the differentially private exponential mechanism of McSherry & Talwar (2007), which (probabilistically) guarantees that the vi selected in each round is almost as good as the true marginal gain maximizer. Remarkably, Gupta et al. (2010) show that the cumulative privacy guarantee of the\nresulting randomized greedy algorithm is not much worse than that of a single run of the exponential mechanism. This analysis is highly tailored to the structure of the combinatorial public projects problem. However, replacing this tailored analysis with the more generic “advanced composition theorem” for differential privacy (Dwork et al., 2010), one still obtains useful results for the more general class of “low-sensitivity” submodular functions."
  }, {
    "heading": "1.2. Our contributions",
    "text": "Table 1 summarizes the approximation guarantees we obtain for Problem 1 under increasingly more general classes of submodular functions fD (read top to bottom), and increasingly more general types of constraints (read left to right). In each entry, OPT denotes the value of the optimal non-private solution. Below we draw attention to a few particular contributions, including some that are not expressed in Table 1. Non-monotone objective functions. Submodular maximization for non-monotone functions is significantly more challenging than it is for monotone objectives. In particular, the basic greedy algorithm of Nemahauser et al. fails, and cannot guarantee any constant-factor approximation. Several works (Feldman et al., 2017; Mirzasoleiman et al., 2016a; Buchbinder et al., 2014; Feldman et al., 2011) have identified variations of the greedy algorithm that do yield constant-factor approximations for non-monotone objectives. However, it is not clear how to modify any of these algorithms to accommodate differential privacy.\nOur starting point is instead the “stochastic greedy” algorithm of Mirzasoleiman et al. (2015), which was originally designed to perform monotone submodular maximization in linear time. Drawing ideas from Buchbinder et al. (2014), we give a new analysis of the stochastic greedy algorithm to show that it also gives a 1e (1 − 1/e)approximation for non-monotone submodular functions. To our knowledge, this is the first algorithm making exactly |V | function evaluations which achieves a constantfactor approximation for either monotone or non-monotone objectives. Moreover, it is immediately clear how to use the exponential mechanism to make this algorithm differentially private.\nThis phenomenon is analogous to how stochastic variants of gradient descent are more naturally suited to providing differential privacy than their deterministic counterparts (Song et al., 2013; Bassily et al., 2014). That is, our results illustrate how techniques for making algorithms fast are also helpful in making them privacy-preserving.\nGeneral constraints. While a cardinality constraint is perhaps the most natural to place on a submodular maximization problem, some machine learning problems, e.g. personalized data summarization (Mirzasoleiman et al., 2016a), require the use of more general types of con-\nstraints. For instance, one may wish to maximize a submodular function f(S) subject to S ∈ I for an arbitrary matroid I, or subject to S being contained in an intersection of p matroids (more generally, a p-extendible system). For these types of constraints, the greedy algorithm still yields a constant factor approximation for monotone objective functions (Fisher et al., 1978; Jenkyns, 1976; Călinescu et al., 2011). We show in this work that the analysis provided by Călinescu et al. (2011) for matroids and p-extendible families can be adapted to handle additional error introduced for differential privacy.\nGeneral selection procedures. For worst-case datasets, the exponential mechanism is optimal within each round of private maximization. However, it may be sub-optimal for datasets enjoying additional structural properties. Fortunately, the greedy framework we use is flexible with regard to the choice of the selection procedure. For instance, one can replace the exponential mechanism in a black-box manner with the “large margin mechanism” of Chaudhuri et al. (2014) to obtain error bounds that replace the explicit dependence on log |V | in Table 1 with a term that may be significantly smaller for real datasets. We give a slightly simplified analysis of the large margin mechanism, and present it in a manner suitable for greedy algorithms which access the same data set multiple times. (These guarantees are more complicated, but spelled out in Section 5.) For submodular functions exhibiting additional structure, one may also be able to perform each maximization step with the “choosing mechanism” of Beimel et al. (2016) and Bun et al. (2015)."
  }, {
    "heading": "2. Preliminaries",
    "text": "Let V be finite set which we will refer to as the ground set and let X be a finite set which we will refer to as the data universe. A dataset is an n-tuple D = (x1, . . . , xn) ∈ Xn. Suppose each dataset D is associated to a set function fD : 2V → R. The manner in which fD depends on D will be application-specific, but it is assumed that the association between D and fD is public information. Definition 2. A set function fD : 2V → R is submodular if for all sets S ⊆ T ⊆ V and every element v ∈ V , we have fD(S ∪ {v})− fD(S) ≥ fD(T ∪ {v})− fD(T ). Moreover, If fD(S) ≤ fD(T ) whenever S ⊆ T , we say fD is monotone. If for every dataset D = (x1, . . . , xn), the function fD = 1n ∑n i=1 fxi for monotone submodular\nfunctions fxi : 2 V → [0, λ], we say fD is λ-decomposable. The problem of maximizing a decomposable submodular function was considered as the “combinatorial public projects problem” by Papadimitriou et al. (2008).\nWe are interested in the problem of approximately maximizing a submodular function subject to differential privacy. The definition of differential privacy relies on the notion of neighboring datasets, which are simply tuples D,D′ ∈ Xn that differ in at most one entry. If D,D′ are neighboring, we write D ∼ D′. Definition 3. A randomized algorithm M : Xn → R satisfies (ε, δ)-differential privacy if for all measurable sets T ⊆ R and all neighboring datasets D ∼ D′,\nPr[M(D) ∈ T ] ≤ eε Pr[M(D′) ∈ T ] + δ.\nDifferentially private algorithms must be calibrated to the sensitivity of the function of interest with respect to small changes in the input dataset, defined formally as follows.\nDefinition 4. The sensitivity of a set function fD : 2V → R (depending on a dataset D) with respect to a constraint C ⊆ 2V is defined as\nmax D∼D′ max S∈C |fD(S)− fD′(S)|.\nComposition of Differential Privacy. The analyses of our algorithms rely crucially on composition theorems for differential privacy. For a sequence of privacy parameters {(εi, δi)}ki=1, we informally refer to the k-fold adaptive composition of (εi, δi)-differentially private algorithms as the output of a mechanism M∗ that behaves as follows on an input D: In each of rounds i = 1, . . . , k, the algorithm M∗ selects an (εi, δi)-differentially private algorithm Mi possibly depending on the previous outcomes M1(D), . . . ,Mi(D) (but not directly on the sensitive dataset D itself), and releases Mi(D). For a formal treatment of adaptive composition, see (Dwork et al., 2010; Dwork & Roth, 2014).\nTheorem 5. (Dwork & Lei, 2009; Dwork et al., 2010; Bun & Steinke, 2016) The k-fold adaptive composition of (ε0, δ0)-differentially private algorithms satisfies (ε, δ)differential privacy where\n1. ε = kε0 and δ = kδ0. (Basic Composition). 2. ε = 12kε 2 0 + √\n2 log(1/δ′)ε0 and δ = δ′+ kδ, for any δ′ > 0. (Advanced Composition)\nExponential Mechanism. The exponential mechanism (McSherry & Talwar, 2007) is a general primitive for solving discrete optimization problems. Let q : V × Xn → R be a “quality” function measuring how good a solution v ∈ V is with respect to a dataset D ∈ Xn. We say a quality function q has sensitivity λ if for all v ∈ V and all neighboring datasets D ∼ D′, we have |q(v,D)− q(v,D′)| ≤ λ. Proposition 6. Let ε > 0 and let q : V ×Xn be a quality function with sensitivity λ. Define the exponential mechanism as the algorithm which selects every v ∈ V with probability proportional to exp(εq(v,D)/2λ). • The exponential mechanism provides (ε, 0)-\ndifferential privacy. • For every D ∈ Xn,\nE[q(v̂, D)] ≥ max v∈V q(v,D)− 2λ · ln |V | ε ,\nwhere v̂ is the output of the exponential mechanism on dataset D. The privacy guarantee and a “with high probability” utility guarantee of the exponential mechanism are due to McSherry & Talwar (2007). A simple proof of the utility guarantee in expectation appears in (Bassily et al., 2016)."
  }, {
    "heading": "3. Monotone Submodular Maximization",
    "text": "In this section, we present a variant of the basic greedy algorithm which will enable maximization of monotone submodular functions. This algorithm simply replaces each greedy selection step with a privacy-preserving selection algorithm denoted O. The selection function O takes as input a quality function q : U × Xn → R and a dataset D, as well as privacy parameters ε0, δ0, and outputs an element u ∈ U . We begin in the simplest case of monotone submodular maximization with a cardinality constraint (Algorithm 1). The algorithm for more general constraints appears in Section 3.1.\nAlgorithm 1 was already studied by Gupta et al. (2010) in the special case where fD is decomposable, and O is the exponential mechanism. We generalize their result to the much broader class of low-sensitivity monotone submodular functions.\nAlgorithm 1 Diff. Private Greedy (Cardinality) GO\nInput: Submodular function fD : 2V → R, dataset D, cardinality constraint k, privacy parameters ε0, δ0 Output: Size k subset of V\n1. Initialize S0 = ∅ 2. For i = 1, . . . , k:\n• Define qi : (V \\Si−1)×Xn → R via qi(v, D̃) = fD̃(Si−1 ∪ {v})− fD̃(Si−1) • Compute vi ←R O(qi, D; ε0, δ0) • Update Si ← (Si−1 ∪ {vi})\n3. Return Sk\nTheorem 7. (Gupta et al., 2010) Suppose fD : 2V → R is λ-decomposable (cf. Definition 2). Let δ > 0 and let ε0 ≥ 0 be such that ε = 2 · ε0 · (e− 1) ln(3e/δ) ≤ 1. Then instantiating Algorithm 1 with O = EM and parameter ε0 > 0 provides (ε, δ)-differential privacy.\nMoreover, for every D ∈ Xn, E [fD(Sk)] ≥ ( 1− 1\ne\n) OPT−2λk ln |V |\nε0 where Sk ←R GEM(D). Unfortunately, the privacy analysis of Theorem 7 makes essential use of the decomposability of fD, and does not directly generalize to arbitrary submodular functions of lowsensitivity. Replacing the privacy analysis of (Gupta et al., 2010) with the Composition Theorem 5 instead gives Theorem 8. Suppose fD : 2V → R is monotone and has sensitivity λ. Then instantiating Algorithm 1 withO = EM and parameter ε0 > 0 provides (ε = kε0, δ = 0)- differential privacy. It also provides (ε, δ)-differential privacy for every δ > 0 with ε = kε20/2 + ε0 · √ 2k ln(1/δ).\nMoreover, for every D ∈ Xn, E [fD(Sk)] ≥ ( 1− 1\ne\n) OPT−2λk ln |V |\nε0 where Sk ←R GEM(D).\n3.1. Matroid and p-Extendible System Constraints We now show how to extend Algorithm 1 to privately maximize monotone submodular functions subject to more general constraints. To start, we review the definition of a pextendible system. Consider a ground set V and a nonempty downward-closed family of subsets I ⊆ 2V (i.e. if T ∈ I, then S ∈ I for every S ⊆ T ). Such an I is called a family of independent sets. The pair (V, I) is said to be a pextendible system (Mestre, 2006) if for all S ⊂ T ∈ I, and v ∈ V such that S∪{v} ∈ I, there exists a set Z ⊆ (T \\S) such that |Z| ≤ p and (T \\ Z) ∪ {v} ∈ I. Let r(I) denote the size of the largest independent set in I.\nThe definition of a matroid coincides with that of a 1- extendible system (with rank r(I)). For p ≥ 2, the notion of a p-extendible system strictly generalizes that of an intersection of p matroids. A slight modification of Algorithm 1 gives a unified algorithm for privately maximizing a monotone submodular function subject to matroid and pextendible system constraints, presented as Algorithm 2. Theorem 9. Suppose fD : 2V → R is λ-decomposable (cf. Definition 2). Let δ > 0 and let ε0 ≥ 0 be such that ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1. Then instantiating Algorithm 2 with O = EM and parameter ε0 > 0 provides (ε, δ)-differential privacy. Moreover, for every D ∈ Xn,\nE [fD(S)] ≥ 1 p+ 1 ·OPT− p p+ 1\n( 2λr(I) ln |V |\nε0 ) where S ←R GEM(D).\nAlgorithm 2 Differentially Private Greedy (p-system) GO\nInput: Submodular function fD : 2V → R, dataset D, pextendible family (V, I), privacy parameters ε0, δ0 Output: Maximal independent subset of V\n1. Initialize S = ∅ 2. While S ∈ I is not maximal:\n• Define q : (V \\ S) × Xn → R via q(v, D̃) = fD̃(S ∪ {v})− fD̃(S) • Compute vi ←R O(q,D; ε0, δ0) • Update S ← (S ∪ {vi})\n3. Return S\nTheorem 10. Suppose fD : 2V → R has sensitivity λ. Then instantiating Algorithm 2 with O = EM and parameter ε0 > 0 provides (ε = r(I)ε0, δ = 0)-differential privacy. It also provides (ε, δ)-differential privacy for every δ > 0 with ε = r(I)ε2/2 + ε · √ 2r(I) ln(1/δ).\nMoreover, for every D ∈ Xn,\nE [fD(S)] ≥ 1 p+ 1 ·OPT− p p+ 1\n( 2λr(I) ln |V |\nε0 ) where S ←R GEM(D)."
  }, {
    "heading": "4. Non-Monotone Submodular Maximization",
    "text": "We now consider the problem of privately maximizing an arbitrary, possibly non-monotone, submodular function under a cardinality constraint. In general, the greedy algorithm presented in Section 3 fails to give any constantfactor approximation. Instead, our algorithm in this section will be based on the “stochastic greedy” algorithm first studied by Mirzasoleiman et al. (2015). In each round, the stochastic greedy algorithm first subsamples a random 1 k · ln(1/α) fraction of the ground set for some α > 0, and then greedily selects the item from this subsample that maximizes marginal gain. Mirzasoleiman et al. (2015) showed that for a monotone objective function f , this algorithm provides a (1− 1/e− α)-approximation to the optimal solution. Their original motivation was to improve the running time of the greedy algorithm: fromO(|V | ·k) evaluations of the objective function to linearO(|V | · ln(1/α)).\nUnfortunately, the stochastic greedy algorithm does not provide any approximation guarantee for non-monotone submodular functions. Buchbinder et al. (2014) instead proposed a “random greedy” algorithm that, in each iteration, randomly selects one of the k elements with the highest marginal gain. Buchbinder et al. (2014) showed that the random greedy algorithm achieves a 1/e approximation to the optimal solution (in expectation), using k|V | function evaluations. However, it is not clear how to adapt this algorithm to accommodate differential privacy, since its analysis has a brittle dependence on the sampling procedure.\nWe make two main contributions to the analysis of the\nstochastic greedy and random greedy algorithms. First, we show that running the stochastic greedy algorithm on an exact 1k fraction of the ground set per iteration still gives a (0.468)-approximation for monotone objectives, and moreover, gives a 1e (1 − 1/e)-approximation even for non-monotone objectives. Note that this algorithm evaluates the objective function on only |V | elements, and still provides a constant factor approximation guarantee. This makes our “subsample-greedy” algorithm the fastest algorithm for maximizing a general submodular function subject to a cardinality constraint (albeit with slightly worse approximation guarantees). Second, we show that the guarantees of this algorithm are robust to using a randomized greedy selection procedure (e.g. the exponential or large margin mechanism), and hence it can be adapted to ensure differential privacy.\nWe present the subsample-greedy algorithm as Algorithm 3 below. Assume that V is augmented by enough “dummy elements” to ensure that |V |/k is an integer; each dummy element u is defined so that fD(S ∪ {u}) = fD(S) for every set S. We also explicitly account for an additional set U of k dummy elements, and ensure that at least one appears in every subsample.\nAlgorithm 3 Diff. Private “Subsample-Greedy” SGO\nInput: Submodular function fD : 2V → R, dataset D, cardinality constraint k, privacy parameters ε0, δ0 Output: Size k subset of V\n1. Initialize S0 = ∅, dummy elements U = {u1, . . . , uk}\n2. For i = 1, . . . , k: • Sample Vi ⊂ V a uniformly random subset of\nsize |V |/k and ui a random dummy element • Define qi : (Vi∪{ui})×Xn → R via qi(v, D̃) = fD̃(Si−1 ∪ {v})− fD̃(Si−1) • Compute vi ←R O(qi, D; ε0, δ0) • Update Si ← (Si−1 ∪ {vi})\n3. Return Sk with all dummy elements removed\nTheorem 11. Suppose fD : 2V → R has sensitivity λ. Then instantiating Algorithm 3 with O = EM provides (ε, δ)-differential privacy, and for every D ∈ Xn,\nE [fD(S)] ≥ 1\ne\n( 1− 1\ne\n) OPT−2λk ln |V |\nε\nwhere S ←R SGEM(D). Moreover, if fD is monotone, then E [fD(S)] ≥ ( 1− e−(1−1/e) ) OPT−2λk ln |V |\nε\n≈ 0.468OPT−2λk ln |V | ε .\nThe guarantees of Theorem 11 are of interest even without privacy. Letting MAX denote the selection procedure which simply outputs the true maximizer (equivalently,\nwhich runs the exponential mechanism with ε0 = +∞), we obtain the following non-private algorithm for maximizing a submodular function fD:\nCorollary 12. Let fD : 2V → R be any submodular function. Instantiating Algorithm 3 with O = MAX gives\nE [fD(S)] ≥ 1\ne\n( 1− 1\ne\n) OPT\nwhere S ←R SGMAX(D). Moreover, if fD is monotone, then\nE [fD(S)] ≥ ( 1− e−(1−1/e) ) OPT ≈ 0.468OPT ."
  }, {
    "heading": "5. The Large Margin Mechanism",
    "text": "The accuracy guarantee of the exponential mechanism can be pessimistic on datasets where q(·, D) exhibits additional structure. For example, suppose that when the elements of V are sorted so that q(v1, D) ≥ q(v2, D) ≥ · · · ≥ q(v|V |, D), there exists an ` such that q(v1, D) q(v`+1, D). Then only the top ` ground set items are relevant to the optimization problem, so running the exponential mechanism on these should maintain differential privacy, but with error proportional to ln ` rather than to ln |V |. The large margin mechanism of Chaudhuri et al. (2014), like the exponential mechanism, generically solves discrete optimization problems. However, it automatically leverages this additional margin structure whenever it exists. Asymptotically, the error guarantee of the large margin mechanism is always at most that of the exponential mechanism, but can be much smaller when the data exhibits a margin for small `.\nFormally, given a quality function q : V × Xn → R and parameters ` ∈ N, γ > 0, a dataset D satisfies the (`, γ)margin condition if q(v`+1, D) < q(v1, D)− γ.\nFor each ` = 1, . . . , |V |, define\ng` = λ · ( 3 + 4 ln(2`/δ)\nε ) G` = 8λ ln(2/δ)\nε +\n16λ ln(7`2/δ)\nε + g`.\nRecall that the Laplace distribution Lap(b) is specified by the density function 12b exp(−|x|/b).\nReplacing the exponential mechanism with the large margin mechanism gives analogues of our results for monotone submodular maximization with a cardinality constraint, monotone submodular maximization over a p-extendible system, and non-monotone submodular maximization with a cardinality constraint:\nTheorem 13. Suppose fD : 2V → R is monotone and has sensitivity λ. Then instantiating Algorithm 1 with O = LMM and parameters ε0, δ0 = 0 provides (kε0, kδ0)-differential privacy. It also provides (ε, δ′ +\nkδ0)-differential privacy for every δ′ > 0 with ε = kε2/2+ ε · √ 2k ln(1/δ′).\nMoreover, for every D ∈ Xn, there exists an event E with Pr[E] ≥ 1− β such that\nE [fD(Sk)|E] ≥ ( 1− 1\ne\n) OPT− k∑ i=1 4λ ln `i ε0\nwhere Sk ←R GLMM(D), and D satisfies the (`i, γi)margin condition with respect to every function of the form qi(v,D) = fD(Ŝi−1 ∪ {v}) − fD(Ŝi−1), with γi = 24λ ln(k/β)/ε+G`i .\nTheorem 14. Instantiating Algorithm 2 with O = LMM under all of the conditions of Theorem 13 gives the same privacy guarantee (replacing k with r(I)) and gives\nE [fD(S)|E] ≥ 1\np+ 1 ·OPT− r(I)∑ i=1 4λ ln `i ε0 .\nTheorem 15. Instantiating Algorithm 3 with O = LMM under all of the conditions of Theorem 13 gives the same privacy guarantee and gives\nE [fD(Sk)|E] ≥ 1\ne\n( 1− 1\ne\n) OPT− k∑ i=1 4λ ln `i ε0 .\nMoreover, if fD is monotone, then E [fD(Sk)|E] ≥ 0.468OPT− k∑ i=1 4λ ln `i ε0 ."
  }, {
    "heading": "6. Experimental Results",
    "text": "In this section we describe two concrete applications of our mechanisms."
  }, {
    "heading": "6.1. Location Privacy",
    "text": "We analyze a dataset of 10,000 Uber pickups in Manhattan in April 2014 (UberDataset). Each individual entry in the dataset consists of the longitude and latitude coordinates of the pickup location. We want to use this dataset to select k public locations as waiting spots for idle Uber drivers, while also guaranteeing differential privacy for the passengers whose locations appear in this dataset.1 We consider two different public sets of locations L:\n• LPopular is a set of 33 popular locations in Manhattan. • LGrid is a set of 33 locations spread evenly across\nManhattan in a grid-like manner.\nWe define a utility function M(i, j) to be the normalized Manhattan distance between a pickup location i and the waiting location j. That is, if pickup location i is located at coordinates (i1, i2) and the waiting location j is located at coordinates (j1, j2), then M(i, j) = |i1 − j1|+ |i2 − j2|\nm ,\nwhere m = 0.266 is simply the Manhattan distance between the two furthest spread apart points in Manhattan.\n1Under the assumption that each pickup corresponds to a unique individual.\nThis normalization ensures that 0 ≤ M(i, j) ≤ 1, for all i, j. In order to make sure we have a maximization problem, we define the following objective function: fD(S) = n−\n∑ i∈D min j∈S M(i, j), where n = |D| = 10000.\nObservation 16. The function fD is λ-decomposable for λ = 1 (and hence has sensitivity 1).\nThis form of objective function is known to be monotone submodular and so we can use the greedy algorithms studied in this paper. We use = 0.1 and δ = 2−20. For our settings of parameters, “basic composition” outperforms “advanced composition,” so the privacy budget of = 0.1 is split equally across the k iterations, meaning the mechanism at each iteration uses 0 = k . Our figures plot the average utility across 100 simulations.\nFrom Figures 1(a) and (b) we see that the results for both LPopular and LGrid are relatively similar and unsurprising. The non-private greedy algorithm achieves the highest utility, but both the exponential mechanism (EM)-based greedy and large margin mechanism (LMM)-based greedy algorithms exhibit comparable utility while preserving a high level of privacy. Interestingly, we also see that the utilities of the EM-based and LMM-based algorithms are almost identical for both LPopular and LGrid. This indicates that our mechanisms are actually selecting good locations, rather than just getting lucky because there are a lot of good locations to choose from.\nFigures 1(c) and (d) show how the utility of the EM-based\nand LMM-based algorithms vary with the privacy parameter . We can also think of this as varying the dataset size for a fixed . We fix k = 3 and take the average of 100 simulations for each value of . We see that even for very small , our algorithms outperform fully random selection. As increases, so does the utility. It is not shown in this figure, but varying δ has very little effect.\nFrom Figures 1(e) - (h), we see that the both the non-private and private algorithms select public locations that are relatively close to each other. For example, for the LPopular set of locations, the Empire State Building is close to the New York Public Library, the Soho Grand Hotel is close to NYU, and the Grand Army Plaza is close to the UN Headquarters. As a result, the private mechanisms manage to achieve comparable utility, while also masking the users’ exact locations.\nThe theory described in Section 5 suggests that, at least asymptotically, the large margin mechanism-based algorithm should outperform the exponential mechanism-based algorithm. However, in our experiments, we find that the large margin mechanism is generally only able to find a margin in the first iteration of the greedy algorithm. This is because the threshold for finding a margin depends only on , δ, and n and thus it stays the same across all k iterations. On the other hand, the marginal gain at each iteration drops very quickly, so the mechanism fails to find a margin and thus samples from all remaining locations. However, since the large margin mechanism spends half of its privacy bud-\nget to try to find a margin, the sampling step gives slightly worse guarantees than does the plain exponential mechanism, thus giving us the slightly weaker results we see in the figures."
  }, {
    "heading": "6.2. Feature Selection Privacy",
    "text": "We analyze a dataset created from a combination of National Health Examination Surveys ranging from 2007 to 2014 (NHANESDataset). There are n = 23, 876 individuals in the dataset with information on whether or not they have diabetes, along with m = 23 other potentially related binary health features. Our goal is to privately select k of these features that provide as much information about the diabetes class variable as possible.\nMore specifically, our goal is to maximize the mutual information between Y and XS , where Y is a binary random variable indicating whether or not an individual has diabetes and XS is a random variable that represents a set S of k binary health features. Mutual information takes the form:\nI(Y ;X) = ∑ y∈Y ∑ x∈X p(x, y) log2 ( p(x, y) p(x)p(y) ) .\nUnder the Naive Bayes assumption, we suppose the joint distribution on (Y,X1, . . . , Xk) takes the form\np(y, x1, . . . , xk) = p(y) k∏ i=1 p(xi | y). Therefore, we can easily specify the entire probability distribution by finding each p(xi | y). We estimate each p(xi | y) by counting frequencies in the dataset.\nOur goal is to choose a size k subset S of the features in order to maximize fD(S) = I(Y ;XS). Mutual information (under the Naive Bayes assumption) for feature selection is known to be monotone submodular in S (Krause & Guestrin, 2005), and thus we can apply the greedy algorithms described in this paper. Claim 17. In iteration i of the greedy algorithm, the sensitivity of fD(S) is (2i+1) log2(n) n . We run 1,000 simulations with = 1.0 and δ = 2−20. As we can see from Figure 2(b), our private mechanisms\nmaintain a comparable utility relative to the non-private algorithm. We also observe an interesting phenomenon where the expected utility obtained by our mechanism is not necessarily monotonically increasing with the number of features selected. This is an artifact of the fact that if we are selecting k features, then composition requires us to divide so that each iteration uses privacy budget k . This is problematic for this particular application because there happens to be one feature (insulin administration) that has much higher value than the rest. Therefore, the reduced probability of picking this single best feature (as a result of the lower privacy parameter k ) is not compensated for by selecting more features.\nFrom Figure 2(c), we see that both the private and nonprivate mechanisms generally select insulin administration as the top feature. However, while all three of the top features selected by the non-private algorithm are clearly related to diabetes, the non-private mechanisms tend to select one feature (in our case, gender or having received a blood transfusion) that may not be quite as relevant."
  }, {
    "heading": "7. Conclusion",
    "text": "We have presented a general framework for maximizing submodular functions while guaranteeing differential privacy. Our results demonstrate that simple and flexible greedy algorithms can preserve privacy while achieving competitive guarantees for a variety of submodular maximization problems: for all functions under cardinality constraints, as well as for monotone functions under matroid and p-extendible system constraints. Via our motivation to identify algorithms that could be made differentially private, we discovered a non-monotone submodular maximization algorithm that achieves guarantees that are novel even without concern for privacy. Finally, our experiments show that our algorithms are indeed competitive with their non-private counterparts.\nAcknowledgments. This work was supported by DARPA Young Faculty Award (D16AP00046), Simons-Berkeley fellowship, and ERC StG 307036. This work was done in part while Amin Karbasi and Andreas Krause were visiting the Simons Institute for the Theory of Computing."
  }],
  "year": 2017,
  "references": [{
    "title": "Private empirical risk minimization: Efficient algorithms and tight error bounds",
    "authors": ["Bassily", "Raef", "Smith", "Adam D", "Thakurta", "Abhradeep"],
    "venue": "In FOCS, pp",
    "year": 2014
  }, {
    "title": "Algorithmic stability for adaptive data analysis",
    "authors": ["Bassily", "Raef", "Nissim", "Kobbi", "Smith", "Adam D", "Steinke", "Thomas", "Stemmer", "Uri", "Ullman", "Jonathan"],
    "venue": "In STOC,",
    "year": 2016
  }, {
    "title": "Private learning and sanitization: Pure vs. approximate differential privacy",
    "authors": ["Beimel", "Amos", "Nissim", "Kobbi", "Stemmer", "Uri"],
    "venue": "Theory of Computing,",
    "year": 2016
  }, {
    "title": "Submodular maximization with cardinality constraints",
    "authors": ["Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy"],
    "venue": "In SODA, pp",
    "year": 2014
  }, {
    "title": "Concentrated differential privacy: Simplifications, extensions, and lower bounds",
    "authors": ["Bun", "Mark", "Steinke", "Thomas"],
    "venue": "In TCC,",
    "year": 2016
  }, {
    "title": "Differentially private release and learning of threshold functions",
    "authors": ["Bun", "Mark", "Nissim", "Kobbi", "Stemmer", "Uri", "Vadhan", "Salil P"],
    "venue": "In FOCS, pp",
    "year": 2015
  }, {
    "title": "Maximizing a monotone submodular function subject to a matroid constraint",
    "authors": ["Călinescu", "Gruia", "Chekuri", "Chandra", "Pál", "Martin", "Vondrák", "Jan"],
    "venue": "SIAM Journal on Computing,",
    "year": 2011
  }, {
    "title": "The large margin mechanism for differentially private maximization",
    "authors": ["Chaudhuri", "Kamalika", "Hsu", "Daniel J", "Song", "Shuang"],
    "venue": "In NIPS, pp",
    "year": 2014
  }, {
    "title": "Submodular functions are noise stable",
    "authors": ["Cheraghchi", "Mahdi"],
    "venue": "In SODA,",
    "year": 2012
  }, {
    "title": "Differential privacy and robust statistics",
    "authors": ["Dwork", "Cynthia", "Lei", "Jing"],
    "venue": "In STOC, pp",
    "year": 2009
  }, {
    "title": "The algorithmic foundations of differential privacy",
    "authors": ["Dwork", "Cynthia", "Roth", "Aaron"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Calibrating noise to sensitivity in private data analysis",
    "authors": ["Dwork", "Cynthia", "McSherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam D"],
    "venue": "In TCC, pp",
    "year": 2006
  }, {
    "title": "Boosting and differential privacy",
    "authors": ["Dwork", "Cynthia", "Rothblum", "Guy N", "Vadhan", "Salil P"],
    "venue": "In FOCS, pp",
    "year": 2010
  }, {
    "title": "A unified continuous greedy algorithm for submodular maximization",
    "authors": ["Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy"],
    "venue": "In FOCS,",
    "year": 2011
  }, {
    "title": "Greed is good: Near-optimal submodular maximization via greedy optimization",
    "authors": ["Feldman", "Moran", "Harshaw", "Christopher", "Karbasi", "Amin"],
    "venue": "In COLT,",
    "year": 2017
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions - II",
    "authors": ["Fisher", "Marshall L", "Nemhauser", "George L", "Wolsey", "Laurence A"],
    "venue": "Mathematical Programming Study,",
    "year": 1978
  }, {
    "title": "Differentially private combinatorial optimization",
    "authors": ["Gupta", "Anupam", "Ligett", "Katrina", "McSherry", "Frank", "Roth", "Aaron", "Talwar", "Kunal"],
    "venue": "In SODA, pp",
    "year": 2010
  }, {
    "title": "Submodular optimization under noise",
    "authors": ["Hassidim", "Avinatan", "Singer", "Yaron"],
    "venue": "CoRR, abs/1601.03095,",
    "year": 2016
  }, {
    "title": "The efficacy of the “greedy",
    "authors": ["T.A. Jenkyns"],
    "venue": "South Eastern Conference on Combinatorics, Graph Theory and Computing,",
    "year": 1976
  }, {
    "title": "Maximizing the spread of influence through a social network",
    "authors": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "Éva"],
    "venue": "In KDD,",
    "year": 2003
  }, {
    "title": "Submodularity for data selection in statistical machine translation",
    "authors": ["Kirchhoff", "Katrin", "Bilmes", "Jeff"],
    "venue": "In EMNLP,",
    "year": 2014
  }, {
    "title": "Near-optimal nonmyopic value of information in graphical models",
    "authors": ["A. Krause", "C. Guestrin"],
    "venue": "In UAI,",
    "year": 2005
  }, {
    "title": "Budgeted nonparametric learning from data streams",
    "authors": ["Krause", "Andreas", "Gomes", "Ryan G"],
    "venue": "In ICML,",
    "year": 2010
  }, {
    "title": "A class of submodular functions for document summarization",
    "authors": ["Lin", "Hui", "Bilmes", "Jeff"],
    "venue": "In ACL,",
    "year": 2011
  }, {
    "title": "Mechanism design via differential privacy",
    "authors": ["McSherry", "Frank", "Talwar", "Kunal"],
    "venue": "In FOCS, pp",
    "year": 2007
  }, {
    "title": "Greedy in approximation algorithms",
    "authors": ["Mestre", "Julián"],
    "venue": "In ESA, pp",
    "year": 2006
  }, {
    "title": "Lazier than lazy greedy",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin", "Vondrak", "Jan", "Krause", "Andreas"],
    "venue": "In AAAI,",
    "year": 2015
  }, {
    "title": "Fast constrained submodular maximization: Personalized data summarization",
    "authors": ["Mirzasoleiman", "Baharan", "Badanidiyuru", "Ashwinkumar", "Karbasi", "Amin"],
    "venue": "In ICML,",
    "year": 2016
  }, {
    "title": "Fast distributed submodular cover: Public-private data summarization",
    "authors": ["Mirzasoleiman", "Baharan", "Zadimoghaddam", "Morteza", "Karbasi", "Amin"],
    "venue": "In NIPS,",
    "year": 2016
  }, {
    "title": "An analysis of approximations for maximizing submodular set functions - I",
    "authors": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"],
    "venue": "Mathematical Programming,",
    "year": 1978
  }, {
    "title": "On the hardness of being truthful",
    "authors": ["Papadimitriou", "Christos H", "Schapira", "Michael", "Singer", "Yaron"],
    "venue": "In FOCS, pp",
    "year": 2008
  }, {
    "title": "Near-optimally teaching the crowd to classify",
    "authors": ["Singla", "Adish", "Bogunovic", "Ilija", "Bartók", "Gábor", "Karbasi", "Amin", "Krause", "Andreas"],
    "venue": "In ICML,",
    "year": 2014
  }, {
    "title": "Temporal corpus summarization using submodular word coverage",
    "authors": ["Sipos", "Ruben", "Swaminathan", "Adith", "Shivaswamy", "Pannaga", "Joachims", "Thorsten"],
    "venue": "In CIKM,",
    "year": 2012
  }, {
    "title": "Stochastic gradient descent with differentially private updates",
    "authors": ["Song", "Shuang", "Chaudhuri", "Kamalika", "Sarwate", "Anand D"],
    "venue": "In GlobalSIP,",
    "year": 2013
  }],
  "id": "SP:0fe41911a6d96579be0452ab805e953e6230fbb1",
  "authors": [{
    "name": "Marko Mitrovic",
    "affiliations": []
  }, {
    "name": "Mark Bun",
    "affiliations": []
  }, {
    "name": "Andreas Krause",
    "affiliations": []
  }, {
    "name": "Amin Karbasi",
    "affiliations": []
  }],
  "abstractText": "Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacypreserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal solutions. Along the way, we analyze a new algorithm for non-monotone submodular maximization under a cardinality constraint, which is the first (even non-privately) to achieve a constant approximation ratio with a linear number of function evaluations. We additionally provide two concrete experiments to validate the efficacy of these algorithms.",
  "title": "Differentially Private Submodular Maximization: Data Summarization in Disguise"
}