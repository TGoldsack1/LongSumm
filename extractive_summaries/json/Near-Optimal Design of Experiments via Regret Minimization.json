{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Experimental design is an important problem in statistics and machine learning research (Pukelsheim, 2006). Consider a linear regression model\ny = Xβ0 +w, (1)\nwhere X ∈ Rn×p is a pool of n design points, y is the response vector, β0 is a p-dimensional unknown regression model and w is a vector of i.i.d. noise variables satisfying Ewi = 0 and Ew2i < ∞. The experimental design problem is to select a small subset of rows (i.e., design points) XS from the design pool X so that the statistical power of estimating β0 is maximized from noisy response yS on the selected designs XS .\nAs an example, consider a material synthesis application where p is the number of variables (e.g., temperature, pressure, duration) that are hypothesized to affect the quality of the synthesized material and n is the total number of combinations of different parameters of experimental conditions. As experiments are expensive and time-consuming,\n*Author names listed in alphabetic order. 1Microsoft Research, Redmond, USA 2Princeton University, Princeton, USA 3Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Yining Wang <yiningwa@cs.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\none wishes to select k n experimental settings from X that are the most statistically efficient for establishing a model that connects experimental parameters with synthesized material quality, y. The experimental design problem is also related to many machine learning tasks, such as linear bandits (Deshpande & Montanari, 2012; Huang et al., 2016), diversity sampling (Kulesza & Taskar, 2012) and active learning (Ma et al., 2013; Chaudhuri et al., 2015; Hazan & Karnin, 2015; Balcan & Long, 2013; Wang & Singh, 2016).\nSince statistical efficiency can be measured in various ways, there exist a number of optimality criteria to guide the selection of experiments. We review some optimality criteria in Sec. 2 and interested readers are referred to Sec. 6 of (Pukelsheim, 2006) for a comprehensive review.\nTypically, an optimality criterion is a function f : S+p → R that maps from the p-dimensional positive definite cone to a real number. The experimental design problem can then be formulated as a combinatorial optimization problem:\nS∗(k) = arg min S∈S(n,k) f(X>SXS), (2)\nwhere S is either a set or a multi-set of size k, and XS ∈ Rk×p is formed by stacking the rows of X that are in S. The constraint set S1/2(n, k) is defined as follows: 1. With replacement: S1(n, k) = {S multi-set : S ⊆\n[n], |S| ≤ k}. Under this setting, XS may contain duplicate rows of the design pool X;\n2. Without replacement: S2(n, k) = {S standard set : S ⊆ [n], |S| ≤ k}. Under this setting, XS only contains distinct rows of the design pool X.\nThe “with replacement” setting is classical in statistics literature, where the multiple measurements in y with respect to the same design point lead to different values with statistically independent noise. The “without replacement” setting, on the other hand, is more relevant in machine learning applications, because labels are not likely to change if the same data point (e.g., the same image) is considered twice. Finally, it is worth pointing out that the “with replacement” setting is easier, because it can be reduced (in polynomial time) to the “without replacement” setting by replicating each row of X for k times.\nFor many popular choices of f , the exact optimization\nproblem in Eq. (2) is NP-hard (Çivril & Magdon-Ismail, 2009; Černỳ & Hladı́k, 2012). In this paper, we propose a computationally tractable algorithm that approximately computes Eq. (2) for a wide range of optimality criteria, and under very weak conditions on n, k and p.\nBelow is our main theorem:\nTheorem 1.1. Suppose b ∈ {1, 2}, n > k > p and let f : S+p → R be a regular optimality criterion (cf. Definition 2.1). There exists a polynomial-time algorithm that outputs Ŝ ∈ Sb(n, k) for any input matrix X ∈ Rn×p with full column rank, and Ŝ satisfies the following:\n1. For b = 1 (with replacement), there exists an absolute constant C0 ≤ 32 such that, for any ε ∈ (0, 1), if k ≥ C0p/ε 2 then\nf(X> Ŝ XŜ) ≤ (1 + ε) · min S∈S1(n,k) f(X>SXS) . (3)\n2. For b = 2 (without replacement) and any ξ > 2, there exists constant C1(ξ) > 0 depending only on ξ such that, if k ≥ ξp then\nf(X> Ŝ XŜ) ≤ C1(ξ) · min S∈S2(n,k) f(X>SXS) . (4)\nMoreover, for ξ ≥ 4 we have C1(ξ) ≤ 32. 3. For b = 2 (without replacement) and any ε ∈ (0, 1/2),\nif k, r satisfy k ≥ 4(1 + 7ε)r and r ≥ p/ε2, then\nf(X> Ŝ XŜ) ≤ (1 + ε) · min S∈S2(n,r) f(X>SXS). (5)\nWe interpret the significance of Theorem 1.1 as follows.\n• Under a very mild condition of k > 2p, our polynomialtime algorithm finds a set Ŝ ⊂ [n] of size k, with objective value f(X>\nŜ XŜ) being at most O(1) a constant\ntimes the optimum. See Eq. (4).\n• If replacement (b = 1) or over-sampling (k > r) is allowed, the approximation ratio can be tightened to 1+ ε for arbitrarily small ε > 0. See Eq. (3) and (5).\n• In all of the three cases, we only require k to grow linearly in p. Recall that k ≥ p is necessary to ensure the singularity of X>\nŜ XŜ . In contrast, no polynomial-\ntime algorithm has achieved O(1) approximation in the regime k = O(p) for non-submodular optimality criteria (e.g., A- and V-optimality) under the without replacement setting.\n• Our algorithm works for any regular optimality criterion. To the best of our knowledge, no known polynomial-time algorithm can achieve a (1 + ε) approximation for the D- and T-optimality criteria, or even an O(1) approximation for the E- and G-optimality criteria. See Table 1 for a comparison.\nThe key idea behind our proof of Theorem 1.1 is a regret minimization characterization of the least eigenvalue of positive semidefinite (PSD) matrices. Similar ideas were developed in (Allen-Zhu et al., 2015; Silva et al., 2016) to construct efficient algorithms for linear-sized graph sparsifiers. In this paper we adopt the regret minimization framework and present novel potential function analysis for the specific application of experimental design."
  }, {
    "heading": "1.1. Notations",
    "text": "S+p is the positive definite cone of p × p matrices: a p × p symmetric matrix A belongs to S+p if and only if v>Av > 0 for all v ∈ Rp\\{0}. For symmetric matrices A and B, we write A B if v>(A − B)v ≥ 0 for all v ∈ Rp. The inner product 〈A,B〉 is defined as 〈A,B〉 = tr(B>A) = ∑pi,j=1 AijBij . We use ‖A‖2 = supv∈Rp\\{0} ‖Av‖2/‖v‖2 to denote the spectral norm, and ‖A‖F = √∑p i,j=1 A 2 ij = √ 〈A,A〉 to denote\nthe Frobenius norm of A. For A 0, we write B = A1/2 as the unique B 0 that satisfies B2 = A. For a design pool X ∈ Rn×p, we use xi ∈ Rp to denote the i-th row of X. We use σmin(A) for the least (smallest) singular value of a PSD matrix A."
  }, {
    "heading": "1.2. Related work",
    "text": "Experimental design is an old topic in statistics research (Pukelsheim, 2006; Fedorov, 1972). Computationally efficient experimental design algorithms (with provable guarantees) are, however, a less studied field. In the case of submodular optimality criteria (e.g., D- and T-optimality), the classical pipage rounding method (Ageev & Sviridenko, 2004; Horel et al., 2014; Ravi et al., 2016) combined with semi-definite programming results in computationally efficient algorithms that enjoy a constant approximation ratio. Bouhtou et al. (2010) improves the approximation ratio when k is very close to n. Deshpande & Rademacher (2010); Li et al. (2017) considered polynomial-time algorithms for sampling from a D-optimality criterion. These algorithms are not applicable to non-submodular criteria, such as A-, V-, E- or G-optimality.\nFor the particular A-optimality criterion, (Avron & Boutsidis, 2013) proposed a greedy algorithm with an approximation ratio of O(n/k) with respect to f(X>X). It was shown that in the worst case min|S|≤k f(X>SXS) ≈ O(n/k) · f(X>X) and hence the bound is tight. However, for general design pool min|S|≤k f(X>SXS) could be far smaller than O(n/k) · f(X>X), making the theoretical results powerless in such scenarios. Wang et al. (2016) considered a variant of the greedy method and showed an approximation ratio quadratic in design dimension p and independent of pool size n.\nWang et al. (2016) derived algorithms based on effective resistance sampling (Spielman & Srivastava, 2011) that attain (1 + ε) approximation ratio if k = Ω(p log p/ε2) and repetitions of design points are allowed. The algorithm fundamentally relies on the capability of “re-weighting” (repeating) design points and cannot be adapted to the more general “without replacement” setting. Naive sampling based methods were considered in (Wang et al., 2016; Chaudhuri et al., 2015; Dhillon et al., 2013), which also achieve (1+ε) approximation but requires the subset size k to be much larger than the condition number of X.\nA related however different topic is low-rank matrix column subset selection and CUR approximation, which seeks column subset C and row subset R such that ‖X − CC†X‖F and/or ‖X − CUR‖F are minimized (Drineas et al., 2008; Boutsidis & Woodruff, 2014; Wang & Singh, 2015b; Drineas & Mahoney, 2005; Wang & Zhang, 2013; Wang & Singh, 2015a). These problems are unsupervised in nature and do not in general correspond to statistical\nproperties under supervised regression settings. Pilanci & Wainwright (2016); Raskutti & Mahoney (2014); Woodruff (2014) considered fast methods for solving ordinary least squares (OLS) problems. They are computationally oriented and typically require knowledge of the full response vector y, which is different from the experimental design problem."
  }, {
    "heading": "2. Regular criteria and continuous relaxation",
    "text": "We start with the definition of regular optimality criteria:\nDefinition 2.1 (Regular criteria). An optimality criterion f : S+p → R is regular if it satisfies the followig properties: 1. Convexity: 1 f(λA + (1 − λ)B) ≤ λf(A) + (1 − λ)f(B) for all λ ∈ [0, 1] and A,B ∈ S+p ;\n2. Monotonicity: If A B then f(A) ≥ f(B); 3. Reciprocal multiplicity: f(tA) = t−1f(A) for all t >\n0 and A ∈ S+p .\nAlmost all optimality criteria used in the experimental design literature are regular. Below we list a few popular examples; their statistical implications can be found in (Pukelsheim, 2006):\n- A-optimality (Average): fA(Σ) = 1p tr(Σ −1);\n- D-optimality (Determinant): fD(Σ) = (det |Σ|)− 1 p ;\n- T-optimality (Trace): fT (Σ) = p/tr(Σ);\n- E-optimality (Eigenvalue): fE(Σ) = ‖Σ−1‖2; - V-optimality (Variance): fV (Σ) = 1n tr(XΣ −1X>); - G-optimality: fG(Σ) = max diag(XΣ−1X>).\nThe (A-, D-, T-, E-) criteria concern estimates of regression coefficients and the (V-, G-) criteria are about insample predictions. All criteria listed above are regular. Note that for D-optimality the proxy function gD(Σ) = − log det(Σ) is considered to satisfy the convexity property. In addition, by the standard arithmetic inequality we have that fT ≤ fD ≤ fA ≤ fE and that fV ≤ fG. Although exact optimization of the combinatorial problem Eq. (2) is intractable, it is nevertheless easy to solve a continuous relaxation of Eq. (2) given the convexity property in Definition 2.1. We consider the following continuous optimization problem:\nπ∗(b) = arg min π=(π1,··· ,πn) f\n( n∑\ni=1\nπixix > i\n) , (6)\ns.t. π ≥ 0, ‖π‖1 ≤ r, I[b = 2] · ‖π‖∞ ≤ 1. 1This property could be relaxed to allow a proxy function g :\nS+p → R being convex, where g(A) ≤ g(B)⇔ f(A) ≤ f(B).\nThe ‖π‖1 ≤ r constraint makes sure only r rows of X are “selected”, where r ≤ k is a parameter that controls the degree of oversampling. The 0 ≤ πi ≤ 1 constraint enforces that each row of X is “selected” at most once and is only applicable to the without replacement setting (b = 2). Eq. (6) is a relaxation of the original combinatorial problem Eq. (2), which we formalize below: Fact 2.1. For b ∈ {1, 2} we have f(∑ni=1 π∗i (b)xix>i ) ≤ minS∈Sb(n,r) f(X > SXS)\nIn addition, because of the monotonicity property of f the sum constraint must bind: Fact 2.2. For b ∈ {1, 2} it holds that∑ni=1 π∗i (b) = r.\nProofs of Facts 2.1 and 2.2 are straightforward and are placed in the supplementary material.\nBoth the objective function and the constraint set in Eq. (6) are convex, and hence it can be efficiently solved to global optimality by conventional convex optimization algorithms. In particular, for differentiable f we suggest the following projected gradient descent (PGD) procedure:\nπ(t+1) = PC ( π(t) − γt∇f(π(t)) ) , (7)\nwhere PC(x) = arg miny∈C ‖x − y‖2 is the projection operator onto the feasible set C = {π ∈ Rp : π ≥ 0, ‖π‖1 ≤ r, I[b = 2] · ‖π‖∞ ≤ 1} and {γt}t≥1 > 0 is a sequence of step sizes typically chosen by backtracking line search. When f is not differentiable everywhere, projected subgradient descent could be used with either constant or diminishing step sizes. We defer detailed gradient computations to the supplementary material. It was shown in (Wang et al., 2016; Su et al., 2012) that the projection operator PC(x) could be efficiently computed up to precision δ in O(n log(‖x‖∞/δ)) operations."
  }, {
    "heading": "3. Sparsification via regret minimization",
    "text": "The optimal solution π∗ of Eq. (6) does not naturally lead to a valid approximation of the combinatorial problem in Eq. (2), because the number of non-zero components in π∗ may far exceed k. The primary focus of this section is to design efficient algorithms that sparsify the optimal solution π∗ into s ∈ [k]n (with replacement) or s ∈ {0, 1}n (without replacement), while at the same time bounding the increase in the objective.\nDue to the monotonicity and reciprocal multiplicity properties of f , it suffices to find a sparsifier s that satisfies\n( n∑\ni=1\nsixix > i\n) τ · ( n∑\ni=1\nπ∗i xix > i\n) (8)\nfor some constant τ ∈ (0, 1). By Definition 2.1, Eq. (8) immediately implies f( ∑n i=1 sixix > i ) ≤\nτ−1f( ∑n i=1 π ∗ i xix > i ). The key idea behind our algorithm is a regret-minimization interpretation of the least eigenvalue of a positive definite matrix, which arises from recent progress in the spectral graph sparsification literature (Silva et al., 2016; Allen-Zhu et al., 2015).\nIn the rest of this section, we adopt the notation that Π = diag(π∗) and S = diag(s), both being n×n non-negative diagonal matrices. We also use I to denote the identity matrix, whose dimension should be clear from the context."
  }, {
    "heading": "3.1. The whitening trick",
    "text": "Consider the linear transform xi 7→ (XΠX>)−1/2xi =: x̃i. It is easy to verify that ∑n i=1 π ∗ i x̃ix̃ > i = I. Such a transform is usually referred to as whitening, because the sample covariance of the transformed data is the identity matrix. Define W = ∑n i=1 six̃ix̃ > i . We then have the following:\nProposition 3.1. For τ > 0, W τI if and only if ( ∑n i=1 sixix > i ) τ( ∑n i=1 π ∗ i xix > i ).\nProof. The proposition holds because W τI if and only if (XΠX>)1/2W(XΠX>)1/2 τXΠX>, and that (XΠX>)1/2W(XΠX>)1/2 = XSX>.\nProposition 3.1 shows that, without loss of generality, we may assume ∑n i=1 π ∗ i xix > i = XΠX\n> = I. The question of proving W = XSX> τI is then reduced to lower bounding the smallest eigenvalue of W.\nRecall that W can be written as a sum of rank-1 PSD matrices W = ∑k t=1 Ft, where Ft = xix > i for some i ∈ [n]. In the next section we give a novel characterization of the least eigenvalue of W from a regret minimization perspective. The problem of lower bounding the least eigenvalue of W can then be reduced to bounding the regret of a particular Follow-The-Regularized-Leader (FTRL) algorithm, which is a much easier task as FTRL admits closed-form solutions."
  }, {
    "heading": "3.2. Smallest eigenvalue as regret minimization",
    "text": "We first review the concept of regret minimization in a classical linear bandit setting. Let ∆p = {A ∈ Rp×p : A 0, tr(A) = 1} be an action space that consists of positive semi-definite matrices of dimension p and unit trace norm. Consider the linear bandit problem, which operates in k iterations. At iteration t, the player chooses an action At ∈ ∆p; afterwards, a “reference” action Ft 0 is observed and the loss 〈Ft,At〉 is incurred. The objective of the player is to minimize his/her regret:\nR({At}kt=1) := k∑\nt=1\n〈Ft,At〉 − inf U∈∆p\nk∑\nt=1\n〈Ft,U〉,\nwhich is the “excess loss” of {At}kt=1 compared to the single optimal action U ∈ ∆p in hindsight, knowing all the reference actions {Ft}kt=1. A popular algorithm for regret minimization is Follow-The-Regularized-Leader (FTRL), also known to be equivalent to Mirror Descent (MD) (McMahan, 2011), which solves for\nAt = arg min A∈∆p\n{ w(A) + α · t−1∑\n`=1\n〈F`,A〉 } . (9)\nHere w(A) is a regularization term and α > 0 is a parameter that balances model fitting and regularization. For the proof of our purpose we adopt the `1/2-regularizerw(A) = −2tr(A1/2) introduced in (Allen-Zhu et al., 2015), which leads to the closed-form solution\nAt = ( ctI + α t−1∑\n`=1\nF`\n)−2 , (10)\nwhere ct ∈ R is the unique constant that ensures At ∈ ∆p. The following lemma from (Allen-Zhu et al., 2015) bounds the regret of FTRL using the particular `1/2-regularizer:\nLemma 3.1 (Theorem 3.2 of (Allen-Zhu et al., 2015), specialized to `1/2-regularization). Suppose α > 0, rank(Ft) = 1 and let {At}kt=1 be FTRL solutions defined in Eq. (10). If α〈Ft,A1/2t 〉 > −1 for all t, then\nR({At}kt=1) := k∑\nt=1\n〈Ft,At〉 − inf U∈∆p\nk∑\nt=1\n〈Ft,U〉\n≤ α k∑\nt=1 〈Ft,At〉〈Ft,A1/2t 〉 1 + α〈Ft,A1/2t 〉\n+ 2 √ p\nα .\nNow consider each Ft = xitx > it\nto be the outer product of a design point selected from the design pool X. One remarkable consequence of Lemma 3.1 is that, in order to lower bound the smallest eigenvalue of ∑k t=1 Ft, which by defi-\nnition is infU∈∆p〈 ∑k t=1 Ft,U〉, it suffices to lower bound∑k\nt=1 〈Ft,At〉. Because At admits closed-form expression in Eq. (10), choosing a sequence of {Ft}kt=1 with large∑k t=1 〈Ft,At〉 becomes a much more manageable analytical task, which we shall formalize in the next section."
  }, {
    "heading": "3.3. Proof of Theorem 1.1",
    "text": "Re-organizing terms in Lemma 3.1 we obtain\ninf U∈∆p\nk∑\nt=1\n〈Ft,U〉 ≥ k∑\nt=1 〈Ft,At〉 1 + α〈Ft,A1/2t 〉\n− 2 √ p\nα . (11)\nThe k near-optimal design points are selected in a sequential manner. Let Λt ∈ Sb(n, t) be the set of selected design points at or prior to iteration t (Λ0 = ∅), and define\nFt = xitx > it\n, where it is the design point selected at iteration t. Define also Λt = ∑t `=1 F` = ∑ i∈Λt xix > i .\nWe first consider the with replacement setting b = 1. Lemma 3.2. Suppose ∑n i=1 π ∗ i xix > i = I where π ∗ i ≥ 0\nand ∑n i=1 π ∗ i = r. Then for 1 ≤ t ≤ k we have that maxi∈[n] 〈xix>i ,At〉\n1+α〈xix>i ,A 1/2 t 〉 ≥ 1r+α√p .\nProof. Recall that tr(At) = 1 and ∑n i=1 π ∗ i xix > i =\nI. Subsequently, ∑n i=1 π ∗ i 〈xix>i ,At〉 = 1. On the other hand, we have that ∑n i=1 π ∗ i (1 + α〈xix>i ,A 1/2 t 〉) = ∑n i=1 π ∗ i + α · tr(A 1/2 t ) (a) ≤ r + α · tr(A1/2t ) (b)\n≤ r + α √ p. Here (a) is due to the optimization constraint that ‖π∗‖1 ≤ r, and (b) is because tr(A1/2t ) = ‖σ(A1/2t )‖1 ≤√ p‖σ(A1/2t )‖2 = √ p √ ‖σ(At)‖1 = √p √ tr(At) =√\np, where σ(·) is the vector of all eigenvalues of a PSD matrix. Combining both inequalities we have that maxi∈[n]\n〈xix>i ,At〉 1+α〈xix>i ,A 1/2 t 〉\n≥ ∑n i=1 π ∗ i 〈xix>i ,At〉∑n\ni=1 π ∗ i (1+α〈xix>i ,A 1/2 t 〉)\n,\nwhere the right-hand side is lower bounded by 1/(r + α √ p).\nLet it = arg maxi∈[n] 〈xix>i ,At〉\n1+α〈xix>i ,A 1/2 t 〉\nbe the design point\nselected at iteration t. Combining Eq. (11) and Lemma 3.2,\nΛk = ∑\ni∈Λk xix\n> i\n( k r + α √ p − 2 √ p α ) I. (12)\nTo prove Eq. (3), set α = 8 √ p/ε. Because k = r ≥\nC0p/ε 2, we have that kr+α√p −\n2 √ p\nα ≥ 11+8ε/C0 − ε 4 . With\nC0 = 32 the right-hand side is lower bounded by 1− ε/2. Eq. (3) is thus proved because (1− ε/2)−1 ≤ 1 + ε. We next consider the without replacement setting b = 2.\nLemma 3.3. Fix arbitrary β ∈ (0, 1] and suppose∑n i=1 π ∗ i xix > i = I where π ∗ i ∈ [0, β] and ∑n i=1 π ∗ i = r. Then for all 1 ≤ t ≤ k,\nmax i/∈Λt−1 〈xix>i ,At〉 1 + α〈xix>i ,A 1/2 t 〉\n≥ 1− βσmin(Λt−1)− √ p/α\nr + α √ p\n.\nProof. On one hand, we have ∑ i/∈Λt−1 π ∗ i 〈xix>i ,At〉 (a)\n≥ 〈At, I− βΛt−1〉 (b) = 1 − tr [ (αΛt−1 + ctI) −2 βΛt−1 ] =\n1+ βctα − β α tr [ (αΛt−1 + ctI) −1 ] = 1+ βctα − tr(A 1/2 t ) α (c)\n≥ 1 + βctα − √ p α . Here (a) is due to ∑n i=1 π ∗ i xix > i = I and π∗i ∈ [0, β]; (b) is due to 〈At, I〉 = tr(At) = 1 and (c) is proved in the proof of Lemma 3.2. Because αΛt−1 +ctI 0, we conclude that ct ≥ −ασmin(Λt−1) and therefore∑ i/∈Λt−1 π ∗ i 〈xix>i ,At〉 ≥ 1 − βσmin(Λt−1) − √ p/α. On the other hand, ∑ i/∈Λt−1 π ∗ i (1 + α〈xix>i ,A 1/2 t 〉) ≤\nr + α √ p by the same argument as in the proof of Lemma 3.2. Subsequently, maxi/∈Λt−1 〈xix>i ,At〉\n1+α〈xix>i ,A 1/2 t 〉 ≥\n∑ i/∈Λt−1 π ∗ i 〈xix>i ,At〉\n∑ i/∈Λt−1 π ∗ i (1+α〈xix>i ,A 1/2 t 〉)\n≥ 1−βσmin(Λt−1)− √ p/α\nr+α √ p .\nLet it = arg maxi/∈Λt−1 〈xix>i ,At〉\n1+α〈xix>i ,A 1/2 t 〉\n. Combining\nEq. (11) and Lemma 3.3 with β = 1, we have that\nΛk ( k∑\nt=1\n1− κt −√p/α r + α √ p − 2 √ p α ) I, (13)\nwhere κt := σmin(Λt). We are now ready to prove Eqs. (4,5) in Theorem 1.1.\nProof of Eq. (4). Note that\nΛk sup u>0 min\n{ u, 1− u−√p/α r + α √ p · k − 2 √ p α } I. (14)\nEq. (14) can be proved by a case analysis: if u ≤ κt for some 1 ≤ t ≤ k then σmin(Λk) ≥ σmin(Λt−1) ≥ u; otherwise 1−κt−√p/α ≥ 1−u−√p/α for all 1 ≤ t ≤ k. Suppose k = r ≥ ξp for some ξ > 2. and let α = ν√p, u = (1−2/ξ)ν−3ν(2+ν/ξ) , where ν > 1 is some parameter to be specified later. Eq. (14) then yields Λk (1−2/ξ)ν−3ν(2+ν/ξ) I. Because ξ > 2, it is possible to select ν > 0 such that C1(ξ)\n−1 = (1−2/ξ)ν−3ν(2+ν/ξ) > 0. Finally, for ξ ≥ 4 and ν = 8 we have C1(ξ)−1 ≥ 1/32. Eq. (4) is thus proved.\nProof of Eq. (5). Let β ∈ (0, 1) be a parameter to be specified later, and define Σ∗β := ∑ π∗i≥β π ∗ i xix > i and Σ̄ ∗ β :=\nI − Σ∗β = ∑ π∗i<β π∗i xix > i . Let Ŝ be constructed such that it includes all points in S∗β := {i : π∗i ≥ β}, plus the resulting set by running Algorithm 1 on the remaining weights smaller than β, with subset size k−k′ = k−|S∗β |. Define α = 2 √ p/ε, r′ :=\n∑ π∗i≥β π ∗ i , k̃ := k − k′ and\nr̃ := r− r′+α√p = r− r′+ 2p/ε. Let Λ = ∑i∈Ŝ xix>i be the sample covariance of the selected subset. By the definition of Ŝ and Lemma 3.3, together with the whitening trick (Sec. 3.1) on Σ̄∗β , we have\nΛ Σ∗β + sup u>0\nmin { u, (1− βu− ε/2)k̃/r̃ − ε } Σ̄∗β\nsup u>0\nmin { u, (1− βu− ε/2)k̃/r̃ − ε } I,\nwhere the second line holds because Σ∗β + Σ̄ ∗ β = I and u ≤ 1. Now set β = 0.5 and note that k′ ≤ r′/β ≤ 2r′ by definition of S∗β . Subsequently, r ≥ p/ε2 and k ≥ 4(1 + 7ε)r for ε ∈ (0, 1/2) implies that k̃r̃ ≥ 1+2ε(1−ε/2)(1−β) , which yields u ≥ 1 − ε/2 and hence f(X>\nŜ XŜ) ≤ (1 +\nε)f(X>S∗XS∗). Eq. (5) is thus proved.\nAlgorithm 1 Near-optimal experimental design 1: Input: design pool X ∈ Rn×p, budget parameters k ≥ r ≥ p, algorithmic parameter α > 0.\n2: Solve the convex optimization problem Eq. (6) with parameter s; Let π∗ be the optimal solution; 3: Whitening: X← X(X>diag(π∗)X)−1/2; 4: Initialization: Λ0 = ∅; 5: for t = 1 to k do 6: ct ← FINDCONSTANT( ∑ i∈Λt−1 xix > i , α);\n7: At ← (ctI + ∑ i∈Λt−1 xix > i ) −2; 8: If b = 1 then Γt = [n]; else Γt = [n]\\Λt−1; 9: it ← arg maxi∈Γt\n〈xix>i ,At〉 1+α〈xix>i ,A 1/2 t 〉 ;\n10: Λt = Λt−1 ∪ {it}; 11: end for 12: Output: Ŝ = Λk.\nAlgorithm 2 FINDCONSTANT(Z, α) 1: Initialization: c` = −σmin(Z), cu = √p; = 10−9; 2: while |c` − cu| > do 3: c̄← (c` + cu)/2; 4: If tr[(c̄I + Z)−2] > 1 then c` ← c̄; else cu ← c̄; 5: end while 6: Output: c = (c` + cu)/2.\nOur proof of Theorem 1.1 is constructive and yields a computationally efficient iterative algorithm which finds subset Ŝ ∈ Sb(n, k) that satisfies the approximation results in Theorem 1.1. In Algorithm 1 we give a pseducode description of the algorithm, which makes use of a binary search routine (Algorithm 2) that finds the unique constant ct for which tr(At) = tr[(ctI + ∑ i∈Λt−1 xix > i ) −2] = 1. Note that for Eq. (5) to be valid, it is necessary to run Algorithm 2 on the remaining set of π∗ after including all points xi with π∗i ≥ 1/2 in Ŝ."
  }, {
    "heading": "4. Extension to generalized linear models",
    "text": "The experimental algorithm presented in this paper could be easily extended beyond the linear regression model. For this purpose we consider the Generalized Linear Model (GLM), which assumes that\ny|x i.i.d.∼ p(y|x>β0), where p(·|·) is a known distribution and β0 is an unknown p-dimensional regression model. Examples include the logistic regression model p(y = 1|x) = exp(x\n>β0) 1+exp(x>β0) , the\nPossion count model p(yi = y|x) = exp(yx >β0−e−x >β0 ) y! , and many others.\nLet S ∈ Sb(n, k) be the set of selected design points from X. Under the classical statistics regime,\nthe maximum likelihood (ML) estimator β̂ ML\n= arg minβ ∑ i∈S log p(yi|x>i β) is asymptotically efficient, and its asymptotic variance equals the Fisher’s information\nI(XS ;β0) := ∑\ni∈S Ey|x>i β0\n[ −∂\n2 log p(y|xi;β0) ∂β∂β>\n]\nηi=x > i β0=\n∑ i∈S Ey|ηi\n[ −∂\n2 log p(y|ηi) ∂η2i\n] · xix>i .\nHere the second equality is due to the sufficiency of x>i β0 in a GLM. Note that for the linear regression model y = Xβ0 + w, the ML estimator is the ordinary least squares (OLS) β̂ = (X>SXS)\n−1XSyS and its Fisher’s information equals the sample covariance X>SXS . The experimental design problem can then be formalized as follows: 2\nmin S∈Sb(n,k) f(I(XS ;β0)) = min S∈Sb(n,k) f\n(∑\ni∈S ziz > i\n) ;\n(15)\nzi = √ −Ey|ηi [ −∂\n2 log p(yi|ηi) ∂η2i\n] , ηi = x > i β0.\nSuppose β̌ is a “pilot” estimate of β0, obtained from a uniformly sampled design subset S1. A near-optimal design set S2 can then be constructed by minimizing Eq. (15) using η̌i = x>i β̌. Such an approach was adopted in sequential design and active learning for ML estimators (Chaudhuri et al., 2015; Khuri et al., 2006); however, with our algorithm the quality of S2 is greatly improved.\n2Under very mild conditions E[− ∂2 log p ∂η2 ] = E[( ∂ log p ∂η )2] is non-negative (Van der Vaart, 2000)."
  }, {
    "heading": "5. Numerical results",
    "text": "We compare the proposed method with several baseline methods on both synthetic and real-world data sets. We only consider the harder “without replacement” setting, where each row of X can be selected at most once."
  }, {
    "heading": "5.1. Methods and their implementation",
    "text": "We compare our algorithm with three simple heuristic methods that apply to all optimality criteria:\n1. Uniform sampling: Ŝ is sampled uniformly at random without replacement from the design pool X;\n2. Weighted sampling: first the optimal solution π∗ of Eq. (6) is computed with r = k; afterwards, Ŝ is sampled without replacement according to the distribution specified by π∗/k. Recall that (Wang et al., 2016) proved that weighted sampling works when k is sufficiently large compared to p (cf. Table 1). 3\n3. Fedorov’s exchange (Miller & Nguyen, 1994): the algorithm starts with a random subset S0 ∈ Sb(n, k) and iteratively exchanges two coordinates i ∈ S0, j /∈ S0 such that the objective is minimized after the exchange. The algorithm terminates if no such exchange can reduce the objective, or T iterations are reached.\nAll algorithms are implemented in MATLAB, except for the Fedorov’s exchange algorithm, which is implemented in C due to efficiency concerns. We also apply the ShermanMorrison formula (A+λuu>)−1 = A−1 + λA −1uu>A−1\n1+λu>A−1u\nand the matrix determinant lemma det(A + λuu>) =\n3Fact 2.2 ensures that π∗/k is a valid probability distribution.\nTable 3. Results on the Minnesota wind speed dataset (n =\n2642, p = 15, k = 30). MSE is defined as\n√\n1 n ‖y −Vβ̂‖22.\nfV MSE fG MSE\nUNIFORM SAMPLING 94.1 1.10 3093 1.34 WEIGHTED SAMPLING 21.4 0.89 2451 1.13\nFEDOROV’S EXCHANGE 10.0 0.86 29.2 0.78 (running time /secs) 15 - 1857 -\nALGORITHM 1 10.8 0.72 29.2 0.76 (running time /secs) < 1 - < 1 -\nFULL-SAMPLE OLS - 0.55 - 0.55\n(1 + λu>A−1u>) det(A) to accelerate computations of rank-1 updates of matrix inverse and determinant. For uniform sampling and weighted sampling, we report the median objective of 50 indpendent trials. We only report the objective for one trial of Fedorov’s exchange method due to time constraints. The maximum number of iterations T for Fedorov’s exchange is set at T = 100. We always set k = r in the optimization problem Eq. (6), and details of solving Eq. (6) are placed in the appendix. In Algorithm 1 we set α = 10; our similuations suggest that the algorithm is not sensitive to α."
  }, {
    "heading": "5.2. Synthetic data",
    "text": "We synthesize a 1000× 50 design pool X as follows:\nX =\n[ XA 0500×25\n0500×25 XB\n] .\nXA is a 500 × 25 random Gaussian matrix, re-scaled so that the eigenvalues of X>AXA satisfy a quadratic decay: σj(X > AXA) ∝ j−2; XB is a 500 × 25 Gaussian matrix with i.i.d. standard Normal variables. Both XA and XB have comparable Frobenius norm.\nIn Table 2 we report results on all 6 optimality criteria (fA, fD, fT , fE , fV , fG) for k ∈ {2p, 3p, 5p, 10p}. We also report the running time (measured in seconds) of Algorithm 1 and the Fedorov’s exchange algorithm. The other two sampling based algorithms are very efficient and always terminate within one second. We observe that our algorithm has the best performance for fE and fG, while still achieving comparable results for the other optimality criteria. It is also robust when k is small compared to p, while sampling based methods occasionally produce designs that are not even full rank. Finally, Algorithm 1 is computationally efficient and terminates within seconds for all settings."
  }, {
    "heading": "5.3. The Minnesota wind speed dataset",
    "text": "The Minnesota wind dataset collects wind speed information across n = 2642 locations in Minnesota, USA for a\nperiod of 24 months (for the purpose of this experiment, we only use wind speed data for one month). The 2642 locations are connected with 3304 bi-directional roads, which form an n× n sparse unweighted undirected graph G. Let L = diag(d)?G be the n×n Laplacian of G, where d is a vector of node degrees, and let V ∈ Rn×p be an orthonormal eigenbasis corresponding to the smallest p eigenvalues of L. (Chen et al., 2015) shows that the relatively smooth wind speed signal y ∈ Rn can be well approximated by using only p = 15 graph Laplacian basis.\nIn Table 3 we compare the mean-square error (MSE) for prediction on the full design pool V: MSE =√\n1 n‖y −Vβ̂‖22. Because the objective is prediction based, we only consider the two prediction related criteria: fV (Σ) = tr(VΣ−1V>) and fG(Σ) = max diag(VΣ−1V>). The subset size k is set as k = 2p = 30, which is much smaller than n = 2642. We observe that Algorithm 1 consistently outperforms the other heuristic methods, and is so efficient that its running time is negligible. It is also interesting that by using k = 30 samples Algorithm 1 already achieves an MSE that is comparable to the OLS on the entire n = 2642 design pool."
  }, {
    "heading": "6. Concluding remarks and open questions",
    "text": "We proposed a computationally efficient algorithm that approximately computes optimal solutions for the experimental design problem, with near-optimal requirement on k (i.e., the number of experiments to choose). In particular, we obtained a constant approximation under the very weak condition k > 2p, and a (1 + ε) approximation if replacement or over-sampling is allowed. Our algorithm works for all regular optimality criteria.\nAn important open question is to achieve (1 + ε) relative approximation ratio under the “proper sampling” regime k = r, or the “slight over-sampling” regime k = (1 + δ)r, for the without replacement model. It was shown in (Wang et al., 2016) that a simple greedy method achieves (1 + ε) approximation ratio for A- and V-optimality provided that k = Ω(p2/ε). Whether such analysis can be extended to other optimality criteria and whether the p2 term can be further reduced to a near linear function of p remain open.\nAnother practical question is to develop fast-converging optimization methods for the continuous problem in Eq. (6), especially for criteria that are not differentiable such as the E- and G-optimality, where subgradient methods have very slow convergence rate.\nAcknowledgement This work is supported by NSF grants CAREER IIS-1252412 and CCF-1563918. We thank Adams Wei Yu for providing an efficient implementation of the projection step, and other useful discussions."
  }],
  "year": 2017,
  "references": [{
    "title": "Pipage rounding: A new method of constructing algorithms with proven performance guarantee",
    "authors": ["Ageev", "Alexander A", "Sviridenko", "Maxim I"],
    "venue": "Journal of Combinatorial Optimization,",
    "year": 2004
  }, {
    "title": "Spectral sparsification and regret minimization beyond matrix multiplicative updates",
    "authors": ["Allen-Zhu", "Zeyuan", "Liao", "Zhenyu", "Orecchia", "Lorenzo"],
    "venue": "In Proceedings of Annual Symposium on the Theory of Computing (STOC),",
    "year": 2015
  }, {
    "title": "Faster subset selection for matrices and applications",
    "authors": ["Avron", "Haim", "Boutsidis", "Christos"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2013
  }, {
    "title": "Active and passive learning of linear separators under log-concave distributions",
    "authors": ["Balcan", "Maria-Florina", "Long", "Philip M"],
    "venue": "In Proceedings of Annual Conference on Learning Theory (COLT),",
    "year": 2013
  }, {
    "title": "Submodularity and randomized rounding techniques for optimal experimental design",
    "authors": ["Bouhtou", "Mustapha", "Gaubert", "Stephane", "Sagnol", "Guillaume"],
    "venue": "Electronic Notes in Discrete Mathematics,",
    "year": 2010
  }, {
    "title": "Optimal CUR matrix decompositions",
    "authors": ["Boutsidis", "Christos", "Woodruff", "David P"],
    "venue": "In Proceedings of Annual Symposium on the Theory of Computing (STOC),",
    "year": 2014
  }, {
    "title": "Two complexity results on C-optimality in experimental design",
    "authors": ["Černỳ", "Michal", "Hladı́k", "Milan"],
    "venue": "Computational Optimization and Applications,",
    "year": 2012
  }, {
    "title": "Convergence rates of active learning for maximum likelihood estimation",
    "authors": ["Chaudhuri", "Kamalika", "Kakade", "Sham", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2015
  }, {
    "title": "Signal representations on graphs: Tools and applications",
    "authors": ["Chen", "Siheng", "Varma", "Rohan", "Singh", "Aarti", "Kovačević", "Jelena"],
    "venue": "arXiv preprint arXiv:1512.05406,",
    "year": 2015
  }, {
    "title": "On selecting a maximum volume sub-matrix of a matrix and related problems",
    "authors": ["Çivril", "Ali", "Magdon-Ismail", "Malik"],
    "venue": "Theoretical Computer Science,",
    "year": 2009
  }, {
    "title": "Efficient volume sampling for row/column subset selection",
    "authors": ["Deshpande", "Amit", "Rademacher", "Luis"],
    "venue": "In Proceedings of Annual Conference on Foundations of Computer Science (FOCS),",
    "year": 2010
  }, {
    "title": "Linear bandits in high dimension and recommendation systems",
    "authors": ["Deshpande", "Yash", "Montanari", "Andrea"],
    "venue": "In Proceedings of Annual Allerton Conference on Communication, Control, and Computing (Allerton),",
    "year": 2012
  }, {
    "title": "New subsampling algorithms for fast least squares regression",
    "authors": ["Dhillon", "Paramveer", "Lu", "Yichao", "Foster", "Dean P", "Ungar", "Lyle"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "On the Nyström method for approximating a gram matrix for improved kernel-based learning",
    "authors": ["Drineas", "Petros", "Mahoney", "Michael W"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2005
  }, {
    "title": "Relative-error CUR matrix decompositions",
    "authors": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2008
  }, {
    "title": "Theory of optimal experiments",
    "authors": ["Fedorov", "Valerii Vadimovich"],
    "year": 1972
  }, {
    "title": "Hard-margin active linear regression",
    "authors": ["Hazan", "Elad", "Karnin", "Zohar"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2015
  }, {
    "title": "Budget feasible mechanisms for experimental design",
    "authors": ["Horel", "Thibaut", "Ioannidis", "Stratis", "S. Muthukrishnan"],
    "venue": "In Proceedings of Latin American Symposium on Theoretical Informatics (LATIN),",
    "year": 2014
  }, {
    "title": "Following the leader and fast rates in linear prediction: Curved constraint sets and other regularities",
    "authors": ["Huang", "Ruitong", "Lattimore", "Tor", "György", "András", "Szepesvári", "Csaba"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2016
  }, {
    "title": "Design issues for generalized linear models: a review",
    "authors": ["Khuri", "Andre", "Mukherjee", "Bhramar", "Sinha", "Bikas", "Ghosh", "Malay"],
    "venue": "Statistical Science,",
    "year": 2006
  }, {
    "title": "Polynomial time algorithms for dual volume sampling",
    "authors": ["Li", "Chengtao", "Jegelka", "Stefanie", "Sra", "Suvrit"],
    "venue": "arXiv preprint arXiv:1703.02674,",
    "year": 2017
  }, {
    "title": "σoptimality for active learning on Gaussian random fields",
    "authors": ["Ma", "Yifei", "Garnett", "Roman", "Schneider", "Jeff"],
    "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),",
    "year": 2013
  }, {
    "title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1 regularization",
    "authors": ["McMahan", "H Brendan"],
    "venue": "In Procedings of International Conference on Artificial Intelligence and Statistics (AISTATS),",
    "year": 2011
  }, {
    "title": "A Fedorov exchange algorithm for d-optimal design",
    "authors": ["Miller", "Alan", "Nguyen", "Nam-Ky"],
    "venue": "Journal of the Royal Statistical Society, Series C (Applied Statistics),",
    "year": 1994
  }, {
    "title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares",
    "authors": ["Pilanci", "Mert", "Wainwright", "Martin J"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Optimal design of experiments",
    "authors": ["Pukelsheim", "Friedrich"],
    "year": 2006
  }, {
    "title": "A statistical perspective on randomized sketching for ordinary leastsquares",
    "authors": ["Raskutti", "Garvesh", "Mahoney", "Michael"],
    "venue": "arXiv preprint arXiv:1406.5986,",
    "year": 2014
  }, {
    "title": "Experimental design on a budget for sparse linear models and applications",
    "authors": ["Ravi", "Sathya N", "Ithapu", "Vamsi K", "Johnson", "Sterling C", "Singh", "Vikas"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Sparse sums of positive semidefinite matrices",
    "authors": ["Silva", "Marcel K", "Harvey", "Nicholas JA", "Sato", "Cristiane M"],
    "venue": "ACM Transactions on Algorithms,",
    "year": 2016
  }, {
    "title": "Graph sparsification by effective resistances",
    "authors": ["Spielman", "Daniel A", "Srivastava", "Nikhil"],
    "venue": "SIAM Journal on Computing,",
    "year": 1913
  }, {
    "title": "Efficient euclidean projections onto the intersection of norm balls",
    "authors": ["Su", "Hao", "Yu", "Adams Wei", "Li", "Fei-Fei"],
    "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
    "year": 2012
  }, {
    "title": "Improving CUR matrix decomposition and the Nyström approximation via adaptive sampling",
    "authors": ["Wang", "Shusen", "Zhang", "Zhihua"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2013
  }, {
    "title": "An empirical comparison of sampling techniques for matrix column subset selection",
    "authors": ["Wang", "Yining", "Singh", "Aarti"],
    "venue": "In Proceedings of Annual Allerton Conference on Communication,",
    "year": 2015
  }, {
    "title": "Provably correct active sampling algorithms for matrix column subset selection with missing data",
    "authors": ["Wang", "Yining", "Singh", "Aarti"],
    "venue": "arXiv preprint arXiv:1505.04343,",
    "year": 2015
  }, {
    "title": "Noise-adaptive marginbased active learning and lower bounds under tsybakov noise condition",
    "authors": ["Wang", "Yining", "Singh", "Aarti"],
    "venue": "In Proceedings of AAAI Conference on Artificial Intelligence (AAAI),",
    "year": 2016
  }, {
    "title": "On computationally tractable selection of experiments in regression models",
    "authors": ["Wang", "Yining", "Yu", "Wei Adams", "Singh", "Aarti"],
    "venue": "arXiv preprints:",
    "year": 2016
  }],
  "id": "SP:75ffdce867d30585bff0111ae7c861433cb474f0",
  "authors": [{
    "name": "Zeyuan Allen-Zhu",
    "affiliations": []
  }, {
    "name": "Yuanzhi Li",
    "affiliations": []
  }, {
    "name": "Aarti Singh",
    "affiliations": []
  }, {
    "name": "Yining Wang",
    "affiliations": []
  }],
  "abstractText": "We consider computationally tractable methods for the experimental design problem, where k out of n design points of dimension p are selected so that certain optimality criteria are approximately satisfied. Our algorithm finds a (1 + ε)approximate optimal design when k is a linear function of p; in contrast, existing results require k to be super-linear in p. Our algorithm also handles all popular optimality criteria, while existing ones only handle one or two such criteria. Numerical results on synthetic and real-world design problems verify the practical effectiveness of the proposed algorithm.",
  "title": "Near-Optimal Design of Experiments via Regret Minimization"
}