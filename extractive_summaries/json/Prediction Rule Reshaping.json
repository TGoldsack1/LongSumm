{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Shape constraints like monotonicity and convexity arise naturally in many real-world regression and classification tasks. For example, holding all other variables fixed, a practitioner might assume that the price of a house is a decreasing function of neighborhood crime rate, that an individual’s utility function is concave in income level, or that phenotypes such as height or the likelihood of contracting a disease are monotonic in certain genetic effects.\nParametric models like linear regression implicity impose monotonicity constraints at the cost of strong assumptions on the true underlying function. On the other hand, nonparametric techniques like kernel regression impose weak assumptions but do not guarantee monotonicity or convexity in their predictions. Shape-constrained nonparametric regression methods attempt to offer the best of both worlds, allowing practitioners to dispense with parametric assumptions while retaining many of their appealing properties.\nHowever, classical approaches to nonparametric regression\n1Department of Statistics, The University of Chicago 2Department of Statistics, University of Illinois at UrbanaChampaign 3Department of Statistics and Data Science, Yale University. Correspondence to: Matt Bonakdarpour <mbonakda@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nunder shape constraints suffer from the curse of dimensionality (Han & Wellner, 2016; Han et al., 2017). Some methods have been developed to mitigate this issue under assumptions like additivity, where the true function f is assumed to have the form f(x) = ∑ j fj(xj) + c, where a subset of the component fj’s are shape-constrained (Pya & Wood, 2015; Chen & Samworth, 2016; Xu et al., 2016). But in many real-world settings, the lack of interaction terms among the predictors can be too restrictive.\nApproaches from the machine learning community like random forests, gradient boosted trees, and deep learning methods have been shown to exhibit outstanding empirical performance on high-dimensional tasks. But these methods do not guarantee monotonicity or convexity.\nIn this paper, we propose two methods for high-dimensional shape-constrained regression and classification. These methods blend the performance of machine learning methods with the classical least-squares approach to nonparametric shape-constrained regression.\nIn Section (2.1), we describe black box reshaping, which takes any pre-trained prediction rule and reshapes it on a set of test inputs to enforce shape constraints. In the case of monotonicity constraints, we develop an efficient algorithm to compute the estimator.\nSection (2.2) presents a second method designed specifically to reshape random forests (Breiman, 2001). This approach reshapes each individual decision tree based on its split rules and estimated leaf values. Again, in the case of monotonicity constraints, we present another efficient reshaping algorithm.\nWe apply our methods to four datasets in Section (3) and show that they enforce the pre-specified shape constraints without sacrificing accuracy."
  }, {
    "heading": "1.1. Related Work",
    "text": "In the context of monotonicity constraints, the black box reshaping method is related to the method of rearrangements (Chernozhukov et al., 2009; 2010). The rearrangement operation takes a pre-trained prediction rule and sorts its predictions to enforce monotonicity. In higher dimensions, the rearranged estimator is the average of one-dimensional rearrangements. In contrast, this paper focuses on isotonization\nof prediction values, jointly reshaping multiple dimensions in tandem. It would be interesting to explore adaptive procedures that average rearranged and isotonized predictions in future work.\nMonotonic decision trees have previously been studied in the context of classification. Several methods require that the training data satisfy monotonicity constraints (Potharst & Feelders, 2002; Makino et al., 1996), a relatively strong assumption in the presence of noise. The methods we propose here do not place any restrictions on the training data.\nAnother class of methods augment the score function for each split to incorporate the degree of non-monotonicity introduced by that split (Ben-David, 1995; González et al., 2015). However, this approach does not guarantee monotonicity. Feelders & Pardoel (2003) apply pruning algorithms to non-monotonic trees as a post-processing step in order to enforce monotonicity. For a comprehensive survey of estimating monotonic functions, see Gupta et al. (2016) .\nA line of recent work has led to a method for learning deep monotonic models by alternating different types of monotone layers (You et al., 2017). Amos et al. (2017) propose a method for fitting neural networks whose predictions are convex with respect to a subset of predictors.\nOur methods differ from this work in several ways. First, our techniques can be used to enforce both monotonic and convex/concave relationships. Unlike pruning methods, neither approach presented here changes the structure of the original tree. Black box reshaping, described in Section (2.1), can be applied to any pre-trained prediction rule, giving practitioners the flexibility of picking the method of their choice. And both methods guarantee that the intended shape constraints are satisfied on test data."
  }, {
    "heading": "2. Prediction Rule Reshaping",
    "text": "In what follows, we say that a function f : Rd → R is monotone with respect to variables R ⊆ [d] = {1, . . . , d} if f(x) ≤ f(y) when xi ≤ yi for i ∈ R, and xi = yi otherwise.\nSimilarly, a function f is convex in R if for all x, y ∈ Rd and α ∈ [0, 1], f(αx+ (1− α)y) ≤ αf(x) + (1− α)f(y) when xi = yi ∀i /∈ R."
  }, {
    "heading": "2.1. Black Box Reshaping",
    "text": "Let f̂ : Rd → R denote an arbitrary prediction rule fit on a training set and assume we have a candidate set of shape constraints with respect to variablesR ⊆ [d]. For example, we might require that the function be monotone increasing in each variable v ∈ R.\nLet F denote the class of functions that satisfy the desired\nshape constraints on each predictor variable v ∈ R. We aim to find a function f∗ ∈ F that is close to f̂ in the L2 norm:\nf∗ = arg min f∈F ‖f − f̂‖2 (1)\nwhere the L2 norm is with respect to the uniform measure on a compact set containing the data.\nWe simplify this infinite-dimensional problem by only considering values of f̂ on certain fixed test points.\nSuppose we take a sequence t1, t2, . . . , tn of test points, each in Rd, that differ only in their v-th coordinate so that tik = t i′\nk for all k 6= v. These points can be ordered by their v-th coordinate, allowing us to consider shape constraints on the vector (f(t1), f(t2), ..., f(tn)) ∈ Rn. For instance, under a monotone-increasing constraint with respect to v, if t1v ≤ t2v ≤ · · · ≤ tnv , then we consider functions f such that (f(t1), f(t2), ..., f(tn)) is a monotone sequence.\nThere is now the question of choosing a test point t as well as a sequence of values t1v, ..., t n v to plug into its v-th coordinate. A natural choice is to use the observed data values as both test points and coordinate values.\nDenote Dn = {(x1, y1), . . . , (xn, yn)} as a set of observed values where yi is the response and xi ∈ Rd are the predictors. From each xi, we construct a sequence of test points that can be ordered according to their v-th coordinate in the following way. Let xi,k,v denote the observed vector xi with its v-th coordinate replaced by the v-th coordinate of xk, so that\nxi,k,v = (xi1, x i 2, . . . , x i v−1, x k v , x i v+1, . . . , x i d). (2)\nThis process yields n points from xi that can be ordered by their v-th coordinate, xi,1,v, xi,2,v, . . . , xi,n,v. We then require (f(xi,1,v), f(xi,2,v), . . . , f(xi,n,v)) ∈ Sv where Sv ⊂ Rd is the appropriate convex cone that enforces the shape constraint for variable v ∈ R, for example the cone of monotone increasing or convex sequences.\nTo summarize, for each coordinate v ∈ R and for each i ∈ [n], we:\n1. Take the i-th observed data point xi as a test point.\n2. Replace its v-th coordinate with the n observed v-th coordinates x1v, ...x n v to produce x i,1,v, xi,2,v, . . . , xi,n,v .\n3. Enforce the appropriate shape constraint on the vector of evaluated function values, (f(xi,1,v), f(xi,2,v), . . . , f(xi,n,v)) ∈ Sv .\nSee Figure (1) for an illustration.\nThis leads to the following relaxation of (1):\nf∗ = arg min f∈Fn ‖f − f̂‖2 (3)\nwhere Fn is the class of functions f such that (f(xi,1,v), f(xi,2,v), . . . , f(xi,n,v)) ∈ Sv ⊂ Rn for each v ∈ R and each i ∈ [n].\nIn other words, we have relaxed the shape constraints on the function f , requiring the constraints to hold relative to the selected test points. However, this optimization is still infinite dimensional.\nWe make the final transition to finite dimensions by changing the objective function to only consider values of f on the test points. Letting Fi,k,v denote the value of f evaluated on test point xi,k,v, we relax (3) to obtain the solution F ∗ = (F ∗i,k,v)v∈R,i∈[n],k∈[n] of the optimization:\narg min F\n∑ i,k,v (Fi,k,v − f̂(xi,k,v))2\nsubject to (Fi,1,v, ..., Fi,n,v) ∈ (Sv)v∈R, ∀ i ∈ [n] (4)\nHowever, this leads to ill-defined predictions on the original data points x1, ..., xn, since for each v, xi,i,v = xi, but we may obtain different values F ∗i,i,v for various v ∈ R.\nWe avoid this issue by adding a consistency constraint (7) to obtain our final black box reshaping optimization (BBOPT):\narg min F\n∑ i,k,v (Fi,k,v − f̂(xi,k,v))2 (5)\nsubject to (Fi,1,v, ..., Fi,n,v) ∈ (Sv)v∈R, ∀ i ∈ [n] (6) and Fi,i,v = Fi,i,w ∀ v, w ∈ R,∀ i ∈ [n] (7)\nWe then take the reshaped predictions to be\nf∗(xi) = F ∗i,i,v\nfor any v ∈ R. Since the constraints depend on each xi independently, BBOPT decomposes into n optimization problems, one for each observed value. Note that the true response values yi are not used when reshaping. We could select optimal shape constraints on a held-out test set."
  }, {
    "heading": "2.1.1. INTERSECTING ISOTONIC REGRESSION",
    "text": "In this section, we present an efficient algorithm for solving BBOPT for the case when each Sv imposes monotonicity constraints. Let R = |R| denote the number of monotonicity constraints.\nWhen reshaping with respect to only one predictor (R = 1), the consistency constraints (7) vanish, so the optimization decomposes into n isotonic regression problems. Each problem is efficiently solved in Θ(n) time with the pool adjacent violators algorithm (PAVA) (Ayer et al., 1955).\nFor R > 1 monotonicity constraints, BBOPT gives rise to n independent intersecting isotonic regression problems. The k-th problem corresponds to the k-th observed value xk; the “intersection” is implied by the consistency constraints (7). For each independent problem, our algorithm takes O(m logR) time, where m = n ·R is the number of variables in each problem.\nWe first state the general problem. Assume v1, v2, . . . , vK are each real-valued vectors with dimensions d1, d2, . . . , dK , respectively. Let ij ∈ {1, . . . , dj} denote an index in the j-th vector vj .\nThe intersecting isotonic regression problem (IISO) is:\nminimize (v̂k)Kk=1 K∑ k=1 ‖v̂k − vk‖2 subject to v̂k1 ≤ v̂k2 ≤ · · · ≤ v̂kdk , ∀ k ∈ [K] and v̂1i1 = v̂ 2 i2 = · · · = v̂ K iK (8)\nFirst consider the simpler constrained isotonic regression problem with a single sequence v ∈ Rd, index i ∈ [d], and fixed value c ∈ R\nminimize v̂\n‖v̂ − v‖2\nsubject to v̂1 ≤ v̂2 ≤ · · · ≤ v̂d and v̂i = c\n(9)\nLemma 2.1. The solution v∗ to (9) can be computed by using index i as a pivot and splitting v into its left and right tails, so that ` = (v1, v2, . . . , vi−1) and r = (vi+1, . . . , vd), then applying PAVA to obtain monotone tails ̂̀and r̂. v∗ is\nAlgorithm 1 IISO Algorithm 1. Apply PAVA to each of the 2K tails. 2. Combine and sort the left and right tails separately. 3. Find segment s∗ in between tail values where the derivative g′(η) changes sign. 4. Compute c∗, the minimizer of g(c) in segment s∗.\nobtained by setting elements of ̂̀and r̂ to `∗k = min( ̂̀ k, c)\nr∗k = max(r̂k, c) (10)\nand concatenating the resulting tails so that v∗ = (`∗, c, r∗) ∈ Rd.\nWe now explain the IISO Algorithm presented in Algorithm (1). First divide each vector vj into two tails, the left tail `j and the right tail rj , using the intersection index ij as a pivot,\nvj = (vj1, v j 2, . . . , v j (ij−1)︸ ︷︷ ︸\n`j\n, vjij , v j (ij+1) , vj(ij+2), . . . , v j dj︸ ︷︷ ︸\nrj\n).\nresulting in 2K tails {`1, . . . , `K , r1, . . . , rK}.\nStep 1 of Algorithm (1) performs an unconstrained isotonic regression on each tail using PAVA to obtain 2K monotone tails {̂̀1, . . . , ̂̀K , r̂1, . . . , r̂K}. This can be done in Θ(n) time, where n is the total number of elements across all vectors so that n = ∑K i=1 di.\nGiven the monotone tails, we can write a closed-form expression for the IISO objective function in terms of the value at the point of intersection.\nLet c be the value of the vectors at the point of intersection so that c = v̂1i1 = v̂ 2 i2\n= · · · = v̂KiK . For a fixed c, we can solve IISO by applying Lemma (2.1) to each sequence separately. This yields the following expression for the squared error as a function of c:\ng(c) = K∑ k=1 (c− vkik) 2 + K∑ k=1 ik−1∑ i=1 (`ki −min(̂̀ki , c))2 +\nK∑ l=1 dl∑ j=il+1 (rlj −max(r̂lj , c))2 (11)\nwhich is piecewise quadratic with knots at each ̂̀ki and r̂lj . Our goal is to find c? = minc g(c). Note that g(c) is convex and differentiable.\nWe proceed by computing the derivative of g at each knot, from smallest to largest, and finding the segment in which the sign of the derivative changes from negative to positive. The minimizer c∗ will live in this segment.\nStep 2 of Algorithm (1) merges the left and right sorted tails into two sorted lists. This can be done in O(n logK) time with a heap data structure. Step 3 computes the derivative of the objective function g at each knot, from smallest to largest, searching for the segment in which the derivative changes sign. Step 4 computes the minimizer of g in the corresponding segment. By updating the derivative incrementally and storing relevant side information, Steps 3 and 4 can be done in linear time.\nThe total time complexity is therefore O(n log(K))."
  }, {
    "heading": "2.2. Reshaping Random Forests",
    "text": "In this section, we describe a framework for reshaping a random forest to ensure monotonicity of its predictions in a subset of its predictor variables. A similar method can be applied to ensure convexity. For both regression and probability trees (Malley et al., 2012), the prediction of the forest is an average of the prediction of each tree; it is therefore sufficient to ensure monotonicity or convexity of the trees. For the rest of this section, we focus on reshaping individual trees to enforce monotonicity.\nOur method is a two-step procedure. The first step is to grow a tree in the usual way. The second step is to reshape the leaf values to enforce monotonicity. We hope to explore the implications of combining these steps in future work.\nLet T (x) be a regression tree andR ⊆ [d] a set of predictor variables to be reshaped. Let x ∈ Rd be an input point and denote the k-th coordinate of x as xk. Assume v ∈ R is a predictor variable to be reshaped. The following thought experiment, illustrated in Figure (2), will motivate\nour approach.\nImagine dropping x down T until it falls in its corresponding leaf, `1. Let p1 be the closest ancestor node to `1 that splits on v and assume it has split rule {xv ≤ t1}. Holding all other coordinates constant, increasing xv until it is greater than t1 would create a new point that drops down to a different leaf `2 in the right subtree of p1.\nIf `1 and `2 both share another ancestor p2 farther up the tree with split rule {xv ≤ t2}, increasing xv beyond t2 would yield another leaf `3. Assume these leaves have no other shared ancestors that split on v. Denoting the value of leaf ` as µ`, in order to ensure monotonicity in v for this point x, we require µ`1 ≤ µ`2 ≤ µ`3 .\nWe use this line of thinking to propose a framework for estimating monotonic random forests and describe two estimators that fall under this framework."
  }, {
    "heading": "2.2.1. EXACT ESTIMATOR",
    "text": "Each leaf ` in a decision tree is a cell (or hyperrectangle) C` which is an intersection of intervals\nC` = d⋂ j=1 {x : xj ∈ I`j}\nWhen we split on a shape-constrained variable v with splitvalue t, each cell in the left subtree is of the form Cl = C̄l ∩{x : xv ≤ t} and each cell in the right subtree is of the form Cr = C̄r ∩ {x : xv > t}.\nFor cells l in the left subtree and r in the right subtree, our goal is to constrain the corresponding leaf values µl ≤ µr only when C̄l ∩ C̄r 6= ∅. See Figure (3) for an illustration. We must devise an algorithm to find the intersecting cells (l, r), and add each to a constraint set E. This can be done efficiently with an interval tree data structure.\nAssume there are n unique leaves appearing inE. The exact estimator is obtained by solving the following optimization:\nmin (µ̂`)n`=1\nn∑ `=1 (µ` − µ̂`)2\nsubject to µ̂i ≤ µ̂j ∀ (i, j) ∈ E (12)\nwhere µ` is the original value of leaf `.\nThis is an instance of L2 isotonic regression on a directed acyclic graph where each leaf value µ` is a node, and each constraint in E is an edge. With n vertices and m edges, the fastest known exact algorithm for this problem has time complexity Θ(n4) (Spouge et al., 2003), and the fastest known δ-approximate algorithm has complexity O(m1.5 log2 n log nδ ) (Kyng et al., 2015).\nWith a corresponding change to the constraints in Equa-\ntion (12), this approach extends naturally to convex regression trees. It can also be applied directly to probability trees for binary classification by reshaping the estimated probabilities in each leaf."
  }, {
    "heading": "2.2.2. OVER-CONSTRAINED ESTIMATOR",
    "text": "In this section, we propose an alternative estimator that can be more efficient to compute, depending on the tree structure. In our experiments below, we find that computing this estimator is always faster.\nLet Ep denote the set of constraints that arise between leaf values under a shape-constrained split node p. By adding additional constraints toEp, we can solve (12) exactly for each shape-constrained split node in O(np log np) time, where np is the number of leaves under p.\nIn this setting, each shape-constrained split node gives rise to an independent optimization involving its leaves. Due to transitivity, we can solve these optimizations sequentially in reverse (bottom-up) level-order on the tree.\nLet np denote the number of leaves under node p. For each node p that is split on a shape-constrained variable, the over-constrained estimator solves the following max-min\nproblem:\nmin (µ̂`) np `=1\nnp∑ `=1 (µ` − µ̂`)2\nsubject to max `∈left(p) µ̂` ≤ min r∈right(p) µ̂r\n(13)\nwhere left(p) denotes all leaves in the left subtree of p and right(p) denotes all leaves in the right subtree.\nThis is equivalent to adding an edge (`, r) to E for every pair of leaves such that ` is in left(p) and r is in right(p). All such pairs do not necessarily exist in E for the exact estimator; see Figure (3). For each shape-constrained split, (13) is an instance of L2 isotonic regression on a complete directed bipartite graph.\nFor a given shape-constrained split node p, let ` = (`1, `2, . . . , `n1) be the values of the leaves in its left subtree, and r = (r1, r2, . . . , rn2) be the values of the leaves in its right subtree, indexed so that `1 ≤ · · · ≤ `n1 and r1 ≤ · · · ≤ rn2 . Then the max-min problem (13) is equivalent to:\nmin˜̀,r̃ n1∑ i=1 (`i − ˜̀i)2 + n2∑ i=1 (ri − r̃i)2\nsubject to ˜̀1 ≤ ˜̀2 ≤ ... ≤ ˜̀n1 ≤ r̃1 ≤ · · · ≤ r̃n2 (14) The solution to this optimization is of the form ˜̀i = min(c, `i) and r̃i = max(c, ri), for some constant c. Given the two sorted vectors ` and r, the optimization becomes:\nmin c n1∑ i=1 (`i −min(c, `i))2 + n2∑ i=1 (ri −max(c, ri))2\nThis objective is convex and differentiable in c. Similar to the black box reshaping method, we can compute the derivatives at the values of the data and find where it flips sign, then compute the minimizer in the corresponding segment. This takes O(n) time where n = n1 + n2, the number of leaves under the shape-constrained split. With sorting, the over-constrained estimator can be computed in O(n log n) time for each shape-constrained split node.\nWe apply this procedure sequentially on the leaves of every shape-constrained node in reverse level-order on the tree."
  }, {
    "heading": "3. Experiments",
    "text": "We apply the reshaping methods described above to two regression tasks and two binary classification tasks. We show that reshaping allows us to enforce shape constraints without compromising predictive accuracy. For convenience, we use the acronyms in Table (2) to refer to each method.\nThe BB method was implemented in R, and the OC and EX methods were implemented in R and C++, extending the R package ranger (Wright & Ziegler, 2017). The exact estimator from Section (2.2.1) is computed using the MOSEK C++ package (ApS, 2017).\nFor binary classification, we use the probability tree implementation found in ranger, enforcing monotonicity of the probability of a positive classification with respect to the chosen predictors.\nFor the purposes of these experiments, black box reshaping is applied to a traditional random forest. The random forest was fit with the default settings found in ranger.\nWe apply 5-fold cross validation on all four tasks and present the results under the relevant performance metrics in Table (1)."
  }, {
    "heading": "3.1. Diabetes Dataset",
    "text": "The diabetes dataset (Efron et al., 2004) consists of ten physiological baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements, for each of 442 patients. The response is a quantitative measure of disease progression measured one year after baseline.\nHolding all other variables constant, we might expect dis-\nease progression to be monotonically increasing in body mass index (Ganz et al., 2014). We estimate a random forest and apply our reshaping techniques, then make predictions for a random test subject as we vary the body mass index predictor variable. The results shown in Figure (4a) illustrate the effect of reshaping on the predictions.\nWe use mean squared error to measure accuracy. The results in Table (1) indicate that the prediction accuracy of all four estimators is approximately the same."
  }, {
    "heading": "3.2. Zillow Dataset",
    "text": "In this section, the regression task is to predict real estate sales prices using property information. The data were obtained from Zillow, an online real estate database company. For each of 206,820 properties, we are given the list price, number of bedrooms and bathrooms, square footage, build decade, sale year, major renovation year (if any), city, and metropolitan area. The response is the actual sale price of the home.\nWe reshape to enforce monotonicity of the sale price with respect to the list price. Due to the size of the constraint set, this problem becomes intractable for MOSEK; the results for the EX method are omitted. An interesting direction for future work is to investigate more efficient algorithms for this method.\nFollowing reported results from Zillow, we use mean absolute percent error (MAPE) as our measure of accuracy. For\nan estimate ŷ of the true value y, the APE is |ŷ − y|/y.\nThe results in Table (1) show that the performance across all estimators is indistinguishable."
  }, {
    "heading": "3.3. Adult Dataset",
    "text": "We apply the reshaping techniques to the binary classification task found in the Adult dataset (Lichman, 2013). The task is to predict whether an individual’s income is less than or greater than $50,000. Following the experiments performed in Milani Fard et al. (2016) and You et al. (2017), we apply monotonic reshaping to four variables: capital gain, weekly hours of work, education level, and the gender wage gap.\nWe illustrate the effect of reshaping on the predictions in Figure (4b). The results in Table (1) show that we achieve similar test set accuracy before and after reshaping the random forest."
  }, {
    "heading": "3.4. Spambase Dataset",
    "text": "Finally, we apply reshaping to classify whether an email is spam or not. The Spambase dataset (Lichman, 2013) contains 4,601 emails each with 57 predictors. There are 48 word frequency predictors, 6 character frequency predictors, and 3 predictors related to the number of capital letters appearing in the email.\nThat data were collected by Hewlett-Packard labs and do-\nnated by George Forman. One of the predictors is the frequency of the word “george”, typically assumed to be an indicator of non-spam for this dataset. We reshape the predictions to enforce the probability of being classified as spam to be monotonically decreasing in the frequency of words “george” and “hp”.\nThe results in Table (1) again show similar performance across all methods."
  }, {
    "heading": "4. Discussion",
    "text": "We presented two strategies for prediction rule reshaping. We developed efficient algorithms to compute the reshaped estimators, and illustrated their properties on four datasets. Both approaches can be viewed as frameworks for developing more sophisticated reshaping techniques.\nThere are several ways that this work can be extended. Extensions to the black box method include adaptively combining rearrangements and isotonization (Chernozhukov et al., 2009), and considering a weighted objective function to account for the distance between test points.\nIn general, the random forest reshaping method can be viewed as operating on pre-trained parameters of a specific model. Applying this line of thinking to gradient boosted trees, deep learning methods, and other machine learning techniques could yield useful variants of this approach.\nAnd finally, while practitioners might require certain shapeconstraints on their predictions, many scientific applications also require inferential quantities, such as confidence intervals and confidence bands. Developing inferential procedures for reshaped predictions, similar to Chernozhukov et al. (2010) for rearrangements and Athey et al. (2018) for random forests, would yield interpretable predictions along with useful measures of uncertainty."
  }, {
    "heading": "Acknowledgements",
    "text": "Research supported in part by ONR grant N00014-12-10762, NSF grants DMS-1513594 and DMS-1654076, and an Alfred P. Sloan fellowship."
  }],
  "year": 2018,
  "references": [{
    "title": "Input convex neural networks",
    "authors": ["B. Amos", "L. Xu", "J.Z. Kolter"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "MOSEK Fusion API for C++",
    "authors": ["M. ApS"],
    "year": 2017
  }, {
    "title": "An empirical distribution function for sampling with incomplete information",
    "authors": ["M. Ayer", "H. Brunk", "G. Ewing", "W. Reid", "E. Silverman"],
    "venue": "Annals of Mathematical Statistics,",
    "year": 1955
  }, {
    "title": "Monotonicity maintenance in informationtheoretic machine learning algorithms",
    "authors": ["A. Ben-David"],
    "venue": "Mach. Learn.,",
    "year": 1995
  }, {
    "title": "Generalized additive and index models with shape constraints",
    "authors": ["Y. Chen", "R.J. Samworth"],
    "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
    "year": 2016
  }, {
    "title": "Improving point and interval estimators of monotone functions by rearrangement",
    "authors": ["V. Chernozhukov", "I. Fernández-Val", "A. Galichon"],
    "year": 2009
  }, {
    "title": "Quantile and probability curves without",
    "authors": ["V. Chernozhukov", "I. Fernández-Val", "A. Galichon"],
    "venue": "crossing. Econometrica,",
    "year": 2010
  }, {
    "title": "Least angle regression",
    "authors": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"],
    "venue": "Annals of Statistics,",
    "year": 2004
  }, {
    "title": "Pruning for monotone classification trees",
    "authors": ["A. Feelders", "M. Pardoel"],
    "venue": "Advances in Intelligent Data Analysis V,",
    "year": 2003
  }, {
    "title": "The association of body mass index with the risk of type 2 diabetes: a case–control study nested in an electronic health records system in the united states",
    "authors": ["M.L. Ganz", "N. Wintfeld", "Q. Li", "V. Alas", "J. Langer", "M. Hammer"],
    "venue": "Diabetology & Metabolic Syndrome,",
    "year": 2014
  }, {
    "title": "Monotonic random forest with an ensemble pruning mechanism based on the degree of monotonicity",
    "authors": ["S. González", "F. Herrera", "S. Garcı́a"],
    "year": 2015
  }, {
    "title": "Monotonic calibrated interpolated look-up tables",
    "authors": ["M. Gupta", "A. Cotter", "J. Pfeifer", "K. Voevodski", "K. Canini", "A. Mangylov", "W. Moczydlowski", "A. van Esbroeck"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Multivariate convex regression: global risk bounds and adaptation",
    "authors": ["Q. Han", "J.A. Wellner"],
    "venue": "arXiv preprint arXiv:1601.06844,",
    "year": 2016
  }, {
    "title": "Isotonic regression in general dimensions",
    "authors": ["Q. Han", "T. Wang", "S. Chatterjee", "R.J. Samworth"],
    "venue": "arXiv preprint arXiv:1708.09468,",
    "year": 2017
  }, {
    "title": "Data analysis by positive decision trees",
    "authors": ["K. Makino", "T. Suda", "K. Yano", "T. Ibaraki"],
    "venue": "In CODAS, pp",
    "year": 1996
  }, {
    "title": "Probability machines: consistent probability estimation using nonparametric learning machines",
    "authors": ["J.D. Malley", "J. Kruppa", "A. Dasgupta", "K.G. Malley", "A. Ziegler"],
    "venue": "Methods of Information in Medicine,",
    "year": 2012
  }, {
    "title": "Classification trees for problems with monotonicity constraints",
    "authors": ["R. Potharst", "A.J. Feelders"],
    "venue": "SIGKDD Explor. Newsl.,",
    "year": 2002
  }, {
    "title": "Shape constrained additive models",
    "authors": ["N. Pya", "S.N. Wood"],
    "venue": "Statistics and Computing,",
    "year": 2015
  }, {
    "title": "Least squares isotonic regression in two dimensions",
    "authors": ["J. Spouge", "H. Wan", "W. Wilbur"],
    "venue": "Journal of Optimization Theory and Applications,",
    "year": 2003
  }, {
    "title": "ranger: A fast implementation of random forests for high dimensional data in C++ and R",
    "authors": ["M.N. Wright", "A. Ziegler"],
    "venue": "Journal of Statistical Software,",
    "year": 2017
  }, {
    "title": "Faithful variable screening for high-dimensional convex regression",
    "authors": ["M. Xu", "M. Chen", "J. Lafferty"],
    "venue": "Ann. Statist., 44(6):2624–2660,",
    "year": 2016
  }, {
    "title": "Deep lattice networks and partial monotonic functions",
    "authors": ["S. You", "D. Ding", "K. Canini", "J. Pfeifer", "M. Gupta"],
    "venue": "In NIPS,",
    "year": 2017
  }],
  "id": "SP:0072e8f0859f781617734903ff320ac96a1325be",
  "authors": [{
    "name": "Matt Bonakdarpour",
    "affiliations": []
  }, {
    "name": "Sabyasachi Chatterjee",
    "affiliations": []
  }, {
    "name": "Rina Foygel Barber",
    "affiliations": []
  }, {
    "name": "John Lafferty",
    "affiliations": []
  }],
  "abstractText": "Two methods are proposed for high-dimensional shape-constrained regression and classification. These methods reshape pre-trained prediction rules to satisfy shape constraints like monotonicity and convexity. The first method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms are developed for computing the estimators, and experiments are performed to demonstrate their performance on four datasets. We find that reshaping methods enforce shape constraints without compromising predictive accuracy.",
  "title": "Prediction Rule Reshaping"
}