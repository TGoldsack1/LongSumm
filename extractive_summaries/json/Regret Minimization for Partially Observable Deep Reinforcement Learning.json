{
  "sections": [{
    "heading": "1 INTRODUCTION",
    "text": "Many reinforcement learning problems of practical interest have the property of partial observability, where observations of state are generally non-Markovian. Despite the importance of partial observation in the real world, value function-based methods such as Q-learning (Mnih et al., 2013; 2015) generally assume a Markovian observation space. On the other hand, Monte Carlo policy gradient methods do not assume Markovian observations, but many practical policy gradient methods such as A3C (Mnih et al., 2016) introduce the Markov assumption when using a critic or state-dependent baseline in order to improve sample efficiency.\nConsider deep reinforcement learning methods that learn a state or state-action value function. One common workaround for the problem of partial observation is to learn value functions on the space of finite-length frame-history observations, under the assumption that frame-histories of sufficient length will give the environment the approximate appearance of full observability. When learning to play Atari 2600 games from images, deep Q-learning algorithms (Mnih et al., 2013; 2015) concatenate the last 4 observed frames of the video screen buffer as input to a state-action value convolutional network. Not all non-Markovian tasks are amenable to finite-length frame-histories; recurrent value functions can incorporate longer and potentially infinite histories (Hausknecht & Stone, 2017; Foerster et al., 2016), but at the cost of solving a harder optimization problem. Can we develop methods that learn a variant of the value function that is more robust to partial observability?\nOur contribution is a new model-free deep reinforcement learning algorithm based on the principle of regret minimization which does not require access to a Markovian state. Our method learns a policy by estimating a cumulative clipped advantage function, which is an approximation to a type of regret that is central to two partial information game-solving algorithms from which we draw our primary inspiration: counterfactual regret minimization (CFR) (Zinkevich et al., 2007) and CFR+ (Tammelin, 2014). Hence we call our algorithm “advantage-based regret minimization” (ARM).\nWe evaluate our approach on three visual reinforcement learning domains: Pong with varying framehistory lengths (Bellemare et al., 2013), and the first-person games Doom (Kempka et al., 2016) and Minecraft (Johnson et al., 2016). Doom and Minecraft exhibit a first-person viewpoint in a 3- dimensional environment and should appear non-Markovian even with frame-history observations.\nWe find that our method offers substantial improvement over prior methods in these partially observable environments: on both Doom and Minecraft, our method can learn well-performing policies within about 1 million simulator steps using only visual input frame-history observations."
  }, {
    "heading": "2 RELATED WORK",
    "text": "Deep reinforcement learning algorithms have been demonstrated to achieve excellent results on a range of complex tasks, including playing games (Mnih et al., 2015; Oh et al., 2016) and continuous control (Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016). Prior deep reinforcement learning algorithms either learn state or state-action value functions (Mnih et al., 2013), learn policies using policy gradients (Schulman et al., 2015), or perform a combination of the two using actor-critic architectures (Mnih et al., 2016). Policy gradient methods typically do not need to assume a Markovian state, but tend to suffer from poor sample complexity, due to their inability to use off-policy data. Methods based on learning Q-functions can use replay buffers to include off-policy data, accelerating learning (Lillicrap et al., 2016). However, learning Q-functions with Bellman error minimization typically requires a Markovian state space. When learning from observations such as images, the inputs might not be Markovian. Prior methods have proposed to mitigate this issue by using recurrent critics and Q-functions (Hausknecht & Stone, 2017; Oh et al., 2016; Mnih et al., 2016; Heess et al., 2015), and learning Q-functions that depend on entire histories of observations. Heuristics such as concatenation of short observation sequences have also been used (Mnih et al., 2015). However, all of these changes increase the size of the input space, increasing variance, and make the optimization problem more complex. Our method instead learns cumulative advantage functions that depend only on the current state, but can still handle non-Markovian problems.\nThe form of our advantage function update resembles positive temporal difference methods (Peng et al., 2016; van Hasselt & Wiering, 2007). Additionally, our update rule for a modified cumulative Q-function resembles the average Q-function (Anschel et al., 2017) used for variance reduction in Q-learning. In both cases, the theoretical foundations of our method are based on cumulative regret minimization, and the motivation is substantively different. Previous work by Ross et al. (2011); Ross & Bagnell (2014) has connected regret minimization to reinforcement learning, imitation learning, and structured prediction, although not with counterfactual regret minimization. Regression regret matching (Waugh et al., 2015) is based on a closely related idea, which is to directly approximate the regret with a linear regression model, however the use of a linear model is limited in representation compared to deep function approximation."
  }, {
    "heading": "3 ADVANTAGE-BASED REGRET MINIMIZATION",
    "text": "In this section, we provide background on CFR and CFR+, describe ARM in detail, and give some intuition for why ARM works."
  }, {
    "heading": "3.1 COUNTERFACTUAL REGRET MINIMIZATION (CFR)",
    "text": "In this section we review the algorithm of counterfactual regret minimization (Zinkevich et al., 2007). We closely follow the version of CFR as described in the Supplementary Material of Bowling et al. (2015), except that we try to use the notation of reinforcement learning where appropriate.\nConsider the setting of an extensive game. There are N players numbered i = 1, . . . , N . An additional player may be considered a “chance” player to simulate random events. At each time step of the game, one player chooses an action a ∈ Ai. Define the following concepts and notation:\n• Sequences: A sequence specifically refers to a sequence of actions starting from an initial game state. (It is assumed that a sequence of actions, including actions of the “chance” player, is sufficient for defining state within the extensive game.) Let H be the space of all sequences, and let Z be the space of terminal sequences. • Information sets: Let I be the space of information sets; that is, for each I ∈ I, I is a set\nof sequences h ∈ I which are indistinguishable to the current player. Information sets are a represention of partial observability.\n• Strategies: Let πi(a|I) be the strategy of the i-th player, where πi(a|I) is a probability distribution over action a conditioned on information set I . Let π = (π1, . . . , πN ) denote the strategy profile for all players, and let π−i = (π1, . . . , πi−1, πi+1, . . . , πN ) denote the strategy profile for all players except the i-th player. • Sequence probabilities: Let ρπ(h) be the probability of reaching the sequence h when all\nplayers follow π. Additionally, let ρπ(h, h′) be the probability of reaching h′ conditioned on h having already been reached. Similarly, define ρπi and ρ π −i to contain the contributions\nof respectively only the i-th player or of all players except the i-th. • Values: Let ui(z) be the value of a terminal sequence z to the i-th player. Let the expected\nvalue of a strategy profile π to the i-th player be Ji(π) = ∑ z∈Z ρ π(z)ui(z).\nDefine the counterfactual value QCFπ,i of all players following strategy π, except the i-th player plays to reach information set I and to then take action a:\nQCFπ,i(I, a) = ∑ h∈I ∑ z∈Z:h@z ρπ−i(z)ρ π|I→a i (h, z)ui(z). (1)\nThe notation h @ h′ denotes that h is a prefix of h′, while π|I → a denotes that action a is to be performed when I is observed. The counterfactual value QCFπ,i(I, a) is a calculation that assumes the i-th player reaches any h ∈ I , and upon reaching any h ∈ I it always chooses a. Consider a learning scenario where at the t-th iteration the players follow a strategy profile πt. The i-th player’s regret after T iterations is defined in terms of the i-th player’s optimal strategy π∗i :\nRTi = T−1∑ t=0 Ji((π t 1, . . . , π t i−1, π ∗ i , π t i+1, . . . , π t N ))− Ji(πt). (2)\nThe average regret is the average over learning iterations: (1/T )RTi . Now define the counterfactual regret of the i-th player for taking action a at information set I:\n(R(CF)i ) T (I, a) = T−1∑ t=0 ( QCFπt,i(I, a)− ∑ a′∈A πti(a ′|I)QCFπt,i(I, a′) ) (3)\n= (R(CF)i ) T−1(I, a) +QCFπT−1,i(I, a)− ∑ a′∈A πT−1i (a ′|I)QCFπT−1,i(I, a ′). (4)\nThe counterfactual regret (Equation (3)) can be shown to majorize the regret (Equation (2)) (Theorem 3, Zinkevich et al. (2007)). CFR can then be described as a learning algorithm where the strategy is updated using regret matching (Hart & Mas-Colell, 2000) applied to the counterfactual regret calculated in the most recent iteration:\nπT+1i (a|I) =  max(0,(R(CF)i ) T+1(I,a))∑ a′∈Amax(0,(R (CF) i ) T+1(I,a′)) if ∑ a′∈Amax(0, (R (CF) i ) T+1(I, a′)) > 0\n1 |A| otherwise.\n(5)\nIf all players follow the CFR regret matching strategy (Equation (5)), then at the T -th iteration the players’ average regrets are bounded by O(T−1/2) (Theorem 4, Zinkevich et al. (2007))."
  }, {
    "heading": "3.2 CFR+",
    "text": "CFR+ (Tammelin, 2014) consists of a modification to CFR, in which instead of calculating the full counterfactual regret as in (4), instead the counterfactual regret is recursively positively clipped to yield the clipped counterfactual regret:\n(R(CF+)i ) T (I, a) = max(0, (R(CF+)i ) T−1(I, a)) +QCFπT−1,i(I, a)− ∑ a′∈A πT−1i (a ′|I)QCFπT−1,i(I, a ′).\n(6)\nComparing Equation (4) with Equation (6), one can see that the only difference in CFR is that the previous iteration’s counterfactual regret is positively clipped in the recursion. The one-line change of CFR+ turns out to yield a large practical improvement in the performance of the algorithm (Bowling et al., 2015), and there is also an associated regret bound for CFR+ that is as strong as the bound for CFR (Tammelin et al., 2015)."
  }, {
    "heading": "3.3 FROM CFR AND CFR+ TO ARM",
    "text": "CFR and CFR+ are formulated for imperfect information extensive-form games, so they are naturally generalized to partially observed stochastic games since a stochastic game can always be represented in extensive form. A 1-player partially observed stochastic game is simply a POMDP with observation space O (Littman, 1994). By mapping information sets I ∈ I to observations o ∈ O, we may rewrite the counterfactual value as a kind of stationary observation-action value QCFπ,i(I, a) ≡ Q (stat) π|o 7→a(o, a) that assumes the agent follows the policy π except on observing o, after which the action a is always performed (Bellemare et al., 2016). We posit that the approximation Q(stat)π|o 7→a(o, a) ≈ Qπ(o, a), where Qπ is the usual action value function, is valid when observations are rarely seen more than once in a trajectory. By approximating Q(stat)π|o7→a(o, a) ≈ Qπ(o, a), we get a recurrence in terms of more familiar value functions (compare Equations (6) and (7)):\nĀ+t (ok, ak) = max(0, Ā + t−1(ok, ak)) +Qπt(ok, ak)− ∑ a′∈A πt(a ′|ok)Qπt(ok, a′) (7)\n= max(0, Ā+t−1(ok, ak)) +Qπt(ok, ak)− Vπt(ok) (8) = max(0, Ā+t−1(ok, ak)) +Aπt(ok, ak) (9)\nwhere Ā+t (o, a) is the cumulative clipped advantage function, and Aπt(o, a) is the ordinary advantage function evaluated at policy πt. Advantage-based regret minimization (ARM) is the resulting reinforcement learning algorithm that updates the policy to regret match on the cumulative clipped advantage function:\nπt+1(ak|ok) =  max(0,Ā+t (ok,ak))∑ a′∈Amax(0,Ā + t (ok,a ′)) if ∑ a′∈Amax(0, Ā + t (ok, a ′)) > 0\n1 |A| otherwise.\n(10)\nEquations (9) and (10) suggest the outline of a batch-mode deep reinforcement learning algorithm. At the t-th sampling iteration, a batch of data is collected by sampling trajectories using the current policy πt, followed by two processing steps: (a) fit Ā+t using Equation (9), then (b) set the next iteration’s policy πt+1 using Equation (10)."
  }, {
    "heading": "3.4 IMPLEMENTATION OF ARM",
    "text": "To implement Equation (9) with deep function approximation, we define two value function approximations, Vπt(ok; θt) and Q̄ + t (ok, ak;ωt), as well as a target value function V\n′(ok;ϕ), where θt, ωt, and ϕ are the learnable parameters. The cumulative clipped advantage function is represented as Ā+t (ok, ak) = Q̄ + t (ok, ak;ωt) − Vπt(ok; θt). Within each sampling iteration, the value functions are fitted using stochastic gradient descent by sampling minibatches and performing gradient steps. The state-value function Vπt(ok; θt) is fit to minimize an n-step temporal difference loss with a moving target V ′(ok+n;ϕ), essentially using the estimator of the deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016). In the same minibatch, Q̄+t (ok, ak; θt) is fit to a similar loss, but with an additional target reward bonus that incorporates the previous iteration’s cumulative clipped advantage, max(0, Ā+t−1(ok, ak)). The regression targets v(ok;ϕ) and q̄\n+(ok, ak;ϕ) are defined in terms of the n-step returns gnk = ∑k+n−1 k′=k γ k′−krk′ :\nv(ok;ϕ) , g n k + γ nV ′(ok+n;ϕ) (11)\nq(ok, ak;ϕ) , rk + γg n−1 k+1 + γ nV ′(ok+n;ϕ) (12)\nq̄+(ok, ak;ϕ) , max(0, Q̄ + t−1(ok, ak;ωt−1)− Vπt−1(ok; θt−1)) + q(ok, ak;ϕ). (13)\nAltogether, each minibatch step of the optimization subproblem consists of the following three parameter updates in terms of the regression targets v(ok;ϕ) and q̄+(ok, ak;ϕ):\nθ (`+1) t ← θ (`) t −\nα 2 ∇ θ (`) t (Vπt(ok; θ (`) t )− v(ok;ϕ(`)))2 (14)\nω (`+1) t ← ω (`) t −\nα 2 ∇ ω (`) t (Q̄+t (ok, ak;ω (`) t )− q̄+(ok, ak;ϕ(`)))2 (15)\nϕ(`+1) ← ϕ(`) + τ(θ(`+1)t − ϕ(`)). (16)\nAlgorithm 1 Advantage-based regret minimization (ARM). initialize π0 ← uniform, θ−1, ω−1 ← arbitrary for t in 0, . . . do\ncollect batch of trajectory data Dt ∼ πt initialize θt ← θt−1, ωt ← ωt−1, ϕ← θt−1 for ` in 0, . . . do\nsample transitions (ok, ak, rk, . . . , ok+n−1, ak+n−1, rk+n−1, ok+n) ∼ Dt calculate n-step returns gnk = ∑k+n−1 k′=k γ\nk′−krk′ set δk+n ← I[ok+n is terminal] if t = 0 then\nset φk ← 0 else\nset φk ← max(0, Q̄+t−1(ok, ak;ωt−1)− Vπt−1(ok; θt−1)) end if set v(ok)← gnk + γn(1− δk+n)V ′(ok+n;ϕ) set q̄+(ok, ak)← φk + rk + γgn−1k+1 + γn(1− δk+n)V ′(ok+n;ϕ) update θt with step size α and targets v(ok) (Equation (14)) update ωt with step size α and targets q̄+(ok, ak) (Equation (15)) update ϕ with moving average step size τ (Equation (16))\nend for set πt+1(a|o) ∝ max(0, Q̄+t (o, a;ωt)− Vπt(o; θt))\nend for\nThe overall advantage-based regret minimization algorithm is summarized in Algorithm 1.\nWe note that the mechanics of the ARM updates are similar to on-policy value function estimation, but ARM learns a modified on-policy Q-function from transitions with the added reward bonus max(0, Ā+t−1(ok, ak)) (Equation (13)). This reward bonus can be thought of a kind of “optimism in the face of uncertainty.”"
  }, {
    "heading": "3.5 ARM VS. EXISTING POLICY GRADIENT METHODS",
    "text": "In this section, we accentuate that ARM represents an inherently different update compared to existing policy gradient methods.\nRecent work has shown that policy gradient methods and Q-learning methods are connected via entropy regularization (O’Donoghue et al., 2017; Haarnoja et al., 2017; Nachum et al., 2017; Schulman et al., 2017; Haarnoja et al., 2018). One perspective is from the soft policy iteration framework for batch-mode reinforcement learning (Haarnoja et al., 2018), where at each batch iteration the updated policy is obtained by minimizing the average KL-divergence between the policy class Π and a target policy f . Below is the soft policy iteration update, where the subscript t refers to the batch iteration:\nπt+1 ← arg min π∈Π Eo∼ρt [DKL(π‖f)] (17)\n= arg min π∈Π Eo∼ρt [Ea∼π(·|o)[log(π(a|o))− log(f(a|o))]]. (18)\nUsing the connection between policy gradient methods and Q-learning, we define the policy gradient target policy as the softmax distribution on the entropy regularized advantage function Aβ-soft:\nfPG(a|o) , exp(βA β-soft t (o, a))∑\na′∈A exp(βA β-soft t (o, a\n′)) . (19)\nWe note that it is more conventional in the literature to use the soft Q-function Qβ-soft(o, a) rather than the soft advantage function Aβ-soft(o, a), however since they differ only by a function of o then they both induce the same target softmax policy. Now, parameterizing the policy π in terms of an explicit parameter θ, we obtain the expression for the existing policy gradient, where b(o) is a baseline function:\n∆θPG ∝ Eo∼ρt [Ea∼π(·|o;θ)[∇θ log(π(o|a; θ))((1/β) log(π(o|a; θ))−A β-soft t (o, a) + b(o))]]. (20)\nThe classic policy gradient arises in the limit β →∞. Note that an alternative choice of target policy f will lead to a different kind of policy gradient update. A policy gradient algorithm based on ARM instead proposes the following target policy based on the regret-matching distribution:\nfARM(a|o) , max(0, Ā + t (o, a))∑\na′∈Amax(0, Ā + t (o, a\n′)) . (21)\nSimilarly, we can express the ARM-like policy gradient, where again b(o) is a baseline:\n∆θARM = Eo∼ρt [Ea∼π(·|o;θ)[∇θ log(π(o|a; θ))(log(π(o|a; θ))− log(max(0, Ā+t (o, a))) + b(o))]]. (22)\nComparing Equations (20) and (22), we see that the ARM-like policy gradient (Equation (22)) has a logarithmic dependence on the advantage-like function Ā+, whereas the existing policy gradient (Equation (20)) is only linearly dependent on the advantage function Aβ-soft. This difference in logarithmic vs. linear dependence is responsible for a large part of the inherent distinction of ARM from existing policy gradient methods. One consequence of the difference in logarithmic vs. linear dependence is that the ARM-like update should be less sensitive to large positive advantages that may result from overestimation compared to existing policy gradient methods.\nWe also see that for the existing policy gradient (Equation (20)), the (1/β) log(π(a|o; θ)) term, which is derived from the policy entropy, is vanishing for large β (e.g. β = 100 is a common choice in practice). On the other hand, for the ARM-like policy gradient (Equation (22)), there is no similar vanishing effect, suggesting that ARM may perform a kind of entropy regularization by default.\nIn practice we cannot implement an ARM-like policy gradient exactly as in Equation (22), as due to the positive clipping max(0, Ā+) there can appear log(0). However we believe this is not an intrinsic obstacle, leaving the issue of implementing an ARM-like policy gradient to future work."
  }, {
    "heading": "3.6 WHY DOES ARM WORK BETTER IN PARTIALLY OBSERVABLE DOMAINS?",
    "text": "In the previous Section 3.5, we showed that ARM and existing policy gradient methods can be distinguished by their choices of target policy and the nature of their dependence on their respective advantage-like functions. In this section, we argue that the convergence results of CFR and CFR+ suggest that ARM, to the degree that it inherits the properties of CFR/CFR+, ought to benefit from greater partial observability compared to other methods.\nWe assume that regret bounds are a useful way to compare the convergence of different RL algorithms, due to the interpretation of regret as “area over the learning curve (and under the optimal expected value J∗).” Specifically, the regret bound of CFR and CFR+ is O(|O| √ T ) where |O| is the size of the observation space (Zinkevich et al., 2007; Tammelin et al., 2015). The policy gradient method with a suitable baseline has a learning rate η-dependent regret bound derived from the stochastic gradient method; assuming parameter norm bound B and gradient estimator second momentsG2, by setting the learning rate η ∝ T−1/2 policy gradient achieves a regret bound ofO( √ T ) with no explicit dependence on the observation space size |O| (Dick, 2015). We argue that possessing a regret bound proportional to the observation space size |O| is beneficial in highly partially observable domains. Let us fix an underlying state space S. Compare two RL algorithms, where algorithm 1 (which is ARM-like) has a regret bound c1|O| √ T , whereas algorithm\n2 (which is policy gradient-like) has a regret bound c2 √ T ; here, c1 and c2 are constants. Note that if c1|O| = c2 or equivalently |O| = c2/c1, then the two RL algorithms possess the exact same regret bound. If on the other hand |O| < c2/c1, then the regret bound of RL algorithm 1 is actually lower than that of RL algorithm 2. Applying this intuition to CFR and hence ARM suggests that ARM can benefit from greater partial observability if the degree of partial observability is above a threshold.\nFor Q-learning per se, we are not aware of any known regret bound. Szepesvári proved that the convergence rate of Q-learning in the L∞-norm, assuming a fixed exploration strategy, depends on a condition number C, which is the ratio of the minimum to maximum state-action occupation frequencies (Szepesvári, 1998), and which describes how “balanced” the exploration strategy is. If partial observability leads to imbalanced exploration due to confounding of states from perceptual aliasing (McCallum, 1997), then Q-learning should be negatively affected.\nWe note that there remains a gap between ARM as implemented and the theory of CFR: the use of (a) function approximation and sampling over tabular enumeration; (b) the “ordinary” Q-function instead of the “stationary” Q-function; and (c) n-step bootstrapped values instead of full returns for value function estimation. Waugh et al. (2015) address CFR with function approximation via a noisy version of a generalized Blackwell’s condition (Cesa-Bianchi & Lugosi, 2003). Even the original implementation of CFR used sampling in place of enumeration (Zinkevich et al., 2007). We refer the reader to Bellemare et al. (2016) for a more in-depth discussion of the stationary Q-function. Although only the full returns are guaranteed to be unbiased in non-Markovian settings, it is quite common for practical RL algorithms to trade off strict unbiasedness in favor of lower variance by using n-step returns or variations thereof (Schulman et al., 2016; Gu et al., 2017)."
  }, {
    "heading": "4 EXPERIMENTS",
    "text": "Because we hypothesize that ARM should perform well in partially observable reinforcement learning environments, we conduct our experiments on visual domains that naturally provide partial observations of state. All of our evaluations use feedforward convnets with frame-history observations. We are interested in comparing ARM with methods that assume Markovian observations, namely double deep Q-learning (van Hasselt et al., 2016), as well as methods that can handle non-Markovian observations, primarily TRPO (Schulman et al., 2015; 2016), and to a lesser extent A3C (Mnih et al., 2016) whose critic assumes Markovian observations. We are also interested in controlling for the advantage structure of ARM by comparing with other advantage-structured methods, which include dueling networks (Wang et al., 2016), as well as policy gradient methods that estimate an empirical advantage using a baseline state-value function or critic (e.g. TRPO, A3C)."
  }, {
    "heading": "4.1 LEARNING TO PLAY PONG WITH A SINGLE FRAME",
    "text": "Atari games consist of a small set of moving sprites with fixed shapes and palettes, and the motion of sprites can be highly deterministic, so that with only 4 recently observed frames as input one can predict hundreds of frames into the future on some games using only a feedforward model (Oh et al., 2015). To increase the partial observability of Atari games, one may artificially limit the amount of frame history fed as input to the networks (Hausknecht & Stone, 2017). As a proof of concept of ARM, we trained agents to play Pong via the Arcade Learning Environment (Bellemare et al., 2013) when the frame-history length is varied between 4 (the default) and 1. We found that the performance of double deep Q-learning degraded noticeably when the frame-history length was reduced from 4 to 1, whereas performance of ARM was not affected nearly as much. Our results on Pong are summarized in Figure 1."
  }, {
    "heading": "4.2 LEARNING TO NAVIGATE IN VIZDOOM",
    "text": "We evaluated ARM on the task of learning first-person navigation in the ViZDoom (Kempka et al., 2016) domain based on the game of Doom. Doom is a substantially more complex domain than Atari, featuring an egocentric viewpoint, 3D perspective, and complex visuals. We expect that Doom exhibits a substantial degree of partial observability and therefore serves as a more difficult evaluation of reinforcement learning algorithms’ effectiveness on partially observable domains. We performed our evaluation on two standard ViZDoom navigation benchmarks, “HealthGathering”\nand “MyWayHome.” In “HealthGathering,” the agent is placed in a toxic room and continually loses life points, but can navigate toward healthkit objects to prolong its life; the goal is to survive for as long as possible. In “MyWayHome,” the agent is randomly placed in a small maze and must find a target object that has a fixed visual appearance and is in a fixed location in the maze; the goal is to reach the target object before time runs out. Figure 2 (top row) shows example observations from the two ViZDoom scenarios.\nUnlike previous evaluations which augmented the raw pixel frames with extra information about the game state, e.g. elapsed time ticks or remaining health (Kempka et al., 2016; Dosovitskiy & Koltun, 2017), in our evaluation we forced all networks to learn using only visual input. Despite this restriction, ARM is still able to quickly learn policies with minimal tuning of hyperparameters and reach close to the maximum score in under 1 million steps. On “HealthGathering,” we observed that ARM very quickly learns a policy that can achieve close to the maximum episode return of 2100. Double deep Q-learning learns a more consistent policy on “HealthGathering” compared to ARM and TRPO, but we believe this to be the result of evaluating double DQN’s -greedy policy with small compared to the truly stochastic policies learned by ARM and TRPO. On “MyWayHome,” we observed that ARM generally learned a well-performing policy more quickly than other methods. Additionally, we found that ARM is able to take advantage of an off-policy replay memory when learning on ViZDoom by storing the trajectories of previous sampling batches and applying an importance sampling correction to the n-step returns; please see Section 6.2 in the Appendix for details. Our Doom results are in Figure 6."
  }, {
    "heading": "4.3 LEARNING TO NAVIGATE IN MINECRAFT",
    "text": "We finally evaluated ARM on the task of learning first-person navigation in the Malmö domain based on the game of Minecraft (Johnson et al., 2016). Minecraft has similar visual complexity to Doom and should possess a comparable degree of partial observability, but Minecraft has the potential to be more difficult than Doom due to the diversity of possible Minecraft environments that can be generated. Our evaluation on Minecraft is adapted from the teacher-student curriculum learning protocol (Matiisen et al., 2017), which consists of 5 consecutive “levels” that successively increase the difficulty of completing the simple task of reaching a target block: the first level (“L1”) consists of a single room; the intermediate levels (“L2”–“L4”) consist of a corridor with lava-bridge and wall-gap obstacles; and the final level (“L5”) consists of a 2×2 arrangement of rooms randomly separated by lava-bridge or wall-gap obstacles. Figure 2 (bottom row) shows example observations from the five Minecraft levels.\nWe performed our Minecraft experiments using fixed curriculum learning schedules to evaluate the sample efficiency of different algorithms: the agent is initially placed in the first level (“L1”), and the agent is advanced to the next level whenever a preselected number of simulator steps have elapsed, until the agent reaches the last level (“L5”). We found that ARM and dueling double DQN both were able to learn on an aggressive “fast” schedule of only 62500 simulator steps between levels. TRPO required a “slow” schedule of 93750 simulator steps between levels to reliably learn. ARM was able to consistently learn a well performing policy on all of the levels, whereas double DQN learned more slowly on some of the intermediate levels. ARM also more consistently reached a high score on the final, most difficult level (“L5”). Our Minecraft results are shown in Figure 4."
  }, {
    "heading": "5 DISCUSSION",
    "text": "In this paper, we presented a novel deep reinforcement learning algorithm based on counterfactual regret minimization (CFR). We call our method advantage-based regret minimization (ARM). Similarly to prior methods that learn state or state-action value functions, our method learns a cumulative clipped advantage function of observation and action. However, in contrast to these prior methods, ARM is well suited to partially observed or non-Markovian environments, making it an appealing choice in a number of difficult domains. When compared to baseline methods, including deep Q-learning and TRPO, on non-Markovian tasks such as the challenging ViZDoom and Malmö firstperson navigation benchmarks, ARM achieves substantially better results. This illustrates the value of ARM for partially observable problems. In future work, we plan to further explore applications of ARM to more complex tasks, including continuous action spaces."
  }, {
    "heading": "6 APPENDIX",
    "text": ""
  }, {
    "heading": "6.1 EXPERIMENTAL DETAILS",
    "text": ""
  }, {
    "heading": "6.1.1 PONG (ARCADE LEARNING ENVIRONMENT)",
    "text": "We use the preprocessing and convolutional network model of (Mnih et al., 2013). Specifically, we view every 4th emulator frame, convert the raw frames to grayscale, and perform downsampling to generate a single observed frame. The input observation of the convnet is a concatenation of the most recent frames (either 4 frames or 1 frame). The convnet consists of an 8× 8 convolution with stride 4 and 16 filters followed by ReLU, a 4 × 4 convolution with stride 2 and 32 filters followed by ReLU, a linear map with 256 filters followed by ReLU, and a linear map with |A| filters where |A| is the action space cardinality (|A| = 6 for Pong). We used Adam with a constant learning rate of α = 10−4, a minibatch size of 32, and the moment decay rates set to their defaults β1 = 0.9 and β2 = 0.999. Our results on each method are averaged across 3 random seeds.\nWe ran ARM with the hyperparameters: sampling batch size of 12500, 4000/3000 minibatches of Adam for the first/subsequent sampling iterations respectively, and target update step size τ = 0.01. Double DQN uses the tuned hyperparameters (van Hasselt et al., 2016). Note that our choice of ARM hyperparameters yields an equivalent number of minibatch gradient updates per sample as used by DQN and double DQN, i.e. 1 minibatch gradient update per 4 simulator steps."
  }, {
    "heading": "6.1.2 DOOM (VIZDOOM)",
    "text": "We used a convolutional network architecture similar to those of (Kempka et al., 2016) and (Dosovitskiy & Koltun, 2017). The Doom screen was rendered at a resolution of 160×120 and downsized to 84 × 84. Only every 4th frame was rendered, and the input observation to the convnet is a concatenation of the last 4 rendered RGB frames for a total of 12 input channels. The convnet contains 3 convolutions with 32 filters each: the first is size 8× 8 with stride 4, the second is size 4× 4 with stride 2, and the third is size 3 × 3 with stride 1. The final convolution is followed by a linear map with 1024 filters. A second linear map yields the output. Hidden activations are gated by ReLUs.\nFor “HealthGathering” only, we scaled rewards by a factor of 0.01. We did not scale rewards for “MyWayHome.” We used Adam with a constant learning rate of α = 10−5 and a minibatch size of 32 to train all networks (except TRPO). For “HealthGathering” we set β1 = 0.95, whereas for “MyWayHome” we set β1 = 0.9. We set β2 = 0.999 for both scenarios. Our results on each method are averaged across 3 random seeds.\nDouble DQN and dueling double DQN: n = 5 step returns; update interval 30000; 1 minibatch gradient update per 4 simulator steps; replay memory uniform initialization size 50000; replay memory maximum size 240000; exploration period 240000; with final exploration rate = 0.01.\nA3C: 16 workers; n = 20 steps for “HealthGathering” and n = 40 steps for “MyWayHome”; negentropy regularization β = 0.01; and gradient norm clip 5.\nTRPO: sampling batch size 12500; KL-divergence step size δ = 0.01; 10 conjugate gradient iterations; and Fisher information/Gauss-Newton damping coefficient λ = 0.1.\nARM: n = 5 step returns; sampling batch size 12500; 4000 Adam minibatches in the first sampling iteration, 3000 Adam minibatches in all subsequent sampling iterations; target update step size τ = 0.01. Again, our choice of ARM hyperparameters yields an equivalent number of minibatch gradient updates per sample as used by DQN and double DQN. For “HealthGathering” only, because ARM converges so quickly we annealed the Adam learning rate to α = 2.5 × 10−6 after 500000 elapsed simulator steps.\nOff-policy ARM: n = 5 step returns; sampling batch size 1563, replay cache sample size 25000; 400 Adam minibatches per sampling iteration; target update step size τ = 0.01; and importance sampling weight clip c = 1."
  }, {
    "heading": "6.1.3 MINECRAFT (MALMÖ)",
    "text": "Our Minecraft tasks generally were the same as the ones used by Matiisen et al. (2017), with a few differences. Instead of using a continuous action space, we used a discrete action space with 4 move and turn actions. To aid learning on the last level (“L5”), we removed the reward penalty upon episode timeout and we increased the timeout on “L5” from 45 seconds to 75 seconds due to the larger size of the environment. We scaled rewards for all levels by 0.001.\nWe use the same convolutional network architecture for Minecraft as we used for ViZDoom in Section 4.2. The Minecraft screen was rendered at a resolution of 320× 240 and downsized to 84× 84. Only every 5th frame was rendered, and the input observation of the convnet is a concatenation of the last 4 rendered RGB frames for a total of 12 input channels. We used Adam with constant learning rate α = 10−5, moment decay rates β1 = 0.9 and β2 = 0.999, and minibatch size 32 to train all networks (except TRPO). Our results on each method are averaged across 5 random seeds.\nDouble DQN and dueling double DQN: n = 5 step returns; update interval 12500; 1 minibatch gradient update per 4 simulator steps; replay memory uniform initialization size 12500; replay memory maximum size 62500; exploration period 62500; with final exploration rate = 0.01.\nTRPO: sampling batch size 6250; KL-divergence step size δ = 0.01; 10 conjugate gradient iterations; and Fisher information/Gauss-Newton damping coefficient λ = 0.1.\nARM: n = 5 step returns; sampling batch size 12500; 4000 Adam minibatches in the first sampling iteration, 3000 Adam minibatches in all subsequent sampling iterations; target update step size τ = 0.01."
  }, {
    "heading": "6.2 OFF-POLICY ARM VIA IMPORTANCE SAMPLING",
    "text": "Our current approach to running ARM with off-policy data consists of applying an importance sampling correction directly to the n-step returns. Given the behavior policy µ under which the data was sampled, the current policy πt under which we want to perform estimation, and an importance sampling weight clip c for variance reduction, the corrected n-step return we use is:\ngnk (µ‖πt) = k+n−1∑ k′=k γk ′−k  k′∏ `=k wµ‖πt(a`|o`)  rk′ (23) where the truncated importance weight wµ‖πt(a|o) is defined (Ionides, 2008):\nwµ‖πt(a|o) = min ( c, πt(a|o) µ(a|o) ) . (24)\nOur choice of c = 1 in our experiments was inspired by Wang et al. (2017). We found that c = 1 worked well but note other choices for c may also be reasonable.\nWhen applying our importance sampling correction, we preserve all details of the ARM algorithm except for two aspects: the transition sampling strategy (a finite memory of previous batches are cached and uniformly sampled) and the regression targets for learning the value functions. Specifically, the regression targets v(ok;ϕ), q(ok, ak;ϕ), and q̄+(ok, ak;ϕ) (Equations (11)–(13)) are modified to the following:\nvµ‖πt(ok;ϕ) = g n k (µ‖πt) + γnV ′(ok+n;ϕ) (25)\nqµ‖πt(ok, ak;ϕ) = rk + γwµ‖πt(ak|ok)g n−1 k+1 (µ‖πt) + γ nV ′(ok+n;ϕ) (26)\nq̄+µ‖πt(ok, ak;ϕ) = max(0, Q̄ + t−1(ok, ak;ωt−1)− Vπt−1(ok; θt−1)) + qµ‖πt(ok, ak;ϕ). (27) Note that the target value function V ′(ok+n;ϕ) does not require an importance sampling correction because V ′ already approximates the on-policy value function Vπt(ok+n; θt)."
  }, {
    "heading": "6.3 ADDITIONAL EXPERIMENTS",
    "text": ""
  }, {
    "heading": "6.3.1 ATARI 2600 GAMES",
    "text": "Although our primary interest is in partially observable reinforcement learning domains, we also want to check that ARM works in nearly fully observable and Markovian environments, such as\nAtari 2600 games. We consider two baselines: double deep Q-learning, and double deep fitted Qiteration which is a batch counterpart to double DQN. We find that double deep Q-learning is a strong baseline for learning to play Atari games, although ARM still successfully learns interesting policies. One major benefit of Q-learning-based methods is the ability to utilize a large off-policy replay memory. Our results on a suite of Atari games are in Figure 5."
  }, {
    "heading": "6.3.2 RECURRENCE IN DOOM MYWAYHOME",
    "text": "We evaluated the effect of recurrent policy and value function estimation in the maze-like MyWayHome scenario of ViZDoom. We found that recurrence has a small positive effect on the convergence of A2C (Mnih et al., 2016), but was much less significant than the choice of algorithm. Our hyperparameters were similar to those described for A3C in Section 6.1.2, except we used a learning rate 10−4 and gradient norm clip 0.5. For the recurrent policy and value function, we replaced the first fully connected operation with an LSTM featuring an equivalent number of hidden units (1024)."
  }],
  "year": 2018,
  "references": [{
    "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning",
    "authors": ["Oron Anschel", "Nir Baram", "Nahum Shimkin"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "The Arcade Learning Environment: An evaluation platform for general agents",
    "authors": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"],
    "venue": "Journal of Artificial Intelligence Research,",
    "year": 2013
  }, {
    "title": "Increasing the Action Gap: New Operators for Reinforcement Learning",
    "authors": ["Mark G. Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S. Thomas", "Rémi Munos"],
    "venue": "In AAAI,",
    "year": 2016
  }, {
    "title": "Heads-up limit hold’em poker is solved",
    "authors": ["Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin"],
    "year": 2015
  }, {
    "title": "Potential-Based Algorithms in On-Line Prediction and Game Theory",
    "authors": ["Nicolò Cesa-Bianchi", "Gábor Lugosi"],
    "venue": "Machine Learning,",
    "year": 2003
  }, {
    "title": "Policy Gradient Reinforcement Learning Without Regret",
    "authors": ["Travis Dick"],
    "venue": "Master’s thesis, University of Alberta,",
    "year": 2015
  }, {
    "title": "Learning to Act by Predicting the Future",
    "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"],
    "venue": "arXiv preprint arXiv:1611.01779v2,",
    "year": 2017
  }, {
    "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
    "authors": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2016
  }, {
    "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning",
    "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Bernhard Schölkopf", "Sergey Levine"],
    "venue": "arXiv preprint arXiv:1706.00387v1,",
    "year": 2017
  }, {
    "title": "Reinforcement Learning with Deep Energy-Based Policies",
    "authors": ["Tuomas Haarnoja", "Haoran Tang", "Pieter Abbeel", "Sergey Levine"],
    "venue": "arXiv preprint arXiv:1702.08165v2,",
    "year": 2017
  }, {
    "title": "Soft Actor-Critic: OffPolicy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
    "authors": ["Tuomas Haarnoja", "Aurick Zhou", "Pieter Abbeel", "Sergey Levine"],
    "venue": "arXiv preprint arXiv:1801.01290v1,",
    "year": 2018
  }, {
    "title": "A Simple Adaptive Procedure Leading to Correlated Equilibrium",
    "authors": ["Sergiu Hart", "Andreu Mas-Colell"],
    "year": 2000
  }, {
    "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
    "authors": ["Matthew Hausknecht", "Peter Stone"],
    "venue": "arXiv preprint arXiv:1507.06527v4,",
    "year": 2017
  }, {
    "title": "Memory-based control with recurrent neural networks",
    "authors": ["Nicolas Heess", "Jonathan J. Hunt", "Timothy P. Lillicrap", "David Silver"],
    "venue": "arXiv preprint arXiv:1512.04455v1,",
    "year": 2015
  }, {
    "title": "Truncated Importance Sampling",
    "authors": ["Edward L Ionides"],
    "venue": "Journal of Computational and Graphical Statistics,",
    "year": 2008
  }, {
    "title": "The Malmo Platform for Artificial Intelligence Experimentation",
    "authors": ["Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell"],
    "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning",
    "authors": ["Michal Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Jaśkowski"],
    "venue": "arXiv preprint arXiv:1605.02097v2,",
    "year": 2016
  }, {
    "title": "End-to-end training of deep visuomotor policies",
    "authors": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Continuous control with deep reinforcement learning",
    "authors": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"],
    "venue": "arXiv preprint arXiv:1509.02971v5,",
    "year": 2016
  }, {
    "title": "Markov games as a framework for multi-agent reinforcement learning",
    "authors": ["Michael L. Littman"],
    "venue": "In Proceedings of the 11th International Conference on Machine Learning,",
    "year": 1994
  }, {
    "title": "Teacher-Student Curriculum Learning",
    "authors": ["Tambet Matiisen", "Avital Oliver", "Taco Cohen", "John Schulman"],
    "venue": "arXiv preprint arXiv:1707.00183v1,",
    "year": 2017
  }, {
    "title": "Efficient Exploration in Reinforcement Learning with Hidden State",
    "authors": ["Andrew Kachites McCallum"],
    "venue": "In AAAI Fall Symposium on Model-directed Autonomous Systems,",
    "year": 1997
  }, {
    "title": "Playing Atari with Deep Reinforcement Learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"],
    "venue": "arXiv preprint arXiv:1312.5602v1,",
    "year": 2013
  }, {
    "title": "Human-level control through deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski"],
    "venue": "Nature, 518(7540):529–533,",
    "year": 2015
  }, {
    "title": "Asynchronous methods for deep reinforcement learning",
    "authors": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning",
    "authors": ["Ofir Nachum", "Mohammad Norouzi", "Kelvin Xu", "Dale Schuurmans"],
    "venue": "In 31st Conference on Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Combining policy gradient and Q-learning",
    "authors": ["Brendan O’Donoghue", "Rémi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"],
    "venue": "arXiv preprint arXiv:1611.01626v3,",
    "year": 2017
  }, {
    "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
    "authors": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L Lewis", "Satinder Singh"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Control of memory, active perception, and action in minecraft",
    "authors": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"],
    "venue": "In Proceedings of The 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning",
    "authors": ["Xue Bin Peng", "Glen Berseth", "Michiel van de Penne"],
    "venue": "ACM Transactions on Graphics,",
    "year": 2016
  }, {
    "title": "Reinforcement and Imitation Learning via Interactive NoRegret Learning",
    "authors": ["Stéphane Ross", "J. Andrew Bagnell"],
    "venue": "arXiv preprint arXiv:1406.5979v1,",
    "year": 2014
  }, {
    "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
    "authors": ["Stéphane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"],
    "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2011
  }, {
    "title": "Trust region policy optimization",
    "authors": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael Jordan", "Philipp Moritz"],
    "venue": "In Proceedings of the 32nd International Conference on Machine Learning",
    "year": 2015
  }, {
    "title": "HighDimensional Continuous Control Using Generalized Advantage Estimation",
    "authors": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael I. Jordan", "Pieter Abbeel"],
    "venue": "arXiv preprint arXiv:1506.02438v5,",
    "year": 2016
  }, {
    "title": "Equivalence Between Policy Gradients and Soft QLearning",
    "authors": ["John Schulman", "Xi Chen", "Pieter Abbeel"],
    "venue": "arXiv preprint arXiv:1704.06440v1,",
    "year": 2017
  }, {
    "title": "The Asymptotic Convergence-Rate of Q-learning",
    "authors": ["Csaba Szepesvári"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 1998
  }, {
    "title": "Solving Large Imperfect Information Games Using CFR+",
    "authors": ["Oskari Tammelin"],
    "venue": "arXiv preprint arXiv:1407.5042v1,",
    "year": 2014
  }, {
    "title": "Solving Heads-up Limit Texas Hold’em",
    "authors": ["Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling"],
    "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Reinforcement Learning in Continuous Action Spaces",
    "authors": ["Hado van Hasselt", "Marco A. Wiering"],
    "venue": "In Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning,",
    "year": 2007
  }, {
    "title": "Deep Reinforcement Learning and Double QLearning",
    "authors": ["Hado van Hasselt", "Arthur Guez", "David Silver"],
    "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Dueling Network Architectures for Deep Reinforcement Learning",
    "authors": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Sample Efficient Actor-Critic with Experience Replay",
    "authors": ["Ziyu Wang", "Victor Bapst", "Nicolas Heess", "Volodymyr Mnih", "Remi Munos", "Koray Kavukcuoglu", "Nando de Freitas"],
    "venue": "arXiv preprint arXiv:1611.01224v2,",
    "year": 2017
  }, {
    "title": "Solving Games with Functional Regret Estimation",
    "authors": ["Kevin Waugh", "Dustin Morrill", "J. Andrew Bagnell", "Michael Bowling"],
    "venue": "In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Regret Minimization in Games with Incomplete Information",
    "authors": ["Martin Zinkevich", "Michael Johanson", "Michael H. Bowling", "Carmelo Piccione"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2007
  }],
  "id": "SP:7f9333bf799831aa077005b9d152b0406c5e336d",
  "authors": [{
    "name": "Peter Jin",
    "affiliations": []
  }, {
    "name": "Sergey Levine",
    "affiliations": []
  }, {
    "name": "Kurt Keutzer",
    "affiliations": []
  }],
  "abstractText": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial or non-Markovian observations by using finite-length frame-history observations or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. We demonstrate that on several partially observed reinforcement learning tasks, this new class of algorithms can substantially outperform strong baseline methods: on Pong with single-frame observations, and on the challenging Doom (ViZDoom) and Minecraft (Malmö) first-person navigation benchmarks.",
  "title": "REGRET MINIMIZATION FOR PARTIALLY OBSERVABLE DEEP REINFORCEMENT LEARNING"
}