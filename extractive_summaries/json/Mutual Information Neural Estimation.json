{
  "sections": [{
    "heading": "1 Introduction",
    "text": "The measure of mutual information [25] has significant applications in data mining [8, 11]. An advantage of mutual information over other distances or similarity measures is that, in addition to linear correlation, it also captures non-linear functional or statistical dependency between different features. Therefore, it has been used to select, extract and cluster features [13, 21] in an unsupervised way. The measure has firm theoretic ground in information theory, and can be understood as the fundamental limits of the rate-distortion function [26], channel capacity [25], and secrecy capacity [1].\nTo apply mutual information to practical scenarios in data mining, one has to estimate it from data samples with limited or no knowledge of the underlying distribution. Mutual information estimation is a well-known difficult problem, especially when the feature vectors are continuous or in a high\n∗This work is supported by a grant from the University Grants Committee of the Hong Kong Special Administrative Region, China (Project No. 21203318).\nPreprint. Under review.\nar X\niv :1\n90 5.\n12 95\n7v 2\n[ cs\n.I T\ndimensional space [4, 21]. Despite the limitation of the well-known histogram approach [20, 28], there are various other estimation methods, including different density estimations using a kernel [18] and the nearest-neighbor distance [15].\nA more recent work considers iterative estimation using a neural network, called the mutual information neural estimation (MINE) [2]. Compared to other approaches, MINE appears to inherit the generalization capability of neural network and can work without careful choice of parameters. However, as the neural network needs to be trained iteratively by a gradient descent algorithm, one has to monitor the convergence of the estimate and decide when to stop. If the convergence rate is slow, one may have to wait for a long time and terminate prematurely, which can result in underfitting. Indeed, we discovered a simple bivariate mixed gaussian distribution where MINE converged very slowly, and the problem is more serious in the higher dimensional cases. The objective of this work is to understand and resolve this short-coming, which is essential before applying the neural estimation to real-world datasets that often have very high dimensions. Despite the huge success in the use of neural networks for various machine learning applications [9, 16, 22, 24, 27], the current understanding of neural network is limited. A proof of the generalization capability is known only for a very simple model [3].\nWe propose an alternative route of neural estimation, called the mutual information neural entropic estimation (MI-NEE), that drastically improves the convergence rate. Roughly speaking, MINE uses a neural network to estimate the divergence from the joint distribution to the product of marginal distributions. If we replace the product of the marginal distributions by a known uniform reference distribution, we can obtain an estimate of the joint entropy instead of the mutual information, but the convergence rate turns out to be much faster. Since the mutual information can be computed simply from the joint and marginal entropies, and the marginal entropies can be estimated more easily than the joint entropy, we can obtain a faster mutual information estimate than MINE.\nOur approach, in the use of a custom reference distribution, may resemble contrastive / ratio estimation methods [12, Sec. 12.2.4, pp 495–497], [10], which provides a neural estimation of the ratio between the unknown and the reference distributions (often by casting the unsupervised problem as a classification problem). However, the objective here is to estimate the KL divergence between the unknown distribution and the reference distribution by maximizing a lower bound, namely, the KL divergence between the neural network’s parameterized distribution and the reference distribution. For more details on the contrastive approach and its relation to MINE, see [19, 23]. Other neural network approaches to estimating density with respect to a reference distribution exist, e.g., in [5, 6], a neural network is used to obtain a deterministic map between a latent random variable with a known distribution and the data.\nDetailed derivations of our approach will be given in Section 3 following the problem formulation in Section 2. Some experimental results will be given in Section 4."
  }, {
    "heading": "2 Problem formulation",
    "text": "We use a sans serif capital letter Z to denote a random vector/variable and the same character Z in the normal math font for its alphabet set. pZ denotes the distribution of Z, which is a pdf if Z is continuous. The support supp(pZ) of (the distribution of) Z is the subset of values in Z with strictly positive probability density. E denotes the expectation operation. For simplicity, all the logarithms are natural logarithms, and so information quantities such as entropy and mutual information are measured in nats instead of bits.\nMutual information estimation Given continuous random vectors/variables X and Y with unknown pdf pXY(x, y) for (x, y) ∈ X × Y , the goal is to estimate the following Shannon’s mutual information from N ≥ 1 i.i.d. samples (X1,Y1), . . . , (XN ,YN ) of (X,Y):\nI(X ∧ Y) = D(pXY‖pXpY) = E [ ln pXY(X,Y)\npX(X)pY(Y) ] = H(X) +H(Y)−H(X,Y),\n(1a)\n(1b)\nwhere D and H denote the information divergence and entropy respectively defined as D(pZ‖pZ′) := E [ ln pZ(Z)\npZ′(Z)\n] ,\nH(Z) := E [ ln 1\npZ(Z)\n] .\n(2)\n(3)\nMINE [2] estimates I(X∧Y) by rewriting the divergence in (1a) as a maximization over a functional and uses a neural network to optimize the functional iteratively. In contrast, we estimate the mutual information by neural estimation of the entropies in (1b).\nEntropy estimation Given a continuous random vector/variable Z with unknown pdf, we want to estimate H(Z) from N i.i.d. samples Z1, . . . ,ZN of Z.\nWith Z chosen to be X, Y, and (X,Y) respectively, we obtain estimates of all the entropy terms in (1b), and therefore, the desired estimate of the mutual information."
  }, {
    "heading": "3 Neural entropic estimation",
    "text": "We derive a neural estimation of the entropy using a custom reference distribution. The desired mutual information estimation then follows from (1b), where MINE is argued to be a special case when the reference distribution is the product of marginal distributions. We end the section by discussing estimations using the uniform reference distribution."
  }, {
    "heading": "3.1 Entropy estimation",
    "text": "To estimate the entropy of Z using a neural network, we rewrite the entropy in terms of the divergence between pZ and a custom reference distribution pZ′ as:\nH(Z) = E [ ln 1\npZ′(Z) ] ︸ ︷︷ ︸\n1,\n−D(pZ‖pZ′). (4)\nNote that the first term (the cross entropy term) in (4) can be estimated using sample average\n1,≈ 1 N N∑ i=1 ln 1 pZ′(Zi) , (5)\nwhich is an unbiased estimate because pZ′ is a known pdf. For the formula to be valid, the divergence should be bounded, which requires\nsupp(pZ) ⊆ supp(pZ′). (6)\nOther than the above restriction, however, one is free to choose any reference Z′ in the calculation of the entropy in (4). Indeed, not only there is no requirement for pZ′ to be close to pZ, we will argue that there is a benefit in choosing pZ′ to be different from pZ, namely, that it can lead to a faster convergence for the neural estimate of the divergence.\nAs in MINE [2], to apply a neural network to estimate the entropy, we rewrite the divergence using the variational formula [7] as follows:\nD(pZ‖pZ′) = D(pZ‖pZ′)− inf pẐ∈P(Z) D(pZ‖pẐ)\n= sup pẐ∈P(Z) E\n[ ln pẐ(Z)\npZ′(Z) ] = sup f :Z 7→R { E [f(Z)]− lnE [ ef(Z ′) ]}\n︸ ︷︷ ︸ 2, .\n(7a)\n(7b)\n(7c)\nIn the first equality (7a), the infimum is over the choices of a distribution pẐ for a random variable Ẑ with alphabet set Z. Equality holds because the divergence D(pZ‖pẐ) is non-negative and equal to 0 if and only if pẐ = pZ. The seemingly redundant infimum term plays an important role in the neural estimation. As can be seen in the equality (7b), the term ln pZ involving the unknown distribution pZ no longer appears inside the expectation. Instead, we have the term ln pẐ which will be evaluated and optimized by a neural network. More precisely, suppose the neural network computes the function f : Z 7→ R, it can be turned into a probability distribution by the formula\npẐ(z) := pZ′(z)e\nf(z) E [ ef(Z′) ] ∀z ∈ Z, which is non-negative and integrates over z ∈ Z to 1. Applying this formula to the supremum in (7b) gives the last equality (7c). Since the supremum is achieved uniquely by PẐ = PZ, it follows that f is optimal to (7c) if and only if\nf(z) = ln pZ(z)\npZ′(z) + c ∀z ∈ Z (8)\nfor some constant c. In summary, we have\nProposition 1 For any continuous random vector/variable Z, and any other random vector/variable Z′ with a larger support (6), we have\nH(Z) = E [ ln 1\npZ′(Z) ] − sup f :Z→R { E[f(Z)]− lnE [ ef(Z ′) ]} . (9)\nFurthermore, any optimal solution f must satisfy the optimality condition (8). 2\nNote that the objective function in (7c) can be estimated from the samples as\n2,≈ 1 N N∑ i=1 f(Zi)− ln 1 N ′ N ′∑ i=1 ef(Z ′ i), (10)\nwhere Z1, . . . ,ZN are i.i.d. samples of Z and Z′1, . . . ,Z ′ N ′ are i.i.d. samples of Z ′. Although the estimate may have bias from the estimate of the log expectation term lnE [ ef(Z ′) ] , we can reduce\nsuch bias by choosing N ′ sufficiently large, which is possible since pZ′ is a known pdf. MINE also has a similar log expectation term but the bias in the estimation of the term and its corresponding gradient may be non-negligible, as the expectation there is with respect to an unknown pdf, namely, the product pXpY of the marginal distributions. (We will briefly revisit this in Section 3.3.)\nTo estimate the supremum in (7c), we apply a neural network as in MINE with parameters θ that outputs\nf(z) := φz(θ) ∀z ∈ Z. (11) Define the loss function as the negation of 2,,\nL(θ) := −E [φZ(θ)] + lnE [ eφZ′ (θ) ] . (12)\nWe can iteratively optimize f to maximize 2,by updating θ with standard gradient descent algorithms that use minibatch estimates of the gradient\n∇L(θ) = −E [∇φZ(θ)] + E [ eφZ′ (θ)∇φZ′(θ) ] E [ eφZ′ (θ)\n] . (13) Again, the expectations in the second term can be estimated by any number of samples from the known reference distribution pZ′ , and so the bias from the estimate of the expectation in the denominator can be made negligible if desired. In practice, the stochasticity involved in the minibatch estimates somehow avoids overfitting even with an over-parameterized neural network [3, 29], and one can often converge to a good minima using a small batch size [17]. To maintain such stochasticity for large N ′, one can simply generate new samples of Z′ for each step of the descend algorithm, which is possible as pZ′ is known.\nAltogether, an estimate H(Z) can be obtained as follows using the estimate (5) of the cross entropy in (4) and the estimate (10) of the divergence in (4) where f is optimized by training the neural network (11) for some t ≥ 0 times using the loss function (12).\nEntropy estimate The estimate of the entropy is given by\nH(Z) ≈ 1 N N∑ i=1 ln 1 pZ′(Zi) − 1 N N∑ i=1 φZi(θt) + ln 1 N ′ N ′∑ i=1 e φZ′ i (θt), (14)\nwhere θt is the parameter after t steps of the gradient descend algorithm.\nWe remark that the above estimate is neither a lower nor an upper bound on the entropy estimate because of the possibilities of underfitting due to insufficient training and overfitting due to the use of sample estimates for the training objective. The same issue applies to MINE. Nevertheless, while one can check whether overfitting occurs using a separate validation set, it is hard to tell if there is underfitting without knowing the ground truth. Indeed, the convergence rate of the parameters θ may be so slow that one may falsely think that the parameters have converged even if they have not. We found that such situation may be avoided by an appropriate choice of the reference distribution."
  }, {
    "heading": "3.2 Mutual information estimation",
    "text": "By expressing the mutual information in terms of the entropies in (1b), it is straightforward to obtain a mutual information estimate by estimating the entropies as explained in the previous section. We simplify the estimate further by choosing the reference distributions appropriately so that the cross entropy terms in the entropy estimates cancel out:\nProposition 2 For any continuous random vectors/variables X and Y,\nI(X ∧ Y) = D(pXY‖pX′Y′)−D(pX‖pX′)−D(pY‖pY′)\n= sup f0:X×Y→R\n{E [f0(X,Y)]− lnE [ ef0(X ′,Y′) ] }\n− sup f1:X→R\n{E [f1(X)]− lnE [ ef1(X ′) ] }\n− sup f2:Y→R\n{E [f2(Y)]− lnE [ ef2(Y ′) ] },\n(15a)\n(15b)\nwhere X′ and Y′ are independent random variables/vectors with larger support than X and Y, i.e.,\npX′Y′(x, y) = pX′(x)pY′(y) > 0 ∀x ∈ X, y ∈ Y : pX′Y′(x, y) > 0. (16)\nFurthermore, the optimal f0, f1 and f2 satisfy\nf0(x, y)− f1(x)− f2(x) = iXY(x,y):=︷ ︸︸ ︷ ln pXY(x, y)\npX(x)pY(y) +c (17)\nfor some constant c. 2\nPROOF (15a) follows from (1b) and (4) with (Z,Z′) set to (X,X′), (Y,Y′), and ((X,Y), (X′,Y′)) respectively. Note that the cross entropy terms cancel out due to the independence of X′ and Y′, i.e., E [ ln 1pX′Y′ (X,Y) ] = E [ 1 ln pX′ (X) ] + E [ 1 ln pY′ (Y) ] . Equation (15b) follows from the variational formula (7), and (17) follows directly from the optimality condition (8).\nThe desired mutual information estimate can be obtained from the sample estimate of (15b) with f0, f1, and f2 optimized independently using three neural networks as described in the previous section, i.e., with the loss functions chosen as (12) with (Z,Z′) set to ((X,Y), (X′,Y′)), (X,X′), and (Y,Y′) respectively.\nAlternatively, one can train a single neural network with three outputs, one for each fi. More precisely, construct a neural network with parameters θ, two inputs x ∈ X and y ∈ Y , and three outputs φx,y := (φ (0) x,y, φ (1) x , φ (2) y ). With\nfi(x, y) := φ (i) x,y(θ) ∀i ∈ {0, 1, 2}, x ∈ X, y ∈ Y,\nwe update the parameters θ to minimize the sum of the loss functions (12) evaluated for the three choices of (Z,Z′), i.e.,\nL(θ) := −E [ φ (0) X,Y(θ) ] + lnE [ e φ (0) X′,Y′ ] − E [ φ (1) X (θ) ] + lnE [ eφ (1) X′\n] − E [ φ (2) Y (θ) ] + lnE [ eφ (2) Y′ ] .\nMutual information estimate The mutual information can then be estimated with\nI(X ∧ Y) ≈ 1 N N∑ i=1 φ (0) Xi,Yi (θt)− ln 1 N ′ N ′∑ i=1 e φ (0) X′ i ,Y′ i (θt)\n− 1 N N∑ i=1 φ (1) Xi (θt) + ln 1 N ′ N ′∑ i=1 e φ (1) X′ i (θt)\n− 1 N N∑ i=1 φ (2) Yi (θt) + ln 1 N ′ N ′∑ i=1 e φ (2) Y′ i (θt) ,\n(18a)\n(18b)\n(18c)\nwhere θt is the parameter after training the neural network t times."
  }, {
    "heading": "3.3 Estimation using a uniform reference",
    "text": "MINE can be viewed as the special case of the mutual information estimation in the last section when the reference distribution is chosen as the product of marginal distribution of X and Y, i.e.,\npX′Y′(x, y) = pX(x)pY(y) ∀x ∈ X, y ∈ Y. (19) In this case, both D(pX‖pX′) and D(pY‖pY′) in (15a) are zero, and so\nI(X ∧ Y) = D(pXY‖pX′Y′)\n= sup f0:X×Y→R\n{E [f0(X,Y)]− lnE [ ef0(X ′,Y′) ] },\n(20a)\n(20b)\nand the optimal solution f0 satisfies\nf0(x, y) = iXY(x, y) + c (21) for some constant c, where iXY(x, y) is defined in (17). With the optimal f0, the first term in (20b) becomes E [f0(X,Y)] = I(X ∧ Y) + c, namely a constant shift of the mutual information, while the second term becomes − lnE [ ef0(X ′,Y′) ] = −c, which cancels out the constant shift to give the\ndesired mutual information.\nNote that there is no need to train the neural network for the outputs φ(1)x and φ (2) y because the corresponding terms (18b) and (18c) do not appear in (20b). To train the remaining output φ(0)x,y , one cannot sample (X′,Y′) from the unknown pdf’s pX and pY. Instead, as done in MINE, the samples X′i’s and Y ′ i’s can be obtained by resampling the samples Xi’s and Yi’s independently. As a result, one cannot arbitrarily reduce the bias in estimating the log expectation term and its gradient in (20b). Different from MINE, we choose the following uniform distribution.\nUniform reference We obtain the mutual information estimate with\npX′Y′(x, y) = pX′(x)pY′(y) =\n{ 1\nVol(B) (x, y) ∈ B 0 otherwise,\n(22a)\nwhere B is a bounding box with volume Vol(B) and containing all the values of (x, y) with\nmin 1≤i≤N Xi ≤ x ≤ max 1≤i≤N Xi\nmin 1≤i≤N Yi ≤ y ≤ max 1≤i≤N\nYi. (22b)\nIf X and Y are vectors, the above minimization, maximization, and inequalities are elementwise.\nThere is, however, a technical issue with the above choice of uniform reference. (15b) is valid only if (16) holds, which requires B to contain all (x, y) with pXY(x, y) > 0. However, such requirement may not be satisfied as pXY is unknown and may have unbounded support. Nevertheless, we argue that the above choice of B can still give a good estimate if the density pXY outside B has negligible contribution to the mutual information. More precisely, define (X̃, Ỹ) with density\npX̃Ỹ(x, y) = pXY(x, y)\nPr{(X,Y) ∈ B} ∀(x, y) ∈ B,\nnamely the conditional density of (X,Y) given (X,Y) ∈ B. Note that Pr{(X,Y) ∈ B} goes to 1 as N goes to infinity by (22b). We therefore make the mild assumption that\nI(X̃ ∧ Ỹ) ≈ I(X ∧ Y) (23)\nfor sufficiently large N . Since (Xi,Yi) can also be viewed as samples of (X̃, Ỹ), we can estimate I(X̃ ∧ Ỹ) using the same formula (18). In particular, it is valid to use a uniform reference because its support covers that of pX̃,Ỹ."
  }, {
    "heading": "4 Experimental results",
    "text": "To evaluate the convergence rate, we plotted the mutual information estimates (18) with uniform reference (22) against the number of training steps and compared the curve to that of MINE. We first consider a simple bivariate mixed gaussian distribution and show that MINE has much slower convergence than our approach even in this low dimensional example. We then consider the higher dimensional case using a basic gaussian distribution and show that our approach can achieve significantly faster convergence rate even with a moderate increase in the dimension.\nThe bivariate mixed gaussian distribution is defined as\nMG(ρ) : pXY(x, y) = 1\n2 N[ xy ]\n( 0, [ 1 ρ ρ 1 ]) + 1 2 N[ xy ] ( 0, [ 1 −ρ −ρ 1 ]) (24)\nwhere Nz(µ,Σ) denotes the multivariate gaussian distribution over z with mean µ and covariance matrix Σ, and ρ ∈ [0, 1) is a model parameter that specifies the positive and negative correlations of X and Y for each gaussian component. The higher dimensional gaussian distribution is defined as\nHG(ρ, d) : pXY(x,y) = d∏ i=1 N[ xiyi ] ( 0, [ 1 ρ ρ 1 ]) where x := (x1, . . . , xd), y := (y1, . . . , yd) ∈ Rd. (25)\nIn addition to the correlation coefficient ρ, there is an additional parameter d that specifies the dimension of X and Y.\nFor the mixed gaussian model MG(0.9) with sample size N = 400 points, Figure 1 plots the mutual information estimates after training with a batch size of 100 and learning rate of 10−4. For MINE,\nwe follow [2] to use moving average in the gradient estimate, where the moving average rate is set to be 0.01. For our approach, instead of using a moving average in the gradient estimate, we increase the reference sample size N ′ to 10 times the data sample size N . For both MINE and our approach, we further apply a moving average of rate 0.01 to smooth out jitters in the estimates. Figure 1a shows that our approach converges to within 10% of the ground truth close to 2 × 104 iterations. Figure 1b shows that MINE requires close to 7 × 104 iterations. Furthermore, MINE exhibits a staircase convergence with two distinct jumps. The estimate remains close to 0 until the first jump at around 104 iterations. The estimate then remains stagnant at a value smaller than 50% of the ground truth until the second jump at around 5× 104 iterations. We remark that the staircase convergence may mislead one to think that neural network has converged while it has not. We found that the issue can be more serious for smaller values of ρ.\nFor the higher dimensional gaussian distribution, we consider HG(0.9, 6) with again a sample size of 400 and a batch size of 100. The learning rate is reduced to 5× 10−5 to avoid excessive jitters. For our approach, we increase the reference sample size to 300 times the data sample size to reduce the effect of overfitting the reference. Figure 2a shows that our approach converges to within 10% of the ground truth close to 6× 103 iterations. However, Figure 2b shows that MINE is unable to converge to within 10% of the ground truth even after 2.5× 104 iterations.2 Indeed, MINE terminates before 3 × 104 iterations due to numerical instability issue, but further reducing the learning rate causes excessive slow down in convergence. In contrast, our approach has slight overfitting as the estimate can go above the ground truth. We found that this issue is more pronounced for higher dimension, but can be alleviated by increasing the reference sample size in the expense of more computations for each training step. One can also use a separate validation set to terminate the training of each neural networks before significant overfitting.\nThe above results can be reproduced by running the corresponding jupyter notebooks using binder [14] at the GitHub repository below:\nhttps://github.com/ccha23/MI-NEE\n2In [2, Fig. 1], MINE reaches about 20% of the ground truth, however, we were unable to reproduce this results since, to the best of our knowledge, the authors’ parameters choice / code are not publicly available. Nevertheless, our observations remain valid since the comparisons made here between MINE and MI-NEE are performed under comparable parameters / neural network architecture."
  }],
  "year": 2019,
  "references": [{
    "title": "Common randomness in information theory and cryptography—Part I: Secret sharing",
    "authors": ["R. Ahlswede", "I. Csiszár"],
    "venue": "IEEE Trans. Inf. Theory, 39(4):1121–1132, July",
    "year": 1993
  }, {
    "title": "Mutual information neural estimation",
    "authors": ["M.I. Belghazi", "A. Baratin", "S. Rajeshwar", "S. Ozair", "Y. Bengio", "A. Courville", "D. Hjelm"],
    "venue": "J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 531–540, Stockholmsmässan, Stockholm Sweden, 10–15 Jul",
    "year": 2018
  }, {
    "title": "Sgd learns over-parameterized networks that provably generalize on linearly separable data",
    "authors": ["A. Brutzkus", "A. Globerson", "E. Malach", "S. Shalev-Shwartz"],
    "venue": "arXiv preprint arXiv:1710.10174,",
    "year": 2017
  }, {
    "title": "Estimating optimal feature subsets using efficient estimation of highdimensional mutual information",
    "authors": ["T.W. Chow", "D. Huang"],
    "venue": "IEEE Transactions on Neural networks, 16(1):213–224,",
    "year": 2005
  }, {
    "title": "NICE: Non-linear independent components estimation",
    "authors": ["L. Dinh", "D. Krueger", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1410.8516,",
    "year": 2014
  }, {
    "title": "Density estimation using real NVP",
    "authors": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"],
    "venue": "arXiv preprint arXiv:1605.08803,",
    "year": 2016
  }, {
    "title": "Asymptotic evaluation of certain markov process expectations for large time",
    "authors": ["M.D. Donsker", "S.S. Varadhan"],
    "venue": "iv. Communications on Pure and Applied Mathematics, 36(2):183–212,",
    "year": 1983
  }, {
    "title": "Normalized mutual information feature selection",
    "authors": ["P.A. Estévez", "M. Tesmer", "C.A. Perez", "J.M. Zurada"],
    "venue": "IEEE Transactions on Neural Networks, 20(2):189–201,",
    "year": 2009
  }, {
    "title": "Generative adversarial nets",
    "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"],
    "venue": "Advances in neural information processing systems, pages 2672–2680,",
    "year": 2014
  }, {
    "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
    "authors": ["M.U. Gutmann", "A. Hyvärinen"],
    "venue": "Journal of Machine Learning Research, 13(Feb):307–361,",
    "year": 2012
  }, {
    "title": "An introduction to variable and feature selection",
    "authors": ["I. Guyon", "A. Elisseeff"],
    "venue": "Journal of machine learning research, 3(Mar):1157–1182,",
    "year": 2003
  }, {
    "title": "The Elements of Statistical Learning: Data mining, Inference, and Prediction, springer series in statistics",
    "authors": ["T. Hastie", "R. Tibshirani", "J. Friedman"],
    "year": 2009
  }, {
    "title": "Learning deep representations by mutual information estimation and maximization",
    "authors": ["R.D. Hjelm", "A. Fedorov", "S. Lavoie-Marchildon", "K. Grewal", "P. Bachman", "A. Trischler", "Y. Bengio"],
    "venue": "International Conference on Learning Representations,",
    "year": 2019
  }, {
    "title": "Binder 2.0-reproducible, interactive, sharable environments for science at scale",
    "authors": ["P. Jupyter", "M. Bussonnier", "J. Forde", "J. Freeman", "B. Granger", "T. Head", "C. Holdgraf", "K. Kelley", "G. Nalvarte", "A. Osheroff"],
    "venue": "In Proceedings of the 17th Python in Science Conference,",
    "year": 2018
  }, {
    "title": "Estimating mutual information",
    "authors": ["A. Kraskov", "H. Stögbauer", "P. Grassberger"],
    "venue": "Phys. Rev. E, 69:066138, Jun",
    "year": 2004
  }, {
    "title": "Imagenet classification with deep convolutional neural networks",
    "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"],
    "venue": "Advances in neural information processing systems, pages 1097–1105,",
    "year": 2012
  }, {
    "title": "Revisiting small batch training for deep neural networks",
    "authors": ["D. Masters", "C. Luschi"],
    "venue": "arXiv preprint arXiv:1804.07612,",
    "year": 2018
  }, {
    "title": "Estimation of mutual information using kernel density estimators",
    "authors": ["Y.-I. Moon", "B. Rajagopalan", "U. Lall"],
    "venue": "Physical Review E, 52(3):2318,",
    "year": 1995
  }, {
    "title": "Representation learning with contrastive predictive coding",
    "authors": ["A. v. d. Oord", "Y. Li", "O. Vinyals"],
    "venue": "arXiv preprint arXiv:1807.03748,",
    "year": 2018
  }, {
    "title": "Estimation of entropy and mutual information",
    "authors": ["L. Paninski"],
    "venue": "Neural computation, 15(6):1191–1253,",
    "year": 2003
  }, {
    "title": "Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy",
    "authors": ["H. Peng", "F. Long", "C. Ding"],
    "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence, (8):1226–1238,",
    "year": 2005
  }, {
    "title": "Deep contextualized word representations",
    "authors": ["M.E. Peters", "M. Neumann", "M. Iyyer", "M. Gardner", "C. Clark", "K. Lee", "L. Zettlemoyer"],
    "venue": "arXiv preprint arXiv:1802.05365,",
    "year": 2018
  }, {
    "title": "On variational lower bounds of mutual information",
    "authors": ["B. Poole", "S. Ozair", "A. van den Oord", "A.A. Alemi", "G. Tucker"],
    "venue": "In NeurIPS Workshop on Bayesian Deep Learning,",
    "year": 2018
  }, {
    "title": "A survey on deep learning: Algorithms, techniques, and applications",
    "authors": ["S. Pouyanfar", "S. Sadiq", "Y. Yan", "H. Tian", "Y. Tao", "M.P. Reyes", "M.-L. Shyu", "S.-C. Chen", "S. Iyengar"],
    "venue": "ACM Computing Surveys (CSUR), 51(5):92,",
    "year": 2018
  }, {
    "title": "A mathematical theory of communication",
    "authors": ["C.E. Shannon"],
    "venue": "The Bell System Technical Journal, 27(3):379– 423, July",
    "year": 1948
  }, {
    "title": "Coding theorems for a discrete source with a fidelity criterion",
    "authors": ["C.E. Shannon"],
    "venue": "IRE Nat. Conv. Rec, 4(142-163):1,",
    "year": 1959
  }, {
    "title": "Mastering the game of go with deep neural networks and tree",
    "authors": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"],
    "venue": "search. nature,",
    "year": 2016
  }, {
    "title": "The mutual information: Detecting and evaluating dependencies between variables",
    "authors": ["R. Steuer", "J. Kurths", "C.O. Daub", "J. Weise", "J. Selbig"],
    "venue": "Bioinformatics, 18(suppl_2):S231–S240,",
    "year": 2002
  }, {
    "title": "Understanding deep learning requires rethinking generalization",
    "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"],
    "venue": "arXiv preprint arXiv:1611.03530,",
    "year": 2016
  }],
  "id": "SP:1c4969136f6a1ce8aa77d68d3d00ade5be5e9ee3",
  "authors": [{
    "name": "Chung Chan",
    "affiliations": []
  }, {
    "name": "Ali Al-Bashabsheh",
    "affiliations": []
  }],
  "abstractText": "We point out a limitation of the mutual information neural estimation (MINE) where the network fails to learn at the initial training phase, leading to slow convergence in the number of training iterations. To solve this problem, we propose a faster method called the mutual information neural entropic estimation (MI-NEE). Our solution first generalizes MINE to estimate the entropy using a custom reference distribution. The entropy estimate can then be used to estimate the mutual information. We argue that the seemingly redundant intermediate step of entropy estimation allows one to improve the convergence by an appropriate reference distribution. In particular, we show that MI-NEE reduces to MINE in the special case when the reference distribution is the product of marginal distributions, but faster convergence is possible by choosing the uniform distribution as the reference distribution instead. Compared to the product of marginals, the uniform distribution introduces more samples in low-density regions and fewer samples in high-density regions, which appear to lead to an overall larger gradient for faster convergence.",
  "title": "Neural Entropic Estimation: A faster path to mutual information estimation"
}