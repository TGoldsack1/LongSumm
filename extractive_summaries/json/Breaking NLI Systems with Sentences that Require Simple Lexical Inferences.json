{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 650–655 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n650"
  }, {
    "heading": "1 Introduction",
    "text": "Recognizing textual entailment (RTE) (Dagan et al., 2013), recently framed as natural language inference (NLI) (Bowman et al., 2015) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence. Following the release of the large-scale SNLI dataset (Bowman et al., 2015), many end-to-end neural models have been developed for the task, achieving high accuracy on the test set. As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain (Chen et al., 2018). This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge.\n1The contradiction example follows the assumption in Bowman et al. (2015) that the premise contains the most prominent information in the event, hence the premise can’t describe the event of a man holding both instruments.\nIn this paper we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that require lexical and world knowledge. Inspired by the work of Jia and Liang (2017) on reading comprehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set.\nThe performance on the new test set is substantially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. (2018) and Poliak et al. (2018), who showed that the label can be identified by looking only at the hypothesis and exploiting annotation artifacts such as word choice and sentence length.\nFurther investigation shows that what mostly affects the systems’ ability to correctly predict a test example is the amount of similar examples found in the training set. Given that training data will always be limited, this is a rather inefficient way to learn lexical inferences, stressing the need to develop methods that do this more\neffectively. Our test set can be used to evaluate such models’ ability to recognize lexical inferences, and it is available at https://github. com/BIU-NLP/Breaking_NLI."
  }, {
    "heading": "2 Background",
    "text": "NLI Datasets. The SNLI dataset (Stanford Natural Language Inference, Bowman et al., 2015) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Young et al. (2014), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Workers were instructed to judge the relation between sentences given that they describe the same event. Hence, sentences that differ by a single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral.\nFollowing criticism on the simplicity of the dataset, stemming mostly from its narrow domain, two additional datasets have been collected. The MultiNLI dataset (Multi-Genre Natural Language Inference, Williams et al., 2018) was collected similarly to SNLI, though covering a wider range of genres, and supporting a cross-genre evaluation. The SciTail dataset (Khot et al., 2018), created from science exams, is somewhat different from the two datasets, being smaller (27,026 examples), and labeled only as entailment or neutral. The domain makes this dataset different in nature from the other two datasets, and it consists of more factual sentences rather than scene descriptions.\nNeural Approaches for NLI. Following the release of SNLI, there has been tremendous interest in the task, and many end-to-end neural models were developed, achieving promising results.2 Methods are divided into two main approaches. Sentence-encoding models (e.g. Bowman et al., 2015, 2016; Nie and Bansal, 2017; Shen et al., 2018) encode the premise and hypothesis individually, while attention-based models align words in the premise with similar words in the hypothesis, encoding the two sentences together (e.g. Rocktäschel et al., 2016; Chen et al., 2017).\n2See the SNLI leaderboard for a comprehensive list: https://nlp.stanford.edu/projects/snli/.\nExternal Lexical Knowledge. Traditional RTE methods typically relied on resources such as WordNet (Fellbaum, 1998) to identify lexical inferences. Conversely, neural methods rely solely on pre-trained word embeddings, yet, they achieve high accuracy on SNLI.\nThe only neural model to date that incorporates external lexical knowledge (from WordNet) is KIM (Chen et al., 2018), however, gaining only a small addition of 0.6 points in accuracy on the SNLI test set. This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the first place."
  }, {
    "heading": "3 Data Collection",
    "text": "We construct a test set with the goal of evaluating the ability of state-of-the-art NLI models to make inferences that require simple lexical knowledge. We automatically generate sentence pairs (§3.1) which are then manually verified (§3.2)."
  }, {
    "heading": "3.1 Generating Adversarial Examples",
    "text": "In order to isolate the lexical knowledge aspects, the premises are taken from the SNLI training set. For each premise we generate several hypotheses by replacing a single word within the premise by a different word. We also allow some multi-word noun phrases (“electric guitar”) and adapt determiners and prepositions when needed.\nWe focus on generating only entailment and contradiction examples, while neutral examples may be generated as a by-product. Entailment examples are generated by replacing a word with its synonym or hypernym, while contradiction examples are created by replacing words with mutually exclusive co-hyponyms and antonyms (see Table 1). The generation steps are detailed below.\nReplacement Words. We collected the replacement words using online resources for English learning.3 The newly introduced words are all present in the SNLI training set: from occurrence in a single training example (“Portugal”) up to 248,051 examples (“man”), with a mean of 3,663.1 and a median of 149.5. The words are also available in the pre-trained embeddings vocabulary. The goal of this constraint is to isolate lexical knowledge aspects, and evaluate the models’ ability to generalize and make new inferences for known words.\n3 www.enchantedlearning.com, www.smart-words.org\nReplacement words are divided into topical categories detailed in Table 4. In several categories we applied additional processing to ensure that examples are indeed mutually-exclusive, topicallysimilar, and interchangeable in context. We included WordNet antonyms with the same part-ofspeech and with a cosine similarity score above a threshold, using GloVe (Pennington et al., 2014). In nationalities and countries we focused on countries which are related geographically (Japan, China) or culturally (Argentina, Spain).\nSentence-Pairs. To avoid introducing new information not present in the training data, we sampled premises from the SNLI training set that contain words from our lists, and generated hypotheses by replacing the selected word with its replacement. Some of the generated sentences may be ungrammatical or nonsensical, for instance, when replacing Jordan with Syria in sentences discussing Michael Jordan. We used Wikipedia bigrams4 to discard sentences in which the replaced word created a bigram with less than 10 occurrences."
  }, {
    "heading": "3.2 Manual Verification",
    "text": "We manually verify the correctness of the automatically constructed examples using crowdsourced workers in Amazon Mechanical Turk. To ensure the quality of workers, we applied a qualification test and required a 99% approval rate for at least 1,000 prior tasks. We assigned each annotation to 3 workers.\nFollowing the SNLI guidelines, we instructed the workers to consider the sentences as describing the same event, but we simplified the annotation process into answering 3 simple yes/no questions:\n1. Do the sentences describe the same event? 4 github.com/rmaestre/Wikipedia-Bigram-Open-Datasets\n2. Does the new sentence (hypothesis) add new information to the original sentence (premise)?\n3. Is the new sentence incorrect/ungrammatical? We then discarded any sentence-pair in which at least one worker answered the third question positively. If the answer to the first question was negative, we considered the label as contradiction. Otherwise, we considered the label as entailment if the answer to the second question was negative and neutral if it was positive. We used the majority vote to determine the gold label.\nThe annotations yielded substantial agreement, with Fleiss’ Kappa κ = 0.61 (Landis and Koch, 1977). We estimate human performance to 94.1%, using the method described in Gong et al. (2018), showing that the new test set is substantially easier to humans than SNLI. Table 2 provides additional statistics on the test set.5"
  }, {
    "heading": "4 Evaluation",
    "text": ""
  }, {
    "heading": "4.1 Models",
    "text": "Without External Knowledge. We chose 3 representative models in different approaches (sentence encoding and/or attention): RESIDUALSTACKED-ENCODER (Nie and Bansal, 2017) is a biLSTM-based single sentence-encoding model without attention. As opposed to traditional multilayer biLSTMs, the input to each next layer is the concatenation of the word embedding and the summation of outputs from previous layers. ESIM (Enhanced Sequential Inference Model, Chen et al., 2017) is a hybrid TreeLSTM-based and biLSTM-based model. We use the biLSTM model, which uses an inter-sentence attention mechanism to align words across sentences. Finally, DECOMPOSABLE ATTENTION (Parikh et al., 2016) performs soft alignment of words from the premise to words in the hypothesis using attention mechanism, and decomposes the task into comparison of aligned words. Lexical-level decisions are merged to produce the final classification. We use the AllenNLP re-implementation,6 which does not implement the optional intrasentence attention, and achieves an accuracy of 84.7% on the SNLI test set, comparable to 86.3% by the original system.\n5We note that due to its bias towards contradiction, the new test set can neither be used for training, nor serve as a main evaluation set for NLI. Instead, we suggest to use it in addition to the original test set in order to test a model’s ability to handle lexical inferences.\n6http://allennlp.org/models\nWe chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (RESIDUAL-STACKED-ENCODER and ESIM) or stay fixed (DECOMPOSABLE ATTENTION). All models predict the label using a concatenation of features derived from the sentence representations (e.g. maximum, mean), for example as in Mou et al. (2016). We use the recommended hyper-parameters for each model, as they appear in the provided code.\nWith External Knowledge. We provide a simple WORDNET BASELINE, in which we classify a sentence-pair according to the WordNet relation that holds between the original word wp and the replaced word wh. We predict entailment if wp is a hyponym of wh or if they are synonyms, neutral if wp is a hypernym of wh, and contradiction if wp and wh are antonyms or if they share a common hypernym ancestor (up to 2 edges). Word pairs with no WordNet relations are classified as other.\nWe also report the performance of KIM (Knowledge-based Inference Model, Chen et al., 2018), an extension of ESIM with external knowledge from WordNet, which was kindly provided to us by Qian Chen. KIM improves the attention mechanism by taking into account the existence of WordNet relations between the words. The lexical inference component, operating over pairs of aligned words, is enriched with a vector encoding the specific WordNet relations between the words."
  }, {
    "heading": "4.2 Experimental Settings",
    "text": "We trained each model on 3 different datasets: (1) SNLI train set, (2) a union of the SNLI train set\nand the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set. The motivation is that while SNLI might lack the training data needed to learn the required lexical knowledge, it may be available in the other datasets, which are presumably richer."
  }, {
    "heading": "4.3 Results",
    "text": "Table 3 displays the results for all the models on the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy. Adding MultiNLI to the training data somewhat mitigates this drop in accuracy, thanks to almost doubling the amount of training data. We note that adding SciTail to the training data did not similarly improve the performance; we conjecture that this stems from the differences between the datasets.\nKIM substantially outperforms the other neural models, demonstrating that lexical knowledge is the only requirement for good performance on the new test set, and stressing the inability of the other models to learn it. Both WordNet-informed models leave room for improvement: possibly due to limited WordNet coverage and the implications of applying lexical inferences within context."
  }, {
    "heading": "5 Analysis",
    "text": "We take a deeper look into the predictions of the models that don’t employ external knowledge, focusing on the models trained on SNLI."
  }, {
    "heading": "5.1 Accuracy by Category",
    "text": "Table 4 displays the accuracy of each model per replacement-word category. The neural models tend to perform well on categories which are frequent in the training set, such as colors, and badly\non categories such as planets, which rarely occur in SNLI. These models perform better than the WordNet baseline on entailment examples (synonyms), suggesting that they do so due to high lexical overlap between the premise and the hypothesis rather than recognizing synonymy. We therefore focus the rest of the discussion on contradiction examples."
  }, {
    "heading": "5.2 Accuracy by Word Similarity",
    "text": "The accuracies for ordinals, nationalities and countries are especially low. We conjecture that this stems from the proximity of the contradicting words in the embedding space. Indeed, the Decomposable Attention model—which does not update its embeddings during training—seems to suffer the most.\nGrouping its prediction accuracy by the cosine similarity between the contradicting words reveals a clear trend that the model errs more on contradicting pairs with similar pre-trained vectors:7\nSimilarity 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Accuracy 46.2% 42.3% 37.5% 29.7% 20.2%"
  }, {
    "heading": "5.3 Accuracy by Frequency in Training",
    "text": "Models that fine-tune the word embeddings may benefit from training examples consisting of test replacement pairs. Namely, for a given replacement pair (wp, wh), if many training examples labeled as contradiction contain wp in the premise and wh in the hypothesis, the model may update their embeddings to optimize predicting contradiction. Indeed, we show that the ESIM accuracy on test pairs increases with the frequency in which\n7We ignore multi-word replacements in §5.2 and §5.3.\ntheir replacement words appear in contradiction examples in the training data: Frequency 0 1-4 5-9 10-49 50-99 100+ Accuracy 40.2% 70.6% 91.4% 92.1% 97.5% 98.5%\nThis demonstrates that the model is capable of learning lexical knowledge when sufficient training data is given, but relying on explicit training examples is a very inefficient way of obtaining simple lexical knowledge."
  }, {
    "heading": "6 Conclusion",
    "text": "We created a new NLI test set with the goal of evaluating systems’ ability to make inferences that require simple lexical knowledge. Although the test set is constructed to be much simpler than SNLI, and does not introduce new vocabulary, the state-of-the-art systems perform poorly on it, suggesting that they are limited in their generalization ability. The test set can be used in the future to assess the lexical inference abilities of NLI systems and to tease apart the performance of otherwise very similarly-performing systems."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Qian Chen for evaluating KIM on our test set. This work was supported in part by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), an Intel ICRI-CI grant, Theo Hoffenberg, and the Israel Science Foundation grants 1951/17 and 1555/15. Vered is also supported by the Clore Scholars Programme (2017), and the AI2 Key Scientific Challenges Program (2017)."
  }],
  "year": 2018,
  "references": [{
    "title": "A large annotated corpus for learning natural language inference",
    "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "D. Christopher Manning."],
    "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-",
    "year": 2015
  }, {
    "title": "A fast unified model for parsing and sentence understanding",
    "authors": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association",
    "year": 2016
  }, {
    "title": "Neural natural language inference models enhanced with external knowledge",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Diana Inkpen", "Si Wei."],
    "venue": "The 56th Annual Meeting of the Association for Computational Linguistics (ACL). Mel-",
    "year": 2018
  }, {
    "title": "Enhanced lstm for natural language inference",
    "authors": ["Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). As-",
    "year": 2017
  }, {
    "title": "Recognizing textual entailment: Models and applications",
    "authors": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."],
    "venue": "Synthesis Lectures on Human Language Technologies 6(4):1–220.",
    "year": 2013
  }, {
    "title": "WordNet",
    "authors": ["Christiane Fellbaum."],
    "venue": "Wiley Online Library.",
    "year": 1998
  }, {
    "title": "Natural language inference over interaction space",
    "authors": ["Yichen Gong", "Heng Luo", "Jian Zhang."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2018
  }, {
    "title": "Annotation artifacts in natural language inference data",
    "authors": ["Suchin Gururangan", "Swabha Swayamdipta", "Omer Levy", "Roy Schwartz", "Samuel R Bowman", "Noah A Smith."],
    "venue": "The 16th Annual Conference of the North American Chapter of the Asso-",
    "year": 2018
  }, {
    "title": "Adversarial examples for evaluating reading comprehension systems",
    "authors": ["Robin Jia", "Percy Liang."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
    "year": 2017
  }, {
    "title": "SciTail: A textual entailment dataset from science question answering",
    "authors": ["Tushar Khot", "Ashish Sabharwal", "Peter Clark."],
    "venue": "The Thirty-Second AAAI",
    "year": 2018
  }, {
    "title": "The measurement of observer agreement for categorical data",
    "authors": ["J Richard Landis", "Gary G Koch."],
    "venue": "biometrics pages 159–174.",
    "year": 1977
  }, {
    "title": "Natural language inference by tree-based convolution and heuristic matching",
    "authors": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
    "year": 2016
  }, {
    "title": "Shortcutstacked sentence encoders for multi-domain inference",
    "authors": ["Yixin Nie", "Mohit Bansal."],
    "venue": "arXiv preprint arXiv:1708.02312 .",
    "year": 2017
  }, {
    "title": "A decomposable attention model for natural language inference",
    "authors": ["Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
    "year": 2014
  }, {
    "title": "Hypothesis Only Baselines in Natural Language Inference",
    "authors": ["Adam Poliak", "Jason Naradowsky", "Aparajita Haldar", "Rachel Rudinger", "Benjamin Van Durme."],
    "venue": "Joint Conference on Lexical and Computational Semantics (StarSem).",
    "year": 2018
  }, {
    "title": "Reasoning about entailment with neural attention",
    "authors": ["Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Phil Blunsom."],
    "venue": "International Conference on Learning Representations (ICLR).",
    "year": 2016
  }, {
    "title": "Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling",
    "authors": ["Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Sen Wang", "Chengqi Zhang."],
    "venue": "arXiv preprint arXiv:1801.10296 .",
    "year": 2018
  }, {
    "title": "A broad-coverage challenge corpus for sentence understanding through inference",
    "authors": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."],
    "venue": "The 16th Annual Conference of the North American Chapter of the Association for Computational Lin-",
    "year": 2018
  }, {
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "authors": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."],
    "venue": "Transactions of the Association for Computational Linguis-",
    "year": 2014
  }],
  "id": "SP:03fb23857a91b379801f1fa225274f7a415ecf87",
  "authors": [{
    "name": "Max Glockner",
    "affiliations": []
  }, {
    "name": "Vered Shwartz",
    "affiliations": []
  }, {
    "name": "Yoav Goldberg",
    "affiliations": []
  }],
  "abstractText": "We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.",
  "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences"
}