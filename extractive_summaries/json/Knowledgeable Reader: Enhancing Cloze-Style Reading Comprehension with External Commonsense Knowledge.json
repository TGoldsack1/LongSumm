{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 821–832 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n821"
  }, {
    "heading": "1 Introduction",
    "text": "Reading comprehension (RC) is a language understanding task similar to question answering, where a system is expected to read a given passage of text and answer questions about it. Cloze-style reading comprehension is a task setting where the question is formed by replacing a token in a sentence of the story with a placeholder (left part of Figure 1).\nIn contrast to many previous complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) that perform multi-turn reading of a story and a question before inferring the correct answer, we aim to tackle the cloze-style RC task in a way that resembles how humans solve it: using, in addition, background knowledge. We develop\na neural model for RC that can successfully deal with tasks where most of the information to infer answers from is given in the document (story), but where additional information is needed to predict the answer, which can be retrieved from a knowledge base and added to the context representations explicitly.1 An illustration is given in Figure 1.\nSuch knowledge may be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2017), BabelNet (Navigli and Ponzetto, 2012), Freebase (Tanon et al., 2016) or domain-specific KBs collected with Information Extraction approaches (Fader et al., 2011; Mausam et al., 2012; Bhutani et al., 2016). Thus, we aim to define a neural model that encodes preselected knowledge in a memory, and that learns to include the available knowledge as an enrichment to the context representation.\nThe main difference of our model to prior state-of-the-art is that instead of relying only on document-to-question interaction or discrete features while performing multiple hops over the document, our model (i) attends to relevant selected\n1‘Context representation’ refers to a vector representation computed from textual information only (i.e., document (story) or question).\nexternal knowledge and (ii) combines this knowledge with the context representation before inferring the answer, in a single hop. This allows the model to explicitly imply knowledge that is not stated in the text, but is relevant for inferring the answer, and that can be found in an external knowledge source. Moreover, by including knowledge explicitly, our model provides evidence and insight about the used knowledge in the RC.\nOur main contributions are: (i) We develop a method for integrating knowledge in a simple but effective reading comprehension model (AS Reader, Kadlec et al. (2016)) and improve its results significantly whereas other models employ features or multiple hops. (ii) We examine two sources of common knowledge: WordNet (Miller et al., 1990) and ConceptNet (Speer et al., 2017) and show that this type of knowledge is important for answering common nouns questions and also improves slightly the performance for named entities.\n(iii) We show that knowledge facts can be added directly to the text-only representation, enriching the neural context encoding. (iv) We demonstrate the effectiveness of the injected knowledge by case studies and data statistics in a qualitative evaluation study."
  }, {
    "heading": "2 Reading Comprehension with Background Knowledge Sources",
    "text": "In this work, we examine the impact of using external knowledge as supporting information for the task of cloze style reading comprehension.\nWe build a system with two modules. The first, Knowledge Retrieval, performs fact retrieval and selects a number of facts f1, ..., fp that might be relevant for connecting story, question and candidate answers. The second, main module, the Knowledgeable Reader, is a knowledge-enhanced neural module. It uses the input of the story context tokens d1..m, the question tokens q1..n, the set of answer candidates a1..k and a set of ‘relevant’ background knowledge facts f1..p in order to select the right answer. To include external knowledge for the RC task, we encode each fact f1..p and use attention to select the most relevant among them for each token in the story and question. We expect that enriching the text with additional knowledge about the mentioned concepts will improve the prediction of correct answers in a strong single-pass system. See Figure 1 for illustration."
  }, {
    "heading": "2.1 Knowledge Retrieval",
    "text": "In our experiments we use knowledge from the Open Mind Common Sense (OMCS, Singh et al. (2002)) part of ConceptNet, a crowd-sourced resource of commonsense knowledge with a total of ∼630k facts. Each fact fi is represented as a triple fi=(subject, relation, object), where subject and object can be multi-word expressions and relation is a relation type. An example is: ([bow]subj , [IsUsedFor]rel, [hunt, animals]obj)\nWe experiment with three set-ups: using (i) all facts from OMCS that pertain to ConceptNet, referred to as CN5All, (ii) using all facts from CN5All excluding some WordNet relations referred to as CN5Sel(ected) (see Section 3), and using (iii) facts from OMCS that have source set to WordNet (CN5WN3).\nRetrieving relevant knowledge. For each instance (D, Q, A1..10) we retrieve relevant commonsense background facts. We first retrieve facts that contain lemmas that can be looked up via tokens contained in any D(ocument), Q(uestion) or A(nswer candidates). We add a weight value for each node: 4, if it contains a lemma of a candidate token from A; 3, if it contains a lemma from the tokens of Q; and 2 if it contains a lemma from the tokens of D. The selected weights are chosen heuristically such that they model relative fact importance in different interactions as A+A > A+Q > A+D>D+Q>D+D. We weight the fact triples that contain these lemmas as nodes, by summing the weights of the subject and object arguments. Next, we sort the knowledge triples by this overall weight value. To limit the memory of our model, we run experiments with different sizes of the top number of facts (P ) selected from all instance fact candidates, P ∈ {50, 100, 200}. As additional retrieval limitation, we force the number of facts per answer candidate to be the same, in order to avoid a frequency bias for an answer candidate that appears more often in the knowledge source. Thus, if we select the maximum 100 facts for each task instance and we have 10 answer candidates ai=1..10, we retrieve the top 10 facts for each candidate ai that has either a subject or an object lemma for a token in ai. If the same fact contains lemmas of two candidates ai and aj (j > i), we add the fact once for ai and do not add the same fact again for aj . If several facts have the same weight, we take\nthe first in the order of the list2, i.e., the order of retrieval from the database. If one candidate has less than 10 facts, the overall fact candidates for the sample will be less than the maximum (100)."
  }, {
    "heading": "2.2 Neural Model: Extending the Attention Sum Reader with a Knowledge Memory",
    "text": "We implement our Knowledgeable Reader (KnReader) using as a basis the Attention Sum Reader as one of the strongest core models for single-hop RC. We extend it with a knowledge fact memory that is filled with pre-selected facts. Our aim is to examine how adding commonsense knowledge to a simple yet effective model can improve the RC process and to show some evidence of that by attending on the incorporated knowledge facts. The model architecture is shown in Figure 2.\nBase Attention Model. The Attention-Sum Reader (Kadlec et al., 2016), our base model for RC reads the input of story tokens d1..n, the question tokens q1..m, and the set of candidates a1..10 that occur in the story text. The model calculates the attention between the question representation rq and the story token context encodings of the candidate tokens a1..10 and sums the attention scores for the candidates that appear multiple times in the story. The model selects as answer the candidate that has the highest attention score.\nWord Embeddings Layer. We represent input document and question tokens w by looking up their embedding representations ei = Emb(wi), where Emb is an embedding lookup function. We apply dropout (Srivastava et al., 2014) with keep\n2We also experimented with re-ranking the facts with the same weight sums using tf-idf but we did not notice a difference in performance.\nprobability p = 0.8 to the output of the embeddings lookup layer.\nContext Representations. To represent the document and question contexts, we first encode the tokens with a Bi-directional GRU (Gated Recurrent Unit) (Chung et al., 2014) to obtain context-encoded representations for document (cctxd1..n) and question (cctxq1..m) encoding:\ncctxd1..n = BiGRU ctx(ed1..n) ∈ Rn×2h (1) cctxq1..m = BiGRU ctx(eq1..m) ∈ Rm×2h (2)\n, where di and qi denote the ith token of a text sequence d (document) and q (question), respectively, n and m is the size of d and q and h the output hidden size (256) of a single GRU unit. BiGRU is defined in (3), with ei a word embedding vector\nBiGRU ctx(ei, hiprev) = [ −−−→ GRU(ei, −−−→ hiprev),\n←−−− GRU(ei, ←−−− hiprev)]\n(3)\n, where hiprev = [ −−−→ hiprev , ←−−− hiprev ], and −−−→ hiprev and ←−−− hiprev are the previous hidden states of the forward and backward layers. Below we use BiGRU ctx(ei) without the hidden state, for short.\nQuestion Query Representation. For the question we construct a single vector representation rctxq by retrieving the token representation at the placeholder (XXXX) index pl (cf. Figure 2):\nrctxq = c ctx qi..m [pl] ∈ R 2h (4)\nwhere [pl] is an element pickup operation. Our question vector representation is different from the original AS Reader that builds the question by concatenating the last states of a forward and backward layer [ −−−→ GRU(em), ←−−− GRU(e1)]. We changed the original representation as we observed some very long questions and in this way aim to prevent the context encoder from ’forgetting’ where the placeholder is.\nAnswer Prediction: Qctx to Dctx Attention. In order to predict the correct answer to the given question, we rank the given answer candidates a1..aL according to the normalized attention sum score between the context (ctx) representation of the question placeholder rctxq and the representation of the candidate tokens in the document:\nP (ai|q, d) = softmax( ∑ αij ) (5)\nαij = Att(r ctx q , c ctx dj ), i ∈ [1..L] (6)\n, where j is an index pointer from the list of indices that point to the candidate ai token occurrences in the document context representation cd. Att is a dot product.\nEnriching Context Representations with Knowledge (Context+Knowledge). To enhance the representation of the context, we add knowledge, retrieved as a set of knowledge facts.\nKnowledge Encoding. For each instance in the dataset, we retrieve a number of relevant facts (cf. Section 2.1). Each retrieved fact is represented as a triple f = (wsubj1..Lsubj , w rel 0 , w obj 1..Lobj ), where wsubj1..Lsubj and w obj 1..Lobj\nare a multi-word expressions representing the subject and object with sequence lengths Lsubj and Lobj , and wrel0 is a word token corresponding to a relation.3 As a result of fact encoding, we obtain a separate knowledge memory for each instance in the data.\nTo encode the knowledge we use a BiGRU to encode the triple argument tokens into the following context-encoded representations:\nfsubjlast = BiGRU(Emb(w subj 1..Lsubj ), 0) (7)\nf rellast = BiGRU(Emb(w rel 0 ), f subj last ) (8)\nfobjlast = BiGRU(Emb(w obj 1..Lsubj ), f rellast) (9)\n, where fsubjlast , f rel last, f obj last are the final hidden states of the context encoder BiGRU , that are also used as initial representations for the encoding of the next triple attribute in left-to-right order. See Supplement for comprehensive visualizations. The motivation behind this encoding is: (i) We encode the knowledge fact attributes in the same vector space as the plain tokens; (ii) we preserve the triple directionality; (iii) we use the relation type as a way of filtering the subject information to initialize the object.\nQuerying the Knowledge Memory. To enrich the context representation of the document and question tokens with the facts collected in the knowledge memory, we select a single sum of weighted fact representations for each token using Key-Value retrieval (Miller et al., 2016). In our model the key Mk(ey)i can be either f subj last or f obj last and the value Mv(alue)i is f obj last.\nFor each context-encoded token cctxsi (s = d, q; i the token index) we attend over all knowledge\n3The 0 in wrel0 indicates that we encode the relation as a single relation type word. Ex. /r/IsUsedFor.\nmemory keys Mki in the retrieved P knowledge facts. We use an attention function Att, scale the scalar attention value using softmax, multiply it with the value representation Mvi and sum the result into a single vector value representation cknsi :\ncknsi = ∑ softmax(Att(cctx,Mk1..P )) TMv1..P\n(10) Att is a dot product, but it can be replaced with another attention function. As a result of this operation, the context token representation cctxsi and the corresponding retrieved knowledge cknsi are in the same vector space ∈ R2h.\nCombine Context and Knowledge (ctx+kn). We combine the original context token representation cctxsi , with the acquired knowledge representation cknsi to obtain c ctx+kn si :\ncctx+knsi = γc ctx si + (1− γ)c kn si (11)\n, where γ = 0.5. We keep γ static but it can be replaced with a gating function.\nAnswer Prediction: Qctx(+kn) to Dctx(+kn). To rank answer candidates a1..aL we use attention sum similar to Eq.5 over an attention αensembleij that combines attentions between context (ctx) and context+knowledge (ctx+kn) representations of the question (rctx(+kn)q ) and candidate token occurrences aij in the document c ctx(+kn) dj :\nP (ai|q, d) = softmax( ∑ αensembleij ) (12)\nαensembleij =\nW1Att(r ctx q , c ctx dj )\n+W2Att(r ctx q , c ctx+kn dj ) +W3Att(r ctx+kn q , c ctx dj )\n+W4Att(r ctx+kn q , c ctx+kn dj )\n(13)\n, where j is an index pointer from the list of indices that point to the candidate ai token occurrences in the document context representation c ctx(+kn) d . W1..4 are scalar weights initialized with 1.0 and optimized during training.4 We propose the combination of ctx and ctx + kn attentions because our task does not provide supervision whether the knowledge is needed or not.\n4An example for learned W1..4 is (2.13, 1.41, 1.49, 1.84) in setting (CBT CN, CN5Sel, Subj-Obj as k-v, 50 facts)."
  }, {
    "heading": "3 Data and Task Description",
    "text": "We experiment with knowledge-enhanced clozestyle reading comprehension using the Common Nouns and Named Entities partitions of the Children’s Book Test (CBT) dataset (Hill et al., 2015).\nIn the CBT cloze-style task a system is asked to read a children story context of 20 sentences. The following 21st sentence involves a placeholder token that the system needs to predict, by choosing from a given set of 10 candidate words from the document. An example with suggested external knowledge facts is given in Figure 1. While in its Common Nouns setup, the task can be considered as a language modeling task, Hill et al. (2015) show that humans can answer the questions without the full context with an accuracy of only 64.4% and a language model alone with 57.7%. By contrast, the human performance when given the full context is at 81.6%. Since the best neural model (Munkhdalai and Yu, 2016) achieves only 72.0% on the task, we hypothesize that the task itself can benefit from external knowledge. The characteristics of the data are shown in Table 1.\nOther popular cloze-style datasets such as CNN/Daily Mail (Hermann et al., 2015) or WhoDidWhat (Onishi et al., 2016) are mainly focused on finding Named Entities where the benefit of adding commonsense knowledge (as we show for the NE part of CBT) would be more limited.\nKnowledge Source. As a source of commonsense knowledge we use the Open Mind Common Sense part of ConceptNet 5.0 that contains 630k fact triples. We refer to this entire source as CN5All. We conduct experiments with subparts of this data: CN5WN3 which is the WordNet 3 part of CN5All (213k triples) and CN5Sel, which excludes the following WordNet relations: RelatedTo, IsA, Synonym, SimilarTo, HasContext."
  }, {
    "heading": "4 Related Work",
    "text": "Cloze-Style Reading Comprehension. Following the original MCTest (Richardson et al., 2013) dataset multiple-choice version of cloze-style RC) recently several large-scale, automatically generated datasets for cloze-style reading comprehension gained a lot of attention, among others the ‘CNN/Daily Mail’ (Hermann et al., 2015; Onishi et al., 2016) and the Children’s Book Test (CBTest) data set (Hill et al., 2015). Early work introduced simple but good single turn models (Hermann et al., 2015; Kadlec et al., 2016; Chen et al., 2016), that read the document once with the question representation ‘in mind’ and select an answer from a given set of candidates. More complex models (Weston et al., 2015; Dhingra et al., 2017; Cui et al., 2017; Munkhdalai and Yu, 2016; Sordoni et al., 2016) perform multi-turn reading of the story context and the question, before inferring the correct answer or use features (GA Reader, Dhingra et al. (2017). Performing multiple hops and modeling a deeper relation between question and document was further developed by several models (Seo et al., 2017; Xiong et al., 2016; Wang et al., 2016, 2017; Shen et al., 2016) on another generation of RC datasets, e.g. SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) or TriviaQA (Joshi et al., 2017).\nIntegrating Background Knowledge in Neural Models. Integrating background knowledge in a neural model was proposed in the neural-checklist model by Kiddon et al. (2016) for text generation of recipes. They copy words from a list of ingredients instead of inferring the word from a global vocabulary. Ahn et al. (2016) proposed a language model that copies fact attributes from a topic knowledge memory. The model predicts a fact in the knowledge memory using a gating mechanism and given this fact, the next word to be selected is copied from the fact attributes. The knowledge facts are encoded using embeddings obtained using TransE (Bordes et al., 2013). Yang et al. (2017) extended a seq2seq model with attention to external facts for dialogue and recipe generation and a co-reference resolution-aware language model. A similar model was adopted by He et al. (2017) for answer generation in dialogue. Incorporating external knowledge in a neural model has proven beneficial for several other tasks: Yang and Mitchell (2017) incorporated knowledge di-\nrectly into the LSTM cell state to improve event and entity extraction. They used knowledge embeddings trained on WordNet (Miller et al., 1990) and NELL (Mitchell et al., 2015) using the BILINEAR (Yang et al., 2014) model.\nWork similar to ours is by Long et al. (2017), who have introduced a new task of Rare Entity Prediction. The task is to read a paragraph from WikiLinks (Singh et al., 2012) and to fill a blank field in place of a missing entity. Each missing entity is characterized with a short description derived from Freebase, and the system needs to choose one from a set of pre-selected candidates to fill the field. While the task is superficially similar to cloze-style reading comprehension, it differs considerably: first, when considering the text without the externally provided entity information, it is clearly ambiguous. In fact, the task is more similar to Entity Linking tasks in the Knowledge Base Population (KBP) tracks at TAC 2013-2017, which aim at detecting specific entities from Freebase. Our work, by contrast, examines the impact of injecting external knowledge in a reading comprehension, or NLU task, where the knowledge is drawn from a commonsense knowledge base, ConceptNet in our case. Another difference is that in their setup, the reference knowledge for the candidates is explicitly provided as a single, fixed set of knowledge facts (the entity description), encoded in a single representation. In our work, we are retrieving (typically) distinct sets of knowledge facts that might (or might not) be relevant for understanding the story and answering the question. Thus, in our setup, we crucially depend on the ability of the attention mechanism to retrieve relevant pieces of knowledge. Our aim is to examine to what extent commonsense knowledge can contribute to and improve the cloze-style RC task, that in principle is supposed to be solvable without explicitly given additional knowledge. We show that by integrating external commonsense knowledge we achieve clear improvements in reading comprehension performance over a strong baseline, and thus we can speculate that humans, when solving this RC task, are similarly using commonsense knowledge as implicitly understood background knowledge.\nRecent unpublished work in Weissenborn et al. (2017) is driven by similar intentions. The authors exploit knowledge from ConceptNet to improve the performance of a reading comprehen-\nsion model, experimenting on the recent SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets. While the source of the background knowledge is the same, the way of integrating this knowledge into the model and task is different. (i) We are using attention to select unordered fact triples using key-value retrieval and (ii) we integrate the knowledge that is considered relevant explicitly for each token in the context. The model of Weissenborn et al. (2017), by contrast, explicitly reads the acquired additional knowledge sequentially after reading the document and question, but transfers the background knowledge implicitly, by refining the word embeddings of the words in the document and the question with the words from the supporting knowledge that share the same lemma. In contrast to the implicit knowledge transfer of Weissenborn et al. (2017), our explicit attention over external knowledge facts can deliver insights about the used knowledge and how it interacts with specific context tokens (see Section 6)."
  }, {
    "heading": "5 Experiments and Results",
    "text": "We perform quantitative analysis through experiments. We study the impact of the used knowledge and different model components that employ the external knowledge. Some of the experiments below focus only on the Common Nouns (CN) dataset, as it has been shown to be more challenging than Named Entities (NE) in prior work."
  }, {
    "heading": "5.1 Model Parameters",
    "text": "We experiment with different model parameters.\nNumber of facts. We explore different sizes of knowledge memories, in terms of number of acquired facts. If not stated otherwise, we use 50 facts per example.\nKey-Value Selection Strategy. We use two strategies for defining key and value (Key/Value): Subj/Obj and Obj/Obj, where Subj and Obj are the subject and object attributes in the fact triples and they are selected as Key and Value for the KV memory (see Section 2.2, Querying the Knowledge Memory). If not stated otherwise, we use the Subj/Obj strategy.\nAnswer Selection Components. If not stated otherwise, we use ensemble attention αensemble (combinations of ctx and ctx+kn) to rank the answers. We call this our Full model (see Sec. 2.2).\nHyper-parameters. For our experiments we use pre-trained Glove (Pennington et al., 2014) embeddings, BiGRU with hidden size 256, batch size of 64 and learning reate of 0.001 as they were shown (Kadlec et al., 2016) to perform good on the AS Reader."
  }, {
    "heading": "5.2 Empirical Results",
    "text": "We perform experiments with the different model parameters described above. We report accuracy on the Dev and Test and use the results on Dev set for pruning the experiments.\nKnowledge Sources. We experiment with different configuration of ConceptNet facts (see Section 3). Results on the CBT CN dataset are shown in Table 2. CN5Sel works best on the Dev set but CN5WN3 works much better on Test. Further experiments use the CN5Sel setup.\nNumber of facts. We further experiment with different numbers of facts on the Common Nouns dataset (Table 3). The best result on the Dev set is for 50 facts so we use it for further experiments.\nComponent ablations. We ensemble the attentions from different combinations of the interaction between the question and document context (ctx) representations and context+knowledge (ctx+kn) representations in order to infer the right answer (see Section 2.2, Answer Ranking).\nTable 4 shows that the combination of different interactions between ctx and ctx+kn representations leads to clear improvement over the w/o knowledge setup, in particular for the Common Nouns dataset. We also performed ablations for a model with 100 facts (see Supplement).\nKey-Value Selection Strategy. Table 5 shows that for the NE dataset, the two strategies perform\nequally well on the Dev set, whereas the Subj/Obj strategy works slightly better on the Test set. For Common Nouns, Subj/Obj is better.\nComparison to Previous Work. Table 6 compares our model (Knowledgeable Reader) to previous work on the CBT datasets. We show the results of our model with the settings that performed best on the Dev sets of the two datasets NE and CN: for NE, (Dctx+kn, Qctx) with 100 facts; for CN the Full model with 50 facts, both with CN5Sel.\nNote that our work focuses on the impact of external knowledge and employs a single inter-\naction (single-hop) between the document context and the question so we primarily compare to and aim at improving over similar models. KnReader clearly outperforms prior single-hop models on both datasets. While we do not improve over the state of the art, our model stands well among other models that perform multiple hops. In the Supplement we also give comparison to ensemble models and some models that use re-ranking strategies."
  }, {
    "heading": "6 Discussion and Analysis",
    "text": ""
  }, {
    "heading": "6.1 Analysis of the empirical results.",
    "text": "Our experiments examined key parameters of the KnReader. As expected, injection of background knowledge yields only small improvements over the baseline model for Named Entities. However, on this dataset our single-hop model is competitive to most multi-hop neural architectures.\nThe integration of knowledge clearly helps for the Common Nouns task. The impact of knowledge sources (Table 2) is different on the Dev and Test sets which indicates that either the model or the data subsets are sensitive to different knowledge types and retrieved knowledge. Table 5 shows that attending over the Subj of the knowledge triple is slightly better than Obj. This shows that using a Key-Value memory is valuable. A reason for lower performance of Obj/Obj is that the model picks facts that are similar to the candidate tokens, not adding much new information. From the empirical results we see that training and evaluation with less facts is slightly better. We hypothesize that this is related to the lack of supervision on the retrieved and attended knowledge."
  }, {
    "heading": "6.2 Interpreting Component Importance",
    "text": "Figure 3 shows the impact on prediction accuracy of individual components of the Full model, including the interaction between D and Q with ctx or ctx + kn (w/o ctx-only). The values for each component are obtained from the attention weights, without retraining the model. The difference between blue (left) and orange (right) values indicates how much the module contributes to the model. Interestingly, the ranking of the contribution (Dctx, Qctx+kn > Dctx+kn, Qctx > Dctx+kn, Qctx+kn) corresponds to the component importance ablation on the Dev set, lines 5-8, Table 4.\nSubj/Obj,  50  facts\nObj/Obj,  50  facts"
  }, {
    "heading": "6.3 Qualitative Data Investigation",
    "text": "We will use the attention values of the interactions between Dctx(+kn) and Qctx(+kn) and attentions to facts from each candidate token and the question placeholder to interpret how knowledge is employed to make a prediction for a single example.\nMethod: Interpreting Model Components. We manually inspect examples from the evaluation sets where KnReader improves prediction (blue (left) category, Fig. 3) or makes the prediction worse (orange (right) category, Fig. 3). Figure 4 shows the question with placeholder, followed by answer candidates and their associated attention weights as assigned by the model w/o knowledge. The matrix shows selected facts and their assigned weights for the question and the candidate tokens. Finally, we show the attention weights determined by the knowledge-enhanced D to Q interactions. The attention to the correct answer (head) is low when the model considers the text alone (w/o knowledge). When adding retrieved knowledge to theQ only (row ctx, ctx+kn) and to both Q and D (row ctx + kn, ctx + kn) the score improves, while when adding knowledge to D alone (row ctx+ kn, ctx) the score remains ambiguous. The combined score Ensemble (see Eq. 13) then takes the final decision for the answer. In this example, the question can be answered without the story. The model tries to find knowledge that is related to eyes. The fact eyes /r/PartOf head is not contained in the retrieved knowledge but in-\nstead the model selects the fact ear /r/PartOf head which receives the highest attention from Q. The weighted Obj representation (head) is added to the question with the highest weight, together with animal and bird from the next highly weighted facts This results in a high score for theQctx toDctx+kn interaction with candidate head. See Supplement for more details.\nUsing the method described above, we analyze several example cases (presented in Supplement) that highlight different aspects of our model. Here we summarize our observations.\n(i.) Answer prediction from Q or Q+D. In both human and machine RC, questions can be answered based on the question alone (Figure 4) or jointly with the story context (Case 2, Suppl.). We show that empirically, enriching the question with knowledge is crucial for the first type, while enrichment of Q and D is required for the second.\n(ii.) Overcoming frequency bias.. We show\nthat when appropriate knowledge is available and selected, the model is able to correct a frequency bias towards an incorrect answer (Cases 1 and 3).\n(iii.) Providing appropriate knowledge. We observe a lack of knowledge regarding events (e.g. take off vs. put on clothes, Case 2; climb up, Case 5). Nevertheless relevant knowledge from CN5 can help predicting infrequent candidates (Case 2).\n(iv.) Knowledge, Q and D encoding. The context encoding of facts allows the model to detect knowledge that is semantically related, but not surface near to phrases in Q and D (Case 2). The model finds facts to non-trivial paraphrases (e.g. undressed–naked, Case 2)."
  }, {
    "heading": "7 Conclusion and Future Work",
    "text": "We propose a neural cloze-style reading comprehension model that incorporates external commonsense knowledge, building on a single-turn neural model. Incorporating external knowledge improves its results with a relative error rate reduction of 9% on Common Nouns, thus the model is able to compete with more complex RC models. We show that the types of knowledge contained in ConceptNet are useful. We provide quantitative and qualitative evidence of the effectiveness of our model, that learns how to select relevant knowledge to improve RC. The attractiveness of our model lies in its transparency and flexibility: due to the attention mechanism, we can trace and analyze the facts considered in answering specific questions. This opens up for deeper investigation and future improvement of RC models in a targeted way, allowing us to investigate what knowledge sources are required for different data sets and domains. Since our model directly integrates background knowledge with the document and questioncontext representations, it can be adapted to very different task settings where we have a pair of two arguments (i.e. entailment, question answering, etc.) In future work, we will investigate even tighter integration of the attended knowledge and stronger reasoning methods."
  }, {
    "heading": "Acknowledgments",
    "text": "This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1. We thank the reviewers for their helpful questions and comments."
  }],
  "year": 2018,
  "references": [{
    "title": "A neural knowledge language model",
    "authors": ["Sungjin Ahn", "Heeyoul Choi", "Tanel Pärnamaa", "Yoshua Bengio."],
    "venue": "CoRR, volume abs/1608.00318.",
    "year": 2016
  }, {
    "title": "Nested propositions in open information extraction",
    "authors": ["Nikita Bhutani", "H V Jagadish", "Dragomir Radev."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 55–64, Austin, Texas. Association for",
    "year": 2016
  }, {
    "title": "Translating embeddings for modeling multirelational data",
    "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."],
    "venue": "Advances in Neural Information Processing Systems 26, pages 2787–2795. Curran",
    "year": 2013
  }, {
    "title": "A thorough examination of the cnn/daily mail reading comprehension task",
    "authors": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
    "year": 2016
  }, {
    "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    "authors": ["Junyoung Chung", "Çalar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "arXiv e-prints, abs/1412.3555. Presented at the Deep Learning workshop at NIPS2014.",
    "year": 2014
  }, {
    "title": "Attention-overattention neural networks for reading comprehension",
    "authors": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2017
  }, {
    "title": "Gatedattention readers for text comprehension",
    "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
    "year": 2017
  }, {
    "title": "Identifying relations for open information extraction",
    "authors": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni."],
    "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, Edinburgh, Scotland, UK.",
    "year": 2011
  }, {
    "title": "Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning",
    "authors": ["Shizhu He", "Cao Liu", "Kang Liu", "Jun Zhao."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational",
    "year": 2017
  }, {
    "title": "Teaching machines to read and comprehend",
    "authors": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."],
    "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,",
    "year": 2015
  }, {
    "title": "The goldilocks principle: Reading children’s books with explicit memory representations",
    "authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."],
    "venue": "volume abs/1511.02301.",
    "year": 2015
  }, {
    "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
    "authors": ["Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2017
  }, {
    "title": "Text understanding with the attention sum reader network",
    "authors": ["Rudolf Kadlec", "Martin Schmid", "Ondřej Bajgar", "Jan Kleindienst."],
    "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
    "year": 2016
  }, {
    "title": "Globally coherent text generation with neural checklist models",
    "authors": ["Chloé Kiddon", "Luke Zettlemoyer", "Yejin Choi."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 329–339, Austin, Texas. Associa-",
    "year": 2016
  }, {
    "title": "World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions",
    "authors": ["Teng Long", "Emmanuel Bengio", "Ryan Lowe", "Jackie Chi Kit Cheung", "Doina Precup."],
    "venue": "Proceedings of the 2017 Con-",
    "year": 2017
  }, {
    "title": "Open language learning for information extraction",
    "authors": ["Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
    "year": 2012
  }, {
    "title": "Key-value memory networks for directly reading documents",
    "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-",
    "year": 2016
  }, {
    "title": "Never-ending learning",
    "authors": ["A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling."],
    "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).",
    "year": 2015
  }, {
    "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension",
    "authors": ["Tsendsuren Munkhdalai", "Hong Yu."],
    "venue": "International Conference on Learning Representations (ICLR) 2017.",
    "year": 2016
  }, {
    "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
    "authors": ["Roberto Navigli", "Simone Paolo Ponzetto."],
    "venue": "Artificial Intelligence, 193:217– 250.",
    "year": 2012
  }, {
    "title": "Who did what: A large-scale person-centered cloze dataset",
    "authors": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2230–",
    "year": 2016
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. Associa-",
    "year": 2014
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
    "year": 2016
  }, {
    "title": "MCTest: A challenge dataset for the open-domain machine comprehension of text",
    "authors": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."],
    "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
    "year": 2013
  }, {
    "title": "Bi-Directional Attention Flow for Machine Comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hananneh Hajishirzi."],
    "venue": "Proceedings of International Conference of Learning Representations 2017, pages 1–12.",
    "year": 2017
  }, {
    "title": "Reasonet: Learning to stop reading in machine comprehension",
    "authors": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."],
    "venue": "Proceedings",
    "year": 2016
  }, {
    "title": "Open mind common sense: Knowledge acquisition from the general public",
    "authors": ["Parmjit Singh", "T Lin", "E.T. Mueller", "G Lim", "T Perkins", "W.L. Zhu."],
    "venue": "Lecture Notes in Computer Science, volume 2519, pages 1223–1237.",
    "year": 2002
  }, {
    "title": "Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia",
    "authors": ["Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum."],
    "venue": "Technical Report UMCS-2012-015.",
    "year": 2012
  }, {
    "title": "Iterative alternating neural attention for machine reading",
    "authors": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."],
    "venue": "abs/1606.02245.",
    "year": 2016
  }, {
    "title": "2017. ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
    "authors": ["Robert Speer", "Joshua Chin", "Catherine Havasi"],
    "year": 2017
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "volume 15, pages 1929–1958.",
    "year": 2014
  }, {
    "title": "From Freebase to Wikidata : The Great Migration",
    "authors": ["Thomas Pellissier Tanon", "Denny Vrande", "San Francisco", "Sebastian Schaffert", "Thomas Steiner."],
    "venue": "Proceedings of the 25th International Conference on World Wide Web, pages 1419–1428.",
    "year": 2016
  }, {
    "title": "Newsqa: A machine comprehension dataset",
    "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."],
    "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages",
    "year": 2017
  }, {
    "title": "Natural language comprehension with the epireader",
    "authors": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Philip Bachman", "Alessandro Sordoni", "Kaheer Suleman."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-",
    "year": 2016
  }, {
    "title": "Gated self-matching networks for reading comprehension and question answering",
    "authors": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
    "year": 2017
  }, {
    "title": "Multi-perspective context matching for machine comprehension",
    "authors": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."],
    "venue": "CoRR, abs/1612.04211.",
    "year": 2016
  }, {
    "title": "Dynamic integration of background knowledge in neural NLU systems",
    "authors": ["Dirk Weissenborn", "Tomas Kocisky", "Chris Dyer."],
    "venue": "CoRR, abs/1706.02596.",
    "year": 2017
  }, {
    "title": "Memory networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "International Conference on Learning Representations (ICLR), 2015.",
    "year": 2015
  }, {
    "title": "Dynamic coattention networks for question answering",
    "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher."],
    "venue": "International Conference on Learning Representations (ICLR), 2017, volume abs/1611.01604.",
    "year": 2016
  }, {
    "title": "Leveraging knowledge bases in lstms for improving machine reading",
    "authors": ["Bishan Yang", "Tom Mitchell."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436–1446. Asso-",
    "year": 2017
  }, {
    "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
    "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."],
    "venue": "International Conference on Learning Representations (ICLR), 2015.",
    "year": 2014
  }, {
    "title": "Reference-aware language models",
    "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling."],
    "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1851–1860. Association for Computational Linguis-",
    "year": 2017
  }],
  "id": "SP:21da1c528d055a134f22e0f8a0b4011fe825a5e7",
  "authors": [{
    "name": "Todor Mihaylov",
    "affiliations": []
  }, {
    "name": "Anette Frank",
    "affiliations": []
  }],
  "abstractText": "We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a keyvalue memory, in a cloze-style setting. Instead of relying only on document-toquestion interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.",
  "title": "Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge"
}