{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Biological organisms can learn to perform tasks (and often do) by observing a sequence of labeled events, just like supervised machine learning. But unlike machine learning, in human learning supervision is often accompanied by a curriculum. Thus the order of presented examples is rarely random when a human teacher teaches another human. Likewise, the task may be divided by the teacher into smaller sub-tasks, a process sometimes called shaping (Krueger & Dayan, 2009) and typically studied in the context of rein-\n1School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel. Correspondence to: Daphna Weinshall <daphna@mail.huji.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nforcement learning (e.g. Graves et al., 2017). Although it remained for the most part in the fringes of machine learning research, curriculum learning has been identified as a key challenge for machine learning throughout (e.g., Mitchell, 1980; 2006; Wang & Cottrell, 2015).\nWe focus here on curriculum learning based on ranking (or weighting as in (Bengio et al., 2009)) of the training examples, which is used to guide the order of presentation of examples to the learner. Risking over simplification, the idea is to first present the learner primarily with examples of higher weight or rank, later to be followed by examples with lower weight or rank. Ranking may be based on the difficulty of each training example as evaluated by the teacher, from easiest to the most difficult.\nIn Section 2 we investigate this strict definition of curriculum learning theoretically, in the context of stochastic gradient descent used to optimize the convex linear regression loss function. We first define the (ideal) difficulty of a training point as its loss with respect to the optimal classifier. We then prove that curriculum learning, when given the ranking of training points by their difficulty thus defined, is expected (probabilistically) to significantly speed up learning especially at the beginning of training. This theoretical result is supported by empirical evidence obtained in the deep learning scenario of curriculum learning described in Section 3, where similar behavior is observed. We also show that when the difficulty of the sampled training points is fixed, convergence is faster when sampling points that incur higher loss with respect to the current hypothesis as suggested in (Shrivastava et al., 2016). This result is not always true when the difficulty of the sampled training points is not fixed.\nBut such ideal ranking is rarely available. In fact, the need for such supervision has rendered curriculum learning less useful in machine learning, since ranking by difficulty is hard to obtain. Moreover, even when it is provided by a human teacher, it may not reflect the true difficulty as it affects the machine learner. For example, in visual object recognition it has been demonstrated that what makes an image difficult to a neural network classifier may not always match whatever makes it difficult to a human observer, an observation that has been taken advantage of in the recent\nwork on adversarial examples (Szegedy et al., 2013). Possibly, this is one of the reasons why curriculum learning is rarely used in practice (but see, e.g., Zaremba & Sutskever, 2014; Amodei et al., 2016; Jesson et al., 2017).\nIn the second part of this paper we focus on this question - how to rank (or weight) the training examples without the aid of a human teacher. This is paramount when a human teacher cannot provide a reliable difficulty score for the task at hand, or when obtaining such a score by human teachers is too costly. This question is also closely related to transfer learning: here we investigate the use of another classifier to provide the ranking of the training examples by their presumed difficulty. This form of transfer should not be confused with the notion of transfer discussed in (Bengio et al., 2009) in the context of multi-task and lifelong learning (Thrun & Pratt, 2012), where knowledge is transferred from earlier tasks (e.g. the discrimination of easy examples) to later tasks (e.g. the discrimination of difficult examples). Rather, we investigate the transfer of knowledge from one classifier to another, as in teacher classifier to student classifier. In this form curriculum learning has not been studied in the context of deep learning, and hardly ever in the context of other classification paradigms.\nDifferently from previous work, it is not the instance representation which is being transferred but rather the ranking of training examples. Why is this a good idea? This kind of transfer assumes that a powerful pre-trained network is only available at train time, and cannot be used at test time even for the computation of a test point’s representation. This may be the case, for example, when the powerful network is too big to run on the target device. One can no longer expect to have access to the transferred representation at test time, while ranking can be used at train time in order to improve the learning of the target smaller network (see related discussion of network compression in (Chen et al., 2015; Kim et al., 2015), for example).\nIn Section 3 we describe our method, an algorithm which uses the ranking to construct a schedule for the order of presentation of training examples. In subsequent empirical evaluations we compare the performance of the method when using a curriculum which is based on different scheduling options, including 2 control conditions where difficult examples are presented first or when using arbitrary scheduling. The main results of this empirical study can be summarized as follows: (i) Learning rate is always faster with curriculum learning, especially at the beginning of training. (ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or when strong regularization is enforced. These results are consistent with prior art (see e.g. Bengio et al., 2009)."
  }, {
    "heading": "2. Theoretical analysis",
    "text": "We start with some notations in Section 2.1, followed in Sections 2.2 by the rigorous analysis of curriculum learning when used to optimize the linear regression loss. In Section 2.3 we report supporting empirical evidence for the main theoretical results, obtained using the deep learning setup described later in Section 3."
  }, {
    "heading": "2.1. Notations and definitions",
    "text": "Let X = {(xi, yi)}ni=1 denote the training data, where xi ∈ Rd denotes the i-th data point and yi its corresponding label. In general, the goal is to find a hypothesis h̄(x) ∈ H that minimizes the risk function (the expected loss). In order to minimize this objective, Stochastic Gradient Descent (SGD) is often used with various extensions and regularization.\nWe start with two definitions:\nDefinition 1 (Ideal Difficulty Score). The difficulty of point x is measured by its minimal loss with respect to the set of optimal hypotheses {L(h̄(xi), yi)}. Definition 2 (Stochastic Curriculum Learning). SCL is a variation on Stochastic Gradient Descent (SGD), where the learner is exposed to the data gradually based on the difficulty score of the training points.\nIn vanilla SGD training, at each iteration the learner is presented with a new datapoint (or mini-batch) sampled from the training data based on some probability function D(X). In SCL, the sampling is biased to favor easier examples at the beginning of the training. This bias is decreased following some scheduling procedure. At the end of training, points are sampled according to D(X) as in vanilla SGD.\nIn practice, an SCL algorithm should solve two problems: (i) Score the training points by difficulty; in prior art this score was typically provided by the teacher in a supervised manner. (ii) Define the scheduling procedure."
  }, {
    "heading": "2.2. The linear regression loss",
    "text": "Here we analyze SCL when used to minimize the linear regression model. Specifically, we investigate the differential effect of a point’s Difficulty Score on convergence towards the global minimum of the expected least squares loss, when the family of hypotheses H includes the linear functions h(x) = atx + b and y ∈ R.\nThe risk function of the regression model is the following\nR(X,w) = ED(X)L(hw(x), y) L(hw(xi), yi) = (h(xi)− yi)2 = (atxi + b− yi)2\n, (xtiw − yi)2 , L(Xi,w) (1)\nIn the last transition above, w = [\na b\n] ∈ Rd+1. With some\nabuse of notation, xi denotes the vector [ xi 1 ] . Xi denotes the vector [xi, yi], with Difficulty Score L(Xi, w̄).\nIn general the output hypothesis hw(x) = xtiw is determined by minimizing R(X,w) with respect to w. The global minimum w̄ of the empirical loss can be computed in closed form from the training data. However, gradient descent can be used to find w̄ with guaranteed convergence, which is efficient when n is very large.\nRecall that SCL computes a sequence of estimators {wt}Tt=1 for the parameters of the optimal hypothesis w̄. This is based on a sequence of training points {Xt = [xt, yt]}Tt=1, sampled from the training data while favoring easy points at the beginning of training. Other than sampling probability, the update step at time t follows SGD:\nwt+1 = wt − η ∂L(Xt,w)\n∂w |w=wt (2)"
  }, {
    "heading": "CONVERGENCE RATE DECREASES WITH DIFFICULTY",
    "text": "The main theorem in this sub-section states that the expected rate of convergence of gradient descent is monotonically decreasing with the Difficulty Score of the sample Xt. We prove it below for the gradient step as defined in (2). If the size of the gradient step is fixed at η, a somewhat stronger theorem can be obtained where the constraint on the step size being small is not required.\nWe first derive the gradient step at time t:\ns = −η ∂L(Xi,w) ∂w = −2η(xtiw − yi)xi (3)\nLet Ωi denote the hyperplane on which this gradient vanishes ∂L(Xi,w)∂w = 0. This hyperplane is defined by x t iw = yi, namely, xi defines its normal direction. Thus (3) implies that the gradient step at time t is perpendicular to Ωi. Let z̄ denote the projection of w̄ on Ωi. Let Ψ2 = L(Xi, w̄) denote the Difficulty Score of Xi.\nLemma 1. Fix the training point Xi. The Difficulty Score of Xi is Ψ2 = r2‖w̄ − z̄‖2."
  }, {
    "heading": "Proof.",
    "text": "Ψ2 = L(Xi, w̄) = L(Xi, z̄ + (w̄ − z̄)) = [xtiz̄ + x t i(w̄ − z̄)− yi]2\n= [xti(w̄ − z̄)]2 = ‖xi‖2‖w̄ − z̄‖2 (4)\nRecall that xi,w ∈ Rd+1. We continue the analysis in the parameter space w ∈ Rd+1, where parameter vector w corresponds to a point, and data vector xi describes a hyperplane. In this space we represent each vector\nxi in a hyperspherical coordinate system [r, ϑ,Φ], with pole (origin) fixed at w̄ and polar axis (zenith direction) ~O = w̄ − wt (see Fig. 1). r denotes the vector’s length, while 0 ≤ ϑ ≤ π denotes the polar angle with respect to ~O. Let Φ = [ϕ1 . . . , ϕd−1] denote the remaining polar angles.\nTo illustrate, Fig. 1 shows a planar section of the parameter space, the 2D plane formed by the two intersecting lines ~O and z̄− w̄. The gradient step s points from wt towards Ωi. Ωi is perpendicular to xi, which is parallel to z̄− w̄ and to s, and therefore Ωi is projected onto a line in this plane. We introduce the notation λ = ‖w̄ −wt‖.\nLet sO denote the projection of the gradient vector s on the polar axis ~O, and let s⊥ denote the perpendicular component. From (3) and the definition of Ψ\ns = −2ηxi(xtiwt − yi) = −2ηxi[xti(wt − w̄)±Ψ] sO = s · w̄ −wt\nλ = 2\nη λ [r2λ2 cos2 ϑ∓Ψrλ cosϑ]\n(5)\nLet x = (r, ϑ,Φ). Let fD(X) = f(r, ϑ,Φ)fY (|y − xtw̄|) denote the density function of the data X. This choice assumes that the density of the label y only depends on the absolute error |y − xtw̄|.\nFor the subsequent derivations we need the conditional distribution of the data X given difficulty score Ψ. Fixing the difficulty score determines one of two labels y(x,Ψ) = xtw̄ ± Ψ. We further assume that both labels are equally likely1, and therefore fD(X)/Ψ ([x, y]) = 12f(r, ϑ,Φ).\nLet ∆(Ψ) denote the expected convergence rate at time t, given fixed difficulty score Ψ.\n∆(Ψ) = E[‖wt − w̄‖2 − ‖wt+1 − w̄‖2/Ψ] (6)\nLemma 2.\n∆(Ψ) = 2λE[sO/Ψ]− E[s 2/Ψ] (7) 1This assumption can be somewhat relaxed, but the strict form is used to simplify the exposition.\nProof. From (6)\nE[∆] = (−λ)2 − E[(−λ+ sO)2 + s2⊥] = λ2 − (λ2 − 2λE[sO] + E[s2O])− E[s2⊥] = 2λE[sO]− E[s2]\nFrom Lemma 2 and (5)2\n1 4 ∆(Ψ) = ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]\n−η2Ψ2E[r2] (8) −ηE[(±Ψ)rλ cosϑ]− 2η2E[(±Ψ)r3λ cosϑ]\nLemma 3.\nE[(±Ψ)rλ cosϑ] = E[(±Ψ)r3λ cosϑ] = 0\nProof. The lemma follows from the assumed symmetry of D(X) with respect to the sign of yi − xtiw̄.\nIt follows from Lemma 3 that 1\n4 ∆(Ψ) =ηE[r2λ2 cos2 ϑ]− η2E[r4λ2 cos2 ϑ]\n− η2Ψ2E[r2] (9)\nWe can now state the main theorem of this section. Theorem 1. At time t the expected convergence rate for training point x is monotonically decreasing with the Difficulty Score Ψ of x. If the step size coefficient is sufficiently small so that η ≤ E[r\n2 cos2 ϑ] E[r4 cos2 ϑ] , it is likewise monotonically\nincreasing with the distance λ between the current estimate of the hypothesis wt and the optimal hypothesis w̄.\nProof. From (9)\n∂∆(Ψ)\n∂Ψ = −8η2E[r2]Ψ ≤ 0\nwhich proves the first statement. In addition,\n∂∆(Ψ)\n∂λ = 8ηλ\n( E[r2 cos2 ϑ]− ηE[r4 cos2 ϑ] ) If η ≤ E[r\n2 cos2 ϑ] E[r4 cos2 ϑ] then ∂∆(Ψ) ∂λ ≥ 0, and the second statement\nfollows.\nCorollary 1. Although E[∆(Ψ)] may be negative, wt always converges faster to w̄ when the training points are sampled from easier examples with smaller Ψ. Corollary 2. If the step size coefficient η is small enough so that η ≤ E[r\n2 cos2 ϑ] E[r4 cos2 ϑ] , we should expect faster convergence\nat the beginning of SCL. 2Below the short-hand notation E[(±Ψ)] implies that the 2 cases of y(x,Ψ) = xtw̄ ± Ψ should be considered, with equal probability 1\n2 by assumption."
  }, {
    "heading": "CONVERGENCE RATE INCREASES WITH CURRENT LOSS",
    "text": "The main theorem in this sub-section states that for a fixed difficulty score Ψ, when the gradient step is small enough, convergence is monotonically increasing with the loss of the point with respect to the current hypothesis. This is not true in general. The second theorem in this section shows that when the difficulty score is not fixed, there exist hypotheses w ∈ H for which the convergence rate is decreasing with the current loss.\nLet Υ2 = L(Xi,wt) denote the loss of Xi with respect to the current hypothesis wt. Define the angle β ∈ [0, π2 ) as follows (see Fig. 1)\nβ = β(r,Ψ, λ) = arccos(min( Ψ\nλr , 1)) (10)\nLemma 4. The relation between Υ,Ψ, r, ϑ can be written separately in 4 regions as follows (see Fig. 1):\nA1 0 ≤ ϑ ≤ π−β, yi = xtiw̄+ Ψ =⇒ yi = xtiwt+ Υ, λr cosϑ = xti(w̄ −wt) = −Ψ + Υ\nA2 π−β ≤ ϑ ≤ π, yi = xtiw̄+Ψ =⇒ yi = xtiwt−Υ, λr cosϑ = −Ψ−Υ\nA3 0 ≤ ϑ ≤ β, yi = xtiw̄ −Ψ =⇒ yi = xtiwt + Υ, λr cosϑ = Ψ + Υ\nA4 β ≤ ϑ ≤ π, yi = xtiw̄ −Ψ =⇒ yi = xtiwt −Υ, λr cosϑ = Ψ−Υ\nProof. We keep in mind that ∀xi and Ψ, there are 2 possible yi with equal probability. Recall that z̄ denotes the projection of w̄ on Ωi. In the planar section shown in Fig. 1,\nz̄ lies in the upper half space ⇐⇒ yi = xtiw̄ + Ψ\nz̄ lies in the lower half space ⇐⇒ yi = xtiw̄ −Ψ\nThis follows from 3 observations: x̄i lies in the upper half space by the definition of the polar coordinate system, xtiw̄ − yi = ±Ψ, and\n0 = xtiz̄− yi = xti(z̄− w̄) + xtiw̄ − yi\nNext, let zt denote the projection of wt on Ωi. Then\n0 = xtizt − yi = xti(zt −wt) + xtiwt − yi\nWhen z̄ lies in the upper half space, the following can be verified geometrically from Fig. 1:\n0 ≤ ϑ ≤ π−β ⇒ xti(zt−wt) ≥ 0 ⇒ yi = xtiwt + Υ\nπ−β ≤ ϑ ≤ π ⇒ xti(zt−wt) ≤ 0 ⇒ yi = xtiwt−Υ\nWhen z̄ lies in the lower half space\n0 ≤ ϑ ≤ β =⇒ xti(zt −wt) ≥ 0 =⇒ yi = xtiwt + Υ\nβ ≤ ϑ ≤ π =⇒ xti(zt −wt) ≤ 0 =⇒ yi = xtiwt −Υ\nNext we analyze how the convergence rate at xi changes with Υ. Let ∆(Ψ,Υ) denote the expected convergence rate at time t, given fixed difficulty score Ψ and fixed loss Υ. From (9) ∆(Ψ,Υ) = 4ηE[r2λ2 cos2 ϑ/Υ] +O(η 2).\nIt is easier to analyze ∆(Ψ,Υ) when using the Cartesian coordinates, rather than polar, in the 2D plane defined by the vectors ~O = w̄−wt and z̄− w̄ (see Fig. 1); thus we define u = r cosϑ, v = r sinϑ. The 4 cases listed in Lemma 4 can be readily transformed to this coordinate system as follows {0 ≤ ϑ ≤ β} ⇔ {λu ≥ Ψ}, {β ≤ ϑ ≤ π − β} ⇔ {−Ψ ≤ λu ≤ Ψ}, and {π − β ≤ ϑ ≤ π} ⇔ {λu ≤ −Ψ}:\nA1 λu ≥ −Ψ : λu = −Ψ + Υ\nA2 λu ≤ −Ψ : λu = −Ψ−Υ\nA3 λu ≥ Ψ : λu = Ψ + Υ\nA4 λu ≤ Ψ : λu = Ψ−Υ\nDefine\n∇ = f(ψ+Υλ )− f( ψ−Υ λ )− f( −ψ+Υ λ ) + f( −ψ−Υ λ )\nf(ψ+Υλ ) + f( ψ−Υ λ ) + f( −ψ+Υ λ ) + f( −ψ−Υ λ )\nClearly −1 ≤ ∇ ≤ 1. Theorem 2. Assume that the gradient step size is small enough so that we can neglect second order terms O(η2), and that ∂∇∂Υ ≥ ψ Υ − Υ ψ ∀Υ. Fix the difficulty score at Ψ. At time t the expected convergence rate is monotonically increasing with the loss Υ of the training point x.\nProof. In the coordinate system defined above ∆(Ψ,Υ) = 4ηE[λ2u2/Υ] + O(η\n2). We compute ∆(Ψ,Υ) separately in each region, marginalizing out v based on the following∫ ∫ ∞\n0\nλ2u2vd−1f(u, v)dvdu = ∫ λ2u2f(u)du\nwhere f(u) denotes the marginal distribution of u.\nLet ui denote the value of u corresponding to loss Υ in each region A1-A4, and 12f(ui) its density. ∆(Ψ,Υ) takes 4 discrete values, one in each region, and its expected value is therefore ∆(Ψ,Υ) = 4η ∑4 i=1 λ 2u2i f(ui)∑4 i=1 f(ui)\n. It can readily be shown that\n1\n4η ∆(ψ,Υ) = ψ2 + Υ2 + 2ψΥ∇ (11)\nand subsequently\n1\n4η\n∂∆(ψ,Υ)\n∂Υ = 2Υ + 2ψΥ ∂∇ ∂Υ + 2ψ ∇\n≥ 2Υ + 2ψΥ ∂∇ ∂Υ − 2ψ\n(12)\nUsing the assumption that ∂∇∂Υ ≥ ψ Υ − Υ ψ ∀Υ, we have that\n1\n8η\n∂∆(ψ,Υ) ∂Υ ≥ Υ + ψΥ ψ −Υ ψΥ − ψ = 0\nCorollary 3. For any c ∈ R+, if∇ is (c− 1c )-lipschitz then ∂∆(ψ,Υ) ∂Υ ≥ 0 for any Υ ≥ c ψ. Corollary 4. If D(X/Ψ) = k(Ψ) over a compact region and η small enough, then ∂∆(ψ,Υ)∂Υ ≥ 0 for all Υ excluding the boundaries of the compact region. If in addition Υ > Ψ, then ∂∆(ψ,Υ)∂Υ ≥ 0 almost surely. Theorem 3. Assume that D(X) is continuous and that w̄ is realizable. Then there are always hypotheses w ∈ H for which the expected convergence rate under D(X) is monotonically decreasing with the loss Υ of the sampled points.\nProof. We shift to a hyperspherical coordinate system in Rd+1 similar as before, but now the pole (origin) is fixed at wt. For the gradient step s, it can be shown that:\ns = − sgn (xtiwt − yi)2ηxiΥ\nsO = s · w̄ −wt λ = ±2η λ rλ cosϑ Υ\n(13)\nLet ∆(Υ) denote the expected convergence rate at time t, given a fixed loss Υ. From Lemma 2\n∆(Υ) = 2ηΥ ( E[r cosϑ/xtiwt − yi = −Υ ]−\nE[r cosϑ/xtiwt − yi = Υ ]\n) − E[(2ηrΥ)2]\n, 2ηΥQ(r, ϑ,wt)− 4η2Υ2E[r2]\nIf w = w̄, then Q(r, ϑ,w) = 0 from the symmetry of D(X) with respect to Ψ. From the continuity of D(X), there exists δ > 0 such that if ‖w − w̄‖2 < δ, then ‖Q(r, ϑ,w)−Q(r, ϑ, w̄)‖2 < ηΥE[r2], which implies that ∆(Υ) < −2η2Υ2E[r2] < 0."
  }, {
    "heading": "2.3. Deep learning: simulation results",
    "text": "While the corollaries above apply to a rather simple situation, when using the Difficulty Score to guide SGD while\nminimizing the convex regression loss, their predictions can be empirically tested with the deep learning architecture and loss which are described in Section 3. There an additional challenge is posed by the fact that the empirical ranking is not based on the ideal definition given in Def. 1, but rather on an estimate derived from another classifier.\nStill, the empirical results as shown in Fig. 2 demonstrate agreement with the theoretical analysis of the linear regression loss. Specifically, in epoch 0 there is a big difference between the average errors in estimating the gradient direction, which is smallest for the easiest examples and highest for the most difficult examples as predicted by Corollary 1. This difference in significantly reduced after 10 epochs, and becomes insignificant after 20 epochs, in agreement with Corollary 2.\nDiscussion. Fig. 2 shows that the variance in the direction of the gradient step defined by easier points is significantly smaller than that defined by difficult points, especially at the beginning of training. This is advantageous when the initial point w0 does not lie in the basin of attraction of the desired global minimum w̄, and if, in agreement with Lemma 1, the pronounced shared component of the easy gradient steps points in the direction of the global minimum, or a more favorable local minimum; then the likelihood of escaping the local minimum decreases with a point’s Difficulty Score. This scenario suggests another possible advantage for curriculum learning at the initial stages of training."
  }, {
    "heading": "3. Curriculum learning in deep networks",
    "text": "As discussed in the introduction, a practical curriculum learning method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking. Solutions to these issues are discussed in Section 3.1. In Section 3.2 we discuss the empirical evaluation of our method."
  }, {
    "heading": "3.1. Method",
    "text": ""
  }, {
    "heading": "RANKING EXAMPLES BY KNOWLEDGE TRANSFER",
    "text": "The main novelty of our proposed method lies in this step, where we rank the training examples by estimated difficulty in the absence of human supervision. Difficulty is estimated based on knowledge transfer from another classifier. Here we investigate transfer from a more powerful learner.\nIt is a common practice now to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer. This layer activation is then used for representing similar objects and train a simpler classifier (such as SVM, or shallower NNs) to perform a different task, related but not identical to the original task the network had been trained on. In computer vision such embeddings are commonly obtained by training a deep network on the recognition of a very large database such as ImageNet (Deng et al., 2009). These embeddings have been shown to provide better semantic representations of images (as compared to more traditional image features) in a number of related tasks, including the classification of small datasets (Sharif Razavian et al., 2014), image annotation (Donahue et al., 2015) and structured predictions (Hu et al., 2016).\nFollowing this practice, the activation in the penultimate layer of a large and powerful pre-trained network is the loci of knowledge transfer from one network to another. Repeatedly, as in (Sharif Razavian et al., 2014), it has been shown that competitive performance can be obtained by training a shallow classifier on this representation in a new related task. Here we propose to use the confidence of such a classifier, e.g. the margin of an SVM classifier, as the estimator for the difficulty of each training example. This measure is then used to sort the training data. We note that unlike the traditional practice of reusing a pre-trained network, here we only transfer information from one learner to another. The goal is to achieve a smaller classifier that can conceivably be used with simpler hardware, without depending on access to the powerful learner at test time."
  }, {
    "heading": "SCHEDULING THE APPEARANCE OF TRAINING EXAMPLES",
    "text": "In agreement with prior art, e.g. the definition of curriculum in (Bengio et al., 2009), we investigate curriculum learning\nwhere the scheduling of examples changes with time, giving priority to easier examples at the beginning of training. We explored two variants of the basic scheduling idea:\nFixed. The distribution used to sample examples from the training data is gradually changed in fixed steps. Initially all the weight is put on the easiest examples. In subsequent steps the weight of more difficult examples is gradually increased, until the final step in which the training data is sampled uniformly (or based on some prior distribution on the training set).\nAdaptive. Similar to the previous mode, but where the length of each step is not fixed, but is being determined adaptively based on the current loss of the training data."
  }, {
    "heading": "3.2. Empirical evaluation",
    "text": ""
  }, {
    "heading": "EXPERIMENTAL SETUP",
    "text": "Datasets. For evaluation we used 2 data sets: CIFAR-100 (Krizhevsky & Hinton, 2009) and STL-10 (Coates et al., 2010). In all cases, as is commonly done, the data was pre-processed using global contrast normalization; cropping and flipping were used for STL-10.\nNetwork architecture. We used convolutional Neural Networks (CNN) which excel at image classification tasks. Specifically, we used two architectures which are henceforth denoted Large and Small, in accordance with the number of parameters. The Large network is comprised of four blocks, each with two convolutional layers, ELU activation, and max-pooling. This is followed by a fully connected layer, for a total of 1,208,101 parameters. The Small network consists of only three hidden layers, for a total of 4,557 parameters. During training, we applied dropout and l2 regularization on the weights, and used either SGD or ADAM to optimize the cross-entropy loss.\nScheduling mechanisms: control. As described above, our method is based on a scheduling design which favors the presentation of easier examples at the beginning of training. In order to isolate the contribution of scheduling by increasing level of difficulty as against other spurious consequences of data scheduling, we compared performance with the following control conditions: control-curriculum, identical scheduling mechanism but where the underlying ranking of the training examples is random and unrelated to estimated difficulty; and anti-curriculum, identical scheduling mechanism but favoring the more difficult examples at the beginning of training."
  }, {
    "heading": "CONTROLLING FOR TASK DIFFICULTY",
    "text": "Evidence from prior art is conflicting regarding where the benefits of curriculum learning lie, which is to be expected given the variability in the unknown sources of the curricu-\nlum supervision information and its quality. We observed in our empirical study that the benefits depended to a large extent on the difficulty of the task. We always saw faster learning at the beginning of the training process, while lower generalization error was seen only when the task was relatively difficult. We therefore employed controls for the following 3 sources of task difficulty:\nInherent task difficulty. To investigate this factor, we take advantage of the fact that CIFAR-100 is a hierarchical dataset with 100 classes and 20 super-classes, each including 5 member classes. We therefore trained a network to discriminate the 5 member classes of 2 super-classes as 2 separate tasks: ‘small mammals’ (task 1) and ‘aquatic mammals’ (task 2). These are expected to be relatively hard learning tasks. We also trained a network to discriminate 5 random well separated classes: ‘camel’, ‘clock’, ‘bus’, ‘dolphin’ and ‘orchid’ (task 3). This task is expected to be relatively easy.\nSize of classification network. For a given task, classification performance is significantly affected by the size of the network and its architecture. We assume, of course, that we operate in the domain where the number of model parameters is smaller than can be justified by the training data (i.e., there is no overfit). We therefore used networks of different sizes in order to evaluate how curriculum learning is affected by task difficulty as determined by the network’s strength (see Fig. 3a-b). In this comparative evaluation, the smaller the network is, the more difficult the task is likely to be (clearly, many other factors participate in the determination of task difficulty).\nRegularization and optimization. Regularization is used to constrain the family of hypotheses, or models, so that they possess such desirable properties as smoothness. Regularization effectively decreases the number of degrees of freedom in the model. In fact, most optimization methods, other then vanilla stochastic gradient descent, incorporate some form of regularization and smoothing, among other inherent properties. Therefore the selection of optimization method also plays a role in determining the effective size of the final network."
  }, {
    "heading": "RESULTS",
    "text": "Fig. 3a shows typical results when training the Large CNN (see network’s details above) to classify a subset of 5 CIFAR100 images (task 1 as defined above), using slow learning rate and Adam optimization. In this setup we see that curriculum learning speeds up the learning rate at the beginning of the training, but converges to the same performance as regular training. When we make learning more difficulty by using the Small network, performance naturally decreases, but now we see that curriculum learning also improves the final generalization performance (Fig. 3b).\nSimilar results are shown for the STL-10 dataset (Fig. 3c).\nFig. 4 shows comparative results when controlling for inherent task difficulty in the 3 tasks described above, using faster learning rate and SGD optimization. Task difficulty can be evaluated in retrospect from the final performance seen in each plot. As can be clearly seen in the figure, the improvement in final accuracy with curriculum learning is larger when the task is more difficult. When manipulating the level of regularization, we see that while too much regularization always harms performance, curriculum learning is least affected by this degradation (results are omitted)."
  }, {
    "heading": "4. Summary and Discussion",
    "text": "We investigated curriculum learning, an extension of stochastic gradient descent in which easy examples are more frequently sampled at the beginning of training. We started\nwith the theoretical investigation of this strict definition in the context of linear regression, showing that curriculum learning accelerates the learning rate in agreement with prior empirical evidence. While not shedding light on its affect on the classifier’s final performance, our analysis suggests that the direction of a gradient step based on ”easy” examples may be more effective in traversing the input space towards the ideal minimum of the loss function. Specifically, we have empirically shown that the variance in the gradient direction of points increases with their difficulty when optimizing a non-convex loss function. Over-sampling the more coherent easier examples may therefore increase the likelihood to escape the basin of attraction of a low quality local minimum in favor of higher quality local minima even in the general non-convex case.\nWe also showed theoretically that when the difficulty score of the training points is fixed, convergence is faster if the loss with respect to the current hypothesis is higher. This seems to be a very intuitive result, an intuition that underlies the boosting method for example. However, as intuitive as it might be, this is not always true when the prior data density is assumed to be continuous and when the optimal hypothesis is realizable. Thus the requirement that the difficulty score is fixed is necessary.\nIn the second part of this paper we described a curriculum learning method for deep networks. The method relies on knowledge transfer from other (pre-trained) networks in order to rank the training examples by difficulty. We described extensive experiments where we evaluated our proposed method under different task difficulty conditions and against a variety of control conditions. In all cases curriculum learning has been shown to increase the rate of convergence at the beginning of training, in agreement with the theoretical results. With more difficult tasks, curriculum learning improved generalization performance."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported in part by a grant from the Israel Science Foundation (ISF) and by the Gatsby Charitable Foundations."
  }],
  "year": 2018,
  "references": [{
    "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
    "authors": ["D. Amodei", "S. Ananthanarayanan", "R. Anubhai", "J. Bai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "Q. Cheng", "G Chen"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Curriculum learning",
    "authors": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"],
    "venue": "In Proceedings of the 26th annual international conference on machine learning,",
    "year": 2009
  }, {
    "title": "Compressing neural networks with the hashing trick",
    "authors": ["W. Chen", "J. Wilson", "S. Tyree", "K. Weinberger", "Y. Chen"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "An analysis of singlelayer networks in unsupervised feature learning",
    "authors": ["A. Coates", "H. Lee", "A.Y. Ng"],
    "venue": "Ann Arbor,",
    "year": 2010
  }, {
    "title": "ImageNet: A Large-Scale Hierarchical Image Database",
    "authors": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"],
    "venue": "In CVPR09,",
    "year": 2009
  }, {
    "title": "Automated curriculum learning for neural networks",
    "authors": ["A. Graves", "M.G. Bellemare", "J. Menick", "R. Munos", "K. Kavukcuoglu"],
    "venue": "arXiv preprint arXiv:1704.03003,",
    "year": 2017
  }, {
    "title": "Learning structured inference neural networks with label relations",
    "authors": ["H. Hu", "Zhou", "G.-T", "Z. Deng", "Z. Liao", "G. Mori"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Learning multiple layers of features from tiny images",
    "authors": ["A. Krizhevsky", "G. Hinton"],
    "year": 2009
  }, {
    "title": "Flexible shaping: How learning in small steps helps",
    "authors": ["K.A. Krueger", "P. Dayan"],
    "year": 2009
  }, {
    "title": "The need for biases in learning generalizations",
    "authors": ["T.M. Mitchell"],
    "year": 1980
  }, {
    "title": "The discipline of machine learning, volume",
    "authors": ["T.M. Mitchell"],
    "venue": "School of Computer Science, Machine Learning Department,",
    "year": 2006
  }, {
    "title": "Cnn features off-the-shelf: an astounding baseline for recognition",
    "authors": ["A. Sharif Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"],
    "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
    "year": 2014
  }, {
    "title": "Training region-based object detectors with online hard example mining",
    "authors": ["A. Shrivastava", "A. Gupta", "R.B. Girshick"],
    "venue": "CoRR, abs/1604.03540,",
    "year": 2016
  }, {
    "title": "Intriguing properties of neural networks",
    "authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"],
    "venue": "arXiv preprint arXiv:1312.6199,",
    "year": 2013
  }, {
    "title": "Learning to learn",
    "authors": ["S. Thrun", "L. Pratt"],
    "venue": "Springer Science & Business Media,",
    "year": 2012
  }, {
    "title": "Basic level categorization facilitates visual object recognition",
    "authors": ["P. Wang", "G.W. Cottrell"],
    "venue": "arXiv preprint arXiv:1511.04103,",
    "year": 2015
  }, {
    "title": "Learning to execute",
    "authors": ["W. Zaremba", "I. Sutskever"],
    "venue": "arXiv preprint arXiv:1410.4615,",
    "year": 2014
  }],
  "id": "SP:c0877abb93cbe0b28e98929591ec57e7ed213fef",
  "authors": [{
    "name": "Daphna Weinshall",
    "affiliations": []
  }, {
    "name": "Gad Cohen",
    "affiliations": []
  }, {
    "name": "Dan Amir",
    "affiliations": []
  }],
  "abstractText": "We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.",
  "title": "Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks"
}