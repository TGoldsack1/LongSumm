{
  "sections": [{
    "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 570–575 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics\n570"
  }, {
    "heading": "1 Introduction",
    "text": "Building Artificial Intelligence (AI) algorithms to teach machines to read and to comprehend text is a long-standing challenge in Natural Language Processing (NLP). A common strategy for assessing these AI algorithms is by treating them as RC tasks. This can be formulated as finding an answer to a question given the document(s) as evidence. Recently, many deep-learning based models (Seo et al., 2017; Xiong et al., 2017; Wang et al., 2017; Shen et al., 2017; Clark and Gardner, 2017) have been proposed to solve RC tasks based on the SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) datasets, reaching human level performance. A common approach in these models is to score and/or extract candidate spans conditioned on a given question-document pair.\nMost of these models have limited applicability to real problems for the following reasons. They do not generalize well to scenarios where the answer is not present as a span, or where several discontinuous parts of the document are required to\n∗ To whom correspondence should be addressed.\nform the answer. In addition, unlike humans, they can not easily skip through irrelevant parts to comprehend long documents (Masson, 1983).\nTo address the issues above we develop a novel context zoom-in network (ConZNet) for RC tasks, which can skip through irrelevant parts of a document and generate an answer using only the relevant regions of text. The ConZNet architecture consists of two phases. In the first phase we identify the relevant regions of text by employing a reinforcement learning algorithm. These relevant regions are not only useful to generate the answer, but can also be presented to the user as supporting information along with the answer. The second phase is based on an encoder-decoder architecture, which comprehends the identified regions of text and generates the answer by using a residual self-attention network as encoder and a RNNbased sequence generator along with a pointer network (Vinyals et al., 2015) as the decoder. It has the ability to generate better well-formed answers not verbatim present in the document than span prediction models.\nRecently, there have been several attempts to adopt condensing documents in RC tasks. Wang et al. (2018) retrieve a relevant paragraph based on the question and predict the answer span. Choi et al. (2017) select sentence(s) to make a summary of the entire document with a feed-forward network and generate an answer based on the summary. Unlike existing approaches, our method has the ability to select relevant regions of text not just based on the question but also on how well regions are related to each other. Moreover, our decoder combines span prediction and sequence generation. This allows the decoder to copy words from the relevant regions of text as well as to generate words from a fixed vocabulary.\nWe evaluate our model using one of the challenging RC datasets, called ‘NarrativeQA’, which\nwas released recently by Kočiskỳ et al. (2017). Experimental results show the usefulness of our framework for RC tasks and we outperform stateof-the-art results on this dataset."
  }, {
    "heading": "2 Proposed Architecture",
    "text": "An overview of our architecture is shown in Figure 1, which consists of two phases. First, the identification of relevant regions of text is computed by the Co-attention and Context Zoom layers as explained in Sections 2.1 and 2.2. Second, the comprehension of identified regions of text and output generation is computed by Answer Generation block as explained in Section 2.3."
  }, {
    "heading": "2.1 Co-attention layer",
    "text": "The words in the document, question and answer are represented using pre-trained word embeddings (Pennington et al., 2014). These wordbased embeddings are concatenated with their corresponding char embeddings. The char embeddings are learned by feeding all the characters of a word into a Convolutional Neural Network (CNN) (Kim, 2014). We further encode the document and question embeddings using a shared bi-directional GRU (Cho et al., 2014) to get context-aware representations.\nWe compute the co-attention between document and question to get question-aware representations for the document by using tri-linear attention as proposed by Seo et al. (2017). Let di be the vector representation for the document word i, qj be the vector for the question word j, and ld and lq be the lengths of the document and question respectively. The tri-linear attention is calculated as\naij = wddi + wqqj + wdq(di qj), (1)\nwhere wd, wq, and wdq are learnable parameters and denotes the element-wise multiplication.\nWe compute the attended document word d̃i by first computing λi = softmax(ai:) and followed by d̃i = ∑lq j=1 λijqj . Similarly, we compute a question to document attention vector q̃ by first computing b = softmax(max(ai:)) and followed by q̃ = ∑ld i=i dibi. Finally, di, d̃i, di d̃i, d̃i q̃ are concatenated to yield a query-aware contextual representation for each word in the document."
  }, {
    "heading": "2.2 Context Zoom Layer",
    "text": "This layer finds relevant regions of text. We use reinforcement learning to do that, with the goal of improving answer generation accuracy – see Section 2.4.\nThe Split Context operation splits the attended document vectors into sentences or fixed size chunks (useful when sentence tokenization is not available for a particular language). This results in n text regions with each having length lk, where ld = ∑n k=1 lk. We then get the representations, denoted as zk, for each text region by running a BiGRU and concatenating the last states of the forward and backward GRUs.\nThe text region representations, zk, encode how well they are related to the question, and their surrounding context. Generating an answer may depend on multiple regions, and it is important for\neach text region to collect cues from other regions which are outside of their surroundings. We can compute this by using a Self-Attention layer. It is a special case of co-attention where both operands (di and qj) are the text fragment itself, computed by setting aij = −∞ when i = j in Eq. 1.\nThese further self-attended text region representations, z̃k, are passed through a linear layer with tanh activation and softmax layer as follows:\nu = tanh(Wc[z̃1, · · · , z̃n] + bc), (2) ψ = softmax(u), (3)\nwhere ψ is the probability distribution of text regions, which is the evidence used to generate the answer. The policy of the reinforcement learner is defined as π(r|u; θz) = ψr, where ψr is the probability of a text region r (agent’s action) being selected, u is the environment state as defined in Eq. 2, and θz are the learnable parameters. During the training time we sample text regions using ψ, in inference time we follow greedy evaluation by selecting most probable region(s)."
  }, {
    "heading": "2.3 Answer Generation",
    "text": "This component is implemented based on the encoder-decoder architecture of (Sutskever et al., 2014). The selected text regions from the Context Zoom layer are given as input to the encoder, where its output is given to the decoder in order to generate the answer.\nThe encoder block uses residual connected selfattention layer followed by a BiGRU. The selected relevant text regions (∈ ψr) are first passed through a separate BiGRU, then we apply a selfattention mechanism similar to the Context Zoom layer followed by a linear layer with ReLU activations. The encoder’s output consists of representations of the relevant text regions, denoted by ei.\nThe decoder block is based on an attention mechanism (Bahdanau et al., 2015) and a copy mechanism by using a pointer network similar to (See et al., 2017). This allows the decoder to predict words from the relevant regions as well as from the fixed vocabulary. At time step t, the decoder predicts the next word in the answer using the attention distribution, context vector and current word embedding. The attention distribution and context vector are obtained as follows:\noti = v T tanh(Weei +Whht + bo), (4)\nγt = softmax(oti), (5)\nwhere ht is hidden state of the decoder, v, We, Wh, bo are learnable parameters. The γt represents a probability distribution over words of relevant regions ei. The context vector is given by ct = ∑ i γ t iei.\nThe probability distribution to predict word wt from the fixed vocabulary (Pfv) is computed by passing state ht and context vector ct to a linear layer followed by a softmax function denoted as\nPfv = softmax(Wv(Xv[ht, ct] + bp)+ bq). (6)\nTo allow decoder to copy words from the encoder sequence, we compute a soft gate (Pcopy), which helps the decoder to generate a word by sampling from the fixed vocabulary or by copying from a selected text regions (ψr). The soft gate is calculated as\nPcopy = σ(w T p ct + v T h ht + w T x xt + bc), (7)\nwhere xt is current word embedding, ht is hidden state of the decoder, ct is the context vector, and wp, vh, wx, and bc are learnable parameters. We maintain a list of out-of-vocabulary (OOV) words for each document. The fixed vocabulary along with this OOV list acts as an extended vocabulary for each document. The final probability distribution (unnormalized) over this extended vocabulary (Pev) is given by\nPev(wt) = (1−Pcopy)Pfv(wt)+Pcopy ∑\ni:wi=wt\nγti .\n(8)"
  }, {
    "heading": "2.4 Training",
    "text": "We jointly estimate the parameters of our model coming from the Co-attention, Context Zoom, and Answer Generation layers, which are denoted as θa, θz , and θg respectively. Estimating θa and θg is straight-forward by using the cross-entropy objective J1({θa, θg}) and the backpropagation algorithm. However, selecting text regions in the Context Zoom layer makes it difficult to estimate θz\ngiven their discrete nature. We therefore formulate the estimation of θz as a reinforcement learning problem via a policy gradient method. Specifically, we design a reward function over θz .\nWe use mean F-score of ROUGE-1, ROUGE-2, and ROUGE-L (Lin and Hovy, 2003) as our reward function R. The objective function to maximize is the expected reward under the probability distribution of current text regions ψr, i.e., J2(θz) = Ep(r|θz)[R]. We approximate the gradient ∇θzJ2(θz) by following the REINFORCE (Williams, 1992) algorithm. To reduce the high variance in estimating∇θzJ2(θz) one widely used mechanism is to subtract a baseline value from the reward. It is shown that any number will reduce the variance (Williams, 1992; Zaremba and Sutskever, 2015), here we used the mean of the mini-batch reward b as our baseline. The final objective is to minimize the following equation:\nJ(θ) = J1({θa, θg})−J2(θz)+ B∑ i=1 (Ri−b), (9)\nwhere, B is the size of mini-batch, and Ri is the reward of example i ∈ B. J(θ) is now fully differentiable and we use backpropagation to estimate θ."
  }, {
    "heading": "3 Experimental Results",
    "text": ""
  }, {
    "heading": "3.1 Dataset",
    "text": "The NarrativeQA dataset (Kočiskỳ et al., 2017) consists of fictional stories gathered from books and movie scripts, where corresponding summaries and question-answer pairs are generated with the help of human experts and Wikipedia articles. The summaries in NarrativeQA are 4-5 times longer than documents in the SQuAD dataset. Moreover, answers are well-formed by human experts and are not verbatim in the story, thus making this dataset ideal for testing our model. The statistics of NarrativeQA are available in Table 11."
  }, {
    "heading": "3.2 Baselines",
    "text": "We compare our model against reported models in Kočiskỳ et al. (2017) (Seq2Seq, ASR, BiDAF) and the Multi-range Reasoning Unit (MRU) in Tay et al. (2018). We implemented two baseline models (Baseline 1, Baseline 2) with Context Zoom layer similar to Wang et al. (2018). In both baselines we replace the span prediction layer with an answer generation layer. In Baseline 1 we use an\n1please refer Kočiskỳ et al. (2017) for more details\nattention based seq2seq layer without using copy mechanism in the answer generation unit similar to Choi et al. (2017). In Baseline 2 the answer generation unit is similar to our ConZNet architecture."
  }, {
    "heading": "3.3 Implementation Details",
    "text": "We split each document into sentences using the sentence tokenizer of the NLTK toolkit (Bird and Loper, 2004). Similarly, we further tokenize each sentence, corresponding question and answer using the word tokenizer of NLTK. The model is implemented using Python and Tensorflow (Abadi et al., 2015). All the weights of the model are initialized by Glorot Initialization (Glorot et al., 2011) and biases are initialized with zeros. We use a 300 dimensional word vectors from GloVe (Pennington et al., 2014) (with 840 billion pre-trained vectors) to initialize the word embeddings, which we kept constant during training. All the words that do not appear in Glove are initialized by sampling from a uniform random distribution between [-0.05, 0.05]. We apply dropout (Srivastava et al., 2014) between the layers with keep probability of 0.8 (i.e dropout=0.2). The number of hidden units are set to 100. We trained our model with the AdaDelta (Zeiler, 2012) optimizer for 50 epochs, an initial learning rate of 0.1, and a minibatch size of 32. The hyperparameter ‘sample size’ (number of relevant sentences) is chosen based on the model performance on the devset."
  }, {
    "heading": "3.4 Results",
    "text": "Table 2 shows the performance of various models on NarrativeQA. It can be noted that our model with sample size 5 (choosing 5 relevant sentences) outperforms the best ROUGE-L score available so far by 12.62% compared to Tay et al. (2018). The low performance of Baseline 1 shows that the hybrid approach (ConZNet) for generating words from a fixed vocabulary as well as copying words from the document is better suited than span prediction models (Seq2Seq, ASR, BiDAF, MRU).\nTo validate the importance of finding relevant sentences in contrast to using an entire document for answer generation, we experimented with sample sizes beyond 5. The performance of our model gradually dropped from sample size 7 onwards. This result shows evidence that only a few relevant sentences are sufficient to answer a question.\nWe also experimented with various sample sizes to see the effect of intra sentence relations for an-\nswer generation. The performance of the model improved dramatically with sample sizes 3 and 5 compared to the sample size of 1. These results show that the importance of selecting multiple relevant sentences for generating an answer. In addition, the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough, they should also be related to each other. This result points out that the self-attention mechanism in the Context zoom layer is an important component to identify related relevant sentences."
  }, {
    "heading": "4 Conclusion",
    "text": "We have proposed a new neural-based architecture which condenses an original document to facilitate fast comprehension in order to generate better well-formed answers than span based prediction models. Our model achieved the best performance on the challenging NarrativeQA dataset. Future work can focus for example on designing an inexpensive preprocess layer, and other strategies for improved performance on answer generation."
  }],
  "year": 2018,
  "references": [{
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2015
  }, {
    "title": "Nltk: the natural language toolkit",
    "authors": ["Steven Bird", "Edward Loper."],
    "venue": "Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics.",
    "year": 2004
  }, {
    "title": "On the properties of neural machine translation: Encoder–decoder approaches",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
    "year": 2014
  }, {
    "title": "Coarse-to-fine question answering for long documents",
    "authors": ["Eunsol Choi", "Daniel Hewlett", "Jakob Uszkoreit", "Alexandre Lacoste", "Illia Polosukhin", "Jonathan Berant."],
    "venue": "Proceedings of the ACL. Association for Computational Linguistics.",
    "year": 2017
  }, {
    "title": "Simple and effective multi-paragraph reading comprehension",
    "authors": ["Christopher Clark", "Matt Gardner."],
    "venue": "CoRR, abs/1710.10723.",
    "year": 2017
  }, {
    "title": "Deep sparse rectifier neural networks",
    "authors": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."],
    "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pages",
    "year": 2011
  }, {
    "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
    "authors": ["Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
    "year": 2017
  }, {
    "title": "Convolutional neural networks for sentence classification",
    "authors": ["Yoon Kim."],
    "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special",
    "year": 2014
  }, {
    "title": "The narrativeqa reading comprehension challenge",
    "authors": ["Tomáš Kočiskỳ", "Jonathan Schwarz", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Gábor Melis", "Edward Grefenstette."],
    "venue": "arXiv preprint arXiv:1712.07040.",
    "year": 2017
  }, {
    "title": "Automatic evaluation of summaries using n-gram cooccurrence statistics",
    "authors": ["Chin-Yew Lin", "Eduard Hovy."],
    "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-",
    "year": 2003
  }, {
    "title": "Conceptual processing of text during skimming and rapid sequential reading",
    "authors": ["Michael E.J. Masson."],
    "venue": "Memory & Cognition, 11(3):262–274.",
    "year": 1983
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."],
    "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
    "year": 2014
  }, {
    "title": "Squad: 100,000+ questions for machine comprehension of text",
    "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
    "year": 2016
  }, {
    "title": "Get to the point: Summarization with pointergenerator networks",
    "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
    "year": 2017
  }, {
    "title": "Bidirectional attention flow for machine comprehension",
    "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Reasonet: Learning to stop reading in machine comprehension",
    "authors": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."],
    "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages",
    "year": 2017
  }, {
    "title": "Dropout: A simple way to prevent neural networks from overfitting",
    "authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."],
    "venue": "Journal of Machine Learning Research, 15:1929–1958.",
    "year": 2014
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."],
    "venue": "Advances in neural information processing systems, pages 3104–3112.",
    "year": 2014
  }, {
    "title": "Multi-range reasoning for machine comprehension",
    "authors": ["Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui."],
    "venue": "CoRR, abs/1803.09074.",
    "year": 2018
  }, {
    "title": "R$ˆ3$: Reinforced reader-ranker for open-domain question answering",
    "authors": ["Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerald Tesauro", "Bowen Zhou", "Jing Jiang"],
    "year": 2018
  }, {
    "title": "Gated self-matching networks for reading comprehension and question answering",
    "authors": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."],
    "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
    "year": 2017
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J. Williams."],
    "venue": "Mach. Learn., 8(3-4):229–256.",
    "year": 1992
  }, {
    "title": "Dynamic coattention networks for question answering",
    "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2017
  }, {
    "title": "Reinforcement learning neural turing machines",
    "authors": ["Wojciech Zaremba", "Ilya Sutskever."],
    "venue": "CoRR, abs/1505.00521.",
    "year": 2015
  }, {
    "title": "Adadelta: an adaptive learning rate method",
    "authors": ["Matthew D Zeiler."],
    "venue": "arXiv preprint arXiv:1212.5701.",
    "year": 2012
  }],
  "id": "SP:d7baaa250fbe7a9f5da4cafa8d0ba4e5f1b903a7",
  "authors": [{
    "name": "Sathish Indurthi",
    "affiliations": []
  }, {
    "name": "Seunghak Yu",
    "affiliations": []
  }, {
    "name": "Seohyun Back",
    "affiliations": []
  }, {
    "name": "Heriberto Cuayáhuitl",
    "affiliations": []
  }],
  "abstractText": "In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks. Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document. We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer. To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset ‘NarrativeQA’. The proposed architecture outperforms state-of-the-art results (Tay et al., 2018) by 12.62% (ROUGE-L) relative improvement.",
  "title": "Cut to the Chase: A Context Zoom-in Network for Reading Comprehension"
}