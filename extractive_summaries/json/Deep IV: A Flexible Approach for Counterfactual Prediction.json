{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Supervised machine learning (ML) provides many effective methods for tasks in which a model is learned based on samples from some data generating process (DGP) and then makes predictions about new samples from the same distribution. However, decision makers would often like to predict the effects of interventions into the DGP through policy changes. Such changes impact the relationship between inputs and outcomes, making straightforward prediction approaches inappropriate. In order to accurately answer such counterfactual questions it is necessary to model the structural (or causal) relationship between policy (or “treatment”) and outcome variables.\nFor example, consider an airline that wants to use historical\n1University of British Columbia, Canada 2Microsoft Research, New England, USA. Correspondence to: Jason Hartford <jasonhar@cs.ubc.ca>, Matt Taddy <taddy@microsoft.com>.\nProceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n1Implementation with all simulation experiments is available at https://github.com/jhartford/DeepIV\ndata to optimize the prices it charges its customers: in this case, price is the treatment variable and the customer’s decision about whether to buy a ticket is the outcome. There are two ways that a naive analysis could lead to incorrect counterfactual predictions. First, imagine that price varies in the training data because the airline gradually increases prices as a plane fills. Around holidays, more people want to fly and hence planes become fuller leading to higher prices. So, in our training set we observe examples with high prices and high sales. A direct ML approach might incorrectly predict that if the airline were to increase prices at other times in the year they would also observe increased sales, whereas the true relationship between price and sales is surely negative. Typically we can observe holidays, and include them in the model, so that we can correct for their effects. This case where an observable feature (holidays) is correlated with both the outcome and treatment variable is called “selection on observables”.\nBut say there is also sometimes high demand because of conferences, which the airline is not able to observe. This presents a more challenging problem: even if price distortions due to holidays were taken into account, a naive analysis could conclude that higher prices drive higher demand, whereas in fact high price is only correlated with high demand via the latent conference variable. This is the more challenging case of “selection on unobservables”.\nThe gold standard for establishing causal relationships is to conduct “AB” experiments, with subjects randomly assigned to different values of the treatment variable. In the airline example, this would mean assigning passengers random prices that do not depend on the number of seats sold. Enough data collected in this way would make it straightforward to identify the true relationship between price and demand even if the latent variable were never observed. However, such a strategy would be exceedingly expensive (the airline would sometimes turn away interested passengers for nearly-empty flights and offer steep discounts for nearly-full flights). Depending on the timescale over which such randomization were conducted, it could also hurt the airline’s long-term interests (because it could be perceived as unfair) and could fail if passengers were able to hide their identities (e.g., by logging in from a different computer if they didn’t like the price).\nThe alternative is to work with observational data, but doing so requires explicit assumptions about the causal structure of the DGP (Bottou et al., 2013). Most recent approaches to using machine learning methods such as trees (Wager & Athey, 2015; Athey & Imbens, 2016) and deep networks (Johansson et al., 2016; Shalit et al., 2016) for causal inference in observational data leverage an “unconfoundedness” assumption that the treatment is conditionally independent of any latent variables given the observed features. This amounts to assuming away selection on unobservables, which may or may not be reasonable depending on the setting.\nWe can do without this assumption and get around both types of selection if we can identify one or more instrumental variables (IVs) that only affect treatment assignment and not the outcome variable. In our airline example, the cost of fuel could be such an instrument: its variation is independent of demand for airline tickets and it affects sales only via ticket prices. Changes in the cost of fuel thus create movement in ticket prices that is independent of our latent variable, and this movement is as good as randomization for the purposes of causal inference. See Figure 1 for an graphical illustration of this example and of the general class of causal graphs that we consider.\nThe IV framework has a long history, especially in economics (e.g., Wright, 1928; Reiersøl., 1945). It provides methods for learning the regression function that relates the treatment and response variables under the “interventional distribution” for DGPs that conform to the graphical model shown in Figure 1 (Pearl, 2009). Most IV applications make use of a two-stage least squares procedure (2SLS; e.g., Angrist & Pischke, 2008) that applies a model of linear and homogeneous treatment effects (e.g., all airline customers must have the same price sensitivity). Nonparametric IV methods from the econometrics literature relax these assumptions (e.g., Newey & Powell, 2003; Darolles et al., 2011). However, these methods typically work by modeling the outcome as an unknown linear combination of a prespecified set of basis functions of the treatment and other covariates (e.g. Hermite polynomials, wavelets, or splines) and then modeling the conditional expectation of each of these basis functions in terms of the instruments (i.e., the number of parameters is quadratic in the number of basis functions). This requires a strong prior understanding of the DGP by the researcher; also, the complexity of both specification and estimation explodes when there are more than a handful of inputs.\nAdvances in deep learning have demonstrated the power of learning latent representations of complex features spaces (for recent surveys, see eg. LeCun et al., 2015; Schmidhuber, 2015). This paper’s goal is to use these powerful learning algorithms for IV analysis. We do this by breaking IV analysis into two supervised stages that can each be tar-\ngeted with deep networks and that, when solved, allow us to make counterfactual claims and perform causal inference. Specifically, we first model the conditional distribution of the treatment variable given the instruments and covariates and then target a loss function involving integration over the conditional treatment distribution from the first stage. Both stages use deep neural nets trained via stochastic gradient descent (Robbins & Monro, 1951; Bottou, 2010). We also present an out-of-sample causal validation procedure for selecting hyper-parameters of the models on a validation set. We refer to this setup as the Deep IV framework.\nSection 2 describes our general IV specification and its decomposition into two learning tasks. Section 3 outlines neural network estimation for these tasks with particular attention paid to the SGD routine used in model training and our causal validation procedure. Section 4 presents experimental results that illustrate the benefits of our methods."
  }, {
    "heading": "2. Counterfactual prediction",
    "text": "We aim to predict the value of some outcome variable y (e.g., sales in our airline example) under an intervention in a policy or treatment variable p (e.g., price). There exists a set of observable covariate features x (e.g., holidays) that we know affect both p and y. There also exist unobservable latent variables e that may affect x, p and y (e.g., conferences). Counterfactual prediction aims to recover E[y|do(p), x] in the context of the graphical model given by Figure 1, where the do(·) operator indicates that we have intervened to set the value of the policy variable p (as per Pearl, 2009). We assume the y is structurally determined by p, x and e as\ny = g (p, x) + e. (1)\nThat is, g (·) is some unknown and potentially non-linear continuous function of both x and p, and we assume that the latent variables (or “error”) e enter additively with unconditional mean Ee = 0. We allow for errors that are potentially correlated with the inputs: E[e|x, p] , 0 and, in particular, E[pe|x] , 0. Define the counterfactual prediction function\nh (p, x) ≡ g (p, x) +E[e|x], (2)\nwhich is the conditional expectation of y given the observables p and x, holding the distribution of e constant as p is changed. Note that we condition only on x and not p in the term E[e|x]; this term is typically nonzero, but it will remain constant under arbitrary changes to our policy variable p.2 Thus h (p, x) is the structural equation that we estimate. It is 2It may be easier to think about a setting where e ⊥ x, so that the latent error is simply defined as being due to factors orthogonal to the observable controls. In that case, h(p, x) = g (p, x). All of our results apply in either setup.\nuseful because to evaluate policy options (e.g. changing the ticket price from p0 to p1) we can look at the difference in outcomes h (p1, x)−h (p0, x) = g (p1, x)− g (p0, x). In standard supervised learning settings, the prediction model is trained to fitE[y|p, x]. This will typically be biased against our structural objective because\nE[y|p, x] = g (p, x) +E[e|p, x] , h (p, x) (3)\nsince our treatment is not independent of the latent errors by assumption and hence E[e|p, x] , E[e|x]. This object is inappropriate for policy analysis as it will lead to biased counterfactuals:\nE[y|p1, x]−E[y|p0, x] = g (p1, x)− g (p0, x) + ( E[e|p1, x]−E[e|p0, x] ) . (4)\nIn our airline example, high prices during conferences imply that E[e|p1, x]−E[e|p0, x] will be positive, resulting in the incorrect prediction that higher prices are associated with higher sales if this bias is sufficiently large.\nFortunately, the presence of instruments allows us to estimate an unbiased ĥ(p, x) that captures the structural relationship between p and y. These are sets of variables z that satisfy the following three conditions.\nRelevance F(p|x,z), the distribution of p given x and z, is not constant in z.\nExclusion z does not enter Eq. (1)—i.e., z ⊥ y | (x, p,e).\nUnconfounded Instrument z is conditionally independent of the error—i.e., z ⊥ e | x.3\n3Under the additive error assumption made in Eq. (1), unconfoundedness of the instrument is not necessary: we could replace this assumption with the weaker mean independence assumption E[e|x,z] = 0 without changing anything that follows. We use the stronger assumption to facilitate extensions, e.g. to estimating counterfactual quantiles. Our assumption is similar to the ‘unconfoundedness’ assumption in the Neyman–Rubin potential outcomes framework (Rosenbaum & Rubin, 1983) (i.e. p⊥ e | x). But our assumption is weaker—in particular, we allow for p 6⊥ e|x— and so the matching and propensity-score re-weighting approaches often used in that literature will not work here.\nTaking the expectation of both sides of Equation (1) conditional on [x,z] and applying these assumptions establishes the relationship (cf. Newey & Powell, 2003):\nE[y|x,z] = E[ g (p, x)|x,z] +E[e|x]\n= ∫ h (p, x)dF(p|x,z), (5)\nwhere, again, dF(p|x,z) is the conditional treatment distribution. The relationship in Equation (5) defines an inverse problem for h in terms of two directly observable functions: E[y|x,z] and F(p|x,z). IV analysis typically splits this into two stages: first estimating F̂(p|xt,zt) ≈ F(p|xt,zt), and then estimating ĥ after replacing F with F̂. Most existing approaches to IV analysis assume linear models for the treatment density function F̂ and the counterfactual prediction function ĥ to solve Equation (5) in closed form. For example, the two-stage least-squares (2SLS) procedure (e.g., Angrist et al., 1996) posits y = γp + xβy + e and p = τz+ xβp +v, with the assumptions that E[e|x,z] = 0, E[v|x,z] = 0, andE[ev] , 0 (which impliesE[ep] , 0). This procedure is straightforward: fit a linear model for p given x and z and use the predicted values p̂ in a second linear model of y. This is a statistically efficient way to estimate the effect of the policy variable (i.e. γ) as long as two strong assumptions hold: linearity (i.e., both first- and second-stage regressions are correctly specified) and homegeneity (i.e., the policy affects all individuals in the same way).4\nFlexible nonparametric extensions of 2SLS either replace the linear regressions with a linear projection onto a series of known basis functions (Newey & Powell, 2003; Blundell et al., 2007; Chen & Pouzo, 2012) or use kernel-based methods as in Hall & Horowitz (2005) and Darolles et al. (2011). This system of series estimators is an effective strategy for introducing flexibility and heterogeneity with low dimensional inputs, but the approach faces the same limitations as kernel methods in general: their performance depends on the choice of kernel function; and they often become com-\n4The estimated γ̂ remains interpretable as a ‘local average treatment effect’ (LATE) under less stringent assumptions (see Angrist et al., 1996, for an overview).\nputationally intractable in high-dimensional feature spaces [x,z] or with large numbers of training examples."
  }, {
    "heading": "3. Estimating and validating DeepIV",
    "text": "We now describe how to use deep networks to perform flexible, scalable IV analysis in a framework we call DeepIV. We make two contributions that are each necessary components of the approach. First, we propose a loss function and optimization procedure that allows us to optimize deep networks for counterfactual prediction. Second, we describe a general procedure for out-of-sample validation of two-stage instrument variable methods. This allows us to perform causally valid hyper-parameter optimization, which in general is necessary for achieving good predictive performance using deep networks.\nOur approach is conceptually simple given the counterfactual prediction framework described in Section 2. Rather than constraining ourselves to analytic solutions to the integral in Equation (5), we instead directly optimize our estimate of the structural equation, ĥ . Specifically, to minimize `2 loss given n data points and given a function space H (which may not include the true h ), we solve\nmin ĥ∈H n∑ t=1 ( yt − ∫ ĥ (p, xt)dF(p|xt,zt) )2 . (6)\nSince the treatment distribution is unknown, we estimate F̂(p|x,z) in a separate first stage. So the DeepIV procedure has two stages: a first stage density estimation procedure to estimate F̂(p|x,z) and a second that optimizes the loss function described in Equation (6). In both stages hyper-parameters can be chosen to minimize the respective loss functions on a held out validation set, and improvements in performance against this metric will correlate with improvements on the true structural loss which cannot be evaluated directly. We briefly discuss these two stages before describing our methods for optimizing the loss given in Equation (6) and our causal validation procedure.\nFirst stage: Treatment network In the first stage we learn F(p|x,z) using an appropriately chosen distribution parameterized by a deep neural network (DNN), say F̂ = Fφ(p|x,z) where φ is the set of network parameters. Since we will be integrating over Fφ in the second stage, we must fully specify this distribution.\nIn the case of discrete p, we model Fφ(p|x,z) as a categorical Cat (p | π(x,z;φ)) with p(p = pk) = πk(x,z;φ) for each treatment category pk and where πk(x,z;φ) is given by a DNN with softmax output. For continuous treatment, we model F as a mixture of Gaussian distributions where component weights πk(x,z;θ) and parameters [ µk(x,z;φ),σk(x,z;φ)\n] form the final layer of a neural network parametrized by\nφ. This model is known as a mixture density network, as detailed in §5.6 of Bishop (2006). With enough mixture components it can approximate arbitrary smooth densities. To obtain mixed continuous–discrete distributions we replace some mixture components with point masses. In each case, fitting Fφ is a standard supervised learning problem.\nSecond stage: Outcome network In the second stage, our counterfactual prediction function h is approximated by a DNN with real-valued output, say hθ. We optimize network parameters θ to minimize the integral loss function in Equation (6) over training data D of size T = |D| from the joint DGPD,\nL(D;θ) = |D|−1 ∑\nt\n( yt − ∫ hθ(p, xt)dF̂φ(p|xt,zt) )2 . (7)\nNote that this loss involves the estimated treatment distribution function, F̂φ, from our first stage.5"
  }, {
    "heading": "3.1. Optimization for DeepIV networks",
    "text": "We use stochastic gradient descent (SGD; see algorithms in, e.g., Duchi et al., 2011; Kingma & Ba, 2014) to train the network weights. For Fφ, standard off-the-shelf methods apply, but second stage optimization (for hθ) needs to account for the integral in Equation (7). SGD convergence only requires that each sampled gradient ∇θLt is unbiased for the population gradient, ∇θL(D;θ). Lower variance for ∇θLt will tend to yield faster convergence (Zinkevich, 2003) while the computational efficiency of SGD on large datasets requires limiting the number of operations going into each gradient calculation (Bousquet & Bottou, 2008).\nWe can approximate the integral with respect to a probability measure with the average of draws from the associated probability distribution: ∫ h(p)dF(p) ≈ B−1 ∑b h(pb) for {pb}B1 iid∼ F. Hence we can get an unbiased estimate of Equation (7) by replacing the integral with a sum over samples from our fitted treatment distribution function, F̂φ:\nL(D;θ)≈ |D|−1 ∑\nt yt − 1B ∑ ṗ∼F̂φ(p|xt ,zt) hθ( ṗ, xt)  2 := L̂(D;θ).\n(8) This equation can be used to estimate ∇θLwith an important caveat: if we want to maintain unbiased gradient estimates, independent samples must be used for each instance of the integral in the gradient calculation. To see this, note that the\n5 We can replace Eq. (7) with other functions, e.g., a softmax for categorical outcomes, but use `2 loss for most of our exposition.\ngradient of Equation (8) has expectation ED∇θLt = −2ED ( EFφ(p|xt ,zt) [ yt −hθ(pk, xt) ] ·EFφ(p|xt ,zt) [ h ′θ(pk, xt) ]) (9)\n, −2EDEFφ(p|xt ,zt) [( yt −hθ(pk, xt) ) h ′θ(pk, xt) ] ,\nwhere the inequality holds so long as covFφ(p|xt ,zt) [( yt −hθ(pk, xt) ) h ′θ(pk, xt) ] , 0. We thus need a gradient estimate based on unbiased MC estimates for each EFφ(p|xt ,zt) term in Equation (9). We obtain such an estimate by taking two samples { ṗb}B1 , { p̈b}B1 iid∼ Fφ(p|xt,zt) and calculating the gradient as\n∇̂BθLt ≡ −2 yt −B−1 ∑\nb\nhθ( ṗb, xt) B−1 ∑\nb\nh ′θ( p̈b, xt). (10)\nIndependence of the two samples ensures that E∇̂BθLt = ED∇θLt = ∇θL(D;θ), as desired. The variance of our estimate depends on B, the number of samples that we draw. Each of these samples is relatively expensive to compute because they require a forward pass through the network ĥθ(ṗ, xt). If this varies significantly with ṗ we might need a large number of samples to get a low-variance estimate of the gradient, which is computationally intensive.\nAn alternative is to optimize an upper bound on Equation (8). By using Jensen’s inequality and the fact that the squared error function is convex we get that\nL̂(D;θ) ≤ |D|−1 ∑\nt ∑ ṗ∼F̂φ(p|xt ,zt) ( yt −hθ( ṗ, xt) )2 . (11)\nTaking the RHS of Equation (11) as the objective and calculating the gradient leads to a version of Equation (10) in which a single draw can be used instead of two independent draws. This objective is easy to implement in practice as it just involves drawing samples during training. This is well-supported in deep network implementations because it is analogous to the data augmentation procedures that are commonly used to encourage invariance in deep networks. The analogy is more than just aesthetic—by optimizing this loss, we are essentially encouraging the network to be invariant to variations in the treatment that cannot be explained by our features and instrument. This encourages the network to ignore the effects of unobserved confounding variables.\nHowever, we do not have theoretical guarantees that optimizing this upper bound onL(D;θ) leads to good counterfactual performance. While it may converge more quickly because it exhibits lower variance, it will have worse asymptotic performance as it only approximates the desired loss function. We evaluated this tradeoff experimentally by comparing optimizing the upper bound to the more computationally\nexpensive unbiased procedure. In our simulations, we found that upper bound loss tended to give better performance under practical computational limitations.\nDiscrete treatment and outcome spaces The discussion thus far has focused on continuous treatment and outcome spaces because they are more challenging mathematically. When the treatment space is discrete and low dimensional, Fφ is modeled as a categorical response so the gradient of Equation (7) can be expressed exactly as\n∇θLt = −2 yt −∑\nk\nπk(xt,zt;φ)hθ(pk, xt) \n· ∑\nk\nπk(xt,zt;φ)h ′θ(pk, xt). (12)\nThus when the outcome space is discrete, we also have less to worry about with respect to bias in gradient updates. For discrete outcomes, we use the softmax loss and can use single-sample MC gradient estimates without introducing bias or large amounts of variance because the gradient with respect to the softmax loss does not involve a product of random variables."
  }, {
    "heading": "3.2. Causal validation",
    "text": "There is a widespread belief that “[s]tandard methods for hyperparameter selection, such as cross-validation, are not applicable when there are no samples of the counterfactual outcome” (Shalit et al., 2016). This would be a significant problem in our setting because out-of-sample (OOS) validation procedures are crucial for tuning deep network hyper-parameters and optimization rules. Fortunately, both steps in our Deep IV procedure can be validated by simply evaluating the respective losses on held out data. Consider a held-out dataset Dh-o. Our first stage, fitting Fφ, is a standard density estimation problem and can be tuned to minimize the OOS deviance criterion\nmin φ ∑ dl∈Dh-o − log fφ(pl|xl,zl), (13)\nwhere fφ is either the probability mass function or density function associated with Fφ, as appropriate. Second stage validation proceeds conditional upon a fitted Fφ, and we seek to minimize the held-out loss criterion\nmin θ ∑ dl∈Dh-o ( yl− ∫ hθ(p, xl)dFφ(p|xl,zl) )2 . (14)\nThe integral here can either be exact or MC approximate via sampling from Fφ.\nEach stage is evaluated in turn, with second stage validation using the best-possible network as selected in the first stage.\nThis procedure guards against the ‘weak instruments’ bias (Bound et al., 1995) that can occur when the instruments are only weakly correlated with the policy variable. To see why, consider the worst case where the instruments are independent of the policy variable, i.e., F(p|z) = F(p). Without validation, a sufficiently powerful model will perfectly overfit by approximating the conditional distribution F(p|zi) with a point mass at pi. As a result, the causal loss function will approximate the standard prediction loss, leading to spurious conclusions. By contrast, the expected first-stage loss minimizer on the validation set is the model that best approximates the unconditional distribution F(p), and given that, the second stage minimizer best approximates g(p) = ȳ which predicts no relationship when there is no evidence in favor of it. That said, it should be noted that these criteria provide relative performance guidance: improving on each criterion will improve performance on counterfactual prediction problems, but without giving any information about how far hθ(p, x) is from true h (p, x). Our causal validation procedure is sequential: stage two’s validation depends on the model chosen in the first stage. This greedy procedure is computationally efficient and causally valid, but potentially suboptimal because the firststage loss is not of independent interest. Peysakhovich & Eckles (2017) concurrently developed a casual validation framework that considers both stages jointly in domains with categorical instruments and no observable features. These assumptions are too restrictive for the problems we consider, but it would be interesting to investigate whether there exists a joint optimization approach that offers practical benefits under more permissive assumptions."
  }, {
    "heading": "4. Experiments",
    "text": "We evaluated our approach on both simulated and real data. We used simulations to assess DeepIV’s ability to recover an underlying counterfactual function both in a lowdimensional domain with informative features and in a highdimensional domain with features consisting of pixels of a handwritten image. We compared our approach to 2SLS and to a standard feed-forward network, evaluated the effectiveness of hyper-parameter optimization, and contrasted our biased and unbiased loss functions with various numbers of samples underlying the SGD step. We also considered a real-world dataset where ground truth was not available, showing that we could replicate the findings of a previous study in a dramatically more automatic fashion."
  }, {
    "heading": "4.1. Simulations",
    "text": "Our simulation models a richer version of the airline example described in Section 1. We assume that there are 7 customer types s ∈ {1, ...,7} that each exhibit different levels of price sensitivity. We model the holiday effect on\nsales by letting the customer’s price sensitivity vary continuously throughout the year according to a complex non-linear function, ψt = 2 ( (t−5)4/600 + exp [ −4(t−5)2 ] + t/10−2 ) . The time of year t is an observed variable, generated as t ∼ unif(0,10). Prices are a function of ψt and of the fuel price z, with the motivation that they are chosen strategically by the airline in order to move with average price sensitivity. In our example, the high demand that results from conferences breaks the conditional independence between our treatment variable p and the latent effects e, thereby violating the “unconfoundedness” assumption. We model this abstractly by generating our latent errors e with a parameter ρ that allows us to smoothly vary the correlation between p and e. Sales y are then generated as\ny = 100 + (10 + p)sψt −2p + e, p = 25 + (z + 3)ψt + v z, v ∼ N(0,1) and e ∼ N(ρv,1−ρ2).\nOur target counterfactual function is h(t, s, p) = (10 + p)sψt − 2p. To evaluate the model, we consider the counterfactual question, “What would sales have been if prices had been changed to p′?” Thus the price in our test set is set deterministically over a fixed grid of price values that spans the range of training set prices. The observed features [t, s] are sampled as in the original data generating process and we compare estimated ĥ against the ground truth h .\nLow dimensional domain We evaluated structural mean square error (MSE) while varying both the number of training examples and ρ, the correlation between e and p. In addition to Deep IV, we considered a regular feed-forward network (FFNet) with the same architecture as our outcome network, a non-parametric IV polynomial kernel regression (NonPar, Darolles et al., 2011) using Hayfield et al. (2008)’s R implementation, and standard two-stage least squares (2SLS). Full details of model architectures and hyperparameter choices for all the models are given in the Appendix.\nThe results are summarized in Figure 2. The performance of NonPar, of 2SLS, and of our Deep IV model was mostly unaffected by changes in ρ, reflecting the fact that these models are designed to be resilient to unobserved confounders. For 1000 data points, NonPar’s mean performance was better than 2SLS but failed to match DeepIV. Because of NonPar’s excessive computational requirements we were not able to fit it to the larger datasets. 2SLS is constrained by its homogeneity and linearity assumptions, and so did not improve with increasing amounts of data. Adding regularized polynomial basis functions to 2SLS (2SLS(poly)) gives some empirical improvements in performance over 2SLS on larger datasets but the procedure is not causally valid because it violates 2SLS’s linearity assumptions. Both forms of 2SLS performed far better than FFNet which did a\ngood job of estimating h (t, s, p)+E[e|p] but a terrible job of recovering the true counterfactual. As ρ dropped, decreasing E[e|p], FFNet’s performance improved but even with low levels of correlation between p and e it remained far worse than simple 2SLS. This occurred because we evaluated the models with respect to a fixed grid of treatment values which induced a covariate shift at test time. In contrast, Deep IV was the best performing model throughout and its performance improved as the amount of data grew.\nHigh dimensional feature space In real applications, we do not typically get to observe variables like customer type that cleanly delineate our training examples into explicit\nclasses, but may instead observe a large number of features that correlate with such types. To simulate this, we replaced the customer type label s ∈ {0,1, ...,7} with the pixels of the corresponding handwritten digit from the MNIST dataset (LeCun & Cortes, 2010). The task remained the same, but the model was no longer explicitly told that there were 7 customer types and instead had to infer the relationship between the image data and the outcome.\nIn this far more challenging domain, performance is sensitive to the choice of hyper-parameters, necessitating optimization on a validation set. Figure 3 shows an evaluation of the appropriateness of our loss function for hyper-parameter tuning, comparing our validation-set loss after grid search over Dropout and L2-regularization parameters to test set loss. We found a clear linear relationship between the losses; the best performing validation set model was among the best five performing models under the true causal loss.\nWe can get an upper bound on the performance of a particular model architecture by comparing its performance to the same architecture trained on data from a simulated randomized experiment on the same data-generating process. We simulated this by generating the outcome y with independent noise e and by generating p uniformly at random over its support. Thus the controlled model had to solve a standard supervised learning problem where errors were generated independently and there was no test-time covariate shift. As before, the naive deep network also shared the same architecture in addition to taking the instrument as input. This experiment showed that DeepIV was able to make up most of the loss in counterfactual prediction performance that the naive network suffered by not accounting for the causal prediction problem. However, there was still a gap in performance: with 20 000 data points the controlled experiment achieved an average mean squared error of 0.20 while DeepIV managed 0.32.\nPerformance of the unbiased loss relative to the upper bound loss We tested how our two approaches to optimization affected performance on the image task with 20 000 training examples. The results are summarized in Table 1. The upper bound loss loss gave significantly better performance than all but the unbiased loss version based on 16 samples, without requiring multiple passes through the network to evaluate the gradient. Thus, the bias introduced by the upper bound did not meaningfully degrade counterfactual performance in our experiments."
  }, {
    "heading": "4.2. Application: Search-advertisement position effects",
    "text": "Our experiments so far have considered synthetic data. We now evaluate the utility of our approach on real data for which we do not have access to ground truth. This means that we cannot evaluate models in terms of their predictions; instead, we show that we can replicate the results of a previously published study in a dramatically more automated fashion. Specifically, we examine how advertiser’s position on the Bing search page (their “slot”) affects the probability of a user click, allowing for different treatment effects for different advertiser-query pairs. For example, we aim to detect differences in the importance of ad position when Coke bids on the word “Coke” (an “on-brand query”) versus when Pepsi bids on “Coke” (an “off-brand query”) versus when Coke bids on “www.coke.com” (an “on-nav query”, occurring when a user types a url in the search box by mistake) versus when Pepsi bids on “www.coke.com” (an “off-nav query”). This question was studied extensively by Goldman & Rao (2014) using a nonparametric IV estimation approach that involved a detailed construction of optimal instruments, as well as a separate hand-coded classification of advertiser–query pairs into the four categories above.\nOur goal is to replicate these results in an automated fashion. Advertiser position is correlated with latent user intent (e.g. when a user searches for “Coke”, it is likely both that they will click and that “Coke” will be the top advertiser), so we need instruments to infer causation. The instruments proposed by Goldman & Rao (2014) are a series of indicators for experiments run by Bing in which advertiser–query pairs\nwere randomly assigned to different algorithms that in turn scored advertisers differently in search auctions, resulting in random variation in position. Our estimation algorithm takes the experiment ID directly as an instrument.\nAs features, we gave the deep nets the query url and user query as text. The url was parsed into tokens on dashes and dots and these tokens were then parsed on punctuation and whitespace. Given the outcome variable (an indicator for user click), the features, and the instruments, we applied our methodology directly to the data without any of the additional feature engineering and construction of optimal instruments performed by Goldman & Rao (2014). This approach was tractable despite the fact that the dataset contained over 20 million observations. Figure 4 shows the results. We were able to replicate the original study’s broad findings, namely that (i) that for “on-brand” queries, position was worth more to small websites; and (ii) that the value of position for on-nav queries was much smaller than for off-nav queries. A naive non-causal regression found an unrealistic average treatment effect (ATE; across sampled advertisers and queries) drop in click rate of 70% from 1st to 2nd position; in contrast, the causal estimate of the ATE was a more modest 12% drop."
  }, {
    "heading": "5. Discussion",
    "text": "We have presented DeepIV, an approach that leverages instrument variables to train deep networks that directly minimize the counterfactual prediction error and validate the resulting models on held-out data. DeepIV significantly reduced counterfactual error measured in simulation experiments and was able to replicate previous IV experiments without extensive feature engineering. In future work, we plan to discuss interference techniques for the DeepIV framework and explore how this approach generalizes to other causal graphs given appropriate assumptions."
  }, {
    "heading": "Acknowledgements",
    "text": "We would like to thank Susan Athey, Xiaohong Chen and Demian Pouzo for their helpful discussions and the anonymous reviewers for their useful comments on the paper. We would also like to thank Holger Hoos for the use of the Ada cluster, without which the experiments would not have been possible."
  }],
  "year": 2017,
  "references": [{
    "title": "Identification of causal effects using instrumental variables",
    "authors": ["J.D. Angrist", "G.W. Imbens", "D.B. Rubin"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1996
  }, {
    "title": "Mostly harmless econometrics: An empiricist’s companion",
    "authors": ["Angrist", "Joshua D", "Pischke", "Jörn-Steffen"],
    "venue": "Princeton university press,",
    "year": 2008
  }, {
    "title": "Recursive partitioning for heterogeneous causal effects",
    "authors": ["Athey", "Susan", "Imbens", "Guido"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Pattern Recognition and Machine Learning",
    "authors": ["Bishop", "Christopher M"],
    "year": 2006
  }, {
    "title": "Semi-nonparametric iv estimation of shape-invariant engel curves",
    "authors": ["Blundell", "Richard", "Chen", "Xiaohong", "Kristensen", "Dennis"],
    "year": 2007
  }, {
    "title": "Large-scale machine learning with stochastic gradient descent",
    "authors": ["L. Bottou"],
    "venue": "In Proceedings of COMPSTAT’2010,",
    "year": 2010
  }, {
    "title": "Problems with instrumental variables estimation when the correlation between the instruments and the endogenous explanatory variable is weak",
    "authors": ["Bound", "John", "Jaeger", "David A", "Baker", "Regina M"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1995
  }, {
    "title": "The tradeoffs of large scale learning. In Advances in neural information processing systems (NIPS)",
    "authors": ["Bousquet", "Olivier", "Bottou", "Léon"],
    "year": 2008
  }, {
    "title": "Estimation of nonparametric conditional moment models with possibly nonsmooth generalized residuals",
    "authors": ["X. Chen", "D. Pouzo"],
    "year": 2012
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2011
  }, {
    "title": "Experiments as instruments: heterogeneous position effects in sponsored search auctions",
    "authors": ["Goldman", "Mathew", "Rao", "Justin M"],
    "year": 2014
  }, {
    "title": "Nonparametric methods for inference in the presence of instrumental variables",
    "authors": ["Hall", "Peter", "Horowitz", "Joel L"],
    "venue": "Ann. Statist., 33(6):2904–2929,",
    "year": 2005
  }, {
    "title": "Nonparametric econometrics: The np package",
    "authors": ["Hayfield", "Tristen", "Racine", "Jeffrey S"],
    "venue": "Journal of statistical software,",
    "year": 2008
  }, {
    "title": "Learning representations for counterfactual inference",
    "authors": ["F.D. Johansson", "U. Shalit", "D. Sontag"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning, (ICML),",
    "year": 2016
  }, {
    "title": "ADAM: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "International Conference on Learning Representations (ICLR),",
    "year": 2014
  }, {
    "title": "URL http://yann.lecun.com/ exdb/mnist",
    "authors": ["LeCun", "Yann", "Cortes", "Corinna"],
    "venue": "MNIST handwritten digit database",
    "year": 2010
  }, {
    "title": "Instrumental variable estimation of nonparametric models",
    "authors": ["W.K. Newey", "J.L. Powell"],
    "year": 2003
  }, {
    "title": "Learning causal effects from many randomized experiments using regularized instrumental variables",
    "authors": ["A. Peysakhovich", "D. Eckles"],
    "venue": "ArXiv e-prints,",
    "year": 2017
  }, {
    "title": "Confluence analysis by means of instrumental sets of variables",
    "authors": ["O. Reiersøl"],
    "venue": "Arkiv för Matematik, Astronomi och Fysik,",
    "year": 1945
  }, {
    "title": "A stochastic approximation method",
    "authors": ["H. Robbins", "S. Monro"],
    "venue": "The Annals of Mathematical Statistics,",
    "year": 1951
  }, {
    "title": "Assessing sensitivity to an unobserved binary covariate in an observational study with binary outcome",
    "authors": ["P.R. Rosenbaum", "D.B. Rubin"],
    "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
    "year": 1983
  }, {
    "title": "Deep learning in neural networks: An overview",
    "authors": ["J. Schmidhuber"],
    "venue": "Neural Networks,",
    "year": 2015
  }, {
    "title": "Estimating individual treatment effect: generalization bounds and algorithms",
    "authors": ["U. Shalit", "F. Johansson", "D. Sontag"],
    "venue": "ArXiv e-prints,",
    "year": 2016
  }, {
    "title": "Inference of heterogeneous treatment effects using random forests",
    "authors": ["Wager", "Stefan", "Athey", "Susan"],
    "year": 2015
  }, {
    "title": "The Tariff on Animal and Vegetable Oils",
    "authors": ["P.G. Wright"],
    "year": 1928
  }, {
    "title": "Online convex programming and generalized infinitesimal gradient ascent",
    "authors": ["Zinkevich", "Martin"],
    "venue": "In Proceedings of the Twentieth International Conference on Machine Learning (ICML),",
    "year": 2003
  }],
  "id": "SP:94d7652764a6cce9cd25c8c5bf8c6ed6785c42b0",
  "authors": [{
    "name": "Jason Hartford",
    "affiliations": []
  }, {
    "name": "Greg Lewis",
    "affiliations": []
  }, {
    "name": "Kevin Leyton-Brown",
    "affiliations": []
  }, {
    "name": "Matt Taddy",
    "affiliations": []
  }],
  "abstractText": "Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs)—sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework1 allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.",
  "title": "Deep IV: A Flexible Approach for Counterfactual Prediction"
}