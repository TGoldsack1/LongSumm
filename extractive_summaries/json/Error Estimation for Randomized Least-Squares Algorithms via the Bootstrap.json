{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Randomized sketching algorithms have been intensively studied in recent years as a general approach to computing fast approximate solutions to large-scale least-squares (LS) problems (Drineas et al., 2006; Rokhlin & Tygert, 2008; Avron et al., 2010; Drineas et al., 2011; Mahoney, 2011;\n1Department of Statistics, UC Davis 2ICSI and Department of Statistics, UC Berkeley. Correspondence to: <melopes@ucdavis.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nDrineas et al., 2012; Clarkson & Woodruff, 2013; Woodruff, 2014; Ma et al., 2014; Meng et al., 2014; Pilanci & Wainwright, 2015; 2016). During this time, much progress has been made in analyzing the performance of these algorithms, and existing theory provides a good qualitative description of approximation error (relative to the exact solution) in terms of various problem parameters. However, in practice, the user rarely knows the actual error of a randomized solution, or how much extra computation may be needed to achieve a desired level of accuracy.\nA basic source of this problem is that it is difficult to translate theoretical error bounds into numerical error bounds that are tight enough to be quantitatively meaningful. For instance, theoretical bounds are often formulated to hold for the worst-case input among a large class of possible inputs. Consequently, they are often pessimistic for “generic” problems, and they may not account for the structure that is unique to the input at hand. Another practical issue is that these bounds typically involve constants that are either conservative, unspecified, or dependent on unknown parameters.\nIn contrast with worst-case error bounds, we are interested in “a posteriori” error estimates. By this, we mean error bounds that can be estimated numerically in terms of the computed solution or other observable information. Although methods for obtaining a posteriori error estimates are well-developed in some areas of computer science and applied mathematics, there has been very little development for randomized sketching algorithms (cf. Section 1.4). (For brevity, we will usually omit the qualifier ‘a posteriori’ from now on when referring to error estimation.)\nThe main purpose of this paper is to show that it is possible to directly estimate the error of randomized LS solutions in a way that is both practical and theoretically-justified. Accordingly, we propose a flexible estimation method that can enhance existing sketching algorithms in a variety of ways. In particular, we will explain how error estimation can help the user to (1) select the “sketch size” parameter, (2) assess the convergence of iterative sketching algorithms, and (3) measure error in a wider range of metrics than can be handled by existing theory."
  }, {
    "heading": "1.1. Setup and Background",
    "text": "Consider a large, overdetermined LS problem, involving a rank d matrix A ∈ Rn×d, and a vector b ∈ Rn, where n d. These inputs are viewed as being deterministic, and the exact solution is denoted\nxopt := argmin x∈Rd\n‖Ax− b‖2. (1)\nThe large number of rows n is often a major computational bottleneck, and sketching algorithms overcome this obstacle by effectively solving a smaller problem involving m rows, where d m n. In general, this reduction is carried out with a random sketching matrix S ∈ Rm×n that maps the full matrix A into a smaller sketched matrix Ã := SA of size m× d. However, various sketching algorithms differ in the way that the matrix S is generated, or the way that Ã is used. Below, we quickly summarize three of the most well-known types of sketching algorithms for LS.\nClassic Sketch (CS). For a given sketching matrix S, this type of algorithm produces a solution\nx̃ := argmin x∈Rd\n‖S(Ax− b)‖2, (2)\nand chronologically, this was the first type of sketching algorithm for LS (Drineas et al., 2006).\nHessian Sketch (HS). The HS algorithm modifies the objective function in the problem (1) so that its Hessian is easier to compute (Pilanci & Wainwright, 2016; Becker et al., 2017), leading to a solution\nx̆ := argmin x∈Rd\n{ 1 2‖SAx‖ 2 2 − 〈A>b, x〉 } . (3)\nThis algorithm is also called “partial sketching”.\nIterative Hessian Sketch (IHS). One way to extend HS is to refine the solution iteratively. For a given iterate x̂i ∈ Rd, the following update rule is used\nx̂i+1 := argmin x∈Rd\n{ 1 2 ‖Si+1A(x− x̂i)‖22 + 〈A>(Ax̂i − b) , x〉 } ,\nwhere Si+1 ∈ Rm×n is a random sketching matrix that is generated independently of S1, . . . , Si, as proposed in (Pilanci & Wainwright, 2016). If we let t ≥ 1 denote the total number of IHS iterations, then we will generally write x̂t to refer to the final output of IHS.\nRemark. If the initial point for IHS is chosen as x̂0 = 0, then the first iterate x̂1 is equivalent to the HS solution x̆ in equation (3). Consequently, HS may be viewed as a special case of IHS, and so we will restrict our discussion to CS and IHS for simplicity.\nWith regard to the choice of the sketching matrix, many options have been considered in the literature, and we refer to the surveys (Mahoney, 2011) and (Woodruff, 2014).\nTypically, the matrix S is generated so that the relation E[S>S] = In holds, and that the rows of S are i.i.d. random vectors (or nearly i.i.d.). Conceptually, our proposed method only relies on these basic properties of S, and in practice, it can be implemented with any sketching matrix.\nTo briefly review the computational benefits of sketching algorithms, first recall that the cost of solving the full leastsquares problem (1) by standard methods isO(nd2) (Golub & Van Loan, 2012). On the other hand, if the cost of computing the matrix product SA is denoted Csketch, and if a standard method is used to solve the sketched problem (2), then the total cost of CS isO(md2 +Csketch). Similarly, the total cost of IHS with t iterations is O(t(md2 + Csketch)). Regarding the sketching cost Csketch, it depends substantially on the choice of S, but there are many types that improve upon the naive O(mnd) cost of unstructured matrix multiplication. For instance, if S is chosen to be a Sub-sampled Randomized Hadamard Transform (SRHT), then Csketch = O(nd log(m)) (Ailon & Chazelle, 2006; Sarlós, 2006; Ailon & Liberty, 2009). Based on these considerations, sketching algorithms can be more efficient than traditional LS algorithms when md2 + nd log(m) nd2."
  }, {
    "heading": "1.2. Problem Formulation",
    "text": "For any problem instance, we will estimate the errors of the random vectors x̃ and x̂t in terms of high-probability bounds. Specifically, if we let ‖ · ‖◦ denote any norm on Rd, and let α ∈ (0, 1) be fixed, then our goal is to construct numerical estimates ε̃(α) and ε̂t(α), such that the bounds\n‖x̃− xopt‖◦ ≤ ε̃(α) (4)\n‖x̂t − xopt‖◦ ≤ ε̂t(α) (5)\neach hold with probability at least 1− α. (This probability will account for the randomness in both the sketching algorithm, and the bootstrap sampling described below.) Also, the algorithm for computing ε̃(α) or ε̂t(α) should be efficient enough so that the total cost of computing (x̃, ε̃(α)) or (x̂t, ε̂t(α)) is still much less than the cost of computing the exact solution xopt — otherwise, the extra step of error estimation would defeat the purpose of sketching. (This cost will be addressed in Section 2.3.) Since xopt is unknown to the user, it might seem surprising that it is possible to construct error estimates that satisfy the conditions above, and indeed, the limited knowledge of xopt is the main source of difficulty in the problem."
  }, {
    "heading": "1.3. Main Contributions",
    "text": "At a high level, a distinguishing feature of our approach is that it applies inferential ideas from statistics in order to enhance large-scale computations. To be more specific, the novelty of this approach is that it differs from the tra-\nditional framework of using bootstrap methods to quantify uncertainty arising from data (Davison & Hinkley, 1997). Instead, we are using these methods to quantify uncertainty in the outputs of randomized algorithms — and there do not seem to be many works that have looked at the bootstrap from this angle. From a more theoretical standpoint, another main contribution is that we offer the first guarantees for a posteriori error estimation in the setting of randomized LS algorithms (to the best of our knowledge). Looking beyond this setting, there may be further opportunities for using bootstrap methods to estimate the errors of other randomized algorithms. In concurrent work, we have taken this approach in the distinct settings of randomized matrix multiplication, and randomized ensemble classifiers (Lopes et al., 2017; Lopes, 2018)."
  }, {
    "heading": "1.4. Related work",
    "text": "The general problem of error estimation for approximation algorithms has been considered in a wide range of situations, and we refer to the following works for surveys and examples: (Pang, 1987; Verfürth, 1994; Jiránek et al., 2010; Ainsworth & Oden, 2011; Colombo & Vlassis, 2016). In the context of sketching algorithms, there is only a handful of papers that address error estimation, and these are geared toward low-rank approximation (Liberty et al., 2007; Woolfe et al., 2008; Halko et al., 2011), or matrix multiplication (Sarlós, 2006; Lopes et al., 2017). In addition to the works just mentioned, the recent preprint (Ahfock et al., 2017) explores statistical properties of the CS and HS algorithms, and it develops analytical formulas for describing how x̃ and x̆ fluctuate around xopt. Although these formulas offer insight into error estimation, their application is limited by the fact that they involve unknown parameters. Also, the approach in (Ahfock et al., 2017) does not address IHS. Lastly, error estimation for LS approximations can be studied from a Bayesian perspective, and this has been pursued in the paper (Bartels & Hennig, 2016), but with a focus on algorithms that differ from the ones studied here.\nNotation. The following notation is needed for our proposed algorithms. Let b̃ := Sb ∈ Rm denote the sketched version of b. If i = (i1, . . . , im) is a vector containing m numbers from {1, . . . ,m}, then Ã(i, :) refers to the m× d matrix whose jth row is equal to the ij th row of Ã. Similarly, the jth component of the vector b̃(i) is the ij th component of b̃. Next, for any fixed α ∈ (0, 1), and any finite set of real numbers C = {c1, . . . , ck}, the expression quantile(c1, . . . , ck; 1 − α) is defined as the smallest element ci0 ∈ C for which the sum 1k ∑k i=1 1{ci ≤ ci0} is at least 1 − α. Lastly, the distribution of a random variable U is denoted L(U), and the conditional distribution of U given a random variable V is denoted L(U |V )."
  }, {
    "heading": "2. Method",
    "text": "The proposed bootstrap method is outlined in Sections 2.1 and 2.2 for the cases of CS and IHS respectively. After the method is presented in algorithmic form (for each case), we give heuristic interpretations to explain why it works. The formal analysis can be found in the proof of Theorem 1 in the supplement (Lopes et al., 2018). Later on, in Section 2.3, we discuss computational cost and speedups."
  }, {
    "heading": "2.1. Error Estimation for CS",
    "text": "The main challenge we face is that the distribution of the random variable ‖x̃− xopt‖◦ is unknown. If we had access to this distribution, we could find the tightest possible upper bound on ‖x̃ − xopt‖◦ that holds with probability at least 1− α. (This bound is commonly referred to as the (1− α)quantile of the random variable ‖x̃− xopt‖◦.)\nFrom an intuitive standpoint, the idea of the proposed bootstrap method is to artificially generate many samples of a random vector, say x̃∗, whose fluctuations around x̃ are statistically similar to the fluctuations of x̃ around xopt. In turn, we can use the empirical (1−α)-quantile of the values ‖x̃∗ − x̃‖◦ to obtain the desired quantity ε̃(α) in (4).\nRemark. As a technical clarification, it is important to note that our method relies only on a single run of CS, involving just one sketching matrix S. Consequently, the bootstrapped vectors x̃∗ will be generated conditionally on the given S. In this way, the bootstrap aims to generate random vectors x̃∗, such that for a given draw of S, the conditional distributionL(x̃∗−x̃ |S) is approximately equal to the unknown distribution L(x̃− xopt).\nAlgorithm 1. (Error estimate for CS)\nInput: A positive integer B, and the sketches Ã, b̃, and x̃.\nFor: l = 1, . . . , B do • Draw a random vector i := (i1, . . . , im) by sampling m numbers with replacement from {1, . . . ,m}.\n• Form the matrix Ã∗ := Ã(i, :), and vector b̃∗ := b̃(i).\n• Compute the vector\nx̃∗ := argmin x∈Rd ‖Ã∗x− b̃∗‖2, (6)\nand the scalar ε∗l := ‖x̃∗ − x̃‖◦.\nReturn: ε̃(α) := quantile(ε∗1, . . . , ε∗B ; 1− α).\nHeuristic interpretation of Algorithm 1. To explain why the bootstrap works, let SA denote the set of positive semidefinite matrices M ∈ Rn×n such that A>MA is invertible, and define the map ψ : SA → Rd according to\nψ(M) = (A>MA)−1A>Mb. (7)\nThis map leads to the relation∗\nx̃− xopt = ψ(S>S)− ψ(In), (8)\nwhere In denotes the n× n identity matrix. By analogy, if we let S∗ ∈ Rm×n denote a matrix obtained by sampling m rows from S with replacement, then x̃∗ can be written as\nx̃∗ = argmin x∈Rd ‖S∗(Ax− b)‖2, (9)\nand the definition of ψ gives\nx̃∗ − x̃ = ψ(S∗>S∗)− ψ(S>S). (10)\nUsing the corresponding relations (8) and (10), it becomes easier to show why the distributions L(x̃ − xopt) and L(x̃∗ − x̃|S) should be nearly equal.\nTo proceed, if we let s1, . . . , sm ∈ Rn denote the rows of√ mS, it is helpful to note the basic algebraic fact\nS>S − In = 1m ∑m i=1(sis > i − In). (11)\nGiven that sketching matrices are commonly constructed so that s1, . . . , sm are i.i.d. (or nearly i.i.d.) with E[s1s>1 ] = In, the matrix S>S becomes an increasingly good approximation to In as m becomes large. Hence, it is natural to consider a first-order expansion of the right side of (8),\nx̃− xopt ≈ ψ′In(S >S − In), (12)\nwhere ψ′In is the differential of the map ψ at In. Likewise, if we define a set of vectors v1, . . . , vm ∈ Rd as vi := ψ ′ In (sis > i − In), then the linearity of ψ′In gives\nx̃− xopt ≈ 1m ∑m i=1 vi, (13)\nand furthermore, the vectors v1, . . . , vm are i.i.d. whenever the vectors s1, . . . , sm are. Consequently, as the sketch size m becomes large, the central limit theorem suggests that the difference √ m(x̃−xopt) should be approximately Gaussian,\nL (√ m(x̃− xopt) ) ≈ N ( 0,Σ ) , (14)\nwhere we put Σ := E[v1v>1 ].\nTo make the connection with x̃∗ − x̃, each of the preceding steps can be carried out in a corresponding manner. Specifically, if the differential of ψ at S>S is sufficiently close to the differential at In, then an expansion of equation (10) leads to the bootstrap analogue of (13),\nx̃∗ − x̃ ≈ 1m ∑m i=1 v ∗ i , (15)\n∗For standard types of sketching matrices, the event S>S ∈ SA occurs with high probability when A>A is invertible and m is sufficiently larger than d (and similarly for S∗>S∗).\nwhere v∗i := ψ ′ In (s∗i s ∗> i − S>S), and the vector s∗i is the\nith row of √ mS∗. Since the row vectors s∗1, . . . , s ∗ m are ob-\ntained by sampling with replacement from √ mS, it follows that the vectors v∗1 , . . . , v ∗ m are conditionally i.i.d. given S, and also, E[v∗i |S] = 0. Therefore, if we condition on S, the central limit theorem suggests that as m becomes large\nL (√ m(x̃∗ − x̃) |S ) ≈ N ( 0, Σ̂ ) , (16)\nwhere the conditional covariance matrix is denoted by Σ̂ := E [ v∗i v ∗> i ∣∣S]. Comparing the Gaussian approximations (14) and (16), this heuristic argument indicates that the distributions L(x̃ − xopt) and L(x̃∗ − x̃|S) should be close as long as Σ̂ is close to Σ, and when m is large, this is enforced by the law of large numbers."
  }, {
    "heading": "2.2. Error Estimation for IHS",
    "text": "At first sight, it might seem that applying the bootstrap to IHS would be substantially different than in the case of CS — given that IHS is an iterative algorithm, whereas CS is a “one-shot” algorithm. However, the bootstrap only needs to be modified slightly. Furthermore, the bootstrap relies on just the final two iterations of a single run of IHS.\nTo fix some notation, recall that t denotes the total number of IHS iterations, and let St ∈ Rm×n denote the sketching matrix used in the last iteration. Also define the matrix Ãt := StA, and the gradient vector gt−1 := A>(Ax̂t−1 − b) that is computed during the second-to-last iteration of IHS. Lastly, we note that the user is free to select any initial point x̂0 for IHS, and this choice does not affect our method.\nAlgorithm 2. (Error estimate for IHS)\nInput: A positive integer B, the sketch Ãt, the gradient gt−1, the second-to-last iterate x̂t−1, and the last iterate x̂t.\nFor l = 1, . . . , B do\n• Draw a random vector i := (i1, . . . , im) by sampling m numbers with replacement from {1, . . . ,m}.\n• Form the matrix Ã∗t := Ãt(i, :).\n• Compute the vector\nx̂∗t := argmin x∈Rd\n{ 1 2 ‖Ã∗t (x− x̂t−1)‖22 + 〈 gt−1 , x 〉} (17)\nand the scalar ε∗t,l := ‖x̂∗t − x̂t‖◦.\nReturn: ε̂t(α) := quantile(ε∗t,1, . . . , ε∗t,B ; 1− α)\nHeuristic interpretation of Algorithm 2. The ideas underlying the IHS version of the bootstrap are broadly similar to\nthose discussed for the CS version. In analogy with Algorithm 1, the main point is to argue that the fluctuations of x̂∗t around x̂t can be used as a proxy for the fluctuations of x̂t around xopt.\nFor a given pair of vectors x̂t−1 and gt−1, define the map ϕt : SA → Rd according to\nϕt(M) = x̂t−1 − (A>MA)−1gt−1, (18)\nwhere we recall that SA denotes the set of positive semidefinite matrices M ∈ Rn×n such that A>MA is invertible. It is straightforward to verify that this definition allows us to represent the difference x̂t − xopt and its bootstrap counterpart x̂∗t − x̂t in terms of the matching relations\nx̂t − xopt = ϕt(S>t St)− ϕt(In), (19)\nx̂∗t − x̂t = ϕt(S∗>t S∗t )− ϕt(S>t St), (20)\nwhere S∗t has m rows sampled with replacement from St. As in the discussion of Algorithm 1, the right sides of the relations (19) and (20) can be expanded to first order, which allows for approximations based on the central limit theorem to be used. Indeed, it can be argued that as m becomes large, the random vectors √ m(x̂t−xopt) and √ m(x̂∗t − x̂t) approach a common Gaussian distribution in a conditional sense. However, the details of this argument are much more complex than in the CS case — because the map ϕt is random and varies with t, whereas the map ψ in the CS case is fixed. A formal analysis may be found in the proof of Theorem 1 in the supplement (Lopes et al., 2018)."
  }, {
    "heading": "2.3. Computational Cost and Speedups",
    "text": "Of course, the quality control that is provided by error estimation does not come for free. Nevertheless, there are several special properties of Algorithms 1 and 2 that keep their computational cost in check, and in particular, the cost of computing (x̃, ε̃(α)) or (x̂t, ε̂t(α)) is much less than the cost of solving the full LS problem (1). These properties are summarized below.\n1. Cost of error estimation is independent of n. An important observation to make about Algorithms 1 and 2 is that their inputs consist of pre-computed matrices of size m × d or pre-computed vectors of dimension d. Consequently, both algorithms are highly scalable in the sense that their costs do not depend on the large dimension n. As a point of comparison, it should also be noted that sketching algorithms for LS generally have costs that scale linearly with n.\n2. Implementation is embarrassingly parallel. Due to the fact that each bootstrap sample is computed independently of the others, the for-loops in\nAlgorithms 1 and 2 can be easily distributed. Furthermore, it turns out that in practice, as few as B = 20 bootstrap samples are often sufficient to obtain good error estimates, as illustrated in Section 4. Consequently, even if the error estimation is done on a single workstation, it is realistic to suppose that the user has access to N processors such that the number of bootstrap samples per processor is B/N = O(1). If this is the case, and if ‖ · ‖◦ is any `p norm on Rd, then it follows that the per-processor cost of both algorithms is only O(md2). Lastly, the communication costs in this situation are also very modest. In fact, it is only necessary to send a single m× d matrix, and at most three d-vectors to each processor. In turn, when the results are aggregated, only B scalars are sent back to the central processor.\n3. Bootstrap computations have free warm starts. The bootstrap samples x̃∗ and x̂∗t can be viewed as perturbations of the actual sketched solutions x̃ and x̂t. This is because the associated optimization problems only differ with respect to resampled versions of Ã and b̃. Therefore, if a sub-routine for computing x̃∗ or x̂∗t relies on an initial point, then x̃ or x̂t can be used as warm starts at no additional cost. By contrast, note that warm starts are not necessarily available when x̃ or x̂t are first computed. In this way, the computation of the bootstrap samples is easier than a naive repetition of the underlying sketching algorithm.\n4. Error estimates can be extrapolated. The basic idea of extrapolation is to estimate the error of a “rough” initial sketched solution, say x̃init or x̂init, and then predict how much additional computation should be done to obtain a better solution x̃ or x̂t. There are two main benefits of doing this. First, the computation is adaptive, in the sense that “just enough” work is done to achieve the desired degree of error. Secondly, when error estimation is based on the rough initial solution x̃init, the bootstrap computations are substantially faster, because x̃init is constructed from a smaller sketching matrix than x̃ is. There are also two ways that extrapolation can be done — either with respect to the sketch sizem, or the number of iterations t, and these techniques are outlined in the following paragraphs."
  }, {
    "heading": "2.4. Extrapolating with respect to m for CS",
    "text": "The reasoning given in Section 2.1 based on the central limit theorem indicates that the standard deviation of ‖x̃− xopt‖◦ scales like 1/ √ m as a function of m. Therefore, if a rough initial solution x̃init is computed with a small sketch size m0 satisfying d < m0 < m, then the fluctuations of ‖x̃init − xopt‖◦ should be larger than those of ‖x̃ − xopt‖◦\nby a factor of √ m/m0. This simple scaling relationship is useful to consider, because if we let ε̃init(α) denote the error estimate obtained by applying Algorithm 1 to x̃init, then it is natural to expect that the re-scaled quantity\nε̃extrap,m(α) := √ m0 m ε̃init(α) (21)\nshould be approximately equal to the ordinary estimate ε̃(α) for x̃. The advantage of ε̃extrap,m(α) is that it is cheaper to compute, since the bootstrapping can be done with am0 × d matrix, rather than an m × d matrix. Furthermore, once ε̃init(α) has been computed, the user can instantly obtain ε̃extrap,m(α) as a function of m for all m > m0, using the scaling rule (21). In turn, this allows the user to “look ahead” and see how large m should be chosen to achieve a desired level of accuracy. Simulations demonstrating the effectiveness of this technique are given in Section 4."
  }, {
    "heading": "2.5. Extrapolating with respect to t for IHS",
    "text": "The IHS algorithm is known to enjoy linear convergence in the `2-norm under certain conditions (Pilanci & Wainwright, 2016). This means that the ith iterate x̂i satisfies the following bound with high probability\n‖x̂i − xopt‖2 ≤ c ηi, (22)\nwhere c > 0 and η ∈ (0, 1) are unknown parameters that do not depend on i.\nThe simple form of this bound lends itself to extrapolation. Namely, if estimates ĉ and η̂ can be obtained after the first 2 iterations of IHS, then the user can construct the extrapolated error estimate\nε̂extrap,i(α) := ĉ η̂ i, (23)\nwhich predicts how the error will decrease at all subsequent iterations i ≥ 3. As a result, the user can adaptively determine how many extra iterations (if any) are needed for a specified error tolerance. Furthermore, with the help of Algorithm 2, it is straightforward to estimate c and η. Indeed, from looking at the condition (22), we desire estimates ĉ and η̂ that solve the two equations\nĉ η̂ = ε̂1(α) and ĉ η̂2 = ε̂2(α), (24)\nand direct inspection shows that the choices\nη̂ := ε̂2(α)ε̂1(α) and ĉ := ε̂1(α) η̂ (25)\nserve this purpose. In Section 4, our experiments show that this simple extrapolation procedure works remarkably well."
  }, {
    "heading": "3. Main Result",
    "text": "In this section, we show that the estimates ε̃(α) and ε̂t(α) are consistent — in the sense that they satisfy the desired\nconditions (4) and (5) as the problem size becomes large. The setup and assumptions for our main result are given below.\nAsymptotics. We consider an asymptotic framework involving a sequence of LS problems indexed by n. This means that we allow each of the objectsA = A(n), S = S(n), and b = b(n) to implicitly depend on n. Likewise, the solutions x̃ = x̃(n) and x̂t = x̂t(n) implicitly depend on n.\nSince sketching algorithms are most commonly used when d n, our results will treat d as fixed while n→∞. Also, the sketch size m is often selected as a large multiple of d, and so we treat m = m(n) as diverging simultaneously with n. However, we make no restriction on the size of the ratiom/n, which may tend to 0 at any rate. In the same way, the number of bootstrap samples B = B(n) is assumed to diverge with n, and the ratio B/n may tend to 0 at any rate. With regard to the number of iterations t, its dependence on n is completely unrestricted, and t = t(n) is allowed to remain fixed or diverge with n. (The fixed case with t = 1 is of interest since it describes the HS algorithm.) Apart from these scaling conditions, we use the following two assumptions on A and b, as well as the sketching matrices.\nAssumption 1. The matrix Hn := 1nA >A is positive definite for each n, and there is a positive definite matrix H∞ ∈ Rd×d such that √ m(Hn −H∞)→ 0 as n→∞. Also, if gn := 1 nA >b, then there is a vector g∞ ∈ Rd such\nthat √ m(gn − g∞)→ 0. Lastly, let a1, . . . , an denote the rows of A, and let e1, . . . , en denote the standard basis vectors in Rn. Then, for any fixed matrix C ∈ Rd×d and fixed vector c ∈ Rd, the sum 1n2 ∑n j=1 ( a>j Caj + e > j bc >aj\n)2 converges to a limit (possibly zero) as n→∞.\nIn essence, this assumption merely ensures that the sequence of LS problems is “asymptotically stable”, in the sense that the solution xopt does not change erratically from n to n+ 1.\nAssumption 2. In the case of CS, the rows of S are generated as i.i.d. vectors, where the ith row is of the form 1√ m\n(si,1, . . . , si,n), and the random variables si,1, . . . , si,n are i.i.d. with mean 0, variance 1, E[s41,1] > 1, and E[s81,1] < ∞. In addition, the distribution of s1,1 remains fixed with respect to n, and in the case of IHS, the matrices S1, . . . , St are i.i.d. copies of S.\nRemarks. To clarify the interpretation of our main result, it is important to emphasize that A and b are viewed as deterministic, and the probability statements arise only from the randomness in the sketching algorithm, and the randomness in the bootstrap sampling. From an operational standpoint, the result says that as the problem size becomes large (n→∞), the outputs ε̃(α) and ε̂t(α) of our method will bound the errors ‖x̃− xopt‖◦ and ‖x̂t − xopt‖◦ with a probability that is effectively 1− α or larger.\nError Estimation for Randomized Least-Squares Algorithms\n(a) MSD (b) CPU (c) Ill-Conditioned (d) Well-Conditioned\nFigure 1. .\n(a) MSD (b) CPU (c) Ill-conditioned (d) Well-conditioned\nFigure 2. .\nFigure 1. Numerical results for CS with extrapolation. The black dashed curve represents the ideal benchmark εCS,m(.05) described in the text. The average extrapolated estimate is shown in blue, with the yellow and green curves being one standard deviation away. Note: The upper row shows results for `2 error (‖ · ‖◦ = ‖ · ‖2), and the lower row shows results for `∞ error (‖ · ‖◦ = ‖ · ‖∞).\nTheorem 1. Let ‖ ·‖◦ be any norm on Rd, and suppose that Assumptions 1 and 2 hold. Also, for any number α ∈ (0, 1) chosen by the user, let ε̃(α) and ε̂t(α) be the outputs of Algorithms 1 and 2 respectively. Then, there is a sequence of numbers δn > 0 satisfying δn → 0 as n→∞, such that the following inequalities hold for all n,\nP ( ‖x̃− xopt‖◦ ≤ ε̃(α) ) ≥ 1− α− δn, (26)\nand\nP ( ‖x̂t − xopt‖◦ ≤ ε̂t(α) ) ≥ 1− α− δn. (27)\nRemarks. Although this result can be stated in a concise form, the proof is actually quite involved (cf. (Lopes et al., 2018)). Perhaps the most significant technical obstacle is the sequential nature of the IHS algorithm. To handle the dependence of x̂t on the previous iterates, it is natural to analyze x̂t conditionally on them. However, because the set of previous iterates can grow with n, it seems necessary to establish distributional limits that hold “uniformly” over those iterates — and this need for uniformity creates difficulties when applying standard arguments.\nMore generally, to place this result within the context of\nthe sketching literature, it is worth noting that guarantees\nfor sketching algorithms typically show that a sketched solution is close to an exact solution with high probability (up to multiplicative constants). By contrast, Theorem 1 is more fine-grained, since it is concerned with distributional approximation, in terms of specific quantiles of the random variables ‖x̃− xopt‖◦ or ‖x̂t − xopt‖◦. In particular, the lower bounds are asymptotically equal to 1− α and do not involve any multiplicative constants. Lastly, it should also be noticed that the norm ‖ · ‖◦ is arbitrary, whereas it is\nmore often the case that analyses of sketching algorithms are restricted to particular norms."
  }, {
    "heading": "4. Experiments",
    "text": "In this section, we present experimental results in the contexts of CS and IHS. At a high level, there are two main takeaways: (1) The extrapolation rules accurately predict how estimation error depends on m or t, and this is shown in a range of conditions. (2) In all of the experiments, the algorithms are implemented with only B = 20 bootstrap samples. The fact that favorable results can be obtained with so few samples underscores the point that the method incurs only modest cost in exchange for an accuracy guarantee.\nData examples. Our numerical results are based on four linear regression datasets; two natural, and two synthetic. The natural datasets ‘YearPredictionMSD’, n = 463,715, d = 90, abbrev. MSD), and ‘cpusmall’ (n = 8, 192, d = 12, abbrev. CPU) are available at the LIBSVM repository (Chang & Lin, 2011). The synthetic datasets are both of size (n = 50,000, d = 100), but they differ with respect to the condition number of A>A. The condition numbers in the ‘I l-cond tioned’ and ‘Well-conditioned’ cases are respectively 1012 and 102. (Details for the synthetic data\nare given in the supplement (Lopes et al., 2018).)\nExperiments for CS. For each value of m in the grid {5d, . . . , 30d}, we generated 1,000 independent SRHT sketching matrices S ∈ Rm×n, leading to 1,000 realizations of of (Ã, b̃, x̃). Then, we computed the .95 sample quantile among the 1,000 values of ‖x̃ − xopt‖ at each grid point. We denote this value as εCS,m(.05), and we view it as an ideal benchmark that satisfies P ( ‖x̃− xopt‖ ≤ εCS,m(.05) ) ≈ .95 for each m. Also, the\nError Estimation for Randomized Least-Squares Algorithms (a) MSD (b) CPU (c) Ill-Conditioned (d) Well-Conditioned Figure 1. .\nvalue εCS,m(.05) is plotted as a function of m with the dashed black line in Figure 1. Next, using an initial sketch size of m0 = 5d, we applied Algorithm 1 to each of the 1,000 realizations of Ã ∈ Rm0×d and b̃ ∈ Rm0 computed previously, leading to 1,000 realizations of the initial error estimate ε̃init(.05). In turn, we applied the extrapolation rule (21) to each realization of ε̃init(.05), providing us with 1,000 extrapolated curves of ε̃extrap,m(.05) at all grid points m ≥ m0. The average of these curves is plotted in blue in Figure 1, with the yellow and green curves being one standard deviation away.\nComments on results for CS. An important conclusion to draw from Figure 1 is that the extrapolated estimate ε̃extrap,m(.05) is a nearly unbiased estimate of ε̃CS,m(.05) at values of m that are well beyond m0. This means that in addition to yielding accurate estimates, the extrapolation rule (21) provides substantial computational savings — because the bootstrap computations can be done at a value m0 that is much smaller than the value m ultimately selected for a higher quality x̃. Furthermore, these conclusions hold regardless of whether the error is measured with the `2-norm (‖ · ‖◦ = ‖ · ‖2) or the `∞-norm (‖ · ‖◦ = ‖ · ‖∞), which correspond to the top and bottom rows of Figure 1.\nExperiments for IHS. The experiments for IHS were organized similarly to the case of CS, except that the sketch size m was fixed (at either m = 10d, or m = 50d), and results were considered as a function of the iteration number. To be specific, the IHS algorithm was run 1,000 times, with t = 10 total iterations on each run, and with SRHT sketching matrices being used at each iteration. For a given run, the successive error values ‖x̂i − xopt‖2 at i = 1, . . . , 10, were recorded. At each i, we computed the .95 sample quantile among the 1,000 error values, which is denoted as\nεIHS,i(.05), and is viewed as an ideal benchmark that satisfies P ( ‖x̂i − xopt‖2 ≤ εIHS,i(.05) ) ≈ .95. In the plots, the value εIHS,i(.05) is plotted with the dashed black curve as a function of i = 1, . . . , 10. In addition, for each of the 1,000 runs, we applied Algorithm 2 at i = 1 and i = 2, producing 1,000 extrapolated values ε̂extrap,i(.05) at each i ≥ 3. The averages of the extrapolated values are plotted in blue, and again, the yellow and green curves are obtained by adding or subtracting one standard deviation.\nComments on results for IHS. At a glance, Figure 2 shows that the extrapolated estimate stays on track with the ideal benchmark, and is a nearly unbiased estimate of εIHS,i(.05), for i = 3, . . . , 10. An interesting feature of the plots is how much the convergence rate of IHS depends on m. Specifically, we see that after 10 iterations, the choice of m = 10d versus m = 50d can lead to a difference in accuracy that is 4 or 5 orders of magnitude. This sensitivity to m illustrates why selecting t is a non-trivial issue in practice, and why the extrapolated estimate can provide a valuable source of extra information to assess convergence."
  }, {
    "heading": "5. Conclusion",
    "text": "We have proposed a systematic approach to answer a very practical question that arises for randomized LS algorithms: “How accurate is a given solution?” A distinctive aspect of the method is that it leverages the bootstrap — a tool ordinarily used for statistical inference — in order to serve a computational purpose. To our knowledge, it is also the first error estimation method for randomized LS that is supported theoretical guarantees. Furthermore, the method does not add much cost to an underlying sketching algorithm, and it has been shown to perform well on several examples."
  }, {
    "heading": "Acknowledgements",
    "text": "MEL thanks the NSF for partial support of this work under grant DMS 1613218. MWM would like to thank the National Science Foundation, the Army Research Office, and the Defense Advanced Research Projects Agency for providing partial support of this work."
  }],
  "year": 2018,
  "references": [{
    "title": "Statistical properties of sketching algorithms",
    "authors": ["D. Ahfock", "W.J. Astle", "S. Richardson"],
    "year": 2017
  }, {
    "title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform",
    "authors": ["N. Ailon", "B. Chazelle"],
    "venue": "In Annual ACM Symposium on Theory of Computing (STOC),",
    "year": 2006
  }, {
    "title": "Fast dimension reduction using Rademacher series on dual BCH codes",
    "authors": ["N. Ailon", "E. Liberty"],
    "venue": "Discrete & Computational Geometry,",
    "year": 2009
  }, {
    "title": "A Posteriori Error Estimation in Finite Element Analysis, volume 37",
    "authors": ["M. Ainsworth", "J.T. Oden"],
    "year": 2011
  }, {
    "title": "Blendenpik: Supercharging LAPACK’s least-squares solver",
    "authors": ["H. Avron", "P. Maymounkov", "S. Toledo"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2010
  }, {
    "title": "Probabilistic approximate leastsquares",
    "authors": ["S. Bartels", "P. Hennig"],
    "venue": "In Artificial Intelligence and Statistics (AISTATS),",
    "year": 2016
  }, {
    "title": "Robust partiallycompressed least-squares",
    "authors": ["S. Becker", "B. Kawas", "M. Petrik"],
    "venue": "In AAAI,",
    "year": 2017
  }, {
    "title": "LIBSVM: a library for support vector machines",
    "authors": ["Chang", "C.-C", "Lin", "C.-J"],
    "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
    "year": 2011
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "In Annual ACM Symposium on theory of computing (STOC),",
    "year": 2013
  }, {
    "title": "A posteriori error bounds for joint matrix decomposition problems",
    "authors": ["N. Colombo", "N. Vlassis"],
    "venue": "In Advances in Neural Information Processing Systems (NIPS)",
    "year": 2016
  }, {
    "title": "Bootstrap Methods and their Application",
    "authors": ["A.C. Davison", "D.V. Hinkley"],
    "year": 1997
  }, {
    "title": "Sampling algorithms for `2 regression and applications",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"],
    "venue": "In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),",
    "year": 2006
  }, {
    "title": "Faster least squares approximation",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarlós"],
    "venue": "Numerische Mathematik,",
    "year": 2011
  }, {
    "title": "Fast approximation of matrix coherence and statistical leverage",
    "authors": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
    "authors": ["N. Halko", "Martinsson", "P.-G", "J.A. Tropp"],
    "venue": "SIAM review,",
    "year": 2011
  }, {
    "title": "A posteriori error estimates including algebraic error and stopping criteria for iterative solvers",
    "authors": ["P. Jiránek", "Z. Strakoŝ", "M. Vohralı́k"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2010
  }, {
    "title": "Randomized algorithms for the low-rank approximation of matrices",
    "authors": ["E. Liberty", "F. Woolfe", "Martinsson", "P.-G", "V. Rokhlin", "M. Tygert"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Estimating the algorithmic variance of randomized ensembles via the bootstrap",
    "authors": ["M.E. Lopes"],
    "venue": "The Annals of Statistics (to appear),",
    "year": 2018
  }, {
    "title": "A bootstrap method for error estimation in randomized matrix multiplication",
    "authors": ["M.E. Lopes", "S. Wang", "M.W. Mahoney"],
    "year": 1945
  }, {
    "title": "Error estimation for randomized least-squares algorithms via the bootstrap",
    "authors": ["M.E. Lopes", "S. Wang", "M.W. Mahoney"],
    "year": 2018
  }, {
    "title": "A statistical perspective on algorithmic leveraging",
    "authors": ["P. Ma", "M. Mahoney", "B. Yu"],
    "venue": "In International Conference on Machine Learning (ICML),",
    "year": 2014
  }, {
    "title": "Randomized algorithms for matrices and data",
    "authors": ["M.W. Mahoney"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2011
  }, {
    "title": "LSRN: A parallel iterative solver for strongly over - or underdetermined systems",
    "authors": ["X. Meng", "M.A. Saunders", "M.W. Mahoney"],
    "venue": "SIAM Journal on Scientific Computing,",
    "year": 2014
  }, {
    "title": "A posteriori error bounds for the linearlyconstrained variational inequality problem",
    "authors": ["Pang", "J.-S"],
    "venue": "Mathematics of Operations Research,",
    "year": 1987
  }, {
    "title": "Randomized sketches of convex programs with sharp guarantees",
    "authors": ["M. Pilanci", "M.J. Wainwright"],
    "venue": "IEEE Transactions on Information Theory,",
    "year": 2015
  }, {
    "title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares",
    "authors": ["M. Pilanci", "M.J. Wainwright"],
    "venue": "The Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "A fast randomized algorithm for overdetermined linear least-squares regression",
    "authors": ["V. Rokhlin", "M. Tygert"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2008
  }, {
    "title": "Improved approximation algorithms for large matrices via random projections",
    "authors": ["T. Sarlós"],
    "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),",
    "year": 2006
  }, {
    "title": "A posteriori error estimation and adaptive mesh-refinement techniques",
    "authors": ["R. Verfürth"],
    "venue": "Journal of Computational and Applied Mathematics,",
    "year": 1994
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["D.P. Woodruff"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "A fast randomized algorithm for the approximation of matrices",
    "authors": ["F. Woolfe", "E. Liberty", "V. Rokhlin", "M. Tygert"],
    "venue": "Applied and Computational Harmonic Analysis,",
    "year": 2008
  }],
  "id": "SP:02e43792646feaac8ec5523b5997697b2f671456",
  "authors": [{
    "name": "Miles E. Lopes",
    "affiliations": []
  }, {
    "name": "Shusen Wang",
    "affiliations": []
  }, {
    "name": "Michael W. Mahoney",
    "affiliations": []
  }],
  "abstractText": "Over the course of the past decade, a variety of randomized algorithms have been proposed for computing approximate least-squares (LS) solutions in large-scale settings. A longstanding practical issue is that, for any given input, the user rarely knows the actual error of an approximate solution (relative to the exact solution). Likewise, it is difficult for the user to know precisely how much computation is needed to achieve the desired error tolerance. Consequently, the user often appeals to worst-case error bounds that tend to offer only qualitative guidance. As a more practical alternative, we propose a bootstrap method to compute a posteriori error estimates for randomized LS algorithms. These estimates permit the user to numerically assess the error of a given solution, and to predict how much work is needed to improve a “preliminary” solution. In addition, we provide theoretical consistency results for the method, which are the first such results in this context (to the best of our knowledge). From a practical standpoint, the method also has considerable flexibility, insofar as it can be applied to several popular sketching algorithms, as well as a variety of error metrics. Moreover, the extra step of error estimation does not add much cost to an underlying sketching algorithm. Finally, we demonstrate the effectiveness of the method with empirical results.",
  "title": "Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap"
}