{
  "sections": [{
    "heading": "1. Introduction",
    "text": "In statistics and machine learning, ridge regression (Gunst & Mason, 1977; Hoerl & Kennard, 1970) (also known as Tikhonov regularization or weight decay) is a variant of regularized least squares problems where the choice of the penalty function is the squared `2-norm. Formally, let A ∈ Rn×d be the design matrix and let b ∈ Rn be the response vector. Then, the linear algebraic formulation of the ridge regression problem is as follows:\nZ∗ = min x∈Rd\n{ ‖Ax− b‖22 + λ‖x‖22 } , (1)\nwhere λ > 0 is the regularization parameter. There are two fundamental motivations underlying the use of ridge regres-\n1Department of Statistics, Purdue University, West Lafayette, IN 2Department of Computer Science, Purdue University, West Lafayette, IN. Correspondence to: Agniva Chowdhury <chowdhu5@purdue.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nsion. First, when d n, i.e., the number of predictor variables d greatly exceeds the number of observations n, fitting the full model without regularization (i.e., setting λ to zero) will result in large prediction intervals and a non-unique regression estimator. Second, if the design matrix A is ill-conditioned, solving the standard least-squares problem without regularization would depend on (ATA)−1. This inversion would be problematic if ATA were singular or nearly singular and thus adding even a little noise to the elements of A could result in large changes in (ATA)−1. Due to these two considerations, solving standard least-squares problems without regularization may provide a good fit to the training data but may not generalize well to test data.\nRidge regression abandons the requirement of an unbiased estimator in order to address the aforementioned problems. At the cost of introducing bias, ridge regression reduces the variance and thus might reduce the overall mean squared error (MSE). The minimizer of eqn. (1) is\nx∗ = ( ATA + λId )−1 ATb, (2)\nor, equivalently (see Saunders et al. (1998) and Lemma 9 in Appendix A),\nx∗ = AT ( AAT + λIn )−1 b. (3)\nBoth formulations work for any λ > 0 for either underconstrained or over-constrained ridge regression problems, regardless of the rank of the design matrix A. It is easy to see that x∗ can be computed in time\nO(ndmin{n, d}+ min{n3, d3}) = O(ndmin{n, d}).\nIn our work, we will focus on design matrices A ∈ Rn×d with d n, which is the most common setting for ridge regression. For simplicity of exposition, we will assume that the rank of A is equal to n.1 In the context of ridge regression, a much more important quantity than the rank of the design matrix is the effective degrees of freedom:\ndλ = n∑ i=1 σ2i σ2i + λ ≤ n, (4)\nwhere σi are the singular values of A.\n1Our results can be slightly improved to depend on the rank ρ of the matrix A instead of n.\nThe recent flurry of activity on Randomized Linear Algebra (RLA) (Drineas & Mahoney, 2016) and the widespread use of sketching as a tool for matrix computations (Woodruff, 2014), resulted in many novel results for ridge regression. In Section 1.2 we discuss relevant prior work."
  }, {
    "heading": "1.1. Our Contributions",
    "text": "We present a novel iterative algorithm (Algorithm 1) for sketched ridge regression and two simple sketching-based structural conditions under which Algorithm 1 guarantees highly accurate approximations to the optimal solution x∗. More precisely, Algorithm 1 guarantees that, as long as a simple structural constraint is satisfied, the resulting approximate solution vector x̂∗ satisfies (after t iterations)\n‖x∗ − x̂∗‖2 ≤ εt‖x∗‖2. (5)\nPrior to discussing the aforementioned constraint, we note that error guarantees of the above form are highly desirable. Indeed, beyond being a relative error guarantee, the dependency on ε drops exponentially fast as the number of iterations increases. It is easy to see that by setting εt = ε′, O(ln(1/ε′)) iterations would suffice to provide a relative error guarantee with accuracy parameter ε′. This means that converging to, say, ten decimal digits of accuracy would necessitate only a constant number of iterations. See Section 1.2 for a comparison of this bound with prior work.\nLet V ∈ Rd×n be the matrix of right singular vectors of A; recall that A has rank n. For eqn. (5) to hold, a sketching matrix S ∈ Rd×s is to be constructed such that (for an appropriate choice of the sketching dimension s d)\n‖VTSSTV − In‖2 ≤ ε\n2 . (6)\nWe note that the constraint of eqn. (6) has been the topic of intense research in the RLA literature; this is precisely the reason why we use eqn. (6) as the building block in our analysis. Indeed, assuming that n d, one can use the (exact or approximate) column leverage scores (Mahoney & Drineas, 2009; Mahoney, 2011) of A to satisfy the aforementioned constraint, in which case S is a samplingand-rescaling matrix. Perhaps more interestingly, a variety of oblivious sketching matrix constructions for S can be used to satisfy eqn. (6). We discuss various constructions for S in Section 2.1.\nOne deficiency of the structural constraint of eqn. (6) is that all known constructions for S that satisfy the constraint need a number of columns s that is proportional to n. As a result, the running time of any algorithm that computes the sketch AS is also proportional to n. It would be much better to design algorithms whose running time depends on the degrees of freedom dλ, which is upper bounded by n, but could be significantly smaller depending on the distribution of the singular values and the choice of λ.\nTowards that end, we analyze Algorithm 1 under a second structural constraint. We define a diagonal matrix Σλ ∈ Rn×n whose i-th diagonal entry is given by\n(Σλ)ii =\n√ σ2i\nσ2i + λ , i = 1, . . . , n. (7)\nNotice that ‖Σλ‖2F = dλ. Our second structural condition is given by\n‖ΣλVTSSTVΣλ −Σ2λ‖2 ≤ ε\n4 √ 2 . (8)\nSimilarly to the constraint of eqn. (6), the constraint of eqn. (8) can also be satisfied by, for example, sampling with respect to the ridge leverage scores of Alaoui & Mahoney (2015); Cohen et al. (2017) or by oblivious sketching matrix constructions for S. The difference is that, instead of having the column size s of the matrix S depend on n, it now depends on dλ, which could be considerably smaller. Indeed, it follows that by sampling-and-rescaling O(dλ ln dλ) predictor variables from the design matrix A (using either exact or approximate ridge leverage scores (Alaoui & Mahoney, 2015; Cohen et al., 2017) we can satisfy the constraint of eqn. (8). Similarly, oblivious sketching matrix constructions for S can be used to satisfy eqn. (8). We discuss constructions for S in Section 2.1.\nHowever, this improved dependency on dλ instead of n comes with a mild loss in accuracy. For simplicity, we only state a result when λ satisfies σ2k+1 ≤ λ ≤ σ2k for some integer k, 1 ≤ k ≤ n.2 In words, λ can be thought of as “regularizing” the bottom n−k singular values of the design matrix A, since it dominates them. In this case, we prove that the approximation x̂∗ returned by Algorithm 1 satisfies\n‖x∗ − x̂∗‖2 ≤ εt\n2\n( ‖x∗‖2 +\n1√ 2λ ∥∥UTk,⊥b∥∥2) . (9) Here Uk,⊥ ∈ Rn×(n−k) denotes the matrix of the bottom n − k left singular vectors of the design matrix A. In words, we achieve an additive-relative error approximation, where the additive error part depends on the norm of the “piece” of the response vector b that lies on the regularized component of the design matrix A. As this piece grows, the quality of the approximation worsens. The error decreases exponentially fast with the number of iterations.\nAnother contribution of our work is Theorem 4, which proves that the mean-square-error (MSE) of the approximate solution x̂∗ is a relative error approximation to the MSE of x∗, under the structural assumptions of eqns. (6) or (8), even after a single iteration.\n2The bound of eqn. (9) can be easily generalized to hold when c1σ 2 k+1 ≤ λ ≤ c2σ2k for some constants c1, c2 > 0. For simplicity of exposition, we assume that both c1 and c2 equal one.\nTo the best of our knowledge, our bounds are a first attempt to provide general structural results that guarantee highquality approximations to the optimal solution vector of ridge regression. Our first structural result can be satisfied by sampling with respect to the leverage scores or by the use of oblivious sketching matrices whose size depends on the rank of the design matrix and guarantees relative error approximations. Our second structural result presents the first accuracy analysis for ridge regression when the ridge leverage scores are used to sample predictor variables. Interestingly, the ridge leverage scores have been used in a number of applications involving matrix approximation, cost-preserving projections, clustering, etc. (Cohen et al., 2017), but their performance in the context of ridge regression has not been analyzed in prior work. Our work here argues that the second structural condition can be satisfied by sampling with respect to the ridge leverage scores. The number of predictor variables to be sampled depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix, and results in a relative-additive error guarantee."
  }, {
    "heading": "1.2. Prior Work",
    "text": "In this section, we discuss our contributions in the context of the large and ever-growing body of prior work on sketching-based algorithms for regression and ridge regression. The work more closely related to ours is Chen et al. (2015), which (in our notation) returns an approximation x̂∗ to x∗ that satisfies (with high probability) a relative error guarantee of the form\n‖x∗ − x̂∗‖2 ≤ ε‖x∗‖2.\nThe running time of the proposed approach is O(nnz(A) + ε−2n3 ln(n/ε)). The proposed approach is also based on sketching A using RLA tools such as the count-min sketch of Clarkson & Woodruff (2013) and the sub-sampled Randomized Hadamard Transform of Ailon & Chazelle (2009); Sarlós (2006); Drineas et al. (2011). Compared to our work, notice that their dependency on ε is exponentially higher: our approach has a running time that grows with ln(1/ε) whereas the above bound grows proportionally to 1/ε2. Additionally, our analysis can be made to depend on the degrees of freedom of the ridge-regression problem (see Theorem 2 and Section 2.1). Finally, we complement the bounds on the MSE for the response vector presented in Theorem 6 of Chen et al. (2015) with a relative-error guarantee on the MSE of the solution vector (see Theorem 4). We should also mention that prior to Chen et al. (2015); Lu et al. (2013) proposed a fast approximation algorithm for the computation of the kernel matrix using the sub-sampled randomized Hadamard transformation (SRHT).\nRecently, Wang et al. (2017) presented many results on ridge-regression problems assuming n d. In this setting,\nthe main motivation for ridge regression is to deal with the potential ill-conditioning of the design matrix A. Wang et al. (2017) presented sketching-based approaches that guarantee relative error approximations to the value of the objective Z∗, as opposed to the actual solution vector. Our approach and analysis is quite different and is applicable where d n; the results of Wang et al. (2017) do not generalize to this setting. However, recent work by Avron et al. (2017a;b) also focused on d n: for example, Theorem 17 of Avron et al. (2017b) presents structural conditions under which the value of the objective Z∗ can be estimated up to relative error accuracy, but no bounds are presented for the approximate solution vector. This last result seems to necessitate two structural conditions: the first one is identical to the condition of eqn. (6), but the second one is on the spectral norm of an approximate matrix product that is not needed in our analysis.\nOur work was partially motivated by Pilanci & Wainwright (2016), where an iterative algorithm (the so-called Iterative Hessian Sketch) was presented for standard (i.e., λ = 0), over-constrained (n d) regression problems. Indeed, the authors provide strong motivation that clarifies the need for algorithms for regression problems whose running times depends on ln(1/ε) in order to achieve ε-relative-error approximations. We emphasize that the transition from standard to regularized regression problems as well as from the overto the under-constrained case is far from trivial. Indeed, algorithms and structural results for over-constrained regression problems date back to 2006 (Drineas et al., 2006b), whereas the analogous results for ridge-regression problems appeared after 2015. Similarly, the only result that we know for under-constrained regression problems (λ = 0, n d) appeared in Section 6.2 of Drineas et al. (2012).\nAnother line of research that motivated our approach was the recent introduction of ridge leverage scores (Alaoui & Mahoney, 2015; Cohen et al., 2017). Indeed, our Theorem 2 presents a structural result that can be satisfied (with high probability) by sampling columns of A with probabilities proportional to (exact or approximate) ridge leverage scores (see Section 2.1). The number of sampled predictor variables (columns of A) is proportional to O(dλ ln dλ). To the best of our knowledge, this is the first result showing a strong accuracy guarantee for ridge regression problems when the ridge leverage scores are used to sample predictor variables, in one or more iterations. We also note a recent application of ridge leverage scores (Calandriello et al., 2017a;b) where the authors presented a row sampling algorithm in order to construct a kernel sketch which is eventually used in a second-order gradient-based method for online kernel convex optimization.\nIn yet another relevant line of work, much research recently focused on the computation and inversion of the kernel ma-\ntrix AAT (or ATA). A number of recent papers have considered the problem of fast kernel approximation for large datasets (Zhang et al., 2015; Avron et al., 2017b; Musco & Musco, 2017; Calandriello et al., 2017c; Wang et al., 2017). However, direct comparison of the bounds presented in the aforementioned papers and our work is not straightforward, since our objective (accuracy of the approximate solution vector) is different than the objective of the above papers. In this context, there are also several recent works (Cutajar et al., 2016; Rudi et al., 2017; Ma & Belkin, 2017) that considered preconditioned gradient-based methods to develop fast and scalable approaches for approximating kernels.\nFinally, Gonen et al. (2016) presented a sketching-based preconditioned SVRG approach for ridge regression problems that converges to the optimal solution in a number of iterations that depends on ln(1/ε), returning an ε-relative-error approximation to the objective value Z∗. However, no such bounds were presented for the actual solution vector."
  }, {
    "heading": "1.3. Notation",
    "text": "We use a,b, . . . to denote vectors and A,B, . . . to denote matrices. For a matrix A, A∗i (Ai∗) denotes the i-th column (row) of A as a column (row) vector. For vector a, ‖a‖2 denotes its Euclidean norm; for a matrix A, ‖A‖2 denotes its spectral norm and ‖A‖F denotes its Frobenius norm. We refer the reader to Golub & Van Loan (1996) for properties of norms that will be quite useful in our work. For a matrix A ∈ Rn×d with d > n of rank n, its (thin) Singular Value Decomposition (SVD) is equal to the product UΣVT, with U ∈ Rn×n (the matrix of the left singular vectors), V ∈ Rd×n (the matrix of the right singular vectors), and Σ ∈ Rn×n a diagonal matrix whose diagonal entries are the singular values of A. Computation of the SVD takes, in this setting, O(n2d) time. We will use the notation Uk ∈ Rn×k to denote the matrix of the top k left singular vectors and Uk,⊥ ∈ Rn×(n−k) to denote the matrix of the bottom n−k left singular vectors. We will often use σi to denote the singular values of a matrix implied by context. Additional notation will be introduced as needed."
  }, {
    "heading": "2. Iterative, Sketching-based Ridge Regression",
    "text": "Algorithm 1 iteratively computes a sequence of vectors x̃(j) ∈ Rd for j = 1, . . . , t and returns the estimator x̂∗ =∑t j=1 x̃ (j) to the true solution vector x∗ of eqn. (3).\nIn words, Algorithm 1 is quite simple: roughly, it solves ridge regression problems with the residual vector b(j) (i.e., the part of the vector b(j−1) that was not captured in the previous iteration) as the new response vector for i = 1, . . . , t. Our main quality-of-approximation results (Theorems 1 and 2) argue that returning the sum of those intermediate solutions results in a highly accurate approximation\nAlgorithm 1 Iterative, sketching-based ridge regression\nInput: A ∈ Rn×d, b ∈ Rn, λ > 0; number of iterations t > 0; sketching matrix S ∈ Rd×s; Initialize: b(0) ← b, x̃(0) ← 0d, y(0) ← 0n; for j = 1 to t do\nb(j) ← b(j−1) − λy(j−1) −Ax̃(j−1); y(j) ← (ASSTAT + λIn)−1b(j); x̃(j) ← ATy(j);\nend for Output: Approximate solution vector x̂∗ = ∑t j=1 x̃ (j);\nto the optimal solution vector x∗. Theorem 1 presents a quality-of-approximation result under the assumption that the sketching matrix S satisfies the constraint of eqn. (6).\nTheorem 1. Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem. Assume that for some constant 0 < ε < 1, the sketching matrix S ∈ Rd×s satisfies the constraint of eqn. (6). Then, the estimator x̂∗ returned by Algorithm 1 satisfies\n‖x̂∗ − x∗‖2 ≤ ε t ‖x∗‖2 .\nHere x∗ is the true solution of the ridge regression problem.\nSimilarly, Theorem 2 presents a quality-of-approximation result under the assumption that the sketching matrix S satisfies the constraint of eqn. (8).\nTheorem 2. Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem. Assume that for some constant 0 < ε < 1, the sketching matrix S ∈ Rd×s satisfies the constraint of eqn. (8). Then, the estimator x̂∗ returned by Algorithm 1 satisfies\n‖x̂∗ − x∗‖2 ≤ εt\n2\n( ‖x∗‖2 +\n1√ 2λ ∥∥UTk,⊥b∥∥2) . Here, k ∈ {1, . . . , n} is an integer with σ2k+1 ≤ λ ≤ σ2k and x∗ is the true solution of the ridge regression problem.\nAs we have already discussed, the bound of Theorem 2 is weaker. However, the structural condition of eqn. (8) on which the above theorem depends, can be satisfied with a sketching matrix S whose dimensionality depends only on the degrees of freedom dλ of the underlying ridge regression problem, as opposed to the dimensions of the design matrix. This could result in significant savings (see Section 2.1).\nOur algorithm can also be viewed as a preconditioned Richardson iteration (see e.g., Chapter 2 of Quarteroni & Valli (1994)) for solving the linear system (AAT+λIn)y = b with pre-conditioner P−1 = (ASSTAT + λIn)−1 and step-size equal to one. More precisely, Algorithm 1 can be formulated as\nȳ(j) = ȳ(j−1) + P−1 ( b− (AAT + λIn)ȳ(j−1) ) ,\nwhere ȳ(j) = ∑j k=1 y\n(k) (see Appendix D for the derivation). Further, subject to the structural conditions of eqns. (6) and (8), it can be shown that ȳ(t) converges to the true solution y∗ = (AAT + λIn)−1b in O(ln(1/ε)) steps (see Appendix D) and, consequently, the output of Algorithm 1 (which can be expressed as x̂∗ = ATȳ(t)) also converges to x∗ = AT(AAT + λIn)−1b, the true solution of the ridge regression problem. Our analysis offers several advantages over preconditioned Richardson iteration. In our case, P−1(AAT + λIn) is not symmetric positive definite which, according to existing literature, implies that the convergence of Richardson’s method is monotone in terms of the energy-norm induced by AAT + λIn, but not the Euclidean norm (see eqn. (2.4.17) in Quarteroni & Valli (1994)). Additionally, standard convergence analysis of the Richardson iteration is with respect to ȳ(t), whereas our vector of interest is x̂∗ (which is ȳ(t) premultiplied by AT). The equality ‖ȳ(t) − y∗‖2 = ‖x̂∗ − x∗‖2 holds if A has orthonormal rows, which is not true in general.\nWe now discuss the running time of Algorithm 1. First, we need to compute Ax̃(j−1) which takes time O(nnz(A)). Next, computing the sketch AS ∈ Rn×s takes T (A,S) time and depends on the particular construction of S (see Section 2.1). Then, in order to invert the matrix Θ = ASSTAT + λIn it suffices to compute the SVD of the matrix AS. Notice that given the singular values of AS we can compute the singular values of Θ; also note that the left and right singular vectors of Θ are the same as the left singular vectors of AS. Interestingly, we do not need to compute Θ−1: we can store it implicitly by storing its left (and right) singular vectors UΘ and its singular values ΣΘ. Then, we can compute all necessary matrix-vector products using this implicit representation of Θ−1. Thus, inverting Θ takes O(sn2) time. Updating the vectors b(j), y(j), and x̃(j) is dominated by the aforementioned running times, as all updates amount to just matrix-vector products. Thus, summing over all t iterations, the running time of Algorithm 1 is given by\nO(t · nnz(A)) +O(sn2) + T (A,S). (10)\nWe conclude this section by noting that our results remain valid when different sampling matrices Sj are used in each iteration j = 1, . . . , t, as long as they satisfy the constraints of eqns. (6) or (8). As a matter of fact, the sketching matrices Sj do not even need to have the same number of columns. See Section 5 for an interesting open problem in this setting."
  }, {
    "heading": "2.1. Satisfying the Conditions of Eqns. (6) or (8)",
    "text": "The conditions of eqns. (6) and (8) essentially boil down to randomized, approximate matrix multiplication (Drineas & Kannan, 2001; Drineas et al., 2006a), a task that has received much attention in the RLA community. We start by discussing sketching-based approaches: a particularly useful\nresult for our purposes appeared in Cohen et al. (2016). Using our notation, Cohen et al. (2016) proved that for X ∈ Rd×n and for a (suitably constructed) sketching matrix S ∈ Rd×s, with probability at least 1− δ,\n∥∥XTSSTX−XTX∥∥ 2 ≤ ε ( ‖X‖22 + ‖X‖2F r ) , (11)\nfor any arbitrary r ≥ 1. The above bound holds for a very broad family of constructions for the sketching matrix S (see Cohen et al. (2016) for details). In particular, Cohen et al. (2016) demonstrated a construction for S with s = O(r/ε2) columns such that, for any n × d matrix A, the product AS can be computed in timeO(nnz(A))+Õ((r3+ r2n)/εγ) for some constant γ. Thus, starting with eqn. (6) and using this particular construction for S, let X = V and note that ‖V‖2F = n and ‖V‖2 = 1. Setting r = n, eqn. (11) implies that∥∥VTSSTV − In∥∥2 ≤ 2 ε. In this case, the running time of the sketch computation is equal to T (A,S) = O(nnz(A)) + Õ(n3/εγ). The running time of the overall algorithm follows from eqn. (10) and our choices for s and r:\nO(t · nnz(A)) + Õ(n3/εmax{2,γ}).\nThe failure probability (hidden in the polylogarithmic terms) can be easily controlled using a union bound. Finally, a simple change of variables (using ε/4 instead of ε) suffices to satisfy the structural condition of eqn. (6) without changing the above running time.\nSimilarly, starting with eqn. (8), let X = VΣλ and note that ‖VΣλ‖2F = dλ and ‖VΣλ‖2 ≤ 1. Setting r = dλ, eqn. (11) implies that ∥∥ΣλVTSSTVΣλ −Σ2λ∥∥2 ≤ 2ε. In this case, the running time of the sketch computation is equal to T (A,S) = O(nnz(A)) + Õ(d2λn/εγ). The running time of the overall algorithm follows from eqn. (10) and our choices for s and r:\nO(t · nnz(A)) + Õ(dλn2/εmax{2,γ}).\nAgain, a change of variables suffices to satisfy the structural condition of eqn. (8) without changing the running time.\nWe now discuss how to satisfy the conditions of eqns. (6) or (8) by sampling, i.e., by selecting a small number of predictor variables. Towards that end, consider Algorithm 2 for the construction of the sampling-and-rescaling matrix S.\nThe following theorem (see Appendix G for its proof) is of independent interest and is a strengthening of Theorem 4.2 of Holodnak & Ipsen (2015), since the sampling complexity s is improved to depend only on ‖X‖2F instead of the stable rank of X for the special case where ‖X‖2 ≤ 1.3\n3We do note that Theorem 3 is implicit in Cohen et al. (2017).\nAlgorithm 2 Construct sampling-and-rescaling matrix Input: Probabilities pi, i = 1, . . . , d; integer s d; S← 0d×s; for j = 1 to s do\nPick ij ∈ {1, . . . , d} with P(ij = i) = pi; Sijj ← (s pij )− 1 2 ;\nend for Output: Sampling-and-rescaling matrix S;\nTheorem 3. Let X ∈ Rd×n with ‖X‖2 ≤ 1 and let S be constructed by Algorithm 2 with pi = ‖Xi∗‖22 / ‖X‖ 2 F for i = 1, . . . , d. Let δ be a failure probability and let ε ∈ (0, 1] be an accuracy parameter. If the number of sampled columns s satisfies\ns ≥ 8 ‖X‖2F\n3 ε2 ln\n( 4 (1 + ‖X‖2F )\nδ\n) ,\nthen, with probability at least 1− δ,∥∥XTSSTX−XTX∥∥ 2 ≤ ε.\nUsing Theorem 3 with X = V we can satisfy the condition of eqn. (6) by simply using the sampling probabilities pi = ‖Vi∗‖22 /n (recall that ‖V‖ 2 F = n and ‖V‖2 = 1), which are the column leverage scores of the design matrix A. Setting s = O(ε−2n lnn) suffices to satisfy the condition of eqn. (6). We note that approximate leverage scores also suffice and that their computation can be done efficiently without computing V (Drineas et al., 2012).\nFinally, using Theorem 3 with X = VΣλ we can satisfy the condition of eqn. (8) using the sampling probabilities pi = ‖(VΣλ)i∗‖22 /dλ (recall that ‖VΣλ‖ 2 F = dλ and ‖VΣλ‖2 ≤ 1). It is easy to see that these probabilities are proportional to the column ridge leverage scores of the design matrix A (see Lemma 21 in Appendix F). Setting s = O(ε−2dλ ln dλ) suffices to satisfy the condition of eqn. (8). We note that approximate ridge leverage scores also suffice and that their computation can be done efficiently without computing V (Cohen et al., 2017)."
  }, {
    "heading": "2.2. Bounding the MSE of x̂∗",
    "text": "Consider the data-generation model\nb = Ax0 + ε, (12)\nwhere b ∈ Rn is the response vector, A ∈ Rn×d is the design matrix, x0 ∈ Rn is the “true” parameter vector, and ε ∈ Rn is the noise satisfying E(ε) = 0 and E(εεT) = σ2In, σ > 0. Then, the ridge regression estimator x∗ of the parameter vector x0 can be expressed as in eqn. (3), with mean squared error (MSE) given by (see Lemma 16 in Appendix E for the derivation)\nMSE(x∗) = σ2 ∥∥(AAT + λIn)−1A∥∥2F\n+ ∥∥(AT(AAT + λIn)−1A− Id)x0∥∥22 . (13)\nSimilarly, we can prove that the MSE of x̂∗ for the special case where t = 1 in Algorithm 1 is equal to\nMSE(x̂∗) = σ2 ∥∥(ASSTAT + λIn)−1A∥∥2F\n+ ∥∥(AT(ASSTAT + λIn)−1A− Id)x0∥∥22 . (14)\nWe present bounds on the MSE of x̂∗ for the special case where Algorithm 1 is run for a single iteration (t = 1) under the assumptions of eqns. (6) or (8). Bounds for t > 1 (more than one iteration) are delegated to future work.\nTheorem 4. Let A ∈ Rn×d be the design matrix and let x̂∗ be the output of Algorithm 1 for t = 1. If the condition of eqn. (6) is satisfied for some constant 0 < ε < 1, then,\nMSE(x̂∗) ≤ (1 + 3εγ21) MSE(x∗),\nwhere γ1 = 1 + σ21 λ . If the condition of eqn. (8) is satisfied for some constant 0 < ε < 1, then,\nMSE(x̂∗) ≤ (1 + 3εγ22) MSE(x∗),\nwhere γ2 = max { 1 + σ21/λ, √ 1 + λ/σ2n } ."
  }, {
    "heading": "3. Sketching the Proof of Theorem 2",
    "text": "Due to space considerations, essentially all our proofs have been deferred to the Appendix. However, to give a flavor of the mathematical derivations underlying our contributions, we present an outline of the proof of Theorem 2, starting with the special case where Algorithm 1 is run for a single iteration (t = 1).\nUsing the quantities defined in Algorithm 1, let\nx∗(j) = AT(AAT + λIn) −1b(j) (15)\nfor j = 1, . . . , t. Notice that x∗ = x∗(1). Our next result expresses the intermediate vectors x̃(j) of Algorithm 1 in terms of the vectors x∗(j). We remind the reader that U ∈ Rn×n and Σ ∈ Rn×n are, respectively, the matrices of the left singular vectors and singular values of A. We will make extensive use of the matrix Σλ defined in eqn. (7). Lemma 5. Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem. Let S ∈ Rd×s be the sketching matrix and define\nE = ΣλV TSSTVΣλ −Σ2λ."
  }, {
    "heading": "If ‖E‖2 < 1, then for all j = 1, . . . , t,",
    "text": "x̃(j) = x∗(j) + VΣλRΣλΣ −1UTb(j), (16)\nwhere R = ∑∞ `=1(−1)`E`.\nNow, consider the case when t = 1. Algorithm 1 returns x̂∗ = x̃(1); also recall that x∗ = x∗(1) and b = b(1). Therefore, applying Lemma 5 yields\nx̂∗ = x∗ + VΣλRΣλΣ −1UTb. (17)\nFurther, for any j = 1, . . . , t,\n‖R‖2 = ∥∥ ∞∑ `=1 (−1)`E` ∥∥ 2 ≤ ∞∑ `=1 ‖E`‖2 ≤ ∞∑ `=1 ‖E‖`2\n≤ ∞∑ `=1 ( ε 4 √ 2 )` = ε 4 √ 2 1− ε 4 √ 2 ≤ ε 2 √ 2 . (18)\nwhere we used the triangle inequality, sub-multiplicativity of the spectral norm, and the fact that ε\n4 √ 2 ≤ 12 . Now, using\neqn. (17), we have\n‖x̂∗ − x∗‖2 = ‖VΣλRΣλΣ−1UTb‖2 ≤ ‖Σλ‖2‖R‖2‖ΣλΣ−1UTb‖2\n≤ ε 2 √ 2 ‖ΣλΣ−1UTb‖2 = ε\n2 √ 2 ‖Σ−1λ Σ 2 λΣ −1UTb‖2. (19)\nwhere the first inequality follows from the unitary invariance and sub-multiplicativity of the spectral norm, and the second inequality is due to eqn. (18) and the fact that ‖Σλ‖2 ≤ 1.\nNow, let (Σ−1λ )k denote the diagonal matrix whose first k diagonal entries are equal to the first k diagonal entries of Σ−1λ and the bottom n− k diagonal entries are set to zero. Let (Σ−1λ )k,⊥ = Σ −1 λ − (Σ −1 λ )k. Then, we have\n‖Σ−1λ Σ 2 λΣ −1UTb‖2 ≤ ‖(Σ−1λ )k Σ 2 λΣ −1UTb‖2︸ ︷︷ ︸\n∆1\n+ ‖(Σ−1λ )k,⊥ Σ 2 λΣ −1UTb‖2︸ ︷︷ ︸\n∆2\n. (20)\nwhere eqn. (20) follows from the triangle inequality and the fact that Σ−1λ = (Σ −1 λ )k + (Σ −1 λ )k,⊥.\nNext, we bound ∆1 and ∆2 separately using eqns. (60) and (62) in Appendix C:\n∆1 ≤ √\n2 ‖x∗‖2 , ∆2 ≤ 1√ λ ∥∥UTk,⊥b∥∥2 . (21)\nFinally, combining eqns. (19), (20) and (21), we obtain\n‖x̂∗ − x∗‖2 ≤ ε\n2 √ 2\n(√ 2 ‖x∗‖2 +\n1√ λ ‖UTk,⊥b‖2 ) = ε\n2\n( ‖x∗‖2 +\n1√ 2λ ‖UTk,⊥b‖2\n) , (22)\nwhich concludes the proof for the t = 1 case.\nInterestingly, the eqn. (22) holds more generally and can be used to bound the distance between the intermediate approximate solution vectors x̃(j) and the intermediate true solution vectors x∗(j)of eqn. (15). Indeed, for j = 1, . . . , t, we have\n‖x̃(j) − x∗(j)‖2 ≤ ε\n2\n( ‖x∗(j)‖2 +\n1√ 2λ ‖UTk,⊥b(j)‖2\n) .\n(23)\nThe next lemma (see Appendix C for its proof) presents a structural result for the optimal solution x∗.\nLemma 6. Let x̃(j), j = 1, . . . , t be the sequence of vectors introduced in Algorithm 1 and let x∗(t) ∈ Rd be defined as in eqn. (15). Then,\nx∗ = x∗(t) + t−1∑ j=1 x̃(j), (24)\nwhere x∗ is the true solution of the ridge regression problem.\nRepeated application of eqns. (23) and (24) yields\n‖x̂∗ − x∗‖2 = ∥∥ t∑ j=1 x̃(j) − x∗ ∥∥ 2\n= ∥∥x̃(t) − (x∗ − t−1∑\nj=1\nx̃(j) )∥∥ 2 = ∥∥x̃(t) − x∗(t)∥∥ 2\n≤ ε 2\n( ‖x∗(t)‖2 +\n1√ 2λ ‖UTk,⊥b(t)‖2\n) . (25)\nThe next bound (see Appendix C for its proof) provides a critical inequality that can be used recursively in order to establish Theorem 2.\nLemma 7. Let b(j), j = 1, . . . , t, be the intermediate response vectors of Algorithm 1 and let x∗(j) be the vector defined in eqn. (15) for j = 1, . . . , t − 1. If the structural condition of eqn. (8) is satisfied, then\n‖x∗(j+1)‖2 + 1√ 2λ ‖UTk,⊥b(j+1)‖2\n≤ ε ( ‖x∗(j)‖2 +\n1√ 2λ ‖UTk,⊥b(j)‖2\n) . (26)\nApplying eqn. (26) iteratively, we obtain\n‖x∗(t)‖2 + 1√ 2λ ‖UTk,⊥b(t)‖2\n≤ ε ( ‖x∗(t−1)‖2 +\n1√ 2λ ‖UTk,⊥b(t−1)‖2 ) ≤ · · · ≤ εt−1 ( ‖x∗‖2 +\n1√ 2λ ‖UTk,⊥b‖2\n) . (27)\nFinally, combining eqns. (25) and (27), we conclude that\n‖x̂∗ − x∗‖2 ≤ εt\n2\n( ‖x∗‖2 +\n1√ 2λ ‖UTk,⊥b‖2\n) . (28)"
  }, {
    "heading": "4. Empirical Evaluation",
    "text": "We perform experiments on the ARCENE dataset (Guyon et al., 2005) from the UCI repository (Lichman, 2013). The design matrix contains 200 samples with 10, 000 real-valued features; we normalize the entries to be within the interval [0, 1]. The response vector consists of ±1 labels. We also perform experiments on synthetic data generated as in Chen et al. (2015); see Appendix H for details.\nIn our experiments, we compare three different choices of sampling probabilities: selecting columns (i) uniformly at random, (ii) proportional to their leverage scores, or (iii) proportional to their ridge leverage scores. For each sampling method, we run Algorithm 1 for 50 iterations with a variety of sketch sizes, and measure (i) the relative error of the solution vector ‖x̂\n∗−x∗‖2 ‖x∗‖2 , where x ∗ is the true optimal solu-\ntion and (ii) the objective sub-optimality f(x̂ ∗)\nf(x∗) − 1, where f(x) = ‖Ax− b‖22 + λ‖x‖22 is the objective function for the ridge-regression problem.\nThe results are shown in Figure 1. Figures 1a and 1b plot the relative error of the solution vector and the objective suboptimality (for a fixed sketch size) as the iterative algorithm progresses. Figure 1c plots the relative error of the solution with respect to varying sketch sizes (the plots for objective sub-optimality are analogous and thus omitted). We observe that both the solution error and the objective sub-optimality decay exponentially as our iterative algorithm progresses.4\n4For these experiments, we have set the regularization parameter λ = 10 in the ridge regression objective as well as when computing the ridge leverage score sampling probabilities.\nNext, we show that the approximation quality depends directly on the degrees of freedom dλ of the ridge-regression problem (eqn. (4)), rather than the dimensions of the design matrix. To this end, we keep the design matrix unchanged (n remains fixed), and vary the regularization parameter λ ∈ {1, 2, 5, 10, 20, 50}. Figure 1d plots the relative solution error against the degrees of freedom dλ (for a fixed sketch size and number of iterations); we observe that the relative error decreases roughly exponentially as dλ decreases (as λ increases). Thus, the sketch size or number of iterations necessary to achieve a certain precision in the solution also decreases with dλ, even though n remains fixed."
  }, {
    "heading": "5. Conclusion and Open Problems",
    "text": "We have presented simple structural results that guarantee high-quality approximations to the optimal solution vector of ridge regression. In particular, our second structural result presents the first accuracy analysis for ridge regression when the ridge leverage scores are used to sample predictor variables. The sample size depends on the degrees of freedom of the ridge regression problem and not the dimensions of the design matrix. An obvious open problem is to either improve the sample size or present lower bounds showing that our bounds are tight. Additionally, the results of Theorem 4 should be generalized to cover the t > 1 case.\nFinally, an interesting open problem would be to investigate whether the use of different sampling matrices in each iteration of Algorithm 1 (i.e., introducing new “randomness” in each iteration) could lead to provably improved bounds for our main theorems. We conjecture that this is indeed the case, and we present further experiment results in Appendix H which support our conjecture. In particular, the results show that using a newly sampled sketching matrix at every iteration enables faster convergence as the iterations progress, and also reduces the minimum sketch size necessary for Algorithm 1 to converge.\nAcknowledgements. We thank an anonymous reviewer for pointing out the connection between our method and the preconditioned Richardson iteration. AC and PD were partially supported by NSF IIS-1661760 and IIS-1661756. JY was supported by NSF IIS-1149789 and IIS-1618690."
  }],
  "year": 2018,
  "references": [{
    "title": "The fast Johnson–Lindenstrauss transform and approximate nearest neighbors",
    "authors": ["N. Ailon", "B. Chazelle"],
    "venue": "SIAM Journal on Computing,",
    "year": 2009
  }, {
    "title": "Fast randomized kernel ridge regression with statistical guarantees",
    "authors": ["A.E. Alaoui", "M.W. Mahoney"],
    "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,",
    "year": 2015
  }, {
    "title": "Sharper bounds for regularized data fitting. In Approximation, Randomization, and Combinatorial Optimization",
    "authors": ["H. Avron", "K.L. Clarkson", "D.P. Woodruff"],
    "venue": "Algorithms and Techniques,",
    "year": 2017
  }, {
    "title": "Faster kernel ridge regression using sketching and preconditioning",
    "authors": ["H. Avron", "K.L. Clarkson", "D.P. Woodruff"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2017
  }, {
    "title": "Second-order kernel online convex optimization with adaptive sketching",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Efficient second-order online kernel learning with adaptive embedding",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Distributed adaptive sampling for kernel matrix approximation",
    "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"],
    "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
    "year": 2017
  }, {
    "title": "Fast relative-error approximation algorithm for ridge regression",
    "authors": ["S. Chen", "Y. Liu", "M.R. Lyu", "I. King", "S. Zhang"],
    "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,",
    "year": 2015
  }, {
    "title": "Low rank approximation and regression in input sparsity time",
    "authors": ["K.L. Clarkson", "D.P. Woodruff"],
    "venue": "In Proceedings of the 45th annual ACM symposium on Symposium on Theory of Computing,",
    "year": 2013
  }, {
    "title": "Optimal approximate matrix product in terms of stable rank",
    "authors": ["M.B. Cohen", "J. Nelson", "D.P. Woodruff"],
    "venue": "In 43rd International Colloquium on Automata, Languages, and Programming,",
    "year": 2016
  }, {
    "title": "Input sparsity time low-rank approximation via ridge leverage score sampling",
    "authors": ["M.B. Cohen", "C. Musco"],
    "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2017
  }, {
    "title": "Preconditioning kernel matrices",
    "authors": ["K. Cutajar", "M.A. Osborne", "J.P. Cunningham", "M. Filippone"],
    "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Fast monte-carlo algorithms for approximate matrix multiplication",
    "authors": ["P. Drineas", "R. Kannan"],
    "venue": "In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science,",
    "year": 2001
  }, {
    "title": "RandNLA: Randomized numerical linear algebra",
    "authors": ["P. Drineas", "M.W. Mahoney"],
    "venue": "Communications of the ACM,",
    "year": 2016
  }, {
    "title": "Fast monte carlo algorithms for matrices I: Approximating matrix multiplication",
    "authors": ["P. Drineas", "R. Kannan", "M.W. Mahoney"],
    "venue": "SIAM Journal on Computing,",
    "year": 2006
  }, {
    "title": "Sampling algorithms for `2 regression and applications",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"],
    "venue": "In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms,",
    "year": 2006
  }, {
    "title": "Faster least squares approximation",
    "authors": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarlós"],
    "venue": "Numerische Mathematik,",
    "year": 2011
  }, {
    "title": "Fast approximation of matrix coherence and statistical leverage",
    "authors": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2012
  }, {
    "title": "Matrix Computations",
    "authors": ["G.H. Golub", "C.F. Van Loan"],
    "year": 1996
  }, {
    "title": "Solving ridge regression using sketched preconditioned SVRG",
    "authors": ["A. Gonen", "F. Orabona", "S. Shalev-Shwartz"],
    "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Biased estimation in regression: An evaluation using mean squared error",
    "authors": ["R.F. Gunst", "R.L. Mason"],
    "venue": "Journal of the American Statistical Association,",
    "year": 1977
  }, {
    "title": "Result analysis of the NIPS 2003 feature selection challenge",
    "authors": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2005
  }, {
    "title": "Ridge regression: Biased estimation for nonorthogonal problems",
    "authors": ["A.E. Hoerl", "R.W. Kennard"],
    "venue": "Technometrics, 12:55–67,",
    "year": 1970
  }, {
    "title": "Randomized approximation of the gram matrix: Exact computation and probabilistic bounds",
    "authors": ["J.T. Holodnak", "I.C.F. Ipsen"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2015
  }, {
    "title": "Approximate Gaussian Elimination",
    "authors": ["R. Kyng"],
    "venue": "Ph.D Thesis, Yale University,",
    "year": 2017
  }, {
    "title": "Faster ridge regression via the subsampled randomized hadamard transform",
    "authors": ["Y. Lu", "P. Dhillon", "D.P. Foster", "L. Ungar"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2013
  }, {
    "title": "Diving into the shallows: a computational perspective on large-scale shallow learning",
    "authors": ["S. Ma", "M. Belkin"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Randomized algorithms for matrices and data",
    "authors": ["M.W. Mahoney"],
    "venue": "Foundations and Trends in Machine Learning",
    "year": 2011
  }, {
    "title": "CUR matrix decompositions for improved data analysis",
    "authors": ["M.W. Mahoney", "P. Drineas"],
    "venue": "Proceedings of the National Academy of Sciences of the United States of America,",
    "year": 2009
  }, {
    "title": "Recursive sampling for the nystrom method",
    "authors": ["C. Musco"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares",
    "authors": ["M. Pilanci", "M.J. Wainwright"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Numerical Approximation of Partial Differential Equations",
    "authors": ["A.M. Quarteroni", "A. Valli"],
    "year": 1994
  }, {
    "title": "Falkon: An optimal large scale kernel method",
    "authors": ["A. Rudi", "L. Carratino", "L. Rosasco"],
    "venue": "In Advances in Neural Information Processing Systems",
    "year": 2017
  }, {
    "title": "Improved approximation algorithms for large matrices via random projections",
    "authors": ["T. Sarlós"],
    "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,",
    "year": 2006
  }, {
    "title": "Ridge regression learning algorithm in dual variables",
    "authors": ["C. Saunders", "A. Gammerman", "V. Vovk"],
    "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning,",
    "year": 1998
  }, {
    "title": "An introduction to matrix concentration inequalities",
    "authors": ["J.A. Tropp"],
    "venue": "Foundations and Trends in Machine Learning,",
    "year": 2015
  }, {
    "title": "Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging",
    "authors": ["S. Wang", "A. Gittens", "M.W. Mahoney"],
    "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Sketching as a tool for numerical linear algebra",
    "authors": ["D.P. Woodruff"],
    "venue": "Foundations and Trends in Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Singular values of differences of positive semidefinite matrices",
    "authors": ["X. Zhan"],
    "venue": "SIAM Journal on Matrix Analysis and Applications,",
    "year": 2001
  }, {
    "title": "Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates",
    "authors": ["Y. Zhang", "J. Duchi", "M. Wainwright"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }],
  "id": "SP:39a6decfc6e1d16181c30279f7bfb0e226b5ec23",
  "authors": [{
    "name": "Agniva Chowdhury",
    "affiliations": []
  }, {
    "name": "Jiasen Yang",
    "affiliations": []
  }, {
    "name": "Petros Drineas",
    "affiliations": []
  }],
  "abstractText": "Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations. We present a simple, iterative, sketching-based algorithm for ridge regression that guarantees highquality approximations to the optimal solution vector. Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and wellunderstood primitive of randomized linear algebra. An important contribution of our work is the analysis of the behavior of sub-sampled ridge regression problems when the ridge leverage scores are used: we prove that accurate approximations can be achieved by a sample whose size depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix. Our empirical evaluations verify our theoretical results on both real and synthetic data.",
  "title": "An Iterative, Sketching-based Framework for Ridge Regression"
}