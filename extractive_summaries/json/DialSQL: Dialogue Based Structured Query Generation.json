{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1339–1349 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1339"
  }, {
    "heading": "1 Introduction",
    "text": "Building natural language interfaces to databases (NLIDB) is a long-standing open problem and has significant implications for many application domains. It can enable users without SQL programming background to freely query the data they have. For this reason, generating SQL queries from natural language questions has gained a renewed interest due to the recent advance in deep learning and semantic parsing (Yaghmazadeh et al., 2017; Zhong et al., 2017; Xu et al., 2017; Iyer et al., 2017).\nWhile new methods race to achieve the stateof-the-art performance on NLIDB datasets such as WikiSQL (Xu et al., 2017; Zhong et al., 2017), the accuracy is still not high enough for real use. For example, SQLNet (Xu et al., 2017) achieves 61.3% accuracy on WikiSQL. After analyzing the error cases of Seq2SQL (Zhong et al., 2017) and SQLNet, we recognized that many wrong translations cannot be easily corrected due to the lack of external knowledge and semantic understanding.\nIn this paper, we aim to alleviate the aforementioned problem by putting human users in the loop. Previous human-in-the-loop NLIDBs (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) rely on users to carefully go through a generated SQL query and revise it accordingly, which is not feasible for users who do not know the SQL language. Instead, we resort to a different approach by introducing a goal-oriented dialogue model, DialSQL, that interacts with users to extract and correct potential errors in the generated queries.\nGiven a SQL query generated from a natural language question, we assume any segment, or span, of the generated query such as a WHERE clause can be potentially erroneous. The goal of DialSQL is to extract the erroneous spans and ask users multi-choice questions to validate and correct these errors. DialSQL is based on a hierarchical encoder-decoder architecture with attention and pointer mechanisms. The model first encodes each turn of interaction and runs a dialogue level RNN network on the dialogue history. The output of the network is then used to predict the error category, i.e., whether it is a selection, projection, or aggregation error. Conditioned on the error category, the output of a second RNN is used to predict the start and end positions of the error span by pointing to the query tokens. Finally, candidate choices are decoded from the error category and span representations. Following previous\nwork (Zhong et al., 2017; Xu et al., 2017), we only use column names and do not utilize table values.\nHow to train and evaluate DialSQL become two challenging issues due to the lack of error data and interaction data. In this work, we construct a simulator to generate simulated dialogues, a general approach practiced by many dialogue studies. Inspired by the agenda-based methods for user simulation (Schatzmann et al., 2007), we keep an agenda of pending actions that are needed to induce the ground truth query. At the start of the dialogue, a new query is carefully synthesized by randomly altering the ground truth query and the agenda is populated by the sequence of altering actions. Each action consists of three sub-actions: (i) Pick an error category and extract a span; (ii) Raise a question; (iii) Update the query by randomly altering the span and remove the action from the agenda. Consider the example in Figure 1: Step1 synthesizes the initial query by randomly altering the WHERE clause and AGGREGATION; Step2 generates the simulated dialogue by validating the altered spans and offering the correct choice.\nTo evaluate our model, we first train DialSQL on the simulated dialogues. Initial queries for new questions are manufactured by running a black box SQL generation system on the new questions. When tested on the WikiSQL (Zhong et al., 2017) dataset, our model increases the query match accuracy of SQLNet (Xu et al., 2017) from 61.3% to 69.0% using on average 2.4 validation questions per query."
  }, {
    "heading": "2 Related Work",
    "text": "Research on natural language interfaces to databases (NLIDBs), or semantic parsing, has spanned several decades. Early rule-based NLIDBs (Woods, 1973; Androutsopoulos et al., 1995; Popescu et al., 2003) employ carefully designed rules to map natural language questions to formal meaning representations like SQL queries. While having a high precision, rule-based systems are brittle when facing with language variations. The rise of statistical models (Zettlemoyer and Collins, 2005; Kate et al., 2005; Berant et al., 2013), especially the ongoing wave of neural network models (Yih et al., 2015; Dong and Lapata, 2016; Sun et al., 2016; Zhong et al., 2017; Xu et al., 2017; Guo and Gao, 2018; Yavuz et al., 2016), has enabled NLIDBs that are more robust to language variations. Such systems allow users to formulate questions with greater flexibility. However, although state-of-the-art systems have achieved a high accuracy of 80% to 90% (Dong and Lapata, 2016) on well-curated datasets like GEO (Zelle and Ray, 1996) and ATIS (Zettlemoyer and Collins, 2007), the best accuracies on datasets with questions formulated by real human users, e.g., WebQuestions (Berant et al., 2013), GraphQuestions (Su et al., 2016), and WikiSQL (Zhong et al., 2017), are still far from enough for real use, typically in the range of 20% to 60%.\nHuman-in-the-loop systems are a promising paradigm for building practical NLIDBs. A number of recent studies have explored this paradigm with two types of user interaction: coarse-grained and fine-grained. Iyer et al. (2017) and Li et\nal. (2016) incorporate coarse-grained user interaction, i.e., asking the user to verify the correctness of the final results. However, for real-world questions, it may not always be possible for users to verify result correctness, especially in the absence of supporting evidence. Li and Jagadish (2014) and Yaghmazadeh et al. (2017) have shown that incorporating fine-grained user interaction can greatly improve the accuracy of NLIDBs. However, they require that the users have intimate knowledge of SQL, an assumption that does not hold for general users. Our method also enables fine-grained user interaction for NLIDBs, but we solicit user feedback via a dialogue between the user and the system.\nOur model architecture is inspired by recent studies on hierarchical neural network models (Sordoni et al., 2015; Serban et al., 2015; Gur et al., 2017). Recently, Saha et al. (2018) propose a hierarchical encoder-decoder model augmented with key-value memory network for sequential question answering over knowledge graphs. Users ask a series of questions, and their system finds the answers by traversing a knowledge graph and resolves coreferences between questions. Our interactive query generation task significantly differs from their setup in that we aim to explicitly detect and correct the errors in the generated SQL query via a dialogue between our model and the user.\nAgenda based user simulations have been investigated in goal-oriented dialogues for model training (Schatzmann et al., 2007). Recently, Seq2seq neural network models are proposed for user simulation (Asri et al., 2016) that utilize additional state tracking signals and encode dialogue turns\nin a more coarse way. We design a simulation method for the proposed task where we generate dialogues with annotated errors by altering queries and tracking the sequence of alteration steps."
  }, {
    "heading": "3 Problem Setup and Datasets",
    "text": "We study the problem of building an interactive natural language interface to databases (INLIDB) for synthesizing SQL queries from natural language questions. In particular, our goal is to design a dialogue system to extract and validate potential errors in generated queries by asking users multi-choice questions over multiple turns. We will first define the problem formally and then explain our simulation strategy."
  }, {
    "heading": "3.1 Interactive Query Generation",
    "text": "At the beginning of each dialogue, we are given a question Q = {q1, q2, · · · , qN}, a table with column names T = {T1, T2, · · · , TK} where each name is a sequence of words, and an initial SQL query U generated using a black box SQL generation system. Each turn t is represented by a tuple of system and user responses, (St, Rt), and augmented with the dialogue history (list of previous turns), Ht. Each system response is a triplet of error category c, error span s, and a set of candidate choices C, i.e., St = (c, s, C). An error category (Table 2) denotes the type of the error that we seek to correct and an error span is the segment of the current query that indicates the actual error. Candidate choices depend on the error category and range over the following possibilities: (i) a column name, (ii) an aggregation operator, or (iii) a where condition. User responses are represented by ei-\nther an affirmation or a negation answer and an index c′ to identify a choice. We define the interactive query generation task as a list of subtasks: at each turn t, (i) predict c, (ii) extract s from U , and (iii) decode C. The task is supervised and each subtask is annotated with labeled data.\nConsider the example dialogue in Table 1. We first predict validate agg as the error category and error span (start = 1, end = 2) is decoded by pointing to the aggregation segment of the query. Candidate choices, (average, no agg), are decoded using the predicted error category, predicted error span, and dialogue history. We use a template based natural language generation (NLG) component to convert system and user responses into natural language."
  }, {
    "heading": "3.2 Dialogue Simulation for INLIDB",
    "text": "In our work, we evaluate our model on the WikiSQL task. Each example in WikiSQL consists of a natural language question and a table to query from. The task is to generate a SQL query that correctly maps the question to the given table. Unfortunately, the original WikiSQL lacks error data and user interaction data to train and evaluate DialSQL. We work around this problem by designing a simulator to bootstrap training dialogues and evaluate DialSQL on the test questions of WikiSQL.\nInspired by the agenda-based methods (Schatzmann et al., 2007), we keep an agenda of pending actions that are needed to induce the ground truth query. At the start of the dialogue, we synthesize a new query by randomly altering the ground truth query and populating the agenda by the sequence of altering actions. Each action launches a sequence of sub-actions: (i) Randomly select an error category and extract a related span from the current query, (ii) randomly generate a valid choice for the chosen span, and (iii) update the current query by replacing the span with the choice. The dialogue is initiated with the final query and a rule-based system interacts with a rule-based user\nsimulator to populate the dialogue. The rule-based system follows the sequence of altering actions previously generated and asks the user simulator a single question at each turn. The user simulator has access to the ground truth query and answers each question by comparing the question (error span and the choice) with the ground truth.\nConsider the example in Figure 1 where Step1 synthesizes the initial query and Step-2 simulates a dialogue using the outputs of Step-1. Step-1 first randomly alters the WHERE clause; the operator is replaced with a random operator. The updated query is further altered and the final query is passed to Step-2. In Step-2, the system starts with validating the aggregation with the user simulator. In this motivating example, the aggregation is incorrect and the user simulator negates and selects the offered choice. During training, there is only a single choice offered and DialSQL trains to produce this choice; however, during testing, it can offer multiple choices. In the next step, the system validates the WHERE clause and generates a no error action to issue the generated query. At the end of this process, we generate a set of labeled dialogues by executing Step-1 and Step2 consecutively. DialSQL interacts with the same rule-based simulator during testing and the SQL queries obtained at the end of the dialogues are used to evaluate the model."
  }, {
    "heading": "4 Dialogue Based SQL Generation",
    "text": "In this section, we present our DialSQL model and describe its operation in a fully supervised setting. DialSQL is composed of three layers linked in a hierarchical structure where each layer solves a different subtask : (i) Predicting error category, (ii) Decoding error span, and (iii) Decoding candidate choices (illustrated in Figure 2). Given a (Q,T, U) triplet, the model first encodes Q, each column name Ti ∈ T , and query U into vector representations in parallel using Recurrent Neural Networks (RNN). Next, the first layer of the\nmodel encodes the dialogue history with an RNN and predicts the error category from this encoding. The second layer is conditioned on the error category and decodes the start and end positions of the error span by attending over the outputs of query encoder. Finally, the last layer is conditioned on both error category and error span and decodes a list of choices to offer to the user."
  }, {
    "heading": "4.1 Preliminaries and Notation",
    "text": "Each token w is associated with a vector ew from rows of an embeddings matrix E. We aim at obtaining vector representations for question, table headers, and query, then generating error category, error span, and candidate choices.\nFor our purposes, we use GRU units (Cho et al., 2014) in our RNN encoders which are defined as\nht = f(xt;ht−1)\nwhere ht is the hidden state at time t. f is a nonlinear function operating on input vector xt and previous state ht−1. We refer to the last hidden state of an RNN encoder as the encoding of a sequence."
  }, {
    "heading": "4.2 Encoding",
    "text": "The core of our model is a hierarchical encoderdecoder neural network that encodes dialogue history and decodes errors and candidate choices at the end of each user turn. The input to the model is the previous system turn and the current user turn and the output is the next system question.\nEncoding Question, Column Names, and Query. Using decoupled RNNs (Enc), we encode natural language question, column names, and query sequences in parallel and produce outputs and hidden states. oQ, oTi , and oU denote the sequence of hidden states at each step and hQ, hTi , and hU denote the last hidden states of question, column name, and query encoders, respectively. Parameters of the encoders are decoupled and only the word embedding matrix E is shared.\nEncoding System and User Turns Since there is only a single candidate choice during training, we ignore the index and encode user turn by doing an embedding lookup using the validation answer (affirmation or negation). Each element (error category, error span, and candidate choice) of the system response is encoded by doing an\nembedding lookup and different elements are used as input at different layers of our model.\nEncoding Dialogue History At the end of each user turn, we first concatenate the previous error category and the current user turn encodings to generate the turn level input. We employ an RNN to encode dialogue history and current turn into a fixed length vector as\nhD10 = h Q\noD1t , g D1 t = Enc([Ec, Ea])\nhD1t = [Attn(g D1 t , H T ), oDt ]\nwhere [.] is vector concatenation, Ec is the error category encoding, Ea is the user turn encoding, hD10 is the initial hidden state, and h D1 t is the current hidden state. Attn is an attention layer with a bilinear product defined as in (Luong et al., 2015)\nAttn(h,O) = ∑ softmax(tanh(hWO)) ∗O\nwhere W is attention parameter."
  }, {
    "heading": "4.3 Predicting Error Category",
    "text": "We predict the error category by attending over query states using the output of the dialogue encoder as\nct = tanh(Lin([Attn(h D1 t , O U ), hD1t ])) lt = softmax(ct · E(C))\nwhere Lin is a linear transformation, E(C) is a matrix with error category embeddings, and lt is the probability distribution over categories."
  }, {
    "heading": "4.4 Decoding Error Span",
    "text": "Consider the case in which there are more than one different WHERE clauses in the query and each clause has an error. In this case, the model needs to monitor previous error spans to avoid decoding the same error. DialSQL runs another RNN to generate a new dialogue encoding to solve the aforementioned problem as\nhD20 = h Q\noD2t , g D2 t = Enc(Ec)\nhD2t = [Attn(g D2 t , H T )oD2t ]\nwhere hD20 is the initial hidden state, and h D2 t is the current hidden state. Start position i of the error span is decoded using the following probability distribution over query tokens\npi = softmax(tanh(h D2 t L1H U ))\nwhere pi is the probability of start position over the ith query token. End position j of the error span is predicted by conditioning on the start position\nci = ∑ pi ∗HU\np̂j = softmax(tanh([h D2 t , ci]L2H U ))\nwhere p̂j is the probability of end position over the jth query token. Conditioning on the error category will localize the span prediction problem as each category is defined by only a small segment of the query."
  }, {
    "heading": "4.5 Decoding Candidate Choices",
    "text": "Given error category c and error span (i, j) , DialSQL decodes a list of choices that will potentially replace the error span based on user feedback. Inspired by SQLNet (Xu et al., 2017), we describe our candidate choice decoding approach as follows. Select column choice. We define the following\nscores over column names,\nh = Attn(Lin([oUi−1, o U j , Ec]), H T )\nssel = u T ∗ tanh(Lin([HT , h]))\nwhere oUi−1 is the output vector of the query encoder preceding the start position, and oUj is the output of query encoder at the end position. Aggregation choice. Conditioned on the encoding e of the select column, we define the following scores over the set of aggregations (MIN, MAX, COUNT, NO AGGREGATION)\nsagg = v T ∗ tanh(Lin(Attn(e,HQ)))\nWhere condition choice. We first decode the condition column name similar to decoding select column. Given the encoding e of condition column, we define the following scores over the set of operators (=, <, >)\nsop = w T ∗ tanh(Lin(Attn(e,HQ)))\nNext, we define the following scores over question tokens for the start and end positions of the condition value\nsst = Attn(e,H Q)\nsed = Attn([e, hst, H Q])\nwhere hst is the context vector generated from the first attention. We denote the number of candidate choices to be decoded by k. We train DialSQL with k = 1. The list of k > 1 candidate choices is decoded similar to beam search during testing. As an example, we select k column names that have the highest scores as the candidate where column choices. For each column name, we first generate k different operators and from the set of k ∗ 2 column name and operator pairs; select k operators that have the highest joint probability. Ideally, DialSQL should be able to learn the type of errors present in the generated query, extract precise error spans by pointing to query tokens, and using the location of the error spans, generate a set of related choices."
  }, {
    "heading": "5 Experimental Results and Discussion",
    "text": "In this section, we evaluate DialSQL on WikiSQL using several evaluation metrics by comparing with previous literature."
  }, {
    "heading": "5.1 Evaluation Setup and Metrics",
    "text": "We measure the query generation accuracy as well as the complexity of the questions and the length of the user interactions. Query-match accuracy. We evaluate DialSQL on WikiSQL using query-match accuracy (Zhong et al., 2017; Xu et al., 2017). Query-match accuracy is the proportion of testing examples for which the generated query is exactly the same as the ground truth, except the ordering of the WHERE clauses. Dialogue length. We count the number of turns to analyze whether DialSQL generates any redundant validation questions. Question complexity. We use the average number of tokens in the generated validation questions to evaluate if DialSQL can generate simple questions without overwhelming users.\nSince SQLNet and Seq2SQL are single-step models, we can not analyze DialSQL’s performance by comparing against these on the last two metrics. We overcome this issue by generating simulated dialogues using an oracle system\nthat has access to the ground truth query. The system compares SELECT and AGGREGATION clauses of the predicted query and the ground truth; asks a validation question if they differ. For each WHERE clause pairs of generated query and the ground truth, the system counts the number of matching segments namely COLUMN, OP, and VALUE. The system takes all the pairs with the highest matching scores and asks a validation question until one of the queries has no remaining WHERE clause. If both queries have no remaining clauses, the dialogue terminates. Otherwise, the system asks a validate where added (validate where removed) question when the generated query (ground truth query) has more remaining clauses. We call this strategy OracleMatching (OM). OM ensures that the generated dialogues have the minimum number of turns possible."
  }, {
    "heading": "5.2 Training Details",
    "text": "We implement DialSQL in TensorFlow (Abadi et al., 2016) using the Adam optimizer (Kingma and Ba, 2014) for the training with a learning rate of 1e−4. We use an embedding size of 300, RNN state size of 50, and a batch size of 64. The embeddings are initialized from pretrained GloVe embeddings (Pennington et al., 2014) and fine-tuned during training. We use bidirectional RNN encoders with two layers for questions, column names, and queries. Stanford CoreNLP tokenizer (Manning et al., 2014) is used to parse questions and column names. Parameters of each layer are decoupled from each other and only the embedding matrix is shared. The total number of turns is limited to 10 and 10 simulated dialogues are generated for each example in the WikiSQL training set. SQLNet and Seq2SQL models are trained on WikiSQL using the existing implemention provided by their authors. The code is available at https://github.com/ izzeddingur/DialSQL."
  }, {
    "heading": "5.3 Evaluation on the WikiSQL Dataset",
    "text": "Table 3 presents the results of query match accuracy. We observe that DialSQL model with a number of 5 choices improves the performance of both SQLNet and Seq2SQL by 7.7% and 9.4%, respectively. The higher gain on Seq2SQL model can be attributed that the single-step Seq2SQL makes more errors: DialSQL has more room for improvement. We also show the results of DialSQL where\nusers are allowed to revisit their previous answers and with more informative user responses; instead the model only validates the error span and the user directly gives the correct choice. In this scenario, the performance further improves on both development and test sets. It seems decoding candidate choices is a hard task and has room for improvement. For the rest of the evaluation, we present results with multi-choice questions."
  }, {
    "heading": "5.4 Query Complexity and Dialogue Length",
    "text": "In Table 4, we compare DialSQL to the OM strategy on query complexity (QC) and dialogue length (DL) metrics. DialSQL and SQLNet-OM both have very similar query complexity scores showing that DialSQL produces simple questions. The number of questions DialSQL asks is around 3 for both query generation models. Even though SQLNet-OM dialogues have much smaller dialogue lengths, we attribute this to the fact that 61.3% of the dialogues have empty interactions since OM will match every segment in the generated query and the ground truth. The average number of turns in dialogues with non-empty interactions, on the other hand, is 3.10 which is close to DialSQL."
  }, {
    "heading": "5.5 A Varying Number of Choices",
    "text": "In Figure 3, we plot the accuracy of DialSQL on WikiSQL with a varying number of choices at each turn. We train DialSQL once and generate a different number of choices at each turn by offering top-k candidates during testing. We observe that offering even a single candidate improves the performance of SQLNet remarkably, 1.9% and\n2.5% for development and test sets, respectively. As the number of choices increases, the performance of DialSQL improves in all the cases. Particularly, for the SQLNet-DialSQL model we observe more accuray gain. We increased the number of choices to 10 and observed no notable further improvement in the development set which suggests that 5 is a good value for the number of choices."
  }, {
    "heading": "5.6 Error Distribution",
    "text": "We examine the error distribution of DialSQL and SQLNet. In DialSQL, almost all the errors are caused by validate sel and validate where change, while in SQLNet validate where change is the major cause of error and other errors are distributed uniformly."
  }, {
    "heading": "5.7 Human Evaluation",
    "text": "We extend our evaluation of DialSQL using human subject experiment so that real users interact with the system instead of our simulated user. We randomly pick 100 questions from WikiSQL development set and run SQLNet to generate initial candidate queries. Next, we run DialSQL using these candidate queries to generate 100 dialogues, each of which is evaluated\nby 3 different users. At each turn, we show users the headers of the corresponding table, original question, system response, and list of candidate choices for users to pick. For each error category, we generate 5 choices except for the validate where added category for which we only show 2 choices (YES or NO). Also, we add an additional choice of None of the above so that users can keep the previous prediction unchanged. At the end of each turn, we also ask users to give an overall score between 1 and 3 to evaluate whether they had a successful interaction with the DialSQL for the current turn. On average, the length of the generated dialogues is 5.6.\nIn Table 5, we compare the performance of SQLNet, DialSQL with user simulation, and DialSQL with real users using QM metric. We present the average performance across 3 different users with the standard deviation estimated over all dialogues. We observe that when real users interact with our system, the overall performance of the generated queries are better than SQLNet model showing that DialSQL can improve the performance of a strong NLIDB system in a real setting. However, there is still a large room for improvement between simulated dialogues and real users.\nIn Figure 4, we present the correlation between DialSQL ranking of the candidate choices and user preferences. We observe that, user answers and\nDialSQL rankings are positively correlated; most of the time users prefer the top-1 choice. Interestingly, 15% of the user answers is None of the above. This commonly happens in the scenario where DialSQL response asks to replace a correct condition and users prefer to keep the original prediction unchanged. Another scenario where users commonly select None of the above is when table headers without the content remain insufficient for users to correctly disambiguate condition values from questions. We also compute the Mean Reciprocal Rank (MMR) for each user to measure the correlation between real users and DialSQL. Average MMR is 0.69 with standard deviation of 0.004 which also shows that users generally prefer the choices ranked higher by DialSQL. The overall score of each turn also suggests that users had a reasonable conversation with DialSQL. The average score is 2.86 with standard deviation of 0.14, showing users can understand DialSQL responses and can pick a choice confidently."
  }, {
    "heading": "6 Conclusion",
    "text": "We demonstrated the efficacy of the DialSQL, improving the state of the art accuracy from 62.5% to 69.0% on the WikiSQL dataset. DialSQL successfully extracts error spans from queries and offers several alternatives to users. It generates simple questions over a small number of turns without overwhelming users. The model learns from only simulated data which makes it easy to adapt to new domains. We further investigate the usability of DialSQL in a real life setting by conducting human evaluations. Our results suggest that the accuracy of the generated queries can be improved via real user feedback."
  }, {
    "heading": "Acknowledgements",
    "text": "The authors would like to thank the anonymous reviewers for their thoughtful comments. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053 and NSF 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein."
  }],
  "year": 2018,
  "references": [{
    "title": "Tensorflow: A system for large-scale machine learning",
    "authors": ["Paul Tucker", "Vijay Vasudevan", "Pete Warden", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."],
    "venue": "12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265–",
    "year": 2016
  }, {
    "title": "Natural language interfaces to databases–an introduction",
    "authors": ["Ion Androutsopoulos", "Graeme D Ritchie", "Peter Thanisch."],
    "venue": "Natural language engineering, 1(1):29–81.",
    "year": 1995
  }, {
    "title": "A sequence-to-sequence model for user simulation in spoken dialogue systems",
    "authors": ["Layla El Asri", "Jing He", "Kaheer Suleman."],
    "venue": "CoRR, abs/1607.00070.",
    "year": 2016
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.",
    "year": 2013
  }, {
    "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
    "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."],
    "venue": "CoRR, abs/1406.1078.",
    "year": 2014
  }, {
    "title": "Language to logical form with neural attention",
    "authors": ["Li Dong", "Mirella Lapata."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
    "year": 2016
  }, {
    "title": "Bidirectional Attention for SQL Generation",
    "authors": ["T. Guo", "H. Gao."],
    "venue": "ArXiv e-prints.",
    "year": 2018
  }, {
    "title": "Accurate supervised and semi-supervised machine reading for long documents",
    "authors": ["Izzeddin Gur", "Daniel Hewlett", "Llion Jones", "Alexandre Lacoste."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.",
    "year": 2017
  }, {
    "title": "Learning a neural semantic parser from user feedback",
    "authors": ["Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Jayant Krishnamurthy", "Luke Zettlemoyer."],
    "venue": "CoRR, abs/1704.08760.",
    "year": 2017
  }, {
    "title": "Learning to transform natural to formal languages",
    "authors": ["Rohit J Kate", "Yuk Wah Wong", "Raymond J Mooney."],
    "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
    "year": 2005
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Diederik P. Kingma", "Jimmy Ba."],
    "venue": "CoRR, abs/1412.6980.",
    "year": 2014
  }, {
    "title": "Constructing an interactive natural language interface for relational databases",
    "authors": ["F. Li", "H.V. Jagadish."],
    "venue": "Proc. VLDB Endow., 8(1):73–84.",
    "year": 2014
  }, {
    "title": "Dialogue learning with human-in-the-loop",
    "authors": ["Jiwei Li", "Alexander H. Miller", "Sumit Chopra", "Marc’Aurelio Ranzato", "Jason Weston"],
    "year": 2016
  }, {
    "title": "Effective approaches to attention-based neural machine translation",
    "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "EMNLP.",
    "year": 2015
  }, {
    "title": "The stanford corenlp natural language processing toolkit",
    "authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."],
    "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Lin-",
    "year": 2014
  }, {
    "title": "Glove: Global vectors for word representation",
    "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."],
    "venue": "In EMNLP.",
    "year": 2014
  }, {
    "title": "Towards a theory of natural language interfaces to databases",
    "authors": ["Ana-Maria Popescu", "Oren Etzioni", "Henry Kautz."],
    "venue": "Proceedings of the 8th international conference on Intelligent user interfaces, pages 149–157. ACM.",
    "year": 2003
  }, {
    "title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph",
    "authors": ["A. Saha", "V. Pahuja", "M.M. Khapra", "K. Sankaranarayanan", "S. Chandar."],
    "venue": "ArXiv e-prints.",
    "year": 2018
  }, {
    "title": "Agenda-based user simulation for bootstrapping a POMDP dialogue system",
    "authors": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young."],
    "venue": "Human Language Technologies 2007: The Conference of the North American Chap-",
    "year": 2007
  }, {
    "title": "Hierarchical neural network generative models for movie dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau."],
    "venue": "CoRR, abs/1507.04808.",
    "year": 2015
  }, {
    "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
    "authors": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."],
    "venue": "CoRR, abs/1605.06069.",
    "year": 2016
  }, {
    "title": "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
    "authors": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie."],
    "venue": "CoRR, abs/1507.02221.",
    "year": 2015
  }, {
    "title": "On generating characteristic-rich question sets for qa evaluation",
    "authors": ["Yu Su", "Huan Sun", "Brian Sadler", "Mudhakar Srivatsa", "Izzeddin Gur", "Zenghui Yan", "Xifeng Yan."],
    "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
    "year": 2016
  }, {
    "title": "Table cell search for question answering",
    "authors": ["Huan Sun", "Hao Ma", "Xiaodong He", "Wen-tau Yih", "Yu Su", "Xifeng Yan."],
    "venue": "Proceedings of the International Conference on World Wide Web.",
    "year": 2016
  }, {
    "title": "Progress in natural language understanding: an application to lunar geology",
    "authors": ["William A Woods."],
    "venue": "Proceedings of the American Federation of Information Processing Societies Conference.",
    "year": 1973
  }, {
    "title": "Sqlnet: Generating structured queries from natural language without reinforcement learning",
    "authors": ["Xiaojun Xu", "Chang Liu", "Dawn Song."],
    "venue": "CoRR, abs/1711.04436.",
    "year": 2017
  }, {
    "title": "Sqlizer: Query synthesis from natural language",
    "authors": ["Navid Yaghmazadeh", "Yuepeng Wang", "Isil Dillig", "Thomas Dillig."],
    "venue": "Proc. ACM Program. Lang., 1(OOPSLA):63:1–63:26.",
    "year": 2017
  }, {
    "title": "Improving semantic parsing via answer type inference",
    "authors": ["Semih Yavuz", "Izzeddin Gur", "Yu Su", "Mudhakar Srivatsa", "Xifeng Yan."],
    "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.",
    "year": 2016
  }, {
    "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base",
    "authors": ["Scott Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."],
    "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-",
    "year": 2015
  }, {
    "title": "Learning to parse database queries using inductive logic programming",
    "authors": ["John M Zelle", "Mooney Ray."],
    "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
    "year": 1996
  }, {
    "title": "Online learning of relaxed ccg grammars for parsing to logical form",
    "authors": ["Luke Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
    "year": 2007
  }, {
    "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
    "authors": ["Luke S. Zettlemoyer", "Michael Collins."],
    "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence, pages 658–666.",
    "year": 2005
  }, {
    "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
    "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher."],
    "venue": "CoRR, abs/1709.00103.",
    "year": 2017
  }],
  "id": "SP:66491e11b684383c833f8855ccb6605ea112725f",
  "authors": [{
    "name": "Izzeddin Gur",
    "affiliations": []
  }, {
    "name": "Semih Yavuz",
    "affiliations": []
  }, {
    "name": "Yu Su",
    "affiliations": []
  }, {
    "name": "Xifeng Yan",
    "affiliations": []
  }],
  "abstractText": "The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialoguebased structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.",
  "title": "DialSQL: Dialogue Based Structured Query Generation"
}