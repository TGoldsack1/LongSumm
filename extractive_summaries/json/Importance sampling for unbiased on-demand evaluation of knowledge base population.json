{
  "sections": [{
    "text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1038–1048 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Harnessing the wealth of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014;\n∗ Authors contributed equally.\nReddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015).\nEvaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, annotated and released as a dataset for researchers to develop and evaluate their systems on. However, during development, if a new system predicts a previously unseen relation it is considered to be wrong even if it is correct. The discrepancy between a system’s true score and the score on the pooled dataset is called pooling bias and is typically assumed to be insignificant in practice (Zobel, 1998).\nThe key finding of this paper contradicts this assumption and shows that the pooling bias is actu-\n1038\nally significant, and it penalizes newly developed systems by 2% F1 on average (Section 3). Novel improvements, which typically increase scores by less than 1% F1 on existing datasets, are therefore likely to be clouded by pooling bias during development. Worse, the bias is larger for a system which predicts qualitatively different relations systematically missing from the pool. Of course, systems participating in the TAC KBP evaluation do not suffer from pooling bias, but this requires researchers to wait a year to get credible feedback on new ideas.\nThis bias is particularly counterproductive for machine learning methods as they are trained assuming the pool is the complete set of positives. Predicting unseen relations and learning novel patterns is penalized. The net effect is that researchers are discouraged from developing innovative approaches, in particular from applying machine learning, thereby slowing progress on the task.\nOur second contribution, described in Section 4, addresses this bias through a new evaluation methodology, on-demand evaluation, which avoids pooling bias by querying crowdworkers, while minimizing cost by leveraging previous systems’ predictions when possible. We then compute the new system’s score based on the predictions of past systems using importance weighting. As more systems are evaluated, the marginal cost of evaluating a new system decreases. We show how the on-demand evaluation methodology can be applied to knowledge base population in Section 5. Through a simulated experiment on evaluation data released through the TAC KBP 2015 Slot Validation track, we show that we are able to obtain unbiased estimates of a new systems score’s while significantly reducing variance.\nFinally, our third contribution is an implementation of our framework as a publicly available evaluation service at https://kbpo.stanford. edu, where researchers can have their own KBP systems evaluated. The data collected through the evaluation process could even be valuable for relation extraction, entity linking and coreference, and will also be made publicly available through the website. We evaluate three systems on the 2016 TAC KBP corpus for about $150 each (a fraction of the cost of official evaluation). We believe the public availability of this service will speed the pace of progress in developing KBP systems."
  }, {
    "heading": "2 Background",
    "text": "In knowledge base population, each relation is a triple (SUBJECT, PREDICATE, OBJECT) where SUBJECT and OBJECT are some globally unique entity identifiers (e.g. Wikipedia page titles) and PREDICATE belongm to a specified schema.1 A KBP system returns an output in the form of relation instances (SUBJECT, PREDICATE, OBJECT, PROVENANCE), where PROVENANCE is a description of where exactly in the document corpus the relation was found. In the example shown in Figure 1, CARRIE FISHER and DEBBIE REYNOLDS are identified as the subject and object, respectively, of the predicate CHILD OF, and the whole sentence is provided as provenance. The provenance also identifies that CARRIE FISHER is referenced by Fisher within the sentence. Note that the same relation can be expressed in multiple sentences across the document corpus; each of these is a different relation instance.\nPooled evaluation. The primary source of evaluation data for KBP comes from the annual TAC KBP competition organized by NIST (Ji et al.,\n1The TAC KBP guidelines specify a total of 65 predicates (including inverses) such as per:title or org:founded on, etc. Subject entities can be people, organizations, geopolitical entities, while object entities also include dates, numbers and arbitrary string-values like job titles.\n2011). Let E be a held-out set of evaluation entities. There are two steps performed in parallel: First, each participating system is run on the document corpus to produce a set of relation instances; those whose subjects are in E are labeled as either positive or negative by annotators. Second, a team of annotators identify and label correct relation instances for the evaluation entities E by manually searching the document corpus within a time budget (Ellis et al., 2012). These labeled relation instances from the two steps are combined and released as the evaluation dataset. In the example in Figure 2, systems A and B were used in constructing the pooling dataset, and there are 3 distinct relations in the dataset, between s1 and o1, o2, o3.\nA system is evaluated on the precision of its predicted relation instances for the evaluation entities E and on the recall of the corresponding predicted relations (not instances) for the same entities (see Figure 2 for a worked example). When using the evaluation data during system development, it is common practice to use the more lenient anydoc score that ignores the provenance when checking if a relation instance is true. Under this metric, predicting the relation (CARRIE FISHER, CHILD OF, DEBBIE REYNOLDS) from an ambiguous provenance like “Carrie Fisher and Debbie Reynolds arrived together at the awards show” would be considered correct even though it would be marked wrong under the official metric."
  }, {
    "heading": "3 Measuring pooling bias",
    "text": "The example in Figure 2 makes it apparent that pooling-based evaluation can introduce a systematic bias against unpooled systems. However, it has been assumed that the bias is insignificant in practice given the large number of systems pooled in the TAC KBP evaluation. We will now show that the assumption is not valid using data from the TAC KBP 2015 evaluation.2\nMeasuring bias. In total, there are 70 system submissions from 18 teams for 317 evaluation entities (E) and the evaluation set consists of 11,008 labeled relation instances.3 The original evalua-\n2Our results are not qualitatively different on data from previous years of the shared task.\n3The evaluation set is actually constructed from compositional queries like, “what does Carrie Fisher’s parents do?”: these queries select relation instances that answer the question “who are Carrie Fisher’s parents?”, and then use those answers (e.g. “Debbie Reynolds”) to select relation instances that answer “what does Debbie Reynolds do?”. We only con-\ntion dataset gives us a good measure of the true scores for the participating systems. Similar to Zobel (1998), which studied pooling bias in information retrieval, we simulate the condition of a team not being part of the pooling process by removing any predictions that are unique to its systems from the evaluation dataset. The pooling bias is then the difference between the true and unpooled scores.\nResults. Figure 3 shows the results of measuring pooling bias on the TAC KBP 2015 evaluation on the F1 metric using the official and anydoc scores.45 We observe that even with lenient anydoc heuristic, the median bias (2.05% F1) is much larger than largest difference between adjacently ranked systems (1.5% F1). This experiment shows that pooling evaluation is significantly and systematically biased against systems that make novel predictions!\nsider instances selected in the first part of this process. 4We note that anydoc scores are on average 0.88%F1 larger than the official scores. 5 The outlier at rank 36 corresponds to a University of Texas, Austin system that only filtered predictions from other systems and hence has no unique predictions itself."
  }, {
    "heading": "4 On-demand evaluation with importance sampling",
    "text": "Pooling bias is fundamentally a sampling bias problem where relation instances from new systems are underrepresented in the evaluation dataset. We could of course sidestep the problem by exhaustively annotating the entire document corpus, by annotating all mentions of entities and checking relations between all pairs of mentions. However, that would be a laborious and prohibitively expensive task: using the interfaces we’ve developed (Section 6), it costs about $15 to annotate a single document by non-expert crowdworkers, resulting in an estimated cost of at least $1,350,000 for a reasonably large corpus of 90,000 documents (Dang, 2016). The annotation effort would cost significantly more with expert annotators. In contrast, labeling relation instances from system predictions can be an order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about $0.18 to verify each relation instance compared to $1.60 per instance extracted through exhaustive annotations.\nWe propose a new paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from systems only when they are underrepresented, thus correcting for pooling bias as it arises. In this section, we’ll formalize the problem solved by ondemand evaluation independent of KBP and describe a cost-effective solution that allows us to accurately estimate evaluation scores without bias using importance sampling. We’ll then instantiate the framework for KBP in Section 5."
  }, {
    "heading": "4.1 Problem statement",
    "text": "Let X be the universe of (relation) instances, Y ⊆ X be the unknown subset of correct instances, X1, . . . Xm ⊆ X be the predictions for m systems, and let Yi = Xi ∩ Y . Let X = ⋃m i=1Xi and Y = ⋃m\ni=1 Yi. Let f(x) def = I[x ∈ Y] and\ngi(x) = I[x ∈ Xi], then the precision, πi, and recall, ri, of the set of predictions Xi is\nπi def = Ex∼pi [f(x)] ri def = Ex∼p0 [gi(x)],\nwhere pi is a distribution over Xi and p0 is a distribution over Y . We assume that pi is known, e.g. the uniform distribution overXi and that we know p0 up to normalization constant and can sample from it.\nIn on-demand evaluation, we can query f(x) (e.g. labeling an instance) or draw a sample from p0; typically, querying f(x) is significantly cheaper than sampling from p0. We obtain prediction sets X1, . . . , Xm sequentially as the systems are submitted for evaluation. Our goal is to estimate πi and ri for each system i = 1, . . . ,m."
  }, {
    "heading": "4.2 Simple estimators",
    "text": "We can estimate each πi and ri independently with simple Monte Carlo integration. Let X̂1, . . . , X̂m be multi-sets of n1, . . . , nj i.i.d. samples from X1, . . . , Xm respectively, and let Ŷ0 be a multiset of n0 samples drawn from Y . Then, the simple estimators for precision and recall are:\nπ̂ (simple) i =\n1\nni\n∑\nx∈X̂i\nf(x) r̂ (simple) i =\n1\nn0\n∑\nx∈Ŷ0\ngi(x).\n4.3 Joint estimators6\nThe simple estimators are unbiased but have wastefully large variance because evaluating a new system does not leverage labels acquired for previous systems.\nOn-demand evaluation with the joint estimator works as follows: First Ŷ0 is randomly sampled from Y once when the evaluation framework is launched. For every new set of predictions Xm submitted for evaluation, the minimum number of samples nm required to accurately evaluate Xm is calculated based on the current evaluation data, Ŷ0 and X̂1, . . . , X̂m−1. Then, the set X̂m is added to the evaluation data by evaluating f(x) on nm samples drawn from Xm. Finally, estimates πi and ri are updated for each system i = 1, . . . ,m using the joint estimators that will be defined next. In the rest of this section, we will answer the following three questions:\n1. How can we use all the samples X̂1, . . . X̂m when estimating the precision πi of system i?\n2. How can we use all the samples X̂1, . . . , X̂m with Ŷ0 when estimating recall ri?\n3. Finally, to form X̂m, how many samples should we draw fromXm given existing samples and X̂1, . . . , X̂m−1 and Ŷ0?\nEstimating precision jointly. Intuitively, if two systems have very similar predictions Xi and Xj ,\n6Proofs for claims made in this section can be found in Appendix B of the supplementary material.\nwe should be able to use samples from one to estimate precision on the other. However, it might also be the case that Xi and Xj only overlap on a small region, in which case the samples from Xj do not accurately represent instances in Xi and could lead to a biased estimate. We address this problem by using importance sampling (Owen, 2013), a standard statistical technique for estimating properties of one distribution using samples from another distribution.\nIn importance sampling, if X̂i is sampled from qi, then 1ni ∑ x∈X̂i pi(x) qi(x)\nf(x) is an unbiased estimate of πi. We would like the proposal distribution qi to both leverage samples from all m systems and be tailored towards system i. To this end, we first define a distribution over systems j, represented by probabilities wij . Then, define qi as sampling a j and drawing x ∼ pj ; formally qi(x) = ∑m j=1wijpj(x).\nWe note that qi(x) not only significantly differs between systems, but also changes as new systems are added to the evaluation pool. Unfortunately, the standard importance sampling procedure requires us to draw and use samples from each distribution qi(x) independently and thus can not effectively reuse samples drawn from different distributions. To this end, we introduce a practical refinement to the importance sampling procedure: we independently draw nj samples according to pj(x) from each of the m systems independently and then numerically integrate over these samples using the weights wij to “mix” them appropriately to produce and unbiased estimate of πi while reducing variance. Formally, we define the joint precision estimator:\nπ̂ (joint) i def =\nm∑\nj=1\nwij nj\n∑\nx∈X̂j\npi(x)f(x)\nqi(x) ,\nwhere each X̂j consists of nj i.i.d. samples drawn from pj .\nIt is a hard problem to determine what the optimal mixing weights wij should be. However, we can formally verify that if Xi and Xj are disjoint, then wij = 0 minimizes the variance of πi, and if Xi = Xj , then wij ∝ nj is optimal. This motivates the following heuristic choice which interpolates between these two extremes: wij ∝ nj ∑ x∈X pj(x)pi(x).\nEstimating recall jointly. The recall of system i can be expressed can be expressed as a product\nri = θνi, where θ is the recall of the pool, which measures the fraction of all positive instances predicted by the pool (any system), and νi is the pooled recall of system i, which measures the fraction of the pool’s positive instances predicted by system i. Letting g(x) def= I[x ∈ X], we can define these as:\nνi def = Ex∼p0 [gi(x) | x ∈ X] θ def = Ex∼p0 [g(x)].\nWe can estimate θ analogous to the simple recall estimator r̂i, except we use the pool g instead a system gi. For νi, the key is to leverage the work from estimating precision. We already evaluated f(x) on X̂i, so we can compute Ŷi def = X̂i ∩Y and\nform the subset Ŷ = ⋃m\ni=1 Ŷi. Ŷ is an approximation of Y whose bias we can correct through importance reweighting. We then define estimators as follows:\nν̂i def =\n∑m j=1 wij nj ∑ x∈Ŷj\np0(x)gi(x) qi(x)∑m\nj=1 wij nj ∑ x∈Ŷj p0(x) qi(x)\nr̂ (joint) i def = θ̂ν̂i θ̂ def =\n1\nn0\n∑\nx∈Ŷ0\ng(x).\nwhere qi and wij are the same as before.\nAdaptively choosing the number of samples. Finally, a desired property for on-demand evaluation is to label new instances only when the current evaluation data is insufficient, e.g. when a new set of predictionsXm contains many instances not covered by other systems. We can measure how well the current evaluation set covers the predictions Xm by using a conservative estimate of the variance of π̂(joint)m .7 In particular, the variance of π̂(joint)m is a monotonically decreasing function in nm, the number of samples drawn from Xm. We can easily solve for the minimum number of samples required to estimate π̂(joint)m within a confidence interval by using the bisection method (Burden and Faires, 1985)."
  }, {
    "heading": "5 On-demand evaluation for KBP",
    "text": "Applying the on-demand evaluation framework to a task requires us to answer three questions:\n1. What is the desired distribution over system predictions pi?\n7Further details can be found in Appendix B of the supplementary material.\n2. How do we label an instance x, i.e. check if x ∈ Y?\n3. How do we sample from the unknown set of true instances x ∼ p0?\nIn this section, we present practical implementations for knowledge base population."
  }, {
    "heading": "5.1 Sampling from system predictions",
    "text": "Both the official TAC-KBP evaluation and the on-demand evaluation we propose use microaveraged precision and recall as metrics. However, in the official evaluation, these metrics are computed over a fixed set of evaluation entities chosen by LDC annotators, resulting in two problems: (a) defining evaluation entities requires human intervention and (b) typically a large source of variability in evaluation scores comes from not having enough evaluation entities (see e.g. (Webber, 2010)). In our methodology, we replace manually chosen evaluation entities by sampling entities from each system’s output according pi. In effect, pi makes explicit the decision process of the annotator who chooses evaluation entities.\nIdentifying a reasonable distribution pi is an important implementation decision that depends on what one wishes to evaluate. Our goal for the ondemand evaluation service we have implemented is to ensure that KBP systems are fairly evaluated on diverse subjects and predicates, while at the same time, ensuring that entities with multiple relations are represented to measure completeness of knowledge base entries. As a result, we propose a distribution that is inversely proportional to the frequency of the subject and predicate and is proportional to the number of unique relations identified for an entity (to measure knowledge base completeness). See Appendix A in the supplementary material for an analysis of this distribution and a study of other potential choices."
  }, {
    "heading": "5.2 Labeling predicted instances",
    "text": "We label predicted relation instances by presenting the instance’s provenance to crowdworkers and asking them to identify if a relation holds between the identified subject and object mentions (Figure 4a). Crowdworkers are also asked to link the subject and object mentions to their canonical mentions within the document and to pages on Wikipedia, if possible, for entity linking. On average, we find that crowdworkers are able to perform this task in about 20 seconds, correspond-\ning to about $0.05 per instance. We requested 5 crowdworkers to annotate a small set of 200 relation instances from the 2015 TAC-KBP corpus and measured a substantial inter-annotator agreement with a Fleiss’ kappa of 0.61 with 3 crowdworkers and 0.62 with 5. Consequently, we take a majority vote over 3 workers in subsequent experiments."
  }, {
    "heading": "5.3 Sampling true instances",
    "text": "Sampling from the set of true instances Y is difficult because we can’t even enumerate the elements of Y . As a proxy, we assume that relations are identically distributed across documents and have crowdworkers annotate a random subset of documents for relations using an interface we developed (Figure 4b). Crowdworkers begin by identifying every mention span in a document. For each mention, they are asked to identify its type, canonical mention within the document and associated Wikipedia page if possible. They are then presented with a separate interface to label predicates between pairs of mentions within a sentence that were identified earlier.\nWe compare crowdsourced annotations against those of expert annotators using data from the TAC KBP 2015 EDL task on 10 randomly chosen documents. We find that 3 crowdworkers together identify 92% of the entity spans identified by expert annotators, while 7 crowdworkers together identify 96%. When using a token-level majority vote to identify entities, 3 crowdworkers identify about 78% of the entity spans; this number does not change significantly with additional crowdworkers. We also measure substantial token-level interannotator agreement using Fleiss’ kappa for identifying typed mention spans (κ = 0.83), canonical mentions (κ = 0.75) and entity links (κ = 0.75) with just three workers. Based on this analysis, we use token-level majority over 3 workers in subsequent experiments.\nThe entity annotation interface is far more involved and takes on average about 13 minutes per document, corresponding to about $2.60 per document, while the relation annotation interface takes on average about $2.25 per document. Because documents vary significantly in length and complexity, we set rewards for each document based on the number of tokens (.75c per token) and mention pairs (5c per pair) respectively. With 3 workers per document, we paid about $15 per document on average. Each document contained an average\n9.2 relations, resulting in a cost of about $1.61 per relation instance. We note that this is about ten times as much as labeling a relation instance.\nWe defer details regarding how documents themselves should be weighted to capture diverse entities that span documents to Appendix A."
  }, {
    "heading": "6 Evaluation",
    "text": "Let us now see how well on-demand evaluation works in practice. We begin by empirically studying the bias and variance of the joint estimator proposed in Section 4 and find it is able to correct for pooling bias while significantly reducing variance in comparison with the simple estimator. We then demonstrate that on-demand evaluation can serve as a practical replacement for the TAC KBP evaluations by piloting a new evaluation service we have developed to evaluate three distinct systems on TAC KBP 2016 document corpus."
  }, {
    "heading": "6.1 Bias and variance of the on-demand evaluation.",
    "text": "Once again, we use the labeled system predictions from the TAC KBP 2015 evaluation and treat them as an exhaustively annotated dataset. To evaluate the pooling methodology we construct an evaluation dataset using instances found by human annotators and labeled instances pooled from 9 randomly chosen teams (i.e. half the total number of participating teams), and use this dataset to evaluate the remaining 9 teams. On average, the pooled evaluation dataset contains between 5,000 and 6,000 labeled instances and evaluates 34 different systems (since each team may have submitted multiple systems). Next, we evaluated sets of 9 randomly chosen teams with our proposed simple and joint estimators using a total of 5,000 samples: about 150 of these samples are drawn from Y , i.e. the full TAC KBP 2015 evaluation data, and 150 samples from each of the systems being evaluated.\nWe repeat the above simulated experiment 500 times and compare the estimated precision and recall with their true values (Figure 4). The simulations once again highlights that the pooled methodology is biased, while the simple and joint estimators are not. Furthermore, the joint estimators significantly reduce variance relative to the simple estimators: the median 90% confidence intervals reduce from 0.14 to 0.06 precision and from 0.14 to 0.08 for recall."
  }, {
    "heading": "6.2 Number of samples required by on-demand evaluation",
    "text": "Separately, we evaluate the efficacy of the adaptive sample selection method described in Section 4.3 through another simulated experiment. In each trial of this experiment, we evaluate the top 40 systems in random order. As each subsequent system is evaluated, the number of samples to pick from the system is chosen to meet a target variance and added to the current pool of labeled instances. To make the experiment more interpretable, we choose the target variance to correspond with the estimated variance of having 500 samples. Figure 4 plots the results of the experiment. The number of samples required to estimate systems quickly drops off from the benchmark of 500 samples as the pool of labeled instances covers more systems. This experiment shows that on-demand evaluation using joint estimation can scale up to an order of magnitude more submissions than a simple estimator for the same cost."
  }, {
    "heading": "6.3 A mock evaluation for TAC KBP 2016",
    "text": "We have implemented the on-demand evaluation framework described here as an evaluation service to which researchers can submit their own system predictions. As a pilot of the service, we evaluated three relation extraction systems that also participated in the official 2016 TAC KBP competition. Each system uses Stanford CoreNLP (Manning et al., 2014) to identify entities, the Illinois Wikifier (Ratinov et al., 2011) to perform entity linking and a combination of a rule-based system (P), a logistic classifier (L), and a neural network classifier (N) for relation extraction. We used 15,000 Newswire documents from the 2016 TAC KBP evaluation as our document corpus. In total, 100 documents were exhaustively annotated for about $2,000 and 500 instances from each system were labeled for about $150 each. Evaluating all three system only took about 2 hours.\nFigure 4f reports scores obtained through ondemand evaluation of these systems as well as their corresponding official TAC evaluation scores. While the relative ordering of systems between the two evaluations is the same, we note that precision and recall as measured through ondemand evaluation are respectively higher and lower than the official scores. This is to be expected because on-demand evaluation measures precision using each systems output as opposed\nto an externally defined set of evaluation entities. Likewise, recall is measured using exhaustive annotations of relations within the corpus instead of annotations from pooled output in the official evaluation."
  }, {
    "heading": "7 Related work",
    "text": "The subject of pooling bias has been extensively studied in the information retrieval (IR) community starting with Zobel (1998), which examined the effects of pooling bias on the TREC AdHoc task, but concluded that pooling bias was not a significant problem. However, when the topic was later revisited, Buckley et al. (2007) identified that the reason for the small bias was because the submissions to the task were too similar; upon repeating the experiment using a novel system as part of the TREC Robust track, they identified a 23% point drop in average precision scores!8\nMany solutions to the pooling bias problem have been proposed in the context of information retrieval, e.g. adaptively constructing the pool to collect relevant data more cost-effectively (Zobel, 1998; Cormack et al., 1998; Aslam et al., 2006), or modifying the scoring metrics to be less sensitive to unassessed data (Buckley and Voorhees, 2004; Sakai and Kando, 2008; Aslam et al., 2006). Many of these ideas exploit the ranking of documents in IR which does not apply to KBP. While both Aslam et al. (2006) and Yilmaz et al. (2008) estimate evaluation metrics by using importance sampling estimators, the techniques they propose require knowing the set of all submissions beforehand. In contrast, our on-demand methodology can produce unbiased evaluation scores for new development systems as well.\nThere have been several approaches taken to crowdsource data pertinent to knowledge base population (Vannella et al., 2014; Angeli et al., 2014; He et al., 2015; Liu et al., 2016). The most extensive annotation effort is probably Pavlick et al. (2016), which crowdsources a knowledge base for gun-violence related events. In contrast to previous work, our focus is on evaluating systems, not collecting a dataset. Furthermore, our main contribution is not a large dataset, but an evaluation service that allows anyone to use crowdsourcing predictions made by their system.\n8For the interested reader, Webber (2010) presents an excellent survey of the literature on pooling bias."
  }, {
    "heading": "8 Discussion",
    "text": "Over the last ten years of the TAC KBP task, the gap between human and system performance has barely narrowed despite the community’s best efforts: top automated systems score less than 36% F1 while human annotators score more than 60%. In this paper, we’ve shown that the current evaluation methodology may be a contributing factor because of its bias against novel system improvements. The new on-demand framework proposed in this work addresses this problem by obtaining human assessments of new system output through crowdsourcing. The framework is made economically feasible by carefully sampling output to be assessed and correcting for sample bias through importance sampling.\nOf course, simply providing better evaluation scores is only part of the solution and it is clear that better datasets are also necessary. However, the very same difficulties in scale that make evaluating KBP difficult also make it hard to collect a high quality dataset for the task. As a result, existing datasets (Angeli et al., 2014; Adel et al., 2016) have relied on the output of existing systems, making it likely that they exhibit the same biases against novel systems that we’ve discussed in this paper. We believe that providing a fair and standardized evaluation platform as a service allows researchers to exploit such datasets and while still being able to accurately measure their performance on the knowledge base population task.\nThere are many other tasks in NLP that are even harder to evaluate than KBP. Existing evaluation metrics for tasks with a generation component— such as summarization or dialogue—leave much to be desired. We believe that adapting the ideas of this paper to those tasks is a fruitful direction, as progress of a research community is strongly tied to the fidelity of evaluation."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Yuhao Zhang, Hoa Deng, Eduard Hovy, and Jacob Steinhardt for discussions, William E. Webber for his excellent thesis that helped shape this project and the anonymous reviewers for their detailed and pertinent feedback. The first and second authors are supported under DARPA DEFT program under ARFL prime contract no. FA8750-13-2-0040."
  }],
  "year": 2017,
  "references": [{
    "title": "Comparing convolutional neural networks to traditional models for slot filling",
    "authors": ["H. Adel", "B. Roth", "H. Schütze."],
    "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).",
    "year": 2016
  }, {
    "title": "Combining distant and partial supervision for relation extraction",
    "authors": ["G. Angeli", "J. Tibshirani", "J.Y. Wu", "C.D. Manning."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2014
  }, {
    "title": "A statistical method for system evaluation using incomplete judgments",
    "authors": ["J.A. Aslam", "V. Pavlu", "E. Yilmaz."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR), pages 541–548.",
    "year": 2006
  }, {
    "title": "Semantic parsing on Freebase from question-answer pairs",
    "authors": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2013
  }, {
    "title": "Bias and the limits of pooling for large collections",
    "authors": ["C. Buckley", "D. Dimmick", "I. Soboroff", "E. Voorhees."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR).",
    "year": 2007
  }, {
    "title": "Retrieval evaluation with incomplete information",
    "authors": ["C. Buckley", "E.M. Voorhees."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR), pages 25–32.",
    "year": 2004
  }, {
    "title": "Numerical Analysis (3rd ed.)",
    "authors": ["R.L. Burden", "J.D. Faires"],
    "year": 1985
  }, {
    "title": "Efficient construction of large test collections",
    "authors": ["G.V. Cormack", "C.R. Palmer", "C.L.A. Clarke."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR).",
    "year": 1998
  }, {
    "title": "Cold start knowledge base population at TAC KBP 2016",
    "authors": ["H.T. Dang."],
    "venue": "Text Analytics Conference.",
    "year": 2016
  }, {
    "title": "Linguistic resources for 2012 knowledge base population evaluations",
    "authors": ["J. Ellis", "X. Li", "K. Griffitt", "S.M. Strassel."],
    "venue": "Text Analytics Conference.",
    "year": 2012
  }, {
    "title": "Open question answering over curated and extracted knowledge bases",
    "authors": ["A. Fader", "L. Zettlemoyer", "O. Etzioni."],
    "venue": "International Conference on Knowledge Discovery and Data Mining (KDD), pages 1156–1165.",
    "year": 2014
  }, {
    "title": "Exploiting knowledge base to generate responses for natural language dialog listening agents",
    "authors": ["S. Han", "J. Bang", "S. Ryu", "G.G. Lee."],
    "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 129–133.",
    "year": 2015
  }, {
    "title": "The first text retrieval conference (trec-1) rockville, md, u.s.a",
    "authors": ["D.K. Harman"],
    "venue": "november,",
    "year": 1993
  }, {
    "title": "Questionanswer driven semantic role labeling: Using natural language to annotate natural language",
    "authors": ["L. He", "M. Lewis", "L. Zettlemoyer."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
    "year": 2015
  }, {
    "title": "Overview of the TAC 2011 knowledge base population track",
    "authors": ["H. Ji", "R. Grishman", "H. Trang Dang."],
    "venue": "Text Analytics Conference.",
    "year": 2011
  }, {
    "title": "Report on the need for and provision of an “ideal test collection",
    "authors": ["K.S. Jones", "C.V. Rijsbergen."],
    "venue": "Information Retrieval Test Collection.",
    "year": 1975
  }, {
    "title": "Structured data and inference in deepqa",
    "authors": ["A. Kalyanpur", "B.K. Boguraev", "S. Patwardhan", "J.W. Murdock", "A. Lally", "C.A. Welty", "J.M. Prager", "B. Coppola", "A. Fokoue-Nkoutche", "L. Zhang", "Y. Pan", "Z.M. Qui."],
    "venue": "IBM Journal of Research and Develop-",
    "year": 2012
  }, {
    "title": "Effective crowd annotation for relation extraction",
    "authors": ["A. Liu", "S. Soderland", "J. Bragg", "C.H. Lin", "X. Ling", "D.S. Weld."],
    "venue": "North American Association for Computational Linguistics (NAACL), pages 897– 906.",
    "year": 2016
  }, {
    "title": "The stanford coreNLP natural language processing toolkit",
    "authors": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky."],
    "venue": "ACL system demonstrations.",
    "year": 2014
  }, {
    "title": "Monte Carlo theory, methods and examples",
    "authors": ["A.B. Owen"],
    "year": 2013
  }, {
    "title": "The gun violence database: A new task and data set for NLP",
    "authors": ["E. Pavlick", "H. Ji", "X. Pan", "C. Callison-Burch."],
    "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1018–1024.",
    "year": 2016
  }, {
    "title": "Local and global algorithms for disambiguation to Wikipedia",
    "authors": ["L. Ratinov", "D. Roth", "D. Downey", "M. Anderson."],
    "venue": "Association for Computational Linguistics (ACL).",
    "year": 2011
  }, {
    "title": "Largescale semantic parsing without question-answer pairs",
    "authors": ["S. Reddy", "M. Lapata", "M. Steedman."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL), 2(10):377–392.",
    "year": 2014
  }, {
    "title": "On information retrieval metrics designed for evaluation with incomplete relevance assessments",
    "authors": ["T. Sakai", "N. Kando."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR), pages 447–470.",
    "year": 2008
  }, {
    "title": "Validating and extending semantic knowledge bases using video games with a purpose",
    "authors": ["D. Vannella", "D. Jurgens", "D. Scarfini", "D. Toscani", "R. Navigli."],
    "venue": "Association for Computational Linguistics (ACL), pages 1294–1304.",
    "year": 2014
  }, {
    "title": "Measurement in Information Retrieval Evaluation",
    "authors": ["W.E. Webber."],
    "venue": "Ph.D. thesis, University of Melbourne.",
    "year": 2010
  }, {
    "title": "A simple and efficient sampling method for estimating AP and NDCG",
    "authors": ["E. Yilmaz", "E. Kanoulas", "J.A. Aslam."],
    "venue": "ACM Special Interest Group on Information Retreival (SIGIR), pages 603–610.",
    "year": 2008
  }, {
    "title": "How reliable are the results of largescale information retrieval experiments? In ACM Special Interest Group on Information Retreival (SIGIR)",
    "authors": ["J. Zobel."],
    "venue": "1048",
    "year": 1998
  }],
  "id": "SP:9e0131aa8709c4e8232ede6aa5f4b2ac5024f743",
  "authors": [{
    "name": "Arun Tejasvi Chaganty",
    "affiliations": []
  }, {
    "name": "Ashwin Pradeep Paranjape",
    "affiliations": []
  }, {
    "name": "Percy Liang",
    "affiliations": []
  }, {
    "name": "Christopher D. Manning",
    "affiliations": []
  }, {
    "name": "Debbie Reynolds",
    "affiliations": []
  }],
  "abstractText": "Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this evaluation is problematic: when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system’s predictions ondemand via crowdsourcing. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our method made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-ofthe-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.",
  "title": "Importance sampling for unbiased on-demand evaluation of knowledge base population"
}