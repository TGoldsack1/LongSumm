{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Recent years have seen rapid progress in generative modeling made possible by advances in deep learning and stochastic variational inference. The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014) has made stochastic variational inference efficient by providing lower-variance gradient estimates. However, reparameterization, as originally proposed, does not easily extend to semi-supervised learning, binary latent attribute models, topic modeling, variational memory addressing, hard attention models, or clustering, which require discrete latentvariables.\nContinuous relaxations have been proposed for accommodating discrete variables in variational inference (Maddison et al., 2016; Jang et al., 2016; Rolfe, 2016). The Gumbel-\n1Quadrant.ai, D-Wave Systems Inc., Burnaby, BC, Canada. Correspondence to: Arash Vahdat <arash@quadrant.ai>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nSoftmax technique (Maddison et al., 2016; Jang et al., 2016) defines a temperature-based continuous distribution that in the zero-temperature limit converges to a discrete distribution. However, it is limited to categorical distributions and does not scale to multivariate models such as Boltzmann machines (BM). The approach presented in (Rolfe, 2016) can train models with BM priors but requires careful handling of the gradients during training.\nWe propose a new class of smoothing transformations for relaxing binary latent variables. The method relies on two distributions with overlapping support that in the zero temperature limit converge to a Bernoulli distribution. We present two variants of smoothing transformations using a mixture of exponential and a mixture of logistic distributions.\nWe demonstrate that overlapping transformations can be used to train discrete directed latent models as in (Maddison et al., 2016; Jang et al., 2016), and models with BMs in their prior as in (Rolfe, 2016). In the case of BM priors, we show that the Kullback-Leibler (KL) contribution to the variational bound can be approximated using an analytic expression that can be optimized using automatic differentiation without requiring the special treatment of gradients in (Rolfe, 2016).\nUsing this analytic bound, we develop a new variational autoencoder (VAE) architecture called DVAE++, which uses a BM prior to model discontinuous latent factors such as object categories or scene configuration in images. DVAE++ is inspired by (Rolfe, 2016) and includes continuous local latent variables to model locally smooth features in the data. DVAE++ achieves comparable results to the state-of-the-art techniques on several datasets and captures semantically meaningful discrete aspects of the data. We show that even when all continuous latent variables are removed, DVAE++ still attains near state-of-the-art generative likelihoods."
  }, {
    "heading": "1.1. Related Work",
    "text": "Training of models with discrete latent variables z requires low-variance estimates of gradients of the form ∇φEqφ(z)[f(z)]. Only when z has a modest number of configurations (as in semi-supervised learning (Kingma et al., 2014) or semi-supervised generation (Maaløe et al., 2017))\ncan the gradient of the expectation be decomposed into a summation over configurations.\nThe REINFORCE technique (Williams, 1992) is a more scalable method that migrates the gradient inside the expectation: ∇φEqφ(z)f(z) = Eqφ(z)[f(z)∇φ log qφ(z)]. Although the REINFORCE estimate is unbiased, it suffers from high variance and carefully designed “control variates” are required to make it practical. Several works use this technique and differ in their choices of the control variates. NVIL (Mnih & Gregor, 2014) uses a running average of the function, f(z), and an input-dependent baseline. VIMCO (Mnih & Rezende, 2016) is a multi-sample version of NVIL that has baselines tailored for each sample based on all the other samples. MuProp (Gu et al., 2015) and DARN (Gregor et al., 2013) are two other REINFORCE-based methods (with non-zero biases) that use a Taylor expansion of the function f(z) to create control variates.\nTo address the high variance of REINFORCE, other work strives to make discrete variables compatible with the reparametrization technique. A primitive form arises from estimating the discrete variables by a continuous function during back-propagation. For instance, in the case of Bernoulli distribution, the latent variables can be approximated by their mean value. This approach is called the straight-through (ST) estimator (Bengio et al., 2013). Another way to make discrete variables compatible with the reparametrization is to relax them into a continuous distribution. Concrete (Maddison et al., 2016) or Gumbel-Softmax (Jang et al., 2016) adopt this strategy by adding Gumbel noise to the logits of a softmax function with a temperature hyperparameter. A slope-annealed version of the ST estimator is proposed by (Chung et al., 2016) and is equivalent to the Gumbel-Softmax approach for binary variables. REBAR (Tucker et al., 2017) is a recent method that blends REINFORCE with Concrete to synthesize control variates. (Rolfe, 2016) pairs discrete variables with auxiliary continuous variables and marginalizes out the discrete variables.\nBoth overlapping transformations and Gumbel-based approaches offer smoothing through non-zero temperature; however, overlapping transformations offer additional freedom through the choice of the mixture distributions."
  }, {
    "heading": "2. Background",
    "text": "Let x represent observed random variables and z latent variables. The joint distribution over these variables is defined by the generative model p(x,z) = p(z)p(x|z), where p(z) is a prior distribution and p(x|z) is a probabilistic decoder. Given a dataset X = {x(1), . . . ,x(N)}, the parameters of the model are trained by maximizing the log-likelihood:\nlog p(X ) =\nN�\ni=1\nlog p(x(i)).\nTypically, computing log p(x) requires an intractable marginalization over the latent variables z . To address this problem, the VAE (Kingma & Welling, 2014) introduces an inference model or probabilistic encoder q(z |x) that infers latent variables for each observation. In the VAE, instead of the maximizing the marginal log-likelihood, a variational lower bound (ELBO) is maximized:\nlog p(x) ≥ Eq(z|x) � log p(x|z) � − KL � q(z |x)||p(z) � . (1)\nThe gradient of this objective is computed for the parameters of both the encoder and decoder using the reparameterization trick. With reparametrization, the expectation with respect to q(z |x) in Eq. (1) is replaced with an expectation with respect to a known optimization-parameterindependent base distribution and a differentiable transformation from the base distribution to q(z |x). This transformation may be a scale-shift transformation, in the case of Gaussian base distributions, or rely on the inverse cumulative distribution function (CDF) in the general case. Following the law of the unconscious statistician, the gradient is then estimated using samples from the base distribution.\nUnfortunately, the reparameterization trick cannot be applied directly to the discrete latent variables because there is no differentiable transformation that maps a base distribution to a discrete distribution. Current remedies address this difficulty using a continuous relaxation of the discrete latent variables (Maddison et al., 2016; Jang et al., 2016). The discrete variational autoencoder (DVAE) (Rolfe, 2016) develops a different approach which applies the reparameterization trick to a marginal distribution constructed by pairing each discrete variable with an auxiliary continuous random variable.\nFor example, let z ∈ {0, 1} represent a binary random variable with the probability mass function q(z|x). A smoothing transformation is defined using spike-and-exponential transformation r(ζ|z), where r(ζ|z = 0) = δ(ζ) is a Dirac δ distribution and r(ζ|z = 1) ∝ exp(βζ) is an exponential distribution defined for ζ ∈ [0, 1] with inverse temperature β that controls the sharpness of the distribution. (Rolfe, 2016) notes that the autoencoding term can be defined as:\n�\nz\nq(z|x) � dζ r(ζ|z) log p(x|ζ) = � dζ q(ζ|x) log p(x|ζ),\nwhere the marginal\nq(ζ|x) = �\nz\nq(z|x)r(ζ|z) (2)\nis a mixture of two continuous distributions. By factoring the inference model so that x depends on ζ rather than z, the discrete variables can be explicitly eliminated from the ELBO and the reparameterization trick applied.\nThe smoothing transformations in (Rolfe, 2016) are limited to spike-and-X type of transformations (e.g., spike-and-exp and spike-and-Gaussian) where r(ζ|z = 0) is assumed to be a Dirac δ distribution. This property is required for computing the gradient of the KL term in the variational lower bound."
  }, {
    "heading": "3. Overlapping Transformations",
    "text": "A symmetric smoothing transformation of binary variables can also be defined using two exponential distributions:\nr(ζ|z = 0) = e −βζ\nZβ and r(ζ|z = 1) = e\nβ(ζ−1)\nZβ ,\nfor ζ ∈ [0, 1], where Zβ = (1−e−β)/β. These conditionals, visualized in Fig. 1(a), define the mixture distribution q(ζ|x) of Eq. (2). The scalar β acts as an inverse temperature as in the Gumbel softmax relaxation, and as β → ∞, q(ζ|x) approaches q(z = 0|x)δ(ζ) + q(z = 1|x)δ(ζ − 1). Application of the reparameterization trick for q(ζ|x) requires the inverse CDF of q(ζ|x). In Appendix A of the supplementary material, we show that the inverse CDF is\nF−1q(ζ|x)(ρ) = − 1\nβ log −b+ √ b2 − 4c 2\n(3)\nwhere b = [ρ + e−β(q − ρ)]/(1 − q) − 1 and c = −[qe−β ]/(1 − q). Eq. (3) is a differentiable function that converts a sample ρ from the uniform distribution U(0, 1) to a sample from q(ζ|x). As shown in Fig. 1(b) the inverse CDF approaches a step function as β → ∞. However, to benefit from gradient information during training, β is set to a finite value. Appendix C provides further visualizations comparing overlapping transformations to Concrete smoothing (Maddison et al., 2016; Jang et al., 2016).\nThe overlapping exponential distributions defined here can\nbe generalized to any pair of smooth distributions converging to δ(ζ) and δ(ζ − 1). In Appendix B, we provide analogous results for logistic smoothing distributions.\nNext, we apply overlapping transformations to the training of generative models with discrete latent variables. We consider both directed and undirected latent variable priors."
  }, {
    "heading": "4. Directed Prior",
    "text": "The simplest discrete prior is factorial; however, with conditioning, we can build complex dependencies. To simplify presentation, we illustrate a VAE prior with one or two groups of conditioning variables, but note that the approach straight-forwardly generalizes to many conditioning groups.\nOur approach parallels the method developed in (Rolfe, 2016) for undirected graphical models. Consider the generative model in Fig. 2(a) and its corresponding inference model in Fig. 2(b). To train this model using smoothing transformations, we introduce the continuous ζ in Figs. 2(c) and 2(d) in which dependencies on z are transferred to dependencies on ζ. In this way, binary latent variables influence other variables only through their continuous counterparts. In Figs. 2(e) and 2(f) we show the same model but with z marginalized out. The joint (z,ζ ) model of Figs. 2(c) and 2(d) gives rise to a looser ELBO than the marginal ζ model of Figs. 2(e) and 2(f)."
  }, {
    "heading": "4.1. Joint ELBO",
    "text": "Assuming that p(z1), p(z2|ζ1), q(z1|x), q(z2|x,ζ1), r(ζ1|z1), and r(ζ2|z2) are factorial in both the inference and generative models, then q(ζ1|x) and q(ζ2|ζ1,x) are also factorial with q(ζ1|x) = � i q(ζ1,i|x)\nwhere q(ζ1,i|x) = �\nz1,i r(ζ1,i|z1,i)q(z1,i|x), and\nq(ζ2|ζ1,x) = �\ni q(ζ2,i|ζ1,x) where q(ζ2,i|ζ1,x) =� z2,i r(ζ2,i|z2,i)q(z2,i|ζ1,x). In this case, the ELBO for\nthe model in Fig. 2(c) and 2(d) is Eq(ζ1|x) � Eq(ζ2|ζ1,x) [log p(x|ζ1,ζ2)] � − KL(q(z1|x)||p(z1))\n− Eq(ζ1|x) [KL(q(z2|x,ζ1)||p(z2|ζ1))] . (4)\nThe KL terms corresponding to the divergence between factorial Bernoulli distributions have a closed form. The expectation over ζ1 and ζ2 is reparameterized using the technique presented in Sec. 3."
  }, {
    "heading": "4.2. Marginal ELBO",
    "text": "The ELBO for the marginal graphical model of Fig. 2(e) and Fig. 2(f) is Eq(ζ1|x) � Eq(ζ2|x,ζ1) [log p(x|ζ1,ζ2)] � − KL(q(ζ1|x)||p(ζ1))\n− Eq(ζ1|x) [KL(q(ζ2|x,ζ1)||p(ζ2|ζ1))] (5)\nwith p(ζ1) = �\ni p(ζ1,i) where p(ζ1,i) =� zi r(ζ1,i|z1,i)p(z1,i) and p(ζ2|ζ1) = � i p(ζ2,i|ζ1)\nwhere p(ζ2,i|ζ1) = � z2,i r(ζ2,i|z2,i)p(z2,i|ζ1). The KL terms no longer have a closed form but can be estimated with the Monte Carlo method. In Appendix D, we show that Eq. (5) provides a tighter bound on log p(x) than does Eq. (4)."
  }, {
    "heading": "5. Boltzmann Machine Prior",
    "text": "(Rolfe, 2016) defined an expressive prior over binary latent variables by using a Boltzmann machine. We build upon that work and present a simpler objective that can still be trained with a low-variance gradient estimate.\nTo simplify notation, we assume that the prior distribution over the latent binary variables is a restricted Boltzmann machine (RBM), but these results can be extended to general BMs. An RBM defines a probability distribution over binary random variables arranged on a bipartite graph as p(z1, z2) = e−E(z1,z2)/Z where E(z1, z2) = −aT1 z1 − aT2 z2 − zT1Wz2 is an energy function with linear biases a1 and a2, and pairwise interactions W . Z is the partition function.\nFig. 2(g) visualizes a generative model with a BM prior. As in Figs. 2(c) and 2(d), conditionals are formed on the auxiliary variables ζ instead of the binary variables z . The inference model in this case is identical to the model in Fig. 2(d) and it infers both z and ζ in a hierarchical structure.\nThe autoencoding contribution to the ELBO with an RBM prior is again the first term in Eq. (4) since both models share the same inference model structure. However, computing the KL term with the RBM prior is more challenging. Here, a novel formulation for the KL term is introduced. Our derivation can be used for training discrete variational autoencoders with a BM prior without any manual coding of gradients.\nWe use Eq(z,ζ |x)[f ] = Eq(ζ |x) � Eq(z|x,ζ)[f ] � to compute the KL contribution to the ELBO: KL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2) � =\nlogZ − H � q(z1|x) � − Eq(ζ1|x) � H � q(z2|x,ζ1) �� + (6) + Eq(ζ1|x) � Eq(ζ2|x,ζ1) � Eq(z1|x,ζ1) � Eq(z2|x,ζ1,ζ2) � E(z1, z2) �� � �� �\ncross-entropy\n�� .\nHere, H(q) is the entropy of the distribution q, which has a closed form when q is factorial Bernoulli. The conditionals q(z1|x,ζ1) and q(z2|x,ζ1,ζ2) are both factorial distributions that have analytic expressions. Denoting\nµ1,i(x) ≡ q(z1,i = 1|x), ν1,i(x,ζ1) ≡ q(z1,i = 1|x,ζ1), µ2,i(x,ζ1) ≡ q(z2,i = 1|x,ζ1),\nν2,i(x,ζ1,ζ2) ≡ q(z2,i = 1|x,ζ1,ζ2), it is straightforward to show that\nν1,i(x, ζ1) = q(z1,i = 1|x)r(ζ1,i|z1,i = 1)�\nz1,i q(z1,i|x)r(ζ1,i|z1,i)\n=\n= σ � g(µ1,i(x)) + log �r(ζ1,i|z = 1) r(ζ1,i|z = 0) �� ,\nwhere σ(x) = 1/(1 + e−x) is the logistic function, and g(µ) ≡ log � µ/ � 1 − µ �� is the logit function. A similar expression holds for ν2(x,ζ1,ζ2). The expectation marked as cross-entropy in Eq. (6) corresponds to the cross-entropy between a factorial distribution and an unnormalized Boltzmann machine which is\n−aT1 ν1(x,ζ1)−aT2 ν2(x,ζ1,ζ2)−ν1(x,ζ1)TWν2(x,ζ1,ζ2). Finally, we use the equalities Eq(ζ1|x)[ν1(x,ζ1)] = µ1(x) and Eq(ζ2|x,ζ1)[ν2(x,ζ1,ζ2)] = µ2(x,ζ1) to simplify the cross-entropy term which defines the KL as\nKL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2) � = logZ\n− H � q(z1|x) � − Eq(ζ1|x) � H � q(z2|x,ζ1) �� − aT1 µ1(x)− Eq(ζ1|x) � aT2 µ2(x,ζ1) � − Eq(ζ1|x) � ν1(x,ζ1) TWµ2(x,ζ1) � .\nAll terms contributing to the KL other than logZ can be computed analytically given samples from the hierarchical encoder. Expectations with respect to q(ζ1|x) are reparameterized using the inverse CDF function. Any automatic differentiation (AD) library can then back-propagate gradients through the network. Only logZ requires special treatment. In Appendix E, we show how this term can also be included in the objective function so that its gradient is computed automatically. The ability of AD to calculate gradients stands in contrast to (Rolfe, 2016) where gradients must be manually coded. This pleasing property is a result of r(ζ|z) having the same support for both z = 0 and z = 1, and having a probabilistic q(z|x, ζ) which is not the case for the spike-and-X transformations of (Rolfe, 2016)."
  }, {
    "heading": "6. DVAE++",
    "text": "In previous sections, we have illustrated with simple examples how overlapping transformations can be used to train discrete latent variable models with either directed or undirected priors. Here, we develop a network architecture (DVAE++) that improves upon convolutional VAEs for generative image modeling.\nDVAE++ features both global discrete latent variables (to capture global properties such as scene or object type) and local continuous latent variables (to capture local properties such as object pose, orientation, or style). Both generative and inference networks rely on an autoregressive structure defined over groups of latent and observed variables. As we are modeling images, conditional dependencies between groups of variables are captured with convolutional neural networks. DVAE++ is similar to the convolutional VAEs used in (Kingma et al., 2016; Chen et al., 2016), but does not use normalizing flows."
  }, {
    "heading": "6.1. Graphical Model",
    "text": "The DVAE++ graphical model is visualized in Fig. 3. Global and local variables are indicated by z and h respectively. Subscripts indicate different groups of random variables. The conditional distribution of each group is factorial – except for z1 and z2 in the prior, which is modeled with an RBM. Global latent variables are represented with boxes and local variables are represented with 3D volumes as they are convolutional.\nGroups of local continuous variables are factorial (independent). This assumption limits the ability of the model to capture correlations at different spatial locations and different depths. While the autoregressive structure mitigates this defect, we rely mainly on the discrete global latent variables to capture long-range dependencies. The discrete nature of the global RBM prior allows DVAE++ to capture richlycorrelated discontinuous hidden factors that influence data generation.\nFig. 3(a) defines the generative model as\np(z,ζ ,h,x) = p(z) �\ni r(ζ1,i|z1,i)r(ζ2,i|z2,i)× �\nj\np(hj |h<j ,ζ )p(x|ζ ,h)\nwhere p(z) is an RBM, ζ = [ζ1,ζ2], and r is the smoothing transformation that is applied elementwise to z . The conditional p(hj |h<j ,ζ ) is defined over the jth local variable group using a factorial normal distribution. Inspired by (Reed et al., 2017; Denton et al., 2015), the conditional on the data variable p(x|ζ ,h) is decomposed into several\nfactors defined on different scales of x:\np(x|ζ ,h) = p(x0|ζ ,h) �\ni\np(xi|ζ ,h,x<i)\nHere, x0 is of size 4 × 4 and it represents downsampled x in the lowest scale. Conditioned on x0, we generate x1 in the next scale, which is of the size 8 × 8. This process is continued until the full-scale image is generated (see Appendix G.1 for more details). Here, each conditional is represented using a factorial distribution. For binary images, a factorial Bernoulli distribution is used; for colored images a factorial mixture of discretized logistic distributions is used (Salimans et al., 2017).\nThe inference model of Fig. 3(b) conditions over latent variables in a similar order as the generative model: q(z,ζ ,h|x) = q(z1|x) �\ni\nr(ζ1,i|z1,i)×\nq(z2|x,ζ1) �\nk\nr(ζ2,k|z2,k) �\nj\nq(hj |ζ ,h<j).\nThe conditionals q(z1|x) and q(z2|x,ζ1) are each modeled with a factorial Bernoulli distribution, and q(hj |ζ ,h<j) represents the conditional on the jth group of local variables.\nDVAE++ is related to VAEs with mixture priors (Makhzani et al., 2015; Tomczak & Welling, 2017). The discrete variables z1 and z2 take exponentially many joint configurations where each configuration corresponds to a mixture component. These components are mixed by p(z1, z2) in the generative model. During training, the inference model maps each data point to a small subset of all the possible mixture components. Thus, the discrete prior learns to suppress the probability of configurations that are not used by the inference model. Training results in a multimodal p(z1, z2) that assigns similar images to a common discrete mode."
  }, {
    "heading": "6.2. Neural Network Architecture",
    "text": "We use a novel neural network architecture to realize the conditional probabilities within the graphical model Fig. 3. The network uses residual connections (He et al., 2016) with squeeze-and-excitation (SE) blocks (Hu et al., 2017) that have shown state-of-the-art image classification performance. Our architecture is explained fully in Appendix G, and here we sketch the main components. We refer to a SEResNet block as a residual block, and the network is created by combining either residual blocks, fully-connected layers, or convolutional layers.\nThe encoder uses a series of downsampling residual blocks to extract convolutional features from an input image. This residual network is considered as a pre-processing step that extracts convolutional feature maps at different scales. The output of this network at the highest level is fed to fullyconnected networks that define q(z i|x,ζ<i) successively for\nall the global latent variables. The feature maps at an intermediate scale are fed to another set of residual networks that define q(hj |x,ζ ,h<j) successively for all the local latent variables.\nThe decoder uses an upsampling network to scale-up the global latent variables to the intermediate scale. Then, the output of this network is fed to a set of residual networks that define p(hj |ζ ,h<j) one at a time at the same scale. Finally, another set of residual networks progressively scales the samples from the latent variables up to the data space. In the data space, a distribution on the smallest scale x0 is formed using a residual network. Given samples at this scale, the distribution at the next scale is formed using another upsampling residual network. This process is repeated until the image is generated at full scale.\nWith many layers of latent variables, the VAE objective often turns off many of the latent variables by matching their distribution in the inference model to the prior. The latent units are usually removed differentially across different groups. Appendix H presents a technique that enables efficient use of latent variables across all groups."
  }, {
    "heading": "7. Experiments",
    "text": "To provide a comprehensive picture of overlapping transformations and DVAE++, we conduct three sets of experiments. In Sec. 7.1 and Sec. 7.2 we train a VAE with several layers of latent variables with a feed-forward encoder and decoder. This allows to compare overlapping transformations with previous work on discrete latent variables. In Sec. 7.3, we\nthen compare DVAE++ to several baselines."
  }, {
    "heading": "7.1. Comparison with Previous Discrete Latent Variable Models",
    "text": "We compare overlapping transformations to NVIL (Mnih & Gregor, 2014), MuProp (Gu et al., 2015), REBAR (Tucker et al., 2017), and Concrete (Maddison et al., 2016) for training discrete single-layer latent variable models. We follow the structure used by (Tucker et al., 2017) in which the prior distribution and inference model are factorial Bernoulli with 200 stochastic variables. In this setting, the inference and generative models are either linear or nonlinear functions. In the latter case, two layers of deterministic hidden units of the size 200 with tanh activation are used.\nWe use the settings in (Tucker et al., 2017) to initialize the parameters, define the model, and optimize the parameters for the same number of iterations. However, (Tucker et al., 2017) uses the Adam optimizer with β2 = 0.99999 in training. We used Adam with its default parameters except for � which is set to 10−3. The learning rate is selected from the set {1 · 10−4, 5 · 10−4}. The inverse temperature β for smoothing is annealed linearly during training with initial and final values chosen using cross validation from {5, 6, 7, 8} and {12, 14, 16, 18} respectively. In Table 1, the performance of our model is compared with several stateof-the-art techniques proposed for training binary latent models on (statically) binarized MNIST (Salakhutdinov & Murray, 2008) and OMNIGLOT (Lake et al., 2015). At test time, all models are evaluated in the binary limit (β = ∞). Smoothing transformations slightly outperform previous\ntechniques in most cases. In the case of the nonlinear model on OMNIGLOT, the difference is about 2.8 nats."
  }, {
    "heading": "7.2. Comparison with Previous RBM Prior VAE",
    "text": "Techniques such as KL annealing (Sønderby et al., 2016), batch normalization (Ioffe & Szegedy, 2015), autoregressive inference/prior, and learning-rate decay can significantly improve the performance of a VAE beyond the results reported in Sec. 7.1. In this second set of experiments, we evaluate overlapping transformations by comparing the training of a VAE with an RBM prior to the original DVAE (Rolfe, 2016), both of which include these improvements. For a fair comparison, we apply only those techniques that were also used in (Rolfe, 2016). We examine VAEs with one and two latent layers with feed-forward linear or nonlinear inference and generative models. In the one-latent-layer case, the KL term in both our model and (Rolfe, 2016) reduces to the mean-field approximation. The only difference in this case lies in the overlapping transformations used here and the original smoothing method of (Rolfe, 2016). In the two-latent-layer case, our inference and generative model have the forms depicted in Fig. 2(d) and Fig. 2(g). Again, all models are evaluated in the binary limit at the test time.\nComparisons are reported in Table 2. For reference, we also provide the performance of the directed VAE models with the structures visualized in Fig. 2(c) to Fig. 2(f). Implementation details are provided in Appendix F. Two observations can be made from Table 2. First, our smoothing transformation outperforms (Rolfe, 2016) in most cases. In some cases the difference is as large as 5.1 nats. Second, the RBM prior performs better than a directed prior of the same size."
  }, {
    "heading": "7.3. Experiments on DVAE++",
    "text": "Lastly, we explore the performance of DVAE++ for density estimation on 2D images. In addition to statically binarized MNIST and OMNIGLOT, we test dynamically binarized MNIST (LeCun et al., 1998) and Caltech-101 silhouettes (Marlin et al., 2010). All datasets have 28 × 28 binary pixel images. We use the same architecture for the MNIST and OMNIGLOT datasets, but because the Caltech101 silhouettes dataset is smaller, our model easily overfits. Consequently, we use a shallower architecture for Caltech101. We also evaluate DVAE++ on the CIFAR10 dataset, which consists of 32×32 pixel natural images. Appendix G lists the details of our architecture for different datasets.\nOur goal is to determine whether we can use overlapping transformations to train a convolutional VAE with an RBM prior, and whether the RBM prior in DVAE++ captures global discrete hidden factors. In addition to DVAE++ (which uses binary global latent variables and continuous local latent variables), four different baselines are introduced by modifying the global and local distributions. These baselines are listed in Table 3. For RBM (Rolfe), the spikeand-exp smoothing transformation is used and the ELBO is optimized using the derivation supplied in (Rolfe, 2016). For Bernoulli latent variables, we used the marginal distributions proposed in Sec. 4.2. For all the models, we used 16 layers of local latent variables each with 32 random variables at each spatial location. For the RBM global variables, we used 16 binary variables for all the binary datasets and 128 binary variables for CIFAR10. We cross-validated the number of the hierarchical layers in the inference model for the global variables from the set {1, 2, 4}. We used an unconditional decoder (i.e., factorial p(x|ζ ,h)) for the MNIST\ndatasets. We measure performance by estimating test set log-likelihood (again, according to the binary model) with 4000 importance weighted samples. Appendix I presents additional ablation experiments.\nTable 3 groups the baselines into three categories: all continuous latent, discrete global and continuous local (mixed), and all discrete. Within the mixed group, DVAE++ with RBM prior generally outperforms the same model trained with (Rolfe, 2016)’s. Replacing the continuous normal local variables with Bernoulli variables does not dramatically hurt the performance. For example, in the case of statically and dynamically binarized MNIST dataset, we achieve −79.72 and −79.55 respectively with unconditional decoder and 3.59 on CIFAR10 with conditional decoder. To the best of our knowledge these are the best reported results on these datasets with binary latent variables. Samples generated from DVAE++ are visualized in Fig. 4. As shown, the discrete global prior clearly captures discontinuous latent factors such as digit category or scene configuration.\nDVAE++ results are comparable to current state-of-theart convolutional latent variable models such as VampPrior (Tomczak & Welling, 2017) and variational lossy autoencoder (VLAE) (Chen et al., 2016). We note two features of these models that may offer room for further improvement for DVAE++. First, the conditional decoder used here\nmakes independence assumptions in each scale, whereas the state-of-the-art techniques are based on PixelCNN (Van Den Oord et al., 2016), which assumes full autoregressive dependencies. Second, methods such as VLAE use normalizing flows for flexible inference models that reduce the KL cost on the convolutional latent variables. Here, the independence assumption in each local group in DVAE++ can cause a significant KL penalty."
  }, {
    "heading": "8. Conclusions",
    "text": "We have introduced a new family of smoothing transformations consisting of a mixture of two overlapping distributions and have demonstrated that these transformations can be used for training latent variable models with either directed or undirected priors. Using variational bounds derived for both cases, we developed DVAE++ having a global RBM prior and local convolutional latent variables. All experiments used exponential mixture components, but it would be interesting to explore the efficacy of other choices."
  }],
  "references": [{
    "title": "Generating sentences from a continuous space",
    "authors": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A. Dai", "R. Jozefowicz", "S. Bengio"],
    "venue": "In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning,",
    "year": 2016
  }, {
    "title": "Importance weighted autoencoders",
    "authors": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"],
    "venue": "arXiv preprint arXiv:1509.00519,",
    "year": 2015
  }, {
    "title": "Variational lossy autoencoder",
    "authors": ["X. Chen", "D.P. Kingma", "T. Salimans", "Y. Duan", "P. Dhariwal", "J. Schulman", "I. Sutskever", "P. Abbeel"],
    "venue": "arXiv preprint arXiv:1611.02731,",
    "year": 2016
  }, {
    "title": "Hierarchical multiscale recurrent neural networks",
    "authors": ["J. Chung", "S. Ahn", "Y. Bengio"],
    "venue": "arXiv preprint arXiv:1609.01704,",
    "year": 2016
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Clevert", "D.-A", "T. Unterthiner", "S. Hochreiter"],
    "venue": "arXiv preprint arXiv:1511.07289,",
    "year": 2015
  }, {
    "title": "Deep generative image models using a Laplacian pyramid of adversarial networks",
    "authors": ["E.L. Denton", "S. Chintala", "R Fergus"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2015
  }, {
    "title": "Muprop: Unbiased backpropagation for stochastic neural networks",
    "authors": ["S. Gu", "S. Levine", "I. Sutskever", "A. Mnih"],
    "venue": "arXiv preprint arXiv:1511.05176,",
    "year": 2015
  }, {
    "title": "Deep residual learning for image recognition",
    "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"],
    "venue": "In Computer Vision and Pattern Recognition,",
    "year": 2016
  }, {
    "title": "Exchange Monte Carlo method and application to spin glass simulations",
    "authors": ["K. Hukushima", "K. Nemoto"],
    "venue": "Journal of the Physical Society of Japan,",
    "year": 1996
  }, {
    "title": "Extended ensemble Monte Carlo",
    "authors": ["Y. Iba"],
    "venue": "International Journal of Modern Physics C,",
    "year": 2001
  }, {
    "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
    "authors": ["S. Ioffe", "C. Szegedy"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2015
  }, {
    "title": "Categorical reparameterization with Gumbel-Softmax",
    "authors": ["E. Jang", "S. Gu", "B. Poole"],
    "venue": "arXiv preprint arXiv:1611.01144,",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["D. Kingma", "J. Ba"],
    "venue": "arXiv preprint arXiv:1412.6980,",
    "year": 2014
  }, {
    "title": "Auto-encoding variational bayes",
    "authors": ["D.P. Kingma", "M. Welling"],
    "year": 2014
  }, {
    "title": "Semi-supervised learning with deep generative models",
    "authors": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2014
  }, {
    "title": "Improved variational inference with inverse autoregressive flow",
    "authors": ["D.P. Kingma", "T. Salimans", "R. Jozefowicz", "X. Chen", "I. Sutskever", "M. Welling"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2016
  }, {
    "title": "Self-normalizing neural networks",
    "authors": ["G. Klambauer", "T. Unterthiner", "A. Mayr", "S. Hochreiter"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Human-level concept learning through probabilistic program induction",
    "authors": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"],
    "year": 2015
  }, {
    "title": "Gradientbased learning applied to document recognition",
    "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"],
    "venue": "Proceedings of the IEEE,",
    "year": 1998
  }, {
    "title": "Semi-supervised generation with cluster-aware generative models",
    "authors": ["L. Maaløe", "M. Fraccaro", "O. Winther"],
    "venue": "arXiv preprint arXiv:1704.00637,",
    "year": 2017
  }, {
    "title": "The concrete distribution: A continuous relaxation of discrete random variables",
    "authors": ["C.J. Maddison", "A. Mnih", "Y.W. Teh"],
    "venue": "arXiv preprint arXiv:1611.00712,",
    "year": 2016
  }, {
    "title": "Inductive principles for restricted Boltzmann machine learning",
    "authors": ["B. Marlin", "K. Swersky", "B. Chen", "N. Freitas"],
    "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
    "year": 2010
  }, {
    "title": "Neural variational inference and learning in belief networks",
    "authors": ["A. Mnih", "K. Gregor"],
    "venue": "arXiv preprint arXiv:1402.0030,",
    "year": 2014
  }, {
    "title": "Variational inference for Monte Carlo objectives",
    "authors": ["A. Mnih", "D. Rezende"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Parallel multiscale autoregressive density estimation",
    "authors": ["S.E. Reed", "A. van den Oord", "N. Kalchbrenner", "S. Gómez", "Z. Wang", "D. Belov", "N. de Freitas"],
    "venue": "In Proceedings of The 34th International Conference on Machine Learning,",
    "year": 2017
  }, {
    "title": "Stochastic backpropagation and approximate inference in deep generative models",
    "authors": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"],
    "venue": "In International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Discrete variational autoencoders",
    "authors": ["J.T. Rolfe"],
    "venue": "arXiv preprint arXiv:1609.02200,",
    "year": 2016
  }, {
    "title": "On the quantitative analysis of deep belief networks",
    "authors": ["R. Salakhutdinov", "I. Murray"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "PixelCNN++: Improving the pixelCNN with discretized logistic mixture likelihood and other modifications",
    "authors": ["T. Salimans", "A. Karpathy", "X. Chen", "D.P. Kingma"],
    "venue": "arXiv preprint arXiv:1701.05517,",
    "year": 2017
  }, {
    "title": "Ladder variational autoencoders",
    "authors": ["C.K. Sønderby", "T. Raiko", "L. Maaløe", "S.K. Sønderby", "O. Winther"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2016
  }, {
    "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient",
    "authors": ["T. Tieleman"],
    "venue": "In Proceedings of the 25th international conference on Machine learning,",
    "year": 2008
  }, {
    "title": "VAE with a VampPrior",
    "authors": ["J.M. Tomczak", "M. Welling"],
    "venue": "arXiv preprint arXiv:1705.07120,",
    "year": 2017
  }, {
    "title": "Low-variance, unbiased gradient estimates for discrete latent variable models",
    "authors": ["G. Tucker", "A. Mnih", "C.J. Maddison", "J. Lawson", "Sohl-Dickstein", "J. Rebar"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2017
  }, {
    "title": "Pixel recurrent neural networks",
    "authors": ["A. Van Den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"],
    "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning,",
    "year": 2016
  }, {
    "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
    "authors": ["R.J. Williams"],
    "venue": "In Reinforcement Learning,",
    "year": 1992
  }, {
    "title": "Parametric inference for imperfectly observed Gibbsian fields",
    "authors": ["L. Younes"],
    "venue": "Probability theory and related fields,",
    "year": 1989
  }],
  "id": "SP:15a8db5462d95745f42dfebae5bc816875b43266",
  "authors": [{
    "name": "Arash Vahdat",
    "affiliations": []
  }, {
    "name": "William G. Macready",
    "affiliations": []
  }, {
    "name": "Zhengbing Bian",
    "affiliations": []
  }, {
    "name": "Amir Khoshaman",
    "affiliations": []
  }, {
    "name": "Evgeny Andriyash",
    "affiliations": []
  }],
  "abstractText": "Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe, 2016).",
  "title": "DVAE++: Discrete Variational Autoencoders with Overlapping Transformations"
}