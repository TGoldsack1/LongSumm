{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 979–988 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n979"
  }, {
    "heading": "1 Introduction",
    "text": "Sentiment-to-sentiment “translation” requires the system to change the underlying sentiment of a sentence while preserving its non-emotional semantic content as much as possible. It can be regarded as a special style transfer task that is important in Natural Language Processing (NLP) (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018). It has broad applications, including review sentiment transformation, news rewriting, etc. Yet the lack of parallel training data poses a great obstacle to a satisfactory performance.\nRecently, several related studies for language style transfer (Hu et al., 2017; Shen et al., 2017) have been proposed. However, when applied\n∗Equal Contribution. 1The released code can be found in\nhttps://github.com/lancopku/unpaired-sentiment-translation\nto the sentiment-to-sentiment “translation” task, most existing studies only change the underlying sentiment and fail in keeping the semantic content. For example, given “The food is delicious” as the source input, the model generates “What a bad movie” as the output. Although the sentiment is successfully transformed from positive to negative, the output text focuses on a different topic. The reason is that these methods attempt to implicitly separate the emotional information from the semantic information in the same dense hidden vector, where all information is mixed together in an uninterpretable way. Due to the lack of supervised parallel data, it is hard to only modify the underlying sentiment without any loss of the nonemotional semantic information.\nTo tackle the problem of lacking parallel data, we propose a cycled reinforcement learning approach that contains two parts: a neutralization module and an emotionalization module. The neutralization module is responsible for extracting non-emotional semantic information by explicitly filtering out emotional words. The advantage is that only emotional words are removed, which does not affect the preservation of non-emotional words. The emotionalization module is responsible for adding sentiment to the neutralized semantic content for sentiment-to-sentiment translation.\nIn cycled training, given an emotional sentence with sentiment s, we first neutralize it to the nonemotional semantic content, and then force the emotionalization module to reconstruct the original sentence by adding the sentiment s. Therefore, the emotionalization module is taught to add sentiment to the semantic context in a supervised way. By adding opposite sentiment, we can achieve the goal of sentiment-to-sentiment translation. Because of the discrete choice of neutral words, the gradient is no longer differentiable over the neutralization module. Thus, we use policy gradient,\none of the reinforcement learning methods, to reward the output of the neutralization module based on the feedback from the emotionalization module. We add different sentiment to the semantic content and use the quality of the generated text as reward. The quality is evaluated by two useful metrics: one for identifying whether the generated text matches the target sentiment; one for evaluating the content preservation performance. The reward guides the neutralization module to better identify non-emotional words. In return, the improved neutralization module further enhances the emotionalization module.\nOur contributions are concluded as follows:\n• For sentiment-to-sentiment translation, we propose a cycled reinforcement learning approach. It enables training with unpaired data, in which only reviews and sentiment labels are available.\n• Our approach tackles the bottleneck of keeping semantic information by explicitly separating sentiment information from semantic content.\n• Experimental results show that our approach significantly outperforms the state-of-the-art systems, especially in content preservation."
  }, {
    "heading": "2 Related Work",
    "text": "Style transfer in computer vision has been studied (Johnson et al., 2016; Gatys et al., 2016; Liao et al., 2017; Li et al., 2017; Zhu et al., 2017). The main idea is to learn the mapping between two image domains by capturing shared representations or correspondences of higher-level structures.\nThere have been some studies on unpaired language style transfer recently. Hu et al. (2017) propose a new neural generative model that combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imposition of style semantic structures. Fu et al. (2018) propose to use an adversarial network to make sure that the input content does not have style information. Shen et al. (2017) focus on separating the underlying content from style information. They learn an encoder that maps the original sentence to style-independent content and a style-dependent decoder for rendering. However, their evaluations only consider the transferred style accuracy. We argue that content preservation is also an indispensable evaluation metric. However, when\napplied to the sentiment-to-sentiment translation task, the previously mentioned models share the same problem. They have the poor preservation of non-emotional semantic content.\nIn this paper, we propose a cycled reinforcement learning method to improve sentiment-tosentiment translation in the absence of parallel data. The key idea is to build supervised training pairs by reconstructing the original sentence. A related study is “back reconstruction” in machine translation (He et al., 2016; Tu et al., 2017). They couple two inverse tasks: one is for translating a sentence in language A to a sentence in language B; the other is for translating a sentence in language B to a sentence in language A. Different from the previous work, we do not introduce the inverse task, but use collaboration between the neutralization module and the emotionalization module.\nSentiment analysis is also related to our work (Socher et al., 2011; Pontiki et al., 2015; Rosenthal et al., 2017; Chen et al., 2017; Ma et al., 2017, 2018b). The task usually involves detecting whether a piece of text expresses positive, negative, or neutral sentiment. The sentiment can be general or about a specific topic."
  }, {
    "heading": "3 Cycled Reinforcement Learning for Unpaired Sentiment-to-Sentiment Translation",
    "text": "In this section, we introduce our proposed method. An overview is presented in Section 3.1. The details of the neutralization module and the emotionalization module are shown in Section 3.2 and Section 3.3. The cycled reinforcement learning mechanism is introduced in Section 3.4."
  }, {
    "heading": "3.1 Overview",
    "text": "The proposed approach contains two modules: a neutralization module and an emotionalization module, as shown in Figure 1. The neutralization module first extracts non-emotional semantic content, and then the emotionalization module attaches sentiment to the semantic content. Two modules are trained by the proposed cycled reinforcement learning method. The proposed method requires the two modules to have initial learning ability. Therefore, we propose a novel pre-training method, which uses a self-attention based sentiment classifier (SASC). A sketch of cycled reinforcement learning is shown in Algorithm 1. The\ndetails are introduced as follows."
  }, {
    "heading": "3.2 Neutralization Module",
    "text": "The neutralization module Nθ is used for explicitly filtering out emotional information. In this paper, we consider this process as an extraction problem. The neutralization module first identifies non-emotional words and then feeds them into the emotionalization module. We use a single Longshort Term Memory Network (LSTM) to generate the probability of being neutral or being polar for every word in a sentence. Given an emotional input sequence x = (x1, x2, . . . , xT ) of T words from Γ, the vocabulary of words, this module is responsible for producing a neutralized sequence.\nSince cycled reinforcement learning requires the modules with initial learning ability, we propose a novel pre-training method to teach the neutralization module to identify non-emotional words. We construct a self-attention based sentiment classifier and use the learned attention weight as the supervisory signal. The motivation comes from the fact that, in a well-trained sentiment classification model, the attention weight reflects the sentiment contribution of each word to\nAlgorithm 1 The cycled reinforcement learning method for training the neutralization module Nθ and the emotionalization module Eφ. 1: Initialize the neutralization module Nθ , the emotional-\nization module Eφ with random weights θ, φ 2: Pre-train Nθ using MLE based on Eq. 6 3: Pre-train Eφ using MLE based on Eq. 7 4: for each iteration i = 1, 2, ...,M do 5: Sample a sequence x with sentiment s from X 6: Generate a neutralized sequence x̂ based on Nθ 7: Given x̂ and s, generate an output based on Eφ 8: Compute the gradient of Eφ based on Eq. 8 9: Compute the reward R1 based on Eq. 11\n10: s̄ = the opposite sentiment 11: Given x̂ and s̄, generate an output based on Eφ 12: Compute the reward R2 based on Eq. 11 13: Compute the combined reward Rc based on Eq. 10 14: Compute the gradient of Nθ based on Eq. 9 15: Update model parameters θ, φ 16: end for\nsome extent. Emotional words tend to get higher attention weights while neutral words usually get lower weights. The details of sentiment classifier are described as follows.\nGiven an input sequence x, a sentiment label y is produced as\ny = softmax(W · c) (1)\nwhere W is a parameter. The term c is computed as a weighted sum of hidden vectors:\nc = T∑ i=0 αihi (2)\nwhere αi is the weight of hi. The term hi is the output of LSTM at the i-th word. The term αi is computed as\nαi = exp(ei)∑T i=0 exp(ei)\n(3)\nwhere ei = f(hi,hT ) is an alignment model. We consider the last hidden state hT as the context vector, which contains all information of an input sequence. The term ei evaluates the contribution of each word for sentiment classification.\nOur experimental results show that the proposed sentiment classifier achieves the accuracy of 89% and 90% on two datasets. With high classification accuracy, the attention weight produced by the classifier is considered to adequately capture the sentiment information of each word.\nTo extract non-emotional words based on continuous attention weights, we map attention\nweights to discrete values, 0 and 1. Since the discrete method is not the key part is this paper, we only use the following method for simplification.\nWe first calculate the averaged attention value in a sentence as\nᾱ = 1\nT T∑ i=0 αi (4)\nwhere ᾱ is used as the threshold to distinguish non-emotional words from emotional words. The discrete attention weight is calculated as\nα̂i = { 1, if αi ≤ ᾱ 0, if αi > ᾱ\n(5)\nwhere α̂i is treated as the identifier. For pre-training the neutralization module, we build the training pair of input text x and a discrete attention weight sequence α̂. The cross entropy loss is computed as\nLθ = − T∑ i=1 PNθ(α̂i|xi) (6)"
  }, {
    "heading": "3.3 Emotionalization Module",
    "text": "The emotionalization module Eφ is responsible for adding sentiment to the neutralized semantic content. In our work, we use a bi-decoder based encoder-decoder framework, which contains one encoder and two decoders. One decoder adds the positive sentiment and the other adds the negative sentiment. The input sentiment signal determines which decoder to use. Specifically, we use the seq2seq model (Sutskever et al., 2014) for implementation. Both the encoder and decoder are LSTM networks. The encoder learns to compress the semantic content into a dense vector. The decoder learns to add sentiment based on the dense vector. Given the neutralized semantic content and the target sentiment, this module is responsible for producing an emotional sequence.\nFor pre-training the emotionalization module, we first generate a neutralized input sequence x̂ by removing emotional words identified by the proposed sentiment classifier. Given the training pair of a neutralized sequence x̂ and an original sentence x with sentiment s, the cross entropy loss is computed as\nLφ = − T∑ i=1 PEφ(xi|x̂i, s) (7)\nwhere a positive example goes through the positive decoder and a negative example goes through the negative decoder.\nWe also explore a simpler method for pretraining the emotionalization module, which uses the product between a continuous vector 1 − α and a word embedding sequence as the neutralized content where α represents an attention weight sequence. Experimental results show that this method achieves much lower results than explicitly removing emotional words based on discrete attention weights. Thus, we do not choose this method in our work."
  }, {
    "heading": "3.4 Cycled Reinforcement Learning",
    "text": "Two modules are trained by the proposed cycled method. The neutralization module first neutralizes an emotional input to semantic content and then the emotionalization module is forced to reconstruct the original sentence based on the source sentiment and the semantic content. Therefore, the emotionalization module is taught to add sentiment to the semantic content in a supervised way. Because of the discrete choice of neutral words, the loss is no longer differentiable over the neutralization module. Therefore, we formulate it as a reinforcement learning problem and use policy gradient to train the neutralization module. The detailed training process is shown as follows.\nWe refer the neutralization module Nθ as the first agent and the emotionalization module Eφ as the second one. Given a sentence x associated with sentiment s, the term x̂ represents the middle neutralized context extracted by α̂, which is generated by PNθ(α̂|x).\nIn cycled training, the original sentence can be viewed as the supervision for training the second agent. Thus, the gradient for the second agent is\n∇φJ(φ) = ∇φ log(PEφ(x|x̂, s)) (8)\nWe denote x̄ as the output generated by PEφ(x̄|x̂, s). We also denote y as the output generated by PEφ(y|x̂, s̄) where s̄ represents the opposite sentiment. Given x̄ and y, we first calculate rewards for training the neutralized module, R1 and R2. The details of calculation process will be introduced in Section 3.4.1. Then, we optimize parameters through policy gradient by maximizing the expected reward to train the neutralization module. It guides the neutralization module to identify non-emotional words better. In return, the\nimproved neutralization module further enhances the emotionalization module.\nAccording to the policy gradient theorem (Williams, 1992), the gradient for the first agent is\n∇θJ(θ) = E[Rc · ∇θ log(PNθ(α̂|x))] (9)\nwhere Rc is calculated as\nRc = R1 +R2 (10)\nBased on Eq. 8 and Eq. 9, we use the sampling approach to estimate the expected reward. This cycled process is repeated until converge."
  }, {
    "heading": "3.4.1 Reward",
    "text": "The reward consists of two parts, sentiment confidence and BLEU. Sentiment confidence evaluates whether the generated text matches the target sentiment. We use a pre-trained classifier to make the judgment. Specially, we use the proposed selfattention based sentiment classifier for implementation. The BLEU (Papineni et al., 2002) score is used to measure the content preservation performance. Considering that the reward should encourage the model to improve both metrics, we use the harmonic mean of sentiment confidence and BLEU as reward, which is formulated as\nR = (1 + β2) 2 ·BLEU · Confid\n(β2 ·BLEU) + Confid (11)\nwhere β is a harmonic weight."
  }, {
    "heading": "4 Experiment",
    "text": "In this section, we evaluate our method on two review datasets. We first introduce the datasets, the training details, the baselines, and the evaluation metrics. Then, we compare our approach with the state-of-the-art systems. Finally, we show the experimental results and provide the detailed analysis of the key components."
  }, {
    "heading": "4.1 Unpaired Datasets",
    "text": "We conduct experiments on two review datasets that contain user ratings associated with each review. Following previous work (Shen et al., 2017), we consider reviews with rating above three as positive reviews and reviews below three as negative reviews. The positive and negative reviews are not paired. Since our approach focuses on sentence-level sentiment-to-sentiment translation\nwhere sentiment annotations are provided at the document level, we process the two datasets with the following steps. First, following previous work (Shen et al., 2017), we filter out the reviews that exceed 20 words. Second, we construct textsentiment pairs by extracting the first sentence in a review associated with its sentiment label, because the first sentence usually expresses the core idea. Finally, we train a sentiment classifier and filter out the text-sentiment pairs with the classifier confidence below 0.8. Specially, we use the proposed self-attention based sentiment classifier for implementation. The details of the processed datasets are introduced as follows.\nYelp Review Dataset (Yelp): This dataset is provided by Yelp Dataset Challenge.2 The processed Yelp dataset contains 400K, 10K, and 3K pairs for training, validation, and testing, respectively.\nAmazon Food Review Dataset (Amazon): This dataset is provided by McAuley and Leskovec (2013). It consists of amounts of food reviews from Amazon.3 The processed Amazon dataset contains 230K, 10K, and 3K pairs for training, validation, and testing, respectively."
  }, {
    "heading": "4.2 Training Details",
    "text": "We tune hyper-parameters based on the performance on the validation sets. The self-attention based sentiment classifier is trained for 10 epochs on two datasets. We set β for calculating reward to 0.5, hidden size to 256, embedding size to 128, vocabulary size to 50K, learning rate to 0.6, and batch size to 64. We use the Adagrad (Duchi et al., 2011) optimizer. All of the gradients are clipped when the norm exceeds 2. Before cycled training, the neutralization module and the emotionalization module are pre-trained for 1 and 4 epochs on the yelp dataset, for 3 and 5 epochs on the Amazon dataset."
  }, {
    "heading": "4.3 Baselines",
    "text": "We compare our proposed method with the following state-of-the-art systems.\nCross-Alignment Auto-Encoder (CAAE): This method is proposed by Shen et al. (2017). They propose a method that uses refined alignment of latent representations in hidden layers to\n2https://www.yelp.com/dataset/ challenge\n3http://amazon.com\nperform style transfer. We treat this model as a baseline and adapt it by using the released code.\nMulti-Decoder with Adversarial Learning (MDAL): This method is proposed by Fu et al. (2018). They use a multi-decoder model with adversarial learning to separate style representations and content representations in hidden layers. We adapt this model by using the released code."
  }, {
    "heading": "4.4 Evaluation Metrics",
    "text": "We conduct two evaluations in this work, including an automatic evaluation and a human evaluation. The details of evaluation metrics are shown as follows."
  }, {
    "heading": "4.4.1 Automatic Evaluation",
    "text": "We quantitatively measure sentiment transformation by evaluating the accuracy of generating designated sentiment. For a fair comparison, we do not use the proposed sentiment classification model. Following previous work (Shen et al., 2017; Hu et al., 2017), we instead use a stateof-the-art sentiment classifier (Vieira and Moura, 2017), called TextCNN, to automatically evaluate the transferred sentiment accuracy. TextCNN achieves the accuracy of 89% and 88% on two datasets. Specifically, we generate sentences given sentiment s, and use the pre-trained sentiment classifier to assign sentiment labels to the generated sentences. The accuracy is calculated as the percentage of the predictions that match the sentiment s.\nTo evaluate the content preservation performance, we use the BLEU score (Papineni et al., 2002) between the transferred sentence and the source sentence as an evaluation metric. BLEU is a widely used metric for text generation tasks, such as machine translation, summarization, etc. The metric compares the automatically produced text with the reference text by computing overlapping lexical n-gram units.\nTo evaluate the overall performance, we use the geometric mean of ACC and BLEU as an evaluation metric. The G-score is one of the most commonly used “single number” measures in Information Retrieval, Natural Language Processing, and Machine Learning."
  }, {
    "heading": "4.4.2 Human Evaluation",
    "text": "While the quantitative evaluation provides indication of sentiment transfer quality, it can not evaluate the quality of transferred text accurately.\nTherefore, we also perform a human evaluation on the test set. We randomly choose 200 items for the human evaluation. Each item contains the transformed sentences generated by different systems given the same source sentence. The items are distributed to annotators who have no knowledge about which system the sentence is from. They are asked to score the transformed text in terms of sentiment and semantic similarity. Sentiment represents whether the sentiment of the source text is transferred correctly. Semantic similarity evaluates the context preservation performance. The score ranges from 1 to 10 (1 is very bad and 10 is very good)."
  }, {
    "heading": "4.5 Experimental Results",
    "text": "Automatic evaluation results are shown in Table 1. ACC evaluates sentiment transformation. BLEU evaluates semantic content preservation. G-score represents the geometric mean of ACC and BLEU. CAAE and MDAL achieve much lower BLEU scores, 1.17 and 1.64 on the Yelp dataset, 0.56 and 0.27 on the Amazon dataset. The low BLEU scores indicate the worrying content preservation performance to some extent. Even with the desired sentiment, the irrelevant generated text leads to worse overall performance. In general, these two systems work more like sentiment-aware language models that generate text only based on the target sentiment and neglect the source input. The main reason is that these two systems attempt to separate emotional information from non-emotional content in a hidden layer, where all information is complicatedly mixed together. It is difficult to only modify emotional information without any loss of non-emotional semantic content.\nIn comparison, our proposed method achieves the best overall performance on the two datasets,\ndemonstrating the ability of learning knowledge from unpaired data. This result is attributed to the improved BLEU score. The BLEU score is largely improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets. The score improvements mainly come from the fact that we separate emotional information from semantic content by explicitly filtering out emotional words. The extracted content is preserved and fed into the emotionalization module. Given the overall quality of transferred text as the reward, the neutralization module is taught to extract non-emotional semantic content better.\nTable 2 shows the human evaluation results. It can be clearly seen that the proposed method obviously improves semantic preservation. The semantic score is increased from 3.87 to 5.08 on the Yelp dataset, and from 3.22 to 4.67 on the Amazon dataset. In general, our proposed model achieves the best overall performance. Furthermore, it also needs to be noticed that with the large improvement in content preservation, the sentiment accuracy of the proposed method is lower than that of CAAE on the two datasets. It shows that simultaneously promoting sentiment transformation and content preservation remains to be studied further.\nBy comparing two evaluation results, we find that there is an agreement between the human evaluation and the automatic evaluation. It indicates the usefulness of automatic evaluation metrics. However, we also notice that the human evaluation has a smaller performance gap between the baselines and the proposed method than the automatic evaluation. It shows the limitation of automatic metrics for giving accurate results. For evaluating sentiment transformation, even with a high accuracy, the sentiment classifier sometimes generates noisy results, especially for those neutral sentences (e.g., “I ate a cheese sandwich”). For evaluating content preservation, the BLEU score\nis computed based on the percentage of overlapping n-grams between the generated text and the reference text. However, the overlapping n-grams contain not only content words but also function words, bringing the noisy results. In general, accurate automatic evaluation metrics are expected in future work.\nTable 3 presents the examples generated by different systems on the Yelp dataset. The two baselines change not only the polarity of examples, but also the semantic content. In comparison, our method precisely changes the sentiment of sentences (and paraphrases slightly to ensure fluency), while keeping the semantic content unchanged."
  }, {
    "heading": "4.6 Incremental Analysis",
    "text": "In this section, we conduct a series of experiments to evaluate the contributions of our key components. The results are shown in Table 4.\nWe treat the emotionalization module as a baseline where the input is the original emotional sentence. The emotionalization module achieves the highest BLEU score but with much lower sentiment transformation accuracy. The encoding of the original sentiment leads to the emotional hidden vector that influences the decoding process and results in worse sentiment transformation performance.\nIt can be seen that the method with all components achieves the best performance. First, we find that the method that only uses cycled reinforcement learning performs badly because it is hard to guide two randomly initialized modules to teach each other. Second, the pre-training method brings a slight improvement in overall performance. The G-score is improved from 32.77 to 34.66 and from 26.46 to 27.87 on the two datasets. The bottleneck of this method is the noisy attention weight because of the limited sentiment classification accuracy. Third, the method that combines cycled reinforcement learning and pre-training achieves the better performance than using one of them. Pre-training gives the two modules initial learning ability. Cycled training teaches the two modules to improve each other based on the feedback signals. Specially, the G-score is improved from 34.66 to 42.38 and from 27.87 to 31.45 on the two datasets. Finally, by comparing the methods with and without the neutralization module, we find that the neutralization mechanism improves a lot in sentiment transformation with a slight reduction on content preservation. It proves the effectiveness of explic-\nitly separating sentiment information from semantic content.\nFurthermore, to analyze the neutralization ability in the proposed method, we randomly sample several examples, as shown in Table 5. It can be clearly seen that emotional words are removed accurately almost without loss of non-emotional information."
  }, {
    "heading": "4.7 Error Analysis",
    "text": "Although the proposed method outperforms the state-of-the-art systems, we also observe several failure cases, such as sentiment-conflicted sentences (e.g., “Outstanding and bad service”), neutral sentences (e.g., “Our first time here”). Sentiment-conflicted sentences indicate that the original sentiment is not removed completely. This problem occurs when the input contains emotional words that are unseen in the training data, or the sentiment is implicitly expressed. Handling complex sentiment expressions is an important problem for future work. Neutral sentences demonstrate that the decoder sometimes fails in adding the target sentiment and only generates text based on the semantic content. A better sentimentaware decoder is expected to be explored in future work."
  }, {
    "heading": "5 Conclusions and Future Work",
    "text": "In this paper, we focus on unpaired sentimentto-sentiment translation and propose a cycled reinforcement learning approach that enables training in the absence of parallel training data. We conduct experiments on two review datasets. Experimental results show that our method substantially outperforms the state-of-the-art systems, especially in terms of semantic preservation. For future work, we would like to explore a fine-grained version of sentiment-to-sentiment translation that\nnot only reverses sentiment, but also changes the strength of sentiment."
  }, {
    "heading": "Acknowledgements",
    "text": "This work was supported in part by National Natural Science Foundation of China (No. 61673028), National High Technology Research and Development Program of China (863 Program, No. 2015AA015404), and the National Thousand Young Talents Program. Xu Sun is the corresponding author of this paper."
  }],
  "year": 2018,
  "references": [{
    "title": "Improving sentiment analysis via sentence type classification using bilstm-crf and CNN",
    "authors": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang."],
    "venue": "Expert Syst. Appl., 72:221–230.",
    "year": 2017
  }, {
    "title": "Learning to generate product reviews from attributes",
    "authors": ["Li Dong", "Shaohan Huang", "Furu Wei", "Mirella Lapata", "Ming Zhou", "Ke Xu."],
    "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume",
    "year": 2017
  }, {
    "title": "Adaptive subgradient methods for online learning and stochastic optimization",
    "authors": ["John C. Duchi", "Elad Hazan", "Yoram Singer."],
    "venue": "Journal of Machine Learning Research, 12:2121–2159.",
    "year": 2011
  }, {
    "title": "Style transfer in text: Exploration and evaluation",
    "authors": ["Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan."],
    "venue": "AAAI 2018.",
    "year": 2018
  }, {
    "title": "Image style transfer using convolutional neural networks",
    "authors": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge."],
    "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages",
    "year": 2016
  }, {
    "title": "Dual learning for machine translation",
    "authors": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma."],
    "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Sys-",
    "year": 2016
  }, {
    "title": "Controllable text generation",
    "authors": ["Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing."],
    "venue": "ICML 2017.",
    "year": 2017
  }, {
    "title": "Perceptual losses for real-time style transfer and super-resolution",
    "authors": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei."],
    "venue": "ECCV, 2016, pages 694–711.",
    "year": 2016
  }, {
    "title": "Demystifying neural style transfer",
    "authors": ["Yanghao Li", "Naiyan Wang", "Jiaying Liu", "Xiaodi Hou."],
    "venue": "Proceedings of the Twenty-Sixth International Joint",
    "year": 2017
  }, {
    "title": "Visual attribute transfer through deep image analogy",
    "authors": ["Jing Liao", "Yuan Yao", "Lu Yuan", "Gang Hua", "Sing Bing Kang."],
    "venue": "ACM Trans. Graph., 36(4):120:1–120:15.",
    "year": 2017
  }, {
    "title": "Decoding-history-based adaptive control of attention for neural machine translation",
    "authors": ["Junyang Lin", "Shuming Ma", "Qi Su", "Xu Sun."],
    "venue": "CoRR, abs/1802.01812.",
    "year": 2018
  }, {
    "title": "Cascading multiway attentions for document-level sentiment classification",
    "authors": ["Dehong Ma", "Sujian Li", "Xiaodong Zhang", "Houfeng Wang", "Xu Sun."],
    "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing,",
    "year": 2017
  }, {
    "title": "Query and output: Generating words by querying distributed word representations for paraphrase generation",
    "authors": ["Shuming Ma", "Xu Sun", "Wei Li", "Sujian Li", "Wenjie Li", "Xuancheng Ren."],
    "venue": "CoRR, abs/1803.01465.",
    "year": 2018
  }, {
    "title": "A hierarchical end-to-end model for jointly improving text summarization and sentiment classification",
    "authors": ["Shuming Ma", "Xu Sun", "Junyang Lin", "Xuancheng Ren."],
    "venue": "CoRR, abs/1805.01089.",
    "year": 2018
  }, {
    "title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews",
    "authors": ["Julian John McAuley", "Jure Leskovec."],
    "venue": "22nd International World Wide Web Conference, WWW ’13, Rio de Janeiro, Brazil, May 13-17, 2013, pages 897–",
    "year": 2013
  }, {
    "title": "Bleu: a method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
    "year": 2002
  }, {
    "title": "Semeval-2015 task 12: Aspect based sentiment analysis",
    "authors": ["Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos."],
    "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval@NAACL-",
    "year": 2015
  }, {
    "title": "Semeval-2017 task 4: Sentiment analysis in twitter",
    "authors": ["Sara Rosenthal", "Noura Farra", "Preslav Nakov."],
    "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017, pages 502–518.",
    "year": 2017
  }, {
    "title": "Style transfer from non-parallel text by cross-alignment",
    "authors": ["Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola."],
    "venue": "NIPS 2017.",
    "year": 2017
  }, {
    "title": "Parsing natural scenes and natural language with recursive neural networks",
    "authors": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning."],
    "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML",
    "year": 2011
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-",
    "year": 2014
  }, {
    "title": "Neural machine translation with reconstruction",
    "authors": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."],
    "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA.,",
    "year": 2017
  }, {
    "title": "An analysis of convolutional neural networks for sentence classification",
    "authors": ["Joao Paulo Albuquerque Vieira", "Raimundo Santos Moura."],
    "venue": "XLIII 2017, pages 1–5.",
    "year": 2017
  }, {
    "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
    "authors": ["Ronald J. Williams."],
    "venue": "Machine Learning, 8:229–256.",
    "year": 1992
  }, {
    "title": "DP-GAN: diversity-promoting generative adversarial network for generating informative and diversified text",
    "authors": ["Jingjing Xu", "Xu Sun", "Xuancheng Ren", "Junyang Lin", "Bingzhen Wei", "Wei Li."],
    "venue": "CoRR, abs/1802.01345.",
    "year": 2018
  }, {
    "title": "Towards automatic generation of product reviews from aspectsentiment scores",
    "authors": ["Hongyu Zang", "Xiaojun Wan."],
    "venue": "Proceedings of the 10th International Conference on Natural Language Generation, pages 168–177.",
    "year": 2017
  }, {
    "title": "Automatic transferring between ancient chinese and contemporary chinese",
    "authors": ["Zhiyuan Zhang", "Wei Li", "Xu Sun."],
    "venue": "CoRR, abs/1803.01557.",
    "year": 2018
  }, {
    "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
    "authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros."],
    "venue": "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-",
    "year": 2017
  }],
  "id": "SP:5c65dcd719aa5c4917b78018f4704171c005c89e",
  "authors": [{
    "name": "Jingjing Xu",
    "affiliations": []
  }, {
    "name": "Xu Sun",
    "affiliations": []
  }, {
    "name": "Qi Zeng",
    "affiliations": []
  }, {
    "name": "Xuancheng Ren",
    "affiliations": []
  }, {
    "name": "Xiaodong Zhang",
    "affiliations": []
  }, {
    "name": "Houfeng Wang",
    "affiliations": []
  }, {
    "name": "Wenjie Li",
    "affiliations": []
  }],
  "abstractText": "The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.1",
  "title": "Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach"
}