{
  "sections": [{
    "heading": "1. Introduction",
    "text": "Making predictions about causal effects of actions is a central problem in many domains. For example, a doctor deciding which medication will cause better outcomes for a patient; a government deciding who would benefit most from subsidized job training; or a teacher deciding which study program would most benefit a specific student. In this paper we focus on the problem of making these predictions based on observational data. Observational data is\n*Equal contribution 1CIMS, New York University, New York, NY 10003 2IMES, MIT, Cambridge, MA 02142 3CSAIL, MIT, Cambridge, MA 02139. Correspondence to: Uri Shalit <shalit@cs.nyu.edu>, Fredrik D. Johansson <fredrikj@mit.edu>, David Sontag <dsontag@csail.mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndata which contains past actions, their outcomes, and possibly more context, but without direct access to the mechanism which gave rise to the action. For example we might have access to records of patients (context), their medications (actions), and outcomes, but we do not have complete knowledge of why a specific action was applied to a patient.\nThe hallmark of learning from observational data is that the actions observed in the data depend on variables which might also affect the outcome, resulting in confounding: For example, richer patients might better afford certain medications, and job training might only be given to those motivated enough to seek it. The challenge is how to untangle these confounding factors and make valid predictions. Specifically, we work under the common simplifying assumption of “no-hidden confounding”, assuming that all the factors determining which actions were taken are observed. In the examples above, it would mean that we have measured a patient’s wealth or an employee’s motivation.\nAs a learning problem, estimating causal effects from observational data is different from classic learning in that in our training data we never see the individual-level effect. For each unit, we only see their response to one of the possible actions - the one they had actually received. This is close to what is known in the machine learning literature as “learning from logged bandit feedback” (Strehl et al., 2010; Swaminathan & Joachims, 2015), with the distinction that we do not have access to the model generating the action.\nOur work differs from much work in causal inference in that we focus on the individual-level causal effect (“cspecific treatment effects” Shpitser & Pearl (2006); Pearl (2015)), rather than the average or population level. Our main contribution is to give what is, to the best of our knowledge, the first generalization-error1 bound for estimating individual-level causal effect, where each individual is identified by its features x. The bound leads naturally to a new family of representation-learning based algorithms (Bengio et al., 2013), which we show to match or outperform state-of-the-art methods on several causal effect inference tasks.\n1Our use of the term generalization is different from its use in the study of transportability, where the goal is to generalize causal conclusion across distributions (Bareinboim & Pearl, 2016).\nWe frame our results using the Neyman-Rubin potential outcomes framework (Rubin, 2011), as follows. We assume that for a unit with features x ∈ X , and an action (also known as treatment or intervention) t ∈ {0, 1}, there are two potential outcomes: Y0 and Y1. For each unit we only observe one of the potential outcomes, according to treatment assignment: if t = 0 we observe y = Y0, if t = 1, we observe y = Y1; this is known as the consistency assumption. For example, x can denote the set of lab tests and demographic factors of a diabetic patient, t = 0 denote the standard medication for controlling blood sugar, t = 1 denotes a new medication, and Y0 and Y1 indicate the patient’s blood sugar level if they were to be given medications t = 0 and t = 1, respectively.\nWe will denote m1(x) = E [Y1|x], m0(x) = E [Y0|x]. We are interested in learning the function τ(x) := E [Y1 − Y0|x] = m1(x) − m0(x). τ(x) is the expected treatment effect of t = 1 relative to t = 0 on a unit with characteristics x, or the Individual Treatment Effect (ITE)2. Our goal is to find an estimate τ̂ of τ such that some loss function, e.g. E [ (τ̂ − τ)2 ] , is small. For example, for a\npatient with features x, we attempt to predict which of two treatments will have a better outcome. The fundamental problem of causal inference is that for any x in our data we only observe Y1 or Y0, but never both.\nAs mentioned above, we make an important “no-hidden confounders” assumption, in order to make the conditional causal effect identifiable. We formalize this assumption by using the standard strong ignorability condition: (Y1, Y0) ⊥ t|x, and 0 < p(t = 1|x) < 1 for all x. Strong ignorability is a sufficient condition for the ITE function τ(x) to be identifiable (Imbens & Wooldridge, 2009; Pearl, 2015; Rolling, 2014): see proof in the supplement. The validity of strong ignorability cannot be assessed from data, and must be determined by domain knowledge and understanding of the causal relationships between the variables.\nOne approach to the problem of estimating the function τ(x) is by learning the two functions m0(x) and m1(x) using samples from p(Yt|x, t). This is similar to a standard machine learning problem of learning from finite samples. However, there is an additional source of variance at work here: For example, if mostly rich patients received treatment t = 1, and mostly poor patients received treatment t = 0, we might have an unreliable estimation of m1(x) for poor patients. In this paper we upper bound this additional source of variance using an Integral Probability Metric (IPM) measure of distance between two distributions p(x|t = 0), and p(x|t = 1), also known as the control and treated distributions. In practice we use two specific IPMs: the Maximum Mean Discrepancy (Gretton\n2Also known as Conditional Average Treatment Effect, CATE.\net al., 2012), and the Wasserstein distance (Villani, 2008; Cuturi & Doucet, 2014). We show that the expected error in learning the individual treatment effect function τ(x) is upper bounded by the error of learning Y1 and Y0, plus the IPM term. In the randomized controlled trial setting, where t ⊥ x, the IPM term is 0, and our bound naturally reduces to a standard learning problem of learning two functions.\nThe bound we derive points the way to a family of algorithms based on the idea of representation learning (Bengio et al., 2013): Jointly learn hypotheses for both treated and control on top of a representation which minimizes a weighted sum of the factual loss (the standard supervised machine learning objective), and the IPM distance between the control and treated distributions induced by the representation. This can be viewed as learning the functions m0 and m1 under a constraint that encourages better generalization across the treated and control populations. In the Experiments section we apply algorithms based on neural nets as representations and hypotheses, along with MMD or Wasserstein distributional distances over the representation layer; see Figure 1 for the basic architecture.\nIn his foundational text on causality, Pearl (2009) writes: “Whereas in traditional learning tasks we attempt to generalize from one set of instances to another, the causal modeling task is to generalize from behavior under one set of conditions to [...] another set. Causal models should therefore be chosen by a criterion that challenges their stability against changing conditions” [emphasis ours]. We believe our work points the way to one such stability criterion, for causal inference in the strongly ignorable case."
  }, {
    "heading": "2. Related work",
    "text": "Much recent work in machine learning for causal inference focuses on causal discovery, with the goal of discovering the underlying causal graph or causal direction from data (Hoyer et al., 2009; Maathuis et al., 2010; Triantafillou & Tsamardinos, 2015; Mooij et al., 2016). We focus on the case when the causal setup is simple and known to be of the form (Y1, Y0) ⊥ x|t, with no hidden confounders. Under the causal model we assume, the most common goal of causal effect inference as used in the ap-\nplied sciences is to obtain the average treatment effect: ATE = Ex∼p(x) [τ(x)]. We will briefly discuss how some standard statistical causal effect inference methods relate to our proposed method. Note that most of these approaches assume some form of ignorability.\nOne of the most widely used approaches to estimating ATE is covariate (or back-door) adjustment, also known as the G-computation formula (Robins, 1986; Pearl, 2009). In their basic version, covariate adjustment methods aim to estimate the functions m1(x), m0(x), and are therefore natural candidates for estimating ITE as well as ATE, using the estimates of mt(x). Most previous work on this subject focused on asymptotic consistency (Belloni et al., 2014; Athey et al., 2016; Chernozhukov et al., 2016), and so far there has not been much work on the generalizationerror of such a procedure. One view of our results is that we point out a previously unaccounted for source of variance when using covariate adjustment to estimate ITE. We suggest a new type of regularization, by learning representations with reduced IPM distance between treated and control, enabling a new type of bias-variance trade-off.\nAnother widely used family of statistical methods used in causal effect inference are weighting methods. Methods such as inverse propensity score weighting (Austin, 2011) re-weight the units in the observational data so as to make the treated and control populations more comparable, and have been used for estimating conditional effects as well (Cole et al., 2003). The major challenge, especially in high-dimensional cases, is controlling the variance of the estimates (Swaminathan & Joachims, 2015). Doubly robust methods go further and combine propensity score reweighting and covariate adjustment in clever ways to reduce model bias (Funk et al., 2011).\nAdapting machine learning methods for causal effect inference, and in particular for individual level treatment effect, has gained much interest recently. For example Wager & Athey (2015); Athey & Imbens (2016) discuss how treebased methods can be adapted to obtain a consistent estimator with semi-parametric asymptotic convergence rate. Recent work has also looked into how machine learning methods can help detect heterogeneous treatment effects when some data from randomized experiments is available (Taddy et al., 2016; Peysakhovich & Lada, 2016). Neural nets have also been used for this purpose, exemplified in early work by Beck et al. (2000), and more recently by Hartford et al. (2016)’s work on deep instrumental variables. Our work differs from all the above by focusing on the generalization-error aspects of estimating individual treatment effect, as opposed to asymptotic consistency, and by focusing solely on the observational study case, with no randomized components or instrumental variables.\nOur work has strong connections with work on domain\nadaptation. In particular, estimating ITE requires prediction of outcomes over a different distribution from the observed one. Our ITE error upper bound has similarities with generalization bounds in domain adaptation given by BenDavid et al. (2007); Mansour et al. (2009); Ben-David et al. (2010); Cortes & Mohri (2014). These bounds employ distribution distance metrics such as the A-distance or the discrepancy metric, which are related to the IPM distance we use. Our algorithm is similar to a recent algorithm for domain adaptation by Ganin et al. (2016), and in principle other domain adaptation methods (e.g. Daumé III (2007); Pan et al. (2011); Sun et al. (2016)) could be adapted for use in ITE estimation as presented here.\nFinally, our paper builds on Johansson et al. (2016), where we showed a connection between covariate shift and the task of estimating counterfactuals. We proposed learning a representation of the data that makes the treated and control distributions more similar, fitting a linear ridge-regression model on top of it. We bounded the relative error of fitting a ridge-regression using the distribution with reverse treatment assignment versus fitting a ridge-regression using the factual distribution. Unfortunately, the relative error bound is not at all informative regarding the absolute quality of the representation. In this paper we focus on a related but more substantive task: estimating the individual treatment effect, building on the counterfactual error term. We provide an informative bound on the absolute quality of the representation. We also derive a much more flexible family of algorithms, including non-linear hypotheses and much more powerful distribution metrics in the form of IPMs such as the Wasserstein and MMD distances. Finally, we conduct significantly more thorough experiments including a real-world dataset and out-of-sample performance, and show our methods outperform previously proposed ones."
  }, {
    "heading": "3. Estimating ITE: Error bounds",
    "text": "In this section we prove a bound on the expected error in estimating the individual treatment effect for a given representation, and a hypothesis defined over that representation. The bound is expressed in terms of (1) the expected loss of the model when learning the observed outcomes y as a function of x and t, denoted F , F standing for “Factual”; (2) an Integral Probability Metric (IPM) distance between the distribution of treated and control units. The term F is the classic machine learning generalization-error, and in turn can be upper bounded using the empirical error and model complexity terms, applying standard machine learning theory (Shalev-Shwartz & Ben-David, 2014)."
  }, {
    "heading": "3.1. Problem setup",
    "text": "We will employ the following assumptions and notations. The most important notations are in the Notation box in the\nsupplement. The space of covariates is a bounded subset X ⊂ Rd. The outcome space is Y ⊂ R. Treatment t is a binary variable. We assume there exists a joint distribution p(x, t, Y0, Y1), such that (Y1, Y0) ⊥ t|x and 0 < p(t = 1|x) < 1 for all x ∈ X (strong ignorability). The treated and control distributions are the distribution of the features x conditioned on treatment: pt=1(x) := p(x|t = 1), and pt=0(x) := p(x|t = 0), respectively.\nThroughout this paper we will discuss representation functions of the form Φ : X → R, where R is the representation space. We make the following assumption about Φ:\nAssumption 1. The representation Φ is a twicedifferentiable, one-to-one function. Without loss of generality we will assume that R is the image of X under Φ. We then have Ψ : R → X as the inverse of Φ, such that Ψ(Φ(x)) = x for all x ∈ X .\nThe representation Φ pushes forward the treated and control distributions into the new space R; we denote the induced distribution by pΦ.\nDefinition 1. Define pt=1Φ (r) := pΦ(r|t = 1), pt=0Φ (r) := pΦ(r|t = 0), to be the treated and control distributions induced over R. For a one-to-one Φ, the distributions pt=1Φ (r) and p t=0 Φ (r) can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r).\nLet Φ : X → R be a representation function, and h : R × {0, 1} → Y be an hypothesis defined over the representation space R. Let L : Y × Y → R+ be a loss function. We define two complimentary loss functions: one is the standard machine learning loss, which we will call the factual loss F , as it relates to observable quantities. The other is the expected loss with respect to the distribution where the treatment assignment is flipped, which we call the counterfactual loss, CF .\nDefinition 2. The expected loss for the unit and treatment pair (x, t) is: `h,Φ(x, t) =∫ Y L(Yt, h(Φ(x), t))p(Yt|x)dYt. The expected factual and counterfactual losses of h and Φ are:\nF (h,Φ) = ∫ X×{0,1} `h,Φ(x, t) p(x, t) dxdt,\nCF (h,Φ) = ∫ X×{0,1} `h,Φ(x, t) p(x, 1− t) dxdt.\nIf x denotes patients’ features, t a treatment, and Yt a potential outcome such as mortality, we think of F as measuring how well do h and Φ predict mortality for the patients and doctors’ actions sampled from the same distribution as our data sample. CF measures how well our prediction with h and Φ would do in a “topsy-turvy” world where the patients are the same but the doctors are inclined to prescribe\nexactly the opposite treatment than the one the real-world doctors would prescribe.\nDefinition 3. The expected factual treated and control losses are:\nt=1F (h,Φ) = ∫ X `h,Φ(x, 1) p t=1(x) dx,\nt=0F (h,Φ) = ∫ X `h,Φ(x, 0) p t=0(x) dx.\nFor u := p(t = 1), it is immediate to show that F (h,Φ) = u t=1F (h,Φ) + (1− u) t=0F (h,Φ). Definition 4. The treatment effect (ITE) for unit x is:\nτ(x) := E [Y1 − Y0|x] .\nLet f : X × {0, 1} → Y by an hypothesis. For example, we could have that f(x, t) = h(Φ(x), t).\nDefinition 5. The treatment effect estimate of the hypothesis f for unit x is:\nτ̂f (x) = f(x, 1)− f(x, 0).\nDefinition 6. The expected Precision in Estimation of Heterogeneous Effect (PEHE, Hill (2011)) loss of f is:\nPEHE(f) = ∫ X (τ̂f (x)− τ(x))2 p(x) dx, (1)\nWhen f(x, t) = h(Φ(x), t), we will also use the notation PEHE(h,Φ) = PEHE(f).\nOur proof relies on the notion of an Integral Probability Metric (IPM), which is a class of metrics between probability distributions (Sriperumbudur et al., 2012; Müller, 1997). For two probability density functions p, q defined over S ⊆ Rd, and for a function family G of functions g : S → R, we have that\nIPMG(p, q) := sup g∈G ∣∣∣∣∫ S g(s)(p(s)− q(s)) ds ∣∣∣∣ . Integral probability metrics are always symmetric and obey the triangle inequality, and trivially satisfy IPMG(p, p) = 0. For rich enough function families G, we also have that IPMG(p, q) = 0 =⇒ p = q, and then IPMG is a true metric. Examples of function families G for which IPMG is a true metric are the family of bounded continuous functions, the family of 1-Lipschitz functions (Sriperumbudur et al., 2012), and the unit-ball of functions in a universal reproducing kernel Hilbert space (Gretton et al., 2012)."
  }, {
    "heading": "3.2. Bounds",
    "text": "We first state a Lemma bounding the counterfactual loss, a key step in obtaining the bound on the error in estimating\nindividual treatment effect. We then give the main Theorem. The proofs and details are in the supplement.\nLet u := p(t = 1) be the marginal probability of treatment. By the strong ignorability assumption, 0 < u < 1.\nLemma 1. Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Let h : R × {0, 1} → Y be an hypothesis. Let G be a family of functions g : R → Y . Assume there exists a constantBΦ > 0, such that for fixed t ∈ {0, 1}, the per-unit expected loss functions `h,Φ(Ψ(r), t) (Definition 2) obey 1BΦ · `h,Φ(Ψ(r), t) ∈ G. We have:\nCF (h,Φ) ≤ (1− u) t=1F (h,Φ) + u t=0F (h,Φ) +BΦ · IPMG ( pt=1Φ , p t=0 Φ ) ,\nwhere CF , t=0F and t=1 F are as in Definitions 2 and 3.\nTheorem 1. Under the conditions of Lemma 1, and assuming the loss L used to define `h,Φ in Definitions 2 and 3 is the squared loss, we have:\nPEHE(h,Φ) ≤ 2 ( CF (h,Φ) + F (h,Φ)− 2σ2Y ) ≤ (2)\n2 ( t=0F (h,Φ)+ t=1 F (h,Φ)+BΦIPMG ( pt=1Φ , p t=0 Φ ) −2σ2Y ) ,\nwhere F and CF are defined w.r.t. the squared loss, and σ2Y is the variance of the outcomes Yt (see Definition A11 in Appendix for detailed definition).\nThe main idea of the proof is showing that PEHE is upper bounded by the sum of the expected factual loss F and expected counterfactual loss CF . However, we cannot estimate CF , since we only have samples relevant to F . We therefore bound the difference CF − F using an IPM.\nChoosing a small function family G makes the bound tighter. However, choosing too small a family could result in an incomputable bound. For example, for the minimal choice G = {`h,Φ(x, 0), `h,Φ(x, 1)}, we will have to evaluate an expectation term of Y1 over pt=0Φ , and of Y0 over pt=1Φ . We cannot in general evaluate these expectations, since by assumption when t = 0 we only observe Y0, and the same for t = 1 and Y1. In addition, for some function families there is no known way to efficiently compute the IPM distance or its gradients. Here, we use two function families for which there are available optimization tools. The first is the family of 1-Lipschitz functions, which leads to IPM being the Wasserstein distance (Villani, 2008), denoted Wass(p, q). The second is the family of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric (Gretton et al., 2012), denoted MMD(p, q). Both the Wasserstein and MMD metrics have consistent estimators which can be efficiently computed for finite samples (Sriperumbudur et al., 2012), and\nhave been used for various machine learning tasks in recent years (Gretton et al., 2009; 2012; Cuturi & Doucet, 2014).\nIn order to explicitly evaluate the constant BΦ in Theorem 1, we have to make some assumptions about the elements of the problem. For the Wasserstein case these are the loss L, the Lipschitz constants of p(Yt|x) and h, and the condition number of the Jacobian of Φ. For the MMD case, we make assumptions about the RKHS representability and RKHS norms of h , Φ, and the standard deviation of Yt|x. The full details are given in the supplement, with the major results stated in Theorems 2 and 3. In all cases we obtain that making Φ smaller increases the constant BΦ precluding trivial solutions such as making Φ arbitrarily small.\nFor an empirical sample, and a family of representations and hypotheses, we can further upper bound t=0F and t=1 F by their respective empirical losses and a model complexity term using standard arguments (Shalev-Shwartz & BenDavid, 2014). The IPMs we use can be consistently estimated from finite samples (Sriperumbudur et al., 2012). The negative variance term σ2Y arises from the fact that, following Hill (2011); Athey & Imbens (2016), we define the error PEHE in terms of the conditional mean functions mt(x), as opposed to fitting the random variables Yt.\nOur results hold for any given h and Φ obeying the Theorem conditions. This immediately suggest an algorithm in which we minimize the upper bound in Eq. (2) with respect to Φ and h and either the Wasserstein or MMD IPM, in order to minimize the error in estimating the individual treatment effect. This leads us to Algorithm 1 below."
  }, {
    "heading": "4. Algorithm for estimating ITE",
    "text": "We propose a general framework called CFR (for Counterfactual Regression) for ITE estimation based on the theoretical results above. Our algorithm is an end-to-end, regularized minimization procedure which fits both a balanced representation of the data and a hypothesis for the outcome. CFR draws on the same intuition as our previous work (Johansson et al., 2016), but overcomes the following limitations: a) Our previous theory requires a two-step optimization procedure and is specific to linear hypotheses (it does not support e.g. deep neural networks), b) The treatment indicator might be washed out in the old model, if the learned representation is high-dimensional (see discussion below).\nWe assume there exists a distribution p(x, t, Y0, Y1) over X × {0, 1} × Y × Y , such that strong ignorability holds. We further assume we have a sample from that distribution (x1, t1, y1), . . . (xn, tn, yn), where yi ∼ p(Y1|xi) if ti = 1, yi ∼ p(Y0|xi) if ti = 0. This standard assumption means that the treatment assignment determines which potential outcome we see. Our goal is to find a representation Φ : X → R and hypothesis h : X × {0, 1} → Y that will\nminimize PEHE(f) for f(x, t) := h(Φ(x), t).\nWe parameterize Φ(x) and h(Φ, t) by deep neural networks trained jointly, see Figure 1. This allows for learning complex non-linear representations and hypotheses with large flexibility. In Johansson et al. (2016), we parameterized h(Φ, t) with a single network, concatenating Φ and t as input. In this case, if Φ is high-dimensional, the influence of t on hmight be lost during training. To combat this, we parameterize h1(Φ) = h(Φ, 1) and h0(Φ) = h(Φ, 0) as two separate “heads” of the joint network, the former used to estimate the outcome under treatment, and the latter under control. This way, statistical power is shared in representation layers, while the effect of treatment is preserved in the separate heads. Note that each sample is used to update only the head corresponding to the observed treatment.\nOur second contribution is to explicitly adjust for the bias induced by treatment group imbalance. To this end, we seek a representation Φ and hypothesis h that minimizes a trade-off between predictive accuracy and imbalance in the representation space, using the following objective:\nmin h,Φ ‖Φ‖=1\n1 n ∑n i=1 wi · L (h(Φ(xi), ti) , yi) + λ ·R(h)\n+α · IPMG ({Φ(xi)}i:ti=0, {Φ(xi)}i:ti=1) , with wi = ti2u + 1−ti 2(1−u) , where u = 1 n ∑n i=1 ti,\nand R is a model complexity term. (3)\nNote that u = p(t = 1) is simply the proportion of treated units in the population. The weights wi compensate for the difference in treatment group size in our sample, see Theorem 1. IPMG(·, ·) is the (empirical) integral probability metric w.r.t. G. For most IPMs, we cannot compute the factor Bφ in (2), but treat it as part of the hyperparameter α. This makes our objective sensitive to the scaling of Φ, even for a constant α. We therefore normalize Φ through either projection or batch-normalization with fixed scale.\nWe refer to the model minimizing (3) with α > 0 as Counterfactual Regression (CFR) and the variant without balance regularization (α = 0) as Treatment-Agnostic Representation Network (TARNet). Both models are trained by minimizing (3) using stochastic gradient descent, as described in Algorithm 1. Both the prediction loss and the penalty term IPMG(·, ·) are computed for one mini-batch at a time. Details of how to obtain the gradient g1 with respect to the empirical IPMs are in the supplement."
  }, {
    "heading": "5. Experiments",
    "text": "Evaluating causal inference algorithms is more difficult than many machine learning tasks, since we rarely have access to the ground truth treatment effect. Existing literature mostly deals with this in two ways. One is by using (semi-)\nAlgorithm 1 CFR: Counterfactual regression with integral probability metrics\n1: Input: Factual sample (x1, t1, y1), . . . , (xn, tn, yn), scaling parameter α > 0, loss function L (·, ·), representation network ΦW with initial weights W, outcome network hV with initial weights V, function family G for IPM.\n2: Compute u = 1n ∑n i=1 ti 3: Compute wi = ti2u + 1−ti\n2(1−u) for i = 1 . . . n 4: while not converged do 5: Sample mini-batch {i1, i2, . . . , im} ⊂ {1, 2, . . . , n} 6: Calculate the gradient of the IPM term: g1 =∇W IPMG({ΦW(xij )}tij=0, {ΦW(xik )}tij=1) 7: Calculate the gradients of the empirical loss:\ng2 = ∇V 1m ∑ j wij · L ( hV(ΦW(xij ), tij ), yij ) g3 = ∇W 1m ∑ j wij · L ( hV(ΦW(xij ), tij ), yij\n) 8: Obtain step size scalar or matrix η with standard neural net methods e.g. Adam (Kingma & Ba, 2015) 9: [W,V]← [W − η(αg1 + g3),V − η(g2 + 2λV)]\n10: Check convergence criterion 11: end while\nsynthetic datasets, where the outcome or treatment assignment are fully known; we use the semi-synthetic IHDP dataset from Hill (2011). The other is using real-world data from randomized controlled trials (RCT). The problem with using data from RCTs is that there is no imbalance between treatment groups, making our method redundant. We partially overcome this problem by using the Jobs dataset from LaLonde (1986), which includes both a randomized and a non-randomized component. We use both components for training, but only use the randomized component for evaluation. This alleviates, but does not solve, the issue of a completely randomized and balanced dataset being unsuited for our method.\nWe evaluate our framework CFR, and its variant without balancing regularization (TARNet), in the task of estimating ITE and ATE. Both versions are implemented3 as feed-forward neural networks with 3 fully-connected exponential-linear layers (Clevert et al., 2016) for the representation and 3 for the hypothesis. Layer sizes were 200 for all layers used for Jobs and 200 and 100 for the representation and hypothesis used for IHDP. The model is trained using Adam (Kingma & Ba, 2015). The hypothesis parameters are regularized with a small `2 weight decay. For continuous data we use mean squared loss and for binary data, we use log-loss. While our theory does not immediately apply to log-loss, we were curious to see how our model performs with it. We use the Wasserstein (CFRWASS) and the squared linear MMD (CFRMMD) distances to penalize\n3https://github.com/clinicalml/cfrnet\nimbalance.\nWe compare our method to Ordinary Least Squares with treatment as a feature (OLS1), OLS with separate regressors for each treatment (OLS2), k-nearest neighbor (k-NN), Targeted Maximum Likelihood (TMLE), which is a doubly robust method (Gruber & van der Laan, 2011), Bayesian Additive Regression Trees (BART) (Chipman et al., 2010; Chipman & McCulloch, 2016), Random Forests (R. For.) (Breiman, 2001), Causal Forests (C. For.) (Wager & Athey, 2015) as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) from Johansson et al. (2016). For classification tasks we substitute Logistic Regression (LR) for OLS. Choosing hyperparameters for estimating PEHE is nontrivial; we detail our general procedure using a validation set, in subsection C.1 of the supplement.\nWe consider two different estimation tasks. One is withinsample, where the task is to estimate ITE for all units in a sample for which the (factual) outcome of one treatment is observed. This corresponds to the common scenario in which a cohort is selected once and not changed. This task is non-trivial, as we never observe the ITE for any unit. The other is out-of-sample, where the goal is to estimate ITE for units with no observed outcomes. This corresponds to the problem of selecting the best treatment for a new patient. Within-sample error is computed over both the training and validation sets, out-of-sample error over the test set."
  }, {
    "heading": "5.1. Simulated outcome: IHDP",
    "text": "Hill (2011) compiled a dataset for causal effect estimation based on the Infant Health and Development Program (IHDP), in which the covariates come from a randomized experiment studying the effects of specialist home visits on cognitive test scores. The treatment groups have been made imbalanced by removing a biased subset of the treated population. The dataset comprises 747 units (139 treated, 608 control) and 25 covariates measuring aspects of children and their mothers. We use the simulated outcome implemented as setting “A” in the NPCI package (Dorie, 2016). Following Hill (2011), we use the noiseless outcome to compute the true effect. We report the estimated (finitesample) PEHE loss PEHE (Eq. 1), and the absolute error in average treatment effect ATE = | 1n ∑n i=1(f(xi, 1)−\nf(xi, 0)) − 1n ∑n i=1(m1(xi) − m0(xi))|. The results of the experiments on IHDP are presented in Table 1 (left). We average over 1000 realizations of the outcomes with 63/27/10 train/validation/test splits.\nWe also investigate the effects of increasing imbalance between the original treatment groups by constructing biased subsamples of the IHDP dataset. A logistic-regression propensity score model is fit to form estimates p̂(t = 1|x) of the conditional treatment probability. Then, repeatedly,\nwith probability q we remove the remaining control observation x that has p̂(t = 1|x) closest to 1, and with probability 1− q, we remove a random control observation. The higher q, the more imbalance. For each value of q, we remove 347 observations from each set, leaving 400."
  }, {
    "heading": "5.2. Real-world outcome: Jobs",
    "text": "The study by LaLonde (1986) is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are income and employment status after training. This dataset combines a randomized study based on the National Supported Work program with observational data to form a larger dataset (Smith & Todd, 2005). The presence of the randomized subgroup gives a way to estimate the “ground truth” causal effect. The study includes 8 covariates such as age and education, as well as previous earnings. We construct a binary classification task, called Jobs, where the goal is to predict unemployment, using the feature set of Dehejia & Wahba (2002). Following Smith & Todd (2005), we use the LaLonde experimental sample (297 treated, 425 control) and the PSID comparison group (2490 control). There were 482 (15%) subjects unemployed by the end of the study. We average\nover 10 train/validation/test splits with ratios 56/24/20.\nBecause all the treated subjects T were part of the original randomized sample E, we can compute the true average treatment effect on the treated by ATT = |T |−1 ∑ i∈T yi−\n|C ∩ E|−1 ∑ i∈C∩E yi, where C is the control group. We\nreport the error ATT = |ATT − 1|T | ∑ i∈T (f(xi, 1) − f(xi, 0))|. We cannot evaluate PEHE on this dataset, since there is no ground truth for the ITE. Instead, in order to evaluate the quality of ITE estimation, we use a measure we call policy risk. The policy risk is defined as the average loss in value when treating according to the policy implied by an ITE estimator. In our case, for a model f , we let the policy be to treat, πf (x) = 1, if f(x, 1) − f(x, 0) > λ, and to not treat, πf (x) = 0 otherwise. The policy risk is RPol(πf ) = 1 − (E[Y1|πf (x) = 1] · p(πf = 1) + E[Y0|πf (x) = 0] · p(πf = 0)) which we can estimate for the randomized trial subset of Jobs by R̂Pol(πf = 1 − (E[Y1|πf (x) = 1, t = 1] · p(πf = 1) + E[Y0|πf (x) = 0, t = 0] · p(πf = 0)). See figure 3 for risk as a function of treatment threshold λ, aligned by proportion of treated, and Table 1 for the risk when λ = 0."
  }, {
    "heading": "5.3. Results",
    "text": "We note that indeed imbalance confers an advantage to using the IPM regularization term, as our theoretical results indicate, see e.g. the results for CFRWASS and TARNet on IHDP in Table 1. We also see in Figure 2 that even for the harder case of increased imbalance (q > 0) between treated and control, the relative gain from using our method remains significant. On Jobs, our proposed methods are better than or competitive with state-of-the-art, but we don’t see a significant gain from using IPM penalties. This might be because we evaluate the predictions only on a randomized subset with treatment groups distributed identically. Non-linear estimators perform significantly better than linear ones in terms of individual effect ( PEHE). On the Jobs dataset, straightforward logistic regression does\nremarkably well in estimating the ATT. However, being a linear model, LR can only ascribe a uniform policy - in this case, “treat everyone”. The more nuanced policies offered by non-linear methods achieve lower policy risk in the case of Causal Forests and CFR. This emphasizes the fact that estimating average effect and individual effect can require different models. Specifically, while smoothing over many units may yield a good ATE estimate, this might significantly hurt ITE estimation. k-nearest neighbors has very good within-sample results on Jobs, because evaluation is performed over the randomized component, but suffers heavily in generalizing out of sample, as expected."
  }, {
    "heading": "6. Conclusion",
    "text": "In this paper we give a meaningful and intuitive errorbound for estimating individual treatment effect. Our bound relates ITE estimation to the classic machine learning problem of learning from samples, along with methods for measuring distributional distances from samples. The bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations and hypotheses. We apply our theory-guided approach to both synthetic and real-world tasks, showing that in every case our method matches or outperforms the state-of-theart. Important open questions are theoretical considerations in choosing the IPM weight α, how to best derive confidence intervals for our model’s predictions, and integrating our work with more complicated causal models such as those with hidden confounding or instrumental variables."
  }, {
    "heading": "ACKNOWLEDGMENTS",
    "text": "We thank Aahlad Puli for his assistance with the experiments; Sanjog Misra and Günter J. Hitsch for suggesting the policy risk evaluation; Jennifer Hill, Marco Cuturi and Esteban Tabak for fruitful conversations; and Stefan Wager for his help with the code for Causal Forests. DS and US were supported by NSF CAREER award #1350965."
  }],
  "year": 2017,
  "references": [{
    "title": "Recursive partitioning for heterogeneous causal effects",
    "authors": ["Athey", "Susan", "Imbens", "Guido"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Efficient inference of average treatment effects in high dimensions via approximate residual balancing",
    "authors": ["Athey", "Susan", "Imbens", "Guido W", "Wager", "Stefan"],
    "venue": "arXiv preprint arXiv:1604.07125,",
    "year": 2016
  }, {
    "title": "An introduction to propensity score methods for reducing the effects of confounding in observational studies",
    "authors": ["Austin", "Peter C"],
    "venue": "Multivariate behavioral research,",
    "year": 2011
  }, {
    "title": "Causal inference and the data-fusion problem",
    "authors": ["Bareinboim", "Elias", "Pearl", "Judea"],
    "venue": "Proceedings of the National Academy of Sciences,",
    "year": 2016
  }, {
    "title": "Improving quantitative studies of international conflict: A conjecture",
    "authors": ["Beck", "Nathaniel", "King", "Gary", "Zeng", "Langche"],
    "venue": "American Political Science Review,",
    "year": 2000
  }, {
    "title": "Inference on treatment effects after selection among highdimensional controls",
    "authors": ["Belloni", "Alexandre", "Chernozhukov", "Victor", "Hansen", "Christian"],
    "venue": "The Review of Economic Studies,",
    "year": 2014
  }, {
    "title": "Analysis of representations for domain adaptation",
    "authors": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Pereira", "Fernando"],
    "venue": "Advances in neural information processing systems,",
    "year": 2007
  }, {
    "title": "A theory of learning from different domains",
    "authors": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Vaughan", "Jennifer Wortman"],
    "venue": "Machine learning,",
    "year": 2010
  }, {
    "title": "Representation learning: A review and new perspectives",
    "authors": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"],
    "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
    "year": 2013
  }, {
    "title": "Double machine learning for treatment and causal parameters",
    "authors": ["Chernozhukov", "Victor", "Chetverikov", "Denis", "Demirer", "Mert", "Duflo", "Esther", "Hansen", "Christian"],
    "venue": "arXiv preprint arXiv:1608.00060,",
    "year": 2016
  }, {
    "title": "BayesTree: Bayesian Additive Regression Trees. https://cran.r-project",
    "authors": ["Chipman", "Hugh", "McCulloch", "Robert"],
    "venue": "org/web/packages/BayesTree,",
    "year": 2016
  }, {
    "title": "BART: Bayesian additive regression trees",
    "authors": ["Chipman", "Hugh A", "George", "Edward I", "McCulloch", "Robert E"],
    "venue": "The Annals of Applied Statistics,",
    "year": 2010
  }, {
    "title": "Fast and accurate deep network learning by exponential linear units (elus)",
    "authors": ["Clevert", "Djork-Arné", "Unterthiner", "Thomas", "Hochreiter", "Sepp"],
    "venue": "International Conference on Learning Representations,",
    "year": 2016
  }, {
    "title": "Domain adaptation and sample bias correction theory and algorithm for regression",
    "authors": ["Cortes", "Corinna", "Mohri", "Mehryar"],
    "venue": "Theoretical Computer Science,",
    "year": 2014
  }, {
    "title": "Fast computation of Wasserstein barycenters",
    "authors": ["Cuturi", "Marco", "Doucet", "Arnaud"],
    "venue": "In Proceedings of The 31st International Conference on Machine Learning,",
    "year": 2014
  }, {
    "title": "Frustratingly easy domain adaptation",
    "authors": ["Daumé III", "Hal"],
    "venue": "Conference of the Association for Computational Linguistics (ACL),",
    "year": 2007
  }, {
    "title": "Propensity score-matching methods for nonexperimental causal studies",
    "authors": ["Dehejia", "Rajeev H", "Wahba", "Sadek"],
    "venue": "Review of Economics and statistics,",
    "year": 2002
  }, {
    "title": "NPCI: Non-parametrics for Causal Inference",
    "authors": ["Dorie", "Vincent"],
    "venue": "https://github.com/vdorie/npci,",
    "year": 2016
  }, {
    "title": "Doubly robust estimation of causal effects",
    "authors": ["Funk", "Michele Jonsson", "Westreich", "Daniel", "Wiesen", "Chris", "Stürmer", "Til", "Brookhart", "M Alan", "Davidian", "Marie"],
    "venue": "American journal of epidemiology,",
    "year": 2011
  }, {
    "title": "Domain-adversarial training of neural networks",
    "authors": ["Ganin", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "François", "Marchand", "Mario", "Lempitsky", "Victor"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Covariate shift by kernel mean matching",
    "authors": ["Gretton", "Arthur", "Smola", "Alex", "Huang", "Jiayuan", "Schmittfull", "Marcel", "Borgwardt", "Karsten", "Schölkopf", "Bernhard"],
    "venue": "Dataset shift in machine learning,",
    "year": 2009
  }, {
    "title": "A kernel twosample test",
    "authors": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Schölkopf", "Bernhard", "Smola", "Alexander"],
    "venue": "J. Mach. Learn. Res.,",
    "year": 2012
  }, {
    "title": "tmle: An r package for targeted maximum likelihood estimation",
    "authors": ["Gruber", "Susan", "van der Laan", "Mark J"],
    "year": 2011
  }, {
    "title": "Counterfactual prediction with deep instrumental variables networks",
    "authors": ["Hartford", "Jason", "Lewis", "Greg", "Leyton-Brown", "Kevin", "Taddy", "Matt"],
    "venue": "arXiv preprint arXiv:1612.09596,",
    "year": 2016
  }, {
    "title": "Bayesian nonparametric modeling for causal inference",
    "authors": ["Hill", "Jennifer L"],
    "venue": "Journal of Computational and Graphical Statistics,",
    "year": 2011
  }, {
    "title": "Nonlinear causal discovery with additive noise models",
    "authors": ["Hoyer", "Patrik O", "Janzing", "Dominik", "Mooij", "Joris M", "Peters", "Jonas", "Schölkopf", "Bernhard"],
    "venue": "In Advances in neural information processing systems,",
    "year": 2009
  }, {
    "title": "Recent developments in the econometrics of program evaluation",
    "authors": ["Imbens", "Guido W", "Wooldridge", "Jeffrey M"],
    "venue": "Journal of economic literature,",
    "year": 2009
  }, {
    "title": "Learning representations for counterfactual inference",
    "authors": ["Johansson", "Fredrik D", "Shalit", "Uri", "Sontag", "David"],
    "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),",
    "year": 2016
  }, {
    "title": "Adam: A method for stochastic optimization",
    "authors": ["Kingma", "Diederik", "Ba", "Jimmy"],
    "venue": "International Conference on Learning Representations,",
    "year": 2015
  }, {
    "title": "Evaluating the econometric evaluations of training programs with experimental data",
    "authors": ["LaLonde", "Robert J"],
    "venue": "The American economic review,",
    "year": 1986
  }, {
    "title": "Predicting causal effects in large-scale systems from observational data",
    "authors": ["Maathuis", "Marloes H", "Colombo", "Diego", "Kalisch", "Markus", "Bühlmann", "Peter"],
    "venue": "Nature Methods,",
    "year": 2010
  }, {
    "title": "Domain adaptation: Learning bounds and algorithms",
    "authors": ["Mansour", "Yishay", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"],
    "year": 2009
  }, {
    "title": "Distinguishing cause from effect using observational data: methods and benchmarks",
    "authors": ["Mooij", "Joris M", "Peters", "Jonas", "Janzing", "Dominik", "Zscheischler", "Jakob", "Schölkopf", "Bernhard"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2016
  }, {
    "title": "Integral probability metrics and their generating classes of functions",
    "authors": ["Müller", "Alfred"],
    "venue": "Advances in Applied Probability,",
    "year": 1997
  }, {
    "title": "Domain adaptation via transfer component analysis",
    "authors": ["Pan", "Sinno Jialin", "Tsang", "Ivor W", "Kwok", "James T", "Yang", "Qiang"],
    "venue": "Neural Networks, IEEE Transactions on,",
    "year": 2011
  }, {
    "title": "Detecting latent heterogeneity",
    "authors": ["Pearl", "Judea"],
    "venue": "Sociological Methods & Research, pp. 0049124115600597,",
    "year": 2015
  }, {
    "title": "Combining observational and experimental data to find heterogeneous treatment effects",
    "authors": ["Peysakhovich", "Alexander", "Lada", "Akos"],
    "venue": "arXiv preprint arXiv:1611.02385,",
    "year": 2016
  }, {
    "title": "A new approach to causal inference in mortality studies with a sustained exposure periodapplication to control of the healthy worker survivor effect",
    "authors": ["Robins", "James"],
    "venue": "Mathematical Modelling,",
    "year": 1986
  }, {
    "title": "Estimation of Conditional Average Treatment Effects",
    "authors": ["Rolling", "Craig Anthony"],
    "venue": "PhD thesis, University of Minnesota,",
    "year": 2014
  }, {
    "title": "Causal inference using potential outcomes",
    "authors": ["Rubin", "Donald B"],
    "venue": "Journal of the American Statistical Association,",
    "year": 2011
  }, {
    "title": "Understanding machine learning: From theory to algorithms",
    "authors": ["Shalev-Shwartz", "Shai", "Ben-David"],
    "year": 2014
  }, {
    "title": "Identification of conditional interventional distributions",
    "authors": ["Shpitser", "Ilya", "Pearl", "Judea"],
    "venue": "In Proceedings of the Twenty-second Conference on Uncertainty in Artificial Intelligence,",
    "year": 2006
  }, {
    "title": "Does matching overcome LaLonde’s critique of nonexperimental estimators",
    "authors": ["Smith", "Jeffrey A", "Todd", "Petra E"],
    "venue": "Journal of econometrics,",
    "year": 2005
  }, {
    "title": "On the empirical estimation of integral probability metrics",
    "authors": ["Sriperumbudur", "Bharath K", "Fukumizu", "Kenji", "Gretton", "Arthur", "Schölkopf", "Bernhard", "Lanckriet", "Gert RG"],
    "venue": "Electronic Journal of Statistics,",
    "year": 2012
  }, {
    "title": "Learning from logged implicit exploration data",
    "authors": ["Strehl", "Alex", "Langford", "John", "Li", "Lihong", "Kakade", "Sham M"],
    "venue": "In Advances in Neural Information Processing Systems,",
    "year": 2010
  }, {
    "title": "Return of frustratingly easy domain adaptation",
    "authors": ["Sun", "Baochen", "Feng", "Jiashi", "Saenko", "Kate"],
    "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,",
    "year": 2016
  }, {
    "title": "Batch learning from logged bandit feedback through counterfactual risk minimization",
    "authors": ["Swaminathan", "Adith", "Joachims", "Thorsten"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "A nonparametric bayesian analysis of heterogenous treatment effects in digital experimentation",
    "authors": ["Taddy", "Matt", "Gardner", "Chen", "Liyun", "Draper", "David"],
    "venue": "Journal of Business & Economic Statistics,",
    "year": 2016
  }, {
    "title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets",
    "authors": ["Triantafillou", "Sofia", "Tsamardinos", "Ioannis"],
    "venue": "Journal of Machine Learning Research,",
    "year": 2015
  }, {
    "title": "Optimal transport: old and new, volume 338",
    "authors": ["Villani", "Cédric"],
    "venue": "Springer Science & Business Media,",
    "year": 2008
  }, {
    "title": "Estimation and inference of heterogeneous treatment effects using random forests. arXiv preprint arXiv:1510.04342",
    "authors": ["Wager", "Stefan", "Athey", "Susan"],
    "venue": "https:// github.com/susanathey/causalTree,",
    "year": 2015
  }],
  "id": "SP:7b70bd7713d3138e07cdcfa5e40c7a3a0bd9100d",
  "authors": [{
    "name": "Uri Shalit",
    "affiliations": []
  }, {
    "name": "Fredrik D. Johansson",
    "affiliations": []
  }, {
    "name": "David Sontag",
    "affiliations": []
  }],
  "abstractText": "There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.",
  "title": "Estimating individual treatment effect: generalization bounds and algorithms"
}