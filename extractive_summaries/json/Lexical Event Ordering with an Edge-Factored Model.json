{
  "sections": [{
    "text": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics"
  }, {
    "heading": "1 Introduction",
    "text": "Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for model-\ning grammatical phenomena such as tense and aspect (Steedman, 2002).\nIn this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences.\nCompiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013).\nPrevious work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo-\n1161\ncused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often precisionbased or extrinsic.\nWe take a graph-based structured prediction approach to the task, motivated by the flexibility it allows in incorporating various feature sets and constraints. We use an edge-factored model, which decomposes over the edges in the graph of events comprising the recipe (§4). We estimate the model using the structured perceptron algorithm. We compare the structured perceptron approach to an approximate greedy baseline and to a locally normalized model reminiscent of common approaches for order learning, obtaining superior results (§8). The learning algorithm is of potential use in other ordering tasks such as machine translation reordering (Tromble and Eisner, 2009).\nWe focus on domains in which the order of events in the text is aligned with their temporal order. By doing so we avoid the costly and error-prone manual annotation of temporal relations by using the textual order of recipes to approximate their temporal order.1 Specifically, we address the cooking recipes domain, which we motivate in §2.\nIn summary, the contribution of this paper is three-fold: (1) we explore the task of lexical event ordering and means for its evaluation; (2) we present an edge-factored model for the task, and show it can be used to predict the order of events well (77.7% according to standard measures for ordering evaluation); (3) we present a method for extracting events and create a dataset of ordered events using recipes extracted from the web."
  }, {
    "heading": "2 Related Work",
    "text": "Temporal semantics is receiving increasing attention in recent years. Lexical features are in frequent use and rely in most part on external resources which are either manually compiled or automatically induced. The line of work most closely related to ours focuses on inducing lexical relations between\n1See Cassidy et al. (2014) for a discussion of inter-annotator agreement in TimeML-based schemes.\nevent types. Most work has been unsupervised, often using pattern-based approaches relying on manually crafted (Chklovski and Pantel, 2004) or induced patterns (Davidov et al., 2007), that correlate with temporal relations (e.g., temporal discourse connectives). Talukdar et al. (2012) uses the textual order of events in Wikipedia biographical articles to induce lexical information. We use both textual order and discourse connectives to define our feature set, and explore a setting which allows for the straightforward incorporation of additional features.\nChambers and Jurafsky (2008b; 2009) addressed the unsupervised induction of partially ordered event chains (or schema) in the news domain, centered around a common protagonist. One of their evaluation scenarios tackles a binary classification related to event ordering, and seeks to distinguish ordered sets of events from randomly permuted ones, yielding an accuracy of 75%. Manshadi et al. (2008) used language models to learn event sequences and conducted a similar evaluation on weblogs with about 65% accuracy. The classification task we explore here is considerably more complex (see §8).\nThe task of script knowledge induction has been frequently addressed in recent years. Balasubramanian et al. (2013) and Pichotta and Mooney (2014) extended Chambers and Jurafsky’s model to include events that have multiple arguments. Jans et al. (2012) use skip-grams to capture event-event relations between not necessarily consecutive events.\nRegneri et al. (2010) constructed a temporal lexical knowledge base through crowd-sourcing. Their approach is appealing as it greatly reduces the costs incurred by manual annotation and can potentially be used in conjunction with lexical information obtained from raw text. Modi and Titov (2014) jointly learns the stereotypical order of events and their distributional representation, in order to capture paraphrased instances of the same event type. Frermann et al. (2014) models the joint task of inducing event paraphrases and their order using a Bayesian framework. All latter three works evaluated their induced temporal ordering knowledge on a binary prediction of whether a temporal relation between a pair of (not necessarily related) events holds, and not on the prediction of a complete permutation given an unordered event set as in this work. Their evaluation was conducted on 30 event pairs manually an-\nnotated through crowd-sourcing, where Modi and Titov (2014) further included an evaluation on a large set of pairs automatically extracted from the Gigaword corpus.\nThe appeal of the cooking domain for studying various semantic phenomena has been recognized by several studies in NLP and AI (Tasse and Smith, 2008; Bollini et al., 2013; Cimiano et al., 2013; Regneri et al., 2013; Malmaud et al., 2014). The domain is here motivated by several considerations. First, recipes mostly describe concrete actions, rather than abstract relations, which are less relevant to temporal ordering. Second, from a practical point of view, many recipes are available online in computerreadable format. Third, the restrictiveness of the cooking domain can also be seen as an advantage, as it can reveal major conceptual challenges raised by the task, without introducing additional confounds."
  }, {
    "heading": "3 Temporally Ordering Lexical Events",
    "text": "We formalize our task as follows. Let U be a set of event types, namely actions or states (represented as predicates) and objects which these actions operate on (represented as arguments to the predicates; mostly ingredients or kitchenware). Formally, each e ∈ U is a tuple 〈a, c1, . . . , cn〉 where a is the main verb or predicate describing the event (such as “stir” or “mix”) and c1, . . . , cn is a list of arguments that the predicate takes (e.g., “salt” or “spoon”). Two additional marked events, s and f , correspond to “start” and “finish” events. A recipe is a sequence of events in U , starting at s and ending at f .\nGiven a recipe R = 〈e1, ..., em〉, we wish to predict the order of the events just from the (multi)set {ei}mi=1. In this work we use the textual order of events to approximate their temporal order (see, e.g., Talukdar et al. (2012) for a similar assumption). The validity of this assumption for cooking recipes is supported in §6.\nFigure 1 gives an example of a set of events extracted from our dataset for the dish “Apple Crisp Ala [sic] Brigitte.” Lexical information places quite a few limitations on the order of this recipe. For instance, in most cases serving is carried out at the end while putting the ingredients in is done prior to baking them. However, lexical knowledge in itself is unlikely to predict the exact ordering of the events\nas given in the recipe (e.g., spreading butter might be done before or after baking).\nOne of the major obstacles in tackling planning problems in AI is the knowledge bottleneck. Lexical event ordering is therefore a step towards more ambitious goals in planning. For instance, temporal relations may be used to induce planning operators (Mourão et al., 2012), which can in turn be used to generate a plan (recipe) given a specified goal and an initial set of ingredients."
  }, {
    "heading": "4 Model, Inference and Learning",
    "text": "In this section we describe the main learning components that compose our approach to event ordering."
  }, {
    "heading": "4.1 Edge-Factored Model for Event Ordering",
    "text": "We hereby detail the linear model we use for ordering events. Let S = {e1, . . . , em} ⊆ U be a set of events as mentioned in §3. Let G(S) = (S ∪ {s, f}, E(S)) be an almost-complete directed graph withE(S) = (S∪{s})×(S∪{f}) ⊆ U×U . Every Hamiltonian path2 inG(S) that starts in s and ends in f defines an ordering of the events in S. The edge (ei, ej) in such a path denotes that ei is the event that comes before ej .\nThe modeling problem is to score Hamiltonian paths in a given directed graph G(S). Here, we use an edge-factored model. Let φ : (U × U) → Rd be a feature function for pairs of events, represented as directed edges. In addition, let θ ∈ Rd be a weight vector. We define the score of a Hamiltonian path h = (h1, . . . , hm+1) (hi ∈ E(S)) as:\nscore(h|S) = m+1∑ i=1 θ>φ(hi) (1)\nGiven a weight vector θ and a set of events S, inference is carried out by computing the highest scoring Hamiltonian path in G(S):\nh∗ = arg max h∈H(S) score(h|S) (2)\nwhere H(S) is the set of Hamiltonian paths inG(S) that start with s and end with f . The path h∗ is the best temporal ordering of the set of events S according to the model in Eq. 1 with weight vector θ.\n2A path in a graph that visits all nodes exactly once.\n(a) e1 = 〈butter, dish〉 e2 = 〈put, apples,water, ... flour, cinnamon, it〉 e3 = 〈mix,with spoon, 〉 e4 = 〈spread, butter, salt, ... over mix〉 e5 = 〈bake,F〉 e6 = 〈serve, cream, cream〉\n(b) Butter a deep baking dish, put apples, water, flour, sugar and cin-\nnamon in it. Mix with spoon and\nspread butter and salt over the ap-\nple mix. Bake at 350 degrees F until\nthe apples are tender and the crust\nbrown, about 30 minutes. Serve\nwith cream or whipped cream.\n(c)\n(a) e 1\n= hbutter, dishi e 2\n= hput, apples,water, ... flour, cinnamon, iti\ne 3 = hmix, spoon, i e 4\n= hspread, butter, salt,mixi e 5\n= hbake,Fi e 6 = hserve, cream, creami\n(b) Butter a deep baking dish, put apples, water, flour, sugar and cinnamon in it. Mix with spoon and spread butter and salt over the apple mix. Bake at 350 degrees F until the apples are tender and the crust brown, about 30 minutes. Serve with cream or whipped cream.\ns\ne 1\ne 2\ne 3\ne 4\ne 5\ne 6\nf\n(c) (a) e1 = hmix, ✏, tarragon, vinegari e2 = hblend, ✏,mustardi e3 = hmix, ✏, salt, pepperi e4 = hblend, ✏,mayonnaise, sour creami e5 = hcover, ✏i e6 = hchill, ✏i (b) you mix the tarragon and vinegar together and blend in the mustard. you mix in the salt and pepper, blending well. you blend in the mayonnaise and then the sour cream. you cover and chill. s\ne1 e2\ne3 e4\ne5\ne6\ne\nFigure 1: (a) Example of events describing a recipe for the dish “.” (b) The actual recipe for this dish. (c) A complete graph over the set of events with start and end states. Each internal node in the graph is one of the events ei for i 2 {1, . . . , 5}. The path in bold denotes the correct Hamiltonian path describing the set of actions that need to be taken to follow the recipe.\n3 Model, Inference and Learning\nIn this section we describe the main learning components that compose our approach to event ordering.\n3.1 Edge-Factored Model for Event Ordering\nWe now turn to explain the linear model we use for ordering events in time. Let S = {v1, . . . , vm} ✓ U be a set of events as mentioned in section 2. Let G(S) = (S [ {s, e}, E(S)) be an almost-complete directed graph with E(S) = (S[{s})⇥(S[{e}) ✓ (U ⇥ U). Every Hamiltonian path5 in G(S) that starts in s and ends in e can be thought of as an ordering of the events in S. The edge (vi, vj) in such a path denotes that vi is the event that comes before vj .\nThe modeling problem, therefore, is to score Hamiltonian paths in a given directed graph G(S) such as the above. Here, we use an edge-factored model. Let : (U ⇥ U) ! Rd be a feature vector for pairs of events, represented as directed edges. In addition, let ✓ 2 Rd be a weight vector. Then, we define the score of an Hamiltonian path h = (h1, . . . , hm+1) (where hi 2 E(S) for i 2 {1, . . . ,m+ 1}) as:\n5An Hamiltonian path in a graph is a path that visits all nodes exactly once.\nscore(h|S) = m+1X\ni=1\n✓> (hi) (1)\nGiven a weight vector w and a set of events S, inference is carried out by computing the highest scoring Hamiltonian path in G(S):\nh⇤ = arg max h2H(S) score(h|S) (2)\nwhere H(S) is the set of Hamiltonian paths in G(S) that start with s and end with e. h⇤ is the best temporal ordering of the set of events S according to the structured model in Equation 1 with weight vector w.\n3.2 Inference As mentioned above, inference with the edgefactored model we presented would have to solve the maximization problem in Eq. 2. This corresponds to finding an Hamiltonian path in a complete graph, which is generally an NP-hard problem6. In the general case there is no reasonable approximation algorithm to solve the maximization algorithm, although\n6The NP complete problem of finding a Hamiltonian cycle in an undirected graph can be trivially reduced to finding the maximal Hamiltonian cycle in a directed graph.\nFigure 1: (a) Example of events describing a recipe for the dish “Apple Crisp Ala [sic] Brigitte.” For brevity, arguments are represented as headwords and their syntactic type is omitted. (b) The actual recipe for this dish. (c) A complete graph over the set of events with start and end states. Each internal node in the graph is one of the events ei for i 2 {1, . . . , 6}. The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered in the recipe. Red edges denote edges from the start state a d to the end state. The edges, in practice, are weighted.\nILP formulation yields superior performance to the other evaluated systems (§8). ILP has been proven to be a practical and flexible tool in various structured prediction tasks in NLP (Roth and tau Yih, 2007; Talukdar et al., 2012; Scaria et al., 2013). Our ILP formulation is given in Appendix A.\nWe experiment with an additional greedy inference algorithm, similar to the one described by Lapata (2003) for sentence ordering. The algorithm iteratively selects an outgoing edge (starting from the node s) that has the largest weight to a node that has not been visited so far, until all vertices are covered, at which point the path terminates by travelling to f . 4.3 Learning The learning problem takes as input a dataset consisting of unordered sets of events, paired with a target ordering. We consider two types of learning algorithms for the edge-factored model in the previous section. The first learns in a global training setting using the averaged structured perceptron (Collins, 2002), with the decoding algorithm being either the one based on ILP (henceforth, GLOBAL-PRC), or the greedy one (GREEDY-PRC).\nThe second learning algorithm we try is based on factored training. This algorithm maximizes the likelihood of a conditional log-linear model p:\np(e2|e1, ✓, S) = exp\n✓> (e1, e2)\nZ(✓, S, e1)\nZ(✓, S, e1) = X\ne : (e1,e)2E(S)\nexp ✓> (e1, e)\nwhere e 1 , e 2 2 S [ {s, f}. This is a locally normalized log-linear model that gives the probability of transitioning to node e\n2 from node e 1 . Maximizing the score in Eq. 1 has an interpretation of finding the highest scoring path according to an edge-factored Markovian model, such that:\np(h|✓, S) = m+1Y\ni=2\np(ei|ei 1, ✓, S),\nwhere h = (h 1 , . . . , h m+1 ) is a Hamiltonian path with h\ni = (e i 1, ei) being a directed edge in\nthe path. Initial experimentation suggested that greedy inference (henceforth, GREEDY-LOGLIN) works better in practice than the ILP formulation for the locally-normalized model. We therefore do not report results on global inference with this loglinear model. GREEDY-LOGLIN closely resembles the learning model of Lapata (2003), except that it is a discriminative log-linear model, rather of a generative Markovian model. 5 The Feature Set Table 1 presents all the complete set of features used for defining the feature function . We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects;\nFigure 1: (a) The sequence of events representing the recipe for the dish “Apple Crisp Ala [sic] Brigitte.” (b) The actual recipe\nfor this dish. (c) A complete graph over the set of events with start and finish states. Each internal node in the graph is one of the\nevents ei for i ∈ {1, . . . , 6}. The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered in\nthe recipe. Red edges denot edges from the start state and to the end state. The edges, in practice, are weighted."
  }, {
    "heading": "4.2 Inference",
    "text": "As mentioned above, inference with the edgefactored model requires solving he maximization problem in Eq. 2. This corresponds to finding a Hamiltonian path in a complete graph, which is generally an NP-hard problem. Reasonable approximations for this problem are also NP-hard. Still tec - niques are developed for specialized cases, due to the problem’s importance in discrete optimization.\nDespite its the retical NP-hardness, this maximization problem can be repr sented as an Integer Linear Program (ILP), and then solved using generic techniques for ILP optimizatio . Due to the relatively short length of recipes (13.8 vents on average in our corpus), th probl m can be effectively solved in most cases.\nThe proposed algorithmic setting is appealing for its flexibility. The linear score formulation allows us to use rich features, while using ILP allows to easily incorporate structural constraints. Indeed, ILP has been proven valuable in various NLP tasks (Roth and Yih, 2007; Talukdar et al., 2012; Scaria al., 2013). See Appendix A for our ILP formulation.\nAs a baseline, we experiment with an additional greedy inference algorithm, similar to the one described by Lapata (2003) for sentence ordering. he algorithm iteratively selects an outgoing edge (starting from the node s) that has the largest weight to a node that has not been visited so far, until all vertices are covered, at which point the path terminates by traveling to f ."
  }, {
    "heading": "4.3 Learning",
    "text": "The learning problem takes as input a dataset con-\nsisting of unordered sets of events, paired with a target ordering. We c nsider two types of learning algorithms for the edge-factored model in the previous section. The first learns in a global training setting using the averaged structured perceptron (Collins, 200 ), with the decoding algorithm being either the one based on ILP (henceforth, GLOBAL-PRC), or the greedy one (GREEDY-PRC). Given a training instance S and its correct label hc, the structured perceptron calls the inference procedure as a subroutine and updates the weight vector θ according to the difference between the value of the feature function on the predicted path ( ∑ h∗ φ(hi)) and on the correct\npath ( ∑\nhc φ(hi)). The second learning algorithm we try is based on factored training. This algorithm maximizes the likelihood of a conditional log-linear model p:\np(e2|e1, θ, S) = exp\n( θ>φ(e1, e2) ) Z(θ, S, e1)\nZ(θ, S, e1) = ∑\ne : (e1,e)∈E(S) exp\n( θ>φ(e1, e) ) where e1, e2 ∈ S ∪ {s, f}. This is a locally normalized log-linear model that gives the probability of transitioning to node e2 from node e1. Maximizing the score in Eq. 1 has an interpretation of finding the highest scoring path according to an edge-factored Markovian model, such that:\np(h|θ, S) = m+1∏ i=1 p(ei|ei−1, θ, S),\nwhere h = (h1, . . . , hm+1) is a Hamiltonian path with hi = (ei−1, ei) being a directed edge in the path. Initial experimentation suggested that greedy inference (henceforth, GREEDY-LOGLIN) works better in practice than the ILP formulation for the locally-normalized model. We therefore do not report results on global inference with this log-linear model. We suspect that greedy inference works better with the log-linear model because it is trained locally, while the perceptron algorithm includes a global inference step in its training, and therefore better matches global decoding.\nGREEDY-LOGLIN closely resembles the learning model of Lapata (2003), as both are firstorder Markovian and use the same (greedy) inference procedure. Lapata’s model differs from GREEDY-LOGLIN in being a generative model, where each event is a tuple of features, and the transition probability between events is defined as the product of transition probabilities between feature pairs. GREEDY-LOGLIN is discriminative, so to be maximally comparable to the presented model."
  }, {
    "heading": "5 The Feature Set",
    "text": "Table 1 presents the complete set of features. We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects; Brown uses Brown clusters (Brown et al., 1992) to encode similar information, but allows generalization between distributionally similar words; and Frequency encodes the empirical distribution of temporally-related phenomena.\nThe feature definitions make use of several functions. For brevity, we sometimes say that an event e is (a, c1) if e’s predicate is a and its first argument is c1, disregarding its other arguments. Let C be a reference corpus of recipes for collecting statistics. The function B(w) gives the Brown cluster of a word w, as determined by clustering C into 50 clusters {1, . . . , 50}. The function ORD(a, c) returns the mean ordinal number of an (a, c) event in C. The ordinal number of the event ei in a recipe (e1, ..., em) is defined as i− m2 .\nWe further encode the tendency of two events to appear with temporal discourse connectives, such as “before” or “until.” We define a linkage between two events as a triplet (e1, e2, `) ∈ (U × U × L), where L is the set of linkage types, defined according to their marker’s written form. §6 details the extraction process of linkages from recipes. We further include a special linkage type linear based on the order of events in the text, and consider every pair of events e1 and e2 that follow one another in a recipe as linked under this linkage type.\nFor each linkage type ` ∈ L, we define an empirical probability distribution P`((a, c1), (a′, c′1)) = P ((a, c1), (a′, c′1)|`), based on simple counting. The function PMI gives the point-wise mutual information of two events and is defined as:\nPMI`((a, c), (a′, c′)) = log ( P`((a, c1), (a′, c′1)) P`(a, c1) · P`(a′, c′1) )\nFrequency-based features encode the empirical estimate of the probabilities that various pairs of features would occur one after the other or linked with a discourse marker. They are equivalent to using probabilities extracted from maximum likelihood estima-\ntion according to a bigram model in the discriminative learning. While some of this information is implicitly found in the lexical features, collecting frequency counts from a large training set is much quicker than running costly structured optimization. Rather the discriminative training can weigh the different empirical probabilities according to their discriminative power. Indeed we find that these features are important in practice and can result in high accuracy even after training on a small training set."
  }, {
    "heading": "6 The Recipe Dataset",
    "text": "Data and Preprocessing. The data is extracted from a recipe repository found on the web.3 The recipes are given as free text. To extract event types we run the Stanford CoreNLP4 pipeline of a tokenizer, POS tagger, a lexical constituency parser (the englishPCFG parsing model) and extract typed Stanford dependencies (de Marneffe and Manning, 2008). As is common with web extractions, the recipes contain occasional spelling, grammatical and formatting errors. The corpus consists of 139 files, 73484 recipes, 1.02M events (13.8 events per recipe on average) and 11.05M words.5\nEvent Extraction. We focus on verbal events and do not extract nominal and adjectival argument structures, which are not as well supported by current parsing technology. Any verb is taken to define an event, aside from modal verbs, auxiliaries and secondary verbs. A secondary verb (e.g., “let,” “begin”) does not describe an action in its own right, but rather modifies an event introduced by another verb. We identify these verbs heuristically using a list given in Dixon (2005, p. 490–491) and a few simple rules defined over parse trees. E.g., from the sentence “you should begin to chop the onion,” we extract a single event with a predicate “chop.” Arguments are taken to be the immediate dependents of the predicate that have an argument dependency type (such as direct or indirect objects) according to the extracted Stanford dependencies. For prepositional phrases, we include the preposition as part of\n3 http://www.ffts.com/recipes.htm 4 http://nlp.stanford.edu/software/corenlp.shtml\n5Links to the original recipes, the preprocessed recipes and all extracted events can be found in http://homepages. inf.ed.ac.uk/oabend/event_order.html.\nthe argument. Argument indices are determined by their order in the text. The order of events is taken to be the order of their verbs in the text.\nLinkage Extraction. We focus on a subset of linkage relations, which are relevant for temporal relations. We use Pitler and Nenkova’s (2009) explicit discourse connectives classifier to identify temporal discourse linkers, discarding all other discourse linkers. Once a discourse linker has been detected, we heuristically extract its arguments (namely the pair of verbs it links) according to a deterministic extraction rule defined over the parse tree. We find 28 distinct connectives in our training set, where the 5 most common linkers “until,” “then,” “before,” “when” and “as” cover over 95% of the instances. We extract 36756 such linkages from the corpus, 0.5 linkages per recipe on average.\nTemporal and Textual Ordering. In order to confirm that temporal and textual order of recipes are generally in agreement, we manually examine the first 20 recipes in our development set. One recipe was excluded as noise6, resulting in 19 recipes and 353 events. We identify the sources of misalignment between the linear order and the temporal order of the events.7 13 events (3.7%) did not have any clear temporal orderings. These consisted of mostly negations and modalities (e.g., “do not overbrown!”), sub-section headings (e.g., “Preparation”) or other general statements that do not constitute actions or states. For the remaining 340 events, we compare their linear and the temporal orderings.\nWe estimate the frequency of sub-sequences that contradict the temporal order and confirm that they occur only infrequently. We find that most disagreements fall into these two categories: (1) disjunctions between several events, only one of which will actually take place (e.g., “roll Springerle pin over dough, or press mold into top”); (2) a pair, or less commonly a triplet, of events are expressed in reverse order. For instance, “place on greased and floured cookie sheet,” where greasing and flouring should occur before the placing action. We note that assuming the alignment of the temporal and textual order\n6This did not result from an extraction problem, but rather from the recipe text itself being too noisy to interpret.\n7Events are parsed manually so to avoid confounding the results with the parser’s performance.\nof recipes does not suggest that the textual order is the only order of events that would yield the same outcome.\nWe compute the Kendall’s Tau correlation, a standard measure for information ordering (Lapata, 2006), between the temporal and linear orderings for each recipe. In cases of several events that happen simultaneously (including disjunctions), we take their ordinals to be equal. For instance, for three events where the last two happen at the same time, we take their ordering to be (1,2,2) in our analysis. We find that indeed temporal and textual orderings are in very high agreement, with 6 recipes of the 19 perfectly aligned. The average Kendall’s Tau between the temporal ordering and the linear one is 0.924."
  }, {
    "heading": "7 Experimental Setup",
    "text": "Evaluation. We compute the accuracy of our algorithms by comparing the predicted order to the one in which the events are written. We first compute the number of exact matches, denoted with EXACT, namely the percentage of recipes in which the predicted and the textual orders are the same.\nFor a more detailed analysis of imperfect predictions, we compute the agreement between subsequences of the orderings. We borrow the notion of a “concordant pair” from the definition of Kendall’s Tau and generalize it to capture agreement of longer sub-sequences. Two k-tuples of integers (x1, ..., xk) and (y1, ..., yk) are said to “agree in order” if for every 1 ≤ i < j ≤ k, xi < xj iff yi < yj . Given two orderings of the same recipe O1 = (eτ(1), ..., eτ(m)) and O2 = (eσ(1), ..., eσ(m)) (where τ and σ are permutations over [m] = {1, . . . ,m}) and given a sequence of k monotonically increasing indices t = (i1, ..., ik), t is said to be a “concordant k-tuple” of O1 andO2 if (τ(i1), ..., τ(ik)) and (σ(i1), ..., σ(ik)) agree in order, as defined above.\nDenote the unordered recipes of the test data as {Ri}Ni=1, where Ri = {ei1, ..., eimi} ⊂ U for all i, and their target orderings Σ = {σi}Ni=1, where σi is a permutation over [mi]. Assume we wish to evaluate a set of predicted orderings for this test data T = {τi}Ni=1, where again τi is a permutation over [mi]. Denote the number of concordant k-tuples of σi and τi as conc(σi, τi). The total number of of\nmonotonically increasing k-tuples of indices is ( mi k ) . The k-wise (micro-averaged) accuracy of T with respect to Σ is:\nacck(Σ,T) = ∑N\ni=1 conc(σi, τi)∑N i=1 ( mi k ) Any k-tuples containing the start node s or the end node f are excluded, as their ordering is trivial. Recipes of length less than k are discarded when computing acck. A micro-averaged accuracy measure is used so as not to disproportionately weigh short recipes. However, in order to allow comparison to mean Kendall’s Tau, commonly used in works on order learning, we further report a macroaveraged acc2 by computing acc2 for each recipe separately, and taking the average of resulting accuracy levels. Average Kendall’s Tau can now be computed by 2acc2−1 for the macro-averaged acc2 score.\nData. We randomly partition the text into training, test and development sets, taking an 80-10-10 percent split. We do not partition the individual files so as to avoid statistical artifacts introduced by recipe duplications or near-duplications. The training, development and test sets contain 58038, 7667 and 7779 recipes respectively. The total number of feature template instantiations in the training data is 8.94M.\nBaselines and Algorithms. We compare three learning algorithms. GLOBAL-PRC is the structured perceptron algorithm that uses ILP inference. GREEDY-PRC is a structured perceptron in which inference is done greedily. GREEDY-LOGLIN is the locally normalized log-linear model with greedy inference. RANDOM randomly (uniformly) selects a permutation of the recipe’s events.\nExperimental Settings. The structured perceptron algorithms, GLOBAL-PRC and GREEDY-PRC, are run with a learning rate of 0.1 for 3 iterations. To avoid exceedingly long runs, we set a time limit in seconds β on the running time of each ILP inference stage used in GLOBAL-PRC. We consider two training scenarios: 4K, which trains on the first 4K recipes of the training set, and 58K, which trains on the full training data of 58K recipes. In GLOBAL-PRC we set β to be 30 seconds for the 4K"
  }, {
    "heading": "K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4",
    "text": "scenario, and 5 seconds in the 58K scenario. The number of threads was limited to 3. Where the time limit is reached before an optimal solution is found, the highest scoring Hamiltonian path found up to that point is returned by the ILP solver. In the infrequent samples where no feasible solution is found during training, the sample is skipped over, while at test time, we perform greedy inference instead.\nWe define the following feature sets. Fr includes only features of class Frequency, while Fr + Lex includes features from both the Frequency and Lexical categories. Full includes all feature sets. All above feature sets take C, the reference corpus for computing FREQUENCY features, to be the entire 58K training samples in both scenarios. In the 4K scenario, we also experiment with FrLim, which includes all features, but takes C to contain only the 4K samples of the training data.\nWe use the Gurobi package for ILP.8 Brown clusters are extracted from the 58K samples of the training data using Liang’s implementation.9 The convex log-likelihood function of GREEDY-LOGLIN is optimized using LBFGS. All features are selected and all parameters are tuned using the development set."
  }, {
    "heading": "8 Results",
    "text": "Table 2 presents the results of the three major algorithms in the two main scenarios 58K and 4K.\n8 http://www.gurobi.com 9 https://github.com/percyliang/brown-cluster\nWe find that the structured perceptron algorithm, GLOBAL-PRC, obtains the best results in both cases and under all evaluation measures. The importance of global optimization was also stressed in other works on event ordering (Chambers and Jurafsky, 2008a; Talukdar et al., 2012).\nIn order to assess the contribution of the different components of the model of the best scoring model, GLOBAL-PRC, we compare the performance of the different feature sets and settings of β on the development set in 4K (Table 3). Results reveal the strong impact of the Frequency feature set on the results. Using this category set alone (Fr) yields slightly lower results than using the full feature set, while estimating the Frequency features on a small corpus (FrLim) lowers results dramatically. Adding Lexical and Brown features yields a small improvement over using Frequency alone.\nWhile Table 3 demonstrates the importance of β in the performance of GLOBAL-PRC, it also shows that on a limited time budget, a small training set and few features (4K, Fr) and a reasonably small β (5) can yield competitive results. Increasing β from 5 to 30 generally improves results by 2 to 3 percent absolute. The importance of β is further demonstrated in Table 2, where performance with 4K training instances and β = 30 is better than with 58K training instances and β = 5. Preliminary experiments conducted on the development data with higher values of β of 60 and 120 suggest that further increasing β\nyields no further improvement. Previous studies evaluated their models on the related problem of distinguishing randomly permuted and correctly ordered chains of events (§2). In this paper we generalize this task to complete event ordering. In order to demonstrate the relative difficulty of the tasks, we apply our highest scoring model (4K, Fr + Le) to the binary task (without re-training it). We do so by computing the percentage of cases in which the correct ordering obtains a higher score than an average ordering. The high resulting accuracy of 93%, as opposed to considerably lower accuracies obtained under ordering evaluation measures, reflects the relative difficulty of the tasks.\nThe proposed edge-factored model can easily capture pair-wise ordering relations between events, but is more limited in accounting for relations between larger sets of events. A simple way of doing so is by adding the feature ∑ e P (ei|e)P (e|ej) between events ei and ej (in addition to the regular transition probabilities P (ei|ej)). However, preliminary experimentation with this technique did not yield improved performance. Future work will address higher-order models that straightforwardly account for such long-distance dependencies.\nTo qualitatively assess what generalizations are learned by the model, we apply GLOBAL-PRC to the development data and look at what event pairs obtained either particularly high or particularly low results. For each pair of predicates and their first arguments (a1,c11), (a\n2,c21), we compute the average weight of an edge connecting events of these types, discarding pairs of frequency less than 20.\nThe 20 highest scoring edges contain pairs such as (“add,” “mixing after addition”), (“beat whites,” “fold into mixture”) and (“cover for minutes,” “cook”), in addition to a few noisy pairs resulting from parser errors. The 20 lowest scoring edges contain event pairs that are likely to appear in the opposite order. 11 of the cases include as a first argument the predicates “serve,” “cool” or “chill,” which are likely to occur at the end of a recipe. 3 other edges linked duplications (e.g., (“reduce heat,” “reduce heat”)), which are indeed unlikely to immediately follow one another. These findings suggest the importance of detecting both lexical pairs that are unlikely to follow one another, in addition to those that are likely to."
  }, {
    "heading": "9 Conclusion",
    "text": "We addressed the problem of lexical event ordering, and developed an edge-factored model for tackling it. We rely on temporally aligned texts, using a new dataset of cooking recipes as a test case, thereby avoiding the need for costly and error-prone manual annotation. We present results of a pair-wise accuracy of over 70% using a basic set of features, and show the utility of the structured perceptron algorithm over simpler greedy and local approaches. The setup we explore, which uses a discriminative model and an ILP formulation, is easy to extend both in terms of features and in terms of more complex formal constraints and edge dependencies, as was done in graph-based dependency parsing (McDonald et al., 2005). Future work will address the extension of the feature set and model, and the application of this model to temporal semantics and planning tasks. We will further address the application of semi-supervised variants of the proposed techniques (e.g., self-training) to other domains, where no sizable corpora of temporally aligned data can be found."
  }, {
    "heading": "Acknowledgments",
    "text": "We would like to thank Nathan Schneider, Roy Schwartz, Bonnie Webber and the members of the Probmodels group at the University of Edinburgh for helpful comments. This work was supported by ERC Advanced Fellowship 249520 GRAMPLUS."
  }, {
    "heading": "Appendix A: Maximal Hamiltonian Path",
    "text": "Let G(S) = (S ∪ {s, f}, E(S)) be an almostcomplete directed graph with E = E(S) = (S ∪ {s}) × (S ∪ {f}). Let cij ∈ R be weights for its edges ((i, j) ∈ E). A Hamiltonian path between s, f ∈ V can be found by solving the following program, returning P = {(i, j)|xij = 1}.\nmax xij∈{0,1} : (i,j)∈E\nui∈Z : i∈V\nn∑ i6=j cijxij\ns.t. n∑\ni=0,i6=j xij = 1 ∀j 6= s; n∑ j=0,j 6=i xij = 1 ∀i 6= e;\nui − uj + |V |xij ≤ |V | − 1 ∀(i, j) ∈ E"
  }],
  "year": 2015,
  "references": [{
    "title": "Exploiting ontology lexica",
    "authors": ["Christina Unger"],
    "year": 2013
  }, {
    "title": "Discriminative training methods",
    "authors": ["Michael Collins"],
    "year": 2002
  }, {
    "title": "A Semantic Approach To English Grammar",
    "authors": ["Robert M.W. Dixon."],
    "venue": "Oxford University Press.",
    "year": 2005
  }, {
    "title": "Classifying temporal relations with rich linguistic knowledge",
    "authors": ["Jennifer D’Souza", "Vincent Ng"],
    "venue": "In NAACL-HLT",
    "year": 2013
  }, {
    "title": "A hierarchical bayesian model for unsupervised induction of script knowledge",
    "authors": ["Lea Frermann", "Ivan Titov", "Manfred Pinkal."],
    "venue": "EACL ’14, pages 49–57.",
    "year": 2014
  }, {
    "title": "Skip n-grams and ranking functions for predicting script events",
    "authors": ["Bram Jans", "Steven Bethard", "Ivan Vulić", "MarieFrancine Moens."],
    "venue": "EACL ’12, pages 336–344.",
    "year": 2012
  }, {
    "title": "Learning sentence-internal temporal relations",
    "authors": ["Maria Lapata", "Alex Lascarides."],
    "venue": "Journal of Artificial Intelligence Research (JAIR), 27:85–117.",
    "year": 2006
  }, {
    "title": "Probabilistic text structuring: Experiments with sentence ordering",
    "authors": ["Mirella Lapata."],
    "venue": "ACL ’03, pages 545–552.",
    "year": 2003
  }, {
    "title": "Automatic evaluation of information ordering: Kendall’s tau",
    "authors": ["Mirella Lapata."],
    "venue": "Computational Linguistics, 32(4):471–484.",
    "year": 2006
  }, {
    "title": "Temporal information extraction",
    "authors": ["Xiao Ling", "Daniel S Weld."],
    "venue": "AAAI ’10, pages 1385 – 1390.",
    "year": 2010
  }, {
    "title": "Cooking with semantics",
    "authors": ["Jonathan Malmaud", "Earl Wagner", "Nancy Chang", "Kevin Murphy."],
    "venue": "the ACL 2014 Workshop on Semantic Parsing, pages 33–38.",
    "year": 2014
  }, {
    "title": "Machine learning of temporal relations",
    "authors": ["Inderjeet Mani", "Marc Verhagen", "Ben Wellner", "Chong Min Lee", "James Pustejovsky."],
    "venue": "ACL-COLING ’06, pages 753–760.",
    "year": 2006
  }, {
    "title": "Learning a probabilistic model of event sequences from internet weblog stories",
    "authors": ["Mehdi Manshadi", "Reid Swanson", "Andrew S. Gordon."],
    "venue": "FLAIRS ’08, pages 159–164.",
    "year": 2008
  }, {
    "title": "Online large-margin training of dependency parsers",
    "authors": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."],
    "venue": "ACL ’05, pages 91–98.",
    "year": 2005
  }, {
    "title": "Classifying temporal relations with simple features",
    "authors": ["Paramita Mirza", "Sara Tonelli."],
    "venue": "EACL ’14, pages 308–317, Gothenburg, Sweden, April.",
    "year": 2014
  }, {
    "title": "Inducing neural models of script knowledge",
    "authors": ["Ashutosh Modi", "Ivan Titov."],
    "venue": "CoNLL ’14, pages 49–",
    "year": 2014
  }, {
    "title": "Learning STRIPS operators from noisy and incomplete observations",
    "authors": ["Kira Mourão", "Luke Zettlemoyer", "Ronald Petrick", "Mark Steedman."],
    "venue": "UAI ’12.",
    "year": 2012
  }, {
    "title": "Statistical script learning with multi-argument events",
    "authors": ["Karl Pichotta", "Raymond Mooney."],
    "venue": "EACL ’14, pages 220–229.",
    "year": 2014
  }, {
    "title": "Using syntax to disambiguate explicit discourse connectives in text",
    "authors": ["Emily Pitler", "Ani Nenkova."],
    "venue": "ACL-IJCNLP ’09: Short Papers, pages 13–16.",
    "year": 2009
  }, {
    "title": "TimeML: Robust specification of event and temporal expressions in text",
    "authors": ["James Pustejovsky", "José M Castano", "Robert Ingria", "Robert J Gaizauskas", "Andrea Setzer", "Graham Katz", "Dragomir R Radev."],
    "venue": "New directions in question answering, 3:28–34.",
    "year": 2003
  }, {
    "title": "Learning script knowledge with web experiments",
    "authors": ["Michaela Regneri", "Alexander Koller", "Manfred Pinkal."],
    "venue": "ACL ’10, pages 979–988.",
    "year": 2010
  }, {
    "title": "Grounding action descriptions in videos",
    "authors": ["Michaela Regneri", "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal."],
    "venue": "Transactions of the Association for Computational Linguistics (TACL), 1:25–36.",
    "year": 2013
  }, {
    "title": "Building natural language generation systems, volume 33",
    "authors": ["Ehud Reiter", "Robert Dale", "Zhiwei Feng."],
    "venue": "Cambridge: Cambridge university press.",
    "year": 2000
  }, {
    "title": "Global inference for entity and relation identification via a linear programming formulation",
    "authors": ["Dan Roth", "Wen-tau Yih."],
    "venue": "Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.",
    "year": 2007
  }, {
    "title": "Learning biological processes with global constraints",
    "authors": ["Aju Thalappillil Scaria", "Jonathan Berant", "Mengqiu Wang", "Christopher D Manning", "Justin Lewis", "Brittany Harding", "Peter Clark."],
    "venue": "EMNLP ’13.",
    "year": 2013
  }, {
    "title": "Plans, affordances, and combinatory grammar",
    "authors": ["Mark Steedman."],
    "venue": "Linguistics and Philosophy, 25(56):723–753.",
    "year": 2002
  }, {
    "title": "Acquiring temporal constraints between relations",
    "authors": ["Partha Pratim Talukdar", "Derry Wijaya", "Tom Mitchell."],
    "venue": "CIKM ’12, pages 992–1001.",
    "year": 2012
  }, {
    "title": "SOUR CREAM: Toward semantic processing of recipes",
    "authors": ["Dan Tasse", "Noah A Smith."],
    "venue": "Technical report, Technical Report CMU-LTI-08-005, Carnegie Mellon University, Pittsburgh, PA.",
    "year": 2008
  }, {
    "title": "Learning linear ordering problems for better translation",
    "authors": ["Roy Tromble", "Jason Eisner."],
    "venue": "EMNLP ’09, pages 1007–1016.",
    "year": 2009
  }, {
    "title": "Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations",
    "authors": ["Naushad UzZaman", "Hector Llorens", "Leon Derczynski", "James Allen", "Marc Verhagen", "James Pustejovsky."],
    "venue": "*SEM-SemEval ’13, pages 1–9.",
    "year": 2013
  }, {
    "title": "Jointly identifying temporal relations with markov logic",
    "authors": ["Katsumasa Yoshikawa", "Sebastian Riedel", "Masayuki Asahara", "Yuji Matsumoto."],
    "venue": "ACL-IJCNLP ’09, pages 405–413.",
    "year": 2009
  }],
  "id": "SP:261070cab14bb887bc3aeefa0d515027b95cf41b",
  "authors": [{
    "name": "Omri Abend",
    "affiliations": []
  }, {
    "name": "Shay B. Cohen",
    "affiliations": []
  }, {
    "name": "Mark Steedman",
    "affiliations": []
  }],
  "abstractText": "Extensive lexical knowledge is necessary for temporal analysis and planning tasks. We address in this paper a lexical setting that allows for the straightforward incorporation of rich features and structural constraints. We explore a lexical event ordering task, namely determining the likely temporal order of events based solely on the identity of their predicates and arguments. We propose an “edgefactored” model for the task that decomposes over the edges of the event graph. We learn it using the structured perceptron. As lexical tasks require large amounts of text, we do not attempt manual annotation and instead use the textual order of events in a domain where this order is aligned with their temporal order, namely cooking recipes.",
  "title": "Lexical Event Ordering with an Edge-Factored Model"
}