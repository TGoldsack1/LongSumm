{
  "sections": [{
    "text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1275–1284 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics\n1275\nWe present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR."
  }, {
    "heading": "1 Introduction",
    "text": "Neural machine translation (NMT) has proven to be powerful (Sutskever et al., 2014; Bahdanau et al., 2015). It is on-par, and in some cases, even surpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering. Despite their flexibility, most neural MT models translate sentences independently. Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a\nfew previous sentences, are neglected in sentencebased translation (Bawden et al., 2017).\nThere are only a handful of attempts to document-wide machine translation in statistical and neural MT camps. Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements. More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts. The latter two report deteriorated performance when using the target-side context.\nIn this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015). We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated. We conduct experiments on three language pairs: French-English, German-English and Estonian-English. The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR."
  }, {
    "heading": "2 Background",
    "text": ""
  }, {
    "heading": "2.1 Neural Machine Translation (NMT)",
    "text": "Our document NMT model is grounded on sentence-based NMT model (Bahdanau et al.,\n2015) which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.\nEncoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence: −→ hi = −−→ RNN( −→ h i−1,ES [xi]), ←− h i = ←−− RNN( ←− h i+1,ES [xi])\nwhere ES [xi] is embedding of the word xi from the embedding table ES of the source language, and −→ h i and ←− h i are the hidden states of the forward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) units. Each word in the source sentence is then represented by the concatenation of the corresponding bidirectional hidden states, hi = [ −→ h i; ←− h i].\nDecoder The generation of each word yj is conditioned on all of the previously generated words y<j via the state of the RNN decoder sj , and the source sentence via a dynamic context vector cj :\nyj ∼ softmax(Wy · rj + br) rj = tanh(sj +Wrc · cj +Wrj ·ET [yj−1]) sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj)\nwhere ET [yj ] is embedding of the word yj from the embedding table ET of the target language, and W matrices and br vector are the parameters. The dynamic context vector cj is computed via cj = ∑ i αjihi, where\nαj = softmax(aj) aji = v · tanh(Wae · hi +Wat · sj−1)\nThis is known as the attention mechanism which dynamically attends to relevant parts of the source necessary for generating the next target word."
  }, {
    "heading": "2.2 Memory Networks (MemNets)",
    "text": "Memory Networks (Weston et al., 2015) are a class of neural models that use external memories to perform inference based on long-range dependencies. A memory is a collection of vectors M = {m1, ..,mK} constituting the memory cells, where each cell mk may potentially correspond to a discrete object xk. The memory is equipped with a read and optionally a write operation. Given a query vector q, the output vector generated by reading from the memory is ∑|M | i=1 pimi, where pi represents the relevance of the query to the i-th memory cell p =\nsoftmax(qT ·M). For the rest of the paper, we denote the read operation by MemNet(M , q)."
  }, {
    "heading": "3 Document NMT as Structured Prediction",
    "text": "We formulate document-wide machine translation as a structured prediction problem. Given a set of sentences {x1, . . . ,x|d|} in a source document d, we are interested in generating the collection of their translations {y1, . . . ,y|d|} taking into account interdependencies among them imposed by the document. We achieve this by the factor graph in Figure 1 to model the probability of the target document given the source document. Our model has two types of factors:\n• fθ(yt;xt,x−t) to capture the interdependencies between the translation yt, the corresponding source sentence xt and all the other sentences in the source document x−t, and\n• gθ(yt;y−t) to capture the interdependencies between the translation yt and all the other translations in the document y−t.\nHence, the probability of a document translation given the source document is\nP (y1, . . . ,y|d||x1, . . . ,x|d|) ∝ exp (∑\nt\nfθ(yt;xt,x−t) + gθ(yt;y−t) ) .\nThe factors fθ and gθ are realised by neural architectures whose parameters are collectively denoted by θ.\nTraining It is challenging to train the model parameters by maximising the (regularised) likelihood since computing the partition function is hard. This is due to the enormity of factors\ngθ(yt;y−t) over a large number of translation variables yt’s (i.e., the number of sentences in the document) as well as their unbounded domain (i.e., all sentences in the target language). Thus, we resort to maximising the pseudo-likelihood (Besag, 1975) for training the parameters:\nargmax θ ∏ d∈D |d|∏ t=1 Pθ(yt|xt,y−t,x−t) (1)\nwhereD is the set of bilingual training documents, and |d| denotes the number of (bilingual) sentences in the document d = {(xt,yt)}|d|t=1. We directly model the document-conditioned NMT model Pθ(yt|xt,y−t,x−t) using a neural architecture which subsumes both the fθ and gθ factors (covered in the next section).\nDecoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem:\narg max y1,...,y|d| |d|∏ t=1 Pθ(yt|xt,y−t,x−t)\nwhich is hard (due to similar reasons as mentioned earlier). We hence resort to a block coordinate descent optimisation algorithm. More specifically, we initialise the translation of each sentence using the base neural MT model P (yt|xt). We then repeatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (yt|xt,y−t,x−t) while the translations of other sentences are kept fixed."
  }, {
    "heading": "4 Context Dependent NMT with MemNets",
    "text": "We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in Figure 2.\nOur model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model. However, it conditions the generation of a target word not only on the previously generated words and the current source sentence (as in the vanilla NMT model), but also on all the other source sentences of the document and their translations. That is, the\ngeneration process is as follows:\nPθ(yt|xt,y−t,x−t) = |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt,y−t,x−t)\n(2)\nwhere yt,j is the j-th word of the t-th target sentence, yt,<j are the previously generated words, and x−t and y−t are as introduced previously.\nOur model represents the source and target document contexts as external memories, and attends to relevant parts of these external memories when generating the translation of a sentence. Let M [x−t] and M [y−t] denote external memories representing the source and target document context, respectively. These contain memory cells corresponding to all sentences in the document except the t-th sentence (described shortly). Let ht and st be representations of the t-th source sentence and its current translation, from the encoder and decoder respectively. We make use of ht as the query to get the relevant context from the source external memory:\ncsrct = MemNet(M [x−t],ht)\nFurthermore, for the t-th sentence, we get the relevant information from the target context:\nctrgt = MemNet(M [y−t], st +Wat · ht)\nwhere the query consists of the representation of the translation st from the decoder endowed with that of the source sentence ht from the encoder to make the query robust to potential noises in the current translation and circumvent error propagation, and Wat projects the source representation into the hidden state space.\nNow that we have representations of the relevant source and target document contexts, Eq. 2 can be re-written as:\nPθ(yt|xt,y−t,x−t) = |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt, ctrgt , c src t )\n(3)\nMore specifically, the memory contexts csrct and ctrgt are incorporated into the NMT decoder as:\n• Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state:\nst,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] + Wsc · ct,j +Wsm · csrct +Wst · c trg t )\n• Memory-to-Output in which the memory contexts are incorporated in the output layer:\nyt,j ∼ softmax(Wy · rt,j +Wym · csrct + Wyt · ctrgt + br)\nwhere Wsm, Wst, Wym, and Wyt are the new parameter matrices. We use only the source, only the target, or both external memories as the additional conditioning contexts. Furthermore, we use either the Memory-to-Context or Memory-toOutput architectures for incorporating the document contexts. In the experiments, we will explore these different options to investigate the most effective combination. We now turn our attention to the construction of the external memories for the source and target sides of a document.\nThe Source Memory We make use of a hierarchical 2-level RNN architecture to construct the external memory of the source document. More specifically, we pass each sentence of the document through a sentence-level bidirectional RNN to get the representation of the sentence (by concatenating the last hidden states of the forward and backward RNNs). We then pass the sentence representations through a document-level bidirectional RNN to propagate sentences’ information across the document. We take the hidden states\nof the document-level bidirectional RNNs as the memory cells of the source external memory.\nThe source external memory is built once for each minibatch, and does not change throughout the document translation. To be able to fit the computational graph of the document NMT model within GPU memory limits, we pre-train the sentence-level bidirectional RNN using the language modelling training objective. However, the document-level bidirectional RNN is trained together with other parameters of the document NMT model by back-propagating the document translation training objective.\nThe Target Memory The memory cells of the target external memory represent the current translations of the document. Recall from the previous section that we use coordinate descent iteratively to update these translations. Let {y1, . . . ,y|d|} be the current translations, and let {s|y1|, . . . , s|y|d||} be the last states of the decoder when these translations were generated. We use these last decoder states as the cells of the external target memory. We could make use of hierarchical sentencedocument RNNs to transform the document translations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have resulted in error propagation. We will show in the experiments that our efficient target memory construction is indeed effective."
  }, {
    "heading": "5 Experiments and Analysis",
    "text": "Datasets. We conducted experiments on three language pairs: French-English, German-English and Estonian-English. Table 1 shows the statistics of the datasets used in our experiments. The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document. The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005). Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents. The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets. For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1https://wit3.fbk.eu/ 2http://www.statmt.org/europarl/ 3http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz\nand news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided.\nWe pre-processed all corpora to remove very short documents and those with missing translations. Out-of-vocabulary and rare words (frequency less than 5) are replaced by the <UNK> token, following Cohn et al. (2016).4\nEvaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations. We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p < 0.05, comparing to the baselines.\nImplementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016). For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively. The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder. GRUs are used instead of LSTMs to reduce the number of parameters in the main model. The RNN hidden dimensions and word embedding sizes are set to 512 in the translation and memory components, and the alignment dimension is set to 256 in the translation model.\nTraining We use a stage-wise method to train the variants of our document context NMT model. Firstly, we pre-train the Memory-toContext/Memory-to-Output models, setting their readings from the source and target memories to\n4We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have.\nthe zero vector. This effectively learns parameters associated with the underlying sentence-based NMT model, which is then used as initialisation when training all parameters in the second stage (including the ones from the first stage). For the first stage, we make use of stochastic gradient descent (SGD)5 with initial learning rate of 0.1 and a decay factor of 0.5 after the fourth epoch for a total of ten epochs. The convergence occurs in 6-8 epochs. For the second stage, we use SGD with an initial learning rate of 0.08 and a decay factor of 0.9 after the first epoch for a total of 15 epochs6. The best model is picked based on the dev-set perplexity. To avoid overfitting, we employ dropout with the rate 0.2 for the single memory model. For the dual memory model, we set dropout for Document RNN to 0.2 and for the encoder and decoder to 0.5. Mini-batching is used in both stages to speed up training. For the largest dataset, the document NMT model takes about 4.5 hours per epoch to train on a single P100 GPU, while the sentence-level model takes about 3 hours per epoch for the same settings.\nWhen training the document NMT model in the second stage, we need the target memory. One option would be to use the ground truth translations for building the memory. However, this may result in inferior training, since at the test time, the decoder iteratively updates the translation of sentences based on the noisy translations of other sentences (accessed via the target memory). Hence, while training the document NMT model, we construct the target memory from the translations generated by the pre-trained sentence-level model7. This effectively exposes the model to its potential test-time mistakes during the training time, resulting in more robust learned parameters."
  }, {
    "heading": "5.1 Main Results",
    "text": "We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or\n5In our initial experiments, we found SGD to be more effective than Adam/Adagrad; an observation also made by Bahar et al. (2017).\n6For the document NMT model training, we did some preliminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow Cohn et al. (2016).\n7We report results for two-pass decoding, i.e., we only update the translations once using the initial translations generated from the base model. We tried multiple passes of decoding at test-time but it was not helpful.\n(iii) both the source and target memories (SNMT+both mems). We compare these variants against the standard sentence-level NMT model (S-NMT). We also compare the source memory variants of our model to the local context-NMT models8 of Jean et al. (2017) and Wang et al. (2017), which use a few previous source sentences as context, added to the decoder hidden state (similar to our Memory-to-Context model).\nMemory-to-Context We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2). Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.\nWe further experiment to train the target memory variants using gold translations instead of the generated ones for German-English. This led to −0.16 and −0.25 decrease9 in the BLEU scores for the target-only and both-memory variants, which confirms the intuition of constructing the target memory by exposing the model to its noises during training time.\nMemory-to-Output From Table 2, we consistently see +.95/+1.00 BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan-\n8We implemented and trained the baseline local context models using the same hyperparameters and training procedure that we used for training our memory models.\n9Latter is statistically significant decrease w.r.t. the both memory model trained on generated target translations.\nguage pairs. For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best. For German→English, the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best. Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts. We attribute this to the large number of parameters in the latter architecture (Table 3) and limited amount of data.\nWe further experiment with more data for train-\nBLEU-1 Fr→En De→En Et→En\nNC-11NC-16\nJean et al. (2017) 52.8 30.6 39.2 51.9 Wang et al. (2017) 52.6 28.2 38.3 52.3 S-NMT 51.4 28.7 36.9 50.4\n+src mem 53.0 30.5 39.1 52.6 +both mems 53.5 33.1 41.3 53.2\nTable 5: Unigram BLEU for our Memory-to-Context Document NMT models vs. S-NMT and Source context NMT baselines. bold: Best performance.\ning the sentence-based NMT to investigate the extent to which document context is useful in this setting. We randomly choose an additional 300K German-English sentence pairs from WMT’14 data to train the base NMT model in stage 1. In stage 2, we use the same document corpus as before to train the document-level models. As seen from Figure 3, the document MT variants still benefit from the document context even when the base model is trained on a larger bilingual corpus. For the Memory-to-Context model, we see massive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the baseline. On the other hand, for the Memory-to-Output model, the target memory model’s METEOR score increases significantly by +1.09 compared to the baseline, slightly differing from the corresponding model using the smaller corpus (+1.2).\nLocal Source Context Models Table 4 shows comparison of our Memory-to-Context model variants to local source context-NMT models (Jean et al., 2017; Wang et al., 2017). For French→English, our source memory model is comparable to both baselines. For German→English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR. For Estonian→English, our model outperforms Jean et al. (2017). Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned). However, the other two context baselines have access to that information, yet our\nmodel’s performance is either better or quite close to those models. We also look into the unigram BLEU scores to see how much our global source memory variants lead to improvement at the word-level. From Table 5, it can be seen that our model’s performance is better than the baselines for majority of the cases. The S-NMT+both mems model gives the best results for all three language pairs, showing that leveraging both source and target document context is indeed beneficial for improving MT performance."
  }, {
    "heading": "5.2 Analysis",
    "text": "Using Global/Local Target Context We first investigate whether using a local target context would have been equally sufficient in comparison to our global target memory model for the three datasets. We condition the decoder on the previous target sentence representation (obtained from the last hidden state of the decoder) by adding it as an additional input to all decoder states (PrevTrg) similar to our Memory-to-Context model. From Table 6, we observe that for French→English and Estonian→English, using all sentences in the target context or just the previous target sentence gives comparable results. We may attribute this to these specific datasets, that is documents from TED talks or European Parliament Proceedings may depend more on the local than on the global context. However, for German→English (NC-11), the target memory model performs the best show-\ning that for documents with richer context (e.g. news articles) we do need the global target document context to improve MT performance.\nOutput Analysis To better understand the dual memory model, we look at the first sentence example in Table 7. It can be seen that the source sentence has the noun “Qimonda” but the sentencelevel NMT model fails to attend to it when generating the translation. On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model’s translation quality surpasses them. This is because the word “Qimonda” was being repeated in this specific document, providing a strong contextual signal to our global document context model while the local context model by Wang et al. (2017) is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.\nWe resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronominal anaphora. By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of Table 7. Here the topic of the sentence is “the country under the dictatorship of Lukashenko” and our target and dual memory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word ‘diktatuur’, hence producing much better translation as compared to both baselines. Apart from these improvements, our models are better in improving the readability of sentences by generating more context appropriate grammatical structures such as verbs and adverbs.\nFurthermore, to validate that our model improves the consistency of translations, we look at five documents (roughly 70 sentences) from the test set of Estonian-English, each of which had a word being repeated in the gold translation. Our model is able to resolve the consistency in 22 out of 32 cases as compared to the sentencebased model which only accurately translates 16 of those. Following Wang et al. (2017), we also investigate the extent to which our model can correct errors made by the baseline system. We randomly choose five documents from the test set. Out of the 20 words/phrases which were incorrectly translated by the sentence-based model, our\nmodel corrects 85% of them while also generating 10% new errors."
  }, {
    "heading": "6 Related Work",
    "text": "Document-level Statistical MT There have been a few SMT-based attempts to document MT, but they are either restrictive or do not lead to significant improvements. Hardmeier and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns. Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation. Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model.\nDocent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing. Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent. In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations. The proposed methods fail to yield improvements upon automatic evaluation.\nLarger Context Neural MT Jean et al. (2017)\nextend the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its words. Extending their model to consider the global source document context would be challenging due to the large size of computation graph over all the words in the source document. Wang et al. (2017) employ a 2-level hierarichal RNN to summarise three previous source sentences, which is then used as an additional input to the decoder hidden state. Bawden et al. (2017) use multi-encoder NMT models to exploit context from the previous source and target sentence. They highlight the importance of targetside context but report deteriorated BLEU scores when using it. All these works consider a very local source/target context and completely ignore the global source and target document contexts."
  }, {
    "heading": "7 Conclusion",
    "text": "We have proposed a document-level neural MT model that captures global source and target document context. Our model augments the vanilla sentence-based NMT model with external memories to incorporate documental interdependencies on both source and target sides. We show statistically significant improvements of the translation quality on three language pairs. For future work, we intend to investigate models which incorporate specific discourse-level phenomena."
  }, {
    "heading": "Acknowledgments",
    "text": "The authors are grateful to André Martins and the anonymous reviewers for their helpful comments and corrections. This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686."
  }],
  "year": 2018,
  "references": [{
    "title": "Empirical investigation of optimization algorithms in neural machine translation",
    "authors": ["Parnia Bahar", "Tamer Alkhouli", "Jan-Thorsten Peter", "Christopher Jan-Steffen Brix", "Hermann Ney."],
    "venue": "Conference of the European Association for Machine Transla-",
    "year": 2017
  }, {
    "title": "Neural machine translation by jointly learning to align and translate",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."],
    "venue": "Proceedings of",
    "year": 2015
  }, {
    "title": "Evaluating discourse phenomena in neural machine translation",
    "authors": ["Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow."],
    "venue": "arXiv:1711.00513.",
    "year": 2017
  }, {
    "title": "Statistical analysis of non-lattice data",
    "authors": ["Julian Besag."],
    "venue": "Journal of the Royal Statistical Society. Series D (The Statistician), 24(3):179–195.",
    "year": 1975
  }, {
    "title": "WIT3: Web inventory of transcribed and translated talks",
    "authors": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."],
    "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation, pages 261–268.",
    "year": 2012
  }, {
    "title": "On the properties of neural machine translation: Encoder-decoder approaches",
    "authors": ["Kyunghyun Cho", "B van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."],
    "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8).",
    "year": 2014
  }, {
    "title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability",
    "authors": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."],
    "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational",
    "year": 2011
  }, {
    "title": "Incorporating structural alignment biases into an attentional neural translation model",
    "authors": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."],
    "venue": "Proceedings of the North American Chapter of",
    "year": 2016
  }, {
    "title": "Using word embeddings to enforce document-level lexical consistency in machine translation",
    "authors": ["Eva Martı́nez Garcia", "Carles Creus", "Cristina EspañaBonet", "Lluı́s Màrquez"],
    "year": 2017
  }, {
    "title": "Document-level machine translation as a re-translation process",
    "authors": ["Eva Martı́nez Garcia", "Cristina España-Bonet", "Lluı́s Màrquez"],
    "venue": "Procesamiento del Lenguaje Natural,",
    "year": 2014
  }, {
    "title": "Document-level machine translation with word vector models",
    "authors": ["Eva Martı́nez Garcia", "Cristina España-Bonet", "Lluı́s Màrquez"],
    "venue": "In Proceedings of the18th Conference of the European Association for Machine Translation,",
    "year": 2015
  }, {
    "title": "Cache-based document-level statistical machine translation",
    "authors": ["Zhengxian Gong", "Min Zhang", "Guodong Zhou."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 909–919. Association for Computa-",
    "year": 2011
  }, {
    "title": "Modelling pronominal anaphora in statistical machine translation",
    "authors": ["Christian Hardmeier", "Marcello Federico."],
    "venue": "International Workshop on Spoken Language Translation, pages 283–289.",
    "year": 2010
  }, {
    "title": "Document-wide decoding for phrasebased statistical machine translation",
    "authors": ["Christian Hardmeier", "Joakim Nivre", "Jörg Tiedemann."],
    "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
    "year": 2012
  }, {
    "title": "Docent: A document-level decoder for phrase-based statistical machine translation",
    "authors": ["Christian Hardmeier", "Sara Stymne", "Jörg Tiedemann", "Joakim Nivre."],
    "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguis-",
    "year": 2013
  }, {
    "title": "Long short-term memory",
    "authors": ["Sepp Hochreiter", "Jürgen Schmidhuber."],
    "venue": "Neural Comput., 9(8):1735– 1780.",
    "year": 1997
  }, {
    "title": "Does neural machine translation benefit from larger context",
    "authors": ["Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho"],
    "year": 2017
  }, {
    "title": "Europarl: A parallel corpus for statistical machine translation",
    "authors": ["Philipp Koehn."],
    "venue": "Conference Proceedings: the 10th Machine Translation Summit, pages 79–86. AAMT.",
    "year": 2005
  }, {
    "title": "Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments",
    "authors": ["Alon Lavie", "Abhaya Agarwal."],
    "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 228–231, Strouds-",
    "year": 2007
  }, {
    "title": "Effective approaches to attentionbased neural machine translation",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1412–1421. Asso-",
    "year": 2015
  }, {
    "title": "Dynet: The dynamic neural network toolkit",
    "authors": ["Swayamdipta", "Pengcheng Yin."],
    "venue": "arXiv preprint arXiv:1701.03980.",
    "year": 2017
  }, {
    "title": "BLEU: A method for automatic evaluation of machine translation",
    "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."],
    "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318. Association",
    "year": 2002
  }, {
    "title": "Neural machine translation of rare words with subword units",
    "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."],
    "venue": "Proceedings of the 54 Annual Meeting of the Association for Computational Linguistics, pages 1715–1725.",
    "year": 2016
  }, {
    "title": "Dirt cheap web-scale parallel text from the common crawl",
    "authors": ["Jason R. Smith", "Herve Saint-Amand", "Chris CallisonBurch", "Magdalena Plamada", "Adam Lopez."],
    "venue": "Proceedings of the Conference of the Association for Computational Linguistics.",
    "year": 2013
  }, {
    "title": "End-to-end memory networks",
    "authors": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."],
    "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, pages 2440–2448. MIT Press.",
    "year": 2015
  }, {
    "title": "Sequence to sequence learning with neural networks",
    "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."],
    "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, pages 3104–3112. MIT Press.",
    "year": 2014
  }, {
    "title": "Exploiting cross-sentence context for neural machine translation",
    "authors": ["Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu."],
    "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2816–2821. Association",
    "year": 2017
  }, {
    "title": "Memory networks",
    "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."],
    "venue": "Proceedings of the International Conference on Learning Representations.",
    "year": 2015
  }],
  "id": "SP:5b187f8b80fa7d7e3c5363852ea0ea00971686b6",
  "authors": [{
    "name": "Sameen Maruf",
    "affiliations": []
  }, {
    "name": "Gholamreza Haffari",
    "affiliations": []
  }],
  "abstractText": "We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",
  "title": "Document Context Neural Machine Translation with Memory Networks"
}